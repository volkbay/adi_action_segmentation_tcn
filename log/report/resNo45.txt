Wed Oct 19 15:27:44 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.2, 0.25, 0.2, 0.25, 0.08], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'tossFirstLastFrames': True}, 'dataPath': '/data/processed/Kinetics/', 'dropoutRate': 0.5, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat'], 'lastLayerInitUniform': True, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 70.33639143730886, 'maxValidationTrainNo': 39, 'modelVersion': 10, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 45, 'validationAccThr': 70, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Train set length:  2547
I - Label distribution: [697. 578. 734. 538.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/test
I - Test set length:  327
I - Label distribution: [88. 78. 75. 86.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(2) data number in dataset:  tensor([ 37546, 122878, 184405,  47364,  47595, 193833, 157086,  94344, 160897,
        142097,  83666, 101362, 158025, 212431, 189929, 135122])
I - Initializing model TCNv10
I - Number of Model Parameters: 91852
I - Model output shape:  torch.Size([16, 4])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv10                                   [16, 4]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 64, 64, 64]          6
│    └─ReLU: 2-1071                      [16, 64, 64, 64]          --
│    └─Conv2d: 2-2                       --                        3,136
│    └─BatchNorm2d: 2-1069               [16, 64, 64, 64]          --
│    └─OutputShiftSqueeze: 2-4           --                        --
│    └─One: 2-5                          [1]                       --
│    └─Scaler: 2-1070                    [16, 64, 64, 64]          --
│    └─OutputScale: 2-7                  --                        --
│    └─Empty: 2-8                        [64, 48, 1, 1]            --
│    └─Empty: 2-9                        [64, 48, 1, 1]            --
│    └─Empty: 2-10                       [64]                      --
│    └─Empty: 2-11                       [64]                      --
│    └─BatchNorm2d: 2-12                 [16, 64, 64, 64]          --
│    └─Scaler: 2-13                      [16, 64, 64, 64]          --
│    └─ReLU: 2-14                        [16, 64, 64, 64]          --
│    └─Empty: 2-15                       [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-80         [16, 64, 32, 32]          (recursive)
│    └─ReLU: 2-1086                      [16, 64, 32, 32]          --
│    └─MaxPool2d: 2-1074                 [16, 64, 32, 32]          --
│    └─Conv2d: 2-18                      --                        36,928
│    └─BatchNorm2d: 2-1084               [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-20                       [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-3          [16, 64, 32, 32]          36,934
│    └─Scaler: 2-1085                    [16, 64, 32, 32]          --
│    └─MaxPool2d: 2-22                   [16, 64, 32, 32]          --
│    └─Empty: 2-23                       [16, 64, 32, 32]          --
│    └─Empty: 2-24                       [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [64, 64, 3, 3]            --
│    └─Empty: 2-29                       [64, 64, 3, 3]            --
│    └─Empty: 2-30                       [64]                      --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 64, 16, 16]          (recursive)
│    └─ReLU: 2-1101                      [16, 64, 16, 16]          --
│    └─MaxPool2d: 2-1089                 [16, 64, 16, 16]          --
│    └─Conv2d: 2-33                      --                        36,928
│    └─BatchNorm2d: 2-1099               [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-35                       [64]                      --
│    └─BatchNorm2d: 2-36                 [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1100                    [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-38                      [16, 64, 32, 32]          --
│    └─ReLU: 2-39                        [16, 64, 32, 32]          --
│    └─Empty: 2-40                       [16, 64, 32, 32]          --
│    └─Clamp: 2-41                       [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-5          [16, 64, 16, 16]          36,674
│    └─MaxPool2d: 2-42                   [16, 64, 16, 16]          --
│    └─Empty: 2-43                       [16, 64, 16, 16]          --
│    └─Empty: 2-1090                     [16, 64, 16, 16]          --
│    └─Empty: 2-1091                     [16, 64, 16, 16]          --
│    └─Empty: 2-46                       [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1113                      [16, 4, 16, 16]           --
│    └─Conv2d: 2-48                      --                        260
│    └─BatchNorm2d: 2-1111               [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputShiftSqueeze: 2-50          --                        --
│    └─One: 2-51                         [1]                       --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1112                    [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-53                 --                        --
│    └─Empty: 2-54                       [64, 64, 3, 3]            --
│    └─Empty: 2-55                       [64, 64, 3, 3]            --
│    └─Empty: 2-56                       [64]                      --
│    └─Empty: 2-57                       [64]                      --
│    └─BatchNorm2d: 2-58                 [16, 64, 16, 16]          --
│    └─Scaler: 2-59                      [16, 64, 16, 16]          --
│    └─ReLU: 2-60                        [16, 64, 16, 16]          --
│    └─Empty: 2-61                       [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-83         [16, 4, 16, 16]           (recursive)
│    └─ReLU: 2-1128                      [16, 4, 16, 16]           --
│    └─MaxPool2d: 2-1116                 [16, 64, 16, 16]          --
│    └─Conv2d: 2-64                      --                        2,308
│    └─BatchNorm2d: 2-1126               [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Clamp: 2-66                       [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-7                 [16, 4, 16, 16]           266
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1127                    [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-68          --                        --
│    └─One: 2-69                         [1]                       --
│    └─OutputScale: 2-70                 --                        --
│    └─Empty: 2-71                       [4, 64, 1, 1]             --
│    └─Empty: 2-72                       [4, 64, 1, 1]             --
│    └─Empty: 2-73                       [4]                       --
│    └─Empty: 2-74                       [4]                       --
│    └─BatchNorm2d: 2-75                 [16, 4, 16, 16]           --
│    └─Scaler: 2-76                      [16, 4, 16, 16]           --
│    └─ReLU: 2-77                        [16, 4, 16, 16]           --
│    └─Empty: 2-78                       [16, 4, 16, 16]           --
│    └─Clamp: 2-79                       [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-8          [16, 4, 16, 16]           2,314
│    └─MaxPool2d: 2-80                   [16, 64, 16, 16]          --
├─Conv1d: 1                              --                        --
│    └─Scaler: 2-1138                    [16, 4, 14]               --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-82                       [16, 64, 16, 16]          --
│    └─Empty: 2-83                       [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-84          --                        --
│    └─One: 2-85                         [1]                       --
│    └─OutputScale: 2-86                 --                        --
│    └─Empty: 2-87                       [4, 64, 3, 3]             --
│    └─Empty: 2-88                       [4, 64, 3, 3]             --
│    └─Empty: 2-89                       [4]                       --
│    └─Empty: 2-90                       [4]                       --
│    └─BatchNorm2d: 2-91                 [16, 4, 16, 16]           --
│    └─Scaler: 2-92                      [16, 4, 16, 16]           --
│    └─ReLU: 2-93                        [16, 4, 16, 16]           --
│    └─Empty: 2-94                       [16, 4, 16, 16]           --
│    └─Clamp: 2-95                       [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-9                 [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-96          --                        --
│    └─One: 2-97                         [1]                       --
│    └─OutputScale: 2-98                 --                        --
│    └─Empty: 2-99                       [64, 48, 1, 1]            --
│    └─Empty: 2-100                      [64, 48, 1, 1]            --
│    └─Empty: 2-101                      [64]                      --
│    └─Empty: 2-102                      [64]                      --
│    └─BatchNorm2d: 2-103                [16, 64, 64, 64]          --
│    └─Scaler: 2-104                     [16, 64, 64, 64]          --
│    └─ReLU: 2-105                       [16, 64, 64, 64]          --
│    └─Empty: 2-106                      [16, 64, 64, 64]          --
│    └─Clamp: 2-107                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-10         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-108                  [16, 64, 32, 32]          --
│    └─Empty: 2-109                      [16, 64, 32, 32]          --
│    └─Empty: 2-110                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-111         --                        --
│    └─One: 2-112                        [1]                       --
│    └─OutputScale: 2-113                --                        --
│    └─Empty: 2-114                      [64, 64, 3, 3]            --
│    └─Empty: 2-115                      [64, 64, 3, 3]            --
│    └─Empty: 2-116                      [64]                      --
│    └─Empty: 2-117                      [64]                      --
│    └─BatchNorm2d: 2-118                [16, 64, 32, 32]          --
│    └─Scaler: 2-119                     [16, 64, 32, 32]          --
│    └─ReLU: 2-120                       [16, 64, 32, 32]          --
│    └─Empty: 2-121                      [16, 64, 32, 32]          --
│    └─Clamp: 2-122                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-11         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-123                  [16, 64, 16, 16]          --
│    └─Empty: 2-124                      [16, 64, 16, 16]          --
│    └─Empty: 2-125                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-126         --                        --
│    └─One: 2-127                        [1]                       --
│    └─OutputScale: 2-128                --                        --
│    └─Empty: 2-129                      [64, 64, 3, 3]            --
│    └─Empty: 2-130                      [64, 64, 3, 3]            --
│    └─Empty: 2-131                      [64]                      --
│    └─Empty: 2-132                      [64]                      --
│    └─BatchNorm2d: 2-133                [16, 64, 16, 16]          --
│    └─Scaler: 2-134                     [16, 64, 16, 16]          --
│    └─ReLU: 2-135                       [16, 64, 16, 16]          --
│    └─Empty: 2-136                      [16, 64, 16, 16]          --
│    └─Clamp: 2-137                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-12                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-138         --                        --
│    └─One: 2-139                        [1]                       --
│    └─OutputScale: 2-140                --                        --
│    └─Empty: 2-141                      [4, 64, 1, 1]             --
│    └─Empty: 2-142                      [4, 64, 1, 1]             --
│    └─Empty: 2-143                      [4]                       --
│    └─Empty: 2-144                      [4]                       --
│    └─BatchNorm2d: 2-145                [16, 4, 16, 16]           --
│    └─Scaler: 2-146                     [16, 4, 16, 16]           --
│    └─ReLU: 2-147                       [16, 4, 16, 16]           --
│    └─Empty: 2-148                      [16, 4, 16, 16]           --
│    └─Clamp: 2-149                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-13         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-150                  [16, 64, 16, 16]          --
│    └─Empty: 2-151                      [16, 64, 16, 16]          --
│    └─Empty: 2-152                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-153         --                        --
│    └─One: 2-154                        [1]                       --
│    └─OutputScale: 2-155                --                        --
│    └─Empty: 2-156                      [4, 64, 3, 3]             --
│    └─Empty: 2-157                      [4, 64, 3, 3]             --
│    └─Empty: 2-158                      [4]                       --
│    └─Empty: 2-159                      [4]                       --
│    └─BatchNorm2d: 2-160                [16, 4, 16, 16]           --
│    └─Scaler: 2-161                     [16, 4, 16, 16]           --
│    └─ReLU: 2-162                       [16, 4, 16, 16]           --
│    └─Empty: 2-163                      [16, 4, 16, 16]           --
│    └─Clamp: 2-164                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-14                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-165         --                        --
│    └─One: 2-166                        [1]                       --
│    └─OutputScale: 2-167                --                        --
│    └─Empty: 2-168                      [64, 48, 1, 1]            --
│    └─Empty: 2-169                      [64, 48, 1, 1]            --
│    └─Empty: 2-170                      [64]                      --
│    └─Empty: 2-171                      [64]                      --
│    └─BatchNorm2d: 2-172                [16, 64, 64, 64]          --
│    └─Scaler: 2-173                     [16, 64, 64, 64]          --
│    └─ReLU: 2-174                       [16, 64, 64, 64]          --
│    └─Empty: 2-175                      [16, 64, 64, 64]          --
│    └─Clamp: 2-176                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-177                  [16, 64, 32, 32]          --
│    └─Empty: 2-178                      [16, 64, 32, 32]          --
│    └─Empty: 2-179                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-180         --                        --
│    └─One: 2-181                        [1]                       --
│    └─OutputScale: 2-182                --                        --
│    └─Empty: 2-183                      [64, 64, 3, 3]            --
│    └─Empty: 2-184                      [64, 64, 3, 3]            --
│    └─Empty: 2-185                      [64]                      --
│    └─Empty: 2-186                      [64]                      --
│    └─BatchNorm2d: 2-187                [16, 64, 32, 32]          --
│    └─Scaler: 2-188                     [16, 64, 32, 32]          --
│    └─ReLU: 2-189                       [16, 64, 32, 32]          --
│    └─Empty: 2-190                      [16, 64, 32, 32]          --
│    └─Clamp: 2-191                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-16         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-192                  [16, 64, 16, 16]          --
│    └─Empty: 2-193                      [16, 64, 16, 16]          --
│    └─Empty: 2-194                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-195         --                        --
│    └─One: 2-196                        [1]                       --
│    └─OutputScale: 2-197                --                        --
│    └─Empty: 2-198                      [64, 64, 3, 3]            --
│    └─Empty: 2-199                      [64, 64, 3, 3]            --
│    └─Empty: 2-200                      [64]                      --
│    └─Empty: 2-201                      [64]                      --
│    └─BatchNorm2d: 2-202                [16, 64, 16, 16]          --
│    └─Scaler: 2-203                     [16, 64, 16, 16]          --
│    └─ReLU: 2-204                       [16, 64, 16, 16]          --
│    └─Empty: 2-205                      [16, 64, 16, 16]          --
│    └─Clamp: 2-206                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-17                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-207         --                        --
│    └─One: 2-208                        [1]                       --
│    └─OutputScale: 2-209                --                        --
│    └─Empty: 2-210                      [4, 64, 1, 1]             --
│    └─Empty: 2-211                      [4, 64, 1, 1]             --
│    └─Empty: 2-212                      [4]                       --
│    └─Empty: 2-213                      [4]                       --
│    └─BatchNorm2d: 2-214                [16, 4, 16, 16]           --
│    └─Scaler: 2-215                     [16, 4, 16, 16]           --
│    └─ReLU: 2-216                       [16, 4, 16, 16]           --
│    └─Empty: 2-217                      [16, 4, 16, 16]           --
│    └─Clamp: 2-218                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-18         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-219                  [16, 64, 16, 16]          --
│    └─Empty: 2-220                      [16, 64, 16, 16]          --
│    └─Empty: 2-221                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-222         --                        --
│    └─One: 2-223                        [1]                       --
│    └─OutputScale: 2-224                --                        --
│    └─Empty: 2-225                      [4, 64, 3, 3]             --
│    └─Empty: 2-226                      [4, 64, 3, 3]             --
│    └─Empty: 2-227                      [4]                       --
│    └─Empty: 2-228                      [4]                       --
│    └─BatchNorm2d: 2-229                [16, 4, 16, 16]           --
│    └─Scaler: 2-230                     [16, 4, 16, 16]           --
│    └─ReLU: 2-231                       [16, 4, 16, 16]           --
│    └─Empty: 2-232                      [16, 4, 16, 16]           --
│    └─Clamp: 2-233                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-19                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-234         --                        --
│    └─One: 2-235                        [1]                       --
│    └─OutputScale: 2-236                --                        --
│    └─Empty: 2-237                      [64, 48, 1, 1]            --
│    └─Empty: 2-238                      [64, 48, 1, 1]            --
│    └─Empty: 2-239                      [64]                      --
│    └─Empty: 2-240                      [64]                      --
│    └─BatchNorm2d: 2-241                [16, 64, 64, 64]          --
│    └─Scaler: 2-242                     [16, 64, 64, 64]          --
│    └─ReLU: 2-243                       [16, 64, 64, 64]          --
│    └─Empty: 2-244                      [16, 64, 64, 64]          --
│    └─Clamp: 2-245                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-20         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-246                  [16, 64, 32, 32]          --
│    └─Empty: 2-247                      [16, 64, 32, 32]          --
│    └─Empty: 2-248                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-249         --                        --
│    └─One: 2-250                        [1]                       --
│    └─OutputScale: 2-251                --                        --
│    └─Empty: 2-252                      [64, 64, 3, 3]            --
│    └─Empty: 2-253                      [64, 64, 3, 3]            --
│    └─Empty: 2-254                      [64]                      --
│    └─Empty: 2-255                      [64]                      --
│    └─BatchNorm2d: 2-256                [16, 64, 32, 32]          --
│    └─Scaler: 2-257                     [16, 64, 32, 32]          --
│    └─ReLU: 2-258                       [16, 64, 32, 32]          --
│    └─Empty: 2-259                      [16, 64, 32, 32]          --
│    └─Clamp: 2-260                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-21         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-261                  [16, 64, 16, 16]          --
│    └─Empty: 2-262                      [16, 64, 16, 16]          --
│    └─Empty: 2-263                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-264         --                        --
│    └─One: 2-265                        [1]                       --
│    └─OutputScale: 2-266                --                        --
│    └─Empty: 2-267                      [64, 64, 3, 3]            --
│    └─Empty: 2-268                      [64, 64, 3, 3]            --
│    └─Empty: 2-269                      [64]                      --
│    └─Empty: 2-270                      [64]                      --
│    └─BatchNorm2d: 2-271                [16, 64, 16, 16]          --
│    └─Scaler: 2-272                     [16, 64, 16, 16]          --
│    └─ReLU: 2-273                       [16, 64, 16, 16]          --
│    └─Empty: 2-274                      [16, 64, 16, 16]          --
│    └─Clamp: 2-275                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-22                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-276         --                        --
│    └─One: 2-277                        [1]                       --
│    └─OutputScale: 2-278                --                        --
│    └─Empty: 2-279                      [4, 64, 1, 1]             --
│    └─Empty: 2-280                      [4, 64, 1, 1]             --
│    └─Empty: 2-281                      [4]                       --
│    └─Empty: 2-282                      [4]                       --
│    └─BatchNorm2d: 2-283                [16, 4, 16, 16]           --
│    └─Scaler: 2-284                     [16, 4, 16, 16]           --
│    └─ReLU: 2-285                       [16, 4, 16, 16]           --
│    └─Empty: 2-286                      [16, 4, 16, 16]           --
│    └─Clamp: 2-287                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-23         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-288                  [16, 64, 16, 16]          --
│    └─Empty: 2-289                      [16, 64, 16, 16]          --
│    └─Empty: 2-290                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-291         --                        --
│    └─One: 2-292                        [1]                       --
│    └─OutputScale: 2-293                --                        --
│    └─Empty: 2-294                      [4, 64, 3, 3]             --
│    └─Empty: 2-295                      [4, 64, 3, 3]             --
│    └─Empty: 2-296                      [4]                       --
│    └─Empty: 2-297                      [4]                       --
│    └─BatchNorm2d: 2-298                [16, 4, 16, 16]           --
│    └─Scaler: 2-299                     [16, 4, 16, 16]           --
│    └─ReLU: 2-300                       [16, 4, 16, 16]           --
│    └─Empty: 2-301                      [16, 4, 16, 16]           --
│    └─Clamp: 2-302                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-24                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-303         --                        --
│    └─One: 2-304                        [1]                       --
│    └─OutputScale: 2-305                --                        --
│    └─Empty: 2-306                      [64, 48, 1, 1]            --
│    └─Empty: 2-307                      [64, 48, 1, 1]            --
│    └─Empty: 2-308                      [64]                      --
│    └─Empty: 2-309                      [64]                      --
│    └─BatchNorm2d: 2-310                [16, 64, 64, 64]          --
│    └─Scaler: 2-311                     [16, 64, 64, 64]          --
│    └─ReLU: 2-312                       [16, 64, 64, 64]          --
│    └─Empty: 2-313                      [16, 64, 64, 64]          --
│    └─Clamp: 2-314                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-25         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-315                  [16, 64, 32, 32]          --
│    └─Empty: 2-316                      [16, 64, 32, 32]          --
│    └─Empty: 2-317                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-318         --                        --
│    └─One: 2-319                        [1]                       --
│    └─OutputScale: 2-320                --                        --
│    └─Empty: 2-321                      [64, 64, 3, 3]            --
│    └─Empty: 2-322                      [64, 64, 3, 3]            --
│    └─Empty: 2-323                      [64]                      --
│    └─Empty: 2-324                      [64]                      --
│    └─BatchNorm2d: 2-325                [16, 64, 32, 32]          --
│    └─Scaler: 2-326                     [16, 64, 32, 32]          --
│    └─ReLU: 2-327                       [16, 64, 32, 32]          --
│    └─Empty: 2-328                      [16, 64, 32, 32]          --
│    └─Clamp: 2-329                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-330                  [16, 64, 16, 16]          --
│    └─Empty: 2-331                      [16, 64, 16, 16]          --
│    └─Empty: 2-332                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-333         --                        --
│    └─One: 2-334                        [1]                       --
│    └─OutputScale: 2-335                --                        --
│    └─Empty: 2-336                      [64, 64, 3, 3]            --
│    └─Empty: 2-337                      [64, 64, 3, 3]            --
│    └─Empty: 2-338                      [64]                      --
│    └─Empty: 2-339                      [64]                      --
│    └─BatchNorm2d: 2-340                [16, 64, 16, 16]          --
│    └─Scaler: 2-341                     [16, 64, 16, 16]          --
│    └─ReLU: 2-342                       [16, 64, 16, 16]          --
│    └─Empty: 2-343                      [16, 64, 16, 16]          --
│    └─Clamp: 2-344                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-27                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-345         --                        --
│    └─One: 2-346                        [1]                       --
│    └─OutputScale: 2-347                --                        --
│    └─Empty: 2-348                      [4, 64, 1, 1]             --
│    └─Empty: 2-349                      [4, 64, 1, 1]             --
│    └─Empty: 2-350                      [4]                       --
│    └─Empty: 2-351                      [4]                       --
│    └─BatchNorm2d: 2-352                [16, 4, 16, 16]           --
│    └─Scaler: 2-353                     [16, 4, 16, 16]           --
│    └─ReLU: 2-354                       [16, 4, 16, 16]           --
│    └─Empty: 2-355                      [16, 4, 16, 16]           --
│    └─Clamp: 2-356                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-28         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-357                  [16, 64, 16, 16]          --
│    └─Empty: 2-358                      [16, 64, 16, 16]          --
│    └─Empty: 2-359                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-360         --                        --
│    └─One: 2-361                        [1]                       --
│    └─OutputScale: 2-362                --                        --
│    └─Empty: 2-363                      [4, 64, 3, 3]             --
│    └─Empty: 2-364                      [4, 64, 3, 3]             --
│    └─Empty: 2-365                      [4]                       --
│    └─Empty: 2-366                      [4]                       --
│    └─BatchNorm2d: 2-367                [16, 4, 16, 16]           --
│    └─Scaler: 2-368                     [16, 4, 16, 16]           --
│    └─ReLU: 2-369                       [16, 4, 16, 16]           --
│    └─Empty: 2-370                      [16, 4, 16, 16]           --
│    └─Clamp: 2-371                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-29                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-372         --                        --
│    └─One: 2-373                        [1]                       --
│    └─OutputScale: 2-374                --                        --
│    └─Empty: 2-375                      [64, 48, 1, 1]            --
│    └─Empty: 2-376                      [64, 48, 1, 1]            --
│    └─Empty: 2-377                      [64]                      --
│    └─Empty: 2-378                      [64]                      --
│    └─BatchNorm2d: 2-379                [16, 64, 64, 64]          --
│    └─Scaler: 2-380                     [16, 64, 64, 64]          --
│    └─ReLU: 2-381                       [16, 64, 64, 64]          --
│    └─Empty: 2-382                      [16, 64, 64, 64]          --
│    └─Clamp: 2-383                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-30         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-384                  [16, 64, 32, 32]          --
│    └─Empty: 2-385                      [16, 64, 32, 32]          --
│    └─Empty: 2-386                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-387         --                        --
│    └─One: 2-388                        [1]                       --
│    └─OutputScale: 2-389                --                        --
│    └─Empty: 2-390                      [64, 64, 3, 3]            --
│    └─Empty: 2-391                      [64, 64, 3, 3]            --
│    └─Empty: 2-392                      [64]                      --
│    └─Empty: 2-393                      [64]                      --
│    └─BatchNorm2d: 2-394                [16, 64, 32, 32]          --
│    └─Scaler: 2-395                     [16, 64, 32, 32]          --
│    └─ReLU: 2-396                       [16, 64, 32, 32]          --
│    └─Empty: 2-397                      [16, 64, 32, 32]          --
│    └─Clamp: 2-398                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-399                  [16, 64, 16, 16]          --
│    └─Empty: 2-400                      [16, 64, 16, 16]          --
│    └─Empty: 2-401                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-402         --                        --
│    └─One: 2-403                        [1]                       --
│    └─OutputScale: 2-404                --                        --
│    └─Empty: 2-405                      [64, 64, 3, 3]            --
│    └─Empty: 2-406                      [64, 64, 3, 3]            --
│    └─Empty: 2-407                      [64]                      --
│    └─Empty: 2-408                      [64]                      --
│    └─BatchNorm2d: 2-409                [16, 64, 16, 16]          --
│    └─Scaler: 2-410                     [16, 64, 16, 16]          --
│    └─ReLU: 2-411                       [16, 64, 16, 16]          --
│    └─Empty: 2-412                      [16, 64, 16, 16]          --
│    └─Clamp: 2-413                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-32                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-414         --                        --
│    └─One: 2-415                        [1]                       --
│    └─OutputScale: 2-416                --                        --
│    └─Empty: 2-417                      [4, 64, 1, 1]             --
│    └─Empty: 2-418                      [4, 64, 1, 1]             --
│    └─Empty: 2-419                      [4]                       --
│    └─Empty: 2-420                      [4]                       --
│    └─BatchNorm2d: 2-421                [16, 4, 16, 16]           --
│    └─Scaler: 2-422                     [16, 4, 16, 16]           --
│    └─ReLU: 2-423                       [16, 4, 16, 16]           --
│    └─Empty: 2-424                      [16, 4, 16, 16]           --
│    └─Clamp: 2-425                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-33         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-426                  [16, 64, 16, 16]          --
│    └─Empty: 2-427                      [16, 64, 16, 16]          --
│    └─Empty: 2-428                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-429         --                        --
│    └─One: 2-430                        [1]                       --
│    └─OutputScale: 2-431                --                        --
│    └─Empty: 2-432                      [4, 64, 3, 3]             --
│    └─Empty: 2-433                      [4, 64, 3, 3]             --
│    └─Empty: 2-434                      [4]                       --
│    └─Empty: 2-435                      [4]                       --
│    └─BatchNorm2d: 2-436                [16, 4, 16, 16]           --
│    └─Scaler: 2-437                     [16, 4, 16, 16]           --
│    └─ReLU: 2-438                       [16, 4, 16, 16]           --
│    └─Empty: 2-439                      [16, 4, 16, 16]           --
│    └─Clamp: 2-440                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-34                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-441         --                        --
│    └─One: 2-442                        [1]                       --
│    └─OutputScale: 2-443                --                        --
│    └─Empty: 2-444                      [64, 48, 1, 1]            --
│    └─Empty: 2-445                      [64, 48, 1, 1]            --
│    └─Empty: 2-446                      [64]                      --
│    └─Empty: 2-447                      [64]                      --
│    └─BatchNorm2d: 2-448                [16, 64, 64, 64]          --
│    └─Scaler: 2-449                     [16, 64, 64, 64]          --
│    └─ReLU: 2-450                       [16, 64, 64, 64]          --
│    └─Empty: 2-451                      [16, 64, 64, 64]          --
│    └─Clamp: 2-452                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-35         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-453                  [16, 64, 32, 32]          --
│    └─Empty: 2-454                      [16, 64, 32, 32]          --
│    └─Empty: 2-455                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-456         --                        --
│    └─One: 2-457                        [1]                       --
│    └─OutputScale: 2-458                --                        --
│    └─Empty: 2-459                      [64, 64, 3, 3]            --
│    └─Empty: 2-460                      [64, 64, 3, 3]            --
│    └─Empty: 2-461                      [64]                      --
│    └─Empty: 2-462                      [64]                      --
│    └─BatchNorm2d: 2-463                [16, 64, 32, 32]          --
│    └─Scaler: 2-464                     [16, 64, 32, 32]          --
│    └─ReLU: 2-465                       [16, 64, 32, 32]          --
│    └─Empty: 2-466                      [16, 64, 32, 32]          --
│    └─Clamp: 2-467                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-36         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-468                  [16, 64, 16, 16]          --
│    └─Empty: 2-469                      [16, 64, 16, 16]          --
│    └─Empty: 2-470                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-471         --                        --
│    └─One: 2-472                        [1]                       --
│    └─OutputScale: 2-473                --                        --
│    └─Empty: 2-474                      [64, 64, 3, 3]            --
│    └─Empty: 2-475                      [64, 64, 3, 3]            --
│    └─Empty: 2-476                      [64]                      --
│    └─Empty: 2-477                      [64]                      --
│    └─BatchNorm2d: 2-478                [16, 64, 16, 16]          --
│    └─Scaler: 2-479                     [16, 64, 16, 16]          --
│    └─ReLU: 2-480                       [16, 64, 16, 16]          --
│    └─Empty: 2-481                      [16, 64, 16, 16]          --
│    └─Clamp: 2-482                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-37                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-483         --                        --
│    └─One: 2-484                        [1]                       --
│    └─OutputScale: 2-485                --                        --
│    └─Empty: 2-486                      [4, 64, 1, 1]             --
│    └─Empty: 2-487                      [4, 64, 1, 1]             --
│    └─Empty: 2-488                      [4]                       --
│    └─Empty: 2-489                      [4]                       --
│    └─BatchNorm2d: 2-490                [16, 4, 16, 16]           --
│    └─Scaler: 2-491                     [16, 4, 16, 16]           --
│    └─ReLU: 2-492                       [16, 4, 16, 16]           --
│    └─Empty: 2-493                      [16, 4, 16, 16]           --
│    └─Clamp: 2-494                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-38         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-495                  [16, 64, 16, 16]          --
│    └─Empty: 2-496                      [16, 64, 16, 16]          --
│    └─Empty: 2-497                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-498         --                        --
│    └─One: 2-499                        [1]                       --
│    └─OutputScale: 2-500                --                        --
│    └─Empty: 2-501                      [4, 64, 3, 3]             --
│    └─Empty: 2-502                      [4, 64, 3, 3]             --
│    └─Empty: 2-503                      [4]                       --
│    └─Empty: 2-504                      [4]                       --
│    └─BatchNorm2d: 2-505                [16, 4, 16, 16]           --
│    └─Scaler: 2-506                     [16, 4, 16, 16]           --
│    └─ReLU: 2-507                       [16, 4, 16, 16]           --
│    └─Empty: 2-508                      [16, 4, 16, 16]           --
│    └─Clamp: 2-509                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-39                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-510         --                        --
│    └─One: 2-511                        [1]                       --
│    └─OutputScale: 2-512                --                        --
│    └─Empty: 2-513                      [64, 48, 1, 1]            --
│    └─Empty: 2-514                      [64, 48, 1, 1]            --
│    └─Empty: 2-515                      [64]                      --
│    └─Empty: 2-516                      [64]                      --
│    └─BatchNorm2d: 2-517                [16, 64, 64, 64]          --
│    └─Scaler: 2-518                     [16, 64, 64, 64]          --
│    └─ReLU: 2-519                       [16, 64, 64, 64]          --
│    └─Empty: 2-520                      [16, 64, 64, 64]          --
│    └─Clamp: 2-521                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-40         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-522                  [16, 64, 32, 32]          --
│    └─Empty: 2-523                      [16, 64, 32, 32]          --
│    └─Empty: 2-524                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-525         --                        --
│    └─One: 2-526                        [1]                       --
│    └─OutputScale: 2-527                --                        --
│    └─Empty: 2-528                      [64, 64, 3, 3]            --
│    └─Empty: 2-529                      [64, 64, 3, 3]            --
│    └─Empty: 2-530                      [64]                      --
│    └─Empty: 2-531                      [64]                      --
│    └─BatchNorm2d: 2-532                [16, 64, 32, 32]          --
│    └─Scaler: 2-533                     [16, 64, 32, 32]          --
│    └─ReLU: 2-534                       [16, 64, 32, 32]          --
│    └─Empty: 2-535                      [16, 64, 32, 32]          --
│    └─Clamp: 2-536                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-41         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-537                  [16, 64, 16, 16]          --
│    └─Empty: 2-538                      [16, 64, 16, 16]          --
│    └─Empty: 2-539                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-540         --                        --
│    └─One: 2-541                        [1]                       --
│    └─OutputScale: 2-542                --                        --
│    └─Empty: 2-543                      [64, 64, 3, 3]            --
│    └─Empty: 2-544                      [64, 64, 3, 3]            --
│    └─Empty: 2-545                      [64]                      --
│    └─Empty: 2-546                      [64]                      --
│    └─BatchNorm2d: 2-547                [16, 64, 16, 16]          --
│    └─Scaler: 2-548                     [16, 64, 16, 16]          --
│    └─ReLU: 2-549                       [16, 64, 16, 16]          --
│    └─Empty: 2-550                      [16, 64, 16, 16]          --
│    └─Clamp: 2-551                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-42                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-552         --                        --
│    └─One: 2-553                        [1]                       --
│    └─OutputScale: 2-554                --                        --
│    └─Empty: 2-555                      [4, 64, 1, 1]             --
│    └─Empty: 2-556                      [4, 64, 1, 1]             --
│    └─Empty: 2-557                      [4]                       --
│    └─Empty: 2-558                      [4]                       --
│    └─BatchNorm2d: 2-559                [16, 4, 16, 16]           --
│    └─Scaler: 2-560                     [16, 4, 16, 16]           --
│    └─ReLU: 2-561                       [16, 4, 16, 16]           --
│    └─Empty: 2-562                      [16, 4, 16, 16]           --
│    └─Clamp: 2-563                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-43         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-564                  [16, 64, 16, 16]          --
│    └─Empty: 2-565                      [16, 64, 16, 16]          --
│    └─Empty: 2-566                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-567         --                        --
│    └─One: 2-568                        [1]                       --
│    └─OutputScale: 2-569                --                        --
│    └─Empty: 2-570                      [4, 64, 3, 3]             --
│    └─Empty: 2-571                      [4, 64, 3, 3]             --
│    └─Empty: 2-572                      [4]                       --
│    └─Empty: 2-573                      [4]                       --
│    └─BatchNorm2d: 2-574                [16, 4, 16, 16]           --
│    └─Scaler: 2-575                     [16, 4, 16, 16]           --
│    └─ReLU: 2-576                       [16, 4, 16, 16]           --
│    └─Empty: 2-577                      [16, 4, 16, 16]           --
│    └─Clamp: 2-578                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-44                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-579         --                        --
│    └─One: 2-580                        [1]                       --
│    └─OutputScale: 2-581                --                        --
│    └─Empty: 2-582                      [64, 48, 1, 1]            --
│    └─Empty: 2-583                      [64, 48, 1, 1]            --
│    └─Empty: 2-584                      [64]                      --
│    └─Empty: 2-585                      [64]                      --
│    └─BatchNorm2d: 2-586                [16, 64, 64, 64]          --
│    └─Scaler: 2-587                     [16, 64, 64, 64]          --
│    └─ReLU: 2-588                       [16, 64, 64, 64]          --
│    └─Empty: 2-589                      [16, 64, 64, 64]          --
│    └─Clamp: 2-590                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-45         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-591                  [16, 64, 32, 32]          --
│    └─Empty: 2-592                      [16, 64, 32, 32]          --
│    └─Empty: 2-593                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-594         --                        --
│    └─One: 2-595                        [1]                       --
│    └─OutputScale: 2-596                --                        --
│    └─Empty: 2-597                      [64, 64, 3, 3]            --
│    └─Empty: 2-598                      [64, 64, 3, 3]            --
│    └─Empty: 2-599                      [64]                      --
│    └─Empty: 2-600                      [64]                      --
│    └─BatchNorm2d: 2-601                [16, 64, 32, 32]          --
│    └─Scaler: 2-602                     [16, 64, 32, 32]          --
│    └─ReLU: 2-603                       [16, 64, 32, 32]          --
│    └─Empty: 2-604                      [16, 64, 32, 32]          --
│    └─Clamp: 2-605                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-606                  [16, 64, 16, 16]          --
│    └─Empty: 2-607                      [16, 64, 16, 16]          --
│    └─Empty: 2-608                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-609         --                        --
│    └─One: 2-610                        [1]                       --
│    └─OutputScale: 2-611                --                        --
│    └─Empty: 2-612                      [64, 64, 3, 3]            --
│    └─Empty: 2-613                      [64, 64, 3, 3]            --
│    └─Empty: 2-614                      [64]                      --
│    └─Empty: 2-615                      [64]                      --
│    └─BatchNorm2d: 2-616                [16, 64, 16, 16]          --
│    └─Scaler: 2-617                     [16, 64, 16, 16]          --
│    └─ReLU: 2-618                       [16, 64, 16, 16]          --
│    └─Empty: 2-619                      [16, 64, 16, 16]          --
│    └─Clamp: 2-620                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-47                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-621         --                        --
│    └─One: 2-622                        [1]                       --
│    └─OutputScale: 2-623                --                        --
│    └─Empty: 2-624                      [4, 64, 1, 1]             --
│    └─Empty: 2-625                      [4, 64, 1, 1]             --
│    └─Empty: 2-626                      [4]                       --
│    └─Empty: 2-627                      [4]                       --
│    └─BatchNorm2d: 2-628                [16, 4, 16, 16]           --
│    └─Scaler: 2-629                     [16, 4, 16, 16]           --
│    └─ReLU: 2-630                       [16, 4, 16, 16]           --
│    └─Empty: 2-631                      [16, 4, 16, 16]           --
│    └─Clamp: 2-632                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-633                  [16, 64, 16, 16]          --
│    └─Empty: 2-634                      [16, 64, 16, 16]          --
│    └─Empty: 2-635                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-636         --                        --
│    └─One: 2-637                        [1]                       --
│    └─OutputScale: 2-638                --                        --
│    └─Empty: 2-639                      [4, 64, 3, 3]             --
│    └─Empty: 2-640                      [4, 64, 3, 3]             --
│    └─Empty: 2-641                      [4]                       --
│    └─Empty: 2-642                      [4]                       --
│    └─BatchNorm2d: 2-643                [16, 4, 16, 16]           --
│    └─Scaler: 2-644                     [16, 4, 16, 16]           --
│    └─ReLU: 2-645                       [16, 4, 16, 16]           --
│    └─Empty: 2-646                      [16, 4, 16, 16]           --
│    └─Clamp: 2-647                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-49                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-648         --                        --
│    └─One: 2-649                        [1]                       --
│    └─OutputScale: 2-650                --                        --
│    └─Empty: 2-651                      [64, 48, 1, 1]            --
│    └─Empty: 2-652                      [64, 48, 1, 1]            --
│    └─Empty: 2-653                      [64]                      --
│    └─Empty: 2-654                      [64]                      --
│    └─BatchNorm2d: 2-655                [16, 64, 64, 64]          --
│    └─Scaler: 2-656                     [16, 64, 64, 64]          --
│    └─ReLU: 2-657                       [16, 64, 64, 64]          --
│    └─Empty: 2-658                      [16, 64, 64, 64]          --
│    └─Clamp: 2-659                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-50         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-660                  [16, 64, 32, 32]          --
│    └─Empty: 2-661                      [16, 64, 32, 32]          --
│    └─Empty: 2-662                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-663         --                        --
│    └─One: 2-664                        [1]                       --
│    └─OutputScale: 2-665                --                        --
│    └─Empty: 2-666                      [64, 64, 3, 3]            --
│    └─Empty: 2-667                      [64, 64, 3, 3]            --
│    └─Empty: 2-668                      [64]                      --
│    └─Empty: 2-669                      [64]                      --
│    └─BatchNorm2d: 2-670                [16, 64, 32, 32]          --
│    └─Scaler: 2-671                     [16, 64, 32, 32]          --
│    └─ReLU: 2-672                       [16, 64, 32, 32]          --
│    └─Empty: 2-673                      [16, 64, 32, 32]          --
│    └─Clamp: 2-674                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-675                  [16, 64, 16, 16]          --
│    └─Empty: 2-676                      [16, 64, 16, 16]          --
│    └─Empty: 2-677                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-678         --                        --
│    └─One: 2-679                        [1]                       --
│    └─OutputScale: 2-680                --                        --
│    └─Empty: 2-681                      [64, 64, 3, 3]            --
│    └─Empty: 2-682                      [64, 64, 3, 3]            --
│    └─Empty: 2-683                      [64]                      --
│    └─Empty: 2-684                      [64]                      --
│    └─BatchNorm2d: 2-685                [16, 64, 16, 16]          --
│    └─Scaler: 2-686                     [16, 64, 16, 16]          --
│    └─ReLU: 2-687                       [16, 64, 16, 16]          --
│    └─Empty: 2-688                      [16, 64, 16, 16]          --
│    └─Clamp: 2-689                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-52                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-690         --                        --
│    └─One: 2-691                        [1]                       --
│    └─OutputScale: 2-692                --                        --
│    └─Empty: 2-693                      [4, 64, 1, 1]             --
│    └─Empty: 2-694                      [4, 64, 1, 1]             --
│    └─Empty: 2-695                      [4]                       --
│    └─Empty: 2-696                      [4]                       --
│    └─BatchNorm2d: 2-697                [16, 4, 16, 16]           --
│    └─Scaler: 2-698                     [16, 4, 16, 16]           --
│    └─ReLU: 2-699                       [16, 4, 16, 16]           --
│    └─Empty: 2-700                      [16, 4, 16, 16]           --
│    └─Clamp: 2-701                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-53         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-702                  [16, 64, 16, 16]          --
│    └─Empty: 2-703                      [16, 64, 16, 16]          --
│    └─Empty: 2-704                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-705         --                        --
│    └─One: 2-706                        [1]                       --
│    └─OutputScale: 2-707                --                        --
│    └─Empty: 2-708                      [4, 64, 3, 3]             --
│    └─Empty: 2-709                      [4, 64, 3, 3]             --
│    └─Empty: 2-710                      [4]                       --
│    └─Empty: 2-711                      [4]                       --
│    └─BatchNorm2d: 2-712                [16, 4, 16, 16]           --
│    └─Scaler: 2-713                     [16, 4, 16, 16]           --
│    └─ReLU: 2-714                       [16, 4, 16, 16]           --
│    └─Empty: 2-715                      [16, 4, 16, 16]           --
│    └─Clamp: 2-716                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-54                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-717         --                        --
│    └─One: 2-718                        [1]                       --
│    └─OutputScale: 2-719                --                        --
│    └─Empty: 2-720                      [64, 48, 1, 1]            --
│    └─Empty: 2-721                      [64, 48, 1, 1]            --
│    └─Empty: 2-722                      [64]                      --
│    └─Empty: 2-723                      [64]                      --
│    └─BatchNorm2d: 2-724                [16, 64, 64, 64]          --
│    └─Scaler: 2-725                     [16, 64, 64, 64]          --
│    └─ReLU: 2-726                       [16, 64, 64, 64]          --
│    └─Empty: 2-727                      [16, 64, 64, 64]          --
│    └─Clamp: 2-728                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-55         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-729                  [16, 64, 32, 32]          --
│    └─Empty: 2-730                      [16, 64, 32, 32]          --
│    └─Empty: 2-731                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-732         --                        --
│    └─One: 2-733                        [1]                       --
│    └─OutputScale: 2-734                --                        --
│    └─Empty: 2-735                      [64, 64, 3, 3]            --
│    └─Empty: 2-736                      [64, 64, 3, 3]            --
│    └─Empty: 2-737                      [64]                      --
│    └─Empty: 2-738                      [64]                      --
│    └─BatchNorm2d: 2-739                [16, 64, 32, 32]          --
│    └─Scaler: 2-740                     [16, 64, 32, 32]          --
│    └─ReLU: 2-741                       [16, 64, 32, 32]          --
│    └─Empty: 2-742                      [16, 64, 32, 32]          --
│    └─Clamp: 2-743                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-56         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-744                  [16, 64, 16, 16]          --
│    └─Empty: 2-745                      [16, 64, 16, 16]          --
│    └─Empty: 2-746                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-747         --                        --
│    └─One: 2-748                        [1]                       --
│    └─OutputScale: 2-749                --                        --
│    └─Empty: 2-750                      [64, 64, 3, 3]            --
│    └─Empty: 2-751                      [64, 64, 3, 3]            --
│    └─Empty: 2-752                      [64]                      --
│    └─Empty: 2-753                      [64]                      --
│    └─BatchNorm2d: 2-754                [16, 64, 16, 16]          --
│    └─Scaler: 2-755                     [16, 64, 16, 16]          --
│    └─ReLU: 2-756                       [16, 64, 16, 16]          --
│    └─Empty: 2-757                      [16, 64, 16, 16]          --
│    └─Clamp: 2-758                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-57                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-759         --                        --
│    └─One: 2-760                        [1]                       --
│    └─OutputScale: 2-761                --                        --
│    └─Empty: 2-762                      [4, 64, 1, 1]             --
│    └─Empty: 2-763                      [4, 64, 1, 1]             --
│    └─Empty: 2-764                      [4]                       --
│    └─Empty: 2-765                      [4]                       --
│    └─BatchNorm2d: 2-766                [16, 4, 16, 16]           --
│    └─Scaler: 2-767                     [16, 4, 16, 16]           --
│    └─ReLU: 2-768                       [16, 4, 16, 16]           --
│    └─Empty: 2-769                      [16, 4, 16, 16]           --
│    └─Clamp: 2-770                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-58         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-771                  [16, 64, 16, 16]          --
│    └─Empty: 2-772                      [16, 64, 16, 16]          --
│    └─Empty: 2-773                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-774         --                        --
│    └─One: 2-775                        [1]                       --
│    └─OutputScale: 2-776                --                        --
│    └─Empty: 2-777                      [4, 64, 3, 3]             --
│    └─Empty: 2-778                      [4, 64, 3, 3]             --
│    └─Empty: 2-779                      [4]                       --
│    └─Empty: 2-780                      [4]                       --
│    └─BatchNorm2d: 2-781                [16, 4, 16, 16]           --
│    └─Scaler: 2-782                     [16, 4, 16, 16]           --
│    └─ReLU: 2-783                       [16, 4, 16, 16]           --
│    └─Empty: 2-784                      [16, 4, 16, 16]           --
│    └─Clamp: 2-785                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-59                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-786         --                        --
│    └─One: 2-787                        [1]                       --
│    └─OutputScale: 2-788                --                        --
│    └─Empty: 2-789                      [64, 48, 1, 1]            --
│    └─Empty: 2-790                      [64, 48, 1, 1]            --
│    └─Empty: 2-791                      [64]                      --
│    └─Empty: 2-792                      [64]                      --
│    └─BatchNorm2d: 2-793                [16, 64, 64, 64]          --
│    └─Scaler: 2-794                     [16, 64, 64, 64]          --
│    └─ReLU: 2-795                       [16, 64, 64, 64]          --
│    └─Empty: 2-796                      [16, 64, 64, 64]          --
│    └─Clamp: 2-797                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-60         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-798                  [16, 64, 32, 32]          --
│    └─Empty: 2-799                      [16, 64, 32, 32]          --
│    └─Empty: 2-800                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-801         --                        --
│    └─One: 2-802                        [1]                       --
│    └─OutputScale: 2-803                --                        --
│    └─Empty: 2-804                      [64, 64, 3, 3]            --
│    └─Empty: 2-805                      [64, 64, 3, 3]            --
│    └─Empty: 2-806                      [64]                      --
│    └─Empty: 2-807                      [64]                      --
│    └─BatchNorm2d: 2-808                [16, 64, 32, 32]          --
│    └─Scaler: 2-809                     [16, 64, 32, 32]          --
│    └─ReLU: 2-810                       [16, 64, 32, 32]          --
│    └─Empty: 2-811                      [16, 64, 32, 32]          --
│    └─Clamp: 2-812                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-61         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-813                  [16, 64, 16, 16]          --
│    └─Empty: 2-814                      [16, 64, 16, 16]          --
│    └─Empty: 2-815                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-816         --                        --
│    └─One: 2-817                        [1]                       --
│    └─OutputScale: 2-818                --                        --
│    └─Empty: 2-819                      [64, 64, 3, 3]            --
│    └─Empty: 2-820                      [64, 64, 3, 3]            --
│    └─Empty: 2-821                      [64]                      --
│    └─Empty: 2-822                      [64]                      --
│    └─BatchNorm2d: 2-823                [16, 64, 16, 16]          --
│    └─Scaler: 2-824                     [16, 64, 16, 16]          --
│    └─ReLU: 2-825                       [16, 64, 16, 16]          --
│    └─Empty: 2-826                      [16, 64, 16, 16]          --
│    └─Clamp: 2-827                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-62                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-828         --                        --
│    └─One: 2-829                        [1]                       --
│    └─OutputScale: 2-830                --                        --
│    └─Empty: 2-831                      [4, 64, 1, 1]             --
│    └─Empty: 2-832                      [4, 64, 1, 1]             --
│    └─Empty: 2-833                      [4]                       --
│    └─Empty: 2-834                      [4]                       --
│    └─BatchNorm2d: 2-835                [16, 4, 16, 16]           --
│    └─Scaler: 2-836                     [16, 4, 16, 16]           --
│    └─ReLU: 2-837                       [16, 4, 16, 16]           --
│    └─Empty: 2-838                      [16, 4, 16, 16]           --
│    └─Clamp: 2-839                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-63         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-840                  [16, 64, 16, 16]          --
│    └─Empty: 2-841                      [16, 64, 16, 16]          --
│    └─Empty: 2-842                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-843         --                        --
│    └─One: 2-844                        [1]                       --
│    └─OutputScale: 2-845                --                        --
│    └─Empty: 2-846                      [4, 64, 3, 3]             --
│    └─Empty: 2-847                      [4, 64, 3, 3]             --
│    └─Empty: 2-848                      [4]                       --
│    └─Empty: 2-849                      [4]                       --
│    └─BatchNorm2d: 2-850                [16, 4, 16, 16]           --
│    └─Scaler: 2-851                     [16, 4, 16, 16]           --
│    └─ReLU: 2-852                       [16, 4, 16, 16]           --
│    └─Empty: 2-853                      [16, 4, 16, 16]           --
│    └─Clamp: 2-854                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-64                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-855         --                        --
│    └─One: 2-856                        [1]                       --
│    └─OutputScale: 2-857                --                        --
│    └─Empty: 2-858                      [64, 48, 1, 1]            --
│    └─Empty: 2-859                      [64, 48, 1, 1]            --
│    └─Empty: 2-860                      [64]                      --
│    └─Empty: 2-861                      [64]                      --
│    └─BatchNorm2d: 2-862                [16, 64, 64, 64]          --
│    └─Scaler: 2-863                     [16, 64, 64, 64]          --
│    └─ReLU: 2-864                       [16, 64, 64, 64]          --
│    └─Empty: 2-865                      [16, 64, 64, 64]          --
│    └─Clamp: 2-866                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-65         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-867                  [16, 64, 32, 32]          --
│    └─Empty: 2-868                      [16, 64, 32, 32]          --
│    └─Empty: 2-869                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-870         --                        --
│    └─One: 2-871                        [1]                       --
│    └─OutputScale: 2-872                --                        --
│    └─Empty: 2-873                      [64, 64, 3, 3]            --
│    └─Empty: 2-874                      [64, 64, 3, 3]            --
│    └─Empty: 2-875                      [64]                      --
│    └─Empty: 2-876                      [64]                      --
│    └─BatchNorm2d: 2-877                [16, 64, 32, 32]          --
│    └─Scaler: 2-878                     [16, 64, 32, 32]          --
│    └─ReLU: 2-879                       [16, 64, 32, 32]          --
│    └─Empty: 2-880                      [16, 64, 32, 32]          --
│    └─Clamp: 2-881                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-882                  [16, 64, 16, 16]          --
│    └─Empty: 2-883                      [16, 64, 16, 16]          --
│    └─Empty: 2-884                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-885         --                        --
│    └─One: 2-886                        [1]                       --
│    └─OutputScale: 2-887                --                        --
│    └─Empty: 2-888                      [64, 64, 3, 3]            --
│    └─Empty: 2-889                      [64, 64, 3, 3]            --
│    └─Empty: 2-890                      [64]                      --
│    └─Empty: 2-891                      [64]                      --
│    └─BatchNorm2d: 2-892                [16, 64, 16, 16]          --
│    └─Scaler: 2-893                     [16, 64, 16, 16]          --
│    └─ReLU: 2-894                       [16, 64, 16, 16]          --
│    └─Empty: 2-895                      [16, 64, 16, 16]          --
│    └─Clamp: 2-896                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-67                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-897         --                        --
│    └─One: 2-898                        [1]                       --
│    └─OutputScale: 2-899                --                        --
│    └─Empty: 2-900                      [4, 64, 1, 1]             --
│    └─Empty: 2-901                      [4, 64, 1, 1]             --
│    └─Empty: 2-902                      [4]                       --
│    └─Empty: 2-903                      [4]                       --
│    └─BatchNorm2d: 2-904                [16, 4, 16, 16]           --
│    └─Scaler: 2-905                     [16, 4, 16, 16]           --
│    └─ReLU: 2-906                       [16, 4, 16, 16]           --
│    └─Empty: 2-907                      [16, 4, 16, 16]           --
│    └─Clamp: 2-908                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-909                  [16, 64, 16, 16]          --
│    └─Empty: 2-910                      [16, 64, 16, 16]          --
│    └─Empty: 2-911                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-912         --                        --
│    └─One: 2-913                        [1]                       --
│    └─OutputScale: 2-914                --                        --
│    └─Empty: 2-915                      [4, 64, 3, 3]             --
│    └─Empty: 2-916                      [4, 64, 3, 3]             --
│    └─Empty: 2-917                      [4]                       --
│    └─Empty: 2-918                      [4]                       --
│    └─BatchNorm2d: 2-919                [16, 4, 16, 16]           --
│    └─Scaler: 2-920                     [16, 4, 16, 16]           --
│    └─ReLU: 2-921                       [16, 4, 16, 16]           --
│    └─Empty: 2-922                      [16, 4, 16, 16]           --
│    └─Clamp: 2-923                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-69                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-924         --                        --
│    └─One: 2-925                        [1]                       --
│    └─OutputScale: 2-926                --                        --
│    └─Empty: 2-927                      [64, 48, 1, 1]            --
│    └─Empty: 2-928                      [64, 48, 1, 1]            --
│    └─Empty: 2-929                      [64]                      --
│    └─Empty: 2-930                      [64]                      --
│    └─BatchNorm2d: 2-931                [16, 64, 64, 64]          --
│    └─Scaler: 2-932                     [16, 64, 64, 64]          --
│    └─ReLU: 2-933                       [16, 64, 64, 64]          --
│    └─Empty: 2-934                      [16, 64, 64, 64]          --
│    └─Clamp: 2-935                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-70         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-936                  [16, 64, 32, 32]          --
│    └─Empty: 2-937                      [16, 64, 32, 32]          --
│    └─Empty: 2-938                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-939         --                        --
│    └─One: 2-940                        [1]                       --
│    └─OutputScale: 2-941                --                        --
│    └─Empty: 2-942                      [64, 64, 3, 3]            --
│    └─Empty: 2-943                      [64, 64, 3, 3]            --
│    └─Empty: 2-944                      [64]                      --
│    └─Empty: 2-945                      [64]                      --
│    └─BatchNorm2d: 2-946                [16, 64, 32, 32]          --
│    └─Scaler: 2-947                     [16, 64, 32, 32]          --
│    └─ReLU: 2-948                       [16, 64, 32, 32]          --
│    └─Empty: 2-949                      [16, 64, 32, 32]          --
│    └─Clamp: 2-950                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-71         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-951                  [16, 64, 16, 16]          --
│    └─Empty: 2-952                      [16, 64, 16, 16]          --
│    └─Empty: 2-953                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-954         --                        --
│    └─One: 2-955                        [1]                       --
│    └─OutputScale: 2-956                --                        --
│    └─Empty: 2-957                      [64, 64, 3, 3]            --
│    └─Empty: 2-958                      [64, 64, 3, 3]            --
│    └─Empty: 2-959                      [64]                      --
│    └─Empty: 2-960                      [64]                      --
│    └─BatchNorm2d: 2-961                [16, 64, 16, 16]          --
│    └─Scaler: 2-962                     [16, 64, 16, 16]          --
│    └─ReLU: 2-963                       [16, 64, 16, 16]          --
│    └─Empty: 2-964                      [16, 64, 16, 16]          --
│    └─Clamp: 2-965                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-72                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-966         --                        --
│    └─One: 2-967                        [1]                       --
│    └─OutputScale: 2-968                --                        --
│    └─Empty: 2-969                      [4, 64, 1, 1]             --
│    └─Empty: 2-970                      [4, 64, 1, 1]             --
│    └─Empty: 2-971                      [4]                       --
│    └─Empty: 2-972                      [4]                       --
│    └─BatchNorm2d: 2-973                [16, 4, 16, 16]           --
│    └─Scaler: 2-974                     [16, 4, 16, 16]           --
│    └─ReLU: 2-975                       [16, 4, 16, 16]           --
│    └─Empty: 2-976                      [16, 4, 16, 16]           --
│    └─Clamp: 2-977                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-978                  [16, 64, 16, 16]          --
│    └─Empty: 2-979                      [16, 64, 16, 16]          --
│    └─Empty: 2-980                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-981         --                        --
│    └─One: 2-982                        [1]                       --
│    └─OutputScale: 2-983                --                        --
│    └─Empty: 2-984                      [4, 64, 3, 3]             --
│    └─Empty: 2-985                      [4, 64, 3, 3]             --
│    └─Empty: 2-986                      [4]                       --
│    └─Empty: 2-987                      [4]                       --
│    └─BatchNorm2d: 2-988                [16, 4, 16, 16]           --
│    └─Scaler: 2-989                     [16, 4, 16, 16]           --
│    └─ReLU: 2-990                       [16, 4, 16, 16]           --
│    └─Empty: 2-991                      [16, 4, 16, 16]           --
│    └─Clamp: 2-992                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-74                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-993         --                        --
│    └─One: 2-994                        [1]                       --
│    └─OutputScale: 2-995                --                        --
│    └─Empty: 2-996                      [64, 48, 1, 1]            --
│    └─Empty: 2-997                      [64, 48, 1, 1]            --
│    └─Empty: 2-998                      [64]                      --
│    └─Empty: 2-999                      [64]                      --
│    └─BatchNorm2d: 2-1000               [16, 64, 64, 64]          --
│    └─Scaler: 2-1001                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1002                      [16, 64, 64, 64]          --
│    └─Empty: 2-1003                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1004                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1005                 [16, 64, 32, 32]          --
│    └─Empty: 2-1006                     [16, 64, 32, 32]          --
│    └─Empty: 2-1007                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1008        --                        --
│    └─One: 2-1009                       [1]                       --
│    └─OutputScale: 2-1010               --                        --
│    └─Empty: 2-1011                     [64, 64, 3, 3]            --
│    └─Empty: 2-1012                     [64, 64, 3, 3]            --
│    └─Empty: 2-1013                     [64]                      --
│    └─Empty: 2-1014                     [64]                      --
│    └─BatchNorm2d: 2-1015               [16, 64, 32, 32]          --
│    └─Scaler: 2-1016                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1017                      [16, 64, 32, 32]          --
│    └─Empty: 2-1018                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1019                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-76         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1020                 [16, 64, 16, 16]          --
│    └─Empty: 2-1021                     [16, 64, 16, 16]          --
│    └─Empty: 2-1022                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1023        --                        --
│    └─One: 2-1024                       [1]                       --
│    └─OutputScale: 2-1025               --                        --
│    └─Empty: 2-1026                     [64, 64, 3, 3]            --
│    └─Empty: 2-1027                     [64, 64, 3, 3]            --
│    └─Empty: 2-1028                     [64]                      --
│    └─Empty: 2-1029                     [64]                      --
│    └─BatchNorm2d: 2-1030               [16, 64, 16, 16]          --
│    └─Scaler: 2-1031                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1032                      [16, 64, 16, 16]          --
│    └─Empty: 2-1033                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1034                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-77                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1035        --                        --
│    └─One: 2-1036                       [1]                       --
│    └─OutputScale: 2-1037               --                        --
│    └─Empty: 2-1038                     [4, 64, 1, 1]             --
│    └─Empty: 2-1039                     [4, 64, 1, 1]             --
│    └─Empty: 2-1040                     [4]                       --
│    └─Empty: 2-1041                     [4]                       --
│    └─BatchNorm2d: 2-1042               [16, 4, 16, 16]           --
│    └─Scaler: 2-1043                    [16, 4, 16, 16]           --
│    └─ReLU: 2-1044                      [16, 4, 16, 16]           --
│    └─Empty: 2-1045                     [16, 4, 16, 16]           --
│    └─Clamp: 2-1046                     [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-78         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1047                 [16, 64, 16, 16]          --
│    └─Empty: 2-1048                     [16, 64, 16, 16]          --
│    └─Empty: 2-1049                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1050        --                        --
│    └─One: 2-1051                       [1]                       --
│    └─OutputScale: 2-1052               --                        --
│    └─Empty: 2-1053                     [4, 64, 3, 3]             --
│    └─Empty: 2-1054                     [4, 64, 3, 3]             --
│    └─Empty: 2-1055                     [4]                       --
│    └─Empty: 2-1056                     [4]                       --
│    └─BatchNorm2d: 2-1057               [16, 4, 16, 16]           --
│    └─Scaler: 2-1058                    [16, 4, 16, 16]           --
│    └─ReLU: 2-1059                      [16, 4, 16, 16]           --
│    └─Empty: 2-1060                     [16, 4, 16, 16]           --
│    └─Clamp: 2-1061                     [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-79                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1062        --                        --
│    └─One: 2-1063                       [1]                       --
│    └─OutputScale: 2-1064               --                        --
│    └─Empty: 2-1065                     [64, 48, 1, 1]            --
│    └─Empty: 2-1066                     [64, 48, 1, 1]            --
│    └─Empty: 2-1067                     [64]                      --
│    └─Empty: 2-1068                     [64]                      --
│    └─BatchNorm2d: 2-1069               [16, 64, 64, 64]          --
│    └─Scaler: 2-1070                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1071                      [16, 64, 64, 64]          --
│    └─Empty: 2-1072                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1073                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-80         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1074                 [16, 64, 32, 32]          --
│    └─Empty: 2-1075                     [16, 64, 32, 32]          --
│    └─Empty: 2-1076                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1077        --                        --
│    └─One: 2-1078                       [1]                       --
│    └─OutputScale: 2-1079               --                        --
│    └─Empty: 2-1080                     [64, 64, 3, 3]            --
│    └─Empty: 2-1081                     [64, 64, 3, 3]            --
│    └─Empty: 2-1082                     [64]                      --
│    └─Empty: 2-1083                     [64]                      --
│    └─BatchNorm2d: 2-1084               [16, 64, 32, 32]          --
│    └─Scaler: 2-1085                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1086                      [16, 64, 32, 32]          --
│    └─Empty: 2-1087                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1088                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1089                 [16, 64, 16, 16]          --
│    └─Empty: 2-1090                     [16, 64, 16, 16]          --
│    └─Empty: 2-1091                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1092        --                        --
│    └─One: 2-1093                       [1]                       --
│    └─OutputScale: 2-1094               --                        --
│    └─Empty: 2-1095                     [64, 64, 3, 3]            --
│    └─Empty: 2-1096                     [64, 64, 3, 3]            --
│    └─Empty: 2-1097                     [64]                      --
│    └─Empty: 2-1098                     [64]                      --
│    └─BatchNorm2d: 2-1099               [16, 64, 16, 16]          --
│    └─Scaler: 2-1100                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1101                      [16, 64, 16, 16]          --
│    └─Empty: 2-1102                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1103                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-82                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1104        --                        --
│    └─One: 2-1105                       [1]                       --
│    └─OutputScale: 2-1106               --                        --
│    └─Empty: 2-1107                     [4, 64, 1, 1]             --
│    └─Empty: 2-1108                     [4, 64, 1, 1]             --
│    └─Empty: 2-1109                     [4]                       --
│    └─Empty: 2-1110                     [4]                       --
│    └─BatchNorm2d: 2-1111               [16, 4, 16, 16]           --
│    └─Scaler: 2-1112                    [16, 4, 16, 16]           --
│    └─ReLU: 2-1113                      [16, 4, 16, 16]           --
│    └─Empty: 2-1114                     [16, 4, 16, 16]           --
│    └─Clamp: 2-1115                     [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-83         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1116                 [16, 64, 16, 16]          --
│    └─Empty: 2-1117                     [16, 64, 16, 16]          --
│    └─Empty: 2-1118                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1119        --                        --
│    └─One: 2-1120                       [1]                       --
│    └─OutputScale: 2-1121               --                        --
│    └─Empty: 2-1122                     [4, 64, 3, 3]             --
│    └─Empty: 2-1123                     [4, 64, 3, 3]             --
│    └─Empty: 2-1124                     [4]                       --
│    └─Empty: 2-1125                     [4]                       --
│    └─BatchNorm2d: 2-1126               [16, 4, 16, 16]           --
│    └─Scaler: 2-1127                    [16, 4, 16, 16]           --
│    └─ReLU: 2-1128                      [16, 4, 16, 16]           --
│    └─Empty: 2-1129                     [16, 4, 16, 16]           --
│    └─Clamp: 2-1130                     [16, 4, 16, 16]           --
├─Conv1d: 1-84                           [16, 4, 14]               12,298
│    └─OutputShiftSqueeze: 2-1131        --                        --
│    └─One: 2-1132                       [1]                       --
│    └─OutputScale: 2-1133               --                        --
│    └─Empty: 2-1134                     [4, 1024, 3]              --
│    └─Empty: 2-1135                     [4, 1024, 3]              --
│    └─Empty: 2-1136                     [4]                       --
│    └─Empty: 2-1137                     [4]                       --
│    └─Scaler: 2-1138                    [16, 4, 14]               --
│    └─Empty: 2-1139                     [16, 4, 14]               --
│    └─Empty: 2-1140                     [16, 4, 14]               --
│    └─Clamp: 2-1141                     [16, 4, 14]               --
==========================================================================================
Total params: 91,888
Trainable params: 91,852
Non-trainable params: 36
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 0.00
Params size (MB): 0.32
Estimated Total Size (MB): 201.64
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.393 | Acc: 31.500% | Wgt Acc: 30.889%
	I - Batch: 100 | Loss: 1.337 | Acc: 35.062% | Wgt Acc: 35.628%
	I - Batch: 150 | Loss: 1.311 | Acc: 36.958% | Wgt Acc: 38.006%
I - num batch: 160
I - Train -- Loss: 1.311 | Acc: 37.299% | Wgt Acc: 38.429% | LR: 1.000000e-03 | Dur: 91.95s
I - Confusion Matrix: [row->prediction - col->label]
[[102.  33.  61.  66.]
 [139. 288. 256. 116.]
 [192. 200. 304. 100.]
 [264.  57. 113. 256.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.228 | Acc: 41.590% | Wgt Acc: 44.769% | Dur: 9.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 7.  2.  2. 10.]
 [30. 66. 52. 23.]
 [ 5.  7. 14.  4.]
 [46.  3.  7. 49.]]

I - Local maximum validation set accuracy:  41.59

I - Validation set results: 
[14-1-1-0.70][50-3-1-0.22][124-2-1--0.34][127-0-3--1.00][443-2-2-0.98][567-0-3--0.71][573-1-1-0.55][615-0-3-0.11][695-1-1--0.99][722-3-0--1.00]
[826-0-3--1.00][878-0-3--0.46][1103-0-1--0.50][1212-3-3-0.23][1368-0-1-0.84][2181-2-3--0.54][2476-2-1--0.03][2721-2-1-0.99][2818-1-2-0.50][2886-2-1-0.99]
[3231-2-1-0.99][3333-2-1--0.12][3482-2-2-0.24][3536-3-1-0.35][3625-1-1-0.99][3909-0-1-0.80][4035-0-3-0.36][4140-0-2--0.94][4214-1-1-0.13][4346-1-3--0.76]
[4581-2-2-0.95][4708-3-3--0.17][4838-3-1-0.99][4845-1-1-0.78][4868-0-1--0.54][4939-0-2-0.99][4984-2-3--0.99][5078-1-1-0.80][5396-0-3--0.36][5479-1-1-0.99]
[5717-0-1--0.28][5843-1-1-0.99][5949-3-3-0.48][5987-2-1-0.99][6014-3-1--0.37][6033-3-0--1.00][6313-0-3--0.83][6421-3-3-0.27][6500-1-1-0.24][6583-3-3--0.09]
[6683-3-1--0.55][6825-2-1--0.99][6998-3-1--0.24][7049-3-3--0.69][7517-1-1-0.99][7521-1-1-0.99][7528-1-0--1.00][7949-1-2-0.42][8135-1-1--0.17][8185-3-0--1.00]
[8269-3-1-0.74][8273-3-0--1.00][8543-3-3--0.80][8666-1-1-0.70][8672-0-3--0.49][8903-1-0--1.00][9001-2-1-0.52][9036-2-2-0.85][9281-3-1--0.10][9300-2-2-0.29]
[9571-0-3--0.73][9617-1-1-0.63][9644-2-1--0.20][9705-2-1-0.96][9801-0-3-0.25][9803-3-1--0.63][9865-3-3--0.23][9896-2-1-0.99][10314-1-1-0.99][10337-3-3-0.49]
[10403-0-2--0.72][10653-2-1-0.95][10704-2-1-0.94][10719-1-1-0.98][10727-1-1-0.99][10836-0-3-0.97][10969-2-3--0.49][11042-0-1--0.94][11088-1-1-0.50][11322-0-3-0.03]
[11398-2-2-0.99][11499-0-1--0.94][11502-3-3--0.20][11512-3-1-0.48][11608-1-1-0.99][11610-0-1-0.40][11692-0-0--1.00][11905-0-3--0.37][11993-1-1-0.99][12002-2-1--0.49]
[12052-0-1--0.88][12201-0-3-0.32][12235-2-1-0.99][12320-1-1-0.93][12377-2-1-0.78][12398-2-1-0.32][12503-1-1-0.64][12617-0-1-0.99][12685-3-1--0.53][12738-2-1-0.14]
[12742-2-1-0.22][12823-0-3-0.89][13110-1-2-0.83][13240-3-3--0.18][13253-1-1-0.99][13273-0-3--0.61][13634-1-1-0.57][13763-2-3-0.04][13905-3-0--1.00][14060-2-1-0.99]
[14065-3-3--0.67][14147-3-3--0.54][14595-2-1-0.98][14687-2-1--0.36][14788-2-1--0.97][14869-1-1-0.48][14872-3-2-0.32][14877-1-1-0.32][14927-0-1--0.35][15066-0-3--0.80]
[15175-1-2-0.51][15178-2-0--1.00][15375-3-1--0.46][15389-3-3--0.17][15568-2-1-0.67][15675-3-1--0.25][15869-1-2--0.22][16207-3-1--0.72][16236-0-3--0.94][16302-3-0--1.00]
[16331-2-2-0.32][16381-0-3--0.92][16488-1-1-0.99][16495-0-0--1.00][16650-0-3--0.26][16719-1-1-0.43][16801-0-3--0.88][16828-0-1--0.85][17137-3-0--1.00][17245-1-1--0.71]
[17278-3-1-0.84][17282-0-1-0.23][17311-2-2--0.75][17336-2-1-0.43][17608-3-3-0.68][17627-0-1--0.21][17877-3-2-0.99][17924-1-1--0.98][17984-3-3--0.73][18211-0-3-0.10]
[18276-3-3--0.02][18287-1-1-0.99][18394-0-3--0.65][18428-0-0--1.00][18442-0-3-0.13][18478-3-3--0.41][18607-0-1--0.31][18616-0-1-0.03][18663-0-3--0.56][18718-0-3--0.62]
[18766-2-1-0.99][18824-2-1-0.99][18890-3-2--0.42][18930-3-2--0.77][18938-3-3-0.07][19817-1-2-0.18][19839-0-1-0.87][19930-3-3--0.41][19944-0-1-0.33][20036-2-2--0.56]
[20101-3-3--0.70][20474-1-1-0.95][20547-3-1-0.48][20929-2-1-0.84][21245-1-1-0.99][21257-3-3--0.29][21293-1-1-0.99][21316-1-1-0.56][21384-1-1-0.16][21448-1-1-0.96]
[21483-0-0--1.00][21487-2-1-0.15][21714-0-3--1.00][21943-3-1--0.87][21947-0-1--0.73][21948-0-3--0.99][21965-2-1-0.94][21998-1-1-0.96][22025-0-3-0.39][22228-3-3--0.52]
[22446-1-1-0.99][22494-3-3--0.40][22757-0-3--0.68][22811-3-3-0.79][22976-3-1-0.99][22985-3-3-0.47][23014-0-3-0.83][23112-1-1-0.99][23144-3-3--0.00][23168-2-1--0.77]
[23219-0-3--0.05][23363-3-3-0.49][23470-0-1-0.56][23486-2-3--0.45][23497-0-3-0.99][23516-0-3--0.75][23690-1-1--0.53][23921-2-1-0.65][23936-1-3--0.59][24040-3-1--0.82]
[24111-1-1-0.99][24182-0-3-0.97][24238-3-3-0.77][24290-2-2-0.23][24345-0-0--1.00][24364-1-3--0.93][24427-3-3--0.04][24477-2-1-0.36][24495-2-1-0.99][24893-2-2-0.19]
[25012-1-1-0.64][25121-2-2-0.71][25165-3-3--0.32][25183-0-1--0.12][25297-3-3-0.57][25398-0-3--0.99][25574-2-1--0.72][25644-1-1-0.80][25718-1-1-0.57][25774-2-1--0.17]
[26032-3-0--1.00][26051-3-3-0.83][26120-0-2--0.35][26321-1-1-0.66][26732-1-1-0.42][26784-3-3-0.96][26827-3-3-0.28][26833-0-3-0.20][26838-2-1--0.25][26860-1-1--0.29]
[26948-0-1--0.46][27049-3-3--0.98][27098-1-1-0.73][27526-0-1--0.03][27639-3-3-0.08][27698-3-3-0.15][27772-0-3--0.13][27890-1-1-0.99][28040-0-2--0.94][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-3--0.56][29198-3-1-0.99][29777-0-0--1.00][29877-2-1-0.19][30035-1-1-0.99][30098-0-3-0.19][30326-1-1-0.99][30572-2-3--0.44][30716-0-1--0.92]
[30806-2-1-0.11][30906-1-1-0.99][31007-0-1--0.44][31181-3-3--0.68][31238-0-3--0.67][31347-0-3--0.11][31422-2-1-0.99][31429-3-0--1.00][31431-0-1--0.84][31432-1-1-0.74]
[31477-0-3-0.82][31524-1-1--0.30][31597-1-1-0.30][31619-1-1--0.86][31701-0-3--0.84][31755-0-3--0.97][31854-3-3--0.14][32074-1-1--0.37][32078-3-1--0.98][32111-1-1-0.99]
[32127-1-2-0.69][32140-3-3--1.00][32263-2-1-0.09][32365-0-1-0.76][32411-2-3-0.60][32429-3-3--0.24][32473-3-3--0.07][32574-3-3-0.14][32584-0-0--1.00][32622-0-1--0.32]
[32858-3-3--0.18][32969-3-0--1.00][33016-2-1-0.99][33031-1-1--0.18][33035-2-2--0.72][33133-2-1--0.33][33173-2-1-0.94][33175-3-1-0.77][33306-3-1--0.11][33309-2-1-0.30]
[33474-0-1--0.35][33478-2-0--1.00][33618-1-1-0.65][33712-0-1--0.94][33782-2-1-0.99][33914-3-3-0.45][34076-3-3--0.61][34112-2-1-0.60][34138-2-1-0.32][34239-1-1-0.99]
[34364-2-1-0.73][34617-1-1-0.99][34751-3-3-0.93][34783-2-1-0.99][35015-3-3--0.24][35018-1-1-0.99][35288-2-1--0.58]
---------------------------
I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.192 | Acc: 46.750% | Wgt Acc: 49.413%
	I - Batch: 100 | Loss: 1.202 | Acc: 45.375% | Wgt Acc: 47.745%
	I - Batch: 150 | Loss: 1.204 | Acc: 45.458% | Wgt Acc: 47.418%
I - num batch: 160
I - Train -- Loss: 1.204 | Acc: 45.230% | Wgt Acc: 47.134% | LR: 1.000000e-03 | Dur: 87.45s
I - Confusion Matrix: [row->prediction - col->label]
[[121.   6.  23.  30.]
 [ 96. 357. 305.  74.]
 [125. 162. 311.  71.]
 [355.  53.  95. 363.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.278 | Acc: 40.673% | Wgt Acc: 41.984% | Dur: 10.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 5.  1.  0.  1.]
 [14. 51. 29. 31.]
 [23. 26. 42. 19.]
 [46.  0.  4. 35.]]

I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.175 | Acc: 46.500% | Wgt Acc: 48.625%
	I - Batch: 100 | Loss: 1.164 | Acc: 48.250% | Wgt Acc: 50.028%
	I - Batch: 150 | Loss: 1.148 | Acc: 50.000% | Wgt Acc: 51.780%
I - num batch: 160
I - Train -- Loss: 1.152 | Acc: 49.823% | Wgt Acc: 51.584% | LR: 1.000000e-03 | Dur: 87.72s
I - Confusion Matrix: [row->prediction - col->label]
[[150.   5.  18.  39.]
 [104. 403. 278.  74.]
 [112. 128. 364.  73.]
 [331.  42.  74. 352.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.169 | Acc: 48.930% | Wgt Acc: 51.427% | Dur: 10.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 6.  1.  2.  0.]
 [10. 47. 20.  9.]
 [11. 20. 37.  7.]
 [61. 10. 16. 70.]]

I - Local maximum validation set accuracy:  48.93

I - Validation set results: 
[14-1-2-0.77][50-3-1-0.72][124-2-3--0.43][127-0-3--0.89][443-2-2-0.99][567-0-3-0.70][573-1-1-0.70][615-0-3-0.93][695-1-2--0.24][722-3-3-0.99]
[826-0-3--0.72][878-0-3--0.91][1103-0-1--0.37][1212-3-3--0.14][1368-0-0--1.00][2181-2-3-0.99][2476-2-2--0.42][2721-2-2-0.99][2818-1-2-0.69][2886-2-1-0.99]
[3231-2-1-0.99][3333-2-3-0.97][3482-2-2-0.14][3536-3-3-0.99][3625-1-1-0.99][3909-0-1-0.25][4035-0-3-0.99][4140-0-0--1.00][4214-1-3-0.20][4346-1-3-0.68]
[4581-2-2-0.82][4708-3-3-0.12][4838-3-2--0.97][4845-1-1-0.16][4868-0-0--1.00][4939-0-2--0.05][4984-2-3-0.12][5078-1-1-0.18][5396-0-3-0.16][5479-1-1-0.53]
[5717-0-1-0.12][5843-1-1-0.87][5949-3-3-0.99][5987-2-1-0.99][6014-3-1-0.12][6033-3-3--0.33][6313-0-3-0.84][6421-3-3-0.99][6500-1-1-0.16][6583-3-3-0.99]
[6683-3-3-0.89][6825-2-3-0.40][6998-3-3-0.53][7049-3-3--0.31][7517-1-1-0.99][7521-1-3-0.11][7528-1-2--0.14][7949-1-2-0.65][8135-1-0--1.00][8185-3-3--0.68]
[8269-3-2-0.99][8273-3-3-0.00][8543-3-3-0.74][8666-1-1-0.60][8672-0-3-0.15][8903-1-2-0.99][9001-2-1-0.90][9036-2-2-0.96][9281-3-2-0.65][9300-2-2-0.36]
[9571-0-3-0.87][9617-1-1-0.43][9644-2-2-0.99][9705-2-2-0.48][9801-0-3-0.95][9803-3-3-0.03][9865-3-3-0.92][9896-2-2-0.99][10314-1-1-0.76][10337-3-3-0.99]
[10403-0-2-0.93][10653-2-1-0.74][10704-2-1-0.99][10719-1-1-0.99][10727-1-1-0.99][10836-0-3-0.99][10969-2-3-0.86][11042-0-3--0.65][11088-1-1-0.95][11322-0-3-0.91]
[11398-2-2-0.98][11499-0-2--0.36][11502-3-3-0.99][11512-3-3-0.26][11608-1-1-0.99][11610-0-3-0.95][11692-0-3-0.80][11905-0-3-0.42][11993-1-1-0.95][12002-2-3--0.08]
[12052-0-3--0.56][12201-0-3-0.77][12235-2-1-0.99][12320-1-2-0.03][12377-2-1-0.93][12398-2-1-0.32][12503-1-2-0.99][12617-0-1-0.73][12685-3-3--0.30][12738-2-3-0.98]
[12742-2-2-0.81][12823-0-3-0.99][13110-1-3--0.60][13240-3-3-0.99][13253-1-1-0.99][13273-0-3-0.99][13634-1-2-0.99][13763-2-3-0.99][13905-3-3-0.24][14060-2-1-0.97]
[14065-3-3-0.99][14147-3-3-0.92][14595-2-2-0.99][14687-2-2-0.42][14788-2-2-0.54][14869-1-1-0.97][14872-3-1--0.93][14877-1-1-0.90][14927-0-3-0.99][15066-0-3-0.15]
[15175-1-1-0.66][15178-2-3--0.57][15375-3-3-0.77][15389-3-3-0.95][15568-2-1-0.99][15675-3-3-0.99][15869-1-3-0.04][16207-3-3--0.73][16236-0-3-0.11][16302-3-3--0.11]
[16331-2-2-0.99][16381-0-3--0.36][16488-1-1-0.87][16495-0-0--1.00][16650-0-3-0.19][16719-1-1-0.84][16801-0-3--0.48][16828-0-2--1.00][17137-3-3-0.80][17245-1-2--0.16]
[17278-3-1--0.60][17282-0-3--0.94][17311-2-2-0.99][17336-2-2-0.99][17608-3-3-0.99][17627-0-3--0.08][17877-3-1-0.95][17924-1-3--0.04][17984-3-3-0.99][18211-0-3-0.83]
[18276-3-3--0.34][18287-1-1-0.83][18394-0-3-0.36][18428-0-1-0.90][18442-0-3-0.95][18478-3-3--0.13][18607-0-1--0.34][18616-0-1--0.36][18663-0-3-0.12][18718-0-3-0.99]
[18766-2-1-0.99][18824-2-1-0.99][18890-3-2--0.32][18930-3-2-0.47][18938-3-3-0.99][19817-1-2-0.70][19839-0-2-0.27][19930-3-3-0.60][19944-0-2-0.99][20036-2-2--0.45]
[20101-3-3-0.28][20474-1-2-0.26][20547-3-1--0.72][20929-2-2-0.99][21245-1-1-0.99][21257-3-3-0.99][21293-1-1-0.99][21316-1-1-0.99][21384-1-1-0.99][21448-1-1-0.99]
[21483-0-3--0.58][21487-2-2-0.99][21714-0-3-0.70][21943-3-1-0.81][21947-0-3--0.86][21948-0-0--1.00][21965-2-2-0.49][21998-1-1-0.17][22025-0-3-0.71][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.84][22757-0-3--0.06][22811-3-3-0.99][22976-3-1-0.32][22985-3-3-0.99][23014-0-3-0.93][23112-1-1-0.99][23144-3-3-0.07][23168-2-0--1.00]
[23219-0-3--0.15][23363-3-3-0.99][23470-0-1-0.38][23486-2-3--0.41][23497-0-3-0.99][23516-0-3--0.83][23690-1-3--0.88][23921-2-2-0.86][23936-1-2-0.10][24040-3-3-0.88]
[24111-1-1-0.99][24182-0-3-0.99][24238-3-3-0.99][24290-2-0--1.00][24345-0-2--0.49][24364-1-2-0.37][24427-3-3-0.87][24477-2-2-0.99][24495-2-1-0.72][24893-2-1-0.99]
[25012-1-1-0.20][25121-2-2-0.23][25165-3-3-0.02][25183-0-3--0.92][25297-3-3-0.99][25398-0-3--0.93][25574-2-1--0.32][25644-1-1-0.99][25718-1-1-0.99][25774-2-3--0.26]
[26032-3-3-0.83][26051-3-3-0.99][26120-0-2--0.45][26321-1-2--0.19][26732-1-1-0.98][26784-3-3-0.99][26827-3-3-0.24][26833-0-3-0.99][26838-2-3--0.41][26860-1-2-0.99]
[26948-0-1--0.94][27049-3-3--0.15][27098-1-3--0.81][27526-0-3--0.37][27639-3-3--0.01][27698-3-3-0.99][27772-0-3-0.93][27890-1-1-0.96][28040-0-2-0.99][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-3--0.22][29198-3-1--0.25][29777-0-3--0.92][29877-2-2-0.40][30035-1-1-0.99][30098-0-3-0.99][30326-1-1-0.99][30572-2-2-0.95][30716-0-3--0.51]
[30806-2-3--0.04][30906-1-1-0.99][31007-0-2-0.47][31181-3-3-0.88][31238-0-3-0.90][31347-0-3-0.72][31422-2-1--0.23][31429-3-3--0.34][31431-0-3-0.58][31432-1-1-0.60]
[31477-0-3-0.87][31524-1-3-0.99][31597-1-2-0.97][31619-1-2--0.24][31701-0-3--0.46][31755-0-3--0.93][31854-3-3-0.78][32074-1-2-0.55][32078-3-3-0.99][32111-1-1-0.99]
[32127-1-2-0.99][32140-3-3-0.99][32263-2-2-0.05][32365-0-1--0.37][32411-2-3-0.99][32429-3-3-0.99][32473-3-3-0.99][32574-3-3--0.05][32584-0-0--1.00][32622-0-2-0.06]
[32858-3-3-0.98][32969-3-3-0.34][33016-2-2-0.95][33031-1-3--0.04][33035-2-2-0.99][33133-2-2-0.99][33173-2-2-0.11][33175-3-2-0.99][33306-3-2-0.13][33309-2-1--0.28]
[33474-0-3--0.90][33478-2-3--0.02][33618-1-1-0.99][33712-0-3-0.77][33782-2-1-0.99][33914-3-3-0.86][34076-3-3-0.31][34112-2-2-0.98][34138-2-2-0.99][34239-1-1-0.44]
[34364-2-2-0.96][34617-1-1-0.81][34751-3-3-0.99][34783-2-1-0.91][35015-3-3-0.74][35018-1-1-0.97][35288-2-2-0.24]
---------------------------
I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.095 | Acc: 54.250% | Wgt Acc: 55.899%
	I - Batch: 100 | Loss: 1.089 | Acc: 53.438% | Wgt Acc: 55.323%
	I - Batch: 150 | Loss: 1.095 | Acc: 54.500% | Wgt Acc: 56.147%
I - num batch: 160
I - Train -- Loss: 1.093 | Acc: 54.810% | Wgt Acc: 56.405% | LR: 1.000000e-03 | Dur: 89.29s
I - Confusion Matrix: [row->prediction - col->label]
[[183.   8.   9.  38.]
 [ 94. 423. 245.  61.]
 [102. 115. 421.  70.]
 [318.  32.  59. 369.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.215 | Acc: 46.177% | Wgt Acc: 46.807% | Dur: 10.02s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  2.  3.  9.]
 [ 8. 37. 18. 15.]
 [19. 37. 51. 14.]
 [46.  2.  3. 48.]]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 1.058 | Acc: 60.250% | Wgt Acc: 61.523%
	I - Batch: 100 | Loss: 1.084 | Acc: 56.375% | Wgt Acc: 57.820%
	I - Batch: 150 | Loss: 1.087 | Acc: 55.667% | Wgt Acc: 57.227%
I - num batch: 160
I - Train -- Loss: 1.083 | Acc: 56.105% | Wgt Acc: 57.626% | LR: 1.000000e-03 | Dur: 85.91s
I - Confusion Matrix: [row->prediction - col->label]
[[196.   4.  12.  22.]
 [ 98. 420. 222.  71.]
 [110. 106. 435.  67.]
 [293.  48.  65. 378.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.178 | Acc: 50.765% | Wgt Acc: 52.378% | Dur: 10.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 9.  1.  1.  1.]
 [ 3. 36. 12.  3.]
 [19. 33. 50. 11.]
 [57.  8. 12. 71.]]

I - Local maximum validation set accuracy:  50.76

I - Validation set results: 
[14-1-2-0.67][50-3-3--0.16][124-2-2-0.32][127-0-3-0.19][443-2-2-0.99][567-0-3--0.24][573-1-1-0.74][615-0-3-0.99][695-1-2-0.99][722-3-3-0.99]
[826-0-3--0.54][878-0-3-0.39][1103-0-2--0.57][1212-3-3-0.07][1368-0-0--1.00][2181-2-3-0.99][2476-2-2-0.54][2721-2-2-0.99][2818-1-2-0.54][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-1-0.57][3482-2-2-0.99][3536-3-3-0.96][3625-1-1-0.99][3909-0-2--0.46][4035-0-3-0.99][4140-0-0--1.00][4214-1-3-0.43][4346-1-3-0.61]
[4581-2-2-0.99][4708-3-2-0.19][4838-3-3--0.96][4845-1-2-0.78][4868-0-0--1.00][4939-0-2-0.90][4984-2-2-0.19][5078-1-2-0.76][5396-0-3-0.29][5479-1-1-0.99]
[5717-0-0--1.00][5843-1-1-0.99][5949-3-3-0.99][5987-2-2-0.99][6014-3-3-0.95][6033-3-3--0.21][6313-0-3-0.99][6421-3-3-0.99][6500-1-1--0.20][6583-3-3-0.99]
[6683-3-3-0.15][6825-2-1-0.36][6998-3-3-0.61][7049-3-3--0.61][7517-1-1-0.99][7521-1-2--0.01][7528-1-3-0.58][7949-1-2-0.99][8135-1-0--1.00][8185-3-3-0.24]
[8269-3-2-0.80][8273-3-3-0.89][8543-3-3-0.79][8666-1-1-0.85][8672-0-3-0.49][8903-1-3-0.90][9001-2-2-0.99][9036-2-2-0.95][9281-3-2-0.66][9300-2-2-0.99]
[9571-0-3-0.61][9617-1-1-0.72][9644-2-2-0.99][9705-2-2-0.99][9801-0-3-0.73][9803-3-3-0.31][9865-3-3-0.99][9896-2-2-0.99][10314-1-1-0.99][10337-3-3-0.99]
[10403-0-2-0.56][10653-2-1-0.87][10704-2-3-0.68][10719-1-1-0.99][10727-1-1-0.99][10836-0-3-0.99][10969-2-3-0.33][11042-0-0--1.00][11088-1-1-0.99][11322-0-3--0.93]
[11398-2-2-0.99][11499-0-3--0.49][11502-3-3-0.99][11512-3-3-0.53][11608-1-1-0.99][11610-0-3--0.71][11692-0-3-0.97][11905-0-3--0.76][11993-1-1-0.99][12002-2-3--0.45]
[12052-0-2--0.58][12201-0-3-0.99][12235-2-1-0.82][12320-1-2--0.58][12377-2-2-0.97][12398-2-2-0.19][12503-1-1-0.99][12617-0-2-0.99][12685-3-3-0.76][12738-2-2-0.47]
[12742-2-2-0.91][12823-0-3-0.99][13110-1-3-0.66][13240-3-3-0.73][13253-1-1-0.99][13273-0-3-0.87][13634-1-2-0.99][13763-2-3-0.99][13905-3-3-0.65][14060-2-1-0.84]
[14065-3-3-0.66][14147-3-3-0.86][14595-2-2-0.99][14687-2-3-0.67][14788-2-2-0.86][14869-1-1-0.50][14872-3-2--0.71][14877-1-1-0.59][14927-0-3-0.99][15066-0-3-0.39]
[15175-1-2-0.25][15178-2-3--0.10][15375-3-3-0.50][15389-3-3-0.99][15568-2-1-0.99][15675-3-3-0.37][15869-1-2-0.18][16207-3-3--0.56][16236-0-3-0.98][16302-3-3-0.29]
[16331-2-2-0.99][16381-0-3--0.34][16488-1-1-0.99][16495-0-3--0.91][16650-0-3--0.76][16719-1-2-0.83][16801-0-0--1.00][16828-0-3--0.61][17137-3-3--0.88][17245-1-2--0.43]
[17278-3-3--0.45][17282-0-2--0.79][17311-2-2-0.99][17336-2-1-0.99][17608-3-3-0.99][17627-0-3-0.16][17877-3-1-0.99][17924-1-3-0.23][17984-3-3-0.82][18211-0-3-0.42]
[18276-3-3--0.29][18287-1-1-0.93][18394-0-3-0.16][18428-0-2--0.33][18442-0-3-0.99][18478-3-3-0.90][18607-0-1--0.90][18616-0-1--0.47][18663-0-3-0.74][18718-0-3-0.99]
[18766-2-2-0.99][18824-2-2-0.98][18890-3-2--0.10][18930-3-2-0.14][18938-3-3-0.99][19817-1-2--0.13][19839-0-2-0.99][19930-3-3-0.29][19944-0-2--0.15][20036-2-2-0.99]
[20101-3-3-0.14][20474-1-2-0.99][20547-3-2--0.72][20929-2-2-0.99][21245-1-1-0.97][21257-3-3-0.98][21293-1-1-0.99][21316-1-1--0.04][21384-1-2-0.99][21448-1-2-0.73]
[21483-0-3--0.07][21487-2-2-0.99][21714-0-2-0.57][21943-3-2-0.14][21947-0-3--0.39][21948-0-0--1.00][21965-2-2-0.99][21998-1-1-0.28][22025-0-3-0.99][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-3-0.99][22811-3-3-0.99][22976-3-2-0.37][22985-3-3-0.99][23014-0-3-0.99][23112-1-2-0.99][23144-3-3-0.35][23168-2-3--0.84]
[23219-0-3--0.68][23363-3-3-0.90][23470-0-2-0.44][23486-2-3-0.45][23497-0-3-0.99][23516-0-0--1.00][23690-1-3-0.12][23921-2-2-0.99][23936-1-2-0.99][24040-3-2-0.96]
[24111-1-1-0.99][24182-0-3-0.99][24238-3-3-0.99][24290-2-0--1.00][24345-0-2-0.35][24364-1-2-0.52][24427-3-3-0.30][24477-2-2-0.99][24495-2-1-0.29][24893-2-2-0.99]
[25012-1-2--0.35][25121-2-2-0.98][25165-3-3-0.12][25183-0-2--0.81][25297-3-3-0.99][25398-0-3--0.59][25574-2-3-0.63][25644-1-1-0.99][25718-1-1-0.99][25774-2-2-0.93]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-2--0.89][26321-1-2-0.24][26732-1-1-0.70][26784-3-3-0.99][26827-3-3-0.94][26833-0-3-0.99][26838-2-2-0.17][26860-1-2-0.99]
[26948-0-3--0.94][27049-3-0--1.00][27098-1-2-0.02][27526-0-3-0.13][27639-3-3-0.95][27698-3-3-0.99][27772-0-3-0.77][27890-1-1-0.22][28040-0-2-0.75][28503-2-2-0.99]
[28577-1-2-0.99][28959-0-3-0.71][29198-3-1--0.01][29777-0-3--0.18][29877-2-2-0.60][30035-1-2-0.99][30098-0-3--0.15][30326-1-1-0.97][30572-2-2-0.99][30716-0-1--0.77]
[30806-2-3--0.13][30906-1-1-0.99][31007-0-0--1.00][31181-3-3-0.91][31238-0-3-0.34][31347-0-3-0.99][31422-2-2--0.27][31429-3-3--0.84][31431-0-3-0.99][31432-1-1-0.73]
[31477-0-3-0.99][31524-1-2-0.06][31597-1-2-0.99][31619-1-2-0.62][31701-0-3--0.40][31755-0-3--0.28][31854-3-3-0.60][32074-1-2--0.09][32078-3-3-0.99][32111-1-1-0.96]
[32127-1-2-0.99][32140-3-3-0.99][32263-2-2--0.04][32365-0-2--0.63][32411-2-3-0.99][32429-3-3-0.99][32473-3-3-0.99][32574-3-3--0.85][32584-0-2--0.97][32622-0-2--0.11]
[32858-3-3-0.87][32969-3-3-0.99][33016-2-2-0.99][33031-1-3--0.07][33035-2-2-0.99][33133-2-2-0.99][33173-2-2-0.99][33175-3-1-0.99][33306-3-2--0.54][33309-2-1--0.28]
[33474-0-3--0.98][33478-2-2-0.48][33618-1-1-0.82][33712-0-3-0.45][33782-2-1-0.99][33914-3-3-0.99][34076-3-3-0.99][34112-2-2-0.99][34138-2-2-0.99][34239-1-2-0.99]
[34364-2-1-0.99][34617-1-2-0.09][34751-3-3-0.99][34783-2-2-0.95][35015-3-3-0.91][35018-1-1-0.99][35288-2-2-0.72]
---------------------------
I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 1.043 | Acc: 58.625% | Wgt Acc: 60.067%
	I - Batch: 100 | Loss: 1.059 | Acc: 57.875% | Wgt Acc: 59.407%
	I - Batch: 150 | Loss: 1.051 | Acc: 57.833% | Wgt Acc: 59.288%
I - num batch: 160
I - Train -- Loss: 1.048 | Acc: 58.108% | Wgt Acc: 59.554% | LR: 1.000000e-03 | Dur: 89.56s
I - Confusion Matrix: [row->prediction - col->label]
[[190.   1.   8.  31.]
 [ 80. 432. 215.  56.]
 [122. 109. 478.  71.]
 [305.  36.  33. 380.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.235 | Acc: 42.813% | Wgt Acc: 44.633% | Dur: 10.21s
I - Confusion Matrix: [row->prediction - col->label]
[[22.  1.  3. 10.]
 [31. 58. 50. 31.]
 [ 8. 17. 21.  6.]
 [27.  2.  1. 39.]]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 1.030 | Acc: 59.375% | Wgt Acc: 61.208%
	I - Batch: 100 | Loss: 1.035 | Acc: 59.250% | Wgt Acc: 60.475%
	I - Batch: 150 | Loss: 1.034 | Acc: 58.583% | Wgt Acc: 59.992%
I - num batch: 160
I - Train -- Loss: 1.030 | Acc: 59.089% | Wgt Acc: 60.474% | LR: 1.000000e-03 | Dur: 86.11s
I - Confusion Matrix: [row->prediction - col->label]
[[210.   6.   9.  28.]
 [ 84. 433. 187.  60.]
 [100. 110. 479.  67.]
 [303.  29.  59. 383.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.178 | Acc: 47.706% | Wgt Acc: 48.234% | Dur: 11.03s
I - Confusion Matrix: [row->prediction - col->label]
[[18.  3.  2.  4.]
 [10. 36. 15.  9.]
 [24. 37. 52. 23.]
 [36.  2.  6. 50.]]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 0.973 | Acc: 64.250% | Wgt Acc: 65.427%
	I - Batch: 100 | Loss: 0.989 | Acc: 62.188% | Wgt Acc: 63.533%
	I - Batch: 150 | Loss: 1.004 | Acc: 62.250% | Wgt Acc: 63.427%
I - num batch: 160
I - Train -- Loss: 1.004 | Acc: 62.073% | Wgt Acc: 63.225% | LR: 1.000000e-03 | Dur: 89.24s
I - Confusion Matrix: [row->prediction - col->label]
[[226.  10.  10.  32.]
 [ 79. 439. 145.  58.]
 [124. 109. 532.  64.]
 [268.  20.  47. 384.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.185 | Acc: 46.789% | Wgt Acc: 49.117% | Dur: 11.63s
I - Confusion Matrix: [row->prediction - col->label]
[[ 4.  0.  1.  3.]
 [ 7. 39. 15.  6.]
 [ 8. 21. 38.  5.]
 [69. 18. 21. 72.]]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 0.979 | Acc: 63.625% | Wgt Acc: 65.051%
	I - Batch: 100 | Loss: 0.988 | Acc: 61.938% | Wgt Acc: 63.488%
	I - Batch: 150 | Loss: 0.991 | Acc: 62.292% | Wgt Acc: 63.800%
I - num batch: 160
I - Train -- Loss: 0.994 | Acc: 62.740% | Wgt Acc: 64.190% | LR: 1.000000e-03 | Dur: 91.13s
I - Confusion Matrix: [row->prediction - col->label]
[[217.   2.   9.  18.]
 [ 89. 460. 161.  49.]
 [102.  83. 517.  67.]
 [289.  33.  47. 404.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.203 | Acc: 45.260% | Wgt Acc: 47.011% | Dur: 10.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 5.  1.  2.  0.]
 [ 9. 32. 14.  6.]
 [16. 36. 43. 12.]
 [58.  9. 16. 68.]]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 0.998 | Acc: 63.500% | Wgt Acc: 64.866%
	I - Batch: 100 | Loss: 0.976 | Acc: 63.250% | Wgt Acc: 64.574%
	I - Batch: 150 | Loss: 0.982 | Acc: 63.417% | Wgt Acc: 64.652%
I - num batch: 160
I - Train -- Loss: 0.985 | Acc: 63.526% | Wgt Acc: 64.738% | LR: 1.000000e-03 | Dur: 86.34s
I - Confusion Matrix: [row->prediction - col->label]
[[228.   5.   9.  31.]
 [ 88. 446. 144.  45.]
 [100. 102. 544.  62.]
 [281.  25.  37. 400.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.315 | Acc: 38.226% | Wgt Acc: 38.791% | Dur: 10.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 9.  0.  0.  2.]
 [17. 48. 29. 18.]
 [39. 29. 45. 43.]
 [23.  1.  1. 23.]]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 0.926 | Acc: 65.875% | Wgt Acc: 66.912%
	I - Batch: 100 | Loss: 0.940 | Acc: 65.500% | Wgt Acc: 66.624%
	I - Batch: 150 | Loss: 0.928 | Acc: 66.000% | Wgt Acc: 67.315%
I - num batch: 160
I - Train -- Loss: 0.926 | Acc: 66.078% | Wgt Acc: 67.401% | LR: 5.000000e-04 | Dur: 92.66s
I - Confusion Matrix: [row->prediction - col->label]
[[213.   7.  10.  17.]
 [ 84. 473. 109.  49.]
 [131.  76. 583.  58.]
 [269.  22.  32. 414.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.169 | Acc: 54.128% | Wgt Acc: 55.027% | Dur: 10.66s
I - Confusion Matrix: [row->prediction - col->label]
[[33.  2.  2.  8.]
 [11. 44. 19.  6.]
 [12. 28. 42. 14.]
 [32.  4. 12. 58.]]

I - Local maximum validation set accuracy:  54.13

I - Validation set results: 
[14-1-1-0.42][50-3-3--0.70][124-2-2--0.99][127-0-0--1.00][443-2-2-0.99][567-0-3--0.93][573-1-1-0.48][615-0-3-0.48][695-1-2-0.78][722-3-3-0.83]
[826-0-0--1.00][878-0-0--1.00][1103-0-1--0.91][1212-3-3-0.50][1368-0-0--1.00][2181-2-3--0.33][2476-2-2--0.37][2721-2-2-0.96][2818-1-2--0.56][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-1-0.94][3482-2-2-0.65][3536-3-3-0.99][3625-1-1-0.99][3909-0-0--1.00][4035-0-3-0.13][4140-0-3--0.95][4214-1-1-0.14][4346-1-3--0.65]
[4581-2-2-0.91][4708-3-2--0.62][4838-3-0--1.00][4845-1-2-0.10][4868-0-0--1.00][4939-0-1--0.39][4984-2-2-0.96][5078-1-2-0.33][5396-0-0--1.00][5479-1-2-0.07]
[5717-0-1--0.88][5843-1-1-0.96][5949-3-3--0.76][5987-2-2-0.98][6014-3-3-0.19][6033-3-0--1.00][6313-0-3-0.68][6421-3-3-0.66][6500-1-1--0.12][6583-3-2-0.67]
[6683-3-3-0.46][6825-2-1--0.58][6998-3-2--0.06][7049-3-3-0.31][7517-1-1-0.99][7521-1-1--0.95][7528-1-3--0.15][7949-1-2-0.77][8135-1-0--1.00][8185-3-0--1.00]
[8269-3-2-0.20][8273-3-3-0.99][8543-3-0--1.00][8666-1-1-0.37][8672-0-0--1.00][8903-1-2--0.32][9001-2-1-0.99][9036-2-2-0.99][9281-3-1-0.71][9300-2-2-0.55]
[9571-0-3--0.01][9617-1-1-0.74][9644-2-2-0.84][9705-2-1--0.45][9801-0-3--0.10][9803-3-3-0.19][9865-3-3-0.38][9896-2-2-0.99][10314-1-1-0.10][10337-3-3-0.99]
[10403-0-2-0.09][10653-2-1-0.79][10704-2-1-0.43][10719-1-1-0.99][10727-1-1-0.74][10836-0-3--0.89][10969-2-3--0.78][11042-0-0--1.00][11088-1-2-0.61][11322-0-3--0.95]
[11398-2-2-0.99][11499-0-1--0.97][11502-3-3--0.70][11512-3-3-0.23][11608-1-1-0.99][11610-0-0--1.00][11692-0-0--1.00][11905-0-0--1.00][11993-1-1-0.85][12002-2-2-0.10]
[12052-0-3--1.00][12201-0-3--0.55][12235-2-1-0.37][12320-1-2--0.33][12377-2-2-0.20][12398-2-3-0.62][12503-1-2-0.99][12617-0-2--0.53][12685-3-3-0.55][12738-2-2--0.01]
[12742-2-2-0.89][12823-0-3-0.01][13110-1-2-0.00][13240-3-3--0.48][13253-1-1-0.99][13273-0-3-0.03][13634-1-2-0.50][13763-2-3-0.22][13905-3-0--1.00][14060-2-1-0.86]
[14065-3-2--0.39][14147-3-3--0.82][14595-2-2-0.99][14687-2-2-0.85][14788-2-3--0.74][14869-1-1-0.82][14872-3-2--0.99][14877-1-1-0.49][14927-0-2--0.74][15066-0-3-0.86]
[15175-1-1-0.31][15178-2-3--0.86][15375-3-3--0.41][15389-3-3--0.48][15568-2-1-0.73][15675-3-3--0.44][15869-1-3--0.49][16207-3-3--0.99][16236-0-2--0.66][16302-3-2--0.26]
[16331-2-2-0.99][16381-0-3--0.87][16488-1-1-0.96][16495-0-0--1.00][16650-0-0--1.00][16719-1-1-0.40][16801-0-0--1.00][16828-0-0--1.00][17137-3-3--0.74][17245-1-1--0.34]
[17278-3-3--0.91][17282-0-1--0.78][17311-2-2-0.84][17336-2-1--0.35][17608-3-3-0.99][17627-0-1--0.73][17877-3-1--0.58][17924-1-0--1.00][17984-3-3-0.99][18211-0-3-0.12]
[18276-3-3--0.96][18287-1-1-0.92][18394-0-0--1.00][18428-0-1--0.82][18442-0-3-0.99][18478-3-3--0.67][18607-0-0--1.00][18616-0-1--0.09][18663-0-3--0.46][18718-0-3--0.28]
[18766-2-2--0.23][18824-2-2-0.11][18890-3-2--0.36][18930-3-2--0.40][18938-3-3-0.15][19817-1-2-0.69][19839-0-2-0.52][19930-3-3-0.02][19944-0-2-0.60][20036-2-2-0.99]
[20101-3-3--0.90][20474-1-2--0.67][20547-3-1--0.24][20929-2-2-0.95][21245-1-1-0.64][21257-3-2-0.08][21293-1-1-0.99][21316-1-1-0.99][21384-1-2-0.99][21448-1-1-0.88]
[21483-0-0--1.00][21487-2-2-0.89][21714-0-3--0.05][21943-3-2--0.42][21947-0-0--1.00][21948-0-0--1.00][21965-2-1-0.87][21998-1-1-0.34][22025-0-3--0.37][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3--0.91][22757-0-3-0.92][22811-3-3-0.99][22976-3-1-0.80][22985-3-3-0.61][23014-0-3--0.86][23112-1-2-0.99][23144-3-3--0.93][23168-2-3--0.97]
[23219-0-3--0.98][23363-3-3--0.37][23470-0-1--0.70][23486-2-3--0.28][23497-0-3-0.67][23516-0-0--1.00][23690-1-2-0.92][23921-2-1-0.38][23936-1-2-0.55][24040-3-2--0.03]
[24111-1-1-0.55][24182-0-3-0.50][24238-3-3--0.16][24290-2-0--1.00][24345-0-2--0.07][24364-1-2-0.96][24427-3-3--0.49][24477-2-2-0.99][24495-2-1--0.03][24893-2-1-0.86]
[25012-1-1--0.63][25121-2-2-0.49][25165-3-0--1.00][25183-0-0--1.00][25297-3-3-0.99][25398-0-0--1.00][25574-2-2--0.92][25644-1-1-0.99][25718-1-1-0.08][25774-2-2--0.40]
[26032-3-3-0.16][26051-3-3-0.89][26120-0-2--0.26][26321-1-2--0.14][26732-1-1-0.61][26784-3-3-0.38][26827-3-3--0.36][26833-0-3-0.99][26838-2-2--0.32][26860-1-2-0.26]
[26948-0-0--1.00][27049-3-0--1.00][27098-1-1--0.93][27526-0-0--1.00][27639-3-3--0.45][27698-3-3-0.97][27772-0-0--1.00][27890-1-1-0.58][28040-0-2-0.59][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-3--0.34][29198-3-1--0.88][29777-0-0--1.00][29877-2-3--0.55][30035-1-2-0.99][30098-0-0--1.00][30326-1-1-0.99][30572-2-2-0.52][30716-0-1--0.85]
[30806-2-3--0.81][30906-1-1-0.99][31007-0-2-0.07][31181-3-2--0.01][31238-0-3-0.28][31347-0-3--0.82][31422-2-0--1.00][31429-3-3--0.97][31431-0-0--1.00][31432-1-1-0.10]
[31477-0-3-0.25][31524-1-2--0.74][31597-1-2-0.99][31619-1-2--0.77][31701-0-0--1.00][31755-0-0--1.00][31854-3-3-0.91][32074-1-2--0.54][32078-3-3-0.14][32111-1-1-0.82]
[32127-1-2-0.99][32140-3-3-0.99][32263-2-2--0.66][32365-0-2--0.90][32411-2-3--0.61][32429-3-3-0.09][32473-3-3--0.40][32574-3-3--0.78][32584-0-0--1.00][32622-0-2-0.10]
[32858-3-3--0.01][32969-3-0--1.00][33016-2-2-0.99][33031-1-3-0.10][33035-2-2-0.99][33133-2-2-0.41][33173-2-1-0.56][33175-3-2-0.99][33306-3-1--0.28][33309-2-1--0.87]
[33474-0-1--0.91][33478-2-3--0.19][33618-1-1-0.99][33712-0-3--0.15][33782-2-2-0.99][33914-3-3-0.99][34076-3-3--0.34][34112-2-2-0.99][34138-2-2-0.97][34239-1-2-0.27]
[34364-2-2-0.81][34617-1-1--0.68][34751-3-3-0.27][34783-2-1-0.29][35015-3-3--0.98][35018-1-1-0.99][35288-2-1--0.69]
---------------------------
I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 0.935 | Acc: 66.625% | Wgt Acc: 67.988%
	I - Batch: 100 | Loss: 0.908 | Acc: 68.438% | Wgt Acc: 69.843%
	I - Batch: 150 | Loss: 0.911 | Acc: 67.875% | Wgt Acc: 69.242%
I - num batch: 160
I - Train -- Loss: 0.914 | Acc: 67.845% | Wgt Acc: 69.161% | LR: 5.000000e-04 | Dur: 89.12s
I - Confusion Matrix: [row->prediction - col->label]
[[219.   2.   2.  15.]
 [ 86. 478.  88.  37.]
 [111.  85. 603.  58.]
 [281.  13.  41. 428.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.175 | Acc: 50.765% | Wgt Acc: 51.359% | Dur: 10.34s
I - Confusion Matrix: [row->prediction - col->label]
[[34.  6.  4. 17.]
 [16. 44. 21. 11.]
 [ 9. 25. 40. 10.]
 [29.  3. 10. 48.]]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 0.890 | Acc: 68.375% | Wgt Acc: 69.818%
	I - Batch: 100 | Loss: 0.900 | Acc: 67.438% | Wgt Acc: 68.897%
	I - Batch: 150 | Loss: 0.898 | Acc: 68.583% | Wgt Acc: 70.115%
I - num batch: 160
I - Train -- Loss: 0.900 | Acc: 68.708% | Wgt Acc: 70.223% | LR: 5.000000e-04 | Dur: 86.71s
I - Confusion Matrix: [row->prediction - col->label]
[[215.   5.   7.  18.]
 [ 86. 490. 102.  34.]
 [107.  63. 597.  38.]
 [289.  20.  28. 448.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.158 | Acc: 50.153% | Wgt Acc: 50.951% | Dur: 10.20s
I - Confusion Matrix: [row->prediction - col->label]
[[18.  2.  2.  4.]
 [14. 42. 14.  8.]
 [22. 31. 52. 22.]
 [34.  3.  7. 52.]]

I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 0.876 | Acc: 71.125% | Wgt Acc: 72.316%
	I - Batch: 100 | Loss: 0.874 | Acc: 70.625% | Wgt Acc: 72.066%
	I - Batch: 150 | Loss: 0.887 | Acc: 70.167% | Wgt Acc: 71.614%
I - num batch: 160
I - Train -- Loss: 0.889 | Acc: 70.004% | Wgt Acc: 71.453% | LR: 5.000000e-04 | Dur: 86.72s
I - Confusion Matrix: [row->prediction - col->label]
[[220.   5.   5.  15.]
 [ 74. 496.  72.  37.]
 [123.  65. 618.  37.]
 [280.  12.  39. 449.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.163 | Acc: 51.376% | Wgt Acc: 52.378% | Dur: 10.27s
I - Confusion Matrix: [row->prediction - col->label]
[[22.  1.  2.  3.]
 [13. 43. 17.  8.]
 [23. 31. 47. 19.]
 [30.  3.  9. 56.]]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 0.894 | Acc: 68.875% | Wgt Acc: 70.734%
	I - Batch: 100 | Loss: 0.871 | Acc: 71.062% | Wgt Acc: 72.630%
	I - Batch: 150 | Loss: 0.871 | Acc: 70.750% | Wgt Acc: 72.269%
I - num batch: 160
I - Train -- Loss: 0.876 | Acc: 70.475% | Wgt Acc: 72.028% | LR: 5.000000e-04 | Dur: 88.74s
I - Confusion Matrix: [row->prediction - col->label]
[[209.   2.   2.   9.]
 [ 79. 510.  79.  31.]
 [110.  56. 624.  46.]
 [299.  10.  29. 452.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.191 | Acc: 51.376% | Wgt Acc: 52.378% | Dur: 11.45s
I - Confusion Matrix: [row->prediction - col->label]
[[30.  0.  2. 11.]
 [16. 50. 24. 13.]
 [10. 25. 39. 13.]
 [32.  3. 10. 49.]]

I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 0.879 | Acc: 72.000% | Wgt Acc: 73.041%
	I - Batch: 100 | Loss: 0.886 | Acc: 70.750% | Wgt Acc: 72.138%
	I - Batch: 150 | Loss: 0.871 | Acc: 71.125% | Wgt Acc: 72.531%
I - num batch: 160
I - Train -- Loss: 0.871 | Acc: 70.946% | Wgt Acc: 72.381% | LR: 5.000000e-04 | Dur: 86.85s
I - Confusion Matrix: [row->prediction - col->label]
[[215.   2.   5.  13.]
 [ 89. 503.  65.  32.]
 [110.  58. 638.  42.]
 [283.  15.  26. 451.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.144 | Acc: 51.070% | Wgt Acc: 52.785% | Dur: 10.11s
I - Confusion Matrix: [row->prediction - col->label]
[[18.  1.  1.  3.]
 [19. 49. 21. 11.]
 [14. 25. 40. 12.]
 [37.  3. 13. 60.]]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 0.866 | Acc: 69.750% | Wgt Acc: 71.380%
	I - Batch: 100 | Loss: 0.860 | Acc: 70.375% | Wgt Acc: 71.927%
	I - Batch: 150 | Loss: 0.856 | Acc: 71.333% | Wgt Acc: 72.863%
I - num batch: 160
I - Train -- Loss: 0.854 | Acc: 71.182% | Wgt Acc: 72.744% | LR: 5.000000e-04 | Dur: 89.47s
I - Confusion Matrix: [row->prediction - col->label]
[[205.   1.   4.  13.]
 [ 94. 516.  66.  27.]
 [ 98.  46. 637.  43.]
 [300.  15.  27. 455.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.190 | Acc: 49.541% | Wgt Acc: 50.136% | Dur: 11.12s
I - Confusion Matrix: [row->prediction - col->label]
[[23.  2.  3.  5.]
 [12. 36. 13.  3.]
 [21. 37. 49. 24.]
 [32.  3. 10. 54.]]

I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 0.847 | Acc: 71.875% | Wgt Acc: 73.367%
	I - Batch: 100 | Loss: 0.839 | Acc: 73.562% | Wgt Acc: 75.018%
	I - Batch: 150 | Loss: 0.844 | Acc: 72.917% | Wgt Acc: 74.428%
I - num batch: 160
I - Train -- Loss: 0.847 | Acc: 72.909% | Wgt Acc: 74.425% | LR: 5.000000e-04 | Dur: 90.20s
I - Confusion Matrix: [row->prediction - col->label]
[[225.   2.   1.  13.]
 [ 83. 521.  58.  26.]
 [105.  46. 647.  35.]
 [284.   9.  28. 464.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.147 | Acc: 51.070% | Wgt Acc: 52.649% | Dur: 10.20s
I - Confusion Matrix: [row->prediction - col->label]
[[13.  0.  2.  3.]
 [19. 49. 19.  8.]
 [15. 24. 47. 17.]
 [41.  5.  7. 58.]]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 0.840 | Acc: 72.000% | Wgt Acc: 73.652%
	I - Batch: 100 | Loss: 0.836 | Acc: 71.750% | Wgt Acc: 73.408%
	I - Batch: 150 | Loss: 0.837 | Acc: 71.792% | Wgt Acc: 73.361%
I - num batch: 160
I - Train -- Loss: 0.834 | Acc: 71.771% | Wgt Acc: 73.363% | LR: 5.000000e-04 | Dur: 93.33s
I - Confusion Matrix: [row->prediction - col->label]
[[193.   3.   2.  10.]
 [ 95. 513.  56.  26.]
 [129.  50. 654.  34.]
 [280.  12.  22. 468.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.162 | Acc: 51.376% | Wgt Acc: 53.261% | Dur: 10.50s
I - Confusion Matrix: [row->prediction - col->label]
[[22.  0.  3.  5.]
 [14. 42. 20.  4.]
 [ 9. 28. 34.  7.]
 [43.  8. 18. 70.]]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 0.825 | Acc: 72.625% | Wgt Acc: 74.437%
	I - Batch: 100 | Loss: 0.825 | Acc: 72.812% | Wgt Acc: 74.539%
	I - Batch: 150 | Loss: 0.836 | Acc: 72.792% | Wgt Acc: 74.495%
I - num batch: 160
I - Train -- Loss: 0.833 | Acc: 72.988% | Wgt Acc: 74.690% | LR: 5.000000e-04 | Dur: 88.69s
I - Confusion Matrix: [row->prediction - col->label]
[[201.   2.   3.   6.]
 [ 88. 530.  54.  18.]
 [129.  37. 651.  37.]
 [279.   9.  26. 477.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.177 | Acc: 45.872% | Wgt Acc: 47.351% | Dur: 10.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 4.  0.  2.  1.]
 [14. 37. 16.  5.]
 [23. 35. 49. 20.]
 [47.  6.  8. 60.]]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.800 | Acc: 74.750% | Wgt Acc: 76.424%
	I - Batch: 100 | Loss: 0.813 | Acc: 74.062% | Wgt Acc: 75.680%
	I - Batch: 150 | Loss: 0.798 | Acc: 75.250% | Wgt Acc: 76.864%
I - num batch: 160
I - Train -- Loss: 0.805 | Acc: 74.715% | Wgt Acc: 76.354% | LR: 2.500000e-04 | Dur: 88.75s
I - Confusion Matrix: [row->prediction - col->label]
[[215.   1.   4.   9.]
 [ 79. 540.  38.  17.]
 [119.  29. 669.  33.]
 [284.   8.  23. 479.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.145 | Acc: 48.930% | Wgt Acc: 50.611% | Dur: 10.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 9.  0.  2.  2.]
 [17. 49. 17. 10.]
 [21. 24. 46. 18.]
 [41.  5. 10. 56.]]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 0.813 | Acc: 74.375% | Wgt Acc: 75.871%
	I - Batch: 100 | Loss: 0.777 | Acc: 76.562% | Wgt Acc: 78.187%
	I - Batch: 150 | Loss: 0.791 | Acc: 75.250% | Wgt Acc: 76.901%
I - num batch: 160
I - Train -- Loss: 0.794 | Acc: 75.383% | Wgt Acc: 76.999% | LR: 2.500000e-04 | Dur: 86.28s
I - Confusion Matrix: [row->prediction - col->label]
[[217.   2.   4.   9.]
 [ 87. 542.  34.  16.]
 [108.  29. 679.  31.]
 [285.   5.  17. 482.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.165 | Acc: 46.789% | Wgt Acc: 47.894% | Dur: 10.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 7.  0.  1.  1.]
 [13. 34. 13.  5.]
 [24. 37. 53. 21.]
 [44.  7.  8. 59.]]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.757 | Acc: 77.125% | Wgt Acc: 78.768%
	I - Batch: 100 | Loss: 0.765 | Acc: 76.125% | Wgt Acc: 77.809%
	I - Batch: 150 | Loss: 0.772 | Acc: 75.875% | Wgt Acc: 77.571%
I - num batch: 160
I - Train -- Loss: 0.771 | Acc: 75.697% | Wgt Acc: 77.380% | LR: 2.500000e-04 | Dur: 87.84s
I - Confusion Matrix: [row->prediction - col->label]
[[208.   5.   3.  14.]
 [ 71. 545.  27.  13.]
 [109.  23. 685.  21.]
 [309.   5.  19. 490.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.175 | Acc: 48.012% | Wgt Acc: 49.117% | Dur: 10.49s
I - Confusion Matrix: [row->prediction - col->label]
[[21.  1.  3.  2.]
 [17. 40. 20. 11.]
 [12. 27. 41. 18.]
 [38. 10. 11. 55.]]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 0.733 | Acc: 80.250% | Wgt Acc: 81.594%
	I - Batch: 100 | Loss: 0.764 | Acc: 77.375% | Wgt Acc: 79.064%
	I - Batch: 150 | Loss: 0.775 | Acc: 76.583% | Wgt Acc: 78.262%
I - num batch: 160
I - Train -- Loss: 0.774 | Acc: 76.561% | Wgt Acc: 78.247% | LR: 2.500000e-04 | Dur: 86.20s
I - Confusion Matrix: [row->prediction - col->label]
[[220.   3.   5.   8.]
 [ 82. 554.  32.  17.]
 [122.  18. 685.  22.]
 [273.   3.  12. 491.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.128 | Acc: 50.153% | Wgt Acc: 51.427% | Dur: 10.29s
I - Confusion Matrix: [row->prediction - col->label]
[[14.  0.  2.  3.]
 [13. 41. 12. 10.]
 [19. 31. 49. 13.]
 [42.  6. 12. 60.]]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.754 | Acc: 78.125% | Wgt Acc: 79.735%
	I - Batch: 100 | Loss: 0.753 | Acc: 77.750% | Wgt Acc: 79.359%
	I - Batch: 150 | Loss: 0.769 | Acc: 77.000% | Wgt Acc: 78.584%
I - num batch: 160
I - Train -- Loss: 0.767 | Acc: 77.071% | Wgt Acc: 78.645% | LR: 2.500000e-04 | Dur: 87.94s
I - Confusion Matrix: [row->prediction - col->label]
[[238.   1.   2.  11.]
 [ 81. 549.  29.  15.]
 [109.  23. 687.  23.]
 [269.   5.  16. 489.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.142 | Acc: 49.847% | Wgt Acc: 52.106% | Dur: 10.05s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  0.  0.  2.]
 [11. 40. 20.  4.]
 [10. 24. 33.  5.]
 [52. 14. 22. 75.]]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.772 | Acc: 75.125% | Wgt Acc: 76.899%
	I - Batch: 100 | Loss: 0.765 | Acc: 76.312% | Wgt Acc: 78.042%
	I - Batch: 150 | Loss: 0.769 | Acc: 76.333% | Wgt Acc: 77.954%
I - num batch: 160
I - Train -- Loss: 0.764 | Acc: 76.561% | Wgt Acc: 78.194% | LR: 1.250000e-04 | Dur: 87.66s
I - Confusion Matrix: [row->prediction - col->label]
[[221.   5.   4.  11.]
 [ 83. 555.  27.  15.]
 [113.  14. 690.  28.]
 [280.   4.  13. 484.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.175 | Acc: 50.459% | Wgt Acc: 52.582% | Dur: 10.43s
I - Confusion Matrix: [row->prediction - col->label]
[[21.  1.  1.  3.]
 [ 7. 38. 19.  4.]
 [ 4. 18. 30.  3.]
 [56. 21. 25. 76.]]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.740 | Acc: 77.250% | Wgt Acc: 79.046%
	I - Batch: 100 | Loss: 0.752 | Acc: 76.812% | Wgt Acc: 78.588%
	I - Batch: 150 | Loss: 0.757 | Acc: 76.458% | Wgt Acc: 78.212%
I - num batch: 160
I - Train -- Loss: 0.749 | Acc: 76.757% | Wgt Acc: 78.503% | LR: 1.250000e-04 | Dur: 90.09s
I - Confusion Matrix: [row->prediction - col->label]
[[209.   1.   3.   9.]
 [ 81. 558.  28.  13.]
 [118.  13. 692.  20.]
 [289.   6.  11. 496.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.133 | Acc: 49.235% | Wgt Acc: 51.291% | Dur: 10.95s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  0.  2.  2.]
 [12. 44. 18.  7.]
 [14. 23. 35. 10.]
 [47. 11. 20. 67.]]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.772 | Acc: 76.125% | Wgt Acc: 78.015%
	I - Batch: 100 | Loss: 0.754 | Acc: 76.938% | Wgt Acc: 78.704%
	I - Batch: 150 | Loss: 0.747 | Acc: 77.417% | Wgt Acc: 79.159%
I - num batch: 160
I - Train -- Loss: 0.747 | Acc: 77.307% | Wgt Acc: 79.052% | LR: 1.250000e-04 | Dur: 87.14s
I - Confusion Matrix: [row->prediction - col->label]
[[217.   1.   3.  10.]
 [ 91. 557.  24.  11.]
 [ 97.  14. 692.  14.]
 [292.   6.  15. 503.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.155 | Acc: 51.682% | Wgt Acc: 52.582% | Dur: 10.12s
I - Confusion Matrix: [row->prediction - col->label]
[[18.  1.  1.  2.]
 [14. 39. 13.  5.]
 [26. 30. 53. 20.]
 [30.  8.  8. 59.]]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.734 | Acc: 78.250% | Wgt Acc: 79.809%
	I - Batch: 100 | Loss: 0.736 | Acc: 78.125% | Wgt Acc: 79.749%
	I - Batch: 150 | Loss: 0.743 | Acc: 78.083% | Wgt Acc: 79.746%
I - num batch: 160
I - Train -- Loss: 0.743 | Acc: 78.210% | Wgt Acc: 79.866% | LR: 1.250000e-04 | Dur: 93.77s
I - Confusion Matrix: [row->prediction - col->label]
[[227.   2.   3.  10.]
 [ 91. 560.  19.   8.]
 [ 95.  12. 705.  20.]
 [284.   4.   7. 500.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.158 | Acc: 47.095% | Wgt Acc: 48.709% | Dur: 10.89s
I - Confusion Matrix: [row->prediction - col->label]
[[13.  0.  2.  3.]
 [21. 46. 26. 10.]
 [22. 30. 40. 18.]
 [32.  2.  7. 55.]]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.752 | Acc: 76.625% | Wgt Acc: 78.377%
	I - Batch: 100 | Loss: 0.743 | Acc: 76.812% | Wgt Acc: 78.583%
	I - Batch: 150 | Loss: 0.741 | Acc: 77.708% | Wgt Acc: 79.409%
I - num batch: 160
I - Train -- Loss: 0.743 | Acc: 77.581% | Wgt Acc: 79.308% | LR: 1.250000e-04 | Dur: 93.61s
I - Confusion Matrix: [row->prediction - col->label]
[[219.   1.   3.   6.]
 [ 90. 555.  23.  13.]
 [122.  18. 696.  13.]
 [266.   4.  12. 506.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.139 | Acc: 48.624% | Wgt Acc: 50.611% | Dur: 10.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 8.  0.  2.  3.]
 [14. 40. 17.  4.]
 [21. 29. 42. 10.]
 [45.  9. 14. 69.]]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.734 | Acc: 80.500% | Wgt Acc: 82.071%
	I - Batch: 100 | Loss: 0.738 | Acc: 79.438% | Wgt Acc: 81.089%
	I - Batch: 150 | Loss: 0.738 | Acc: 79.042% | Wgt Acc: 80.719%
I - num batch: 160
I - Train -- Loss: 0.739 | Acc: 79.113% | Wgt Acc: 80.732% | LR: 1.250000e-04 | Dur: 92.41s
I - Confusion Matrix: [row->prediction - col->label]
[[245.   4.   4.   6.]
 [ 69. 561.  15.  16.]
 [113.   6. 704.  11.]
 [270.   7.  11. 505.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.167 | Acc: 47.401% | Wgt Acc: 49.253% | Dur: 10.03s
I - Confusion Matrix: [row->prediction - col->label]
[[11.  0.  1.  3.]
 [ 6. 31. 11.  0.]
 [14. 32. 39.  9.]
 [57. 15. 24. 74.]]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.760 | Acc: 76.625% | Wgt Acc: 78.359%
	I - Batch: 100 | Loss: 0.735 | Acc: 77.625% | Wgt Acc: 79.367%
	I - Batch: 150 | Loss: 0.739 | Acc: 77.333% | Wgt Acc: 79.078%
I - num batch: 160
I - Train -- Loss: 0.741 | Acc: 77.267% | Wgt Acc: 78.990% | LR: 1.250000e-04 | Dur: 91.29s
I - Confusion Matrix: [row->prediction - col->label]
[[212.   2.   4.   7.]
 [ 85. 559.  18.  13.]
 [120.  14. 699.  20.]
 [280.   3.  13. 498.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.143 | Acc: 49.235% | Wgt Acc: 51.087% | Dur: 10.02s
I - Confusion Matrix: [row->prediction - col->label]
[[13.  1.  1.  2.]
 [14. 40. 15.  6.]
 [15. 27. 40. 10.]
 [46. 10. 19. 68.]]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.740 | Acc: 79.125% | Wgt Acc: 80.729%
	I - Batch: 100 | Loss: 0.739 | Acc: 78.312% | Wgt Acc: 79.994%
	I - Batch: 150 | Loss: 0.735 | Acc: 78.958% | Wgt Acc: 80.605%
I - num batch: 160
I - Train -- Loss: 0.732 | Acc: 79.388% | Wgt Acc: 81.016% | LR: 1.250000e-04 | Dur: 88.32s
I - Confusion Matrix: [row->prediction - col->label]
[[246.   2.   4.   9.]
 [ 82. 563.  13.   7.]
 [101.   9. 706.  15.]
 [268.   4.  11. 507.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.126 | Acc: 49.847% | Wgt Acc: 51.902% | Dur: 10.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 9.  1.  1.  2.]
 [13. 44. 15.  3.]
 [17. 26. 42. 13.]
 [49.  7. 17. 68.]]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.720 | Acc: 80.000% | Wgt Acc: 81.586%
	I - Batch: 100 | Loss: 0.732 | Acc: 78.875% | Wgt Acc: 80.543%
	I - Batch: 150 | Loss: 0.730 | Acc: 78.625% | Wgt Acc: 80.263%
I - num batch: 160
I - Train -- Loss: 0.727 | Acc: 78.642% | Wgt Acc: 80.281% | LR: 1.250000e-04 | Dur: 86.89s
I - Confusion Matrix: [row->prediction - col->label]
[[234.   3.   3.   8.]
 [ 76. 560.  15.  10.]
 [121.  10. 706.  17.]
 [266.   5.  10. 503.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.158 | Acc: 48.624% | Wgt Acc: 49.932% | Dur: 10.12s
I - Confusion Matrix: [row->prediction - col->label]
[[12.  1.  2.  3.]
 [16. 38. 12.  6.]
 [22. 34. 48. 16.]
 [38.  5. 13. 61.]]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.734 | Acc: 76.500% | Wgt Acc: 78.304%
	I - Batch: 100 | Loss: 0.717 | Acc: 78.750% | Wgt Acc: 80.308%
	I - Batch: 150 | Loss: 0.722 | Acc: 78.792% | Wgt Acc: 80.402%
I - num batch: 160
I - Train -- Loss: 0.725 | Acc: 78.406% | Wgt Acc: 80.060% | LR: 1.250000e-04 | Dur: 87.82s
I - Confusion Matrix: [row->prediction - col->label]
[[230.   4.   5.   7.]
 [ 73. 561.  16.  13.]
 [113.   8. 705.  17.]
 [281.   5.   8. 501.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.148 | Acc: 49.847% | Wgt Acc: 51.562% | Dur: 10.17s
I - Confusion Matrix: [row->prediction - col->label]
[[13.  1.  2.  3.]
 [19. 42. 15.  7.]
 [18. 29. 43. 11.]
 [38.  6. 15. 65.]]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.713 | Acc: 81.750% | Wgt Acc: 83.315%
	I - Batch: 100 | Loss: 0.727 | Acc: 79.812% | Wgt Acc: 81.523%
	I - Batch: 150 | Loss: 0.722 | Acc: 79.667% | Wgt Acc: 81.316%
I - num batch: 160
I - Train -- Loss: 0.726 | Acc: 79.230% | Wgt Acc: 80.892% | LR: 1.250000e-04 | Dur: 86.42s
I - Confusion Matrix: [row->prediction - col->label]
[[238.   1.   2.   6.]
 [ 82. 566.  12.  12.]
 [116.   6. 708.  14.]
 [261.   5.  12. 506.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.133 | Acc: 52.294% | Wgt Acc: 53.736% | Dur: 10.09s
I - Confusion Matrix: [row->prediction - col->label]
[[17.  1.  2.  2.]
 [ 8. 42. 11.  4.]
 [18. 29. 47. 15.]
 [45.  6. 15. 65.]]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.702 | Acc: 80.500% | Wgt Acc: 82.048%
	I - Batch: 100 | Loss: 0.709 | Acc: 80.250% | Wgt Acc: 81.798%
	I - Batch: 150 | Loss: 0.719 | Acc: 79.375% | Wgt Acc: 81.004%
I - num batch: 160
I - Train -- Loss: 0.721 | Acc: 79.230% | Wgt Acc: 80.865% | LR: 1.250000e-04 | Dur: 85.36s
I - Confusion Matrix: [row->prediction - col->label]
[[241.   3.   4.   6.]
 [ 77. 560.  12.  11.]
 [106.  12. 708.  12.]
 [273.   3.  10. 509.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.144 | Acc: 47.095% | Wgt Acc: 49.117% | Dur: 10.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 8.  0.  1.  2.]
 [11. 44. 14.  5.]
 [16. 27. 39. 16.]
 [53.  7. 21. 63.]]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.749 | Acc: 77.875% | Wgt Acc: 79.594%
	I - Batch: 100 | Loss: 0.729 | Acc: 78.375% | Wgt Acc: 80.034%
	I - Batch: 150 | Loss: 0.722 | Acc: 78.667% | Wgt Acc: 80.331%
I - num batch: 160
I - Train -- Loss: 0.717 | Acc: 79.034% | Wgt Acc: 80.679% | LR: 1.250000e-04 | Dur: 88.94s
I - Confusion Matrix: [row->prediction - col->label]
[[237.   3.   4.   5.]
 [ 78. 559.  12.   9.]
 [115.   9. 708.  15.]
 [267.   7.  10. 509.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.165 | Acc: 48.624% | Wgt Acc: 50.136% | Dur: 11.13s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  0.  2.  2.]
 [18. 38. 17.  7.]
 [15. 32. 42. 13.]
 [40.  8. 14. 64.]]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.712 | Acc: 80.125% | Wgt Acc: 81.739%
	I - Batch: 100 | Loss: 0.722 | Acc: 78.938% | Wgt Acc: 80.621%
	I - Batch: 150 | Loss: 0.718 | Acc: 79.375% | Wgt Acc: 81.029%
I - num batch: 160
I - Train -- Loss: 0.719 | Acc: 79.427% | Wgt Acc: 81.077% | LR: 1.250000e-04 | Dur: 88.72s
I - Confusion Matrix: [row->prediction - col->label]
[[238.   2.   4.   7.]
 [ 90. 565.  11.   8.]
 [ 96.   7. 712.  15.]
 [273.   4.   7. 508.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.165 | Acc: 48.318% | Wgt Acc: 50.068% | Dur: 10.40s
I - Confusion Matrix: [row->prediction - col->label]
[[11.  0.  1.  2.]
 [18. 49. 20. 11.]
 [20. 25. 42. 17.]
 [39.  4. 12. 56.]]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.731 | Acc: 78.625% | Wgt Acc: 80.113%
	I - Batch: 100 | Loss: 0.722 | Acc: 79.875% | Wgt Acc: 81.411%
	I - Batch: 150 | Loss: 0.718 | Acc: 79.625% | Wgt Acc: 81.224%
I - num batch: 160
I - Train -- Loss: 0.717 | Acc: 79.623% | Wgt Acc: 81.237% | LR: 1.250000e-04 | Dur: 86.96s
I - Confusion Matrix: [row->prediction - col->label]
[[239.   2.   3.   6.]
 [ 77. 560.   6.   9.]
 [125.  13. 718.  12.]
 [256.   3.   7. 511.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.161 | Acc: 52.905% | Wgt Acc: 53.736% | Dur: 10.08s
I - Confusion Matrix: [row->prediction - col->label]
[[27.  0.  2.  3.]
 [16. 38. 14.  5.]
 [19. 33. 47. 17.]
 [26.  7. 12. 61.]]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.703 | Acc: 79.875% | Wgt Acc: 81.632%
	I - Batch: 100 | Loss: 0.708 | Acc: 79.375% | Wgt Acc: 81.157%
	I - Batch: 150 | Loss: 0.713 | Acc: 79.167% | Wgt Acc: 80.936%
I - num batch: 160
I - Train -- Loss: 0.715 | Acc: 78.995% | Wgt Acc: 80.777% | LR: 1.250000e-04 | Dur: 88.28s
I - Confusion Matrix: [row->prediction - col->label]
[[216.   1.   5.   6.]
 [ 81. 565.   8.   5.]
 [126.  10. 713.   9.]
 [274.   2.   8. 518.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.146 | Acc: 50.459% | Wgt Acc: 51.766% | Dur: 9.92s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  1.  2.  1.]
 [19. 45. 15.  8.]
 [21. 27. 48. 20.]
 [33.  5. 10. 57.]]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.711 | Acc: 79.375% | Wgt Acc: 81.083%
	I - Batch: 100 | Loss: 0.708 | Acc: 80.188% | Wgt Acc: 81.808%
	I - Batch: 150 | Loss: 0.715 | Acc: 80.083% | Wgt Acc: 81.709%
I - num batch: 160
I - Train -- Loss: 0.715 | Acc: 80.212% | Wgt Acc: 81.812% | LR: 1.250000e-04 | Dur: 88.66s
I - Confusion Matrix: [row->prediction - col->label]
[[249.   1.   3.   4.]
 [ 90. 563.   8.  10.]
 [110.   9. 718.  11.]
 [248.   5.   5. 513.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.144 | Acc: 49.235% | Wgt Acc: 51.087% | Dur: 11.05s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  2.  1.  2.]
 [ 5. 32. 12.  3.]
 [ 8. 30. 38.  5.]
 [60. 14. 24. 76.]]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.749 | Acc: 76.125% | Wgt Acc: 78.054%
	I - Batch: 100 | Loss: 0.718 | Acc: 79.500% | Wgt Acc: 81.174%
	I - Batch: 150 | Loss: 0.714 | Acc: 79.625% | Wgt Acc: 81.239%
I - num batch: 160
I - Train -- Loss: 0.713 | Acc: 79.819% | Wgt Acc: 81.423% | LR: 1.250000e-04 | Dur: 88.86s
I - Confusion Matrix: [row->prediction - col->label]
[[246.   1.   6.   5.]
 [ 85. 566.   5.  11.]
 [105.   9. 715.  16.]
 [261.   2.   8. 506.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.154 | Acc: 50.459% | Wgt Acc: 52.106% | Dur: 10.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 7.  1.  1.  0.]
 [18. 42. 11.  4.]
 [21. 29. 51. 17.]
 [42.  6. 12. 65.]]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.706 | Acc: 79.500% | Wgt Acc: 81.181%
	I - Batch: 100 | Loss: 0.697 | Acc: 80.812% | Wgt Acc: 82.428%
	I - Batch: 150 | Loss: 0.704 | Acc: 80.542% | Wgt Acc: 82.056%
I - num batch: 160
I - Train -- Loss: 0.706 | Acc: 80.212% | Wgt Acc: 81.759% | LR: 1.250000e-04 | Dur: 86.69s
I - Confusion Matrix: [row->prediction - col->label]
[[256.   1.   2.   5.]
 [ 81. 561.  11.   8.]
 [110.  13. 717.  16.]
 [250.   3.   4. 509.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.150 | Acc: 50.459% | Wgt Acc: 51.698% | Dur: 10.07s
I - Confusion Matrix: [row->prediction - col->label]
[[19.  2.  1.  1.]
 [11. 42. 17.  6.]
 [24. 31. 45. 20.]
 [34.  3. 12. 59.]]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.720 | Acc: 79.750% | Wgt Acc: 81.179%
	I - Batch: 100 | Loss: 0.721 | Acc: 79.438% | Wgt Acc: 80.945%
	I - Batch: 150 | Loss: 0.713 | Acc: 80.208% | Wgt Acc: 81.763%
I - num batch: 160
I - Train -- Loss: 0.710 | Acc: 80.408% | Wgt Acc: 81.953% | LR: 1.250000e-04 | Dur: 86.38s
I - Confusion Matrix: [row->prediction - col->label]
[[257.   2.   2.   7.]
 [ 69. 566.   8.  11.]
 [118.   8. 719.  14.]
 [253.   2.   5. 506.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.195 | Acc: 53.823% | Wgt Acc: 54.416% | Dur: 10.13s
I - Confusion Matrix: [row->prediction - col->label]
[[37.  2.  4.  8.]
 [20. 43. 17. 11.]
 [11. 29. 42. 13.]
 [20.  4. 12. 54.]]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.699 | Acc: 79.250% | Wgt Acc: 80.974%
	I - Batch: 100 | Loss: 0.703 | Acc: 80.375% | Wgt Acc: 81.995%
	I - Batch: 150 | Loss: 0.709 | Acc: 80.083% | Wgt Acc: 81.691%
I - num batch: 160
I - Train -- Loss: 0.704 | Acc: 80.251% | Wgt Acc: 81.838% | LR: 1.250000e-04 | Dur: 85.76s
I - Confusion Matrix: [row->prediction - col->label]
[[253.   0.   4.   8.]
 [ 79. 565.   8.   8.]
 [101.   7. 716.  12.]
 [264.   6.   6. 510.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.150 | Acc: 52.294% | Wgt Acc: 53.804% | Dur: 11.07s
I - Confusion Matrix: [row->prediction - col->label]
[[24.  2.  1.  3.]
 [11. 39. 12.  4.]
 [11. 26. 39. 10.]
 [42. 11. 23. 69.]]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.679 | Acc: 81.125% | Wgt Acc: 82.820%
	I - Batch: 100 | Loss: 0.697 | Acc: 79.750% | Wgt Acc: 81.471%
	I - Batch: 150 | Loss: 0.701 | Acc: 79.542% | Wgt Acc: 81.234%
I - num batch: 160
I - Train -- Loss: 0.701 | Acc: 79.859% | Wgt Acc: 81.520% | LR: 1.250000e-04 | Dur: 87.44s
I - Confusion Matrix: [row->prediction - col->label]
[[237.   1.   3.   7.]
 [109. 566.   3.  10.]
 [115.   6. 718.   8.]
 [236.   5.  10. 513.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.158 | Acc: 53.823% | Wgt Acc: 54.959% | Dur: 10.27s
I - Confusion Matrix: [row->prediction - col->label]
[[23.  1.  2.  2.]
 [16. 43. 17.  6.]
 [21. 28. 48. 16.]
 [28.  6.  8. 62.]]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.708 | Acc: 80.000% | Wgt Acc: 81.872%
	I - Batch: 100 | Loss: 0.709 | Acc: 80.312% | Wgt Acc: 82.006%
	I - Batch: 150 | Loss: 0.707 | Acc: 80.583% | Wgt Acc: 82.227%
I - num batch: 160
I - Train -- Loss: 0.704 | Acc: 80.605% | Wgt Acc: 82.236% | LR: 1.250000e-04 | Dur: 88.95s
I - Confusion Matrix: [row->prediction - col->label]
[[252.   2.   2.   6.]
 [ 81. 567.   7.   4.]
 [120.   4. 717.  11.]
 [244.   5.   8. 517.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.135 | Acc: 48.930% | Wgt Acc: 51.019% | Dur: 10.08s
I - Confusion Matrix: [row->prediction - col->label]
[[11.  1.  1.  2.]
 [18. 41. 20.  4.]
 [14. 26. 38. 10.]
 [45. 10. 16. 70.]]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.685 | Acc: 80.500% | Wgt Acc: 82.005%
	I - Batch: 100 | Loss: 0.688 | Acc: 81.062% | Wgt Acc: 82.609%
	I - Batch: 150 | Loss: 0.700 | Acc: 80.417% | Wgt Acc: 82.025%
I - num batch: 160
I - Train -- Loss: 0.704 | Acc: 80.330% | Wgt Acc: 81.936% | LR: 1.250000e-04 | Dur: 87.70s
I - Confusion Matrix: [row->prediction - col->label]
[[248.   2.   4.   4.]
 [ 69. 567.   9.   9.]
 [113.   7. 720.  14.]
 [267.   2.   1. 511.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.210 | Acc: 50.153% | Wgt Acc: 50.747% | Dur: 11.02s
I - Confusion Matrix: [row->prediction - col->label]
[[14.  0.  0.  3.]
 [20. 44. 14. 12.]
 [32. 33. 59. 24.]
 [22.  1.  2. 47.]]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.703 | Acc: 80.875% | Wgt Acc: 82.524%
	I - Batch: 100 | Loss: 0.695 | Acc: 81.188% | Wgt Acc: 82.796%
	I - Batch: 150 | Loss: 0.691 | Acc: 81.250% | Wgt Acc: 82.863%
I - num batch: 160
I - Train -- Loss: 0.693 | Acc: 80.919% | Wgt Acc: 82.528% | LR: 1.250000e-04 | Dur: 88.68s
I - Confusion Matrix: [row->prediction - col->label]
[[256.   3.   3.   5.]
 [ 80. 568.   5.   5.]
 [112.   4. 720.  11.]
 [249.   3.   6. 517.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.144 | Acc: 47.401% | Wgt Acc: 49.321% | Dur: 9.88s
I - Confusion Matrix: [row->prediction - col->label]
[[ 8.  0.  1.  2.]
 [ 9. 36.  9.  2.]
 [16. 29. 41. 12.]
 [55. 13. 24. 70.]]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.691 | Acc: 83.000% | Wgt Acc: 84.516%
	I - Batch: 100 | Loss: 0.693 | Acc: 81.688% | Wgt Acc: 83.279%
	I - Batch: 150 | Loss: 0.698 | Acc: 80.875% | Wgt Acc: 82.514%
I - num batch: 160
I - Train -- Loss: 0.696 | Acc: 81.115% | Wgt Acc: 82.723% | LR: 1.250000e-04 | Dur: 88.71s
I - Confusion Matrix: [row->prediction - col->label]
[[259.   0.   3.   4.]
 [ 77. 570.   8.   3.]
 [109.   3. 720.  14.]
 [252.   5.   3. 517.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.152 | Acc: 48.012% | Wgt Acc: 49.592% | Dur: 10.07s
I - Confusion Matrix: [row->prediction - col->label]
[[12.  1.  1.  2.]
 [15. 34. 15.  5.]
 [22. 33. 43. 11.]
 [39. 10. 16. 68.]]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.690 | Acc: 81.000% | Wgt Acc: 82.683%
	I - Batch: 100 | Loss: 0.684 | Acc: 80.750% | Wgt Acc: 82.403%
	I - Batch: 150 | Loss: 0.688 | Acc: 80.458% | Wgt Acc: 82.172%
I - num batch: 160
I - Train -- Loss: 0.693 | Acc: 80.330% | Wgt Acc: 82.042% | LR: 1.250000e-04 | Dur: 85.79s
I - Confusion Matrix: [row->prediction - col->label]
[[235.   0.   2.   4.]
 [ 84. 572.   7.   7.]
 [106.   5. 721.   9.]
 [272.   1.   4. 518.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.155 | Acc: 49.847% | Wgt Acc: 50.951% | Dur: 10.23s
I - Confusion Matrix: [row->prediction - col->label]
[[17.  1.  2.  2.]
 [16. 45. 17.  9.]
 [23. 28. 48. 22.]
 [32.  4.  8. 53.]]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.694 | Acc: 79.500% | Wgt Acc: 81.280%
	I - Batch: 100 | Loss: 0.699 | Acc: 79.438% | Wgt Acc: 81.202%
	I - Batch: 150 | Loss: 0.689 | Acc: 80.417% | Wgt Acc: 82.108%
I - num batch: 160
I - Train -- Loss: 0.693 | Acc: 80.291% | Wgt Acc: 81.989% | LR: 1.250000e-04 | Dur: 89.86s
I - Confusion Matrix: [row->prediction - col->label]
[[236.   0.   2.   7.]
 [ 91. 572.   7.   8.]
 [119.   4. 721.   7.]
 [251.   2.   4. 516.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.189 | Acc: 50.459% | Wgt Acc: 52.106% | Dur: 10.17s
I - Confusion Matrix: [row->prediction - col->label]
[[25.  2.  1.  6.]
 [17. 40. 16.  3.]
 [ 6. 24. 33. 10.]
 [40. 12. 25. 67.]]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.702 | Acc: 79.500% | Wgt Acc: 81.176%
	I - Batch: 100 | Loss: 0.699 | Acc: 79.938% | Wgt Acc: 81.584%
	I - Batch: 150 | Loss: 0.693 | Acc: 80.958% | Wgt Acc: 82.537%
I - num batch: 160
I - Train -- Loss: 0.695 | Acc: 80.801% | Wgt Acc: 82.396% | LR: 1.250000e-04 | Dur: 85.59s
I - Confusion Matrix: [row->prediction - col->label]
[[259.   0.   0.   8.]
 [ 92. 569.  11.   9.]
 [100.   7. 717.   8.]
 [246.   2.   6. 513.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.184 | Acc: 48.012% | Wgt Acc: 49.185% | Dur: 10.05s
I - Confusion Matrix: [row->prediction - col->label]
[[10.  1.  0.  3.]
 [20. 41. 19.  8.]
 [25. 31. 51. 20.]
 [33.  5.  5. 55.]]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.701 | Acc: 80.500% | Wgt Acc: 82.222%
	I - Batch: 100 | Loss: 0.690 | Acc: 80.688% | Wgt Acc: 82.303%
	I - Batch: 150 | Loss: 0.686 | Acc: 80.792% | Wgt Acc: 82.448%
I - num batch: 160
I - Train -- Loss: 0.686 | Acc: 80.722% | Wgt Acc: 82.387% | LR: 1.250000e-04 | Dur: 86.44s
I - Confusion Matrix: [row->prediction - col->label]
[[244.   0.   3.   6.]
 [ 93. 571.   5.   6.]
 [111.   5. 723.   8.]
 [249.   2.   3. 518.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.148 | Acc: 47.706% | Wgt Acc: 48.845% | Dur: 10.21s
I - Confusion Matrix: [row->prediction - col->label]
[[16.  1.  1.  3.]
 [15. 37. 17.  8.]
 [19. 31. 45. 17.]
 [38.  9. 12. 58.]]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.679 | Acc: 81.375% | Wgt Acc: 83.037%
	I - Batch: 100 | Loss: 0.678 | Acc: 82.438% | Wgt Acc: 83.976%
	I - Batch: 150 | Loss: 0.686 | Acc: 81.708% | Wgt Acc: 83.257%
I - num batch: 160
I - Train -- Loss: 0.684 | Acc: 81.900% | Wgt Acc: 83.439% | LR: 1.250000e-04 | Dur: 93.37s
I - Confusion Matrix: [row->prediction - col->label]
[[274.   1.   3.   7.]
 [ 87. 572.   4.   6.]
 [114.   4. 724.   9.]
 [222.   1.   3. 516.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.145 | Acc: 48.012% | Wgt Acc: 49.864% | Dur: 11.23s
I - Confusion Matrix: [row->prediction - col->label]
[[10.  1.  1.  1.]
 [13. 37. 11.  3.]
 [20. 31. 41. 13.]
 [45.  9. 22. 69.]]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.688 | Acc: 80.125% | Wgt Acc: 81.780%
	I - Batch: 100 | Loss: 0.684 | Acc: 81.188% | Wgt Acc: 82.805%
	I - Batch: 150 | Loss: 0.687 | Acc: 81.000% | Wgt Acc: 82.646%
I - num batch: 160
I - Train -- Loss: 0.683 | Acc: 81.194% | Wgt Acc: 82.829% | LR: 1.250000e-04 | Dur: 87.76s
I - Confusion Matrix: [row->prediction - col->label]
[[253.   0.   3.   6.]
 [ 87. 570.   3.   3.]
 [106.   7. 724.   8.]
 [251.   1.   4. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.168 | Acc: 52.599% | Wgt Acc: 53.736% | Dur: 10.26s
I - Confusion Matrix: [row->prediction - col->label]
[[31.  3.  2.  8.]
 [16. 40. 15.  7.]
 [ 5. 23. 38.  8.]
 [36. 12. 20. 63.]]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.694 | Acc: 80.000% | Wgt Acc: 81.759%
	I - Batch: 100 | Loss: 0.695 | Acc: 80.688% | Wgt Acc: 82.320%
	I - Batch: 150 | Loss: 0.687 | Acc: 80.833% | Wgt Acc: 82.485%
I - num batch: 160
I - Train -- Loss: 0.687 | Acc: 80.801% | Wgt Acc: 82.449% | LR: 1.250000e-04 | Dur: 88.77s
I - Confusion Matrix: [row->prediction - col->label]
[[246.   1.   2.   7.]
 [ 95. 571.   2.   6.]
 [115.   5. 724.   8.]
 [241.   1.   6. 517.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.129 | Acc: 50.153% | Wgt Acc: 51.834% | Dur: 10.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 8.  1.  1.  2.]
 [18. 45. 13.  5.]
 [23. 26. 49. 17.]
 [39.  6. 12. 62.]]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.665 | Acc: 81.125% | Wgt Acc: 82.791%
	I - Batch: 100 | Loss: 0.675 | Acc: 81.688% | Wgt Acc: 83.286%
	I - Batch: 150 | Loss: 0.682 | Acc: 80.750% | Wgt Acc: 82.407%
I - num batch: 160
I - Train -- Loss: 0.683 | Acc: 80.801% | Wgt Acc: 82.440% | LR: 1.250000e-04 | Dur: 89.67s
I - Confusion Matrix: [row->prediction - col->label]
[[245.   0.   1.   8.]
 [ 99. 573.   3.   7.]
 [101.   4. 726.   9.]
 [252.   1.   4. 514.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.143 | Acc: 47.706% | Wgt Acc: 49.117% | Dur: 10.56s
I - Confusion Matrix: [row->prediction - col->label]
[[13.  0.  1.  3.]
 [10. 36. 13.  5.]
 [21. 34. 44. 15.]
 [44.  8. 17. 63.]]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.683 | Acc: 82.250% | Wgt Acc: 83.796%
	I - Batch: 100 | Loss: 0.676 | Acc: 82.312% | Wgt Acc: 83.810%
	I - Batch: 150 | Loss: 0.678 | Acc: 82.500% | Wgt Acc: 83.995%
I - num batch: 160
I - Train -- Loss: 0.680 | Acc: 82.057% | Wgt Acc: 83.581% | LR: 1.250000e-04 | Dur: 88.16s
I - Confusion Matrix: [row->prediction - col->label]
[[278.   1.   4.   6.]
 [ 79. 569.   3.   5.]
 [109.   5. 724.   8.]
 [231.   3.   3. 519.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.141 | Acc: 52.294% | Wgt Acc: 53.668% | Dur: 10.11s
I - Confusion Matrix: [row->prediction - col->label]
[[17.  1.  2.  2.]
 [18. 43. 11.  7.]
 [18. 29. 48. 14.]
 [35.  5. 14. 63.]]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.659 | Acc: 82.625% | Wgt Acc: 84.143%
	I - Batch: 100 | Loss: 0.669 | Acc: 82.375% | Wgt Acc: 83.933%
	I - Batch: 150 | Loss: 0.678 | Acc: 82.000% | Wgt Acc: 83.523%
I - num batch: 160
I - Train -- Loss: 0.678 | Acc: 81.822% | Wgt Acc: 83.369% | LR: 1.250000e-04 | Dur: 86.77s
I - Confusion Matrix: [row->prediction - col->label]
[[274.   1.   4.   7.]
 [ 80. 572.   3.   4.]
 [106.   2. 722.  11.]
 [237.   3.   5. 516.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.183 | Acc: 50.459% | Wgt Acc: 51.359% | Dur: 10.18s
I - Confusion Matrix: [row->prediction - col->label]
[[25.  1.  1.  5.]
 [15. 37. 15.  6.]
 [16. 29. 44. 16.]
 [32. 11. 15. 59.]]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.682 | Acc: 81.250% | Wgt Acc: 82.784%
	I - Batch: 100 | Loss: 0.679 | Acc: 81.938% | Wgt Acc: 83.408%
	I - Batch: 150 | Loss: 0.677 | Acc: 82.208% | Wgt Acc: 83.726%
I - num batch: 160
I - Train -- Loss: 0.685 | Acc: 81.900% | Wgt Acc: 83.439% | LR: 1.250000e-04 | Dur: 86.32s
I - Confusion Matrix: [row->prediction - col->label]
[[275.   1.   2.   7.]
 [ 78. 569.   5.   2.]
 [111.   6. 723.  10.]
 [233.   2.   4. 519.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.133 | Acc: 51.376% | Wgt Acc: 53.872% | Dur: 10.04s
I - Confusion Matrix: [row->prediction - col->label]
[[14.  0.  1.  5.]
 [13. 46. 11.  3.]
 [ 9. 20. 33.  3.]
 [52. 12. 30. 75.]]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.661 | Acc: 82.875% | Wgt Acc: 84.484%
	I - Batch: 100 | Loss: 0.669 | Acc: 82.438% | Wgt Acc: 83.961%
	I - Batch: 150 | Loss: 0.675 | Acc: 81.708% | Wgt Acc: 83.318%
I - num batch: 160
I - Train -- Loss: 0.674 | Acc: 81.665% | Wgt Acc: 83.289% | LR: 1.250000e-04 | Dur: 85.25s
I - Confusion Matrix: [row->prediction - col->label]
[[260.   1.   3.   6.]
 [ 87. 575.   1.   4.]
 [108.   1. 725.   8.]
 [242.   1.   5. 520.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.133 | Acc: 50.765% | Wgt Acc: 52.242% | Dur: 10.10s
I - Confusion Matrix: [row->prediction - col->label]
[[13.  0.  2.  3.]
 [22. 48. 13. 11.]
 [17. 23. 48. 15.]
 [36.  7. 12. 57.]]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.678 | Acc: 81.000% | Wgt Acc: 82.711%
	I - Batch: 100 | Loss: 0.668 | Acc: 81.625% | Wgt Acc: 83.275%
	I - Batch: 150 | Loss: 0.671 | Acc: 81.583% | Wgt Acc: 83.222%
I - num batch: 160
I - Train -- Loss: 0.680 | Acc: 81.076% | Wgt Acc: 82.723% | LR: 1.250000e-04 | Dur: 90.73s
I - Confusion Matrix: [row->prediction - col->label]
[[252.   0.   2.   7.]
 [ 91. 572.   7.   3.]
 [122.   4. 722.   9.]
 [232.   2.   3. 519.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.149 | Acc: 48.318% | Wgt Acc: 50.272% | Dur: 10.93s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  1.  1.  2.]
 [24. 43. 24.  8.]
 [12. 26. 35. 11.]
 [37.  8. 15. 65.]]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.680 | Acc: 82.375% | Wgt Acc: 83.910%
	I - Batch: 100 | Loss: 0.672 | Acc: 82.688% | Wgt Acc: 84.216%
	I - Batch: 150 | Loss: 0.677 | Acc: 82.500% | Wgt Acc: 84.017%
I - num batch: 160
I - Train -- Loss: 0.678 | Acc: 82.214% | Wgt Acc: 83.767% | LR: 1.250000e-04 | Dur: 89.97s
I - Confusion Matrix: [row->prediction - col->label]
[[278.   1.   2.   8.]
 [ 88. 575.   6.   4.]
 [100.   0. 723.   8.]
 [231.   2.   3. 518.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.158 | Acc: 52.905% | Wgt Acc: 54.416% | Dur: 10.03s
I - Confusion Matrix: [row->prediction - col->label]
[[24.  2.  1.  2.]
 [19. 42. 16.  6.]
 [13. 26. 40. 11.]
 [32.  8. 18. 67.]]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.658 | Acc: 83.625% | Wgt Acc: 85.070%
	I - Batch: 100 | Loss: 0.666 | Acc: 82.750% | Wgt Acc: 84.286%
	I - Batch: 150 | Loss: 0.671 | Acc: 82.125% | Wgt Acc: 83.711%
I - num batch: 160
I - Train -- Loss: 0.674 | Acc: 82.254% | Wgt Acc: 83.846% | LR: 1.250000e-04 | Dur: 87.97s
I - Confusion Matrix: [row->prediction - col->label]
[[270.   0.   2.   8.]
 [ 81. 575.   2.   7.]
 [108.   2. 727.   0.]
 [238.   1.   3. 523.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.159 | Acc: 52.905% | Wgt Acc: 54.008% | Dur: 9.95s
I - Confusion Matrix: [row->prediction - col->label]
[[26.  1.  2.  5.]
 [17. 41. 18.  7.]
 [18. 28. 44. 12.]
 [27.  8. 11. 62.]]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.691 | Acc: 81.250% | Wgt Acc: 82.914%
	I - Batch: 100 | Loss: 0.684 | Acc: 81.875% | Wgt Acc: 83.491%
	I - Batch: 150 | Loss: 0.677 | Acc: 82.250% | Wgt Acc: 83.812%
I - num batch: 160
I - Train -- Loss: 0.676 | Acc: 82.136% | Wgt Acc: 83.723% | LR: 1.250000e-04 | Dur: 88.60s
I - Confusion Matrix: [row->prediction - col->label]
[[269.   1.   2.   6.]
 [ 93. 575.   2.   2.]
 [110.   1. 727.   9.]
 [225.   1.   3. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.162 | Acc: 48.318% | Wgt Acc: 50.747% | Dur: 10.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 9.  1.  0.  2.]
 [14. 44. 16.  2.]
 [12. 22. 34. 11.]
 [53. 11. 25. 71.]]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.652 | Acc: 84.125% | Wgt Acc: 85.662%
	I - Batch: 100 | Loss: 0.672 | Acc: 83.562% | Wgt Acc: 85.022%
	I - Batch: 150 | Loss: 0.673 | Acc: 83.292% | Wgt Acc: 84.759%
I - num batch: 160
I - Train -- Loss: 0.671 | Acc: 83.431% | Wgt Acc: 84.899% | LR: 1.250000e-04 | Dur: 87.47s
I - Confusion Matrix: [row->prediction - col->label]
[[302.   0.   2.   5.]
 [ 80. 574.   3.   1.]
 [ 99.   2. 726.   9.]
 [216.   2.   3. 523.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.158 | Acc: 49.235% | Wgt Acc: 50.476% | Dur: 10.24s
I - Confusion Matrix: [row->prediction - col->label]
[[19.  1.  1.  1.]
 [12. 37. 13.  7.]
 [15. 30. 43. 16.]
 [42. 10. 18. 62.]]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.661 | Acc: 82.875% | Wgt Acc: 84.384%
	I - Batch: 100 | Loss: 0.671 | Acc: 82.875% | Wgt Acc: 84.381%
	I - Batch: 150 | Loss: 0.672 | Acc: 82.917% | Wgt Acc: 84.384%
I - num batch: 160
I - Train -- Loss: 0.673 | Acc: 82.882% | Wgt Acc: 84.351% | LR: 1.250000e-04 | Dur: 88.61s
I - Confusion Matrix: [row->prediction - col->label]
[[291.   0.   2.   7.]
 [ 75. 575.   0.   4.]
 [105.   2. 729.  11.]
 [226.   1.   3. 516.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.163 | Acc: 50.765% | Wgt Acc: 52.106% | Dur: 10.94s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  1.  1.  2.]
 [19. 42. 13.  6.]
 [20. 29. 48. 17.]
 [34.  6. 13. 61.]]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.672 | Acc: 80.375% | Wgt Acc: 82.089%
	I - Batch: 100 | Loss: 0.666 | Acc: 81.625% | Wgt Acc: 83.254%
	I - Batch: 150 | Loss: 0.670 | Acc: 81.667% | Wgt Acc: 83.300%
I - num batch: 160
I - Train -- Loss: 0.672 | Acc: 81.625% | Wgt Acc: 83.254% | LR: 1.250000e-04 | Dur: 92.17s
I - Confusion Matrix: [row->prediction - col->label]
[[257.   1.   2.   4.]
 [ 83. 576.   2.   9.]
 [111.   0. 727.   6.]
 [246.   1.   3. 519.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.156 | Acc: 48.012% | Wgt Acc: 49.457% | Dur: 10.21s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  1.  0.  1.]
 [12. 30. 13.  3.]
 [14. 37. 42. 12.]
 [47. 10. 20. 70.]]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.700 | Acc: 79.375% | Wgt Acc: 80.899%
	I - Batch: 100 | Loss: 0.673 | Acc: 81.938% | Wgt Acc: 83.420%
	I - Batch: 150 | Loss: 0.674 | Acc: 81.792% | Wgt Acc: 83.341%
I - num batch: 160
I - Train -- Loss: 0.672 | Acc: 81.822% | Wgt Acc: 83.369% | LR: 1.250000e-04 | Dur: 86.00s
I - Confusion Matrix: [row->prediction - col->label]
[[269.   1.   3.   5.]
 [ 86. 572.   2.   6.]
 [ 89.   5. 727.  11.]
 [253.   0.   2. 516.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.136 | Acc: 48.930% | Wgt Acc: 50.543% | Dur: 10.49s
I - Confusion Matrix: [row->prediction - col->label]
[[14.  2.  1.  4.]
 [18. 41. 13. 10.]
 [15. 28. 42.  9.]
 [41.  7. 19. 63.]]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.677 | Acc: 81.500% | Wgt Acc: 83.230%
	I - Batch: 100 | Loss: 0.682 | Acc: 81.375% | Wgt Acc: 83.040%
	I - Batch: 150 | Loss: 0.670 | Acc: 81.750% | Wgt Acc: 83.402%
I - num batch: 160
I - Train -- Loss: 0.672 | Acc: 81.704% | Wgt Acc: 83.351% | LR: 1.250000e-04 | Dur: 88.05s
I - Confusion Matrix: [row->prediction - col->label]
[[256.   0.   2.   7.]
 [ 83. 577.   3.   2.]
 [123.   1. 727.   8.]
 [235.   0.   2. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.132 | Acc: 48.624% | Wgt Acc: 50.476% | Dur: 10.46s
I - Confusion Matrix: [row->prediction - col->label]
[[13.  1.  1.  6.]
 [14. 44. 18.  6.]
 [12. 27. 39. 11.]
 [49.  6. 17. 63.]]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.679 | Acc: 81.250% | Wgt Acc: 82.860%
	I - Batch: 100 | Loss: 0.668 | Acc: 82.375% | Wgt Acc: 83.862%
	I - Batch: 150 | Loss: 0.668 | Acc: 82.458% | Wgt Acc: 83.984%
I - num batch: 160
I - Train -- Loss: 0.666 | Acc: 82.175% | Wgt Acc: 83.740% | LR: 1.250000e-04 | Dur: 87.15s
I - Confusion Matrix: [row->prediction - col->label]
[[270.   0.   3.   4.]
 [ 90. 577.   1.   7.]
 [102.   1. 729.  10.]
 [235.   0.   1. 517.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.149 | Acc: 50.765% | Wgt Acc: 52.242% | Dur: 9.93s
I - Confusion Matrix: [row->prediction - col->label]
[[18.  1.  1.  4.]
 [15. 38. 15.  4.]
 [19. 30. 43. 11.]
 [36.  9. 16. 67.]]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.657 | Acc: 83.000% | Wgt Acc: 84.500%
	I - Batch: 100 | Loss: 0.665 | Acc: 83.062% | Wgt Acc: 84.580%
	I - Batch: 150 | Loss: 0.669 | Acc: 82.417% | Wgt Acc: 84.002%
I - num batch: 160
I - Train -- Loss: 0.667 | Acc: 82.528% | Wgt Acc: 84.103% | LR: 1.250000e-04 | Dur: 78.78s
I - Confusion Matrix: [row->prediction - col->label]
[[276.   0.   2.   6.]
 [ 80. 575.   2.   1.]
 [112.   3. 727.   7.]
 [229.   0.   3. 524.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.170 | Acc: 46.789% | Wgt Acc: 48.573% | Dur: 9.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 4.  0.  0.  1.]
 [11. 34. 12.  4.]
 [20. 32. 46. 12.]
 [53. 12. 17. 69.]]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.648 | Acc: 84.875% | Wgt Acc: 86.225%
	I - Batch: 100 | Loss: 0.659 | Acc: 82.875% | Wgt Acc: 84.417%
	I - Batch: 150 | Loss: 0.662 | Acc: 82.833% | Wgt Acc: 84.366%
I - num batch: 160
I - Train -- Loss: 0.665 | Acc: 82.960% | Wgt Acc: 84.483% | LR: 1.250000e-04 | Dur: 80.67s
I - Confusion Matrix: [row->prediction - col->label]
[[288.   0.   1.   6.]
 [ 75. 576.   3.   2.]
 [112.   2. 727.   8.]
 [222.   0.   3. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.182 | Acc: 47.706% | Wgt Acc: 48.166% | Dur: 9.13s
I - Confusion Matrix: [row->prediction - col->label]
[[12.  1.  0.  3.]
 [20. 37. 13. 10.]
 [28. 36. 59. 25.]
 [28.  4.  3. 48.]]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.665 | Acc: 83.250% | Wgt Acc: 84.730%
	I - Batch: 100 | Loss: 0.653 | Acc: 83.438% | Wgt Acc: 84.932%
	I - Batch: 150 | Loss: 0.663 | Acc: 82.917% | Wgt Acc: 84.414%
I - num batch: 160
I - Train -- Loss: 0.660 | Acc: 82.882% | Wgt Acc: 84.386% | LR: 1.250000e-04 | Dur: 78.74s
I - Confusion Matrix: [row->prediction - col->label]
[[286.   1.   2.   6.]
 [ 74. 574.   1.   3.]
 [110.   2. 730.   8.]
 [227.   1.   1. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.158 | Acc: 48.624% | Wgt Acc: 50.543% | Dur: 9.06s
I - Confusion Matrix: [row->prediction - col->label]
[[18.  2.  1.  5.]
 [18. 42. 21.  6.]
 [11. 25. 33.  9.]
 [41.  9. 20. 66.]]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.652 | Acc: 83.625% | Wgt Acc: 85.131%
	I - Batch: 100 | Loss: 0.664 | Acc: 83.375% | Wgt Acc: 84.872%
	I - Batch: 150 | Loss: 0.665 | Acc: 83.167% | Wgt Acc: 84.684%
I - num batch: 160
I - Train -- Loss: 0.663 | Acc: 83.235% | Wgt Acc: 84.749% | LR: 1.250000e-04 | Dur: 78.29s
I - Confusion Matrix: [row->prediction - col->label]
[[292.   0.   2.   5.]
 [ 81. 575.   3.   5.]
 [116.   3. 728.   3.]
 [208.   0.   1. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.179 | Acc: 45.872% | Wgt Acc: 47.486% | Dur: 10.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 8.  0.  0.  1.]
 [10. 34.  9.  3.]
 [17. 30. 43. 17.]
 [53. 14. 23. 65.]]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.667 | Acc: 81.250% | Wgt Acc: 83.010%
	I - Batch: 100 | Loss: 0.663 | Acc: 82.625% | Wgt Acc: 84.236%
	I - Batch: 150 | Loss: 0.668 | Acc: 82.042% | Wgt Acc: 83.665%
I - num batch: 160
I - Train -- Loss: 0.668 | Acc: 82.018% | Wgt Acc: 83.616% | LR: 1.250000e-04 | Dur: 81.94s
I - Confusion Matrix: [row->prediction - col->label]
[[267.   1.   2.   5.]
 [ 88. 574.   6.   3.]
 [102.   3. 726.   8.]
 [240.   0.   0. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.185 | Acc: 45.872% | Wgt Acc: 47.147% | Dur: 9.14s
I - Confusion Matrix: [row->prediction - col->label]
[[10.  1.  2.  1.]
 [19. 38. 19.  8.]
 [24. 32. 46. 21.]
 [35.  7.  8. 56.]]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.666 | Acc: 82.625% | Wgt Acc: 84.218%
	I - Batch: 100 | Loss: 0.655 | Acc: 83.125% | Wgt Acc: 84.625%
	I - Batch: 150 | Loss: 0.663 | Acc: 82.833% | Wgt Acc: 84.333%
I - num batch: 160
I - Train -- Loss: 0.665 | Acc: 82.686% | Wgt Acc: 84.209% | LR: 1.250000e-04 | Dur: 78.18s
I - Confusion Matrix: [row->prediction - col->label]
[[283.   0.   2.   8.]
 [ 85. 575.   0.   2.]
 [110.   2. 728.   8.]
 [219.   1.   4. 520.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.153 | Acc: 48.930% | Wgt Acc: 49.932% | Dur: 9.34s
I - Confusion Matrix: [row->prediction - col->label]
[[13.  0.  1.  2.]
 [22. 41. 17.  7.]
 [22. 33. 52. 23.]
 [31.  4.  5. 54.]]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.656 | Acc: 82.125% | Wgt Acc: 83.691%
	I - Batch: 100 | Loss: 0.657 | Acc: 83.000% | Wgt Acc: 84.531%
	I - Batch: 150 | Loss: 0.659 | Acc: 82.708% | Wgt Acc: 84.248%
I - num batch: 160
I - Train -- Loss: 0.658 | Acc: 82.843% | Wgt Acc: 84.368% | LR: 1.250000e-04 | Dur: 79.99s
I - Confusion Matrix: [row->prediction - col->label]
[[286.   0.   2.   6.]
 [ 77. 578.   4.   7.]
 [117.   0. 727.   6.]
 [217.   0.   1. 519.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.146 | Acc: 53.211% | Wgt Acc: 54.891% | Dur: 9.07s
I - Confusion Matrix: [row->prediction - col->label]
[[28.  1.  1.  3.]
 [16. 45. 18.  6.]
 [ 9. 24. 34. 10.]
 [35.  8. 22. 67.]]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.668 | Acc: 83.125% | Wgt Acc: 84.678%
	I - Batch: 100 | Loss: 0.663 | Acc: 83.500% | Wgt Acc: 84.997%
	I - Batch: 150 | Loss: 0.661 | Acc: 83.625% | Wgt Acc: 85.070%
I - num batch: 160
I - Train -- Loss: 0.661 | Acc: 83.314% | Wgt Acc: 84.784% | LR: 1.250000e-04 | Dur: 80.03s
I - Confusion Matrix: [row->prediction - col->label]
[[297.   0.   3.   7.]
 [ 78. 576.   1.   6.]
 [109.   2. 729.   5.]
 [213.   0.   1. 520.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.136 | Acc: 53.211% | Wgt Acc: 55.299% | Dur: 9.05s
I - Confusion Matrix: [row->prediction - col->label]
[[24.  3.  1.  9.]
 [12. 50. 19.  6.]
 [ 6. 16. 32.  3.]
 [46.  9. 23. 68.]]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.675 | Acc: 81.250% | Wgt Acc: 82.848%
	I - Batch: 100 | Loss: 0.662 | Acc: 82.875% | Wgt Acc: 84.453%
	I - Batch: 150 | Loss: 0.661 | Acc: 82.750% | Wgt Acc: 84.289%
I - num batch: 160
I - Train -- Loss: 0.662 | Acc: 83.157% | Wgt Acc: 84.660% | LR: 1.250000e-04 | Dur: 80.74s
I - Confusion Matrix: [row->prediction - col->label]
[[292.   0.   2.   5.]
 [ 89. 575.   1.   3.]
 [108.   3. 728.   7.]
 [208.   0.   3. 523.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.175 | Acc: 47.401% | Wgt Acc: 48.505% | Dur: 9.31s
I - Confusion Matrix: [row->prediction - col->label]
[[12.  1.  1.  5.]
 [20. 40. 16.  8.]
 [27. 31. 49. 19.]
 [29.  6.  9. 54.]]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.668 | Acc: 82.375% | Wgt Acc: 83.915%
	I - Batch: 100 | Loss: 0.661 | Acc: 82.875% | Wgt Acc: 84.382%
	I - Batch: 150 | Loss: 0.660 | Acc: 82.708% | Wgt Acc: 84.271%
I - num batch: 160
I - Train -- Loss: 0.660 | Acc: 82.568% | Wgt Acc: 84.130% | LR: 1.250000e-04 | Dur: 81.91s
I - Confusion Matrix: [row->prediction - col->label]
[[278.   0.   1.   7.]
 [ 85. 576.   2.   2.]
 [ 95.   2. 727.   7.]
 [239.   0.   4. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.172 | Acc: 51.070% | Wgt Acc: 52.310% | Dur: 9.82s
I - Confusion Matrix: [row->prediction - col->label]
[[22.  0.  1.  4.]
 [12. 35. 11.  4.]
 [12. 28. 43. 11.]
 [42. 15. 20. 67.]]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.637 | Acc: 83.750% | Wgt Acc: 85.274%
	I - Batch: 100 | Loss: 0.643 | Acc: 84.125% | Wgt Acc: 85.590%
	I - Batch: 150 | Loss: 0.656 | Acc: 82.833% | Wgt Acc: 84.405%
I - num batch: 160
I - Train -- Loss: 0.658 | Acc: 82.921% | Wgt Acc: 84.475% | LR: 1.250000e-04 | Dur: 77.52s
I - Confusion Matrix: [row->prediction - col->label]
[[283.   0.   3.   5.]
 [ 85. 577.   1.   5.]
 [104.   1. 728.   4.]
 [225.   0.   2. 524.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.155 | Acc: 51.682% | Wgt Acc: 53.736% | Dur: 8.90s
I - Confusion Matrix: [row->prediction - col->label]
[[20.  1.  0.  1.]
 [11. 41. 18.  3.]
 [11. 23. 34.  8.]
 [46. 13. 23. 74.]]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.648 | Acc: 85.125% | Wgt Acc: 86.438%
	I - Batch: 100 | Loss: 0.648 | Acc: 84.500% | Wgt Acc: 85.861%
	I - Batch: 150 | Loss: 0.652 | Acc: 83.875% | Wgt Acc: 85.303%
I - num batch: 160
I - Train -- Loss: 0.660 | Acc: 83.785% | Wgt Acc: 85.209% | LR: 1.250000e-04 | Dur: 79.60s
I - Confusion Matrix: [row->prediction - col->label]
[[311.   0.   3.   8.]
 [ 80. 576.   3.   6.]
 [100.   2. 727.   4.]
 [206.   0.   1. 520.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.145 | Acc: 51.682% | Wgt Acc: 53.261% | Dur: 9.06s
I - Confusion Matrix: [row->prediction - col->label]
[[22.  2.  1.  3.]
 [25. 47. 22. 11.]
 [14. 22. 39. 11.]
 [27.  7. 13. 61.]]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.646 | Acc: 83.500% | Wgt Acc: 85.028%
	I - Batch: 100 | Loss: 0.656 | Acc: 83.125% | Wgt Acc: 84.678%
	I - Batch: 150 | Loss: 0.655 | Acc: 83.750% | Wgt Acc: 85.216%
I - num batch: 160
I - Train -- Loss: 0.657 | Acc: 83.510% | Wgt Acc: 84.996% | LR: 1.250000e-04 | Dur: 77.53s
I - Confusion Matrix: [row->prediction - col->label]
[[297.   1.   4.   8.]
 [ 72. 577.   0.   3.]
 [106.   0. 730.   4.]
 [222.   0.   0. 523.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.145 | Acc: 50.459% | Wgt Acc: 51.766% | Dur: 9.23s
I - Confusion Matrix: [row->prediction - col->label]
[[14.  1.  2.  2.]
 [20. 39. 13.  4.]
 [22. 30. 49. 17.]
 [32.  8. 11. 63.]]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.648 | Acc: 84.250% | Wgt Acc: 85.654%
	I - Batch: 100 | Loss: 0.653 | Acc: 83.125% | Wgt Acc: 84.623%
	I - Batch: 150 | Loss: 0.654 | Acc: 83.667% | Wgt Acc: 85.113%
I - num batch: 160
I - Train -- Loss: 0.654 | Acc: 83.589% | Wgt Acc: 85.032% | LR: 1.250000e-04 | Dur: 78.12s
I - Confusion Matrix: [row->prediction - col->label]
[[305.   0.   2.   6.]
 [ 78. 575.   2.   3.]
 [104.   3. 728.   8.]
 [210.   0.   2. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.137 | Acc: 51.376% | Wgt Acc: 53.261% | Dur: 9.13s
I - Confusion Matrix: [row->prediction - col->label]
[[19.  1.  1.  6.]
 [15. 47. 18.  6.]
 [16. 25. 37.  9.]
 [38.  5. 19. 65.]]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.645 | Acc: 83.250% | Wgt Acc: 84.778%
	I - Batch: 100 | Loss: 0.661 | Acc: 82.375% | Wgt Acc: 83.919%
	I - Batch: 150 | Loss: 0.654 | Acc: 83.458% | Wgt Acc: 84.943%
I - num batch: 160
I - Train -- Loss: 0.656 | Acc: 83.196% | Wgt Acc: 84.722% | LR: 1.250000e-04 | Dur: 81.97s
I - Confusion Matrix: [row->prediction - col->label]
[[289.   1.   4.   4.]
 [ 90. 575.   1.   5.]
 [ 92.   2. 729.   3.]
 [226.   0.   0. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.161 | Acc: 46.177% | Wgt Acc: 47.622% | Dur: 10.17s
I - Confusion Matrix: [row->prediction - col->label]
[[13.  1.  1.  3.]
 [18. 40. 21.  9.]
 [17. 32. 41. 17.]
 [40.  5. 12. 57.]]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.666 | Acc: 83.375% | Wgt Acc: 84.752%
	I - Batch: 100 | Loss: 0.653 | Acc: 84.562% | Wgt Acc: 85.974%
	I - Batch: 150 | Loss: 0.662 | Acc: 83.708% | Wgt Acc: 85.135%
I - num batch: 160
I - Train -- Loss: 0.657 | Acc: 83.706% | Wgt Acc: 85.156% | LR: 1.250000e-04 | Dur: 85.76s
I - Confusion Matrix: [row->prediction - col->label]
[[304.   0.   3.   6.]
 [ 76. 576.   1.   3.]
 [111.   2. 730.   7.]
 [206.   0.   0. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.166 | Acc: 50.153% | Wgt Acc: 51.834% | Dur: 10.35s
I - Confusion Matrix: [row->prediction - col->label]
[[21.  1.  1.  4.]
 [16. 40. 20.  6.]
 [15. 30. 36.  9.]
 [36.  7. 18. 67.]]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.640 | Acc: 85.125% | Wgt Acc: 86.528%
	I - Batch: 100 | Loss: 0.656 | Acc: 83.188% | Wgt Acc: 84.736%
	I - Batch: 150 | Loss: 0.658 | Acc: 83.250% | Wgt Acc: 84.745%
I - num batch: 160
I - Train -- Loss: 0.655 | Acc: 83.706% | Wgt Acc: 85.165% | LR: 1.250000e-04 | Dur: 89.51s
I - Confusion Matrix: [row->prediction - col->label]
[[306.   0.   3.   6.]
 [ 78. 577.   2.   5.]
 [114.   1. 727.   5.]
 [199.   0.   2. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.145 | Acc: 48.624% | Wgt Acc: 50.408% | Dur: 10.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 7.  0.  0.  2.]
 [10. 39. 16.  5.]
 [17. 31. 46. 12.]
 [54.  8. 13. 67.]]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.656 | Acc: 86.125% | Wgt Acc: 87.345%
	I - Batch: 100 | Loss: 0.661 | Acc: 84.062% | Wgt Acc: 85.501%
	I - Batch: 150 | Loss: 0.652 | Acc: 84.333% | Wgt Acc: 85.746%
I - num batch: 160
I - Train -- Loss: 0.654 | Acc: 84.295% | Wgt Acc: 85.695% | LR: 1.250000e-04 | Dur: 91.89s
I - Confusion Matrix: [row->prediction - col->label]
[[318.   0.   3.   5.]
 [ 67. 577.   0.   1.]
 [107.   1. 730.  10.]
 [205.   0.   1. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.148 | Acc: 52.294% | Wgt Acc: 53.668% | Dur: 10.76s
I - Confusion Matrix: [row->prediction - col->label]
[[23.  1.  1.  1.]
 [18. 42. 19.  7.]
 [17. 29. 42. 14.]
 [30.  6. 13. 64.]]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.672 | Acc: 83.500% | Wgt Acc: 85.027%
	I - Batch: 100 | Loss: 0.656 | Acc: 83.812% | Wgt Acc: 85.274%
	I - Batch: 150 | Loss: 0.654 | Acc: 83.708% | Wgt Acc: 85.179%
I - num batch: 160
I - Train -- Loss: 0.653 | Acc: 83.903% | Wgt Acc: 85.359% | LR: 1.250000e-04 | Dur: 89.44s
I - Confusion Matrix: [row->prediction - col->label]
[[309.   1.   1.   4.]
 [ 77. 576.   1.   3.]
 [ 99.   1. 727.   6.]
 [212.   0.   5. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.149 | Acc: 46.789% | Wgt Acc: 48.845% | Dur: 10.03s
I - Confusion Matrix: [row->prediction - col->label]
[[11.  1.  0.  2.]
 [ 6. 37. 16.  3.]
 [12. 29. 35. 11.]
 [59. 11. 24. 70.]]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.647 | Acc: 84.500% | Wgt Acc: 85.903%
	I - Batch: 100 | Loss: 0.652 | Acc: 83.688% | Wgt Acc: 85.176%
	I - Batch: 150 | Loss: 0.653 | Acc: 84.000% | Wgt Acc: 85.452%
I - num batch: 160
I - Train -- Loss: 0.656 | Acc: 83.785% | Wgt Acc: 85.253% | LR: 1.250000e-04 | Dur: 85.88s
I - Confusion Matrix: [row->prediction - col->label]
[[303.   0.   1.   7.]
 [ 71. 578.   1.   2.]
 [103.   0. 730.   6.]
 [220.   0.   2. 523.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.171 | Acc: 43.731% | Wgt Acc: 45.448% | Dur: 9.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 2.  0.  1.  2.]
 [22. 40. 21.  8.]
 [21. 35. 44. 19.]
 [43.  3.  9. 57.]]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.663 | Acc: 84.250% | Wgt Acc: 85.730%
	I - Batch: 100 | Loss: 0.645 | Acc: 85.062% | Wgt Acc: 86.488%
	I - Batch: 150 | Loss: 0.655 | Acc: 84.042% | Wgt Acc: 85.486%
I - num batch: 160
I - Train -- Loss: 0.653 | Acc: 84.295% | Wgt Acc: 85.713% | LR: 1.250000e-04 | Dur: 87.32s
I - Confusion Matrix: [row->prediction - col->label]
[[317.   0.   0.   7.]
 [ 74. 576.   2.   4.]
 [105.   2. 729.   2.]
 [201.   0.   3. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.181 | Acc: 46.483% | Wgt Acc: 48.098% | Dur: 9.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 7.  0.  1.  2.]
 [14. 32.  9.  5.]
 [18. 36. 45. 11.]
 [49. 10. 20. 68.]]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.646 | Acc: 84.875% | Wgt Acc: 86.126%
	I - Batch: 100 | Loss: 0.646 | Acc: 84.875% | Wgt Acc: 86.207%
	I - Batch: 150 | Loss: 0.652 | Acc: 84.083% | Wgt Acc: 85.509%
I - num batch: 160
I - Train -- Loss: 0.649 | Acc: 84.256% | Wgt Acc: 85.669% | LR: 1.250000e-04 | Dur: 89.09s
I - Confusion Matrix: [row->prediction - col->label]
[[317.   0.   2.   5.]
 [ 76. 576.   2.   3.]
 [ 99.   2. 729.   6.]
 [205.   0.   1. 524.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.156 | Acc: 50.153% | Wgt Acc: 52.446% | Dur: 10.24s
I - Confusion Matrix: [row->prediction - col->label]
[[17.  1.  0.  3.]
 [15. 45. 21.  6.]
 [ 6. 20. 31.  6.]
 [50. 12. 23. 71.]]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.670 | Acc: 83.875% | Wgt Acc: 85.413%
	I - Batch: 100 | Loss: 0.645 | Acc: 84.375% | Wgt Acc: 85.853%
	I - Batch: 150 | Loss: 0.648 | Acc: 84.375% | Wgt Acc: 85.804%
I - num batch: 160
I - Train -- Loss: 0.651 | Acc: 84.177% | Wgt Acc: 85.616% | LR: 1.250000e-04 | Dur: 87.52s
I - Confusion Matrix: [row->prediction - col->label]
[[312.   0.   1.   6.]
 [ 67. 577.   1.   3.]
 [100.   0. 730.   4.]
 [218.   1.   2. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.167 | Acc: 50.153% | Wgt Acc: 51.766% | Dur: 10.91s
I - Confusion Matrix: [row->prediction - col->label]
[[19.  1.  0.  3.]
 [ 7. 36. 15.  2.]
 [11. 29. 39. 11.]
 [51. 12. 21. 70.]]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.670 | Acc: 83.375% | Wgt Acc: 84.906%
	I - Batch: 100 | Loss: 0.657 | Acc: 83.812% | Wgt Acc: 85.282%
	I - Batch: 150 | Loss: 0.651 | Acc: 84.083% | Wgt Acc: 85.512%
I - num batch: 160
I - Train -- Loss: 0.651 | Acc: 83.942% | Wgt Acc: 85.386% | LR: 1.250000e-04 | Dur: 86.97s
I - Confusion Matrix: [row->prediction - col->label]
[[308.   0.   2.   4.]
 [ 88. 577.   1.   5.]
 [ 89.   1. 730.   6.]
 [212.   0.   1. 523.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.163 | Acc: 49.235% | Wgt Acc: 51.019% | Dur: 10.04s
I - Confusion Matrix: [row->prediction - col->label]
[[11.  0.  1.  3.]
 [18. 40. 18.  3.]
 [16. 30. 43. 13.]
 [43.  8. 13. 67.]]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.655 | Acc: 84.625% | Wgt Acc: 85.984%
	I - Batch: 100 | Loss: 0.649 | Acc: 85.062% | Wgt Acc: 86.409%
	I - Batch: 150 | Loss: 0.649 | Acc: 84.375% | Wgt Acc: 85.815%
I - num batch: 160
I - Train -- Loss: 0.650 | Acc: 84.020% | Wgt Acc: 85.483% | LR: 1.250000e-04 | Dur: 86.29s
I - Confusion Matrix: [row->prediction - col->label]
[[309.   0.   1.   3.]
 [ 69. 578.   2.   2.]
 [102.   0. 728.   8.]
 [217.   0.   3. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.162 | Acc: 48.930% | Wgt Acc: 50.747% | Dur: 9.69s
I - Confusion Matrix: [row->prediction - col->label]
[[ 9.  0.  1.  1.]
 [19. 43. 17.  6.]
 [13. 31. 44. 15.]
 [47.  4. 13. 64.]]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.647 | Acc: 84.125% | Wgt Acc: 85.602%
	I - Batch: 100 | Loss: 0.653 | Acc: 84.125% | Wgt Acc: 85.590%
	I - Batch: 150 | Loss: 0.650 | Acc: 84.167% | Wgt Acc: 85.600%
I - num batch: 160
I - Train -- Loss: 0.651 | Acc: 84.020% | Wgt Acc: 85.474% | LR: 1.250000e-04 | Dur: 88.80s
I - Confusion Matrix: [row->prediction - col->label]
[[309.   0.   3.   5.]
 [ 77. 577.   0.   3.]
 [ 99.   1. 729.   5.]
 [212.   0.   2. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.150 | Acc: 49.235% | Wgt Acc: 51.291% | Dur: 9.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 7.  1.  1.  4.]
 [16. 44. 16.  4.]
 [18. 27. 43. 11.]
 [47.  6. 15. 67.]]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.649 | Acc: 83.750% | Wgt Acc: 85.285%
	I - Batch: 100 | Loss: 0.643 | Acc: 85.062% | Wgt Acc: 86.476%
	I - Batch: 150 | Loss: 0.644 | Acc: 85.208% | Wgt Acc: 86.550%
I - num batch: 160
I - Train -- Loss: 0.650 | Acc: 85.002% | Wgt Acc: 86.350% | LR: 1.250000e-04 | Dur: 86.71s
I - Confusion Matrix: [row->prediction - col->label]
[[334.   0.   1.   3.]
 [ 80. 576.   1.   4.]
 [ 88.   2. 730.   6.]
 [195.   0.   2. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.171 | Acc: 48.624% | Wgt Acc: 50.272% | Dur: 10.59s
I - Confusion Matrix: [row->prediction - col->label]
[[19.  0.  1.  5.]
 [13. 37. 17.  5.]
 [12. 32. 36.  9.]
 [44.  9. 21. 67.]]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.647 | Acc: 85.125% | Wgt Acc: 86.554%
	I - Batch: 100 | Loss: 0.642 | Acc: 84.250% | Wgt Acc: 85.730%
	I - Batch: 150 | Loss: 0.643 | Acc: 84.417% | Wgt Acc: 85.858%
I - num batch: 160
I - Train -- Loss: 0.649 | Acc: 84.335% | Wgt Acc: 85.757% | LR: 1.250000e-04 | Dur: 88.01s
I - Confusion Matrix: [row->prediction - col->label]
[[318.   0.   2.   7.]
 [ 78. 577.   1.   2.]
 [ 90.   1. 728.   4.]
 [211.   0.   3. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.164 | Acc: 50.153% | Wgt Acc: 51.834% | Dur: 9.97s
I - Confusion Matrix: [row->prediction - col->label]
[[19.  2.  1.  3.]
 [ 9. 37. 16.  2.]
 [10. 25. 38. 11.]
 [50. 14. 20. 70.]]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.653 | Acc: 82.875% | Wgt Acc: 84.503%
	I - Batch: 100 | Loss: 0.655 | Acc: 83.375% | Wgt Acc: 84.837%
	I - Batch: 150 | Loss: 0.650 | Acc: 83.750% | Wgt Acc: 85.209%
I - num batch: 160
I - Train -- Loss: 0.648 | Acc: 83.942% | Wgt Acc: 85.386% | LR: 1.250000e-04 | Dur: 88.20s
I - Confusion Matrix: [row->prediction - col->label]
[[310.   0.   2.   4.]
 [ 89. 578.   2.   3.]
 [105.   0. 728.   9.]
 [193.   0.   2. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.148 | Acc: 54.128% | Wgt Acc: 55.231% | Dur: 9.74s
I - Confusion Matrix: [row->prediction - col->label]
[[23.  0.  1.  2.]
 [17. 40. 14.  8.]
 [15. 32. 49. 11.]
 [33.  6. 11. 65.]]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.682 | Acc: 82.875% | Wgt Acc: 84.370%
	I - Batch: 100 | Loss: 0.655 | Acc: 83.250% | Wgt Acc: 84.718%
	I - Batch: 150 | Loss: 0.646 | Acc: 83.792% | Wgt Acc: 85.257%
I - num batch: 160
I - Train -- Loss: 0.644 | Acc: 83.824% | Wgt Acc: 85.280% | LR: 1.250000e-04 | Dur: 85.56s
I - Confusion Matrix: [row->prediction - col->label]
[[305.   1.   1.   6.]
 [ 81. 576.   1.   2.]
 [101.   1. 730.   6.]
 [210.   0.   2. 524.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.183 | Acc: 47.706% | Wgt Acc: 49.796% | Dur: 10.04s
I - Confusion Matrix: [row->prediction - col->label]
[[14.  2.  1.  5.]
 [14. 38. 15.  4.]
 [ 9. 25. 33.  6.]
 [51. 13. 26. 71.]]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.667 | Acc: 84.250% | Wgt Acc: 85.630%
	I - Batch: 100 | Loss: 0.642 | Acc: 84.812% | Wgt Acc: 86.188%
	I - Batch: 150 | Loss: 0.647 | Acc: 84.625% | Wgt Acc: 86.011%
I - num batch: 160
I - Train -- Loss: 0.650 | Acc: 84.531% | Wgt Acc: 85.934% | LR: 1.250000e-04 | Dur: 87.36s
I - Confusion Matrix: [row->prediction - col->label]
[[323.   1.   2.   5.]
 [ 74. 577.   2.   3.]
 [ 89.   0. 728.   5.]
 [211.   0.   2. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.180 | Acc: 46.789% | Wgt Acc: 47.962% | Dur: 9.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 9.  0.  1.  4.]
 [17. 39. 18.  6.]
 [25. 32. 50. 21.]
 [37.  7.  6. 55.]]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.633 | Acc: 85.500% | Wgt Acc: 86.881%
	I - Batch: 100 | Loss: 0.644 | Acc: 84.375% | Wgt Acc: 85.813%
	I - Batch: 150 | Loss: 0.647 | Acc: 84.333% | Wgt Acc: 85.734%
I - num batch: 160
I - Train -- Loss: 0.645 | Acc: 84.413% | Wgt Acc: 85.810% | LR: 1.250000e-04 | Dur: 84.51s
I - Confusion Matrix: [row->prediction - col->label]
[[320.   0.   2.   6.]
 [ 72. 576.   1.   1.]
 [ 99.   1. 730.   7.]
 [206.   1.   1. 524.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.154 | Acc: 52.294% | Wgt Acc: 53.940% | Dur: 9.80s
I - Confusion Matrix: [row->prediction - col->label]
[[19.  1.  2.  4.]
 [18. 47. 21.  9.]
 [12. 25. 42. 10.]
 [39.  5. 10. 63.]]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.621 | Acc: 85.250% | Wgt Acc: 86.702%
	I - Batch: 100 | Loss: 0.637 | Acc: 84.875% | Wgt Acc: 86.265%
	I - Batch: 150 | Loss: 0.644 | Acc: 84.167% | Wgt Acc: 85.600%
I - num batch: 160
I - Train -- Loss: 0.647 | Acc: 84.020% | Wgt Acc: 85.465% | LR: 1.250000e-04 | Dur: 86.15s
I - Confusion Matrix: [row->prediction - col->label]
[[310.   1.   2.   3.]
 [ 70. 576.   0.   6.]
 [ 98.   1. 729.   4.]
 [219.   0.   3. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.161 | Acc: 48.012% | Wgt Acc: 49.185% | Dur: 10.10s
I - Confusion Matrix: [row->prediction - col->label]
[[12.  1.  0.  2.]
 [18. 43. 17.  9.]
 [20. 27. 49. 22.]
 [38.  7.  9. 53.]]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.654 | Acc: 84.750% | Wgt Acc: 86.113%
	I - Batch: 100 | Loss: 0.653 | Acc: 84.125% | Wgt Acc: 85.549%
	I - Batch: 150 | Loss: 0.651 | Acc: 84.083% | Wgt Acc: 85.539%
I - num batch: 160
I - Train -- Loss: 0.649 | Acc: 84.138% | Wgt Acc: 85.598% | LR: 1.250000e-04 | Dur: 86.27s
I - Confusion Matrix: [row->prediction - col->label]
[[310.   0.   3.   6.]
 [ 80. 578.   2.   2.]
 [107.   0. 729.   4.]
 [200.   0.   0. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.164 | Acc: 47.706% | Wgt Acc: 49.389% | Dur: 10.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 7.  0.  0.  1.]
 [14. 42. 15.  7.]
 [22. 30. 46. 17.]
 [45.  6. 14. 61.]]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.619 | Acc: 85.500% | Wgt Acc: 86.856%
	I - Batch: 100 | Loss: 0.631 | Acc: 84.938% | Wgt Acc: 86.287%
	I - Batch: 150 | Loss: 0.639 | Acc: 84.417% | Wgt Acc: 85.850%
I - num batch: 160
I - Train -- Loss: 0.646 | Acc: 84.099% | Wgt Acc: 85.545% | LR: 1.250000e-04 | Dur: 89.09s
I - Confusion Matrix: [row->prediction - col->label]
[[310.   0.   2.   4.]
 [ 86. 578.   1.   4.]
 [ 90.   0. 730.   6.]
 [211.   0.   1. 524.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.179 | Acc: 52.294% | Wgt Acc: 53.261% | Dur: 9.91s
I - Confusion Matrix: [row->prediction - col->label]
[[27.  2.  1.  7.]
 [27. 48. 17. 14.]
 [12. 23. 44. 13.]
 [22.  5. 13. 52.]]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.660 | Acc: 81.750% | Wgt Acc: 83.413%
	I - Batch: 100 | Loss: 0.656 | Acc: 84.000% | Wgt Acc: 85.413%
	I - Batch: 150 | Loss: 0.652 | Acc: 84.500% | Wgt Acc: 85.872%
I - num batch: 160
I - Train -- Loss: 0.652 | Acc: 84.374% | Wgt Acc: 85.757% | LR: 1.250000e-04 | Dur: 89.48s
I - Confusion Matrix: [row->prediction - col->label]
[[322.   0.   1.   8.]
 [ 80. 575.   3.   2.]
 [103.   3. 729.   5.]
 [192.   0.   1. 523.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.145 | Acc: 49.847% | Wgt Acc: 51.563% | Dur: 9.99s
I - Confusion Matrix: [row->prediction - col->label]
[[14.  0.  2.  1.]
 [21. 46. 17. 10.]
 [16. 27. 42. 14.]
 [37.  5. 14. 61.]]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.634 | Acc: 85.500% | Wgt Acc: 86.861%
	I - Batch: 100 | Loss: 0.633 | Acc: 85.125% | Wgt Acc: 86.534%
	I - Batch: 150 | Loss: 0.642 | Acc: 84.125% | Wgt Acc: 85.588%
I - num batch: 160
I - Train -- Loss: 0.646 | Acc: 84.177% | Wgt Acc: 85.633% | LR: 1.250000e-04 | Dur: 88.45s
I - Confusion Matrix: [row->prediction - col->label]
[[310.   0.   2.   6.]
 [ 71. 578.   1.   3.]
 [100.   0. 730.   3.]
 [216.   0.   1. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.136 | Acc: 51.988% | Wgt Acc: 53.940% | Dur: 10.48s
I - Confusion Matrix: [row->prediction - col->label]
[[18.  1.  0.  4.]
 [22. 47. 19.  7.]
 [13. 25. 38.  8.]
 [35.  5. 18. 67.]]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.627 | Acc: 86.375% | Wgt Acc: 87.658%
	I - Batch: 100 | Loss: 0.645 | Acc: 84.500% | Wgt Acc: 85.931%
	I - Batch: 150 | Loss: 0.647 | Acc: 84.000% | Wgt Acc: 85.451%
I - num batch: 160
I - Train -- Loss: 0.643 | Acc: 83.942% | Wgt Acc: 85.403% | LR: 1.250000e-04 | Dur: 86.87s
I - Confusion Matrix: [row->prediction - col->label]
[[307.   0.   3.   5.]
 [ 77. 577.   1.   5.]
 [ 90.   1. 729.   3.]
 [223.   0.   1. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.175 | Acc: 48.012% | Wgt Acc: 49.864% | Dur: 9.91s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  1.  1.  4.]
 [15. 40. 23.  8.]
 [17. 28. 36.  8.]
 [41.  9. 15. 66.]]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.656 | Acc: 84.125% | Wgt Acc: 85.602%
	I - Batch: 100 | Loss: 0.647 | Acc: 84.188% | Wgt Acc: 85.620%
	I - Batch: 150 | Loss: 0.644 | Acc: 84.000% | Wgt Acc: 85.461%
I - num batch: 160
I - Train -- Loss: 0.643 | Acc: 84.374% | Wgt Acc: 85.793% | LR: 1.250000e-04 | Dur: 88.32s
I - Confusion Matrix: [row->prediction - col->label]
[[317.   0.   2.   5.]
 [ 79. 577.   1.   2.]
 [ 91.   1. 730.   6.]
 [210.   0.   1. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.158 | Acc: 48.012% | Wgt Acc: 49.321% | Dur: 9.68s
I - Confusion Matrix: [row->prediction - col->label]
[[12.  1.  1.  2.]
 [21. 42. 20. 10.]
 [25. 31. 47. 18.]
 [30.  4.  7. 56.]]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.640 | Acc: 85.125% | Wgt Acc: 86.445%
	I - Batch: 100 | Loss: 0.653 | Acc: 84.188% | Wgt Acc: 85.591%
	I - Batch: 150 | Loss: 0.645 | Acc: 84.583% | Wgt Acc: 85.980%
I - num batch: 160
I - Train -- Loss: 0.644 | Acc: 85.041% | Wgt Acc: 86.403% | LR: 1.250000e-04 | Dur: 86.36s
I - Confusion Matrix: [row->prediction - col->label]
[[333.   0.   3.   7.]
 [ 78. 577.   1.   3.]
 [ 99.   1. 730.   2.]
 [187.   0.   0. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.194 | Acc: 50.765% | Wgt Acc: 51.630% | Dur: 9.87s
I - Confusion Matrix: [row->prediction - col->label]
[[24.  0.  2.  4.]
 [20. 39. 18.  7.]
 [17. 33. 46. 18.]
 [27.  6.  9. 57.]]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 0.647 | Acc: 84.625% | Wgt Acc: 85.983%
	I - Batch: 100 | Loss: 0.654 | Acc: 83.562% | Wgt Acc: 85.030%
	I - Batch: 150 | Loss: 0.650 | Acc: 84.042% | Wgt Acc: 85.470%
I - num batch: 160
I - Train -- Loss: 0.646 | Acc: 84.609% | Wgt Acc: 85.987% | LR: 1.250000e-04 | Dur: 87.68s
I - Confusion Matrix: [row->prediction - col->label]
[[326.   0.   3.   6.]
 [ 78. 578.   2.   5.]
 [ 87.   0. 729.   5.]
 [206.   0.   0. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.153 | Acc: 51.988% | Wgt Acc: 52.853% | Dur: 9.97s
I - Confusion Matrix: [row->prediction - col->label]
[[19.  1.  2.  2.]
 [18. 44. 13.  7.]
 [21. 28. 53. 23.]
 [30.  5.  7. 54.]]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 0.654 | Acc: 83.750% | Wgt Acc: 85.203%
	I - Batch: 100 | Loss: 0.643 | Acc: 85.188% | Wgt Acc: 86.561%
	I - Batch: 150 | Loss: 0.641 | Acc: 85.000% | Wgt Acc: 86.356%
I - num batch: 160
I - Train -- Loss: 0.642 | Acc: 85.041% | Wgt Acc: 86.377% | LR: 1.250000e-04 | Dur: 87.96s
I - Confusion Matrix: [row->prediction - col->label]
[[337.   0.   3.   6.]
 [ 73. 577.   1.   3.]
 [ 85.   1. 729.   6.]
 [202.   0.   1. 523.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.195 | Acc: 46.483% | Wgt Acc: 48.098% | Dur: 10.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 7.  1.  0.  3.]
 [14. 40. 18.  6.]
 [17. 31. 45. 17.]
 [50.  6. 12. 60.]]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 0.630 | Acc: 85.875% | Wgt Acc: 87.156%
	I - Batch: 100 | Loss: 0.637 | Acc: 85.812% | Wgt Acc: 87.048%
	I - Batch: 150 | Loss: 0.640 | Acc: 85.542% | Wgt Acc: 86.847%
I - num batch: 160
I - Train -- Loss: 0.639 | Acc: 85.512% | Wgt Acc: 86.819% | LR: 1.250000e-04 | Dur: 86.36s
I - Confusion Matrix: [row->prediction - col->label]
[[346.   0.   3.   7.]
 [ 59. 577.   1.   3.]
 [ 94.   1. 730.   3.]
 [198.   0.   0. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.141 | Acc: 50.765% | Wgt Acc: 52.038% | Dur: 10.13s
I - Confusion Matrix: [row->prediction - col->label]
[[18.  1.  1.  2.]
 [15. 42. 19. 11.]
 [16. 28. 46. 13.]
 [39.  7.  9. 60.]]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 0.666 | Acc: 83.250% | Wgt Acc: 84.652%
	I - Batch: 100 | Loss: 0.656 | Acc: 83.375% | Wgt Acc: 84.787%
	I - Batch: 150 | Loss: 0.648 | Acc: 84.750% | Wgt Acc: 86.094%
I - num batch: 160
I - Train -- Loss: 0.642 | Acc: 85.159% | Wgt Acc: 86.492% | LR: 1.250000e-04 | Dur: 89.49s
I - Confusion Matrix: [row->prediction - col->label]
[[339.   0.   2.   8.]
 [ 75. 577.   1.   3.]
 [ 89.   1. 729.   3.]
 [194.   0.   2. 524.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.156 | Acc: 51.070% | Wgt Acc: 52.378% | Dur: 10.29s
I - Confusion Matrix: [row->prediction - col->label]
[[18.  0.  1.  5.]
 [14. 42. 19.  7.]
 [16. 26. 46. 13.]
 [40. 10.  9. 61.]]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 0.630 | Acc: 88.000% | Wgt Acc: 89.027%
	I - Batch: 100 | Loss: 0.638 | Acc: 85.750% | Wgt Acc: 86.991%
	I - Batch: 150 | Loss: 0.643 | Acc: 85.583% | Wgt Acc: 86.874%
I - num batch: 160
I - Train -- Loss: 0.641 | Acc: 85.709% | Wgt Acc: 87.005% | LR: 1.250000e-04 | Dur: 87.53s
I - Confusion Matrix: [row->prediction - col->label]
[[350.   0.   3.   6.]
 [ 73. 577.   1.   1.]
 [ 79.   1. 730.   5.]
 [195.   0.   0. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.157 | Acc: 50.153% | Wgt Acc: 51.155% | Dur: 10.95s
I - Confusion Matrix: [row->prediction - col->label]
[[21.  1.  1.  1.]
 [21. 44. 18. 13.]
 [14. 27. 46. 19.]
 [32.  6. 10. 53.]]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 0.668 | Acc: 84.875% | Wgt Acc: 86.306%
	I - Batch: 100 | Loss: 0.651 | Acc: 85.250% | Wgt Acc: 86.607%
	I - Batch: 150 | Loss: 0.642 | Acc: 85.292% | Wgt Acc: 86.668%
I - num batch: 160
I - Train -- Loss: 0.646 | Acc: 85.198% | Wgt Acc: 86.562% | LR: 1.250000e-04 | Dur: 87.95s
I - Confusion Matrix: [row->prediction - col->label]
[[336.   0.   3.   5.]
 [ 75. 578.   1.   2.]
 [ 88.   0. 729.   4.]
 [198.   0.   1. 527.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.157 | Acc: 49.847% | Wgt Acc: 51.087% | Dur: 9.99s
I - Confusion Matrix: [row->prediction - col->label]
[[15.  0.  1.  1.]
 [20. 41. 13.  8.]
 [18. 31. 48. 18.]
 [35.  6. 13. 59.]]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 0.635 | Acc: 86.500% | Wgt Acc: 87.683%
	I - Batch: 100 | Loss: 0.603 | Acc: 90.812% | Wgt Acc: 91.501%
	I - Batch: 150 | Loss: 0.570 | Acc: 92.792% | Wgt Acc: 93.253%
I - num batch: 160
I - Train -- Loss: 0.567 | Acc: 92.933% | Wgt Acc: 93.392% | LR: 1.250000e-04 | Dur: 87.36s
I - Confusion Matrix: [row->prediction - col->label]
[[553.   2.   7.  20.]
 [ 35. 576.   2.   1.]
 [ 46.   0. 725.   4.]
 [ 63.   0.   0. 513.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.056 | Acc: 59.939% | Wgt Acc: 59.715% | Dur: 10.44s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  4. 21.]
 [ 8. 39. 18.  6.]
 [ 4. 26. 40.  3.]
 [15.  8. 13. 56.]]

I - Local maximum validation set accuracy:  59.94

I - Validation set results: 
[14-1-2--0.10][50-3-3-0.43][124-2-1--0.06][127-0-0-0.99][443-2-2-0.99][567-0-0-0.29][573-1-1-0.55][615-0-3-0.98][695-1-2--0.59][722-3-3-0.99]
[826-0-0-0.84][878-0-0-0.71][1103-0-2--0.07][1212-3-3-0.72][1368-0-0-0.05][2181-2-2-0.19][2476-2-2-0.59][2721-2-2-0.77][2818-1-3-0.99][2886-2-1-0.53]
[3231-2-2-0.99][3333-2-1-0.99][3482-2-2--0.08][3536-3-3-0.87][3625-1-1--0.08][3909-0-0-0.28][4035-0-0-0.68][4140-0-0-0.09][4214-1-3-0.99][4346-1-0--0.23]
[4581-2-2-0.79][4708-3-3-0.53][4838-3-3-0.31][4845-1-1-0.38][4868-0-0-0.83][4939-0-0--0.62][4984-2-2-0.99][5078-1-2-0.56][5396-0-0-0.99][5479-1-2-0.88]
[5717-0-0-0.62][5843-1-3-0.99][5949-3-3-0.43][5987-2-2-0.65][6014-3-1--0.02][6033-3-0-0.55][6313-0-0--0.01][6421-3-3-0.45][6500-1-1--0.44][6583-3-3-0.67]
[6683-3-3-0.85][6825-2-1-0.50][6998-3-3--0.53][7049-3-3-0.36][7517-1-1-0.14][7521-1-1--0.35][7528-1-3-0.21][7949-1-2--0.02][8135-1-0--0.25][8185-3-0-0.40]
[8269-3-1-0.99][8273-3-3-0.53][8543-3-0-0.99][8666-1-0--0.14][8672-0-0-0.95][8903-1-2-0.18][9001-2-2--0.08][9036-2-2-0.29][9281-3-3--0.07][9300-2-2-0.82]
[9571-0-3-0.20][9617-1-1--0.38][9644-2-1-0.76][9705-2-2-0.92][9801-0-3-0.97][9803-3-3-0.52][9865-3-3-0.99][9896-2-2-0.79][10314-1-1-0.15][10337-3-3-0.83]
[10403-0-2-0.18][10653-2-1--0.46][10704-2-1-0.61][10719-1-2-0.95][10727-1-2-0.99][10836-0-0-0.99][10969-2-3-0.64][11042-0-0--0.11][11088-1-1-0.99][11322-0-0-0.56]
[11398-2-2-0.80][11499-0-0-0.08][11502-3-2--0.25][11512-3-3-0.99][11608-1-1-0.99][11610-0-0-0.30][11692-0-3-0.88][11905-0-0-0.95][11993-1-1-0.99][12002-2-0-0.71]
[12052-0-0-0.27][12201-0-0-0.49][12235-2-2-0.56][12320-1-3--0.52][12377-2-2-0.33][12398-2-3--0.36][12503-1-1-0.68][12617-0-1--0.30][12685-3-3--0.41][12738-2-3--0.87]
[12742-2-2-0.99][12823-0-3-0.78][13110-1-3-0.43][13240-3-3--0.26][13253-1-1-0.10][13273-0-0-0.99][13634-1-3-0.19][13763-2-2--0.18][13905-3-0--0.40][14060-2-2--0.25]
[14065-3-0--0.05][14147-3-3--0.21][14595-2-1-0.96][14687-2-3-0.90][14788-2-3-0.20][14869-1-1-0.47][14872-3-0--0.41][14877-1-1-0.99][14927-0-0--0.44][15066-0-0-0.86]
[15175-1-1-0.99][15178-2-3-0.46][15375-3-1--0.67][15389-3-3-0.99][15568-2-1-0.64][15675-3-3-0.99][15869-1-2--0.25][16207-3-0-0.19][16236-0-0--0.70][16302-3-0--0.06]
[16331-2-2-0.99][16381-0-0-0.17][16488-1-1-0.10][16495-0-0-0.37][16650-0-0-0.99][16719-1-1-0.29][16801-0-0-0.99][16828-0-0-0.50][17137-3-3-0.77][17245-1-1--0.26]
[17278-3-3--0.39][17282-0-0-0.21][17311-2-2-0.10][17336-2-1-0.95][17608-3-3-0.99][17627-0-3--0.24][17877-3-0--0.27][17924-1-3--0.33][17984-3-0-0.97][18211-0-1-0.14]
[18276-3-0-0.51][18287-1-1--0.35][18394-0-0-0.44][18428-0-0-0.10][18442-0-3-0.55][18478-3-3-0.99][18607-0-0-0.27][18616-0-0--0.03][18663-0-0-0.07][18718-0-0-0.52]
[18766-2-2-0.74][18824-2-2-0.11][18890-3-3--0.12][18930-3-0--0.51][18938-3-3-0.98][19817-1-2-0.87][19839-0-2--0.06][19930-3-0-0.03][19944-0-2-0.27][20036-2-2-0.98]
[20101-3-3-0.24][20474-1-2--0.22][20547-3-3--0.53][20929-2-2-0.46][21245-1-2-0.83][21257-3-3-0.28][21293-1-2-0.52][21316-1-1-0.99][21384-1-2-0.99][21448-1-1-0.40]
[21483-0-0-0.03][21487-2-2-0.99][21714-0-3-0.89][21943-3-3-0.28][21947-0-0-0.51][21948-0-0-0.60][21965-2-2--0.23][21998-1-1-0.89][22025-0-3-0.87][22228-3-3-0.99]
[22446-1-1-0.79][22494-3-0-0.39][22757-0-0-0.28][22811-3-3-0.99][22976-3-2-0.29][22985-3-3-0.99][23014-0-3-0.93][23112-1-2-0.44][23144-3-0-0.13][23168-2-0--0.18]
[23219-0-3-0.09][23363-3-3-0.91][23470-0-1--0.45][23486-2-3--0.24][23497-0-3-0.99][23516-0-0-0.94][23690-1-2-0.99][23921-2-2-0.84][23936-1-2-0.52][24040-3-0--0.38]
[24111-1-2-0.15][24182-0-0-0.62][24238-3-3-0.99][24290-2-0-0.42][24345-0-0--0.18][24364-1-2-0.81][24427-3-0--0.01][24477-2-2-0.98][24495-2-1-0.97][24893-2-1--0.05]
[25012-1-2-0.39][25121-2-1-0.54][25165-3-3-0.99][25183-0-0-0.32][25297-3-3-0.99][25398-0-0-0.54][25574-2-2-0.60][25644-1-1-0.99][25718-1-1--0.55][25774-2-2-0.54]
[26032-3-3-0.95][26051-3-3-0.99][26120-0-1--0.34][26321-1-1-0.93][26732-1-1-0.56][26784-3-3-0.99][26827-3-3-0.20][26833-0-3-0.99][26838-2-2--0.44][26860-1-2-0.07]
[26948-0-1--0.30][27049-3-0-0.76][27098-1-1--0.11][27526-0-0-0.89][27639-3-3-0.99][27698-3-3-0.85][27772-0-0-0.99][27890-1-1-0.38][28040-0-3--0.05][28503-2-2-0.99]
[28577-1-1-0.98][28959-0-0-0.99][29198-3-2--0.44][29777-0-0-0.99][29877-2-1-0.14][30035-1-1-0.99][30098-0-0--0.10][30326-1-1-0.99][30572-2-3-0.54][30716-0-1-0.04]
[30806-2-3--0.22][30906-1-1-0.63][31007-0-0-0.29][31181-3-3-0.98][31238-0-0--0.25][31347-0-3-0.99][31422-2-0--0.47][31429-3-3--0.31][31431-0-1-0.41][31432-1-1-0.94]
[31477-0-0-0.90][31524-1-0--0.40][31597-1-1-0.96][31619-1-2-0.99][31701-0-0-0.85][31755-0-0-0.10][31854-3-3-0.99][32074-1-2-0.61][32078-3-3-0.63][32111-1-1-0.42]
[32127-1-2-0.49][32140-3-3-0.93][32263-2-3--0.34][32365-0-0-0.19][32411-2-3-0.99][32429-3-3-0.99][32473-3-0--0.21][32574-3-0-0.32][32584-0-0-0.30][32622-0-1-0.99]
[32858-3-0-0.35][32969-3-3-0.99][33016-2-2-0.37][33031-1-0--0.02][33035-2-2-0.77][33133-2-1-0.88][33173-2-1-0.06][33175-3-1--0.15][33306-3-1-0.99][33309-2-3--0.24]
[33474-0-0--0.32][33478-2-3--0.45][33618-1-1--0.15][33712-0-0-0.49][33782-2-2-0.84][33914-3-3-0.99][34076-3-1--0.30][34112-2-2--0.19][34138-2-1-0.99][34239-1-1-0.67]
[34364-2-1-0.71][34617-1-2-0.51][34751-3-3--0.32][34783-2-2-0.15][35015-3-3-0.98][35018-1-2-0.87][35288-2-2-0.41]
---------------------------
I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 0.480 | Acc: 96.875% | Wgt Acc: 96.928%
	I - Batch: 100 | Loss: 0.485 | Acc: 96.562% | Wgt Acc: 96.611%
	I - Batch: 150 | Loss: 0.476 | Acc: 97.042% | Wgt Acc: 97.026%
I - num batch: 160
I - Train -- Loss: 0.476 | Acc: 96.977% | Wgt Acc: 96.975% | LR: 1.250000e-04 | Dur: 91.41s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   1.   7.  30.]
 [  6. 576.   0.   2.]
 [ 13.   1. 727.   0.]
 [ 17.   0.   0. 506.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.051 | Acc: 60.550% | Wgt Acc: 59.307% | Dur: 9.72s
I - Confusion Matrix: [row->prediction - col->label]
[[79. 12. 12. 34.]
 [ 2. 34. 11.  2.]
 [ 1. 24. 38.  3.]
 [ 6.  8. 14. 47.]]

I - Local maximum validation set accuracy:  60.55

I - Validation set results: 
[14-1-1--0.40][50-3-0--0.38][124-2-2-0.43][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.21][615-0-3-0.95][695-1-0-0.15][722-3-3-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-0--0.00][1212-3-3-0.85][1368-0-0-0.93][2181-2-0-0.18][2476-2-2--0.03][2721-2-2-0.83][2818-1-3-0.97][2886-2-1-0.80]
[3231-2-2-0.99][3333-2-3-0.60][3482-2-3--0.01][3536-3-3-0.99][3625-1-1-0.73][3909-0-0--0.04][4035-0-0-0.99][4140-0-0-0.99][4214-1-3-0.99][4346-1-0-0.80]
[4581-2-2-0.69][4708-3-3-0.99][4838-3-3-0.67][4845-1-1-0.12][4868-0-0-0.99][4939-0-0--0.27][4984-2-2-0.81][5078-1-2-0.59][5396-0-0-0.99][5479-1-2-0.28]
[5717-0-0-0.95][5843-1-2-0.78][5949-3-0-0.95][5987-2-2-0.71][6014-3-3-0.13][6033-3-0-0.99][6313-0-0-0.59][6421-3-3-0.12][6500-1-0--0.41][6583-3-3-0.93]
[6683-3-3-0.99][6825-2-0-0.05][6998-3-0--0.61][7049-3-3-0.97][7517-1-1-0.28][7521-1-1--0.64][7528-1-3-0.66][7949-1-2--0.49][8135-1-0-0.08][8185-3-0-0.99]
[8269-3-1--0.42][8273-3-3-0.63][8543-3-0-0.99][8666-1-0--0.18][8672-0-0-0.64][8903-1-1--0.49][9001-2-2-0.99][9036-2-2-0.55][9281-3-1--0.86][9300-2-2-0.11]
[9571-0-0-0.70][9617-1-2--0.22][9644-2-1--0.04][9705-2-0--0.22][9801-0-0-0.84][9803-3-3-0.95][9865-3-0-0.99][9896-2-2-0.42][10314-1-0--0.41][10337-3-0-0.99]
[10403-0-2-0.61][10653-2-2-0.40][10704-2-1-0.28][10719-1-2-0.72][10727-1-2-0.99][10836-0-0-0.99][10969-2-3-0.97][11042-0-0-0.85][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-0--0.22][11499-0-0-0.85][11502-3-0-0.50][11512-3-3-0.99][11608-1-1-0.99][11610-0-0-0.59][11692-0-0-0.99][11905-0-0-0.99][11993-1-1-0.99][12002-2-3--0.16]
[12052-0-0-0.80][12201-0-0-0.97][12235-2-1-0.36][12320-1-0--0.10][12377-2-2-0.12][12398-2-0--0.01][12503-1-3--0.12][12617-0-1--0.70][12685-3-3--0.19][12738-2-0--0.18]
[12742-2-2-0.99][12823-0-3-0.99][13110-1-2--0.02][13240-3-3-0.88][13253-1-1-0.99][13273-0-0-0.99][13634-1-3-0.28][13763-2-2--0.29][13905-3-0-0.75][14060-2-1-0.40]
[14065-3-0-0.70][14147-3-3-0.56][14595-2-2--0.10][14687-2-2-0.99][14788-2-3-0.75][14869-1-2-0.75][14872-3-0-0.22][14877-1-1-0.99][14927-0-3-0.68][15066-0-0-0.99]
[15175-1-1-0.74][15178-2-3-0.63][15375-3-0-0.99][15389-3-3-0.99][15568-2-1-0.53][15675-3-3-0.99][15869-1-0--0.18][16207-3-0-0.80][16236-0-0-0.13][16302-3-0-0.60]
[16331-2-2-0.99][16381-0-0-0.76][16488-1-1-0.88][16495-0-0-0.25][16650-0-0-0.99][16719-1-1-0.05][16801-0-0-0.99][16828-0-0-0.81][17137-3-0-0.98][17245-1-2--0.43]
[17278-3-0-0.01][17282-0-0-0.51][17311-2-2-0.40][17336-2-3-0.23][17608-3-3-0.99][17627-0-0--0.29][17877-3-0-0.18][17924-1-3-0.85][17984-3-0-0.99][18211-0-0--0.23]
[18276-3-0-0.99][18287-1-1--0.21][18394-0-0-0.99][18428-0-0-0.40][18442-0-3-0.83][18478-3-0-0.99][18607-0-0-0.64][18616-0-0-0.63][18663-0-0-0.82][18718-0-0-0.99]
[18766-2-2--0.57][18824-2-2-0.28][18890-3-3--0.01][18930-3-0-0.02][18938-3-3-0.99][19817-1-2-0.98][19839-0-0-0.49][19930-3-0-0.50][19944-0-0-0.48][20036-2-2-0.65]
[20101-3-3-0.49][20474-1-2--0.19][20547-3-3-0.42][20929-2-2-0.96][21245-1-2-0.05][21257-3-2--0.33][21293-1-1-0.65][21316-1-3--0.01][21384-1-3-0.17][21448-1-1--0.22]
[21483-0-0-0.99][21487-2-2-0.91][21714-0-0-0.24][21943-3-3--0.03][21947-0-0-0.90][21948-0-0-0.99][21965-2-2-0.98][21998-1-1-0.77][22025-0-3-0.47][22228-3-3-0.99]
[22446-1-2-0.99][22494-3-0-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-3--0.15][22985-3-3-0.86][23014-0-0-0.99][23112-1-2-0.95][23144-3-0-0.81][23168-2-0-0.74]
[23219-0-0-0.92][23363-3-3-0.07][23470-0-0--0.17][23486-2-3-0.29][23497-0-0-0.99][23516-0-0-0.99][23690-1-0--0.16][23921-2-2-0.69][23936-1-2--0.12][24040-3-0-0.32]
[24111-1-1-0.83][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.75][24364-1-2-0.98][24427-3-0-0.99][24477-2-2-0.49][24495-2-1-0.59][24893-2-1--0.07]
[25012-1-1-0.24][25121-2-1-0.21][25165-3-3-0.99][25183-0-0-0.89][25297-3-3-0.99][25398-0-0-0.96][25574-2-2-0.41][25644-1-1-0.20][25718-1-2-0.52][25774-2-3-0.60]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-0-0.48][26321-1-1-0.66][26732-1-1-0.66][26784-3-3-0.99][26827-3-3-0.90][26833-0-3-0.99][26838-2-3--0.03][26860-1-0-0.39]
[26948-0-0-0.42][27049-3-0-0.99][27098-1-0-0.06][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.47][28040-0-0-0.57][28503-2-2-0.99]
[28577-1-1-0.55][28959-0-0-0.99][29198-3-2--0.35][29777-0-0-0.99][29877-2-1-0.23][30035-1-1-0.99][30098-0-0-0.99][30326-1-1-0.99][30572-2-3-0.83][30716-0-0-0.13]
[30806-2-3-0.48][30906-1-1-0.90][31007-0-0-0.37][31181-3-3-0.58][31238-0-0-0.22][31347-0-0-0.99][31422-2-0--0.36][31429-3-0-0.45][31431-0-0-0.99][31432-1-1-0.41]
[31477-0-0-0.99][31524-1-1-0.21][31597-1-1-0.18][31619-1-2-0.99][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.88][32074-1-2-0.34][32078-3-3-0.99][32111-1-1-0.85]
[32127-1-2-0.32][32140-3-3-0.86][32263-2-0--0.57][32365-0-0-0.47][32411-2-0-0.99][32429-3-0-0.99][32473-3-0-0.98][32574-3-0-0.97][32584-0-0-0.88][32622-0-1-0.29]
[32858-3-0-0.99][32969-3-0-0.89][33016-2-2-0.71][33031-1-0-0.08][33035-2-2--0.45][33133-2-1-0.28][33173-2-2--0.63][33175-3-2-0.18][33306-3-3-0.99][33309-2-3-0.64]
[33474-0-0-0.33][33478-2-0--0.10][33618-1-1-0.35][33712-0-0-0.97][33782-2-2-0.99][33914-3-3-0.99][34076-3-3-0.99][34112-2-2-0.87][34138-2-3-0.99][34239-1-2--0.40]
[34364-2-2-0.12][34617-1-2-0.74][34751-3-3-0.20][34783-2-2-0.34][35015-3-3-0.99][35018-1-2-0.09][35288-2-2--0.02]
---------------------------
I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 0.468 | Acc: 97.875% | Wgt Acc: 97.766%
	I - Batch: 100 | Loss: 0.459 | Acc: 97.688% | Wgt Acc: 97.642%
	I - Batch: 150 | Loss: 0.466 | Acc: 97.292% | Wgt Acc: 97.258%
I - num batch: 160
I - Train -- Loss: 0.469 | Acc: 97.173% | Wgt Acc: 97.134% | LR: 1.250000e-04 | Dur: 85.52s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   2.   5.  29.]
 [ 10. 574.   0.   1.]
 [  8.   1. 728.   2.]
 [ 12.   1.   1. 506.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.120 | Acc: 56.269% | Wgt Acc: 55.163% | Dur: 10.03s
I - Confusion Matrix: [row->prediction - col->label]
[[78. 14. 10. 37.]
 [ 1. 29. 13.  1.]
 [ 2. 21. 30.  1.]
 [ 7. 14. 22. 47.]]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 0.475 | Acc: 95.625% | Wgt Acc: 95.594%
	I - Batch: 100 | Loss: 0.469 | Acc: 96.750% | Wgt Acc: 96.734%
	I - Batch: 150 | Loss: 0.467 | Acc: 97.000% | Wgt Acc: 97.023%
I - num batch: 160
I - Train -- Loss: 0.465 | Acc: 97.173% | Wgt Acc: 97.196% | LR: 1.250000e-04 | Dur: 88.56s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   1.  10.  21.]
 [  8. 574.   2.   1.]
 [ 10.   3. 722.   3.]
 [ 13.   0.   0. 513.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.093 | Acc: 55.352% | Wgt Acc: 55.163% | Dur: 10.17s
I - Confusion Matrix: [row->prediction - col->label]
[[65. 11.  8. 27.]
 [ 3. 31. 12.  1.]
 [ 0. 16. 28.  1.]
 [20. 20. 27. 57.]]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 0.459 | Acc: 97.500% | Wgt Acc: 97.494%
	I - Batch: 100 | Loss: 0.455 | Acc: 97.625% | Wgt Acc: 97.594%
	I - Batch: 150 | Loss: 0.452 | Acc: 97.708% | Wgt Acc: 97.681%
I - num batch: 160
I - Train -- Loss: 0.452 | Acc: 97.801% | Wgt Acc: 97.771% | LR: 1.250000e-04 | Dur: 88.91s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   5.  22.]
 [  6. 576.   0.   1.]
 [  8.   2. 729.   3.]
 [  9.   0.   0. 512.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.041 | Acc: 62.080% | Wgt Acc: 61.209% | Dur: 10.33s
I - Confusion Matrix: [row->prediction - col->label]
[[77. 11. 11. 33.]
 [ 4. 43. 16.  4.]
 [ 1. 18. 37.  3.]
 [ 6.  6. 11. 46.]]

I - Local maximum validation set accuracy:  62.08

I - Validation set results: 
[14-1-1--0.34][50-3-0--0.20][124-2-1--0.44][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.10][615-0-3-0.74][695-1-0-0.69][722-3-3-0.63]
[826-0-0-0.96][878-0-0-0.99][1103-0-0-0.50][1212-3-3-0.52][1368-0-0-0.88][2181-2-0-0.17][2476-2-2-0.56][2721-2-2-0.87][2818-1-3--0.02][2886-2-1-0.97]
[3231-2-2-0.99][3333-2-1-0.99][3482-2-2-0.53][3536-3-3--0.06][3625-1-1--0.41][3909-0-0-0.10][4035-0-0-0.99][4140-0-0-0.66][4214-1-3-0.71][4346-1-0-0.31]
[4581-2-2--0.15][4708-3-3-0.79][4838-3-3--0.59][4845-1-1-0.62][4868-0-0-0.96][4939-0-0--0.23][4984-2-2-0.99][5078-1-2-0.64][5396-0-0-0.99][5479-1-2-0.89]
[5717-0-0-0.99][5843-1-3-0.31][5949-3-0-0.42][5987-2-2-0.24][6014-3-3-0.38][6033-3-0-0.49][6313-0-0-0.37][6421-3-3--0.11][6500-1-1--0.38][6583-3-3-0.51]
[6683-3-3-0.78][6825-2-1-0.26][6998-3-0--0.37][7049-3-3-0.80][7517-1-1-0.24][7521-1-1--0.19][7528-1-0--0.27][7949-1-1--0.12][8135-1-0-0.32][8185-3-0-0.83]
[8269-3-1--0.18][8273-3-3-0.55][8543-3-0-0.99][8666-1-2--0.09][8672-0-0-0.87][8903-1-1-0.32][9001-2-2-0.99][9036-2-2-0.93][9281-3-1--0.30][9300-2-2-0.99]
[9571-0-0-0.21][9617-1-1--0.33][9644-2-1-0.66][9705-2-0-0.18][9801-0-3-0.77][9803-3-0-0.23][9865-3-3-0.88][9896-2-2-0.42][10314-1-0-0.56][10337-3-3-0.99]
[10403-0-0--0.21][10653-2-1-0.22][10704-2-1-0.15][10719-1-1-0.83][10727-1-2-0.14][10836-0-0-0.99][10969-2-3-0.92][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-1--0.67][11499-0-0-0.94][11502-3-0-0.17][11512-3-3-0.99][11608-1-1-0.99][11610-0-0-0.61][11692-0-0-0.35][11905-0-0-0.99][11993-1-1-0.80][12002-2-3--0.52]
[12052-0-0-0.99][12201-0-0-0.76][12235-2-2-0.44][12320-1-0-0.72][12377-2-2--0.40][12398-2-0--0.61][12503-1-1-0.99][12617-0-1-0.10][12685-3-2-0.19][12738-2-0-0.68]
[12742-2-2-0.99][12823-0-0-0.69][13110-1-2-0.09][13240-3-3-0.25][13253-1-1-0.99][13273-0-0-0.99][13634-1-3-0.65][13763-2-2-0.25][13905-3-0-0.46][14060-2-1--0.22]
[14065-3-0-0.63][14147-3-3-0.26][14595-2-0--0.56][14687-2-3-0.89][14788-2-2-0.99][14869-1-2-0.14][14872-3-0-0.03][14877-1-1-0.99][14927-0-0-0.16][15066-0-0-0.97]
[15175-1-1-0.99][15178-2-3-0.75][15375-3-0-0.93][15389-3-3-0.99][15568-2-1-0.95][15675-3-3-0.99][15869-1-1-0.34][16207-3-0-0.29][16236-0-0--0.20][16302-3-2--0.09]
[16331-2-2-0.99][16381-0-0-0.01][16488-1-1--0.38][16495-0-0-0.99][16650-0-0-0.99][16719-1-1-0.77][16801-0-0-0.99][16828-0-0-0.96][17137-3-0-0.22][17245-1-1--0.20]
[17278-3-0-0.18][17282-0-0-0.81][17311-2-2--0.33][17336-2-3-0.01][17608-3-3-0.99][17627-0-0-0.18][17877-3-0-0.74][17924-1-3-0.02][17984-3-0-0.97][18211-0-1--0.14]
[18276-3-3-0.85][18287-1-1-0.38][18394-0-0-0.99][18428-0-0-0.99][18442-0-3-0.86][18478-3-3-0.76][18607-0-0-0.87][18616-0-0-0.73][18663-0-0-0.54][18718-0-0-0.99]
[18766-2-2--0.13][18824-2-2-0.46][18890-3-3--0.14][18930-3-0-0.97][18938-3-3-0.99][19817-1-2-0.73][19839-0-0-0.92][19930-3-0-0.62][19944-0-1-0.71][20036-2-2-0.99]
[20101-3-0-0.03][20474-1-2-0.89][20547-3-0--0.03][20929-2-2-0.67][21245-1-2-0.28][21257-3-2--0.65][21293-1-2-0.53][21316-1-1-0.99][21384-1-2-0.05][21448-1-1-0.12]
[21483-0-0-0.99][21487-2-2-0.95][21714-0-0-0.04][21943-3-3-0.03][21947-0-0-0.93][21948-0-0-0.99][21965-2-1-0.79][21998-1-1-0.93][22025-0-2--0.44][22228-3-3-0.97]
[22446-1-1-0.99][22494-3-0-0.64][22757-0-0-0.95][22811-3-3-0.99][22976-3-1-0.47][22985-3-3-0.44][23014-0-0-0.99][23112-1-2-0.40][23144-3-0-0.23][23168-2-0--0.08]
[23219-0-0-0.31][23363-3-3-0.07][23470-0-0-0.50][23486-2-2--0.71][23497-0-3-0.99][23516-0-0-0.96][23690-1-1-0.05][23921-2-2-0.56][23936-1-2-0.97][24040-3-0-0.93]
[24111-1-1-0.35][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.46][24345-0-0-0.97][24364-1-2-0.51][24427-3-0-0.99][24477-2-2-0.96][24495-2-1-0.42][24893-2-1--0.07]
[25012-1-1-0.23][25121-2-1-0.62][25165-3-3-0.98][25183-0-0-0.69][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.78][25644-1-1-0.99][25718-1-2-0.05][25774-2-3-0.00]
[26032-3-3-0.83][26051-3-3-0.99][26120-0-0-0.74][26321-1-1-0.79][26732-1-1-0.89][26784-3-3-0.97][26827-3-3-0.10][26833-0-3-0.99][26838-2-2--0.47][26860-1-0-0.27]
[26948-0-0-0.30][27049-3-0-0.99][27098-1-0-0.08][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.78][27772-0-0-0.91][27890-1-1-0.76][28040-0-0-0.50][28503-2-2-0.99]
[28577-1-1-0.96][28959-0-0-0.99][29198-3-3-0.45][29777-0-0-0.99][29877-2-1-0.05][30035-1-1-0.83][30098-0-0-0.69][30326-1-1-0.99][30572-2-3-0.88][30716-0-0-0.16]
[30806-2-3-0.35][30906-1-1-0.92][31007-0-0-0.89][31181-3-3-0.96][31238-0-0-0.04][31347-0-3-0.99][31422-2-0-0.08][31429-3-0--0.36][31431-0-0-0.70][31432-1-1-0.90]
[31477-0-0-0.99][31524-1-0--0.16][31597-1-1-0.19][31619-1-0--0.14][31701-0-0-0.99][31755-0-0-0.99][31854-3-0-0.39][32074-1-3--0.27][32078-3-3-0.97][32111-1-1-0.90]
[32127-1-1-0.99][32140-3-3-0.11][32263-2-0--0.40][32365-0-0-0.95][32411-2-0-0.95][32429-3-0-0.84][32473-3-0-0.95][32574-3-0-0.79][32584-0-0-0.17][32622-0-1-0.62]
[32858-3-0-0.99][32969-3-3-0.45][33016-2-2-0.60][33031-1-0-0.07][33035-2-2-0.53][33133-2-2-0.82][33173-2-3--0.38][33175-3-3-0.10][33306-3-1-0.99][33309-2-3--0.42]
[33474-0-0-0.15][33478-2-0--0.55][33618-1-1-0.53][33712-0-0-0.95][33782-2-2--0.11][33914-3-3-0.52][34076-3-3-0.83][34112-2-2-0.14][34138-2-3-0.99][34239-1-2--0.24]
[34364-2-1--0.01][34617-1-2-0.84][34751-3-0--0.50][34783-2-2-0.04][35015-3-3-0.99][35018-1-2-0.13][35288-2-2--0.07]
---------------------------
I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 0.443 | Acc: 98.000% | Wgt Acc: 97.965%
	I - Batch: 100 | Loss: 0.445 | Acc: 97.688% | Wgt Acc: 97.672%
	I - Batch: 150 | Loss: 0.445 | Acc: 97.708% | Wgt Acc: 97.688%
I - num batch: 160
I - Train -- Loss: 0.446 | Acc: 97.723% | Wgt Acc: 97.700% | LR: 1.250000e-04 | Dur: 89.39s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   1.   6.  24.]
 [  5. 577.   2.   0.]
 [  8.   0. 725.   3.]
 [  8.   0.   1. 511.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.061 | Acc: 59.633% | Wgt Acc: 58.764% | Dur: 10.05s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  8.  9. 30.]
 [ 3. 34. 12.  2.]
 [ 2. 28. 35.  3.]
 [ 8.  8. 19. 51.]]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 0.450 | Acc: 96.875% | Wgt Acc: 96.864%
	I - Batch: 100 | Loss: 0.442 | Acc: 97.312% | Wgt Acc: 97.279%
	I - Batch: 150 | Loss: 0.439 | Acc: 97.750% | Wgt Acc: 97.736%
I - num batch: 160
I - Train -- Loss: 0.440 | Acc: 97.605% | Wgt Acc: 97.611% | LR: 1.250000e-04 | Dur: 84.55s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   1.   7.  19.]
 [  8. 577.   0.   2.]
 [ 10.   0. 726.   4.]
 [  9.   0.   1. 513.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.031 | Acc: 61.468% | Wgt Acc: 60.598% | Dur: 9.76s
I - Confusion Matrix: [row->prediction - col->label]
[[73. 10.  6. 30.]
 [ 5. 41. 15.  3.]
 [ 2. 20. 40.  6.]
 [ 8.  7. 14. 47.]]

I - Epoch: 126
I - Training: 
	I - Batch: 50 | Loss: 0.435 | Acc: 98.750% | Wgt Acc: 98.699%
	I - Batch: 100 | Loss: 0.433 | Acc: 98.562% | Wgt Acc: 98.505%
	I - Batch: 150 | Loss: 0.436 | Acc: 98.250% | Wgt Acc: 98.225%
I - num batch: 160
I - Train -- Loss: 0.437 | Acc: 98.233% | Wgt Acc: 98.204% | LR: 1.250000e-04 | Dur: 85.05s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   1.   5.  16.]
 [  4. 575.   0.   2.]
 [  7.   2. 728.   2.]
 [  5.   0.   1. 518.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.013 | Acc: 64.220% | Wgt Acc: 63.519% | Dur: 10.41s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  2.  6. 17.]
 [ 6. 44. 13.  6.]
 [ 4. 28. 50. 12.]
 [13.  4.  6. 51.]]

I - Local maximum validation set accuracy:  64.22

I - Validation set results: 
[14-1-2-0.23][50-3-1--0.71][124-2-2-0.81][127-0-0-0.99][443-2-2-0.99][567-0-0-0.63][573-1-1-0.31][615-0-3-0.68][695-1-2-0.73][722-3-3-0.78]
[826-0-0-0.99][878-0-3-0.82][1103-0-0--0.08][1212-3-3-0.15][1368-0-0-0.88][2181-2-2-0.16][2476-2-2-0.74][2721-2-2-0.99][2818-1-1--0.23][2886-2-1-0.87]
[3231-2-2-0.99][3333-2-1-0.99][3482-2-2-0.98][3536-3-3--0.54][3625-1-1-0.19][3909-0-0-0.08][4035-0-0-0.15][4140-0-0-0.92][4214-1-3-0.80][4346-1-0--0.30]
[4581-2-2-0.98][4708-3-3--0.05][4838-3-3-0.14][4845-1-1--0.03][4868-0-0-0.84][4939-0-0--0.39][4984-2-2-0.99][5078-1-2-0.93][5396-0-0-0.99][5479-1-2-0.57]
[5717-0-0-0.69][5843-1-2-0.83][5949-3-3-0.29][5987-2-2-0.70][6014-3-1--0.04][6033-3-0-0.06][6313-0-0--0.10][6421-3-3--0.36][6500-1-1-0.32][6583-3-2-0.94]
[6683-3-3-0.23][6825-2-1-0.88][6998-3-2--0.42][7049-3-3-0.93][7517-1-1-0.06][7521-1-1--0.29][7528-1-3-0.16][7949-1-2-0.84][8135-1-0-0.09][8185-3-0-0.42]
[8269-3-2-0.10][8273-3-3--0.01][8543-3-0-0.99][8666-1-1-0.11][8672-0-3--0.10][8903-1-2-0.83][9001-2-2-0.38][9036-2-2-0.99][9281-3-1-0.32][9300-2-2-0.99]
[9571-0-3--0.25][9617-1-1-0.36][9644-2-2-0.49][9705-2-2-0.97][9801-0-3-0.99][9803-3-3-0.10][9865-3-3-0.96][9896-2-2-0.99][10314-1-1-0.36][10337-3-3-0.99]
[10403-0-2-0.45][10653-2-1-0.28][10704-2-2--0.46][10719-1-1-0.99][10727-1-2-0.22][10836-0-0-0.99][10969-2-1-0.33][11042-0-0-0.80][11088-1-1-0.99][11322-0-0-0.58]
[11398-2-2--0.11][11499-0-0-0.01][11502-3-2-0.33][11512-3-3-0.99][11608-1-1-0.59][11610-0-0--0.18][11692-0-0--0.21][11905-0-0-0.99][11993-1-1-0.99][12002-2-0-0.97]
[12052-0-0-0.25][12201-0-3-0.80][12235-2-1-0.61][12320-1-2-0.19][12377-2-2--0.01][12398-2-2--0.58][12503-1-1-0.48][12617-0-2--0.55][12685-3-2-0.04][12738-2-1--0.68]
[12742-2-2-0.99][12823-0-3-0.93][13110-1-1--0.00][13240-3-3-0.10][13253-1-1-0.99][13273-0-0-0.99][13634-1-3--0.40][13763-2-2-0.99][13905-3-2--0.21][14060-2-1-0.15]
[14065-3-2-0.29][14147-3-3-0.34][14595-2-1-0.57][14687-2-2-0.98][14788-2-2-0.79][14869-1-2-0.49][14872-3-0--0.13][14877-1-1-0.99][14927-0-0--0.38][15066-0-0-0.84]
[15175-1-1-0.84][15178-2-3-0.82][15375-3-1--0.60][15389-3-3-0.99][15568-2-2-0.59][15675-3-3-0.99][15869-1-2--0.18][16207-3-0--0.04][16236-0-0--0.25][16302-3-3-0.19]
[16331-2-2-0.99][16381-0-0--0.51][16488-1-1-0.55][16495-0-0-0.51][16650-0-0-0.99][16719-1-1-0.94][16801-0-0-0.99][16828-0-0-0.55][17137-3-3-0.42][17245-1-1--0.22]
[17278-3-1--0.22][17282-0-0-0.45][17311-2-2-0.96][17336-2-2-0.40][17608-3-3-0.99][17627-0-1--0.06][17877-3-0--0.75][17924-1-3-0.46][17984-3-0-0.72][18211-0-1-0.22]
[18276-3-0-0.40][18287-1-1-0.41][18394-0-0-0.79][18428-0-0-0.10][18442-0-3-0.57][18478-3-3-0.90][18607-0-0-0.70][18616-0-0-0.78][18663-0-0--0.00][18718-0-0-0.90]
[18766-2-2-0.98][18824-2-1-0.20][18890-3-3-0.36][18930-3-2-0.30][18938-3-3-0.96][19817-1-2-0.93][19839-0-2-0.95][19930-3-0--0.12][19944-0-0-0.14][20036-2-2-0.99]
[20101-3-3-0.59][20474-1-2-0.92][20547-3-0--0.35][20929-2-2-0.99][21245-1-2-0.87][21257-3-2--0.01][21293-1-2-0.99][21316-1-1-0.99][21384-1-2-0.99][21448-1-1-0.34]
[21483-0-0-0.67][21487-2-2-0.99][21714-0-0--0.62][21943-3-3-0.37][21947-0-0-0.06][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.95][22025-0-1--0.43][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-0--0.12][22757-0-0-0.19][22811-3-3-0.41][22976-3-2-0.20][22985-3-3-0.95][23014-0-3-0.99][23112-1-2-0.55][23144-3-3-0.33][23168-2-0-0.09]
[23219-0-0-0.22][23363-3-3-0.18][23470-0-0--0.36][23486-2-2-0.07][23497-0-3-0.99][23516-0-0-0.99][23690-1-1-0.41][23921-2-2-0.86][23936-1-2-0.65][24040-3-2-0.57]
[24111-1-1-0.34][24182-0-0-0.70][24238-3-3-0.66][24290-2-0-0.42][24345-0-0-0.76][24364-1-2-0.89][24427-3-0-0.56][24477-2-2-0.99][24495-2-1-0.88][24893-2-1-0.42]
[25012-1-2--0.06][25121-2-0-0.31][25165-3-3-0.99][25183-0-0-0.76][25297-3-3-0.99][25398-0-0-0.51][25574-2-2-0.04][25644-1-1-0.33][25718-1-2--0.17][25774-2-2-0.98]
[26032-3-3-0.99][26051-3-3-0.84][26120-0-0-0.41][26321-1-1-0.48][26732-1-1-0.45][26784-3-3-0.96][26827-3-3-0.70][26833-0-3-0.99][26838-2-2-0.10][26860-1-2-0.14]
[26948-0-0--0.41][27049-3-0-0.94][27098-1-1-0.31][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.82][27772-0-0-0.17][27890-1-1-0.64][28040-0-0-0.12][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-0--0.33][29777-0-0-0.99][29877-2-2-0.52][30035-1-1-0.94][30098-0-0-0.09][30326-1-1-0.99][30572-2-3-0.81][30716-0-1--0.02]
[30806-2-2-0.48][30906-1-2-0.74][31007-0-0-0.41][31181-3-3--0.33][31238-0-3--0.29][31347-0-3-0.99][31422-2-0--0.60][31429-3-3--0.14][31431-0-1-0.78][31432-1-1-0.98]
[31477-0-0-0.92][31524-1-2-0.32][31597-1-1-0.49][31619-1-2-0.99][31701-0-0-0.94][31755-0-0-0.43][31854-3-3--0.36][32074-1-2-0.22][32078-3-3-0.84][32111-1-1-0.12]
[32127-1-1-0.53][32140-3-3-0.36][32263-2-0--0.68][32365-0-0-0.93][32411-2-3-0.90][32429-3-0-0.39][32473-3-0-0.63][32574-3-3-0.96][32584-0-2-0.45][32622-0-1-0.54]
[32858-3-0-0.25][32969-3-3-0.77][33016-2-2-0.79][33031-1-1--0.00][33035-2-2-0.89][33133-2-2-0.99][33173-2-1-0.29][33175-3-2-0.34][33306-3-3-0.77][33309-2-3--0.03]
[33474-0-0-0.36][33478-2-3--0.47][33618-1-1-0.92][33712-0-0-0.67][33782-2-2--0.06][33914-3-3-0.34][34076-3-3-0.10][34112-2-2-0.99][34138-2-3-0.90][34239-1-1-0.10]
[34364-2-2-0.80][34617-1-2-0.66][34751-3-1--0.63][34783-2-2-0.86][35015-3-3-0.10][35018-1-2-0.32][35288-2-2-0.32]
---------------------------
I - Epoch: 127
I - Training: 
	I - Batch: 50 | Loss: 0.431 | Acc: 98.250% | Wgt Acc: 98.172%
	I - Batch: 100 | Loss: 0.433 | Acc: 98.250% | Wgt Acc: 98.225%
	I - Batch: 150 | Loss: 0.433 | Acc: 98.167% | Wgt Acc: 98.132%
I - num batch: 160
I - Train -- Loss: 0.433 | Acc: 98.194% | Wgt Acc: 98.169% | LR: 1.250000e-04 | Dur: 85.81s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   4.  19.]
 [  7. 578.   0.   1.]
 [  2.   0. 730.   3.]
 [ 10.   0.   0. 515.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.044 | Acc: 61.468% | Wgt Acc: 60.938% | Dur: 10.02s
I - Confusion Matrix: [row->prediction - col->label]
[[75. 13. 12. 27.]
 [ 4. 37. 12.  2.]
 [ 1. 20. 33.  1.]
 [ 8.  8. 18. 56.]]

I - Epoch: 128
I - Training: 
	I - Batch: 50 | Loss: 0.442 | Acc: 97.750% | Wgt Acc: 97.740%
	I - Batch: 100 | Loss: 0.442 | Acc: 97.938% | Wgt Acc: 97.901%
	I - Batch: 150 | Loss: 0.435 | Acc: 98.250% | Wgt Acc: 98.215%
I - num batch: 160
I - Train -- Loss: 0.433 | Acc: 98.351% | Wgt Acc: 98.319% | LR: 1.250000e-04 | Dur: 84.29s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   1.   4.  17.]
 [  1. 577.   0.   0.]
 [  5.   0. 729.   4.]
 [  9.   0.   1. 517.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.040 | Acc: 59.021% | Wgt Acc: 58.560% | Dur: 10.14s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  3.  4. 16.]
 [ 9. 44. 19.  9.]
 [ 7. 25. 45. 15.]
 [14.  6.  7. 46.]]

I - Epoch: 129
I - Training: 
	I - Batch: 50 | Loss: 0.427 | Acc: 98.750% | Wgt Acc: 98.707%
	I - Batch: 100 | Loss: 0.427 | Acc: 98.688% | Wgt Acc: 98.665%
	I - Batch: 150 | Loss: 0.431 | Acc: 98.167% | Wgt Acc: 98.142%
I - num batch: 160
I - Train -- Loss: 0.429 | Acc: 98.233% | Wgt Acc: 98.213% | LR: 1.250000e-04 | Dur: 88.56s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   1.   3.  16.]
 [  5. 575.   1.   0.]
 [  7.   2. 729.   3.]
 [  6.   0.   1. 519.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.071 | Acc: 59.021% | Wgt Acc: 58.084% | Dur: 9.79s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  7.  9. 30.]
 [ 5. 36. 19.  4.]
 [ 3. 29. 37.  5.]
 [ 7.  6. 10. 47.]]

I - Epoch: 130
I - Training: 
	I - Batch: 50 | Loss: 0.422 | Acc: 98.125% | Wgt Acc: 98.127%
	I - Batch: 100 | Loss: 0.426 | Acc: 98.000% | Wgt Acc: 97.984%
	I - Batch: 150 | Loss: 0.425 | Acc: 98.042% | Wgt Acc: 98.028%
I - num batch: 160
I - Train -- Loss: 0.425 | Acc: 98.076% | Wgt Acc: 98.063% | LR: 1.250000e-04 | Dur: 86.58s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   6.  16.]
 [  3. 577.   2.   5.]
 [  4.   1. 725.   1.]
 [ 10.   0.   1. 516.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.017 | Acc: 61.162% | Wgt Acc: 60.870% | Dur: 9.94s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  8. 28.]
 [ 6. 42. 16.  3.]
 [ 1. 21. 35.  1.]
 [12. 10. 16. 54.]]

I - Epoch: 131
I - Training: 
	I - Batch: 50 | Loss: 0.414 | Acc: 98.500% | Wgt Acc: 98.538%
	I - Batch: 100 | Loss: 0.421 | Acc: 98.125% | Wgt Acc: 98.138%
	I - Batch: 150 | Loss: 0.425 | Acc: 98.042% | Wgt Acc: 98.085%
I - num batch: 160
I - Train -- Loss: 0.425 | Acc: 98.115% | Wgt Acc: 98.151% | LR: 1.250000e-04 | Dur: 85.69s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   8.  14.]
 [  4. 578.   2.   2.]
 [ 10.   0. 724.   1.]
 [  7.   0.   0. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.105 | Acc: 58.410% | Wgt Acc: 57.201% | Dur: 10.09s
I - Confusion Matrix: [row->prediction - col->label]
[[81. 21. 16. 40.]
 [ 1. 35. 12.  2.]
 [ 3. 16. 32.  1.]
 [ 3.  6. 15. 43.]]

I - Epoch: 132
I - Training: 
	I - Batch: 50 | Loss: 0.420 | Acc: 99.000% | Wgt Acc: 98.932%
	I - Batch: 100 | Loss: 0.428 | Acc: 98.000% | Wgt Acc: 97.943%
	I - Batch: 150 | Loss: 0.424 | Acc: 98.167% | Wgt Acc: 98.122%
I - num batch: 160
I - Train -- Loss: 0.423 | Acc: 98.155% | Wgt Acc: 98.116% | LR: 1.250000e-04 | Dur: 86.01s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   2.   5.  17.]
 [  5. 576.   0.   1.]
 [  5.   0. 728.   5.]
 [  6.   0.   1. 515.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.042 | Acc: 60.245% | Wgt Acc: 59.307% | Dur: 9.91s
I - Confusion Matrix: [row->prediction - col->label]
[[76. 13. 10. 31.]
 [ 4. 35. 16.  2.]
 [ 2. 23. 36.  3.]
 [ 6.  7. 13. 50.]]

I - Epoch: 133
I - Training: 
	I - Batch: 50 | Loss: 0.419 | Acc: 98.625% | Wgt Acc: 98.564%
	I - Batch: 100 | Loss: 0.420 | Acc: 98.750% | Wgt Acc: 98.718%
	I - Batch: 150 | Loss: 0.418 | Acc: 98.583% | Wgt Acc: 98.564%
I - num batch: 160
I - Train -- Loss: 0.420 | Acc: 98.547% | Wgt Acc: 98.523% | LR: 1.250000e-04 | Dur: 91.26s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   1.   5.  13.]
 [  5. 575.   0.   1.]
 [  4.   1. 728.   2.]
 [  3.   1.   1. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.081 | Acc: 57.187% | Wgt Acc: 56.658% | Dur: 9.68s
I - Confusion Matrix: [row->prediction - col->label]
[[70. 11.  6. 28.]
 [ 1. 31. 14.  2.]
 [ 1. 17. 31.  1.]
 [16. 19. 24. 55.]]

I - Epoch: 134
I - Training: 
	I - Batch: 50 | Loss: 0.464 | Acc: 96.625% | Wgt Acc: 96.638%
	I - Batch: 100 | Loss: 0.444 | Acc: 97.688% | Wgt Acc: 97.690%
	I - Batch: 150 | Loss: 0.439 | Acc: 97.833% | Wgt Acc: 97.832%
I - num batch: 160
I - Train -- Loss: 0.438 | Acc: 97.919% | Wgt Acc: 97.912% | LR: 1.250000e-04 | Dur: 88.91s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   1.   4.  19.]
 [  5. 576.   2.   2.]
 [  6.   1. 728.   1.]
 [ 12.   0.   0. 516.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.009 | Acc: 63.609% | Wgt Acc: 62.976% | Dur: 10.16s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  8. 27.]
 [ 5. 38.  7.  1.]
 [ 1. 20. 43.  1.]
 [12. 13. 17. 57.]]

I - Epoch: 135
I - Training: 
	I - Batch: 50 | Loss: 0.425 | Acc: 97.875% | Wgt Acc: 97.830%
	I - Batch: 100 | Loss: 0.424 | Acc: 97.938% | Wgt Acc: 97.886%
	I - Batch: 150 | Loss: 0.421 | Acc: 98.083% | Wgt Acc: 98.094%
I - num batch: 160
I - Train -- Loss: 0.422 | Acc: 98.037% | Wgt Acc: 98.036% | LR: 1.250000e-04 | Dur: 86.40s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   8.  15.]
 [  8. 575.   0.   0.]
 [  7.   3. 726.   4.]
 [  5.   0.   0. 519.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.037 | Acc: 59.633% | Wgt Acc: 58.492% | Dur: 10.16s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  5. 25.]
 [ 5. 40. 15.  6.]
 [ 7. 27. 49. 14.]
 [11.  5.  6. 41.]]

I - Epoch: 136
I - Training: 
	I - Batch: 50 | Loss: 0.427 | Acc: 97.125% | Wgt Acc: 97.135%
	I - Batch: 100 | Loss: 0.420 | Acc: 98.000% | Wgt Acc: 97.978%
	I - Batch: 150 | Loss: 0.418 | Acc: 98.083% | Wgt Acc: 98.068%
I - num batch: 160
I - Train -- Loss: 0.418 | Acc: 98.155% | Wgt Acc: 98.133% | LR: 1.250000e-04 | Dur: 85.45s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   7.  20.]
 [  4. 577.   0.   0.]
 [  6.   1. 727.   2.]
 [  7.   0.   0. 516.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.038 | Acc: 61.162% | Wgt Acc: 60.054% | Dur: 9.65s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  8.  4. 20.]
 [ 6. 38. 14.  5.]
 [ 4. 27. 47. 15.]
 [ 9.  5. 10. 46.]]

I - Epoch: 137
I - Training: 
	I - Batch: 50 | Loss: 0.408 | Acc: 98.500% | Wgt Acc: 98.483%
	I - Batch: 100 | Loss: 0.412 | Acc: 98.688% | Wgt Acc: 98.646%
	I - Batch: 150 | Loss: 0.411 | Acc: 98.750% | Wgt Acc: 98.705%
I - num batch: 160
I - Train -- Loss: 0.413 | Acc: 98.587% | Wgt Acc: 98.549% | LR: 1.250000e-04 | Dur: 86.40s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   2.   5.  12.]
 [  5. 575.   0.   3.]
 [  2.   1. 728.   2.]
 [  3.   0.   1. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.054 | Acc: 61.774% | Wgt Acc: 60.530% | Dur: 9.57s
I - Confusion Matrix: [row->prediction - col->label]
[[80. 14. 12. 36.]
 [ 3. 42. 14.  5.]
 [ 1. 19. 39.  4.]
 [ 4.  3. 10. 41.]]

I - Epoch: 138
I - Training: 
	I - Batch: 50 | Loss: 0.410 | Acc: 98.250% | Wgt Acc: 98.282%
	I - Batch: 100 | Loss: 0.408 | Acc: 98.750% | Wgt Acc: 98.759%
	I - Batch: 150 | Loss: 0.413 | Acc: 98.458% | Wgt Acc: 98.423%
I - num batch: 160
I - Train -- Loss: 0.413 | Acc: 98.508% | Wgt Acc: 98.478% | LR: 1.250000e-04 | Dur: 84.42s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   1.   4.  11.]
 [  6. 575.   0.   3.]
 [  3.   2. 729.   3.]
 [  4.   0.   1. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.016 | Acc: 63.914% | Wgt Acc: 63.315% | Dur: 10.07s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  5.  8. 24.]
 [ 5. 42. 10.  3.]
 [ 1. 24. 41.  5.]
 [10.  7. 16. 54.]]

I - Epoch: 139
I - Training: 
	I - Batch: 50 | Loss: 0.409 | Acc: 98.125% | Wgt Acc: 98.095%
	I - Batch: 100 | Loss: 0.407 | Acc: 98.750% | Wgt Acc: 98.735%
	I - Batch: 150 | Loss: 0.411 | Acc: 98.667% | Wgt Acc: 98.648%
I - num batch: 160
I - Train -- Loss: 0.412 | Acc: 98.704% | Wgt Acc: 98.682% | LR: 1.250000e-04 | Dur: 86.76s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   4.  14.]
 [  3. 578.   1.   2.]
 [  4.   0. 729.   1.]
 [  4.   0.   0. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.054 | Acc: 59.021% | Wgt Acc: 58.560% | Dur: 10.38s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  8.  7. 24.]
 [ 4. 32. 11.  1.]
 [ 1. 26. 32.  3.]
 [12. 12. 25. 58.]]

I - Epoch: 140
I - Training: 
	I - Batch: 50 | Loss: 0.422 | Acc: 97.875% | Wgt Acc: 97.936%
	I - Batch: 100 | Loss: 0.415 | Acc: 98.312% | Wgt Acc: 98.321%
	I - Batch: 150 | Loss: 0.411 | Acc: 98.458% | Wgt Acc: 98.479%
I - num batch: 160
I - Train -- Loss: 0.410 | Acc: 98.547% | Wgt Acc: 98.567% | LR: 1.250000e-04 | Dur: 87.12s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   3.  11.]
 [  3. 577.   0.   1.]
 [ 10.   1. 730.   1.]
 [  6.   0.   1. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.024 | Acc: 61.162% | Wgt Acc: 60.666% | Dur: 10.02s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  6.  3.  9.]
 [ 8. 37. 15.  5.]
 [ 7. 29. 47. 16.]
 [13.  6. 10. 56.]]

I - Epoch: 141
I - Training: 
	I - Batch: 50 | Loss: 0.403 | Acc: 98.625% | Wgt Acc: 98.597%
	I - Batch: 100 | Loss: 0.404 | Acc: 98.750% | Wgt Acc: 98.720%
	I - Batch: 150 | Loss: 0.408 | Acc: 98.792% | Wgt Acc: 98.761%
I - num batch: 160
I - Train -- Loss: 0.407 | Acc: 98.822% | Wgt Acc: 98.797% | LR: 1.250000e-04 | Dur: 85.31s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   2.   4.  10.]
 [  4. 575.   0.   1.]
 [  1.   1. 728.   2.]
 [  3.   0.   2. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.010 | Acc: 62.385% | Wgt Acc: 62.092% | Dur: 10.19s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  3.  5. 20.]
 [ 9. 42. 11.  3.]
 [ 5. 26. 46.  7.]
 [14.  7. 13. 56.]]

I - Epoch: 142
I - Training: 
	I - Batch: 50 | Loss: 0.406 | Acc: 98.875% | Wgt Acc: 98.900%
	I - Batch: 100 | Loss: 0.406 | Acc: 98.938% | Wgt Acc: 98.941%
	I - Batch: 150 | Loss: 0.406 | Acc: 98.750% | Wgt Acc: 98.742%
I - num batch: 160
I - Train -- Loss: 0.406 | Acc: 98.822% | Wgt Acc: 98.815% | LR: 1.250000e-04 | Dur: 89.88s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   5.   8.]
 [  5. 578.   0.   1.]
 [  0.   0. 728.   5.]
 [  5.   0.   1. 524.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.028 | Acc: 62.385% | Wgt Acc: 61.685% | Dur: 10.05s
I - Confusion Matrix: [row->prediction - col->label]
[[76. 13.  7. 26.]
 [ 3. 35. 13.  1.]
 [ 0. 22. 36.  2.]
 [ 9.  8. 19. 57.]]

I - Epoch: 143
I - Training: 
	I - Batch: 50 | Loss: 0.402 | Acc: 99.000% | Wgt Acc: 98.962%
	I - Batch: 100 | Loss: 0.405 | Acc: 99.000% | Wgt Acc: 98.971%
	I - Batch: 150 | Loss: 0.403 | Acc: 98.875% | Wgt Acc: 98.864%
I - num batch: 160
I - Train -- Loss: 0.404 | Acc: 98.822% | Wgt Acc: 98.815% | LR: 1.250000e-04 | Dur: 88.97s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   4.   9.]
 [  3. 577.   0.   3.]
 [  4.   1. 730.   1.]
 [  5.   0.   0. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.058 | Acc: 59.939% | Wgt Acc: 59.579% | Dur: 9.91s
I - Confusion Matrix: [row->prediction - col->label]
[[79. 11. 14. 34.]
 [ 2. 44. 12.  2.]
 [ 1. 13. 24.  1.]
 [ 6. 10. 25. 49.]]

I - Epoch: 144
I - Training: 
	I - Batch: 50 | Loss: 0.418 | Acc: 98.125% | Wgt Acc: 98.056%
	I - Batch: 100 | Loss: 0.411 | Acc: 98.688% | Wgt Acc: 98.635%
	I - Batch: 150 | Loss: 0.406 | Acc: 98.917% | Wgt Acc: 98.883%
I - num batch: 160
I - Train -- Loss: 0.405 | Acc: 98.901% | Wgt Acc: 98.877% | LR: 1.250000e-04 | Dur: 85.58s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   1.   4.   9.]
 [  3. 577.   1.   1.]
 [  3.   0. 729.   4.]
 [  2.   0.   0. 524.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.007 | Acc: 64.220% | Wgt Acc: 63.043% | Dur: 10.66s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7.  8. 29.]
 [ 6. 42. 12.  3.]
 [ 1. 22. 47.  8.]
 [ 6.  7.  8. 46.]]

I - Epoch: 145
I - Training: 
	I - Batch: 50 | Loss: 0.401 | Acc: 98.750% | Wgt Acc: 98.765%
	I - Batch: 100 | Loss: 0.407 | Acc: 98.562% | Wgt Acc: 98.580%
	I - Batch: 150 | Loss: 0.405 | Acc: 98.792% | Wgt Acc: 98.798%
I - num batch: 160
I - Train -- Loss: 0.405 | Acc: 98.822% | Wgt Acc: 98.832% | LR: 1.250000e-04 | Dur: 86.68s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   6.   7.]
 [  2. 576.   0.   1.]
 [  5.   2. 728.   2.]
 [  5.   0.   0. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.032 | Acc: 62.691% | Wgt Acc: 61.821% | Dur: 10.06s
I - Confusion Matrix: [row->prediction - col->label]
[[78. 11. 10. 31.]
 [ 4. 38. 10.  1.]
 [ 0. 17. 37.  2.]
 [ 6. 12. 18. 52.]]

I - Epoch: 146
I - Training: 
	I - Batch: 50 | Loss: 0.399 | Acc: 98.750% | Wgt Acc: 98.793%
	I - Batch: 100 | Loss: 0.400 | Acc: 99.000% | Wgt Acc: 99.028%
	I - Batch: 150 | Loss: 0.401 | Acc: 99.000% | Wgt Acc: 99.005%
I - num batch: 160
I - Train -- Loss: 0.401 | Acc: 98.940% | Wgt Acc: 98.947% | LR: 1.250000e-04 | Dur: 89.62s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   3.   8.]
 [  4. 577.   1.   1.]
 [  5.   1. 729.   1.]
 [  2.   0.   1. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.033 | Acc: 62.385% | Wgt Acc: 61.549% | Dur: 10.14s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  6.  6. 25.]
 [ 8. 39. 12.  3.]
 [ 2. 25. 41.  7.]
 [ 5.  8. 16. 51.]]

I - Epoch: 147
I - Training: 
	I - Batch: 50 | Loss: 0.399 | Acc: 99.375% | Wgt Acc: 99.324%
	I - Batch: 100 | Loss: 0.401 | Acc: 99.125% | Wgt Acc: 99.127%
	I - Batch: 150 | Loss: 0.398 | Acc: 99.125% | Wgt Acc: 99.146%
I - num batch: 160
I - Train -- Loss: 0.399 | Acc: 99.058% | Wgt Acc: 99.071% | LR: 1.250000e-04 | Dur: 88.48s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   1.   5.   4.]
 [  3. 576.   0.   1.]
 [  1.   1. 726.   2.]
 [  3.   0.   3. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.032 | Acc: 63.303% | Wgt Acc: 62.772% | Dur: 9.90s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  7.  9. 27.]
 [ 4. 46. 11.  4.]
 [ 5. 19. 39.  5.]
 [ 7.  6. 16. 50.]]

I - Epoch: 148
I - Training: 
	I - Batch: 50 | Loss: 0.404 | Acc: 98.875% | Wgt Acc: 98.897%
	I - Batch: 100 | Loss: 0.402 | Acc: 98.938% | Wgt Acc: 98.898%
	I - Batch: 150 | Loss: 0.402 | Acc: 98.958% | Wgt Acc: 98.940%
I - num batch: 160
I - Train -- Loss: 0.401 | Acc: 98.979% | Wgt Acc: 98.965% | LR: 1.250000e-04 | Dur: 88.53s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   1.   5.   8.]
 [  4. 577.   0.   1.]
 [  3.   0. 729.   3.]
 [  1.   0.   0. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.003 | Acc: 62.080% | Wgt Acc: 61.481% | Dur: 10.36s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  7.  6. 21.]
 [ 6. 38. 11.  2.]
 [ 7. 25. 44.  8.]
 [ 9.  8. 14. 55.]]

I - Epoch: 149
I - Training: 
	I - Batch: 50 | Loss: 0.393 | Acc: 98.875% | Wgt Acc: 98.900%
	I - Batch: 100 | Loss: 0.398 | Acc: 98.562% | Wgt Acc: 98.593%
	I - Batch: 150 | Loss: 0.397 | Acc: 98.833% | Wgt Acc: 98.855%
I - num batch: 160
I - Train -- Loss: 0.398 | Acc: 98.822% | Wgt Acc: 98.841% | LR: 1.250000e-04 | Dur: 86.91s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   3.   8.]
 [  2. 578.   1.   1.]
 [  6.   0. 729.   2.]
 [  6.   0.   1. 527.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.043 | Acc: 61.468% | Wgt Acc: 61.073% | Dur: 10.02s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  8.  4. 27.]
 [ 6. 39. 11.  1.]
 [ 1. 18. 35.  2.]
 [10. 13. 25. 56.]]

I - Epoch: 150
I - Training: 
	I - Batch: 50 | Loss: 0.395 | Acc: 99.000% | Wgt Acc: 99.044%
	I - Batch: 100 | Loss: 0.394 | Acc: 99.000% | Wgt Acc: 99.069%
	I - Batch: 150 | Loss: 0.396 | Acc: 99.000% | Wgt Acc: 99.023%
I - num batch: 160
I - Train -- Loss: 0.395 | Acc: 98.979% | Wgt Acc: 99.000% | LR: 1.250000e-04 | Dur: 91.83s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   1.   7.]
 [  5. 577.   0.   0.]
 [  8.   1. 730.   1.]
 [  0.   0.   3. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.017 | Acc: 62.385% | Wgt Acc: 61.413% | Dur: 9.68s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  9.  8. 32.]
 [ 6. 40. 11.  3.]
 [ 2. 20. 42.  3.]
 [ 6.  9. 14. 48.]]

I - Epoch: 151
I - Training: 
	I - Batch: 50 | Loss: 0.394 | Acc: 98.875% | Wgt Acc: 98.925%
	I - Batch: 100 | Loss: 0.399 | Acc: 98.750% | Wgt Acc: 98.758%
	I - Batch: 150 | Loss: 0.399 | Acc: 98.792% | Wgt Acc: 98.808%
I - num batch: 160
I - Train -- Loss: 0.402 | Acc: 98.783% | Wgt Acc: 98.797% | LR: 1.250000e-04 | Dur: 85.94s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   1.   5.   9.]
 [  3. 577.   0.   0.]
 [  8.   0. 729.   2.]
 [  3.   0.   0. 527.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 0.999 | Acc: 64.220% | Wgt Acc: 63.995% | Dur: 9.96s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  5.  3. 18.]
 [ 6. 46. 14.  4.]
 [ 2. 19. 41.  8.]
 [13.  8. 17. 56.]]

I - Epoch: 152
I - Training: 
	I - Batch: 50 | Loss: 0.427 | Acc: 97.250% | Wgt Acc: 97.268%
	I - Batch: 100 | Loss: 0.413 | Acc: 98.188% | Wgt Acc: 98.172%
	I - Batch: 150 | Loss: 0.408 | Acc: 98.417% | Wgt Acc: 98.415%
I - num batch: 160
I - Train -- Loss: 0.409 | Acc: 98.390% | Wgt Acc: 98.390% | LR: 1.250000e-04 | Dur: 86.35s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   2.   2.  10.]
 [  8. 576.   1.   1.]
 [  5.   0. 728.   5.]
 [  4.   0.   3. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.050 | Acc: 60.856% | Wgt Acc: 60.530% | Dur: 10.24s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  8.  7. 16.]
 [ 1. 32.  6.  1.]
 [ 1. 28. 36.  6.]
 [18. 10. 26. 63.]]

I - Epoch: 153
I - Training: 
	I - Batch: 50 | Loss: 0.398 | Acc: 98.750% | Wgt Acc: 98.672%
	I - Batch: 100 | Loss: 0.395 | Acc: 98.938% | Wgt Acc: 98.902%
	I - Batch: 150 | Loss: 0.396 | Acc: 98.958% | Wgt Acc: 98.949%
I - num batch: 160
I - Train -- Loss: 0.395 | Acc: 98.979% | Wgt Acc: 98.974% | LR: 1.250000e-04 | Dur: 87.93s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   2.   9.]
 [  3. 577.   1.   2.]
 [  4.   1. 730.   0.]
 [  3.   0.   1. 527.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 0.995 | Acc: 62.385% | Wgt Acc: 62.432% | Dur: 10.23s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  6.  6. 20.]
 [ 5. 43. 15.  2.]
 [ 0. 21. 34.  4.]
 [16.  8. 20. 60.]]

I - Epoch: 154
I - Training: 
	I - Batch: 50 | Loss: 0.391 | Acc: 99.375% | Wgt Acc: 99.297%
	I - Batch: 100 | Loss: 0.390 | Acc: 99.312% | Wgt Acc: 99.267%
	I - Batch: 150 | Loss: 0.392 | Acc: 99.125% | Wgt Acc: 99.108%
I - num batch: 160
I - Train -- Loss: 0.396 | Acc: 99.058% | Wgt Acc: 99.045% | LR: 1.250000e-04 | Dur: 90.98s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   3.   8.]
 [  2. 578.   0.   1.]
 [  1.   0. 729.   3.]
 [  4.   0.   2. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.053 | Acc: 59.021% | Wgt Acc: 58.764% | Dur: 10.17s
I - Confusion Matrix: [row->prediction - col->label]
[[69. 15.  9. 26.]
 [ 3. 34.  9.  1.]
 [ 0. 12. 31.  0.]
 [16. 17. 26. 59.]]

I - Epoch: 155
I - Training: 
	I - Batch: 50 | Loss: 0.390 | Acc: 99.375% | Wgt Acc: 99.407%
	I - Batch: 100 | Loss: 0.396 | Acc: 99.062% | Wgt Acc: 99.055%
	I - Batch: 150 | Loss: 0.398 | Acc: 98.917% | Wgt Acc: 98.919%
I - num batch: 160
I - Train -- Loss: 0.397 | Acc: 98.940% | Wgt Acc: 98.938% | LR: 1.250000e-04 | Dur: 87.55s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   3.  11.]
 [  4. 578.   0.   0.]
 [  4.   0. 730.   1.]
 [  3.   0.   1. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.052 | Acc: 60.550% | Wgt Acc: 59.986% | Dur: 10.03s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  2.  1. 16.]
 [11. 44. 19.  8.]
 [ 5. 28. 48. 15.]
 [13.  4.  7. 47.]]

I - Epoch: 156
I - Training: 
	I - Batch: 50 | Loss: 0.389 | Acc: 99.375% | Wgt Acc: 99.355%
	I - Batch: 100 | Loss: 0.394 | Acc: 99.000% | Wgt Acc: 99.013%
	I - Batch: 150 | Loss: 0.393 | Acc: 99.042% | Wgt Acc: 99.051%
I - num batch: 160
I - Train -- Loss: 0.393 | Acc: 99.018% | Wgt Acc: 99.027% | LR: 1.250000e-04 | Dur: 88.45s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   1.   5.   8.]
 [  3. 577.   0.   0.]
 [  3.   0. 727.   1.]
 [  2.   0.   2. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.010 | Acc: 62.385% | Wgt Acc: 61.549% | Dur: 9.98s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  5.  4. 24.]
 [ 6. 37. 13.  2.]
 [ 5. 30. 46.  7.]
 [ 9.  6. 12. 53.]]

I - Epoch: 157
I - Training: 
	I - Batch: 50 | Loss: 0.399 | Acc: 98.750% | Wgt Acc: 98.741%
	I - Batch: 100 | Loss: 0.393 | Acc: 98.938% | Wgt Acc: 98.973%
	I - Batch: 150 | Loss: 0.392 | Acc: 99.000% | Wgt Acc: 99.014%
I - num batch: 160
I - Train -- Loss: 0.393 | Acc: 98.940% | Wgt Acc: 98.947% | LR: 1.250000e-04 | Dur: 90.50s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   3.   6.]
 [  4. 577.   1.   1.]
 [  3.   1. 729.   3.]
 [  4.   0.   1. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.035 | Acc: 60.245% | Wgt Acc: 59.986% | Dur: 10.91s
I - Confusion Matrix: [row->prediction - col->label]
[[72. 11. 10. 27.]
 [ 2. 41. 19.  4.]
 [ 1. 17. 30.  1.]
 [13.  9. 16. 54.]]

I - Epoch: 158
I - Training: 
	I - Batch: 50 | Loss: 0.408 | Acc: 98.750% | Wgt Acc: 98.731%
	I - Batch: 100 | Loss: 0.399 | Acc: 99.125% | Wgt Acc: 99.109%
	I - Batch: 150 | Loss: 0.397 | Acc: 99.000% | Wgt Acc: 99.013%
I - num batch: 160
I - Train -- Loss: 0.395 | Acc: 99.058% | Wgt Acc: 99.071% | LR: 1.250000e-04 | Dur: 87.60s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   2.   6.]
 [  2. 577.   1.   1.]
 [  4.   1. 729.   1.]
 [  4.   0.   2. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.037 | Acc: 61.162% | Wgt Acc: 60.666% | Dur: 10.58s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  8.  8. 24.]
 [ 6. 40. 13.  4.]
 [ 1. 21. 37.  5.]
 [11.  9. 17. 53.]]

I - Epoch: 159
I - Training: 
	I - Batch: 50 | Loss: 0.399 | Acc: 98.125% | Wgt Acc: 98.195%
	I - Batch: 100 | Loss: 0.394 | Acc: 98.562% | Wgt Acc: 98.590%
	I - Batch: 150 | Loss: 0.392 | Acc: 98.917% | Wgt Acc: 98.920%
I - num batch: 160
I - Train -- Loss: 0.391 | Acc: 98.979% | Wgt Acc: 98.983% | LR: 1.250000e-04 | Dur: 85.02s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   0.   8.]
 [  4. 577.   1.   0.]
 [  3.   1. 730.   2.]
 [  4.   0.   3. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.012 | Acc: 62.385% | Wgt Acc: 61.753% | Dur: 10.00s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  8.  4. 24.]
 [ 7. 39. 12.  5.]
 [ 2. 25. 41.  3.]
 [ 9.  6. 18. 54.]]

I - Epoch: 160
I - Training: 
	I - Batch: 50 | Loss: 0.388 | Acc: 99.125% | Wgt Acc: 99.157%
	I - Batch: 100 | Loss: 0.391 | Acc: 99.188% | Wgt Acc: 99.156%
	I - Batch: 150 | Loss: 0.390 | Acc: 99.333% | Wgt Acc: 99.315%
I - num batch: 160
I - Train -- Loss: 0.390 | Acc: 99.372% | Wgt Acc: 99.354% | LR: 1.250000e-04 | Dur: 89.17s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   1.   5.]
 [  1. 577.   0.   1.]
 [  1.   1. 730.   2.]
 [  1.   0.   3. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.032 | Acc: 61.162% | Wgt Acc: 60.122% | Dur: 10.46s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  7.  8. 34.]
 [ 5. 39. 12.  2.]
 [ 1. 26. 41.  4.]
 [ 8.  6. 14. 46.]]

I - Epoch: 161
I - Training: 
	I - Batch: 50 | Loss: 0.386 | Acc: 99.250% | Wgt Acc: 99.266%
	I - Batch: 100 | Loss: 0.386 | Acc: 99.188% | Wgt Acc: 99.169%
	I - Batch: 150 | Loss: 0.388 | Acc: 99.125% | Wgt Acc: 99.108%
I - num batch: 160
I - Train -- Loss: 0.388 | Acc: 99.136% | Wgt Acc: 99.115% | LR: 1.250000e-04 | Dur: 85.86s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   1.   8.]
 [  3. 577.   0.   0.]
 [  2.   1. 731.   3.]
 [  2.   0.   2. 527.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.017 | Acc: 64.526% | Wgt Acc: 64.062% | Dur: 9.50s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  4.  3. 21.]
 [ 2. 40. 13.  3.]
 [ 3. 25. 42.  3.]
 [13.  9. 17. 59.]]

I - Local maximum validation set accuracy:  64.53

I - Validation set results: 
[14-1-1--0.05][50-3-3--0.33][124-2-2-0.84][127-0-0-0.99][443-2-2-0.99][567-0-0-0.89][573-1-1-0.62][615-0-3-0.90][695-1-2-0.11][722-3-3-0.98]
[826-0-0-0.87][878-0-3-0.98][1103-0-0-0.20][1212-3-3-0.49][1368-0-0-0.68][2181-2-3--0.69][2476-2-2-0.36][2721-2-2-0.78][2818-1-3-0.94][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-1-0.99][3482-2-2-0.94][3536-3-3--0.41][3625-1-2-0.40][3909-0-0-0.07][4035-0-0-0.88][4140-0-0-0.49][4214-1-3-0.42][4346-1-3--0.12]
[4581-2-2-0.20][4708-3-3-0.25][4838-3-3-0.02][4845-1-1-0.09][4868-0-0-0.95][4939-0-2--0.40][4984-2-2-0.99][5078-1-2-0.79][5396-0-0-0.99][5479-1-1-0.15]
[5717-0-0-0.97][5843-1-3-0.95][5949-3-3-0.30][5987-2-2-0.24][6014-3-1-0.64][6033-3-0-0.14][6313-0-0-0.87][6421-3-3-0.99][6500-1-1--0.53][6583-3-3-0.51]
[6683-3-3-0.99][6825-2-1-0.47][6998-3-3--0.48][7049-3-3-0.63][7517-1-1--0.02][7521-1-1-0.26][7528-1-3--0.11][7949-1-2--0.13][8135-1-0--0.58][8185-3-0-0.99]
[8269-3-2--0.24][8273-3-3-0.84][8543-3-0-0.99][8666-1-3--0.26][8672-0-3-0.54][8903-1-1--0.40][9001-2-2-0.95][9036-2-2-0.68][9281-3-3-0.39][9300-2-2-0.99]
[9571-0-0--0.01][9617-1-2--0.31][9644-2-2--0.16][9705-2-1-0.24][9801-0-3-0.62][9803-3-3-0.60][9865-3-3-0.88][9896-2-2-0.99][10314-1-1-0.57][10337-3-3-0.99]
[10403-0-2--0.27][10653-2-2--0.02][10704-2-3--0.30][10719-1-1-0.89][10727-1-2-0.98][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.51][11088-1-1-0.99][11322-0-0-0.96]
[11398-2-2-0.92][11499-0-0-0.74][11502-3-2--0.53][11512-3-3-0.99][11608-1-1-0.99][11610-0-0--0.10][11692-0-0-0.59][11905-0-0-0.99][11993-1-1-0.99][12002-2-3--0.50]
[12052-0-0-0.86][12201-0-0-0.55][12235-2-2-0.95][12320-1-0-0.88][12377-2-2-0.15][12398-2-0-0.09][12503-1-1--0.91][12617-0-2--0.37][12685-3-3-0.35][12738-2-3--0.54]
[12742-2-2-0.99][12823-0-3-0.92][13110-1-2-0.15][13240-3-2-0.27][13253-1-1-0.99][13273-0-0-0.99][13634-1-3-0.85][13763-2-2-0.29][13905-3-0-0.72][14060-2-1--0.00]
[14065-3-0--0.21][14147-3-0-0.30][14595-2-1-0.26][14687-2-3-0.98][14788-2-3-0.95][14869-1-2-0.93][14872-3-0-0.10][14877-1-1-0.99][14927-0-0--0.46][15066-0-0-0.57]
[15175-1-1-0.64][15178-2-3-0.52][15375-3-0-0.35][15389-3-3-0.99][15568-2-1-0.64][15675-3-3-0.99][15869-1-1--0.07][16207-3-0--0.34][16236-0-0--0.21][16302-3-3-0.95]
[16331-2-2-0.99][16381-0-0--0.26][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-1-0.86][16801-0-0-0.99][16828-0-0-0.78][17137-3-0-0.54][17245-1-1--0.11]
[17278-3-3--0.36][17282-0-0-0.41][17311-2-2-0.49][17336-2-1-0.70][17608-3-3-0.99][17627-0-0-0.19][17877-3-0-0.39][17924-1-0--0.38][17984-3-3-0.51][18211-0-3-0.15]
[18276-3-3-0.98][18287-1-1-0.59][18394-0-0-0.99][18428-0-0-0.99][18442-0-3-0.52][18478-3-3-0.98][18607-0-0-0.70][18616-0-0-0.69][18663-0-0-0.39][18718-0-0-0.99]
[18766-2-2--0.48][18824-2-2-0.58][18890-3-3-0.74][18930-3-0--0.13][18938-3-3-0.38][19817-1-2-0.98][19839-0-0-0.80][19930-3-0-0.42][19944-0-0-0.75][20036-2-2-0.98]
[20101-3-3--0.02][20474-1-2-0.81][20547-3-3-0.31][20929-2-2-0.56][21245-1-2-0.53][21257-3-3-0.10][21293-1-2-0.92][21316-1-1-0.99][21384-1-3-0.35][21448-1-1-0.04]
[21483-0-0-0.98][21487-2-2-0.99][21714-0-3--0.19][21943-3-3-0.93][21947-0-0-0.71][21948-0-0-0.99][21965-2-2-0.93][21998-1-1-0.87][22025-0-3-0.61][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-0-0.23][22757-0-0-0.99][22811-3-3-0.99][22976-3-3-0.53][22985-3-3-0.90][23014-0-0-0.99][23112-1-2-0.84][23144-3-3-0.20][23168-2-3--0.09]
[23219-0-0-0.71][23363-3-3-0.99][23470-0-1--0.15][23486-2-2--0.59][23497-0-3-0.99][23516-0-0-0.98][23690-1-2-0.99][23921-2-1-0.92][23936-1-2-0.85][24040-3-0-0.65]
[24111-1-2-0.76][24182-0-0-0.85][24238-3-3-0.91][24290-2-0-0.98][24345-0-0-0.28][24364-1-2-0.21][24427-3-0-0.99][24477-2-2-0.99][24495-2-1-0.71][24893-2-1-0.02]
[25012-1-2--0.35][25121-2-2-0.79][25165-3-3-0.99][25183-0-0-0.79][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.81][25644-1-1-0.99][25718-1-1--0.02][25774-2-2-0.46]
[26032-3-3-0.54][26051-3-3-0.99][26120-0-0-0.98][26321-1-1-0.99][26732-1-1-0.61][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-3--0.20][26860-1-0-0.09]
[26948-0-3-0.61][27049-3-0-0.99][27098-1-1-0.38][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.42][28040-0-0-0.99][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3-0.24][29777-0-0-0.99][29877-2-1-0.29][30035-1-1-0.62][30098-0-0-0.86][30326-1-1-0.99][30572-2-3-0.76][30716-0-3-0.36]
[30806-2-3--0.55][30906-1-2-0.90][31007-0-0-0.95][31181-3-0--0.33][31238-0-0-0.07][31347-0-0-0.99][31422-2-0--0.66][31429-3-3-0.14][31431-0-0--0.19][31432-1-1-0.86]
[31477-0-0-0.98][31524-1-2--0.03][31597-1-1-0.72][31619-1-2--0.20][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.94][32074-1-2-0.80][32078-3-3-0.32][32111-1-1-0.86]
[32127-1-2-0.31][32140-3-3-0.96][32263-2-2-0.13][32365-0-0-0.52][32411-2-3-0.99][32429-3-0-0.79][32473-3-0-0.99][32574-3-3-0.96][32584-0-0--0.12][32622-0-1-0.91]
[32858-3-0-0.99][32969-3-3-0.56][33016-2-2-0.70][33031-1-3-0.21][33035-2-2-0.40][33133-2-2-0.94][33173-2-3--0.26][33175-3-1--0.05][33306-3-1-0.99][33309-2-3-0.26]
[33474-0-0-0.14][33478-2-3-0.60][33618-1-1--0.61][33712-0-0-0.76][33782-2-2-0.49][33914-3-3-0.99][34076-3-3--0.58][34112-2-2-0.67][34138-2-3-0.99][34239-1-1--0.69]
[34364-2-1-0.51][34617-1-2-0.99][34751-3-3--0.12][34783-2-2-0.19][35015-3-3-0.75][35018-1-2-0.96][35288-2-2-0.19]
---------------------------
I - Epoch: 162
I - Training: 
	I - Batch: 50 | Loss: 0.392 | Acc: 99.250% | Wgt Acc: 99.244%
	I - Batch: 100 | Loss: 0.389 | Acc: 99.250% | Wgt Acc: 99.256%
	I - Batch: 150 | Loss: 0.390 | Acc: 99.292% | Wgt Acc: 99.286%
I - num batch: 160
I - Train -- Loss: 0.389 | Acc: 99.333% | Wgt Acc: 99.328% | LR: 1.250000e-04 | Dur: 87.26s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   4.   6.]
 [  1. 578.   0.   0.]
 [  3.   0. 730.   2.]
 [  1.   0.   0. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.021 | Acc: 60.856% | Wgt Acc: 60.258% | Dur: 9.67s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  5. 18.]
 [ 9. 37. 11.  3.]
 [ 6. 30. 49. 11.]
 [14.  7. 10. 54.]]

I - Epoch: 163
I - Training: 
	I - Batch: 50 | Loss: 0.392 | Acc: 99.000% | Wgt Acc: 98.987%
	I - Batch: 100 | Loss: 0.391 | Acc: 99.250% | Wgt Acc: 99.253%
	I - Batch: 150 | Loss: 0.389 | Acc: 99.292% | Wgt Acc: 99.286%
I - num batch: 160
I - Train -- Loss: 0.388 | Acc: 99.333% | Wgt Acc: 99.328% | LR: 1.250000e-04 | Dur: 86.68s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   7.]
 [  2. 578.   0.   0.]
 [  1.   0. 731.   1.]
 [  3.   0.   3. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.028 | Acc: 65.443% | Wgt Acc: 64.810% | Dur: 10.31s
I - Confusion Matrix: [row->prediction - col->label]
[[76.  8.  9. 23.]
 [ 3. 41.  5.  3.]
 [ 0. 16. 40.  3.]
 [ 9. 13. 21. 57.]]

I - Local maximum validation set accuracy:  65.44

I - Validation set results: 
[14-1-1--0.40][50-3-0-0.65][124-2-2--0.68][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1--0.22][615-0-3-0.81][695-1-0-0.99][722-3-3-0.91]
[826-0-0-0.85][878-0-0-0.99][1103-0-0-0.57][1212-3-3-0.14][1368-0-0-0.99][2181-2-3--0.20][2476-2-2-0.18][2721-2-2-0.90][2818-1-3-0.64][2886-2-2--0.20]
[3231-2-2-0.99][3333-2-1-0.99][3482-2-2-0.85][3536-3-3-0.93][3625-1-1-0.02][3909-0-0-0.50][4035-0-0-0.99][4140-0-0-0.99][4214-1-3-0.95][4346-1-1--0.02]
[4581-2-2--0.18][4708-3-3-0.96][4838-3-3-0.91][4845-1-1--0.43][4868-0-0-0.98][4939-0-0-0.03][4984-2-2-0.89][5078-1-2-0.82][5396-0-0-0.99][5479-1-1-0.19]
[5717-0-0-0.35][5843-1-3-0.48][5949-3-3-0.27][5987-2-0-0.34][6014-3-3-0.51][6033-3-2-0.18][6313-0-0-0.99][6421-3-3-0.69][6500-1-0--0.50][6583-3-3-0.77]
[6683-3-3-0.99][6825-2-1--0.11][6998-3-3-0.01][7049-3-3-0.96][7517-1-1-0.01][7521-1-0--0.19][7528-1-3-0.04][7949-1-0-0.12][8135-1-0--0.08][8185-3-0-0.99]
[8269-3-1--0.58][8273-3-3-0.99][8543-3-0-0.99][8666-1-1--0.23][8672-0-0-0.99][8903-1-1--0.48][9001-2-1-0.73][9036-2-2-0.99][9281-3-0-0.29][9300-2-2-0.99]
[9571-0-3-0.22][9617-1-1-0.11][9644-2-2-0.72][9705-2-2-0.04][9801-0-0-0.34][9803-3-3-0.91][9865-3-3-0.94][9896-2-2-0.33][10314-1-0-0.80][10337-3-3-0.82]
[10403-0-0--0.19][10653-2-3--0.24][10704-2-3--0.30][10719-1-1-0.94][10727-1-2-0.92][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.96]
[11398-2-2--0.19][11499-0-0-0.89][11502-3-0-0.09][11512-3-3-0.99][11608-1-1-0.98][11610-0-0-0.27][11692-0-0-0.65][11905-0-0-0.99][11993-1-3-0.37][12002-2-3--0.08]
[12052-0-0-0.97][12201-0-3-0.69][12235-2-2-0.62][12320-1-0-0.99][12377-2-0--0.52][12398-2-0-0.67][12503-1-1--0.38][12617-0-1-0.26][12685-3-3--0.48][12738-2-3-0.68]
[12742-2-2-0.99][12823-0-0-0.87][13110-1-2--0.26][13240-3-0-0.86][13253-1-1-0.94][13273-0-0-0.99][13634-1-3-0.71][13763-2-2--0.27][13905-3-0-0.38][14060-2-3-0.12]
[14065-3-0-0.00][14147-3-0--0.01][14595-2-2--0.27][14687-2-3-0.99][14788-2-2-0.41][14869-1-1-0.07][14872-3-0-0.53][14877-1-1-0.99][14927-0-3--0.28][15066-0-0-0.90]
[15175-1-1-0.99][15178-2-3-0.99][15375-3-0-0.83][15389-3-3-0.99][15568-2-1-0.98][15675-3-3-0.99][15869-1-1--0.07][16207-3-0-0.60][16236-0-0-0.15][16302-3-2-0.02]
[16331-2-2-0.99][16381-0-0--0.09][16488-1-1--0.59][16495-0-0-0.99][16650-0-0-0.99][16719-1-1-0.99][16801-0-0-0.99][16828-0-0-0.99][17137-3-3-0.65][17245-1-1--0.33]
[17278-3-0-0.48][17282-0-0-0.99][17311-2-2--0.26][17336-2-3-0.05][17608-3-3-0.99][17627-0-0--0.46][17877-3-0-0.30][17924-1-3--0.00][17984-3-3-0.99][18211-0-0--0.59]
[18276-3-3-0.96][18287-1-3-0.11][18394-0-0-0.99][18428-0-0-0.83][18442-0-3-0.53][18478-3-3-0.98][18607-0-0-0.99][18616-0-0-0.99][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.51][18824-2-2-0.46][18890-3-3-0.59][18930-3-0-0.99][18938-3-3-0.70][19817-1-2-0.28][19839-0-0-0.92][19930-3-0-0.66][19944-0-1-0.18][20036-2-2-0.99]
[20101-3-3-0.69][20474-1-2-0.99][20547-3-3-0.48][20929-2-2-0.77][21245-1-1-0.23][21257-3-3-0.16][21293-1-2-0.62][21316-1-1-0.99][21384-1-3-0.16][21448-1-2--0.49]
[21483-0-0-0.99][21487-2-2-0.72][21714-0-0-0.07][21943-3-3-0.40][21947-0-0-0.98][21948-0-0-0.99][21965-2-2--0.03][21998-1-1-0.32][22025-0-3--0.25][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-0-0.24][22757-0-0-0.99][22811-3-3-0.99][22976-3-1-0.26][22985-3-3-0.71][23014-0-0-0.98][23112-1-2-0.48][23144-3-3-0.91][23168-2-0-0.14]
[23219-0-0-0.40][23363-3-1--0.52][23470-0-0-0.78][23486-2-3-0.11][23497-0-3-0.99][23516-0-0-0.99][23690-1-2-0.71][23921-2-2--0.18][23936-1-2-0.98][24040-3-2-0.53]
[24111-1-2-0.29][24182-0-0-0.99][24238-3-3-0.90][24290-2-0-0.99][24345-0-0-0.98][24364-1-3--0.56][24427-3-0-0.99][24477-2-2-0.94][24495-2-1--0.31][24893-2-3--0.27]
[25012-1-1--0.02][25121-2-0-0.37][25165-3-3-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.69][25574-2-2-0.08][25644-1-1-0.99][25718-1-2-0.07][25774-2-3-0.32]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-0-0.99][26321-1-1-0.35][26732-1-1-0.26][26784-3-3-0.99][26827-3-3-0.86][26833-0-3-0.99][26838-2-2--0.26][26860-1-1--0.22]
[26948-0-0-0.63][27049-3-0-0.99][27098-1-0-0.26][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.57][28040-0-0-0.97][28503-2-2-0.99]
[28577-1-1-0.84][28959-0-0-0.99][29198-3-3-0.86][29777-0-0-0.99][29877-2-3-0.84][30035-1-1-0.47][30098-0-0-0.99][30326-1-1-0.99][30572-2-3-0.90][30716-0-0-0.67]
[30806-2-3-0.88][30906-1-1-0.58][31007-0-0-0.25][31181-3-3-0.92][31238-0-0-0.62][31347-0-3-0.96][31422-2-0--0.40][31429-3-3--0.39][31431-0-0-0.99][31432-1-1-0.72]
[31477-0-0-0.99][31524-1-2-0.18][31597-1-1-0.80][31619-1-2-0.80][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.98][32074-1-3--0.00][32078-3-3-0.97][32111-1-1-0.55]
[32127-1-1-0.75][32140-3-3-0.03][32263-2-0--0.05][32365-0-0-0.99][32411-2-3-0.99][32429-3-0-0.75][32473-3-0-0.99][32574-3-3-0.33][32584-0-0-0.97][32622-0-1-0.95]
[32858-3-0-0.99][32969-3-3-0.64][33016-2-2-0.45][33031-1-3--0.08][33035-2-2-0.12][33133-2-2-0.83][33173-2-3-0.89][33175-3-3-0.43][33306-3-3-0.99][33309-2-3--0.26]
[33474-0-0-0.46][33478-2-3-0.68][33618-1-1--0.39][33712-0-0-0.99][33782-2-2--0.53][33914-3-3-0.29][34076-3-3--0.09][34112-2-2-0.35][34138-2-3-0.99][34239-1-3--0.06]
[34364-2-2-0.71][34617-1-2-0.69][34751-3-0--0.68][34783-2-0--0.05][35015-3-3-0.57][35018-1-2-0.89][35288-2-2--0.20]
---------------------------
I - Epoch: 164
I - Training: 
	I - Batch: 50 | Loss: 0.389 | Acc: 99.000% | Wgt Acc: 99.021%
	I - Batch: 100 | Loss: 0.390 | Acc: 99.188% | Wgt Acc: 99.186%
	I - Batch: 150 | Loss: 0.391 | Acc: 99.292% | Wgt Acc: 99.286%
I - num batch: 160
I - Train -- Loss: 0.390 | Acc: 99.333% | Wgt Acc: 99.328% | LR: 1.250000e-04 | Dur: 86.94s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   1.   3.   6.]
 [  3. 577.   0.   0.]
 [  1.   0. 730.   1.]
 [  1.   0.   1. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.000 | Acc: 63.303% | Wgt Acc: 62.568% | Dur: 9.94s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  6.  7. 25.]
 [ 6. 40. 11.  1.]
 [ 3. 23. 44.  7.]
 [ 9.  9. 13. 53.]]

I - Epoch: 165
I - Training: 
	I - Batch: 50 | Loss: 0.377 | Acc: 99.625% | Wgt Acc: 99.632%
	I - Batch: 100 | Loss: 0.380 | Acc: 99.438% | Wgt Acc: 99.451%
	I - Batch: 150 | Loss: 0.386 | Acc: 99.333% | Wgt Acc: 99.342%
I - num batch: 160
I - Train -- Loss: 0.388 | Acc: 99.293% | Wgt Acc: 99.301% | LR: 1.250000e-04 | Dur: 86.53s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   2.   5.]
 [  1. 577.   2.   1.]
 [  2.   1. 728.   0.]
 [  2.   0.   2. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.031 | Acc: 61.774% | Wgt Acc: 61.005% | Dur: 9.85s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  8.  5. 25.]
 [ 7. 39. 12.  1.]
 [ 5. 26. 46.  9.]
 [10.  5. 12. 51.]]

I - Epoch: 166
I - Training: 
	I - Batch: 50 | Loss: 0.389 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.388 | Acc: 99.562% | Wgt Acc: 99.578%
	I - Batch: 150 | Loss: 0.389 | Acc: 99.333% | Wgt Acc: 99.342%
I - num batch: 160
I - Train -- Loss: 0.390 | Acc: 99.372% | Wgt Acc: 99.381% | LR: 1.250000e-04 | Dur: 88.96s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   2.   6.]
 [  3. 578.   0.   0.]
 [  3.   0. 732.   0.]
 [  2.   0.   0. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.019 | Acc: 62.385% | Wgt Acc: 62.024% | Dur: 10.05s
I - Confusion Matrix: [row->prediction - col->label]
[[73. 10.  8. 24.]
 [ 6. 39. 11.  1.]
 [ 1. 20. 34.  3.]
 [ 8.  9. 22. 58.]]

I - Epoch: 167
I - Training: 
	I - Batch: 50 | Loss: 0.399 | Acc: 98.375% | Wgt Acc: 98.475%
	I - Batch: 100 | Loss: 0.390 | Acc: 99.125% | Wgt Acc: 99.184%
	I - Batch: 150 | Loss: 0.391 | Acc: 99.083% | Wgt Acc: 99.090%
I - num batch: 160
I - Train -- Loss: 0.394 | Acc: 99.018% | Wgt Acc: 99.018% | LR: 1.250000e-04 | Dur: 89.49s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   1.   2.   6.]
 [  5. 576.   0.   1.]
 [  0.   1. 731.   2.]
 [  6.   0.   1. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.037 | Acc: 61.774% | Wgt Acc: 61.141% | Dur: 9.78s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  9.  7. 26.]
 [ 4. 36. 11.  3.]
 [ 1. 22. 38.  1.]
 [11. 11. 19. 56.]]

I - Epoch: 168
I - Training: 
	I - Batch: 50 | Loss: 0.391 | Acc: 99.500% | Wgt Acc: 99.494%
	I - Batch: 100 | Loss: 0.384 | Acc: 99.625% | Wgt Acc: 99.606%
	I - Batch: 150 | Loss: 0.385 | Acc: 99.542% | Wgt Acc: 99.540%
I - num batch: 160
I - Train -- Loss: 0.385 | Acc: 99.490% | Wgt Acc: 99.487% | LR: 1.250000e-04 | Dur: 84.72s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   2.   4.]
 [  2. 578.   0.   0.]
 [  1.   0. 731.   2.]
 [  1.   0.   1. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.064 | Acc: 62.691% | Wgt Acc: 61.481% | Dur: 9.80s
I - Confusion Matrix: [row->prediction - col->label]
[[77. 13. 11. 32.]
 [ 5. 40. 13.  4.]
 [ 2. 19. 43.  5.]
 [ 4.  6.  8. 45.]]

I - Epoch: 169
I - Training: 
	I - Batch: 50 | Loss: 0.385 | Acc: 99.500% | Wgt Acc: 99.521%
	I - Batch: 100 | Loss: 0.389 | Acc: 99.250% | Wgt Acc: 99.281%
	I - Batch: 150 | Loss: 0.387 | Acc: 99.375% | Wgt Acc: 99.399%
I - num batch: 160
I - Train -- Loss: 0.387 | Acc: 99.411% | Wgt Acc: 99.434% | LR: 1.250000e-04 | Dur: 87.30s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   2.   3.]
 [  4. 577.   0.   0.]
 [  2.   1. 731.   0.]
 [  2.   0.   1. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.043 | Acc: 60.856% | Wgt Acc: 59.511% | Dur: 9.71s
I - Confusion Matrix: [row->prediction - col->label]
[[79. 12.  8. 32.]
 [ 3. 32. 11.  2.]
 [ 1. 23. 40.  4.]
 [ 5. 11. 16. 48.]]

I - Epoch: 170
I - Training: 
	I - Batch: 50 | Loss: 0.389 | Acc: 99.125% | Wgt Acc: 99.130%
	I - Batch: 100 | Loss: 0.385 | Acc: 99.312% | Wgt Acc: 99.296%
	I - Batch: 150 | Loss: 0.386 | Acc: 99.375% | Wgt Acc: 99.353%
I - num batch: 160
I - Train -- Loss: 0.386 | Acc: 99.372% | Wgt Acc: 99.345% | LR: 1.250000e-04 | Dur: 90.18s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   2.   6.]
 [  1. 576.   0.   1.]
 [  3.   2. 732.   1.]
 [  0.   0.   0. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.030 | Acc: 59.939% | Wgt Acc: 59.511% | Dur: 9.72s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  2.  5. 21.]
 [ 6. 39. 13.  3.]
 [ 5. 29. 45.  9.]
 [18.  8. 12. 53.]]

I - Epoch: 171
I - Training: 
	I - Batch: 50 | Loss: 0.382 | Acc: 99.750% | Wgt Acc: 99.775%
	I - Batch: 100 | Loss: 0.380 | Acc: 99.688% | Wgt Acc: 99.690%
	I - Batch: 150 | Loss: 0.383 | Acc: 99.583% | Wgt Acc: 99.578%
I - num batch: 160
I - Train -- Loss: 0.382 | Acc: 99.607% | Wgt Acc: 99.602% | LR: 1.250000e-04 | Dur: 85.75s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   3.   4.]
 [  0. 578.   0.   0.]
 [  1.   0. 731.   1.]
 [  1.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.076 | Acc: 57.492% | Wgt Acc: 56.386% | Dur: 10.78s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  2. 19.]
 [ 7. 36. 14.  4.]
 [11. 31. 52. 21.]
 [12.  7.  7. 42.]]

I - Epoch: 172
I - Training: 
	I - Batch: 50 | Loss: 0.387 | Acc: 99.375% | Wgt Acc: 99.295%
	I - Batch: 100 | Loss: 0.384 | Acc: 99.500% | Wgt Acc: 99.451%
	I - Batch: 150 | Loss: 0.385 | Acc: 99.375% | Wgt Acc: 99.334%
I - num batch: 160
I - Train -- Loss: 0.384 | Acc: 99.411% | Wgt Acc: 99.372% | LR: 1.250000e-04 | Dur: 88.88s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   9.]
 [  1. 577.   0.   1.]
 [  0.   1. 733.   0.]
 [  2.   0.   1. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.046 | Acc: 62.691% | Wgt Acc: 61.957% | Dur: 10.03s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  9.  8. 23.]
 [ 3. 36. 10.  3.]
 [ 1. 24. 44.  4.]
 [15.  9. 13. 56.]]

I - Epoch: 173
I - Training: 
	I - Batch: 50 | Loss: 0.394 | Acc: 99.125% | Wgt Acc: 99.127%
	I - Batch: 100 | Loss: 0.387 | Acc: 99.562% | Wgt Acc: 99.563%
	I - Batch: 150 | Loss: 0.386 | Acc: 99.583% | Wgt Acc: 99.587%
I - num batch: 160
I - Train -- Loss: 0.386 | Acc: 99.607% | Wgt Acc: 99.611% | LR: 1.250000e-04 | Dur: 86.61s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   4.]
 [  3. 578.   0.   0.]
 [  3.   0. 734.   0.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.026 | Acc: 60.245% | Wgt Acc: 59.443% | Dur: 10.31s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  9.  9. 28.]
 [ 6. 37. 10.  3.]
 [ 2. 25. 42.  5.]
 [12.  7. 14. 50.]]

I - Epoch: 174
I - Training: 
	I - Batch: 50 | Loss: 0.392 | Acc: 99.250% | Wgt Acc: 99.241%
	I - Batch: 100 | Loss: 0.391 | Acc: 99.125% | Wgt Acc: 99.126%
	I - Batch: 150 | Loss: 0.388 | Acc: 99.292% | Wgt Acc: 99.296%
I - num batch: 160
I - Train -- Loss: 0.386 | Acc: 99.333% | Wgt Acc: 99.337% | LR: 1.250000e-04 | Dur: 86.54s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   1.   5.]
 [  1. 578.   0.   0.]
 [  4.   0. 733.   2.]
 [  4.   0.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.040 | Acc: 59.939% | Wgt Acc: 59.171% | Dur: 9.79s
I - Confusion Matrix: [row->prediction - col->label]
[[74. 11.  8. 27.]
 [ 6. 39. 17.  5.]
 [ 1. 20. 35.  6.]
 [ 7.  8. 15. 48.]]

I - Epoch: 175
I - Training: 
	I - Batch: 50 | Loss: 0.381 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.383 | Acc: 99.750% | Wgt Acc: 99.733%
	I - Batch: 150 | Loss: 0.382 | Acc: 99.708% | Wgt Acc: 99.699%
I - num batch: 160
I - Train -- Loss: 0.384 | Acc: 99.686% | Wgt Acc: 99.673% | LR: 1.250000e-04 | Dur: 86.66s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   3.]
 [  1. 578.   0.   0.]
 [  1.   0. 734.   2.]
 [  1.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.054 | Acc: 56.575% | Wgt Acc: 55.639% | Dur: 10.04s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  9. 12. 37.]
 [ 3. 35. 15.  2.]
 [ 1. 24. 32.  3.]
 [10. 10. 16. 44.]]

I - Epoch: 176
I - Training: 
	I - Batch: 50 | Loss: 0.391 | Acc: 99.250% | Wgt Acc: 99.242%
	I - Batch: 100 | Loss: 0.387 | Acc: 99.438% | Wgt Acc: 99.422%
	I - Batch: 150 | Loss: 0.387 | Acc: 99.292% | Wgt Acc: 99.296%
I - num batch: 160
I - Train -- Loss: 0.387 | Acc: 99.254% | Wgt Acc: 99.248% | LR: 1.250000e-04 | Dur: 87.41s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   7.]
 [  2. 577.   0.   0.]
 [  7.   1. 734.   1.]
 [  1.   0.   0. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.025 | Acc: 61.162% | Wgt Acc: 60.122% | Dur: 10.25s
I - Confusion Matrix: [row->prediction - col->label]
[[73. 12.  8. 31.]
 [ 5. 41. 11.  6.]
 [ 3. 19. 42.  5.]
 [ 7.  6. 14. 44.]]

I - Epoch: 177
I - Training: 
	I - Batch: 50 | Loss: 0.373 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.377 | Acc: 99.688% | Wgt Acc: 99.661%
	I - Batch: 150 | Loss: 0.378 | Acc: 99.583% | Wgt Acc: 99.568%
I - num batch: 160
I - Train -- Loss: 0.380 | Acc: 99.568% | Wgt Acc: 99.558% | LR: 1.250000e-04 | Dur: 87.31s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   4.]
 [  2. 577.   0.   0.]
 [  1.   1. 734.   1.]
 [  2.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.043 | Acc: 60.856% | Wgt Acc: 60.190% | Dur: 10.07s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  9.  3. 23.]
 [ 5. 43. 17.  5.]
 [ 3. 19. 40. 11.]
 [11.  7. 15. 47.]]

I - Epoch: 178
I - Training: 
	I - Batch: 50 | Loss: 0.383 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 100 | Loss: 0.385 | Acc: 99.500% | Wgt Acc: 99.480%
	I - Batch: 150 | Loss: 0.385 | Acc: 99.458% | Wgt Acc: 99.446%
I - num batch: 160
I - Train -- Loss: 0.385 | Acc: 99.490% | Wgt Acc: 99.478% | LR: 1.250000e-04 | Dur: 87.87s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   1.   4.]
 [  0. 578.   0.   0.]
 [  3.   0. 733.   3.]
 [  2.   0.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.029 | Acc: 63.303% | Wgt Acc: 62.296% | Dur: 10.17s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  6.  4. 22.]
 [ 3. 39.  7.  3.]
 [ 3. 25. 50. 11.]
 [14.  8. 14. 50.]]

I - Epoch: 179
I - Training: 
	I - Batch: 50 | Loss: 0.382 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 100 | Loss: 0.381 | Acc: 99.625% | Wgt Acc: 99.605%
	I - Batch: 150 | Loss: 0.378 | Acc: 99.625% | Wgt Acc: 99.596%
I - num batch: 160
I - Train -- Loss: 0.378 | Acc: 99.607% | Wgt Acc: 99.584% | LR: 1.250000e-04 | Dur: 86.52s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   1.   6.]
 [  1. 578.   0.   0.]
 [  1.   0. 733.   1.]
 [  0.   0.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.031 | Acc: 64.526% | Wgt Acc: 63.587% | Dur: 9.67s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  7.  5. 22.]
 [ 6. 43. 12.  3.]
 [ 5. 21. 50. 12.]
 [ 8.  7.  8. 49.]]

I - Epoch: 180
I - Training: 
	I - Batch: 50 | Loss: 0.375 | Acc: 99.875% | Wgt Acc: 99.887%
	I - Batch: 100 | Loss: 0.377 | Acc: 99.562% | Wgt Acc: 99.550%
	I - Batch: 150 | Loss: 0.378 | Acc: 99.625% | Wgt Acc: 99.615%
I - num batch: 160
I - Train -- Loss: 0.379 | Acc: 99.568% | Wgt Acc: 99.558% | LR: 1.250000e-04 | Dur: 87.74s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   3.]
 [  0. 578.   0.   1.]
 [  3.   0. 734.   2.]
 [  2.   0.   0. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.008 | Acc: 62.080% | Wgt Acc: 61.005% | Dur: 9.93s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  5.  5. 28.]
 [ 5. 37. 13.  4.]
 [ 2. 28. 45.  5.]
 [ 9.  8. 12. 49.]]

I - Epoch: 181
I - Training: 
	I - Batch: 50 | Loss: 0.377 | Acc: 99.750% | Wgt Acc: 99.719%
	I - Batch: 100 | Loss: 0.377 | Acc: 99.750% | Wgt Acc: 99.733%
	I - Batch: 150 | Loss: 0.375 | Acc: 99.792% | Wgt Acc: 99.784%
I - num batch: 160
I - Train -- Loss: 0.376 | Acc: 99.764% | Wgt Acc: 99.752% | LR: 1.250000e-04 | Dur: 88.03s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   4.]
 [  1. 578.   0.   0.]
 [  1.   0. 734.   0.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.040 | Acc: 60.856% | Wgt Acc: 59.986% | Dur: 10.01s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  8.  8. 24.]
 [ 7. 37. 18.  4.]
 [ 2. 27. 42.  8.]
 [ 9.  6.  7. 50.]]

I - Epoch: 182
I - Training: 
	I - Batch: 50 | Loss: 0.378 | Acc: 99.625% | Wgt Acc: 99.608%
	I - Batch: 100 | Loss: 0.375 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 150 | Loss: 0.377 | Acc: 99.750% | Wgt Acc: 99.746%
I - num batch: 160
I - Train -- Loss: 0.379 | Acc: 99.686% | Wgt Acc: 99.682% | LR: 1.250000e-04 | Dur: 89.48s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   3.]
 [  2. 578.   0.   0.]
 [  0.   0. 734.   1.]
 [  2.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.060 | Acc: 58.716% | Wgt Acc: 57.405% | Dur: 10.15s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  7.  9. 37.]
 [ 7. 42. 16.  5.]
 [ 3. 25. 41.  9.]
 [ 4.  4.  9. 35.]]

I - Epoch: 183
I - Training: 
	I - Batch: 50 | Loss: 0.379 | Acc: 99.500% | Wgt Acc: 99.463%
	I - Batch: 100 | Loss: 0.379 | Acc: 99.688% | Wgt Acc: 99.676%
	I - Batch: 150 | Loss: 0.379 | Acc: 99.625% | Wgt Acc: 99.615%
I - num batch: 160
I - Train -- Loss: 0.380 | Acc: 99.568% | Wgt Acc: 99.549% | LR: 1.250000e-04 | Dur: 87.59s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   1.   7.]
 [  1. 578.   0.   0.]
 [  1.   0. 733.   0.]
 [  1.   0.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.011 | Acc: 60.856% | Wgt Acc: 60.326% | Dur: 10.11s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  5. 10. 27.]
 [ 5. 39. 14.  3.]
 [ 0. 25. 36.  3.]
 [12.  9. 15. 53.]]

I - Epoch: 184
I - Training: 
	I - Batch: 50 | Loss: 0.379 | Acc: 99.500% | Wgt Acc: 99.493%
	I - Batch: 100 | Loss: 0.380 | Acc: 99.562% | Wgt Acc: 99.535%
	I - Batch: 150 | Loss: 0.378 | Acc: 99.625% | Wgt Acc: 99.606%
I - num batch: 160
I - Train -- Loss: 0.378 | Acc: 99.607% | Wgt Acc: 99.584% | LR: 1.250000e-04 | Dur: 86.44s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   5.]
 [  2. 577.   0.   0.]
 [  0.   1. 734.   1.]
 [  1.   0.   0. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.016 | Acc: 61.162% | Wgt Acc: 60.666% | Dur: 10.49s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  6.  8. 25.]
 [ 8. 40. 16.  7.]
 [ 1. 24. 37.  1.]
 [ 9.  8. 14. 53.]]

I - Epoch: 185
I - Training: 
	I - Batch: 50 | Loss: 0.378 | Acc: 99.625% | Wgt Acc: 99.578%
	I - Batch: 100 | Loss: 0.375 | Acc: 99.688% | Wgt Acc: 99.676%
	I - Batch: 150 | Loss: 0.376 | Acc: 99.667% | Wgt Acc: 99.662%
I - num batch: 160
I - Train -- Loss: 0.376 | Acc: 99.686% | Wgt Acc: 99.682% | LR: 1.250000e-04 | Dur: 87.76s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   4.]
 [  3. 578.   1.   0.]
 [  0.   0. 733.   0.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.036 | Acc: 61.468% | Wgt Acc: 60.190% | Dur: 9.92s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  6. 25.]
 [ 5. 35.  9.  4.]
 [ 4. 31. 50. 10.]
 [10.  6. 10. 47.]]

I - Epoch: 186
I - Training: 
	I - Batch: 50 | Loss: 0.385 | Acc: 99.375% | Wgt Acc: 99.352%
	I - Batch: 100 | Loss: 0.380 | Acc: 99.438% | Wgt Acc: 99.422%
	I - Batch: 150 | Loss: 0.379 | Acc: 99.417% | Wgt Acc: 99.409%
I - num batch: 160
I - Train -- Loss: 0.379 | Acc: 99.450% | Wgt Acc: 99.443% | LR: 1.250000e-04 | Dur: 89.25s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   1.   2.   5.]
 [  3. 576.   0.   0.]
 [  0.   1. 732.   0.]
 [  2.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.025 | Acc: 63.609% | Wgt Acc: 62.636% | Dur: 10.82s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  9.  7. 32.]
 [ 6. 42. 12.  1.]
 [ 1. 21. 43.  5.]
 [ 6.  6. 13. 48.]]

I - Epoch: 187
I - Training: 
	I - Batch: 50 | Loss: 0.389 | Acc: 99.625% | Wgt Acc: 99.632%
	I - Batch: 100 | Loss: 0.383 | Acc: 99.562% | Wgt Acc: 99.563%
	I - Batch: 150 | Loss: 0.381 | Acc: 99.625% | Wgt Acc: 99.624%
I - num batch: 160
I - Train -- Loss: 0.382 | Acc: 99.607% | Wgt Acc: 99.602% | LR: 1.250000e-04 | Dur: 86.41s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   3.]
 [  1. 577.   0.   0.]
 [  2.   1. 734.   1.]
 [  2.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.012 | Acc: 60.550% | Wgt Acc: 59.851% | Dur: 10.05s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  7.  5. 23.]
 [ 8. 35. 13.  2.]
 [ 1. 25. 42.  7.]
 [12. 11. 15. 54.]]

I - Epoch: 188
I - Training: 
	I - Batch: 50 | Loss: 0.376 | Acc: 99.500% | Wgt Acc: 99.495%
	I - Batch: 100 | Loss: 0.374 | Acc: 99.625% | Wgt Acc: 99.621%
	I - Batch: 150 | Loss: 0.375 | Acc: 99.708% | Wgt Acc: 99.700%
I - num batch: 160
I - Train -- Loss: 0.374 | Acc: 99.725% | Wgt Acc: 99.717% | LR: 1.250000e-04 | Dur: 88.52s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   1.   4.]
 [  0. 578.   0.   0.]
 [  0.   0. 733.   0.]
 [  2.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.059 | Acc: 59.327% | Wgt Acc: 58.084% | Dur: 11.18s
I - Confusion Matrix: [row->prediction - col->label]
[[75. 10. 11. 35.]
 [ 3. 37. 16.  5.]
 [ 3. 23. 40.  4.]
 [ 7.  8.  8. 42.]]

I - Epoch: 189
I - Training: 
	I - Batch: 50 | Loss: 0.380 | Acc: 99.500% | Wgt Acc: 99.463%
	I - Batch: 100 | Loss: 0.377 | Acc: 99.625% | Wgt Acc: 99.605%
	I - Batch: 150 | Loss: 0.377 | Acc: 99.625% | Wgt Acc: 99.605%
I - num batch: 160
I - Train -- Loss: 0.377 | Acc: 99.607% | Wgt Acc: 99.593% | LR: 1.250000e-04 | Dur: 86.23s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   5.]
 [  2. 577.   0.   0.]
 [  1.   1. 734.   0.]
 [  1.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.038 | Acc: 61.774% | Wgt Acc: 60.598% | Dur: 9.97s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  6. 28.]
 [ 8. 37.  9.  4.]
 [ 3. 27. 49.  7.]
 [ 8.  9. 11. 47.]]

I - Epoch: 190
I - Training: 
	I - Batch: 50 | Loss: 0.378 | Acc: 99.875% | Wgt Acc: 99.888%
	I - Batch: 100 | Loss: 0.379 | Acc: 99.750% | Wgt Acc: 99.746%
	I - Batch: 150 | Loss: 0.379 | Acc: 99.708% | Wgt Acc: 99.690%
I - num batch: 160
I - Train -- Loss: 0.380 | Acc: 99.725% | Wgt Acc: 99.708% | LR: 1.250000e-04 | Dur: 89.67s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   4.]
 [  0. 578.   0.   0.]
 [  1.   0. 734.   1.]
 [  1.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.032 | Acc: 58.104% | Wgt Acc: 56.997% | Dur: 10.00s
I - Confusion Matrix: [row->prediction - col->label]
[[74. 11. 10. 30.]
 [ 3. 33. 16.  5.]
 [ 3. 25. 37.  5.]
 [ 8.  9. 12. 46.]]

I - Epoch: 191
I - Training: 
	I - Batch: 50 | Loss: 0.386 | Acc: 99.250% | Wgt Acc: 99.210%
	I - Batch: 100 | Loss: 0.382 | Acc: 99.312% | Wgt Acc: 99.269%
	I - Batch: 150 | Loss: 0.381 | Acc: 99.417% | Wgt Acc: 99.389%
I - num batch: 160
I - Train -- Loss: 0.381 | Acc: 99.372% | Wgt Acc: 99.345% | LR: 1.250000e-04 | Dur: 85.28s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   1.   7.]
 [  2. 578.   0.   0.]
 [  1.   0. 733.   3.]
 [  2.   0.   0. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.034 | Acc: 58.410% | Wgt Acc: 58.288% | Dur: 9.94s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  5. 20.]
 [ 6. 37. 14.  2.]
 [ 4. 27. 36.  7.]
 [17.  9. 20. 57.]]

I - Epoch: 192
I - Training: 
	I - Batch: 50 | Loss: 0.376 | Acc: 99.875% | Wgt Acc: 99.888%
	I - Batch: 100 | Loss: 0.375 | Acc: 99.812% | Wgt Acc: 99.803%
	I - Batch: 150 | Loss: 0.375 | Acc: 99.750% | Wgt Acc: 99.737%
I - num batch: 160
I - Train -- Loss: 0.374 | Acc: 99.764% | Wgt Acc: 99.752% | LR: 1.250000e-04 | Dur: 86.39s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   4.]
 [  1. 578.   0.   0.]
 [  1.   0. 734.   0.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.028 | Acc: 62.385% | Wgt Acc: 61.549% | Dur: 9.98s
I - Confusion Matrix: [row->prediction - col->label]
[[73. 13.  8. 26.]
 [ 5. 37.  8.  2.]
 [ 1. 21. 41.  5.]
 [ 9.  7. 18. 53.]]

I - Epoch: 193
I - Training: 
	I - Batch: 50 | Loss: 0.373 | Acc: 99.500% | Wgt Acc: 99.466%
	I - Batch: 100 | Loss: 0.373 | Acc: 99.625% | Wgt Acc: 99.621%
	I - Batch: 150 | Loss: 0.373 | Acc: 99.667% | Wgt Acc: 99.671%
I - num batch: 160
I - Train -- Loss: 0.374 | Acc: 99.647% | Wgt Acc: 99.646% | LR: 1.250000e-04 | Dur: 88.46s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   4.]
 [  3. 578.   0.   0.]
 [  1.   0. 733.   0.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.039 | Acc: 61.774% | Wgt Acc: 61.277% | Dur: 9.93s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  6. 21.]
 [ 7. 39. 19.  3.]
 [ 3. 25. 38.  7.]
 [ 8.  7. 12. 55.]]

I - Epoch: 194
I - Training: 
	I - Batch: 50 | Loss: 0.396 | Acc: 99.125% | Wgt Acc: 99.100%
	I - Batch: 100 | Loss: 0.387 | Acc: 99.312% | Wgt Acc: 99.268%
	I - Batch: 150 | Loss: 0.380 | Acc: 99.542% | Wgt Acc: 99.512%
I - num batch: 160
I - Train -- Loss: 0.380 | Acc: 99.568% | Wgt Acc: 99.540% | LR: 1.250000e-04 | Dur: 85.97s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   7.]
 [  0. 577.   0.   0.]
 [  2.   1. 734.   0.]
 [  1.   0.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.054 | Acc: 61.162% | Wgt Acc: 60.258% | Dur: 10.48s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  4.  3. 20.]
 [ 6. 38. 12.  6.]
 [ 5. 29. 49. 11.]
 [13.  7. 11. 49.]]

I - Epoch: 195
I - Training: 
	I - Batch: 50 | Loss: 0.371 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.370 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.374 | Acc: 99.708% | Wgt Acc: 99.690%
I - num batch: 160
I - Train -- Loss: 0.375 | Acc: 99.686% | Wgt Acc: 99.673% | LR: 1.250000e-04 | Dur: 87.29s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   5.]
 [  1. 578.   0.   0.]
 [  0.   0. 734.   0.]
 [  2.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.017 | Acc: 64.220% | Wgt Acc: 63.791% | Dur: 10.46s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  2. 17.]
 [ 4. 36. 11.  1.]
 [ 5. 29. 46.  5.]
 [14.  8. 16. 63.]]

I - Epoch: 196
I - Training: 
	I - Batch: 50 | Loss: 0.396 | Acc: 99.500% | Wgt Acc: 99.521%
	I - Batch: 100 | Loss: 0.386 | Acc: 99.625% | Wgt Acc: 99.620%
	I - Batch: 150 | Loss: 0.383 | Acc: 99.583% | Wgt Acc: 99.578%
I - num batch: 160
I - Train -- Loss: 0.384 | Acc: 99.568% | Wgt Acc: 99.558% | LR: 1.250000e-04 | Dur: 89.58s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   5.]
 [  1. 578.   0.   0.]
 [  1.   0. 734.   1.]
 [  3.   0.   0. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.051 | Acc: 60.856% | Wgt Acc: 59.783% | Dur: 10.83s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7.  9. 33.]
 [ 4. 39. 11.  3.]
 [ 2. 22. 40.  5.]
 [ 7. 10. 15. 45.]]

I - Epoch: 197
I - Training: 
	I - Batch: 50 | Loss: 0.391 | Acc: 99.250% | Wgt Acc: 99.191%
	I - Batch: 100 | Loss: 0.385 | Acc: 99.438% | Wgt Acc: 99.409%
	I - Batch: 150 | Loss: 0.381 | Acc: 99.542% | Wgt Acc: 99.512%
I - num batch: 160
I - Train -- Loss: 0.381 | Acc: 99.568% | Wgt Acc: 99.540% | LR: 1.250000e-04 | Dur: 91.36s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   0.   6.]
 [  2. 576.   0.   0.]
 [  0.   1. 734.   0.]
 [  1.   0.   0. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.014 | Acc: 64.220% | Wgt Acc: 63.111% | Dur: 10.72s
I - Confusion Matrix: [row->prediction - col->label]
[[76.  8.  8. 31.]
 [ 5. 45. 14.  4.]
 [ 2. 22. 45.  7.]
 [ 5.  3.  8. 44.]]

I - Epoch: 198
I - Training: 
	I - Batch: 50 | Loss: 0.378 | Acc: 99.500% | Wgt Acc: 99.495%
	I - Batch: 100 | Loss: 0.374 | Acc: 99.688% | Wgt Acc: 99.675%
	I - Batch: 150 | Loss: 0.376 | Acc: 99.542% | Wgt Acc: 99.512%
I - num batch: 160
I - Train -- Loss: 0.376 | Acc: 99.568% | Wgt Acc: 99.540% | LR: 1.250000e-04 | Dur: 84.46s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   6.]
 [  0. 577.   0.   0.]
 [  1.   1. 734.   1.]
 [  2.   0.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.067 | Acc: 61.162% | Wgt Acc: 59.986% | Dur: 10.07s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  5.  6. 28.]
 [10. 42. 17.  6.]
 [ 3. 27. 47. 11.]
 [ 5.  4.  5. 41.]]

I - Epoch: 199
I - Training: 
	I - Batch: 50 | Loss: 0.375 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.374 | Acc: 99.812% | Wgt Acc: 99.803%
	I - Batch: 150 | Loss: 0.373 | Acc: 99.833% | Wgt Acc: 99.822%
I - num batch: 160
I - Train -- Loss: 0.373 | Acc: 99.804% | Wgt Acc: 99.797% | LR: 1.250000e-04 | Dur: 89.17s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   2.]
 [  0. 578.   0.   0.]
 [  1.   0. 734.   1.]
 [  1.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.014 | Acc: 61.774% | Wgt Acc: 61.005% | Dur: 10.35s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  7.  7. 19.]
 [ 5. 36. 11.  2.]
 [ 2. 24. 45. 11.]
 [14. 11. 12. 54.]]

I - Epoch: 200
I - Training: 
	I - Batch: 50 | Loss: 0.371 | Acc: 99.875% | Wgt Acc: 99.858%
	I - Batch: 100 | Loss: 0.373 | Acc: 99.812% | Wgt Acc: 99.803%
	I - Batch: 150 | Loss: 0.374 | Acc: 99.708% | Wgt Acc: 99.709%
I - num batch: 160
I - Train -- Loss: 0.374 | Acc: 99.725% | Wgt Acc: 99.726% | LR: 1.250000e-04 | Dur: 87.08s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   1.   2.]
 [  2. 578.   0.   0.]
 [  0.   0. 733.   1.]
 [  1.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.038 | Acc: 62.080% | Wgt Acc: 61.005% | Dur: 10.97s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  9.  3. 26.]
 [ 5. 39. 16.  4.]
 [ 3. 23. 44.  9.]
 [ 7.  7. 12. 47.]]

I - Epoch: 201
I - Training: 
	I - Batch: 50 | Loss: 0.372 | Acc: 99.625% | Wgt Acc: 99.577%
	I - Batch: 100 | Loss: 0.372 | Acc: 99.625% | Wgt Acc: 99.591%
	I - Batch: 150 | Loss: 0.372 | Acc: 99.667% | Wgt Acc: 99.643%
I - num batch: 160
I - Train -- Loss: 0.374 | Acc: 99.607% | Wgt Acc: 99.575% | LR: 1.250000e-04 | Dur: 92.80s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   6.]
 [  0. 578.   0.   1.]
 [  0.   0. 734.   1.]
 [  2.   0.   0. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.050 | Acc: 59.021% | Wgt Acc: 58.220% | Dur: 11.18s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  3.  3. 20.]
 [ 6. 37. 12.  2.]
 [ 9. 29. 47. 16.]
 [12.  9. 13. 48.]]

I - Epoch: 202
I - Training: 
	I - Batch: 50 | Loss: 0.394 | Acc: 99.250% | Wgt Acc: 99.295%
	I - Batch: 100 | Loss: 0.386 | Acc: 99.438% | Wgt Acc: 99.452%
	I - Batch: 150 | Loss: 0.382 | Acc: 99.500% | Wgt Acc: 99.512%
I - num batch: 160
I - Train -- Loss: 0.382 | Acc: 99.529% | Wgt Acc: 99.540% | LR: 1.250000e-04 | Dur: 87.73s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   3.]
 [  1. 578.   1.   1.]
 [  2.   0. 732.   0.]
 [  3.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.038 | Acc: 62.997% | Wgt Acc: 61.481% | Dur: 10.28s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  2.  4. 25.]
 [ 1. 38. 10.  2.]
 [ 6. 33. 52. 16.]
 [ 8.  5.  9. 43.]]

I - Epoch: 203
I - Training: 
	I - Batch: 50 | Loss: 0.367 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.373 | Acc: 99.812% | Wgt Acc: 99.803%
	I - Batch: 150 | Loss: 0.374 | Acc: 99.667% | Wgt Acc: 99.643%
I - num batch: 160
I - Train -- Loss: 0.374 | Acc: 99.686% | Wgt Acc: 99.664% | LR: 1.250000e-04 | Dur: 86.60s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   5.]
 [  0. 578.   0.   0.]
 [  2.   0. 734.   1.]
 [  0.   0.   0. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.042 | Acc: 60.856% | Wgt Acc: 59.851% | Dur: 10.17s
I - Confusion Matrix: [row->prediction - col->label]
[[72. 10.  6. 26.]
 [ 2. 37. 13.  4.]
 [ 4. 23. 42.  8.]
 [10.  8. 14. 48.]]

I - Epoch: 204
I - Training: 
	I - Batch: 50 | Loss: 0.371 | Acc: 99.750% | Wgt Acc: 99.748%
	I - Batch: 100 | Loss: 0.374 | Acc: 99.750% | Wgt Acc: 99.733%
	I - Batch: 150 | Loss: 0.374 | Acc: 99.792% | Wgt Acc: 99.784%
I - num batch: 160
I - Train -- Loss: 0.373 | Acc: 99.804% | Wgt Acc: 99.797% | LR: 1.250000e-04 | Dur: 91.02s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   3.]
 [  1. 578.   0.   0.]
 [  0.   0. 734.   0.]
 [  1.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.041 | Acc: 62.080% | Wgt Acc: 60.870% | Dur: 10.11s
I - Confusion Matrix: [row->prediction - col->label]
[[77.  9.  7. 34.]
 [ 2. 41. 13.  4.]
 [ 3. 21. 42.  5.]
 [ 6.  7. 13. 43.]]

I - Epoch: 205
I - Training: 
	I - Batch: 50 | Loss: 0.372 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.369 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 150 | Loss: 0.371 | Acc: 99.833% | Wgt Acc: 99.812%
I - num batch: 160
I - Train -- Loss: 0.374 | Acc: 99.804% | Wgt Acc: 99.779% | LR: 1.250000e-04 | Dur: 84.82s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   4.]
 [  0. 578.   0.   0.]
 [  0.   0. 734.   1.]
 [  0.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.038 | Acc: 61.162% | Wgt Acc: 60.326% | Dur: 9.94s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  9.  8. 28.]
 [ 1. 37. 11.  2.]
 [ 3. 23. 40.  5.]
 [12.  9. 16. 51.]]

I - Epoch: 206
I - Training: 
	I - Batch: 50 | Loss: 0.397 | Acc: 99.250% | Wgt Acc: 99.213%
	I - Batch: 100 | Loss: 0.386 | Acc: 99.438% | Wgt Acc: 99.423%
	I - Batch: 150 | Loss: 0.383 | Acc: 99.500% | Wgt Acc: 99.493%
I - num batch: 160
I - Train -- Loss: 0.384 | Acc: 99.490% | Wgt Acc: 99.487% | LR: 1.250000e-04 | Dur: 87.27s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   3.]
 [  2. 577.   0.   1.]
 [  4.   0. 733.   1.]
 [  0.   1.   1. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.160 | Acc: 53.823% | Wgt Acc: 52.785% | Dur: 9.94s
I - Confusion Matrix: [row->prediction - col->label]
[[81. 18. 29. 47.]
 [ 3. 41. 13.  4.]
 [ 0. 15. 22.  3.]
 [ 4.  4. 11. 32.]]

I - Epoch: 207
I - Training: 
	I - Batch: 50 | Loss: 0.395 | Acc: 98.750% | Wgt Acc: 98.733%
	I - Batch: 100 | Loss: 0.384 | Acc: 99.375% | Wgt Acc: 99.367%
	I - Batch: 150 | Loss: 0.382 | Acc: 99.458% | Wgt Acc: 99.446%
I - num batch: 160
I - Train -- Loss: 0.380 | Acc: 99.490% | Wgt Acc: 99.478% | LR: 1.250000e-04 | Dur: 88.24s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   3.   2.]
 [  1. 575.   0.   1.]
 [  2.   1. 731.   1.]
 [  0.   1.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.040 | Acc: 59.633% | Wgt Acc: 59.035% | Dur: 9.78s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  7.  4. 26.]
 [ 6. 36. 14.  2.]
 [ 3. 25. 38.  5.]
 [11. 10. 19. 53.]]

I - Epoch: 208
I - Training: 
	I - Batch: 50 | Loss: 0.369 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.368 | Acc: 99.938% | Wgt Acc: 99.944%
	I - Batch: 150 | Loss: 0.372 | Acc: 99.792% | Wgt Acc: 99.784%
I - num batch: 160
I - Train -- Loss: 0.372 | Acc: 99.804% | Wgt Acc: 99.797% | LR: 1.250000e-04 | Dur: 84.42s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   2.]
 [  1. 578.   0.   0.]
 [  0.   0. 734.   1.]
 [  1.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 0.995 | Acc: 63.914% | Wgt Acc: 63.451% | Dur: 9.36s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  2.  2. 18.]
 [ 8. 43. 13.  4.]
 [ 6. 29. 49.  9.]
 [12.  4. 11. 55.]]

I - Epoch: 209
I - Training: 
	I - Batch: 50 | Loss: 0.374 | Acc: 99.750% | Wgt Acc: 99.717%
	I - Batch: 100 | Loss: 0.371 | Acc: 99.812% | Wgt Acc: 99.788%
	I - Batch: 150 | Loss: 0.371 | Acc: 99.750% | Wgt Acc: 99.737%
I - num batch: 160
I - Train -- Loss: 0.371 | Acc: 99.764% | Wgt Acc: 99.752% | LR: 1.250000e-04 | Dur: 79.84s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   2.]
 [  0. 577.   0.   0.]
 [  1.   1. 734.   1.]
 [  1.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.029 | Acc: 63.303% | Wgt Acc: 62.160% | Dur: 9.01s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  5.  7. 26.]
 [ 5. 42. 12.  4.]
 [ 2. 26. 47. 11.]
 [ 8.  5.  9. 45.]]

I - Epoch: 210
I - Training: 
	I - Batch: 50 | Loss: 0.375 | Acc: 99.750% | Wgt Acc: 99.718%
	I - Batch: 100 | Loss: 0.370 | Acc: 99.812% | Wgt Acc: 99.803%
	I - Batch: 150 | Loss: 0.368 | Acc: 99.875% | Wgt Acc: 99.869%
I - num batch: 160
I - Train -- Loss: 0.368 | Acc: 99.843% | Wgt Acc: 99.832% | LR: 1.250000e-04 | Dur: 83.49s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   3.]
 [  0. 578.   0.   0.]
 [  0.   0. 734.   0.]
 [  1.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.057 | Acc: 59.633% | Wgt Acc: 58.560% | Dur: 9.94s
I - Confusion Matrix: [row->prediction - col->label]
[[79. 11. 10. 33.]
 [ 1. 33. 10.  1.]
 [ 1. 24. 34.  3.]
 [ 7. 10. 21. 49.]]

I - Epoch: 211
I - Training: 
	I - Batch: 50 | Loss: 0.373 | Acc: 99.125% | Wgt Acc: 99.071%
	I - Batch: 100 | Loss: 0.376 | Acc: 99.438% | Wgt Acc: 99.407%
	I - Batch: 150 | Loss: 0.376 | Acc: 99.500% | Wgt Acc: 99.474%
I - num batch: 160
I - Train -- Loss: 0.375 | Acc: 99.529% | Wgt Acc: 99.505% | LR: 1.250000e-04 | Dur: 79.28s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   1.   0.   6.]
 [  0. 577.   0.   0.]
 [  2.   0. 734.   1.]
 [  2.   0.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.016 | Acc: 59.021% | Wgt Acc: 58.288% | Dur: 9.17s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  5. 20.]
 [ 3. 31. 10.  3.]
 [ 8. 32. 44.  8.]
 [14. 10. 16. 55.]]

I - Epoch: 212
I - Training: 
	I - Batch: 50 | Loss: 0.372 | Acc: 99.375% | Wgt Acc: 99.351%
	I - Batch: 100 | Loss: 0.371 | Acc: 99.625% | Wgt Acc: 99.606%
	I - Batch: 150 | Loss: 0.372 | Acc: 99.625% | Wgt Acc: 99.615%
I - num batch: 160
I - Train -- Loss: 0.373 | Acc: 99.607% | Wgt Acc: 99.593% | LR: 1.250000e-04 | Dur: 82.74s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   1.   2.]
 [  0. 576.   0.   0.]
 [  2.   1. 733.   2.]
 [  1.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.028 | Acc: 60.550% | Wgt Acc: 59.851% | Dur: 10.41s
I - Confusion Matrix: [row->prediction - col->label]
[[74. 10.  8. 28.]
 [ 2. 34. 10.  2.]
 [ 2. 21. 35.  1.]
 [10. 13. 22. 55.]]

I - Epoch: 213
I - Training: 
	I - Batch: 50 | Loss: 0.376 | Acc: 99.750% | Wgt Acc: 99.719%
	I - Batch: 100 | Loss: 0.373 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 150 | Loss: 0.374 | Acc: 99.750% | Wgt Acc: 99.737%
I - num batch: 160
I - Train -- Loss: 0.377 | Acc: 99.725% | Wgt Acc: 99.717% | LR: 1.250000e-04 | Dur: 81.32s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   3.]
 [  1. 577.   1.   0.]
 [  1.   1. 733.   0.]
 [  0.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.038 | Acc: 61.162% | Wgt Acc: 60.802% | Dur: 8.93s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  9. 10. 27.]
 [ 4. 46. 15.  6.]
 [ 1. 14. 32.  4.]
 [10.  9. 18. 49.]]

I - Epoch: 214
I - Training: 
	I - Batch: 50 | Loss: 0.415 | Acc: 99.250% | Wgt Acc: 99.158%
	I - Batch: 100 | Loss: 0.397 | Acc: 99.438% | Wgt Acc: 99.381%
	I - Batch: 150 | Loss: 0.391 | Acc: 99.542% | Wgt Acc: 99.493%
I - num batch: 160
I - Train -- Loss: 0.390 | Acc: 99.529% | Wgt Acc: 99.478% | LR: 1.250000e-04 | Dur: 82.06s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   9.]
 [  0. 578.   0.   2.]
 [  0.   0. 734.   0.]
 [  1.   0.   0. 527.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.028 | Acc: 63.303% | Wgt Acc: 62.568% | Dur: 8.92s
I - Confusion Matrix: [row->prediction - col->label]
[[76.  8.  8. 27.]
 [ 5. 42. 15.  3.]
 [ 0. 19. 38.  5.]
 [ 7.  9. 14. 51.]]

I - Epoch: 215
I - Training: 
	I - Batch: 50 | Loss: 0.374 | Acc: 99.375% | Wgt Acc: 99.322%
	I - Batch: 100 | Loss: 0.370 | Acc: 99.625% | Wgt Acc: 99.591%
	I - Batch: 150 | Loss: 0.370 | Acc: 99.667% | Wgt Acc: 99.634%
I - num batch: 160
I - Train -- Loss: 0.370 | Acc: 99.686% | Wgt Acc: 99.655% | LR: 1.250000e-04 | Dur: 79.82s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   5.]
 [  0. 578.   0.   0.]
 [  1.   0. 734.   2.]
 [  0.   0.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.048 | Acc: 59.021% | Wgt Acc: 58.288% | Dur: 8.96s
I - Confusion Matrix: [row->prediction - col->label]
[[71. 10. 10. 30.]
 [ 7. 41. 16.  3.]
 [ 4. 21. 36.  8.]
 [ 6.  6. 13. 45.]]

I - Epoch: 216
I - Training: 
	I - Batch: 50 | Loss: 0.365 | Acc: 99.875% | Wgt Acc: 99.888%
	I - Batch: 100 | Loss: 0.368 | Acc: 99.812% | Wgt Acc: 99.803%
	I - Batch: 150 | Loss: 0.369 | Acc: 99.792% | Wgt Acc: 99.793%
I - num batch: 160
I - Train -- Loss: 0.370 | Acc: 99.804% | Wgt Acc: 99.805% | LR: 1.250000e-04 | Dur: 80.03s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   1.   1.]
 [  1. 578.   0.   0.]
 [  1.   0. 733.   1.]
 [  0.   0.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.023 | Acc: 60.245% | Wgt Acc: 60.122% | Dur: 9.17s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  6.  4. 19.]
 [ 9. 36. 14.  1.]
 [ 3. 24. 38.  5.]
 [14. 12. 19. 61.]]

I - Epoch: 217
I - Training: 
	I - Batch: 50 | Loss: 0.363 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.365 | Acc: 99.812% | Wgt Acc: 99.817%
	I - Batch: 150 | Loss: 0.369 | Acc: 99.750% | Wgt Acc: 99.737%
I - num batch: 160
I - Train -- Loss: 0.369 | Acc: 99.764% | Wgt Acc: 99.752% | LR: 1.250000e-04 | Dur: 79.05s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   3.]
 [  1. 578.   0.   0.]
 [  0.   0. 734.   1.]
 [  1.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.015 | Acc: 63.303% | Wgt Acc: 62.976% | Dur: 9.05s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  7.  7. 25.]
 [ 3. 43. 10.  2.]
 [ 1. 20. 39.  3.]
 [15.  8. 19. 56.]]

I - Epoch: 218
I - Training: 
	I - Batch: 50 | Loss: 0.380 | Acc: 99.625% | Wgt Acc: 99.634%
	I - Batch: 100 | Loss: 0.374 | Acc: 99.812% | Wgt Acc: 99.817%
	I - Batch: 150 | Loss: 0.373 | Acc: 99.833% | Wgt Acc: 99.831%
I - num batch: 160
I - Train -- Loss: 0.374 | Acc: 99.764% | Wgt Acc: 99.752% | LR: 1.250000e-04 | Dur: 79.34s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   4.]
 [  1. 578.   0.   0.]
 [  0.   0. 734.   0.]
 [  1.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.025 | Acc: 61.774% | Wgt Acc: 61.345% | Dur: 10.04s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  5.  8. 22.]
 [ 5. 38. 14.  4.]
 [ 1. 25. 39.  3.]
 [14. 10. 14. 57.]]

I - Epoch: 219
I - Training: 
	I - Batch: 50 | Loss: 0.382 | Acc: 99.250% | Wgt Acc: 99.269%
	I - Batch: 100 | Loss: 0.376 | Acc: 99.562% | Wgt Acc: 99.577%
	I - Batch: 150 | Loss: 0.374 | Acc: 99.625% | Wgt Acc: 99.624%
I - num batch: 160
I - Train -- Loss: 0.374 | Acc: 99.647% | Wgt Acc: 99.646% | LR: 1.250000e-04 | Dur: 80.73s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   3.]
 [  0. 578.   0.   0.]
 [  4.   0. 733.   1.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.054 | Acc: 60.550% | Wgt Acc: 59.783% | Dur: 9.06s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  7.  6. 22.]
 [ 6. 35. 14.  2.]
 [ 4. 28. 44.  9.]
 [12.  8. 11. 53.]]

I - Epoch: 220
I - Training: 
	I - Batch: 50 | Loss: 0.374 | Acc: 99.625% | Wgt Acc: 99.579%
	I - Batch: 100 | Loss: 0.376 | Acc: 99.438% | Wgt Acc: 99.410%
	I - Batch: 150 | Loss: 0.373 | Acc: 99.625% | Wgt Acc: 99.606%
I - num batch: 160
I - Train -- Loss: 0.372 | Acc: 99.647% | Wgt Acc: 99.628% | LR: 1.250000e-04 | Dur: 80.20s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   4.]
 [  1. 577.   0.   0.]
 [  1.   1. 734.   1.]
 [  1.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.077 | Acc: 60.550% | Wgt Acc: 59.511% | Dur: 9.18s
I - Confusion Matrix: [row->prediction - col->label]
[[78. 13. 11. 28.]
 [ 2. 31. 10.  3.]
 [ 0. 23. 36.  2.]
 [ 8. 11. 18. 53.]]

I - Epoch: 221
I - Training: 
	I - Batch: 50 | Loss: 0.367 | Acc: 99.875% | Wgt Acc: 99.860%
	I - Batch: 100 | Loss: 0.369 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 150 | Loss: 0.370 | Acc: 99.750% | Wgt Acc: 99.718%
I - num batch: 160
I - Train -- Loss: 0.370 | Acc: 99.764% | Wgt Acc: 99.735% | LR: 1.250000e-04 | Dur: 79.18s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   5.]
 [  0. 578.   0.   0.]
 [  0.   0. 734.   1.]
 [  0.   0.   0. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.025 | Acc: 62.691% | Wgt Acc: 62.296% | Dur: 10.17s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  5.  3. 21.]
 [ 4. 40. 14.  1.]
 [ 2. 23. 41.  7.]
 [15. 10. 17. 57.]]

I - Epoch: 222
I - Training: 
	I - Batch: 50 | Loss: 0.364 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.363 | Acc: 99.875% | Wgt Acc: 99.873%
	I - Batch: 150 | Loss: 0.365 | Acc: 99.792% | Wgt Acc: 99.784%
I - num batch: 160
I - Train -- Loss: 0.365 | Acc: 99.804% | Wgt Acc: 99.797% | LR: 1.250000e-04 | Dur: 80.10s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   2.]
 [  0. 577.   0.   0.]
 [  2.   1. 734.   0.]
 [  0.   0.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.072 | Acc: 61.468% | Wgt Acc: 59.986% | Dur: 8.86s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  6.  9. 32.]
 [ 4. 39. 14.  5.]
 [ 4. 31. 47.  9.]
 [ 5.  2.  5. 40.]]

I - Epoch: 223
I - Training: 
	I - Batch: 50 | Loss: 0.369 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.373 | Acc: 99.625% | Wgt Acc: 99.592%
	I - Batch: 150 | Loss: 0.370 | Acc: 99.750% | Wgt Acc: 99.728%
I - num batch: 160
I - Train -- Loss: 0.369 | Acc: 99.764% | Wgt Acc: 99.743% | LR: 1.250000e-04 | Dur: 80.37s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   3.]
 [  0. 577.   0.   0.]
 [  0.   1. 734.   1.]
 [  1.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.065 | Acc: 60.856% | Wgt Acc: 59.851% | Dur: 9.12s
I - Confusion Matrix: [row->prediction - col->label]
[[73. 11. 10. 30.]
 [ 4. 38. 13.  2.]
 [ 3. 23. 41.  7.]
 [ 8.  6. 11. 47.]]

I - Epoch: 224
I - Training: 
	I - Batch: 50 | Loss: 0.367 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 100 | Loss: 0.367 | Acc: 99.812% | Wgt Acc: 99.803%
	I - Batch: 150 | Loss: 0.367 | Acc: 99.708% | Wgt Acc: 99.690%
I - num batch: 160
I - Train -- Loss: 0.370 | Acc: 99.725% | Wgt Acc: 99.708% | LR: 1.250000e-04 | Dur: 80.00s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   4.]
 [  0. 578.   0.   0.]
 [  2.   0. 734.   1.]
 [  0.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.037 | Acc: 59.327% | Wgt Acc: 58.356% | Dur: 9.33s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  8.  9. 31.]
 [ 4. 35. 19.  3.]
 [ 2. 26. 37.  4.]
 [ 8.  9. 10. 48.]]

I - Epoch: 225
I - Training: 
	I - Batch: 50 | Loss: 0.376 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.375 | Acc: 99.562% | Wgt Acc: 99.536%
	I - Batch: 150 | Loss: 0.373 | Acc: 99.583% | Wgt Acc: 99.558%
I - num batch: 160
I - Train -- Loss: 0.373 | Acc: 99.568% | Wgt Acc: 99.540% | LR: 1.250000e-04 | Dur: 81.17s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   6.]
 [  1. 578.   0.   0.]
 [  0.   0. 734.   2.]
 [  2.   0.   0. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.012 | Acc: 64.832% | Wgt Acc: 64.130% | Dur: 8.97s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7.  6. 23.]
 [ 4. 40. 15.  2.]
 [ 2. 25. 41.  5.]
 [ 7.  6. 13. 56.]]

I - Epoch: 226
I - Training: 
	I - Batch: 50 | Loss: 0.367 | Acc: 99.875% | Wgt Acc: 99.887%
	I - Batch: 100 | Loss: 0.366 | Acc: 99.812% | Wgt Acc: 99.817%
	I - Batch: 150 | Loss: 0.368 | Acc: 99.708% | Wgt Acc: 99.690%
I - num batch: 160
I - Train -- Loss: 0.368 | Acc: 99.725% | Wgt Acc: 99.708% | LR: 1.250000e-04 | Dur: 78.58s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   3.]
 [  2. 578.   0.   0.]
 [  0.   0. 734.   2.]
 [  0.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.022 | Acc: 64.832% | Wgt Acc: 64.130% | Dur: 8.96s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  7.  7. 23.]
 [ 5. 42. 11.  3.]
 [ 0. 21. 43.  6.]
 [10.  8. 14. 54.]]

I - Epoch: 227
I - Training: 
	I - Batch: 50 | Loss: 0.362 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.367 | Acc: 99.750% | Wgt Acc: 99.733%
	I - Batch: 150 | Loss: 0.369 | Acc: 99.708% | Wgt Acc: 99.690%
I - num batch: 160
I - Train -- Loss: 0.369 | Acc: 99.725% | Wgt Acc: 99.708% | LR: 1.250000e-04 | Dur: 78.21s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   4.]
 [  1. 578.   0.   0.]
 [  1.   0. 734.   1.]
 [  0.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.044 | Acc: 62.080% | Wgt Acc: 60.802% | Dur: 9.12s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  4.  7. 27.]
 [ 6. 40. 12.  0.]
 [ 3. 30. 48. 16.]
 [ 7.  4.  8. 43.]]

I - Epoch: 228
I - Training: 
	I - Batch: 50 | Loss: 0.368 | Acc: 99.750% | Wgt Acc: 99.718%
	I - Batch: 100 | Loss: 0.366 | Acc: 99.812% | Wgt Acc: 99.803%
	I - Batch: 150 | Loss: 0.366 | Acc: 99.875% | Wgt Acc: 99.869%
I - num batch: 160
I - Train -- Loss: 0.368 | Acc: 99.843% | Wgt Acc: 99.832% | LR: 1.250000e-04 | Dur: 79.35s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   2.]
 [  0. 577.   0.   0.]
 [  0.   1. 734.   0.]
 [  1.   0.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.049 | Acc: 63.914% | Wgt Acc: 62.500% | Dur: 8.80s
I - Confusion Matrix: [row->prediction - col->label]
[[78. 11.  8. 32.]
 [ 3. 38. 10.  3.]
 [ 3. 22. 47.  5.]
 [ 4.  7. 10. 46.]]

I - Epoch: 229
I - Training: 
	I - Batch: 50 | Loss: 0.369 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.368 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 150 | Loss: 0.368 | Acc: 99.833% | Wgt Acc: 99.812%
I - num batch: 160
I - Train -- Loss: 0.369 | Acc: 99.843% | Wgt Acc: 99.823% | LR: 1.250000e-04 | Dur: 79.85s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   3.]
 [  0. 578.   0.   0.]
 [  0.   0. 734.   1.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 0.987 | Acc: 61.468% | Wgt Acc: 60.870% | Dur: 9.01s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  4. 20.]
 [ 2. 31. 12.  2.]
 [ 1. 27. 39.  3.]
 [15. 13. 20. 61.]]

I - Epoch: 230
I - Training: 
	I - Batch: 50 | Loss: 0.410 | Acc: 98.000% | Wgt Acc: 98.034%
	I - Batch: 100 | Loss: 0.394 | Acc: 98.875% | Wgt Acc: 98.876%
	I - Batch: 150 | Loss: 0.386 | Acc: 99.208% | Wgt Acc: 99.203%
I - num batch: 160
I - Train -- Loss: 0.385 | Acc: 99.254% | Wgt Acc: 99.248% | LR: 1.250000e-04 | Dur: 79.90s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   5.]
 [  1. 574.   1.   0.]
 [  1.   2. 731.   0.]
 [  5.   2.   2. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.046 | Acc: 62.691% | Wgt Acc: 61.549% | Dur: 9.18s
I - Confusion Matrix: [row->prediction - col->label]
[[69. 11.  6. 20.]
 [ 5. 33.  7.  1.]
 [ 2. 28. 50. 12.]
 [12.  6. 12. 53.]]

I - Epoch: 231
I - Training: 
	I - Batch: 50 | Loss: 0.369 | Acc: 99.625% | Wgt Acc: 99.605%
	I - Batch: 100 | Loss: 0.367 | Acc: 99.688% | Wgt Acc: 99.677%
	I - Batch: 150 | Loss: 0.368 | Acc: 99.750% | Wgt Acc: 99.747%
I - num batch: 160
I - Train -- Loss: 0.368 | Acc: 99.725% | Wgt Acc: 99.726% | LR: 1.250000e-04 | Dur: 77.73s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 578.   0.   0.]
 [  2.   0. 734.   1.]
 [  2.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.005 | Acc: 64.220% | Wgt Acc: 63.791% | Dur: 8.94s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  2.  6. 23.]
 [ 4. 44. 13.  2.]
 [ 4. 27. 42.  6.]
 [11.  5. 14. 55.]]

I - Epoch: 232
I - Training: 
	I - Batch: 50 | Loss: 0.360 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.362 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 150 | Loss: 0.364 | Acc: 99.875% | Wgt Acc: 99.859%
I - num batch: 160
I - Train -- Loss: 0.364 | Acc: 99.882% | Wgt Acc: 99.867% | LR: 1.250000e-04 | Dur: 78.59s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   2.]
 [  0. 578.   0.   0.]
 [  0.   0. 734.   1.]
 [  0.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.051 | Acc: 60.856% | Wgt Acc: 59.579% | Dur: 8.18s
I - Confusion Matrix: [row->prediction - col->label]
[[79. 11. 10. 31.]
 [ 2. 28. 13.  0.]
 [ 2. 30. 39.  2.]
 [ 5.  9. 13. 53.]]

I - Epoch: 233
I - Training: 
	I - Batch: 50 | Loss: 0.364 | Acc: 99.875% | Wgt Acc: 99.860%
	I - Batch: 100 | Loss: 0.364 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 150 | Loss: 0.365 | Acc: 99.833% | Wgt Acc: 99.812%
I - num batch: 160
I - Train -- Loss: 0.369 | Acc: 99.804% | Wgt Acc: 99.788% | LR: 1.250000e-04 | Dur: 72.23s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   1.   3.]
 [  0. 578.   0.   0.]
 [  0.   0. 733.   1.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.085 | Acc: 56.575% | Wgt Acc: 55.910% | Dur: 8.24s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  1.  2. 15.]
 [ 2. 29. 12.  1.]
 [11. 37. 44. 16.]
 [17. 11. 17. 54.]]

I - Epoch: 234
I - Training: 
	I - Batch: 50 | Loss: 0.423 | Acc: 97.500% | Wgt Acc: 97.523%
	I - Batch: 100 | Loss: 0.397 | Acc: 98.688% | Wgt Acc: 98.703%
	I - Batch: 150 | Loss: 0.389 | Acc: 98.958% | Wgt Acc: 98.966%
I - num batch: 160
I - Train -- Loss: 0.388 | Acc: 99.018% | Wgt Acc: 99.027% | LR: 1.250000e-04 | Dur: 72.54s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   1.   2.   5.]
 [  1. 576.   2.   0.]
 [  7.   1. 729.   3.]
 [  2.   0.   1. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.077 | Acc: 56.269% | Wgt Acc: 55.639% | Dur: 8.10s
I - Confusion Matrix: [row->prediction - col->label]
[[72. 11. 12. 29.]
 [ 2. 34. 17.  4.]
 [ 2. 25. 29.  4.]
 [12.  8. 17. 49.]]

I - Epoch: 235
I - Training: 
	I - Batch: 50 | Loss: 0.382 | Acc: 99.375% | Wgt Acc: 99.324%
	I - Batch: 100 | Loss: 0.376 | Acc: 99.625% | Wgt Acc: 99.592%
	I - Batch: 150 | Loss: 0.374 | Acc: 99.667% | Wgt Acc: 99.634%
I - num batch: 160
I - Train -- Loss: 0.373 | Acc: 99.686% | Wgt Acc: 99.655% | LR: 1.250000e-04 | Dur: 71.10s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   4.]
 [  1. 578.   0.   1.]
 [  0.   0. 734.   2.]
 [  0.   0.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.050 | Acc: 60.856% | Wgt Acc: 59.918% | Dur: 8.07s
I - Confusion Matrix: [row->prediction - col->label]
[[76. 12. 11. 32.]
 [ 5. 41. 15.  6.]
 [ 1. 16. 37.  3.]
 [ 6.  9. 12. 45.]]

I - Epoch: 236
I - Training: 
	I - Batch: 50 | Loss: 0.364 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.365 | Acc: 99.750% | Wgt Acc: 99.719%
	I - Batch: 150 | Loss: 0.365 | Acc: 99.792% | Wgt Acc: 99.765%
I - num batch: 160
I - Train -- Loss: 0.365 | Acc: 99.804% | Wgt Acc: 99.779% | LR: 1.250000e-04 | Dur: 70.90s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   5.]
 [  0. 578.   0.   0.]
 [  0.   0. 734.   0.]
 [  0.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.042 | Acc: 60.856% | Wgt Acc: 59.851% | Dur: 8.10s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  5.  6. 27.]
 [ 5. 36. 14.  1.]
 [ 7. 33. 44.  9.]
 [ 6.  4. 11. 49.]]

I - Epoch: 237
I - Training: 
	I - Batch: 50 | Loss: 0.361 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.365 | Acc: 99.812% | Wgt Acc: 99.788%
	I - Batch: 150 | Loss: 0.363 | Acc: 99.875% | Wgt Acc: 99.859%
I - num batch: 160
I - Train -- Loss: 0.363 | Acc: 99.882% | Wgt Acc: 99.867% | LR: 1.250000e-04 | Dur: 70.96s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   3.]
 [  0. 578.   0.   0.]
 [  0.   0. 734.   0.]
 [  0.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.017 | Acc: 64.526% | Wgt Acc: 63.587% | Dur: 8.09s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  5.  4. 20.]
 [ 3. 40. 14.  3.]
 [ 4. 27. 46. 11.]
 [ 8.  6. 11. 52.]]

I - Epoch: 238
I - Training: 
	I - Batch: 50 | Loss: 0.360 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.366 | Acc: 99.750% | Wgt Acc: 99.718%
	I - Batch: 150 | Loss: 0.365 | Acc: 99.792% | Wgt Acc: 99.765%
I - num batch: 160
I - Train -- Loss: 0.366 | Acc: 99.804% | Wgt Acc: 99.779% | LR: 1.250000e-04 | Dur: 70.90s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   4.]
 [  0. 578.   0.   0.]
 [  0.   0. 734.   1.]
 [  0.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.019 | Acc: 64.220% | Wgt Acc: 63.791% | Dur: 8.12s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  3.  6. 20.]
 [ 6. 45. 16.  4.]
 [ 6. 24. 43.  8.]
 [ 8.  6. 10. 54.]]

I - Epoch: 239
I - Training: 
	I - Batch: 50 | Loss: 0.373 | Acc: 99.625% | Wgt Acc: 99.577%
	I - Batch: 100 | Loss: 0.369 | Acc: 99.750% | Wgt Acc: 99.718%
	I - Batch: 150 | Loss: 0.368 | Acc: 99.750% | Wgt Acc: 99.728%
I - num batch: 160
I - Train -- Loss: 0.369 | Acc: 99.764% | Wgt Acc: 99.743% | LR: 1.250000e-04 | Dur: 70.90s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   5.]
 [  0. 578.   0.   0.]
 [  0.   0. 734.   0.]
 [  1.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.088 | Acc: 59.021% | Wgt Acc: 57.473% | Dur: 9.19s
I - Confusion Matrix: [row->prediction - col->label]
[[77.  9.  5. 34.]
 [ 5. 40. 20.  7.]
 [ 2. 27. 42. 11.]
 [ 4.  2.  8. 34.]]

I - Epoch: 240
I - Training: 
	I - Batch: 50 | Loss: 0.392 | Acc: 99.125% | Wgt Acc: 99.095%
	I - Batch: 100 | Loss: 0.383 | Acc: 99.375% | Wgt Acc: 99.352%
	I - Batch: 150 | Loss: 0.378 | Acc: 99.458% | Wgt Acc: 99.437%
I - num batch: 160
I - Train -- Loss: 0.377 | Acc: 99.450% | Wgt Acc: 99.425% | LR: 1.250000e-04 | Dur: 85.06s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   7.]
 [  3. 577.   0.   0.]
 [  1.   0. 733.   1.]
 [  0.   1.   1. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.041 | Acc: 62.385% | Wgt Acc: 61.481% | Dur: 9.97s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  3.  6. 19.]
 [ 5. 38. 11.  0.]
 [ 5. 30. 47. 16.]
 [10.  7. 11. 51.]]

I - Epoch: 241
I - Training: 
	I - Batch: 50 | Loss: 0.368 | Acc: 99.500% | Wgt Acc: 99.436%
	I - Batch: 100 | Loss: 0.368 | Acc: 99.688% | Wgt Acc: 99.648%
	I - Batch: 150 | Loss: 0.366 | Acc: 99.750% | Wgt Acc: 99.718%
I - num batch: 160
I - Train -- Loss: 0.366 | Acc: 99.764% | Wgt Acc: 99.735% | LR: 1.250000e-04 | Dur: 79.07s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   4.]
 [  0. 577.   0.   0.]
 [  0.   1. 734.   1.]
 [  0.   0.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.083 | Acc: 61.162% | Wgt Acc: 59.783% | Dur: 8.32s
I - Confusion Matrix: [row->prediction - col->label]
[[75. 10.  7. 29.]
 [ 4. 36. 13.  6.]
 [ 2. 25. 45.  7.]
 [ 7.  7. 10. 44.]]

I - Epoch: 242
I - Training: 
	I - Batch: 50 | Loss: 0.360 | Acc: 99.875% | Wgt Acc: 99.887%
	I - Batch: 100 | Loss: 0.360 | Acc: 99.938% | Wgt Acc: 99.943%
	I - Batch: 150 | Loss: 0.365 | Acc: 99.833% | Wgt Acc: 99.822%
I - num batch: 160
I - Train -- Loss: 0.365 | Acc: 99.843% | Wgt Acc: 99.832% | LR: 1.250000e-04 | Dur: 78.87s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   2.]
 [  0. 578.   0.   0.]
 [  1.   0. 734.   1.]
 [  0.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.024 | Acc: 60.856% | Wgt Acc: 60.122% | Dur: 8.32s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  3. 23.]
 [ 8. 40. 16.  2.]
 [ 4. 27. 47. 12.]
 [13.  7.  9. 49.]]

I - Epoch: 243
I - Training: 
	I - Batch: 50 | Loss: 0.361 | Acc: 99.875% | Wgt Acc: 99.887%
	I - Batch: 100 | Loss: 0.360 | Acc: 99.938% | Wgt Acc: 99.944%
	I - Batch: 150 | Loss: 0.365 | Acc: 99.792% | Wgt Acc: 99.775%
I - num batch: 160
I - Train -- Loss: 0.365 | Acc: 99.804% | Wgt Acc: 99.788% | LR: 1.250000e-04 | Dur: 77.36s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   4.]
 [  1. 578.   0.   0.]
 [  0.   0. 734.   0.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.032 | Acc: 62.385% | Wgt Acc: 61.413% | Dur: 8.26s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  5. 22.]
 [ 7. 39. 12.  3.]
 [ 4. 26. 50. 12.]
 [11.  7.  8. 49.]]

I - Epoch: 244
I - Training: 
	I - Batch: 50 | Loss: 0.366 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 100 | Loss: 0.364 | Acc: 99.875% | Wgt Acc: 99.873%
	I - Batch: 150 | Loss: 0.363 | Acc: 99.833% | Wgt Acc: 99.840%
I - num batch: 160
I - Train -- Loss: 0.363 | Acc: 99.843% | Wgt Acc: 99.850% | LR: 1.250000e-04 | Dur: 72.15s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   1.   1.]
 [  1. 578.   0.   0.]
 [  1.   0. 733.   0.]
 [  0.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.014 | Acc: 63.914% | Wgt Acc: 62.840% | Dur: 8.70s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7.  6. 30.]
 [ 3. 40.  8.  2.]
 [ 1. 24. 45.  5.]
 [ 9.  7. 16. 49.]]

I - Epoch: 245
I - Training: 
	I - Batch: 50 | Loss: 0.370 | Acc: 99.625% | Wgt Acc: 99.607%
	I - Batch: 100 | Loss: 0.368 | Acc: 99.688% | Wgt Acc: 99.662%
	I - Batch: 150 | Loss: 0.366 | Acc: 99.750% | Wgt Acc: 99.737%
I - num batch: 160
I - Train -- Loss: 0.366 | Acc: 99.764% | Wgt Acc: 99.752% | LR: 1.250000e-04 | Dur: 73.69s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   4.]
 [  1. 578.   0.   0.]
 [  1.   0. 734.   0.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.016 | Acc: 64.526% | Wgt Acc: 63.655% | Dur: 8.18s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  7.  7. 23.]
 [ 6. 40. 10.  3.]
 [ 1. 22. 44.  7.]
 [ 7.  9. 14. 53.]]

I - Epoch: 246
I - Training: 
	I - Batch: 50 | Loss: 0.364 | Acc: 99.625% | Wgt Acc: 99.636%
	I - Batch: 100 | Loss: 0.364 | Acc: 99.750% | Wgt Acc: 99.761%
	I - Batch: 150 | Loss: 0.364 | Acc: 99.792% | Wgt Acc: 99.793%
I - num batch: 160
I - Train -- Loss: 0.364 | Acc: 99.764% | Wgt Acc: 99.761% | LR: 1.250000e-04 | Dur: 78.19s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   3.]
 [  1. 578.   0.   0.]
 [  2.   0. 734.   0.]
 [  0.   0.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.047 | Acc: 59.327% | Wgt Acc: 58.832% | Dur: 8.31s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  7.  5. 17.]
 [ 3. 34. 13.  1.]
 [ 7. 28. 43. 12.]
 [17.  9. 14. 56.]]

I - Epoch: 247
I - Training: 
	I - Batch: 50 | Loss: 0.360 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.364 | Acc: 99.688% | Wgt Acc: 99.676%
	I - Batch: 150 | Loss: 0.364 | Acc: 99.708% | Wgt Acc: 99.699%
I - num batch: 160
I - Train -- Loss: 0.364 | Acc: 99.725% | Wgt Acc: 99.717% | LR: 1.250000e-04 | Dur: 74.05s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   4.]
 [  1. 578.   0.   0.]
 [  2.   0. 734.   0.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.047 | Acc: 56.881% | Wgt Acc: 55.639% | Dur: 8.53s
I - Confusion Matrix: [row->prediction - col->label]
[[76. 10. 12. 35.]
 [ 3. 32.  9.  3.]
 [ 1. 21. 35.  5.]
 [ 8. 15. 19. 43.]]

I - Epoch: 248
I - Training: 
	I - Batch: 50 | Loss: 0.373 | Acc: 99.500% | Wgt Acc: 99.466%
	I - Batch: 100 | Loss: 0.368 | Acc: 99.688% | Wgt Acc: 99.662%
	I - Batch: 150 | Loss: 0.366 | Acc: 99.750% | Wgt Acc: 99.728%
I - num batch: 160
I - Train -- Loss: 0.366 | Acc: 99.725% | Wgt Acc: 99.699% | LR: 1.250000e-04 | Dur: 72.28s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   3.]
 [  1. 578.   0.   1.]
 [  0.   0. 734.   2.]
 [  0.   0.   0. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.036 | Acc: 63.303% | Wgt Acc: 62.024% | Dur: 8.13s
I - Confusion Matrix: [row->prediction - col->label]
[[78. 10.  9. 30.]
 [ 2. 40. 11.  2.]
 [ 3. 22. 44.  9.]
 [ 5.  6. 11. 45.]]

I - Epoch: 249
I - Training: 
	I - Batch: 50 | Loss: 0.359 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.361 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 150 | Loss: 0.362 | Acc: 99.792% | Wgt Acc: 99.775%
I - num batch: 160
I - Train -- Loss: 0.363 | Acc: 99.804% | Wgt Acc: 99.788% | LR: 1.250000e-04 | Dur: 71.53s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   3.]
 [  0. 578.   0.   0.]
 [  1.   0. 734.   1.]
 [  0.   0.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 1.049 | Acc: 58.716% | Wgt Acc: 58.016% | Dur: 8.17s
I - Confusion Matrix: [row->prediction - col->label]
[[75. 16.  9. 26.]
 [ 1. 28. 11.  0.]
 [ 0. 20. 31.  2.]
 [12. 14. 24. 58.]]

I - Maximum validation set accuracy in current training:  65.44
