Sun Oct 16 14:52:14 2022
I - CONFIGURATION: {'batchSize': 1, 'bias': True, 'classWeights': [0.2, 0.25, 0.2, 0.25, 0.08], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'tossFirstLastFrames': True}, 'dataPath': '/data/processed/Kinetics/', 'dropoutRate': 0.5, 'epochNo': 250, 'foldRatio': 1, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat'], 'lastLayerInitUniform': True, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 66.05504587155963, 'maxValidationTrainNo': 31, 'modelVersion': 5, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 34, 'validationAccThr': 65, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Train set length:  2547
I - Label distribution: [697. 578. 734. 538.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/test
I - Test set length:  327
I - Label distribution: [88. 78. 75. 86.]
I - Batch size:  1  tensor shape:  torch.Size([16, 3, 256, 256])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(0) data number in dataset:  tensor([30765])
I - Initializing model TCNv5
I - Number of Model Parameters: 772064
I - Model output shape:  torch.Size([1, 4])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv5                                    [1, 4]                    --
├─FusedConv2dBNReLU: 1-1                 [1, 64, 256, 256]         262
│    └─OutputShiftSqueeze: 2-1           --                        --
│    └─One: 2-2                          [1]                       --
│    └─OutputScale: 2-3                  --                        --
│    └─Empty: 2-4                        [64, 3, 1, 1]             --
│    └─Empty: 2-5                        [64, 3, 1, 1]             --
│    └─Empty: 2-6                        [64]                      --
│    └─Empty: 2-7                        [64]                      --
│    └─BatchNorm2d: 2-8                  [1, 64, 256, 256]         --
│    └─Scaler: 2-9                       [1, 64, 256, 256]         --
│    └─ReLU: 2-10                        [1, 64, 256, 256]         --
│    └─Empty: 2-11                       [1, 64, 256, 256]         --
│    └─Clamp: 2-12                       [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-2                 [1, 64, 256, 256]         36,934
│    └─OutputShiftSqueeze: 2-13          --                        --
│    └─One: 2-14                         [1]                       --
│    └─OutputScale: 2-15                 --                        --
│    └─Empty: 2-16                       [64, 64, 3, 3]            --
│    └─Empty: 2-17                       [64, 64, 3, 3]            --
│    └─Empty: 2-18                       [64]                      --
│    └─Empty: 2-19                       [64]                      --
│    └─BatchNorm2d: 2-20                 [1, 64, 256, 256]         --
│    └─Scaler: 2-21                      [1, 64, 256, 256]         --
│    └─ReLU: 2-22                        [1, 64, 256, 256]         --
│    └─Empty: 2-23                       [1, 64, 256, 256]         --
│    └─Clamp: 2-24                       [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-3                 [1, 64, 256, 256]         4,166
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [64, 64, 1, 1]            --
│    └─Empty: 2-29                       [64, 64, 1, 1]            --
│    └─Empty: 2-30                       [64]                      --
│    └─Empty: 2-31                       [64]                      --
│    └─BatchNorm2d: 2-32                 [1, 64, 256, 256]         --
│    └─Scaler: 2-33                      [1, 64, 256, 256]         --
│    └─ReLU: 2-34                        [1, 64, 256, 256]         --
│    └─Empty: 2-35                       [1, 64, 256, 256]         --
│    └─Clamp: 2-36                       [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-4                 [1, 64, 256, 256]         36,934
│    └─OutputShiftSqueeze: 2-37          --                        --
│    └─One: 2-38                         [1]                       --
│    └─OutputScale: 2-39                 --                        --
│    └─Empty: 2-40                       [64, 64, 3, 3]            --
│    └─Empty: 2-41                       [64, 64, 3, 3]            --
│    └─Empty: 2-42                       [64]                      --
│    └─Empty: 2-43                       [64]                      --
│    └─BatchNorm2d: 2-44                 [1, 64, 256, 256]         --
│    └─Scaler: 2-45                      [1, 64, 256, 256]         --
│    └─ReLU: 2-46                        [1, 64, 256, 256]         --
│    └─Empty: 2-47                       [1, 64, 256, 256]         --
│    └─Clamp: 2-48                       [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-5          [1, 64, 128, 128]         36,934
│    └─MaxPool2d: 2-49                   [1, 64, 128, 128]         --
│    └─Empty: 2-50                       [1, 64, 128, 128]         --
│    └─Empty: 2-51                       [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-52          --                        --
│    └─One: 2-53                         [1]                       --
│    └─OutputScale: 2-54                 --                        --
│    └─Empty: 2-55                       [64, 64, 3, 3]            --
│    └─Empty: 2-56                       [64, 64, 3, 3]            --
│    └─Empty: 2-57                       [64]                      --
│    └─Empty: 2-58                       [64]                      --
│    └─BatchNorm2d: 2-59                 [1, 64, 128, 128]         --
│    └─Scaler: 2-60                      [1, 64, 128, 128]         --
│    └─ReLU: 2-61                        [1, 64, 128, 128]         --
│    └─Empty: 2-62                       [1, 64, 128, 128]         --
│    └─Clamp: 2-63                       [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-6                 [1, 64, 128, 128]         36,934
│    └─OutputShiftSqueeze: 2-64          --                        --
│    └─One: 2-65                         [1]                       --
│    └─OutputScale: 2-66                 --                        --
│    └─Empty: 2-67                       [64, 64, 3, 3]            --
│    └─Empty: 2-68                       [64, 64, 3, 3]            --
│    └─Empty: 2-69                       [64]                      --
│    └─Empty: 2-70                       [64]                      --
│    └─BatchNorm2d: 2-71                 [1, 64, 128, 128]         --
│    └─Scaler: 2-72                      [1, 64, 128, 128]         --
│    └─ReLU: 2-73                        [1, 64, 128, 128]         --
│    └─Empty: 2-74                       [1, 64, 128, 128]         --
│    └─Clamp: 2-75                       [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-7          [1, 64, 64, 64]           36,934
│    └─MaxPool2d: 2-76                   [1, 64, 64, 64]           --
│    └─Empty: 2-77                       [1, 64, 64, 64]           --
│    └─Empty: 2-78                       [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-79          --                        --
│    └─One: 2-80                         [1]                       --
│    └─OutputScale: 2-81                 --                        --
│    └─Empty: 2-82                       [64, 64, 3, 3]            --
│    └─Empty: 2-83                       [64, 64, 3, 3]            --
│    └─Empty: 2-84                       [64]                      --
│    └─Empty: 2-85                       [64]                      --
│    └─BatchNorm2d: 2-86                 [1, 64, 64, 64]           --
│    └─Scaler: 2-87                      [1, 64, 64, 64]           --
│    └─ReLU: 2-88                        [1, 64, 64, 64]           --
│    └─Empty: 2-89                       [1, 64, 64, 64]           --
│    └─Clamp: 2-90                       [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-8                 [1, 64, 64, 64]           36,934
│    └─OutputShiftSqueeze: 2-91          --                        --
│    └─One: 2-92                         [1]                       --
│    └─OutputScale: 2-93                 --                        --
│    └─Empty: 2-94                       [64, 64, 3, 3]            --
│    └─Empty: 2-95                       [64, 64, 3, 3]            --
│    └─Empty: 2-96                       [64]                      --
│    └─Empty: 2-97                       [64]                      --
│    └─BatchNorm2d: 2-98                 [1, 64, 64, 64]           --
│    └─Scaler: 2-99                      [1, 64, 64, 64]           --
│    └─ReLU: 2-100                       [1, 64, 64, 64]           --
│    └─Empty: 2-101                      [1, 64, 64, 64]           --
│    └─Clamp: 2-102                      [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-9          [1, 64, 32, 32]           36,934
│    └─MaxPool2d: 2-103                  [1, 64, 32, 32]           --
│    └─Empty: 2-104                      [1, 64, 32, 32]           --
│    └─Empty: 2-105                      [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-106         --                        --
│    └─One: 2-107                        [1]                       --
│    └─OutputScale: 2-108                --                        --
│    └─Empty: 2-109                      [64, 64, 3, 3]            --
│    └─Empty: 2-110                      [64, 64, 3, 3]            --
│    └─Empty: 2-111                      [64]                      --
│    └─Empty: 2-112                      [64]                      --
│    └─BatchNorm2d: 2-113                [1, 64, 32, 32]           --
│    └─Scaler: 2-114                     [1, 64, 32, 32]           --
│    └─ReLU: 2-115                       [1, 64, 32, 32]           --
│    └─Empty: 2-116                      [1, 64, 32, 32]           --
│    └─Clamp: 2-117                      [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-10                [1, 64, 32, 32]           4,166
│    └─OutputShiftSqueeze: 2-118         --                        --
│    └─One: 2-119                        [1]                       --
│    └─OutputScale: 2-120                --                        --
│    └─Empty: 2-121                      [64, 64, 1, 1]            --
│    └─Empty: 2-122                      [64, 64, 1, 1]            --
│    └─Empty: 2-123                      [64]                      --
│    └─Empty: 2-124                      [64]                      --
│    └─BatchNorm2d: 2-125                [1, 64, 32, 32]           --
│    └─Scaler: 2-126                     [1, 64, 32, 32]           --
│    └─ReLU: 2-127                       [1, 64, 32, 32]           --
│    └─Empty: 2-128                      [1, 64, 32, 32]           --
│    └─Clamp: 2-129                      [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-11         [1, 64, 32, 32]           36,934
│    └─MaxPool2d: 2-130                  [1, 64, 32, 32]           --
│    └─Empty: 2-131                      [1, 64, 32, 32]           --
│    └─Empty: 2-132                      [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-133         --                        --
│    └─One: 2-134                        [1]                       --
│    └─OutputScale: 2-135                --                        --
│    └─Empty: 2-136                      [64, 64, 3, 3]            --
│    └─Empty: 2-137                      [64, 64, 3, 3]            --
│    └─Empty: 2-138                      [64]                      --
│    └─Empty: 2-139                      [64]                      --
│    └─BatchNorm2d: 2-140                [1, 64, 32, 32]           --
│    └─Scaler: 2-141                     [1, 64, 32, 32]           --
│    └─ReLU: 2-142                       [1, 64, 32, 32]           --
│    └─Empty: 2-143                      [1, 64, 32, 32]           --
│    └─Clamp: 2-144                      [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-12         [1, 64, 16, 16]           36,934
│    └─MaxPool2d: 2-145                  [1, 64, 16, 16]           --
│    └─Empty: 2-146                      [1, 64, 16, 16]           --
│    └─Empty: 2-147                      [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-148         --                        --
│    └─One: 2-149                        [1]                       --
│    └─OutputScale: 2-150                --                        --
│    └─Empty: 2-151                      [64, 64, 3, 3]            --
│    └─Empty: 2-152                      [64, 64, 3, 3]            --
│    └─Empty: 2-153                      [64]                      --
│    └─Empty: 2-154                      [64]                      --
│    └─BatchNorm2d: 2-155                [1, 64, 16, 16]           --
│    └─Scaler: 2-156                     [1, 64, 16, 16]           --
│    └─ReLU: 2-157                       [1, 64, 16, 16]           --
│    └─Empty: 2-158                      [1, 64, 16, 16]           --
│    └─Clamp: 2-159                      [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-13                [1, 64, 16, 16]           4,166
│    └─OutputShiftSqueeze: 2-160         --                        --
│    └─One: 2-161                        [1]                       --
│    └─OutputScale: 2-162                --                        --
│    └─Empty: 2-163                      [64, 64, 1, 1]            --
│    └─Empty: 2-164                      [64, 64, 1, 1]            --
│    └─Empty: 2-165                      [64]                      --
│    └─Empty: 2-166                      [64]                      --
│    └─BatchNorm2d: 2-167                [1, 64, 16, 16]           --
│    └─Scaler: 2-168                     [1, 64, 16, 16]           --
│    └─ReLU: 2-169                       [1, 64, 16, 16]           --
│    └─Empty: 2-170                      [1, 64, 16, 16]           --
│    └─Clamp: 2-171                      [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-14         [1, 64, 16, 16]           36,934
│    └─MaxPool2d: 2-172                  [1, 64, 16, 16]           --
│    └─Empty: 2-173                      [1, 64, 16, 16]           --
│    └─Empty: 2-174                      [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-175         --                        --
│    └─One: 2-176                        [1]                       --
│    └─OutputScale: 2-177                --                        --
│    └─Empty: 2-178                      [64, 64, 3, 3]            --
│    └─Empty: 2-179                      [64, 64, 3, 3]            --
│    └─Empty: 2-180                      [64]                      --
│    └─Empty: 2-181                      [64]                      --
│    └─BatchNorm2d: 2-182                [1, 64, 16, 16]           --
│    └─Scaler: 2-183                     [1, 64, 16, 16]           --
│    └─ReLU: 2-184                       [1, 64, 16, 16]           --
│    └─Empty: 2-185                      [1, 64, 16, 16]           --
│    └─Clamp: 2-186                      [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-15         [1, 64, 8, 8]             4,166
│    └─MaxPool2d: 2-187                  [1, 64, 8, 8]             --
│    └─Empty: 2-188                      [1, 64, 8, 8]             --
│    └─Empty: 2-189                      [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-190         --                        --
│    └─One: 2-191                        [1]                       --
│    └─OutputScale: 2-192                --                        --
│    └─Empty: 2-193                      [64, 64, 1, 1]            --
│    └─Empty: 2-194                      [64, 64, 1, 1]            --
│    └─Empty: 2-195                      [64]                      --
│    └─Empty: 2-196                      [64]                      --
│    └─BatchNorm2d: 2-197                [1, 64, 8, 8]             --
│    └─Scaler: 2-198                     [1, 64, 8, 8]             --
│    └─ReLU: 2-199                       [1, 64, 8, 8]             --
│    └─Empty: 2-200                      [1, 64, 8, 8]             --
│    └─Clamp: 2-201                      [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-16                [1, 64, 8, 8]             4,166
│    └─OutputShiftSqueeze: 2-202         --                        --
│    └─One: 2-203                        [1]                       --
│    └─OutputScale: 2-204                --                        --
│    └─Empty: 2-205                      [64, 64, 1, 1]            --
│    └─Empty: 2-206                      [64, 64, 1, 1]            --
│    └─Empty: 2-207                      [64]                      --
│    └─Empty: 2-208                      [64]                      --
│    └─BatchNorm2d: 2-209                [1, 64, 8, 8]             --
│    └─Scaler: 2-210                     [1, 64, 8, 8]             --
│    └─ReLU: 2-211                       [1, 64, 8, 8]             --
│    └─Empty: 2-212                      [1, 64, 8, 8]             --
│    └─Clamp: 2-213                      [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-17         [1, 64, 8, 8]             36,934
│    └─MaxPool2d: 2-214                  [1, 64, 8, 8]             --
│    └─Empty: 2-215                      [1, 64, 8, 8]             --
│    └─Empty: 2-216                      [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-217         --                        --
│    └─One: 2-218                        [1]                       --
│    └─OutputScale: 2-219                --                        --
│    └─Empty: 2-220                      [64, 64, 3, 3]            --
│    └─Empty: 2-221                      [64, 64, 3, 3]            --
│    └─Empty: 2-222                      [64]                      --
│    └─Empty: 2-223                      [64]                      --
│    └─BatchNorm2d: 2-224                [1, 64, 8, 8]             --
│    └─Scaler: 2-225                     [1, 64, 8, 8]             --
│    └─ReLU: 2-226                       [1, 64, 8, 8]             --
│    └─Empty: 2-227                      [1, 64, 8, 8]             --
│    └─Clamp: 2-228                      [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-18                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-229         --                        --
│    └─One: 2-230                        [1]                       --
│    └─OutputScale: 2-231                --                        --
│    └─Empty: 2-232                      [64, 3, 1, 1]             --
│    └─Empty: 2-233                      [64, 3, 1, 1]             --
│    └─Empty: 2-234                      [64]                      --
│    └─Empty: 2-235                      [64]                      --
│    └─BatchNorm2d: 2-236                [1, 64, 256, 256]         --
│    └─Scaler: 2-237                     [1, 64, 256, 256]         --
│    └─ReLU: 2-238                       [1, 64, 256, 256]         --
│    └─Empty: 2-239                      [1, 64, 256, 256]         --
│    └─Clamp: 2-240                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-19                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-241         --                        --
│    └─One: 2-242                        [1]                       --
│    └─OutputScale: 2-243                --                        --
│    └─Empty: 2-244                      [64, 64, 3, 3]            --
│    └─Empty: 2-245                      [64, 64, 3, 3]            --
│    └─Empty: 2-246                      [64]                      --
│    └─Empty: 2-247                      [64]                      --
│    └─BatchNorm2d: 2-248                [1, 64, 256, 256]         --
│    └─Scaler: 2-249                     [1, 64, 256, 256]         --
│    └─ReLU: 2-250                       [1, 64, 256, 256]         --
│    └─Empty: 2-251                      [1, 64, 256, 256]         --
│    └─Clamp: 2-252                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-20                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-253         --                        --
│    └─One: 2-254                        [1]                       --
│    └─OutputScale: 2-255                --                        --
│    └─Empty: 2-256                      [64, 64, 1, 1]            --
│    └─Empty: 2-257                      [64, 64, 1, 1]            --
│    └─Empty: 2-258                      [64]                      --
│    └─Empty: 2-259                      [64]                      --
│    └─BatchNorm2d: 2-260                [1, 64, 256, 256]         --
│    └─Scaler: 2-261                     [1, 64, 256, 256]         --
│    └─ReLU: 2-262                       [1, 64, 256, 256]         --
│    └─Empty: 2-263                      [1, 64, 256, 256]         --
│    └─Clamp: 2-264                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-21                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-265         --                        --
│    └─One: 2-266                        [1]                       --
│    └─OutputScale: 2-267                --                        --
│    └─Empty: 2-268                      [64, 64, 3, 3]            --
│    └─Empty: 2-269                      [64, 64, 3, 3]            --
│    └─Empty: 2-270                      [64]                      --
│    └─Empty: 2-271                      [64]                      --
│    └─BatchNorm2d: 2-272                [1, 64, 256, 256]         --
│    └─Scaler: 2-273                     [1, 64, 256, 256]         --
│    └─ReLU: 2-274                       [1, 64, 256, 256]         --
│    └─Empty: 2-275                      [1, 64, 256, 256]         --
│    └─Clamp: 2-276                      [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-22         [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-277                  [1, 64, 128, 128]         --
│    └─Empty: 2-278                      [1, 64, 128, 128]         --
│    └─Empty: 2-279                      [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-280         --                        --
│    └─One: 2-281                        [1]                       --
│    └─OutputScale: 2-282                --                        --
│    └─Empty: 2-283                      [64, 64, 3, 3]            --
│    └─Empty: 2-284                      [64, 64, 3, 3]            --
│    └─Empty: 2-285                      [64]                      --
│    └─Empty: 2-286                      [64]                      --
│    └─BatchNorm2d: 2-287                [1, 64, 128, 128]         --
│    └─Scaler: 2-288                     [1, 64, 128, 128]         --
│    └─ReLU: 2-289                       [1, 64, 128, 128]         --
│    └─Empty: 2-290                      [1, 64, 128, 128]         --
│    └─Clamp: 2-291                      [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-23                [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-292         --                        --
│    └─One: 2-293                        [1]                       --
│    └─OutputScale: 2-294                --                        --
│    └─Empty: 2-295                      [64, 64, 3, 3]            --
│    └─Empty: 2-296                      [64, 64, 3, 3]            --
│    └─Empty: 2-297                      [64]                      --
│    └─Empty: 2-298                      [64]                      --
│    └─BatchNorm2d: 2-299                [1, 64, 128, 128]         --
│    └─Scaler: 2-300                     [1, 64, 128, 128]         --
│    └─ReLU: 2-301                       [1, 64, 128, 128]         --
│    └─Empty: 2-302                      [1, 64, 128, 128]         --
│    └─Clamp: 2-303                      [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-24         [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-304                  [1, 64, 64, 64]           --
│    └─Empty: 2-305                      [1, 64, 64, 64]           --
│    └─Empty: 2-306                      [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-307         --                        --
│    └─One: 2-308                        [1]                       --
│    └─OutputScale: 2-309                --                        --
│    └─Empty: 2-310                      [64, 64, 3, 3]            --
│    └─Empty: 2-311                      [64, 64, 3, 3]            --
│    └─Empty: 2-312                      [64]                      --
│    └─Empty: 2-313                      [64]                      --
│    └─BatchNorm2d: 2-314                [1, 64, 64, 64]           --
│    └─Scaler: 2-315                     [1, 64, 64, 64]           --
│    └─ReLU: 2-316                       [1, 64, 64, 64]           --
│    └─Empty: 2-317                      [1, 64, 64, 64]           --
│    └─Clamp: 2-318                      [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-25                [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-319         --                        --
│    └─One: 2-320                        [1]                       --
│    └─OutputScale: 2-321                --                        --
│    └─Empty: 2-322                      [64, 64, 3, 3]            --
│    └─Empty: 2-323                      [64, 64, 3, 3]            --
│    └─Empty: 2-324                      [64]                      --
│    └─Empty: 2-325                      [64]                      --
│    └─BatchNorm2d: 2-326                [1, 64, 64, 64]           --
│    └─Scaler: 2-327                     [1, 64, 64, 64]           --
│    └─ReLU: 2-328                       [1, 64, 64, 64]           --
│    └─Empty: 2-329                      [1, 64, 64, 64]           --
│    └─Clamp: 2-330                      [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-26         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-331                  [1, 64, 32, 32]           --
│    └─Empty: 2-332                      [1, 64, 32, 32]           --
│    └─Empty: 2-333                      [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-334         --                        --
│    └─One: 2-335                        [1]                       --
│    └─OutputScale: 2-336                --                        --
│    └─Empty: 2-337                      [64, 64, 3, 3]            --
│    └─Empty: 2-338                      [64, 64, 3, 3]            --
│    └─Empty: 2-339                      [64]                      --
│    └─Empty: 2-340                      [64]                      --
│    └─BatchNorm2d: 2-341                [1, 64, 32, 32]           --
│    └─Scaler: 2-342                     [1, 64, 32, 32]           --
│    └─ReLU: 2-343                       [1, 64, 32, 32]           --
│    └─Empty: 2-344                      [1, 64, 32, 32]           --
│    └─Clamp: 2-345                      [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-27                [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-346         --                        --
│    └─One: 2-347                        [1]                       --
│    └─OutputScale: 2-348                --                        --
│    └─Empty: 2-349                      [64, 64, 1, 1]            --
│    └─Empty: 2-350                      [64, 64, 1, 1]            --
│    └─Empty: 2-351                      [64]                      --
│    └─Empty: 2-352                      [64]                      --
│    └─BatchNorm2d: 2-353                [1, 64, 32, 32]           --
│    └─Scaler: 2-354                     [1, 64, 32, 32]           --
│    └─ReLU: 2-355                       [1, 64, 32, 32]           --
│    └─Empty: 2-356                      [1, 64, 32, 32]           --
│    └─Clamp: 2-357                      [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-28         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-358                  [1, 64, 32, 32]           --
│    └─Empty: 2-359                      [1, 64, 32, 32]           --
│    └─Empty: 2-360                      [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-361         --                        --
│    └─One: 2-362                        [1]                       --
│    └─OutputScale: 2-363                --                        --
│    └─Empty: 2-364                      [64, 64, 3, 3]            --
│    └─Empty: 2-365                      [64, 64, 3, 3]            --
│    └─Empty: 2-366                      [64]                      --
│    └─Empty: 2-367                      [64]                      --
│    └─BatchNorm2d: 2-368                [1, 64, 32, 32]           --
│    └─Scaler: 2-369                     [1, 64, 32, 32]           --
│    └─ReLU: 2-370                       [1, 64, 32, 32]           --
│    └─Empty: 2-371                      [1, 64, 32, 32]           --
│    └─Clamp: 2-372                      [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-29         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-373                  [1, 64, 16, 16]           --
│    └─Empty: 2-374                      [1, 64, 16, 16]           --
│    └─Empty: 2-375                      [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-376         --                        --
│    └─One: 2-377                        [1]                       --
│    └─OutputScale: 2-378                --                        --
│    └─Empty: 2-379                      [64, 64, 3, 3]            --
│    └─Empty: 2-380                      [64, 64, 3, 3]            --
│    └─Empty: 2-381                      [64]                      --
│    └─Empty: 2-382                      [64]                      --
│    └─BatchNorm2d: 2-383                [1, 64, 16, 16]           --
│    └─Scaler: 2-384                     [1, 64, 16, 16]           --
│    └─ReLU: 2-385                       [1, 64, 16, 16]           --
│    └─Empty: 2-386                      [1, 64, 16, 16]           --
│    └─Clamp: 2-387                      [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-30                [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-388         --                        --
│    └─One: 2-389                        [1]                       --
│    └─OutputScale: 2-390                --                        --
│    └─Empty: 2-391                      [64, 64, 1, 1]            --
│    └─Empty: 2-392                      [64, 64, 1, 1]            --
│    └─Empty: 2-393                      [64]                      --
│    └─Empty: 2-394                      [64]                      --
│    └─BatchNorm2d: 2-395                [1, 64, 16, 16]           --
│    └─Scaler: 2-396                     [1, 64, 16, 16]           --
│    └─ReLU: 2-397                       [1, 64, 16, 16]           --
│    └─Empty: 2-398                      [1, 64, 16, 16]           --
│    └─Clamp: 2-399                      [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-31         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-400                  [1, 64, 16, 16]           --
│    └─Empty: 2-401                      [1, 64, 16, 16]           --
│    └─Empty: 2-402                      [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-403         --                        --
│    └─One: 2-404                        [1]                       --
│    └─OutputScale: 2-405                --                        --
│    └─Empty: 2-406                      [64, 64, 3, 3]            --
│    └─Empty: 2-407                      [64, 64, 3, 3]            --
│    └─Empty: 2-408                      [64]                      --
│    └─Empty: 2-409                      [64]                      --
│    └─BatchNorm2d: 2-410                [1, 64, 16, 16]           --
│    └─Scaler: 2-411                     [1, 64, 16, 16]           --
│    └─ReLU: 2-412                       [1, 64, 16, 16]           --
│    └─Empty: 2-413                      [1, 64, 16, 16]           --
│    └─Clamp: 2-414                      [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-32         [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-415                  [1, 64, 8, 8]             --
│    └─Empty: 2-416                      [1, 64, 8, 8]             --
│    └─Empty: 2-417                      [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-418         --                        --
│    └─One: 2-419                        [1]                       --
│    └─OutputScale: 2-420                --                        --
│    └─Empty: 2-421                      [64, 64, 1, 1]            --
│    └─Empty: 2-422                      [64, 64, 1, 1]            --
│    └─Empty: 2-423                      [64]                      --
│    └─Empty: 2-424                      [64]                      --
│    └─BatchNorm2d: 2-425                [1, 64, 8, 8]             --
│    └─Scaler: 2-426                     [1, 64, 8, 8]             --
│    └─ReLU: 2-427                       [1, 64, 8, 8]             --
│    └─Empty: 2-428                      [1, 64, 8, 8]             --
│    └─Clamp: 2-429                      [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-33                [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-430         --                        --
│    └─One: 2-431                        [1]                       --
│    └─OutputScale: 2-432                --                        --
│    └─Empty: 2-433                      [64, 64, 1, 1]            --
│    └─Empty: 2-434                      [64, 64, 1, 1]            --
│    └─Empty: 2-435                      [64]                      --
│    └─Empty: 2-436                      [64]                      --
│    └─BatchNorm2d: 2-437                [1, 64, 8, 8]             --
│    └─Scaler: 2-438                     [1, 64, 8, 8]             --
│    └─ReLU: 2-439                       [1, 64, 8, 8]             --
│    └─Empty: 2-440                      [1, 64, 8, 8]             --
│    └─Clamp: 2-441                      [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-34         [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-442                  [1, 64, 8, 8]             --
│    └─Empty: 2-443                      [1, 64, 8, 8]             --
│    └─Empty: 2-444                      [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-445         --                        --
│    └─One: 2-446                        [1]                       --
│    └─OutputScale: 2-447                --                        --
│    └─Empty: 2-448                      [64, 64, 3, 3]            --
│    └─Empty: 2-449                      [64, 64, 3, 3]            --
│    └─Empty: 2-450                      [64]                      --
│    └─Empty: 2-451                      [64]                      --
│    └─BatchNorm2d: 2-452                [1, 64, 8, 8]             --
│    └─Scaler: 2-453                     [1, 64, 8, 8]             --
│    └─ReLU: 2-454                       [1, 64, 8, 8]             --
│    └─Empty: 2-455                      [1, 64, 8, 8]             --
│    └─Clamp: 2-456                      [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-35                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-457         --                        --
│    └─One: 2-458                        [1]                       --
│    └─OutputScale: 2-459                --                        --
│    └─Empty: 2-460                      [64, 3, 1, 1]             --
│    └─Empty: 2-461                      [64, 3, 1, 1]             --
│    └─Empty: 2-462                      [64]                      --
│    └─Empty: 2-463                      [64]                      --
│    └─BatchNorm2d: 2-464                [1, 64, 256, 256]         --
│    └─Scaler: 2-465                     [1, 64, 256, 256]         --
│    └─ReLU: 2-466                       [1, 64, 256, 256]         --
│    └─Empty: 2-467                      [1, 64, 256, 256]         --
│    └─Clamp: 2-468                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-36                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-469         --                        --
│    └─One: 2-470                        [1]                       --
│    └─OutputScale: 2-471                --                        --
│    └─Empty: 2-472                      [64, 64, 3, 3]            --
│    └─Empty: 2-473                      [64, 64, 3, 3]            --
│    └─Empty: 2-474                      [64]                      --
│    └─Empty: 2-475                      [64]                      --
│    └─BatchNorm2d: 2-476                [1, 64, 256, 256]         --
│    └─Scaler: 2-477                     [1, 64, 256, 256]         --
│    └─ReLU: 2-478                       [1, 64, 256, 256]         --
│    └─Empty: 2-479                      [1, 64, 256, 256]         --
│    └─Clamp: 2-480                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-37                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-481         --                        --
│    └─One: 2-482                        [1]                       --
│    └─OutputScale: 2-483                --                        --
│    └─Empty: 2-484                      [64, 64, 1, 1]            --
│    └─Empty: 2-485                      [64, 64, 1, 1]            --
│    └─Empty: 2-486                      [64]                      --
│    └─Empty: 2-487                      [64]                      --
│    └─BatchNorm2d: 2-488                [1, 64, 256, 256]         --
│    └─Scaler: 2-489                     [1, 64, 256, 256]         --
│    └─ReLU: 2-490                       [1, 64, 256, 256]         --
│    └─Empty: 2-491                      [1, 64, 256, 256]         --
│    └─Clamp: 2-492                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-38                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-493         --                        --
│    └─One: 2-494                        [1]                       --
│    └─OutputScale: 2-495                --                        --
│    └─Empty: 2-496                      [64, 64, 3, 3]            --
│    └─Empty: 2-497                      [64, 64, 3, 3]            --
│    └─Empty: 2-498                      [64]                      --
│    └─Empty: 2-499                      [64]                      --
│    └─BatchNorm2d: 2-500                [1, 64, 256, 256]         --
│    └─Scaler: 2-501                     [1, 64, 256, 256]         --
│    └─ReLU: 2-502                       [1, 64, 256, 256]         --
│    └─Empty: 2-503                      [1, 64, 256, 256]         --
│    └─Clamp: 2-504                      [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-39         [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-505                  [1, 64, 128, 128]         --
│    └─Empty: 2-506                      [1, 64, 128, 128]         --
│    └─Empty: 2-507                      [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-508         --                        --
│    └─One: 2-509                        [1]                       --
│    └─OutputScale: 2-510                --                        --
│    └─Empty: 2-511                      [64, 64, 3, 3]            --
│    └─Empty: 2-512                      [64, 64, 3, 3]            --
│    └─Empty: 2-513                      [64]                      --
│    └─Empty: 2-514                      [64]                      --
│    └─BatchNorm2d: 2-515                [1, 64, 128, 128]         --
│    └─Scaler: 2-516                     [1, 64, 128, 128]         --
│    └─ReLU: 2-517                       [1, 64, 128, 128]         --
│    └─Empty: 2-518                      [1, 64, 128, 128]         --
│    └─Clamp: 2-519                      [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-40                [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-520         --                        --
│    └─One: 2-521                        [1]                       --
│    └─OutputScale: 2-522                --                        --
│    └─Empty: 2-523                      [64, 64, 3, 3]            --
│    └─Empty: 2-524                      [64, 64, 3, 3]            --
│    └─Empty: 2-525                      [64]                      --
│    └─Empty: 2-526                      [64]                      --
│    └─BatchNorm2d: 2-527                [1, 64, 128, 128]         --
│    └─Scaler: 2-528                     [1, 64, 128, 128]         --
│    └─ReLU: 2-529                       [1, 64, 128, 128]         --
│    └─Empty: 2-530                      [1, 64, 128, 128]         --
│    └─Clamp: 2-531                      [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-41         [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-532                  [1, 64, 64, 64]           --
│    └─Empty: 2-533                      [1, 64, 64, 64]           --
│    └─Empty: 2-534                      [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-535         --                        --
│    └─One: 2-536                        [1]                       --
│    └─OutputScale: 2-537                --                        --
│    └─Empty: 2-538                      [64, 64, 3, 3]            --
│    └─Empty: 2-539                      [64, 64, 3, 3]            --
│    └─Empty: 2-540                      [64]                      --
│    └─Empty: 2-541                      [64]                      --
│    └─BatchNorm2d: 2-542                [1, 64, 64, 64]           --
│    └─Scaler: 2-543                     [1, 64, 64, 64]           --
│    └─ReLU: 2-544                       [1, 64, 64, 64]           --
│    └─Empty: 2-545                      [1, 64, 64, 64]           --
│    └─Clamp: 2-546                      [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-42                [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-547         --                        --
│    └─One: 2-548                        [1]                       --
│    └─OutputScale: 2-549                --                        --
│    └─Empty: 2-550                      [64, 64, 3, 3]            --
│    └─Empty: 2-551                      [64, 64, 3, 3]            --
│    └─Empty: 2-552                      [64]                      --
│    └─Empty: 2-553                      [64]                      --
│    └─BatchNorm2d: 2-554                [1, 64, 64, 64]           --
│    └─Scaler: 2-555                     [1, 64, 64, 64]           --
│    └─ReLU: 2-556                       [1, 64, 64, 64]           --
│    └─Empty: 2-557                      [1, 64, 64, 64]           --
│    └─Clamp: 2-558                      [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-43         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-559                  [1, 64, 32, 32]           --
│    └─Empty: 2-560                      [1, 64, 32, 32]           --
│    └─Empty: 2-561                      [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-562         --                        --
│    └─One: 2-563                        [1]                       --
│    └─OutputScale: 2-564                --                        --
│    └─Empty: 2-565                      [64, 64, 3, 3]            --
│    └─Empty: 2-566                      [64, 64, 3, 3]            --
│    └─Empty: 2-567                      [64]                      --
│    └─Empty: 2-568                      [64]                      --
│    └─BatchNorm2d: 2-569                [1, 64, 32, 32]           --
│    └─Scaler: 2-570                     [1, 64, 32, 32]           --
│    └─ReLU: 2-571                       [1, 64, 32, 32]           --
│    └─Empty: 2-572                      [1, 64, 32, 32]           --
│    └─Clamp: 2-573                      [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-44                [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-574         --                        --
│    └─One: 2-575                        [1]                       --
│    └─OutputScale: 2-576                --                        --
│    └─Empty: 2-577                      [64, 64, 1, 1]            --
│    └─Empty: 2-578                      [64, 64, 1, 1]            --
│    └─Empty: 2-579                      [64]                      --
│    └─Empty: 2-580                      [64]                      --
│    └─BatchNorm2d: 2-581                [1, 64, 32, 32]           --
│    └─Scaler: 2-582                     [1, 64, 32, 32]           --
│    └─ReLU: 2-583                       [1, 64, 32, 32]           --
│    └─Empty: 2-584                      [1, 64, 32, 32]           --
│    └─Clamp: 2-585                      [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-45         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-586                  [1, 64, 32, 32]           --
│    └─Empty: 2-587                      [1, 64, 32, 32]           --
│    └─Empty: 2-588                      [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-589         --                        --
│    └─One: 2-590                        [1]                       --
│    └─OutputScale: 2-591                --                        --
│    └─Empty: 2-592                      [64, 64, 3, 3]            --
│    └─Empty: 2-593                      [64, 64, 3, 3]            --
│    └─Empty: 2-594                      [64]                      --
│    └─Empty: 2-595                      [64]                      --
│    └─BatchNorm2d: 2-596                [1, 64, 32, 32]           --
│    └─Scaler: 2-597                     [1, 64, 32, 32]           --
│    └─ReLU: 2-598                       [1, 64, 32, 32]           --
│    └─Empty: 2-599                      [1, 64, 32, 32]           --
│    └─Clamp: 2-600                      [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-46         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-601                  [1, 64, 16, 16]           --
│    └─Empty: 2-602                      [1, 64, 16, 16]           --
│    └─Empty: 2-603                      [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-604         --                        --
│    └─One: 2-605                        [1]                       --
│    └─OutputScale: 2-606                --                        --
│    └─Empty: 2-607                      [64, 64, 3, 3]            --
│    └─Empty: 2-608                      [64, 64, 3, 3]            --
│    └─Empty: 2-609                      [64]                      --
│    └─Empty: 2-610                      [64]                      --
│    └─BatchNorm2d: 2-611                [1, 64, 16, 16]           --
│    └─Scaler: 2-612                     [1, 64, 16, 16]           --
│    └─ReLU: 2-613                       [1, 64, 16, 16]           --
│    └─Empty: 2-614                      [1, 64, 16, 16]           --
│    └─Clamp: 2-615                      [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-47                [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-616         --                        --
│    └─One: 2-617                        [1]                       --
│    └─OutputScale: 2-618                --                        --
│    └─Empty: 2-619                      [64, 64, 1, 1]            --
│    └─Empty: 2-620                      [64, 64, 1, 1]            --
│    └─Empty: 2-621                      [64]                      --
│    └─Empty: 2-622                      [64]                      --
│    └─BatchNorm2d: 2-623                [1, 64, 16, 16]           --
│    └─Scaler: 2-624                     [1, 64, 16, 16]           --
│    └─ReLU: 2-625                       [1, 64, 16, 16]           --
│    └─Empty: 2-626                      [1, 64, 16, 16]           --
│    └─Clamp: 2-627                      [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-48         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-628                  [1, 64, 16, 16]           --
│    └─Empty: 2-629                      [1, 64, 16, 16]           --
│    └─Empty: 2-630                      [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-631         --                        --
│    └─One: 2-632                        [1]                       --
│    └─OutputScale: 2-633                --                        --
│    └─Empty: 2-634                      [64, 64, 3, 3]            --
│    └─Empty: 2-635                      [64, 64, 3, 3]            --
│    └─Empty: 2-636                      [64]                      --
│    └─Empty: 2-637                      [64]                      --
│    └─BatchNorm2d: 2-638                [1, 64, 16, 16]           --
│    └─Scaler: 2-639                     [1, 64, 16, 16]           --
│    └─ReLU: 2-640                       [1, 64, 16, 16]           --
│    └─Empty: 2-641                      [1, 64, 16, 16]           --
│    └─Clamp: 2-642                      [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-49         [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-643                  [1, 64, 8, 8]             --
│    └─Empty: 2-644                      [1, 64, 8, 8]             --
│    └─Empty: 2-645                      [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-646         --                        --
│    └─One: 2-647                        [1]                       --
│    └─OutputScale: 2-648                --                        --
│    └─Empty: 2-649                      [64, 64, 1, 1]            --
│    └─Empty: 2-650                      [64, 64, 1, 1]            --
│    └─Empty: 2-651                      [64]                      --
│    └─Empty: 2-652                      [64]                      --
│    └─BatchNorm2d: 2-653                [1, 64, 8, 8]             --
│    └─Scaler: 2-654                     [1, 64, 8, 8]             --
│    └─ReLU: 2-655                       [1, 64, 8, 8]             --
│    └─Empty: 2-656                      [1, 64, 8, 8]             --
│    └─Clamp: 2-657                      [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-50                [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-658         --                        --
│    └─One: 2-659                        [1]                       --
│    └─OutputScale: 2-660                --                        --
│    └─Empty: 2-661                      [64, 64, 1, 1]            --
│    └─Empty: 2-662                      [64, 64, 1, 1]            --
│    └─Empty: 2-663                      [64]                      --
│    └─Empty: 2-664                      [64]                      --
│    └─BatchNorm2d: 2-665                [1, 64, 8, 8]             --
│    └─Scaler: 2-666                     [1, 64, 8, 8]             --
│    └─ReLU: 2-667                       [1, 64, 8, 8]             --
│    └─Empty: 2-668                      [1, 64, 8, 8]             --
│    └─Clamp: 2-669                      [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-51         [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-670                  [1, 64, 8, 8]             --
│    └─Empty: 2-671                      [1, 64, 8, 8]             --
│    └─Empty: 2-672                      [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-673         --                        --
│    └─One: 2-674                        [1]                       --
│    └─OutputScale: 2-675                --                        --
│    └─Empty: 2-676                      [64, 64, 3, 3]            --
│    └─Empty: 2-677                      [64, 64, 3, 3]            --
│    └─Empty: 2-678                      [64]                      --
│    └─Empty: 2-679                      [64]                      --
│    └─BatchNorm2d: 2-680                [1, 64, 8, 8]             --
│    └─Scaler: 2-681                     [1, 64, 8, 8]             --
│    └─ReLU: 2-682                       [1, 64, 8, 8]             --
│    └─Empty: 2-683                      [1, 64, 8, 8]             --
│    └─Clamp: 2-684                      [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-52                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-685         --                        --
│    └─One: 2-686                        [1]                       --
│    └─OutputScale: 2-687                --                        --
│    └─Empty: 2-688                      [64, 3, 1, 1]             --
│    └─Empty: 2-689                      [64, 3, 1, 1]             --
│    └─Empty: 2-690                      [64]                      --
│    └─Empty: 2-691                      [64]                      --
│    └─BatchNorm2d: 2-692                [1, 64, 256, 256]         --
│    └─Scaler: 2-693                     [1, 64, 256, 256]         --
│    └─ReLU: 2-694                       [1, 64, 256, 256]         --
│    └─Empty: 2-695                      [1, 64, 256, 256]         --
│    └─Clamp: 2-696                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-53                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-697         --                        --
│    └─One: 2-698                        [1]                       --
│    └─OutputScale: 2-699                --                        --
│    └─Empty: 2-700                      [64, 64, 3, 3]            --
│    └─Empty: 2-701                      [64, 64, 3, 3]            --
│    └─Empty: 2-702                      [64]                      --
│    └─Empty: 2-703                      [64]                      --
│    └─BatchNorm2d: 2-704                [1, 64, 256, 256]         --
│    └─Scaler: 2-705                     [1, 64, 256, 256]         --
│    └─ReLU: 2-706                       [1, 64, 256, 256]         --
│    └─Empty: 2-707                      [1, 64, 256, 256]         --
│    └─Clamp: 2-708                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-54                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-709         --                        --
│    └─One: 2-710                        [1]                       --
│    └─OutputScale: 2-711                --                        --
│    └─Empty: 2-712                      [64, 64, 1, 1]            --
│    └─Empty: 2-713                      [64, 64, 1, 1]            --
│    └─Empty: 2-714                      [64]                      --
│    └─Empty: 2-715                      [64]                      --
│    └─BatchNorm2d: 2-716                [1, 64, 256, 256]         --
│    └─Scaler: 2-717                     [1, 64, 256, 256]         --
│    └─ReLU: 2-718                       [1, 64, 256, 256]         --
│    └─Empty: 2-719                      [1, 64, 256, 256]         --
│    └─Clamp: 2-720                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-55                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-721         --                        --
│    └─One: 2-722                        [1]                       --
│    └─OutputScale: 2-723                --                        --
│    └─Empty: 2-724                      [64, 64, 3, 3]            --
│    └─Empty: 2-725                      [64, 64, 3, 3]            --
│    └─Empty: 2-726                      [64]                      --
│    └─Empty: 2-727                      [64]                      --
│    └─BatchNorm2d: 2-728                [1, 64, 256, 256]         --
│    └─Scaler: 2-729                     [1, 64, 256, 256]         --
│    └─ReLU: 2-730                       [1, 64, 256, 256]         --
│    └─Empty: 2-731                      [1, 64, 256, 256]         --
│    └─Clamp: 2-732                      [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-56         [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-733                  [1, 64, 128, 128]         --
│    └─Empty: 2-734                      [1, 64, 128, 128]         --
│    └─Empty: 2-735                      [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-736         --                        --
│    └─One: 2-737                        [1]                       --
│    └─OutputScale: 2-738                --                        --
│    └─Empty: 2-739                      [64, 64, 3, 3]            --
│    └─Empty: 2-740                      [64, 64, 3, 3]            --
│    └─Empty: 2-741                      [64]                      --
│    └─Empty: 2-742                      [64]                      --
│    └─BatchNorm2d: 2-743                [1, 64, 128, 128]         --
│    └─Scaler: 2-744                     [1, 64, 128, 128]         --
│    └─ReLU: 2-745                       [1, 64, 128, 128]         --
│    └─Empty: 2-746                      [1, 64, 128, 128]         --
│    └─Clamp: 2-747                      [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-57                [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-748         --                        --
│    └─One: 2-749                        [1]                       --
│    └─OutputScale: 2-750                --                        --
│    └─Empty: 2-751                      [64, 64, 3, 3]            --
│    └─Empty: 2-752                      [64, 64, 3, 3]            --
│    └─Empty: 2-753                      [64]                      --
│    └─Empty: 2-754                      [64]                      --
│    └─BatchNorm2d: 2-755                [1, 64, 128, 128]         --
│    └─Scaler: 2-756                     [1, 64, 128, 128]         --
│    └─ReLU: 2-757                       [1, 64, 128, 128]         --
│    └─Empty: 2-758                      [1, 64, 128, 128]         --
│    └─Clamp: 2-759                      [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-58         [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-760                  [1, 64, 64, 64]           --
│    └─Empty: 2-761                      [1, 64, 64, 64]           --
│    └─Empty: 2-762                      [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-763         --                        --
│    └─One: 2-764                        [1]                       --
│    └─OutputScale: 2-765                --                        --
│    └─Empty: 2-766                      [64, 64, 3, 3]            --
│    └─Empty: 2-767                      [64, 64, 3, 3]            --
│    └─Empty: 2-768                      [64]                      --
│    └─Empty: 2-769                      [64]                      --
│    └─BatchNorm2d: 2-770                [1, 64, 64, 64]           --
│    └─Scaler: 2-771                     [1, 64, 64, 64]           --
│    └─ReLU: 2-772                       [1, 64, 64, 64]           --
│    └─Empty: 2-773                      [1, 64, 64, 64]           --
│    └─Clamp: 2-774                      [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-59                [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-775         --                        --
│    └─One: 2-776                        [1]                       --
│    └─OutputScale: 2-777                --                        --
│    └─Empty: 2-778                      [64, 64, 3, 3]            --
│    └─Empty: 2-779                      [64, 64, 3, 3]            --
│    └─Empty: 2-780                      [64]                      --
│    └─Empty: 2-781                      [64]                      --
│    └─BatchNorm2d: 2-782                [1, 64, 64, 64]           --
│    └─Scaler: 2-783                     [1, 64, 64, 64]           --
│    └─ReLU: 2-784                       [1, 64, 64, 64]           --
│    └─Empty: 2-785                      [1, 64, 64, 64]           --
│    └─Clamp: 2-786                      [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-60         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-787                  [1, 64, 32, 32]           --
│    └─Empty: 2-788                      [1, 64, 32, 32]           --
│    └─Empty: 2-789                      [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-790         --                        --
│    └─One: 2-791                        [1]                       --
│    └─OutputScale: 2-792                --                        --
│    └─Empty: 2-793                      [64, 64, 3, 3]            --
│    └─Empty: 2-794                      [64, 64, 3, 3]            --
│    └─Empty: 2-795                      [64]                      --
│    └─Empty: 2-796                      [64]                      --
│    └─BatchNorm2d: 2-797                [1, 64, 32, 32]           --
│    └─Scaler: 2-798                     [1, 64, 32, 32]           --
│    └─ReLU: 2-799                       [1, 64, 32, 32]           --
│    └─Empty: 2-800                      [1, 64, 32, 32]           --
│    └─Clamp: 2-801                      [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-61                [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-802         --                        --
│    └─One: 2-803                        [1]                       --
│    └─OutputScale: 2-804                --                        --
│    └─Empty: 2-805                      [64, 64, 1, 1]            --
│    └─Empty: 2-806                      [64, 64, 1, 1]            --
│    └─Empty: 2-807                      [64]                      --
│    └─Empty: 2-808                      [64]                      --
│    └─BatchNorm2d: 2-809                [1, 64, 32, 32]           --
│    └─Scaler: 2-810                     [1, 64, 32, 32]           --
│    └─ReLU: 2-811                       [1, 64, 32, 32]           --
│    └─Empty: 2-812                      [1, 64, 32, 32]           --
│    └─Clamp: 2-813                      [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-62         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-814                  [1, 64, 32, 32]           --
│    └─Empty: 2-815                      [1, 64, 32, 32]           --
│    └─Empty: 2-816                      [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-817         --                        --
│    └─One: 2-818                        [1]                       --
│    └─OutputScale: 2-819                --                        --
│    └─Empty: 2-820                      [64, 64, 3, 3]            --
│    └─Empty: 2-821                      [64, 64, 3, 3]            --
│    └─Empty: 2-822                      [64]                      --
│    └─Empty: 2-823                      [64]                      --
│    └─BatchNorm2d: 2-824                [1, 64, 32, 32]           --
│    └─Scaler: 2-825                     [1, 64, 32, 32]           --
│    └─ReLU: 2-826                       [1, 64, 32, 32]           --
│    └─Empty: 2-827                      [1, 64, 32, 32]           --
│    └─Clamp: 2-828                      [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-63         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-829                  [1, 64, 16, 16]           --
│    └─Empty: 2-830                      [1, 64, 16, 16]           --
│    └─Empty: 2-831                      [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-832         --                        --
│    └─One: 2-833                        [1]                       --
│    └─OutputScale: 2-834                --                        --
│    └─Empty: 2-835                      [64, 64, 3, 3]            --
│    └─Empty: 2-836                      [64, 64, 3, 3]            --
│    └─Empty: 2-837                      [64]                      --
│    └─Empty: 2-838                      [64]                      --
│    └─BatchNorm2d: 2-839                [1, 64, 16, 16]           --
│    └─Scaler: 2-840                     [1, 64, 16, 16]           --
│    └─ReLU: 2-841                       [1, 64, 16, 16]           --
│    └─Empty: 2-842                      [1, 64, 16, 16]           --
│    └─Clamp: 2-843                      [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-64                [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-844         --                        --
│    └─One: 2-845                        [1]                       --
│    └─OutputScale: 2-846                --                        --
│    └─Empty: 2-847                      [64, 64, 1, 1]            --
│    └─Empty: 2-848                      [64, 64, 1, 1]            --
│    └─Empty: 2-849                      [64]                      --
│    └─Empty: 2-850                      [64]                      --
│    └─BatchNorm2d: 2-851                [1, 64, 16, 16]           --
│    └─Scaler: 2-852                     [1, 64, 16, 16]           --
│    └─ReLU: 2-853                       [1, 64, 16, 16]           --
│    └─Empty: 2-854                      [1, 64, 16, 16]           --
│    └─Clamp: 2-855                      [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-65         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-856                  [1, 64, 16, 16]           --
│    └─Empty: 2-857                      [1, 64, 16, 16]           --
│    └─Empty: 2-858                      [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-859         --                        --
│    └─One: 2-860                        [1]                       --
│    └─OutputScale: 2-861                --                        --
│    └─Empty: 2-862                      [64, 64, 3, 3]            --
│    └─Empty: 2-863                      [64, 64, 3, 3]            --
│    └─Empty: 2-864                      [64]                      --
│    └─Empty: 2-865                      [64]                      --
│    └─BatchNorm2d: 2-866                [1, 64, 16, 16]           --
│    └─Scaler: 2-867                     [1, 64, 16, 16]           --
│    └─ReLU: 2-868                       [1, 64, 16, 16]           --
│    └─Empty: 2-869                      [1, 64, 16, 16]           --
│    └─Clamp: 2-870                      [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-66         [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-871                  [1, 64, 8, 8]             --
│    └─Empty: 2-872                      [1, 64, 8, 8]             --
│    └─Empty: 2-873                      [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-874         --                        --
│    └─One: 2-875                        [1]                       --
│    └─OutputScale: 2-876                --                        --
│    └─Empty: 2-877                      [64, 64, 1, 1]            --
│    └─Empty: 2-878                      [64, 64, 1, 1]            --
│    └─Empty: 2-879                      [64]                      --
│    └─Empty: 2-880                      [64]                      --
│    └─BatchNorm2d: 2-881                [1, 64, 8, 8]             --
│    └─Scaler: 2-882                     [1, 64, 8, 8]             --
│    └─ReLU: 2-883                       [1, 64, 8, 8]             --
│    └─Empty: 2-884                      [1, 64, 8, 8]             --
│    └─Clamp: 2-885                      [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-67                [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-886         --                        --
│    └─One: 2-887                        [1]                       --
│    └─OutputScale: 2-888                --                        --
│    └─Empty: 2-889                      [64, 64, 1, 1]            --
│    └─Empty: 2-890                      [64, 64, 1, 1]            --
│    └─Empty: 2-891                      [64]                      --
│    └─Empty: 2-892                      [64]                      --
│    └─BatchNorm2d: 2-893                [1, 64, 8, 8]             --
│    └─Scaler: 2-894                     [1, 64, 8, 8]             --
│    └─ReLU: 2-895                       [1, 64, 8, 8]             --
│    └─Empty: 2-896                      [1, 64, 8, 8]             --
│    └─Clamp: 2-897                      [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-68         [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-898                  [1, 64, 8, 8]             --
│    └─Empty: 2-899                      [1, 64, 8, 8]             --
│    └─Empty: 2-900                      [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-901         --                        --
│    └─One: 2-902                        [1]                       --
│    └─OutputScale: 2-903                --                        --
│    └─Empty: 2-904                      [64, 64, 3, 3]            --
│    └─Empty: 2-905                      [64, 64, 3, 3]            --
│    └─Empty: 2-906                      [64]                      --
│    └─Empty: 2-907                      [64]                      --
│    └─BatchNorm2d: 2-908                [1, 64, 8, 8]             --
│    └─Scaler: 2-909                     [1, 64, 8, 8]             --
│    └─ReLU: 2-910                       [1, 64, 8, 8]             --
│    └─Empty: 2-911                      [1, 64, 8, 8]             --
│    └─Clamp: 2-912                      [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-69                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-913         --                        --
│    └─One: 2-914                        [1]                       --
│    └─OutputScale: 2-915                --                        --
│    └─Empty: 2-916                      [64, 3, 1, 1]             --
│    └─Empty: 2-917                      [64, 3, 1, 1]             --
│    └─Empty: 2-918                      [64]                      --
│    └─Empty: 2-919                      [64]                      --
│    └─BatchNorm2d: 2-920                [1, 64, 256, 256]         --
│    └─Scaler: 2-921                     [1, 64, 256, 256]         --
│    └─ReLU: 2-922                       [1, 64, 256, 256]         --
│    └─Empty: 2-923                      [1, 64, 256, 256]         --
│    └─Clamp: 2-924                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-70                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-925         --                        --
│    └─One: 2-926                        [1]                       --
│    └─OutputScale: 2-927                --                        --
│    └─Empty: 2-928                      [64, 64, 3, 3]            --
│    └─Empty: 2-929                      [64, 64, 3, 3]            --
│    └─Empty: 2-930                      [64]                      --
│    └─Empty: 2-931                      [64]                      --
│    └─BatchNorm2d: 2-932                [1, 64, 256, 256]         --
│    └─Scaler: 2-933                     [1, 64, 256, 256]         --
│    └─ReLU: 2-934                       [1, 64, 256, 256]         --
│    └─Empty: 2-935                      [1, 64, 256, 256]         --
│    └─Clamp: 2-936                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-71                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-937         --                        --
│    └─One: 2-938                        [1]                       --
│    └─OutputScale: 2-939                --                        --
│    └─Empty: 2-940                      [64, 64, 1, 1]            --
│    └─Empty: 2-941                      [64, 64, 1, 1]            --
│    └─Empty: 2-942                      [64]                      --
│    └─Empty: 2-943                      [64]                      --
│    └─BatchNorm2d: 2-944                [1, 64, 256, 256]         --
│    └─Scaler: 2-945                     [1, 64, 256, 256]         --
│    └─ReLU: 2-946                       [1, 64, 256, 256]         --
│    └─Empty: 2-947                      [1, 64, 256, 256]         --
│    └─Clamp: 2-948                      [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-72                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-949         --                        --
│    └─One: 2-950                        [1]                       --
│    └─OutputScale: 2-951                --                        --
│    └─Empty: 2-952                      [64, 64, 3, 3]            --
│    └─Empty: 2-953                      [64, 64, 3, 3]            --
│    └─Empty: 2-954                      [64]                      --
│    └─Empty: 2-955                      [64]                      --
│    └─BatchNorm2d: 2-956                [1, 64, 256, 256]         --
│    └─Scaler: 2-957                     [1, 64, 256, 256]         --
│    └─ReLU: 2-958                       [1, 64, 256, 256]         --
│    └─Empty: 2-959                      [1, 64, 256, 256]         --
│    └─Clamp: 2-960                      [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-73         [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-961                  [1, 64, 128, 128]         --
│    └─Empty: 2-962                      [1, 64, 128, 128]         --
│    └─Empty: 2-963                      [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-964         --                        --
│    └─One: 2-965                        [1]                       --
│    └─OutputScale: 2-966                --                        --
│    └─Empty: 2-967                      [64, 64, 3, 3]            --
│    └─Empty: 2-968                      [64, 64, 3, 3]            --
│    └─Empty: 2-969                      [64]                      --
│    └─Empty: 2-970                      [64]                      --
│    └─BatchNorm2d: 2-971                [1, 64, 128, 128]         --
│    └─Scaler: 2-972                     [1, 64, 128, 128]         --
│    └─ReLU: 2-973                       [1, 64, 128, 128]         --
│    └─Empty: 2-974                      [1, 64, 128, 128]         --
│    └─Clamp: 2-975                      [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-74                [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-976         --                        --
│    └─One: 2-977                        [1]                       --
│    └─OutputScale: 2-978                --                        --
│    └─Empty: 2-979                      [64, 64, 3, 3]            --
│    └─Empty: 2-980                      [64, 64, 3, 3]            --
│    └─Empty: 2-981                      [64]                      --
│    └─Empty: 2-982                      [64]                      --
│    └─BatchNorm2d: 2-983                [1, 64, 128, 128]         --
│    └─Scaler: 2-984                     [1, 64, 128, 128]         --
│    └─ReLU: 2-985                       [1, 64, 128, 128]         --
│    └─Empty: 2-986                      [1, 64, 128, 128]         --
│    └─Clamp: 2-987                      [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-75         [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-988                  [1, 64, 64, 64]           --
│    └─Empty: 2-989                      [1, 64, 64, 64]           --
│    └─Empty: 2-990                      [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-991         --                        --
│    └─One: 2-992                        [1]                       --
│    └─OutputScale: 2-993                --                        --
│    └─Empty: 2-994                      [64, 64, 3, 3]            --
│    └─Empty: 2-995                      [64, 64, 3, 3]            --
│    └─Empty: 2-996                      [64]                      --
│    └─Empty: 2-997                      [64]                      --
│    └─BatchNorm2d: 2-998                [1, 64, 64, 64]           --
│    └─Scaler: 2-999                     [1, 64, 64, 64]           --
│    └─ReLU: 2-1000                      [1, 64, 64, 64]           --
│    └─Empty: 2-1001                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1002                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-76                [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-1003        --                        --
│    └─One: 2-1004                       [1]                       --
│    └─OutputScale: 2-1005               --                        --
│    └─Empty: 2-1006                     [64, 64, 3, 3]            --
│    └─Empty: 2-1007                     [64, 64, 3, 3]            --
│    └─Empty: 2-1008                     [64]                      --
│    └─Empty: 2-1009                     [64]                      --
│    └─BatchNorm2d: 2-1010               [1, 64, 64, 64]           --
│    └─Scaler: 2-1011                    [1, 64, 64, 64]           --
│    └─ReLU: 2-1012                      [1, 64, 64, 64]           --
│    └─Empty: 2-1013                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1014                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-77         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1015                 [1, 64, 32, 32]           --
│    └─Empty: 2-1016                     [1, 64, 32, 32]           --
│    └─Empty: 2-1017                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1018        --                        --
│    └─One: 2-1019                       [1]                       --
│    └─OutputScale: 2-1020               --                        --
│    └─Empty: 2-1021                     [64, 64, 3, 3]            --
│    └─Empty: 2-1022                     [64, 64, 3, 3]            --
│    └─Empty: 2-1023                     [64]                      --
│    └─Empty: 2-1024                     [64]                      --
│    └─BatchNorm2d: 2-1025               [1, 64, 32, 32]           --
│    └─Scaler: 2-1026                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1027                      [1, 64, 32, 32]           --
│    └─Empty: 2-1028                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1029                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-78                [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1030        --                        --
│    └─One: 2-1031                       [1]                       --
│    └─OutputScale: 2-1032               --                        --
│    └─Empty: 2-1033                     [64, 64, 1, 1]            --
│    └─Empty: 2-1034                     [64, 64, 1, 1]            --
│    └─Empty: 2-1035                     [64]                      --
│    └─Empty: 2-1036                     [64]                      --
│    └─BatchNorm2d: 2-1037               [1, 64, 32, 32]           --
│    └─Scaler: 2-1038                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1039                      [1, 64, 32, 32]           --
│    └─Empty: 2-1040                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1041                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-79         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1042                 [1, 64, 32, 32]           --
│    └─Empty: 2-1043                     [1, 64, 32, 32]           --
│    └─Empty: 2-1044                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1045        --                        --
│    └─One: 2-1046                       [1]                       --
│    └─OutputScale: 2-1047               --                        --
│    └─Empty: 2-1048                     [64, 64, 3, 3]            --
│    └─Empty: 2-1049                     [64, 64, 3, 3]            --
│    └─Empty: 2-1050                     [64]                      --
│    └─Empty: 2-1051                     [64]                      --
│    └─BatchNorm2d: 2-1052               [1, 64, 32, 32]           --
│    └─Scaler: 2-1053                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1054                      [1, 64, 32, 32]           --
│    └─Empty: 2-1055                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1056                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-80         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1057                 [1, 64, 16, 16]           --
│    └─Empty: 2-1058                     [1, 64, 16, 16]           --
│    └─Empty: 2-1059                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1060        --                        --
│    └─One: 2-1061                       [1]                       --
│    └─OutputScale: 2-1062               --                        --
│    └─Empty: 2-1063                     [64, 64, 3, 3]            --
│    └─Empty: 2-1064                     [64, 64, 3, 3]            --
│    └─Empty: 2-1065                     [64]                      --
│    └─Empty: 2-1066                     [64]                      --
│    └─BatchNorm2d: 2-1067               [1, 64, 16, 16]           --
│    └─Scaler: 2-1068                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1069                      [1, 64, 16, 16]           --
│    └─Empty: 2-1070                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1071                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-81                [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1072        --                        --
│    └─One: 2-1073                       [1]                       --
│    └─OutputScale: 2-1074               --                        --
│    └─Empty: 2-1075                     [64, 64, 1, 1]            --
│    └─Empty: 2-1076                     [64, 64, 1, 1]            --
│    └─Empty: 2-1077                     [64]                      --
│    └─Empty: 2-1078                     [64]                      --
│    └─BatchNorm2d: 2-1079               [1, 64, 16, 16]           --
│    └─Scaler: 2-1080                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1081                      [1, 64, 16, 16]           --
│    └─Empty: 2-1082                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1083                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-82         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1084                 [1, 64, 16, 16]           --
│    └─Empty: 2-1085                     [1, 64, 16, 16]           --
│    └─Empty: 2-1086                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1087        --                        --
│    └─One: 2-1088                       [1]                       --
│    └─OutputScale: 2-1089               --                        --
│    └─Empty: 2-1090                     [64, 64, 3, 3]            --
│    └─Empty: 2-1091                     [64, 64, 3, 3]            --
│    └─Empty: 2-1092                     [64]                      --
│    └─Empty: 2-1093                     [64]                      --
│    └─BatchNorm2d: 2-1094               [1, 64, 16, 16]           --
│    └─Scaler: 2-1095                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1096                      [1, 64, 16, 16]           --
│    └─Empty: 2-1097                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1098                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-83         [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1099                 [1, 64, 8, 8]             --
│    └─Empty: 2-1100                     [1, 64, 8, 8]             --
│    └─Empty: 2-1101                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1102        --                        --
│    └─One: 2-1103                       [1]                       --
│    └─OutputScale: 2-1104               --                        --
│    └─Empty: 2-1105                     [64, 64, 1, 1]            --
│    └─Empty: 2-1106                     [64, 64, 1, 1]            --
│    └─Empty: 2-1107                     [64]                      --
│    └─Empty: 2-1108                     [64]                      --
│    └─BatchNorm2d: 2-1109               [1, 64, 8, 8]             --
│    └─Scaler: 2-1110                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1111                      [1, 64, 8, 8]             --
│    └─Empty: 2-1112                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1113                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-84                [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-1114        --                        --
│    └─One: 2-1115                       [1]                       --
│    └─OutputScale: 2-1116               --                        --
│    └─Empty: 2-1117                     [64, 64, 1, 1]            --
│    └─Empty: 2-1118                     [64, 64, 1, 1]            --
│    └─Empty: 2-1119                     [64]                      --
│    └─Empty: 2-1120                     [64]                      --
│    └─BatchNorm2d: 2-1121               [1, 64, 8, 8]             --
│    └─Scaler: 2-1122                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1123                      [1, 64, 8, 8]             --
│    └─Empty: 2-1124                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1125                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-85         [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1126                 [1, 64, 8, 8]             --
│    └─Empty: 2-1127                     [1, 64, 8, 8]             --
│    └─Empty: 2-1128                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1129        --                        --
│    └─One: 2-1130                       [1]                       --
│    └─OutputScale: 2-1131               --                        --
│    └─Empty: 2-1132                     [64, 64, 3, 3]            --
│    └─Empty: 2-1133                     [64, 64, 3, 3]            --
│    └─Empty: 2-1134                     [64]                      --
│    └─Empty: 2-1135                     [64]                      --
│    └─BatchNorm2d: 2-1136               [1, 64, 8, 8]             --
│    └─Scaler: 2-1137                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1138                      [1, 64, 8, 8]             --
│    └─Empty: 2-1139                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1140                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-86                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1141        --                        --
│    └─One: 2-1142                       [1]                       --
│    └─OutputScale: 2-1143               --                        --
│    └─Empty: 2-1144                     [64, 3, 1, 1]             --
│    └─Empty: 2-1145                     [64, 3, 1, 1]             --
│    └─Empty: 2-1146                     [64]                      --
│    └─Empty: 2-1147                     [64]                      --
│    └─BatchNorm2d: 2-1148               [1, 64, 256, 256]         --
│    └─Scaler: 2-1149                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1150                      [1, 64, 256, 256]         --
│    └─Empty: 2-1151                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1152                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-87                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1153        --                        --
│    └─One: 2-1154                       [1]                       --
│    └─OutputScale: 2-1155               --                        --
│    └─Empty: 2-1156                     [64, 64, 3, 3]            --
│    └─Empty: 2-1157                     [64, 64, 3, 3]            --
│    └─Empty: 2-1158                     [64]                      --
│    └─Empty: 2-1159                     [64]                      --
│    └─BatchNorm2d: 2-1160               [1, 64, 256, 256]         --
│    └─Scaler: 2-1161                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1162                      [1, 64, 256, 256]         --
│    └─Empty: 2-1163                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1164                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-88                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1165        --                        --
│    └─One: 2-1166                       [1]                       --
│    └─OutputScale: 2-1167               --                        --
│    └─Empty: 2-1168                     [64, 64, 1, 1]            --
│    └─Empty: 2-1169                     [64, 64, 1, 1]            --
│    └─Empty: 2-1170                     [64]                      --
│    └─Empty: 2-1171                     [64]                      --
│    └─BatchNorm2d: 2-1172               [1, 64, 256, 256]         --
│    └─Scaler: 2-1173                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1174                      [1, 64, 256, 256]         --
│    └─Empty: 2-1175                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1176                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-89                [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1177        --                        --
│    └─One: 2-1178                       [1]                       --
│    └─OutputScale: 2-1179               --                        --
│    └─Empty: 2-1180                     [64, 64, 3, 3]            --
│    └─Empty: 2-1181                     [64, 64, 3, 3]            --
│    └─Empty: 2-1182                     [64]                      --
│    └─Empty: 2-1183                     [64]                      --
│    └─BatchNorm2d: 2-1184               [1, 64, 256, 256]         --
│    └─Scaler: 2-1185                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1186                      [1, 64, 256, 256]         --
│    └─Empty: 2-1187                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1188                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-90         [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-1189                 [1, 64, 128, 128]         --
│    └─Empty: 2-1190                     [1, 64, 128, 128]         --
│    └─Empty: 2-1191                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-1192        --                        --
│    └─One: 2-1193                       [1]                       --
│    └─OutputScale: 2-1194               --                        --
│    └─Empty: 2-1195                     [64, 64, 3, 3]            --
│    └─Empty: 2-1196                     [64, 64, 3, 3]            --
│    └─Empty: 2-1197                     [64]                      --
│    └─Empty: 2-1198                     [64]                      --
│    └─BatchNorm2d: 2-1199               [1, 64, 128, 128]         --
│    └─Scaler: 2-1200                    [1, 64, 128, 128]         --
│    └─ReLU: 2-1201                      [1, 64, 128, 128]         --
│    └─Empty: 2-1202                     [1, 64, 128, 128]         --
│    └─Clamp: 2-1203                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-91                [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1204        --                        --
│    └─One: 2-1205                       [1]                       --
│    └─OutputScale: 2-1206               --                        --
│    └─Empty: 2-1207                     [64, 64, 3, 3]            --
│    └─Empty: 2-1208                     [64, 64, 3, 3]            --
│    └─Empty: 2-1209                     [64]                      --
│    └─Empty: 2-1210                     [64]                      --
│    └─BatchNorm2d: 2-1211               [1, 64, 128, 128]         --
│    └─Scaler: 2-1212                    [1, 64, 128, 128]         --
│    └─ReLU: 2-1213                      [1, 64, 128, 128]         --
│    └─Empty: 2-1214                     [1, 64, 128, 128]         --
│    └─Clamp: 2-1215                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-92         [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-1216                 [1, 64, 64, 64]           --
│    └─Empty: 2-1217                     [1, 64, 64, 64]           --
│    └─Empty: 2-1218                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-1219        --                        --
│    └─One: 2-1220                       [1]                       --
│    └─OutputScale: 2-1221               --                        --
│    └─Empty: 2-1222                     [64, 64, 3, 3]            --
│    └─Empty: 2-1223                     [64, 64, 3, 3]            --
│    └─Empty: 2-1224                     [64]                      --
│    └─Empty: 2-1225                     [64]                      --
│    └─BatchNorm2d: 2-1226               [1, 64, 64, 64]           --
│    └─Scaler: 2-1227                    [1, 64, 64, 64]           --
│    └─ReLU: 2-1228                      [1, 64, 64, 64]           --
│    └─Empty: 2-1229                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1230                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-93                [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-1231        --                        --
│    └─One: 2-1232                       [1]                       --
│    └─OutputScale: 2-1233               --                        --
│    └─Empty: 2-1234                     [64, 64, 3, 3]            --
│    └─Empty: 2-1235                     [64, 64, 3, 3]            --
│    └─Empty: 2-1236                     [64]                      --
│    └─Empty: 2-1237                     [64]                      --
│    └─BatchNorm2d: 2-1238               [1, 64, 64, 64]           --
│    └─Scaler: 2-1239                    [1, 64, 64, 64]           --
│    └─ReLU: 2-1240                      [1, 64, 64, 64]           --
│    └─Empty: 2-1241                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1242                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-94         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1243                 [1, 64, 32, 32]           --
│    └─Empty: 2-1244                     [1, 64, 32, 32]           --
│    └─Empty: 2-1245                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1246        --                        --
│    └─One: 2-1247                       [1]                       --
│    └─OutputScale: 2-1248               --                        --
│    └─Empty: 2-1249                     [64, 64, 3, 3]            --
│    └─Empty: 2-1250                     [64, 64, 3, 3]            --
│    └─Empty: 2-1251                     [64]                      --
│    └─Empty: 2-1252                     [64]                      --
│    └─BatchNorm2d: 2-1253               [1, 64, 32, 32]           --
│    └─Scaler: 2-1254                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1255                      [1, 64, 32, 32]           --
│    └─Empty: 2-1256                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1257                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-95                [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1258        --                        --
│    └─One: 2-1259                       [1]                       --
│    └─OutputScale: 2-1260               --                        --
│    └─Empty: 2-1261                     [64, 64, 1, 1]            --
│    └─Empty: 2-1262                     [64, 64, 1, 1]            --
│    └─Empty: 2-1263                     [64]                      --
│    └─Empty: 2-1264                     [64]                      --
│    └─BatchNorm2d: 2-1265               [1, 64, 32, 32]           --
│    └─Scaler: 2-1266                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1267                      [1, 64, 32, 32]           --
│    └─Empty: 2-1268                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1269                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-96         [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1270                 [1, 64, 32, 32]           --
│    └─Empty: 2-1271                     [1, 64, 32, 32]           --
│    └─Empty: 2-1272                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1273        --                        --
│    └─One: 2-1274                       [1]                       --
│    └─OutputScale: 2-1275               --                        --
│    └─Empty: 2-1276                     [64, 64, 3, 3]            --
│    └─Empty: 2-1277                     [64, 64, 3, 3]            --
│    └─Empty: 2-1278                     [64]                      --
│    └─Empty: 2-1279                     [64]                      --
│    └─BatchNorm2d: 2-1280               [1, 64, 32, 32]           --
│    └─Scaler: 2-1281                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1282                      [1, 64, 32, 32]           --
│    └─Empty: 2-1283                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1284                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-97         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1285                 [1, 64, 16, 16]           --
│    └─Empty: 2-1286                     [1, 64, 16, 16]           --
│    └─Empty: 2-1287                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1288        --                        --
│    └─One: 2-1289                       [1]                       --
│    └─OutputScale: 2-1290               --                        --
│    └─Empty: 2-1291                     [64, 64, 3, 3]            --
│    └─Empty: 2-1292                     [64, 64, 3, 3]            --
│    └─Empty: 2-1293                     [64]                      --
│    └─Empty: 2-1294                     [64]                      --
│    └─BatchNorm2d: 2-1295               [1, 64, 16, 16]           --
│    └─Scaler: 2-1296                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1297                      [1, 64, 16, 16]           --
│    └─Empty: 2-1298                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1299                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-98                [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1300        --                        --
│    └─One: 2-1301                       [1]                       --
│    └─OutputScale: 2-1302               --                        --
│    └─Empty: 2-1303                     [64, 64, 1, 1]            --
│    └─Empty: 2-1304                     [64, 64, 1, 1]            --
│    └─Empty: 2-1305                     [64]                      --
│    └─Empty: 2-1306                     [64]                      --
│    └─BatchNorm2d: 2-1307               [1, 64, 16, 16]           --
│    └─Scaler: 2-1308                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1309                      [1, 64, 16, 16]           --
│    └─Empty: 2-1310                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1311                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-99         [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1312                 [1, 64, 16, 16]           --
│    └─Empty: 2-1313                     [1, 64, 16, 16]           --
│    └─Empty: 2-1314                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1315        --                        --
│    └─One: 2-1316                       [1]                       --
│    └─OutputScale: 2-1317               --                        --
│    └─Empty: 2-1318                     [64, 64, 3, 3]            --
│    └─Empty: 2-1319                     [64, 64, 3, 3]            --
│    └─Empty: 2-1320                     [64]                      --
│    └─Empty: 2-1321                     [64]                      --
│    └─BatchNorm2d: 2-1322               [1, 64, 16, 16]           --
│    └─Scaler: 2-1323                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1324                      [1, 64, 16, 16]           --
│    └─Empty: 2-1325                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1326                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-100        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1327                 [1, 64, 8, 8]             --
│    └─Empty: 2-1328                     [1, 64, 8, 8]             --
│    └─Empty: 2-1329                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1330        --                        --
│    └─One: 2-1331                       [1]                       --
│    └─OutputScale: 2-1332               --                        --
│    └─Empty: 2-1333                     [64, 64, 1, 1]            --
│    └─Empty: 2-1334                     [64, 64, 1, 1]            --
│    └─Empty: 2-1335                     [64]                      --
│    └─Empty: 2-1336                     [64]                      --
│    └─BatchNorm2d: 2-1337               [1, 64, 8, 8]             --
│    └─Scaler: 2-1338                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1339                      [1, 64, 8, 8]             --
│    └─Empty: 2-1340                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1341                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-101               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-1342        --                        --
│    └─One: 2-1343                       [1]                       --
│    └─OutputScale: 2-1344               --                        --
│    └─Empty: 2-1345                     [64, 64, 1, 1]            --
│    └─Empty: 2-1346                     [64, 64, 1, 1]            --
│    └─Empty: 2-1347                     [64]                      --
│    └─Empty: 2-1348                     [64]                      --
│    └─BatchNorm2d: 2-1349               [1, 64, 8, 8]             --
│    └─Scaler: 2-1350                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1351                      [1, 64, 8, 8]             --
│    └─Empty: 2-1352                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1353                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-102        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1354                 [1, 64, 8, 8]             --
│    └─Empty: 2-1355                     [1, 64, 8, 8]             --
│    └─Empty: 2-1356                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1357        --                        --
│    └─One: 2-1358                       [1]                       --
│    └─OutputScale: 2-1359               --                        --
│    └─Empty: 2-1360                     [64, 64, 3, 3]            --
│    └─Empty: 2-1361                     [64, 64, 3, 3]            --
│    └─Empty: 2-1362                     [64]                      --
│    └─Empty: 2-1363                     [64]                      --
│    └─BatchNorm2d: 2-1364               [1, 64, 8, 8]             --
│    └─Scaler: 2-1365                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1366                      [1, 64, 8, 8]             --
│    └─Empty: 2-1367                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1368                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-103               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1369        --                        --
│    └─One: 2-1370                       [1]                       --
│    └─OutputScale: 2-1371               --                        --
│    └─Empty: 2-1372                     [64, 3, 1, 1]             --
│    └─Empty: 2-1373                     [64, 3, 1, 1]             --
│    └─Empty: 2-1374                     [64]                      --
│    └─Empty: 2-1375                     [64]                      --
│    └─BatchNorm2d: 2-1376               [1, 64, 256, 256]         --
│    └─Scaler: 2-1377                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1378                      [1, 64, 256, 256]         --
│    └─Empty: 2-1379                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1380                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-104               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1381        --                        --
│    └─One: 2-1382                       [1]                       --
│    └─OutputScale: 2-1383               --                        --
│    └─Empty: 2-1384                     [64, 64, 3, 3]            --
│    └─Empty: 2-1385                     [64, 64, 3, 3]            --
│    └─Empty: 2-1386                     [64]                      --
│    └─Empty: 2-1387                     [64]                      --
│    └─BatchNorm2d: 2-1388               [1, 64, 256, 256]         --
│    └─Scaler: 2-1389                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1390                      [1, 64, 256, 256]         --
│    └─Empty: 2-1391                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1392                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-105               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1393        --                        --
│    └─One: 2-1394                       [1]                       --
│    └─OutputScale: 2-1395               --                        --
│    └─Empty: 2-1396                     [64, 64, 1, 1]            --
│    └─Empty: 2-1397                     [64, 64, 1, 1]            --
│    └─Empty: 2-1398                     [64]                      --
│    └─Empty: 2-1399                     [64]                      --
│    └─BatchNorm2d: 2-1400               [1, 64, 256, 256]         --
│    └─Scaler: 2-1401                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1402                      [1, 64, 256, 256]         --
│    └─Empty: 2-1403                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1404                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-106               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1405        --                        --
│    └─One: 2-1406                       [1]                       --
│    └─OutputScale: 2-1407               --                        --
│    └─Empty: 2-1408                     [64, 64, 3, 3]            --
│    └─Empty: 2-1409                     [64, 64, 3, 3]            --
│    └─Empty: 2-1410                     [64]                      --
│    └─Empty: 2-1411                     [64]                      --
│    └─BatchNorm2d: 2-1412               [1, 64, 256, 256]         --
│    └─Scaler: 2-1413                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1414                      [1, 64, 256, 256]         --
│    └─Empty: 2-1415                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1416                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-107        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-1417                 [1, 64, 128, 128]         --
│    └─Empty: 2-1418                     [1, 64, 128, 128]         --
│    └─Empty: 2-1419                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-1420        --                        --
│    └─One: 2-1421                       [1]                       --
│    └─OutputScale: 2-1422               --                        --
│    └─Empty: 2-1423                     [64, 64, 3, 3]            --
│    └─Empty: 2-1424                     [64, 64, 3, 3]            --
│    └─Empty: 2-1425                     [64]                      --
│    └─Empty: 2-1426                     [64]                      --
│    └─BatchNorm2d: 2-1427               [1, 64, 128, 128]         --
│    └─Scaler: 2-1428                    [1, 64, 128, 128]         --
│    └─ReLU: 2-1429                      [1, 64, 128, 128]         --
│    └─Empty: 2-1430                     [1, 64, 128, 128]         --
│    └─Clamp: 2-1431                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-108               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1432        --                        --
│    └─One: 2-1433                       [1]                       --
│    └─OutputScale: 2-1434               --                        --
│    └─Empty: 2-1435                     [64, 64, 3, 3]            --
│    └─Empty: 2-1436                     [64, 64, 3, 3]            --
│    └─Empty: 2-1437                     [64]                      --
│    └─Empty: 2-1438                     [64]                      --
│    └─BatchNorm2d: 2-1439               [1, 64, 128, 128]         --
│    └─Scaler: 2-1440                    [1, 64, 128, 128]         --
│    └─ReLU: 2-1441                      [1, 64, 128, 128]         --
│    └─Empty: 2-1442                     [1, 64, 128, 128]         --
│    └─Clamp: 2-1443                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-109        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-1444                 [1, 64, 64, 64]           --
│    └─Empty: 2-1445                     [1, 64, 64, 64]           --
│    └─Empty: 2-1446                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-1447        --                        --
│    └─One: 2-1448                       [1]                       --
│    └─OutputScale: 2-1449               --                        --
│    └─Empty: 2-1450                     [64, 64, 3, 3]            --
│    └─Empty: 2-1451                     [64, 64, 3, 3]            --
│    └─Empty: 2-1452                     [64]                      --
│    └─Empty: 2-1453                     [64]                      --
│    └─BatchNorm2d: 2-1454               [1, 64, 64, 64]           --
│    └─Scaler: 2-1455                    [1, 64, 64, 64]           --
│    └─ReLU: 2-1456                      [1, 64, 64, 64]           --
│    └─Empty: 2-1457                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1458                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-110               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-1459        --                        --
│    └─One: 2-1460                       [1]                       --
│    └─OutputScale: 2-1461               --                        --
│    └─Empty: 2-1462                     [64, 64, 3, 3]            --
│    └─Empty: 2-1463                     [64, 64, 3, 3]            --
│    └─Empty: 2-1464                     [64]                      --
│    └─Empty: 2-1465                     [64]                      --
│    └─BatchNorm2d: 2-1466               [1, 64, 64, 64]           --
│    └─Scaler: 2-1467                    [1, 64, 64, 64]           --
│    └─ReLU: 2-1468                      [1, 64, 64, 64]           --
│    └─Empty: 2-1469                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1470                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-111        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1471                 [1, 64, 32, 32]           --
│    └─Empty: 2-1472                     [1, 64, 32, 32]           --
│    └─Empty: 2-1473                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1474        --                        --
│    └─One: 2-1475                       [1]                       --
│    └─OutputScale: 2-1476               --                        --
│    └─Empty: 2-1477                     [64, 64, 3, 3]            --
│    └─Empty: 2-1478                     [64, 64, 3, 3]            --
│    └─Empty: 2-1479                     [64]                      --
│    └─Empty: 2-1480                     [64]                      --
│    └─BatchNorm2d: 2-1481               [1, 64, 32, 32]           --
│    └─Scaler: 2-1482                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1483                      [1, 64, 32, 32]           --
│    └─Empty: 2-1484                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1485                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-112               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1486        --                        --
│    └─One: 2-1487                       [1]                       --
│    └─OutputScale: 2-1488               --                        --
│    └─Empty: 2-1489                     [64, 64, 1, 1]            --
│    └─Empty: 2-1490                     [64, 64, 1, 1]            --
│    └─Empty: 2-1491                     [64]                      --
│    └─Empty: 2-1492                     [64]                      --
│    └─BatchNorm2d: 2-1493               [1, 64, 32, 32]           --
│    └─Scaler: 2-1494                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1495                      [1, 64, 32, 32]           --
│    └─Empty: 2-1496                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1497                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-113        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1498                 [1, 64, 32, 32]           --
│    └─Empty: 2-1499                     [1, 64, 32, 32]           --
│    └─Empty: 2-1500                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1501        --                        --
│    └─One: 2-1502                       [1]                       --
│    └─OutputScale: 2-1503               --                        --
│    └─Empty: 2-1504                     [64, 64, 3, 3]            --
│    └─Empty: 2-1505                     [64, 64, 3, 3]            --
│    └─Empty: 2-1506                     [64]                      --
│    └─Empty: 2-1507                     [64]                      --
│    └─BatchNorm2d: 2-1508               [1, 64, 32, 32]           --
│    └─Scaler: 2-1509                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1510                      [1, 64, 32, 32]           --
│    └─Empty: 2-1511                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1512                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-114        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1513                 [1, 64, 16, 16]           --
│    └─Empty: 2-1514                     [1, 64, 16, 16]           --
│    └─Empty: 2-1515                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1516        --                        --
│    └─One: 2-1517                       [1]                       --
│    └─OutputScale: 2-1518               --                        --
│    └─Empty: 2-1519                     [64, 64, 3, 3]            --
│    └─Empty: 2-1520                     [64, 64, 3, 3]            --
│    └─Empty: 2-1521                     [64]                      --
│    └─Empty: 2-1522                     [64]                      --
│    └─BatchNorm2d: 2-1523               [1, 64, 16, 16]           --
│    └─Scaler: 2-1524                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1525                      [1, 64, 16, 16]           --
│    └─Empty: 2-1526                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1527                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-115               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1528        --                        --
│    └─One: 2-1529                       [1]                       --
│    └─OutputScale: 2-1530               --                        --
│    └─Empty: 2-1531                     [64, 64, 1, 1]            --
│    └─Empty: 2-1532                     [64, 64, 1, 1]            --
│    └─Empty: 2-1533                     [64]                      --
│    └─Empty: 2-1534                     [64]                      --
│    └─BatchNorm2d: 2-1535               [1, 64, 16, 16]           --
│    └─Scaler: 2-1536                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1537                      [1, 64, 16, 16]           --
│    └─Empty: 2-1538                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1539                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-116        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1540                 [1, 64, 16, 16]           --
│    └─Empty: 2-1541                     [1, 64, 16, 16]           --
│    └─Empty: 2-1542                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1543        --                        --
│    └─One: 2-1544                       [1]                       --
│    └─OutputScale: 2-1545               --                        --
│    └─Empty: 2-1546                     [64, 64, 3, 3]            --
│    └─Empty: 2-1547                     [64, 64, 3, 3]            --
│    └─Empty: 2-1548                     [64]                      --
│    └─Empty: 2-1549                     [64]                      --
│    └─BatchNorm2d: 2-1550               [1, 64, 16, 16]           --
│    └─Scaler: 2-1551                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1552                      [1, 64, 16, 16]           --
│    └─Empty: 2-1553                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1554                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-117        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1555                 [1, 64, 8, 8]             --
│    └─Empty: 2-1556                     [1, 64, 8, 8]             --
│    └─Empty: 2-1557                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1558        --                        --
│    └─One: 2-1559                       [1]                       --
│    └─OutputScale: 2-1560               --                        --
│    └─Empty: 2-1561                     [64, 64, 1, 1]            --
│    └─Empty: 2-1562                     [64, 64, 1, 1]            --
│    └─Empty: 2-1563                     [64]                      --
│    └─Empty: 2-1564                     [64]                      --
│    └─BatchNorm2d: 2-1565               [1, 64, 8, 8]             --
│    └─Scaler: 2-1566                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1567                      [1, 64, 8, 8]             --
│    └─Empty: 2-1568                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1569                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-118               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-1570        --                        --
│    └─One: 2-1571                       [1]                       --
│    └─OutputScale: 2-1572               --                        --
│    └─Empty: 2-1573                     [64, 64, 1, 1]            --
│    └─Empty: 2-1574                     [64, 64, 1, 1]            --
│    └─Empty: 2-1575                     [64]                      --
│    └─Empty: 2-1576                     [64]                      --
│    └─BatchNorm2d: 2-1577               [1, 64, 8, 8]             --
│    └─Scaler: 2-1578                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1579                      [1, 64, 8, 8]             --
│    └─Empty: 2-1580                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1581                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-119        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1582                 [1, 64, 8, 8]             --
│    └─Empty: 2-1583                     [1, 64, 8, 8]             --
│    └─Empty: 2-1584                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1585        --                        --
│    └─One: 2-1586                       [1]                       --
│    └─OutputScale: 2-1587               --                        --
│    └─Empty: 2-1588                     [64, 64, 3, 3]            --
│    └─Empty: 2-1589                     [64, 64, 3, 3]            --
│    └─Empty: 2-1590                     [64]                      --
│    └─Empty: 2-1591                     [64]                      --
│    └─BatchNorm2d: 2-1592               [1, 64, 8, 8]             --
│    └─Scaler: 2-1593                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1594                      [1, 64, 8, 8]             --
│    └─Empty: 2-1595                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1596                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-120               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1597        --                        --
│    └─One: 2-1598                       [1]                       --
│    └─OutputScale: 2-1599               --                        --
│    └─Empty: 2-1600                     [64, 3, 1, 1]             --
│    └─Empty: 2-1601                     [64, 3, 1, 1]             --
│    └─Empty: 2-1602                     [64]                      --
│    └─Empty: 2-1603                     [64]                      --
│    └─BatchNorm2d: 2-1604               [1, 64, 256, 256]         --
│    └─Scaler: 2-1605                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1606                      [1, 64, 256, 256]         --
│    └─Empty: 2-1607                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1608                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-121               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1609        --                        --
│    └─One: 2-1610                       [1]                       --
│    └─OutputScale: 2-1611               --                        --
│    └─Empty: 2-1612                     [64, 64, 3, 3]            --
│    └─Empty: 2-1613                     [64, 64, 3, 3]            --
│    └─Empty: 2-1614                     [64]                      --
│    └─Empty: 2-1615                     [64]                      --
│    └─BatchNorm2d: 2-1616               [1, 64, 256, 256]         --
│    └─Scaler: 2-1617                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1618                      [1, 64, 256, 256]         --
│    └─Empty: 2-1619                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1620                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-122               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1621        --                        --
│    └─One: 2-1622                       [1]                       --
│    └─OutputScale: 2-1623               --                        --
│    └─Empty: 2-1624                     [64, 64, 1, 1]            --
│    └─Empty: 2-1625                     [64, 64, 1, 1]            --
│    └─Empty: 2-1626                     [64]                      --
│    └─Empty: 2-1627                     [64]                      --
│    └─BatchNorm2d: 2-1628               [1, 64, 256, 256]         --
│    └─Scaler: 2-1629                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1630                      [1, 64, 256, 256]         --
│    └─Empty: 2-1631                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1632                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-123               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1633        --                        --
│    └─One: 2-1634                       [1]                       --
│    └─OutputScale: 2-1635               --                        --
│    └─Empty: 2-1636                     [64, 64, 3, 3]            --
│    └─Empty: 2-1637                     [64, 64, 3, 3]            --
│    └─Empty: 2-1638                     [64]                      --
│    └─Empty: 2-1639                     [64]                      --
│    └─BatchNorm2d: 2-1640               [1, 64, 256, 256]         --
│    └─Scaler: 2-1641                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1642                      [1, 64, 256, 256]         --
│    └─Empty: 2-1643                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1644                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-124        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-1645                 [1, 64, 128, 128]         --
│    └─Empty: 2-1646                     [1, 64, 128, 128]         --
│    └─Empty: 2-1647                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-1648        --                        --
│    └─One: 2-1649                       [1]                       --
│    └─OutputScale: 2-1650               --                        --
│    └─Empty: 2-1651                     [64, 64, 3, 3]            --
│    └─Empty: 2-1652                     [64, 64, 3, 3]            --
│    └─Empty: 2-1653                     [64]                      --
│    └─Empty: 2-1654                     [64]                      --
│    └─BatchNorm2d: 2-1655               [1, 64, 128, 128]         --
│    └─Scaler: 2-1656                    [1, 64, 128, 128]         --
│    └─ReLU: 2-1657                      [1, 64, 128, 128]         --
│    └─Empty: 2-1658                     [1, 64, 128, 128]         --
│    └─Clamp: 2-1659                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-125               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1660        --                        --
│    └─One: 2-1661                       [1]                       --
│    └─OutputScale: 2-1662               --                        --
│    └─Empty: 2-1663                     [64, 64, 3, 3]            --
│    └─Empty: 2-1664                     [64, 64, 3, 3]            --
│    └─Empty: 2-1665                     [64]                      --
│    └─Empty: 2-1666                     [64]                      --
│    └─BatchNorm2d: 2-1667               [1, 64, 128, 128]         --
│    └─Scaler: 2-1668                    [1, 64, 128, 128]         --
│    └─ReLU: 2-1669                      [1, 64, 128, 128]         --
│    └─Empty: 2-1670                     [1, 64, 128, 128]         --
│    └─Clamp: 2-1671                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-126        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-1672                 [1, 64, 64, 64]           --
│    └─Empty: 2-1673                     [1, 64, 64, 64]           --
│    └─Empty: 2-1674                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-1675        --                        --
│    └─One: 2-1676                       [1]                       --
│    └─OutputScale: 2-1677               --                        --
│    └─Empty: 2-1678                     [64, 64, 3, 3]            --
│    └─Empty: 2-1679                     [64, 64, 3, 3]            --
│    └─Empty: 2-1680                     [64]                      --
│    └─Empty: 2-1681                     [64]                      --
│    └─BatchNorm2d: 2-1682               [1, 64, 64, 64]           --
│    └─Scaler: 2-1683                    [1, 64, 64, 64]           --
│    └─ReLU: 2-1684                      [1, 64, 64, 64]           --
│    └─Empty: 2-1685                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1686                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-127               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-1687        --                        --
│    └─One: 2-1688                       [1]                       --
│    └─OutputScale: 2-1689               --                        --
│    └─Empty: 2-1690                     [64, 64, 3, 3]            --
│    └─Empty: 2-1691                     [64, 64, 3, 3]            --
│    └─Empty: 2-1692                     [64]                      --
│    └─Empty: 2-1693                     [64]                      --
│    └─BatchNorm2d: 2-1694               [1, 64, 64, 64]           --
│    └─Scaler: 2-1695                    [1, 64, 64, 64]           --
│    └─ReLU: 2-1696                      [1, 64, 64, 64]           --
│    └─Empty: 2-1697                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1698                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-128        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1699                 [1, 64, 32, 32]           --
│    └─Empty: 2-1700                     [1, 64, 32, 32]           --
│    └─Empty: 2-1701                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1702        --                        --
│    └─One: 2-1703                       [1]                       --
│    └─OutputScale: 2-1704               --                        --
│    └─Empty: 2-1705                     [64, 64, 3, 3]            --
│    └─Empty: 2-1706                     [64, 64, 3, 3]            --
│    └─Empty: 2-1707                     [64]                      --
│    └─Empty: 2-1708                     [64]                      --
│    └─BatchNorm2d: 2-1709               [1, 64, 32, 32]           --
│    └─Scaler: 2-1710                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1711                      [1, 64, 32, 32]           --
│    └─Empty: 2-1712                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1713                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-129               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1714        --                        --
│    └─One: 2-1715                       [1]                       --
│    └─OutputScale: 2-1716               --                        --
│    └─Empty: 2-1717                     [64, 64, 1, 1]            --
│    └─Empty: 2-1718                     [64, 64, 1, 1]            --
│    └─Empty: 2-1719                     [64]                      --
│    └─Empty: 2-1720                     [64]                      --
│    └─BatchNorm2d: 2-1721               [1, 64, 32, 32]           --
│    └─Scaler: 2-1722                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1723                      [1, 64, 32, 32]           --
│    └─Empty: 2-1724                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1725                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-130        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1726                 [1, 64, 32, 32]           --
│    └─Empty: 2-1727                     [1, 64, 32, 32]           --
│    └─Empty: 2-1728                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1729        --                        --
│    └─One: 2-1730                       [1]                       --
│    └─OutputScale: 2-1731               --                        --
│    └─Empty: 2-1732                     [64, 64, 3, 3]            --
│    └─Empty: 2-1733                     [64, 64, 3, 3]            --
│    └─Empty: 2-1734                     [64]                      --
│    └─Empty: 2-1735                     [64]                      --
│    └─BatchNorm2d: 2-1736               [1, 64, 32, 32]           --
│    └─Scaler: 2-1737                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1738                      [1, 64, 32, 32]           --
│    └─Empty: 2-1739                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1740                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-131        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1741                 [1, 64, 16, 16]           --
│    └─Empty: 2-1742                     [1, 64, 16, 16]           --
│    └─Empty: 2-1743                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1744        --                        --
│    └─One: 2-1745                       [1]                       --
│    └─OutputScale: 2-1746               --                        --
│    └─Empty: 2-1747                     [64, 64, 3, 3]            --
│    └─Empty: 2-1748                     [64, 64, 3, 3]            --
│    └─Empty: 2-1749                     [64]                      --
│    └─Empty: 2-1750                     [64]                      --
│    └─BatchNorm2d: 2-1751               [1, 64, 16, 16]           --
│    └─Scaler: 2-1752                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1753                      [1, 64, 16, 16]           --
│    └─Empty: 2-1754                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1755                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-132               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1756        --                        --
│    └─One: 2-1757                       [1]                       --
│    └─OutputScale: 2-1758               --                        --
│    └─Empty: 2-1759                     [64, 64, 1, 1]            --
│    └─Empty: 2-1760                     [64, 64, 1, 1]            --
│    └─Empty: 2-1761                     [64]                      --
│    └─Empty: 2-1762                     [64]                      --
│    └─BatchNorm2d: 2-1763               [1, 64, 16, 16]           --
│    └─Scaler: 2-1764                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1765                      [1, 64, 16, 16]           --
│    └─Empty: 2-1766                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1767                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-133        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1768                 [1, 64, 16, 16]           --
│    └─Empty: 2-1769                     [1, 64, 16, 16]           --
│    └─Empty: 2-1770                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1771        --                        --
│    └─One: 2-1772                       [1]                       --
│    └─OutputScale: 2-1773               --                        --
│    └─Empty: 2-1774                     [64, 64, 3, 3]            --
│    └─Empty: 2-1775                     [64, 64, 3, 3]            --
│    └─Empty: 2-1776                     [64]                      --
│    └─Empty: 2-1777                     [64]                      --
│    └─BatchNorm2d: 2-1778               [1, 64, 16, 16]           --
│    └─Scaler: 2-1779                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1780                      [1, 64, 16, 16]           --
│    └─Empty: 2-1781                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1782                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-134        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1783                 [1, 64, 8, 8]             --
│    └─Empty: 2-1784                     [1, 64, 8, 8]             --
│    └─Empty: 2-1785                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1786        --                        --
│    └─One: 2-1787                       [1]                       --
│    └─OutputScale: 2-1788               --                        --
│    └─Empty: 2-1789                     [64, 64, 1, 1]            --
│    └─Empty: 2-1790                     [64, 64, 1, 1]            --
│    └─Empty: 2-1791                     [64]                      --
│    └─Empty: 2-1792                     [64]                      --
│    └─BatchNorm2d: 2-1793               [1, 64, 8, 8]             --
│    └─Scaler: 2-1794                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1795                      [1, 64, 8, 8]             --
│    └─Empty: 2-1796                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1797                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-135               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-1798        --                        --
│    └─One: 2-1799                       [1]                       --
│    └─OutputScale: 2-1800               --                        --
│    └─Empty: 2-1801                     [64, 64, 1, 1]            --
│    └─Empty: 2-1802                     [64, 64, 1, 1]            --
│    └─Empty: 2-1803                     [64]                      --
│    └─Empty: 2-1804                     [64]                      --
│    └─BatchNorm2d: 2-1805               [1, 64, 8, 8]             --
│    └─Scaler: 2-1806                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1807                      [1, 64, 8, 8]             --
│    └─Empty: 2-1808                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1809                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-136        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1810                 [1, 64, 8, 8]             --
│    └─Empty: 2-1811                     [1, 64, 8, 8]             --
│    └─Empty: 2-1812                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1813        --                        --
│    └─One: 2-1814                       [1]                       --
│    └─OutputScale: 2-1815               --                        --
│    └─Empty: 2-1816                     [64, 64, 3, 3]            --
│    └─Empty: 2-1817                     [64, 64, 3, 3]            --
│    └─Empty: 2-1818                     [64]                      --
│    └─Empty: 2-1819                     [64]                      --
│    └─BatchNorm2d: 2-1820               [1, 64, 8, 8]             --
│    └─Scaler: 2-1821                    [1, 64, 8, 8]             --
│    └─ReLU: 2-1822                      [1, 64, 8, 8]             --
│    └─Empty: 2-1823                     [1, 64, 8, 8]             --
│    └─Clamp: 2-1824                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-137               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1825        --                        --
│    └─One: 2-1826                       [1]                       --
│    └─OutputScale: 2-1827               --                        --
│    └─Empty: 2-1828                     [64, 3, 1, 1]             --
│    └─Empty: 2-1829                     [64, 3, 1, 1]             --
│    └─Empty: 2-1830                     [64]                      --
│    └─Empty: 2-1831                     [64]                      --
│    └─BatchNorm2d: 2-1832               [1, 64, 256, 256]         --
│    └─Scaler: 2-1833                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1834                      [1, 64, 256, 256]         --
│    └─Empty: 2-1835                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1836                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-138               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1837        --                        --
│    └─One: 2-1838                       [1]                       --
│    └─OutputScale: 2-1839               --                        --
│    └─Empty: 2-1840                     [64, 64, 3, 3]            --
│    └─Empty: 2-1841                     [64, 64, 3, 3]            --
│    └─Empty: 2-1842                     [64]                      --
│    └─Empty: 2-1843                     [64]                      --
│    └─BatchNorm2d: 2-1844               [1, 64, 256, 256]         --
│    └─Scaler: 2-1845                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1846                      [1, 64, 256, 256]         --
│    └─Empty: 2-1847                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1848                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-139               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1849        --                        --
│    └─One: 2-1850                       [1]                       --
│    └─OutputScale: 2-1851               --                        --
│    └─Empty: 2-1852                     [64, 64, 1, 1]            --
│    └─Empty: 2-1853                     [64, 64, 1, 1]            --
│    └─Empty: 2-1854                     [64]                      --
│    └─Empty: 2-1855                     [64]                      --
│    └─BatchNorm2d: 2-1856               [1, 64, 256, 256]         --
│    └─Scaler: 2-1857                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1858                      [1, 64, 256, 256]         --
│    └─Empty: 2-1859                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1860                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-140               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-1861        --                        --
│    └─One: 2-1862                       [1]                       --
│    └─OutputScale: 2-1863               --                        --
│    └─Empty: 2-1864                     [64, 64, 3, 3]            --
│    └─Empty: 2-1865                     [64, 64, 3, 3]            --
│    └─Empty: 2-1866                     [64]                      --
│    └─Empty: 2-1867                     [64]                      --
│    └─BatchNorm2d: 2-1868               [1, 64, 256, 256]         --
│    └─Scaler: 2-1869                    [1, 64, 256, 256]         --
│    └─ReLU: 2-1870                      [1, 64, 256, 256]         --
│    └─Empty: 2-1871                     [1, 64, 256, 256]         --
│    └─Clamp: 2-1872                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-141        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-1873                 [1, 64, 128, 128]         --
│    └─Empty: 2-1874                     [1, 64, 128, 128]         --
│    └─Empty: 2-1875                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-1876        --                        --
│    └─One: 2-1877                       [1]                       --
│    └─OutputScale: 2-1878               --                        --
│    └─Empty: 2-1879                     [64, 64, 3, 3]            --
│    └─Empty: 2-1880                     [64, 64, 3, 3]            --
│    └─Empty: 2-1881                     [64]                      --
│    └─Empty: 2-1882                     [64]                      --
│    └─BatchNorm2d: 2-1883               [1, 64, 128, 128]         --
│    └─Scaler: 2-1884                    [1, 64, 128, 128]         --
│    └─ReLU: 2-1885                      [1, 64, 128, 128]         --
│    └─Empty: 2-1886                     [1, 64, 128, 128]         --
│    └─Clamp: 2-1887                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-142               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1888        --                        --
│    └─One: 2-1889                       [1]                       --
│    └─OutputScale: 2-1890               --                        --
│    └─Empty: 2-1891                     [64, 64, 3, 3]            --
│    └─Empty: 2-1892                     [64, 64, 3, 3]            --
│    └─Empty: 2-1893                     [64]                      --
│    └─Empty: 2-1894                     [64]                      --
│    └─BatchNorm2d: 2-1895               [1, 64, 128, 128]         --
│    └─Scaler: 2-1896                    [1, 64, 128, 128]         --
│    └─ReLU: 2-1897                      [1, 64, 128, 128]         --
│    └─Empty: 2-1898                     [1, 64, 128, 128]         --
│    └─Clamp: 2-1899                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-143        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-1900                 [1, 64, 64, 64]           --
│    └─Empty: 2-1901                     [1, 64, 64, 64]           --
│    └─Empty: 2-1902                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-1903        --                        --
│    └─One: 2-1904                       [1]                       --
│    └─OutputScale: 2-1905               --                        --
│    └─Empty: 2-1906                     [64, 64, 3, 3]            --
│    └─Empty: 2-1907                     [64, 64, 3, 3]            --
│    └─Empty: 2-1908                     [64]                      --
│    └─Empty: 2-1909                     [64]                      --
│    └─BatchNorm2d: 2-1910               [1, 64, 64, 64]           --
│    └─Scaler: 2-1911                    [1, 64, 64, 64]           --
│    └─ReLU: 2-1912                      [1, 64, 64, 64]           --
│    └─Empty: 2-1913                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1914                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-144               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-1915        --                        --
│    └─One: 2-1916                       [1]                       --
│    └─OutputScale: 2-1917               --                        --
│    └─Empty: 2-1918                     [64, 64, 3, 3]            --
│    └─Empty: 2-1919                     [64, 64, 3, 3]            --
│    └─Empty: 2-1920                     [64]                      --
│    └─Empty: 2-1921                     [64]                      --
│    └─BatchNorm2d: 2-1922               [1, 64, 64, 64]           --
│    └─Scaler: 2-1923                    [1, 64, 64, 64]           --
│    └─ReLU: 2-1924                      [1, 64, 64, 64]           --
│    └─Empty: 2-1925                     [1, 64, 64, 64]           --
│    └─Clamp: 2-1926                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-145        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1927                 [1, 64, 32, 32]           --
│    └─Empty: 2-1928                     [1, 64, 32, 32]           --
│    └─Empty: 2-1929                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1930        --                        --
│    └─One: 2-1931                       [1]                       --
│    └─OutputScale: 2-1932               --                        --
│    └─Empty: 2-1933                     [64, 64, 3, 3]            --
│    └─Empty: 2-1934                     [64, 64, 3, 3]            --
│    └─Empty: 2-1935                     [64]                      --
│    └─Empty: 2-1936                     [64]                      --
│    └─BatchNorm2d: 2-1937               [1, 64, 32, 32]           --
│    └─Scaler: 2-1938                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1939                      [1, 64, 32, 32]           --
│    └─Empty: 2-1940                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1941                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-146               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1942        --                        --
│    └─One: 2-1943                       [1]                       --
│    └─OutputScale: 2-1944               --                        --
│    └─Empty: 2-1945                     [64, 64, 1, 1]            --
│    └─Empty: 2-1946                     [64, 64, 1, 1]            --
│    └─Empty: 2-1947                     [64]                      --
│    └─Empty: 2-1948                     [64]                      --
│    └─BatchNorm2d: 2-1949               [1, 64, 32, 32]           --
│    └─Scaler: 2-1950                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1951                      [1, 64, 32, 32]           --
│    └─Empty: 2-1952                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1953                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-147        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1954                 [1, 64, 32, 32]           --
│    └─Empty: 2-1955                     [1, 64, 32, 32]           --
│    └─Empty: 2-1956                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1957        --                        --
│    └─One: 2-1958                       [1]                       --
│    └─OutputScale: 2-1959               --                        --
│    └─Empty: 2-1960                     [64, 64, 3, 3]            --
│    └─Empty: 2-1961                     [64, 64, 3, 3]            --
│    └─Empty: 2-1962                     [64]                      --
│    └─Empty: 2-1963                     [64]                      --
│    └─BatchNorm2d: 2-1964               [1, 64, 32, 32]           --
│    └─Scaler: 2-1965                    [1, 64, 32, 32]           --
│    └─ReLU: 2-1966                      [1, 64, 32, 32]           --
│    └─Empty: 2-1967                     [1, 64, 32, 32]           --
│    └─Clamp: 2-1968                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-148        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1969                 [1, 64, 16, 16]           --
│    └─Empty: 2-1970                     [1, 64, 16, 16]           --
│    └─Empty: 2-1971                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1972        --                        --
│    └─One: 2-1973                       [1]                       --
│    └─OutputScale: 2-1974               --                        --
│    └─Empty: 2-1975                     [64, 64, 3, 3]            --
│    └─Empty: 2-1976                     [64, 64, 3, 3]            --
│    └─Empty: 2-1977                     [64]                      --
│    └─Empty: 2-1978                     [64]                      --
│    └─BatchNorm2d: 2-1979               [1, 64, 16, 16]           --
│    └─Scaler: 2-1980                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1981                      [1, 64, 16, 16]           --
│    └─Empty: 2-1982                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1983                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-149               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1984        --                        --
│    └─One: 2-1985                       [1]                       --
│    └─OutputScale: 2-1986               --                        --
│    └─Empty: 2-1987                     [64, 64, 1, 1]            --
│    └─Empty: 2-1988                     [64, 64, 1, 1]            --
│    └─Empty: 2-1989                     [64]                      --
│    └─Empty: 2-1990                     [64]                      --
│    └─BatchNorm2d: 2-1991               [1, 64, 16, 16]           --
│    └─Scaler: 2-1992                    [1, 64, 16, 16]           --
│    └─ReLU: 2-1993                      [1, 64, 16, 16]           --
│    └─Empty: 2-1994                     [1, 64, 16, 16]           --
│    └─Clamp: 2-1995                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-150        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1996                 [1, 64, 16, 16]           --
│    └─Empty: 2-1997                     [1, 64, 16, 16]           --
│    └─Empty: 2-1998                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1999        --                        --
│    └─One: 2-2000                       [1]                       --
│    └─OutputScale: 2-2001               --                        --
│    └─Empty: 2-2002                     [64, 64, 3, 3]            --
│    └─Empty: 2-2003                     [64, 64, 3, 3]            --
│    └─Empty: 2-2004                     [64]                      --
│    └─Empty: 2-2005                     [64]                      --
│    └─BatchNorm2d: 2-2006               [1, 64, 16, 16]           --
│    └─Scaler: 2-2007                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2008                      [1, 64, 16, 16]           --
│    └─Empty: 2-2009                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2010                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-151        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2011                 [1, 64, 8, 8]             --
│    └─Empty: 2-2012                     [1, 64, 8, 8]             --
│    └─Empty: 2-2013                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2014        --                        --
│    └─One: 2-2015                       [1]                       --
│    └─OutputScale: 2-2016               --                        --
│    └─Empty: 2-2017                     [64, 64, 1, 1]            --
│    └─Empty: 2-2018                     [64, 64, 1, 1]            --
│    └─Empty: 2-2019                     [64]                      --
│    └─Empty: 2-2020                     [64]                      --
│    └─BatchNorm2d: 2-2021               [1, 64, 8, 8]             --
│    └─Scaler: 2-2022                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2023                      [1, 64, 8, 8]             --
│    └─Empty: 2-2024                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2025                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-152               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-2026        --                        --
│    └─One: 2-2027                       [1]                       --
│    └─OutputScale: 2-2028               --                        --
│    └─Empty: 2-2029                     [64, 64, 1, 1]            --
│    └─Empty: 2-2030                     [64, 64, 1, 1]            --
│    └─Empty: 2-2031                     [64]                      --
│    └─Empty: 2-2032                     [64]                      --
│    └─BatchNorm2d: 2-2033               [1, 64, 8, 8]             --
│    └─Scaler: 2-2034                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2035                      [1, 64, 8, 8]             --
│    └─Empty: 2-2036                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2037                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-153        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2038                 [1, 64, 8, 8]             --
│    └─Empty: 2-2039                     [1, 64, 8, 8]             --
│    └─Empty: 2-2040                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2041        --                        --
│    └─One: 2-2042                       [1]                       --
│    └─OutputScale: 2-2043               --                        --
│    └─Empty: 2-2044                     [64, 64, 3, 3]            --
│    └─Empty: 2-2045                     [64, 64, 3, 3]            --
│    └─Empty: 2-2046                     [64]                      --
│    └─Empty: 2-2047                     [64]                      --
│    └─BatchNorm2d: 2-2048               [1, 64, 8, 8]             --
│    └─Scaler: 2-2049                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2050                      [1, 64, 8, 8]             --
│    └─Empty: 2-2051                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2052                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-154               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2053        --                        --
│    └─One: 2-2054                       [1]                       --
│    └─OutputScale: 2-2055               --                        --
│    └─Empty: 2-2056                     [64, 3, 1, 1]             --
│    └─Empty: 2-2057                     [64, 3, 1, 1]             --
│    └─Empty: 2-2058                     [64]                      --
│    └─Empty: 2-2059                     [64]                      --
│    └─BatchNorm2d: 2-2060               [1, 64, 256, 256]         --
│    └─Scaler: 2-2061                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2062                      [1, 64, 256, 256]         --
│    └─Empty: 2-2063                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2064                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-155               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2065        --                        --
│    └─One: 2-2066                       [1]                       --
│    └─OutputScale: 2-2067               --                        --
│    └─Empty: 2-2068                     [64, 64, 3, 3]            --
│    └─Empty: 2-2069                     [64, 64, 3, 3]            --
│    └─Empty: 2-2070                     [64]                      --
│    └─Empty: 2-2071                     [64]                      --
│    └─BatchNorm2d: 2-2072               [1, 64, 256, 256]         --
│    └─Scaler: 2-2073                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2074                      [1, 64, 256, 256]         --
│    └─Empty: 2-2075                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2076                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-156               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2077        --                        --
│    └─One: 2-2078                       [1]                       --
│    └─OutputScale: 2-2079               --                        --
│    └─Empty: 2-2080                     [64, 64, 1, 1]            --
│    └─Empty: 2-2081                     [64, 64, 1, 1]            --
│    └─Empty: 2-2082                     [64]                      --
│    └─Empty: 2-2083                     [64]                      --
│    └─BatchNorm2d: 2-2084               [1, 64, 256, 256]         --
│    └─Scaler: 2-2085                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2086                      [1, 64, 256, 256]         --
│    └─Empty: 2-2087                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2088                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-157               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2089        --                        --
│    └─One: 2-2090                       [1]                       --
│    └─OutputScale: 2-2091               --                        --
│    └─Empty: 2-2092                     [64, 64, 3, 3]            --
│    └─Empty: 2-2093                     [64, 64, 3, 3]            --
│    └─Empty: 2-2094                     [64]                      --
│    └─Empty: 2-2095                     [64]                      --
│    └─BatchNorm2d: 2-2096               [1, 64, 256, 256]         --
│    └─Scaler: 2-2097                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2098                      [1, 64, 256, 256]         --
│    └─Empty: 2-2099                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2100                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-158        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-2101                 [1, 64, 128, 128]         --
│    └─Empty: 2-2102                     [1, 64, 128, 128]         --
│    └─Empty: 2-2103                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-2104        --                        --
│    └─One: 2-2105                       [1]                       --
│    └─OutputScale: 2-2106               --                        --
│    └─Empty: 2-2107                     [64, 64, 3, 3]            --
│    └─Empty: 2-2108                     [64, 64, 3, 3]            --
│    └─Empty: 2-2109                     [64]                      --
│    └─Empty: 2-2110                     [64]                      --
│    └─BatchNorm2d: 2-2111               [1, 64, 128, 128]         --
│    └─Scaler: 2-2112                    [1, 64, 128, 128]         --
│    └─ReLU: 2-2113                      [1, 64, 128, 128]         --
│    └─Empty: 2-2114                     [1, 64, 128, 128]         --
│    └─Clamp: 2-2115                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-159               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-2116        --                        --
│    └─One: 2-2117                       [1]                       --
│    └─OutputScale: 2-2118               --                        --
│    └─Empty: 2-2119                     [64, 64, 3, 3]            --
│    └─Empty: 2-2120                     [64, 64, 3, 3]            --
│    └─Empty: 2-2121                     [64]                      --
│    └─Empty: 2-2122                     [64]                      --
│    └─BatchNorm2d: 2-2123               [1, 64, 128, 128]         --
│    └─Scaler: 2-2124                    [1, 64, 128, 128]         --
│    └─ReLU: 2-2125                      [1, 64, 128, 128]         --
│    └─Empty: 2-2126                     [1, 64, 128, 128]         --
│    └─Clamp: 2-2127                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-160        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-2128                 [1, 64, 64, 64]           --
│    └─Empty: 2-2129                     [1, 64, 64, 64]           --
│    └─Empty: 2-2130                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-2131        --                        --
│    └─One: 2-2132                       [1]                       --
│    └─OutputScale: 2-2133               --                        --
│    └─Empty: 2-2134                     [64, 64, 3, 3]            --
│    └─Empty: 2-2135                     [64, 64, 3, 3]            --
│    └─Empty: 2-2136                     [64]                      --
│    └─Empty: 2-2137                     [64]                      --
│    └─BatchNorm2d: 2-2138               [1, 64, 64, 64]           --
│    └─Scaler: 2-2139                    [1, 64, 64, 64]           --
│    └─ReLU: 2-2140                      [1, 64, 64, 64]           --
│    └─Empty: 2-2141                     [1, 64, 64, 64]           --
│    └─Clamp: 2-2142                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-161               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-2143        --                        --
│    └─One: 2-2144                       [1]                       --
│    └─OutputScale: 2-2145               --                        --
│    └─Empty: 2-2146                     [64, 64, 3, 3]            --
│    └─Empty: 2-2147                     [64, 64, 3, 3]            --
│    └─Empty: 2-2148                     [64]                      --
│    └─Empty: 2-2149                     [64]                      --
│    └─BatchNorm2d: 2-2150               [1, 64, 64, 64]           --
│    └─Scaler: 2-2151                    [1, 64, 64, 64]           --
│    └─ReLU: 2-2152                      [1, 64, 64, 64]           --
│    └─Empty: 2-2153                     [1, 64, 64, 64]           --
│    └─Clamp: 2-2154                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-162        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-2155                 [1, 64, 32, 32]           --
│    └─Empty: 2-2156                     [1, 64, 32, 32]           --
│    └─Empty: 2-2157                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-2158        --                        --
│    └─One: 2-2159                       [1]                       --
│    └─OutputScale: 2-2160               --                        --
│    └─Empty: 2-2161                     [64, 64, 3, 3]            --
│    └─Empty: 2-2162                     [64, 64, 3, 3]            --
│    └─Empty: 2-2163                     [64]                      --
│    └─Empty: 2-2164                     [64]                      --
│    └─BatchNorm2d: 2-2165               [1, 64, 32, 32]           --
│    └─Scaler: 2-2166                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2167                      [1, 64, 32, 32]           --
│    └─Empty: 2-2168                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2169                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-163               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-2170        --                        --
│    └─One: 2-2171                       [1]                       --
│    └─OutputScale: 2-2172               --                        --
│    └─Empty: 2-2173                     [64, 64, 1, 1]            --
│    └─Empty: 2-2174                     [64, 64, 1, 1]            --
│    └─Empty: 2-2175                     [64]                      --
│    └─Empty: 2-2176                     [64]                      --
│    └─BatchNorm2d: 2-2177               [1, 64, 32, 32]           --
│    └─Scaler: 2-2178                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2179                      [1, 64, 32, 32]           --
│    └─Empty: 2-2180                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2181                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-164        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-2182                 [1, 64, 32, 32]           --
│    └─Empty: 2-2183                     [1, 64, 32, 32]           --
│    └─Empty: 2-2184                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-2185        --                        --
│    └─One: 2-2186                       [1]                       --
│    └─OutputScale: 2-2187               --                        --
│    └─Empty: 2-2188                     [64, 64, 3, 3]            --
│    └─Empty: 2-2189                     [64, 64, 3, 3]            --
│    └─Empty: 2-2190                     [64]                      --
│    └─Empty: 2-2191                     [64]                      --
│    └─BatchNorm2d: 2-2192               [1, 64, 32, 32]           --
│    └─Scaler: 2-2193                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2194                      [1, 64, 32, 32]           --
│    └─Empty: 2-2195                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2196                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-165        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2197                 [1, 64, 16, 16]           --
│    └─Empty: 2-2198                     [1, 64, 16, 16]           --
│    └─Empty: 2-2199                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2200        --                        --
│    └─One: 2-2201                       [1]                       --
│    └─OutputScale: 2-2202               --                        --
│    └─Empty: 2-2203                     [64, 64, 3, 3]            --
│    └─Empty: 2-2204                     [64, 64, 3, 3]            --
│    └─Empty: 2-2205                     [64]                      --
│    └─Empty: 2-2206                     [64]                      --
│    └─BatchNorm2d: 2-2207               [1, 64, 16, 16]           --
│    └─Scaler: 2-2208                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2209                      [1, 64, 16, 16]           --
│    └─Empty: 2-2210                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2211                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-166               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-2212        --                        --
│    └─One: 2-2213                       [1]                       --
│    └─OutputScale: 2-2214               --                        --
│    └─Empty: 2-2215                     [64, 64, 1, 1]            --
│    └─Empty: 2-2216                     [64, 64, 1, 1]            --
│    └─Empty: 2-2217                     [64]                      --
│    └─Empty: 2-2218                     [64]                      --
│    └─BatchNorm2d: 2-2219               [1, 64, 16, 16]           --
│    └─Scaler: 2-2220                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2221                      [1, 64, 16, 16]           --
│    └─Empty: 2-2222                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2223                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-167        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2224                 [1, 64, 16, 16]           --
│    └─Empty: 2-2225                     [1, 64, 16, 16]           --
│    └─Empty: 2-2226                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2227        --                        --
│    └─One: 2-2228                       [1]                       --
│    └─OutputScale: 2-2229               --                        --
│    └─Empty: 2-2230                     [64, 64, 3, 3]            --
│    └─Empty: 2-2231                     [64, 64, 3, 3]            --
│    └─Empty: 2-2232                     [64]                      --
│    └─Empty: 2-2233                     [64]                      --
│    └─BatchNorm2d: 2-2234               [1, 64, 16, 16]           --
│    └─Scaler: 2-2235                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2236                      [1, 64, 16, 16]           --
│    └─Empty: 2-2237                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2238                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-168        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2239                 [1, 64, 8, 8]             --
│    └─Empty: 2-2240                     [1, 64, 8, 8]             --
│    └─Empty: 2-2241                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2242        --                        --
│    └─One: 2-2243                       [1]                       --
│    └─OutputScale: 2-2244               --                        --
│    └─Empty: 2-2245                     [64, 64, 1, 1]            --
│    └─Empty: 2-2246                     [64, 64, 1, 1]            --
│    └─Empty: 2-2247                     [64]                      --
│    └─Empty: 2-2248                     [64]                      --
│    └─BatchNorm2d: 2-2249               [1, 64, 8, 8]             --
│    └─Scaler: 2-2250                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2251                      [1, 64, 8, 8]             --
│    └─Empty: 2-2252                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2253                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-169               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-2254        --                        --
│    └─One: 2-2255                       [1]                       --
│    └─OutputScale: 2-2256               --                        --
│    └─Empty: 2-2257                     [64, 64, 1, 1]            --
│    └─Empty: 2-2258                     [64, 64, 1, 1]            --
│    └─Empty: 2-2259                     [64]                      --
│    └─Empty: 2-2260                     [64]                      --
│    └─BatchNorm2d: 2-2261               [1, 64, 8, 8]             --
│    └─Scaler: 2-2262                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2263                      [1, 64, 8, 8]             --
│    └─Empty: 2-2264                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2265                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-170        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2266                 [1, 64, 8, 8]             --
│    └─Empty: 2-2267                     [1, 64, 8, 8]             --
│    └─Empty: 2-2268                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2269        --                        --
│    └─One: 2-2270                       [1]                       --
│    └─OutputScale: 2-2271               --                        --
│    └─Empty: 2-2272                     [64, 64, 3, 3]            --
│    └─Empty: 2-2273                     [64, 64, 3, 3]            --
│    └─Empty: 2-2274                     [64]                      --
│    └─Empty: 2-2275                     [64]                      --
│    └─BatchNorm2d: 2-2276               [1, 64, 8, 8]             --
│    └─Scaler: 2-2277                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2278                      [1, 64, 8, 8]             --
│    └─Empty: 2-2279                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2280                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-171               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2281        --                        --
│    └─One: 2-2282                       [1]                       --
│    └─OutputScale: 2-2283               --                        --
│    └─Empty: 2-2284                     [64, 3, 1, 1]             --
│    └─Empty: 2-2285                     [64, 3, 1, 1]             --
│    └─Empty: 2-2286                     [64]                      --
│    └─Empty: 2-2287                     [64]                      --
│    └─BatchNorm2d: 2-2288               [1, 64, 256, 256]         --
│    └─Scaler: 2-2289                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2290                      [1, 64, 256, 256]         --
│    └─Empty: 2-2291                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2292                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-172               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2293        --                        --
│    └─One: 2-2294                       [1]                       --
│    └─OutputScale: 2-2295               --                        --
│    └─Empty: 2-2296                     [64, 64, 3, 3]            --
│    └─Empty: 2-2297                     [64, 64, 3, 3]            --
│    └─Empty: 2-2298                     [64]                      --
│    └─Empty: 2-2299                     [64]                      --
│    └─BatchNorm2d: 2-2300               [1, 64, 256, 256]         --
│    └─Scaler: 2-2301                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2302                      [1, 64, 256, 256]         --
│    └─Empty: 2-2303                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2304                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-173               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2305        --                        --
│    └─One: 2-2306                       [1]                       --
│    └─OutputScale: 2-2307               --                        --
│    └─Empty: 2-2308                     [64, 64, 1, 1]            --
│    └─Empty: 2-2309                     [64, 64, 1, 1]            --
│    └─Empty: 2-2310                     [64]                      --
│    └─Empty: 2-2311                     [64]                      --
│    └─BatchNorm2d: 2-2312               [1, 64, 256, 256]         --
│    └─Scaler: 2-2313                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2314                      [1, 64, 256, 256]         --
│    └─Empty: 2-2315                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2316                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-174               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2317        --                        --
│    └─One: 2-2318                       [1]                       --
│    └─OutputScale: 2-2319               --                        --
│    └─Empty: 2-2320                     [64, 64, 3, 3]            --
│    └─Empty: 2-2321                     [64, 64, 3, 3]            --
│    └─Empty: 2-2322                     [64]                      --
│    └─Empty: 2-2323                     [64]                      --
│    └─BatchNorm2d: 2-2324               [1, 64, 256, 256]         --
│    └─Scaler: 2-2325                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2326                      [1, 64, 256, 256]         --
│    └─Empty: 2-2327                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2328                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-175        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-2329                 [1, 64, 128, 128]         --
│    └─Empty: 2-2330                     [1, 64, 128, 128]         --
│    └─Empty: 2-2331                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-2332        --                        --
│    └─One: 2-2333                       [1]                       --
│    └─OutputScale: 2-2334               --                        --
│    └─Empty: 2-2335                     [64, 64, 3, 3]            --
│    └─Empty: 2-2336                     [64, 64, 3, 3]            --
│    └─Empty: 2-2337                     [64]                      --
│    └─Empty: 2-2338                     [64]                      --
│    └─BatchNorm2d: 2-2339               [1, 64, 128, 128]         --
│    └─Scaler: 2-2340                    [1, 64, 128, 128]         --
│    └─ReLU: 2-2341                      [1, 64, 128, 128]         --
│    └─Empty: 2-2342                     [1, 64, 128, 128]         --
│    └─Clamp: 2-2343                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-176               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-2344        --                        --
│    └─One: 2-2345                       [1]                       --
│    └─OutputScale: 2-2346               --                        --
│    └─Empty: 2-2347                     [64, 64, 3, 3]            --
│    └─Empty: 2-2348                     [64, 64, 3, 3]            --
│    └─Empty: 2-2349                     [64]                      --
│    └─Empty: 2-2350                     [64]                      --
│    └─BatchNorm2d: 2-2351               [1, 64, 128, 128]         --
│    └─Scaler: 2-2352                    [1, 64, 128, 128]         --
│    └─ReLU: 2-2353                      [1, 64, 128, 128]         --
│    └─Empty: 2-2354                     [1, 64, 128, 128]         --
│    └─Clamp: 2-2355                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-177        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-2356                 [1, 64, 64, 64]           --
│    └─Empty: 2-2357                     [1, 64, 64, 64]           --
│    └─Empty: 2-2358                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-2359        --                        --
│    └─One: 2-2360                       [1]                       --
│    └─OutputScale: 2-2361               --                        --
│    └─Empty: 2-2362                     [64, 64, 3, 3]            --
│    └─Empty: 2-2363                     [64, 64, 3, 3]            --
│    └─Empty: 2-2364                     [64]                      --
│    └─Empty: 2-2365                     [64]                      --
│    └─BatchNorm2d: 2-2366               [1, 64, 64, 64]           --
│    └─Scaler: 2-2367                    [1, 64, 64, 64]           --
│    └─ReLU: 2-2368                      [1, 64, 64, 64]           --
│    └─Empty: 2-2369                     [1, 64, 64, 64]           --
│    └─Clamp: 2-2370                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-178               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-2371        --                        --
│    └─One: 2-2372                       [1]                       --
│    └─OutputScale: 2-2373               --                        --
│    └─Empty: 2-2374                     [64, 64, 3, 3]            --
│    └─Empty: 2-2375                     [64, 64, 3, 3]            --
│    └─Empty: 2-2376                     [64]                      --
│    └─Empty: 2-2377                     [64]                      --
│    └─BatchNorm2d: 2-2378               [1, 64, 64, 64]           --
│    └─Scaler: 2-2379                    [1, 64, 64, 64]           --
│    └─ReLU: 2-2380                      [1, 64, 64, 64]           --
│    └─Empty: 2-2381                     [1, 64, 64, 64]           --
│    └─Clamp: 2-2382                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-179        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-2383                 [1, 64, 32, 32]           --
│    └─Empty: 2-2384                     [1, 64, 32, 32]           --
│    └─Empty: 2-2385                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-2386        --                        --
│    └─One: 2-2387                       [1]                       --
│    └─OutputScale: 2-2388               --                        --
│    └─Empty: 2-2389                     [64, 64, 3, 3]            --
│    └─Empty: 2-2390                     [64, 64, 3, 3]            --
│    └─Empty: 2-2391                     [64]                      --
│    └─Empty: 2-2392                     [64]                      --
│    └─BatchNorm2d: 2-2393               [1, 64, 32, 32]           --
│    └─Scaler: 2-2394                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2395                      [1, 64, 32, 32]           --
│    └─Empty: 2-2396                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2397                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-180               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-2398        --                        --
│    └─One: 2-2399                       [1]                       --
│    └─OutputScale: 2-2400               --                        --
│    └─Empty: 2-2401                     [64, 64, 1, 1]            --
│    └─Empty: 2-2402                     [64, 64, 1, 1]            --
│    └─Empty: 2-2403                     [64]                      --
│    └─Empty: 2-2404                     [64]                      --
│    └─BatchNorm2d: 2-2405               [1, 64, 32, 32]           --
│    └─Scaler: 2-2406                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2407                      [1, 64, 32, 32]           --
│    └─Empty: 2-2408                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2409                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-181        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-2410                 [1, 64, 32, 32]           --
│    └─Empty: 2-2411                     [1, 64, 32, 32]           --
│    └─Empty: 2-2412                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-2413        --                        --
│    └─One: 2-2414                       [1]                       --
│    └─OutputScale: 2-2415               --                        --
│    └─Empty: 2-2416                     [64, 64, 3, 3]            --
│    └─Empty: 2-2417                     [64, 64, 3, 3]            --
│    └─Empty: 2-2418                     [64]                      --
│    └─Empty: 2-2419                     [64]                      --
│    └─BatchNorm2d: 2-2420               [1, 64, 32, 32]           --
│    └─Scaler: 2-2421                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2422                      [1, 64, 32, 32]           --
│    └─Empty: 2-2423                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2424                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-182        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2425                 [1, 64, 16, 16]           --
│    └─Empty: 2-2426                     [1, 64, 16, 16]           --
│    └─Empty: 2-2427                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2428        --                        --
│    └─One: 2-2429                       [1]                       --
│    └─OutputScale: 2-2430               --                        --
│    └─Empty: 2-2431                     [64, 64, 3, 3]            --
│    └─Empty: 2-2432                     [64, 64, 3, 3]            --
│    └─Empty: 2-2433                     [64]                      --
│    └─Empty: 2-2434                     [64]                      --
│    └─BatchNorm2d: 2-2435               [1, 64, 16, 16]           --
│    └─Scaler: 2-2436                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2437                      [1, 64, 16, 16]           --
│    └─Empty: 2-2438                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2439                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-183               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-2440        --                        --
│    └─One: 2-2441                       [1]                       --
│    └─OutputScale: 2-2442               --                        --
│    └─Empty: 2-2443                     [64, 64, 1, 1]            --
│    └─Empty: 2-2444                     [64, 64, 1, 1]            --
│    └─Empty: 2-2445                     [64]                      --
│    └─Empty: 2-2446                     [64]                      --
│    └─BatchNorm2d: 2-2447               [1, 64, 16, 16]           --
│    └─Scaler: 2-2448                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2449                      [1, 64, 16, 16]           --
│    └─Empty: 2-2450                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2451                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-184        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2452                 [1, 64, 16, 16]           --
│    └─Empty: 2-2453                     [1, 64, 16, 16]           --
│    └─Empty: 2-2454                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2455        --                        --
│    └─One: 2-2456                       [1]                       --
│    └─OutputScale: 2-2457               --                        --
│    └─Empty: 2-2458                     [64, 64, 3, 3]            --
│    └─Empty: 2-2459                     [64, 64, 3, 3]            --
│    └─Empty: 2-2460                     [64]                      --
│    └─Empty: 2-2461                     [64]                      --
│    └─BatchNorm2d: 2-2462               [1, 64, 16, 16]           --
│    └─Scaler: 2-2463                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2464                      [1, 64, 16, 16]           --
│    └─Empty: 2-2465                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2466                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-185        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2467                 [1, 64, 8, 8]             --
│    └─Empty: 2-2468                     [1, 64, 8, 8]             --
│    └─Empty: 2-2469                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2470        --                        --
│    └─One: 2-2471                       [1]                       --
│    └─OutputScale: 2-2472               --                        --
│    └─Empty: 2-2473                     [64, 64, 1, 1]            --
│    └─Empty: 2-2474                     [64, 64, 1, 1]            --
│    └─Empty: 2-2475                     [64]                      --
│    └─Empty: 2-2476                     [64]                      --
│    └─BatchNorm2d: 2-2477               [1, 64, 8, 8]             --
│    └─Scaler: 2-2478                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2479                      [1, 64, 8, 8]             --
│    └─Empty: 2-2480                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2481                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-186               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-2482        --                        --
│    └─One: 2-2483                       [1]                       --
│    └─OutputScale: 2-2484               --                        --
│    └─Empty: 2-2485                     [64, 64, 1, 1]            --
│    └─Empty: 2-2486                     [64, 64, 1, 1]            --
│    └─Empty: 2-2487                     [64]                      --
│    └─Empty: 2-2488                     [64]                      --
│    └─BatchNorm2d: 2-2489               [1, 64, 8, 8]             --
│    └─Scaler: 2-2490                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2491                      [1, 64, 8, 8]             --
│    └─Empty: 2-2492                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2493                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-187        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2494                 [1, 64, 8, 8]             --
│    └─Empty: 2-2495                     [1, 64, 8, 8]             --
│    └─Empty: 2-2496                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2497        --                        --
│    └─One: 2-2498                       [1]                       --
│    └─OutputScale: 2-2499               --                        --
│    └─Empty: 2-2500                     [64, 64, 3, 3]            --
│    └─Empty: 2-2501                     [64, 64, 3, 3]            --
│    └─Empty: 2-2502                     [64]                      --
│    └─Empty: 2-2503                     [64]                      --
│    └─BatchNorm2d: 2-2504               [1, 64, 8, 8]             --
│    └─Scaler: 2-2505                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2506                      [1, 64, 8, 8]             --
│    └─Empty: 2-2507                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2508                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-188               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2509        --                        --
│    └─One: 2-2510                       [1]                       --
│    └─OutputScale: 2-2511               --                        --
│    └─Empty: 2-2512                     [64, 3, 1, 1]             --
│    └─Empty: 2-2513                     [64, 3, 1, 1]             --
│    └─Empty: 2-2514                     [64]                      --
│    └─Empty: 2-2515                     [64]                      --
│    └─BatchNorm2d: 2-2516               [1, 64, 256, 256]         --
│    └─Scaler: 2-2517                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2518                      [1, 64, 256, 256]         --
│    └─Empty: 2-2519                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2520                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-189               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2521        --                        --
│    └─One: 2-2522                       [1]                       --
│    └─OutputScale: 2-2523               --                        --
│    └─Empty: 2-2524                     [64, 64, 3, 3]            --
│    └─Empty: 2-2525                     [64, 64, 3, 3]            --
│    └─Empty: 2-2526                     [64]                      --
│    └─Empty: 2-2527                     [64]                      --
│    └─BatchNorm2d: 2-2528               [1, 64, 256, 256]         --
│    └─Scaler: 2-2529                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2530                      [1, 64, 256, 256]         --
│    └─Empty: 2-2531                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2532                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-190               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2533        --                        --
│    └─One: 2-2534                       [1]                       --
│    └─OutputScale: 2-2535               --                        --
│    └─Empty: 2-2536                     [64, 64, 1, 1]            --
│    └─Empty: 2-2537                     [64, 64, 1, 1]            --
│    └─Empty: 2-2538                     [64]                      --
│    └─Empty: 2-2539                     [64]                      --
│    └─BatchNorm2d: 2-2540               [1, 64, 256, 256]         --
│    └─Scaler: 2-2541                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2542                      [1, 64, 256, 256]         --
│    └─Empty: 2-2543                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2544                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-191               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2545        --                        --
│    └─One: 2-2546                       [1]                       --
│    └─OutputScale: 2-2547               --                        --
│    └─Empty: 2-2548                     [64, 64, 3, 3]            --
│    └─Empty: 2-2549                     [64, 64, 3, 3]            --
│    └─Empty: 2-2550                     [64]                      --
│    └─Empty: 2-2551                     [64]                      --
│    └─BatchNorm2d: 2-2552               [1, 64, 256, 256]         --
│    └─Scaler: 2-2553                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2554                      [1, 64, 256, 256]         --
│    └─Empty: 2-2555                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2556                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-192        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-2557                 [1, 64, 128, 128]         --
│    └─Empty: 2-2558                     [1, 64, 128, 128]         --
│    └─Empty: 2-2559                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-2560        --                        --
│    └─One: 2-2561                       [1]                       --
│    └─OutputScale: 2-2562               --                        --
│    └─Empty: 2-2563                     [64, 64, 3, 3]            --
│    └─Empty: 2-2564                     [64, 64, 3, 3]            --
│    └─Empty: 2-2565                     [64]                      --
│    └─Empty: 2-2566                     [64]                      --
│    └─BatchNorm2d: 2-2567               [1, 64, 128, 128]         --
│    └─Scaler: 2-2568                    [1, 64, 128, 128]         --
│    └─ReLU: 2-2569                      [1, 64, 128, 128]         --
│    └─Empty: 2-2570                     [1, 64, 128, 128]         --
│    └─Clamp: 2-2571                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-193               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-2572        --                        --
│    └─One: 2-2573                       [1]                       --
│    └─OutputScale: 2-2574               --                        --
│    └─Empty: 2-2575                     [64, 64, 3, 3]            --
│    └─Empty: 2-2576                     [64, 64, 3, 3]            --
│    └─Empty: 2-2577                     [64]                      --
│    └─Empty: 2-2578                     [64]                      --
│    └─BatchNorm2d: 2-2579               [1, 64, 128, 128]         --
│    └─Scaler: 2-2580                    [1, 64, 128, 128]         --
│    └─ReLU: 2-2581                      [1, 64, 128, 128]         --
│    └─Empty: 2-2582                     [1, 64, 128, 128]         --
│    └─Clamp: 2-2583                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-194        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-2584                 [1, 64, 64, 64]           --
│    └─Empty: 2-2585                     [1, 64, 64, 64]           --
│    └─Empty: 2-2586                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-2587        --                        --
│    └─One: 2-2588                       [1]                       --
│    └─OutputScale: 2-2589               --                        --
│    └─Empty: 2-2590                     [64, 64, 3, 3]            --
│    └─Empty: 2-2591                     [64, 64, 3, 3]            --
│    └─Empty: 2-2592                     [64]                      --
│    └─Empty: 2-2593                     [64]                      --
│    └─BatchNorm2d: 2-2594               [1, 64, 64, 64]           --
│    └─Scaler: 2-2595                    [1, 64, 64, 64]           --
│    └─ReLU: 2-2596                      [1, 64, 64, 64]           --
│    └─Empty: 2-2597                     [1, 64, 64, 64]           --
│    └─Clamp: 2-2598                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-195               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-2599        --                        --
│    └─One: 2-2600                       [1]                       --
│    └─OutputScale: 2-2601               --                        --
│    └─Empty: 2-2602                     [64, 64, 3, 3]            --
│    └─Empty: 2-2603                     [64, 64, 3, 3]            --
│    └─Empty: 2-2604                     [64]                      --
│    └─Empty: 2-2605                     [64]                      --
│    └─BatchNorm2d: 2-2606               [1, 64, 64, 64]           --
│    └─Scaler: 2-2607                    [1, 64, 64, 64]           --
│    └─ReLU: 2-2608                      [1, 64, 64, 64]           --
│    └─Empty: 2-2609                     [1, 64, 64, 64]           --
│    └─Clamp: 2-2610                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-196        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-2611                 [1, 64, 32, 32]           --
│    └─Empty: 2-2612                     [1, 64, 32, 32]           --
│    └─Empty: 2-2613                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-2614        --                        --
│    └─One: 2-2615                       [1]                       --
│    └─OutputScale: 2-2616               --                        --
│    └─Empty: 2-2617                     [64, 64, 3, 3]            --
│    └─Empty: 2-2618                     [64, 64, 3, 3]            --
│    └─Empty: 2-2619                     [64]                      --
│    └─Empty: 2-2620                     [64]                      --
│    └─BatchNorm2d: 2-2621               [1, 64, 32, 32]           --
│    └─Scaler: 2-2622                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2623                      [1, 64, 32, 32]           --
│    └─Empty: 2-2624                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2625                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-197               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-2626        --                        --
│    └─One: 2-2627                       [1]                       --
│    └─OutputScale: 2-2628               --                        --
│    └─Empty: 2-2629                     [64, 64, 1, 1]            --
│    └─Empty: 2-2630                     [64, 64, 1, 1]            --
│    └─Empty: 2-2631                     [64]                      --
│    └─Empty: 2-2632                     [64]                      --
│    └─BatchNorm2d: 2-2633               [1, 64, 32, 32]           --
│    └─Scaler: 2-2634                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2635                      [1, 64, 32, 32]           --
│    └─Empty: 2-2636                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2637                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-198        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-2638                 [1, 64, 32, 32]           --
│    └─Empty: 2-2639                     [1, 64, 32, 32]           --
│    └─Empty: 2-2640                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-2641        --                        --
│    └─One: 2-2642                       [1]                       --
│    └─OutputScale: 2-2643               --                        --
│    └─Empty: 2-2644                     [64, 64, 3, 3]            --
│    └─Empty: 2-2645                     [64, 64, 3, 3]            --
│    └─Empty: 2-2646                     [64]                      --
│    └─Empty: 2-2647                     [64]                      --
│    └─BatchNorm2d: 2-2648               [1, 64, 32, 32]           --
│    └─Scaler: 2-2649                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2650                      [1, 64, 32, 32]           --
│    └─Empty: 2-2651                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2652                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-199        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2653                 [1, 64, 16, 16]           --
│    └─Empty: 2-2654                     [1, 64, 16, 16]           --
│    └─Empty: 2-2655                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2656        --                        --
│    └─One: 2-2657                       [1]                       --
│    └─OutputScale: 2-2658               --                        --
│    └─Empty: 2-2659                     [64, 64, 3, 3]            --
│    └─Empty: 2-2660                     [64, 64, 3, 3]            --
│    └─Empty: 2-2661                     [64]                      --
│    └─Empty: 2-2662                     [64]                      --
│    └─BatchNorm2d: 2-2663               [1, 64, 16, 16]           --
│    └─Scaler: 2-2664                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2665                      [1, 64, 16, 16]           --
│    └─Empty: 2-2666                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2667                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-200               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-2668        --                        --
│    └─One: 2-2669                       [1]                       --
│    └─OutputScale: 2-2670               --                        --
│    └─Empty: 2-2671                     [64, 64, 1, 1]            --
│    └─Empty: 2-2672                     [64, 64, 1, 1]            --
│    └─Empty: 2-2673                     [64]                      --
│    └─Empty: 2-2674                     [64]                      --
│    └─BatchNorm2d: 2-2675               [1, 64, 16, 16]           --
│    └─Scaler: 2-2676                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2677                      [1, 64, 16, 16]           --
│    └─Empty: 2-2678                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2679                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-201        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2680                 [1, 64, 16, 16]           --
│    └─Empty: 2-2681                     [1, 64, 16, 16]           --
│    └─Empty: 2-2682                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2683        --                        --
│    └─One: 2-2684                       [1]                       --
│    └─OutputScale: 2-2685               --                        --
│    └─Empty: 2-2686                     [64, 64, 3, 3]            --
│    └─Empty: 2-2687                     [64, 64, 3, 3]            --
│    └─Empty: 2-2688                     [64]                      --
│    └─Empty: 2-2689                     [64]                      --
│    └─BatchNorm2d: 2-2690               [1, 64, 16, 16]           --
│    └─Scaler: 2-2691                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2692                      [1, 64, 16, 16]           --
│    └─Empty: 2-2693                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2694                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-202        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2695                 [1, 64, 8, 8]             --
│    └─Empty: 2-2696                     [1, 64, 8, 8]             --
│    └─Empty: 2-2697                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2698        --                        --
│    └─One: 2-2699                       [1]                       --
│    └─OutputScale: 2-2700               --                        --
│    └─Empty: 2-2701                     [64, 64, 1, 1]            --
│    └─Empty: 2-2702                     [64, 64, 1, 1]            --
│    └─Empty: 2-2703                     [64]                      --
│    └─Empty: 2-2704                     [64]                      --
│    └─BatchNorm2d: 2-2705               [1, 64, 8, 8]             --
│    └─Scaler: 2-2706                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2707                      [1, 64, 8, 8]             --
│    └─Empty: 2-2708                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2709                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-203               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-2710        --                        --
│    └─One: 2-2711                       [1]                       --
│    └─OutputScale: 2-2712               --                        --
│    └─Empty: 2-2713                     [64, 64, 1, 1]            --
│    └─Empty: 2-2714                     [64, 64, 1, 1]            --
│    └─Empty: 2-2715                     [64]                      --
│    └─Empty: 2-2716                     [64]                      --
│    └─BatchNorm2d: 2-2717               [1, 64, 8, 8]             --
│    └─Scaler: 2-2718                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2719                      [1, 64, 8, 8]             --
│    └─Empty: 2-2720                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2721                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-204        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2722                 [1, 64, 8, 8]             --
│    └─Empty: 2-2723                     [1, 64, 8, 8]             --
│    └─Empty: 2-2724                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2725        --                        --
│    └─One: 2-2726                       [1]                       --
│    └─OutputScale: 2-2727               --                        --
│    └─Empty: 2-2728                     [64, 64, 3, 3]            --
│    └─Empty: 2-2729                     [64, 64, 3, 3]            --
│    └─Empty: 2-2730                     [64]                      --
│    └─Empty: 2-2731                     [64]                      --
│    └─BatchNorm2d: 2-2732               [1, 64, 8, 8]             --
│    └─Scaler: 2-2733                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2734                      [1, 64, 8, 8]             --
│    └─Empty: 2-2735                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2736                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-205               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2737        --                        --
│    └─One: 2-2738                       [1]                       --
│    └─OutputScale: 2-2739               --                        --
│    └─Empty: 2-2740                     [64, 3, 1, 1]             --
│    └─Empty: 2-2741                     [64, 3, 1, 1]             --
│    └─Empty: 2-2742                     [64]                      --
│    └─Empty: 2-2743                     [64]                      --
│    └─BatchNorm2d: 2-2744               [1, 64, 256, 256]         --
│    └─Scaler: 2-2745                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2746                      [1, 64, 256, 256]         --
│    └─Empty: 2-2747                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2748                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-206               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2749        --                        --
│    └─One: 2-2750                       [1]                       --
│    └─OutputScale: 2-2751               --                        --
│    └─Empty: 2-2752                     [64, 64, 3, 3]            --
│    └─Empty: 2-2753                     [64, 64, 3, 3]            --
│    └─Empty: 2-2754                     [64]                      --
│    └─Empty: 2-2755                     [64]                      --
│    └─BatchNorm2d: 2-2756               [1, 64, 256, 256]         --
│    └─Scaler: 2-2757                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2758                      [1, 64, 256, 256]         --
│    └─Empty: 2-2759                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2760                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-207               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2761        --                        --
│    └─One: 2-2762                       [1]                       --
│    └─OutputScale: 2-2763               --                        --
│    └─Empty: 2-2764                     [64, 64, 1, 1]            --
│    └─Empty: 2-2765                     [64, 64, 1, 1]            --
│    └─Empty: 2-2766                     [64]                      --
│    └─Empty: 2-2767                     [64]                      --
│    └─BatchNorm2d: 2-2768               [1, 64, 256, 256]         --
│    └─Scaler: 2-2769                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2770                      [1, 64, 256, 256]         --
│    └─Empty: 2-2771                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2772                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-208               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2773        --                        --
│    └─One: 2-2774                       [1]                       --
│    └─OutputScale: 2-2775               --                        --
│    └─Empty: 2-2776                     [64, 64, 3, 3]            --
│    └─Empty: 2-2777                     [64, 64, 3, 3]            --
│    └─Empty: 2-2778                     [64]                      --
│    └─Empty: 2-2779                     [64]                      --
│    └─BatchNorm2d: 2-2780               [1, 64, 256, 256]         --
│    └─Scaler: 2-2781                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2782                      [1, 64, 256, 256]         --
│    └─Empty: 2-2783                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2784                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-209        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-2785                 [1, 64, 128, 128]         --
│    └─Empty: 2-2786                     [1, 64, 128, 128]         --
│    └─Empty: 2-2787                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-2788        --                        --
│    └─One: 2-2789                       [1]                       --
│    └─OutputScale: 2-2790               --                        --
│    └─Empty: 2-2791                     [64, 64, 3, 3]            --
│    └─Empty: 2-2792                     [64, 64, 3, 3]            --
│    └─Empty: 2-2793                     [64]                      --
│    └─Empty: 2-2794                     [64]                      --
│    └─BatchNorm2d: 2-2795               [1, 64, 128, 128]         --
│    └─Scaler: 2-2796                    [1, 64, 128, 128]         --
│    └─ReLU: 2-2797                      [1, 64, 128, 128]         --
│    └─Empty: 2-2798                     [1, 64, 128, 128]         --
│    └─Clamp: 2-2799                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-210               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-2800        --                        --
│    └─One: 2-2801                       [1]                       --
│    └─OutputScale: 2-2802               --                        --
│    └─Empty: 2-2803                     [64, 64, 3, 3]            --
│    └─Empty: 2-2804                     [64, 64, 3, 3]            --
│    └─Empty: 2-2805                     [64]                      --
│    └─Empty: 2-2806                     [64]                      --
│    └─BatchNorm2d: 2-2807               [1, 64, 128, 128]         --
│    └─Scaler: 2-2808                    [1, 64, 128, 128]         --
│    └─ReLU: 2-2809                      [1, 64, 128, 128]         --
│    └─Empty: 2-2810                     [1, 64, 128, 128]         --
│    └─Clamp: 2-2811                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-211        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-2812                 [1, 64, 64, 64]           --
│    └─Empty: 2-2813                     [1, 64, 64, 64]           --
│    └─Empty: 2-2814                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-2815        --                        --
│    └─One: 2-2816                       [1]                       --
│    └─OutputScale: 2-2817               --                        --
│    └─Empty: 2-2818                     [64, 64, 3, 3]            --
│    └─Empty: 2-2819                     [64, 64, 3, 3]            --
│    └─Empty: 2-2820                     [64]                      --
│    └─Empty: 2-2821                     [64]                      --
│    └─BatchNorm2d: 2-2822               [1, 64, 64, 64]           --
│    └─Scaler: 2-2823                    [1, 64, 64, 64]           --
│    └─ReLU: 2-2824                      [1, 64, 64, 64]           --
│    └─Empty: 2-2825                     [1, 64, 64, 64]           --
│    └─Clamp: 2-2826                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-212               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-2827        --                        --
│    └─One: 2-2828                       [1]                       --
│    └─OutputScale: 2-2829               --                        --
│    └─Empty: 2-2830                     [64, 64, 3, 3]            --
│    └─Empty: 2-2831                     [64, 64, 3, 3]            --
│    └─Empty: 2-2832                     [64]                      --
│    └─Empty: 2-2833                     [64]                      --
│    └─BatchNorm2d: 2-2834               [1, 64, 64, 64]           --
│    └─Scaler: 2-2835                    [1, 64, 64, 64]           --
│    └─ReLU: 2-2836                      [1, 64, 64, 64]           --
│    └─Empty: 2-2837                     [1, 64, 64, 64]           --
│    └─Clamp: 2-2838                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-213        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-2839                 [1, 64, 32, 32]           --
│    └─Empty: 2-2840                     [1, 64, 32, 32]           --
│    └─Empty: 2-2841                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-2842        --                        --
│    └─One: 2-2843                       [1]                       --
│    └─OutputScale: 2-2844               --                        --
│    └─Empty: 2-2845                     [64, 64, 3, 3]            --
│    └─Empty: 2-2846                     [64, 64, 3, 3]            --
│    └─Empty: 2-2847                     [64]                      --
│    └─Empty: 2-2848                     [64]                      --
│    └─BatchNorm2d: 2-2849               [1, 64, 32, 32]           --
│    └─Scaler: 2-2850                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2851                      [1, 64, 32, 32]           --
│    └─Empty: 2-2852                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2853                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-214               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-2854        --                        --
│    └─One: 2-2855                       [1]                       --
│    └─OutputScale: 2-2856               --                        --
│    └─Empty: 2-2857                     [64, 64, 1, 1]            --
│    └─Empty: 2-2858                     [64, 64, 1, 1]            --
│    └─Empty: 2-2859                     [64]                      --
│    └─Empty: 2-2860                     [64]                      --
│    └─BatchNorm2d: 2-2861               [1, 64, 32, 32]           --
│    └─Scaler: 2-2862                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2863                      [1, 64, 32, 32]           --
│    └─Empty: 2-2864                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2865                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-215        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-2866                 [1, 64, 32, 32]           --
│    └─Empty: 2-2867                     [1, 64, 32, 32]           --
│    └─Empty: 2-2868                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-2869        --                        --
│    └─One: 2-2870                       [1]                       --
│    └─OutputScale: 2-2871               --                        --
│    └─Empty: 2-2872                     [64, 64, 3, 3]            --
│    └─Empty: 2-2873                     [64, 64, 3, 3]            --
│    └─Empty: 2-2874                     [64]                      --
│    └─Empty: 2-2875                     [64]                      --
│    └─BatchNorm2d: 2-2876               [1, 64, 32, 32]           --
│    └─Scaler: 2-2877                    [1, 64, 32, 32]           --
│    └─ReLU: 2-2878                      [1, 64, 32, 32]           --
│    └─Empty: 2-2879                     [1, 64, 32, 32]           --
│    └─Clamp: 2-2880                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-216        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2881                 [1, 64, 16, 16]           --
│    └─Empty: 2-2882                     [1, 64, 16, 16]           --
│    └─Empty: 2-2883                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2884        --                        --
│    └─One: 2-2885                       [1]                       --
│    └─OutputScale: 2-2886               --                        --
│    └─Empty: 2-2887                     [64, 64, 3, 3]            --
│    └─Empty: 2-2888                     [64, 64, 3, 3]            --
│    └─Empty: 2-2889                     [64]                      --
│    └─Empty: 2-2890                     [64]                      --
│    └─BatchNorm2d: 2-2891               [1, 64, 16, 16]           --
│    └─Scaler: 2-2892                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2893                      [1, 64, 16, 16]           --
│    └─Empty: 2-2894                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2895                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-217               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-2896        --                        --
│    └─One: 2-2897                       [1]                       --
│    └─OutputScale: 2-2898               --                        --
│    └─Empty: 2-2899                     [64, 64, 1, 1]            --
│    └─Empty: 2-2900                     [64, 64, 1, 1]            --
│    └─Empty: 2-2901                     [64]                      --
│    └─Empty: 2-2902                     [64]                      --
│    └─BatchNorm2d: 2-2903               [1, 64, 16, 16]           --
│    └─Scaler: 2-2904                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2905                      [1, 64, 16, 16]           --
│    └─Empty: 2-2906                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2907                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-218        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2908                 [1, 64, 16, 16]           --
│    └─Empty: 2-2909                     [1, 64, 16, 16]           --
│    └─Empty: 2-2910                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2911        --                        --
│    └─One: 2-2912                       [1]                       --
│    └─OutputScale: 2-2913               --                        --
│    └─Empty: 2-2914                     [64, 64, 3, 3]            --
│    └─Empty: 2-2915                     [64, 64, 3, 3]            --
│    └─Empty: 2-2916                     [64]                      --
│    └─Empty: 2-2917                     [64]                      --
│    └─BatchNorm2d: 2-2918               [1, 64, 16, 16]           --
│    └─Scaler: 2-2919                    [1, 64, 16, 16]           --
│    └─ReLU: 2-2920                      [1, 64, 16, 16]           --
│    └─Empty: 2-2921                     [1, 64, 16, 16]           --
│    └─Clamp: 2-2922                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-219        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2923                 [1, 64, 8, 8]             --
│    └─Empty: 2-2924                     [1, 64, 8, 8]             --
│    └─Empty: 2-2925                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2926        --                        --
│    └─One: 2-2927                       [1]                       --
│    └─OutputScale: 2-2928               --                        --
│    └─Empty: 2-2929                     [64, 64, 1, 1]            --
│    └─Empty: 2-2930                     [64, 64, 1, 1]            --
│    └─Empty: 2-2931                     [64]                      --
│    └─Empty: 2-2932                     [64]                      --
│    └─BatchNorm2d: 2-2933               [1, 64, 8, 8]             --
│    └─Scaler: 2-2934                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2935                      [1, 64, 8, 8]             --
│    └─Empty: 2-2936                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2937                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-220               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-2938        --                        --
│    └─One: 2-2939                       [1]                       --
│    └─OutputScale: 2-2940               --                        --
│    └─Empty: 2-2941                     [64, 64, 1, 1]            --
│    └─Empty: 2-2942                     [64, 64, 1, 1]            --
│    └─Empty: 2-2943                     [64]                      --
│    └─Empty: 2-2944                     [64]                      --
│    └─BatchNorm2d: 2-2945               [1, 64, 8, 8]             --
│    └─Scaler: 2-2946                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2947                      [1, 64, 8, 8]             --
│    └─Empty: 2-2948                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2949                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-221        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2950                 [1, 64, 8, 8]             --
│    └─Empty: 2-2951                     [1, 64, 8, 8]             --
│    └─Empty: 2-2952                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2953        --                        --
│    └─One: 2-2954                       [1]                       --
│    └─OutputScale: 2-2955               --                        --
│    └─Empty: 2-2956                     [64, 64, 3, 3]            --
│    └─Empty: 2-2957                     [64, 64, 3, 3]            --
│    └─Empty: 2-2958                     [64]                      --
│    └─Empty: 2-2959                     [64]                      --
│    └─BatchNorm2d: 2-2960               [1, 64, 8, 8]             --
│    └─Scaler: 2-2961                    [1, 64, 8, 8]             --
│    └─ReLU: 2-2962                      [1, 64, 8, 8]             --
│    └─Empty: 2-2963                     [1, 64, 8, 8]             --
│    └─Clamp: 2-2964                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-222               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2965        --                        --
│    └─One: 2-2966                       [1]                       --
│    └─OutputScale: 2-2967               --                        --
│    └─Empty: 2-2968                     [64, 3, 1, 1]             --
│    └─Empty: 2-2969                     [64, 3, 1, 1]             --
│    └─Empty: 2-2970                     [64]                      --
│    └─Empty: 2-2971                     [64]                      --
│    └─BatchNorm2d: 2-2972               [1, 64, 256, 256]         --
│    └─Scaler: 2-2973                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2974                      [1, 64, 256, 256]         --
│    └─Empty: 2-2975                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2976                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-223               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2977        --                        --
│    └─One: 2-2978                       [1]                       --
│    └─OutputScale: 2-2979               --                        --
│    └─Empty: 2-2980                     [64, 64, 3, 3]            --
│    └─Empty: 2-2981                     [64, 64, 3, 3]            --
│    └─Empty: 2-2982                     [64]                      --
│    └─Empty: 2-2983                     [64]                      --
│    └─BatchNorm2d: 2-2984               [1, 64, 256, 256]         --
│    └─Scaler: 2-2985                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2986                      [1, 64, 256, 256]         --
│    └─Empty: 2-2987                     [1, 64, 256, 256]         --
│    └─Clamp: 2-2988                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-224               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-2989        --                        --
│    └─One: 2-2990                       [1]                       --
│    └─OutputScale: 2-2991               --                        --
│    └─Empty: 2-2992                     [64, 64, 1, 1]            --
│    └─Empty: 2-2993                     [64, 64, 1, 1]            --
│    └─Empty: 2-2994                     [64]                      --
│    └─Empty: 2-2995                     [64]                      --
│    └─BatchNorm2d: 2-2996               [1, 64, 256, 256]         --
│    └─Scaler: 2-2997                    [1, 64, 256, 256]         --
│    └─ReLU: 2-2998                      [1, 64, 256, 256]         --
│    └─Empty: 2-2999                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3000                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-225               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-3001        --                        --
│    └─One: 2-3002                       [1]                       --
│    └─OutputScale: 2-3003               --                        --
│    └─Empty: 2-3004                     [64, 64, 3, 3]            --
│    └─Empty: 2-3005                     [64, 64, 3, 3]            --
│    └─Empty: 2-3006                     [64]                      --
│    └─Empty: 2-3007                     [64]                      --
│    └─BatchNorm2d: 2-3008               [1, 64, 256, 256]         --
│    └─Scaler: 2-3009                    [1, 64, 256, 256]         --
│    └─ReLU: 2-3010                      [1, 64, 256, 256]         --
│    └─Empty: 2-3011                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3012                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-226        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-3013                 [1, 64, 128, 128]         --
│    └─Empty: 2-3014                     [1, 64, 128, 128]         --
│    └─Empty: 2-3015                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-3016        --                        --
│    └─One: 2-3017                       [1]                       --
│    └─OutputScale: 2-3018               --                        --
│    └─Empty: 2-3019                     [64, 64, 3, 3]            --
│    └─Empty: 2-3020                     [64, 64, 3, 3]            --
│    └─Empty: 2-3021                     [64]                      --
│    └─Empty: 2-3022                     [64]                      --
│    └─BatchNorm2d: 2-3023               [1, 64, 128, 128]         --
│    └─Scaler: 2-3024                    [1, 64, 128, 128]         --
│    └─ReLU: 2-3025                      [1, 64, 128, 128]         --
│    └─Empty: 2-3026                     [1, 64, 128, 128]         --
│    └─Clamp: 2-3027                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-227               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-3028        --                        --
│    └─One: 2-3029                       [1]                       --
│    └─OutputScale: 2-3030               --                        --
│    └─Empty: 2-3031                     [64, 64, 3, 3]            --
│    └─Empty: 2-3032                     [64, 64, 3, 3]            --
│    └─Empty: 2-3033                     [64]                      --
│    └─Empty: 2-3034                     [64]                      --
│    └─BatchNorm2d: 2-3035               [1, 64, 128, 128]         --
│    └─Scaler: 2-3036                    [1, 64, 128, 128]         --
│    └─ReLU: 2-3037                      [1, 64, 128, 128]         --
│    └─Empty: 2-3038                     [1, 64, 128, 128]         --
│    └─Clamp: 2-3039                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-228        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-3040                 [1, 64, 64, 64]           --
│    └─Empty: 2-3041                     [1, 64, 64, 64]           --
│    └─Empty: 2-3042                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-3043        --                        --
│    └─One: 2-3044                       [1]                       --
│    └─OutputScale: 2-3045               --                        --
│    └─Empty: 2-3046                     [64, 64, 3, 3]            --
│    └─Empty: 2-3047                     [64, 64, 3, 3]            --
│    └─Empty: 2-3048                     [64]                      --
│    └─Empty: 2-3049                     [64]                      --
│    └─BatchNorm2d: 2-3050               [1, 64, 64, 64]           --
│    └─Scaler: 2-3051                    [1, 64, 64, 64]           --
│    └─ReLU: 2-3052                      [1, 64, 64, 64]           --
│    └─Empty: 2-3053                     [1, 64, 64, 64]           --
│    └─Clamp: 2-3054                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-229               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-3055        --                        --
│    └─One: 2-3056                       [1]                       --
│    └─OutputScale: 2-3057               --                        --
│    └─Empty: 2-3058                     [64, 64, 3, 3]            --
│    └─Empty: 2-3059                     [64, 64, 3, 3]            --
│    └─Empty: 2-3060                     [64]                      --
│    └─Empty: 2-3061                     [64]                      --
│    └─BatchNorm2d: 2-3062               [1, 64, 64, 64]           --
│    └─Scaler: 2-3063                    [1, 64, 64, 64]           --
│    └─ReLU: 2-3064                      [1, 64, 64, 64]           --
│    └─Empty: 2-3065                     [1, 64, 64, 64]           --
│    └─Clamp: 2-3066                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-230        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-3067                 [1, 64, 32, 32]           --
│    └─Empty: 2-3068                     [1, 64, 32, 32]           --
│    └─Empty: 2-3069                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-3070        --                        --
│    └─One: 2-3071                       [1]                       --
│    └─OutputScale: 2-3072               --                        --
│    └─Empty: 2-3073                     [64, 64, 3, 3]            --
│    └─Empty: 2-3074                     [64, 64, 3, 3]            --
│    └─Empty: 2-3075                     [64]                      --
│    └─Empty: 2-3076                     [64]                      --
│    └─BatchNorm2d: 2-3077               [1, 64, 32, 32]           --
│    └─Scaler: 2-3078                    [1, 64, 32, 32]           --
│    └─ReLU: 2-3079                      [1, 64, 32, 32]           --
│    └─Empty: 2-3080                     [1, 64, 32, 32]           --
│    └─Clamp: 2-3081                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-231               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-3082        --                        --
│    └─One: 2-3083                       [1]                       --
│    └─OutputScale: 2-3084               --                        --
│    └─Empty: 2-3085                     [64, 64, 1, 1]            --
│    └─Empty: 2-3086                     [64, 64, 1, 1]            --
│    └─Empty: 2-3087                     [64]                      --
│    └─Empty: 2-3088                     [64]                      --
│    └─BatchNorm2d: 2-3089               [1, 64, 32, 32]           --
│    └─Scaler: 2-3090                    [1, 64, 32, 32]           --
│    └─ReLU: 2-3091                      [1, 64, 32, 32]           --
│    └─Empty: 2-3092                     [1, 64, 32, 32]           --
│    └─Clamp: 2-3093                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-232        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-3094                 [1, 64, 32, 32]           --
│    └─Empty: 2-3095                     [1, 64, 32, 32]           --
│    └─Empty: 2-3096                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-3097        --                        --
│    └─One: 2-3098                       [1]                       --
│    └─OutputScale: 2-3099               --                        --
│    └─Empty: 2-3100                     [64, 64, 3, 3]            --
│    └─Empty: 2-3101                     [64, 64, 3, 3]            --
│    └─Empty: 2-3102                     [64]                      --
│    └─Empty: 2-3103                     [64]                      --
│    └─BatchNorm2d: 2-3104               [1, 64, 32, 32]           --
│    └─Scaler: 2-3105                    [1, 64, 32, 32]           --
│    └─ReLU: 2-3106                      [1, 64, 32, 32]           --
│    └─Empty: 2-3107                     [1, 64, 32, 32]           --
│    └─Clamp: 2-3108                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-233        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-3109                 [1, 64, 16, 16]           --
│    └─Empty: 2-3110                     [1, 64, 16, 16]           --
│    └─Empty: 2-3111                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-3112        --                        --
│    └─One: 2-3113                       [1]                       --
│    └─OutputScale: 2-3114               --                        --
│    └─Empty: 2-3115                     [64, 64, 3, 3]            --
│    └─Empty: 2-3116                     [64, 64, 3, 3]            --
│    └─Empty: 2-3117                     [64]                      --
│    └─Empty: 2-3118                     [64]                      --
│    └─BatchNorm2d: 2-3119               [1, 64, 16, 16]           --
│    └─Scaler: 2-3120                    [1, 64, 16, 16]           --
│    └─ReLU: 2-3121                      [1, 64, 16, 16]           --
│    └─Empty: 2-3122                     [1, 64, 16, 16]           --
│    └─Clamp: 2-3123                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-234               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-3124        --                        --
│    └─One: 2-3125                       [1]                       --
│    └─OutputScale: 2-3126               --                        --
│    └─Empty: 2-3127                     [64, 64, 1, 1]            --
│    └─Empty: 2-3128                     [64, 64, 1, 1]            --
│    └─Empty: 2-3129                     [64]                      --
│    └─Empty: 2-3130                     [64]                      --
│    └─BatchNorm2d: 2-3131               [1, 64, 16, 16]           --
│    └─Scaler: 2-3132                    [1, 64, 16, 16]           --
│    └─ReLU: 2-3133                      [1, 64, 16, 16]           --
│    └─Empty: 2-3134                     [1, 64, 16, 16]           --
│    └─Clamp: 2-3135                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-235        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-3136                 [1, 64, 16, 16]           --
│    └─Empty: 2-3137                     [1, 64, 16, 16]           --
│    └─Empty: 2-3138                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-3139        --                        --
│    └─One: 2-3140                       [1]                       --
│    └─OutputScale: 2-3141               --                        --
│    └─Empty: 2-3142                     [64, 64, 3, 3]            --
│    └─Empty: 2-3143                     [64, 64, 3, 3]            --
│    └─Empty: 2-3144                     [64]                      --
│    └─Empty: 2-3145                     [64]                      --
│    └─BatchNorm2d: 2-3146               [1, 64, 16, 16]           --
│    └─Scaler: 2-3147                    [1, 64, 16, 16]           --
│    └─ReLU: 2-3148                      [1, 64, 16, 16]           --
│    └─Empty: 2-3149                     [1, 64, 16, 16]           --
│    └─Clamp: 2-3150                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-236        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-3151                 [1, 64, 8, 8]             --
│    └─Empty: 2-3152                     [1, 64, 8, 8]             --
│    └─Empty: 2-3153                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-3154        --                        --
│    └─One: 2-3155                       [1]                       --
│    └─OutputScale: 2-3156               --                        --
│    └─Empty: 2-3157                     [64, 64, 1, 1]            --
│    └─Empty: 2-3158                     [64, 64, 1, 1]            --
│    └─Empty: 2-3159                     [64]                      --
│    └─Empty: 2-3160                     [64]                      --
│    └─BatchNorm2d: 2-3161               [1, 64, 8, 8]             --
│    └─Scaler: 2-3162                    [1, 64, 8, 8]             --
│    └─ReLU: 2-3163                      [1, 64, 8, 8]             --
│    └─Empty: 2-3164                     [1, 64, 8, 8]             --
│    └─Clamp: 2-3165                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-237               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-3166        --                        --
│    └─One: 2-3167                       [1]                       --
│    └─OutputScale: 2-3168               --                        --
│    └─Empty: 2-3169                     [64, 64, 1, 1]            --
│    └─Empty: 2-3170                     [64, 64, 1, 1]            --
│    └─Empty: 2-3171                     [64]                      --
│    └─Empty: 2-3172                     [64]                      --
│    └─BatchNorm2d: 2-3173               [1, 64, 8, 8]             --
│    └─Scaler: 2-3174                    [1, 64, 8, 8]             --
│    └─ReLU: 2-3175                      [1, 64, 8, 8]             --
│    └─Empty: 2-3176                     [1, 64, 8, 8]             --
│    └─Clamp: 2-3177                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-238        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-3178                 [1, 64, 8, 8]             --
│    └─Empty: 2-3179                     [1, 64, 8, 8]             --
│    └─Empty: 2-3180                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-3181        --                        --
│    └─One: 2-3182                       [1]                       --
│    └─OutputScale: 2-3183               --                        --
│    └─Empty: 2-3184                     [64, 64, 3, 3]            --
│    └─Empty: 2-3185                     [64, 64, 3, 3]            --
│    └─Empty: 2-3186                     [64]                      --
│    └─Empty: 2-3187                     [64]                      --
│    └─BatchNorm2d: 2-3188               [1, 64, 8, 8]             --
│    └─Scaler: 2-3189                    [1, 64, 8, 8]             --
│    └─ReLU: 2-3190                      [1, 64, 8, 8]             --
│    └─Empty: 2-3191                     [1, 64, 8, 8]             --
│    └─Clamp: 2-3192                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-239               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-3193        --                        --
│    └─One: 2-3194                       [1]                       --
│    └─OutputScale: 2-3195               --                        --
│    └─Empty: 2-3196                     [64, 3, 1, 1]             --
│    └─Empty: 2-3197                     [64, 3, 1, 1]             --
│    └─Empty: 2-3198                     [64]                      --
│    └─Empty: 2-3199                     [64]                      --
│    └─BatchNorm2d: 2-3200               [1, 64, 256, 256]         --
│    └─Scaler: 2-3201                    [1, 64, 256, 256]         --
│    └─ReLU: 2-3202                      [1, 64, 256, 256]         --
│    └─Empty: 2-3203                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3204                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-240               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-3205        --                        --
│    └─One: 2-3206                       [1]                       --
│    └─OutputScale: 2-3207               --                        --
│    └─Empty: 2-3208                     [64, 64, 3, 3]            --
│    └─Empty: 2-3209                     [64, 64, 3, 3]            --
│    └─Empty: 2-3210                     [64]                      --
│    └─Empty: 2-3211                     [64]                      --
│    └─BatchNorm2d: 2-3212               [1, 64, 256, 256]         --
│    └─Scaler: 2-3213                    [1, 64, 256, 256]         --
│    └─ReLU: 2-3214                      [1, 64, 256, 256]         --
│    └─Empty: 2-3215                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3216                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-241               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-3217        --                        --
│    └─One: 2-3218                       [1]                       --
│    └─OutputScale: 2-3219               --                        --
│    └─Empty: 2-3220                     [64, 64, 1, 1]            --
│    └─Empty: 2-3221                     [64, 64, 1, 1]            --
│    └─Empty: 2-3222                     [64]                      --
│    └─Empty: 2-3223                     [64]                      --
│    └─BatchNorm2d: 2-3224               [1, 64, 256, 256]         --
│    └─Scaler: 2-3225                    [1, 64, 256, 256]         --
│    └─ReLU: 2-3226                      [1, 64, 256, 256]         --
│    └─Empty: 2-3227                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3228                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-242               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-3229        --                        --
│    └─One: 2-3230                       [1]                       --
│    └─OutputScale: 2-3231               --                        --
│    └─Empty: 2-3232                     [64, 64, 3, 3]            --
│    └─Empty: 2-3233                     [64, 64, 3, 3]            --
│    └─Empty: 2-3234                     [64]                      --
│    └─Empty: 2-3235                     [64]                      --
│    └─BatchNorm2d: 2-3236               [1, 64, 256, 256]         --
│    └─Scaler: 2-3237                    [1, 64, 256, 256]         --
│    └─ReLU: 2-3238                      [1, 64, 256, 256]         --
│    └─Empty: 2-3239                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3240                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-243        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-3241                 [1, 64, 128, 128]         --
│    └─Empty: 2-3242                     [1, 64, 128, 128]         --
│    └─Empty: 2-3243                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-3244        --                        --
│    └─One: 2-3245                       [1]                       --
│    └─OutputScale: 2-3246               --                        --
│    └─Empty: 2-3247                     [64, 64, 3, 3]            --
│    └─Empty: 2-3248                     [64, 64, 3, 3]            --
│    └─Empty: 2-3249                     [64]                      --
│    └─Empty: 2-3250                     [64]                      --
│    └─BatchNorm2d: 2-3251               [1, 64, 128, 128]         --
│    └─Scaler: 2-3252                    [1, 64, 128, 128]         --
│    └─ReLU: 2-3253                      [1, 64, 128, 128]         --
│    └─Empty: 2-3254                     [1, 64, 128, 128]         --
│    └─Clamp: 2-3255                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-244               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-3256        --                        --
│    └─One: 2-3257                       [1]                       --
│    └─OutputScale: 2-3258               --                        --
│    └─Empty: 2-3259                     [64, 64, 3, 3]            --
│    └─Empty: 2-3260                     [64, 64, 3, 3]            --
│    └─Empty: 2-3261                     [64]                      --
│    └─Empty: 2-3262                     [64]                      --
│    └─BatchNorm2d: 2-3263               [1, 64, 128, 128]         --
│    └─Scaler: 2-3264                    [1, 64, 128, 128]         --
│    └─ReLU: 2-3265                      [1, 64, 128, 128]         --
│    └─Empty: 2-3266                     [1, 64, 128, 128]         --
│    └─Clamp: 2-3267                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-245        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-3268                 [1, 64, 64, 64]           --
│    └─Empty: 2-3269                     [1, 64, 64, 64]           --
│    └─Empty: 2-3270                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-3271        --                        --
│    └─One: 2-3272                       [1]                       --
│    └─OutputScale: 2-3273               --                        --
│    └─Empty: 2-3274                     [64, 64, 3, 3]            --
│    └─Empty: 2-3275                     [64, 64, 3, 3]            --
│    └─Empty: 2-3276                     [64]                      --
│    └─Empty: 2-3277                     [64]                      --
│    └─BatchNorm2d: 2-3278               [1, 64, 64, 64]           --
│    └─Scaler: 2-3279                    [1, 64, 64, 64]           --
│    └─ReLU: 2-3280                      [1, 64, 64, 64]           --
│    └─Empty: 2-3281                     [1, 64, 64, 64]           --
│    └─Clamp: 2-3282                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-246               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-3283        --                        --
│    └─One: 2-3284                       [1]                       --
│    └─OutputScale: 2-3285               --                        --
│    └─Empty: 2-3286                     [64, 64, 3, 3]            --
│    └─Empty: 2-3287                     [64, 64, 3, 3]            --
│    └─Empty: 2-3288                     [64]                      --
│    └─Empty: 2-3289                     [64]                      --
│    └─BatchNorm2d: 2-3290               [1, 64, 64, 64]           --
│    └─Scaler: 2-3291                    [1, 64, 64, 64]           --
│    └─ReLU: 2-3292                      [1, 64, 64, 64]           --
│    └─Empty: 2-3293                     [1, 64, 64, 64]           --
│    └─Clamp: 2-3294                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-247        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-3295                 [1, 64, 32, 32]           --
│    └─Empty: 2-3296                     [1, 64, 32, 32]           --
│    └─Empty: 2-3297                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-3298        --                        --
│    └─One: 2-3299                       [1]                       --
│    └─OutputScale: 2-3300               --                        --
│    └─Empty: 2-3301                     [64, 64, 3, 3]            --
│    └─Empty: 2-3302                     [64, 64, 3, 3]            --
│    └─Empty: 2-3303                     [64]                      --
│    └─Empty: 2-3304                     [64]                      --
│    └─BatchNorm2d: 2-3305               [1, 64, 32, 32]           --
│    └─Scaler: 2-3306                    [1, 64, 32, 32]           --
│    └─ReLU: 2-3307                      [1, 64, 32, 32]           --
│    └─Empty: 2-3308                     [1, 64, 32, 32]           --
│    └─Clamp: 2-3309                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-248               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-3310        --                        --
│    └─One: 2-3311                       [1]                       --
│    └─OutputScale: 2-3312               --                        --
│    └─Empty: 2-3313                     [64, 64, 1, 1]            --
│    └─Empty: 2-3314                     [64, 64, 1, 1]            --
│    └─Empty: 2-3315                     [64]                      --
│    └─Empty: 2-3316                     [64]                      --
│    └─BatchNorm2d: 2-3317               [1, 64, 32, 32]           --
│    └─Scaler: 2-3318                    [1, 64, 32, 32]           --
│    └─ReLU: 2-3319                      [1, 64, 32, 32]           --
│    └─Empty: 2-3320                     [1, 64, 32, 32]           --
│    └─Clamp: 2-3321                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-249        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-3322                 [1, 64, 32, 32]           --
│    └─Empty: 2-3323                     [1, 64, 32, 32]           --
│    └─Empty: 2-3324                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-3325        --                        --
│    └─One: 2-3326                       [1]                       --
│    └─OutputScale: 2-3327               --                        --
│    └─Empty: 2-3328                     [64, 64, 3, 3]            --
│    └─Empty: 2-3329                     [64, 64, 3, 3]            --
│    └─Empty: 2-3330                     [64]                      --
│    └─Empty: 2-3331                     [64]                      --
│    └─BatchNorm2d: 2-3332               [1, 64, 32, 32]           --
│    └─Scaler: 2-3333                    [1, 64, 32, 32]           --
│    └─ReLU: 2-3334                      [1, 64, 32, 32]           --
│    └─Empty: 2-3335                     [1, 64, 32, 32]           --
│    └─Clamp: 2-3336                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-250        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-3337                 [1, 64, 16, 16]           --
│    └─Empty: 2-3338                     [1, 64, 16, 16]           --
│    └─Empty: 2-3339                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-3340        --                        --
│    └─One: 2-3341                       [1]                       --
│    └─OutputScale: 2-3342               --                        --
│    └─Empty: 2-3343                     [64, 64, 3, 3]            --
│    └─Empty: 2-3344                     [64, 64, 3, 3]            --
│    └─Empty: 2-3345                     [64]                      --
│    └─Empty: 2-3346                     [64]                      --
│    └─BatchNorm2d: 2-3347               [1, 64, 16, 16]           --
│    └─Scaler: 2-3348                    [1, 64, 16, 16]           --
│    └─ReLU: 2-3349                      [1, 64, 16, 16]           --
│    └─Empty: 2-3350                     [1, 64, 16, 16]           --
│    └─Clamp: 2-3351                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-251               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-3352        --                        --
│    └─One: 2-3353                       [1]                       --
│    └─OutputScale: 2-3354               --                        --
│    └─Empty: 2-3355                     [64, 64, 1, 1]            --
│    └─Empty: 2-3356                     [64, 64, 1, 1]            --
│    └─Empty: 2-3357                     [64]                      --
│    └─Empty: 2-3358                     [64]                      --
│    └─BatchNorm2d: 2-3359               [1, 64, 16, 16]           --
│    └─Scaler: 2-3360                    [1, 64, 16, 16]           --
│    └─ReLU: 2-3361                      [1, 64, 16, 16]           --
│    └─Empty: 2-3362                     [1, 64, 16, 16]           --
│    └─Clamp: 2-3363                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-252        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-3364                 [1, 64, 16, 16]           --
│    └─Empty: 2-3365                     [1, 64, 16, 16]           --
│    └─Empty: 2-3366                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-3367        --                        --
│    └─One: 2-3368                       [1]                       --
│    └─OutputScale: 2-3369               --                        --
│    └─Empty: 2-3370                     [64, 64, 3, 3]            --
│    └─Empty: 2-3371                     [64, 64, 3, 3]            --
│    └─Empty: 2-3372                     [64]                      --
│    └─Empty: 2-3373                     [64]                      --
│    └─BatchNorm2d: 2-3374               [1, 64, 16, 16]           --
│    └─Scaler: 2-3375                    [1, 64, 16, 16]           --
│    └─ReLU: 2-3376                      [1, 64, 16, 16]           --
│    └─Empty: 2-3377                     [1, 64, 16, 16]           --
│    └─Clamp: 2-3378                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-253        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-3379                 [1, 64, 8, 8]             --
│    └─Empty: 2-3380                     [1, 64, 8, 8]             --
│    └─Empty: 2-3381                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-3382        --                        --
│    └─One: 2-3383                       [1]                       --
│    └─OutputScale: 2-3384               --                        --
│    └─Empty: 2-3385                     [64, 64, 1, 1]            --
│    └─Empty: 2-3386                     [64, 64, 1, 1]            --
│    └─Empty: 2-3387                     [64]                      --
│    └─Empty: 2-3388                     [64]                      --
│    └─BatchNorm2d: 2-3389               [1, 64, 8, 8]             --
│    └─Scaler: 2-3390                    [1, 64, 8, 8]             --
│    └─ReLU: 2-3391                      [1, 64, 8, 8]             --
│    └─Empty: 2-3392                     [1, 64, 8, 8]             --
│    └─Clamp: 2-3393                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-254               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-3394        --                        --
│    └─One: 2-3395                       [1]                       --
│    └─OutputScale: 2-3396               --                        --
│    └─Empty: 2-3397                     [64, 64, 1, 1]            --
│    └─Empty: 2-3398                     [64, 64, 1, 1]            --
│    └─Empty: 2-3399                     [64]                      --
│    └─Empty: 2-3400                     [64]                      --
│    └─BatchNorm2d: 2-3401               [1, 64, 8, 8]             --
│    └─Scaler: 2-3402                    [1, 64, 8, 8]             --
│    └─ReLU: 2-3403                      [1, 64, 8, 8]             --
│    └─Empty: 2-3404                     [1, 64, 8, 8]             --
│    └─Clamp: 2-3405                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-255        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-3406                 [1, 64, 8, 8]             --
│    └─Empty: 2-3407                     [1, 64, 8, 8]             --
│    └─Empty: 2-3408                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-3409        --                        --
│    └─One: 2-3410                       [1]                       --
│    └─OutputScale: 2-3411               --                        --
│    └─Empty: 2-3412                     [64, 64, 3, 3]            --
│    └─Empty: 2-3413                     [64, 64, 3, 3]            --
│    └─Empty: 2-3414                     [64]                      --
│    └─Empty: 2-3415                     [64]                      --
│    └─BatchNorm2d: 2-3416               [1, 64, 8, 8]             --
│    └─Scaler: 2-3417                    [1, 64, 8, 8]             --
│    └─ReLU: 2-3418                      [1, 64, 8, 8]             --
│    └─Empty: 2-3419                     [1, 64, 8, 8]             --
│    └─Clamp: 2-3420                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-256               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-3421        --                        --
│    └─One: 2-3422                       [1]                       --
│    └─OutputScale: 2-3423               --                        --
│    └─Empty: 2-3424                     [64, 3, 1, 1]             --
│    └─Empty: 2-3425                     [64, 3, 1, 1]             --
│    └─Empty: 2-3426                     [64]                      --
│    └─Empty: 2-3427                     [64]                      --
│    └─BatchNorm2d: 2-3428               [1, 64, 256, 256]         --
│    └─Scaler: 2-3429                    [1, 64, 256, 256]         --
│    └─ReLU: 2-3430                      [1, 64, 256, 256]         --
│    └─Empty: 2-3431                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3432                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-257               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-3433        --                        --
│    └─One: 2-3434                       [1]                       --
│    └─OutputScale: 2-3435               --                        --
│    └─Empty: 2-3436                     [64, 64, 3, 3]            --
│    └─Empty: 2-3437                     [64, 64, 3, 3]            --
│    └─Empty: 2-3438                     [64]                      --
│    └─Empty: 2-3439                     [64]                      --
│    └─BatchNorm2d: 2-3440               [1, 64, 256, 256]         --
│    └─Scaler: 2-3441                    [1, 64, 256, 256]         --
│    └─ReLU: 2-3442                      [1, 64, 256, 256]         --
│    └─Empty: 2-3443                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3444                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-258               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-3445        --                        --
│    └─One: 2-3446                       [1]                       --
│    └─OutputScale: 2-3447               --                        --
│    └─Empty: 2-3448                     [64, 64, 1, 1]            --
│    └─Empty: 2-3449                     [64, 64, 1, 1]            --
│    └─Empty: 2-3450                     [64]                      --
│    └─Empty: 2-3451                     [64]                      --
│    └─BatchNorm2d: 2-3452               [1, 64, 256, 256]         --
│    └─Scaler: 2-3453                    [1, 64, 256, 256]         --
│    └─ReLU: 2-3454                      [1, 64, 256, 256]         --
│    └─Empty: 2-3455                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3456                     [1, 64, 256, 256]         --
├─FusedConv2dBNReLU: 1-259               [1, 64, 256, 256]         (recursive)
│    └─OutputShiftSqueeze: 2-3457        --                        --
│    └─One: 2-3458                       [1]                       --
│    └─OutputScale: 2-3459               --                        --
│    └─Empty: 2-3460                     [64, 64, 3, 3]            --
│    └─Empty: 2-3461                     [64, 64, 3, 3]            --
│    └─Empty: 2-3462                     [64]                      --
│    └─Empty: 2-3463                     [64]                      --
│    └─BatchNorm2d: 2-3464               [1, 64, 256, 256]         --
│    └─Scaler: 2-3465                    [1, 64, 256, 256]         --
│    └─ReLU: 2-3466                      [1, 64, 256, 256]         --
│    └─Empty: 2-3467                     [1, 64, 256, 256]         --
│    └─Clamp: 2-3468                     [1, 64, 256, 256]         --
├─FusedMaxPoolConv2dBNReLU: 1-260        [1, 64, 128, 128]         (recursive)
│    └─MaxPool2d: 2-3469                 [1, 64, 128, 128]         --
│    └─Empty: 2-3470                     [1, 64, 128, 128]         --
│    └─Empty: 2-3471                     [1, 64, 128, 128]         --
│    └─OutputShiftSqueeze: 2-3472        --                        --
│    └─One: 2-3473                       [1]                       --
│    └─OutputScale: 2-3474               --                        --
│    └─Empty: 2-3475                     [64, 64, 3, 3]            --
│    └─Empty: 2-3476                     [64, 64, 3, 3]            --
│    └─Empty: 2-3477                     [64]                      --
│    └─Empty: 2-3478                     [64]                      --
│    └─BatchNorm2d: 2-3479               [1, 64, 128, 128]         --
│    └─Scaler: 2-3480                    [1, 64, 128, 128]         --
│    └─ReLU: 2-3481                      [1, 64, 128, 128]         --
│    └─Empty: 2-3482                     [1, 64, 128, 128]         --
│    └─Clamp: 2-3483                     [1, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-261               [1, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-3484        --                        --
│    └─One: 2-3485                       [1]                       --
│    └─OutputScale: 2-3486               --                        --
│    └─Empty: 2-3487                     [64, 64, 3, 3]            --
│    └─Empty: 2-3488                     [64, 64, 3, 3]            --
│    └─Empty: 2-3489                     [64]                      --
│    └─Empty: 2-3490                     [64]                      --
│    └─BatchNorm2d: 2-3491               [1, 64, 128, 128]         --
│    └─Scaler: 2-3492                    [1, 64, 128, 128]         --
│    └─ReLU: 2-3493                      [1, 64, 128, 128]         --
│    └─Empty: 2-3494                     [1, 64, 128, 128]         --
│    └─Clamp: 2-3495                     [1, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-262        [1, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-3496                 [1, 64, 64, 64]           --
│    └─Empty: 2-3497                     [1, 64, 64, 64]           --
│    └─Empty: 2-3498                     [1, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-3499        --                        --
│    └─One: 2-3500                       [1]                       --
│    └─OutputScale: 2-3501               --                        --
│    └─Empty: 2-3502                     [64, 64, 3, 3]            --
│    └─Empty: 2-3503                     [64, 64, 3, 3]            --
│    └─Empty: 2-3504                     [64]                      --
│    └─Empty: 2-3505                     [64]                      --
│    └─BatchNorm2d: 2-3506               [1, 64, 64, 64]           --
│    └─Scaler: 2-3507                    [1, 64, 64, 64]           --
│    └─ReLU: 2-3508                      [1, 64, 64, 64]           --
│    └─Empty: 2-3509                     [1, 64, 64, 64]           --
│    └─Clamp: 2-3510                     [1, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-263               [1, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-3511        --                        --
│    └─One: 2-3512                       [1]                       --
│    └─OutputScale: 2-3513               --                        --
│    └─Empty: 2-3514                     [64, 64, 3, 3]            --
│    └─Empty: 2-3515                     [64, 64, 3, 3]            --
│    └─Empty: 2-3516                     [64]                      --
│    └─Empty: 2-3517                     [64]                      --
│    └─BatchNorm2d: 2-3518               [1, 64, 64, 64]           --
│    └─Scaler: 2-3519                    [1, 64, 64, 64]           --
│    └─ReLU: 2-3520                      [1, 64, 64, 64]           --
│    └─Empty: 2-3521                     [1, 64, 64, 64]           --
│    └─Clamp: 2-3522                     [1, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-264        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-3523                 [1, 64, 32, 32]           --
│    └─Empty: 2-3524                     [1, 64, 32, 32]           --
│    └─Empty: 2-3525                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-3526        --                        --
│    └─One: 2-3527                       [1]                       --
│    └─OutputScale: 2-3528               --                        --
│    └─Empty: 2-3529                     [64, 64, 3, 3]            --
│    └─Empty: 2-3530                     [64, 64, 3, 3]            --
│    └─Empty: 2-3531                     [64]                      --
│    └─Empty: 2-3532                     [64]                      --
│    └─BatchNorm2d: 2-3533               [1, 64, 32, 32]           --
│    └─Scaler: 2-3534                    [1, 64, 32, 32]           --
│    └─ReLU: 2-3535                      [1, 64, 32, 32]           --
│    └─Empty: 2-3536                     [1, 64, 32, 32]           --
│    └─Clamp: 2-3537                     [1, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-265               [1, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-3538        --                        --
│    └─One: 2-3539                       [1]                       --
│    └─OutputScale: 2-3540               --                        --
│    └─Empty: 2-3541                     [64, 64, 1, 1]            --
│    └─Empty: 2-3542                     [64, 64, 1, 1]            --
│    └─Empty: 2-3543                     [64]                      --
│    └─Empty: 2-3544                     [64]                      --
│    └─BatchNorm2d: 2-3545               [1, 64, 32, 32]           --
│    └─Scaler: 2-3546                    [1, 64, 32, 32]           --
│    └─ReLU: 2-3547                      [1, 64, 32, 32]           --
│    └─Empty: 2-3548                     [1, 64, 32, 32]           --
│    └─Clamp: 2-3549                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-266        [1, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-3550                 [1, 64, 32, 32]           --
│    └─Empty: 2-3551                     [1, 64, 32, 32]           --
│    └─Empty: 2-3552                     [1, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-3553        --                        --
│    └─One: 2-3554                       [1]                       --
│    └─OutputScale: 2-3555               --                        --
│    └─Empty: 2-3556                     [64, 64, 3, 3]            --
│    └─Empty: 2-3557                     [64, 64, 3, 3]            --
│    └─Empty: 2-3558                     [64]                      --
│    └─Empty: 2-3559                     [64]                      --
│    └─BatchNorm2d: 2-3560               [1, 64, 32, 32]           --
│    └─Scaler: 2-3561                    [1, 64, 32, 32]           --
│    └─ReLU: 2-3562                      [1, 64, 32, 32]           --
│    └─Empty: 2-3563                     [1, 64, 32, 32]           --
│    └─Clamp: 2-3564                     [1, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-267        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-3565                 [1, 64, 16, 16]           --
│    └─Empty: 2-3566                     [1, 64, 16, 16]           --
│    └─Empty: 2-3567                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-3568        --                        --
│    └─One: 2-3569                       [1]                       --
│    └─OutputScale: 2-3570               --                        --
│    └─Empty: 2-3571                     [64, 64, 3, 3]            --
│    └─Empty: 2-3572                     [64, 64, 3, 3]            --
│    └─Empty: 2-3573                     [64]                      --
│    └─Empty: 2-3574                     [64]                      --
│    └─BatchNorm2d: 2-3575               [1, 64, 16, 16]           --
│    └─Scaler: 2-3576                    [1, 64, 16, 16]           --
│    └─ReLU: 2-3577                      [1, 64, 16, 16]           --
│    └─Empty: 2-3578                     [1, 64, 16, 16]           --
│    └─Clamp: 2-3579                     [1, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-268               [1, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-3580        --                        --
│    └─One: 2-3581                       [1]                       --
│    └─OutputScale: 2-3582               --                        --
│    └─Empty: 2-3583                     [64, 64, 1, 1]            --
│    └─Empty: 2-3584                     [64, 64, 1, 1]            --
│    └─Empty: 2-3585                     [64]                      --
│    └─Empty: 2-3586                     [64]                      --
│    └─BatchNorm2d: 2-3587               [1, 64, 16, 16]           --
│    └─Scaler: 2-3588                    [1, 64, 16, 16]           --
│    └─ReLU: 2-3589                      [1, 64, 16, 16]           --
│    └─Empty: 2-3590                     [1, 64, 16, 16]           --
│    └─Clamp: 2-3591                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-269        [1, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-3592                 [1, 64, 16, 16]           --
│    └─Empty: 2-3593                     [1, 64, 16, 16]           --
│    └─Empty: 2-3594                     [1, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-3595        --                        --
│    └─One: 2-3596                       [1]                       --
│    └─OutputScale: 2-3597               --                        --
│    └─Empty: 2-3598                     [64, 64, 3, 3]            --
│    └─Empty: 2-3599                     [64, 64, 3, 3]            --
│    └─Empty: 2-3600                     [64]                      --
│    └─Empty: 2-3601                     [64]                      --
│    └─BatchNorm2d: 2-3602               [1, 64, 16, 16]           --
│    └─Scaler: 2-3603                    [1, 64, 16, 16]           --
│    └─ReLU: 2-3604                      [1, 64, 16, 16]           --
│    └─Empty: 2-3605                     [1, 64, 16, 16]           --
│    └─Clamp: 2-3606                     [1, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-270        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-3607                 [1, 64, 8, 8]             --
│    └─Empty: 2-3608                     [1, 64, 8, 8]             --
│    └─Empty: 2-3609                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-3610        --                        --
│    └─One: 2-3611                       [1]                       --
│    └─OutputScale: 2-3612               --                        --
│    └─Empty: 2-3613                     [64, 64, 1, 1]            --
│    └─Empty: 2-3614                     [64, 64, 1, 1]            --
│    └─Empty: 2-3615                     [64]                      --
│    └─Empty: 2-3616                     [64]                      --
│    └─BatchNorm2d: 2-3617               [1, 64, 8, 8]             --
│    └─Scaler: 2-3618                    [1, 64, 8, 8]             --
│    └─ReLU: 2-3619                      [1, 64, 8, 8]             --
│    └─Empty: 2-3620                     [1, 64, 8, 8]             --
│    └─Clamp: 2-3621                     [1, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-271               [1, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-3622        --                        --
│    └─One: 2-3623                       [1]                       --
│    └─OutputScale: 2-3624               --                        --
│    └─Empty: 2-3625                     [64, 64, 1, 1]            --
│    └─Empty: 2-3626                     [64, 64, 1, 1]            --
│    └─Empty: 2-3627                     [64]                      --
│    └─Empty: 2-3628                     [64]                      --
│    └─BatchNorm2d: 2-3629               [1, 64, 8, 8]             --
│    └─Scaler: 2-3630                    [1, 64, 8, 8]             --
│    └─ReLU: 2-3631                      [1, 64, 8, 8]             --
│    └─Empty: 2-3632                     [1, 64, 8, 8]             --
│    └─Clamp: 2-3633                     [1, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-272        [1, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-3634                 [1, 64, 8, 8]             --
│    └─Empty: 2-3635                     [1, 64, 8, 8]             --
│    └─Empty: 2-3636                     [1, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-3637        --                        --
│    └─One: 2-3638                       [1]                       --
│    └─OutputScale: 2-3639               --                        --
│    └─Empty: 2-3640                     [64, 64, 3, 3]            --
│    └─Empty: 2-3641                     [64, 64, 3, 3]            --
│    └─Empty: 2-3642                     [64]                      --
│    └─Empty: 2-3643                     [64]                      --
│    └─BatchNorm2d: 2-3644               [1, 64, 8, 8]             --
│    └─Scaler: 2-3645                    [1, 64, 8, 8]             --
│    └─ReLU: 2-3646                      [1, 64, 8, 8]             --
│    └─Empty: 2-3647                     [1, 64, 8, 8]             --
│    └─Clamp: 2-3648                     [1, 64, 8, 8]             --
├─Conv1d: 1-273                          [1, 64, 16]               262,214
│    └─OutputShiftSqueeze: 2-3649        --                        --
│    └─One: 2-3650                       [1]                       --
│    └─OutputScale: 2-3651               --                        --
│    └─Empty: 2-3652                     [64, 4096, 1]             --
│    └─Empty: 2-3653                     [64, 4096, 1]             --
│    └─Empty: 2-3654                     [64]                      --
│    └─Empty: 2-3655                     [64]                      --
│    └─Scaler: 2-3656                    [1, 64, 16]               --
│    └─Empty: 2-3657                     [1, 64, 16]               --
│    └─Empty: 2-3658                     [1, 64, 16]               --
│    └─Clamp: 2-3659                     [1, 64, 16]               --
├─FusedConv1dBNReLU: 1-274               [1, 64, 16]               12,358
│    └─OutputShiftSqueeze: 2-3660        --                        --
│    └─One: 2-3661                       [1]                       --
│    └─OutputScale: 2-3662               --                        --
│    └─Empty: 2-3663                     [64, 64, 3]               --
│    └─Empty: 2-3664                     [64, 64, 3]               --
│    └─Empty: 2-3665                     [64]                      --
│    └─Empty: 2-3666                     [64]                      --
│    └─BatchNorm1d: 2-3667               [1, 64, 16]               --
│    └─Scaler: 2-3668                    [1, 64, 16]               --
│    └─ReLU: 2-3669                      [1, 64, 16]               --
│    └─Empty: 2-3670                     [1, 64, 16]               --
│    └─Clamp: 2-3671                     [1, 64, 16]               --
├─Conv1d: 1-275                          [1, 64, 16]               4,166
│    └─OutputShiftSqueeze: 2-3672        --                        --
│    └─One: 2-3673                       [1]                       --
│    └─OutputScale: 2-3674               --                        --
│    └─Empty: 2-3675                     [64, 64, 1]               --
│    └─Empty: 2-3676                     [64, 64, 1]               --
│    └─Empty: 2-3677                     [64]                      --
│    └─Empty: 2-3678                     [64]                      --
│    └─Scaler: 2-3679                    [1, 64, 16]               --
│    └─Empty: 2-3680                     [1, 64, 16]               --
│    └─Empty: 2-3681                     [1, 64, 16]               --
│    └─Clamp: 2-3682                     [1, 64, 16]               --
├─FusedConv1dBNReLU: 1-276               [1, 64, 16]               12,358
│    └─OutputShiftSqueeze: 2-3683        --                        --
│    └─One: 2-3684                       [1]                       --
│    └─OutputScale: 2-3685               --                        --
│    └─Empty: 2-3686                     [64, 64, 3]               --
│    └─Empty: 2-3687                     [64, 64, 3]               --
│    └─Empty: 2-3688                     [64]                      --
│    └─Empty: 2-3689                     [64]                      --
│    └─BatchNorm1d: 2-3690               [1, 64, 16]               --
│    └─Scaler: 2-3691                    [1, 64, 16]               --
│    └─ReLU: 2-3692                      [1, 64, 16]               --
│    └─Empty: 2-3693                     [1, 64, 16]               --
│    └─Clamp: 2-3694                     [1, 64, 16]               --
├─Conv1d: 1-277                          [1, 64, 16]               4,166
│    └─OutputShiftSqueeze: 2-3695        --                        --
│    └─One: 2-3696                       [1]                       --
│    └─OutputScale: 2-3697               --                        --
│    └─Empty: 2-3698                     [64, 64, 1]               --
│    └─Empty: 2-3699                     [64, 64, 1]               --
│    └─Empty: 2-3700                     [64]                      --
│    └─Empty: 2-3701                     [64]                      --
│    └─Scaler: 2-3702                    [1, 64, 16]               --
│    └─Empty: 2-3703                     [1, 64, 16]               --
│    └─Empty: 2-3704                     [1, 64, 16]               --
│    └─Clamp: 2-3705                     [1, 64, 16]               --
├─FusedConv1dBNReLU: 1-278               [1, 64, 12]               12,358
│    └─OutputShiftSqueeze: 2-3706        --                        --
│    └─One: 2-3707                       [1]                       --
│    └─OutputScale: 2-3708               --                        --
│    └─Empty: 2-3709                     [64, 64, 3]               --
│    └─Empty: 2-3710                     [64, 64, 3]               --
│    └─Empty: 2-3711                     [64]                      --
│    └─Empty: 2-3712                     [64]                      --
│    └─BatchNorm1d: 2-3713               [1, 64, 12]               --
│    └─Scaler: 2-3714                    [1, 64, 12]               --
│    └─ReLU: 2-3715                      [1, 64, 12]               --
│    └─Empty: 2-3716                     [1, 64, 12]               --
│    └─Clamp: 2-3717                     [1, 64, 12]               --
├─Conv1d: 1-279                          [1, 64, 12]               4,166
│    └─OutputShiftSqueeze: 2-3718        --                        --
│    └─One: 2-3719                       [1]                       --
│    └─OutputScale: 2-3720               --                        --
│    └─Empty: 2-3721                     [64, 64, 1]               --
│    └─Empty: 2-3722                     [64, 64, 1]               --
│    └─Empty: 2-3723                     [64]                      --
│    └─Empty: 2-3724                     [64]                      --
│    └─Scaler: 2-3725                    [1, 64, 12]               --
│    └─Empty: 2-3726                     [1, 64, 12]               --
│    └─Empty: 2-3727                     [1, 64, 12]               --
│    └─Clamp: 2-3728                     [1, 64, 12]               --
├─FusedConv1dBNReLU: 1-280               [1, 64, 8]                12,358
│    └─OutputShiftSqueeze: 2-3729        --                        --
│    └─One: 2-3730                       [1]                       --
│    └─OutputScale: 2-3731               --                        --
│    └─Empty: 2-3732                     [64, 64, 3]               --
│    └─Empty: 2-3733                     [64, 64, 3]               --
│    └─Empty: 2-3734                     [64]                      --
│    └─Empty: 2-3735                     [64]                      --
│    └─BatchNorm1d: 2-3736               [1, 64, 8]                --
│    └─Scaler: 2-3737                    [1, 64, 8]                --
│    └─ReLU: 2-3738                      [1, 64, 8]                --
│    └─Empty: 2-3739                     [1, 64, 8]                --
│    └─Clamp: 2-3740                     [1, 64, 8]                --
├─Conv1d: 1-281                          [1, 64, 8]                4,166
│    └─OutputShiftSqueeze: 2-3741        --                        --
│    └─One: 2-3742                       [1]                       --
│    └─OutputScale: 2-3743               --                        --
│    └─Empty: 2-3744                     [64, 64, 1]               --
│    └─Empty: 2-3745                     [64, 64, 1]               --
│    └─Empty: 2-3746                     [64]                      --
│    └─Empty: 2-3747                     [64]                      --
│    └─Scaler: 2-3748                    [1, 64, 8]                --
│    └─Empty: 2-3749                     [1, 64, 8]                --
│    └─Empty: 2-3750                     [1, 64, 8]                --
│    └─Clamp: 2-3751                     [1, 64, 8]                --
├─FusedLinearReLU: 1-282                 [1, 32]                   16,422
│    └─OutputShiftSqueeze: 2-3752        --                        --
│    └─One: 2-3753                       [1]                       --
│    └─OutputScale: 2-3754               --                        --
│    └─Empty: 2-3755                     [32, 512]                 --
│    └─Empty: 2-3756                     [32, 512]                 --
│    └─Empty: 2-3757                     [32]                      --
│    └─Empty: 2-3758                     [32]                      --
│    └─Scaler: 2-3759                    [1, 32]                   --
│    └─ReLU: 2-3760                      [1, 32]                   --
│    └─Empty: 2-3761                     [1, 32]                   --
│    └─Clamp: 2-3762                     [1, 32]                   --
├─Linear: 1-283                          [1, 4]                    134
│    └─OutputShiftSqueeze: 2-3763        --                        --
│    └─One: 2-3764                       [1]                       --
│    └─OutputScale: 2-3765               --                        --
│    └─Empty: 2-3766                     [4, 32]                   --
│    └─Empty: 2-3767                     [4, 32]                   --
│    └─Empty: 2-3768                     [1, 4]                    --
│    └─Empty: 2-3769                     [1, 4]                    --
│    └─Clamp: 2-3770                     [1, 4]                    --
==========================================================================================
Total params: 772,232
Trainable params: 772,064
Non-trainable params: 168
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 12.58
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 12.58
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.389 | Acc: 28.000% | Wgt Acc: 24.889%
	I - Batch: 100 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 28.054%
	I - Batch: 150 | Loss: 1.387 | Acc: 28.667% | Wgt Acc: 26.506%
	I - Batch: 200 | Loss: 1.387 | Acc: 28.000% | Wgt Acc: 25.676%
	I - Batch: 250 | Loss: 1.389 | Acc: 26.400% | Wgt Acc: 24.169%
	I - Batch: 300 | Loss: 1.388 | Acc: 27.333% | Wgt Acc: 24.944%
	I - Batch: 350 | Loss: 1.390 | Acc: 26.286% | Wgt Acc: 23.880%
	I - Batch: 400 | Loss: 1.388 | Acc: 27.750% | Wgt Acc: 25.211%
	I - Batch: 450 | Loss: 1.389 | Acc: 27.556% | Wgt Acc: 24.988%
	I - Batch: 500 | Loss: 1.391 | Acc: 27.000% | Wgt Acc: 24.461%
	I - Batch: 550 | Loss: 1.388 | Acc: 28.000% | Wgt Acc: 25.388%
	I - Batch: 600 | Loss: 1.388 | Acc: 28.167% | Wgt Acc: 25.553%
	I - Batch: 650 | Loss: 1.387 | Acc: 27.231% | Wgt Acc: 24.714%
	I - Batch: 700 | Loss: 1.386 | Acc: 27.857% | Wgt Acc: 25.282%
	I - Batch: 750 | Loss: 1.387 | Acc: 27.867% | Wgt Acc: 25.270%
	I - Batch: 800 | Loss: 1.387 | Acc: 27.750% | Wgt Acc: 25.162%
	I - Batch: 850 | Loss: 1.387 | Acc: 28.000% | Wgt Acc: 25.391%
	I - Batch: 900 | Loss: 1.387 | Acc: 28.111% | Wgt Acc: 25.482%
	I - Batch: 950 | Loss: 1.388 | Acc: 27.684% | Wgt Acc: 25.107%
	I - Batch: 1000 | Loss: 1.389 | Acc: 27.700% | Wgt Acc: 25.327%
	I - Batch: 1050 | Loss: 1.388 | Acc: 27.905% | Wgt Acc: 25.515%
	I - Batch: 1100 | Loss: 1.387 | Acc: 28.091% | Wgt Acc: 25.666%
	I - Batch: 1150 | Loss: 1.388 | Acc: 28.000% | Wgt Acc: 25.569%
	I - Batch: 1200 | Loss: 1.388 | Acc: 27.917% | Wgt Acc: 25.460%
	I - Batch: 1250 | Loss: 1.386 | Acc: 28.160% | Wgt Acc: 25.681%
	I - Batch: 1300 | Loss: 1.387 | Acc: 27.846% | Wgt Acc: 25.399%
	I - Batch: 1350 | Loss: 1.386 | Acc: 28.000% | Wgt Acc: 25.526%
	I - Batch: 1400 | Loss: 1.386 | Acc: 28.000% | Wgt Acc: 25.527%
	I - Batch: 1450 | Loss: 1.387 | Acc: 28.000% | Wgt Acc: 25.513%
	I - Batch: 1500 | Loss: 1.388 | Acc: 27.467% | Wgt Acc: 25.004%
	I - Batch: 1550 | Loss: 1.388 | Acc: 27.097% | Wgt Acc: 24.702%
	I - Batch: 1600 | Loss: 1.389 | Acc: 27.000% | Wgt Acc: 24.596%
	I - Batch: 1650 | Loss: 1.388 | Acc: 27.030% | Wgt Acc: 24.621%
	I - Batch: 1700 | Loss: 1.389 | Acc: 26.588% | Wgt Acc: 24.196%
	I - Batch: 1750 | Loss: 1.390 | Acc: 26.743% | Wgt Acc: 24.328%
	I - Batch: 1800 | Loss: 1.389 | Acc: 26.667% | Wgt Acc: 24.272%
	I - Batch: 1850 | Loss: 1.387 | Acc: 26.703% | Wgt Acc: 24.324%
	I - Batch: 1900 | Loss: 1.387 | Acc: 27.000% | Wgt Acc: 24.591%
	I - Batch: 1950 | Loss: 1.388 | Acc: 27.128% | Wgt Acc: 24.694%
	I - Batch: 2000 | Loss: 1.388 | Acc: 27.200% | Wgt Acc: 24.758%
	I - Batch: 2050 | Loss: 1.387 | Acc: 27.415% | Wgt Acc: 24.962%
	I - Batch: 2100 | Loss: 1.387 | Acc: 27.571% | Wgt Acc: 25.099%
	I - Batch: 2150 | Loss: 1.386 | Acc: 27.907% | Wgt Acc: 25.404%
	I - Batch: 2200 | Loss: 1.387 | Acc: 27.909% | Wgt Acc: 25.392%
	I - Batch: 2250 | Loss: 1.387 | Acc: 27.556% | Wgt Acc: 25.055%
	I - Batch: 2300 | Loss: 1.388 | Acc: 27.391% | Wgt Acc: 24.892%
	I - Batch: 2350 | Loss: 1.388 | Acc: 27.234% | Wgt Acc: 24.828%
	I - Batch: 2400 | Loss: 1.388 | Acc: 27.250% | Wgt Acc: 24.843%
	I - Batch: 2450 | Loss: 1.388 | Acc: 27.265% | Wgt Acc: 24.853%
	I - Batch: 2500 | Loss: 1.388 | Acc: 27.240% | Wgt Acc: 24.836%
I - num batch: 2547
I - Train -- Loss: 1.387 | Acc: 27.287% | Wgt Acc: 24.876% | LR: 1.000000e-03 | Dur: 1363.94s
I - Confusion Matrix: [row->prediction - col->label]
[[271. 214. 274. 216.]
 [ 15.   9.  25.  21.]
 [379. 327. 392. 278.]
 [ 32.  28.  43.  23.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.398 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.419 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.415 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.409 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.412 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.411 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.411 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 65.89s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Local maximum validation set accuracy:  26.91

I - Validation set results: 
[14.0-1.0-0.0-3.634761333465576][50.0-3.0-0.0-3.634761333465576][124.0-2.0-0.0-3.634761333465576][127.0-0.0-0.0-3.634761333465576][443.0-2.0-0.0-3.634761333465576][567.0-0.0-0.0-3.634761333465576][573.0-1.0-0.0-3.634761333465576][615.0-0.0-0.0-3.634761333465576][695.0-1.0-0.0-3.634761333465576][722.0-3.0-0.0-3.634761333465576]
[826.0-0.0-0.0-3.634761333465576][878.0-0.0-0.0-3.634761333465576][1103.0-0.0-0.0-3.634761333465576][1212.0-3.0-0.0-3.634761333465576][1368.0-0.0-0.0-3.634761333465576][2181.0-2.0-0.0-3.634761333465576][2476.0-2.0-0.0-3.634761333465576][2721.0-2.0-0.0-3.634761333465576][2818.0-1.0-0.0-3.634761333465576][2886.0-2.0-0.0-3.634761333465576]
[3231.0-2.0-0.0-3.634761333465576][3333.0-2.0-0.0-3.634761333465576][3482.0-2.0-0.0-3.634761333465576][3536.0-3.0-0.0-3.634761333465576][3625.0-1.0-0.0-3.634761333465576][3909.0-0.0-0.0-3.634761333465576][4035.0-0.0-0.0-3.634761333465576][4140.0-0.0-0.0-3.634761333465576][4214.0-1.0-0.0-3.634761333465576][4346.0-1.0-0.0-3.634761333465576]
[4581.0-2.0-0.0-3.634761333465576][4708.0-3.0-0.0-3.634761333465576][4838.0-3.0-0.0-3.634761333465576][4845.0-1.0-0.0-3.634761333465576][4868.0-0.0-0.0-3.634761333465576][4939.0-0.0-0.0-3.634761333465576][4984.0-2.0-0.0-3.634761333465576][5078.0-1.0-0.0-3.634761333465576][5396.0-0.0-0.0-3.634761333465576][5479.0-1.0-0.0-3.634761333465576]
[5717.0-0.0-0.0-3.634761333465576][5843.0-1.0-0.0-3.634761333465576][5949.0-3.0-0.0-3.634761333465576][5987.0-2.0-0.0-3.634761333465576][6014.0-3.0-0.0-3.634761333465576][6033.0-3.0-0.0-3.634761333465576][6313.0-0.0-0.0-3.634761333465576][6421.0-3.0-0.0-3.634761333465576][6500.0-1.0-0.0-3.634761333465576][6583.0-3.0-0.0-3.634761333465576]
[6683.0-3.0-0.0-3.634761333465576][6825.0-2.0-0.0-3.634761333465576][6998.0-3.0-0.0-3.634761333465576][7049.0-3.0-0.0-3.634761333465576][7517.0-1.0-0.0-3.634761333465576][7521.0-1.0-0.0-3.634761333465576][7528.0-1.0-0.0-3.634761333465576][7949.0-1.0-0.0-3.634761333465576][8135.0-1.0-0.0-3.634761333465576][8185.0-3.0-0.0-3.634761333465576]
[8269.0-3.0-0.0-3.634761333465576][8273.0-3.0-0.0-3.634761333465576][8543.0-3.0-0.0-3.634761333465576][8666.0-1.0-0.0-3.634761333465576][8672.0-0.0-0.0-3.634761333465576][8903.0-1.0-0.0-3.634761333465576][9001.0-2.0-0.0-3.634761333465576][9036.0-2.0-0.0-3.634761333465576][9281.0-3.0-0.0-3.634761333465576][9300.0-2.0-0.0-3.634761333465576]
[9571.0-0.0-0.0-3.634761333465576][9617.0-1.0-0.0-3.634761333465576][9644.0-2.0-0.0-3.634761333465576][9705.0-2.0-0.0-3.634761333465576][9801.0-0.0-0.0-3.634761333465576][9803.0-3.0-0.0-3.634761333465576][9865.0-3.0-0.0-3.634761333465576][9896.0-2.0-0.0-3.634761333465576][10314.0-1.0-0.0-3.634761333465576][10337.0-3.0-0.0-3.634761333465576]
[10403.0-0.0-0.0-3.634761333465576][10653.0-2.0-0.0-3.634761333465576][10704.0-2.0-0.0-3.634761333465576][10719.0-1.0-0.0-3.634761333465576][10727.0-1.0-0.0-3.634761333465576][10836.0-0.0-0.0-3.634761333465576][10969.0-2.0-0.0-3.634761333465576][11042.0-0.0-0.0-3.634761333465576][11088.0-1.0-0.0-3.634761333465576][11322.0-0.0-0.0-3.634761333465576]
[11398.0-2.0-0.0-3.634761333465576][11499.0-0.0-0.0-3.634761333465576][11502.0-3.0-0.0-3.634761333465576][11512.0-3.0-0.0-3.634761333465576][11608.0-1.0-0.0-3.634761333465576][11610.0-0.0-0.0-3.634761333465576][11692.0-0.0-0.0-3.634761333465576][11905.0-0.0-0.0-3.634761333465576][11993.0-1.0-0.0-3.634761333465576][12002.0-2.0-0.0-3.634761333465576]
[12052.0-0.0-0.0-3.634761333465576][12201.0-0.0-0.0-3.634761333465576][12235.0-2.0-0.0-3.634761333465576][12320.0-1.0-0.0-3.634761333465576][12377.0-2.0-0.0-3.634761333465576][12398.0-2.0-0.0-3.634761333465576][12503.0-1.0-0.0-3.634761333465576][12617.0-0.0-0.0-3.634761333465576][12685.0-3.0-0.0-3.634761333465576][12738.0-2.0-0.0-3.634761333465576]
[12742.0-2.0-0.0-3.634761333465576][12823.0-0.0-0.0-3.634761333465576][13110.0-1.0-0.0-3.634761333465576][13240.0-3.0-0.0-3.634761333465576][13253.0-1.0-0.0-3.634761333465576][13273.0-0.0-0.0-3.634761333465576][13634.0-1.0-0.0-3.634761333465576][13763.0-2.0-0.0-3.634761333465576][13905.0-3.0-0.0-3.634761333465576][14060.0-2.0-0.0-3.634761333465576]
[14065.0-3.0-0.0-3.634761333465576][14147.0-3.0-0.0-3.634761333465576][14595.0-2.0-0.0-3.634761333465576][14687.0-2.0-0.0-3.634761333465576][14788.0-2.0-0.0-3.634761333465576][14869.0-1.0-0.0-3.634761333465576][14872.0-3.0-0.0-3.634761333465576][14877.0-1.0-0.0-3.634761333465576][14927.0-0.0-0.0-3.634761333465576][15066.0-0.0-0.0-3.634761333465576]
[15175.0-1.0-0.0-3.634761333465576][15178.0-2.0-0.0-3.634761333465576][15375.0-3.0-0.0-3.634761333465576][15389.0-3.0-0.0-3.634761333465576][15568.0-2.0-0.0-3.634761333465576][15675.0-3.0-0.0-3.634761333465576][15869.0-1.0-0.0-3.634761333465576][16207.0-3.0-0.0-3.634761333465576][16236.0-0.0-0.0-3.634761333465576][16302.0-3.0-0.0-3.634761333465576]
[16331.0-2.0-0.0-3.634761333465576][16381.0-0.0-0.0-3.634761333465576][16488.0-1.0-0.0-3.634761333465576][16495.0-0.0-0.0-3.634761333465576][16650.0-0.0-0.0-3.634761333465576][16719.0-1.0-0.0-3.634761333465576][16801.0-0.0-0.0-3.634761333465576][16828.0-0.0-0.0-3.634761333465576][17137.0-3.0-0.0-3.634761333465576][17245.0-1.0-0.0-3.634761333465576]
[17278.0-3.0-0.0-3.634761333465576][17282.0-0.0-0.0-3.634761333465576][17311.0-2.0-0.0-3.634761333465576][17336.0-2.0-0.0-3.634761333465576][17608.0-3.0-0.0-3.634761333465576][17627.0-0.0-0.0-3.634761333465576][17877.0-3.0-0.0-3.634761333465576][17924.0-1.0-0.0-3.634761333465576][17984.0-3.0-0.0-3.634761333465576][18211.0-0.0-0.0-3.634761333465576]
[18276.0-3.0-0.0-3.634761333465576][18287.0-1.0-0.0-3.634761333465576][18394.0-0.0-0.0-3.634761333465576][18428.0-0.0-0.0-3.634761333465576][18442.0-0.0-0.0-3.634761333465576][18478.0-3.0-0.0-3.634761333465576][18607.0-0.0-0.0-3.634761333465576][18616.0-0.0-0.0-3.634761333465576][18663.0-0.0-0.0-3.634761333465576][18718.0-0.0-0.0-3.634761333465576]
[18766.0-2.0-0.0-3.634761333465576][18824.0-2.0-0.0-3.634761333465576][18890.0-3.0-0.0-3.634761333465576][18930.0-3.0-0.0-3.634761333465576][18938.0-3.0-0.0-3.634761333465576][19817.0-1.0-0.0-3.634761333465576][19839.0-0.0-0.0-3.634761333465576][19930.0-3.0-0.0-3.634761333465576][19944.0-0.0-0.0-3.634761333465576][20036.0-2.0-0.0-3.634761333465576]
[20101.0-3.0-0.0-3.634761333465576][20474.0-1.0-0.0-3.634761333465576][20547.0-3.0-0.0-3.634761333465576][20929.0-2.0-0.0-3.634761333465576][21245.0-1.0-0.0-3.634761333465576][21257.0-3.0-0.0-3.634761333465576][21293.0-1.0-0.0-3.634761333465576][21316.0-1.0-0.0-3.634761333465576][21384.0-1.0-0.0-3.634761333465576][21448.0-1.0-0.0-3.634761333465576]
[21483.0-0.0-0.0-3.634761333465576][21487.0-2.0-0.0-3.634761333465576][21714.0-0.0-0.0-3.634761333465576][21943.0-3.0-0.0-3.634761333465576][21947.0-0.0-0.0-3.634761333465576][21948.0-0.0-0.0-3.634761333465576][21965.0-2.0-0.0-3.634761333465576][21998.0-1.0-0.0-3.634761333465576][22025.0-0.0-0.0-3.634761333465576][22228.0-3.0-0.0-3.634761333465576]
[22446.0-1.0-0.0-3.634761333465576][22494.0-3.0-0.0-3.634761333465576][22757.0-0.0-0.0-3.634761333465576][22811.0-3.0-0.0-3.634761333465576][22976.0-3.0-0.0-3.634761333465576][22985.0-3.0-0.0-3.634761333465576][23014.0-0.0-0.0-3.634761333465576][23112.0-1.0-0.0-3.634761333465576][23144.0-3.0-0.0-3.634761333465576][23168.0-2.0-0.0-3.634761333465576]
[23219.0-0.0-0.0-3.634761333465576][23363.0-3.0-0.0-3.634761333465576][23470.0-0.0-0.0-3.634761333465576][23486.0-2.0-0.0-3.634761333465576][23497.0-0.0-0.0-3.634761333465576][23516.0-0.0-0.0-3.634761333465576][23690.0-1.0-0.0-3.634761333465576][23921.0-2.0-0.0-3.634761333465576][23936.0-1.0-0.0-3.634761333465576][24040.0-3.0-0.0-3.634761333465576]
[24111.0-1.0-0.0-3.634761333465576][24182.0-0.0-0.0-3.634761333465576][24238.0-3.0-0.0-3.634761333465576][24290.0-2.0-0.0-3.634761333465576][24345.0-0.0-0.0-3.634761333465576][24364.0-1.0-0.0-3.634761333465576][24427.0-3.0-0.0-3.634761333465576][24477.0-2.0-0.0-3.634761333465576][24495.0-2.0-0.0-3.634761333465576][24893.0-2.0-0.0-3.634761333465576]
[25012.0-1.0-0.0-3.634761333465576][25121.0-2.0-0.0-3.634761333465576][25165.0-3.0-0.0-3.634761333465576][25183.0-0.0-0.0-3.634761333465576][25297.0-3.0-0.0-3.634761333465576][25398.0-0.0-0.0-3.634761333465576][25574.0-2.0-0.0-3.634761333465576][25644.0-1.0-0.0-3.634761333465576][25718.0-1.0-0.0-3.634761333465576][25774.0-2.0-0.0-3.634761333465576]
[26032.0-3.0-0.0-3.634761333465576][26051.0-3.0-0.0-3.634761333465576][26120.0-0.0-0.0-3.634761333465576][26321.0-1.0-0.0-3.634761333465576][26732.0-1.0-0.0-3.634761333465576][26784.0-3.0-0.0-3.634761333465576][26827.0-3.0-0.0-3.634761333465576][26833.0-0.0-0.0-3.634761333465576][26838.0-2.0-0.0-3.634761333465576][26860.0-1.0-0.0-3.634761333465576]
[26948.0-0.0-0.0-3.634761333465576][27049.0-3.0-0.0-3.634761333465576][27098.0-1.0-0.0-3.634761333465576][27526.0-0.0-0.0-3.634761333465576][27639.0-3.0-0.0-3.634761333465576][27698.0-3.0-0.0-3.634761333465576][27772.0-0.0-0.0-3.634761333465576][27890.0-1.0-0.0-3.634761333465576][28040.0-0.0-0.0-3.634761333465576][28503.0-2.0-0.0-3.634761333465576]
[28577.0-1.0-0.0-3.634761333465576][28959.0-0.0-0.0-3.634761333465576][29198.0-3.0-0.0-3.634761333465576][29777.0-0.0-0.0-3.634761333465576][29877.0-2.0-0.0-3.634761333465576][30035.0-1.0-0.0-3.634761333465576][30098.0-0.0-0.0-3.634761333465576][30326.0-1.0-0.0-3.634761333465576][30572.0-2.0-0.0-3.634761333465576][30716.0-0.0-0.0-3.634761333465576]
[30806.0-2.0-0.0-3.634761333465576][30906.0-1.0-0.0-3.634761333465576][31007.0-0.0-0.0-3.634761333465576][31181.0-3.0-0.0-3.634761333465576][31238.0-0.0-0.0-3.634761333465576][31347.0-0.0-0.0-3.634761333465576][31422.0-2.0-0.0-3.634761333465576][31429.0-3.0-0.0-3.634761333465576][31431.0-0.0-0.0-3.634761333465576][31432.0-1.0-0.0-3.634761333465576]
[31477.0-0.0-0.0-3.634761333465576][31524.0-1.0-0.0-3.634761333465576][31597.0-1.0-0.0-3.634761333465576][31619.0-1.0-0.0-3.634761333465576][31701.0-0.0-0.0-3.634761333465576][31755.0-0.0-0.0-3.634761333465576][31854.0-3.0-0.0-3.634761333465576][32074.0-1.0-0.0-3.634761333465576][32078.0-3.0-0.0-3.634761333465576][32111.0-1.0-0.0-3.634761333465576]
[32127.0-1.0-0.0-3.634761333465576][32140.0-3.0-0.0-3.634761333465576][32263.0-2.0-0.0-3.634761333465576][32365.0-0.0-0.0-3.634761333465576][32411.0-2.0-0.0-3.634761333465576][32429.0-3.0-0.0-3.634761333465576][32473.0-3.0-0.0-3.634761333465576][32574.0-3.0-0.0-3.634761333465576][32584.0-0.0-0.0-3.634761333465576][32622.0-0.0-0.0-3.634761333465576]
[32858.0-3.0-0.0-3.634761333465576][32969.0-3.0-0.0-3.634761333465576][33016.0-2.0-0.0-3.634761333465576][33031.0-1.0-0.0-3.634761333465576][33035.0-2.0-0.0-3.634761333465576][33133.0-2.0-0.0-3.634761333465576][33173.0-2.0-0.0-3.634761333465576][33175.0-3.0-0.0-3.634761333465576][33306.0-3.0-0.0-3.634761333465576][33309.0-2.0-0.0-3.634761333465576]
[33474.0-0.0-0.0-3.634761333465576][33478.0-2.0-0.0-3.634761333465576][33618.0-1.0-0.0-3.634761333465576][33712.0-0.0-0.0-3.634761333465576][33782.0-2.0-0.0-3.634761333465576][33914.0-3.0-0.0-3.634761333465576][34076.0-3.0-0.0-3.634761333465576][34112.0-2.0-0.0-3.634761333465576][34138.0-2.0-0.0-3.634761333465576][34239.0-1.0-0.0-3.634761333465576]
[34364.0-2.0-0.0-3.634761333465576][34617.0-1.0-0.0-3.634761333465576][34751.0-3.0-0.0-3.634761333465576][34783.0-2.0-0.0-3.634761333465576][35015.0-3.0-0.0-3.634761333465576][35018.0-1.0-0.0-3.634761333465576][35288.0-2.0-0.0-3.634761333465576]
---------------------------
I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.385 | Acc: 30.000% | Wgt Acc: 27.149%
	I - Batch: 100 | Loss: 1.393 | Acc: 26.000% | Wgt Acc: 23.583%
	I - Batch: 150 | Loss: 1.387 | Acc: 28.000% | Wgt Acc: 25.455%
	I - Batch: 200 | Loss: 1.391 | Acc: 27.000% | Wgt Acc: 24.434%
	I - Batch: 250 | Loss: 1.383 | Acc: 28.800% | Wgt Acc: 26.083%
	I - Batch: 300 | Loss: 1.379 | Acc: 30.667% | Wgt Acc: 27.828%
	I - Batch: 350 | Loss: 1.381 | Acc: 30.571% | Wgt Acc: 27.767%
	I - Batch: 400 | Loss: 1.383 | Acc: 30.000% | Wgt Acc: 27.191%
	I - Batch: 450 | Loss: 1.382 | Acc: 30.444% | Wgt Acc: 27.616%
	I - Batch: 500 | Loss: 1.383 | Acc: 30.400% | Wgt Acc: 27.544%
	I - Batch: 550 | Loss: 1.386 | Acc: 29.818% | Wgt Acc: 26.937%
	I - Batch: 600 | Loss: 1.386 | Acc: 30.000% | Wgt Acc: 27.095%
	I - Batch: 650 | Loss: 1.387 | Acc: 29.538% | Wgt Acc: 26.618%
	I - Batch: 700 | Loss: 1.387 | Acc: 29.286% | Wgt Acc: 26.348%
	I - Batch: 750 | Loss: 1.387 | Acc: 28.800% | Wgt Acc: 26.259%
	I - Batch: 800 | Loss: 1.387 | Acc: 29.250% | Wgt Acc: 26.644%
	I - Batch: 850 | Loss: 1.388 | Acc: 28.824% | Wgt Acc: 26.236%
	I - Batch: 900 | Loss: 1.387 | Acc: 29.333% | Wgt Acc: 26.700%
	I - Batch: 950 | Loss: 1.386 | Acc: 29.368% | Wgt Acc: 26.749%
	I - Batch: 1000 | Loss: 1.385 | Acc: 29.500% | Wgt Acc: 26.865%
	I - Batch: 1050 | Loss: 1.385 | Acc: 29.333% | Wgt Acc: 26.712%
	I - Batch: 1100 | Loss: 1.385 | Acc: 29.000% | Wgt Acc: 26.415%
	I - Batch: 1150 | Loss: 1.387 | Acc: 28.435% | Wgt Acc: 25.867%
	I - Batch: 1200 | Loss: 1.387 | Acc: 28.000% | Wgt Acc: 25.474%
	I - Batch: 1250 | Loss: 1.387 | Acc: 27.760% | Wgt Acc: 25.342%
	I - Batch: 1300 | Loss: 1.388 | Acc: 27.769% | Wgt Acc: 25.329%
	I - Batch: 1350 | Loss: 1.388 | Acc: 27.556% | Wgt Acc: 25.154%
	I - Batch: 1400 | Loss: 1.388 | Acc: 27.714% | Wgt Acc: 25.534%
	I - Batch: 1450 | Loss: 1.389 | Acc: 27.448% | Wgt Acc: 25.306%
	I - Batch: 1500 | Loss: 1.389 | Acc: 27.467% | Wgt Acc: 25.389%
	I - Batch: 1550 | Loss: 1.389 | Acc: 27.484% | Wgt Acc: 25.565%
	I - Batch: 1600 | Loss: 1.389 | Acc: 26.875% | Wgt Acc: 25.004%
	I - Batch: 1650 | Loss: 1.390 | Acc: 26.848% | Wgt Acc: 24.949%
	I - Batch: 1700 | Loss: 1.390 | Acc: 26.824% | Wgt Acc: 24.911%
	I - Batch: 1750 | Loss: 1.389 | Acc: 26.800% | Wgt Acc: 24.862%
	I - Batch: 1800 | Loss: 1.389 | Acc: 26.722% | Wgt Acc: 24.788%
	I - Batch: 1850 | Loss: 1.389 | Acc: 26.649% | Wgt Acc: 24.703%
	I - Batch: 1900 | Loss: 1.389 | Acc: 26.579% | Wgt Acc: 24.625%
	I - Batch: 1950 | Loss: 1.389 | Acc: 26.410% | Wgt Acc: 24.453%
	I - Batch: 2000 | Loss: 1.387 | Acc: 26.500% | Wgt Acc: 24.539%
	I - Batch: 2050 | Loss: 1.387 | Acc: 26.537% | Wgt Acc: 24.569%
	I - Batch: 2100 | Loss: 1.387 | Acc: 26.667% | Wgt Acc: 24.671%
	I - Batch: 2150 | Loss: 1.386 | Acc: 26.791% | Wgt Acc: 24.775%
	I - Batch: 2200 | Loss: 1.386 | Acc: 26.864% | Wgt Acc: 24.829%
	I - Batch: 2250 | Loss: 1.386 | Acc: 26.933% | Wgt Acc: 24.875%
	I - Batch: 2300 | Loss: 1.385 | Acc: 27.130% | Wgt Acc: 25.051%
	I - Batch: 2350 | Loss: 1.386 | Acc: 27.191% | Wgt Acc: 25.089%
	I - Batch: 2400 | Loss: 1.386 | Acc: 27.208% | Wgt Acc: 25.091%
	I - Batch: 2450 | Loss: 1.386 | Acc: 27.429% | Wgt Acc: 25.285%
	I - Batch: 2500 | Loss: 1.386 | Acc: 27.400% | Wgt Acc: 25.252%
I - num batch: 2547
I - Train -- Loss: 1.385 | Acc: 27.483% | Wgt Acc: 25.327% | LR: 1.000000e-03 | Dur: 889.85s
I - Confusion Matrix: [row->prediction - col->label]
[[240. 184. 256. 187.]
 [ 59.  55.  74.  48.]
 [384. 328. 397. 295.]
 [ 14.  11.   7.   8.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.392 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.413 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.411 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.411 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.416 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.414 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.415 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 66.29s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.325 | Acc: 40.000% | Wgt Acc: 37.037%
	I - Batch: 100 | Loss: 1.357 | Acc: 36.000% | Wgt Acc: 32.877%
	I - Batch: 150 | Loss: 1.346 | Acc: 38.000% | Wgt Acc: 34.862%
	I - Batch: 200 | Loss: 1.359 | Acc: 33.000% | Wgt Acc: 30.206%
	I - Batch: 250 | Loss: 1.359 | Acc: 31.600% | Wgt Acc: 28.911%
	I - Batch: 300 | Loss: 1.374 | Acc: 29.667% | Wgt Acc: 26.909%
	I - Batch: 350 | Loss: 1.376 | Acc: 30.571% | Wgt Acc: 27.720%
	I - Batch: 400 | Loss: 1.377 | Acc: 29.500% | Wgt Acc: 26.757%
	I - Batch: 450 | Loss: 1.380 | Acc: 28.667% | Wgt Acc: 25.943%
	I - Batch: 500 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.268%
	I - Batch: 550 | Loss: 1.382 | Acc: 28.000% | Wgt Acc: 25.308%
	I - Batch: 600 | Loss: 1.380 | Acc: 28.667% | Wgt Acc: 25.962%
	I - Batch: 650 | Loss: 1.380 | Acc: 29.231% | Wgt Acc: 26.444%
	I - Batch: 700 | Loss: 1.383 | Acc: 28.286% | Wgt Acc: 25.565%
	I - Batch: 750 | Loss: 1.383 | Acc: 28.400% | Wgt Acc: 25.624%
	I - Batch: 800 | Loss: 1.384 | Acc: 28.250% | Wgt Acc: 25.472%
	I - Batch: 850 | Loss: 1.385 | Acc: 28.353% | Wgt Acc: 25.564%
	I - Batch: 900 | Loss: 1.385 | Acc: 28.333% | Wgt Acc: 25.532%
	I - Batch: 950 | Loss: 1.385 | Acc: 28.316% | Wgt Acc: 25.510%
	I - Batch: 1000 | Loss: 1.386 | Acc: 28.200% | Wgt Acc: 25.360%
	I - Batch: 1050 | Loss: 1.385 | Acc: 28.286% | Wgt Acc: 25.632%
	I - Batch: 1100 | Loss: 1.384 | Acc: 28.364% | Wgt Acc: 25.731%
	I - Batch: 1150 | Loss: 1.384 | Acc: 28.696% | Wgt Acc: 26.022%
	I - Batch: 1200 | Loss: 1.385 | Acc: 28.417% | Wgt Acc: 25.764%
	I - Batch: 1250 | Loss: 1.385 | Acc: 28.320% | Wgt Acc: 25.675%
	I - Batch: 1300 | Loss: 1.385 | Acc: 28.231% | Wgt Acc: 25.589%
	I - Batch: 1350 | Loss: 1.386 | Acc: 28.222% | Wgt Acc: 25.562%
	I - Batch: 1400 | Loss: 1.386 | Acc: 28.071% | Wgt Acc: 25.405%
	I - Batch: 1450 | Loss: 1.386 | Acc: 28.207% | Wgt Acc: 25.543%
	I - Batch: 1500 | Loss: 1.385 | Acc: 28.467% | Wgt Acc: 25.780%
	I - Batch: 1550 | Loss: 1.384 | Acc: 28.516% | Wgt Acc: 25.843%
	I - Batch: 1600 | Loss: 1.384 | Acc: 28.625% | Wgt Acc: 25.929%
	I - Batch: 1650 | Loss: 1.385 | Acc: 28.364% | Wgt Acc: 25.675%
	I - Batch: 1700 | Loss: 1.385 | Acc: 28.529% | Wgt Acc: 25.838%
	I - Batch: 1750 | Loss: 1.385 | Acc: 28.629% | Wgt Acc: 25.927%
	I - Batch: 1800 | Loss: 1.385 | Acc: 28.722% | Wgt Acc: 26.004%
	I - Batch: 1850 | Loss: 1.385 | Acc: 28.865% | Wgt Acc: 26.123%
	I - Batch: 1900 | Loss: 1.385 | Acc: 28.842% | Wgt Acc: 26.112%
	I - Batch: 1950 | Loss: 1.386 | Acc: 28.821% | Wgt Acc: 26.089%
	I - Batch: 2000 | Loss: 1.386 | Acc: 28.950% | Wgt Acc: 26.191%
	I - Batch: 2050 | Loss: 1.386 | Acc: 28.927% | Wgt Acc: 26.173%
	I - Batch: 2100 | Loss: 1.385 | Acc: 28.905% | Wgt Acc: 26.159%
	I - Batch: 2150 | Loss: 1.386 | Acc: 28.837% | Wgt Acc: 26.090%
	I - Batch: 2200 | Loss: 1.386 | Acc: 28.682% | Wgt Acc: 25.942%
	I - Batch: 2250 | Loss: 1.385 | Acc: 28.800% | Wgt Acc: 26.062%
	I - Batch: 2300 | Loss: 1.385 | Acc: 28.783% | Wgt Acc: 26.054%
	I - Batch: 2350 | Loss: 1.385 | Acc: 28.766% | Wgt Acc: 26.036%
	I - Batch: 2400 | Loss: 1.385 | Acc: 28.833% | Wgt Acc: 26.094%
	I - Batch: 2450 | Loss: 1.385 | Acc: 28.898% | Wgt Acc: 26.150%
	I - Batch: 2500 | Loss: 1.385 | Acc: 28.800% | Wgt Acc: 26.052%
I - num batch: 2547
I - Train -- Loss: 1.386 | Acc: 28.661% | Wgt Acc: 25.920% | LR: 1.000000e-03 | Dur: 893.98s
I - Confusion Matrix: [row->prediction - col->label]
[[259. 216. 258. 195.]
 [  5.  10.  15.   3.]
 [432. 352. 461. 340.]
 [  1.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.391 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.390 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.393 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.395 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.395 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 66.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.367 | Acc: 26.000% | Wgt Acc: 24.074%
	I - Batch: 100 | Loss: 1.352 | Acc: 31.000% | Wgt Acc: 28.704%
	I - Batch: 150 | Loss: 1.364 | Acc: 30.667% | Wgt Acc: 28.178%
	I - Batch: 200 | Loss: 1.366 | Acc: 32.000% | Wgt Acc: 29.324%
	I - Batch: 250 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 27.248%
	I - Batch: 300 | Loss: 1.377 | Acc: 29.333% | Wgt Acc: 26.606%
	I - Batch: 350 | Loss: 1.377 | Acc: 30.286% | Wgt Acc: 27.461%
	I - Batch: 400 | Loss: 1.377 | Acc: 30.250% | Wgt Acc: 27.422%
	I - Batch: 450 | Loss: 1.375 | Acc: 30.444% | Wgt Acc: 27.579%
	I - Batch: 500 | Loss: 1.378 | Acc: 30.200% | Wgt Acc: 27.355%
	I - Batch: 550 | Loss: 1.380 | Acc: 29.455% | Wgt Acc: 26.634%
	I - Batch: 600 | Loss: 1.380 | Acc: 29.833% | Wgt Acc: 26.978%
	I - Batch: 650 | Loss: 1.378 | Acc: 29.538% | Wgt Acc: 26.760%
	I - Batch: 700 | Loss: 1.380 | Acc: 29.429% | Wgt Acc: 26.632%
	I - Batch: 750 | Loss: 1.380 | Acc: 29.600% | Wgt Acc: 26.787%
	I - Batch: 800 | Loss: 1.378 | Acc: 30.000% | Wgt Acc: 27.180%
	I - Batch: 850 | Loss: 1.379 | Acc: 30.000% | Wgt Acc: 27.157%
	I - Batch: 900 | Loss: 1.379 | Acc: 29.667% | Wgt Acc: 26.861%
	I - Batch: 950 | Loss: 1.380 | Acc: 29.579% | Wgt Acc: 26.781%
	I - Batch: 1000 | Loss: 1.380 | Acc: 29.400% | Wgt Acc: 26.606%
	I - Batch: 1050 | Loss: 1.381 | Acc: 29.048% | Wgt Acc: 26.270%
	I - Batch: 1100 | Loss: 1.382 | Acc: 28.727% | Wgt Acc: 25.955%
	I - Batch: 1150 | Loss: 1.382 | Acc: 28.522% | Wgt Acc: 25.776%
	I - Batch: 1200 | Loss: 1.382 | Acc: 28.417% | Wgt Acc: 25.687%
	I - Batch: 1250 | Loss: 1.382 | Acc: 28.560% | Wgt Acc: 25.818%
	I - Batch: 1300 | Loss: 1.381 | Acc: 28.692% | Wgt Acc: 25.943%
	I - Batch: 1350 | Loss: 1.382 | Acc: 28.519% | Wgt Acc: 25.783%
	I - Batch: 1400 | Loss: 1.382 | Acc: 28.143% | Wgt Acc: 25.411%
	I - Batch: 1450 | Loss: 1.383 | Acc: 28.138% | Wgt Acc: 25.447%
	I - Batch: 1500 | Loss: 1.383 | Acc: 27.867% | Wgt Acc: 25.263%
	I - Batch: 1550 | Loss: 1.382 | Acc: 27.548% | Wgt Acc: 25.004%
	I - Batch: 1600 | Loss: 1.382 | Acc: 27.500% | Wgt Acc: 24.954%
	I - Batch: 1650 | Loss: 1.383 | Acc: 27.394% | Wgt Acc: 24.833%
	I - Batch: 1700 | Loss: 1.383 | Acc: 27.765% | Wgt Acc: 25.166%
	I - Batch: 1750 | Loss: 1.383 | Acc: 27.771% | Wgt Acc: 25.174%
	I - Batch: 1800 | Loss: 1.383 | Acc: 27.778% | Wgt Acc: 25.166%
	I - Batch: 1850 | Loss: 1.383 | Acc: 27.730% | Wgt Acc: 25.131%
	I - Batch: 1900 | Loss: 1.382 | Acc: 27.842% | Wgt Acc: 25.244%
	I - Batch: 1950 | Loss: 1.383 | Acc: 27.897% | Wgt Acc: 25.281%
	I - Batch: 2000 | Loss: 1.383 | Acc: 27.950% | Wgt Acc: 25.324%
	I - Batch: 2050 | Loss: 1.383 | Acc: 27.951% | Wgt Acc: 25.316%
	I - Batch: 2100 | Loss: 1.383 | Acc: 28.000% | Wgt Acc: 25.360%
	I - Batch: 2150 | Loss: 1.383 | Acc: 28.093% | Wgt Acc: 25.446%
	I - Batch: 2200 | Loss: 1.383 | Acc: 28.136% | Wgt Acc: 25.474%
	I - Batch: 2250 | Loss: 1.384 | Acc: 28.133% | Wgt Acc: 25.458%
	I - Batch: 2300 | Loss: 1.383 | Acc: 28.348% | Wgt Acc: 25.657%
	I - Batch: 2350 | Loss: 1.383 | Acc: 28.468% | Wgt Acc: 25.760%
	I - Batch: 2400 | Loss: 1.383 | Acc: 28.500% | Wgt Acc: 25.796%
	I - Batch: 2450 | Loss: 1.383 | Acc: 28.408% | Wgt Acc: 25.706%
	I - Batch: 2500 | Loss: 1.384 | Acc: 28.360% | Wgt Acc: 25.651%
I - num batch: 2547
I - Train -- Loss: 1.384 | Acc: 28.072% | Wgt Acc: 25.389% | LR: 1.000000e-03 | Dur: 989.42s
I - Confusion Matrix: [row->prediction - col->label]
[[198. 165. 212. 135.]
 [ 19.   9.  11.   9.]
 [477. 399. 507. 393.]
 [  3.   5.   4.   1.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.389 | Acc: 22.000% | Wgt Acc: 24.664%
	I - Batch: 100 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 26.667%
	I - Batch: 150 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 26.706%
	I - Batch: 200 | Loss: 1.386 | Acc: 25.500% | Wgt Acc: 28.365%
	I - Batch: 250 | Loss: 1.386 | Acc: 26.400% | Wgt Acc: 29.307%
	I - Batch: 300 | Loss: 1.386 | Acc: 26.000% | Wgt Acc: 28.868%
I - num batch: 327
I - Val -- Loss: 1.386 | Acc: 26.300% | Wgt Acc: 29.212% | Dur: 120.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 1.390 | Acc: 26.000% | Wgt Acc: 28.440%
	I - Batch: 100 | Loss: 1.386 | Acc: 26.000% | Wgt Acc: 26.027%
	I - Batch: 150 | Loss: 1.386 | Acc: 28.667% | Wgt Acc: 27.534%
	I - Batch: 200 | Loss: 1.383 | Acc: 30.000% | Wgt Acc: 28.345%
	I - Batch: 250 | Loss: 1.382 | Acc: 30.000% | Wgt Acc: 28.131%
	I - Batch: 300 | Loss: 1.376 | Acc: 30.667% | Wgt Acc: 28.658%
	I - Batch: 350 | Loss: 1.384 | Acc: 28.000% | Wgt Acc: 25.952%
	I - Batch: 400 | Loss: 1.382 | Acc: 28.250% | Wgt Acc: 26.131%
	I - Batch: 450 | Loss: 1.381 | Acc: 28.444% | Wgt Acc: 26.244%
	I - Batch: 500 | Loss: 1.384 | Acc: 28.000% | Wgt Acc: 25.769%
	I - Batch: 550 | Loss: 1.382 | Acc: 28.364% | Wgt Acc: 26.080%
	I - Batch: 600 | Loss: 1.385 | Acc: 28.333% | Wgt Acc: 25.979%
	I - Batch: 650 | Loss: 1.385 | Acc: 28.462% | Wgt Acc: 26.069%
	I - Batch: 700 | Loss: 1.386 | Acc: 28.571% | Wgt Acc: 26.137%
	I - Batch: 750 | Loss: 1.388 | Acc: 27.867% | Wgt Acc: 25.421%
	I - Batch: 800 | Loss: 1.388 | Acc: 27.750% | Wgt Acc: 25.486%
	I - Batch: 850 | Loss: 1.388 | Acc: 27.882% | Wgt Acc: 25.584%
	I - Batch: 900 | Loss: 1.387 | Acc: 27.778% | Wgt Acc: 25.483%
	I - Batch: 950 | Loss: 1.386 | Acc: 28.316% | Wgt Acc: 25.951%
	I - Batch: 1000 | Loss: 1.388 | Acc: 27.900% | Wgt Acc: 25.530%
	I - Batch: 1050 | Loss: 1.388 | Acc: 28.000% | Wgt Acc: 25.618%
	I - Batch: 1100 | Loss: 1.387 | Acc: 27.818% | Wgt Acc: 25.457%
	I - Batch: 1150 | Loss: 1.387 | Acc: 27.652% | Wgt Acc: 25.290%
	I - Batch: 1200 | Loss: 1.388 | Acc: 27.333% | Wgt Acc: 24.986%
	I - Batch: 1250 | Loss: 1.387 | Acc: 27.440% | Wgt Acc: 25.081%
	I - Batch: 1300 | Loss: 1.389 | Acc: 27.231% | Wgt Acc: 24.857%
	I - Batch: 1350 | Loss: 1.389 | Acc: 27.259% | Wgt Acc: 24.858%
	I - Batch: 1400 | Loss: 1.389 | Acc: 27.143% | Wgt Acc: 24.734%
	I - Batch: 1450 | Loss: 1.388 | Acc: 27.724% | Wgt Acc: 25.272%
	I - Batch: 1500 | Loss: 1.389 | Acc: 27.400% | Wgt Acc: 24.974%
	I - Batch: 1550 | Loss: 1.390 | Acc: 27.226% | Wgt Acc: 24.800%
	I - Batch: 1600 | Loss: 1.389 | Acc: 27.312% | Wgt Acc: 24.877%
	I - Batch: 1650 | Loss: 1.389 | Acc: 27.273% | Wgt Acc: 24.836%
	I - Batch: 1700 | Loss: 1.389 | Acc: 27.235% | Wgt Acc: 24.801%
	I - Batch: 1750 | Loss: 1.389 | Acc: 27.143% | Wgt Acc: 24.707%
	I - Batch: 1800 | Loss: 1.389 | Acc: 27.222% | Wgt Acc: 24.831%
	I - Batch: 1850 | Loss: 1.389 | Acc: 27.189% | Wgt Acc: 24.948%
	I - Batch: 1900 | Loss: 1.389 | Acc: 27.211% | Wgt Acc: 24.979%
	I - Batch: 1950 | Loss: 1.390 | Acc: 27.128% | Wgt Acc: 24.876%
	I - Batch: 2000 | Loss: 1.390 | Acc: 26.850% | Wgt Acc: 24.597%
	I - Batch: 2050 | Loss: 1.390 | Acc: 26.878% | Wgt Acc: 24.613%
	I - Batch: 2100 | Loss: 1.390 | Acc: 27.095% | Wgt Acc: 24.810%
	I - Batch: 2150 | Loss: 1.389 | Acc: 27.302% | Wgt Acc: 24.995%
	I - Batch: 2200 | Loss: 1.389 | Acc: 27.318% | Wgt Acc: 25.000%
	I - Batch: 2250 | Loss: 1.389 | Acc: 27.467% | Wgt Acc: 25.123%
	I - Batch: 2300 | Loss: 1.389 | Acc: 27.391% | Wgt Acc: 25.056%
	I - Batch: 2350 | Loss: 1.389 | Acc: 27.277% | Wgt Acc: 24.938%
	I - Batch: 2400 | Loss: 1.389 | Acc: 27.167% | Wgt Acc: 24.831%
	I - Batch: 2450 | Loss: 1.389 | Acc: 27.184% | Wgt Acc: 24.841%
	I - Batch: 2500 | Loss: 1.389 | Acc: 27.080% | Wgt Acc: 24.741%
I - num batch: 2547
I - Train -- Loss: 1.389 | Acc: 27.012% | Wgt Acc: 24.664% | LR: 1.000000e-03 | Dur: 1393.00s
I - Confusion Matrix: [row->prediction - col->label]
[[271. 235. 302. 191.]
 [ 18.  20.  28.  18.]
 [379. 309. 381. 313.]
 [ 29.  14.  23.  16.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.384 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.391 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.390 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.391 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.393 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.393 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.393 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 1.385 | Acc: 28.000% | Wgt Acc: 25.112%
	I - Batch: 100 | Loss: 1.391 | Acc: 26.000% | Wgt Acc: 23.266%
	I - Batch: 150 | Loss: 1.391 | Acc: 25.333% | Wgt Acc: 22.552%
	I - Batch: 200 | Loss: 1.385 | Acc: 27.500% | Wgt Acc: 24.719%
	I - Batch: 250 | Loss: 1.381 | Acc: 28.400% | Wgt Acc: 25.632%
	I - Batch: 300 | Loss: 1.380 | Acc: 27.667% | Wgt Acc: 24.962%
	I - Batch: 350 | Loss: 1.379 | Acc: 28.571% | Wgt Acc: 25.823%
	I - Batch: 400 | Loss: 1.384 | Acc: 27.000% | Wgt Acc: 24.338%
	I - Batch: 450 | Loss: 1.384 | Acc: 27.333% | Wgt Acc: 24.612%
	I - Batch: 500 | Loss: 1.381 | Acc: 27.800% | Wgt Acc: 25.124%
	I - Batch: 550 | Loss: 1.380 | Acc: 28.545% | Wgt Acc: 25.801%
	I - Batch: 600 | Loss: 1.378 | Acc: 28.167% | Wgt Acc: 25.500%
	I - Batch: 650 | Loss: 1.375 | Acc: 28.923% | Wgt Acc: 26.220%
	I - Batch: 700 | Loss: 1.379 | Acc: 28.286% | Wgt Acc: 25.598%
	I - Batch: 750 | Loss: 1.379 | Acc: 28.267% | Wgt Acc: 25.565%
	I - Batch: 800 | Loss: 1.378 | Acc: 28.875% | Wgt Acc: 26.139%
	I - Batch: 850 | Loss: 1.377 | Acc: 29.294% | Wgt Acc: 26.532%
	I - Batch: 900 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.258%
	I - Batch: 950 | Loss: 1.378 | Acc: 28.632% | Wgt Acc: 25.911%
	I - Batch: 1000 | Loss: 1.378 | Acc: 28.500% | Wgt Acc: 25.774%
	I - Batch: 1050 | Loss: 1.378 | Acc: 28.286% | Wgt Acc: 25.576%
	I - Batch: 1100 | Loss: 1.379 | Acc: 28.182% | Wgt Acc: 25.483%
	I - Batch: 1150 | Loss: 1.380 | Acc: 27.739% | Wgt Acc: 25.049%
	I - Batch: 1200 | Loss: 1.380 | Acc: 27.667% | Wgt Acc: 24.995%
	I - Batch: 1250 | Loss: 1.379 | Acc: 27.760% | Wgt Acc: 25.099%
	I - Batch: 1300 | Loss: 1.380 | Acc: 27.462% | Wgt Acc: 24.813%
	I - Batch: 1350 | Loss: 1.380 | Acc: 27.407% | Wgt Acc: 24.762%
	I - Batch: 1400 | Loss: 1.381 | Acc: 27.429% | Wgt Acc: 24.766%
	I - Batch: 1450 | Loss: 1.381 | Acc: 27.586% | Wgt Acc: 24.891%
	I - Batch: 1500 | Loss: 1.381 | Acc: 27.933% | Wgt Acc: 25.207%
	I - Batch: 1550 | Loss: 1.381 | Acc: 27.806% | Wgt Acc: 25.091%
	I - Batch: 1600 | Loss: 1.380 | Acc: 28.188% | Wgt Acc: 25.444%
	I - Batch: 1650 | Loss: 1.380 | Acc: 28.303% | Wgt Acc: 25.558%
	I - Batch: 1700 | Loss: 1.380 | Acc: 28.294% | Wgt Acc: 25.555%
	I - Batch: 1750 | Loss: 1.380 | Acc: 28.229% | Wgt Acc: 25.487%
	I - Batch: 1800 | Loss: 1.380 | Acc: 28.444% | Wgt Acc: 25.683%
	I - Batch: 1850 | Loss: 1.381 | Acc: 28.378% | Wgt Acc: 25.616%
	I - Batch: 1900 | Loss: 1.381 | Acc: 28.316% | Wgt Acc: 25.561%
	I - Batch: 1950 | Loss: 1.381 | Acc: 28.103% | Wgt Acc: 25.356%
	I - Batch: 2000 | Loss: 1.381 | Acc: 28.100% | Wgt Acc: 25.364%
	I - Batch: 2050 | Loss: 1.381 | Acc: 28.146% | Wgt Acc: 25.407%
	I - Batch: 2100 | Loss: 1.381 | Acc: 28.143% | Wgt Acc: 25.395%
	I - Batch: 2150 | Loss: 1.381 | Acc: 28.186% | Wgt Acc: 25.430%
	I - Batch: 2200 | Loss: 1.382 | Acc: 28.045% | Wgt Acc: 25.289%
	I - Batch: 2250 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.245%
	I - Batch: 2300 | Loss: 1.381 | Acc: 27.913% | Wgt Acc: 25.174%
	I - Batch: 2350 | Loss: 1.381 | Acc: 27.915% | Wgt Acc: 25.185%
	I - Batch: 2400 | Loss: 1.382 | Acc: 27.833% | Wgt Acc: 25.103%
	I - Batch: 2450 | Loss: 1.382 | Acc: 27.673% | Wgt Acc: 24.954%
	I - Batch: 2500 | Loss: 1.382 | Acc: 27.720% | Wgt Acc: 24.989%
I - num batch: 2547
I - Train -- Loss: 1.382 | Acc: 27.719% | Wgt Acc: 24.982% | LR: 1.000000e-03 | Dur: 1398.51s
I - Confusion Matrix: [row->prediction - col->label]
[[224. 193. 250. 156.]
 [  0.   0.   0.   0.]
 [471. 385. 482. 382.]
 [  2.   0.   2.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.390 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.389 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.392 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.393 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.394 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.393 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.56s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 1.369 | Acc: 34.000% | Wgt Acc: 30.909%
	I - Batch: 100 | Loss: 1.371 | Acc: 34.000% | Wgt Acc: 30.839%
	I - Batch: 150 | Loss: 1.370 | Acc: 34.000% | Wgt Acc: 30.862%
	I - Batch: 200 | Loss: 1.379 | Acc: 31.000% | Wgt Acc: 27.959%
	I - Batch: 250 | Loss: 1.380 | Acc: 29.600% | Wgt Acc: 26.643%
	I - Batch: 300 | Loss: 1.383 | Acc: 29.000% | Wgt Acc: 26.048%
	I - Batch: 350 | Loss: 1.384 | Acc: 27.429% | Wgt Acc: 24.647%
	I - Batch: 400 | Loss: 1.386 | Acc: 26.250% | Wgt Acc: 23.529%
	I - Batch: 450 | Loss: 1.387 | Acc: 26.000% | Wgt Acc: 23.295%
	I - Batch: 500 | Loss: 1.388 | Acc: 26.000% | Wgt Acc: 23.287%
	I - Batch: 550 | Loss: 1.389 | Acc: 25.273% | Wgt Acc: 22.648%
	I - Batch: 600 | Loss: 1.387 | Acc: 25.833% | Wgt Acc: 23.195%
	I - Batch: 650 | Loss: 1.391 | Acc: 25.231% | Wgt Acc: 22.597%
	I - Batch: 700 | Loss: 1.390 | Acc: 25.286% | Wgt Acc: 22.685%
	I - Batch: 750 | Loss: 1.389 | Acc: 25.600% | Wgt Acc: 22.980%
	I - Batch: 800 | Loss: 1.389 | Acc: 26.000% | Wgt Acc: 23.364%
	I - Batch: 850 | Loss: 1.390 | Acc: 26.000% | Wgt Acc: 23.325%
	I - Batch: 900 | Loss: 1.391 | Acc: 25.444% | Wgt Acc: 22.989%
	I - Batch: 950 | Loss: 1.390 | Acc: 25.789% | Wgt Acc: 23.400%
	I - Batch: 1000 | Loss: 1.387 | Acc: 26.700% | Wgt Acc: 24.247%
	I - Batch: 1050 | Loss: 1.386 | Acc: 26.667% | Wgt Acc: 24.234%
	I - Batch: 1100 | Loss: 1.388 | Acc: 26.636% | Wgt Acc: 24.177%
	I - Batch: 1150 | Loss: 1.387 | Acc: 26.435% | Wgt Acc: 24.002%
	I - Batch: 1200 | Loss: 1.385 | Acc: 26.750% | Wgt Acc: 24.306%
	I - Batch: 1250 | Loss: 1.386 | Acc: 26.560% | Wgt Acc: 24.113%
	I - Batch: 1300 | Loss: 1.386 | Acc: 26.538% | Wgt Acc: 24.091%
	I - Batch: 1350 | Loss: 1.387 | Acc: 26.444% | Wgt Acc: 24.003%
	I - Batch: 1400 | Loss: 1.386 | Acc: 26.643% | Wgt Acc: 24.187%
	I - Batch: 1450 | Loss: 1.386 | Acc: 26.483% | Wgt Acc: 24.037%
	I - Batch: 1500 | Loss: 1.386 | Acc: 26.400% | Wgt Acc: 23.960%
	I - Batch: 1550 | Loss: 1.387 | Acc: 26.452% | Wgt Acc: 23.994%
	I - Batch: 1600 | Loss: 1.386 | Acc: 26.312% | Wgt Acc: 23.860%
	I - Batch: 1650 | Loss: 1.385 | Acc: 26.424% | Wgt Acc: 24.031%
	I - Batch: 1700 | Loss: 1.385 | Acc: 26.235% | Wgt Acc: 23.973%
	I - Batch: 1750 | Loss: 1.386 | Acc: 26.343% | Wgt Acc: 24.070%
	I - Batch: 1800 | Loss: 1.385 | Acc: 26.278% | Wgt Acc: 24.005%
	I - Batch: 1850 | Loss: 1.386 | Acc: 26.108% | Wgt Acc: 23.840%
	I - Batch: 1900 | Loss: 1.386 | Acc: 26.105% | Wgt Acc: 23.826%
	I - Batch: 1950 | Loss: 1.386 | Acc: 26.000% | Wgt Acc: 23.701%
	I - Batch: 2000 | Loss: 1.386 | Acc: 26.000% | Wgt Acc: 23.686%
	I - Batch: 2050 | Loss: 1.386 | Acc: 26.390% | Wgt Acc: 24.052%
	I - Batch: 2100 | Loss: 1.386 | Acc: 26.524% | Wgt Acc: 24.158%
	I - Batch: 2150 | Loss: 1.385 | Acc: 26.837% | Wgt Acc: 24.447%
	I - Batch: 2200 | Loss: 1.384 | Acc: 27.182% | Wgt Acc: 24.762%
	I - Batch: 2250 | Loss: 1.384 | Acc: 27.067% | Wgt Acc: 24.647%
	I - Batch: 2300 | Loss: 1.384 | Acc: 27.174% | Wgt Acc: 24.748%
	I - Batch: 2350 | Loss: 1.384 | Acc: 27.191% | Wgt Acc: 24.755%
	I - Batch: 2400 | Loss: 1.384 | Acc: 27.375% | Wgt Acc: 24.922%
	I - Batch: 2450 | Loss: 1.384 | Acc: 27.265% | Wgt Acc: 24.807%
	I - Batch: 2500 | Loss: 1.384 | Acc: 27.160% | Wgt Acc: 24.707%
I - num batch: 2547
I - Train -- Loss: 1.384 | Acc: 27.248% | Wgt Acc: 24.779% | LR: 1.000000e-03 | Dur: 1396.30s
I - Confusion Matrix: [row->prediction - col->label]
[[209. 172. 227. 144.]
 [ 30.  25.  47.  27.]
 [457. 381. 460. 367.]
 [  1.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 122.65s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 1.401 | Acc: 26.000% | Wgt Acc: 22.907%
	I - Batch: 100 | Loss: 1.389 | Acc: 29.000% | Wgt Acc: 25.893%
	I - Batch: 150 | Loss: 1.391 | Acc: 27.333% | Wgt Acc: 24.441%
	I - Batch: 200 | Loss: 1.392 | Acc: 26.000% | Wgt Acc: 23.188%
	I - Batch: 250 | Loss: 1.392 | Acc: 26.800% | Wgt Acc: 23.907%
	I - Batch: 300 | Loss: 1.392 | Acc: 26.667% | Wgt Acc: 23.704%
	I - Batch: 350 | Loss: 1.391 | Acc: 26.857% | Wgt Acc: 23.828%
	I - Batch: 400 | Loss: 1.391 | Acc: 27.250% | Wgt Acc: 24.249%
	I - Batch: 450 | Loss: 1.389 | Acc: 28.222% | Wgt Acc: 25.211%
	I - Batch: 500 | Loss: 1.387 | Acc: 28.000% | Wgt Acc: 25.067%
	I - Batch: 550 | Loss: 1.389 | Acc: 27.091% | Wgt Acc: 24.247%
	I - Batch: 600 | Loss: 1.391 | Acc: 26.667% | Wgt Acc: 23.827%
	I - Batch: 650 | Loss: 1.390 | Acc: 26.308% | Wgt Acc: 23.562%
	I - Batch: 700 | Loss: 1.388 | Acc: 26.429% | Wgt Acc: 23.695%
	I - Batch: 750 | Loss: 1.386 | Acc: 26.533% | Wgt Acc: 23.804%
	I - Batch: 800 | Loss: 1.384 | Acc: 27.250% | Wgt Acc: 24.494%
	I - Batch: 850 | Loss: 1.387 | Acc: 26.941% | Wgt Acc: 24.194%
	I - Batch: 900 | Loss: 1.386 | Acc: 27.000% | Wgt Acc: 24.276%
	I - Batch: 950 | Loss: 1.388 | Acc: 26.421% | Wgt Acc: 23.741%
	I - Batch: 1000 | Loss: 1.386 | Acc: 26.800% | Wgt Acc: 24.106%
	I - Batch: 1050 | Loss: 1.387 | Acc: 26.762% | Wgt Acc: 24.074%
	I - Batch: 1100 | Loss: 1.386 | Acc: 26.727% | Wgt Acc: 24.059%
	I - Batch: 1150 | Loss: 1.387 | Acc: 26.435% | Wgt Acc: 23.783%
	I - Batch: 1200 | Loss: 1.386 | Acc: 26.750% | Wgt Acc: 24.095%
	I - Batch: 1250 | Loss: 1.386 | Acc: 26.800% | Wgt Acc: 24.144%
	I - Batch: 1300 | Loss: 1.386 | Acc: 27.000% | Wgt Acc: 24.329%
	I - Batch: 1350 | Loss: 1.385 | Acc: 27.111% | Wgt Acc: 24.445%
	I - Batch: 1400 | Loss: 1.385 | Acc: 27.143% | Wgt Acc: 24.481%
	I - Batch: 1450 | Loss: 1.386 | Acc: 27.034% | Wgt Acc: 24.367%
	I - Batch: 1500 | Loss: 1.386 | Acc: 26.867% | Wgt Acc: 24.208%
	I - Batch: 1550 | Loss: 1.386 | Acc: 26.968% | Wgt Acc: 24.306%
	I - Batch: 1600 | Loss: 1.386 | Acc: 27.250% | Wgt Acc: 24.567%
	I - Batch: 1650 | Loss: 1.387 | Acc: 27.091% | Wgt Acc: 24.413%
	I - Batch: 1700 | Loss: 1.386 | Acc: 27.059% | Wgt Acc: 24.390%
	I - Batch: 1750 | Loss: 1.386 | Acc: 27.143% | Wgt Acc: 24.475%
	I - Batch: 1800 | Loss: 1.386 | Acc: 27.222% | Wgt Acc: 24.549%
	I - Batch: 1850 | Loss: 1.386 | Acc: 27.081% | Wgt Acc: 24.415%
	I - Batch: 1900 | Loss: 1.386 | Acc: 27.000% | Wgt Acc: 24.339%
	I - Batch: 1950 | Loss: 1.386 | Acc: 27.179% | Wgt Acc: 24.503%
	I - Batch: 2000 | Loss: 1.387 | Acc: 27.150% | Wgt Acc: 24.465%
	I - Batch: 2050 | Loss: 1.387 | Acc: 27.268% | Wgt Acc: 24.582%
	I - Batch: 2100 | Loss: 1.386 | Acc: 27.571% | Wgt Acc: 24.863%
	I - Batch: 2150 | Loss: 1.386 | Acc: 27.721% | Wgt Acc: 25.000%
	I - Batch: 2200 | Loss: 1.387 | Acc: 27.591% | Wgt Acc: 24.864%
	I - Batch: 2250 | Loss: 1.386 | Acc: 27.778% | Wgt Acc: 25.043%
	I - Batch: 2300 | Loss: 1.387 | Acc: 27.739% | Wgt Acc: 25.002%
	I - Batch: 2350 | Loss: 1.386 | Acc: 28.000% | Wgt Acc: 25.254%
	I - Batch: 2400 | Loss: 1.387 | Acc: 27.792% | Wgt Acc: 25.054%
	I - Batch: 2450 | Loss: 1.387 | Acc: 27.837% | Wgt Acc: 25.099%
	I - Batch: 2500 | Loss: 1.387 | Acc: 27.760% | Wgt Acc: 25.020%
I - num batch: 2547
I - Train -- Loss: 1.387 | Acc: 27.758% | Wgt Acc: 25.018% | LR: 1.000000e-03 | Dur: 1402.98s
I - Confusion Matrix: [row->prediction - col->label]
[[209. 186. 236. 179.]
 [  0.   0.   0.   0.]
 [488. 392. 498. 359.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.383 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.393 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.392 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.387 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.389 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.388 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.389 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 122.86s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 1.391 | Acc: 22.000% | Wgt Acc: 19.731%
	I - Batch: 100 | Loss: 1.390 | Acc: 25.000% | Wgt Acc: 22.422%
	I - Batch: 150 | Loss: 1.390 | Acc: 25.333% | Wgt Acc: 22.754%
	I - Batch: 200 | Loss: 1.393 | Acc: 25.500% | Wgt Acc: 22.844%
	I - Batch: 250 | Loss: 1.393 | Acc: 26.000% | Wgt Acc: 23.339%
	I - Batch: 300 | Loss: 1.388 | Acc: 28.000% | Wgt Acc: 25.169%
	I - Batch: 350 | Loss: 1.389 | Acc: 28.000% | Wgt Acc: 25.193%
	I - Batch: 400 | Loss: 1.385 | Acc: 29.000% | Wgt Acc: 26.126%
	I - Batch: 450 | Loss: 1.387 | Acc: 28.889% | Wgt Acc: 26.039%
	I - Batch: 500 | Loss: 1.388 | Acc: 29.000% | Wgt Acc: 26.091%
	I - Batch: 550 | Loss: 1.389 | Acc: 28.909% | Wgt Acc: 26.044%
	I - Batch: 600 | Loss: 1.390 | Acc: 27.833% | Wgt Acc: 25.047%
	I - Batch: 650 | Loss: 1.389 | Acc: 28.000% | Wgt Acc: 25.208%
	I - Batch: 700 | Loss: 1.390 | Acc: 27.857% | Wgt Acc: 25.048%
	I - Batch: 750 | Loss: 1.390 | Acc: 28.267% | Wgt Acc: 25.427%
	I - Batch: 800 | Loss: 1.391 | Acc: 27.625% | Wgt Acc: 24.811%
	I - Batch: 850 | Loss: 1.391 | Acc: 27.647% | Wgt Acc: 25.040%
	I - Batch: 900 | Loss: 1.390 | Acc: 28.000% | Wgt Acc: 25.721%
	I - Batch: 950 | Loss: 1.392 | Acc: 27.158% | Wgt Acc: 25.041%
	I - Batch: 1000 | Loss: 1.392 | Acc: 26.900% | Wgt Acc: 24.938%
	I - Batch: 1050 | Loss: 1.392 | Acc: 26.762% | Wgt Acc: 25.032%
	I - Batch: 1100 | Loss: 1.391 | Acc: 26.545% | Wgt Acc: 24.919%
	I - Batch: 1150 | Loss: 1.390 | Acc: 26.696% | Wgt Acc: 25.024%
	I - Batch: 1200 | Loss: 1.390 | Acc: 26.667% | Wgt Acc: 24.953%
	I - Batch: 1250 | Loss: 1.389 | Acc: 26.880% | Wgt Acc: 25.135%
	I - Batch: 1300 | Loss: 1.387 | Acc: 27.154% | Wgt Acc: 25.372%
	I - Batch: 1350 | Loss: 1.388 | Acc: 27.111% | Wgt Acc: 25.287%
	I - Batch: 1400 | Loss: 1.389 | Acc: 26.929% | Wgt Acc: 25.088%
	I - Batch: 1450 | Loss: 1.388 | Acc: 27.172% | Wgt Acc: 25.283%
	I - Batch: 1500 | Loss: 1.387 | Acc: 27.267% | Wgt Acc: 25.356%
	I - Batch: 1550 | Loss: 1.388 | Acc: 27.226% | Wgt Acc: 25.290%
	I - Batch: 1600 | Loss: 1.387 | Acc: 27.250% | Wgt Acc: 25.288%
	I - Batch: 1650 | Loss: 1.387 | Acc: 27.333% | Wgt Acc: 25.337%
	I - Batch: 1700 | Loss: 1.387 | Acc: 27.059% | Wgt Acc: 25.050%
	I - Batch: 1750 | Loss: 1.387 | Acc: 27.543% | Wgt Acc: 25.479%
	I - Batch: 1800 | Loss: 1.386 | Acc: 27.778% | Wgt Acc: 25.681%
	I - Batch: 1850 | Loss: 1.386 | Acc: 27.838% | Wgt Acc: 25.717%
	I - Batch: 1900 | Loss: 1.386 | Acc: 27.842% | Wgt Acc: 25.711%
	I - Batch: 1950 | Loss: 1.387 | Acc: 27.846% | Wgt Acc: 25.689%
	I - Batch: 2000 | Loss: 1.386 | Acc: 28.050% | Wgt Acc: 25.858%
	I - Batch: 2050 | Loss: 1.385 | Acc: 28.341% | Wgt Acc: 26.114%
	I - Batch: 2100 | Loss: 1.385 | Acc: 28.333% | Wgt Acc: 26.115%
	I - Batch: 2150 | Loss: 1.385 | Acc: 28.465% | Wgt Acc: 26.220%
	I - Batch: 2200 | Loss: 1.384 | Acc: 28.409% | Wgt Acc: 26.168%
	I - Batch: 2250 | Loss: 1.385 | Acc: 28.133% | Wgt Acc: 25.904%
	I - Batch: 2300 | Loss: 1.384 | Acc: 28.217% | Wgt Acc: 25.975%
	I - Batch: 2350 | Loss: 1.383 | Acc: 28.170% | Wgt Acc: 25.928%
	I - Batch: 2400 | Loss: 1.384 | Acc: 28.042% | Wgt Acc: 25.793%
	I - Batch: 2450 | Loss: 1.384 | Acc: 27.959% | Wgt Acc: 25.708%
	I - Batch: 2500 | Loss: 1.384 | Acc: 27.840% | Wgt Acc: 25.586%
I - num batch: 2547
I - Train -- Loss: 1.385 | Acc: 27.797% | Wgt Acc: 25.540% | LR: 1.000000e-03 | Dur: 1406.48s
I - Confusion Matrix: [row->prediction - col->label]
[[303. 269. 321. 228.]
 [ 36.  38.  43.  40.]
 [332. 260. 350. 253.]
 [ 26.  11.  20.  17.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.393 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.395 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.394 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 123.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 1.350 | Acc: 24.000% | Wgt Acc: 22.222%
	I - Batch: 100 | Loss: 1.360 | Acc: 22.000% | Wgt Acc: 20.183%
	I - Batch: 150 | Loss: 1.385 | Acc: 20.667% | Wgt Acc: 18.703%
	I - Batch: 200 | Loss: 1.381 | Acc: 25.000% | Wgt Acc: 22.624%
	I - Batch: 250 | Loss: 1.379 | Acc: 27.200% | Wgt Acc: 24.593%
	I - Batch: 300 | Loss: 1.382 | Acc: 26.667% | Wgt Acc: 24.133%
	I - Batch: 350 | Loss: 1.386 | Acc: 25.429% | Wgt Acc: 22.938%
	I - Batch: 400 | Loss: 1.386 | Acc: 26.250% | Wgt Acc: 23.675%
	I - Batch: 450 | Loss: 1.385 | Acc: 26.222% | Wgt Acc: 23.671%
	I - Batch: 500 | Loss: 1.386 | Acc: 26.200% | Wgt Acc: 23.614%
	I - Batch: 550 | Loss: 1.385 | Acc: 26.727% | Wgt Acc: 24.108%
	I - Batch: 600 | Loss: 1.386 | Acc: 26.667% | Wgt Acc: 24.069%
	I - Batch: 650 | Loss: 1.389 | Acc: 25.385% | Wgt Acc: 22.837%
	I - Batch: 700 | Loss: 1.389 | Acc: 24.857% | Wgt Acc: 22.336%
	I - Batch: 750 | Loss: 1.390 | Acc: 24.933% | Wgt Acc: 22.402%
	I - Batch: 800 | Loss: 1.390 | Acc: 25.000% | Wgt Acc: 22.491%
	I - Batch: 850 | Loss: 1.390 | Acc: 24.941% | Wgt Acc: 22.703%
	I - Batch: 900 | Loss: 1.390 | Acc: 24.667% | Wgt Acc: 22.693%
	I - Batch: 950 | Loss: 1.390 | Acc: 24.947% | Wgt Acc: 22.926%
	I - Batch: 1000 | Loss: 1.390 | Acc: 25.400% | Wgt Acc: 23.321%
	I - Batch: 1050 | Loss: 1.391 | Acc: 25.333% | Wgt Acc: 23.215%
	I - Batch: 1100 | Loss: 1.391 | Acc: 25.364% | Wgt Acc: 23.401%
	I - Batch: 1150 | Loss: 1.392 | Acc: 25.304% | Wgt Acc: 23.385%
	I - Batch: 1200 | Loss: 1.390 | Acc: 26.000% | Wgt Acc: 24.003%
	I - Batch: 1250 | Loss: 1.389 | Acc: 26.240% | Wgt Acc: 24.213%
	I - Batch: 1300 | Loss: 1.387 | Acc: 26.692% | Wgt Acc: 24.615%
	I - Batch: 1350 | Loss: 1.387 | Acc: 27.037% | Wgt Acc: 24.904%
	I - Batch: 1400 | Loss: 1.389 | Acc: 26.714% | Wgt Acc: 24.590%
	I - Batch: 1450 | Loss: 1.389 | Acc: 26.483% | Wgt Acc: 24.356%
	I - Batch: 1500 | Loss: 1.389 | Acc: 26.733% | Wgt Acc: 24.569%
	I - Batch: 1550 | Loss: 1.390 | Acc: 26.645% | Wgt Acc: 24.463%
	I - Batch: 1600 | Loss: 1.389 | Acc: 26.688% | Wgt Acc: 24.494%
	I - Batch: 1650 | Loss: 1.388 | Acc: 26.848% | Wgt Acc: 24.652%
	I - Batch: 1700 | Loss: 1.388 | Acc: 27.118% | Wgt Acc: 24.874%
	I - Batch: 1750 | Loss: 1.388 | Acc: 27.257% | Wgt Acc: 24.987%
	I - Batch: 1800 | Loss: 1.388 | Acc: 27.167% | Wgt Acc: 24.884%
	I - Batch: 1850 | Loss: 1.389 | Acc: 27.027% | Wgt Acc: 24.742%
	I - Batch: 1900 | Loss: 1.389 | Acc: 27.158% | Wgt Acc: 24.849%
	I - Batch: 1950 | Loss: 1.389 | Acc: 27.179% | Wgt Acc: 24.861%
	I - Batch: 2000 | Loss: 1.388 | Acc: 27.300% | Wgt Acc: 24.963%
	I - Batch: 2050 | Loss: 1.388 | Acc: 27.366% | Wgt Acc: 25.011%
	I - Batch: 2100 | Loss: 1.388 | Acc: 27.333% | Wgt Acc: 25.094%
	I - Batch: 2150 | Loss: 1.388 | Acc: 27.163% | Wgt Acc: 24.948%
	I - Batch: 2200 | Loss: 1.388 | Acc: 27.045% | Wgt Acc: 24.841%
	I - Batch: 2250 | Loss: 1.387 | Acc: 27.156% | Wgt Acc: 24.935%
	I - Batch: 2300 | Loss: 1.387 | Acc: 27.261% | Wgt Acc: 25.022%
	I - Batch: 2350 | Loss: 1.387 | Acc: 27.362% | Wgt Acc: 25.101%
	I - Batch: 2400 | Loss: 1.386 | Acc: 27.333% | Wgt Acc: 25.077%
	I - Batch: 2450 | Loss: 1.387 | Acc: 27.347% | Wgt Acc: 25.076%
	I - Batch: 2500 | Loss: 1.387 | Acc: 27.520% | Wgt Acc: 25.228%
I - num batch: 2547
I - Train -- Loss: 1.387 | Acc: 27.366% | Wgt Acc: 25.080% | LR: 1.000000e-03 | Dur: 1408.19s
I - Confusion Matrix: [row->prediction - col->label]
[[183. 160. 212. 152.]
 [ 16.  13.  21.  16.]
 [458. 374. 467. 336.]
 [ 40.  31.  34.  34.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.387 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.398 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.396 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.401 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.404 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.404 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.403 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 1.385 | Acc: 22.000% | Wgt Acc: 19.910%
	I - Batch: 100 | Loss: 1.377 | Acc: 23.000% | Wgt Acc: 20.909%
	I - Batch: 150 | Loss: 1.378 | Acc: 26.000% | Wgt Acc: 23.529%
	I - Batch: 200 | Loss: 1.374 | Acc: 26.500% | Wgt Acc: 23.982%
	I - Batch: 250 | Loss: 1.373 | Acc: 26.000% | Wgt Acc: 23.572%
	I - Batch: 300 | Loss: 1.373 | Acc: 28.333% | Wgt Acc: 25.680%
	I - Batch: 350 | Loss: 1.375 | Acc: 28.286% | Wgt Acc: 25.548%
	I - Batch: 400 | Loss: 1.376 | Acc: 28.500% | Wgt Acc: 25.777%
	I - Batch: 450 | Loss: 1.376 | Acc: 28.444% | Wgt Acc: 25.716%
	I - Batch: 500 | Loss: 1.371 | Acc: 28.400% | Wgt Acc: 25.678%
	I - Batch: 550 | Loss: 1.374 | Acc: 28.909% | Wgt Acc: 26.130%
	I - Batch: 600 | Loss: 1.374 | Acc: 29.667% | Wgt Acc: 26.777%
	I - Batch: 650 | Loss: 1.375 | Acc: 29.538% | Wgt Acc: 26.685%
	I - Batch: 700 | Loss: 1.374 | Acc: 29.857% | Wgt Acc: 26.985%
	I - Batch: 750 | Loss: 1.374 | Acc: 30.400% | Wgt Acc: 27.462%
	I - Batch: 800 | Loss: 1.375 | Acc: 30.125% | Wgt Acc: 27.201%
	I - Batch: 850 | Loss: 1.376 | Acc: 29.647% | Wgt Acc: 26.752%
	I - Batch: 900 | Loss: 1.377 | Acc: 29.556% | Wgt Acc: 26.667%
	I - Batch: 950 | Loss: 1.377 | Acc: 29.895% | Wgt Acc: 26.958%
	I - Batch: 1000 | Loss: 1.377 | Acc: 30.100% | Wgt Acc: 27.142%
	I - Batch: 1050 | Loss: 1.377 | Acc: 29.905% | Wgt Acc: 26.953%
	I - Batch: 1100 | Loss: 1.377 | Acc: 30.182% | Wgt Acc: 27.213%
	I - Batch: 1150 | Loss: 1.378 | Acc: 29.913% | Wgt Acc: 26.959%
	I - Batch: 1200 | Loss: 1.378 | Acc: 30.000% | Wgt Acc: 27.052%
	I - Batch: 1250 | Loss: 1.378 | Acc: 29.920% | Wgt Acc: 26.989%
	I - Batch: 1300 | Loss: 1.378 | Acc: 29.923% | Wgt Acc: 26.976%
	I - Batch: 1350 | Loss: 1.378 | Acc: 30.074% | Wgt Acc: 27.121%
	I - Batch: 1400 | Loss: 1.378 | Acc: 29.786% | Wgt Acc: 26.864%
	I - Batch: 1450 | Loss: 1.379 | Acc: 29.586% | Wgt Acc: 26.679%
	I - Batch: 1500 | Loss: 1.379 | Acc: 29.533% | Wgt Acc: 26.619%
	I - Batch: 1550 | Loss: 1.379 | Acc: 29.613% | Wgt Acc: 26.694%
	I - Batch: 1600 | Loss: 1.379 | Acc: 29.438% | Wgt Acc: 26.543%
	I - Batch: 1650 | Loss: 1.379 | Acc: 29.576% | Wgt Acc: 26.681%
	I - Batch: 1700 | Loss: 1.378 | Acc: 29.647% | Wgt Acc: 26.752%
	I - Batch: 1750 | Loss: 1.379 | Acc: 29.429% | Wgt Acc: 26.540%
	I - Batch: 1800 | Loss: 1.379 | Acc: 29.444% | Wgt Acc: 26.573%
	I - Batch: 1850 | Loss: 1.379 | Acc: 29.514% | Wgt Acc: 26.628%
	I - Batch: 1900 | Loss: 1.379 | Acc: 29.579% | Wgt Acc: 26.682%
	I - Batch: 1950 | Loss: 1.378 | Acc: 29.641% | Wgt Acc: 26.744%
	I - Batch: 2000 | Loss: 1.379 | Acc: 29.700% | Wgt Acc: 26.790%
	I - Batch: 2050 | Loss: 1.379 | Acc: 29.610% | Wgt Acc: 26.705%
	I - Batch: 2100 | Loss: 1.379 | Acc: 29.571% | Wgt Acc: 26.687%
	I - Batch: 2150 | Loss: 1.380 | Acc: 29.302% | Wgt Acc: 26.418%
	I - Batch: 2200 | Loss: 1.380 | Acc: 29.000% | Wgt Acc: 26.134%
	I - Batch: 2250 | Loss: 1.380 | Acc: 29.022% | Wgt Acc: 26.154%
	I - Batch: 2300 | Loss: 1.380 | Acc: 29.043% | Wgt Acc: 26.173%
	I - Batch: 2350 | Loss: 1.380 | Acc: 29.234% | Wgt Acc: 26.345%
	I - Batch: 2400 | Loss: 1.381 | Acc: 29.125% | Wgt Acc: 26.236%
	I - Batch: 2450 | Loss: 1.381 | Acc: 29.184% | Wgt Acc: 26.289%
	I - Batch: 2500 | Loss: 1.381 | Acc: 29.120% | Wgt Acc: 26.239%
I - num batch: 2547
I - Train -- Loss: 1.380 | Acc: 29.407% | Wgt Acc: 26.504% | LR: 5.000000e-04 | Dur: 1408.20s
I - Confusion Matrix: [row->prediction - col->label]
[[205. 143. 190. 161.]
 [  0.   0.   0.   0.]
 [492. 435. 544. 377.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.382 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.397 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.395 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.390 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.393 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.390 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.393 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 120.83s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 1.399 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.411 | Acc: 20.000% | Wgt Acc: 17.621%
	I - Batch: 150 | Loss: 1.402 | Acc: 20.667% | Wgt Acc: 18.370%
	I - Batch: 200 | Loss: 1.396 | Acc: 22.000% | Wgt Acc: 19.643%
	I - Batch: 250 | Loss: 1.393 | Acc: 23.600% | Wgt Acc: 21.147%
	I - Batch: 300 | Loss: 1.386 | Acc: 24.333% | Wgt Acc: 21.922%
	I - Batch: 350 | Loss: 1.387 | Acc: 24.571% | Wgt Acc: 22.094%
	I - Batch: 400 | Loss: 1.386 | Acc: 24.500% | Wgt Acc: 22.035%
	I - Batch: 450 | Loss: 1.389 | Acc: 24.889% | Wgt Acc: 22.355%
	I - Batch: 500 | Loss: 1.385 | Acc: 24.800% | Wgt Acc: 22.332%
	I - Batch: 550 | Loss: 1.381 | Acc: 26.182% | Wgt Acc: 23.645%
	I - Batch: 600 | Loss: 1.381 | Acc: 26.500% | Wgt Acc: 23.910%
	I - Batch: 650 | Loss: 1.383 | Acc: 26.154% | Wgt Acc: 23.578%
	I - Batch: 700 | Loss: 1.383 | Acc: 26.286% | Wgt Acc: 23.704%
	I - Batch: 750 | Loss: 1.381 | Acc: 26.933% | Wgt Acc: 24.315%
	I - Batch: 800 | Loss: 1.382 | Acc: 26.125% | Wgt Acc: 23.576%
	I - Batch: 850 | Loss: 1.382 | Acc: 25.882% | Wgt Acc: 23.361%
	I - Batch: 900 | Loss: 1.382 | Acc: 25.889% | Wgt Acc: 23.353%
	I - Batch: 950 | Loss: 1.381 | Acc: 26.211% | Wgt Acc: 23.658%
	I - Batch: 1000 | Loss: 1.382 | Acc: 26.300% | Wgt Acc: 23.720%
	I - Batch: 1050 | Loss: 1.382 | Acc: 26.667% | Wgt Acc: 24.060%
	I - Batch: 1100 | Loss: 1.382 | Acc: 26.455% | Wgt Acc: 23.852%
	I - Batch: 1150 | Loss: 1.382 | Acc: 26.435% | Wgt Acc: 23.829%
	I - Batch: 1200 | Loss: 1.382 | Acc: 26.583% | Wgt Acc: 23.958%
	I - Batch: 1250 | Loss: 1.381 | Acc: 27.120% | Wgt Acc: 24.459%
	I - Batch: 1300 | Loss: 1.380 | Acc: 27.308% | Wgt Acc: 24.627%
	I - Batch: 1350 | Loss: 1.380 | Acc: 27.556% | Wgt Acc: 24.858%
	I - Batch: 1400 | Loss: 1.380 | Acc: 27.500% | Wgt Acc: 24.807%
	I - Batch: 1450 | Loss: 1.382 | Acc: 27.172% | Wgt Acc: 24.491%
	I - Batch: 1500 | Loss: 1.382 | Acc: 26.933% | Wgt Acc: 24.275%
	I - Batch: 1550 | Loss: 1.382 | Acc: 27.161% | Wgt Acc: 24.480%
	I - Batch: 1600 | Loss: 1.381 | Acc: 27.562% | Wgt Acc: 24.873%
	I - Batch: 1650 | Loss: 1.381 | Acc: 27.455% | Wgt Acc: 24.778%
	I - Batch: 1700 | Loss: 1.381 | Acc: 27.412% | Wgt Acc: 24.735%
	I - Batch: 1750 | Loss: 1.380 | Acc: 27.657% | Wgt Acc: 24.968%
	I - Batch: 1800 | Loss: 1.380 | Acc: 27.611% | Wgt Acc: 24.928%
	I - Batch: 1850 | Loss: 1.380 | Acc: 27.676% | Wgt Acc: 24.994%
	I - Batch: 1900 | Loss: 1.380 | Acc: 27.579% | Wgt Acc: 24.902%
	I - Batch: 1950 | Loss: 1.381 | Acc: 27.538% | Wgt Acc: 24.847%
	I - Batch: 2000 | Loss: 1.381 | Acc: 27.550% | Wgt Acc: 24.856%
	I - Batch: 2050 | Loss: 1.381 | Acc: 27.512% | Wgt Acc: 24.821%
	I - Batch: 2100 | Loss: 1.381 | Acc: 27.476% | Wgt Acc: 24.791%
	I - Batch: 2150 | Loss: 1.381 | Acc: 27.395% | Wgt Acc: 24.701%
	I - Batch: 2200 | Loss: 1.381 | Acc: 27.545% | Wgt Acc: 24.849%
	I - Batch: 2250 | Loss: 1.381 | Acc: 27.644% | Wgt Acc: 24.932%
	I - Batch: 2300 | Loss: 1.380 | Acc: 27.739% | Wgt Acc: 25.037%
	I - Batch: 2350 | Loss: 1.381 | Acc: 27.830% | Wgt Acc: 25.108%
	I - Batch: 2400 | Loss: 1.380 | Acc: 27.833% | Wgt Acc: 25.115%
	I - Batch: 2450 | Loss: 1.381 | Acc: 27.714% | Wgt Acc: 25.002%
	I - Batch: 2500 | Loss: 1.381 | Acc: 27.680% | Wgt Acc: 24.968%
I - num batch: 2547
I - Train -- Loss: 1.381 | Acc: 27.601% | Wgt Acc: 24.876% | LR: 5.000000e-04 | Dur: 1393.33s
I - Confusion Matrix: [row->prediction - col->label]
[[202. 169. 233. 150.]
 [  0.   0.   0.   0.]
 [495. 409. 501. 388.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.389 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.388 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.389 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.389 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.391 | Acc: 21.333% | Wgt Acc: 18.949%
I - num batch: 327
I - Val -- Loss: 1.388 | Acc: 23.242% | Wgt Acc: 20.652% | Dur: 121.72s
I - Confusion Matrix: [row->prediction - col->label]
[[ 1.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [87. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 1.389 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.391 | Acc: 24.000% | Wgt Acc: 21.381%
	I - Batch: 150 | Loss: 1.388 | Acc: 26.667% | Wgt Acc: 23.810%
	I - Batch: 200 | Loss: 1.384 | Acc: 29.000% | Wgt Acc: 25.980%
	I - Batch: 250 | Loss: 1.387 | Acc: 28.400% | Wgt Acc: 25.357%
	I - Batch: 300 | Loss: 1.387 | Acc: 28.000% | Wgt Acc: 25.000%
	I - Batch: 350 | Loss: 1.385 | Acc: 28.286% | Wgt Acc: 25.304%
	I - Batch: 400 | Loss: 1.379 | Acc: 30.500% | Wgt Acc: 27.324%
	I - Batch: 450 | Loss: 1.375 | Acc: 31.556% | Wgt Acc: 28.357%
	I - Batch: 500 | Loss: 1.382 | Acc: 30.200% | Wgt Acc: 27.037%
	I - Batch: 550 | Loss: 1.382 | Acc: 30.364% | Wgt Acc: 27.199%
	I - Batch: 600 | Loss: 1.383 | Acc: 29.667% | Wgt Acc: 26.597%
	I - Batch: 650 | Loss: 1.383 | Acc: 29.538% | Wgt Acc: 26.501%
	I - Batch: 700 | Loss: 1.383 | Acc: 29.571% | Wgt Acc: 26.530%
	I - Batch: 750 | Loss: 1.380 | Acc: 30.133% | Wgt Acc: 27.082%
	I - Batch: 800 | Loss: 1.383 | Acc: 29.625% | Wgt Acc: 26.599%
	I - Batch: 850 | Loss: 1.383 | Acc: 29.647% | Wgt Acc: 26.624%
	I - Batch: 900 | Loss: 1.381 | Acc: 29.889% | Wgt Acc: 26.873%
	I - Batch: 950 | Loss: 1.382 | Acc: 29.684% | Wgt Acc: 26.692%
	I - Batch: 1000 | Loss: 1.382 | Acc: 29.600% | Wgt Acc: 26.619%
	I - Batch: 1050 | Loss: 1.381 | Acc: 29.429% | Wgt Acc: 26.484%
	I - Batch: 1100 | Loss: 1.381 | Acc: 29.091% | Wgt Acc: 26.181%
	I - Batch: 1150 | Loss: 1.382 | Acc: 28.696% | Wgt Acc: 25.817%
	I - Batch: 1200 | Loss: 1.383 | Acc: 28.750% | Wgt Acc: 25.848%
	I - Batch: 1250 | Loss: 1.382 | Acc: 28.960% | Wgt Acc: 26.048%
	I - Batch: 1300 | Loss: 1.383 | Acc: 28.692% | Wgt Acc: 25.777%
	I - Batch: 1350 | Loss: 1.383 | Acc: 28.519% | Wgt Acc: 25.603%
	I - Batch: 1400 | Loss: 1.383 | Acc: 28.643% | Wgt Acc: 25.722%
	I - Batch: 1450 | Loss: 1.382 | Acc: 28.897% | Wgt Acc: 25.984%
	I - Batch: 1500 | Loss: 1.382 | Acc: 28.933% | Wgt Acc: 26.011%
	I - Batch: 1550 | Loss: 1.383 | Acc: 28.839% | Wgt Acc: 25.924%
	I - Batch: 1600 | Loss: 1.382 | Acc: 29.250% | Wgt Acc: 26.314%
	I - Batch: 1650 | Loss: 1.382 | Acc: 29.273% | Wgt Acc: 26.332%
	I - Batch: 1700 | Loss: 1.382 | Acc: 29.353% | Wgt Acc: 26.409%
	I - Batch: 1750 | Loss: 1.381 | Acc: 29.429% | Wgt Acc: 26.488%
	I - Batch: 1800 | Loss: 1.382 | Acc: 29.056% | Wgt Acc: 26.147%
	I - Batch: 1850 | Loss: 1.383 | Acc: 28.865% | Wgt Acc: 25.960%
	I - Batch: 1900 | Loss: 1.382 | Acc: 29.158% | Wgt Acc: 26.237%
	I - Batch: 1950 | Loss: 1.382 | Acc: 29.487% | Wgt Acc: 26.547%
	I - Batch: 2000 | Loss: 1.382 | Acc: 29.500% | Wgt Acc: 26.541%
	I - Batch: 2050 | Loss: 1.382 | Acc: 29.561% | Wgt Acc: 26.605%
	I - Batch: 2100 | Loss: 1.382 | Acc: 29.476% | Wgt Acc: 26.530%
	I - Batch: 2150 | Loss: 1.382 | Acc: 29.535% | Wgt Acc: 26.589%
	I - Batch: 2200 | Loss: 1.382 | Acc: 29.500% | Wgt Acc: 26.558%
	I - Batch: 2250 | Loss: 1.382 | Acc: 29.467% | Wgt Acc: 26.539%
	I - Batch: 2300 | Loss: 1.381 | Acc: 29.739% | Wgt Acc: 26.795%
	I - Batch: 2350 | Loss: 1.380 | Acc: 29.957% | Wgt Acc: 27.004%
	I - Batch: 2400 | Loss: 1.381 | Acc: 29.708% | Wgt Acc: 26.769%
	I - Batch: 2450 | Loss: 1.382 | Acc: 29.551% | Wgt Acc: 26.618%
	I - Batch: 2500 | Loss: 1.381 | Acc: 29.640% | Wgt Acc: 26.705%
I - num batch: 2547
I - Train -- Loss: 1.381 | Acc: 29.564% | Wgt Acc: 26.645% | LR: 5.000000e-04 | Dur: 1396.02s
I - Confusion Matrix: [row->prediction - col->label]
[[203. 145. 184. 148.]
 [  1.   0.   0.   0.]
 [493. 433. 550. 390.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.396 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.394 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.396 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.399 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.399 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.398 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 1.354 | Acc: 28.000% | Wgt Acc: 25.688%
	I - Batch: 100 | Loss: 1.355 | Acc: 26.000% | Wgt Acc: 23.799%
	I - Batch: 150 | Loss: 1.360 | Acc: 25.333% | Wgt Acc: 23.100%
	I - Batch: 200 | Loss: 1.367 | Acc: 26.500% | Wgt Acc: 24.036%
	I - Batch: 250 | Loss: 1.370 | Acc: 26.000% | Wgt Acc: 23.529%
	I - Batch: 300 | Loss: 1.376 | Acc: 25.333% | Wgt Acc: 22.840%
	I - Batch: 350 | Loss: 1.377 | Acc: 27.143% | Wgt Acc: 24.500%
	I - Batch: 400 | Loss: 1.370 | Acc: 29.000% | Wgt Acc: 26.289%
	I - Batch: 450 | Loss: 1.374 | Acc: 28.667% | Wgt Acc: 25.943%
	I - Batch: 500 | Loss: 1.372 | Acc: 27.800% | Wgt Acc: 25.193%
	I - Batch: 550 | Loss: 1.374 | Acc: 27.273% | Wgt Acc: 24.691%
	I - Batch: 600 | Loss: 1.377 | Acc: 27.833% | Wgt Acc: 25.132%
	I - Batch: 650 | Loss: 1.377 | Acc: 28.154% | Wgt Acc: 25.434%
	I - Batch: 700 | Loss: 1.376 | Acc: 28.286% | Wgt Acc: 25.590%
	I - Batch: 750 | Loss: 1.376 | Acc: 28.533% | Wgt Acc: 25.799%
	I - Batch: 800 | Loss: 1.377 | Acc: 28.625% | Wgt Acc: 25.861%
	I - Batch: 850 | Loss: 1.378 | Acc: 28.471% | Wgt Acc: 25.710%
	I - Batch: 900 | Loss: 1.378 | Acc: 28.556% | Wgt Acc: 25.803%
	I - Batch: 950 | Loss: 1.378 | Acc: 28.316% | Wgt Acc: 25.595%
	I - Batch: 1000 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.288%
	I - Batch: 1050 | Loss: 1.379 | Acc: 27.714% | Wgt Acc: 25.016%
	I - Batch: 1100 | Loss: 1.380 | Acc: 27.818% | Wgt Acc: 25.092%
	I - Batch: 1150 | Loss: 1.378 | Acc: 28.435% | Wgt Acc: 25.687%
	I - Batch: 1200 | Loss: 1.379 | Acc: 28.083% | Wgt Acc: 25.367%
	I - Batch: 1250 | Loss: 1.381 | Acc: 27.680% | Wgt Acc: 24.964%
	I - Batch: 1300 | Loss: 1.380 | Acc: 28.154% | Wgt Acc: 25.412%
	I - Batch: 1350 | Loss: 1.380 | Acc: 28.000% | Wgt Acc: 25.272%
	I - Batch: 1400 | Loss: 1.381 | Acc: 27.929% | Wgt Acc: 25.185%
	I - Batch: 1450 | Loss: 1.381 | Acc: 27.931% | Wgt Acc: 25.175%
	I - Batch: 1500 | Loss: 1.382 | Acc: 27.733% | Wgt Acc: 24.989%
	I - Batch: 1550 | Loss: 1.382 | Acc: 27.871% | Wgt Acc: 25.105%
	I - Batch: 1600 | Loss: 1.382 | Acc: 28.000% | Wgt Acc: 25.222%
	I - Batch: 1650 | Loss: 1.382 | Acc: 27.758% | Wgt Acc: 25.017%
	I - Batch: 1700 | Loss: 1.382 | Acc: 27.647% | Wgt Acc: 24.911%
	I - Batch: 1750 | Loss: 1.382 | Acc: 27.600% | Wgt Acc: 24.868%
	I - Batch: 1800 | Loss: 1.382 | Acc: 27.222% | Wgt Acc: 24.515%
	I - Batch: 1850 | Loss: 1.382 | Acc: 27.135% | Wgt Acc: 24.443%
	I - Batch: 1900 | Loss: 1.381 | Acc: 27.421% | Wgt Acc: 24.712%
	I - Batch: 1950 | Loss: 1.381 | Acc: 27.385% | Wgt Acc: 24.682%
	I - Batch: 2000 | Loss: 1.382 | Acc: 27.350% | Wgt Acc: 24.642%
	I - Batch: 2050 | Loss: 1.382 | Acc: 27.317% | Wgt Acc: 24.613%
	I - Batch: 2100 | Loss: 1.382 | Acc: 27.476% | Wgt Acc: 24.759%
	I - Batch: 2150 | Loss: 1.382 | Acc: 27.628% | Wgt Acc: 24.900%
	I - Batch: 2200 | Loss: 1.381 | Acc: 27.727% | Wgt Acc: 25.000%
	I - Batch: 2250 | Loss: 1.381 | Acc: 27.733% | Wgt Acc: 25.005%
	I - Batch: 2300 | Loss: 1.381 | Acc: 27.696% | Wgt Acc: 24.975%
	I - Batch: 2350 | Loss: 1.382 | Acc: 27.787% | Wgt Acc: 25.048%
	I - Batch: 2400 | Loss: 1.382 | Acc: 27.917% | Wgt Acc: 25.153%
	I - Batch: 2450 | Loss: 1.382 | Acc: 27.959% | Wgt Acc: 25.188%
	I - Batch: 2500 | Loss: 1.382 | Acc: 27.880% | Wgt Acc: 25.126%
I - num batch: 2547
I - Train -- Loss: 1.382 | Acc: 27.876% | Wgt Acc: 25.124% | LR: 5.000000e-04 | Dur: 1393.77s
I - Confusion Matrix: [row->prediction - col->label]
[[283. 225. 307. 214.]
 [  0.   0.   0.   0.]
 [414. 353. 427. 324.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.387 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.396 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.394 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 1.372 | Acc: 34.000% | Wgt Acc: 30.769%
	I - Batch: 100 | Loss: 1.364 | Acc: 35.000% | Wgt Acc: 31.891%
	I - Batch: 150 | Loss: 1.362 | Acc: 34.000% | Wgt Acc: 31.050%
	I - Batch: 200 | Loss: 1.373 | Acc: 31.000% | Wgt Acc: 28.150%
	I - Batch: 250 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 27.174%
	I - Batch: 300 | Loss: 1.373 | Acc: 30.000% | Wgt Acc: 27.231%
	I - Batch: 350 | Loss: 1.375 | Acc: 28.571% | Wgt Acc: 25.907%
	I - Batch: 400 | Loss: 1.377 | Acc: 27.750% | Wgt Acc: 25.113%
	I - Batch: 450 | Loss: 1.380 | Acc: 27.556% | Wgt Acc: 24.887%
	I - Batch: 500 | Loss: 1.379 | Acc: 27.800% | Wgt Acc: 25.136%
	I - Batch: 550 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.256%
	I - Batch: 600 | Loss: 1.382 | Acc: 27.333% | Wgt Acc: 24.643%
	I - Batch: 650 | Loss: 1.382 | Acc: 26.769% | Wgt Acc: 24.125%
	I - Batch: 700 | Loss: 1.380 | Acc: 27.571% | Wgt Acc: 24.879%
	I - Batch: 750 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.263%
	I - Batch: 800 | Loss: 1.380 | Acc: 28.000% | Wgt Acc: 25.247%
	I - Batch: 850 | Loss: 1.380 | Acc: 27.882% | Wgt Acc: 25.146%
	I - Batch: 900 | Loss: 1.380 | Acc: 27.889% | Wgt Acc: 25.138%
	I - Batch: 950 | Loss: 1.378 | Acc: 27.789% | Wgt Acc: 25.059%
	I - Batch: 1000 | Loss: 1.380 | Acc: 27.900% | Wgt Acc: 25.158%
	I - Batch: 1050 | Loss: 1.380 | Acc: 27.810% | Wgt Acc: 25.081%
	I - Batch: 1100 | Loss: 1.380 | Acc: 28.091% | Wgt Acc: 25.307%
	I - Batch: 1150 | Loss: 1.380 | Acc: 28.435% | Wgt Acc: 25.632%
	I - Batch: 1200 | Loss: 1.381 | Acc: 27.917% | Wgt Acc: 25.164%
	I - Batch: 1250 | Loss: 1.382 | Acc: 27.440% | Wgt Acc: 24.703%
	I - Batch: 1300 | Loss: 1.382 | Acc: 27.538% | Wgt Acc: 24.801%
	I - Batch: 1350 | Loss: 1.382 | Acc: 27.852% | Wgt Acc: 25.079%
	I - Batch: 1400 | Loss: 1.382 | Acc: 27.929% | Wgt Acc: 25.145%
	I - Batch: 1450 | Loss: 1.382 | Acc: 27.793% | Wgt Acc: 25.016%
	I - Batch: 1500 | Loss: 1.383 | Acc: 27.800% | Wgt Acc: 25.019%
	I - Batch: 1550 | Loss: 1.382 | Acc: 27.935% | Wgt Acc: 25.149%
	I - Batch: 1600 | Loss: 1.383 | Acc: 27.938% | Wgt Acc: 25.141%
	I - Batch: 1650 | Loss: 1.382 | Acc: 28.000% | Wgt Acc: 25.205%
	I - Batch: 1700 | Loss: 1.382 | Acc: 28.176% | Wgt Acc: 25.357%
	I - Batch: 1750 | Loss: 1.382 | Acc: 28.171% | Wgt Acc: 25.354%
	I - Batch: 1800 | Loss: 1.381 | Acc: 28.389% | Wgt Acc: 25.576%
	I - Batch: 1850 | Loss: 1.382 | Acc: 28.324% | Wgt Acc: 25.514%
	I - Batch: 1900 | Loss: 1.382 | Acc: 28.158% | Wgt Acc: 25.361%
	I - Batch: 1950 | Loss: 1.382 | Acc: 28.205% | Wgt Acc: 25.410%
	I - Batch: 2000 | Loss: 1.382 | Acc: 28.100% | Wgt Acc: 25.312%
	I - Batch: 2050 | Loss: 1.381 | Acc: 28.049% | Wgt Acc: 25.275%
	I - Batch: 2100 | Loss: 1.380 | Acc: 28.048% | Wgt Acc: 25.293%
	I - Batch: 2150 | Loss: 1.381 | Acc: 27.721% | Wgt Acc: 24.987%
	I - Batch: 2200 | Loss: 1.381 | Acc: 27.682% | Wgt Acc: 24.956%
	I - Batch: 2250 | Loss: 1.381 | Acc: 27.644% | Wgt Acc: 24.925%
	I - Batch: 2300 | Loss: 1.381 | Acc: 27.652% | Wgt Acc: 24.931%
	I - Batch: 2350 | Loss: 1.380 | Acc: 27.745% | Wgt Acc: 25.024%
	I - Batch: 2400 | Loss: 1.380 | Acc: 27.458% | Wgt Acc: 24.763%
	I - Batch: 2450 | Loss: 1.381 | Acc: 27.388% | Wgt Acc: 24.687%
	I - Batch: 2500 | Loss: 1.381 | Acc: 27.400% | Wgt Acc: 24.702%
I - num batch: 2547
I - Train -- Loss: 1.381 | Acc: 27.405% | Wgt Acc: 24.699% | LR: 5.000000e-04 | Dur: 984.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.  68. 107.  70.]
 [  0.   0.   0.   0.]
 [626. 510. 627. 468.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.396 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 65.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 1.403 | Acc: 12.000% | Wgt Acc: 10.573%
	I - Batch: 100 | Loss: 1.388 | Acc: 19.000% | Wgt Acc: 17.040%
	I - Batch: 150 | Loss: 1.387 | Acc: 22.667% | Wgt Acc: 20.359%
	I - Batch: 200 | Loss: 1.387 | Acc: 25.000% | Wgt Acc: 22.396%
	I - Batch: 250 | Loss: 1.384 | Acc: 25.600% | Wgt Acc: 23.022%
	I - Batch: 300 | Loss: 1.382 | Acc: 25.667% | Wgt Acc: 23.123%
	I - Batch: 350 | Loss: 1.378 | Acc: 25.143% | Wgt Acc: 22.710%
	I - Batch: 400 | Loss: 1.379 | Acc: 25.750% | Wgt Acc: 23.251%
	I - Batch: 450 | Loss: 1.379 | Acc: 26.222% | Wgt Acc: 23.659%
	I - Batch: 500 | Loss: 1.380 | Acc: 26.200% | Wgt Acc: 23.614%
	I - Batch: 550 | Loss: 1.379 | Acc: 26.545% | Wgt Acc: 23.944%
	I - Batch: 600 | Loss: 1.381 | Acc: 26.167% | Wgt Acc: 23.574%
	I - Batch: 650 | Loss: 1.383 | Acc: 26.308% | Wgt Acc: 23.676%
	I - Batch: 700 | Loss: 1.383 | Acc: 26.429% | Wgt Acc: 23.794%
	I - Batch: 750 | Loss: 1.384 | Acc: 26.533% | Wgt Acc: 23.868%
	I - Batch: 800 | Loss: 1.384 | Acc: 26.500% | Wgt Acc: 23.827%
	I - Batch: 850 | Loss: 1.384 | Acc: 26.588% | Wgt Acc: 23.909%
	I - Batch: 900 | Loss: 1.385 | Acc: 26.556% | Wgt Acc: 23.870%
	I - Batch: 950 | Loss: 1.384 | Acc: 27.053% | Wgt Acc: 24.320%
	I - Batch: 1000 | Loss: 1.385 | Acc: 27.200% | Wgt Acc: 24.438%
	I - Batch: 1050 | Loss: 1.385 | Acc: 27.143% | Wgt Acc: 24.380%
	I - Batch: 1100 | Loss: 1.385 | Acc: 27.273% | Wgt Acc: 24.495%
	I - Batch: 1150 | Loss: 1.385 | Acc: 27.391% | Wgt Acc: 24.619%
	I - Batch: 1200 | Loss: 1.384 | Acc: 27.667% | Wgt Acc: 24.888%
	I - Batch: 1250 | Loss: 1.384 | Acc: 27.520% | Wgt Acc: 24.753%
	I - Batch: 1300 | Loss: 1.385 | Acc: 27.308% | Wgt Acc: 24.542%
	I - Batch: 1350 | Loss: 1.385 | Acc: 27.185% | Wgt Acc: 24.418%
	I - Batch: 1400 | Loss: 1.385 | Acc: 27.143% | Wgt Acc: 24.386%
	I - Batch: 1450 | Loss: 1.386 | Acc: 27.172% | Wgt Acc: 24.404%
	I - Batch: 1500 | Loss: 1.386 | Acc: 27.067% | Wgt Acc: 24.311%
	I - Batch: 1550 | Loss: 1.385 | Acc: 27.097% | Wgt Acc: 24.358%
	I - Batch: 1600 | Loss: 1.386 | Acc: 27.188% | Wgt Acc: 24.421%
	I - Batch: 1650 | Loss: 1.386 | Acc: 26.788% | Wgt Acc: 24.058%
	I - Batch: 1700 | Loss: 1.386 | Acc: 26.882% | Wgt Acc: 24.151%
	I - Batch: 1750 | Loss: 1.385 | Acc: 26.857% | Wgt Acc: 24.146%
	I - Batch: 1800 | Loss: 1.385 | Acc: 26.944% | Wgt Acc: 24.226%
	I - Batch: 1850 | Loss: 1.385 | Acc: 26.973% | Wgt Acc: 24.259%
	I - Batch: 1900 | Loss: 1.385 | Acc: 26.947% | Wgt Acc: 24.231%
	I - Batch: 1950 | Loss: 1.385 | Acc: 26.974% | Wgt Acc: 24.251%
	I - Batch: 2000 | Loss: 1.384 | Acc: 27.250% | Wgt Acc: 24.508%
	I - Batch: 2050 | Loss: 1.384 | Acc: 27.366% | Wgt Acc: 24.616%
	I - Batch: 2100 | Loss: 1.384 | Acc: 27.429% | Wgt Acc: 24.676%
	I - Batch: 2150 | Loss: 1.385 | Acc: 27.488% | Wgt Acc: 24.718%
	I - Batch: 2200 | Loss: 1.384 | Acc: 27.682% | Wgt Acc: 24.916%
	I - Batch: 2250 | Loss: 1.383 | Acc: 27.733% | Wgt Acc: 24.972%
	I - Batch: 2300 | Loss: 1.383 | Acc: 27.783% | Wgt Acc: 25.022%
	I - Batch: 2350 | Loss: 1.382 | Acc: 27.915% | Wgt Acc: 25.153%
	I - Batch: 2400 | Loss: 1.383 | Acc: 27.667% | Wgt Acc: 24.923%
	I - Batch: 2450 | Loss: 1.382 | Acc: 27.755% | Wgt Acc: 25.007%
	I - Batch: 2500 | Loss: 1.382 | Acc: 27.720% | Wgt Acc: 24.975%
I - num batch: 2547
I - Train -- Loss: 1.382 | Acc: 27.758% | Wgt Acc: 25.018% | LR: 5.000000e-04 | Dur: 887.89s
I - Confusion Matrix: [row->prediction - col->label]
[[206. 173. 233. 162.]
 [  0.   0.   0.   0.]
 [491. 405. 501. 376.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.390 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.405 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.403 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.407 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.411 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.411 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.410 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 66.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 1.389 | Acc: 18.000% | Wgt Acc: 16.216%
	I - Batch: 100 | Loss: 1.387 | Acc: 19.000% | Wgt Acc: 17.079%
	I - Batch: 150 | Loss: 1.386 | Acc: 21.333% | Wgt Acc: 19.248%
	I - Batch: 200 | Loss: 1.391 | Acc: 22.000% | Wgt Acc: 20.201%
	I - Batch: 250 | Loss: 1.393 | Acc: 20.400% | Wgt Acc: 19.355%
	I - Batch: 300 | Loss: 1.392 | Acc: 21.000% | Wgt Acc: 19.970%
	I - Batch: 350 | Loss: 1.393 | Acc: 21.143% | Wgt Acc: 19.885%
	I - Batch: 400 | Loss: 1.389 | Acc: 23.250% | Wgt Acc: 21.754%
	I - Batch: 450 | Loss: 1.388 | Acc: 23.556% | Wgt Acc: 21.928%
	I - Batch: 500 | Loss: 1.386 | Acc: 23.800% | Wgt Acc: 22.087%
	I - Batch: 550 | Loss: 1.385 | Acc: 24.364% | Wgt Acc: 22.582%
	I - Batch: 600 | Loss: 1.384 | Acc: 24.500% | Wgt Acc: 22.661%
	I - Batch: 650 | Loss: 1.383 | Acc: 25.385% | Wgt Acc: 23.462%
	I - Batch: 700 | Loss: 1.380 | Acc: 26.286% | Wgt Acc: 24.281%
	I - Batch: 750 | Loss: 1.379 | Acc: 26.800% | Wgt Acc: 24.706%
	I - Batch: 800 | Loss: 1.380 | Acc: 26.875% | Wgt Acc: 24.718%
	I - Batch: 850 | Loss: 1.380 | Acc: 26.235% | Wgt Acc: 24.122%
	I - Batch: 900 | Loss: 1.381 | Acc: 26.444% | Wgt Acc: 24.278%
	I - Batch: 950 | Loss: 1.382 | Acc: 26.316% | Wgt Acc: 24.121%
	I - Batch: 1000 | Loss: 1.381 | Acc: 26.300% | Wgt Acc: 24.097%
	I - Batch: 1050 | Loss: 1.381 | Acc: 26.571% | Wgt Acc: 24.323%
	I - Batch: 1100 | Loss: 1.382 | Acc: 26.455% | Wgt Acc: 24.195%
	I - Batch: 1150 | Loss: 1.382 | Acc: 26.783% | Wgt Acc: 24.480%
	I - Batch: 1200 | Loss: 1.381 | Acc: 27.000% | Wgt Acc: 24.680%
	I - Batch: 1250 | Loss: 1.380 | Acc: 26.800% | Wgt Acc: 24.498%
	I - Batch: 1300 | Loss: 1.381 | Acc: 26.923% | Wgt Acc: 24.579%
	I - Batch: 1350 | Loss: 1.382 | Acc: 26.741% | Wgt Acc: 24.390%
	I - Batch: 1400 | Loss: 1.381 | Acc: 26.786% | Wgt Acc: 24.428%
	I - Batch: 1450 | Loss: 1.381 | Acc: 26.690% | Wgt Acc: 24.334%
	I - Batch: 1500 | Loss: 1.383 | Acc: 26.400% | Wgt Acc: 24.031%
	I - Batch: 1550 | Loss: 1.383 | Acc: 26.323% | Wgt Acc: 23.953%
	I - Batch: 1600 | Loss: 1.381 | Acc: 26.438% | Wgt Acc: 24.080%
	I - Batch: 1650 | Loss: 1.381 | Acc: 26.667% | Wgt Acc: 24.278%
	I - Batch: 1700 | Loss: 1.382 | Acc: 26.529% | Wgt Acc: 24.128%
	I - Batch: 1750 | Loss: 1.382 | Acc: 26.743% | Wgt Acc: 24.323%
	I - Batch: 1800 | Loss: 1.383 | Acc: 26.556% | Wgt Acc: 24.136%
	I - Batch: 1850 | Loss: 1.383 | Acc: 26.595% | Wgt Acc: 24.156%
	I - Batch: 1900 | Loss: 1.383 | Acc: 26.684% | Wgt Acc: 24.241%
	I - Batch: 1950 | Loss: 1.383 | Acc: 26.564% | Wgt Acc: 24.116%
	I - Batch: 2000 | Loss: 1.383 | Acc: 26.700% | Wgt Acc: 24.226%
	I - Batch: 2050 | Loss: 1.383 | Acc: 26.780% | Wgt Acc: 24.297%
	I - Batch: 2100 | Loss: 1.382 | Acc: 26.952% | Wgt Acc: 24.455%
	I - Batch: 2150 | Loss: 1.383 | Acc: 26.930% | Wgt Acc: 24.424%
	I - Batch: 2200 | Loss: 1.383 | Acc: 26.909% | Wgt Acc: 24.406%
	I - Batch: 2250 | Loss: 1.382 | Acc: 27.156% | Wgt Acc: 24.634%
	I - Batch: 2300 | Loss: 1.382 | Acc: 27.087% | Wgt Acc: 24.566%
	I - Batch: 2350 | Loss: 1.382 | Acc: 27.191% | Wgt Acc: 24.660%
	I - Batch: 2400 | Loss: 1.382 | Acc: 27.333% | Wgt Acc: 24.782%
	I - Batch: 2450 | Loss: 1.383 | Acc: 27.265% | Wgt Acc: 24.708%
	I - Batch: 2500 | Loss: 1.383 | Acc: 27.200% | Wgt Acc: 24.646%
I - num batch: 2547
I - Train -- Loss: 1.382 | Acc: 27.405% | Wgt Acc: 24.832% | LR: 5.000000e-04 | Dur: 892.01s
I - Confusion Matrix: [row->prediction - col->label]
[[327. 252. 354. 234.]
 [ 22.  15.  24.  18.]
 [348. 311. 356. 286.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.393 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.397 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.399 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.400 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.398 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 65.72s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 1.355 | Acc: 34.000% | Wgt Acc: 31.050%
	I - Batch: 100 | Loss: 1.376 | Acc: 33.000% | Wgt Acc: 29.797%
	I - Batch: 150 | Loss: 1.379 | Acc: 32.000% | Wgt Acc: 28.829%
	I - Batch: 200 | Loss: 1.378 | Acc: 31.000% | Wgt Acc: 27.991%
	I - Batch: 250 | Loss: 1.381 | Acc: 30.800% | Wgt Acc: 27.698%
	I - Batch: 300 | Loss: 1.381 | Acc: 31.333% | Wgt Acc: 28.123%
	I - Batch: 350 | Loss: 1.384 | Acc: 30.000% | Wgt Acc: 26.854%
	I - Batch: 400 | Loss: 1.381 | Acc: 31.250% | Wgt Acc: 28.011%
	I - Batch: 450 | Loss: 1.380 | Acc: 30.889% | Wgt Acc: 27.689%
	I - Batch: 500 | Loss: 1.378 | Acc: 31.000% | Wgt Acc: 27.803%
	I - Batch: 550 | Loss: 1.380 | Acc: 30.364% | Wgt Acc: 27.276%
	I - Batch: 600 | Loss: 1.380 | Acc: 30.500% | Wgt Acc: 27.405%
	I - Batch: 650 | Loss: 1.383 | Acc: 29.692% | Wgt Acc: 26.612%
	I - Batch: 700 | Loss: 1.385 | Acc: 28.857% | Wgt Acc: 25.806%
	I - Batch: 750 | Loss: 1.385 | Acc: 29.067% | Wgt Acc: 25.968%
	I - Batch: 800 | Loss: 1.384 | Acc: 29.250% | Wgt Acc: 26.138%
	I - Batch: 850 | Loss: 1.384 | Acc: 29.059% | Wgt Acc: 26.000%
	I - Batch: 900 | Loss: 1.384 | Acc: 29.000% | Wgt Acc: 25.957%
	I - Batch: 950 | Loss: 1.383 | Acc: 29.053% | Wgt Acc: 26.032%
	I - Batch: 1000 | Loss: 1.384 | Acc: 29.000% | Wgt Acc: 25.991%
	I - Batch: 1050 | Loss: 1.383 | Acc: 29.429% | Wgt Acc: 26.399%
	I - Batch: 1100 | Loss: 1.383 | Acc: 29.455% | Wgt Acc: 26.422%
	I - Batch: 1150 | Loss: 1.383 | Acc: 29.304% | Wgt Acc: 26.287%
	I - Batch: 1200 | Loss: 1.383 | Acc: 29.083% | Wgt Acc: 26.089%
	I - Batch: 1250 | Loss: 1.383 | Acc: 29.040% | Wgt Acc: 26.045%
	I - Batch: 1300 | Loss: 1.383 | Acc: 29.154% | Wgt Acc: 26.165%
	I - Batch: 1350 | Loss: 1.384 | Acc: 28.667% | Wgt Acc: 25.714%
	I - Batch: 1400 | Loss: 1.383 | Acc: 28.643% | Wgt Acc: 25.717%
	I - Batch: 1450 | Loss: 1.383 | Acc: 28.552% | Wgt Acc: 25.643%
	I - Batch: 1500 | Loss: 1.383 | Acc: 28.733% | Wgt Acc: 25.805%
	I - Batch: 1550 | Loss: 1.382 | Acc: 28.968% | Wgt Acc: 26.033%
	I - Batch: 1600 | Loss: 1.382 | Acc: 28.688% | Wgt Acc: 25.779%
	I - Batch: 1650 | Loss: 1.382 | Acc: 28.848% | Wgt Acc: 25.937%
	I - Batch: 1700 | Loss: 1.382 | Acc: 29.118% | Wgt Acc: 26.173%
	I - Batch: 1750 | Loss: 1.382 | Acc: 29.029% | Wgt Acc: 26.098%
	I - Batch: 1800 | Loss: 1.381 | Acc: 29.111% | Wgt Acc: 26.177%
	I - Batch: 1850 | Loss: 1.381 | Acc: 29.243% | Wgt Acc: 26.313%
	I - Batch: 1900 | Loss: 1.381 | Acc: 29.211% | Wgt Acc: 26.288%
	I - Batch: 1950 | Loss: 1.379 | Acc: 29.641% | Wgt Acc: 26.701%
	I - Batch: 2000 | Loss: 1.379 | Acc: 29.500% | Wgt Acc: 26.583%
	I - Batch: 2050 | Loss: 1.378 | Acc: 29.415% | Wgt Acc: 26.511%
	I - Batch: 2100 | Loss: 1.379 | Acc: 29.381% | Wgt Acc: 26.484%
	I - Batch: 2150 | Loss: 1.378 | Acc: 29.442% | Wgt Acc: 26.555%
	I - Batch: 2200 | Loss: 1.378 | Acc: 29.273% | Wgt Acc: 26.399%
	I - Batch: 2250 | Loss: 1.379 | Acc: 29.333% | Wgt Acc: 26.453%
	I - Batch: 2300 | Loss: 1.379 | Acc: 29.217% | Wgt Acc: 26.343%
	I - Batch: 2350 | Loss: 1.379 | Acc: 29.489% | Wgt Acc: 26.595%
	I - Batch: 2400 | Loss: 1.379 | Acc: 29.375% | Wgt Acc: 26.499%
	I - Batch: 2450 | Loss: 1.380 | Acc: 29.184% | Wgt Acc: 26.309%
	I - Batch: 2500 | Loss: 1.380 | Acc: 29.240% | Wgt Acc: 26.357%
I - num batch: 2547
I - Train -- Loss: 1.380 | Acc: 29.211% | Wgt Acc: 26.327% | LR: 5.000000e-04 | Dur: 1349.66s
I - Confusion Matrix: [row->prediction - col->label]
[[190. 136. 180. 142.]
 [  0.   0.   0.   0.]
 [507. 442. 554. 396.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.396 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 122.59s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 1.384 | Acc: 28.000% | Wgt Acc: 25.225%
	I - Batch: 100 | Loss: 1.382 | Acc: 30.000% | Wgt Acc: 26.846%
	I - Batch: 150 | Loss: 1.382 | Acc: 29.333% | Wgt Acc: 26.426%
	I - Batch: 200 | Loss: 1.376 | Acc: 31.000% | Wgt Acc: 28.054%
	I - Batch: 250 | Loss: 1.378 | Acc: 29.600% | Wgt Acc: 26.812%
	I - Batch: 300 | Loss: 1.375 | Acc: 31.000% | Wgt Acc: 28.118%
	I - Batch: 350 | Loss: 1.373 | Acc: 32.286% | Wgt Acc: 29.275%
	I - Batch: 400 | Loss: 1.375 | Acc: 31.000% | Wgt Acc: 28.118%
	I - Batch: 450 | Loss: 1.378 | Acc: 30.444% | Wgt Acc: 27.565%
	I - Batch: 500 | Loss: 1.380 | Acc: 30.200% | Wgt Acc: 27.306%
	I - Batch: 550 | Loss: 1.379 | Acc: 31.091% | Wgt Acc: 28.090%
	I - Batch: 600 | Loss: 1.380 | Acc: 30.500% | Wgt Acc: 27.560%
	I - Batch: 650 | Loss: 1.378 | Acc: 30.923% | Wgt Acc: 27.975%
	I - Batch: 700 | Loss: 1.378 | Acc: 30.857% | Wgt Acc: 27.925%
	I - Batch: 750 | Loss: 1.372 | Acc: 31.600% | Wgt Acc: 28.684%
	I - Batch: 800 | Loss: 1.376 | Acc: 31.000% | Wgt Acc: 28.094%
	I - Batch: 850 | Loss: 1.378 | Acc: 30.824% | Wgt Acc: 27.902%
	I - Batch: 900 | Loss: 1.380 | Acc: 30.000% | Wgt Acc: 27.122%
	I - Batch: 950 | Loss: 1.379 | Acc: 30.316% | Wgt Acc: 27.409%
	I - Batch: 1000 | Loss: 1.380 | Acc: 30.200% | Wgt Acc: 27.293%
	I - Batch: 1050 | Loss: 1.382 | Acc: 29.714% | Wgt Acc: 26.816%
	I - Batch: 1100 | Loss: 1.382 | Acc: 29.727% | Wgt Acc: 26.809%
	I - Batch: 1150 | Loss: 1.383 | Acc: 29.043% | Wgt Acc: 26.170%
	I - Batch: 1200 | Loss: 1.383 | Acc: 28.667% | Wgt Acc: 25.836%
	I - Batch: 1250 | Loss: 1.383 | Acc: 28.800% | Wgt Acc: 25.951%
	I - Batch: 1300 | Loss: 1.383 | Acc: 28.538% | Wgt Acc: 25.710%
	I - Batch: 1350 | Loss: 1.383 | Acc: 28.741% | Wgt Acc: 25.901%
	I - Batch: 1400 | Loss: 1.384 | Acc: 28.357% | Wgt Acc: 25.539%
	I - Batch: 1450 | Loss: 1.383 | Acc: 28.483% | Wgt Acc: 25.680%
	I - Batch: 1500 | Loss: 1.383 | Acc: 28.667% | Wgt Acc: 25.837%
	I - Batch: 1550 | Loss: 1.383 | Acc: 28.645% | Wgt Acc: 25.818%
	I - Batch: 1600 | Loss: 1.382 | Acc: 28.812% | Wgt Acc: 25.968%
	I - Batch: 1650 | Loss: 1.383 | Acc: 28.667% | Wgt Acc: 25.843%
	I - Batch: 1700 | Loss: 1.383 | Acc: 28.647% | Wgt Acc: 25.825%
	I - Batch: 1750 | Loss: 1.383 | Acc: 28.286% | Wgt Acc: 25.506%
	I - Batch: 1800 | Loss: 1.382 | Acc: 28.667% | Wgt Acc: 25.861%
	I - Batch: 1850 | Loss: 1.383 | Acc: 28.270% | Wgt Acc: 25.484%
	I - Batch: 1900 | Loss: 1.383 | Acc: 28.368% | Wgt Acc: 25.581%
	I - Batch: 1950 | Loss: 1.383 | Acc: 28.256% | Wgt Acc: 25.474%
	I - Batch: 2000 | Loss: 1.384 | Acc: 28.050% | Wgt Acc: 25.276%
	I - Batch: 2050 | Loss: 1.384 | Acc: 28.098% | Wgt Acc: 25.316%
	I - Batch: 2100 | Loss: 1.384 | Acc: 28.095% | Wgt Acc: 25.319%
	I - Batch: 2150 | Loss: 1.383 | Acc: 28.000% | Wgt Acc: 25.241%
	I - Batch: 2200 | Loss: 1.384 | Acc: 27.818% | Wgt Acc: 25.064%
	I - Batch: 2250 | Loss: 1.384 | Acc: 27.644% | Wgt Acc: 24.910%
	I - Batch: 2300 | Loss: 1.383 | Acc: 27.783% | Wgt Acc: 25.049%
	I - Batch: 2350 | Loss: 1.383 | Acc: 27.745% | Wgt Acc: 25.014%
	I - Batch: 2400 | Loss: 1.383 | Acc: 27.542% | Wgt Acc: 24.831%
	I - Batch: 2450 | Loss: 1.383 | Acc: 27.429% | Wgt Acc: 24.724%
	I - Batch: 2500 | Loss: 1.383 | Acc: 27.400% | Wgt Acc: 24.694%
I - num batch: 2547
I - Train -- Loss: 1.383 | Acc: 27.640% | Wgt Acc: 24.912% | LR: 5.000000e-04 | Dur: 1392.74s
I - Confusion Matrix: [row->prediction - col->label]
[[220. 206. 250. 197.]
 [  0.   0.   0.   0.]
 [477. 372. 484. 341.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.390 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.396 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.394 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.397 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.399 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.401 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.397 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 1.390 | Acc: 30.000% | Wgt Acc: 26.432%
	I - Batch: 100 | Loss: 1.383 | Acc: 31.000% | Wgt Acc: 27.679%
	I - Batch: 150 | Loss: 1.380 | Acc: 31.333% | Wgt Acc: 28.186%
	I - Batch: 200 | Loss: 1.383 | Acc: 29.000% | Wgt Acc: 26.097%
	I - Batch: 250 | Loss: 1.380 | Acc: 29.200% | Wgt Acc: 26.354%
	I - Batch: 300 | Loss: 1.378 | Acc: 30.000% | Wgt Acc: 27.129%
	I - Batch: 350 | Loss: 1.376 | Acc: 30.857% | Wgt Acc: 27.925%
	I - Batch: 400 | Loss: 1.378 | Acc: 30.250% | Wgt Acc: 27.329%
	I - Batch: 450 | Loss: 1.380 | Acc: 30.222% | Wgt Acc: 27.282%
	I - Batch: 500 | Loss: 1.379 | Acc: 30.000% | Wgt Acc: 27.100%
	I - Batch: 550 | Loss: 1.381 | Acc: 29.455% | Wgt Acc: 26.568%
	I - Batch: 600 | Loss: 1.381 | Acc: 29.167% | Wgt Acc: 26.257%
	I - Batch: 650 | Loss: 1.379 | Acc: 30.154% | Wgt Acc: 27.203%
	I - Batch: 700 | Loss: 1.381 | Acc: 29.571% | Wgt Acc: 26.675%
	I - Batch: 750 | Loss: 1.381 | Acc: 29.867% | Wgt Acc: 26.939%
	I - Batch: 800 | Loss: 1.381 | Acc: 30.125% | Wgt Acc: 27.178%
	I - Batch: 850 | Loss: 1.382 | Acc: 29.765% | Wgt Acc: 26.815%
	I - Batch: 900 | Loss: 1.384 | Acc: 29.222% | Wgt Acc: 26.300%
	I - Batch: 950 | Loss: 1.382 | Acc: 29.263% | Wgt Acc: 26.382%
	I - Batch: 1000 | Loss: 1.381 | Acc: 29.400% | Wgt Acc: 26.522%
	I - Batch: 1050 | Loss: 1.381 | Acc: 29.619% | Wgt Acc: 26.724%
	I - Batch: 1100 | Loss: 1.379 | Acc: 29.545% | Wgt Acc: 26.694%
	I - Batch: 1150 | Loss: 1.379 | Acc: 29.391% | Wgt Acc: 26.562%
	I - Batch: 1200 | Loss: 1.378 | Acc: 29.250% | Wgt Acc: 26.441%
	I - Batch: 1250 | Loss: 1.378 | Acc: 28.960% | Wgt Acc: 26.184%
	I - Batch: 1300 | Loss: 1.378 | Acc: 28.846% | Wgt Acc: 26.082%
	I - Batch: 1350 | Loss: 1.378 | Acc: 28.963% | Wgt Acc: 26.184%
	I - Batch: 1400 | Loss: 1.379 | Acc: 28.571% | Wgt Acc: 25.806%
	I - Batch: 1450 | Loss: 1.378 | Acc: 29.310% | Wgt Acc: 26.484%
	I - Batch: 1500 | Loss: 1.379 | Acc: 28.933% | Wgt Acc: 26.129%
	I - Batch: 1550 | Loss: 1.381 | Acc: 28.645% | Wgt Acc: 25.848%
	I - Batch: 1600 | Loss: 1.381 | Acc: 28.625% | Wgt Acc: 25.825%
	I - Batch: 1650 | Loss: 1.381 | Acc: 28.545% | Wgt Acc: 25.752%
	I - Batch: 1700 | Loss: 1.381 | Acc: 28.353% | Wgt Acc: 25.570%
	I - Batch: 1750 | Loss: 1.382 | Acc: 27.943% | Wgt Acc: 25.180%
	I - Batch: 1800 | Loss: 1.382 | Acc: 27.778% | Wgt Acc: 25.022%
	I - Batch: 1850 | Loss: 1.382 | Acc: 27.622% | Wgt Acc: 24.866%
	I - Batch: 1900 | Loss: 1.382 | Acc: 27.474% | Wgt Acc: 24.736%
	I - Batch: 1950 | Loss: 1.383 | Acc: 27.231% | Wgt Acc: 24.504%
	I - Batch: 2000 | Loss: 1.383 | Acc: 27.200% | Wgt Acc: 24.474%
	I - Batch: 2050 | Loss: 1.383 | Acc: 27.122% | Wgt Acc: 24.405%
	I - Batch: 2100 | Loss: 1.383 | Acc: 27.095% | Wgt Acc: 24.376%
	I - Batch: 2150 | Loss: 1.383 | Acc: 26.884% | Wgt Acc: 24.187%
	I - Batch: 2200 | Loss: 1.383 | Acc: 26.682% | Wgt Acc: 24.006%
	I - Batch: 2250 | Loss: 1.383 | Acc: 26.622% | Wgt Acc: 23.955%
	I - Batch: 2300 | Loss: 1.383 | Acc: 26.913% | Wgt Acc: 24.218%
	I - Batch: 2350 | Loss: 1.382 | Acc: 26.936% | Wgt Acc: 24.251%
	I - Batch: 2400 | Loss: 1.382 | Acc: 27.125% | Wgt Acc: 24.423%
	I - Batch: 2450 | Loss: 1.382 | Acc: 27.184% | Wgt Acc: 24.479%
	I - Batch: 2500 | Loss: 1.382 | Acc: 27.240% | Wgt Acc: 24.536%
I - num batch: 2547
I - Train -- Loss: 1.381 | Acc: 27.208% | Wgt Acc: 24.522% | LR: 5.000000e-04 | Dur: 1396.27s
I - Confusion Matrix: [row->prediction - col->label]
[[220. 182. 261. 152.]
 [  0.   0.   0.   1.]
 [477. 396. 473. 385.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.387 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.401 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.399 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.401 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.405 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.404 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.404 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 122.15s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 1.424 | Acc: 24.000% | Wgt Acc: 21.145%
	I - Batch: 100 | Loss: 1.409 | Acc: 27.000% | Wgt Acc: 23.947%
	I - Batch: 150 | Loss: 1.390 | Acc: 32.000% | Wgt Acc: 28.657%
	I - Batch: 200 | Loss: 1.385 | Acc: 31.000% | Wgt Acc: 27.865%
	I - Batch: 250 | Loss: 1.377 | Acc: 31.600% | Wgt Acc: 28.546%
	I - Batch: 300 | Loss: 1.381 | Acc: 29.333% | Wgt Acc: 26.446%
	I - Batch: 350 | Loss: 1.379 | Acc: 29.714% | Wgt Acc: 26.821%
	I - Batch: 400 | Loss: 1.381 | Acc: 28.750% | Wgt Acc: 25.901%
	I - Batch: 450 | Loss: 1.383 | Acc: 28.222% | Wgt Acc: 25.375%
	I - Batch: 500 | Loss: 1.382 | Acc: 29.400% | Wgt Acc: 26.475%
	I - Batch: 550 | Loss: 1.378 | Acc: 29.818% | Wgt Acc: 26.907%
	I - Batch: 600 | Loss: 1.380 | Acc: 29.167% | Wgt Acc: 26.296%
	I - Batch: 650 | Loss: 1.382 | Acc: 28.615% | Wgt Acc: 25.753%
	I - Batch: 700 | Loss: 1.380 | Acc: 29.000% | Wgt Acc: 26.135%
	I - Batch: 750 | Loss: 1.379 | Acc: 28.933% | Wgt Acc: 26.082%
	I - Batch: 800 | Loss: 1.378 | Acc: 28.750% | Wgt Acc: 25.945%
	I - Batch: 850 | Loss: 1.379 | Acc: 28.471% | Wgt Acc: 25.663%
	I - Batch: 900 | Loss: 1.379 | Acc: 28.667% | Wgt Acc: 25.852%
	I - Batch: 950 | Loss: 1.379 | Acc: 28.316% | Wgt Acc: 25.534%
	I - Batch: 1000 | Loss: 1.379 | Acc: 28.300% | Wgt Acc: 25.518%
	I - Batch: 1050 | Loss: 1.379 | Acc: 28.571% | Wgt Acc: 25.757%
	I - Batch: 1100 | Loss: 1.380 | Acc: 28.727% | Wgt Acc: 25.891%
	I - Batch: 1150 | Loss: 1.379 | Acc: 28.696% | Wgt Acc: 25.872%
	I - Batch: 1200 | Loss: 1.380 | Acc: 28.500% | Wgt Acc: 25.685%
	I - Batch: 1250 | Loss: 1.380 | Acc: 28.480% | Wgt Acc: 25.667%
	I - Batch: 1300 | Loss: 1.381 | Acc: 28.077% | Wgt Acc: 25.281%
	I - Batch: 1350 | Loss: 1.381 | Acc: 28.296% | Wgt Acc: 25.475%
	I - Batch: 1400 | Loss: 1.381 | Acc: 28.143% | Wgt Acc: 25.342%
	I - Batch: 1450 | Loss: 1.382 | Acc: 27.793% | Wgt Acc: 25.012%
	I - Batch: 1500 | Loss: 1.382 | Acc: 27.800% | Wgt Acc: 25.007%
	I - Batch: 1550 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.207%
	I - Batch: 1600 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.207%
	I - Batch: 1650 | Loss: 1.380 | Acc: 28.545% | Wgt Acc: 25.717%
	I - Batch: 1700 | Loss: 1.381 | Acc: 28.294% | Wgt Acc: 25.477%
	I - Batch: 1750 | Loss: 1.381 | Acc: 28.457% | Wgt Acc: 25.630%
	I - Batch: 1800 | Loss: 1.381 | Acc: 28.611% | Wgt Acc: 25.776%
	I - Batch: 1850 | Loss: 1.380 | Acc: 28.595% | Wgt Acc: 25.767%
	I - Batch: 1900 | Loss: 1.380 | Acc: 28.789% | Wgt Acc: 25.940%
	I - Batch: 1950 | Loss: 1.381 | Acc: 28.667% | Wgt Acc: 25.829%
	I - Batch: 2000 | Loss: 1.381 | Acc: 28.500% | Wgt Acc: 25.681%
	I - Batch: 2050 | Loss: 1.379 | Acc: 28.439% | Wgt Acc: 25.655%
	I - Batch: 2100 | Loss: 1.380 | Acc: 28.286% | Wgt Acc: 25.505%
	I - Batch: 2150 | Loss: 1.379 | Acc: 28.558% | Wgt Acc: 25.766%
	I - Batch: 2200 | Loss: 1.380 | Acc: 28.273% | Wgt Acc: 25.484%
	I - Batch: 2250 | Loss: 1.380 | Acc: 28.044% | Wgt Acc: 25.273%
	I - Batch: 2300 | Loss: 1.380 | Acc: 28.261% | Wgt Acc: 25.468%
	I - Batch: 2350 | Loss: 1.381 | Acc: 28.085% | Wgt Acc: 25.302%
	I - Batch: 2400 | Loss: 1.380 | Acc: 28.292% | Wgt Acc: 25.498%
	I - Batch: 2450 | Loss: 1.380 | Acc: 28.286% | Wgt Acc: 25.501%
	I - Batch: 2500 | Loss: 1.381 | Acc: 28.320% | Wgt Acc: 25.523%
I - num batch: 2547
I - Train -- Loss: 1.381 | Acc: 28.269% | Wgt Acc: 25.478% | LR: 2.500000e-04 | Dur: 1392.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.  70.  80.  60.]
 [  0.   0.   0.   0.]
 [631. 508. 654. 478.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.402 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.399 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.402 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.405 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.405 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.404 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 1.368 | Acc: 36.000% | Wgt Acc: 32.727%
	I - Batch: 100 | Loss: 1.374 | Acc: 36.000% | Wgt Acc: 32.506%
	I - Batch: 150 | Loss: 1.377 | Acc: 33.333% | Wgt Acc: 30.075%
	I - Batch: 200 | Loss: 1.375 | Acc: 33.000% | Wgt Acc: 29.831%
	I - Batch: 250 | Loss: 1.366 | Acc: 34.800% | Wgt Acc: 31.608%
	I - Batch: 300 | Loss: 1.372 | Acc: 34.333% | Wgt Acc: 31.047%
	I - Batch: 350 | Loss: 1.373 | Acc: 34.286% | Wgt Acc: 30.948%
	I - Batch: 400 | Loss: 1.376 | Acc: 32.250% | Wgt Acc: 29.120%
	I - Batch: 450 | Loss: 1.372 | Acc: 32.444% | Wgt Acc: 29.391%
	I - Batch: 500 | Loss: 1.369 | Acc: 32.600% | Wgt Acc: 29.596%
	I - Batch: 550 | Loss: 1.374 | Acc: 31.455% | Wgt Acc: 28.489%
	I - Batch: 600 | Loss: 1.374 | Acc: 31.000% | Wgt Acc: 28.086%
	I - Batch: 650 | Loss: 1.376 | Acc: 31.077% | Wgt Acc: 28.104%
	I - Batch: 700 | Loss: 1.378 | Acc: 30.857% | Wgt Acc: 27.871%
	I - Batch: 750 | Loss: 1.377 | Acc: 30.800% | Wgt Acc: 27.840%
	I - Batch: 800 | Loss: 1.376 | Acc: 31.375% | Wgt Acc: 28.362%
	I - Batch: 850 | Loss: 1.375 | Acc: 31.529% | Wgt Acc: 28.533%
	I - Batch: 900 | Loss: 1.375 | Acc: 31.222% | Wgt Acc: 28.255%
	I - Batch: 950 | Loss: 1.376 | Acc: 30.947% | Wgt Acc: 27.987%
	I - Batch: 1000 | Loss: 1.376 | Acc: 31.200% | Wgt Acc: 28.216%
	I - Batch: 1050 | Loss: 1.375 | Acc: 30.952% | Wgt Acc: 28.023%
	I - Batch: 1100 | Loss: 1.374 | Acc: 30.909% | Wgt Acc: 27.989%
	I - Batch: 1150 | Loss: 1.375 | Acc: 30.348% | Wgt Acc: 27.475%
	I - Batch: 1200 | Loss: 1.375 | Acc: 30.667% | Wgt Acc: 27.768%
	I - Batch: 1250 | Loss: 1.376 | Acc: 30.240% | Wgt Acc: 27.367%
	I - Batch: 1300 | Loss: 1.375 | Acc: 30.462% | Wgt Acc: 27.577%
	I - Batch: 1350 | Loss: 1.375 | Acc: 30.296% | Wgt Acc: 27.436%
	I - Batch: 1400 | Loss: 1.375 | Acc: 30.286% | Wgt Acc: 27.421%
	I - Batch: 1450 | Loss: 1.376 | Acc: 30.552% | Wgt Acc: 27.649%
	I - Batch: 1500 | Loss: 1.376 | Acc: 30.133% | Wgt Acc: 27.262%
	I - Batch: 1550 | Loss: 1.377 | Acc: 30.000% | Wgt Acc: 27.122%
	I - Batch: 1600 | Loss: 1.378 | Acc: 29.688% | Wgt Acc: 26.836%
	I - Batch: 1650 | Loss: 1.378 | Acc: 29.576% | Wgt Acc: 26.714%
	I - Batch: 1700 | Loss: 1.379 | Acc: 29.471% | Wgt Acc: 26.606%
	I - Batch: 1750 | Loss: 1.379 | Acc: 29.257% | Wgt Acc: 26.399%
	I - Batch: 1800 | Loss: 1.379 | Acc: 29.222% | Wgt Acc: 26.369%
	I - Batch: 1850 | Loss: 1.379 | Acc: 29.189% | Wgt Acc: 26.329%
	I - Batch: 1900 | Loss: 1.380 | Acc: 29.000% | Wgt Acc: 26.148%
	I - Batch: 1950 | Loss: 1.380 | Acc: 28.923% | Wgt Acc: 26.060%
	I - Batch: 2000 | Loss: 1.380 | Acc: 29.050% | Wgt Acc: 26.171%
	I - Batch: 2050 | Loss: 1.381 | Acc: 28.927% | Wgt Acc: 26.054%
	I - Batch: 2100 | Loss: 1.381 | Acc: 28.952% | Wgt Acc: 26.086%
	I - Batch: 2150 | Loss: 1.380 | Acc: 28.977% | Wgt Acc: 26.111%
	I - Batch: 2200 | Loss: 1.380 | Acc: 28.864% | Wgt Acc: 26.011%
	I - Batch: 2250 | Loss: 1.380 | Acc: 28.756% | Wgt Acc: 25.911%
	I - Batch: 2300 | Loss: 1.380 | Acc: 28.826% | Wgt Acc: 25.975%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.979% | Wgt Acc: 26.132%
	I - Batch: 2400 | Loss: 1.380 | Acc: 28.875% | Wgt Acc: 26.031%
	I - Batch: 2450 | Loss: 1.380 | Acc: 28.898% | Wgt Acc: 26.053%
	I - Batch: 2500 | Loss: 1.380 | Acc: 28.760% | Wgt Acc: 25.919%
I - num batch: 2547
I - Train -- Loss: 1.380 | Acc: 28.622% | Wgt Acc: 25.796% | LR: 2.500000e-04 | Dur: 1389.94s
I - Confusion Matrix: [row->prediction - col->label]
[[207. 178. 212. 148.]
 [  0.   0.   0.   0.]
 [490. 400. 522. 390.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.393 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.396 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.399 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.398 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 119.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 1.401 | Acc: 20.000% | Wgt Acc: 17.699%
	I - Batch: 100 | Loss: 1.397 | Acc: 23.000% | Wgt Acc: 20.399%
	I - Batch: 150 | Loss: 1.385 | Acc: 22.667% | Wgt Acc: 20.268%
	I - Batch: 200 | Loss: 1.386 | Acc: 20.500% | Wgt Acc: 18.324%
	I - Batch: 250 | Loss: 1.391 | Acc: 19.600% | Wgt Acc: 17.376%
	I - Batch: 300 | Loss: 1.391 | Acc: 19.333% | Wgt Acc: 17.160%
	I - Batch: 350 | Loss: 1.390 | Acc: 20.571% | Wgt Acc: 18.297%
	I - Batch: 400 | Loss: 1.390 | Acc: 21.000% | Wgt Acc: 18.719%
	I - Batch: 450 | Loss: 1.389 | Acc: 21.333% | Wgt Acc: 19.010%
	I - Batch: 500 | Loss: 1.386 | Acc: 22.800% | Wgt Acc: 20.403%
	I - Batch: 550 | Loss: 1.385 | Acc: 22.909% | Wgt Acc: 20.538%
	I - Batch: 600 | Loss: 1.384 | Acc: 23.333% | Wgt Acc: 20.911%
	I - Batch: 650 | Loss: 1.386 | Acc: 23.692% | Wgt Acc: 21.227%
	I - Batch: 700 | Loss: 1.385 | Acc: 24.571% | Wgt Acc: 22.037%
	I - Batch: 750 | Loss: 1.387 | Acc: 24.133% | Wgt Acc: 21.599%
	I - Batch: 800 | Loss: 1.386 | Acc: 24.500% | Wgt Acc: 21.936%
	I - Batch: 850 | Loss: 1.386 | Acc: 24.471% | Wgt Acc: 21.912%
	I - Batch: 900 | Loss: 1.386 | Acc: 24.222% | Wgt Acc: 21.697%
	I - Batch: 950 | Loss: 1.385 | Acc: 24.947% | Wgt Acc: 22.382%
	I - Batch: 1000 | Loss: 1.383 | Acc: 25.500% | Wgt Acc: 22.892%
	I - Batch: 1050 | Loss: 1.383 | Acc: 25.619% | Wgt Acc: 22.988%
	I - Batch: 1100 | Loss: 1.382 | Acc: 25.727% | Wgt Acc: 23.099%
	I - Batch: 1150 | Loss: 1.382 | Acc: 25.652% | Wgt Acc: 23.039%
	I - Batch: 1200 | Loss: 1.384 | Acc: 25.417% | Wgt Acc: 22.810%
	I - Batch: 1250 | Loss: 1.383 | Acc: 25.520% | Wgt Acc: 22.910%
	I - Batch: 1300 | Loss: 1.383 | Acc: 25.923% | Wgt Acc: 23.307%
	I - Batch: 1350 | Loss: 1.383 | Acc: 26.222% | Wgt Acc: 23.585%
	I - Batch: 1400 | Loss: 1.383 | Acc: 26.643% | Wgt Acc: 23.961%
	I - Batch: 1450 | Loss: 1.382 | Acc: 26.966% | Wgt Acc: 24.267%
	I - Batch: 1500 | Loss: 1.381 | Acc: 27.267% | Wgt Acc: 24.557%
	I - Batch: 1550 | Loss: 1.380 | Acc: 27.548% | Wgt Acc: 24.837%
	I - Batch: 1600 | Loss: 1.380 | Acc: 27.438% | Wgt Acc: 24.736%
	I - Batch: 1650 | Loss: 1.378 | Acc: 27.758% | Wgt Acc: 25.051%
	I - Batch: 1700 | Loss: 1.379 | Acc: 27.765% | Wgt Acc: 25.050%
	I - Batch: 1750 | Loss: 1.378 | Acc: 28.171% | Wgt Acc: 25.429%
	I - Batch: 1800 | Loss: 1.377 | Acc: 28.500% | Wgt Acc: 25.724%
	I - Batch: 1850 | Loss: 1.378 | Acc: 28.486% | Wgt Acc: 25.713%
	I - Batch: 1900 | Loss: 1.377 | Acc: 28.789% | Wgt Acc: 25.995%
	I - Batch: 1950 | Loss: 1.377 | Acc: 28.718% | Wgt Acc: 25.928%
	I - Batch: 2000 | Loss: 1.379 | Acc: 28.500% | Wgt Acc: 25.719%
	I - Batch: 2050 | Loss: 1.379 | Acc: 28.488% | Wgt Acc: 25.712%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.143% | Wgt Acc: 25.395%
	I - Batch: 2150 | Loss: 1.380 | Acc: 28.047% | Wgt Acc: 25.307%
	I - Batch: 2200 | Loss: 1.380 | Acc: 28.091% | Wgt Acc: 25.338%
	I - Batch: 2250 | Loss: 1.381 | Acc: 27.956% | Wgt Acc: 25.200%
	I - Batch: 2300 | Loss: 1.380 | Acc: 28.087% | Wgt Acc: 25.328%
	I - Batch: 2350 | Loss: 1.380 | Acc: 28.128% | Wgt Acc: 25.374%
	I - Batch: 2400 | Loss: 1.380 | Acc: 28.042% | Wgt Acc: 25.294%
	I - Batch: 2450 | Loss: 1.381 | Acc: 27.959% | Wgt Acc: 25.200%
	I - Batch: 2500 | Loss: 1.381 | Acc: 27.960% | Wgt Acc: 25.205%
I - num batch: 2547
I - Train -- Loss: 1.381 | Acc: 27.954% | Wgt Acc: 25.203% | LR: 2.500000e-04 | Dur: 1400.08s
I - Confusion Matrix: [row->prediction - col->label]
[[116. 123. 137. 107.]
 [  0.   0.   2.   0.]
 [581. 455. 595. 430.]
 [  0.   0.   0.   1.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.397 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.394 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 23.500% | Wgt Acc: 20.912%
	I - Batch: 250 | Loss: 1.397 | Acc: 23.200% | Wgt Acc: 20.604%
	I - Batch: 300 | Loss: 1.397 | Acc: 21.667% | Wgt Acc: 19.245%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 23.242% | Wgt Acc: 20.652% | Dur: 123.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 3.  1.  2.  1.]
 [ 0.  0.  0.  0.]
 [85. 77. 73. 85.]
 [ 0.  0.  0.  0.]]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 1.381 | Acc: 24.000% | Wgt Acc: 21.622%
	I - Batch: 100 | Loss: 1.388 | Acc: 28.000% | Wgt Acc: 25.056%
	I - Batch: 150 | Loss: 1.386 | Acc: 26.667% | Wgt Acc: 23.916%
	I - Batch: 200 | Loss: 1.382 | Acc: 26.500% | Wgt Acc: 23.847%
	I - Batch: 250 | Loss: 1.375 | Acc: 27.200% | Wgt Acc: 24.638%
	I - Batch: 300 | Loss: 1.372 | Acc: 27.667% | Wgt Acc: 25.113%
	I - Batch: 350 | Loss: 1.370 | Acc: 29.714% | Wgt Acc: 26.960%
	I - Batch: 400 | Loss: 1.371 | Acc: 29.250% | Wgt Acc: 26.456%
	I - Batch: 450 | Loss: 1.374 | Acc: 28.667% | Wgt Acc: 25.930%
	I - Batch: 500 | Loss: 1.373 | Acc: 29.400% | Wgt Acc: 26.606%
	I - Batch: 550 | Loss: 1.374 | Acc: 29.455% | Wgt Acc: 26.623%
	I - Batch: 600 | Loss: 1.376 | Acc: 29.000% | Wgt Acc: 26.185%
	I - Batch: 650 | Loss: 1.378 | Acc: 28.154% | Wgt Acc: 25.408%
	I - Batch: 700 | Loss: 1.376 | Acc: 28.286% | Wgt Acc: 25.540%
	I - Batch: 750 | Loss: 1.377 | Acc: 28.133% | Wgt Acc: 25.391%
	I - Batch: 800 | Loss: 1.377 | Acc: 28.250% | Wgt Acc: 25.494%
	I - Batch: 850 | Loss: 1.376 | Acc: 28.471% | Wgt Acc: 25.710%
	I - Batch: 900 | Loss: 1.378 | Acc: 28.111% | Wgt Acc: 25.370%
	I - Batch: 950 | Loss: 1.377 | Acc: 28.000% | Wgt Acc: 25.267%
	I - Batch: 1000 | Loss: 1.378 | Acc: 27.800% | Wgt Acc: 25.062%
	I - Batch: 1050 | Loss: 1.379 | Acc: 27.905% | Wgt Acc: 25.150%
	I - Batch: 1100 | Loss: 1.379 | Acc: 28.182% | Wgt Acc: 25.405%
	I - Batch: 1150 | Loss: 1.378 | Acc: 28.522% | Wgt Acc: 25.736%
	I - Batch: 1200 | Loss: 1.377 | Acc: 28.833% | Wgt Acc: 26.030%
	I - Batch: 1250 | Loss: 1.377 | Acc: 28.960% | Wgt Acc: 26.142%
	I - Batch: 1300 | Loss: 1.377 | Acc: 29.077% | Wgt Acc: 26.255%
	I - Batch: 1350 | Loss: 1.377 | Acc: 29.037% | Wgt Acc: 26.212%
	I - Batch: 1400 | Loss: 1.378 | Acc: 28.929% | Wgt Acc: 26.100%
	I - Batch: 1450 | Loss: 1.378 | Acc: 29.034% | Wgt Acc: 26.186%
	I - Batch: 1500 | Loss: 1.378 | Acc: 28.933% | Wgt Acc: 26.101%
	I - Batch: 1550 | Loss: 1.378 | Acc: 29.097% | Wgt Acc: 26.248%
	I - Batch: 1600 | Loss: 1.378 | Acc: 29.062% | Wgt Acc: 26.208%
	I - Batch: 1650 | Loss: 1.378 | Acc: 28.545% | Wgt Acc: 25.741%
	I - Batch: 1700 | Loss: 1.378 | Acc: 28.706% | Wgt Acc: 25.885%
	I - Batch: 1750 | Loss: 1.378 | Acc: 28.571% | Wgt Acc: 25.777%
	I - Batch: 1800 | Loss: 1.378 | Acc: 28.611% | Wgt Acc: 25.808%
	I - Batch: 1850 | Loss: 1.376 | Acc: 28.486% | Wgt Acc: 25.729%
	I - Batch: 1900 | Loss: 1.378 | Acc: 28.263% | Wgt Acc: 25.505%
	I - Batch: 1950 | Loss: 1.378 | Acc: 28.205% | Wgt Acc: 25.436%
	I - Batch: 2000 | Loss: 1.378 | Acc: 28.250% | Wgt Acc: 25.491%
	I - Batch: 2050 | Loss: 1.377 | Acc: 28.293% | Wgt Acc: 25.542%
	I - Batch: 2100 | Loss: 1.378 | Acc: 28.238% | Wgt Acc: 25.486%
	I - Batch: 2150 | Loss: 1.378 | Acc: 28.372% | Wgt Acc: 25.611%
	I - Batch: 2200 | Loss: 1.378 | Acc: 28.273% | Wgt Acc: 25.515%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.178% | Wgt Acc: 25.426%
	I - Batch: 2300 | Loss: 1.379 | Acc: 27.870% | Wgt Acc: 25.145%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.085% | Wgt Acc: 25.338%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.125% | Wgt Acc: 25.364%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.122% | Wgt Acc: 25.350%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.120% | Wgt Acc: 25.347%
I - num batch: 2547
I - Train -- Loss: 1.380 | Acc: 28.112% | Wgt Acc: 25.336% | LR: 2.500000e-04 | Dur: 1410.36s
I - Confusion Matrix: [row->prediction - col->label]
[[254. 202. 272. 183.]
 [  0.   0.   0.   0.]
 [443. 376. 462. 355.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.393 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.391 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.395 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.397 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.394 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 125.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 1.382 | Acc: 26.000% | Wgt Acc: 23.529%
	I - Batch: 100 | Loss: 1.378 | Acc: 26.000% | Wgt Acc: 23.636%
	I - Batch: 150 | Loss: 1.387 | Acc: 24.667% | Wgt Acc: 22.156%
	I - Batch: 200 | Loss: 1.385 | Acc: 25.500% | Wgt Acc: 22.973%
	I - Batch: 250 | Loss: 1.386 | Acc: 25.200% | Wgt Acc: 22.642%
	I - Batch: 300 | Loss: 1.385 | Acc: 25.000% | Wgt Acc: 22.472%
	I - Batch: 350 | Loss: 1.383 | Acc: 26.571% | Wgt Acc: 23.938%
	I - Batch: 400 | Loss: 1.386 | Acc: 25.250% | Wgt Acc: 22.633%
	I - Batch: 450 | Loss: 1.385 | Acc: 26.667% | Wgt Acc: 23.952%
	I - Batch: 500 | Loss: 1.384 | Acc: 27.000% | Wgt Acc: 24.291%
	I - Batch: 550 | Loss: 1.383 | Acc: 27.273% | Wgt Acc: 24.570%
	I - Batch: 600 | Loss: 1.384 | Acc: 26.167% | Wgt Acc: 23.538%
	I - Batch: 650 | Loss: 1.384 | Acc: 26.462% | Wgt Acc: 23.806%
	I - Batch: 700 | Loss: 1.382 | Acc: 26.143% | Wgt Acc: 23.552%
	I - Batch: 750 | Loss: 1.382 | Acc: 26.267% | Wgt Acc: 23.657%
	I - Batch: 800 | Loss: 1.382 | Acc: 26.750% | Wgt Acc: 24.086%
	I - Batch: 850 | Loss: 1.383 | Acc: 26.824% | Wgt Acc: 24.133%
	I - Batch: 900 | Loss: 1.382 | Acc: 27.111% | Wgt Acc: 24.418%
	I - Batch: 950 | Loss: 1.381 | Acc: 27.579% | Wgt Acc: 24.852%
	I - Batch: 1000 | Loss: 1.382 | Acc: 27.400% | Wgt Acc: 24.674%
	I - Batch: 1050 | Loss: 1.381 | Acc: 27.619% | Wgt Acc: 24.882%
	I - Batch: 1100 | Loss: 1.382 | Acc: 27.455% | Wgt Acc: 24.724%
	I - Batch: 1150 | Loss: 1.382 | Acc: 27.739% | Wgt Acc: 24.966%
	I - Batch: 1200 | Loss: 1.382 | Acc: 28.083% | Wgt Acc: 25.296%
	I - Batch: 1250 | Loss: 1.382 | Acc: 27.760% | Wgt Acc: 24.986%
	I - Batch: 1300 | Loss: 1.382 | Acc: 27.615% | Wgt Acc: 24.849%
	I - Batch: 1350 | Loss: 1.382 | Acc: 27.778% | Wgt Acc: 24.979%
	I - Batch: 1400 | Loss: 1.382 | Acc: 27.643% | Wgt Acc: 24.875%
	I - Batch: 1450 | Loss: 1.382 | Acc: 28.000% | Wgt Acc: 25.221%
	I - Batch: 1500 | Loss: 1.382 | Acc: 28.200% | Wgt Acc: 25.405%
	I - Batch: 1550 | Loss: 1.382 | Acc: 28.065% | Wgt Acc: 25.276%
	I - Batch: 1600 | Loss: 1.382 | Acc: 28.062% | Wgt Acc: 25.278%
	I - Batch: 1650 | Loss: 1.382 | Acc: 28.121% | Wgt Acc: 25.334%
	I - Batch: 1700 | Loss: 1.382 | Acc: 27.941% | Wgt Acc: 25.162%
	I - Batch: 1750 | Loss: 1.382 | Acc: 28.171% | Wgt Acc: 25.376%
	I - Batch: 1800 | Loss: 1.382 | Acc: 28.333% | Wgt Acc: 25.529%
	I - Batch: 1850 | Loss: 1.382 | Acc: 28.162% | Wgt Acc: 25.359%
	I - Batch: 1900 | Loss: 1.382 | Acc: 28.158% | Wgt Acc: 25.355%
	I - Batch: 1950 | Loss: 1.382 | Acc: 28.308% | Wgt Acc: 25.491%
	I - Batch: 2000 | Loss: 1.382 | Acc: 28.400% | Wgt Acc: 25.571%
	I - Batch: 2050 | Loss: 1.382 | Acc: 28.293% | Wgt Acc: 25.483%
	I - Batch: 2100 | Loss: 1.382 | Acc: 28.048% | Wgt Acc: 25.244%
	I - Batch: 2150 | Loss: 1.383 | Acc: 28.186% | Wgt Acc: 25.364%
	I - Batch: 2200 | Loss: 1.383 | Acc: 28.227% | Wgt Acc: 25.396%
	I - Batch: 2250 | Loss: 1.383 | Acc: 28.222% | Wgt Acc: 25.392%
	I - Batch: 2300 | Loss: 1.382 | Acc: 28.391% | Wgt Acc: 25.550%
	I - Batch: 2350 | Loss: 1.382 | Acc: 28.383% | Wgt Acc: 25.541%
	I - Batch: 2400 | Loss: 1.382 | Acc: 28.500% | Wgt Acc: 25.666%
	I - Batch: 2450 | Loss: 1.382 | Acc: 28.490% | Wgt Acc: 25.659%
	I - Batch: 2500 | Loss: 1.382 | Acc: 28.400% | Wgt Acc: 25.583%
I - num batch: 2547
I - Train -- Loss: 1.381 | Acc: 28.426% | Wgt Acc: 25.619% | LR: 2.500000e-04 | Dur: 1397.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 93.  68. 102.  66.]
 [  1.   0.   1.   1.]
 [603. 510. 631. 471.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.393 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.396 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.398 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.397 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 119.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 1.352 | Acc: 30.000% | Wgt Acc: 27.778%
	I - Batch: 100 | Loss: 1.359 | Acc: 33.000% | Wgt Acc: 30.345%
	I - Batch: 150 | Loss: 1.374 | Acc: 31.333% | Wgt Acc: 28.356%
	I - Batch: 200 | Loss: 1.372 | Acc: 31.500% | Wgt Acc: 28.507%
	I - Batch: 250 | Loss: 1.372 | Acc: 30.000% | Wgt Acc: 27.125%
	I - Batch: 300 | Loss: 1.372 | Acc: 30.333% | Wgt Acc: 27.451%
	I - Batch: 350 | Loss: 1.378 | Acc: 27.429% | Wgt Acc: 24.726%
	I - Batch: 400 | Loss: 1.375 | Acc: 27.750% | Wgt Acc: 25.085%
	I - Batch: 450 | Loss: 1.377 | Acc: 28.444% | Wgt Acc: 25.677%
	I - Batch: 500 | Loss: 1.377 | Acc: 28.000% | Wgt Acc: 25.248%
	I - Batch: 550 | Loss: 1.376 | Acc: 28.364% | Wgt Acc: 25.595%
	I - Batch: 600 | Loss: 1.375 | Acc: 28.500% | Wgt Acc: 25.743%
	I - Batch: 650 | Loss: 1.375 | Acc: 28.000% | Wgt Acc: 25.287%
	I - Batch: 700 | Loss: 1.378 | Acc: 27.286% | Wgt Acc: 24.613%
	I - Batch: 750 | Loss: 1.377 | Acc: 27.467% | Wgt Acc: 24.789%
	I - Batch: 800 | Loss: 1.378 | Acc: 26.875% | Wgt Acc: 24.232%
	I - Batch: 850 | Loss: 1.377 | Acc: 26.824% | Wgt Acc: 24.197%
	I - Batch: 900 | Loss: 1.378 | Acc: 26.556% | Wgt Acc: 23.954%
	I - Batch: 950 | Loss: 1.377 | Acc: 26.211% | Wgt Acc: 23.652%
	I - Batch: 1000 | Loss: 1.377 | Acc: 26.500% | Wgt Acc: 23.917%
	I - Batch: 1050 | Loss: 1.377 | Acc: 26.571% | Wgt Acc: 23.964%
	I - Batch: 1100 | Loss: 1.377 | Acc: 26.909% | Wgt Acc: 24.262%
	I - Batch: 1150 | Loss: 1.379 | Acc: 26.696% | Wgt Acc: 24.041%
	I - Batch: 1200 | Loss: 1.379 | Acc: 26.583% | Wgt Acc: 23.922%
	I - Batch: 1250 | Loss: 1.380 | Acc: 26.480% | Wgt Acc: 23.813%
	I - Batch: 1300 | Loss: 1.381 | Acc: 26.462% | Wgt Acc: 23.794%
	I - Batch: 1350 | Loss: 1.381 | Acc: 26.444% | Wgt Acc: 23.780%
	I - Batch: 1400 | Loss: 1.380 | Acc: 26.357% | Wgt Acc: 23.711%
	I - Batch: 1450 | Loss: 1.380 | Acc: 26.414% | Wgt Acc: 23.778%
	I - Batch: 1500 | Loss: 1.379 | Acc: 26.667% | Wgt Acc: 24.013%
	I - Batch: 1550 | Loss: 1.379 | Acc: 26.645% | Wgt Acc: 24.001%
	I - Batch: 1600 | Loss: 1.381 | Acc: 26.438% | Wgt Acc: 23.787%
	I - Batch: 1650 | Loss: 1.380 | Acc: 26.485% | Wgt Acc: 23.844%
	I - Batch: 1700 | Loss: 1.380 | Acc: 26.647% | Wgt Acc: 23.981%
	I - Batch: 1750 | Loss: 1.380 | Acc: 26.743% | Wgt Acc: 24.074%
	I - Batch: 1800 | Loss: 1.380 | Acc: 26.889% | Wgt Acc: 24.203%
	I - Batch: 1850 | Loss: 1.380 | Acc: 26.865% | Wgt Acc: 24.170%
	I - Batch: 1900 | Loss: 1.380 | Acc: 27.105% | Wgt Acc: 24.396%
	I - Batch: 1950 | Loss: 1.380 | Acc: 27.282% | Wgt Acc: 24.564%
	I - Batch: 2000 | Loss: 1.380 | Acc: 27.400% | Wgt Acc: 24.668%
	I - Batch: 2050 | Loss: 1.380 | Acc: 27.610% | Wgt Acc: 24.855%
	I - Batch: 2100 | Loss: 1.379 | Acc: 27.952% | Wgt Acc: 25.188%
	I - Batch: 2150 | Loss: 1.379 | Acc: 27.767% | Wgt Acc: 25.016%
	I - Batch: 2200 | Loss: 1.379 | Acc: 27.955% | Wgt Acc: 25.192%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.044% | Wgt Acc: 25.275%
	I - Batch: 2300 | Loss: 1.379 | Acc: 27.826% | Wgt Acc: 25.073%
	I - Batch: 2350 | Loss: 1.379 | Acc: 27.745% | Wgt Acc: 25.007%
	I - Batch: 2400 | Loss: 1.379 | Acc: 27.708% | Wgt Acc: 24.965%
	I - Batch: 2450 | Loss: 1.379 | Acc: 27.837% | Wgt Acc: 25.076%
	I - Batch: 2500 | Loss: 1.379 | Acc: 27.840% | Wgt Acc: 25.081%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.072% | Wgt Acc: 25.301% | LR: 1.250000e-04 | Dur: 1392.58s
I - Confusion Matrix: [row->prediction - col->label]
[[100.  79. 119.  85.]
 [  0.   0.   0.   0.]
 [597. 499. 615. 453.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.398 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.397 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 1.383 | Acc: 28.000% | Wgt Acc: 25.112%
	I - Batch: 100 | Loss: 1.373 | Acc: 29.000% | Wgt Acc: 26.244%
	I - Batch: 150 | Loss: 1.377 | Acc: 30.000% | Wgt Acc: 27.027%
	I - Batch: 200 | Loss: 1.366 | Acc: 32.000% | Wgt Acc: 29.124%
	I - Batch: 250 | Loss: 1.361 | Acc: 32.400% | Wgt Acc: 29.643%
	I - Batch: 300 | Loss: 1.359 | Acc: 33.000% | Wgt Acc: 30.229%
	I - Batch: 350 | Loss: 1.359 | Acc: 33.143% | Wgt Acc: 30.366%
	I - Batch: 400 | Loss: 1.363 | Acc: 32.250% | Wgt Acc: 29.469%
	I - Batch: 450 | Loss: 1.364 | Acc: 32.889% | Wgt Acc: 30.005%
	I - Batch: 500 | Loss: 1.367 | Acc: 33.000% | Wgt Acc: 30.041%
	I - Batch: 550 | Loss: 1.366 | Acc: 33.273% | Wgt Acc: 30.285%
	I - Batch: 600 | Loss: 1.370 | Acc: 32.667% | Wgt Acc: 29.630%
	I - Batch: 650 | Loss: 1.369 | Acc: 32.615% | Wgt Acc: 29.609%
	I - Batch: 700 | Loss: 1.371 | Acc: 31.571% | Wgt Acc: 28.627%
	I - Batch: 750 | Loss: 1.373 | Acc: 30.800% | Wgt Acc: 27.882%
	I - Batch: 800 | Loss: 1.374 | Acc: 30.250% | Wgt Acc: 27.376%
	I - Batch: 850 | Loss: 1.375 | Acc: 30.000% | Wgt Acc: 27.142%
	I - Batch: 900 | Loss: 1.374 | Acc: 30.000% | Wgt Acc: 27.163%
	I - Batch: 950 | Loss: 1.374 | Acc: 30.316% | Wgt Acc: 27.448%
	I - Batch: 1000 | Loss: 1.375 | Acc: 29.900% | Wgt Acc: 27.040%
	I - Batch: 1050 | Loss: 1.376 | Acc: 29.619% | Wgt Acc: 26.764%
	I - Batch: 1100 | Loss: 1.377 | Acc: 29.091% | Wgt Acc: 26.262%
	I - Batch: 1150 | Loss: 1.376 | Acc: 29.478% | Wgt Acc: 26.640%
	I - Batch: 1200 | Loss: 1.376 | Acc: 29.667% | Wgt Acc: 26.802%
	I - Batch: 1250 | Loss: 1.377 | Acc: 29.360% | Wgt Acc: 26.503%
	I - Batch: 1300 | Loss: 1.378 | Acc: 29.308% | Wgt Acc: 26.440%
	I - Batch: 1350 | Loss: 1.378 | Acc: 29.259% | Wgt Acc: 26.386%
	I - Batch: 1400 | Loss: 1.379 | Acc: 28.929% | Wgt Acc: 26.074%
	I - Batch: 1450 | Loss: 1.378 | Acc: 29.103% | Wgt Acc: 26.248%
	I - Batch: 1500 | Loss: 1.378 | Acc: 29.333% | Wgt Acc: 26.458%
	I - Batch: 1550 | Loss: 1.377 | Acc: 29.742% | Wgt Acc: 26.849%
	I - Batch: 1600 | Loss: 1.376 | Acc: 29.562% | Wgt Acc: 26.693%
	I - Batch: 1650 | Loss: 1.375 | Acc: 29.939% | Wgt Acc: 27.061%
	I - Batch: 1700 | Loss: 1.377 | Acc: 29.647% | Wgt Acc: 26.769%
	I - Batch: 1750 | Loss: 1.377 | Acc: 29.543% | Wgt Acc: 26.677%
	I - Batch: 1800 | Loss: 1.377 | Acc: 29.333% | Wgt Acc: 26.469%
	I - Batch: 1850 | Loss: 1.378 | Acc: 29.135% | Wgt Acc: 26.257%
	I - Batch: 1900 | Loss: 1.378 | Acc: 29.368% | Wgt Acc: 26.477%
	I - Batch: 1950 | Loss: 1.379 | Acc: 29.077% | Wgt Acc: 26.208%
	I - Batch: 2000 | Loss: 1.379 | Acc: 28.950% | Wgt Acc: 26.087%
	I - Batch: 2050 | Loss: 1.379 | Acc: 28.732% | Wgt Acc: 25.887%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.905% | Wgt Acc: 26.054%
	I - Batch: 2150 | Loss: 1.379 | Acc: 28.884% | Wgt Acc: 26.030%
	I - Batch: 2200 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.148%
	I - Batch: 2250 | Loss: 1.378 | Acc: 28.978% | Wgt Acc: 26.130%
	I - Batch: 2300 | Loss: 1.378 | Acc: 29.087% | Wgt Acc: 26.235%
	I - Batch: 2350 | Loss: 1.378 | Acc: 28.894% | Wgt Acc: 26.055%
	I - Batch: 2400 | Loss: 1.378 | Acc: 28.875% | Wgt Acc: 26.043%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.776% | Wgt Acc: 25.943%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.840% | Wgt Acc: 25.998%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1392.05s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.393 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.395 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 1.400 | Acc: 16.000% | Wgt Acc: 14.159%
	I - Batch: 100 | Loss: 1.393 | Acc: 23.000% | Wgt Acc: 20.399%
	I - Batch: 150 | Loss: 1.389 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.383 | Acc: 26.000% | Wgt Acc: 23.266%
	I - Batch: 250 | Loss: 1.386 | Acc: 26.400% | Wgt Acc: 23.529%
	I - Batch: 300 | Loss: 1.386 | Acc: 26.667% | Wgt Acc: 23.774%
	I - Batch: 350 | Loss: 1.384 | Acc: 28.000% | Wgt Acc: 24.952%
	I - Batch: 400 | Loss: 1.383 | Acc: 28.750% | Wgt Acc: 25.684%
	I - Batch: 450 | Loss: 1.384 | Acc: 28.667% | Wgt Acc: 25.633%
	I - Batch: 500 | Loss: 1.383 | Acc: 29.200% | Wgt Acc: 26.153%
	I - Batch: 550 | Loss: 1.382 | Acc: 29.455% | Wgt Acc: 26.427%
	I - Batch: 600 | Loss: 1.383 | Acc: 28.667% | Wgt Acc: 25.710%
	I - Batch: 650 | Loss: 1.382 | Acc: 28.923% | Wgt Acc: 25.985%
	I - Batch: 700 | Loss: 1.382 | Acc: 28.429% | Wgt Acc: 25.570%
	I - Batch: 750 | Loss: 1.380 | Acc: 28.800% | Wgt Acc: 25.930%
	I - Batch: 800 | Loss: 1.382 | Acc: 28.250% | Wgt Acc: 25.393%
	I - Batch: 850 | Loss: 1.380 | Acc: 29.059% | Wgt Acc: 26.165%
	I - Batch: 900 | Loss: 1.381 | Acc: 28.667% | Wgt Acc: 25.794%
	I - Batch: 950 | Loss: 1.381 | Acc: 28.947% | Wgt Acc: 26.036%
	I - Batch: 1000 | Loss: 1.383 | Acc: 28.500% | Wgt Acc: 25.589%
	I - Batch: 1050 | Loss: 1.383 | Acc: 28.476% | Wgt Acc: 25.566%
	I - Batch: 1100 | Loss: 1.383 | Acc: 28.273% | Wgt Acc: 25.377%
	I - Batch: 1150 | Loss: 1.383 | Acc: 27.826% | Wgt Acc: 24.980%
	I - Batch: 1200 | Loss: 1.384 | Acc: 27.667% | Wgt Acc: 24.822%
	I - Batch: 1250 | Loss: 1.383 | Acc: 27.920% | Wgt Acc: 25.076%
	I - Batch: 1300 | Loss: 1.383 | Acc: 27.769% | Wgt Acc: 24.935%
	I - Batch: 1350 | Loss: 1.382 | Acc: 28.222% | Wgt Acc: 25.366%
	I - Batch: 1400 | Loss: 1.383 | Acc: 28.000% | Wgt Acc: 25.160%
	I - Batch: 1450 | Loss: 1.382 | Acc: 28.276% | Wgt Acc: 25.415%
	I - Batch: 1500 | Loss: 1.383 | Acc: 28.267% | Wgt Acc: 25.401%
	I - Batch: 1550 | Loss: 1.382 | Acc: 28.129% | Wgt Acc: 25.301%
	I - Batch: 1600 | Loss: 1.381 | Acc: 28.125% | Wgt Acc: 25.309%
	I - Batch: 1650 | Loss: 1.381 | Acc: 27.879% | Wgt Acc: 25.089%
	I - Batch: 1700 | Loss: 1.381 | Acc: 27.824% | Wgt Acc: 25.050%
	I - Batch: 1750 | Loss: 1.381 | Acc: 27.486% | Wgt Acc: 24.749%
	I - Batch: 1800 | Loss: 1.381 | Acc: 27.444% | Wgt Acc: 24.709%
	I - Batch: 1850 | Loss: 1.380 | Acc: 27.297% | Wgt Acc: 24.592%
	I - Batch: 1900 | Loss: 1.380 | Acc: 27.158% | Wgt Acc: 24.475%
	I - Batch: 1950 | Loss: 1.380 | Acc: 27.333% | Wgt Acc: 24.630%
	I - Batch: 2000 | Loss: 1.380 | Acc: 27.350% | Wgt Acc: 24.648%
	I - Batch: 2050 | Loss: 1.379 | Acc: 27.366% | Wgt Acc: 24.670%
	I - Batch: 2100 | Loss: 1.379 | Acc: 27.381% | Wgt Acc: 24.681%
	I - Batch: 2150 | Loss: 1.380 | Acc: 27.488% | Wgt Acc: 24.775%
	I - Batch: 2200 | Loss: 1.380 | Acc: 27.227% | Wgt Acc: 24.524%
	I - Batch: 2250 | Loss: 1.380 | Acc: 27.156% | Wgt Acc: 24.469%
	I - Batch: 2300 | Loss: 1.380 | Acc: 27.130% | Wgt Acc: 24.444%
	I - Batch: 2350 | Loss: 1.380 | Acc: 26.979% | Wgt Acc: 24.305%
	I - Batch: 2400 | Loss: 1.380 | Acc: 27.042% | Wgt Acc: 24.362%
	I - Batch: 2450 | Loss: 1.380 | Acc: 27.265% | Wgt Acc: 24.572%
	I - Batch: 2500 | Loss: 1.379 | Acc: 27.320% | Wgt Acc: 24.624%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 27.601% | Wgt Acc: 24.876% | LR: 1.250000e-04 | Dur: 1200.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.  36.  82.  48.]
 [  0.   0.   0.   0.]
 [646. 542. 652. 490.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.387 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.396 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.394 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.398 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.400 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.401 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.399 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 64.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 1.403 | Acc: 24.000% | Wgt Acc: 21.145%
	I - Batch: 100 | Loss: 1.382 | Acc: 28.000% | Wgt Acc: 25.169%
	I - Batch: 150 | Loss: 1.383 | Acc: 26.000% | Wgt Acc: 23.353%
	I - Batch: 200 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.097%
	I - Batch: 250 | Loss: 1.379 | Acc: 29.600% | Wgt Acc: 26.595%
	I - Batch: 300 | Loss: 1.378 | Acc: 30.667% | Wgt Acc: 27.566%
	I - Batch: 350 | Loss: 1.376 | Acc: 31.143% | Wgt Acc: 28.039%
	I - Batch: 400 | Loss: 1.376 | Acc: 31.250% | Wgt Acc: 28.121%
	I - Batch: 450 | Loss: 1.374 | Acc: 31.333% | Wgt Acc: 28.271%
	I - Batch: 500 | Loss: 1.373 | Acc: 31.600% | Wgt Acc: 28.494%
	I - Batch: 550 | Loss: 1.374 | Acc: 31.636% | Wgt Acc: 28.525%
	I - Batch: 600 | Loss: 1.375 | Acc: 31.167% | Wgt Acc: 28.078%
	I - Batch: 650 | Loss: 1.375 | Acc: 30.462% | Wgt Acc: 27.481%
	I - Batch: 700 | Loss: 1.377 | Acc: 30.143% | Wgt Acc: 27.164%
	I - Batch: 750 | Loss: 1.377 | Acc: 30.133% | Wgt Acc: 27.147%
	I - Batch: 800 | Loss: 1.377 | Acc: 29.875% | Wgt Acc: 26.907%
	I - Batch: 850 | Loss: 1.377 | Acc: 29.765% | Wgt Acc: 26.815%
	I - Batch: 900 | Loss: 1.378 | Acc: 29.556% | Wgt Acc: 26.593%
	I - Batch: 950 | Loss: 1.378 | Acc: 29.789% | Wgt Acc: 26.812%
	I - Batch: 1000 | Loss: 1.379 | Acc: 29.600% | Wgt Acc: 26.619%
	I - Batch: 1050 | Loss: 1.381 | Acc: 28.857% | Wgt Acc: 25.931%
	I - Batch: 1100 | Loss: 1.380 | Acc: 28.727% | Wgt Acc: 25.828%
	I - Batch: 1150 | Loss: 1.380 | Acc: 28.696% | Wgt Acc: 25.822%
	I - Batch: 1200 | Loss: 1.380 | Acc: 28.750% | Wgt Acc: 25.877%
	I - Batch: 1250 | Loss: 1.380 | Acc: 28.720% | Wgt Acc: 25.851%
	I - Batch: 1300 | Loss: 1.380 | Acc: 28.769% | Wgt Acc: 25.891%
	I - Batch: 1350 | Loss: 1.380 | Acc: 28.519% | Wgt Acc: 25.654%
	I - Batch: 1400 | Loss: 1.380 | Acc: 28.643% | Wgt Acc: 25.779%
	I - Batch: 1450 | Loss: 1.380 | Acc: 28.552% | Wgt Acc: 25.690%
	I - Batch: 1500 | Loss: 1.380 | Acc: 28.400% | Wgt Acc: 25.559%
	I - Batch: 1550 | Loss: 1.380 | Acc: 28.645% | Wgt Acc: 25.791%
	I - Batch: 1600 | Loss: 1.379 | Acc: 29.000% | Wgt Acc: 26.122%
	I - Batch: 1650 | Loss: 1.379 | Acc: 28.909% | Wgt Acc: 26.041%
	I - Batch: 1700 | Loss: 1.380 | Acc: 28.353% | Wgt Acc: 25.533%
	I - Batch: 1750 | Loss: 1.379 | Acc: 28.229% | Wgt Acc: 25.438%
	I - Batch: 1800 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.238%
	I - Batch: 1850 | Loss: 1.379 | Acc: 28.108% | Wgt Acc: 25.341%
	I - Batch: 1900 | Loss: 1.379 | Acc: 27.789% | Wgt Acc: 25.039%
	I - Batch: 1950 | Loss: 1.380 | Acc: 27.795% | Wgt Acc: 25.038%
	I - Batch: 2000 | Loss: 1.380 | Acc: 27.950% | Wgt Acc: 25.177%
	I - Batch: 2050 | Loss: 1.379 | Acc: 27.902% | Wgt Acc: 25.151%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.190% | Wgt Acc: 25.416%
	I - Batch: 2150 | Loss: 1.379 | Acc: 27.814% | Wgt Acc: 25.079%
	I - Batch: 2200 | Loss: 1.378 | Acc: 28.000% | Wgt Acc: 25.269%
	I - Batch: 2250 | Loss: 1.379 | Acc: 27.778% | Wgt Acc: 25.058%
	I - Batch: 2300 | Loss: 1.379 | Acc: 27.696% | Wgt Acc: 24.980%
	I - Batch: 2350 | Loss: 1.379 | Acc: 27.532% | Wgt Acc: 24.820%
	I - Batch: 2400 | Loss: 1.379 | Acc: 27.583% | Wgt Acc: 24.878%
	I - Batch: 2450 | Loss: 1.379 | Acc: 27.510% | Wgt Acc: 24.811%
	I - Batch: 2500 | Loss: 1.379 | Acc: 27.400% | Wgt Acc: 24.698%
I - num batch: 2547
I - Train -- Loss: 1.380 | Acc: 27.248% | Wgt Acc: 24.558% | LR: 1.250000e-04 | Dur: 878.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 82.  81. 122.  75.]
 [  0.   0.   0.   0.]
 [615. 497. 612. 463.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.383 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.390 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.389 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.389 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.390 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.390 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.389 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 66.23s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 1.397 | Acc: 26.000% | Wgt Acc: 22.907%
	I - Batch: 100 | Loss: 1.378 | Acc: 28.000% | Wgt Acc: 25.339%
	I - Batch: 150 | Loss: 1.383 | Acc: 28.000% | Wgt Acc: 25.150%
	I - Batch: 200 | Loss: 1.384 | Acc: 28.000% | Wgt Acc: 25.112%
	I - Batch: 250 | Loss: 1.381 | Acc: 30.400% | Wgt Acc: 27.387%
	I - Batch: 300 | Loss: 1.378 | Acc: 30.667% | Wgt Acc: 27.732%
	I - Batch: 350 | Loss: 1.379 | Acc: 30.000% | Wgt Acc: 27.079%
	I - Batch: 400 | Loss: 1.378 | Acc: 30.250% | Wgt Acc: 27.345%
	I - Batch: 450 | Loss: 1.380 | Acc: 30.222% | Wgt Acc: 27.241%
	I - Batch: 500 | Loss: 1.379 | Acc: 30.800% | Wgt Acc: 27.798%
	I - Batch: 550 | Loss: 1.379 | Acc: 31.091% | Wgt Acc: 28.044%
	I - Batch: 600 | Loss: 1.379 | Acc: 30.500% | Wgt Acc: 27.508%
	I - Batch: 650 | Loss: 1.376 | Acc: 31.231% | Wgt Acc: 28.263%
	I - Batch: 700 | Loss: 1.377 | Acc: 30.429% | Wgt Acc: 27.528%
	I - Batch: 750 | Loss: 1.377 | Acc: 30.000% | Wgt Acc: 27.133%
	I - Batch: 800 | Loss: 1.377 | Acc: 29.875% | Wgt Acc: 27.021%
	I - Batch: 850 | Loss: 1.378 | Acc: 29.412% | Wgt Acc: 26.575%
	I - Batch: 900 | Loss: 1.378 | Acc: 29.111% | Wgt Acc: 26.312%
	I - Batch: 950 | Loss: 1.376 | Acc: 29.368% | Wgt Acc: 26.584%
	I - Batch: 1000 | Loss: 1.376 | Acc: 29.300% | Wgt Acc: 26.522%
	I - Batch: 1050 | Loss: 1.376 | Acc: 29.048% | Wgt Acc: 26.299%
	I - Batch: 1100 | Loss: 1.377 | Acc: 28.727% | Wgt Acc: 25.976%
	I - Batch: 1150 | Loss: 1.378 | Acc: 28.348% | Wgt Acc: 25.609%
	I - Batch: 1200 | Loss: 1.377 | Acc: 28.500% | Wgt Acc: 25.763%
	I - Batch: 1250 | Loss: 1.378 | Acc: 28.160% | Wgt Acc: 25.434%
	I - Batch: 1300 | Loss: 1.378 | Acc: 28.462% | Wgt Acc: 25.699%
	I - Batch: 1350 | Loss: 1.378 | Acc: 28.370% | Wgt Acc: 25.619%
	I - Batch: 1400 | Loss: 1.379 | Acc: 28.071% | Wgt Acc: 25.338%
	I - Batch: 1450 | Loss: 1.378 | Acc: 28.414% | Wgt Acc: 25.674%
	I - Batch: 1500 | Loss: 1.377 | Acc: 28.733% | Wgt Acc: 25.972%
	I - Batch: 1550 | Loss: 1.378 | Acc: 28.710% | Wgt Acc: 25.940%
	I - Batch: 1600 | Loss: 1.377 | Acc: 28.812% | Wgt Acc: 26.034%
	I - Batch: 1650 | Loss: 1.378 | Acc: 28.788% | Wgt Acc: 25.999%
	I - Batch: 1700 | Loss: 1.378 | Acc: 28.882% | Wgt Acc: 26.082%
	I - Batch: 1750 | Loss: 1.378 | Acc: 28.686% | Wgt Acc: 25.903%
	I - Batch: 1800 | Loss: 1.379 | Acc: 28.611% | Wgt Acc: 25.815%
	I - Batch: 1850 | Loss: 1.379 | Acc: 28.541% | Wgt Acc: 25.740%
	I - Batch: 1900 | Loss: 1.379 | Acc: 28.263% | Wgt Acc: 25.483%
	I - Batch: 1950 | Loss: 1.379 | Acc: 28.308% | Wgt Acc: 25.517%
	I - Batch: 2000 | Loss: 1.380 | Acc: 28.250% | Wgt Acc: 25.453%
	I - Batch: 2050 | Loss: 1.380 | Acc: 28.146% | Wgt Acc: 25.349%
	I - Batch: 2100 | Loss: 1.380 | Acc: 28.190% | Wgt Acc: 25.380%
	I - Batch: 2150 | Loss: 1.381 | Acc: 28.140% | Wgt Acc: 25.330%
	I - Batch: 2200 | Loss: 1.381 | Acc: 27.955% | Wgt Acc: 25.159%
	I - Batch: 2250 | Loss: 1.380 | Acc: 28.178% | Wgt Acc: 25.373%
	I - Batch: 2300 | Loss: 1.380 | Acc: 28.304% | Wgt Acc: 25.492%
	I - Batch: 2350 | Loss: 1.381 | Acc: 28.298% | Wgt Acc: 25.474%
	I - Batch: 2400 | Loss: 1.380 | Acc: 28.333% | Wgt Acc: 25.523%
	I - Batch: 2450 | Loss: 1.380 | Acc: 28.490% | Wgt Acc: 25.669%
	I - Batch: 2500 | Loss: 1.380 | Acc: 28.560% | Wgt Acc: 25.737%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.622% | Wgt Acc: 25.796% | LR: 1.250000e-04 | Dur: 892.43s
I - Confusion Matrix: [row->prediction - col->label]
[[323. 254. 328. 229.]
 [  0.   0.   0.   0.]
 [374. 324. 406. 309.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.384 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.392 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.390 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.392 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.393 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.394 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.393 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 65.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 1.388 | Acc: 32.000% | Wgt Acc: 28.444%
	I - Batch: 100 | Loss: 1.370 | Acc: 35.000% | Wgt Acc: 31.818%
	I - Batch: 150 | Loss: 1.363 | Acc: 36.000% | Wgt Acc: 32.977%
	I - Batch: 200 | Loss: 1.365 | Acc: 33.500% | Wgt Acc: 30.629%
	I - Batch: 250 | Loss: 1.370 | Acc: 31.600% | Wgt Acc: 28.753%
	I - Batch: 300 | Loss: 1.371 | Acc: 29.667% | Wgt Acc: 26.990%
	I - Batch: 350 | Loss: 1.372 | Acc: 31.429% | Wgt Acc: 28.553%
	I - Batch: 400 | Loss: 1.372 | Acc: 30.750% | Wgt Acc: 27.939%
	I - Batch: 450 | Loss: 1.373 | Acc: 30.444% | Wgt Acc: 27.607%
	I - Batch: 500 | Loss: 1.373 | Acc: 30.800% | Wgt Acc: 27.937%
	I - Batch: 550 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 27.127%
	I - Batch: 600 | Loss: 1.375 | Acc: 30.333% | Wgt Acc: 27.430%
	I - Batch: 650 | Loss: 1.376 | Acc: 30.154% | Wgt Acc: 27.251%
	I - Batch: 700 | Loss: 1.376 | Acc: 29.571% | Wgt Acc: 26.718%
	I - Batch: 750 | Loss: 1.377 | Acc: 29.867% | Wgt Acc: 26.955%
	I - Batch: 800 | Loss: 1.377 | Acc: 29.875% | Wgt Acc: 26.960%
	I - Batch: 850 | Loss: 1.377 | Acc: 29.529% | Wgt Acc: 26.674%
	I - Batch: 900 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.152%
	I - Batch: 950 | Loss: 1.378 | Acc: 29.158% | Wgt Acc: 26.300%
	I - Batch: 1000 | Loss: 1.379 | Acc: 29.100% | Wgt Acc: 26.234%
	I - Batch: 1050 | Loss: 1.379 | Acc: 29.048% | Wgt Acc: 26.175%
	I - Batch: 1100 | Loss: 1.380 | Acc: 28.818% | Wgt Acc: 25.941%
	I - Batch: 1150 | Loss: 1.380 | Acc: 28.783% | Wgt Acc: 25.900%
	I - Batch: 1200 | Loss: 1.380 | Acc: 28.667% | Wgt Acc: 25.806%
	I - Batch: 1250 | Loss: 1.380 | Acc: 28.480% | Wgt Acc: 25.639%
	I - Batch: 1300 | Loss: 1.380 | Acc: 28.692% | Wgt Acc: 25.844%
	I - Batch: 1350 | Loss: 1.379 | Acc: 28.667% | Wgt Acc: 25.834%
	I - Batch: 1400 | Loss: 1.379 | Acc: 28.786% | Wgt Acc: 25.946%
	I - Batch: 1450 | Loss: 1.379 | Acc: 28.828% | Wgt Acc: 25.987%
	I - Batch: 1500 | Loss: 1.379 | Acc: 28.667% | Wgt Acc: 25.837%
	I - Batch: 1550 | Loss: 1.379 | Acc: 28.710% | Wgt Acc: 25.876%
	I - Batch: 1600 | Loss: 1.378 | Acc: 29.125% | Wgt Acc: 26.272%
	I - Batch: 1650 | Loss: 1.379 | Acc: 28.606% | Wgt Acc: 25.778%
	I - Batch: 1700 | Loss: 1.379 | Acc: 28.765% | Wgt Acc: 25.935%
	I - Batch: 1750 | Loss: 1.378 | Acc: 28.800% | Wgt Acc: 25.979%
	I - Batch: 1800 | Loss: 1.378 | Acc: 29.056% | Wgt Acc: 26.209%
	I - Batch: 1850 | Loss: 1.378 | Acc: 29.027% | Wgt Acc: 26.189%
	I - Batch: 1900 | Loss: 1.379 | Acc: 28.895% | Wgt Acc: 26.050%
	I - Batch: 1950 | Loss: 1.378 | Acc: 29.077% | Wgt Acc: 26.220%
	I - Batch: 2000 | Loss: 1.378 | Acc: 29.200% | Wgt Acc: 26.339%
	I - Batch: 2050 | Loss: 1.379 | Acc: 29.024% | Wgt Acc: 26.151%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.952% | Wgt Acc: 26.089%
	I - Batch: 2150 | Loss: 1.379 | Acc: 28.930% | Wgt Acc: 26.077%
	I - Batch: 2200 | Loss: 1.379 | Acc: 28.955% | Wgt Acc: 26.101%
	I - Batch: 2250 | Loss: 1.379 | Acc: 29.022% | Wgt Acc: 26.154%
	I - Batch: 2300 | Loss: 1.379 | Acc: 29.000% | Wgt Acc: 26.141%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.936% | Wgt Acc: 26.081%
	I - Batch: 2400 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.148%
	I - Batch: 2450 | Loss: 1.378 | Acc: 29.061% | Wgt Acc: 26.208%
	I - Batch: 2500 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.150%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1109.71s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.393 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.391 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.393 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.396 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 1.384 | Acc: 26.000% | Wgt Acc: 23.214%
	I - Batch: 100 | Loss: 1.383 | Acc: 27.000% | Wgt Acc: 24.161%
	I - Batch: 150 | Loss: 1.391 | Acc: 24.667% | Wgt Acc: 21.991%
	I - Batch: 200 | Loss: 1.381 | Acc: 26.000% | Wgt Acc: 23.397%
	I - Batch: 250 | Loss: 1.382 | Acc: 28.000% | Wgt Acc: 25.157%
	I - Batch: 300 | Loss: 1.380 | Acc: 28.667% | Wgt Acc: 25.787%
	I - Batch: 350 | Loss: 1.380 | Acc: 28.857% | Wgt Acc: 25.964%
	I - Batch: 400 | Loss: 1.379 | Acc: 29.500% | Wgt Acc: 26.562%
	I - Batch: 450 | Loss: 1.380 | Acc: 28.222% | Wgt Acc: 25.413%
	I - Batch: 500 | Loss: 1.379 | Acc: 28.600% | Wgt Acc: 25.789%
	I - Batch: 550 | Loss: 1.377 | Acc: 28.727% | Wgt Acc: 25.923%
	I - Batch: 600 | Loss: 1.373 | Acc: 30.167% | Wgt Acc: 27.341%
	I - Batch: 650 | Loss: 1.374 | Acc: 30.308% | Wgt Acc: 27.447%
	I - Batch: 700 | Loss: 1.373 | Acc: 30.857% | Wgt Acc: 27.970%
	I - Batch: 750 | Loss: 1.374 | Acc: 30.933% | Wgt Acc: 28.002%
	I - Batch: 800 | Loss: 1.374 | Acc: 31.250% | Wgt Acc: 28.281%
	I - Batch: 850 | Loss: 1.374 | Acc: 31.059% | Wgt Acc: 28.100%
	I - Batch: 900 | Loss: 1.375 | Acc: 30.667% | Wgt Acc: 27.711%
	I - Batch: 950 | Loss: 1.377 | Acc: 30.211% | Wgt Acc: 27.255%
	I - Batch: 1000 | Loss: 1.376 | Acc: 30.300% | Wgt Acc: 27.353%
	I - Batch: 1050 | Loss: 1.377 | Acc: 29.905% | Wgt Acc: 26.976%
	I - Batch: 1100 | Loss: 1.376 | Acc: 30.091% | Wgt Acc: 27.148%
	I - Batch: 1150 | Loss: 1.377 | Acc: 30.087% | Wgt Acc: 27.143%
	I - Batch: 1200 | Loss: 1.376 | Acc: 30.083% | Wgt Acc: 27.143%
	I - Batch: 1250 | Loss: 1.376 | Acc: 30.080% | Wgt Acc: 27.128%
	I - Batch: 1300 | Loss: 1.376 | Acc: 30.231% | Wgt Acc: 27.273%
	I - Batch: 1350 | Loss: 1.376 | Acc: 30.148% | Wgt Acc: 27.210%
	I - Batch: 1400 | Loss: 1.376 | Acc: 29.929% | Wgt Acc: 27.002%
	I - Batch: 1450 | Loss: 1.377 | Acc: 29.724% | Wgt Acc: 26.799%
	I - Batch: 1500 | Loss: 1.377 | Acc: 29.667% | Wgt Acc: 26.763%
	I - Batch: 1550 | Loss: 1.377 | Acc: 29.871% | Wgt Acc: 26.942%
	I - Batch: 1600 | Loss: 1.377 | Acc: 29.750% | Wgt Acc: 26.817%
	I - Batch: 1650 | Loss: 1.377 | Acc: 29.576% | Wgt Acc: 26.656%
	I - Batch: 1700 | Loss: 1.377 | Acc: 29.588% | Wgt Acc: 26.660%
	I - Batch: 1750 | Loss: 1.378 | Acc: 29.600% | Wgt Acc: 26.674%
	I - Batch: 1800 | Loss: 1.377 | Acc: 29.778% | Wgt Acc: 26.854%
	I - Batch: 1850 | Loss: 1.377 | Acc: 29.622% | Wgt Acc: 26.702%
	I - Batch: 1900 | Loss: 1.377 | Acc: 29.842% | Wgt Acc: 26.901%
	I - Batch: 1950 | Loss: 1.377 | Acc: 29.692% | Wgt Acc: 26.762%
	I - Batch: 2000 | Loss: 1.378 | Acc: 29.550% | Wgt Acc: 26.619%
	I - Batch: 2050 | Loss: 1.379 | Acc: 29.220% | Wgt Acc: 26.309%
	I - Batch: 2100 | Loss: 1.379 | Acc: 29.190% | Wgt Acc: 26.284%
	I - Batch: 2150 | Loss: 1.379 | Acc: 29.163% | Wgt Acc: 26.256%
	I - Batch: 2200 | Loss: 1.380 | Acc: 28.864% | Wgt Acc: 25.979%
	I - Batch: 2250 | Loss: 1.379 | Acc: 29.067% | Wgt Acc: 26.170%
	I - Batch: 2300 | Loss: 1.379 | Acc: 29.174% | Wgt Acc: 26.275%
	I - Batch: 2350 | Loss: 1.379 | Acc: 29.362% | Wgt Acc: 26.455%
	I - Batch: 2400 | Loss: 1.378 | Acc: 29.208% | Wgt Acc: 26.321%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.980% | Wgt Acc: 26.105%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.920% | Wgt Acc: 26.061%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1392.78s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.393 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.396 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.225%
	I - Batch: 100 | Loss: 1.367 | Acc: 32.000% | Wgt Acc: 29.091%
	I - Batch: 150 | Loss: 1.368 | Acc: 32.000% | Wgt Acc: 29.091%
	I - Batch: 200 | Loss: 1.370 | Acc: 33.500% | Wgt Acc: 30.385%
	I - Batch: 250 | Loss: 1.369 | Acc: 34.400% | Wgt Acc: 31.244%
	I - Batch: 300 | Loss: 1.373 | Acc: 33.667% | Wgt Acc: 30.468%
	I - Batch: 350 | Loss: 1.369 | Acc: 34.286% | Wgt Acc: 31.149%
	I - Batch: 400 | Loss: 1.372 | Acc: 33.250% | Wgt Acc: 30.108%
	I - Batch: 450 | Loss: 1.366 | Acc: 34.889% | Wgt Acc: 31.749%
	I - Batch: 500 | Loss: 1.367 | Acc: 33.800% | Wgt Acc: 30.741%
	I - Batch: 550 | Loss: 1.368 | Acc: 33.636% | Wgt Acc: 30.553%
	I - Batch: 600 | Loss: 1.370 | Acc: 33.333% | Wgt Acc: 30.246%
	I - Batch: 650 | Loss: 1.371 | Acc: 32.923% | Wgt Acc: 29.836%
	I - Batch: 700 | Loss: 1.371 | Acc: 32.857% | Wgt Acc: 29.783%
	I - Batch: 750 | Loss: 1.373 | Acc: 32.000% | Wgt Acc: 28.959%
	I - Batch: 800 | Loss: 1.375 | Acc: 31.250% | Wgt Acc: 28.233%
	I - Batch: 850 | Loss: 1.375 | Acc: 30.941% | Wgt Acc: 27.956%
	I - Batch: 900 | Loss: 1.375 | Acc: 30.778% | Wgt Acc: 27.790%
	I - Batch: 950 | Loss: 1.375 | Acc: 30.842% | Wgt Acc: 27.858%
	I - Batch: 1000 | Loss: 1.375 | Acc: 30.500% | Wgt Acc: 27.521%
	I - Batch: 1050 | Loss: 1.375 | Acc: 30.571% | Wgt Acc: 27.589%
	I - Batch: 1100 | Loss: 1.375 | Acc: 30.636% | Wgt Acc: 27.651%
	I - Batch: 1150 | Loss: 1.376 | Acc: 30.522% | Wgt Acc: 27.524%
	I - Batch: 1200 | Loss: 1.377 | Acc: 30.167% | Wgt Acc: 27.192%
	I - Batch: 1250 | Loss: 1.377 | Acc: 30.480% | Wgt Acc: 27.484%
	I - Batch: 1300 | Loss: 1.375 | Acc: 30.923% | Wgt Acc: 27.917%
	I - Batch: 1350 | Loss: 1.376 | Acc: 30.667% | Wgt Acc: 27.660%
	I - Batch: 1400 | Loss: 1.377 | Acc: 30.500% | Wgt Acc: 27.513%
	I - Batch: 1450 | Loss: 1.377 | Acc: 30.483% | Wgt Acc: 27.492%
	I - Batch: 1500 | Loss: 1.377 | Acc: 30.200% | Wgt Acc: 27.244%
	I - Batch: 1550 | Loss: 1.376 | Acc: 30.323% | Wgt Acc: 27.373%
	I - Batch: 1600 | Loss: 1.377 | Acc: 30.188% | Wgt Acc: 27.238%
	I - Batch: 1650 | Loss: 1.377 | Acc: 30.182% | Wgt Acc: 27.235%
	I - Batch: 1700 | Loss: 1.377 | Acc: 30.000% | Wgt Acc: 27.077%
	I - Batch: 1750 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 27.083%
	I - Batch: 1800 | Loss: 1.377 | Acc: 29.778% | Wgt Acc: 26.884%
	I - Batch: 1850 | Loss: 1.377 | Acc: 29.892% | Wgt Acc: 26.989%
	I - Batch: 1900 | Loss: 1.378 | Acc: 29.632% | Wgt Acc: 26.730%
	I - Batch: 1950 | Loss: 1.377 | Acc: 29.641% | Wgt Acc: 26.747%
	I - Batch: 2000 | Loss: 1.377 | Acc: 29.600% | Wgt Acc: 26.709%
	I - Batch: 2050 | Loss: 1.378 | Acc: 29.561% | Wgt Acc: 26.670%
	I - Batch: 2100 | Loss: 1.378 | Acc: 29.571% | Wgt Acc: 26.667%
	I - Batch: 2150 | Loss: 1.377 | Acc: 29.535% | Wgt Acc: 26.647%
	I - Batch: 2200 | Loss: 1.377 | Acc: 29.545% | Wgt Acc: 26.658%
	I - Batch: 2250 | Loss: 1.378 | Acc: 29.333% | Wgt Acc: 26.458%
	I - Batch: 2300 | Loss: 1.378 | Acc: 29.261% | Wgt Acc: 26.390%
	I - Batch: 2350 | Loss: 1.378 | Acc: 29.149% | Wgt Acc: 26.288%
	I - Batch: 2400 | Loss: 1.378 | Acc: 29.125% | Wgt Acc: 26.266%
	I - Batch: 2450 | Loss: 1.378 | Acc: 29.143% | Wgt Acc: 26.279%
	I - Batch: 2500 | Loss: 1.378 | Acc: 28.960% | Wgt Acc: 26.111%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.857% | Wgt Acc: 26.008% | LR: 1.250000e-04 | Dur: 1390.03s
I - Confusion Matrix: [row->prediction - col->label]
[[  2.   0.   1.   1.]
 [  0.   0.   0.   0.]
 [695. 578. 733. 537.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.383 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.393 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.391 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.392 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.395 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.394 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.394 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 119.68s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 1.382 | Acc: 26.000% | Wgt Acc: 23.214%
	I - Batch: 100 | Loss: 1.371 | Acc: 34.000% | Wgt Acc: 30.769%
	I - Batch: 150 | Loss: 1.376 | Acc: 30.667% | Wgt Acc: 27.586%
	I - Batch: 200 | Loss: 1.379 | Acc: 29.500% | Wgt Acc: 26.517%
	I - Batch: 250 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.157%
	I - Batch: 300 | Loss: 1.383 | Acc: 27.667% | Wgt Acc: 24.813%
	I - Batch: 350 | Loss: 1.382 | Acc: 29.429% | Wgt Acc: 26.427%
	I - Batch: 400 | Loss: 1.380 | Acc: 30.250% | Wgt Acc: 27.191%
	I - Batch: 450 | Loss: 1.381 | Acc: 30.222% | Wgt Acc: 27.173%
	I - Batch: 500 | Loss: 1.381 | Acc: 30.000% | Wgt Acc: 26.966%
	I - Batch: 550 | Loss: 1.379 | Acc: 31.273% | Wgt Acc: 28.139%
	I - Batch: 600 | Loss: 1.379 | Acc: 31.333% | Wgt Acc: 28.196%
	I - Batch: 650 | Loss: 1.379 | Acc: 31.846% | Wgt Acc: 28.641%
	I - Batch: 700 | Loss: 1.379 | Acc: 31.429% | Wgt Acc: 28.269%
	I - Batch: 750 | Loss: 1.381 | Acc: 30.400% | Wgt Acc: 27.297%
	I - Batch: 800 | Loss: 1.380 | Acc: 30.625% | Wgt Acc: 27.536%
	I - Batch: 850 | Loss: 1.379 | Acc: 30.824% | Wgt Acc: 27.732%
	I - Batch: 900 | Loss: 1.379 | Acc: 30.778% | Wgt Acc: 27.707%
	I - Batch: 950 | Loss: 1.378 | Acc: 31.158% | Wgt Acc: 28.050%
	I - Batch: 1000 | Loss: 1.379 | Acc: 30.800% | Wgt Acc: 27.729%
	I - Batch: 1050 | Loss: 1.380 | Acc: 30.381% | Wgt Acc: 27.335%
	I - Batch: 1100 | Loss: 1.378 | Acc: 30.909% | Wgt Acc: 27.840%
	I - Batch: 1150 | Loss: 1.378 | Acc: 30.783% | Wgt Acc: 27.738%
	I - Batch: 1200 | Loss: 1.377 | Acc: 31.167% | Wgt Acc: 28.099%
	I - Batch: 1250 | Loss: 1.377 | Acc: 30.960% | Wgt Acc: 27.932%
	I - Batch: 1300 | Loss: 1.377 | Acc: 30.846% | Wgt Acc: 27.838%
	I - Batch: 1350 | Loss: 1.376 | Acc: 30.667% | Wgt Acc: 27.706%
	I - Batch: 1400 | Loss: 1.376 | Acc: 30.714% | Wgt Acc: 27.742%
	I - Batch: 1450 | Loss: 1.376 | Acc: 30.207% | Wgt Acc: 27.281%
	I - Batch: 1500 | Loss: 1.377 | Acc: 30.000% | Wgt Acc: 27.080%
	I - Batch: 1550 | Loss: 1.377 | Acc: 29.742% | Wgt Acc: 26.845%
	I - Batch: 1600 | Loss: 1.378 | Acc: 29.250% | Wgt Acc: 26.385%
	I - Batch: 1650 | Loss: 1.379 | Acc: 29.152% | Wgt Acc: 26.281%
	I - Batch: 1700 | Loss: 1.379 | Acc: 28.941% | Wgt Acc: 26.083%
	I - Batch: 1750 | Loss: 1.380 | Acc: 28.800% | Wgt Acc: 25.943%
	I - Batch: 1800 | Loss: 1.379 | Acc: 28.667% | Wgt Acc: 25.823%
	I - Batch: 1850 | Loss: 1.379 | Acc: 28.703% | Wgt Acc: 25.877%
	I - Batch: 1900 | Loss: 1.378 | Acc: 28.737% | Wgt Acc: 25.917%
	I - Batch: 1950 | Loss: 1.379 | Acc: 28.667% | Wgt Acc: 25.847%
	I - Batch: 2000 | Loss: 1.378 | Acc: 28.600% | Wgt Acc: 25.795%
	I - Batch: 2050 | Loss: 1.379 | Acc: 28.585% | Wgt Acc: 25.775%
	I - Batch: 2100 | Loss: 1.378 | Acc: 28.381% | Wgt Acc: 25.596%
	I - Batch: 2150 | Loss: 1.378 | Acc: 28.186% | Wgt Acc: 25.422%
	I - Batch: 2200 | Loss: 1.378 | Acc: 28.273% | Wgt Acc: 25.507%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.089% | Wgt Acc: 25.326%
	I - Batch: 2300 | Loss: 1.379 | Acc: 27.913% | Wgt Acc: 25.162%
	I - Batch: 2350 | Loss: 1.379 | Acc: 27.830% | Wgt Acc: 25.084%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.042% | Wgt Acc: 25.284%
	I - Batch: 2450 | Loss: 1.378 | Acc: 27.918% | Wgt Acc: 25.182%
	I - Batch: 2500 | Loss: 1.379 | Acc: 27.760% | Wgt Acc: 25.029%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 27.640% | Wgt Acc: 24.912% | LR: 1.250000e-04 | Dur: 1391.71s
I - Confusion Matrix: [row->prediction - col->label]
[[107.  96. 137.  78.]
 [  0.   0.   0.   0.]
 [590. 482. 597. 460.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.384 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.397 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.395 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.397 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.400 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.399 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.399 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 121.34s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 1.373 | Acc: 30.000% | Wgt Acc: 27.027%
	I - Batch: 100 | Loss: 1.361 | Acc: 37.000% | Wgt Acc: 33.636%
	I - Batch: 150 | Loss: 1.373 | Acc: 33.333% | Wgt Acc: 30.075%
	I - Batch: 200 | Loss: 1.374 | Acc: 33.500% | Wgt Acc: 30.248%
	I - Batch: 250 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 27.051%
	I - Batch: 300 | Loss: 1.378 | Acc: 28.333% | Wgt Acc: 25.526%
	I - Batch: 350 | Loss: 1.382 | Acc: 28.000% | Wgt Acc: 25.177%
	I - Batch: 400 | Loss: 1.384 | Acc: 26.750% | Wgt Acc: 23.991%
	I - Batch: 450 | Loss: 1.382 | Acc: 27.333% | Wgt Acc: 24.575%
	I - Batch: 500 | Loss: 1.385 | Acc: 26.800% | Wgt Acc: 24.047%
	I - Batch: 550 | Loss: 1.385 | Acc: 26.364% | Wgt Acc: 23.664%
	I - Batch: 600 | Loss: 1.384 | Acc: 26.667% | Wgt Acc: 23.934%
	I - Batch: 650 | Loss: 1.385 | Acc: 27.077% | Wgt Acc: 24.293%
	I - Batch: 700 | Loss: 1.385 | Acc: 27.857% | Wgt Acc: 25.000%
	I - Batch: 750 | Loss: 1.384 | Acc: 28.133% | Wgt Acc: 25.269%
	I - Batch: 800 | Loss: 1.385 | Acc: 28.250% | Wgt Acc: 25.343%
	I - Batch: 850 | Loss: 1.385 | Acc: 28.118% | Wgt Acc: 25.231%
	I - Batch: 900 | Loss: 1.385 | Acc: 28.111% | Wgt Acc: 25.224%
	I - Batch: 950 | Loss: 1.385 | Acc: 27.789% | Wgt Acc: 24.917%
	I - Batch: 1000 | Loss: 1.386 | Acc: 27.900% | Wgt Acc: 25.011%
	I - Batch: 1050 | Loss: 1.386 | Acc: 27.619% | Wgt Acc: 24.760%
	I - Batch: 1100 | Loss: 1.385 | Acc: 27.636% | Wgt Acc: 24.781%
	I - Batch: 1150 | Loss: 1.386 | Acc: 27.130% | Wgt Acc: 24.313%
	I - Batch: 1200 | Loss: 1.385 | Acc: 27.250% | Wgt Acc: 24.449%
	I - Batch: 1250 | Loss: 1.384 | Acc: 27.280% | Wgt Acc: 24.493%
	I - Batch: 1300 | Loss: 1.384 | Acc: 27.077% | Wgt Acc: 24.322%
	I - Batch: 1350 | Loss: 1.383 | Acc: 27.185% | Wgt Acc: 24.434%
	I - Batch: 1400 | Loss: 1.382 | Acc: 27.429% | Wgt Acc: 24.667%
	I - Batch: 1450 | Loss: 1.383 | Acc: 27.172% | Wgt Acc: 24.419%
	I - Batch: 1500 | Loss: 1.382 | Acc: 27.467% | Wgt Acc: 24.715%
	I - Batch: 1550 | Loss: 1.382 | Acc: 27.290% | Wgt Acc: 24.547%
	I - Batch: 1600 | Loss: 1.382 | Acc: 27.062% | Wgt Acc: 24.340%
	I - Batch: 1650 | Loss: 1.382 | Acc: 27.273% | Wgt Acc: 24.537%
	I - Batch: 1700 | Loss: 1.382 | Acc: 27.294% | Wgt Acc: 24.541%
	I - Batch: 1750 | Loss: 1.383 | Acc: 27.086% | Wgt Acc: 24.345%
	I - Batch: 1800 | Loss: 1.383 | Acc: 26.778% | Wgt Acc: 24.076%
	I - Batch: 1850 | Loss: 1.383 | Acc: 26.865% | Wgt Acc: 24.150%
	I - Batch: 1900 | Loss: 1.383 | Acc: 26.947% | Wgt Acc: 24.222%
	I - Batch: 1950 | Loss: 1.382 | Acc: 26.974% | Wgt Acc: 24.251%
	I - Batch: 2000 | Loss: 1.383 | Acc: 26.800% | Wgt Acc: 24.090%
	I - Batch: 2050 | Loss: 1.383 | Acc: 26.634% | Wgt Acc: 23.934%
	I - Batch: 2100 | Loss: 1.382 | Acc: 26.857% | Wgt Acc: 24.154%
	I - Batch: 2150 | Loss: 1.381 | Acc: 27.209% | Wgt Acc: 24.490%
	I - Batch: 2200 | Loss: 1.381 | Acc: 27.364% | Wgt Acc: 24.642%
	I - Batch: 2250 | Loss: 1.380 | Acc: 27.511% | Wgt Acc: 24.772%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.240%
	I - Batch: 2350 | Loss: 1.378 | Acc: 28.426% | Wgt Acc: 25.636%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.375% | Wgt Acc: 25.573%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.367% | Wgt Acc: 25.573%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.360% | Wgt Acc: 25.566%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.347% | Wgt Acc: 25.548% | LR: 1.250000e-04 | Dur: 1401.39s
I - Confusion Matrix: [row->prediction - col->label]
[[399. 331. 411. 331.]
 [  0.   0.   0.   0.]
 [298. 247. 323. 207.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.398 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.396 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.400 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.402 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.403 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.401 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 123.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 1.373 | Acc: 34.000% | Wgt Acc: 30.631%
	I - Batch: 100 | Loss: 1.391 | Acc: 31.000% | Wgt Acc: 27.617%
	I - Batch: 150 | Loss: 1.374 | Acc: 33.333% | Wgt Acc: 30.075%
	I - Batch: 200 | Loss: 1.372 | Acc: 33.000% | Wgt Acc: 29.831%
	I - Batch: 250 | Loss: 1.376 | Acc: 32.400% | Wgt Acc: 29.189%
	I - Batch: 300 | Loss: 1.367 | Acc: 33.000% | Wgt Acc: 29.955%
	I - Batch: 350 | Loss: 1.365 | Acc: 33.143% | Wgt Acc: 30.130%
	I - Batch: 400 | Loss: 1.370 | Acc: 32.500% | Wgt Acc: 29.445%
	I - Batch: 450 | Loss: 1.368 | Acc: 32.889% | Wgt Acc: 29.809%
	I - Batch: 500 | Loss: 1.370 | Acc: 33.000% | Wgt Acc: 29.864%
	I - Batch: 550 | Loss: 1.367 | Acc: 33.273% | Wgt Acc: 30.173%
	I - Batch: 600 | Loss: 1.368 | Acc: 32.833% | Wgt Acc: 29.770%
	I - Batch: 650 | Loss: 1.373 | Acc: 31.385% | Wgt Acc: 28.353%
	I - Batch: 700 | Loss: 1.374 | Acc: 31.143% | Wgt Acc: 28.138%
	I - Batch: 750 | Loss: 1.374 | Acc: 30.400% | Wgt Acc: 27.462%
	I - Batch: 800 | Loss: 1.376 | Acc: 29.875% | Wgt Acc: 26.975%
	I - Batch: 850 | Loss: 1.376 | Acc: 29.765% | Wgt Acc: 26.872%
	I - Batch: 900 | Loss: 1.377 | Acc: 30.000% | Wgt Acc: 27.074%
	I - Batch: 950 | Loss: 1.378 | Acc: 30.105% | Wgt Acc: 27.141%
	I - Batch: 1000 | Loss: 1.379 | Acc: 29.900% | Wgt Acc: 26.925%
	I - Batch: 1050 | Loss: 1.378 | Acc: 30.095% | Wgt Acc: 27.119%
	I - Batch: 1100 | Loss: 1.379 | Acc: 30.000% | Wgt Acc: 27.021%
	I - Batch: 1150 | Loss: 1.379 | Acc: 29.913% | Wgt Acc: 26.949%
	I - Batch: 1200 | Loss: 1.378 | Acc: 30.333% | Wgt Acc: 27.348%
	I - Batch: 1250 | Loss: 1.377 | Acc: 30.160% | Wgt Acc: 27.210%
	I - Batch: 1300 | Loss: 1.376 | Acc: 30.154% | Wgt Acc: 27.236%
	I - Batch: 1350 | Loss: 1.378 | Acc: 29.926% | Wgt Acc: 26.996%
	I - Batch: 1400 | Loss: 1.377 | Acc: 30.143% | Wgt Acc: 27.191%
	I - Batch: 1450 | Loss: 1.377 | Acc: 29.931% | Wgt Acc: 26.994%
	I - Batch: 1500 | Loss: 1.377 | Acc: 30.067% | Wgt Acc: 27.124%
	I - Batch: 1550 | Loss: 1.378 | Acc: 29.677% | Wgt Acc: 26.756%
	I - Batch: 1600 | Loss: 1.378 | Acc: 29.438% | Wgt Acc: 26.550%
	I - Batch: 1650 | Loss: 1.379 | Acc: 29.212% | Wgt Acc: 26.339%
	I - Batch: 1700 | Loss: 1.379 | Acc: 29.412% | Wgt Acc: 26.522%
	I - Batch: 1750 | Loss: 1.379 | Acc: 29.257% | Wgt Acc: 26.382%
	I - Batch: 1800 | Loss: 1.379 | Acc: 29.333% | Wgt Acc: 26.446%
	I - Batch: 1850 | Loss: 1.379 | Acc: 29.297% | Wgt Acc: 26.407%
	I - Batch: 1900 | Loss: 1.379 | Acc: 29.368% | Wgt Acc: 26.483%
	I - Batch: 1950 | Loss: 1.379 | Acc: 29.385% | Wgt Acc: 26.491%
	I - Batch: 2000 | Loss: 1.380 | Acc: 29.300% | Wgt Acc: 26.405%
	I - Batch: 2050 | Loss: 1.379 | Acc: 29.463% | Wgt Acc: 26.558%
	I - Batch: 2100 | Loss: 1.379 | Acc: 29.381% | Wgt Acc: 26.486%
	I - Batch: 2150 | Loss: 1.380 | Acc: 29.256% | Wgt Acc: 26.368%
	I - Batch: 2200 | Loss: 1.380 | Acc: 29.273% | Wgt Acc: 26.385%
	I - Batch: 2250 | Loss: 1.379 | Acc: 29.022% | Wgt Acc: 26.167%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.696% | Wgt Acc: 25.870%
	I - Batch: 2350 | Loss: 1.380 | Acc: 28.553% | Wgt Acc: 25.731%
	I - Batch: 2400 | Loss: 1.380 | Acc: 28.417% | Wgt Acc: 25.603%
	I - Batch: 2450 | Loss: 1.380 | Acc: 28.531% | Wgt Acc: 25.708%
	I - Batch: 2500 | Loss: 1.380 | Acc: 28.600% | Wgt Acc: 25.770%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.583% | Wgt Acc: 25.761% | LR: 1.250000e-04 | Dur: 1395.33s
I - Confusion Matrix: [row->prediction - col->label]
[[  1.   5.   7.   3.]
 [  0.   0.   0.   0.]
 [696. 573. 727. 535.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.397 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.395 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.399 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.402 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.401 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.401 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 1.356 | Acc: 26.000% | Wgt Acc: 23.853%
	I - Batch: 100 | Loss: 1.369 | Acc: 19.000% | Wgt Acc: 17.234%
	I - Batch: 150 | Loss: 1.382 | Acc: 20.667% | Wgt Acc: 18.619%
	I - Batch: 200 | Loss: 1.384 | Acc: 22.500% | Wgt Acc: 20.225%
	I - Batch: 250 | Loss: 1.387 | Acc: 22.800% | Wgt Acc: 20.467%
	I - Batch: 300 | Loss: 1.387 | Acc: 22.667% | Wgt Acc: 20.314%
	I - Batch: 350 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.511%
	I - Batch: 400 | Loss: 1.385 | Acc: 25.500% | Wgt Acc: 22.896%
	I - Batch: 450 | Loss: 1.388 | Acc: 25.556% | Wgt Acc: 22.908%
	I - Batch: 500 | Loss: 1.387 | Acc: 25.600% | Wgt Acc: 22.970%
	I - Batch: 550 | Loss: 1.386 | Acc: 25.818% | Wgt Acc: 23.184%
	I - Batch: 600 | Loss: 1.384 | Acc: 26.500% | Wgt Acc: 23.820%
	I - Batch: 650 | Loss: 1.382 | Acc: 27.538% | Wgt Acc: 24.784%
	I - Batch: 700 | Loss: 1.384 | Acc: 27.000% | Wgt Acc: 24.277%
	I - Batch: 750 | Loss: 1.382 | Acc: 27.600% | Wgt Acc: 24.850%
	I - Batch: 800 | Loss: 1.383 | Acc: 27.125% | Wgt Acc: 24.389%
	I - Batch: 850 | Loss: 1.382 | Acc: 26.824% | Wgt Acc: 24.140%
	I - Batch: 900 | Loss: 1.383 | Acc: 26.667% | Wgt Acc: 23.994%
	I - Batch: 950 | Loss: 1.382 | Acc: 26.737% | Wgt Acc: 24.053%
	I - Batch: 1000 | Loss: 1.383 | Acc: 26.600% | Wgt Acc: 23.926%
	I - Batch: 1050 | Loss: 1.382 | Acc: 26.571% | Wgt Acc: 23.907%
	I - Batch: 1100 | Loss: 1.382 | Acc: 27.182% | Wgt Acc: 24.463%
	I - Batch: 1150 | Loss: 1.382 | Acc: 27.043% | Wgt Acc: 24.330%
	I - Batch: 1200 | Loss: 1.382 | Acc: 26.917% | Wgt Acc: 24.217%
	I - Batch: 1250 | Loss: 1.383 | Acc: 26.640% | Wgt Acc: 23.957%
	I - Batch: 1300 | Loss: 1.383 | Acc: 26.538% | Wgt Acc: 23.851%
	I - Batch: 1350 | Loss: 1.384 | Acc: 26.222% | Wgt Acc: 23.557%
	I - Batch: 1400 | Loss: 1.384 | Acc: 26.214% | Wgt Acc: 23.548%
	I - Batch: 1450 | Loss: 1.384 | Acc: 26.207% | Wgt Acc: 23.544%
	I - Batch: 1500 | Loss: 1.383 | Acc: 26.267% | Wgt Acc: 23.600%
	I - Batch: 1550 | Loss: 1.383 | Acc: 26.194% | Wgt Acc: 23.529%
	I - Batch: 1600 | Loss: 1.384 | Acc: 26.250% | Wgt Acc: 23.566%
	I - Batch: 1650 | Loss: 1.383 | Acc: 26.424% | Wgt Acc: 23.751%
	I - Batch: 1700 | Loss: 1.382 | Acc: 26.588% | Wgt Acc: 23.909%
	I - Batch: 1750 | Loss: 1.382 | Acc: 26.800% | Wgt Acc: 24.098%
	I - Batch: 1800 | Loss: 1.383 | Acc: 26.611% | Wgt Acc: 23.923%
	I - Batch: 1850 | Loss: 1.382 | Acc: 26.811% | Wgt Acc: 24.113%
	I - Batch: 1900 | Loss: 1.382 | Acc: 27.000% | Wgt Acc: 24.293%
	I - Batch: 1950 | Loss: 1.381 | Acc: 26.923% | Wgt Acc: 24.224%
	I - Batch: 2000 | Loss: 1.381 | Acc: 26.850% | Wgt Acc: 24.170%
	I - Batch: 2050 | Loss: 1.381 | Acc: 26.634% | Wgt Acc: 23.976%
	I - Batch: 2100 | Loss: 1.381 | Acc: 26.762% | Wgt Acc: 24.092%
	I - Batch: 2150 | Loss: 1.381 | Acc: 27.070% | Wgt Acc: 24.372%
	I - Batch: 2200 | Loss: 1.381 | Acc: 27.136% | Wgt Acc: 24.427%
	I - Batch: 2250 | Loss: 1.381 | Acc: 27.200% | Wgt Acc: 24.487%
	I - Batch: 2300 | Loss: 1.381 | Acc: 27.217% | Wgt Acc: 24.501%
	I - Batch: 2350 | Loss: 1.381 | Acc: 27.149% | Wgt Acc: 24.440%
	I - Batch: 2400 | Loss: 1.380 | Acc: 27.333% | Wgt Acc: 24.615%
	I - Batch: 2450 | Loss: 1.380 | Acc: 27.551% | Wgt Acc: 24.821%
	I - Batch: 2500 | Loss: 1.380 | Acc: 27.600% | Wgt Acc: 24.869%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 27.719% | Wgt Acc: 24.982% | LR: 1.250000e-04 | Dur: 1401.59s
I - Confusion Matrix: [row->prediction - col->label]
[[131. 133. 159. 113.]
 [  0.   0.   0.   0.]
 [566. 445. 575. 425.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.398 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.396 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.400 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.402 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.403 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.401 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 1.361 | Acc: 22.000% | Wgt Acc: 20.183%
	I - Batch: 100 | Loss: 1.371 | Acc: 22.000% | Wgt Acc: 20.000%
	I - Batch: 150 | Loss: 1.366 | Acc: 28.667% | Wgt Acc: 26.140%
	I - Batch: 200 | Loss: 1.367 | Acc: 30.000% | Wgt Acc: 27.304%
	I - Batch: 250 | Loss: 1.366 | Acc: 30.400% | Wgt Acc: 27.687%
	I - Batch: 300 | Loss: 1.373 | Acc: 31.000% | Wgt Acc: 28.075%
	I - Batch: 350 | Loss: 1.365 | Acc: 32.571% | Wgt Acc: 29.687%
	I - Batch: 400 | Loss: 1.372 | Acc: 31.500% | Wgt Acc: 28.555%
	I - Batch: 450 | Loss: 1.374 | Acc: 30.889% | Wgt Acc: 27.940%
	I - Batch: 500 | Loss: 1.376 | Acc: 31.000% | Wgt Acc: 27.966%
	I - Batch: 550 | Loss: 1.369 | Acc: 33.273% | Wgt Acc: 30.123%
	I - Batch: 600 | Loss: 1.371 | Acc: 32.500% | Wgt Acc: 29.412%
	I - Batch: 650 | Loss: 1.370 | Acc: 32.462% | Wgt Acc: 29.397%
	I - Batch: 700 | Loss: 1.373 | Acc: 31.714% | Wgt Acc: 28.673%
	I - Batch: 750 | Loss: 1.376 | Acc: 30.800% | Wgt Acc: 27.806%
	I - Batch: 800 | Loss: 1.377 | Acc: 30.750% | Wgt Acc: 27.750%
	I - Batch: 850 | Loss: 1.377 | Acc: 30.824% | Wgt Acc: 27.791%
	I - Batch: 900 | Loss: 1.380 | Acc: 29.778% | Wgt Acc: 26.820%
	I - Batch: 950 | Loss: 1.379 | Acc: 29.789% | Wgt Acc: 26.856%
	I - Batch: 1000 | Loss: 1.378 | Acc: 30.100% | Wgt Acc: 27.148%
	I - Batch: 1050 | Loss: 1.379 | Acc: 29.714% | Wgt Acc: 26.781%
	I - Batch: 1100 | Loss: 1.379 | Acc: 29.727% | Wgt Acc: 26.781%
	I - Batch: 1150 | Loss: 1.378 | Acc: 29.652% | Wgt Acc: 26.745%
	I - Batch: 1200 | Loss: 1.378 | Acc: 29.667% | Wgt Acc: 26.762%
	I - Batch: 1250 | Loss: 1.378 | Acc: 29.440% | Wgt Acc: 26.556%
	I - Batch: 1300 | Loss: 1.378 | Acc: 29.538% | Wgt Acc: 26.662%
	I - Batch: 1350 | Loss: 1.378 | Acc: 29.185% | Wgt Acc: 26.346%
	I - Batch: 1400 | Loss: 1.377 | Acc: 29.429% | Wgt Acc: 26.585%
	I - Batch: 1450 | Loss: 1.378 | Acc: 29.310% | Wgt Acc: 26.463%
	I - Batch: 1500 | Loss: 1.378 | Acc: 29.533% | Wgt Acc: 26.663%
	I - Batch: 1550 | Loss: 1.377 | Acc: 29.613% | Wgt Acc: 26.740%
	I - Batch: 1600 | Loss: 1.377 | Acc: 29.688% | Wgt Acc: 26.810%
	I - Batch: 1650 | Loss: 1.377 | Acc: 29.818% | Wgt Acc: 26.918%
	I - Batch: 1700 | Loss: 1.378 | Acc: 29.471% | Wgt Acc: 26.603%
	I - Batch: 1750 | Loss: 1.377 | Acc: 29.657% | Wgt Acc: 26.794%
	I - Batch: 1800 | Loss: 1.378 | Acc: 29.278% | Wgt Acc: 26.433%
	I - Batch: 1850 | Loss: 1.377 | Acc: 29.514% | Wgt Acc: 26.660%
	I - Batch: 1900 | Loss: 1.376 | Acc: 29.684% | Wgt Acc: 26.822%
	I - Batch: 1950 | Loss: 1.378 | Acc: 29.385% | Wgt Acc: 26.528%
	I - Batch: 2000 | Loss: 1.377 | Acc: 29.550% | Wgt Acc: 26.682%
	I - Batch: 2050 | Loss: 1.378 | Acc: 29.317% | Wgt Acc: 26.464%
	I - Batch: 2100 | Loss: 1.378 | Acc: 29.095% | Wgt Acc: 26.268%
	I - Batch: 2150 | Loss: 1.378 | Acc: 28.930% | Wgt Acc: 26.110%
	I - Batch: 2200 | Loss: 1.378 | Acc: 28.864% | Wgt Acc: 26.043%
	I - Batch: 2250 | Loss: 1.378 | Acc: 28.978% | Wgt Acc: 26.153%
	I - Batch: 2300 | Loss: 1.378 | Acc: 28.913% | Wgt Acc: 26.099%
	I - Batch: 2350 | Loss: 1.378 | Acc: 29.064% | Wgt Acc: 26.234%
	I - Batch: 2400 | Loss: 1.378 | Acc: 29.125% | Wgt Acc: 26.288%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.980% | Wgt Acc: 26.134%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.840% | Wgt Acc: 25.994%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.700% | Wgt Acc: 25.867% | LR: 1.250000e-04 | Dur: 1390.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.  44.  57.  43.]
 [  0.   0.   0.   0.]
 [643. 534. 677. 495.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.397 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 1.357 | Acc: 34.000% | Wgt Acc: 31.481%
	I - Batch: 100 | Loss: 1.375 | Acc: 32.000% | Wgt Acc: 29.025%
	I - Batch: 150 | Loss: 1.370 | Acc: 34.667% | Wgt Acc: 31.467%
	I - Batch: 200 | Loss: 1.368 | Acc: 32.000% | Wgt Acc: 29.058%
	I - Batch: 250 | Loss: 1.367 | Acc: 31.200% | Wgt Acc: 28.415%
	I - Batch: 300 | Loss: 1.363 | Acc: 32.333% | Wgt Acc: 29.573%
	I - Batch: 350 | Loss: 1.367 | Acc: 30.857% | Wgt Acc: 28.143%
	I - Batch: 400 | Loss: 1.372 | Acc: 30.000% | Wgt Acc: 27.226%
	I - Batch: 450 | Loss: 1.371 | Acc: 30.444% | Wgt Acc: 27.663%
	I - Batch: 500 | Loss: 1.371 | Acc: 29.400% | Wgt Acc: 26.703%
	I - Batch: 550 | Loss: 1.373 | Acc: 28.545% | Wgt Acc: 25.886%
	I - Batch: 600 | Loss: 1.375 | Acc: 28.333% | Wgt Acc: 25.641%
	I - Batch: 650 | Loss: 1.376 | Acc: 28.923% | Wgt Acc: 26.147%
	I - Batch: 700 | Loss: 1.375 | Acc: 29.000% | Wgt Acc: 26.253%
	I - Batch: 750 | Loss: 1.376 | Acc: 28.800% | Wgt Acc: 26.048%
	I - Batch: 800 | Loss: 1.376 | Acc: 28.750% | Wgt Acc: 26.003%
	I - Batch: 850 | Loss: 1.376 | Acc: 28.941% | Wgt Acc: 26.177%
	I - Batch: 900 | Loss: 1.376 | Acc: 29.333% | Wgt Acc: 26.526%
	I - Batch: 950 | Loss: 1.375 | Acc: 29.368% | Wgt Acc: 26.590%
	I - Batch: 1000 | Loss: 1.375 | Acc: 29.400% | Wgt Acc: 26.618%
	I - Batch: 1050 | Loss: 1.377 | Acc: 28.667% | Wgt Acc: 25.904%
	I - Batch: 1100 | Loss: 1.378 | Acc: 28.455% | Wgt Acc: 25.682%
	I - Batch: 1150 | Loss: 1.377 | Acc: 28.348% | Wgt Acc: 25.584%
	I - Batch: 1200 | Loss: 1.377 | Acc: 28.333% | Wgt Acc: 25.574%
	I - Batch: 1250 | Loss: 1.378 | Acc: 28.320% | Wgt Acc: 25.537%
	I - Batch: 1300 | Loss: 1.378 | Acc: 28.846% | Wgt Acc: 26.024%
	I - Batch: 1350 | Loss: 1.377 | Acc: 29.037% | Wgt Acc: 26.203%
	I - Batch: 1400 | Loss: 1.379 | Acc: 28.429% | Wgt Acc: 25.636%
	I - Batch: 1450 | Loss: 1.379 | Acc: 28.345% | Wgt Acc: 25.548%
	I - Batch: 1500 | Loss: 1.379 | Acc: 28.200% | Wgt Acc: 25.421%
	I - Batch: 1550 | Loss: 1.380 | Acc: 28.323% | Wgt Acc: 25.520%
	I - Batch: 1600 | Loss: 1.380 | Acc: 28.500% | Wgt Acc: 25.672%
	I - Batch: 1650 | Loss: 1.379 | Acc: 28.970% | Wgt Acc: 26.120%
	I - Batch: 1700 | Loss: 1.378 | Acc: 29.235% | Wgt Acc: 26.356%
	I - Batch: 1750 | Loss: 1.378 | Acc: 29.314% | Wgt Acc: 26.426%
	I - Batch: 1800 | Loss: 1.379 | Acc: 29.111% | Wgt Acc: 26.233%
	I - Batch: 1850 | Loss: 1.379 | Acc: 28.973% | Wgt Acc: 26.105%
	I - Batch: 1900 | Loss: 1.380 | Acc: 28.737% | Wgt Acc: 25.874%
	I - Batch: 1950 | Loss: 1.379 | Acc: 28.718% | Wgt Acc: 25.875%
	I - Batch: 2000 | Loss: 1.380 | Acc: 28.650% | Wgt Acc: 25.805%
	I - Batch: 2050 | Loss: 1.379 | Acc: 28.537% | Wgt Acc: 25.711%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.524% | Wgt Acc: 25.708%
	I - Batch: 2150 | Loss: 1.379 | Acc: 28.837% | Wgt Acc: 25.996%
	I - Batch: 2200 | Loss: 1.379 | Acc: 28.682% | Wgt Acc: 25.858%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.756% | Wgt Acc: 25.919%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.739% | Wgt Acc: 25.911%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.723% | Wgt Acc: 25.899%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.708% | Wgt Acc: 25.888%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.571% | Wgt Acc: 25.754%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.480% | Wgt Acc: 25.674%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.543% | Wgt Acc: 25.725% | LR: 1.250000e-04 | Dur: 1386.69s
I - Confusion Matrix: [row->prediction - col->label]
[[ 12.  12.  19.  17.]
 [  0.   0.   0.   0.]
 [685. 566. 715. 521.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.396 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 122.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 1.389 | Acc: 24.000% | Wgt Acc: 21.429%
	I - Batch: 100 | Loss: 1.383 | Acc: 31.000% | Wgt Acc: 27.803%
	I - Batch: 150 | Loss: 1.378 | Acc: 32.000% | Wgt Acc: 28.872%
	I - Batch: 200 | Loss: 1.376 | Acc: 31.000% | Wgt Acc: 28.023%
	I - Batch: 250 | Loss: 1.376 | Acc: 29.600% | Wgt Acc: 26.787%
	I - Batch: 300 | Loss: 1.376 | Acc: 29.667% | Wgt Acc: 26.848%
	I - Batch: 350 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 27.132%
	I - Batch: 400 | Loss: 1.374 | Acc: 30.750% | Wgt Acc: 27.844%
	I - Batch: 450 | Loss: 1.375 | Acc: 30.444% | Wgt Acc: 27.538%
	I - Batch: 500 | Loss: 1.377 | Acc: 30.200% | Wgt Acc: 27.244%
	I - Batch: 550 | Loss: 1.380 | Acc: 29.273% | Wgt Acc: 26.350%
	I - Batch: 600 | Loss: 1.379 | Acc: 28.833% | Wgt Acc: 25.986%
	I - Batch: 650 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.234%
	I - Batch: 700 | Loss: 1.381 | Acc: 27.429% | Wgt Acc: 24.687%
	I - Batch: 750 | Loss: 1.380 | Acc: 27.467% | Wgt Acc: 24.737%
	I - Batch: 800 | Loss: 1.380 | Acc: 27.750% | Wgt Acc: 24.993%
	I - Batch: 850 | Loss: 1.381 | Acc: 27.176% | Wgt Acc: 24.470%
	I - Batch: 900 | Loss: 1.380 | Acc: 27.333% | Wgt Acc: 24.637%
	I - Batch: 950 | Loss: 1.381 | Acc: 26.737% | Wgt Acc: 24.053%
	I - Batch: 1000 | Loss: 1.382 | Acc: 26.700% | Wgt Acc: 24.005%
	I - Batch: 1050 | Loss: 1.382 | Acc: 26.667% | Wgt Acc: 23.978%
	I - Batch: 1100 | Loss: 1.381 | Acc: 26.818% | Wgt Acc: 24.136%
	I - Batch: 1150 | Loss: 1.383 | Acc: 26.609% | Wgt Acc: 23.920%
	I - Batch: 1200 | Loss: 1.383 | Acc: 26.750% | Wgt Acc: 24.045%
	I - Batch: 1250 | Loss: 1.382 | Acc: 26.560% | Wgt Acc: 23.889%
	I - Batch: 1300 | Loss: 1.382 | Acc: 26.462% | Wgt Acc: 23.810%
	I - Batch: 1350 | Loss: 1.382 | Acc: 26.815% | Wgt Acc: 24.129%
	I - Batch: 1400 | Loss: 1.380 | Acc: 27.286% | Wgt Acc: 24.582%
	I - Batch: 1450 | Loss: 1.380 | Acc: 27.241% | Wgt Acc: 24.553%
	I - Batch: 1500 | Loss: 1.380 | Acc: 27.333% | Wgt Acc: 24.643%
	I - Batch: 1550 | Loss: 1.379 | Acc: 27.161% | Wgt Acc: 24.495%
	I - Batch: 1600 | Loss: 1.380 | Acc: 27.125% | Wgt Acc: 24.440%
	I - Batch: 1650 | Loss: 1.381 | Acc: 27.152% | Wgt Acc: 24.444%
	I - Batch: 1700 | Loss: 1.380 | Acc: 27.647% | Wgt Acc: 24.920%
	I - Batch: 1750 | Loss: 1.380 | Acc: 27.714% | Wgt Acc: 24.981%
	I - Batch: 1800 | Loss: 1.379 | Acc: 27.667% | Wgt Acc: 24.953%
	I - Batch: 1850 | Loss: 1.379 | Acc: 27.676% | Wgt Acc: 24.960%
	I - Batch: 1900 | Loss: 1.379 | Acc: 27.842% | Wgt Acc: 25.110%
	I - Batch: 1950 | Loss: 1.379 | Acc: 27.897% | Wgt Acc: 25.165%
	I - Batch: 2000 | Loss: 1.379 | Acc: 27.800% | Wgt Acc: 25.079%
	I - Batch: 2050 | Loss: 1.379 | Acc: 27.805% | Wgt Acc: 25.077%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.143% | Wgt Acc: 25.392%
	I - Batch: 2150 | Loss: 1.378 | Acc: 28.326% | Wgt Acc: 25.559%
	I - Batch: 2200 | Loss: 1.378 | Acc: 28.364% | Wgt Acc: 25.597%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.133% | Wgt Acc: 25.368%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.087% | Wgt Acc: 25.326%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.235%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.167% | Wgt Acc: 25.385%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.286% | Wgt Acc: 25.501%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.120% | Wgt Acc: 25.345%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.033% | Wgt Acc: 25.265% | LR: 1.250000e-04 | Dur: 1394.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 77.  66.  97.  75.]
 [  0.   0.   0.   0.]
 [620. 512. 637. 463.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.396 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.398 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.397 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 1.375 | Acc: 36.000% | Wgt Acc: 32.727%
	I - Batch: 100 | Loss: 1.380 | Acc: 30.000% | Wgt Acc: 27.027%
	I - Batch: 150 | Loss: 1.384 | Acc: 28.667% | Wgt Acc: 25.749%
	I - Batch: 200 | Loss: 1.380 | Acc: 30.500% | Wgt Acc: 27.447%
	I - Batch: 250 | Loss: 1.380 | Acc: 31.600% | Wgt Acc: 28.417%
	I - Batch: 300 | Loss: 1.380 | Acc: 30.333% | Wgt Acc: 27.307%
	I - Batch: 350 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 27.132%
	I - Batch: 400 | Loss: 1.376 | Acc: 29.500% | Wgt Acc: 26.667%
	I - Batch: 450 | Loss: 1.377 | Acc: 29.778% | Wgt Acc: 26.908%
	I - Batch: 500 | Loss: 1.378 | Acc: 29.800% | Wgt Acc: 26.920%
	I - Batch: 550 | Loss: 1.376 | Acc: 30.364% | Wgt Acc: 27.467%
	I - Batch: 600 | Loss: 1.378 | Acc: 30.500% | Wgt Acc: 27.529%
	I - Batch: 650 | Loss: 1.381 | Acc: 29.538% | Wgt Acc: 26.584%
	I - Batch: 700 | Loss: 1.381 | Acc: 29.286% | Wgt Acc: 26.341%
	I - Batch: 750 | Loss: 1.380 | Acc: 29.200% | Wgt Acc: 26.291%
	I - Batch: 800 | Loss: 1.381 | Acc: 28.375% | Wgt Acc: 25.527%
	I - Batch: 850 | Loss: 1.380 | Acc: 28.588% | Wgt Acc: 25.755%
	I - Batch: 900 | Loss: 1.380 | Acc: 28.778% | Wgt Acc: 25.919%
	I - Batch: 950 | Loss: 1.380 | Acc: 28.737% | Wgt Acc: 25.883%
	I - Batch: 1000 | Loss: 1.381 | Acc: 28.500% | Wgt Acc: 25.658%
	I - Batch: 1050 | Loss: 1.381 | Acc: 28.571% | Wgt Acc: 25.701%
	I - Batch: 1100 | Loss: 1.381 | Acc: 28.455% | Wgt Acc: 25.609%
	I - Batch: 1150 | Loss: 1.381 | Acc: 28.696% | Wgt Acc: 25.817%
	I - Batch: 1200 | Loss: 1.380 | Acc: 28.583% | Wgt Acc: 25.727%
	I - Batch: 1250 | Loss: 1.380 | Acc: 28.480% | Wgt Acc: 25.644%
	I - Batch: 1300 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.199%
	I - Batch: 1350 | Loss: 1.381 | Acc: 27.926% | Wgt Acc: 25.129%
	I - Batch: 1400 | Loss: 1.380 | Acc: 28.214% | Wgt Acc: 25.422%
	I - Batch: 1450 | Loss: 1.380 | Acc: 28.276% | Wgt Acc: 25.486%
	I - Batch: 1500 | Loss: 1.379 | Acc: 28.733% | Wgt Acc: 25.913%
	I - Batch: 1550 | Loss: 1.380 | Acc: 28.710% | Wgt Acc: 25.872%
	I - Batch: 1600 | Loss: 1.381 | Acc: 28.250% | Wgt Acc: 25.440%
	I - Batch: 1650 | Loss: 1.380 | Acc: 28.545% | Wgt Acc: 25.710%
	I - Batch: 1700 | Loss: 1.380 | Acc: 28.471% | Wgt Acc: 25.636%
	I - Batch: 1750 | Loss: 1.380 | Acc: 28.571% | Wgt Acc: 25.730%
	I - Batch: 1800 | Loss: 1.380 | Acc: 28.611% | Wgt Acc: 25.766%
	I - Batch: 1850 | Loss: 1.380 | Acc: 28.541% | Wgt Acc: 25.706%
	I - Batch: 1900 | Loss: 1.380 | Acc: 28.474% | Wgt Acc: 25.637%
	I - Batch: 1950 | Loss: 1.379 | Acc: 28.615% | Wgt Acc: 25.789%
	I - Batch: 2000 | Loss: 1.380 | Acc: 28.450% | Wgt Acc: 25.625%
	I - Batch: 2050 | Loss: 1.380 | Acc: 28.537% | Wgt Acc: 25.709%
	I - Batch: 2100 | Loss: 1.380 | Acc: 28.381% | Wgt Acc: 25.566%
	I - Batch: 2150 | Loss: 1.380 | Acc: 28.372% | Wgt Acc: 25.558%
	I - Batch: 2200 | Loss: 1.380 | Acc: 28.409% | Wgt Acc: 25.591%
	I - Batch: 2250 | Loss: 1.380 | Acc: 28.489% | Wgt Acc: 25.666%
	I - Batch: 2300 | Loss: 1.380 | Acc: 28.652% | Wgt Acc: 25.815%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.681% | Wgt Acc: 25.849%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.792% | Wgt Acc: 25.955%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.857% | Wgt Acc: 26.002%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.760% | Wgt Acc: 25.919%
I - num batch: 2547
I - Train -- Loss: 1.380 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1393.60s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.393 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.395 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.395 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.394 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.65s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 1.380 | Acc: 28.000% | Wgt Acc: 25.225%
	I - Batch: 100 | Loss: 1.391 | Acc: 24.000% | Wgt Acc: 21.477%
	I - Batch: 150 | Loss: 1.380 | Acc: 27.333% | Wgt Acc: 24.625%
	I - Batch: 200 | Loss: 1.379 | Acc: 28.500% | Wgt Acc: 25.647%
	I - Batch: 250 | Loss: 1.380 | Acc: 27.200% | Wgt Acc: 24.482%
	I - Batch: 300 | Loss: 1.373 | Acc: 29.333% | Wgt Acc: 26.566%
	I - Batch: 350 | Loss: 1.367 | Acc: 31.429% | Wgt Acc: 28.534%
	I - Batch: 400 | Loss: 1.370 | Acc: 31.500% | Wgt Acc: 28.555%
	I - Batch: 450 | Loss: 1.370 | Acc: 32.667% | Wgt Acc: 29.607%
	I - Batch: 500 | Loss: 1.373 | Acc: 31.400% | Wgt Acc: 28.403%
	I - Batch: 550 | Loss: 1.376 | Acc: 31.273% | Wgt Acc: 28.231%
	I - Batch: 600 | Loss: 1.378 | Acc: 31.000% | Wgt Acc: 27.959%
	I - Batch: 650 | Loss: 1.379 | Acc: 30.615% | Wgt Acc: 27.581%
	I - Batch: 700 | Loss: 1.378 | Acc: 31.429% | Wgt Acc: 28.351%
	I - Batch: 750 | Loss: 1.379 | Acc: 31.067% | Wgt Acc: 28.013%
	I - Batch: 800 | Loss: 1.377 | Acc: 31.500% | Wgt Acc: 28.450%
	I - Batch: 850 | Loss: 1.375 | Acc: 32.353% | Wgt Acc: 29.248%
	I - Batch: 900 | Loss: 1.375 | Acc: 32.333% | Wgt Acc: 29.239%
	I - Batch: 950 | Loss: 1.375 | Acc: 32.105% | Wgt Acc: 29.061%
	I - Batch: 1000 | Loss: 1.376 | Acc: 31.300% | Wgt Acc: 28.307%
	I - Batch: 1050 | Loss: 1.375 | Acc: 31.238% | Wgt Acc: 28.270%
	I - Batch: 1100 | Loss: 1.374 | Acc: 31.636% | Wgt Acc: 28.654%
	I - Batch: 1150 | Loss: 1.375 | Acc: 31.304% | Wgt Acc: 28.346%
	I - Batch: 1200 | Loss: 1.374 | Acc: 31.250% | Wgt Acc: 28.307%
	I - Batch: 1250 | Loss: 1.375 | Acc: 30.880% | Wgt Acc: 27.951%
	I - Batch: 1300 | Loss: 1.375 | Acc: 31.077% | Wgt Acc: 28.139%
	I - Batch: 1350 | Loss: 1.374 | Acc: 31.407% | Wgt Acc: 28.442%
	I - Batch: 1400 | Loss: 1.375 | Acc: 31.143% | Wgt Acc: 28.193%
	I - Batch: 1450 | Loss: 1.376 | Acc: 30.897% | Wgt Acc: 27.956%
	I - Batch: 1500 | Loss: 1.377 | Acc: 30.667% | Wgt Acc: 27.719%
	I - Batch: 1550 | Loss: 1.377 | Acc: 30.452% | Wgt Acc: 27.514%
	I - Batch: 1600 | Loss: 1.378 | Acc: 30.250% | Wgt Acc: 27.314%
	I - Batch: 1650 | Loss: 1.377 | Acc: 30.303% | Wgt Acc: 27.382%
	I - Batch: 1700 | Loss: 1.377 | Acc: 30.176% | Wgt Acc: 27.265%
	I - Batch: 1750 | Loss: 1.377 | Acc: 30.114% | Wgt Acc: 27.207%
	I - Batch: 1800 | Loss: 1.378 | Acc: 30.167% | Wgt Acc: 27.242%
	I - Batch: 1850 | Loss: 1.378 | Acc: 30.108% | Wgt Acc: 27.194%
	I - Batch: 1900 | Loss: 1.377 | Acc: 30.000% | Wgt Acc: 27.101%
	I - Batch: 1950 | Loss: 1.378 | Acc: 30.103% | Wgt Acc: 27.182%
	I - Batch: 2000 | Loss: 1.378 | Acc: 29.850% | Wgt Acc: 26.950%
	I - Batch: 2050 | Loss: 1.379 | Acc: 29.659% | Wgt Acc: 26.758%
	I - Batch: 2100 | Loss: 1.379 | Acc: 29.476% | Wgt Acc: 26.584%
	I - Batch: 2150 | Loss: 1.379 | Acc: 29.302% | Wgt Acc: 26.429%
	I - Batch: 2200 | Loss: 1.379 | Acc: 29.318% | Wgt Acc: 26.451%
	I - Batch: 2250 | Loss: 1.379 | Acc: 29.156% | Wgt Acc: 26.293%
	I - Batch: 2300 | Loss: 1.379 | Acc: 29.261% | Wgt Acc: 26.387%
	I - Batch: 2350 | Loss: 1.379 | Acc: 29.319% | Wgt Acc: 26.444%
	I - Batch: 2400 | Loss: 1.379 | Acc: 29.250% | Wgt Acc: 26.386%
	I - Batch: 2450 | Loss: 1.379 | Acc: 29.184% | Wgt Acc: 26.323%
	I - Batch: 2500 | Loss: 1.380 | Acc: 28.960% | Wgt Acc: 26.095%
I - num batch: 2547
I - Train -- Loss: 1.380 | Acc: 29.093% | Wgt Acc: 26.221% | LR: 1.250000e-04 | Dur: 894.60s
I - Confusion Matrix: [row->prediction - col->label]
[[124. 102. 114. 101.]
 [  0.   0.   2.   0.]
 [571. 475. 617. 437.]
 [  2.   1.   1.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.382 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.390 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.389 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.389 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.391 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.390 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.391 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 65.94s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 1.376 | Acc: 28.000% | Wgt Acc: 25.339%
	I - Batch: 100 | Loss: 1.379 | Acc: 29.000% | Wgt Acc: 26.126%
	I - Batch: 150 | Loss: 1.383 | Acc: 26.667% | Wgt Acc: 23.916%
	I - Batch: 200 | Loss: 1.385 | Acc: 26.500% | Wgt Acc: 23.714%
	I - Batch: 250 | Loss: 1.381 | Acc: 26.000% | Wgt Acc: 23.402%
	I - Batch: 300 | Loss: 1.382 | Acc: 26.333% | Wgt Acc: 23.653%
	I - Batch: 350 | Loss: 1.378 | Acc: 28.571% | Wgt Acc: 25.790%
	I - Batch: 400 | Loss: 1.381 | Acc: 27.750% | Wgt Acc: 24.958%
	I - Batch: 450 | Loss: 1.380 | Acc: 28.889% | Wgt Acc: 26.013%
	I - Batch: 500 | Loss: 1.377 | Acc: 29.800% | Wgt Acc: 26.920%
	I - Batch: 550 | Loss: 1.378 | Acc: 29.455% | Wgt Acc: 26.568%
	I - Batch: 600 | Loss: 1.378 | Acc: 28.833% | Wgt Acc: 25.995%
	I - Batch: 650 | Loss: 1.380 | Acc: 27.846% | Wgt Acc: 25.052%
	I - Batch: 700 | Loss: 1.380 | Acc: 27.143% | Wgt Acc: 24.414%
	I - Batch: 750 | Loss: 1.380 | Acc: 27.200% | Wgt Acc: 24.453%
	I - Batch: 800 | Loss: 1.381 | Acc: 27.125% | Wgt Acc: 24.368%
	I - Batch: 850 | Loss: 1.380 | Acc: 26.941% | Wgt Acc: 24.233%
	I - Batch: 900 | Loss: 1.380 | Acc: 26.778% | Wgt Acc: 24.082%
	I - Batch: 950 | Loss: 1.380 | Acc: 26.947% | Wgt Acc: 24.231%
	I - Batch: 1000 | Loss: 1.381 | Acc: 26.500% | Wgt Acc: 23.788%
	I - Batch: 1050 | Loss: 1.380 | Acc: 26.952% | Wgt Acc: 24.224%
	I - Batch: 1100 | Loss: 1.380 | Acc: 27.182% | Wgt Acc: 24.448%
	I - Batch: 1150 | Loss: 1.381 | Acc: 27.043% | Wgt Acc: 24.321%
	I - Batch: 1200 | Loss: 1.380 | Acc: 27.167% | Wgt Acc: 24.442%
	I - Batch: 1250 | Loss: 1.380 | Acc: 27.280% | Wgt Acc: 24.563%
	I - Batch: 1300 | Loss: 1.379 | Acc: 27.769% | Wgt Acc: 25.026%
	I - Batch: 1350 | Loss: 1.378 | Acc: 28.000% | Wgt Acc: 25.246%
	I - Batch: 1400 | Loss: 1.377 | Acc: 28.143% | Wgt Acc: 25.399%
	I - Batch: 1450 | Loss: 1.378 | Acc: 28.069% | Wgt Acc: 25.319%
	I - Batch: 1500 | Loss: 1.379 | Acc: 27.800% | Wgt Acc: 25.064%
	I - Batch: 1550 | Loss: 1.378 | Acc: 27.871% | Wgt Acc: 25.127%
	I - Batch: 1600 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.225%
	I - Batch: 1650 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.239%
	I - Batch: 1700 | Loss: 1.378 | Acc: 27.941% | Wgt Acc: 25.199%
	I - Batch: 1750 | Loss: 1.378 | Acc: 27.771% | Wgt Acc: 25.052%
	I - Batch: 1800 | Loss: 1.378 | Acc: 27.722% | Wgt Acc: 25.006%
	I - Batch: 1850 | Loss: 1.379 | Acc: 27.243% | Wgt Acc: 24.546%
	I - Batch: 1900 | Loss: 1.379 | Acc: 27.316% | Wgt Acc: 24.620%
	I - Batch: 1950 | Loss: 1.379 | Acc: 27.385% | Wgt Acc: 24.679%
	I - Batch: 2000 | Loss: 1.378 | Acc: 27.500% | Wgt Acc: 24.800%
	I - Batch: 2050 | Loss: 1.379 | Acc: 27.317% | Wgt Acc: 24.621%
	I - Batch: 2100 | Loss: 1.379 | Acc: 27.429% | Wgt Acc: 24.718%
	I - Batch: 2150 | Loss: 1.379 | Acc: 27.535% | Wgt Acc: 24.809%
	I - Batch: 2200 | Loss: 1.379 | Acc: 27.636% | Wgt Acc: 24.898%
	I - Batch: 2250 | Loss: 1.379 | Acc: 27.867% | Wgt Acc: 25.118%
	I - Batch: 2300 | Loss: 1.379 | Acc: 27.913% | Wgt Acc: 25.159%
	I - Batch: 2350 | Loss: 1.379 | Acc: 27.872% | Wgt Acc: 25.129%
	I - Batch: 2400 | Loss: 1.379 | Acc: 27.917% | Wgt Acc: 25.174%
	I - Batch: 2450 | Loss: 1.379 | Acc: 27.959% | Wgt Acc: 25.205%
	I - Batch: 2500 | Loss: 1.380 | Acc: 27.880% | Wgt Acc: 25.119%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 27.837% | Wgt Acc: 25.088% | LR: 1.250000e-04 | Dur: 889.68s
I - Confusion Matrix: [row->prediction - col->label]
[[242. 226. 267. 185.]
 [  0.   0.   0.   0.]
 [455. 352. 467. 353.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.393 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.396 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.397 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 65.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 1.401 | Acc: 28.000% | Wgt Acc: 24.670%
	I - Batch: 100 | Loss: 1.385 | Acc: 30.000% | Wgt Acc: 26.846%
	I - Batch: 150 | Loss: 1.383 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 200 | Loss: 1.383 | Acc: 30.500% | Wgt Acc: 27.324%
	I - Batch: 250 | Loss: 1.384 | Acc: 28.800% | Wgt Acc: 25.830%
	I - Batch: 300 | Loss: 1.381 | Acc: 29.000% | Wgt Acc: 26.087%
	I - Batch: 350 | Loss: 1.374 | Acc: 31.143% | Wgt Acc: 28.165%
	I - Batch: 400 | Loss: 1.373 | Acc: 32.250% | Wgt Acc: 29.169%
	I - Batch: 450 | Loss: 1.373 | Acc: 31.556% | Wgt Acc: 28.557%
	I - Batch: 500 | Loss: 1.375 | Acc: 31.000% | Wgt Acc: 28.029%
	I - Batch: 550 | Loss: 1.373 | Acc: 31.455% | Wgt Acc: 28.477%
	I - Batch: 600 | Loss: 1.370 | Acc: 31.500% | Wgt Acc: 28.582%
	I - Batch: 650 | Loss: 1.369 | Acc: 32.000% | Wgt Acc: 29.060%
	I - Batch: 700 | Loss: 1.369 | Acc: 31.714% | Wgt Acc: 28.794%
	I - Batch: 750 | Loss: 1.372 | Acc: 31.733% | Wgt Acc: 28.744%
	I - Batch: 800 | Loss: 1.372 | Acc: 31.375% | Wgt Acc: 28.418%
	I - Batch: 850 | Loss: 1.371 | Acc: 31.765% | Wgt Acc: 28.777%
	I - Batch: 900 | Loss: 1.373 | Acc: 31.556% | Wgt Acc: 28.536%
	I - Batch: 950 | Loss: 1.373 | Acc: 31.263% | Wgt Acc: 28.272%
	I - Batch: 1000 | Loss: 1.374 | Acc: 30.800% | Wgt Acc: 27.842%
	I - Batch: 1050 | Loss: 1.374 | Acc: 30.762% | Wgt Acc: 27.809%
	I - Batch: 1100 | Loss: 1.375 | Acc: 30.636% | Wgt Acc: 27.680%
	I - Batch: 1150 | Loss: 1.376 | Acc: 29.913% | Wgt Acc: 27.017%
	I - Batch: 1200 | Loss: 1.377 | Acc: 29.583% | Wgt Acc: 26.697%
	I - Batch: 1250 | Loss: 1.376 | Acc: 29.520% | Wgt Acc: 26.643%
	I - Batch: 1300 | Loss: 1.376 | Acc: 29.462% | Wgt Acc: 26.588%
	I - Batch: 1350 | Loss: 1.377 | Acc: 29.111% | Wgt Acc: 26.261%
	I - Batch: 1400 | Loss: 1.377 | Acc: 28.786% | Wgt Acc: 25.958%
	I - Batch: 1450 | Loss: 1.377 | Acc: 28.828% | Wgt Acc: 25.995%
	I - Batch: 1500 | Loss: 1.378 | Acc: 29.133% | Wgt Acc: 26.258%
	I - Batch: 1550 | Loss: 1.378 | Acc: 28.903% | Wgt Acc: 26.039%
	I - Batch: 1600 | Loss: 1.378 | Acc: 28.812% | Wgt Acc: 25.954%
	I - Batch: 1650 | Loss: 1.379 | Acc: 28.485% | Wgt Acc: 25.641%
	I - Batch: 1700 | Loss: 1.379 | Acc: 28.529% | Wgt Acc: 25.672%
	I - Batch: 1750 | Loss: 1.378 | Acc: 28.800% | Wgt Acc: 25.939%
	I - Batch: 1800 | Loss: 1.378 | Acc: 28.667% | Wgt Acc: 25.823%
	I - Batch: 1850 | Loss: 1.378 | Acc: 28.595% | Wgt Acc: 25.748%
	I - Batch: 1900 | Loss: 1.378 | Acc: 28.579% | Wgt Acc: 25.753%
	I - Batch: 1950 | Loss: 1.378 | Acc: 28.667% | Wgt Acc: 25.835%
	I - Batch: 2000 | Loss: 1.378 | Acc: 28.750% | Wgt Acc: 25.918%
	I - Batch: 2050 | Loss: 1.377 | Acc: 28.683% | Wgt Acc: 25.858%
	I - Batch: 2100 | Loss: 1.377 | Acc: 28.952% | Wgt Acc: 26.114%
	I - Batch: 2150 | Loss: 1.377 | Acc: 28.791% | Wgt Acc: 25.957%
	I - Batch: 2200 | Loss: 1.377 | Acc: 28.955% | Wgt Acc: 26.107%
	I - Batch: 2250 | Loss: 1.377 | Acc: 28.889% | Wgt Acc: 26.044%
	I - Batch: 2300 | Loss: 1.377 | Acc: 28.957% | Wgt Acc: 26.102%
	I - Batch: 2350 | Loss: 1.377 | Acc: 28.894% | Wgt Acc: 26.048%
	I - Batch: 2400 | Loss: 1.377 | Acc: 29.000% | Wgt Acc: 26.148%
	I - Batch: 2450 | Loss: 1.377 | Acc: 29.061% | Wgt Acc: 26.203%
	I - Batch: 2500 | Loss: 1.377 | Acc: 28.960% | Wgt Acc: 26.114%
I - num batch: 2547
I - Train -- Loss: 1.378 | Acc: 28.779% | Wgt Acc: 25.938% | LR: 1.250000e-04 | Dur: 912.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 22.  26.  23.  15.]
 [  0.   0.   0.   0.]
 [675. 552. 711. 523.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.384 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.392 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.391 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.396 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 100.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 1.384 | Acc: 32.000% | Wgt Acc: 28.959%
	I - Batch: 100 | Loss: 1.377 | Acc: 32.000% | Wgt Acc: 29.091%
	I - Batch: 150 | Loss: 1.383 | Acc: 30.000% | Wgt Acc: 27.027%
	I - Batch: 200 | Loss: 1.379 | Acc: 29.500% | Wgt Acc: 26.607%
	I - Batch: 250 | Loss: 1.379 | Acc: 29.200% | Wgt Acc: 26.330%
	I - Batch: 300 | Loss: 1.385 | Acc: 28.000% | Wgt Acc: 25.075%
	I - Batch: 350 | Loss: 1.386 | Acc: 26.857% | Wgt Acc: 24.026%
	I - Batch: 400 | Loss: 1.386 | Acc: 26.000% | Wgt Acc: 23.227%
	I - Batch: 450 | Loss: 1.385 | Acc: 26.667% | Wgt Acc: 23.857%
	I - Batch: 500 | Loss: 1.383 | Acc: 27.600% | Wgt Acc: 24.731%
	I - Batch: 550 | Loss: 1.382 | Acc: 27.636% | Wgt Acc: 24.796%
	I - Batch: 600 | Loss: 1.384 | Acc: 26.500% | Wgt Acc: 23.714%
	I - Batch: 650 | Loss: 1.383 | Acc: 27.692% | Wgt Acc: 24.802%
	I - Batch: 700 | Loss: 1.383 | Acc: 28.143% | Wgt Acc: 25.232%
	I - Batch: 750 | Loss: 1.383 | Acc: 28.000% | Wgt Acc: 25.097%
	I - Batch: 800 | Loss: 1.384 | Acc: 27.500% | Wgt Acc: 24.622%
	I - Batch: 850 | Loss: 1.383 | Acc: 28.000% | Wgt Acc: 25.079%
	I - Batch: 900 | Loss: 1.383 | Acc: 27.889% | Wgt Acc: 24.994%
	I - Batch: 950 | Loss: 1.382 | Acc: 27.789% | Wgt Acc: 24.917%
	I - Batch: 1000 | Loss: 1.383 | Acc: 27.900% | Wgt Acc: 25.017%
	I - Batch: 1050 | Loss: 1.383 | Acc: 27.524% | Wgt Acc: 24.674%
	I - Batch: 1100 | Loss: 1.382 | Acc: 27.545% | Wgt Acc: 24.709%
	I - Batch: 1150 | Loss: 1.381 | Acc: 27.826% | Wgt Acc: 24.985%
	I - Batch: 1200 | Loss: 1.380 | Acc: 28.167% | Wgt Acc: 25.323%
	I - Batch: 1250 | Loss: 1.381 | Acc: 28.160% | Wgt Acc: 25.319%
	I - Batch: 1300 | Loss: 1.380 | Acc: 28.231% | Wgt Acc: 25.394%
	I - Batch: 1350 | Loss: 1.380 | Acc: 28.222% | Wgt Acc: 25.379%
	I - Batch: 1400 | Loss: 1.381 | Acc: 27.929% | Wgt Acc: 25.096%
	I - Batch: 1450 | Loss: 1.381 | Acc: 27.724% | Wgt Acc: 24.911%
	I - Batch: 1500 | Loss: 1.381 | Acc: 27.667% | Wgt Acc: 24.861%
	I - Batch: 1550 | Loss: 1.381 | Acc: 27.548% | Wgt Acc: 24.739%
	I - Batch: 1600 | Loss: 1.381 | Acc: 27.875% | Wgt Acc: 25.046%
	I - Batch: 1650 | Loss: 1.381 | Acc: 27.818% | Wgt Acc: 24.993%
	I - Batch: 1700 | Loss: 1.382 | Acc: 27.412% | Wgt Acc: 24.624%
	I - Batch: 1750 | Loss: 1.382 | Acc: 27.714% | Wgt Acc: 24.897%
	I - Batch: 1800 | Loss: 1.381 | Acc: 27.889% | Wgt Acc: 25.062%
	I - Batch: 1850 | Loss: 1.381 | Acc: 28.054% | Wgt Acc: 25.222%
	I - Batch: 1900 | Loss: 1.381 | Acc: 28.105% | Wgt Acc: 25.284%
	I - Batch: 1950 | Loss: 1.381 | Acc: 27.949% | Wgt Acc: 25.138%
	I - Batch: 2000 | Loss: 1.380 | Acc: 28.200% | Wgt Acc: 25.371%
	I - Batch: 2050 | Loss: 1.381 | Acc: 28.049% | Wgt Acc: 25.225%
	I - Batch: 2100 | Loss: 1.381 | Acc: 28.048% | Wgt Acc: 25.225%
	I - Batch: 2150 | Loss: 1.380 | Acc: 28.140% | Wgt Acc: 25.319%
	I - Batch: 2200 | Loss: 1.380 | Acc: 28.409% | Wgt Acc: 25.568%
	I - Batch: 2250 | Loss: 1.380 | Acc: 28.444% | Wgt Acc: 25.610%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.522% | Wgt Acc: 25.693%
	I - Batch: 2350 | Loss: 1.380 | Acc: 28.468% | Wgt Acc: 25.635%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.542% | Wgt Acc: 25.708%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.449% | Wgt Acc: 25.630%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.520% | Wgt Acc: 25.696%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1397.53s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.389 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.398 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.396 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.398 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.400 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.399 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.398 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 122.81s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 1.360 | Acc: 32.000% | Wgt Acc: 29.224%
	I - Batch: 100 | Loss: 1.364 | Acc: 31.000% | Wgt Acc: 28.246%
	I - Batch: 150 | Loss: 1.372 | Acc: 30.000% | Wgt Acc: 27.190%
	I - Batch: 200 | Loss: 1.376 | Acc: 30.500% | Wgt Acc: 27.540%
	I - Batch: 250 | Loss: 1.376 | Acc: 30.400% | Wgt Acc: 27.462%
	I - Batch: 300 | Loss: 1.376 | Acc: 31.000% | Wgt Acc: 28.012%
	I - Batch: 350 | Loss: 1.376 | Acc: 31.714% | Wgt Acc: 28.645%
	I - Batch: 400 | Loss: 1.377 | Acc: 31.250% | Wgt Acc: 28.185%
	I - Batch: 450 | Loss: 1.378 | Acc: 31.333% | Wgt Acc: 28.242%
	I - Batch: 500 | Loss: 1.377 | Acc: 30.800% | Wgt Acc: 27.798%
	I - Batch: 550 | Loss: 1.379 | Acc: 30.000% | Wgt Acc: 27.049%
	I - Batch: 600 | Loss: 1.379 | Acc: 29.667% | Wgt Acc: 26.747%
	I - Batch: 650 | Loss: 1.378 | Acc: 29.077% | Wgt Acc: 26.259%
	I - Batch: 700 | Loss: 1.378 | Acc: 28.714% | Wgt Acc: 25.919%
	I - Batch: 750 | Loss: 1.380 | Acc: 27.467% | Wgt Acc: 24.760%
	I - Batch: 800 | Loss: 1.378 | Acc: 27.625% | Wgt Acc: 24.944%
	I - Batch: 850 | Loss: 1.377 | Acc: 27.765% | Wgt Acc: 25.086%
	I - Batch: 900 | Loss: 1.376 | Acc: 27.889% | Wgt Acc: 25.226%
	I - Batch: 950 | Loss: 1.374 | Acc: 28.316% | Wgt Acc: 25.650%
	I - Batch: 1000 | Loss: 1.376 | Acc: 28.200% | Wgt Acc: 25.497%
	I - Batch: 1050 | Loss: 1.376 | Acc: 28.095% | Wgt Acc: 25.409%
	I - Batch: 1100 | Loss: 1.377 | Acc: 27.909% | Wgt Acc: 25.216%
	I - Batch: 1150 | Loss: 1.378 | Acc: 27.913% | Wgt Acc: 25.201%
	I - Batch: 1200 | Loss: 1.379 | Acc: 27.500% | Wgt Acc: 24.807%
	I - Batch: 1250 | Loss: 1.380 | Acc: 27.360% | Wgt Acc: 24.649%
	I - Batch: 1300 | Loss: 1.379 | Acc: 27.692% | Wgt Acc: 24.965%
	I - Batch: 1350 | Loss: 1.379 | Acc: 27.926% | Wgt Acc: 25.171%
	I - Batch: 1400 | Loss: 1.379 | Acc: 27.929% | Wgt Acc: 25.177%
	I - Batch: 1450 | Loss: 1.380 | Acc: 28.000% | Wgt Acc: 25.221%
	I - Batch: 1500 | Loss: 1.379 | Acc: 28.067% | Wgt Acc: 25.300%
	I - Batch: 1550 | Loss: 1.379 | Acc: 28.194% | Wgt Acc: 25.422%
	I - Batch: 1600 | Loss: 1.379 | Acc: 28.125% | Wgt Acc: 25.359%
	I - Batch: 1650 | Loss: 1.379 | Acc: 28.121% | Wgt Acc: 25.359%
	I - Batch: 1700 | Loss: 1.379 | Acc: 28.118% | Wgt Acc: 25.351%
	I - Batch: 1750 | Loss: 1.379 | Acc: 28.343% | Wgt Acc: 25.557%
	I - Batch: 1800 | Loss: 1.379 | Acc: 28.278% | Wgt Acc: 25.501%
	I - Batch: 1850 | Loss: 1.379 | Acc: 28.378% | Wgt Acc: 25.591%
	I - Batch: 1900 | Loss: 1.379 | Acc: 28.263% | Wgt Acc: 25.474%
	I - Batch: 1950 | Loss: 1.379 | Acc: 28.256% | Wgt Acc: 25.471%
	I - Batch: 2000 | Loss: 1.380 | Acc: 28.200% | Wgt Acc: 25.405%
	I - Batch: 2050 | Loss: 1.379 | Acc: 28.244% | Wgt Acc: 25.459%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.429% | Wgt Acc: 25.614%
	I - Batch: 2150 | Loss: 1.379 | Acc: 28.605% | Wgt Acc: 25.786%
	I - Batch: 2200 | Loss: 1.379 | Acc: 28.591% | Wgt Acc: 25.752%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.578% | Wgt Acc: 25.753%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.609% | Wgt Acc: 25.784%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.638% | Wgt Acc: 25.813%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.750% | Wgt Acc: 25.920%
	I - Batch: 2450 | Loss: 1.378 | Acc: 28.653% | Wgt Acc: 25.835%
	I - Batch: 2500 | Loss: 1.378 | Acc: 28.680% | Wgt Acc: 25.856%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.661% | Wgt Acc: 25.832% | LR: 1.250000e-04 | Dur: 1394.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.  25.  47.  19.]
 [  0.   0.   0.   0.]
 [654. 553. 687. 519.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.390 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.398 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.397 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.399 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.400 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.399 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.397 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 121.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 1.349 | Acc: 30.000% | Wgt Acc: 27.650%
	I - Batch: 100 | Loss: 1.373 | Acc: 25.000% | Wgt Acc: 22.676%
	I - Batch: 150 | Loss: 1.369 | Acc: 26.667% | Wgt Acc: 24.279%
	I - Batch: 200 | Loss: 1.379 | Acc: 27.500% | Wgt Acc: 24.803%
	I - Batch: 250 | Loss: 1.379 | Acc: 27.200% | Wgt Acc: 24.527%
	I - Batch: 300 | Loss: 1.377 | Acc: 28.000% | Wgt Acc: 25.282%
	I - Batch: 350 | Loss: 1.378 | Acc: 28.000% | Wgt Acc: 25.274%
	I - Batch: 400 | Loss: 1.380 | Acc: 27.250% | Wgt Acc: 24.577%
	I - Batch: 450 | Loss: 1.377 | Acc: 27.556% | Wgt Acc: 24.887%
	I - Batch: 500 | Loss: 1.378 | Acc: 27.400% | Wgt Acc: 24.740%
	I - Batch: 550 | Loss: 1.378 | Acc: 27.455% | Wgt Acc: 24.805%
	I - Batch: 600 | Loss: 1.376 | Acc: 28.667% | Wgt Acc: 25.913%
	I - Batch: 650 | Loss: 1.378 | Acc: 28.154% | Wgt Acc: 25.417%
	I - Batch: 700 | Loss: 1.378 | Acc: 28.000% | Wgt Acc: 25.282%
	I - Batch: 750 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.240%
	I - Batch: 800 | Loss: 1.379 | Acc: 28.125% | Wgt Acc: 25.359%
	I - Batch: 850 | Loss: 1.378 | Acc: 28.235% | Wgt Acc: 25.457%
	I - Batch: 900 | Loss: 1.379 | Acc: 28.222% | Wgt Acc: 25.432%
	I - Batch: 950 | Loss: 1.377 | Acc: 28.947% | Wgt Acc: 26.128%
	I - Batch: 1000 | Loss: 1.375 | Acc: 29.100% | Wgt Acc: 26.311%
	I - Batch: 1050 | Loss: 1.375 | Acc: 28.667% | Wgt Acc: 25.932%
	I - Batch: 1100 | Loss: 1.375 | Acc: 28.818% | Wgt Acc: 26.064%
	I - Batch: 1150 | Loss: 1.376 | Acc: 28.348% | Wgt Acc: 25.619%
	I - Batch: 1200 | Loss: 1.378 | Acc: 27.917% | Wgt Acc: 25.197%
	I - Batch: 1250 | Loss: 1.378 | Acc: 27.840% | Wgt Acc: 25.108%
	I - Batch: 1300 | Loss: 1.380 | Acc: 27.615% | Wgt Acc: 24.883%
	I - Batch: 1350 | Loss: 1.380 | Acc: 27.259% | Wgt Acc: 24.562%
	I - Batch: 1400 | Loss: 1.380 | Acc: 26.929% | Wgt Acc: 24.244%
	I - Batch: 1450 | Loss: 1.380 | Acc: 27.241% | Wgt Acc: 24.530%
	I - Batch: 1500 | Loss: 1.380 | Acc: 27.333% | Wgt Acc: 24.617%
	I - Batch: 1550 | Loss: 1.380 | Acc: 27.548% | Wgt Acc: 24.811%
	I - Batch: 1600 | Loss: 1.381 | Acc: 27.375% | Wgt Acc: 24.638%
	I - Batch: 1650 | Loss: 1.380 | Acc: 27.273% | Wgt Acc: 24.560%
	I - Batch: 1700 | Loss: 1.380 | Acc: 27.294% | Wgt Acc: 24.573%
	I - Batch: 1750 | Loss: 1.380 | Acc: 27.371% | Wgt Acc: 24.640%
	I - Batch: 1800 | Loss: 1.380 | Acc: 27.556% | Wgt Acc: 24.806%
	I - Batch: 1850 | Loss: 1.380 | Acc: 27.676% | Wgt Acc: 24.909%
	I - Batch: 1900 | Loss: 1.380 | Acc: 28.000% | Wgt Acc: 25.210%
	I - Batch: 1950 | Loss: 1.380 | Acc: 28.154% | Wgt Acc: 25.340%
	I - Batch: 2000 | Loss: 1.380 | Acc: 27.850% | Wgt Acc: 25.076%
	I - Batch: 2050 | Loss: 1.380 | Acc: 27.854% | Wgt Acc: 25.074%
	I - Batch: 2100 | Loss: 1.380 | Acc: 27.952% | Wgt Acc: 25.161%
	I - Batch: 2150 | Loss: 1.380 | Acc: 28.047% | Wgt Acc: 25.249%
	I - Batch: 2200 | Loss: 1.380 | Acc: 27.818% | Wgt Acc: 25.038%
	I - Batch: 2250 | Loss: 1.381 | Acc: 27.778% | Wgt Acc: 25.000%
	I - Batch: 2300 | Loss: 1.380 | Acc: 27.913% | Wgt Acc: 25.135%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.213% | Wgt Acc: 25.417%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.500% | Wgt Acc: 25.690%
	I - Batch: 2450 | Loss: 1.378 | Acc: 28.653% | Wgt Acc: 25.837%
	I - Batch: 2500 | Loss: 1.378 | Acc: 28.600% | Wgt Acc: 25.791%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.543% | Wgt Acc: 25.725% | LR: 1.250000e-04 | Dur: 1391.12s
I - Confusion Matrix: [row->prediction - col->label]
[[  4.   5.  11.   1.]
 [  0.   0.   0.   0.]
 [693. 573. 723. 537.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.391 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.399 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.397 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.400 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.401 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.400 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.398 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 1.391 | Acc: 28.000% | Wgt Acc: 24.779%
	I - Batch: 100 | Loss: 1.379 | Acc: 31.000% | Wgt Acc: 27.803%
	I - Batch: 150 | Loss: 1.380 | Acc: 30.667% | Wgt Acc: 27.545%
	I - Batch: 200 | Loss: 1.380 | Acc: 31.500% | Wgt Acc: 28.283%
	I - Batch: 250 | Loss: 1.385 | Acc: 29.200% | Wgt Acc: 26.118%
	I - Batch: 300 | Loss: 1.385 | Acc: 28.333% | Wgt Acc: 25.373%
	I - Batch: 350 | Loss: 1.385 | Acc: 28.286% | Wgt Acc: 25.352%
	I - Batch: 400 | Loss: 1.385 | Acc: 27.000% | Wgt Acc: 24.229%
	I - Batch: 450 | Loss: 1.382 | Acc: 27.556% | Wgt Acc: 24.800%
	I - Batch: 500 | Loss: 1.382 | Acc: 28.400% | Wgt Acc: 25.563%
	I - Batch: 550 | Loss: 1.378 | Acc: 30.000% | Wgt Acc: 27.082%
	I - Batch: 600 | Loss: 1.379 | Acc: 30.000% | Wgt Acc: 27.068%
	I - Batch: 650 | Loss: 1.378 | Acc: 30.154% | Wgt Acc: 27.213%
	I - Batch: 700 | Loss: 1.377 | Acc: 30.429% | Wgt Acc: 27.484%
	I - Batch: 750 | Loss: 1.376 | Acc: 30.267% | Wgt Acc: 27.358%
	I - Batch: 800 | Loss: 1.376 | Acc: 29.875% | Wgt Acc: 27.013%
	I - Batch: 850 | Loss: 1.375 | Acc: 30.235% | Wgt Acc: 27.355%
	I - Batch: 900 | Loss: 1.374 | Acc: 30.333% | Wgt Acc: 27.465%
	I - Batch: 950 | Loss: 1.375 | Acc: 29.895% | Wgt Acc: 27.048%
	I - Batch: 1000 | Loss: 1.375 | Acc: 29.400% | Wgt Acc: 26.606%
	I - Batch: 1050 | Loss: 1.375 | Acc: 29.714% | Wgt Acc: 26.897%
	I - Batch: 1100 | Loss: 1.374 | Acc: 29.727% | Wgt Acc: 26.925%
	I - Batch: 1150 | Loss: 1.375 | Acc: 29.739% | Wgt Acc: 26.919%
	I - Batch: 1200 | Loss: 1.374 | Acc: 29.917% | Wgt Acc: 27.089%
	I - Batch: 1250 | Loss: 1.374 | Acc: 29.760% | Wgt Acc: 26.942%
	I - Batch: 1300 | Loss: 1.375 | Acc: 29.385% | Wgt Acc: 26.592%
	I - Batch: 1350 | Loss: 1.376 | Acc: 29.111% | Wgt Acc: 26.318%
	I - Batch: 1400 | Loss: 1.376 | Acc: 29.357% | Wgt Acc: 26.546%
	I - Batch: 1450 | Loss: 1.375 | Acc: 29.517% | Wgt Acc: 26.712%
	I - Batch: 1500 | Loss: 1.375 | Acc: 29.533% | Wgt Acc: 26.723%
	I - Batch: 1550 | Loss: 1.376 | Acc: 29.484% | Wgt Acc: 26.655%
	I - Batch: 1600 | Loss: 1.376 | Acc: 29.125% | Wgt Acc: 26.331%
	I - Batch: 1650 | Loss: 1.376 | Acc: 28.909% | Wgt Acc: 26.133%
	I - Batch: 1700 | Loss: 1.378 | Acc: 28.588% | Wgt Acc: 25.810%
	I - Batch: 1750 | Loss: 1.378 | Acc: 28.514% | Wgt Acc: 25.738%
	I - Batch: 1800 | Loss: 1.377 | Acc: 28.556% | Wgt Acc: 25.784%
	I - Batch: 1850 | Loss: 1.378 | Acc: 28.432% | Wgt Acc: 25.671%
	I - Batch: 1900 | Loss: 1.377 | Acc: 28.737% | Wgt Acc: 25.969%
	I - Batch: 1950 | Loss: 1.377 | Acc: 28.513% | Wgt Acc: 25.756%
	I - Batch: 2000 | Loss: 1.378 | Acc: 28.500% | Wgt Acc: 25.731%
	I - Batch: 2050 | Loss: 1.377 | Acc: 28.732% | Wgt Acc: 25.947%
	I - Batch: 2100 | Loss: 1.378 | Acc: 28.714% | Wgt Acc: 25.916%
	I - Batch: 2150 | Loss: 1.378 | Acc: 28.605% | Wgt Acc: 25.805%
	I - Batch: 2200 | Loss: 1.379 | Acc: 28.591% | Wgt Acc: 25.768%
	I - Batch: 2250 | Loss: 1.380 | Acc: 28.578% | Wgt Acc: 25.753%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.565% | Wgt Acc: 25.757%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.723% | Wgt Acc: 25.904%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.708% | Wgt Acc: 25.897%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.612% | Wgt Acc: 25.803%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.680% | Wgt Acc: 25.852%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.779% | Wgt Acc: 25.938% | LR: 1.250000e-04 | Dur: 1392.97s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   1.   1.]
 [  0.   0.   0.   0.]
 [697. 578. 733. 537.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.393 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 1.356 | Acc: 36.000% | Wgt Acc: 33.028%
	I - Batch: 100 | Loss: 1.362 | Acc: 32.000% | Wgt Acc: 29.358%
	I - Batch: 150 | Loss: 1.375 | Acc: 28.000% | Wgt Acc: 25.339%
	I - Batch: 200 | Loss: 1.380 | Acc: 27.000% | Wgt Acc: 24.324%
	I - Batch: 250 | Loss: 1.376 | Acc: 29.600% | Wgt Acc: 26.763%
	I - Batch: 300 | Loss: 1.373 | Acc: 30.667% | Wgt Acc: 27.753%
	I - Batch: 350 | Loss: 1.373 | Acc: 29.714% | Wgt Acc: 26.891%
	I - Batch: 400 | Loss: 1.375 | Acc: 29.250% | Wgt Acc: 26.441%
	I - Batch: 450 | Loss: 1.373 | Acc: 29.111% | Wgt Acc: 26.332%
	I - Batch: 500 | Loss: 1.373 | Acc: 28.400% | Wgt Acc: 25.701%
	I - Batch: 550 | Loss: 1.373 | Acc: 28.727% | Wgt Acc: 25.998%
	I - Batch: 600 | Loss: 1.373 | Acc: 28.500% | Wgt Acc: 25.782%
	I - Batch: 650 | Loss: 1.372 | Acc: 29.385% | Wgt Acc: 26.620%
	I - Batch: 700 | Loss: 1.371 | Acc: 29.429% | Wgt Acc: 26.675%
	I - Batch: 750 | Loss: 1.373 | Acc: 29.200% | Wgt Acc: 26.441%
	I - Batch: 800 | Loss: 1.374 | Acc: 29.750% | Wgt Acc: 26.915%
	I - Batch: 850 | Loss: 1.375 | Acc: 29.294% | Wgt Acc: 26.468%
	I - Batch: 900 | Loss: 1.375 | Acc: 29.444% | Wgt Acc: 26.600%
	I - Batch: 950 | Loss: 1.376 | Acc: 29.368% | Wgt Acc: 26.527%
	I - Batch: 1000 | Loss: 1.378 | Acc: 29.100% | Wgt Acc: 26.228%
	I - Batch: 1050 | Loss: 1.378 | Acc: 28.952% | Wgt Acc: 26.094%
	I - Batch: 1100 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.115%
	I - Batch: 1150 | Loss: 1.377 | Acc: 29.304% | Wgt Acc: 26.411%
	I - Batch: 1200 | Loss: 1.377 | Acc: 29.833% | Wgt Acc: 26.892%
	I - Batch: 1250 | Loss: 1.377 | Acc: 30.080% | Wgt Acc: 27.089%
	I - Batch: 1300 | Loss: 1.377 | Acc: 30.154% | Wgt Acc: 27.156%
	I - Batch: 1350 | Loss: 1.377 | Acc: 29.852% | Wgt Acc: 26.885%
	I - Batch: 1400 | Loss: 1.376 | Acc: 30.214% | Wgt Acc: 27.224%
	I - Batch: 1450 | Loss: 1.378 | Acc: 29.793% | Wgt Acc: 26.832%
	I - Batch: 1500 | Loss: 1.378 | Acc: 29.800% | Wgt Acc: 26.835%
	I - Batch: 1550 | Loss: 1.377 | Acc: 29.935% | Wgt Acc: 26.957%
	I - Batch: 1600 | Loss: 1.377 | Acc: 29.875% | Wgt Acc: 26.918%
	I - Batch: 1650 | Loss: 1.377 | Acc: 29.879% | Wgt Acc: 26.925%
	I - Batch: 1700 | Loss: 1.377 | Acc: 29.941% | Wgt Acc: 26.981%
	I - Batch: 1750 | Loss: 1.377 | Acc: 30.000% | Wgt Acc: 27.031%
	I - Batch: 1800 | Loss: 1.377 | Acc: 29.944% | Wgt Acc: 26.984%
	I - Batch: 1850 | Loss: 1.378 | Acc: 29.514% | Wgt Acc: 26.573%
	I - Batch: 1900 | Loss: 1.378 | Acc: 29.263% | Wgt Acc: 26.354%
	I - Batch: 1950 | Loss: 1.378 | Acc: 29.538% | Wgt Acc: 26.611%
	I - Batch: 2000 | Loss: 1.378 | Acc: 29.550% | Wgt Acc: 26.625%
	I - Batch: 2050 | Loss: 1.378 | Acc: 29.463% | Wgt Acc: 26.558%
	I - Batch: 2100 | Loss: 1.378 | Acc: 29.238% | Wgt Acc: 26.358%
	I - Batch: 2150 | Loss: 1.379 | Acc: 29.023% | Wgt Acc: 26.155%
	I - Batch: 2200 | Loss: 1.379 | Acc: 29.000% | Wgt Acc: 26.129%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.844% | Wgt Acc: 25.999%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.957% | Wgt Acc: 26.102%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.894% | Wgt Acc: 26.045%
	I - Batch: 2400 | Loss: 1.379 | Acc: 29.000% | Wgt Acc: 26.138%
	I - Batch: 2450 | Loss: 1.379 | Acc: 29.020% | Wgt Acc: 26.152%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.880% | Wgt Acc: 26.025%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1394.59s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.387 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.396 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.394 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.396 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.398 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.397 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 1.392 | Acc: 26.000% | Wgt Acc: 23.009%
	I - Batch: 100 | Loss: 1.389 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.371 | Acc: 30.667% | Wgt Acc: 27.711%
	I - Batch: 200 | Loss: 1.369 | Acc: 33.000% | Wgt Acc: 29.831%
	I - Batch: 250 | Loss: 1.370 | Acc: 32.000% | Wgt Acc: 28.933%
	I - Batch: 300 | Loss: 1.372 | Acc: 31.333% | Wgt Acc: 28.271%
	I - Batch: 350 | Loss: 1.373 | Acc: 31.714% | Wgt Acc: 28.590%
	I - Batch: 400 | Loss: 1.371 | Acc: 31.750% | Wgt Acc: 28.701%
	I - Batch: 450 | Loss: 1.373 | Acc: 30.444% | Wgt Acc: 27.469%
	I - Batch: 500 | Loss: 1.373 | Acc: 31.200% | Wgt Acc: 28.172%
	I - Batch: 550 | Loss: 1.375 | Acc: 30.909% | Wgt Acc: 27.835%
	I - Batch: 600 | Loss: 1.376 | Acc: 30.333% | Wgt Acc: 27.327%
	I - Batch: 650 | Loss: 1.376 | Acc: 30.154% | Wgt Acc: 27.147%
	I - Batch: 700 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 27.018%
	I - Batch: 750 | Loss: 1.377 | Acc: 29.867% | Wgt Acc: 26.891%
	I - Batch: 800 | Loss: 1.377 | Acc: 30.000% | Wgt Acc: 27.012%
	I - Batch: 850 | Loss: 1.377 | Acc: 29.529% | Wgt Acc: 26.589%
	I - Batch: 900 | Loss: 1.376 | Acc: 29.444% | Wgt Acc: 26.546%
	I - Batch: 950 | Loss: 1.376 | Acc: 29.368% | Wgt Acc: 26.489%
	I - Batch: 1000 | Loss: 1.376 | Acc: 29.800% | Wgt Acc: 26.895%
	I - Batch: 1050 | Loss: 1.375 | Acc: 29.810% | Wgt Acc: 26.919%
	I - Batch: 1100 | Loss: 1.375 | Acc: 29.545% | Wgt Acc: 26.689%
	I - Batch: 1150 | Loss: 1.375 | Acc: 29.826% | Wgt Acc: 26.939%
	I - Batch: 1200 | Loss: 1.375 | Acc: 29.667% | Wgt Acc: 26.777%
	I - Batch: 1250 | Loss: 1.375 | Acc: 29.760% | Wgt Acc: 26.864%
	I - Batch: 1300 | Loss: 1.376 | Acc: 29.154% | Wgt Acc: 26.297%
	I - Batch: 1350 | Loss: 1.375 | Acc: 29.333% | Wgt Acc: 26.475%
	I - Batch: 1400 | Loss: 1.376 | Acc: 29.143% | Wgt Acc: 26.284%
	I - Batch: 1450 | Loss: 1.376 | Acc: 29.241% | Wgt Acc: 26.372%
	I - Batch: 1500 | Loss: 1.377 | Acc: 29.200% | Wgt Acc: 26.330%
	I - Batch: 1550 | Loss: 1.376 | Acc: 29.226% | Wgt Acc: 26.364%
	I - Batch: 1600 | Loss: 1.377 | Acc: 29.000% | Wgt Acc: 26.156%
	I - Batch: 1650 | Loss: 1.377 | Acc: 28.909% | Wgt Acc: 26.080%
	I - Batch: 1700 | Loss: 1.377 | Acc: 29.000% | Wgt Acc: 26.150%
	I - Batch: 1750 | Loss: 1.377 | Acc: 28.971% | Wgt Acc: 26.137%
	I - Batch: 1800 | Loss: 1.376 | Acc: 29.167% | Wgt Acc: 26.332%
	I - Batch: 1850 | Loss: 1.377 | Acc: 29.081% | Wgt Acc: 26.228%
	I - Batch: 1900 | Loss: 1.378 | Acc: 29.105% | Wgt Acc: 26.237%
	I - Batch: 1950 | Loss: 1.378 | Acc: 29.128% | Wgt Acc: 26.239%
	I - Batch: 2000 | Loss: 1.379 | Acc: 29.200% | Wgt Acc: 26.303%
	I - Batch: 2050 | Loss: 1.378 | Acc: 29.171% | Wgt Acc: 26.300%
	I - Batch: 2100 | Loss: 1.378 | Acc: 28.857% | Wgt Acc: 26.011%
	I - Batch: 2150 | Loss: 1.378 | Acc: 28.744% | Wgt Acc: 25.920%
	I - Batch: 2200 | Loss: 1.378 | Acc: 28.545% | Wgt Acc: 25.735%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.444% | Wgt Acc: 25.631%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.478% | Wgt Acc: 25.664%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.426% | Wgt Acc: 25.616%
	I - Batch: 2400 | Loss: 1.378 | Acc: 28.625% | Wgt Acc: 25.800%
	I - Batch: 2450 | Loss: 1.378 | Acc: 28.735% | Wgt Acc: 25.904%
	I - Batch: 2500 | Loss: 1.378 | Acc: 28.800% | Wgt Acc: 25.962%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1393.05s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.396 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 122.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 1.397 | Acc: 18.000% | Wgt Acc: 16.000%
	I - Batch: 100 | Loss: 1.395 | Acc: 21.000% | Wgt Acc: 18.667%
	I - Batch: 150 | Loss: 1.397 | Acc: 24.667% | Wgt Acc: 21.829%
	I - Batch: 200 | Loss: 1.397 | Acc: 27.000% | Wgt Acc: 23.867%
	I - Batch: 250 | Loss: 1.390 | Acc: 28.400% | Wgt Acc: 25.267%
	I - Batch: 300 | Loss: 1.385 | Acc: 29.000% | Wgt Acc: 25.951%
	I - Batch: 350 | Loss: 1.380 | Acc: 30.857% | Wgt Acc: 27.746%
	I - Batch: 400 | Loss: 1.378 | Acc: 31.750% | Wgt Acc: 28.571%
	I - Batch: 450 | Loss: 1.378 | Acc: 30.889% | Wgt Acc: 27.828%
	I - Batch: 500 | Loss: 1.377 | Acc: 30.800% | Wgt Acc: 27.760%
	I - Batch: 550 | Loss: 1.380 | Acc: 29.818% | Wgt Acc: 26.797%
	I - Batch: 600 | Loss: 1.380 | Acc: 29.833% | Wgt Acc: 26.827%
	I - Batch: 650 | Loss: 1.378 | Acc: 30.615% | Wgt Acc: 27.562%
	I - Batch: 700 | Loss: 1.379 | Acc: 30.143% | Wgt Acc: 27.103%
	I - Batch: 750 | Loss: 1.379 | Acc: 30.000% | Wgt Acc: 26.987%
	I - Batch: 800 | Loss: 1.378 | Acc: 30.500% | Wgt Acc: 27.454%
	I - Batch: 850 | Loss: 1.377 | Acc: 30.235% | Wgt Acc: 27.232%
	I - Batch: 900 | Loss: 1.378 | Acc: 30.000% | Wgt Acc: 27.000%
	I - Batch: 950 | Loss: 1.377 | Acc: 30.421% | Wgt Acc: 27.406%
	I - Batch: 1000 | Loss: 1.377 | Acc: 30.600% | Wgt Acc: 27.580%
	I - Batch: 1050 | Loss: 1.377 | Acc: 30.667% | Wgt Acc: 27.628%
	I - Batch: 1100 | Loss: 1.378 | Acc: 30.273% | Wgt Acc: 27.245%
	I - Batch: 1150 | Loss: 1.378 | Acc: 29.913% | Wgt Acc: 26.922%
	I - Batch: 1200 | Loss: 1.379 | Acc: 29.417% | Wgt Acc: 26.467%
	I - Batch: 1250 | Loss: 1.379 | Acc: 29.520% | Wgt Acc: 26.556%
	I - Batch: 1300 | Loss: 1.380 | Acc: 29.308% | Wgt Acc: 26.344%
	I - Batch: 1350 | Loss: 1.379 | Acc: 29.259% | Wgt Acc: 26.325%
	I - Batch: 1400 | Loss: 1.379 | Acc: 29.071% | Wgt Acc: 26.169%
	I - Batch: 1450 | Loss: 1.380 | Acc: 28.897% | Wgt Acc: 25.993%
	I - Batch: 1500 | Loss: 1.381 | Acc: 28.933% | Wgt Acc: 26.007%
	I - Batch: 1550 | Loss: 1.380 | Acc: 29.161% | Wgt Acc: 26.214%
	I - Batch: 1600 | Loss: 1.381 | Acc: 28.875% | Wgt Acc: 25.937%
	I - Batch: 1650 | Loss: 1.381 | Acc: 29.152% | Wgt Acc: 26.198%
	I - Batch: 1700 | Loss: 1.381 | Acc: 28.882% | Wgt Acc: 25.955%
	I - Batch: 1750 | Loss: 1.381 | Acc: 28.686% | Wgt Acc: 25.780%
	I - Batch: 1800 | Loss: 1.380 | Acc: 28.944% | Wgt Acc: 26.034%
	I - Batch: 1850 | Loss: 1.380 | Acc: 29.243% | Wgt Acc: 26.313%
	I - Batch: 1900 | Loss: 1.380 | Acc: 29.211% | Wgt Acc: 26.294%
	I - Batch: 1950 | Loss: 1.380 | Acc: 29.128% | Wgt Acc: 26.211%
	I - Batch: 2000 | Loss: 1.380 | Acc: 29.200% | Wgt Acc: 26.283%
	I - Batch: 2050 | Loss: 1.380 | Acc: 28.878% | Wgt Acc: 25.988%
	I - Batch: 2100 | Loss: 1.380 | Acc: 28.810% | Wgt Acc: 25.929%
	I - Batch: 2150 | Loss: 1.380 | Acc: 28.698% | Wgt Acc: 25.821%
	I - Batch: 2200 | Loss: 1.380 | Acc: 28.500% | Wgt Acc: 25.657%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.711% | Wgt Acc: 25.863%
	I - Batch: 2300 | Loss: 1.379 | Acc: 28.652% | Wgt Acc: 25.810%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.851% | Wgt Acc: 25.999%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.875% | Wgt Acc: 26.021%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.816% | Wgt Acc: 25.963%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.720% | Wgt Acc: 25.874%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1396.76s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.387 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.396 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.394 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 122.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 1.367 | Acc: 32.000% | Wgt Acc: 29.091%
	I - Batch: 100 | Loss: 1.373 | Acc: 33.000% | Wgt Acc: 29.864%
	I - Batch: 150 | Loss: 1.365 | Acc: 29.333% | Wgt Acc: 26.748%
	I - Batch: 200 | Loss: 1.373 | Acc: 27.500% | Wgt Acc: 24.887%
	I - Batch: 250 | Loss: 1.373 | Acc: 29.600% | Wgt Acc: 26.787%
	I - Batch: 300 | Loss: 1.372 | Acc: 29.333% | Wgt Acc: 26.626%
	I - Batch: 350 | Loss: 1.371 | Acc: 29.714% | Wgt Acc: 26.960%
	I - Batch: 400 | Loss: 1.373 | Acc: 29.000% | Wgt Acc: 26.259%
	I - Batch: 450 | Loss: 1.375 | Acc: 28.000% | Wgt Acc: 25.301%
	I - Batch: 500 | Loss: 1.377 | Acc: 27.400% | Wgt Acc: 24.707%
	I - Batch: 550 | Loss: 1.378 | Acc: 27.636% | Wgt Acc: 24.867%
	I - Batch: 600 | Loss: 1.378 | Acc: 28.000% | Wgt Acc: 25.206%
	I - Batch: 650 | Loss: 1.378 | Acc: 28.462% | Wgt Acc: 25.606%
	I - Batch: 700 | Loss: 1.379 | Acc: 28.286% | Wgt Acc: 25.434%
	I - Batch: 750 | Loss: 1.378 | Acc: 28.800% | Wgt Acc: 25.930%
	I - Batch: 800 | Loss: 1.377 | Acc: 29.250% | Wgt Acc: 26.366%
	I - Batch: 850 | Loss: 1.376 | Acc: 29.529% | Wgt Acc: 26.645%
	I - Batch: 900 | Loss: 1.376 | Acc: 30.000% | Wgt Acc: 27.074%
	I - Batch: 950 | Loss: 1.375 | Acc: 30.105% | Wgt Acc: 27.186%
	I - Batch: 1000 | Loss: 1.375 | Acc: 30.400% | Wgt Acc: 27.486%
	I - Batch: 1050 | Loss: 1.377 | Acc: 29.810% | Wgt Acc: 26.913%
	I - Batch: 1100 | Loss: 1.376 | Acc: 30.091% | Wgt Acc: 27.181%
	I - Batch: 1150 | Loss: 1.376 | Acc: 30.348% | Wgt Acc: 27.426%
	I - Batch: 1200 | Loss: 1.375 | Acc: 30.833% | Wgt Acc: 27.888%
	I - Batch: 1250 | Loss: 1.375 | Acc: 30.480% | Wgt Acc: 27.574%
	I - Batch: 1300 | Loss: 1.376 | Acc: 30.077% | Wgt Acc: 27.181%
	I - Batch: 1350 | Loss: 1.376 | Acc: 29.926% | Wgt Acc: 27.028%
	I - Batch: 1400 | Loss: 1.376 | Acc: 29.929% | Wgt Acc: 27.058%
	I - Batch: 1450 | Loss: 1.375 | Acc: 29.724% | Wgt Acc: 26.879%
	I - Batch: 1500 | Loss: 1.376 | Acc: 29.733% | Wgt Acc: 26.876%
	I - Batch: 1550 | Loss: 1.376 | Acc: 29.677% | Wgt Acc: 26.810%
	I - Batch: 1600 | Loss: 1.378 | Acc: 29.312% | Wgt Acc: 26.445%
	I - Batch: 1650 | Loss: 1.379 | Acc: 28.970% | Wgt Acc: 26.120%
	I - Batch: 1700 | Loss: 1.379 | Acc: 28.882% | Wgt Acc: 26.037%
	I - Batch: 1750 | Loss: 1.380 | Acc: 28.743% | Wgt Acc: 25.898%
	I - Batch: 1800 | Loss: 1.380 | Acc: 28.722% | Wgt Acc: 25.873%
	I - Batch: 1850 | Loss: 1.380 | Acc: 28.595% | Wgt Acc: 25.758%
	I - Batch: 1900 | Loss: 1.380 | Acc: 28.368% | Wgt Acc: 25.551%
	I - Batch: 1950 | Loss: 1.380 | Acc: 28.513% | Wgt Acc: 25.684%
	I - Batch: 2000 | Loss: 1.380 | Acc: 28.550% | Wgt Acc: 25.718%
	I - Batch: 2050 | Loss: 1.379 | Acc: 28.927% | Wgt Acc: 26.075%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.952% | Wgt Acc: 26.103%
	I - Batch: 2150 | Loss: 1.379 | Acc: 28.791% | Wgt Acc: 25.954%
	I - Batch: 2200 | Loss: 1.379 | Acc: 28.591% | Wgt Acc: 25.760%
	I - Batch: 2250 | Loss: 1.379 | Acc: 28.489% | Wgt Acc: 25.673%
	I - Batch: 2300 | Loss: 1.380 | Acc: 28.304% | Wgt Acc: 25.504%
	I - Batch: 2350 | Loss: 1.380 | Acc: 28.170% | Wgt Acc: 25.381%
	I - Batch: 2400 | Loss: 1.380 | Acc: 27.958% | Wgt Acc: 25.176%
	I - Batch: 2450 | Loss: 1.380 | Acc: 27.918% | Wgt Acc: 25.152%
	I - Batch: 2500 | Loss: 1.380 | Acc: 27.920% | Wgt Acc: 25.160%
I - num batch: 2547
I - Train -- Loss: 1.380 | Acc: 27.994% | Wgt Acc: 25.230% | LR: 1.250000e-04 | Dur: 1392.97s
I - Confusion Matrix: [row->prediction - col->label]
[[174. 145. 195. 159.]
 [  0.   0.   0.   0.]
 [523. 433. 539. 379.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.384 | Acc: 30.000% | Wgt Acc: 26.906%
	I - Batch: 100 | Loss: 1.394 | Acc: 26.000% | Wgt Acc: 23.111%
	I - Batch: 150 | Loss: 1.392 | Acc: 26.000% | Wgt Acc: 23.145%
	I - Batch: 200 | Loss: 1.393 | Acc: 28.000% | Wgt Acc: 24.917%
	I - Batch: 250 | Loss: 1.395 | Acc: 27.200% | Wgt Acc: 24.156%
	I - Batch: 300 | Loss: 1.395 | Acc: 28.667% | Wgt Acc: 25.463%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 121.36s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 1.369 | Acc: 38.000% | Wgt Acc: 34.545%
	I - Batch: 100 | Loss: 1.373 | Acc: 33.000% | Wgt Acc: 29.864%
	I - Batch: 150 | Loss: 1.372 | Acc: 33.333% | Wgt Acc: 30.211%
	I - Batch: 200 | Loss: 1.378 | Acc: 32.000% | Wgt Acc: 28.861%
	I - Batch: 250 | Loss: 1.378 | Acc: 30.000% | Wgt Acc: 27.051%
	I - Batch: 300 | Loss: 1.373 | Acc: 30.667% | Wgt Acc: 27.774%
	I - Batch: 350 | Loss: 1.373 | Acc: 30.000% | Wgt Acc: 27.184%
	I - Batch: 400 | Loss: 1.376 | Acc: 29.250% | Wgt Acc: 26.426%
	I - Batch: 450 | Loss: 1.376 | Acc: 28.667% | Wgt Acc: 25.917%
	I - Batch: 500 | Loss: 1.378 | Acc: 29.400% | Wgt Acc: 26.534%
	I - Batch: 550 | Loss: 1.377 | Acc: 29.273% | Wgt Acc: 26.437%
	I - Batch: 600 | Loss: 1.378 | Acc: 28.833% | Wgt Acc: 26.025%
	I - Batch: 650 | Loss: 1.378 | Acc: 28.923% | Wgt Acc: 26.093%
	I - Batch: 700 | Loss: 1.379 | Acc: 28.571% | Wgt Acc: 25.748%
	I - Batch: 750 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.180%
	I - Batch: 800 | Loss: 1.382 | Acc: 27.750% | Wgt Acc: 24.951%
	I - Batch: 850 | Loss: 1.382 | Acc: 27.647% | Wgt Acc: 24.855%
	I - Batch: 900 | Loss: 1.383 | Acc: 26.889% | Wgt Acc: 24.140%
	I - Batch: 950 | Loss: 1.383 | Acc: 26.526% | Wgt Acc: 23.830%
	I - Batch: 1000 | Loss: 1.382 | Acc: 27.000% | Wgt Acc: 24.259%
	I - Batch: 1050 | Loss: 1.382 | Acc: 26.667% | Wgt Acc: 23.962%
	I - Batch: 1100 | Loss: 1.382 | Acc: 26.455% | Wgt Acc: 23.770%
	I - Batch: 1150 | Loss: 1.383 | Acc: 26.261% | Wgt Acc: 23.571%
	I - Batch: 1200 | Loss: 1.383 | Acc: 26.250% | Wgt Acc: 23.573%
	I - Batch: 1250 | Loss: 1.382 | Acc: 26.560% | Wgt Acc: 23.868%
	I - Batch: 1300 | Loss: 1.382 | Acc: 26.385% | Wgt Acc: 23.712%
	I - Batch: 1350 | Loss: 1.382 | Acc: 26.370% | Wgt Acc: 23.702%
	I - Batch: 1400 | Loss: 1.382 | Acc: 26.429% | Wgt Acc: 23.760%
	I - Batch: 1450 | Loss: 1.382 | Acc: 26.483% | Wgt Acc: 23.803%
	I - Batch: 1500 | Loss: 1.381 | Acc: 26.333% | Wgt Acc: 23.681%
	I - Batch: 1550 | Loss: 1.381 | Acc: 26.516% | Wgt Acc: 23.854%
	I - Batch: 1600 | Loss: 1.381 | Acc: 26.438% | Wgt Acc: 23.781%
	I - Batch: 1650 | Loss: 1.381 | Acc: 26.242% | Wgt Acc: 23.594%
	I - Batch: 1700 | Loss: 1.381 | Acc: 26.412% | Wgt Acc: 23.753%
	I - Batch: 1750 | Loss: 1.381 | Acc: 26.457% | Wgt Acc: 23.802%
	I - Batch: 1800 | Loss: 1.380 | Acc: 26.611% | Wgt Acc: 23.947%
	I - Batch: 1850 | Loss: 1.380 | Acc: 26.703% | Wgt Acc: 24.036%
	I - Batch: 1900 | Loss: 1.380 | Acc: 26.684% | Wgt Acc: 24.014%
	I - Batch: 1950 | Loss: 1.380 | Acc: 26.718% | Wgt Acc: 24.048%
	I - Batch: 2000 | Loss: 1.380 | Acc: 27.000% | Wgt Acc: 24.305%
	I - Batch: 2050 | Loss: 1.380 | Acc: 26.927% | Wgt Acc: 24.232%
	I - Batch: 2100 | Loss: 1.380 | Acc: 27.095% | Wgt Acc: 24.397%
	I - Batch: 2150 | Loss: 1.380 | Acc: 27.349% | Wgt Acc: 24.631%
	I - Batch: 2200 | Loss: 1.379 | Acc: 27.500% | Wgt Acc: 24.770%
	I - Batch: 2250 | Loss: 1.379 | Acc: 27.556% | Wgt Acc: 24.822%
	I - Batch: 2300 | Loss: 1.380 | Acc: 27.565% | Wgt Acc: 24.819%
	I - Batch: 2350 | Loss: 1.380 | Acc: 27.617% | Wgt Acc: 24.871%
	I - Batch: 2400 | Loss: 1.380 | Acc: 27.625% | Wgt Acc: 24.880%
	I - Batch: 2450 | Loss: 1.379 | Acc: 27.633% | Wgt Acc: 24.899%
	I - Batch: 2500 | Loss: 1.379 | Acc: 27.880% | Wgt Acc: 25.126%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 27.915% | Wgt Acc: 25.159% | LR: 1.250000e-04 | Dur: 1393.87s
I - Confusion Matrix: [row->prediction - col->label]
[[362. 304. 385. 290.]
 [  0.   0.   0.   0.]
 [335. 274. 349. 248.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.395 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.393 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.397 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 1.388 | Acc: 22.000% | Wgt Acc: 19.731%
	I - Batch: 100 | Loss: 1.378 | Acc: 31.000% | Wgt Acc: 27.991%
	I - Batch: 150 | Loss: 1.378 | Acc: 30.667% | Wgt Acc: 27.669%
	I - Batch: 200 | Loss: 1.384 | Acc: 29.000% | Wgt Acc: 26.009%
	I - Batch: 250 | Loss: 1.381 | Acc: 27.600% | Wgt Acc: 24.865%
	I - Batch: 300 | Loss: 1.383 | Acc: 27.333% | Wgt Acc: 24.588%
	I - Batch: 350 | Loss: 1.380 | Acc: 28.286% | Wgt Acc: 25.499%
	I - Batch: 400 | Loss: 1.382 | Acc: 28.000% | Wgt Acc: 25.183%
	I - Batch: 450 | Loss: 1.379 | Acc: 28.222% | Wgt Acc: 25.451%
	I - Batch: 500 | Loss: 1.377 | Acc: 27.800% | Wgt Acc: 25.124%
	I - Batch: 550 | Loss: 1.376 | Acc: 28.727% | Wgt Acc: 25.976%
	I - Batch: 600 | Loss: 1.374 | Acc: 29.000% | Wgt Acc: 26.264%
	I - Batch: 650 | Loss: 1.374 | Acc: 29.231% | Wgt Acc: 26.453%
	I - Batch: 700 | Loss: 1.376 | Acc: 29.000% | Wgt Acc: 26.219%
	I - Batch: 750 | Loss: 1.375 | Acc: 29.200% | Wgt Acc: 26.425%
	I - Batch: 800 | Loss: 1.374 | Acc: 29.500% | Wgt Acc: 26.704%
	I - Batch: 850 | Loss: 1.375 | Acc: 29.176% | Wgt Acc: 26.404%
	I - Batch: 900 | Loss: 1.378 | Acc: 28.778% | Wgt Acc: 25.978%
	I - Batch: 950 | Loss: 1.377 | Acc: 29.158% | Wgt Acc: 26.331%
	I - Batch: 1000 | Loss: 1.378 | Acc: 28.900% | Wgt Acc: 26.089%
	I - Batch: 1050 | Loss: 1.378 | Acc: 29.048% | Wgt Acc: 26.214%
	I - Batch: 1100 | Loss: 1.377 | Acc: 28.636% | Wgt Acc: 25.851%
	I - Batch: 1150 | Loss: 1.378 | Acc: 28.000% | Wgt Acc: 25.270%
	I - Batch: 1200 | Loss: 1.378 | Acc: 27.750% | Wgt Acc: 25.033%
	I - Batch: 1250 | Loss: 1.378 | Acc: 27.840% | Wgt Acc: 25.126%
	I - Batch: 1300 | Loss: 1.379 | Acc: 27.769% | Wgt Acc: 25.043%
	I - Batch: 1350 | Loss: 1.379 | Acc: 27.926% | Wgt Acc: 25.184%
	I - Batch: 1400 | Loss: 1.379 | Acc: 28.214% | Wgt Acc: 25.439%
	I - Batch: 1450 | Loss: 1.379 | Acc: 28.483% | Wgt Acc: 25.680%
	I - Batch: 1500 | Loss: 1.379 | Acc: 28.267% | Wgt Acc: 25.485%
	I - Batch: 1550 | Loss: 1.378 | Acc: 28.645% | Wgt Acc: 25.833%
	I - Batch: 1600 | Loss: 1.379 | Acc: 28.250% | Wgt Acc: 25.454%
	I - Batch: 1650 | Loss: 1.379 | Acc: 28.424% | Wgt Acc: 25.621%
	I - Batch: 1700 | Loss: 1.379 | Acc: 28.353% | Wgt Acc: 25.543%
	I - Batch: 1750 | Loss: 1.379 | Acc: 28.571% | Wgt Acc: 25.747%
	I - Batch: 1800 | Loss: 1.379 | Acc: 28.833% | Wgt Acc: 25.992%
	I - Batch: 1850 | Loss: 1.379 | Acc: 28.649% | Wgt Acc: 25.806%
	I - Batch: 1900 | Loss: 1.379 | Acc: 28.526% | Wgt Acc: 25.705%
	I - Batch: 1950 | Loss: 1.379 | Acc: 28.462% | Wgt Acc: 25.656%
	I - Batch: 2000 | Loss: 1.378 | Acc: 28.450% | Wgt Acc: 25.657%
	I - Batch: 2050 | Loss: 1.378 | Acc: 28.488% | Wgt Acc: 25.690%
	I - Batch: 2100 | Loss: 1.378 | Acc: 28.714% | Wgt Acc: 25.899%
	I - Batch: 2150 | Loss: 1.378 | Acc: 28.744% | Wgt Acc: 25.926%
	I - Batch: 2200 | Loss: 1.378 | Acc: 28.773% | Wgt Acc: 25.953%
	I - Batch: 2250 | Loss: 1.378 | Acc: 28.711% | Wgt Acc: 25.897%
	I - Batch: 2300 | Loss: 1.378 | Acc: 28.609% | Wgt Acc: 25.799%
	I - Batch: 2350 | Loss: 1.378 | Acc: 28.681% | Wgt Acc: 25.858%
	I - Batch: 2400 | Loss: 1.378 | Acc: 28.833% | Wgt Acc: 25.993%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.653% | Wgt Acc: 25.825%
	I - Batch: 2500 | Loss: 1.378 | Acc: 28.840% | Wgt Acc: 26.001%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1395.18s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.397 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 120.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 1.386 | Acc: 22.000% | Wgt Acc: 19.643%
	I - Batch: 100 | Loss: 1.383 | Acc: 22.000% | Wgt Acc: 19.731%
	I - Batch: 150 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 21.397%
	I - Batch: 200 | Loss: 1.385 | Acc: 26.000% | Wgt Acc: 23.240%
	I - Batch: 250 | Loss: 1.383 | Acc: 26.400% | Wgt Acc: 23.656%
	I - Batch: 300 | Loss: 1.376 | Acc: 27.667% | Wgt Acc: 24.962%
	I - Batch: 350 | Loss: 1.375 | Acc: 27.714% | Wgt Acc: 25.032%
	I - Batch: 400 | Loss: 1.379 | Acc: 26.500% | Wgt Acc: 23.847%
	I - Batch: 450 | Loss: 1.381 | Acc: 25.778% | Wgt Acc: 23.142%
	I - Batch: 500 | Loss: 1.380 | Acc: 26.400% | Wgt Acc: 23.741%
	I - Batch: 550 | Loss: 1.381 | Acc: 25.636% | Wgt Acc: 23.039%
	I - Batch: 600 | Loss: 1.380 | Acc: 26.333% | Wgt Acc: 23.688%
	I - Batch: 650 | Loss: 1.382 | Acc: 25.846% | Wgt Acc: 23.212%
	I - Batch: 700 | Loss: 1.382 | Acc: 26.286% | Wgt Acc: 23.612%
	I - Batch: 750 | Loss: 1.383 | Acc: 26.533% | Wgt Acc: 23.832%
	I - Batch: 800 | Loss: 1.382 | Acc: 26.750% | Wgt Acc: 24.052%
	I - Batch: 850 | Loss: 1.381 | Acc: 27.412% | Wgt Acc: 24.656%
	I - Batch: 900 | Loss: 1.380 | Acc: 28.000% | Wgt Acc: 25.213%
	I - Batch: 950 | Loss: 1.379 | Acc: 28.737% | Wgt Acc: 25.883%
	I - Batch: 1000 | Loss: 1.379 | Acc: 28.300% | Wgt Acc: 25.484%
	I - Batch: 1050 | Loss: 1.380 | Acc: 28.286% | Wgt Acc: 25.461%
	I - Batch: 1100 | Loss: 1.379 | Acc: 28.364% | Wgt Acc: 25.542%
	I - Batch: 1150 | Loss: 1.380 | Acc: 28.261% | Wgt Acc: 25.430%
	I - Batch: 1200 | Loss: 1.379 | Acc: 28.500% | Wgt Acc: 25.666%
	I - Batch: 1250 | Loss: 1.380 | Acc: 28.400% | Wgt Acc: 25.558%
	I - Batch: 1300 | Loss: 1.380 | Acc: 28.154% | Wgt Acc: 25.346%
	I - Batch: 1350 | Loss: 1.379 | Acc: 28.222% | Wgt Acc: 25.430%
	I - Batch: 1400 | Loss: 1.379 | Acc: 28.214% | Wgt Acc: 25.422%
	I - Batch: 1450 | Loss: 1.378 | Acc: 28.345% | Wgt Acc: 25.560%
	I - Batch: 1500 | Loss: 1.379 | Acc: 28.133% | Wgt Acc: 25.357%
	I - Batch: 1550 | Loss: 1.378 | Acc: 28.452% | Wgt Acc: 25.669%
	I - Batch: 1600 | Loss: 1.378 | Acc: 28.500% | Wgt Acc: 25.708%
	I - Batch: 1650 | Loss: 1.378 | Acc: 28.303% | Wgt Acc: 25.523%
	I - Batch: 1700 | Loss: 1.379 | Acc: 28.294% | Wgt Acc: 25.497%
	I - Batch: 1750 | Loss: 1.379 | Acc: 28.171% | Wgt Acc: 25.383%
	I - Batch: 1800 | Loss: 1.379 | Acc: 28.278% | Wgt Acc: 25.479%
	I - Batch: 1850 | Loss: 1.379 | Acc: 28.541% | Wgt Acc: 25.722%
	I - Batch: 1900 | Loss: 1.378 | Acc: 28.737% | Wgt Acc: 25.904%
	I - Batch: 1950 | Loss: 1.378 | Acc: 29.026% | Wgt Acc: 26.176%
	I - Batch: 2000 | Loss: 1.377 | Acc: 29.050% | Wgt Acc: 26.207%
	I - Batch: 2050 | Loss: 1.377 | Acc: 29.220% | Wgt Acc: 26.356%
	I - Batch: 2100 | Loss: 1.377 | Acc: 29.238% | Wgt Acc: 26.375%
	I - Batch: 2150 | Loss: 1.376 | Acc: 29.349% | Wgt Acc: 26.490%
	I - Batch: 2200 | Loss: 1.376 | Acc: 29.500% | Wgt Acc: 26.626%
	I - Batch: 2250 | Loss: 1.378 | Acc: 29.200% | Wgt Acc: 26.338%
	I - Batch: 2300 | Loss: 1.377 | Acc: 29.217% | Wgt Acc: 26.358%
	I - Batch: 2350 | Loss: 1.377 | Acc: 29.191% | Wgt Acc: 26.337%
	I - Batch: 2400 | Loss: 1.378 | Acc: 29.208% | Wgt Acc: 26.341%
	I - Batch: 2450 | Loss: 1.378 | Acc: 28.980% | Wgt Acc: 26.125%
	I - Batch: 2500 | Loss: 1.378 | Acc: 29.040% | Wgt Acc: 26.181%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1133.70s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.393 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 65.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 1.362 | Acc: 24.000% | Wgt Acc: 21.918%
	I - Batch: 100 | Loss: 1.374 | Acc: 25.000% | Wgt Acc: 22.676%
	I - Batch: 150 | Loss: 1.383 | Acc: 23.333% | Wgt Acc: 21.021%
	I - Batch: 200 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 21.501%
	I - Batch: 250 | Loss: 1.388 | Acc: 24.800% | Wgt Acc: 22.182%
	I - Batch: 300 | Loss: 1.386 | Acc: 25.333% | Wgt Acc: 22.704%
	I - Batch: 350 | Loss: 1.382 | Acc: 27.143% | Wgt Acc: 24.390%
	I - Batch: 400 | Loss: 1.382 | Acc: 26.500% | Wgt Acc: 23.834%
	I - Batch: 450 | Loss: 1.383 | Acc: 25.778% | Wgt Acc: 23.154%
	I - Batch: 500 | Loss: 1.381 | Acc: 26.200% | Wgt Acc: 23.582%
	I - Batch: 550 | Loss: 1.381 | Acc: 26.182% | Wgt Acc: 23.549%
	I - Batch: 600 | Loss: 1.382 | Acc: 27.000% | Wgt Acc: 24.270%
	I - Batch: 650 | Loss: 1.380 | Acc: 26.462% | Wgt Acc: 23.814%
	I - Batch: 700 | Loss: 1.381 | Acc: 26.714% | Wgt Acc: 24.028%
	I - Batch: 750 | Loss: 1.381 | Acc: 27.333% | Wgt Acc: 24.588%
	I - Batch: 800 | Loss: 1.380 | Acc: 27.750% | Wgt Acc: 24.972%
	I - Batch: 850 | Loss: 1.380 | Acc: 28.235% | Wgt Acc: 25.410%
	I - Batch: 900 | Loss: 1.379 | Acc: 28.222% | Wgt Acc: 25.425%
	I - Batch: 950 | Loss: 1.379 | Acc: 27.684% | Wgt Acc: 24.947%
	I - Batch: 1000 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.225%
	I - Batch: 1050 | Loss: 1.379 | Acc: 28.190% | Wgt Acc: 25.397%
	I - Batch: 1100 | Loss: 1.378 | Acc: 28.273% | Wgt Acc: 25.481%
	I - Batch: 1150 | Loss: 1.379 | Acc: 27.913% | Wgt Acc: 25.147%
	I - Batch: 1200 | Loss: 1.377 | Acc: 28.667% | Wgt Acc: 25.860%
	I - Batch: 1250 | Loss: 1.378 | Acc: 28.720% | Wgt Acc: 25.902%
	I - Batch: 1300 | Loss: 1.378 | Acc: 28.692% | Wgt Acc: 25.876%
	I - Batch: 1350 | Loss: 1.378 | Acc: 28.519% | Wgt Acc: 25.705%
	I - Batch: 1400 | Loss: 1.378 | Acc: 28.857% | Wgt Acc: 26.018%
	I - Batch: 1450 | Loss: 1.377 | Acc: 29.310% | Wgt Acc: 26.447%
	I - Batch: 1500 | Loss: 1.377 | Acc: 29.467% | Wgt Acc: 26.590%
	I - Batch: 1550 | Loss: 1.377 | Acc: 29.484% | Wgt Acc: 26.593%
	I - Batch: 1600 | Loss: 1.376 | Acc: 29.938% | Wgt Acc: 27.032%
	I - Batch: 1650 | Loss: 1.375 | Acc: 29.939% | Wgt Acc: 27.054%
	I - Batch: 1700 | Loss: 1.375 | Acc: 29.765% | Wgt Acc: 26.893%
	I - Batch: 1750 | Loss: 1.375 | Acc: 29.600% | Wgt Acc: 26.756%
	I - Batch: 1800 | Loss: 1.376 | Acc: 29.222% | Wgt Acc: 26.396%
	I - Batch: 1850 | Loss: 1.376 | Acc: 29.243% | Wgt Acc: 26.403%
	I - Batch: 1900 | Loss: 1.376 | Acc: 29.263% | Wgt Acc: 26.432%
	I - Batch: 1950 | Loss: 1.376 | Acc: 29.436% | Wgt Acc: 26.589%
	I - Batch: 2000 | Loss: 1.376 | Acc: 29.350% | Wgt Acc: 26.510%
	I - Batch: 2050 | Loss: 1.376 | Acc: 29.512% | Wgt Acc: 26.655%
	I - Batch: 2100 | Loss: 1.376 | Acc: 29.524% | Wgt Acc: 26.661%
	I - Batch: 2150 | Loss: 1.376 | Acc: 29.535% | Wgt Acc: 26.669%
	I - Batch: 2200 | Loss: 1.377 | Acc: 29.182% | Wgt Acc: 26.330%
	I - Batch: 2250 | Loss: 1.377 | Acc: 29.156% | Wgt Acc: 26.300%
	I - Batch: 2300 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.141%
	I - Batch: 2350 | Loss: 1.378 | Acc: 28.936% | Wgt Acc: 26.086%
	I - Batch: 2400 | Loss: 1.378 | Acc: 29.000% | Wgt Acc: 26.146%
	I - Batch: 2450 | Loss: 1.378 | Acc: 28.816% | Wgt Acc: 25.975%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.760% | Wgt Acc: 25.924%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.779% | Wgt Acc: 25.938% | LR: 1.250000e-04 | Dur: 884.41s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   1.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 733. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.386 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.395 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.393 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.395 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.398 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.398 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.397 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 66.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 1.371 | Acc: 36.000% | Wgt Acc: 32.579%
	I - Batch: 100 | Loss: 1.369 | Acc: 35.000% | Wgt Acc: 31.818%
	I - Batch: 150 | Loss: 1.379 | Acc: 32.667% | Wgt Acc: 29.385%
	I - Batch: 200 | Loss: 1.386 | Acc: 31.000% | Wgt Acc: 27.740%
	I - Batch: 250 | Loss: 1.383 | Acc: 32.000% | Wgt Acc: 28.751%
	I - Batch: 300 | Loss: 1.382 | Acc: 30.333% | Wgt Acc: 27.266%
	I - Batch: 350 | Loss: 1.378 | Acc: 30.286% | Wgt Acc: 27.355%
	I - Batch: 400 | Loss: 1.380 | Acc: 28.250% | Wgt Acc: 25.422%
	I - Batch: 450 | Loss: 1.378 | Acc: 29.556% | Wgt Acc: 26.653%
	I - Batch: 500 | Loss: 1.380 | Acc: 30.000% | Wgt Acc: 27.027%
	I - Batch: 550 | Loss: 1.381 | Acc: 29.455% | Wgt Acc: 26.503%
	I - Batch: 600 | Loss: 1.382 | Acc: 28.833% | Wgt Acc: 25.908%
	I - Batch: 650 | Loss: 1.382 | Acc: 28.769% | Wgt Acc: 25.864%
	I - Batch: 700 | Loss: 1.383 | Acc: 28.143% | Wgt Acc: 25.256%
	I - Batch: 750 | Loss: 1.381 | Acc: 28.800% | Wgt Acc: 25.923%
	I - Batch: 800 | Loss: 1.380 | Acc: 29.500% | Wgt Acc: 26.584%
	I - Batch: 850 | Loss: 1.380 | Acc: 29.294% | Wgt Acc: 26.405%
	I - Batch: 900 | Loss: 1.380 | Acc: 29.000% | Wgt Acc: 26.126%
	I - Batch: 950 | Loss: 1.380 | Acc: 29.684% | Wgt Acc: 26.749%
	I - Batch: 1000 | Loss: 1.379 | Acc: 30.200% | Wgt Acc: 27.232%
	I - Batch: 1050 | Loss: 1.378 | Acc: 30.095% | Wgt Acc: 27.148%
	I - Batch: 1100 | Loss: 1.379 | Acc: 30.000% | Wgt Acc: 27.055%
	I - Batch: 1150 | Loss: 1.379 | Acc: 29.565% | Wgt Acc: 26.646%
	I - Batch: 1200 | Loss: 1.380 | Acc: 29.083% | Wgt Acc: 26.177%
	I - Batch: 1250 | Loss: 1.381 | Acc: 29.040% | Wgt Acc: 26.120%
	I - Batch: 1300 | Loss: 1.381 | Acc: 29.231% | Wgt Acc: 26.293%
	I - Batch: 1350 | Loss: 1.381 | Acc: 28.815% | Wgt Acc: 25.912%
	I - Batch: 1400 | Loss: 1.380 | Acc: 29.214% | Wgt Acc: 26.281%
	I - Batch: 1450 | Loss: 1.379 | Acc: 29.586% | Wgt Acc: 26.642%
	I - Batch: 1500 | Loss: 1.379 | Acc: 29.333% | Wgt Acc: 26.430%
	I - Batch: 1550 | Loss: 1.379 | Acc: 29.161% | Wgt Acc: 26.279%
	I - Batch: 1600 | Loss: 1.379 | Acc: 29.125% | Wgt Acc: 26.242%
	I - Batch: 1650 | Loss: 1.379 | Acc: 28.788% | Wgt Acc: 25.931%
	I - Batch: 1700 | Loss: 1.379 | Acc: 29.294% | Wgt Acc: 26.391%
	I - Batch: 1750 | Loss: 1.378 | Acc: 29.314% | Wgt Acc: 26.430%
	I - Batch: 1800 | Loss: 1.378 | Acc: 29.278% | Wgt Acc: 26.396%
	I - Batch: 1850 | Loss: 1.378 | Acc: 29.189% | Wgt Acc: 26.325%
	I - Batch: 1900 | Loss: 1.379 | Acc: 28.947% | Wgt Acc: 26.097%
	I - Batch: 1950 | Loss: 1.378 | Acc: 28.872% | Wgt Acc: 26.035%
	I - Batch: 2000 | Loss: 1.378 | Acc: 28.850% | Wgt Acc: 26.023%
	I - Batch: 2050 | Loss: 1.378 | Acc: 28.732% | Wgt Acc: 25.916%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.476% | Wgt Acc: 25.674%
	I - Batch: 2150 | Loss: 1.378 | Acc: 28.651% | Wgt Acc: 25.842%
	I - Batch: 2200 | Loss: 1.379 | Acc: 28.409% | Wgt Acc: 25.612%
	I - Batch: 2250 | Loss: 1.378 | Acc: 28.356% | Wgt Acc: 25.569%
	I - Batch: 2300 | Loss: 1.378 | Acc: 28.391% | Wgt Acc: 25.600%
	I - Batch: 2350 | Loss: 1.378 | Acc: 28.596% | Wgt Acc: 25.801%
	I - Batch: 2400 | Loss: 1.378 | Acc: 28.542% | Wgt Acc: 25.740%
	I - Batch: 2450 | Loss: 1.378 | Acc: 28.735% | Wgt Acc: 25.920%
	I - Batch: 2500 | Loss: 1.378 | Acc: 28.680% | Wgt Acc: 25.856%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.583% | Wgt Acc: 25.761% | LR: 1.250000e-04 | Dur: 883.51s
I - Confusion Matrix: [row->prediction - col->label]
[[  3.   2.   9.   3.]
 [  0.   0.   0.   0.]
 [694. 576. 725. 535.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.384 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.392 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.391 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.393 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.395 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.395 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 67.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 1.365 | Acc: 26.000% | Wgt Acc: 23.853%
	I - Batch: 100 | Loss: 1.371 | Acc: 25.000% | Wgt Acc: 22.727%
	I - Batch: 150 | Loss: 1.371 | Acc: 28.667% | Wgt Acc: 26.021%
	I - Batch: 200 | Loss: 1.374 | Acc: 29.000% | Wgt Acc: 26.274%
	I - Batch: 250 | Loss: 1.373 | Acc: 28.400% | Wgt Acc: 25.725%
	I - Batch: 300 | Loss: 1.379 | Acc: 27.000% | Wgt Acc: 24.343%
	I - Batch: 350 | Loss: 1.377 | Acc: 27.143% | Wgt Acc: 24.500%
	I - Batch: 400 | Loss: 1.379 | Acc: 26.500% | Wgt Acc: 23.887%
	I - Batch: 450 | Loss: 1.379 | Acc: 27.111% | Wgt Acc: 24.449%
	I - Batch: 500 | Loss: 1.378 | Acc: 27.600% | Wgt Acc: 24.921%
	I - Batch: 550 | Loss: 1.376 | Acc: 28.545% | Wgt Acc: 25.812%
	I - Batch: 600 | Loss: 1.375 | Acc: 28.833% | Wgt Acc: 26.084%
	I - Batch: 650 | Loss: 1.375 | Acc: 28.769% | Wgt Acc: 26.026%
	I - Batch: 700 | Loss: 1.375 | Acc: 29.143% | Wgt Acc: 26.340%
	I - Batch: 750 | Loss: 1.375 | Acc: 29.867% | Wgt Acc: 27.012%
	I - Batch: 800 | Loss: 1.376 | Acc: 29.625% | Wgt Acc: 26.780%
	I - Batch: 850 | Loss: 1.375 | Acc: 29.765% | Wgt Acc: 26.915%
	I - Batch: 900 | Loss: 1.377 | Acc: 29.333% | Wgt Acc: 26.486%
	I - Batch: 950 | Loss: 1.377 | Acc: 29.263% | Wgt Acc: 26.432%
	I - Batch: 1000 | Loss: 1.377 | Acc: 29.500% | Wgt Acc: 26.643%
	I - Batch: 1050 | Loss: 1.378 | Acc: 28.571% | Wgt Acc: 25.784%
	I - Batch: 1100 | Loss: 1.379 | Acc: 28.000% | Wgt Acc: 25.241%
	I - Batch: 1150 | Loss: 1.379 | Acc: 28.522% | Wgt Acc: 25.715%
	I - Batch: 1200 | Loss: 1.380 | Acc: 28.500% | Wgt Acc: 25.676%
	I - Batch: 1250 | Loss: 1.379 | Acc: 28.720% | Wgt Acc: 25.893%
	I - Batch: 1300 | Loss: 1.379 | Acc: 28.923% | Wgt Acc: 26.084%
	I - Batch: 1350 | Loss: 1.378 | Acc: 29.111% | Wgt Acc: 26.261%
	I - Batch: 1400 | Loss: 1.378 | Acc: 29.429% | Wgt Acc: 26.555%
	I - Batch: 1450 | Loss: 1.376 | Acc: 29.793% | Wgt Acc: 26.916%
	I - Batch: 1500 | Loss: 1.377 | Acc: 29.667% | Wgt Acc: 26.791%
	I - Batch: 1550 | Loss: 1.378 | Acc: 29.548% | Wgt Acc: 26.651%
	I - Batch: 1600 | Loss: 1.377 | Acc: 29.625% | Wgt Acc: 26.749%
	I - Batch: 1650 | Loss: 1.377 | Acc: 29.818% | Wgt Acc: 26.929%
	I - Batch: 1700 | Loss: 1.376 | Acc: 29.882% | Wgt Acc: 26.996%
	I - Batch: 1750 | Loss: 1.377 | Acc: 29.829% | Wgt Acc: 26.932%
	I - Batch: 1800 | Loss: 1.378 | Acc: 29.722% | Wgt Acc: 26.817%
	I - Batch: 1850 | Loss: 1.377 | Acc: 29.676% | Wgt Acc: 26.780%
	I - Batch: 1900 | Loss: 1.378 | Acc: 29.474% | Wgt Acc: 26.581%
	I - Batch: 1950 | Loss: 1.379 | Acc: 29.385% | Wgt Acc: 26.485%
	I - Batch: 2000 | Loss: 1.379 | Acc: 29.200% | Wgt Acc: 26.309%
	I - Batch: 2050 | Loss: 1.379 | Acc: 28.927% | Wgt Acc: 26.054%
	I - Batch: 2100 | Loss: 1.379 | Acc: 29.000% | Wgt Acc: 26.126%
	I - Batch: 2150 | Loss: 1.379 | Acc: 29.070% | Wgt Acc: 26.183%
	I - Batch: 2200 | Loss: 1.379 | Acc: 29.182% | Wgt Acc: 26.290%
	I - Batch: 2250 | Loss: 1.379 | Acc: 29.022% | Wgt Acc: 26.144%
	I - Batch: 2300 | Loss: 1.379 | Acc: 29.087% | Wgt Acc: 26.207%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.936% | Wgt Acc: 26.074%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.917% | Wgt Acc: 26.056%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.857% | Wgt Acc: 26.005%
	I - Batch: 2500 | Loss: 1.378 | Acc: 28.920% | Wgt Acc: 26.075%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1117.28s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.393 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.396 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.396 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 109.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 1.360 | Acc: 40.000% | Wgt Acc: 36.530%
	I - Batch: 100 | Loss: 1.358 | Acc: 32.000% | Wgt Acc: 29.425%
	I - Batch: 150 | Loss: 1.356 | Acc: 33.333% | Wgt Acc: 30.628%
	I - Batch: 200 | Loss: 1.365 | Acc: 30.500% | Wgt Acc: 27.759%
	I - Batch: 250 | Loss: 1.373 | Acc: 29.200% | Wgt Acc: 26.425%
	I - Batch: 300 | Loss: 1.371 | Acc: 27.667% | Wgt Acc: 25.094%
	I - Batch: 350 | Loss: 1.370 | Acc: 28.857% | Wgt Acc: 26.200%
	I - Batch: 400 | Loss: 1.371 | Acc: 29.250% | Wgt Acc: 26.531%
	I - Batch: 450 | Loss: 1.376 | Acc: 27.556% | Wgt Acc: 24.887%
	I - Batch: 500 | Loss: 1.379 | Acc: 27.200% | Wgt Acc: 24.505%
	I - Batch: 550 | Loss: 1.378 | Acc: 28.000% | Wgt Acc: 25.225%
	I - Batch: 600 | Loss: 1.380 | Acc: 28.000% | Wgt Acc: 25.187%
	I - Batch: 650 | Loss: 1.379 | Acc: 28.308% | Wgt Acc: 25.485%
	I - Batch: 700 | Loss: 1.381 | Acc: 28.429% | Wgt Acc: 25.554%
	I - Batch: 750 | Loss: 1.382 | Acc: 28.133% | Wgt Acc: 25.262%
	I - Batch: 800 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.154%
	I - Batch: 850 | Loss: 1.380 | Acc: 28.235% | Wgt Acc: 25.397%
	I - Batch: 900 | Loss: 1.379 | Acc: 28.111% | Wgt Acc: 25.313%
	I - Batch: 950 | Loss: 1.380 | Acc: 27.895% | Wgt Acc: 25.083%
	I - Batch: 1000 | Loss: 1.379 | Acc: 28.300% | Wgt Acc: 25.478%
	I - Batch: 1050 | Loss: 1.378 | Acc: 28.762% | Wgt Acc: 25.917%
	I - Batch: 1100 | Loss: 1.379 | Acc: 28.545% | Wgt Acc: 25.706%
	I - Batch: 1150 | Loss: 1.378 | Acc: 28.435% | Wgt Acc: 25.622%
	I - Batch: 1200 | Loss: 1.377 | Acc: 29.000% | Wgt Acc: 26.151%
	I - Batch: 1250 | Loss: 1.377 | Acc: 29.280% | Wgt Acc: 26.412%
	I - Batch: 1300 | Loss: 1.377 | Acc: 29.000% | Wgt Acc: 26.149%
	I - Batch: 1350 | Loss: 1.376 | Acc: 29.111% | Wgt Acc: 26.261%
	I - Batch: 1400 | Loss: 1.376 | Acc: 29.357% | Wgt Acc: 26.473%
	I - Batch: 1450 | Loss: 1.376 | Acc: 29.586% | Wgt Acc: 26.683%
	I - Batch: 1500 | Loss: 1.376 | Acc: 29.400% | Wgt Acc: 26.502%
	I - Batch: 1550 | Loss: 1.376 | Acc: 29.161% | Wgt Acc: 26.294%
	I - Batch: 1600 | Loss: 1.376 | Acc: 29.562% | Wgt Acc: 26.678%
	I - Batch: 1650 | Loss: 1.375 | Acc: 29.758% | Wgt Acc: 26.871%
	I - Batch: 1700 | Loss: 1.375 | Acc: 29.706% | Wgt Acc: 26.833%
	I - Batch: 1750 | Loss: 1.376 | Acc: 29.486% | Wgt Acc: 26.608%
	I - Batch: 1800 | Loss: 1.376 | Acc: 29.722% | Wgt Acc: 26.830%
	I - Batch: 1850 | Loss: 1.376 | Acc: 29.730% | Wgt Acc: 26.842%
	I - Batch: 1900 | Loss: 1.377 | Acc: 29.579% | Wgt Acc: 26.692%
	I - Batch: 1950 | Loss: 1.376 | Acc: 29.641% | Wgt Acc: 26.744%
	I - Batch: 2000 | Loss: 1.378 | Acc: 29.250% | Wgt Acc: 26.369%
	I - Batch: 2050 | Loss: 1.378 | Acc: 29.171% | Wgt Acc: 26.297%
	I - Batch: 2100 | Loss: 1.378 | Acc: 29.143% | Wgt Acc: 26.280%
	I - Batch: 2150 | Loss: 1.377 | Acc: 29.488% | Wgt Acc: 26.602%
	I - Batch: 2200 | Loss: 1.378 | Acc: 29.227% | Wgt Acc: 26.355%
	I - Batch: 2250 | Loss: 1.378 | Acc: 29.067% | Wgt Acc: 26.194%
	I - Batch: 2300 | Loss: 1.378 | Acc: 29.087% | Wgt Acc: 26.222%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.979% | Wgt Acc: 26.112%
	I - Batch: 2400 | Loss: 1.378 | Acc: 28.917% | Wgt Acc: 26.063%
	I - Batch: 2450 | Loss: 1.378 | Acc: 28.939% | Wgt Acc: 26.095%
	I - Batch: 2500 | Loss: 1.378 | Acc: 28.960% | Wgt Acc: 26.121%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1303.38s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.397 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 109.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 1.390 | Acc: 24.000% | Wgt Acc: 21.429%
	I - Batch: 100 | Loss: 1.374 | Acc: 29.000% | Wgt Acc: 26.304%
	I - Batch: 150 | Loss: 1.372 | Acc: 28.000% | Wgt Acc: 25.378%
	I - Batch: 200 | Loss: 1.371 | Acc: 31.500% | Wgt Acc: 28.571%
	I - Batch: 250 | Loss: 1.373 | Acc: 30.800% | Wgt Acc: 27.873%
	I - Batch: 300 | Loss: 1.373 | Acc: 30.333% | Wgt Acc: 27.451%
	I - Batch: 350 | Loss: 1.376 | Acc: 29.143% | Wgt Acc: 26.306%
	I - Batch: 400 | Loss: 1.378 | Acc: 28.500% | Wgt Acc: 25.690%
	I - Batch: 450 | Loss: 1.380 | Acc: 28.000% | Wgt Acc: 25.200%
	I - Batch: 500 | Loss: 1.381 | Acc: 27.800% | Wgt Acc: 24.989%
	I - Batch: 550 | Loss: 1.382 | Acc: 28.000% | Wgt Acc: 25.153%
	I - Batch: 600 | Loss: 1.383 | Acc: 27.833% | Wgt Acc: 24.981%
	I - Batch: 650 | Loss: 1.383 | Acc: 28.615% | Wgt Acc: 25.691%
	I - Batch: 700 | Loss: 1.381 | Acc: 28.286% | Wgt Acc: 25.450%
	I - Batch: 750 | Loss: 1.381 | Acc: 27.867% | Wgt Acc: 25.067%
	I - Batch: 800 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.197%
	I - Batch: 850 | Loss: 1.379 | Acc: 28.353% | Wgt Acc: 25.557%
	I - Batch: 900 | Loss: 1.380 | Acc: 27.556% | Wgt Acc: 24.812%
	I - Batch: 950 | Loss: 1.380 | Acc: 27.895% | Wgt Acc: 25.124%
	I - Batch: 1000 | Loss: 1.380 | Acc: 27.500% | Wgt Acc: 24.752%
	I - Batch: 1050 | Loss: 1.381 | Acc: 27.619% | Wgt Acc: 24.850%
	I - Batch: 1100 | Loss: 1.381 | Acc: 27.909% | Wgt Acc: 25.102%
	I - Batch: 1150 | Loss: 1.381 | Acc: 27.913% | Wgt Acc: 25.112%
	I - Batch: 1200 | Loss: 1.381 | Acc: 27.917% | Wgt Acc: 25.103%
	I - Batch: 1250 | Loss: 1.381 | Acc: 28.320% | Wgt Acc: 25.463%
	I - Batch: 1300 | Loss: 1.380 | Acc: 28.846% | Wgt Acc: 25.965%
	I - Batch: 1350 | Loss: 1.380 | Acc: 28.815% | Wgt Acc: 25.938%
	I - Batch: 1400 | Loss: 1.380 | Acc: 28.929% | Wgt Acc: 26.041%
	I - Batch: 1450 | Loss: 1.380 | Acc: 28.966% | Wgt Acc: 26.071%
	I - Batch: 1500 | Loss: 1.379 | Acc: 29.067% | Wgt Acc: 26.174%
	I - Batch: 1550 | Loss: 1.380 | Acc: 28.839% | Wgt Acc: 25.954%
	I - Batch: 1600 | Loss: 1.381 | Acc: 28.938% | Wgt Acc: 26.026%
	I - Batch: 1650 | Loss: 1.381 | Acc: 28.848% | Wgt Acc: 25.929%
	I - Batch: 1700 | Loss: 1.381 | Acc: 28.647% | Wgt Acc: 25.754%
	I - Batch: 1750 | Loss: 1.381 | Acc: 28.571% | Wgt Acc: 25.677%
	I - Batch: 1800 | Loss: 1.382 | Acc: 28.667% | Wgt Acc: 25.755%
	I - Batch: 1850 | Loss: 1.381 | Acc: 28.811% | Wgt Acc: 25.899%
	I - Batch: 1900 | Loss: 1.381 | Acc: 29.053% | Wgt Acc: 26.121%
	I - Batch: 1950 | Loss: 1.380 | Acc: 28.974% | Wgt Acc: 26.070%
	I - Batch: 2000 | Loss: 1.380 | Acc: 29.000% | Wgt Acc: 26.103%
	I - Batch: 2050 | Loss: 1.380 | Acc: 28.878% | Wgt Acc: 25.993%
	I - Batch: 2100 | Loss: 1.379 | Acc: 28.952% | Wgt Acc: 26.064%
	I - Batch: 2150 | Loss: 1.380 | Acc: 28.744% | Wgt Acc: 25.877%
	I - Batch: 2200 | Loss: 1.380 | Acc: 28.727% | Wgt Acc: 25.843%
	I - Batch: 2250 | Loss: 1.380 | Acc: 28.667% | Wgt Acc: 25.790%
	I - Batch: 2300 | Loss: 1.380 | Acc: 28.913% | Wgt Acc: 26.030%
	I - Batch: 2350 | Loss: 1.379 | Acc: 28.979% | Wgt Acc: 26.099%
	I - Batch: 2400 | Loss: 1.379 | Acc: 28.958% | Wgt Acc: 26.093%
	I - Batch: 2450 | Loss: 1.379 | Acc: 28.898% | Wgt Acc: 26.041%
	I - Batch: 2500 | Loss: 1.379 | Acc: 28.800% | Wgt Acc: 25.955%
I - num batch: 2547
I - Train -- Loss: 1.379 | Acc: 28.818% | Wgt Acc: 25.973% | LR: 1.250000e-04 | Dur: 1302.80s
I - Confusion Matrix: [row->prediction - col->label]
[[  0.   0.   0.   0.]
 [  0.   0.   0.   0.]
 [697. 578. 734. 538.]
 [  0.   0.   0.   0.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.385 | Acc: 24.000% | Wgt Acc: 21.525%
	I - Batch: 100 | Loss: 1.394 | Acc: 24.000% | Wgt Acc: 21.333%
	I - Batch: 150 | Loss: 1.392 | Acc: 24.667% | Wgt Acc: 21.958%
	I - Batch: 200 | Loss: 1.394 | Acc: 22.500% | Wgt Acc: 20.022%
	I - Batch: 250 | Loss: 1.397 | Acc: 22.400% | Wgt Acc: 19.893%
	I - Batch: 300 | Loss: 1.397 | Acc: 21.000% | Wgt Acc: 18.653%
I - num batch: 327
I - Val -- Loss: 1.396 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 109.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 1.373 | Acc: 34.000% | Wgt Acc: 30.631%
	I - Batch: 100 | Loss: 1.371 | Acc: 30.000% | Wgt Acc: 27.149%
	I - Batch: 150 | Loss: 1.381 | Acc: 28.000% | Wgt Acc: 25.150%
	I - Batch: 200 | Loss: 1.386 | Acc: 26.000% | Wgt Acc: 23.240%
	I - Batch: 250 | Loss: 1.384 | Acc: 27.600% | Wgt Acc: 24.753%
	I - Batch: 300 | Loss: 1.385 | Acc: 25.667% | Wgt Acc: 23.002%
	I - Batch: 350 | Loss: 1.383 | Acc: 28.286% | Wgt Acc: 25.401%
	I - Batch: 400 | Loss: 1.383 | Acc: 28.750% | Wgt Acc: 25.828%
	I - Batch: 450 | Loss: 1.380 | Acc: 28.667% | Wgt Acc: 25.813%
	I - Batch: 500 | Loss: 1.379 | Acc: 28.800% | Wgt Acc: 25.946%
	I - Batch: 550 | Loss: 1.379 | Acc: 29.455% | Wgt Acc: 26.536%
	I - Batch: 600 | Loss: 1.378 | Acc: 29.667% | Wgt Acc: 26.737%
	I - Batch: 650 | Loss: 1.375 | Acc: 30.154% | Wgt Acc: 27.270%
	I - Batch: 700 | Loss: 1.373 | Acc: 29.714% | Wgt Acc: 26.908%
	I - Batch: 750 | Loss: 1.374 | Acc: 29.600% | Wgt Acc: 26.787%
	I - Batch: 800 | Loss: 1.374 | Acc: 29.625% | Wgt Acc: 26.810%
	I - Batch: 850 | Loss: 1.376 | Acc: 28.824% | Wgt Acc: 26.043%
	I - Batch: 900 | Loss: 1.377 | Acc: 28.667% | Wgt Acc: 25.891%
	I - Batch: 950 | Loss: 1.378 | Acc: 28.737% | Wgt Acc: 25.920%
