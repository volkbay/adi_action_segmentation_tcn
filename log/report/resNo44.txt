Wed Oct 19 04:42:07 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.2, 0.25, 0.2, 0.25, 0.08], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': True, 'tossFirstLastFrames': True}, 'dataPath': '/data/processed/Kinetics/', 'dropoutRate': 0.5, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat'], 'lastLayerInitUniform': True, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 70.33639143730886, 'maxValidationTrainNo': 39, 'modelVersion': 11, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 44, 'validationAccThr': 70, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Train set length:  7641
I - Label distribution: [2091. 1734. 2202. 1614.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/test
I - Test set length:  981
I - Label distribution: [264. 234. 225. 258.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(3) data number in dataset:  tensor([149144,  31293, 120373, 101176, 164625, 163541, 205093, 186046,   3413,
         31290, 102073, 121594, 216388,  67775, 183563,  25167])
I - Initializing model TCNv11
I - Number of Model Parameters: 247392
I - Model output shape:  torch.Size([16, 4])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv11                                   [16, 4]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 64, 64, 64]          --
│    └─ReLU: 2-2319                      [16, 64, 64, 64]          --
│    └─Conv2d: 2-2                       --                        3,136
│    └─BatchNorm2d: 2-2317               [16, 64, 64, 64]          --
│    └─OutputShiftSqueeze: 2-4           --                        --
│    └─One: 2-5                          [1]                       --
│    └─Scaler: 2-2318                    [16, 64, 64, 64]          --
│    └─OutputScale: 2-7                  --                        --
│    └─Empty: 2-8                        [64, 48, 1, 1]            --
│    └─Empty: 2-9                        [64, 48, 1, 1]            --
│    └─Empty: 2-10                       [64]                      --
│    └─Empty: 2-11                       [64]                      --
│    └─BatchNorm2d: 2-12                 [16, 64, 64, 64]          --
│    └─Scaler: 2-13                      [16, 64, 64, 64]          --
│    └─Empty: 2-14                       --                        --
│    └─Empty: 2-15                       --                        --
│    └─ReLU: 2-16                        [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-2331                      [16, 64, 64, 64]          --
│    └─Conv2d: 2-18                      --                        36,928
│    └─BatchNorm2d: 2-2329               [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-20                       [16, 64, 64, 64]          --
│    └─Clamp: 2-21                       [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-2330                    [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-2                 [16, 64, 64, 64]          36,934
│    └─OutputShiftSqueeze: 2-23          --                        --
│    └─One: 2-24                         [1]                       --
│    └─OutputScale: 2-25                 --                        --
│    └─Empty: 2-26                       [64, 64, 3, 3]            --
│    └─Empty: 2-27                       [64, 64, 3, 3]            --
│    └─Empty: 2-28                       [64]                      --
│    └─Empty: 2-29                       [64]                      --
│    └─BatchNorm2d: 2-30                 [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 64, 32, 32]          (recursive)
│    └─ReLU: 2-2346                      [16, 64, 32, 32]          --
│    └─MaxPool2d: 2-2334                 [16, 64, 32, 32]          --
│    └─Conv2d: 2-33                      --                        36,928
│    └─BatchNorm2d: 2-2344               [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-35                      [16, 64, 64, 64]          --
│    └─ReLU: 2-36                        [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-2345                    [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-38                       [16, 64, 64, 64]          --
│    └─Clamp: 2-39                       [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-4          [16, 64, 32, 32]          32,774
│    └─MaxPool2d: 2-40                   [16, 64, 32, 32]          --
│    └─Empty: 2-41                       [16, 64, 32, 32]          --
│    └─Empty: 2-42                       [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-43          --                        --
│    └─Empty: 2-2335                     [16, 64, 32, 32]          --
│    └─Empty: 2-2336                     [16, 64, 32, 32]          --
│    └─One: 2-46                         [1]                       --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-2358                      [16, 64, 32, 32]          --
│    └─Conv2d: 2-48                      --                        4,160
│    └─BatchNorm2d: 2-2356               [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-50                 --                        --
│    └─Empty: 2-51                       [64, 64, 3, 3]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-2357                    [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-53                       [64, 64, 3, 3]            --
│    └─Empty: 2-54                       [64]                      --
│    └─Empty: 2-55                       [64]                      --
│    └─BatchNorm2d: 2-56                 [16, 64, 32, 32]          --
│    └─Scaler: 2-57                      [16, 64, 32, 32]          --
│    └─ReLU: 2-58                        [16, 64, 32, 32]          --
│    └─Empty: 2-59                       [16, 64, 32, 32]          --
│    └─Clamp: 2-60                       [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-5                 [16, 64, 32, 32]          4,166
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 64, 32, 32]          (recursive)
│    └─ReLU: 2-2373                      [16, 64, 32, 32]          --
│    └─MaxPool2d: 2-2361                 [16, 64, 32, 32]          --
│    └─Conv2d: 2-63                      --                        36,928
│    └─BatchNorm2d: 2-2371               [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-65          --                        --
│    └─One: 2-66                         [1]                       --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-2372                    [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputScale: 2-68                 --                        --
│    └─Empty: 2-69                       [64, 64, 1, 1]            --
│    └─Empty: 2-70                       [64, 64, 1, 1]            --
│    └─Empty: 2-71                       [64]                      --
│    └─Empty: 2-72                       [64]                      --
│    └─BatchNorm2d: 2-73                 [16, 64, 32, 32]          --
│    └─Scaler: 2-74                      [16, 64, 32, 32]          --
│    └─ReLU: 2-75                        [16, 64, 32, 32]          --
│    └─Empty: 2-76                       [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-177        [16, 64, 16, 16]          (recursive)
│    └─ReLU: 2-2388                      [16, 64, 16, 16]          --
│    └─MaxPool2d: 2-2376                 [16, 64, 16, 16]          --
│    └─Conv2d: 2-79                      --                        36,928
│    └─BatchNorm2d: 2-2386               [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-81                       [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-8          [16, 64, 32, 32]          32,774
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-2387                    [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─MaxPool2d: 2-83                   [16, 64, 32, 32]          --
│    └─Empty: 2-84                       [16, 64, 32, 32]          --
│    └─Empty: 2-85                       [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-86          --                        --
│    └─One: 2-87                         [1]                       --
│    └─OutputScale: 2-88                 --                        --
│    └─Empty: 2-89                       [64, 64, 3, 3]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-2377                     [16, 64, 16, 16]          --
│    └─Empty: 2-2378                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-92                       [64, 64, 3, 3]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-2400                      [16, 64, 16, 16]          --
│    └─Conv2d: 2-94                      --                        4,160
│    └─BatchNorm2d: 2-2398               [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-96                       [64]                      --
│    └─Empty: 2-97                       [64]                      --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-2399                    [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─BatchNorm2d: 2-99                 [16, 64, 32, 32]          --
│    └─Scaler: 2-100                     [16, 64, 32, 32]          --
│    └─ReLU: 2-101                       [16, 64, 32, 32]          --
│    └─Empty: 2-102                      [16, 64, 32, 32]          --
│    └─Clamp: 2-103                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-9          [16, 64, 16, 16]          36,934
│    └─MaxPool2d: 2-104                  [16, 64, 16, 16]          --
│    └─Empty: 2-105                      [16, 64, 16, 16]          --
│    └─Empty: 2-106                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-179        [16, 64, 16, 16]          (recursive)
│    └─ReLU: 2-2415                      [16, 64, 16, 16]          --
│    └─MaxPool2d: 2-2403                 [16, 64, 16, 16]          --
│    └─Conv2d: 2-109                     --                        36,928
│    └─BatchNorm2d: 2-2413               [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputShiftSqueeze: 2-111         --                        --
│    └─One: 2-112                        [1]                       --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-2414                    [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-114                --                        --
│    └─Empty: 2-115                      [64, 64, 3, 3]            --
│    └─Empty: 2-116                      [64, 64, 3, 3]            --
│    └─Empty: 2-117                      [64]                      --
│    └─Empty: 2-118                      [64]                      --
│    └─BatchNorm2d: 2-119                [16, 64, 16, 16]          --
│    └─Scaler: 2-120                     [16, 64, 16, 16]          --
│    └─ReLU: 2-121                       [16, 64, 16, 16]          --
│    └─Empty: 2-122                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 64, 8, 8]            (recursive)
│    └─ReLU: 2-2430                      [16, 64, 8, 8]            --
│    └─MaxPool2d: 2-2418                 [16, 64, 8, 8]            --
│    └─Conv2d: 2-125                     --                        36,928
│    └─BatchNorm2d: 2-2428               [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Clamp: 2-127                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-12                [16, 64, 16, 16]          3,126
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-2429                    [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-129         --                        --
│    └─One: 2-130                        [1]                       --
│    └─OutputScale: 2-131                --                        --
│    └─Empty: 2-132                      [64, 64, 1, 1]            --
│    └─Empty: 2-133                      [64, 64, 1, 1]            --
│    └─Empty: 2-134                      [64]                      --
│    └─Empty: 2-135                      [64]                      --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-2419                     [16, 64, 8, 8]            --
│    └─Empty: 2-2420                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─BatchNorm2d: 2-138                [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-2442                      [16, 16, 8, 8]            --
│    └─Conv2d: 2-140                     --                        1,040
│    └─BatchNorm2d: 2-2440               [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-142                     [16, 64, 16, 16]          --
│    └─ReLU: 2-143                       [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-2441                    [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-145                      [16, 64, 16, 16]          --
│    └─Clamp: 2-146                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-13         [16, 64, 16, 16]          36,934
│    └─MaxPool2d: 2-147                  [16, 64, 16, 16]          --
│    └─Empty: 2-148                      [16, 64, 16, 16]          --
│    └─Empty: 2-149                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-150         --                        --
│    └─One: 2-151                        [1]                       --
│    └─OutputScale: 2-152                --                        --
├─FusedMaxPoolConv2dBNReLU: 1-182        [16, 16, 8, 8]            (recursive)
│    └─ReLU: 2-2457                      [16, 16, 8, 8]            --
│    └─MaxPool2d: 2-2445                 [16, 64, 8, 8]            --
│    └─Conv2d: 2-155                     --                        9,232
│    └─BatchNorm2d: 2-2455               [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-157                      [64, 64, 3, 3]            --
│    └─Empty: 2-158                      [64, 64, 3, 3]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-2456                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-160                      [64]                      --
│    └─Empty: 2-161                      [64]                      --
│    └─BatchNorm2d: 2-162                [16, 64, 16, 16]          --
│    └─Scaler: 2-163                     [16, 64, 16, 16]          --
│    └─ReLU: 2-164                       [16, 64, 16, 16]          --
│    └─Empty: 2-165                      [16, 64, 16, 16]          --
│    └─Clamp: 2-166                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 64, 8, 8]            36,934
│    └─MaxPool2d: 2-167                  [16, 64, 8, 8]            --
│    └─Empty: 2-168                      [16, 64, 8, 8]            --
│    └─Empty: 2-169                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-170         --                        --
│    └─One: 2-171                        [1]                       --
│    └─OutputScale: 2-172                --                        --
├─Linear: 1                              --                        --
│    └─Scaler: 2-173                     --                        --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-174                      [64, 64, 3, 3]            --
│    └─Empty: 2-175                      [64, 64, 3, 3]            --
│    └─Empty: 2-176                      [64]                      --
│    └─Empty: 2-177                      [64]                      --
│    └─BatchNorm2d: 2-178                [16, 64, 8, 8]            --
│    └─Scaler: 2-179                     [16, 64, 8, 8]            --
│    └─ReLU: 2-180                       [16, 64, 8, 8]            --
│    └─Empty: 2-181                      [16, 64, 8, 8]            --
│    └─Clamp: 2-182                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-16                [16, 16, 8, 8]            1,046
│    └─OutputShiftSqueeze: 2-183         --                        --
│    └─One: 2-184                        [1]                       --
│    └─OutputScale: 2-185                --                        --
│    └─Empty: 2-186                      [16, 64, 1, 1]            --
│    └─Empty: 2-187                      [16, 64, 1, 1]            --
│    └─Empty: 2-188                      [16]                      --
│    └─Empty: 2-189                      [16]                      --
│    └─BatchNorm2d: 2-190                [16, 16, 8, 8]            --
│    └─Scaler: 2-191                     [16, 16, 8, 8]            --
│    └─ReLU: 2-192                       [16, 16, 8, 8]            --
│    └─Empty: 2-193                      [16, 16, 8, 8]            --
│    └─Clamp: 2-194                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-17         [16, 16, 8, 8]            9,238
│    └─MaxPool2d: 2-195                  [16, 64, 8, 8]            --
│    └─Empty: 2-196                      [16, 64, 8, 8]            --
│    └─Empty: 2-197                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-198         --                        --
│    └─One: 2-199                        [1]                       --
│    └─OutputScale: 2-200                --                        --
│    └─Empty: 2-201                      [16, 64, 3, 3]            --
│    └─Empty: 2-202                      [16, 64, 3, 3]            --
│    └─Empty: 2-203                      [16]                      --
│    └─Empty: 2-204                      [16]                      --
│    └─BatchNorm2d: 2-205                [16, 16, 8, 8]            --
│    └─Scaler: 2-206                     [16, 16, 8, 8]            --
│    └─ReLU: 2-207                       [16, 16, 8, 8]            --
│    └─Empty: 2-208                      [16, 16, 8, 8]            --
│    └─Clamp: 2-209                      [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-18                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-210         --                        --
│    └─One: 2-211                        [1]                       --
│    └─OutputScale: 2-212                --                        --
│    └─Empty: 2-213                      [64, 48, 1, 1]            --
│    └─Empty: 2-214                      [64, 48, 1, 1]            --
│    └─Empty: 2-215                      [64]                      --
│    └─Empty: 2-216                      [64]                      --
│    └─BatchNorm2d: 2-217                [16, 64, 64, 64]          --
│    └─Scaler: 2-218                     [16, 64, 64, 64]          --
│    └─ReLU: 2-219                       [16, 64, 64, 64]          --
│    └─Empty: 2-220                      [16, 64, 64, 64]          --
│    └─Clamp: 2-221                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-19                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-222         --                        --
│    └─One: 2-223                        [1]                       --
│    └─OutputScale: 2-224                --                        --
│    └─Empty: 2-225                      [64, 64, 3, 3]            --
│    └─Empty: 2-226                      [64, 64, 3, 3]            --
│    └─Empty: 2-227                      [64]                      --
│    └─Empty: 2-228                      [64]                      --
│    └─BatchNorm2d: 2-229                [16, 64, 64, 64]          --
│    └─Scaler: 2-230                     [16, 64, 64, 64]          --
│    └─ReLU: 2-231                       [16, 64, 64, 64]          --
│    └─Empty: 2-232                      [16, 64, 64, 64]          --
│    └─Clamp: 2-233                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-20         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-234                  [16, 64, 32, 32]          --
│    └─Empty: 2-235                      [16, 64, 32, 32]          --
│    └─Empty: 2-236                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-237         --                        --
│    └─One: 2-238                        [1]                       --
│    └─OutputScale: 2-239                --                        --
│    └─Empty: 2-240                      [64, 64, 3, 3]            --
│    └─Empty: 2-241                      [64, 64, 3, 3]            --
│    └─Empty: 2-242                      [64]                      --
│    └─Empty: 2-243                      [64]                      --
│    └─BatchNorm2d: 2-244                [16, 64, 32, 32]          --
│    └─Scaler: 2-245                     [16, 64, 32, 32]          --
│    └─ReLU: 2-246                       [16, 64, 32, 32]          --
│    └─Empty: 2-247                      [16, 64, 32, 32]          --
│    └─Clamp: 2-248                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-21                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-249         --                        --
│    └─One: 2-250                        [1]                       --
│    └─OutputScale: 2-251                --                        --
│    └─Empty: 2-252                      [64, 64, 1, 1]            --
│    └─Empty: 2-253                      [64, 64, 1, 1]            --
│    └─Empty: 2-254                      [64]                      --
│    └─Empty: 2-255                      [64]                      --
│    └─BatchNorm2d: 2-256                [16, 64, 32, 32]          --
│    └─Scaler: 2-257                     [16, 64, 32, 32]          --
│    └─ReLU: 2-258                       [16, 64, 32, 32]          --
│    └─Empty: 2-259                      [16, 64, 32, 32]          --
│    └─Clamp: 2-260                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-22         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-261                  [16, 64, 32, 32]          --
│    └─Empty: 2-262                      [16, 64, 32, 32]          --
│    └─Empty: 2-263                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-264         --                        --
│    └─One: 2-265                        [1]                       --
│    └─OutputScale: 2-266                --                        --
│    └─Empty: 2-267                      [64, 64, 3, 3]            --
│    └─Empty: 2-268                      [64, 64, 3, 3]            --
│    └─Empty: 2-269                      [64]                      --
│    └─Empty: 2-270                      [64]                      --
│    └─BatchNorm2d: 2-271                [16, 64, 32, 32]          --
│    └─Scaler: 2-272                     [16, 64, 32, 32]          --
│    └─ReLU: 2-273                       [16, 64, 32, 32]          --
│    └─Empty: 2-274                      [16, 64, 32, 32]          --
│    └─Clamp: 2-275                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-23         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-276                  [16, 64, 16, 16]          --
│    └─Empty: 2-277                      [16, 64, 16, 16]          --
│    └─Empty: 2-278                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-279         --                        --
│    └─One: 2-280                        [1]                       --
│    └─OutputScale: 2-281                --                        --
│    └─Empty: 2-282                      [64, 64, 3, 3]            --
│    └─Empty: 2-283                      [64, 64, 3, 3]            --
│    └─Empty: 2-284                      [64]                      --
│    └─Empty: 2-285                      [64]                      --
│    └─BatchNorm2d: 2-286                [16, 64, 16, 16]          --
│    └─Scaler: 2-287                     [16, 64, 16, 16]          --
│    └─ReLU: 2-288                       [16, 64, 16, 16]          --
│    └─Empty: 2-289                      [16, 64, 16, 16]          --
│    └─Clamp: 2-290                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-24                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-291         --                        --
│    └─One: 2-292                        [1]                       --
│    └─OutputScale: 2-293                --                        --
│    └─Empty: 2-294                      [64, 64, 1, 1]            --
│    └─Empty: 2-295                      [64, 64, 1, 1]            --
│    └─Empty: 2-296                      [64]                      --
│    └─Empty: 2-297                      [64]                      --
│    └─BatchNorm2d: 2-298                [16, 64, 16, 16]          --
│    └─Scaler: 2-299                     [16, 64, 16, 16]          --
│    └─ReLU: 2-300                       [16, 64, 16, 16]          --
│    └─Empty: 2-301                      [16, 64, 16, 16]          --
│    └─Clamp: 2-302                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-25         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-303                  [16, 64, 16, 16]          --
│    └─Empty: 2-304                      [16, 64, 16, 16]          --
│    └─Empty: 2-305                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-306         --                        --
│    └─One: 2-307                        [1]                       --
│    └─OutputScale: 2-308                --                        --
│    └─Empty: 2-309                      [64, 64, 3, 3]            --
│    └─Empty: 2-310                      [64, 64, 3, 3]            --
│    └─Empty: 2-311                      [64]                      --
│    └─Empty: 2-312                      [64]                      --
│    └─BatchNorm2d: 2-313                [16, 64, 16, 16]          --
│    └─Scaler: 2-314                     [16, 64, 16, 16]          --
│    └─ReLU: 2-315                       [16, 64, 16, 16]          --
│    └─Empty: 2-316                      [16, 64, 16, 16]          --
│    └─Clamp: 2-317                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-318                  [16, 64, 8, 8]            --
│    └─Empty: 2-319                      [16, 64, 8, 8]            --
│    └─Empty: 2-320                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-321         --                        --
│    └─One: 2-322                        [1]                       --
│    └─OutputScale: 2-323                --                        --
│    └─Empty: 2-324                      [64, 64, 3, 3]            --
│    └─Empty: 2-325                      [64, 64, 3, 3]            --
│    └─Empty: 2-326                      [64]                      --
│    └─Empty: 2-327                      [64]                      --
│    └─BatchNorm2d: 2-328                [16, 64, 8, 8]            --
│    └─Scaler: 2-329                     [16, 64, 8, 8]            --
│    └─ReLU: 2-330                       [16, 64, 8, 8]            --
│    └─Empty: 2-331                      [16, 64, 8, 8]            --
│    └─Clamp: 2-332                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-27                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-333         --                        --
│    └─One: 2-334                        [1]                       --
│    └─OutputScale: 2-335                --                        --
│    └─Empty: 2-336                      [16, 64, 1, 1]            --
│    └─Empty: 2-337                      [16, 64, 1, 1]            --
│    └─Empty: 2-338                      [16]                      --
│    └─Empty: 2-339                      [16]                      --
│    └─BatchNorm2d: 2-340                [16, 16, 8, 8]            --
│    └─Scaler: 2-341                     [16, 16, 8, 8]            --
│    └─ReLU: 2-342                       [16, 16, 8, 8]            --
│    └─Empty: 2-343                      [16, 16, 8, 8]            --
│    └─Clamp: 2-344                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-28         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-345                  [16, 64, 8, 8]            --
│    └─Empty: 2-346                      [16, 64, 8, 8]            --
│    └─Empty: 2-347                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-348         --                        --
│    └─One: 2-349                        [1]                       --
│    └─OutputScale: 2-350                --                        --
│    └─Empty: 2-351                      [16, 64, 3, 3]            --
│    └─Empty: 2-352                      [16, 64, 3, 3]            --
│    └─Empty: 2-353                      [16]                      --
│    └─Empty: 2-354                      [16]                      --
│    └─BatchNorm2d: 2-355                [16, 16, 8, 8]            --
│    └─Scaler: 2-356                     [16, 16, 8, 8]            --
│    └─ReLU: 2-357                       [16, 16, 8, 8]            --
│    └─Empty: 2-358                      [16, 16, 8, 8]            --
│    └─Clamp: 2-359                      [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-29                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-360         --                        --
│    └─One: 2-361                        [1]                       --
│    └─OutputScale: 2-362                --                        --
│    └─Empty: 2-363                      [64, 48, 1, 1]            --
│    └─Empty: 2-364                      [64, 48, 1, 1]            --
│    └─Empty: 2-365                      [64]                      --
│    └─Empty: 2-366                      [64]                      --
│    └─BatchNorm2d: 2-367                [16, 64, 64, 64]          --
│    └─Scaler: 2-368                     [16, 64, 64, 64]          --
│    └─ReLU: 2-369                       [16, 64, 64, 64]          --
│    └─Empty: 2-370                      [16, 64, 64, 64]          --
│    └─Clamp: 2-371                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-30                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-372         --                        --
│    └─One: 2-373                        [1]                       --
│    └─OutputScale: 2-374                --                        --
│    └─Empty: 2-375                      [64, 64, 3, 3]            --
│    └─Empty: 2-376                      [64, 64, 3, 3]            --
│    └─Empty: 2-377                      [64]                      --
│    └─Empty: 2-378                      [64]                      --
│    └─BatchNorm2d: 2-379                [16, 64, 64, 64]          --
│    └─Scaler: 2-380                     [16, 64, 64, 64]          --
│    └─ReLU: 2-381                       [16, 64, 64, 64]          --
│    └─Empty: 2-382                      [16, 64, 64, 64]          --
│    └─Clamp: 2-383                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-384                  [16, 64, 32, 32]          --
│    └─Empty: 2-385                      [16, 64, 32, 32]          --
│    └─Empty: 2-386                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-387         --                        --
│    └─One: 2-388                        [1]                       --
│    └─OutputScale: 2-389                --                        --
│    └─Empty: 2-390                      [64, 64, 3, 3]            --
│    └─Empty: 2-391                      [64, 64, 3, 3]            --
│    └─Empty: 2-392                      [64]                      --
│    └─Empty: 2-393                      [64]                      --
│    └─BatchNorm2d: 2-394                [16, 64, 32, 32]          --
│    └─Scaler: 2-395                     [16, 64, 32, 32]          --
│    └─ReLU: 2-396                       [16, 64, 32, 32]          --
│    └─Empty: 2-397                      [16, 64, 32, 32]          --
│    └─Clamp: 2-398                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-32                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-399         --                        --
│    └─One: 2-400                        [1]                       --
│    └─OutputScale: 2-401                --                        --
│    └─Empty: 2-402                      [64, 64, 1, 1]            --
│    └─Empty: 2-403                      [64, 64, 1, 1]            --
│    └─Empty: 2-404                      [64]                      --
│    └─Empty: 2-405                      [64]                      --
│    └─BatchNorm2d: 2-406                [16, 64, 32, 32]          --
│    └─Scaler: 2-407                     [16, 64, 32, 32]          --
│    └─ReLU: 2-408                       [16, 64, 32, 32]          --
│    └─Empty: 2-409                      [16, 64, 32, 32]          --
│    └─Clamp: 2-410                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-33         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-411                  [16, 64, 32, 32]          --
│    └─Empty: 2-412                      [16, 64, 32, 32]          --
│    └─Empty: 2-413                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-414         --                        --
│    └─One: 2-415                        [1]                       --
│    └─OutputScale: 2-416                --                        --
│    └─Empty: 2-417                      [64, 64, 3, 3]            --
│    └─Empty: 2-418                      [64, 64, 3, 3]            --
│    └─Empty: 2-419                      [64]                      --
│    └─Empty: 2-420                      [64]                      --
│    └─BatchNorm2d: 2-421                [16, 64, 32, 32]          --
│    └─Scaler: 2-422                     [16, 64, 32, 32]          --
│    └─ReLU: 2-423                       [16, 64, 32, 32]          --
│    └─Empty: 2-424                      [16, 64, 32, 32]          --
│    └─Clamp: 2-425                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-34         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-426                  [16, 64, 16, 16]          --
│    └─Empty: 2-427                      [16, 64, 16, 16]          --
│    └─Empty: 2-428                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-429         --                        --
│    └─One: 2-430                        [1]                       --
│    └─OutputScale: 2-431                --                        --
│    └─Empty: 2-432                      [64, 64, 3, 3]            --
│    └─Empty: 2-433                      [64, 64, 3, 3]            --
│    └─Empty: 2-434                      [64]                      --
│    └─Empty: 2-435                      [64]                      --
│    └─BatchNorm2d: 2-436                [16, 64, 16, 16]          --
│    └─Scaler: 2-437                     [16, 64, 16, 16]          --
│    └─ReLU: 2-438                       [16, 64, 16, 16]          --
│    └─Empty: 2-439                      [16, 64, 16, 16]          --
│    └─Clamp: 2-440                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-35                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-441         --                        --
│    └─One: 2-442                        [1]                       --
│    └─OutputScale: 2-443                --                        --
│    └─Empty: 2-444                      [64, 64, 1, 1]            --
│    └─Empty: 2-445                      [64, 64, 1, 1]            --
│    └─Empty: 2-446                      [64]                      --
│    └─Empty: 2-447                      [64]                      --
│    └─BatchNorm2d: 2-448                [16, 64, 16, 16]          --
│    └─Scaler: 2-449                     [16, 64, 16, 16]          --
│    └─ReLU: 2-450                       [16, 64, 16, 16]          --
│    └─Empty: 2-451                      [16, 64, 16, 16]          --
│    └─Clamp: 2-452                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-36         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-453                  [16, 64, 16, 16]          --
│    └─Empty: 2-454                      [16, 64, 16, 16]          --
│    └─Empty: 2-455                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-456         --                        --
│    └─One: 2-457                        [1]                       --
│    └─OutputScale: 2-458                --                        --
│    └─Empty: 2-459                      [64, 64, 3, 3]            --
│    └─Empty: 2-460                      [64, 64, 3, 3]            --
│    └─Empty: 2-461                      [64]                      --
│    └─Empty: 2-462                      [64]                      --
│    └─BatchNorm2d: 2-463                [16, 64, 16, 16]          --
│    └─Scaler: 2-464                     [16, 64, 16, 16]          --
│    └─ReLU: 2-465                       [16, 64, 16, 16]          --
│    └─Empty: 2-466                      [16, 64, 16, 16]          --
│    └─Clamp: 2-467                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-37         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-468                  [16, 64, 8, 8]            --
│    └─Empty: 2-469                      [16, 64, 8, 8]            --
│    └─Empty: 2-470                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-471         --                        --
│    └─One: 2-472                        [1]                       --
│    └─OutputScale: 2-473                --                        --
│    └─Empty: 2-474                      [64, 64, 3, 3]            --
│    └─Empty: 2-475                      [64, 64, 3, 3]            --
│    └─Empty: 2-476                      [64]                      --
│    └─Empty: 2-477                      [64]                      --
│    └─BatchNorm2d: 2-478                [16, 64, 8, 8]            --
│    └─Scaler: 2-479                     [16, 64, 8, 8]            --
│    └─ReLU: 2-480                       [16, 64, 8, 8]            --
│    └─Empty: 2-481                      [16, 64, 8, 8]            --
│    └─Clamp: 2-482                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-38                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-483         --                        --
│    └─One: 2-484                        [1]                       --
│    └─OutputScale: 2-485                --                        --
│    └─Empty: 2-486                      [16, 64, 1, 1]            --
│    └─Empty: 2-487                      [16, 64, 1, 1]            --
│    └─Empty: 2-488                      [16]                      --
│    └─Empty: 2-489                      [16]                      --
│    └─BatchNorm2d: 2-490                [16, 16, 8, 8]            --
│    └─Scaler: 2-491                     [16, 16, 8, 8]            --
│    └─ReLU: 2-492                       [16, 16, 8, 8]            --
│    └─Empty: 2-493                      [16, 16, 8, 8]            --
│    └─Clamp: 2-494                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-39         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-495                  [16, 64, 8, 8]            --
│    └─Empty: 2-496                      [16, 64, 8, 8]            --
│    └─Empty: 2-497                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-498         --                        --
│    └─One: 2-499                        [1]                       --
│    └─OutputScale: 2-500                --                        --
│    └─Empty: 2-501                      [16, 64, 3, 3]            --
│    └─Empty: 2-502                      [16, 64, 3, 3]            --
│    └─Empty: 2-503                      [16]                      --
│    └─Empty: 2-504                      [16]                      --
│    └─BatchNorm2d: 2-505                [16, 16, 8, 8]            --
│    └─Scaler: 2-506                     [16, 16, 8, 8]            --
│    └─ReLU: 2-507                       [16, 16, 8, 8]            --
│    └─Empty: 2-508                      [16, 16, 8, 8]            --
│    └─Clamp: 2-509                      [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-40                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-510         --                        --
│    └─One: 2-511                        [1]                       --
│    └─OutputScale: 2-512                --                        --
│    └─Empty: 2-513                      [64, 48, 1, 1]            --
│    └─Empty: 2-514                      [64, 48, 1, 1]            --
│    └─Empty: 2-515                      [64]                      --
│    └─Empty: 2-516                      [64]                      --
│    └─BatchNorm2d: 2-517                [16, 64, 64, 64]          --
│    └─Scaler: 2-518                     [16, 64, 64, 64]          --
│    └─ReLU: 2-519                       [16, 64, 64, 64]          --
│    └─Empty: 2-520                      [16, 64, 64, 64]          --
│    └─Clamp: 2-521                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-41                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-522         --                        --
│    └─One: 2-523                        [1]                       --
│    └─OutputScale: 2-524                --                        --
│    └─Empty: 2-525                      [64, 64, 3, 3]            --
│    └─Empty: 2-526                      [64, 64, 3, 3]            --
│    └─Empty: 2-527                      [64]                      --
│    └─Empty: 2-528                      [64]                      --
│    └─BatchNorm2d: 2-529                [16, 64, 64, 64]          --
│    └─Scaler: 2-530                     [16, 64, 64, 64]          --
│    └─ReLU: 2-531                       [16, 64, 64, 64]          --
│    └─Empty: 2-532                      [16, 64, 64, 64]          --
│    └─Clamp: 2-533                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-42         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-534                  [16, 64, 32, 32]          --
│    └─Empty: 2-535                      [16, 64, 32, 32]          --
│    └─Empty: 2-536                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-537         --                        --
│    └─One: 2-538                        [1]                       --
│    └─OutputScale: 2-539                --                        --
│    └─Empty: 2-540                      [64, 64, 3, 3]            --
│    └─Empty: 2-541                      [64, 64, 3, 3]            --
│    └─Empty: 2-542                      [64]                      --
│    └─Empty: 2-543                      [64]                      --
│    └─BatchNorm2d: 2-544                [16, 64, 32, 32]          --
│    └─Scaler: 2-545                     [16, 64, 32, 32]          --
│    └─ReLU: 2-546                       [16, 64, 32, 32]          --
│    └─Empty: 2-547                      [16, 64, 32, 32]          --
│    └─Clamp: 2-548                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-43                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-549         --                        --
│    └─One: 2-550                        [1]                       --
│    └─OutputScale: 2-551                --                        --
│    └─Empty: 2-552                      [64, 64, 1, 1]            --
│    └─Empty: 2-553                      [64, 64, 1, 1]            --
│    └─Empty: 2-554                      [64]                      --
│    └─Empty: 2-555                      [64]                      --
│    └─BatchNorm2d: 2-556                [16, 64, 32, 32]          --
│    └─Scaler: 2-557                     [16, 64, 32, 32]          --
│    └─ReLU: 2-558                       [16, 64, 32, 32]          --
│    └─Empty: 2-559                      [16, 64, 32, 32]          --
│    └─Clamp: 2-560                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-44         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-561                  [16, 64, 32, 32]          --
│    └─Empty: 2-562                      [16, 64, 32, 32]          --
│    └─Empty: 2-563                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-564         --                        --
│    └─One: 2-565                        [1]                       --
│    └─OutputScale: 2-566                --                        --
│    └─Empty: 2-567                      [64, 64, 3, 3]            --
│    └─Empty: 2-568                      [64, 64, 3, 3]            --
│    └─Empty: 2-569                      [64]                      --
│    └─Empty: 2-570                      [64]                      --
│    └─BatchNorm2d: 2-571                [16, 64, 32, 32]          --
│    └─Scaler: 2-572                     [16, 64, 32, 32]          --
│    └─ReLU: 2-573                       [16, 64, 32, 32]          --
│    └─Empty: 2-574                      [16, 64, 32, 32]          --
│    └─Clamp: 2-575                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-45         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-576                  [16, 64, 16, 16]          --
│    └─Empty: 2-577                      [16, 64, 16, 16]          --
│    └─Empty: 2-578                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-579         --                        --
│    └─One: 2-580                        [1]                       --
│    └─OutputScale: 2-581                --                        --
│    └─Empty: 2-582                      [64, 64, 3, 3]            --
│    └─Empty: 2-583                      [64, 64, 3, 3]            --
│    └─Empty: 2-584                      [64]                      --
│    └─Empty: 2-585                      [64]                      --
│    └─BatchNorm2d: 2-586                [16, 64, 16, 16]          --
│    └─Scaler: 2-587                     [16, 64, 16, 16]          --
│    └─ReLU: 2-588                       [16, 64, 16, 16]          --
│    └─Empty: 2-589                      [16, 64, 16, 16]          --
│    └─Clamp: 2-590                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-46                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-591         --                        --
│    └─One: 2-592                        [1]                       --
│    └─OutputScale: 2-593                --                        --
│    └─Empty: 2-594                      [64, 64, 1, 1]            --
│    └─Empty: 2-595                      [64, 64, 1, 1]            --
│    └─Empty: 2-596                      [64]                      --
│    └─Empty: 2-597                      [64]                      --
│    └─BatchNorm2d: 2-598                [16, 64, 16, 16]          --
│    └─Scaler: 2-599                     [16, 64, 16, 16]          --
│    └─ReLU: 2-600                       [16, 64, 16, 16]          --
│    └─Empty: 2-601                      [16, 64, 16, 16]          --
│    └─Clamp: 2-602                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-47         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-603                  [16, 64, 16, 16]          --
│    └─Empty: 2-604                      [16, 64, 16, 16]          --
│    └─Empty: 2-605                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-606         --                        --
│    └─One: 2-607                        [1]                       --
│    └─OutputScale: 2-608                --                        --
│    └─Empty: 2-609                      [64, 64, 3, 3]            --
│    └─Empty: 2-610                      [64, 64, 3, 3]            --
│    └─Empty: 2-611                      [64]                      --
│    └─Empty: 2-612                      [64]                      --
│    └─BatchNorm2d: 2-613                [16, 64, 16, 16]          --
│    └─Scaler: 2-614                     [16, 64, 16, 16]          --
│    └─ReLU: 2-615                       [16, 64, 16, 16]          --
│    └─Empty: 2-616                      [16, 64, 16, 16]          --
│    └─Clamp: 2-617                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-618                  [16, 64, 8, 8]            --
│    └─Empty: 2-619                      [16, 64, 8, 8]            --
│    └─Empty: 2-620                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-621         --                        --
│    └─One: 2-622                        [1]                       --
│    └─OutputScale: 2-623                --                        --
│    └─Empty: 2-624                      [64, 64, 3, 3]            --
│    └─Empty: 2-625                      [64, 64, 3, 3]            --
│    └─Empty: 2-626                      [64]                      --
│    └─Empty: 2-627                      [64]                      --
│    └─BatchNorm2d: 2-628                [16, 64, 8, 8]            --
│    └─Scaler: 2-629                     [16, 64, 8, 8]            --
│    └─ReLU: 2-630                       [16, 64, 8, 8]            --
│    └─Empty: 2-631                      [16, 64, 8, 8]            --
│    └─Clamp: 2-632                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-49                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-633         --                        --
│    └─One: 2-634                        [1]                       --
│    └─OutputScale: 2-635                --                        --
│    └─Empty: 2-636                      [16, 64, 1, 1]            --
│    └─Empty: 2-637                      [16, 64, 1, 1]            --
│    └─Empty: 2-638                      [16]                      --
│    └─Empty: 2-639                      [16]                      --
│    └─BatchNorm2d: 2-640                [16, 16, 8, 8]            --
│    └─Scaler: 2-641                     [16, 16, 8, 8]            --
│    └─ReLU: 2-642                       [16, 16, 8, 8]            --
│    └─Empty: 2-643                      [16, 16, 8, 8]            --
│    └─Clamp: 2-644                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-50         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-645                  [16, 64, 8, 8]            --
│    └─Empty: 2-646                      [16, 64, 8, 8]            --
│    └─Empty: 2-647                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-648         --                        --
│    └─One: 2-649                        [1]                       --
│    └─OutputScale: 2-650                --                        --
│    └─Empty: 2-651                      [16, 64, 3, 3]            --
│    └─Empty: 2-652                      [16, 64, 3, 3]            --
│    └─Empty: 2-653                      [16]                      --
│    └─Empty: 2-654                      [16]                      --
│    └─BatchNorm2d: 2-655                [16, 16, 8, 8]            --
│    └─Scaler: 2-656                     [16, 16, 8, 8]            --
│    └─ReLU: 2-657                       [16, 16, 8, 8]            --
│    └─Empty: 2-658                      [16, 16, 8, 8]            --
│    └─Clamp: 2-659                      [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-51                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-660         --                        --
│    └─One: 2-661                        [1]                       --
│    └─OutputScale: 2-662                --                        --
│    └─Empty: 2-663                      [64, 48, 1, 1]            --
│    └─Empty: 2-664                      [64, 48, 1, 1]            --
│    └─Empty: 2-665                      [64]                      --
│    └─Empty: 2-666                      [64]                      --
│    └─BatchNorm2d: 2-667                [16, 64, 64, 64]          --
│    └─Scaler: 2-668                     [16, 64, 64, 64]          --
│    └─ReLU: 2-669                       [16, 64, 64, 64]          --
│    └─Empty: 2-670                      [16, 64, 64, 64]          --
│    └─Clamp: 2-671                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-52                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-672         --                        --
│    └─One: 2-673                        [1]                       --
│    └─OutputScale: 2-674                --                        --
│    └─Empty: 2-675                      [64, 64, 3, 3]            --
│    └─Empty: 2-676                      [64, 64, 3, 3]            --
│    └─Empty: 2-677                      [64]                      --
│    └─Empty: 2-678                      [64]                      --
│    └─BatchNorm2d: 2-679                [16, 64, 64, 64]          --
│    └─Scaler: 2-680                     [16, 64, 64, 64]          --
│    └─ReLU: 2-681                       [16, 64, 64, 64]          --
│    └─Empty: 2-682                      [16, 64, 64, 64]          --
│    └─Clamp: 2-683                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-53         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-684                  [16, 64, 32, 32]          --
│    └─Empty: 2-685                      [16, 64, 32, 32]          --
│    └─Empty: 2-686                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-687         --                        --
│    └─One: 2-688                        [1]                       --
│    └─OutputScale: 2-689                --                        --
│    └─Empty: 2-690                      [64, 64, 3, 3]            --
│    └─Empty: 2-691                      [64, 64, 3, 3]            --
│    └─Empty: 2-692                      [64]                      --
│    └─Empty: 2-693                      [64]                      --
│    └─BatchNorm2d: 2-694                [16, 64, 32, 32]          --
│    └─Scaler: 2-695                     [16, 64, 32, 32]          --
│    └─ReLU: 2-696                       [16, 64, 32, 32]          --
│    └─Empty: 2-697                      [16, 64, 32, 32]          --
│    └─Clamp: 2-698                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-54                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-699         --                        --
│    └─One: 2-700                        [1]                       --
│    └─OutputScale: 2-701                --                        --
│    └─Empty: 2-702                      [64, 64, 1, 1]            --
│    └─Empty: 2-703                      [64, 64, 1, 1]            --
│    └─Empty: 2-704                      [64]                      --
│    └─Empty: 2-705                      [64]                      --
│    └─BatchNorm2d: 2-706                [16, 64, 32, 32]          --
│    └─Scaler: 2-707                     [16, 64, 32, 32]          --
│    └─ReLU: 2-708                       [16, 64, 32, 32]          --
│    └─Empty: 2-709                      [16, 64, 32, 32]          --
│    └─Clamp: 2-710                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-55         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-711                  [16, 64, 32, 32]          --
│    └─Empty: 2-712                      [16, 64, 32, 32]          --
│    └─Empty: 2-713                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-714         --                        --
│    └─One: 2-715                        [1]                       --
│    └─OutputScale: 2-716                --                        --
│    └─Empty: 2-717                      [64, 64, 3, 3]            --
│    └─Empty: 2-718                      [64, 64, 3, 3]            --
│    └─Empty: 2-719                      [64]                      --
│    └─Empty: 2-720                      [64]                      --
│    └─BatchNorm2d: 2-721                [16, 64, 32, 32]          --
│    └─Scaler: 2-722                     [16, 64, 32, 32]          --
│    └─ReLU: 2-723                       [16, 64, 32, 32]          --
│    └─Empty: 2-724                      [16, 64, 32, 32]          --
│    └─Clamp: 2-725                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-56         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-726                  [16, 64, 16, 16]          --
│    └─Empty: 2-727                      [16, 64, 16, 16]          --
│    └─Empty: 2-728                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-729         --                        --
│    └─One: 2-730                        [1]                       --
│    └─OutputScale: 2-731                --                        --
│    └─Empty: 2-732                      [64, 64, 3, 3]            --
│    └─Empty: 2-733                      [64, 64, 3, 3]            --
│    └─Empty: 2-734                      [64]                      --
│    └─Empty: 2-735                      [64]                      --
│    └─BatchNorm2d: 2-736                [16, 64, 16, 16]          --
│    └─Scaler: 2-737                     [16, 64, 16, 16]          --
│    └─ReLU: 2-738                       [16, 64, 16, 16]          --
│    └─Empty: 2-739                      [16, 64, 16, 16]          --
│    └─Clamp: 2-740                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-57                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-741         --                        --
│    └─One: 2-742                        [1]                       --
│    └─OutputScale: 2-743                --                        --
│    └─Empty: 2-744                      [64, 64, 1, 1]            --
│    └─Empty: 2-745                      [64, 64, 1, 1]            --
│    └─Empty: 2-746                      [64]                      --
│    └─Empty: 2-747                      [64]                      --
│    └─BatchNorm2d: 2-748                [16, 64, 16, 16]          --
│    └─Scaler: 2-749                     [16, 64, 16, 16]          --
│    └─ReLU: 2-750                       [16, 64, 16, 16]          --
│    └─Empty: 2-751                      [16, 64, 16, 16]          --
│    └─Clamp: 2-752                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-58         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-753                  [16, 64, 16, 16]          --
│    └─Empty: 2-754                      [16, 64, 16, 16]          --
│    └─Empty: 2-755                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-756         --                        --
│    └─One: 2-757                        [1]                       --
│    └─OutputScale: 2-758                --                        --
│    └─Empty: 2-759                      [64, 64, 3, 3]            --
│    └─Empty: 2-760                      [64, 64, 3, 3]            --
│    └─Empty: 2-761                      [64]                      --
│    └─Empty: 2-762                      [64]                      --
│    └─BatchNorm2d: 2-763                [16, 64, 16, 16]          --
│    └─Scaler: 2-764                     [16, 64, 16, 16]          --
│    └─ReLU: 2-765                       [16, 64, 16, 16]          --
│    └─Empty: 2-766                      [16, 64, 16, 16]          --
│    └─Clamp: 2-767                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-59         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-768                  [16, 64, 8, 8]            --
│    └─Empty: 2-769                      [16, 64, 8, 8]            --
│    └─Empty: 2-770                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-771         --                        --
│    └─One: 2-772                        [1]                       --
│    └─OutputScale: 2-773                --                        --
│    └─Empty: 2-774                      [64, 64, 3, 3]            --
│    └─Empty: 2-775                      [64, 64, 3, 3]            --
│    └─Empty: 2-776                      [64]                      --
│    └─Empty: 2-777                      [64]                      --
│    └─BatchNorm2d: 2-778                [16, 64, 8, 8]            --
│    └─Scaler: 2-779                     [16, 64, 8, 8]            --
│    └─ReLU: 2-780                       [16, 64, 8, 8]            --
│    └─Empty: 2-781                      [16, 64, 8, 8]            --
│    └─Clamp: 2-782                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-60                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-783         --                        --
│    └─One: 2-784                        [1]                       --
│    └─OutputScale: 2-785                --                        --
│    └─Empty: 2-786                      [16, 64, 1, 1]            --
│    └─Empty: 2-787                      [16, 64, 1, 1]            --
│    └─Empty: 2-788                      [16]                      --
│    └─Empty: 2-789                      [16]                      --
│    └─BatchNorm2d: 2-790                [16, 16, 8, 8]            --
│    └─Scaler: 2-791                     [16, 16, 8, 8]            --
│    └─ReLU: 2-792                       [16, 16, 8, 8]            --
│    └─Empty: 2-793                      [16, 16, 8, 8]            --
│    └─Clamp: 2-794                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-61         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-795                  [16, 64, 8, 8]            --
│    └─Empty: 2-796                      [16, 64, 8, 8]            --
│    └─Empty: 2-797                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-798         --                        --
│    └─One: 2-799                        [1]                       --
│    └─OutputScale: 2-800                --                        --
│    └─Empty: 2-801                      [16, 64, 3, 3]            --
│    └─Empty: 2-802                      [16, 64, 3, 3]            --
│    └─Empty: 2-803                      [16]                      --
│    └─Empty: 2-804                      [16]                      --
│    └─BatchNorm2d: 2-805                [16, 16, 8, 8]            --
│    └─Scaler: 2-806                     [16, 16, 8, 8]            --
│    └─ReLU: 2-807                       [16, 16, 8, 8]            --
│    └─Empty: 2-808                      [16, 16, 8, 8]            --
│    └─Clamp: 2-809                      [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-62                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-810         --                        --
│    └─One: 2-811                        [1]                       --
│    └─OutputScale: 2-812                --                        --
│    └─Empty: 2-813                      [64, 48, 1, 1]            --
│    └─Empty: 2-814                      [64, 48, 1, 1]            --
│    └─Empty: 2-815                      [64]                      --
│    └─Empty: 2-816                      [64]                      --
│    └─BatchNorm2d: 2-817                [16, 64, 64, 64]          --
│    └─Scaler: 2-818                     [16, 64, 64, 64]          --
│    └─ReLU: 2-819                       [16, 64, 64, 64]          --
│    └─Empty: 2-820                      [16, 64, 64, 64]          --
│    └─Clamp: 2-821                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-63                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-822         --                        --
│    └─One: 2-823                        [1]                       --
│    └─OutputScale: 2-824                --                        --
│    └─Empty: 2-825                      [64, 64, 3, 3]            --
│    └─Empty: 2-826                      [64, 64, 3, 3]            --
│    └─Empty: 2-827                      [64]                      --
│    └─Empty: 2-828                      [64]                      --
│    └─BatchNorm2d: 2-829                [16, 64, 64, 64]          --
│    └─Scaler: 2-830                     [16, 64, 64, 64]          --
│    └─ReLU: 2-831                       [16, 64, 64, 64]          --
│    └─Empty: 2-832                      [16, 64, 64, 64]          --
│    └─Clamp: 2-833                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-64         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-834                  [16, 64, 32, 32]          --
│    └─Empty: 2-835                      [16, 64, 32, 32]          --
│    └─Empty: 2-836                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-837         --                        --
│    └─One: 2-838                        [1]                       --
│    └─OutputScale: 2-839                --                        --
│    └─Empty: 2-840                      [64, 64, 3, 3]            --
│    └─Empty: 2-841                      [64, 64, 3, 3]            --
│    └─Empty: 2-842                      [64]                      --
│    └─Empty: 2-843                      [64]                      --
│    └─BatchNorm2d: 2-844                [16, 64, 32, 32]          --
│    └─Scaler: 2-845                     [16, 64, 32, 32]          --
│    └─ReLU: 2-846                       [16, 64, 32, 32]          --
│    └─Empty: 2-847                      [16, 64, 32, 32]          --
│    └─Clamp: 2-848                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-65                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-849         --                        --
│    └─One: 2-850                        [1]                       --
│    └─OutputScale: 2-851                --                        --
│    └─Empty: 2-852                      [64, 64, 1, 1]            --
│    └─Empty: 2-853                      [64, 64, 1, 1]            --
│    └─Empty: 2-854                      [64]                      --
│    └─Empty: 2-855                      [64]                      --
│    └─BatchNorm2d: 2-856                [16, 64, 32, 32]          --
│    └─Scaler: 2-857                     [16, 64, 32, 32]          --
│    └─ReLU: 2-858                       [16, 64, 32, 32]          --
│    └─Empty: 2-859                      [16, 64, 32, 32]          --
│    └─Clamp: 2-860                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-861                  [16, 64, 32, 32]          --
│    └─Empty: 2-862                      [16, 64, 32, 32]          --
│    └─Empty: 2-863                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-864         --                        --
│    └─One: 2-865                        [1]                       --
│    └─OutputScale: 2-866                --                        --
│    └─Empty: 2-867                      [64, 64, 3, 3]            --
│    └─Empty: 2-868                      [64, 64, 3, 3]            --
│    └─Empty: 2-869                      [64]                      --
│    └─Empty: 2-870                      [64]                      --
│    └─BatchNorm2d: 2-871                [16, 64, 32, 32]          --
│    └─Scaler: 2-872                     [16, 64, 32, 32]          --
│    └─ReLU: 2-873                       [16, 64, 32, 32]          --
│    └─Empty: 2-874                      [16, 64, 32, 32]          --
│    └─Clamp: 2-875                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-67         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-876                  [16, 64, 16, 16]          --
│    └─Empty: 2-877                      [16, 64, 16, 16]          --
│    └─Empty: 2-878                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-879         --                        --
│    └─One: 2-880                        [1]                       --
│    └─OutputScale: 2-881                --                        --
│    └─Empty: 2-882                      [64, 64, 3, 3]            --
│    └─Empty: 2-883                      [64, 64, 3, 3]            --
│    └─Empty: 2-884                      [64]                      --
│    └─Empty: 2-885                      [64]                      --
│    └─BatchNorm2d: 2-886                [16, 64, 16, 16]          --
│    └─Scaler: 2-887                     [16, 64, 16, 16]          --
│    └─ReLU: 2-888                       [16, 64, 16, 16]          --
│    └─Empty: 2-889                      [16, 64, 16, 16]          --
│    └─Clamp: 2-890                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-68                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-891         --                        --
│    └─One: 2-892                        [1]                       --
│    └─OutputScale: 2-893                --                        --
│    └─Empty: 2-894                      [64, 64, 1, 1]            --
│    └─Empty: 2-895                      [64, 64, 1, 1]            --
│    └─Empty: 2-896                      [64]                      --
│    └─Empty: 2-897                      [64]                      --
│    └─BatchNorm2d: 2-898                [16, 64, 16, 16]          --
│    └─Scaler: 2-899                     [16, 64, 16, 16]          --
│    └─ReLU: 2-900                       [16, 64, 16, 16]          --
│    └─Empty: 2-901                      [16, 64, 16, 16]          --
│    └─Clamp: 2-902                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-69         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-903                  [16, 64, 16, 16]          --
│    └─Empty: 2-904                      [16, 64, 16, 16]          --
│    └─Empty: 2-905                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-906         --                        --
│    └─One: 2-907                        [1]                       --
│    └─OutputScale: 2-908                --                        --
│    └─Empty: 2-909                      [64, 64, 3, 3]            --
│    └─Empty: 2-910                      [64, 64, 3, 3]            --
│    └─Empty: 2-911                      [64]                      --
│    └─Empty: 2-912                      [64]                      --
│    └─BatchNorm2d: 2-913                [16, 64, 16, 16]          --
│    └─Scaler: 2-914                     [16, 64, 16, 16]          --
│    └─ReLU: 2-915                       [16, 64, 16, 16]          --
│    └─Empty: 2-916                      [16, 64, 16, 16]          --
│    └─Clamp: 2-917                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-70         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-918                  [16, 64, 8, 8]            --
│    └─Empty: 2-919                      [16, 64, 8, 8]            --
│    └─Empty: 2-920                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-921         --                        --
│    └─One: 2-922                        [1]                       --
│    └─OutputScale: 2-923                --                        --
│    └─Empty: 2-924                      [64, 64, 3, 3]            --
│    └─Empty: 2-925                      [64, 64, 3, 3]            --
│    └─Empty: 2-926                      [64]                      --
│    └─Empty: 2-927                      [64]                      --
│    └─BatchNorm2d: 2-928                [16, 64, 8, 8]            --
│    └─Scaler: 2-929                     [16, 64, 8, 8]            --
│    └─ReLU: 2-930                       [16, 64, 8, 8]            --
│    └─Empty: 2-931                      [16, 64, 8, 8]            --
│    └─Clamp: 2-932                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-71                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-933         --                        --
│    └─One: 2-934                        [1]                       --
│    └─OutputScale: 2-935                --                        --
│    └─Empty: 2-936                      [16, 64, 1, 1]            --
│    └─Empty: 2-937                      [16, 64, 1, 1]            --
│    └─Empty: 2-938                      [16]                      --
│    └─Empty: 2-939                      [16]                      --
│    └─BatchNorm2d: 2-940                [16, 16, 8, 8]            --
│    └─Scaler: 2-941                     [16, 16, 8, 8]            --
│    └─ReLU: 2-942                       [16, 16, 8, 8]            --
│    └─Empty: 2-943                      [16, 16, 8, 8]            --
│    └─Clamp: 2-944                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-72         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-945                  [16, 64, 8, 8]            --
│    └─Empty: 2-946                      [16, 64, 8, 8]            --
│    └─Empty: 2-947                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-948         --                        --
│    └─One: 2-949                        [1]                       --
│    └─OutputScale: 2-950                --                        --
│    └─Empty: 2-951                      [16, 64, 3, 3]            --
│    └─Empty: 2-952                      [16, 64, 3, 3]            --
│    └─Empty: 2-953                      [16]                      --
│    └─Empty: 2-954                      [16]                      --
│    └─BatchNorm2d: 2-955                [16, 16, 8, 8]            --
│    └─Scaler: 2-956                     [16, 16, 8, 8]            --
│    └─ReLU: 2-957                       [16, 16, 8, 8]            --
│    └─Empty: 2-958                      [16, 16, 8, 8]            --
│    └─Clamp: 2-959                      [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-73                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-960         --                        --
│    └─One: 2-961                        [1]                       --
│    └─OutputScale: 2-962                --                        --
│    └─Empty: 2-963                      [64, 48, 1, 1]            --
│    └─Empty: 2-964                      [64, 48, 1, 1]            --
│    └─Empty: 2-965                      [64]                      --
│    └─Empty: 2-966                      [64]                      --
│    └─BatchNorm2d: 2-967                [16, 64, 64, 64]          --
│    └─Scaler: 2-968                     [16, 64, 64, 64]          --
│    └─ReLU: 2-969                       [16, 64, 64, 64]          --
│    └─Empty: 2-970                      [16, 64, 64, 64]          --
│    └─Clamp: 2-971                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-74                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-972         --                        --
│    └─One: 2-973                        [1]                       --
│    └─OutputScale: 2-974                --                        --
│    └─Empty: 2-975                      [64, 64, 3, 3]            --
│    └─Empty: 2-976                      [64, 64, 3, 3]            --
│    └─Empty: 2-977                      [64]                      --
│    └─Empty: 2-978                      [64]                      --
│    └─BatchNorm2d: 2-979                [16, 64, 64, 64]          --
│    └─Scaler: 2-980                     [16, 64, 64, 64]          --
│    └─ReLU: 2-981                       [16, 64, 64, 64]          --
│    └─Empty: 2-982                      [16, 64, 64, 64]          --
│    └─Clamp: 2-983                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-984                  [16, 64, 32, 32]          --
│    └─Empty: 2-985                      [16, 64, 32, 32]          --
│    └─Empty: 2-986                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-987         --                        --
│    └─One: 2-988                        [1]                       --
│    └─OutputScale: 2-989                --                        --
│    └─Empty: 2-990                      [64, 64, 3, 3]            --
│    └─Empty: 2-991                      [64, 64, 3, 3]            --
│    └─Empty: 2-992                      [64]                      --
│    └─Empty: 2-993                      [64]                      --
│    └─BatchNorm2d: 2-994                [16, 64, 32, 32]          --
│    └─Scaler: 2-995                     [16, 64, 32, 32]          --
│    └─ReLU: 2-996                       [16, 64, 32, 32]          --
│    └─Empty: 2-997                      [16, 64, 32, 32]          --
│    └─Clamp: 2-998                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-76                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-999         --                        --
│    └─One: 2-1000                       [1]                       --
│    └─OutputScale: 2-1001               --                        --
│    └─Empty: 2-1002                     [64, 64, 1, 1]            --
│    └─Empty: 2-1003                     [64, 64, 1, 1]            --
│    └─Empty: 2-1004                     [64]                      --
│    └─Empty: 2-1005                     [64]                      --
│    └─BatchNorm2d: 2-1006               [16, 64, 32, 32]          --
│    └─Scaler: 2-1007                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1008                      [16, 64, 32, 32]          --
│    └─Empty: 2-1009                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1010                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-77         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1011                 [16, 64, 32, 32]          --
│    └─Empty: 2-1012                     [16, 64, 32, 32]          --
│    └─Empty: 2-1013                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1014        --                        --
│    └─One: 2-1015                       [1]                       --
│    └─OutputScale: 2-1016               --                        --
│    └─Empty: 2-1017                     [64, 64, 3, 3]            --
│    └─Empty: 2-1018                     [64, 64, 3, 3]            --
│    └─Empty: 2-1019                     [64]                      --
│    └─Empty: 2-1020                     [64]                      --
│    └─BatchNorm2d: 2-1021               [16, 64, 32, 32]          --
│    └─Scaler: 2-1022                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1023                      [16, 64, 32, 32]          --
│    └─Empty: 2-1024                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1025                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-78         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1026                 [16, 64, 16, 16]          --
│    └─Empty: 2-1027                     [16, 64, 16, 16]          --
│    └─Empty: 2-1028                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1029        --                        --
│    └─One: 2-1030                       [1]                       --
│    └─OutputScale: 2-1031               --                        --
│    └─Empty: 2-1032                     [64, 64, 3, 3]            --
│    └─Empty: 2-1033                     [64, 64, 3, 3]            --
│    └─Empty: 2-1034                     [64]                      --
│    └─Empty: 2-1035                     [64]                      --
│    └─BatchNorm2d: 2-1036               [16, 64, 16, 16]          --
│    └─Scaler: 2-1037                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1038                      [16, 64, 16, 16]          --
│    └─Empty: 2-1039                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1040                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-79                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1041        --                        --
│    └─One: 2-1042                       [1]                       --
│    └─OutputScale: 2-1043               --                        --
│    └─Empty: 2-1044                     [64, 64, 1, 1]            --
│    └─Empty: 2-1045                     [64, 64, 1, 1]            --
│    └─Empty: 2-1046                     [64]                      --
│    └─Empty: 2-1047                     [64]                      --
│    └─BatchNorm2d: 2-1048               [16, 64, 16, 16]          --
│    └─Scaler: 2-1049                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1050                      [16, 64, 16, 16]          --
│    └─Empty: 2-1051                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1052                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-80         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1053                 [16, 64, 16, 16]          --
│    └─Empty: 2-1054                     [16, 64, 16, 16]          --
│    └─Empty: 2-1055                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1056        --                        --
│    └─One: 2-1057                       [1]                       --
│    └─OutputScale: 2-1058               --                        --
│    └─Empty: 2-1059                     [64, 64, 3, 3]            --
│    └─Empty: 2-1060                     [64, 64, 3, 3]            --
│    └─Empty: 2-1061                     [64]                      --
│    └─Empty: 2-1062                     [64]                      --
│    └─BatchNorm2d: 2-1063               [16, 64, 16, 16]          --
│    └─Scaler: 2-1064                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1065                      [16, 64, 16, 16]          --
│    └─Empty: 2-1066                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1067                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1068                 [16, 64, 8, 8]            --
│    └─Empty: 2-1069                     [16, 64, 8, 8]            --
│    └─Empty: 2-1070                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1071        --                        --
│    └─One: 2-1072                       [1]                       --
│    └─OutputScale: 2-1073               --                        --
│    └─Empty: 2-1074                     [64, 64, 3, 3]            --
│    └─Empty: 2-1075                     [64, 64, 3, 3]            --
│    └─Empty: 2-1076                     [64]                      --
│    └─Empty: 2-1077                     [64]                      --
│    └─BatchNorm2d: 2-1078               [16, 64, 8, 8]            --
│    └─Scaler: 2-1079                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1080                      [16, 64, 8, 8]            --
│    └─Empty: 2-1081                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1082                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-82                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1083        --                        --
│    └─One: 2-1084                       [1]                       --
│    └─OutputScale: 2-1085               --                        --
│    └─Empty: 2-1086                     [16, 64, 1, 1]            --
│    └─Empty: 2-1087                     [16, 64, 1, 1]            --
│    └─Empty: 2-1088                     [16]                      --
│    └─Empty: 2-1089                     [16]                      --
│    └─BatchNorm2d: 2-1090               [16, 16, 8, 8]            --
│    └─Scaler: 2-1091                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1092                      [16, 16, 8, 8]            --
│    └─Empty: 2-1093                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1094                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-83         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1095                 [16, 64, 8, 8]            --
│    └─Empty: 2-1096                     [16, 64, 8, 8]            --
│    └─Empty: 2-1097                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1098        --                        --
│    └─One: 2-1099                       [1]                       --
│    └─OutputScale: 2-1100               --                        --
│    └─Empty: 2-1101                     [16, 64, 3, 3]            --
│    └─Empty: 2-1102                     [16, 64, 3, 3]            --
│    └─Empty: 2-1103                     [16]                      --
│    └─Empty: 2-1104                     [16]                      --
│    └─BatchNorm2d: 2-1105               [16, 16, 8, 8]            --
│    └─Scaler: 2-1106                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1107                      [16, 16, 8, 8]            --
│    └─Empty: 2-1108                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1109                     [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-84                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1110        --                        --
│    └─One: 2-1111                       [1]                       --
│    └─OutputScale: 2-1112               --                        --
│    └─Empty: 2-1113                     [64, 48, 1, 1]            --
│    └─Empty: 2-1114                     [64, 48, 1, 1]            --
│    └─Empty: 2-1115                     [64]                      --
│    └─Empty: 2-1116                     [64]                      --
│    └─BatchNorm2d: 2-1117               [16, 64, 64, 64]          --
│    └─Scaler: 2-1118                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1119                      [16, 64, 64, 64]          --
│    └─Empty: 2-1120                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1121                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-85                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1122        --                        --
│    └─One: 2-1123                       [1]                       --
│    └─OutputScale: 2-1124               --                        --
│    └─Empty: 2-1125                     [64, 64, 3, 3]            --
│    └─Empty: 2-1126                     [64, 64, 3, 3]            --
│    └─Empty: 2-1127                     [64]                      --
│    └─Empty: 2-1128                     [64]                      --
│    └─BatchNorm2d: 2-1129               [16, 64, 64, 64]          --
│    └─Scaler: 2-1130                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1131                      [16, 64, 64, 64]          --
│    └─Empty: 2-1132                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1133                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-86         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1134                 [16, 64, 32, 32]          --
│    └─Empty: 2-1135                     [16, 64, 32, 32]          --
│    └─Empty: 2-1136                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1137        --                        --
│    └─One: 2-1138                       [1]                       --
│    └─OutputScale: 2-1139               --                        --
│    └─Empty: 2-1140                     [64, 64, 3, 3]            --
│    └─Empty: 2-1141                     [64, 64, 3, 3]            --
│    └─Empty: 2-1142                     [64]                      --
│    └─Empty: 2-1143                     [64]                      --
│    └─BatchNorm2d: 2-1144               [16, 64, 32, 32]          --
│    └─Scaler: 2-1145                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1146                      [16, 64, 32, 32]          --
│    └─Empty: 2-1147                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1148                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-87                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1149        --                        --
│    └─One: 2-1150                       [1]                       --
│    └─OutputScale: 2-1151               --                        --
│    └─Empty: 2-1152                     [64, 64, 1, 1]            --
│    └─Empty: 2-1153                     [64, 64, 1, 1]            --
│    └─Empty: 2-1154                     [64]                      --
│    └─Empty: 2-1155                     [64]                      --
│    └─BatchNorm2d: 2-1156               [16, 64, 32, 32]          --
│    └─Scaler: 2-1157                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1158                      [16, 64, 32, 32]          --
│    └─Empty: 2-1159                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1160                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-88         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1161                 [16, 64, 32, 32]          --
│    └─Empty: 2-1162                     [16, 64, 32, 32]          --
│    └─Empty: 2-1163                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1164        --                        --
│    └─One: 2-1165                       [1]                       --
│    └─OutputScale: 2-1166               --                        --
│    └─Empty: 2-1167                     [64, 64, 3, 3]            --
│    └─Empty: 2-1168                     [64, 64, 3, 3]            --
│    └─Empty: 2-1169                     [64]                      --
│    └─Empty: 2-1170                     [64]                      --
│    └─BatchNorm2d: 2-1171               [16, 64, 32, 32]          --
│    └─Scaler: 2-1172                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1173                      [16, 64, 32, 32]          --
│    └─Empty: 2-1174                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1175                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-89         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1176                 [16, 64, 16, 16]          --
│    └─Empty: 2-1177                     [16, 64, 16, 16]          --
│    └─Empty: 2-1178                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1179        --                        --
│    └─One: 2-1180                       [1]                       --
│    └─OutputScale: 2-1181               --                        --
│    └─Empty: 2-1182                     [64, 64, 3, 3]            --
│    └─Empty: 2-1183                     [64, 64, 3, 3]            --
│    └─Empty: 2-1184                     [64]                      --
│    └─Empty: 2-1185                     [64]                      --
│    └─BatchNorm2d: 2-1186               [16, 64, 16, 16]          --
│    └─Scaler: 2-1187                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1188                      [16, 64, 16, 16]          --
│    └─Empty: 2-1189                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1190                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-90                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1191        --                        --
│    └─One: 2-1192                       [1]                       --
│    └─OutputScale: 2-1193               --                        --
│    └─Empty: 2-1194                     [64, 64, 1, 1]            --
│    └─Empty: 2-1195                     [64, 64, 1, 1]            --
│    └─Empty: 2-1196                     [64]                      --
│    └─Empty: 2-1197                     [64]                      --
│    └─BatchNorm2d: 2-1198               [16, 64, 16, 16]          --
│    └─Scaler: 2-1199                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1200                      [16, 64, 16, 16]          --
│    └─Empty: 2-1201                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1202                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-91         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1203                 [16, 64, 16, 16]          --
│    └─Empty: 2-1204                     [16, 64, 16, 16]          --
│    └─Empty: 2-1205                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1206        --                        --
│    └─One: 2-1207                       [1]                       --
│    └─OutputScale: 2-1208               --                        --
│    └─Empty: 2-1209                     [64, 64, 3, 3]            --
│    └─Empty: 2-1210                     [64, 64, 3, 3]            --
│    └─Empty: 2-1211                     [64]                      --
│    └─Empty: 2-1212                     [64]                      --
│    └─BatchNorm2d: 2-1213               [16, 64, 16, 16]          --
│    └─Scaler: 2-1214                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1215                      [16, 64, 16, 16]          --
│    └─Empty: 2-1216                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1217                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-92         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1218                 [16, 64, 8, 8]            --
│    └─Empty: 2-1219                     [16, 64, 8, 8]            --
│    └─Empty: 2-1220                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1221        --                        --
│    └─One: 2-1222                       [1]                       --
│    └─OutputScale: 2-1223               --                        --
│    └─Empty: 2-1224                     [64, 64, 3, 3]            --
│    └─Empty: 2-1225                     [64, 64, 3, 3]            --
│    └─Empty: 2-1226                     [64]                      --
│    └─Empty: 2-1227                     [64]                      --
│    └─BatchNorm2d: 2-1228               [16, 64, 8, 8]            --
│    └─Scaler: 2-1229                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1230                      [16, 64, 8, 8]            --
│    └─Empty: 2-1231                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1232                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-93                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1233        --                        --
│    └─One: 2-1234                       [1]                       --
│    └─OutputScale: 2-1235               --                        --
│    └─Empty: 2-1236                     [16, 64, 1, 1]            --
│    └─Empty: 2-1237                     [16, 64, 1, 1]            --
│    └─Empty: 2-1238                     [16]                      --
│    └─Empty: 2-1239                     [16]                      --
│    └─BatchNorm2d: 2-1240               [16, 16, 8, 8]            --
│    └─Scaler: 2-1241                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1242                      [16, 16, 8, 8]            --
│    └─Empty: 2-1243                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1244                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-94         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1245                 [16, 64, 8, 8]            --
│    └─Empty: 2-1246                     [16, 64, 8, 8]            --
│    └─Empty: 2-1247                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1248        --                        --
│    └─One: 2-1249                       [1]                       --
│    └─OutputScale: 2-1250               --                        --
│    └─Empty: 2-1251                     [16, 64, 3, 3]            --
│    └─Empty: 2-1252                     [16, 64, 3, 3]            --
│    └─Empty: 2-1253                     [16]                      --
│    └─Empty: 2-1254                     [16]                      --
│    └─BatchNorm2d: 2-1255               [16, 16, 8, 8]            --
│    └─Scaler: 2-1256                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1257                      [16, 16, 8, 8]            --
│    └─Empty: 2-1258                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1259                     [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-95                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1260        --                        --
│    └─One: 2-1261                       [1]                       --
│    └─OutputScale: 2-1262               --                        --
│    └─Empty: 2-1263                     [64, 48, 1, 1]            --
│    └─Empty: 2-1264                     [64, 48, 1, 1]            --
│    └─Empty: 2-1265                     [64]                      --
│    └─Empty: 2-1266                     [64]                      --
│    └─BatchNorm2d: 2-1267               [16, 64, 64, 64]          --
│    └─Scaler: 2-1268                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1269                      [16, 64, 64, 64]          --
│    └─Empty: 2-1270                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1271                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-96                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1272        --                        --
│    └─One: 2-1273                       [1]                       --
│    └─OutputScale: 2-1274               --                        --
│    └─Empty: 2-1275                     [64, 64, 3, 3]            --
│    └─Empty: 2-1276                     [64, 64, 3, 3]            --
│    └─Empty: 2-1277                     [64]                      --
│    └─Empty: 2-1278                     [64]                      --
│    └─BatchNorm2d: 2-1279               [16, 64, 64, 64]          --
│    └─Scaler: 2-1280                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1281                      [16, 64, 64, 64]          --
│    └─Empty: 2-1282                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1283                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-97         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1284                 [16, 64, 32, 32]          --
│    └─Empty: 2-1285                     [16, 64, 32, 32]          --
│    └─Empty: 2-1286                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1287        --                        --
│    └─One: 2-1288                       [1]                       --
│    └─OutputScale: 2-1289               --                        --
│    └─Empty: 2-1290                     [64, 64, 3, 3]            --
│    └─Empty: 2-1291                     [64, 64, 3, 3]            --
│    └─Empty: 2-1292                     [64]                      --
│    └─Empty: 2-1293                     [64]                      --
│    └─BatchNorm2d: 2-1294               [16, 64, 32, 32]          --
│    └─Scaler: 2-1295                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1296                      [16, 64, 32, 32]          --
│    └─Empty: 2-1297                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1298                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-98                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1299        --                        --
│    └─One: 2-1300                       [1]                       --
│    └─OutputScale: 2-1301               --                        --
│    └─Empty: 2-1302                     [64, 64, 1, 1]            --
│    └─Empty: 2-1303                     [64, 64, 1, 1]            --
│    └─Empty: 2-1304                     [64]                      --
│    └─Empty: 2-1305                     [64]                      --
│    └─BatchNorm2d: 2-1306               [16, 64, 32, 32]          --
│    └─Scaler: 2-1307                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1308                      [16, 64, 32, 32]          --
│    └─Empty: 2-1309                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1310                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-99         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1311                 [16, 64, 32, 32]          --
│    └─Empty: 2-1312                     [16, 64, 32, 32]          --
│    └─Empty: 2-1313                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1314        --                        --
│    └─One: 2-1315                       [1]                       --
│    └─OutputScale: 2-1316               --                        --
│    └─Empty: 2-1317                     [64, 64, 3, 3]            --
│    └─Empty: 2-1318                     [64, 64, 3, 3]            --
│    └─Empty: 2-1319                     [64]                      --
│    └─Empty: 2-1320                     [64]                      --
│    └─BatchNorm2d: 2-1321               [16, 64, 32, 32]          --
│    └─Scaler: 2-1322                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1323                      [16, 64, 32, 32]          --
│    └─Empty: 2-1324                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1325                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-100        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1326                 [16, 64, 16, 16]          --
│    └─Empty: 2-1327                     [16, 64, 16, 16]          --
│    └─Empty: 2-1328                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1329        --                        --
│    └─One: 2-1330                       [1]                       --
│    └─OutputScale: 2-1331               --                        --
│    └─Empty: 2-1332                     [64, 64, 3, 3]            --
│    └─Empty: 2-1333                     [64, 64, 3, 3]            --
│    └─Empty: 2-1334                     [64]                      --
│    └─Empty: 2-1335                     [64]                      --
│    └─BatchNorm2d: 2-1336               [16, 64, 16, 16]          --
│    └─Scaler: 2-1337                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1338                      [16, 64, 16, 16]          --
│    └─Empty: 2-1339                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1340                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-101               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1341        --                        --
│    └─One: 2-1342                       [1]                       --
│    └─OutputScale: 2-1343               --                        --
│    └─Empty: 2-1344                     [64, 64, 1, 1]            --
│    └─Empty: 2-1345                     [64, 64, 1, 1]            --
│    └─Empty: 2-1346                     [64]                      --
│    └─Empty: 2-1347                     [64]                      --
│    └─BatchNorm2d: 2-1348               [16, 64, 16, 16]          --
│    └─Scaler: 2-1349                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1350                      [16, 64, 16, 16]          --
│    └─Empty: 2-1351                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1352                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-102        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1353                 [16, 64, 16, 16]          --
│    └─Empty: 2-1354                     [16, 64, 16, 16]          --
│    └─Empty: 2-1355                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1356        --                        --
│    └─One: 2-1357                       [1]                       --
│    └─OutputScale: 2-1358               --                        --
│    └─Empty: 2-1359                     [64, 64, 3, 3]            --
│    └─Empty: 2-1360                     [64, 64, 3, 3]            --
│    └─Empty: 2-1361                     [64]                      --
│    └─Empty: 2-1362                     [64]                      --
│    └─BatchNorm2d: 2-1363               [16, 64, 16, 16]          --
│    └─Scaler: 2-1364                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1365                      [16, 64, 16, 16]          --
│    └─Empty: 2-1366                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1367                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-103        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1368                 [16, 64, 8, 8]            --
│    └─Empty: 2-1369                     [16, 64, 8, 8]            --
│    └─Empty: 2-1370                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1371        --                        --
│    └─One: 2-1372                       [1]                       --
│    └─OutputScale: 2-1373               --                        --
│    └─Empty: 2-1374                     [64, 64, 3, 3]            --
│    └─Empty: 2-1375                     [64, 64, 3, 3]            --
│    └─Empty: 2-1376                     [64]                      --
│    └─Empty: 2-1377                     [64]                      --
│    └─BatchNorm2d: 2-1378               [16, 64, 8, 8]            --
│    └─Scaler: 2-1379                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1380                      [16, 64, 8, 8]            --
│    └─Empty: 2-1381                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1382                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-104               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1383        --                        --
│    └─One: 2-1384                       [1]                       --
│    └─OutputScale: 2-1385               --                        --
│    └─Empty: 2-1386                     [16, 64, 1, 1]            --
│    └─Empty: 2-1387                     [16, 64, 1, 1]            --
│    └─Empty: 2-1388                     [16]                      --
│    └─Empty: 2-1389                     [16]                      --
│    └─BatchNorm2d: 2-1390               [16, 16, 8, 8]            --
│    └─Scaler: 2-1391                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1392                      [16, 16, 8, 8]            --
│    └─Empty: 2-1393                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1394                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-105        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1395                 [16, 64, 8, 8]            --
│    └─Empty: 2-1396                     [16, 64, 8, 8]            --
│    └─Empty: 2-1397                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1398        --                        --
│    └─One: 2-1399                       [1]                       --
│    └─OutputScale: 2-1400               --                        --
│    └─Empty: 2-1401                     [16, 64, 3, 3]            --
│    └─Empty: 2-1402                     [16, 64, 3, 3]            --
│    └─Empty: 2-1403                     [16]                      --
│    └─Empty: 2-1404                     [16]                      --
│    └─BatchNorm2d: 2-1405               [16, 16, 8, 8]            --
│    └─Scaler: 2-1406                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1407                      [16, 16, 8, 8]            --
│    └─Empty: 2-1408                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1409                     [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-106               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1410        --                        --
│    └─One: 2-1411                       [1]                       --
│    └─OutputScale: 2-1412               --                        --
│    └─Empty: 2-1413                     [64, 48, 1, 1]            --
│    └─Empty: 2-1414                     [64, 48, 1, 1]            --
│    └─Empty: 2-1415                     [64]                      --
│    └─Empty: 2-1416                     [64]                      --
│    └─BatchNorm2d: 2-1417               [16, 64, 64, 64]          --
│    └─Scaler: 2-1418                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1419                      [16, 64, 64, 64]          --
│    └─Empty: 2-1420                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1421                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-107               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1422        --                        --
│    └─One: 2-1423                       [1]                       --
│    └─OutputScale: 2-1424               --                        --
│    └─Empty: 2-1425                     [64, 64, 3, 3]            --
│    └─Empty: 2-1426                     [64, 64, 3, 3]            --
│    └─Empty: 2-1427                     [64]                      --
│    └─Empty: 2-1428                     [64]                      --
│    └─BatchNorm2d: 2-1429               [16, 64, 64, 64]          --
│    └─Scaler: 2-1430                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1431                      [16, 64, 64, 64]          --
│    └─Empty: 2-1432                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1433                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-108        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1434                 [16, 64, 32, 32]          --
│    └─Empty: 2-1435                     [16, 64, 32, 32]          --
│    └─Empty: 2-1436                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1437        --                        --
│    └─One: 2-1438                       [1]                       --
│    └─OutputScale: 2-1439               --                        --
│    └─Empty: 2-1440                     [64, 64, 3, 3]            --
│    └─Empty: 2-1441                     [64, 64, 3, 3]            --
│    └─Empty: 2-1442                     [64]                      --
│    └─Empty: 2-1443                     [64]                      --
│    └─BatchNorm2d: 2-1444               [16, 64, 32, 32]          --
│    └─Scaler: 2-1445                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1446                      [16, 64, 32, 32]          --
│    └─Empty: 2-1447                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1448                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-109               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1449        --                        --
│    └─One: 2-1450                       [1]                       --
│    └─OutputScale: 2-1451               --                        --
│    └─Empty: 2-1452                     [64, 64, 1, 1]            --
│    └─Empty: 2-1453                     [64, 64, 1, 1]            --
│    └─Empty: 2-1454                     [64]                      --
│    └─Empty: 2-1455                     [64]                      --
│    └─BatchNorm2d: 2-1456               [16, 64, 32, 32]          --
│    └─Scaler: 2-1457                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1458                      [16, 64, 32, 32]          --
│    └─Empty: 2-1459                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1460                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-110        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1461                 [16, 64, 32, 32]          --
│    └─Empty: 2-1462                     [16, 64, 32, 32]          --
│    └─Empty: 2-1463                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1464        --                        --
│    └─One: 2-1465                       [1]                       --
│    └─OutputScale: 2-1466               --                        --
│    └─Empty: 2-1467                     [64, 64, 3, 3]            --
│    └─Empty: 2-1468                     [64, 64, 3, 3]            --
│    └─Empty: 2-1469                     [64]                      --
│    └─Empty: 2-1470                     [64]                      --
│    └─BatchNorm2d: 2-1471               [16, 64, 32, 32]          --
│    └─Scaler: 2-1472                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1473                      [16, 64, 32, 32]          --
│    └─Empty: 2-1474                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1475                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-111        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1476                 [16, 64, 16, 16]          --
│    └─Empty: 2-1477                     [16, 64, 16, 16]          --
│    └─Empty: 2-1478                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1479        --                        --
│    └─One: 2-1480                       [1]                       --
│    └─OutputScale: 2-1481               --                        --
│    └─Empty: 2-1482                     [64, 64, 3, 3]            --
│    └─Empty: 2-1483                     [64, 64, 3, 3]            --
│    └─Empty: 2-1484                     [64]                      --
│    └─Empty: 2-1485                     [64]                      --
│    └─BatchNorm2d: 2-1486               [16, 64, 16, 16]          --
│    └─Scaler: 2-1487                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1488                      [16, 64, 16, 16]          --
│    └─Empty: 2-1489                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1490                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-112               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1491        --                        --
│    └─One: 2-1492                       [1]                       --
│    └─OutputScale: 2-1493               --                        --
│    └─Empty: 2-1494                     [64, 64, 1, 1]            --
│    └─Empty: 2-1495                     [64, 64, 1, 1]            --
│    └─Empty: 2-1496                     [64]                      --
│    └─Empty: 2-1497                     [64]                      --
│    └─BatchNorm2d: 2-1498               [16, 64, 16, 16]          --
│    └─Scaler: 2-1499                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1500                      [16, 64, 16, 16]          --
│    └─Empty: 2-1501                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1502                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-113        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1503                 [16, 64, 16, 16]          --
│    └─Empty: 2-1504                     [16, 64, 16, 16]          --
│    └─Empty: 2-1505                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1506        --                        --
│    └─One: 2-1507                       [1]                       --
│    └─OutputScale: 2-1508               --                        --
│    └─Empty: 2-1509                     [64, 64, 3, 3]            --
│    └─Empty: 2-1510                     [64, 64, 3, 3]            --
│    └─Empty: 2-1511                     [64]                      --
│    └─Empty: 2-1512                     [64]                      --
│    └─BatchNorm2d: 2-1513               [16, 64, 16, 16]          --
│    └─Scaler: 2-1514                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1515                      [16, 64, 16, 16]          --
│    └─Empty: 2-1516                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1517                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-114        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1518                 [16, 64, 8, 8]            --
│    └─Empty: 2-1519                     [16, 64, 8, 8]            --
│    └─Empty: 2-1520                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1521        --                        --
│    └─One: 2-1522                       [1]                       --
│    └─OutputScale: 2-1523               --                        --
│    └─Empty: 2-1524                     [64, 64, 3, 3]            --
│    └─Empty: 2-1525                     [64, 64, 3, 3]            --
│    └─Empty: 2-1526                     [64]                      --
│    └─Empty: 2-1527                     [64]                      --
│    └─BatchNorm2d: 2-1528               [16, 64, 8, 8]            --
│    └─Scaler: 2-1529                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1530                      [16, 64, 8, 8]            --
│    └─Empty: 2-1531                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1532                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-115               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1533        --                        --
│    └─One: 2-1534                       [1]                       --
│    └─OutputScale: 2-1535               --                        --
│    └─Empty: 2-1536                     [16, 64, 1, 1]            --
│    └─Empty: 2-1537                     [16, 64, 1, 1]            --
│    └─Empty: 2-1538                     [16]                      --
│    └─Empty: 2-1539                     [16]                      --
│    └─BatchNorm2d: 2-1540               [16, 16, 8, 8]            --
│    └─Scaler: 2-1541                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1542                      [16, 16, 8, 8]            --
│    └─Empty: 2-1543                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1544                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-116        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1545                 [16, 64, 8, 8]            --
│    └─Empty: 2-1546                     [16, 64, 8, 8]            --
│    └─Empty: 2-1547                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1548        --                        --
│    └─One: 2-1549                       [1]                       --
│    └─OutputScale: 2-1550               --                        --
│    └─Empty: 2-1551                     [16, 64, 3, 3]            --
│    └─Empty: 2-1552                     [16, 64, 3, 3]            --
│    └─Empty: 2-1553                     [16]                      --
│    └─Empty: 2-1554                     [16]                      --
│    └─BatchNorm2d: 2-1555               [16, 16, 8, 8]            --
│    └─Scaler: 2-1556                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1557                      [16, 16, 8, 8]            --
│    └─Empty: 2-1558                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1559                     [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-117               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1560        --                        --
│    └─One: 2-1561                       [1]                       --
│    └─OutputScale: 2-1562               --                        --
│    └─Empty: 2-1563                     [64, 48, 1, 1]            --
│    └─Empty: 2-1564                     [64, 48, 1, 1]            --
│    └─Empty: 2-1565                     [64]                      --
│    └─Empty: 2-1566                     [64]                      --
│    └─BatchNorm2d: 2-1567               [16, 64, 64, 64]          --
│    └─Scaler: 2-1568                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1569                      [16, 64, 64, 64]          --
│    └─Empty: 2-1570                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1571                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-118               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1572        --                        --
│    └─One: 2-1573                       [1]                       --
│    └─OutputScale: 2-1574               --                        --
│    └─Empty: 2-1575                     [64, 64, 3, 3]            --
│    └─Empty: 2-1576                     [64, 64, 3, 3]            --
│    └─Empty: 2-1577                     [64]                      --
│    └─Empty: 2-1578                     [64]                      --
│    └─BatchNorm2d: 2-1579               [16, 64, 64, 64]          --
│    └─Scaler: 2-1580                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1581                      [16, 64, 64, 64]          --
│    └─Empty: 2-1582                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1583                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-119        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1584                 [16, 64, 32, 32]          --
│    └─Empty: 2-1585                     [16, 64, 32, 32]          --
│    └─Empty: 2-1586                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1587        --                        --
│    └─One: 2-1588                       [1]                       --
│    └─OutputScale: 2-1589               --                        --
│    └─Empty: 2-1590                     [64, 64, 3, 3]            --
│    └─Empty: 2-1591                     [64, 64, 3, 3]            --
│    └─Empty: 2-1592                     [64]                      --
│    └─Empty: 2-1593                     [64]                      --
│    └─BatchNorm2d: 2-1594               [16, 64, 32, 32]          --
│    └─Scaler: 2-1595                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1596                      [16, 64, 32, 32]          --
│    └─Empty: 2-1597                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1598                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-120               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1599        --                        --
│    └─One: 2-1600                       [1]                       --
│    └─OutputScale: 2-1601               --                        --
│    └─Empty: 2-1602                     [64, 64, 1, 1]            --
│    └─Empty: 2-1603                     [64, 64, 1, 1]            --
│    └─Empty: 2-1604                     [64]                      --
│    └─Empty: 2-1605                     [64]                      --
│    └─BatchNorm2d: 2-1606               [16, 64, 32, 32]          --
│    └─Scaler: 2-1607                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1608                      [16, 64, 32, 32]          --
│    └─Empty: 2-1609                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1610                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-121        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1611                 [16, 64, 32, 32]          --
│    └─Empty: 2-1612                     [16, 64, 32, 32]          --
│    └─Empty: 2-1613                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1614        --                        --
│    └─One: 2-1615                       [1]                       --
│    └─OutputScale: 2-1616               --                        --
│    └─Empty: 2-1617                     [64, 64, 3, 3]            --
│    └─Empty: 2-1618                     [64, 64, 3, 3]            --
│    └─Empty: 2-1619                     [64]                      --
│    └─Empty: 2-1620                     [64]                      --
│    └─BatchNorm2d: 2-1621               [16, 64, 32, 32]          --
│    └─Scaler: 2-1622                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1623                      [16, 64, 32, 32]          --
│    └─Empty: 2-1624                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1625                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-122        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1626                 [16, 64, 16, 16]          --
│    └─Empty: 2-1627                     [16, 64, 16, 16]          --
│    └─Empty: 2-1628                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1629        --                        --
│    └─One: 2-1630                       [1]                       --
│    └─OutputScale: 2-1631               --                        --
│    └─Empty: 2-1632                     [64, 64, 3, 3]            --
│    └─Empty: 2-1633                     [64, 64, 3, 3]            --
│    └─Empty: 2-1634                     [64]                      --
│    └─Empty: 2-1635                     [64]                      --
│    └─BatchNorm2d: 2-1636               [16, 64, 16, 16]          --
│    └─Scaler: 2-1637                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1638                      [16, 64, 16, 16]          --
│    └─Empty: 2-1639                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1640                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-123               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1641        --                        --
│    └─One: 2-1642                       [1]                       --
│    └─OutputScale: 2-1643               --                        --
│    └─Empty: 2-1644                     [64, 64, 1, 1]            --
│    └─Empty: 2-1645                     [64, 64, 1, 1]            --
│    └─Empty: 2-1646                     [64]                      --
│    └─Empty: 2-1647                     [64]                      --
│    └─BatchNorm2d: 2-1648               [16, 64, 16, 16]          --
│    └─Scaler: 2-1649                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1650                      [16, 64, 16, 16]          --
│    └─Empty: 2-1651                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1652                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-124        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1653                 [16, 64, 16, 16]          --
│    └─Empty: 2-1654                     [16, 64, 16, 16]          --
│    └─Empty: 2-1655                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1656        --                        --
│    └─One: 2-1657                       [1]                       --
│    └─OutputScale: 2-1658               --                        --
│    └─Empty: 2-1659                     [64, 64, 3, 3]            --
│    └─Empty: 2-1660                     [64, 64, 3, 3]            --
│    └─Empty: 2-1661                     [64]                      --
│    └─Empty: 2-1662                     [64]                      --
│    └─BatchNorm2d: 2-1663               [16, 64, 16, 16]          --
│    └─Scaler: 2-1664                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1665                      [16, 64, 16, 16]          --
│    └─Empty: 2-1666                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1667                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-125        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1668                 [16, 64, 8, 8]            --
│    └─Empty: 2-1669                     [16, 64, 8, 8]            --
│    └─Empty: 2-1670                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1671        --                        --
│    └─One: 2-1672                       [1]                       --
│    └─OutputScale: 2-1673               --                        --
│    └─Empty: 2-1674                     [64, 64, 3, 3]            --
│    └─Empty: 2-1675                     [64, 64, 3, 3]            --
│    └─Empty: 2-1676                     [64]                      --
│    └─Empty: 2-1677                     [64]                      --
│    └─BatchNorm2d: 2-1678               [16, 64, 8, 8]            --
│    └─Scaler: 2-1679                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1680                      [16, 64, 8, 8]            --
│    └─Empty: 2-1681                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1682                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-126               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1683        --                        --
│    └─One: 2-1684                       [1]                       --
│    └─OutputScale: 2-1685               --                        --
│    └─Empty: 2-1686                     [16, 64, 1, 1]            --
│    └─Empty: 2-1687                     [16, 64, 1, 1]            --
│    └─Empty: 2-1688                     [16]                      --
│    └─Empty: 2-1689                     [16]                      --
│    └─BatchNorm2d: 2-1690               [16, 16, 8, 8]            --
│    └─Scaler: 2-1691                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1692                      [16, 16, 8, 8]            --
│    └─Empty: 2-1693                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1694                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-127        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1695                 [16, 64, 8, 8]            --
│    └─Empty: 2-1696                     [16, 64, 8, 8]            --
│    └─Empty: 2-1697                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1698        --                        --
│    └─One: 2-1699                       [1]                       --
│    └─OutputScale: 2-1700               --                        --
│    └─Empty: 2-1701                     [16, 64, 3, 3]            --
│    └─Empty: 2-1702                     [16, 64, 3, 3]            --
│    └─Empty: 2-1703                     [16]                      --
│    └─Empty: 2-1704                     [16]                      --
│    └─BatchNorm2d: 2-1705               [16, 16, 8, 8]            --
│    └─Scaler: 2-1706                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1707                      [16, 16, 8, 8]            --
│    └─Empty: 2-1708                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1709                     [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-128               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1710        --                        --
│    └─One: 2-1711                       [1]                       --
│    └─OutputScale: 2-1712               --                        --
│    └─Empty: 2-1713                     [64, 48, 1, 1]            --
│    └─Empty: 2-1714                     [64, 48, 1, 1]            --
│    └─Empty: 2-1715                     [64]                      --
│    └─Empty: 2-1716                     [64]                      --
│    └─BatchNorm2d: 2-1717               [16, 64, 64, 64]          --
│    └─Scaler: 2-1718                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1719                      [16, 64, 64, 64]          --
│    └─Empty: 2-1720                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1721                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-129               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1722        --                        --
│    └─One: 2-1723                       [1]                       --
│    └─OutputScale: 2-1724               --                        --
│    └─Empty: 2-1725                     [64, 64, 3, 3]            --
│    └─Empty: 2-1726                     [64, 64, 3, 3]            --
│    └─Empty: 2-1727                     [64]                      --
│    └─Empty: 2-1728                     [64]                      --
│    └─BatchNorm2d: 2-1729               [16, 64, 64, 64]          --
│    └─Scaler: 2-1730                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1731                      [16, 64, 64, 64]          --
│    └─Empty: 2-1732                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1733                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-130        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1734                 [16, 64, 32, 32]          --
│    └─Empty: 2-1735                     [16, 64, 32, 32]          --
│    └─Empty: 2-1736                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1737        --                        --
│    └─One: 2-1738                       [1]                       --
│    └─OutputScale: 2-1739               --                        --
│    └─Empty: 2-1740                     [64, 64, 3, 3]            --
│    └─Empty: 2-1741                     [64, 64, 3, 3]            --
│    └─Empty: 2-1742                     [64]                      --
│    └─Empty: 2-1743                     [64]                      --
│    └─BatchNorm2d: 2-1744               [16, 64, 32, 32]          --
│    └─Scaler: 2-1745                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1746                      [16, 64, 32, 32]          --
│    └─Empty: 2-1747                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1748                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-131               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1749        --                        --
│    └─One: 2-1750                       [1]                       --
│    └─OutputScale: 2-1751               --                        --
│    └─Empty: 2-1752                     [64, 64, 1, 1]            --
│    └─Empty: 2-1753                     [64, 64, 1, 1]            --
│    └─Empty: 2-1754                     [64]                      --
│    └─Empty: 2-1755                     [64]                      --
│    └─BatchNorm2d: 2-1756               [16, 64, 32, 32]          --
│    └─Scaler: 2-1757                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1758                      [16, 64, 32, 32]          --
│    └─Empty: 2-1759                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1760                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-132        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1761                 [16, 64, 32, 32]          --
│    └─Empty: 2-1762                     [16, 64, 32, 32]          --
│    └─Empty: 2-1763                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1764        --                        --
│    └─One: 2-1765                       [1]                       --
│    └─OutputScale: 2-1766               --                        --
│    └─Empty: 2-1767                     [64, 64, 3, 3]            --
│    └─Empty: 2-1768                     [64, 64, 3, 3]            --
│    └─Empty: 2-1769                     [64]                      --
│    └─Empty: 2-1770                     [64]                      --
│    └─BatchNorm2d: 2-1771               [16, 64, 32, 32]          --
│    └─Scaler: 2-1772                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1773                      [16, 64, 32, 32]          --
│    └─Empty: 2-1774                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1775                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-133        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1776                 [16, 64, 16, 16]          --
│    └─Empty: 2-1777                     [16, 64, 16, 16]          --
│    └─Empty: 2-1778                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1779        --                        --
│    └─One: 2-1780                       [1]                       --
│    └─OutputScale: 2-1781               --                        --
│    └─Empty: 2-1782                     [64, 64, 3, 3]            --
│    └─Empty: 2-1783                     [64, 64, 3, 3]            --
│    └─Empty: 2-1784                     [64]                      --
│    └─Empty: 2-1785                     [64]                      --
│    └─BatchNorm2d: 2-1786               [16, 64, 16, 16]          --
│    └─Scaler: 2-1787                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1788                      [16, 64, 16, 16]          --
│    └─Empty: 2-1789                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1790                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-134               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1791        --                        --
│    └─One: 2-1792                       [1]                       --
│    └─OutputScale: 2-1793               --                        --
│    └─Empty: 2-1794                     [64, 64, 1, 1]            --
│    └─Empty: 2-1795                     [64, 64, 1, 1]            --
│    └─Empty: 2-1796                     [64]                      --
│    └─Empty: 2-1797                     [64]                      --
│    └─BatchNorm2d: 2-1798               [16, 64, 16, 16]          --
│    └─Scaler: 2-1799                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1800                      [16, 64, 16, 16]          --
│    └─Empty: 2-1801                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1802                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-135        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1803                 [16, 64, 16, 16]          --
│    └─Empty: 2-1804                     [16, 64, 16, 16]          --
│    └─Empty: 2-1805                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1806        --                        --
│    └─One: 2-1807                       [1]                       --
│    └─OutputScale: 2-1808               --                        --
│    └─Empty: 2-1809                     [64, 64, 3, 3]            --
│    └─Empty: 2-1810                     [64, 64, 3, 3]            --
│    └─Empty: 2-1811                     [64]                      --
│    └─Empty: 2-1812                     [64]                      --
│    └─BatchNorm2d: 2-1813               [16, 64, 16, 16]          --
│    └─Scaler: 2-1814                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1815                      [16, 64, 16, 16]          --
│    └─Empty: 2-1816                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1817                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-136        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1818                 [16, 64, 8, 8]            --
│    └─Empty: 2-1819                     [16, 64, 8, 8]            --
│    └─Empty: 2-1820                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1821        --                        --
│    └─One: 2-1822                       [1]                       --
│    └─OutputScale: 2-1823               --                        --
│    └─Empty: 2-1824                     [64, 64, 3, 3]            --
│    └─Empty: 2-1825                     [64, 64, 3, 3]            --
│    └─Empty: 2-1826                     [64]                      --
│    └─Empty: 2-1827                     [64]                      --
│    └─BatchNorm2d: 2-1828               [16, 64, 8, 8]            --
│    └─Scaler: 2-1829                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1830                      [16, 64, 8, 8]            --
│    └─Empty: 2-1831                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1832                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-137               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1833        --                        --
│    └─One: 2-1834                       [1]                       --
│    └─OutputScale: 2-1835               --                        --
│    └─Empty: 2-1836                     [16, 64, 1, 1]            --
│    └─Empty: 2-1837                     [16, 64, 1, 1]            --
│    └─Empty: 2-1838                     [16]                      --
│    └─Empty: 2-1839                     [16]                      --
│    └─BatchNorm2d: 2-1840               [16, 16, 8, 8]            --
│    └─Scaler: 2-1841                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1842                      [16, 16, 8, 8]            --
│    └─Empty: 2-1843                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1844                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-138        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1845                 [16, 64, 8, 8]            --
│    └─Empty: 2-1846                     [16, 64, 8, 8]            --
│    └─Empty: 2-1847                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1848        --                        --
│    └─One: 2-1849                       [1]                       --
│    └─OutputScale: 2-1850               --                        --
│    └─Empty: 2-1851                     [16, 64, 3, 3]            --
│    └─Empty: 2-1852                     [16, 64, 3, 3]            --
│    └─Empty: 2-1853                     [16]                      --
│    └─Empty: 2-1854                     [16]                      --
│    └─BatchNorm2d: 2-1855               [16, 16, 8, 8]            --
│    └─Scaler: 2-1856                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1857                      [16, 16, 8, 8]            --
│    └─Empty: 2-1858                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1859                     [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-139               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1860        --                        --
│    └─One: 2-1861                       [1]                       --
│    └─OutputScale: 2-1862               --                        --
│    └─Empty: 2-1863                     [64, 48, 1, 1]            --
│    └─Empty: 2-1864                     [64, 48, 1, 1]            --
│    └─Empty: 2-1865                     [64]                      --
│    └─Empty: 2-1866                     [64]                      --
│    └─BatchNorm2d: 2-1867               [16, 64, 64, 64]          --
│    └─Scaler: 2-1868                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1869                      [16, 64, 64, 64]          --
│    └─Empty: 2-1870                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1871                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-140               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1872        --                        --
│    └─One: 2-1873                       [1]                       --
│    └─OutputScale: 2-1874               --                        --
│    └─Empty: 2-1875                     [64, 64, 3, 3]            --
│    └─Empty: 2-1876                     [64, 64, 3, 3]            --
│    └─Empty: 2-1877                     [64]                      --
│    └─Empty: 2-1878                     [64]                      --
│    └─BatchNorm2d: 2-1879               [16, 64, 64, 64]          --
│    └─Scaler: 2-1880                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1881                      [16, 64, 64, 64]          --
│    └─Empty: 2-1882                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1883                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-141        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1884                 [16, 64, 32, 32]          --
│    └─Empty: 2-1885                     [16, 64, 32, 32]          --
│    └─Empty: 2-1886                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1887        --                        --
│    └─One: 2-1888                       [1]                       --
│    └─OutputScale: 2-1889               --                        --
│    └─Empty: 2-1890                     [64, 64, 3, 3]            --
│    └─Empty: 2-1891                     [64, 64, 3, 3]            --
│    └─Empty: 2-1892                     [64]                      --
│    └─Empty: 2-1893                     [64]                      --
│    └─BatchNorm2d: 2-1894               [16, 64, 32, 32]          --
│    └─Scaler: 2-1895                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1896                      [16, 64, 32, 32]          --
│    └─Empty: 2-1897                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1898                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-142               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1899        --                        --
│    └─One: 2-1900                       [1]                       --
│    └─OutputScale: 2-1901               --                        --
│    └─Empty: 2-1902                     [64, 64, 1, 1]            --
│    └─Empty: 2-1903                     [64, 64, 1, 1]            --
│    └─Empty: 2-1904                     [64]                      --
│    └─Empty: 2-1905                     [64]                      --
│    └─BatchNorm2d: 2-1906               [16, 64, 32, 32]          --
│    └─Scaler: 2-1907                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1908                      [16, 64, 32, 32]          --
│    └─Empty: 2-1909                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1910                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-143        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1911                 [16, 64, 32, 32]          --
│    └─Empty: 2-1912                     [16, 64, 32, 32]          --
│    └─Empty: 2-1913                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1914        --                        --
│    └─One: 2-1915                       [1]                       --
│    └─OutputScale: 2-1916               --                        --
│    └─Empty: 2-1917                     [64, 64, 3, 3]            --
│    └─Empty: 2-1918                     [64, 64, 3, 3]            --
│    └─Empty: 2-1919                     [64]                      --
│    └─Empty: 2-1920                     [64]                      --
│    └─BatchNorm2d: 2-1921               [16, 64, 32, 32]          --
│    └─Scaler: 2-1922                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1923                      [16, 64, 32, 32]          --
│    └─Empty: 2-1924                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1925                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-144        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1926                 [16, 64, 16, 16]          --
│    └─Empty: 2-1927                     [16, 64, 16, 16]          --
│    └─Empty: 2-1928                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1929        --                        --
│    └─One: 2-1930                       [1]                       --
│    └─OutputScale: 2-1931               --                        --
│    └─Empty: 2-1932                     [64, 64, 3, 3]            --
│    └─Empty: 2-1933                     [64, 64, 3, 3]            --
│    └─Empty: 2-1934                     [64]                      --
│    └─Empty: 2-1935                     [64]                      --
│    └─BatchNorm2d: 2-1936               [16, 64, 16, 16]          --
│    └─Scaler: 2-1937                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1938                      [16, 64, 16, 16]          --
│    └─Empty: 2-1939                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1940                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-145               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1941        --                        --
│    └─One: 2-1942                       [1]                       --
│    └─OutputScale: 2-1943               --                        --
│    └─Empty: 2-1944                     [64, 64, 1, 1]            --
│    └─Empty: 2-1945                     [64, 64, 1, 1]            --
│    └─Empty: 2-1946                     [64]                      --
│    └─Empty: 2-1947                     [64]                      --
│    └─BatchNorm2d: 2-1948               [16, 64, 16, 16]          --
│    └─Scaler: 2-1949                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1950                      [16, 64, 16, 16]          --
│    └─Empty: 2-1951                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1952                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-146        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1953                 [16, 64, 16, 16]          --
│    └─Empty: 2-1954                     [16, 64, 16, 16]          --
│    └─Empty: 2-1955                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1956        --                        --
│    └─One: 2-1957                       [1]                       --
│    └─OutputScale: 2-1958               --                        --
│    └─Empty: 2-1959                     [64, 64, 3, 3]            --
│    └─Empty: 2-1960                     [64, 64, 3, 3]            --
│    └─Empty: 2-1961                     [64]                      --
│    └─Empty: 2-1962                     [64]                      --
│    └─BatchNorm2d: 2-1963               [16, 64, 16, 16]          --
│    └─Scaler: 2-1964                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1965                      [16, 64, 16, 16]          --
│    └─Empty: 2-1966                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1967                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-147        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1968                 [16, 64, 8, 8]            --
│    └─Empty: 2-1969                     [16, 64, 8, 8]            --
│    └─Empty: 2-1970                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1971        --                        --
│    └─One: 2-1972                       [1]                       --
│    └─OutputScale: 2-1973               --                        --
│    └─Empty: 2-1974                     [64, 64, 3, 3]            --
│    └─Empty: 2-1975                     [64, 64, 3, 3]            --
│    └─Empty: 2-1976                     [64]                      --
│    └─Empty: 2-1977                     [64]                      --
│    └─BatchNorm2d: 2-1978               [16, 64, 8, 8]            --
│    └─Scaler: 2-1979                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1980                      [16, 64, 8, 8]            --
│    └─Empty: 2-1981                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1982                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-148               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1983        --                        --
│    └─One: 2-1984                       [1]                       --
│    └─OutputScale: 2-1985               --                        --
│    └─Empty: 2-1986                     [16, 64, 1, 1]            --
│    └─Empty: 2-1987                     [16, 64, 1, 1]            --
│    └─Empty: 2-1988                     [16]                      --
│    └─Empty: 2-1989                     [16]                      --
│    └─BatchNorm2d: 2-1990               [16, 16, 8, 8]            --
│    └─Scaler: 2-1991                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1992                      [16, 16, 8, 8]            --
│    └─Empty: 2-1993                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1994                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-149        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1995                 [16, 64, 8, 8]            --
│    └─Empty: 2-1996                     [16, 64, 8, 8]            --
│    └─Empty: 2-1997                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1998        --                        --
│    └─One: 2-1999                       [1]                       --
│    └─OutputScale: 2-2000               --                        --
│    └─Empty: 2-2001                     [16, 64, 3, 3]            --
│    └─Empty: 2-2002                     [16, 64, 3, 3]            --
│    └─Empty: 2-2003                     [16]                      --
│    └─Empty: 2-2004                     [16]                      --
│    └─BatchNorm2d: 2-2005               [16, 16, 8, 8]            --
│    └─Scaler: 2-2006                    [16, 16, 8, 8]            --
│    └─ReLU: 2-2007                      [16, 16, 8, 8]            --
│    └─Empty: 2-2008                     [16, 16, 8, 8]            --
│    └─Clamp: 2-2009                     [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-150               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2010        --                        --
│    └─One: 2-2011                       [1]                       --
│    └─OutputScale: 2-2012               --                        --
│    └─Empty: 2-2013                     [64, 48, 1, 1]            --
│    └─Empty: 2-2014                     [64, 48, 1, 1]            --
│    └─Empty: 2-2015                     [64]                      --
│    └─Empty: 2-2016                     [64]                      --
│    └─BatchNorm2d: 2-2017               [16, 64, 64, 64]          --
│    └─Scaler: 2-2018                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2019                      [16, 64, 64, 64]          --
│    └─Empty: 2-2020                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2021                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-151               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2022        --                        --
│    └─One: 2-2023                       [1]                       --
│    └─OutputScale: 2-2024               --                        --
│    └─Empty: 2-2025                     [64, 64, 3, 3]            --
│    └─Empty: 2-2026                     [64, 64, 3, 3]            --
│    └─Empty: 2-2027                     [64]                      --
│    └─Empty: 2-2028                     [64]                      --
│    └─BatchNorm2d: 2-2029               [16, 64, 64, 64]          --
│    └─Scaler: 2-2030                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2031                      [16, 64, 64, 64]          --
│    └─Empty: 2-2032                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2033                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-152        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2034                 [16, 64, 32, 32]          --
│    └─Empty: 2-2035                     [16, 64, 32, 32]          --
│    └─Empty: 2-2036                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2037        --                        --
│    └─One: 2-2038                       [1]                       --
│    └─OutputScale: 2-2039               --                        --
│    └─Empty: 2-2040                     [64, 64, 3, 3]            --
│    └─Empty: 2-2041                     [64, 64, 3, 3]            --
│    └─Empty: 2-2042                     [64]                      --
│    └─Empty: 2-2043                     [64]                      --
│    └─BatchNorm2d: 2-2044               [16, 64, 32, 32]          --
│    └─Scaler: 2-2045                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2046                      [16, 64, 32, 32]          --
│    └─Empty: 2-2047                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2048                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-153               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2049        --                        --
│    └─One: 2-2050                       [1]                       --
│    └─OutputScale: 2-2051               --                        --
│    └─Empty: 2-2052                     [64, 64, 1, 1]            --
│    └─Empty: 2-2053                     [64, 64, 1, 1]            --
│    └─Empty: 2-2054                     [64]                      --
│    └─Empty: 2-2055                     [64]                      --
│    └─BatchNorm2d: 2-2056               [16, 64, 32, 32]          --
│    └─Scaler: 2-2057                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2058                      [16, 64, 32, 32]          --
│    └─Empty: 2-2059                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2060                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-154        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2061                 [16, 64, 32, 32]          --
│    └─Empty: 2-2062                     [16, 64, 32, 32]          --
│    └─Empty: 2-2063                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2064        --                        --
│    └─One: 2-2065                       [1]                       --
│    └─OutputScale: 2-2066               --                        --
│    └─Empty: 2-2067                     [64, 64, 3, 3]            --
│    └─Empty: 2-2068                     [64, 64, 3, 3]            --
│    └─Empty: 2-2069                     [64]                      --
│    └─Empty: 2-2070                     [64]                      --
│    └─BatchNorm2d: 2-2071               [16, 64, 32, 32]          --
│    └─Scaler: 2-2072                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2073                      [16, 64, 32, 32]          --
│    └─Empty: 2-2074                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2075                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-155        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2076                 [16, 64, 16, 16]          --
│    └─Empty: 2-2077                     [16, 64, 16, 16]          --
│    └─Empty: 2-2078                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2079        --                        --
│    └─One: 2-2080                       [1]                       --
│    └─OutputScale: 2-2081               --                        --
│    └─Empty: 2-2082                     [64, 64, 3, 3]            --
│    └─Empty: 2-2083                     [64, 64, 3, 3]            --
│    └─Empty: 2-2084                     [64]                      --
│    └─Empty: 2-2085                     [64]                      --
│    └─BatchNorm2d: 2-2086               [16, 64, 16, 16]          --
│    └─Scaler: 2-2087                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2088                      [16, 64, 16, 16]          --
│    └─Empty: 2-2089                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2090                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-156               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2091        --                        --
│    └─One: 2-2092                       [1]                       --
│    └─OutputScale: 2-2093               --                        --
│    └─Empty: 2-2094                     [64, 64, 1, 1]            --
│    └─Empty: 2-2095                     [64, 64, 1, 1]            --
│    └─Empty: 2-2096                     [64]                      --
│    └─Empty: 2-2097                     [64]                      --
│    └─BatchNorm2d: 2-2098               [16, 64, 16, 16]          --
│    └─Scaler: 2-2099                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2100                      [16, 64, 16, 16]          --
│    └─Empty: 2-2101                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2102                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-157        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2103                 [16, 64, 16, 16]          --
│    └─Empty: 2-2104                     [16, 64, 16, 16]          --
│    └─Empty: 2-2105                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2106        --                        --
│    └─One: 2-2107                       [1]                       --
│    └─OutputScale: 2-2108               --                        --
│    └─Empty: 2-2109                     [64, 64, 3, 3]            --
│    └─Empty: 2-2110                     [64, 64, 3, 3]            --
│    └─Empty: 2-2111                     [64]                      --
│    └─Empty: 2-2112                     [64]                      --
│    └─BatchNorm2d: 2-2113               [16, 64, 16, 16]          --
│    └─Scaler: 2-2114                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2115                      [16, 64, 16, 16]          --
│    └─Empty: 2-2116                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2117                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-158        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2118                 [16, 64, 8, 8]            --
│    └─Empty: 2-2119                     [16, 64, 8, 8]            --
│    └─Empty: 2-2120                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2121        --                        --
│    └─One: 2-2122                       [1]                       --
│    └─OutputScale: 2-2123               --                        --
│    └─Empty: 2-2124                     [64, 64, 3, 3]            --
│    └─Empty: 2-2125                     [64, 64, 3, 3]            --
│    └─Empty: 2-2126                     [64]                      --
│    └─Empty: 2-2127                     [64]                      --
│    └─BatchNorm2d: 2-2128               [16, 64, 8, 8]            --
│    └─Scaler: 2-2129                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2130                      [16, 64, 8, 8]            --
│    └─Empty: 2-2131                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2132                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-159               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2133        --                        --
│    └─One: 2-2134                       [1]                       --
│    └─OutputScale: 2-2135               --                        --
│    └─Empty: 2-2136                     [16, 64, 1, 1]            --
│    └─Empty: 2-2137                     [16, 64, 1, 1]            --
│    └─Empty: 2-2138                     [16]                      --
│    └─Empty: 2-2139                     [16]                      --
│    └─BatchNorm2d: 2-2140               [16, 16, 8, 8]            --
│    └─Scaler: 2-2141                    [16, 16, 8, 8]            --
│    └─ReLU: 2-2142                      [16, 16, 8, 8]            --
│    └─Empty: 2-2143                     [16, 16, 8, 8]            --
│    └─Clamp: 2-2144                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-160        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2145                 [16, 64, 8, 8]            --
│    └─Empty: 2-2146                     [16, 64, 8, 8]            --
│    └─Empty: 2-2147                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2148        --                        --
│    └─One: 2-2149                       [1]                       --
│    └─OutputScale: 2-2150               --                        --
│    └─Empty: 2-2151                     [16, 64, 3, 3]            --
│    └─Empty: 2-2152                     [16, 64, 3, 3]            --
│    └─Empty: 2-2153                     [16]                      --
│    └─Empty: 2-2154                     [16]                      --
│    └─BatchNorm2d: 2-2155               [16, 16, 8, 8]            --
│    └─Scaler: 2-2156                    [16, 16, 8, 8]            --
│    └─ReLU: 2-2157                      [16, 16, 8, 8]            --
│    └─Empty: 2-2158                     [16, 16, 8, 8]            --
│    └─Clamp: 2-2159                     [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-161               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2160        --                        --
│    └─One: 2-2161                       [1]                       --
│    └─OutputScale: 2-2162               --                        --
│    └─Empty: 2-2163                     [64, 48, 1, 1]            --
│    └─Empty: 2-2164                     [64, 48, 1, 1]            --
│    └─Empty: 2-2165                     [64]                      --
│    └─Empty: 2-2166                     [64]                      --
│    └─BatchNorm2d: 2-2167               [16, 64, 64, 64]          --
│    └─Scaler: 2-2168                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2169                      [16, 64, 64, 64]          --
│    └─Empty: 2-2170                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2171                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-162               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2172        --                        --
│    └─One: 2-2173                       [1]                       --
│    └─OutputScale: 2-2174               --                        --
│    └─Empty: 2-2175                     [64, 64, 3, 3]            --
│    └─Empty: 2-2176                     [64, 64, 3, 3]            --
│    └─Empty: 2-2177                     [64]                      --
│    └─Empty: 2-2178                     [64]                      --
│    └─BatchNorm2d: 2-2179               [16, 64, 64, 64]          --
│    └─Scaler: 2-2180                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2181                      [16, 64, 64, 64]          --
│    └─Empty: 2-2182                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2183                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-163        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2184                 [16, 64, 32, 32]          --
│    └─Empty: 2-2185                     [16, 64, 32, 32]          --
│    └─Empty: 2-2186                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2187        --                        --
│    └─One: 2-2188                       [1]                       --
│    └─OutputScale: 2-2189               --                        --
│    └─Empty: 2-2190                     [64, 64, 3, 3]            --
│    └─Empty: 2-2191                     [64, 64, 3, 3]            --
│    └─Empty: 2-2192                     [64]                      --
│    └─Empty: 2-2193                     [64]                      --
│    └─BatchNorm2d: 2-2194               [16, 64, 32, 32]          --
│    └─Scaler: 2-2195                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2196                      [16, 64, 32, 32]          --
│    └─Empty: 2-2197                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2198                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-164               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2199        --                        --
│    └─One: 2-2200                       [1]                       --
│    └─OutputScale: 2-2201               --                        --
│    └─Empty: 2-2202                     [64, 64, 1, 1]            --
│    └─Empty: 2-2203                     [64, 64, 1, 1]            --
│    └─Empty: 2-2204                     [64]                      --
│    └─Empty: 2-2205                     [64]                      --
│    └─BatchNorm2d: 2-2206               [16, 64, 32, 32]          --
│    └─Scaler: 2-2207                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2208                      [16, 64, 32, 32]          --
│    └─Empty: 2-2209                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2210                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-165        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2211                 [16, 64, 32, 32]          --
│    └─Empty: 2-2212                     [16, 64, 32, 32]          --
│    └─Empty: 2-2213                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2214        --                        --
│    └─One: 2-2215                       [1]                       --
│    └─OutputScale: 2-2216               --                        --
│    └─Empty: 2-2217                     [64, 64, 3, 3]            --
│    └─Empty: 2-2218                     [64, 64, 3, 3]            --
│    └─Empty: 2-2219                     [64]                      --
│    └─Empty: 2-2220                     [64]                      --
│    └─BatchNorm2d: 2-2221               [16, 64, 32, 32]          --
│    └─Scaler: 2-2222                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2223                      [16, 64, 32, 32]          --
│    └─Empty: 2-2224                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2225                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-166        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2226                 [16, 64, 16, 16]          --
│    └─Empty: 2-2227                     [16, 64, 16, 16]          --
│    └─Empty: 2-2228                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2229        --                        --
│    └─One: 2-2230                       [1]                       --
│    └─OutputScale: 2-2231               --                        --
│    └─Empty: 2-2232                     [64, 64, 3, 3]            --
│    └─Empty: 2-2233                     [64, 64, 3, 3]            --
│    └─Empty: 2-2234                     [64]                      --
│    └─Empty: 2-2235                     [64]                      --
│    └─BatchNorm2d: 2-2236               [16, 64, 16, 16]          --
│    └─Scaler: 2-2237                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2238                      [16, 64, 16, 16]          --
│    └─Empty: 2-2239                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2240                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-167               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2241        --                        --
│    └─One: 2-2242                       [1]                       --
│    └─OutputScale: 2-2243               --                        --
│    └─Empty: 2-2244                     [64, 64, 1, 1]            --
│    └─Empty: 2-2245                     [64, 64, 1, 1]            --
│    └─Empty: 2-2246                     [64]                      --
│    └─Empty: 2-2247                     [64]                      --
│    └─BatchNorm2d: 2-2248               [16, 64, 16, 16]          --
│    └─Scaler: 2-2249                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2250                      [16, 64, 16, 16]          --
│    └─Empty: 2-2251                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2252                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-168        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2253                 [16, 64, 16, 16]          --
│    └─Empty: 2-2254                     [16, 64, 16, 16]          --
│    └─Empty: 2-2255                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2256        --                        --
│    └─One: 2-2257                       [1]                       --
│    └─OutputScale: 2-2258               --                        --
│    └─Empty: 2-2259                     [64, 64, 3, 3]            --
│    └─Empty: 2-2260                     [64, 64, 3, 3]            --
│    └─Empty: 2-2261                     [64]                      --
│    └─Empty: 2-2262                     [64]                      --
│    └─BatchNorm2d: 2-2263               [16, 64, 16, 16]          --
│    └─Scaler: 2-2264                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2265                      [16, 64, 16, 16]          --
│    └─Empty: 2-2266                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2267                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-169        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2268                 [16, 64, 8, 8]            --
│    └─Empty: 2-2269                     [16, 64, 8, 8]            --
│    └─Empty: 2-2270                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2271        --                        --
│    └─One: 2-2272                       [1]                       --
│    └─OutputScale: 2-2273               --                        --
│    └─Empty: 2-2274                     [64, 64, 3, 3]            --
│    └─Empty: 2-2275                     [64, 64, 3, 3]            --
│    └─Empty: 2-2276                     [64]                      --
│    └─Empty: 2-2277                     [64]                      --
│    └─BatchNorm2d: 2-2278               [16, 64, 8, 8]            --
│    └─Scaler: 2-2279                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2280                      [16, 64, 8, 8]            --
│    └─Empty: 2-2281                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2282                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-170               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2283        --                        --
│    └─One: 2-2284                       [1]                       --
│    └─OutputScale: 2-2285               --                        --
│    └─Empty: 2-2286                     [16, 64, 1, 1]            --
│    └─Empty: 2-2287                     [16, 64, 1, 1]            --
│    └─Empty: 2-2288                     [16]                      --
│    └─Empty: 2-2289                     [16]                      --
│    └─BatchNorm2d: 2-2290               [16, 16, 8, 8]            --
│    └─Scaler: 2-2291                    [16, 16, 8, 8]            --
│    └─ReLU: 2-2292                      [16, 16, 8, 8]            --
│    └─Empty: 2-2293                     [16, 16, 8, 8]            --
│    └─Clamp: 2-2294                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-171        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2295                 [16, 64, 8, 8]            --
│    └─Empty: 2-2296                     [16, 64, 8, 8]            --
│    └─Empty: 2-2297                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2298        --                        --
│    └─One: 2-2299                       [1]                       --
│    └─OutputScale: 2-2300               --                        --
│    └─Empty: 2-2301                     [16, 64, 3, 3]            --
│    └─Empty: 2-2302                     [16, 64, 3, 3]            --
│    └─Empty: 2-2303                     [16]                      --
│    └─Empty: 2-2304                     [16]                      --
│    └─BatchNorm2d: 2-2305               [16, 16, 8, 8]            --
│    └─Scaler: 2-2306                    [16, 16, 8, 8]            --
│    └─ReLU: 2-2307                      [16, 16, 8, 8]            --
│    └─Empty: 2-2308                     [16, 16, 8, 8]            --
│    └─Clamp: 2-2309                     [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-172               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2310        --                        --
│    └─One: 2-2311                       [1]                       --
│    └─OutputScale: 2-2312               --                        --
│    └─Empty: 2-2313                     [64, 48, 1, 1]            --
│    └─Empty: 2-2314                     [64, 48, 1, 1]            --
│    └─Empty: 2-2315                     [64]                      --
│    └─Empty: 2-2316                     [64]                      --
│    └─BatchNorm2d: 2-2317               [16, 64, 64, 64]          --
│    └─Scaler: 2-2318                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2319                      [16, 64, 64, 64]          --
│    └─Empty: 2-2320                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2321                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-173               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2322        --                        --
│    └─One: 2-2323                       [1]                       --
│    └─OutputScale: 2-2324               --                        --
│    └─Empty: 2-2325                     [64, 64, 3, 3]            --
│    └─Empty: 2-2326                     [64, 64, 3, 3]            --
│    └─Empty: 2-2327                     [64]                      --
│    └─Empty: 2-2328                     [64]                      --
│    └─BatchNorm2d: 2-2329               [16, 64, 64, 64]          --
│    └─Scaler: 2-2330                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2331                      [16, 64, 64, 64]          --
│    └─Empty: 2-2332                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2333                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2334                 [16, 64, 32, 32]          --
│    └─Empty: 2-2335                     [16, 64, 32, 32]          --
│    └─Empty: 2-2336                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2337        --                        --
│    └─One: 2-2338                       [1]                       --
│    └─OutputScale: 2-2339               --                        --
│    └─Empty: 2-2340                     [64, 64, 3, 3]            --
│    └─Empty: 2-2341                     [64, 64, 3, 3]            --
│    └─Empty: 2-2342                     [64]                      --
│    └─Empty: 2-2343                     [64]                      --
│    └─BatchNorm2d: 2-2344               [16, 64, 32, 32]          --
│    └─Scaler: 2-2345                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2346                      [16, 64, 32, 32]          --
│    └─Empty: 2-2347                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2348                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-175               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2349        --                        --
│    └─One: 2-2350                       [1]                       --
│    └─OutputScale: 2-2351               --                        --
│    └─Empty: 2-2352                     [64, 64, 1, 1]            --
│    └─Empty: 2-2353                     [64, 64, 1, 1]            --
│    └─Empty: 2-2354                     [64]                      --
│    └─Empty: 2-2355                     [64]                      --
│    └─BatchNorm2d: 2-2356               [16, 64, 32, 32]          --
│    └─Scaler: 2-2357                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2358                      [16, 64, 32, 32]          --
│    └─Empty: 2-2359                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2360                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2361                 [16, 64, 32, 32]          --
│    └─Empty: 2-2362                     [16, 64, 32, 32]          --
│    └─Empty: 2-2363                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2364        --                        --
│    └─One: 2-2365                       [1]                       --
│    └─OutputScale: 2-2366               --                        --
│    └─Empty: 2-2367                     [64, 64, 3, 3]            --
│    └─Empty: 2-2368                     [64, 64, 3, 3]            --
│    └─Empty: 2-2369                     [64]                      --
│    └─Empty: 2-2370                     [64]                      --
│    └─BatchNorm2d: 2-2371               [16, 64, 32, 32]          --
│    └─Scaler: 2-2372                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2373                      [16, 64, 32, 32]          --
│    └─Empty: 2-2374                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2375                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-177        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2376                 [16, 64, 16, 16]          --
│    └─Empty: 2-2377                     [16, 64, 16, 16]          --
│    └─Empty: 2-2378                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2379        --                        --
│    └─One: 2-2380                       [1]                       --
│    └─OutputScale: 2-2381               --                        --
│    └─Empty: 2-2382                     [64, 64, 3, 3]            --
│    └─Empty: 2-2383                     [64, 64, 3, 3]            --
│    └─Empty: 2-2384                     [64]                      --
│    └─Empty: 2-2385                     [64]                      --
│    └─BatchNorm2d: 2-2386               [16, 64, 16, 16]          --
│    └─Scaler: 2-2387                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2388                      [16, 64, 16, 16]          --
│    └─Empty: 2-2389                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2390                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-178               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2391        --                        --
│    └─One: 2-2392                       [1]                       --
│    └─OutputScale: 2-2393               --                        --
│    └─Empty: 2-2394                     [64, 64, 1, 1]            --
│    └─Empty: 2-2395                     [64, 64, 1, 1]            --
│    └─Empty: 2-2396                     [64]                      --
│    └─Empty: 2-2397                     [64]                      --
│    └─BatchNorm2d: 2-2398               [16, 64, 16, 16]          --
│    └─Scaler: 2-2399                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2400                      [16, 64, 16, 16]          --
│    └─Empty: 2-2401                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2402                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-179        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2403                 [16, 64, 16, 16]          --
│    └─Empty: 2-2404                     [16, 64, 16, 16]          --
│    └─Empty: 2-2405                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2406        --                        --
│    └─One: 2-2407                       [1]                       --
│    └─OutputScale: 2-2408               --                        --
│    └─Empty: 2-2409                     [64, 64, 3, 3]            --
│    └─Empty: 2-2410                     [64, 64, 3, 3]            --
│    └─Empty: 2-2411                     [64]                      --
│    └─Empty: 2-2412                     [64]                      --
│    └─BatchNorm2d: 2-2413               [16, 64, 16, 16]          --
│    └─Scaler: 2-2414                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2415                      [16, 64, 16, 16]          --
│    └─Empty: 2-2416                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2417                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2418                 [16, 64, 8, 8]            --
│    └─Empty: 2-2419                     [16, 64, 8, 8]            --
│    └─Empty: 2-2420                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2421        --                        --
│    └─One: 2-2422                       [1]                       --
│    └─OutputScale: 2-2423               --                        --
│    └─Empty: 2-2424                     [64, 64, 3, 3]            --
│    └─Empty: 2-2425                     [64, 64, 3, 3]            --
│    └─Empty: 2-2426                     [64]                      --
│    └─Empty: 2-2427                     [64]                      --
│    └─BatchNorm2d: 2-2428               [16, 64, 8, 8]            --
│    └─Scaler: 2-2429                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2430                      [16, 64, 8, 8]            --
│    └─Empty: 2-2431                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2432                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-181               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2433        --                        --
│    └─One: 2-2434                       [1]                       --
│    └─OutputScale: 2-2435               --                        --
│    └─Empty: 2-2436                     [16, 64, 1, 1]            --
│    └─Empty: 2-2437                     [16, 64, 1, 1]            --
│    └─Empty: 2-2438                     [16]                      --
│    └─Empty: 2-2439                     [16]                      --
│    └─BatchNorm2d: 2-2440               [16, 16, 8, 8]            --
│    └─Scaler: 2-2441                    [16, 16, 8, 8]            --
│    └─ReLU: 2-2442                      [16, 16, 8, 8]            --
│    └─Empty: 2-2443                     [16, 16, 8, 8]            --
│    └─Clamp: 2-2444                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-182        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2445                 [16, 64, 8, 8]            --
│    └─Empty: 2-2446                     [16, 64, 8, 8]            --
│    └─Empty: 2-2447                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2448        --                        --
│    └─One: 2-2449                       [1]                       --
│    └─OutputScale: 2-2450               --                        --
│    └─Empty: 2-2451                     [16, 64, 3, 3]            --
│    └─Empty: 2-2452                     [16, 64, 3, 3]            --
│    └─Empty: 2-2453                     [16]                      --
│    └─Empty: 2-2454                     [16]                      --
│    └─BatchNorm2d: 2-2455               [16, 16, 8, 8]            --
│    └─Scaler: 2-2456                    [16, 16, 8, 8]            --
│    └─ReLU: 2-2457                      [16, 16, 8, 8]            --
│    └─Empty: 2-2458                     [16, 16, 8, 8]            --
│    └─Clamp: 2-2459                     [16, 16, 8, 8]            --
├─Linear: 1-183                          [16, 4]                   4,102
│    └─OutputShiftSqueeze: 2-2460        --                        --
│    └─One: 2-2461                       [1]                       --
│    └─OutputScale: 2-2462               --                        --
│    └─Empty: 2-2463                     [4, 1024]                 --
│    └─Empty: 2-2464                     [4, 1024]                 --
│    └─Empty: 2-2465                     [16, 4]                   --
│    └─Empty: 2-2466                     [16, 4]                   --
│    └─Clamp: 2-2467                     [16, 4]                   --
├─Softmax: 1-184                         [16, 4]                   --
├─Linear: 1-185                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2468        --                        --
│    └─One: 2-2469                       [1]                       --
│    └─OutputScale: 2-2470               --                        --
│    └─Empty: 2-2471                     [4, 1024]                 --
│    └─Empty: 2-2472                     [4, 1024]                 --
│    └─Empty: 2-2473                     [16, 4]                   --
│    └─Empty: 2-2474                     [16, 4]                   --
│    └─Clamp: 2-2475                     [16, 4]                   --
├─Softmax: 1-186                         [16, 4]                   --
├─Linear: 1-187                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2476        --                        --
│    └─One: 2-2477                       [1]                       --
│    └─OutputScale: 2-2478               --                        --
│    └─Empty: 2-2479                     [4, 1024]                 --
│    └─Empty: 2-2480                     [4, 1024]                 --
│    └─Empty: 2-2481                     [16, 4]                   --
│    └─Empty: 2-2482                     [16, 4]                   --
│    └─Clamp: 2-2483                     [16, 4]                   --
├─Softmax: 1-188                         [16, 4]                   --
├─Linear: 1-189                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2484        --                        --
│    └─One: 2-2485                       [1]                       --
│    └─OutputScale: 2-2486               --                        --
│    └─Empty: 2-2487                     [4, 1024]                 --
│    └─Empty: 2-2488                     [4, 1024]                 --
│    └─Empty: 2-2489                     [16, 4]                   --
│    └─Empty: 2-2490                     [16, 4]                   --
│    └─Clamp: 2-2491                     [16, 4]                   --
├─Softmax: 1-190                         [16, 4]                   --
├─Linear: 1-191                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2492        --                        --
│    └─One: 2-2493                       [1]                       --
│    └─OutputScale: 2-2494               --                        --
│    └─Empty: 2-2495                     [4, 1024]                 --
│    └─Empty: 2-2496                     [4, 1024]                 --
│    └─Empty: 2-2497                     [16, 4]                   --
│    └─Empty: 2-2498                     [16, 4]                   --
│    └─Clamp: 2-2499                     [16, 4]                   --
├─Softmax: 1-192                         [16, 4]                   --
├─Linear: 1-193                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2500        --                        --
│    └─One: 2-2501                       [1]                       --
│    └─OutputScale: 2-2502               --                        --
│    └─Empty: 2-2503                     [4, 1024]                 --
│    └─Empty: 2-2504                     [4, 1024]                 --
│    └─Empty: 2-2505                     [16, 4]                   --
│    └─Empty: 2-2506                     [16, 4]                   --
│    └─Clamp: 2-2507                     [16, 4]                   --
├─Softmax: 1-194                         [16, 4]                   --
├─Linear: 1-195                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2508        --                        --
│    └─One: 2-2509                       [1]                       --
│    └─OutputScale: 2-2510               --                        --
│    └─Empty: 2-2511                     [4, 1024]                 --
│    └─Empty: 2-2512                     [4, 1024]                 --
│    └─Empty: 2-2513                     [16, 4]                   --
│    └─Empty: 2-2514                     [16, 4]                   --
│    └─Clamp: 2-2515                     [16, 4]                   --
├─Softmax: 1-196                         [16, 4]                   --
├─Linear: 1-197                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2516        --                        --
│    └─One: 2-2517                       [1]                       --
│    └─OutputScale: 2-2518               --                        --
│    └─Empty: 2-2519                     [4, 1024]                 --
│    └─Empty: 2-2520                     [4, 1024]                 --
│    └─Empty: 2-2521                     [16, 4]                   --
│    └─Empty: 2-2522                     [16, 4]                   --
│    └─Clamp: 2-2523                     [16, 4]                   --
├─Softmax: 1-198                         [16, 4]                   --
├─Linear: 1-199                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2524        --                        --
│    └─One: 2-2525                       [1]                       --
│    └─OutputScale: 2-2526               --                        --
│    └─Empty: 2-2527                     [4, 1024]                 --
│    └─Empty: 2-2528                     [4, 1024]                 --
│    └─Empty: 2-2529                     [16, 4]                   --
│    └─Empty: 2-2530                     [16, 4]                   --
│    └─Clamp: 2-2531                     [16, 4]                   --
├─Softmax: 1-200                         [16, 4]                   --
├─Linear: 1-201                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2532        --                        --
│    └─One: 2-2533                       [1]                       --
│    └─OutputScale: 2-2534               --                        --
│    └─Empty: 2-2535                     [4, 1024]                 --
│    └─Empty: 2-2536                     [4, 1024]                 --
│    └─Empty: 2-2537                     [16, 4]                   --
│    └─Empty: 2-2538                     [16, 4]                   --
│    └─Clamp: 2-2539                     [16, 4]                   --
├─Softmax: 1-202                         [16, 4]                   --
├─Linear: 1-203                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2540        --                        --
│    └─One: 2-2541                       [1]                       --
│    └─OutputScale: 2-2542               --                        --
│    └─Empty: 2-2543                     [4, 1024]                 --
│    └─Empty: 2-2544                     [4, 1024]                 --
│    └─Empty: 2-2545                     [16, 4]                   --
│    └─Empty: 2-2546                     [16, 4]                   --
│    └─Clamp: 2-2547                     [16, 4]                   --
├─Softmax: 1-204                         [16, 4]                   --
├─Linear: 1-205                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2548        --                        --
│    └─One: 2-2549                       [1]                       --
│    └─OutputScale: 2-2550               --                        --
│    └─Empty: 2-2551                     [4, 1024]                 --
│    └─Empty: 2-2552                     [4, 1024]                 --
│    └─Empty: 2-2553                     [16, 4]                   --
│    └─Empty: 2-2554                     [16, 4]                   --
│    └─Clamp: 2-2555                     [16, 4]                   --
├─Softmax: 1-206                         [16, 4]                   --
├─Linear: 1-207                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2556        --                        --
│    └─One: 2-2557                       [1]                       --
│    └─OutputScale: 2-2558               --                        --
│    └─Empty: 2-2559                     [4, 1024]                 --
│    └─Empty: 2-2560                     [4, 1024]                 --
│    └─Empty: 2-2561                     [16, 4]                   --
│    └─Empty: 2-2562                     [16, 4]                   --
│    └─Clamp: 2-2563                     [16, 4]                   --
├─Softmax: 1-208                         [16, 4]                   --
├─Linear: 1-209                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2564        --                        --
│    └─One: 2-2565                       [1]                       --
│    └─OutputScale: 2-2566               --                        --
│    └─Empty: 2-2567                     [4, 1024]                 --
│    └─Empty: 2-2568                     [4, 1024]                 --
│    └─Empty: 2-2569                     [16, 4]                   --
│    └─Empty: 2-2570                     [16, 4]                   --
│    └─Clamp: 2-2571                     [16, 4]                   --
├─Softmax: 1-210                         [16, 4]                   --
├─Linear: 1-211                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2572        --                        --
│    └─One: 2-2573                       [1]                       --
│    └─OutputScale: 2-2574               --                        --
│    └─Empty: 2-2575                     [4, 1024]                 --
│    └─Empty: 2-2576                     [4, 1024]                 --
│    └─Empty: 2-2577                     [16, 4]                   --
│    └─Empty: 2-2578                     [16, 4]                   --
│    └─Clamp: 2-2579                     [16, 4]                   --
├─Softmax: 1-212                         [16, 4]                   --
├─Linear: 1-213                          [16, 4]                   (recursive)
│    └─OutputShiftSqueeze: 2-2580        --                        --
│    └─One: 2-2581                       [1]                       --
│    └─OutputScale: 2-2582               --                        --
│    └─Empty: 2-2583                     [4, 1024]                 --
│    └─Empty: 2-2584                     [4, 1024]                 --
│    └─Empty: 2-2585                     [16, 4]                   --
│    └─Empty: 2-2586                     [16, 4]                   --
│    └─Clamp: 2-2587                     [16, 4]                   --
├─Softmax: 1-214                         [16, 4]                   --
==========================================================================================
Total params: 247,464
Trainable params: 247,392
Non-trainable params: 72
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 0.00
Params size (MB): 0.97
Estimated Total Size (MB): 202.30
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.338 | Acc: 35.375% | Wgt Acc: 34.970%
	I - Batch: 100 | Loss: 1.287 | Acc: 42.438% | Wgt Acc: 42.107%
	I - Batch: 150 | Loss: 1.259 | Acc: 46.500% | Wgt Acc: 46.105%
	I - Batch: 200 | Loss: 1.241 | Acc: 49.000% | Wgt Acc: 48.682%
	I - Batch: 250 | Loss: 1.228 | Acc: 50.450% | Wgt Acc: 50.130%
	I - Batch: 300 | Loss: 1.220 | Acc: 51.458% | Wgt Acc: 51.185%
	I - Batch: 350 | Loss: 1.211 | Acc: 52.518% | Wgt Acc: 52.255%
	I - Batch: 400 | Loss: 1.201 | Acc: 53.688% | Wgt Acc: 53.443%
	I - Batch: 450 | Loss: 1.195 | Acc: 54.444% | Wgt Acc: 54.162%
I - num batch: 478
I - Train -- Loss: 1.190 | Acc: 54.980% | Wgt Acc: 54.712% | LR: 1.000000e-03 | Dur: 346.47s
I - Confusion Matrix: [row->prediction - col->label]
[[1293.  103.  166.  484.]
 [ 148.  971.  623.  173.]
 [ 192.  519. 1158.  178.]
 [ 458.  141.  255.  779.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.190 | Acc: 54.625% | Wgt Acc: 55.081%
I - num batch: 62
I - Val -- Loss: 1.199 | Acc: 53.721% | Wgt Acc: 54.235% | Dur: 30.50s
I - Confusion Matrix: [row->prediction - col->label]
[[180.  40.  21.  74.]
 [ 19. 149. 108.  39.]
 [  3.  32.  60.   7.]
 [ 62.  13.  36. 138.]]

I - Local maximum validation set accuracy:  53.72

I - Validation set results: 
[14-1-2-0.55][14-1-1-0.99][14-1-1-0.89][50-3-0-0.74][50-3-0-0.76][50-3-0-0.77][124-2-1-0.82][124-2-1-0.61][124-2-2-0.86][127-0-0-1.00]
[127-0-0-1.00][127-0-0-1.00][443-2-2-0.87][443-2-2-0.91][443-2-2-0.95][567-0-0-0.96][567-0-0-0.99][567-0-0-0.99][573-1-3-0.51][573-1-1-0.84]
[573-1-1-0.87][615-0-0-0.85][615-0-0-0.66][615-0-0-0.81][695-1-2-0.37][695-1-2-0.40][695-1-0-0.43][722-3-0-0.99][722-3-0-0.72][722-3-0-0.83]
[826-0-0-0.56][826-0-0-0.83][826-0-0-0.80][878-0-0-0.92][878-0-0-0.98][878-0-0-0.99][1103-0-0-0.80][1103-0-0-0.59][1103-0-0-0.62][1212-3-3-0.54]
[1212-3-3-0.57][1212-3-3-0.52][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-3-0.51][2181-2-3-0.49][2181-2-3-0.53][2476-2-1-0.44][2476-2-1-0.52]
[2476-2-1-0.68][2721-2-1-0.85][2721-2-1-0.79][2721-2-1-0.80][2818-1-2-0.47][2818-1-0-0.55][2818-1-0-0.78][2886-2-1-0.96][2886-2-1-0.93][2886-2-1-0.91]
[3231-2-1-0.98][3231-2-1-0.98][3231-2-1-0.98][3333-2-1-0.60][3333-2-1-0.66][3333-2-1-0.62][3482-2-2-0.56][3482-2-2-0.55][3482-2-2-0.62][3536-3-3-0.81]
[3536-3-3-0.90][3536-3-3-0.94][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-1-0.80][3909-0-1-0.79][3909-0-0-0.55][4035-0-3-0.90][4035-0-3-0.88]
[4035-0-3-0.90][4140-0-0-0.99][4140-0-0-0.89][4140-0-0-0.97][4214-1-3-0.50][4214-1-1-0.57][4214-1-1-0.98][4346-1-0-0.79][4346-1-0-0.96][4346-1-0-0.96]
[4581-2-2-0.96][4581-2-2-0.94][4581-2-2-0.77][4708-3-1-0.77][4708-3-1-0.76][4708-3-1-0.86][4838-3-0-0.88][4838-3-0-0.94][4838-3-0-0.88][4845-1-1-0.61]
[4845-1-1-0.50][4845-1-1-0.48][4868-0-0-0.97][4868-0-0-0.99][4868-0-0-0.97][4939-0-2-0.37][4939-0-2-0.50][4939-0-2-0.75][4984-2-3-0.82][4984-2-3-0.79]
[4984-2-3-0.73][5078-1-1-0.56][5078-1-1-0.39][5078-1-2-0.51][5396-0-0-1.00][5396-0-0-0.99][5396-0-0-0.99][5479-1-1-0.99][5479-1-1-1.00][5479-1-1-0.99]
[5717-0-0-0.60][5717-0-0-0.82][5717-0-1-0.40][5843-1-1-0.99][5843-1-1-0.94][5843-1-1-0.98][5949-3-0-0.51][5949-3-0-0.82][5949-3-0-0.67][5987-2-1-0.76]
[5987-2-1-0.76][5987-2-1-0.89][6014-3-1-0.74][6014-3-1-0.60][6014-3-1-0.58][6033-3-0-0.98][6033-3-0-0.91][6033-3-0-0.99][6313-0-0-0.69][6313-0-0-0.59]
[6313-0-3-0.78][6421-3-3-0.94][6421-3-3-0.90][6421-3-3-0.86][6500-1-1-0.39][6500-1-1-0.41][6500-1-1-0.47][6583-3-3-0.98][6583-3-3-0.97][6583-3-3-0.98]
[6683-3-3-0.71][6683-3-3-0.81][6683-3-3-0.91][6825-2-3-0.50][6825-2-3-0.44][6825-2-3-0.41][6998-3-3-0.57][6998-3-3-0.66][6998-3-3-0.44][7049-3-1-0.67]
[7049-3-1-0.80][7049-3-1-0.59][7517-1-1-1.00][7517-1-1-1.00][7517-1-1-1.00][7521-1-1-0.97][7521-1-1-0.95][7521-1-1-0.98][7528-1-3-0.76][7528-1-2-0.53]
[7528-1-2-0.71][7949-1-2-0.80][7949-1-2-0.84][7949-1-2-0.71][8135-1-0-0.98][8135-1-0-0.94][8135-1-0-0.97][8185-3-0-0.68][8185-3-0-0.81][8185-3-0-0.60]
[8269-3-0-0.44][8269-3-1-0.38][8269-3-1-0.82][8273-3-3-0.94][8273-3-3-0.93][8273-3-3-0.97][8543-3-0-0.87][8543-3-0-1.00][8543-3-0-1.00][8666-1-3-0.52]
[8666-1-1-0.67][8666-1-1-0.73][8672-0-0-1.00][8672-0-0-0.99][8672-0-0-0.79][8903-1-3-0.76][8903-1-3-0.63][8903-1-3-0.56][9001-2-0-0.57][9001-2-1-0.99]
[9001-2-1-0.99][9036-2-2-0.83][9036-2-2-0.58][9036-2-2-0.81][9281-3-3-0.55][9281-3-3-0.41][9281-3-3-0.52][9300-2-1-0.87][9300-2-1-0.82][9300-2-1-0.69]
[9571-0-3-0.74][9571-0-3-0.53][9571-0-0-0.61][9617-1-1-0.82][9617-1-1-0.72][9617-1-1-0.49][9644-2-2-0.79][9644-2-2-0.86][9644-2-2-0.77][9705-2-1-0.80]
[9705-2-1-0.74][9705-2-1-0.78][9801-0-3-0.76][9801-0-3-0.97][9801-0-3-0.96][9803-3-0-0.78][9803-3-3-0.66][9803-3-3-0.69][9865-3-3-0.96][9865-3-3-0.65]
[9865-3-0-0.78][9896-2-1-0.75][9896-2-1-0.66][9896-2-2-0.73][10314-1-1-0.84][10314-1-1-0.83][10314-1-1-0.51][10337-3-3-0.98][10337-3-3-0.92][10337-3-3-0.89]
[10403-0-0-0.70][10403-0-0-0.66][10403-0-0-0.73][10653-2-1-0.64][10653-2-1-0.80][10653-2-1-0.55][10704-2-1-0.50][10704-2-3-0.58][10704-2-3-0.61][10719-1-1-0.96]
[10719-1-1-0.99][10719-1-1-0.99][10727-1-1-0.96][10727-1-1-0.93][10727-1-1-0.93][10836-0-0-0.74][10836-0-0-0.79][10836-0-0-0.84][10969-2-3-0.69][10969-2-3-0.81]
[10969-2-3-0.68][11042-0-0-0.96][11042-0-0-0.92][11042-0-0-0.99][11088-1-1-0.51][11088-1-1-0.67][11088-1-2-0.53][11322-0-0-0.90][11322-0-0-0.94][11322-0-0-0.97]
[11398-2-2-1.00][11398-2-2-0.90][11398-2-2-0.50][11499-0-3-0.61][11499-0-3-0.51][11499-0-3-0.55][11502-3-3-0.61][11502-3-3-0.84][11502-3-3-0.69][11512-3-1-0.76]
[11512-3-1-0.65][11512-3-1-0.79][11608-1-1-1.00][11608-1-1-1.00][11608-1-1-0.63][11610-0-3-0.69][11610-0-3-0.79][11610-0-1-0.48][11692-0-0-0.72][11692-0-0-0.69]
[11692-0-0-0.77][11905-0-0-1.00][11905-0-0-0.98][11905-0-0-0.96][11993-1-1-0.98][11993-1-1-0.98][11993-1-1-0.98][12002-2-0-0.82][12002-2-1-0.62][12002-2-3-0.75]
[12052-0-0-0.44][12052-0-0-0.86][12052-0-3-0.67][12201-0-0-0.53][12201-0-3-0.57][12201-0-0-0.50][12235-2-1-0.63][12235-2-1-0.82][12235-2-1-0.57][12320-1-0-0.92]
[12320-1-0-0.89][12320-1-0-0.96][12377-2-1-0.37][12377-2-1-0.52][12377-2-2-0.49][12398-2-0-0.59][12398-2-0-0.64][12398-2-3-0.49][12503-1-2-0.51][12503-1-1-0.44]
[12503-1-1-0.79][12617-0-1-0.88][12617-0-1-0.87][12617-0-1-0.83][12685-3-1-0.51][12685-3-3-0.69][12685-3-3-0.81][12738-2-1-0.98][12738-2-1-0.81][12738-2-1-0.65]
[12742-2-2-0.85][12742-2-2-0.82][12742-2-2-0.74][12823-0-3-0.93][12823-0-3-0.96][12823-0-3-0.89][13110-1-0-0.48][13110-1-1-0.41][13110-1-1-0.76][13240-3-3-0.62]
[13240-3-3-0.48][13240-3-3-0.57][13253-1-1-0.84][13253-1-1-0.80][13253-1-1-0.90][13273-0-0-0.98][13273-0-0-0.99][13273-0-0-0.88][13634-1-1-0.49][13634-1-1-0.47]
[13634-1-1-0.53][13763-2-3-0.98][13763-2-3-0.97][13763-2-3-0.96][13905-3-3-0.57][13905-3-3-0.57][13905-3-3-0.68][14060-2-1-0.99][14060-2-1-0.99][14060-2-1-0.98]
[14065-3-3-0.55][14065-3-0-0.82][14065-3-3-0.48][14147-3-3-0.51][14147-3-1-0.86][14147-3-3-0.79][14595-2-1-0.96][14595-2-1-0.95][14595-2-1-0.92][14687-2-3-0.48]
[14687-2-3-0.50][14687-2-1-0.42][14788-2-3-0.56][14788-2-3-0.68][14788-2-3-0.54][14869-1-1-0.58][14869-1-1-0.65][14869-1-1-0.61][14872-3-0-0.95][14872-3-0-0.99]
[14872-3-0-0.90][14877-1-0-0.47][14877-1-0-0.46][14877-1-0-0.36][14927-0-3-1.00][14927-0-3-1.00][14927-0-3-1.00][15066-0-0-0.94][15066-0-0-0.92][15066-0-0-0.98]
[15175-1-2-0.49][15175-1-2-0.42][15175-1-2-0.42][15178-2-0-0.89][15178-2-3-0.54][15178-2-3-0.51][15375-3-0-0.58][15375-3-3-0.82][15375-3-3-0.57][15389-3-3-0.97]
[15389-3-3-0.99][15389-3-3-0.98][15568-2-1-0.54][15568-2-1-0.81][15568-2-1-0.98][15675-3-3-0.92][15675-3-3-0.98][15675-3-3-0.71][15869-1-2-0.39][15869-1-2-0.37]
[15869-1-3-0.35][16207-3-0-0.88][16207-3-0-0.87][16207-3-0-0.95][16236-0-0-0.71][16236-0-0-0.61][16236-0-0-0.57][16302-3-3-0.83][16302-3-3-0.88][16302-3-3-0.77]
[16331-2-2-0.88][16331-2-2-0.82][16331-2-2-0.82][16381-0-0-0.80][16381-0-0-0.81][16381-0-0-0.65][16488-1-1-1.00][16488-1-1-1.00][16488-1-1-0.99][16495-0-0-0.68]
[16495-0-0-1.00][16495-0-0-0.84][16650-0-0-0.94][16650-0-0-0.95][16650-0-0-0.97][16719-1-0-0.53][16719-1-0-0.66][16719-1-0-0.50][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.94][16828-0-0-0.90][16828-0-0-0.78][17137-3-0-0.96][17137-3-0-0.86][17137-3-0-0.95][17245-1-2-0.45][17245-1-1-0.72][17245-1-1-0.54]
[17278-3-3-0.61][17278-3-3-0.68][17278-3-0-0.86][17282-0-0-0.69][17282-0-0-0.77][17282-0-0-0.43][17311-2-2-0.51][17311-2-2-0.56][17311-2-1-0.60][17336-2-1-0.98]
[17336-2-1-0.97][17336-2-1-1.00][17608-3-3-0.57][17608-3-3-0.84][17608-3-3-0.76][17627-0-3-0.84][17627-0-1-0.47][17627-0-3-0.58][17877-3-1-0.99][17877-3-1-0.68]
[17877-3-0-0.51][17924-1-0-0.42][17924-1-0-0.69][17924-1-0-0.45][17984-3-0-0.99][17984-3-0-0.93][17984-3-0-0.99][18211-0-3-0.47][18211-0-3-0.84][18211-0-3-0.81]
[18276-3-0-0.84][18276-3-0-0.90][18276-3-0-0.81][18287-1-0-0.48][18287-1-1-0.69][18287-1-1-0.69][18394-0-0-0.94][18394-0-0-0.93][18394-0-0-0.93][18428-0-1-0.87]
[18428-0-0-0.51][18428-0-0-0.97][18442-0-3-0.98][18442-0-3-0.97][18442-0-3-0.96][18478-3-0-0.78][18478-3-0-0.75][18478-3-0-0.72][18607-0-0-0.78][18607-0-0-0.64]
[18607-0-0-0.61][18616-0-0-0.67][18616-0-0-0.81][18616-0-0-0.79][18663-0-3-0.49][18663-0-3-0.80][18663-0-0-0.46][18718-0-0-0.75][18718-0-0-0.74][18718-0-0-0.86]
[18766-2-1-0.96][18766-2-1-0.99][18766-2-1-0.99][18824-2-2-0.67][18824-2-2-0.61][18824-2-1-0.64][18890-3-1-0.45][18890-3-2-0.42][18890-3-2-0.38][18930-3-2-0.45]
[18930-3-2-0.49][18930-3-0-0.47][18938-3-3-0.96][18938-3-3-0.95][18938-3-3-0.91][19817-1-2-0.50][19817-1-2-0.45][19817-1-2-0.42][19839-0-1-0.87][19839-0-1-0.90]
[19839-0-1-0.83][19930-3-3-0.97][19930-3-3-0.96][19930-3-3-0.96][19944-0-0-0.62][19944-0-1-0.85][19944-0-1-0.91][20036-2-2-0.70][20036-2-2-0.54][20036-2-2-0.56]
[20101-3-1-0.90][20101-3-3-0.63][20101-3-3-0.74][20474-1-1-0.63][20474-1-1-0.52][20474-1-1-0.51][20547-3-0-0.52][20547-3-0-0.97][20547-3-0-0.98][20929-2-1-0.61]
[20929-2-2-0.86][20929-2-2-0.82][21245-1-1-0.99][21245-1-1-0.99][21245-1-1-0.99][21257-3-3-0.73][21257-3-3-0.95][21257-3-3-0.96][21293-1-1-1.00][21293-1-1-1.00]
[21293-1-1-1.00][21316-1-1-1.00][21316-1-1-1.00][21316-1-1-0.56][21384-1-2-0.76][21384-1-2-0.72][21384-1-2-0.72][21448-1-1-0.95][21448-1-1-0.97][21448-1-1-0.93]
[21483-0-0-0.81][21483-0-0-0.77][21483-0-0-0.84][21487-2-2-0.57][21487-2-2-0.58][21487-2-2-0.71][21714-0-0-0.70][21714-0-0-0.78][21714-0-0-0.92][21943-3-2-0.47]
[21943-3-2-0.50][21943-3-2-0.54][21947-0-0-1.00][21947-0-0-1.00][21947-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-0.93][21965-2-2-0.86]
[21965-2-1-0.75][21998-1-1-0.56][21998-1-1-0.83][21998-1-1-0.76][22025-0-3-0.67][22025-0-3-0.53][22025-0-3-0.51][22228-3-3-0.92][22228-3-3-0.91][22228-3-3-0.90]
[22446-1-1-1.00][22446-1-1-1.00][22446-1-1-0.99][22494-3-0-0.84][22494-3-0-0.67][22494-3-0-0.55][22757-0-3-0.87][22757-0-0-0.63][22757-0-0-0.66][22811-3-3-0.49]
[22811-3-3-1.00][22811-3-3-1.00][22976-3-3-0.47][22976-3-1-0.58][22976-3-1-0.95][22985-3-3-0.55][22985-3-0-0.58][22985-3-0-0.66][23014-0-3-0.96][23014-0-3-0.93]
[23014-0-3-0.92][23112-1-1-0.91][23112-1-1-0.91][23112-1-1-0.92][23144-3-3-0.97][23144-3-3-0.92][23144-3-3-0.96][23168-2-0-0.99][23168-2-0-0.96][23168-2-0-0.99]
[23219-0-3-0.59][23219-0-0-0.69][23219-0-0-0.63][23363-3-3-1.00][23363-3-3-1.00][23363-3-3-1.00][23470-0-0-0.37][23470-0-0-0.35][23470-0-1-0.63][23486-2-3-0.51]
[23486-2-3-0.72][23486-2-3-0.70][23497-0-3-0.92][23497-0-3-0.92][23497-0-3-0.95][23516-0-0-0.96][23516-0-0-1.00][23516-0-0-1.00][23690-1-0-0.41][23690-1-0-0.71]
[23690-1-0-0.62][23921-2-1-0.95][23921-2-1-0.61][23921-2-1-0.68][23936-1-1-0.39][23936-1-3-0.54][23936-1-3-0.52][24040-3-1-0.77][24040-3-1-0.83][24040-3-1-0.76]
[24111-1-2-0.55][24111-1-2-0.57][24111-1-2-0.59][24182-0-3-0.53][24182-0-3-0.93][24182-0-3-0.80][24238-3-3-0.99][24238-3-3-0.98][24238-3-3-0.98][24290-2-0-1.00]
[24290-2-0-1.00][24290-2-0-1.00][24345-0-0-0.82][24345-0-0-0.69][24345-0-1-0.76][24364-1-3-0.75][24364-1-2-0.42][24364-1-3-0.45][24427-3-3-0.74][24427-3-3-0.77]
[24427-3-3-0.51][24477-2-2-0.84][24477-2-2-0.79][24477-2-1-0.55][24495-2-1-0.94][24495-2-1-0.90][24495-2-1-1.00][24893-2-1-0.82][24893-2-1-0.83][24893-2-1-0.87]
[25012-1-1-0.43][25012-1-1-0.40][25012-1-0-0.56][25121-2-2-0.51][25121-2-0-0.56][25121-2-0-0.51][25165-3-3-0.68][25165-3-3-0.67][25165-3-3-0.67][25183-0-0-0.96]
[25183-0-0-0.96][25183-0-0-0.95][25297-3-3-0.94][25297-3-3-0.97][25297-3-3-0.99][25398-0-0-0.74][25398-0-0-0.92][25398-0-0-0.93][25574-2-1-0.77][25574-2-3-0.70]
[25574-2-3-0.59][25644-1-1-0.99][25644-1-1-0.99][25644-1-1-0.56][25718-1-1-0.88][25718-1-1-0.58][25718-1-0-0.59][25774-2-2-0.52][25774-2-2-0.47][25774-2-1-0.63]
[26032-3-3-1.00][26032-3-3-1.00][26032-3-3-0.98][26051-3-3-0.98][26051-3-3-0.98][26051-3-3-0.99][26120-0-0-0.59][26120-0-0-0.60][26120-0-0-1.00][26321-1-1-0.89]
[26321-1-1-0.91][26321-1-0-0.40][26732-1-1-0.66][26732-1-1-0.97][26732-1-1-0.83][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-0.92][26827-3-3-0.90]
[26827-3-3-0.72][26833-0-3-1.00][26833-0-3-0.97][26833-0-3-0.91][26838-2-1-0.75][26838-2-1-0.67][26838-2-1-0.66][26860-1-1-0.74][26860-1-1-0.83][26860-1-2-0.77]
[26948-0-0-0.93][26948-0-0-0.86][26948-0-0-0.96][27049-3-0-0.90][27049-3-0-0.89][27049-3-0-0.93][27098-1-1-0.92][27098-1-1-0.88][27098-1-1-0.86][27526-0-0-0.99]
[27526-0-0-1.00][27526-0-0-0.91][27639-3-0-0.91][27639-3-0-0.80][27639-3-0-0.92][27698-3-3-0.96][27698-3-3-0.91][27698-3-3-0.95][27772-0-0-0.88][27772-0-0-0.79]
[27772-0-3-0.86][27890-1-1-0.87][27890-1-1-0.81][27890-1-1-0.84][28040-0-0-0.91][28040-0-0-0.94][28040-0-0-0.98][28503-2-2-0.70][28503-2-1-0.51][28503-2-2-0.50]
[28577-1-1-0.58][28577-1-1-0.58][28577-1-1-0.63][28959-0-0-1.00][28959-0-0-0.98][28959-0-0-1.00][29198-3-1-0.76][29198-3-1-0.81][29198-3-1-0.81][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-1-0.57][29877-2-1-0.92][29877-2-1-0.82][30035-1-1-0.93][30035-1-1-0.61][30035-1-1-0.95][30098-0-3-0.96][30098-0-3-0.92]
[30098-0-3-0.84][30326-1-1-1.00][30326-1-1-0.99][30326-1-1-1.00][30572-2-2-0.64][30572-2-3-0.58][30572-2-2-0.87][30716-0-3-0.36][30716-0-0-0.46][30716-0-0-0.58]
[30806-2-1-0.90][30806-2-1-0.89][30806-2-1-0.91][30906-1-1-0.95][30906-1-1-0.97][30906-1-1-0.94][31007-0-0-0.99][31007-0-0-0.98][31007-0-1-0.52][31181-3-3-0.65]
[31181-3-1-0.51][31181-3-1-0.89][31238-0-3-0.57][31238-0-3-0.95][31238-0-3-0.83][31347-0-3-0.58][31347-0-3-0.94][31347-0-3-0.91][31422-2-1-0.97][31422-2-1-0.93]
[31422-2-1-0.68][31429-3-0-0.58][31429-3-0-0.53][31429-3-3-0.56][31431-0-0-0.77][31431-0-3-0.77][31431-0-0-0.63][31432-1-1-0.76][31432-1-1-0.96][31432-1-1-0.98]
[31477-0-0-0.68][31477-0-0-0.78][31477-0-0-0.52][31524-1-3-0.45][31524-1-1-0.39][31524-1-0-0.56][31597-1-1-0.56][31597-1-1-0.57][31597-1-1-0.52][31619-1-0-0.71]
[31619-1-0-0.68][31619-1-0-0.62][31701-0-0-0.97][31701-0-0-0.96][31701-0-0-0.89][31755-0-0-0.94][31755-0-0-0.73][31755-0-0-0.85][31854-3-3-0.83][31854-3-3-0.95]
[31854-3-3-0.88][32074-1-1-0.62][32074-1-1-0.41][32074-1-0-0.51][32078-3-1-0.91][32078-3-1-0.83][32078-3-1-0.82][32111-1-1-0.75][32111-1-1-0.93][32111-1-1-0.96]
[32127-1-2-0.78][32127-1-2-0.79][32127-1-2-0.72][32140-3-3-1.00][32140-3-3-1.00][32140-3-3-0.99][32263-2-0-0.35][32263-2-3-0.51][32263-2-3-0.56][32365-0-0-0.93]
[32365-0-0-0.86][32365-0-0-0.85][32411-2-0-0.89][32411-2-0-0.77][32411-2-0-0.74][32429-3-3-0.51][32429-3-0-0.80][32429-3-0-0.55][32473-3-0-0.95][32473-3-0-0.96]
[32473-3-0-0.96][32574-3-3-0.84][32574-3-3-0.87][32574-3-3-0.89][32584-0-0-0.51][32584-0-0-0.42][32584-0-0-0.45][32622-0-1-0.50][32622-0-3-0.35][32622-0-1-0.62]
[32858-3-0-0.75][32858-3-0-0.77][32858-3-0-0.74][32969-3-0-0.75][32969-3-0-0.73][32969-3-0-0.76][33016-2-0-0.43][33016-2-2-0.91][33016-2-2-0.92][33031-1-0-0.95]
[33031-1-0-0.89][33031-1-0-0.80][33035-2-2-0.97][33035-2-2-0.92][33035-2-2-0.94][33133-2-1-0.88][33133-2-1-0.90][33133-2-1-0.96][33173-2-1-0.57][33173-2-1-0.64]
[33173-2-1-0.56][33175-3-1-0.84][33175-3-1-0.84][33175-3-1-0.85][33306-3-1-0.87][33306-3-1-0.88][33306-3-1-0.88][33309-2-1-0.56][33309-2-1-0.62][33309-2-1-0.66]
[33474-0-0-0.59][33474-0-0-0.67][33474-0-0-0.52][33478-2-0-0.85][33478-2-0-0.76][33478-2-0-0.81][33618-1-0-0.40][33618-1-0-0.44][33618-1-0-0.44][33712-0-0-0.57]
[33712-0-0-0.54][33712-0-0-0.75][33782-2-2-0.85][33782-2-2-0.93][33782-2-2-0.86][33914-3-3-0.50][33914-3-3-0.99][33914-3-3-1.00][34076-3-3-0.81][34076-3-3-0.82]
[34076-3-3-0.91][34112-2-1-1.00][34112-2-1-0.99][34112-2-1-0.99][34138-2-1-0.89][34138-2-1-0.90][34138-2-1-0.89][34239-1-1-0.67][34239-1-1-0.60][34239-1-1-0.73]
[34364-2-1-0.95][34364-2-1-0.97][34364-2-1-0.94][34617-1-1-0.78][34617-1-1-0.87][34617-1-1-0.47][34751-3-3-1.00][34751-3-3-1.00][34751-3-3-1.00][34783-2-2-0.57]
[34783-2-2-0.73][34783-2-1-0.85][35015-3-3-0.89][35015-3-3-0.65][35015-3-3-0.74][35018-1-1-0.70][35018-1-1-0.92][35018-1-1-0.69][35288-2-3-0.97][35288-2-1-0.57]
[35288-2-3-0.60]
---------------------------
I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.128 | Acc: 62.750% | Wgt Acc: 62.767%
	I - Batch: 100 | Loss: 1.122 | Acc: 63.062% | Wgt Acc: 63.044%
	I - Batch: 150 | Loss: 1.125 | Acc: 62.667% | Wgt Acc: 62.548%
	I - Batch: 200 | Loss: 1.110 | Acc: 64.438% | Wgt Acc: 64.205%
	I - Batch: 250 | Loss: 1.106 | Acc: 64.950% | Wgt Acc: 64.655%
	I - Batch: 300 | Loss: 1.103 | Acc: 65.354% | Wgt Acc: 64.971%
	I - Batch: 350 | Loss: 1.104 | Acc: 65.214% | Wgt Acc: 64.800%
	I - Batch: 400 | Loss: 1.103 | Acc: 65.125% | Wgt Acc: 64.707%
	I - Batch: 450 | Loss: 1.103 | Acc: 65.014% | Wgt Acc: 64.582%
I - num batch: 478
I - Train -- Loss: 1.102 | Acc: 65.162% | Wgt Acc: 64.759% | LR: 1.000000e-03 | Dur: 300.12s
I - Confusion Matrix: [row->prediction - col->label]
[[1460.   82.   94.  334.]
 [ 125. 1098.  462.  165.]
 [ 133.  448. 1474.  168.]
 [ 373.  106.  172.  947.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.153 | Acc: 57.875% | Wgt Acc: 57.746%
I - num batch: 62
I - Val -- Loss: 1.165 | Acc: 56.881% | Wgt Acc: 56.680% | Dur: 30.48s
I - Confusion Matrix: [row->prediction - col->label]
[[156.   9.  11.  59.]
 [ 45. 149.  63.  40.]
 [ 12.  74. 131.  37.]
 [ 51.   2.  20. 122.]]

I - Local maximum validation set accuracy:  56.88

I - Validation set results: 
[14-1-2-0.95][14-1-2-0.82][14-1-2-0.80][50-3-1-0.92][50-3-1-1.00][50-3-1-0.99][124-2-2-0.95][124-2-2-0.99][124-2-2-0.99][127-0-0-1.00]
[127-0-0-1.00][127-0-0-0.99][443-2-2-0.98][443-2-2-0.97][443-2-2-0.95][567-0-0-0.98][567-0-0-1.00][567-0-0-0.95][573-1-1-0.45][573-1-1-0.99]
[573-1-1-0.95][615-0-3-0.67][615-0-3-0.55][615-0-0-0.64][695-1-2-0.99][695-1-2-0.99][695-1-2-0.99][722-3-0-0.84][722-3-0-0.54][722-3-0-0.89]
[826-0-3-0.62][826-0-0-0.77][826-0-0-0.82][878-0-0-0.62][878-0-0-0.86][878-0-0-0.90][1103-0-0-0.49][1103-0-1-0.50][1103-0-1-0.51][1212-3-3-0.87]
[1212-3-3-0.85][1212-3-3-0.78][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-3-0.53][2181-2-3-0.53][2181-2-3-0.57][2476-2-1-0.86][2476-2-1-0.77]
[2476-2-1-0.77][2721-2-2-0.99][2721-2-2-0.99][2721-2-2-0.99][2818-1-1-0.53][2818-1-1-0.77][2818-1-1-0.76][2886-2-1-0.95][2886-2-1-0.87][2886-2-1-0.88]
[3231-2-2-0.99][3231-2-2-0.99][3231-2-2-0.99][3333-2-1-0.82][3333-2-1-0.77][3333-2-1-0.66][3482-2-2-0.94][3482-2-2-0.96][3482-2-2-0.96][3536-3-3-0.86]
[3536-3-3-0.88][3536-3-3-0.80][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-1-0.80][3909-0-1-0.87][3909-0-1-0.75][4035-0-3-0.52][4035-0-3-0.57]
[4035-0-3-0.71][4140-0-0-0.99][4140-0-0-0.85][4140-0-0-0.98][4214-1-2-0.84][4214-1-2-0.57][4214-1-1-0.50][4346-1-0-0.64][4346-1-0-0.98][4346-1-0-0.98]
[4581-2-2-0.99][4581-2-2-0.97][4581-2-2-0.83][4708-3-1-0.67][4708-3-2-0.73][4708-3-2-0.75][4838-3-1-0.86][4838-3-1-0.76][4838-3-1-0.95][4845-1-2-0.55]
[4845-1-2-0.45][4845-1-2-0.52][4868-0-0-0.94][4868-0-0-1.00][4868-0-0-0.84][4939-0-1-0.34][4939-0-1-0.95][4939-0-1-0.65][4984-2-2-0.86][4984-2-2-0.98]
[4984-2-2-0.97][5078-1-1-0.85][5078-1-1-0.99][5078-1-1-0.92][5396-0-0-1.00][5396-0-0-1.00][5396-0-0-1.00][5479-1-1-0.88][5479-1-1-0.94][5479-1-1-0.87]
[5717-0-0-0.53][5717-0-0-0.80][5717-0-1-0.67][5843-1-1-1.00][5843-1-1-1.00][5843-1-1-1.00][5949-3-3-0.50][5949-3-3-0.85][5949-3-3-0.94][5987-2-1-0.93]
[5987-2-1-0.77][5987-2-1-0.96][6014-3-1-0.89][6014-3-1-0.99][6014-3-1-0.94][6033-3-0-0.87][6033-3-0-0.75][6033-3-0-0.92][6313-0-0-0.96][6313-0-0-0.88]
[6313-0-0-0.53][6421-3-3-1.00][6421-3-3-1.00][6421-3-3-1.00][6500-1-1-0.99][6500-1-1-0.99][6500-1-1-0.99][6583-3-3-0.59][6583-3-3-0.66][6583-3-3-0.68]
[6683-3-3-0.50][6683-3-3-0.65][6683-3-3-0.88][6825-2-0-0.67][6825-2-0-0.56][6825-2-0-0.45][6998-3-2-0.67][6998-3-3-0.50][6998-3-2-0.55][7049-3-2-0.71]
[7049-3-2-0.83][7049-3-2-0.61][7517-1-2-0.67][7517-1-1-0.55][7517-1-1-0.71][7521-1-1-0.91][7521-1-1-0.92][7521-1-1-0.97][7528-1-2-0.51][7528-1-2-0.82]
[7528-1-2-0.95][7949-1-2-0.91][7949-1-2-0.93][7949-1-2-0.77][8135-1-0-0.98][8135-1-0-0.54][8135-1-0-0.82][8185-3-0-0.60][8185-3-0-0.57][8185-3-0-0.75]
[8269-3-1-0.41][8269-3-1-0.50][8269-3-2-0.60][8273-3-3-0.99][8273-3-3-0.96][8273-3-3-1.00][8543-3-0-0.68][8543-3-0-1.00][8543-3-0-1.00][8666-1-1-0.81]
[8666-1-1-0.91][8666-1-1-0.87][8672-0-0-1.00][8672-0-0-0.89][8672-0-3-0.65][8903-1-3-0.36][8903-1-2-0.57][8903-1-2-0.79][9001-2-1-0.55][9001-2-1-1.00]
[9001-2-1-1.00][9036-2-2-0.99][9036-2-2-0.95][9036-2-2-1.00][9281-3-1-0.94][9281-3-1-0.98][9281-3-1-1.00][9300-2-2-0.90][9300-2-2-0.98][9300-2-2-0.93]
[9571-0-3-0.91][9571-0-3-0.46][9571-0-3-0.38][9617-1-1-0.95][9617-1-1-0.58][9617-1-1-0.80][9644-2-2-0.94][9644-2-2-0.97][9644-2-2-0.95][9705-2-1-0.70]
[9705-2-2-0.55][9705-2-2-0.50][9801-0-3-0.93][9801-0-3-0.97][9801-0-3-0.98][9803-3-0-0.60][9803-3-3-0.54][9803-3-3-0.55][9865-3-3-0.96][9865-3-3-0.95]
[9865-3-0-0.50][9896-2-2-0.80][9896-2-2-0.58][9896-2-1-0.59][10314-1-1-0.59][10314-1-1-0.51][10314-1-2-0.64][10337-3-3-1.00][10337-3-3-1.00][10337-3-3-1.00]
[10403-0-0-0.44][10403-0-0-0.50][10403-0-0-0.40][10653-2-1-1.00][10653-2-1-1.00][10653-2-1-0.88][10704-2-1-0.45][10704-2-1-0.36][10704-2-3-0.39][10719-1-1-0.66]
[10719-1-1-0.80][10719-1-1-0.82][10727-1-1-1.00][10727-1-1-1.00][10727-1-1-1.00][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-3-0.67][10969-2-3-0.80]
[10969-2-3-0.65][11042-0-0-0.96][11042-0-0-0.81][11042-0-0-0.98][11088-1-2-0.99][11088-1-2-0.64][11088-1-1-0.59][11322-0-0-0.60][11322-0-0-0.73][11322-0-0-0.82]
[11398-2-2-1.00][11398-2-2-0.95][11398-2-2-0.84][11499-0-0-0.47][11499-0-0-0.49][11499-0-0-0.45][11502-3-0-0.50][11502-3-3-0.83][11502-3-0-0.54][11512-3-1-0.89]
[11512-3-1-0.65][11512-3-1-0.68][11608-1-1-0.84][11608-1-1-0.87][11608-1-1-0.92][11610-0-3-0.67][11610-0-0-0.54][11610-0-0-0.63][11692-0-0-0.76][11692-0-0-0.75]
[11692-0-0-0.81][11905-0-0-0.94][11905-0-0-0.83][11905-0-0-0.88][11993-1-2-0.70][11993-1-2-0.76][11993-1-2-0.84][12002-2-0-0.72][12002-2-2-0.56][12002-2-3-0.75]
[12052-0-0-0.58][12052-0-0-0.96][12052-0-0-0.73][12201-0-0-0.54][12201-0-3-0.69][12201-0-3-0.54][12235-2-1-0.99][12235-2-1-1.00][12235-2-1-0.95][12320-1-1-0.45]
[12320-1-1-0.53][12320-1-0-0.74][12377-2-1-0.94][12377-2-1-0.81][12377-2-1-0.99][12398-2-3-0.69][12398-2-3-0.57][12398-2-1-0.63][12503-1-2-1.00][12503-1-2-0.63]
[12503-1-1-0.83][12617-0-1-0.97][12617-0-1-0.96][12617-0-1-0.97][12685-3-2-0.67][12685-3-2-0.69][12685-3-2-0.61][12738-2-2-0.66][12738-2-0-0.40][12738-2-0-0.38]
[12742-2-2-0.96][12742-2-2-0.95][12742-2-2-0.98][12823-0-0-0.52][12823-0-3-0.56][12823-0-3-0.54][13110-1-1-1.00][13110-1-1-0.98][13110-1-2-0.55][13240-3-3-0.50]
[13240-3-0-0.49][13240-3-0-0.52][13253-1-1-0.88][13253-1-1-0.95][13253-1-1-0.89][13273-0-0-0.99][13273-0-0-0.99][13273-0-0-1.00][13634-1-2-0.84][13634-1-2-0.86]
[13634-1-2-0.89][13763-2-2-0.58][13763-2-2-0.89][13763-2-2-0.51][13905-3-3-0.56][13905-3-0-0.58][13905-3-3-0.60][14060-2-1-1.00][14060-2-1-1.00][14060-2-1-1.00]
[14065-3-3-0.92][14065-3-0-0.59][14065-3-3-0.45][14147-3-2-0.78][14147-3-2-0.97][14147-3-0-0.40][14595-2-2-0.67][14595-2-1-0.49][14595-2-2-0.59][14687-2-2-1.00]
[14687-2-2-1.00][14687-2-2-1.00][14788-2-2-0.95][14788-2-2-0.78][14788-2-2-0.95][14869-1-1-0.80][14869-1-1-0.78][14869-1-1-0.81][14872-3-0-0.67][14872-3-0-0.98]
[14872-3-0-0.72][14877-1-1-0.80][14877-1-1-0.77][14877-1-1-0.80][14927-0-3-0.97][14927-0-3-0.97][14927-0-3-0.96][15066-0-3-0.53][15066-0-3-0.60][15066-0-0-0.63]
[15175-1-1-0.74][15175-1-1-0.73][15175-1-1-0.83][15178-2-3-0.83][15178-2-3-0.90][15178-2-3-0.88][15375-3-0-0.59][15375-3-3-0.58][15375-3-1-0.67][15389-3-3-0.99]
[15389-3-3-0.99][15389-3-3-0.98][15568-2-1-1.00][15568-2-1-1.00][15568-2-1-1.00][15675-3-3-0.80][15675-3-3-0.89][15675-3-3-0.56][15869-1-2-0.79][15869-1-2-0.87]
[15869-1-2-0.89][16207-3-0-0.66][16207-3-0-0.49][16207-3-0-0.75][16236-0-2-0.47][16236-0-2-0.58][16236-0-0-0.46][16302-3-0-0.46][16302-3-3-0.61][16302-3-3-0.82]
[16331-2-2-1.00][16331-2-2-1.00][16331-2-2-1.00][16381-0-3-0.83][16381-0-3-0.90][16381-0-3-0.93][16488-1-1-0.87][16488-1-1-0.91][16488-1-2-0.56][16495-0-0-0.56]
[16495-0-0-1.00][16495-0-0-1.00][16650-0-0-0.97][16650-0-0-0.96][16650-0-0-0.98][16719-1-1-1.00][16719-1-1-1.00][16719-1-1-1.00][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.56][16828-0-0-0.50][16828-0-0-0.52][17137-3-0-0.98][17137-3-0-0.68][17137-3-0-0.96][17245-1-2-0.72][17245-1-2-0.66][17245-1-1-0.64]
[17278-3-1-0.68][17278-3-1-0.63][17278-3-1-0.85][17282-0-1-0.58][17282-0-1-0.50][17282-0-1-0.54][17311-2-2-0.99][17311-2-2-0.99][17311-2-2-1.00][17336-2-2-0.70]
[17336-2-2-0.89][17336-2-2-0.80][17608-3-0-0.66][17608-3-3-0.78][17608-3-3-0.73][17627-0-3-0.50][17627-0-2-0.55][17627-0-3-0.43][17877-3-2-1.00][17877-3-2-0.50]
[17877-3-1-0.80][17924-1-2-0.75][17924-1-2-0.45][17924-1-2-0.76][17984-3-0-0.90][17984-3-0-0.76][17984-3-0-0.89][18211-0-1-0.96][18211-0-1-0.53][18211-0-1-0.78]
[18276-3-0-0.75][18276-3-0-0.77][18276-3-0-0.66][18287-1-1-0.76][18287-1-1-1.00][18287-1-1-1.00][18394-0-0-0.97][18394-0-0-0.88][18394-0-0-0.93][18428-0-1-0.66]
[18428-0-0-0.55][18428-0-0-0.99][18442-0-3-0.84][18442-0-3-0.89][18442-0-3-0.91][18478-3-0-0.66][18478-3-0-0.61][18478-3-0-0.62][18607-0-1-0.56][18607-0-1-0.70]
[18607-0-1-0.58][18616-0-1-0.82][18616-0-1-0.83][18616-0-1-0.66][18663-0-0-0.63][18663-0-0-0.45][18663-0-0-0.62][18718-0-0-1.00][18718-0-0-1.00][18718-0-0-1.00]
[18766-2-2-1.00][18766-2-2-0.99][18766-2-2-1.00][18824-2-1-0.82][18824-2-1-0.87][18824-2-1-0.89][18890-3-1-0.88][18890-3-1-0.84][18890-3-1-0.73][18930-3-1-0.51]
[18930-3-1-0.50][18930-3-1-0.60][18938-3-3-0.73][18938-3-3-0.53][18938-3-3-0.39][19817-1-1-0.50][19817-1-1-0.57][19817-1-1-0.56][19839-0-2-0.69][19839-0-2-0.67]
[19839-0-2-0.68][19930-3-3-0.74][19930-3-3-0.92][19930-3-3-0.98][19944-0-0-0.61][19944-0-2-0.74][19944-0-2-0.96][20036-2-2-1.00][20036-2-2-1.00][20036-2-2-1.00]
[20101-3-1-0.99][20101-3-3-0.62][20101-3-3-0.85][20474-1-2-0.94][20474-1-2-0.98][20474-1-2-0.96][20547-3-1-0.67][20547-3-0-0.68][20547-3-0-0.70][20929-2-2-0.63]
[20929-2-2-0.99][20929-2-2-0.98][21245-1-1-0.97][21245-1-1-0.66][21245-1-1-0.76][21257-3-3-0.61][21257-3-3-0.93][21257-3-3-0.97][21293-1-1-0.87][21293-1-1-0.88]
[21293-1-1-0.86][21316-1-1-0.98][21316-1-1-1.00][21316-1-1-0.43][21384-1-2-0.84][21384-1-2-0.82][21384-1-2-0.83][21448-1-1-0.67][21448-1-1-0.54][21448-1-1-0.73]
[21483-0-0-0.99][21483-0-0-0.99][21483-0-0-0.99][21487-2-2-0.85][21487-2-2-0.79][21487-2-2-0.78][21714-0-0-0.93][21714-0-0-0.94][21714-0-0-0.96][21943-3-2-1.00]
[21943-3-2-1.00][21943-3-2-1.00][21947-0-0-0.91][21947-0-0-0.74][21947-0-0-0.66][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-0.76][21965-2-2-1.00]
[21965-2-1-0.95][21998-1-2-0.77][21998-1-1-0.87][21998-1-1-0.62][22025-0-1-0.51][22025-0-3-0.74][22025-0-3-0.51][22228-3-3-0.99][22228-3-3-0.98][22228-3-3-0.98]
[22446-1-1-1.00][22446-1-1-1.00][22446-1-1-1.00][22494-3-3-0.59][22494-3-3-0.73][22494-3-3-0.60][22757-0-3-0.89][22757-0-3-0.60][22757-0-3-0.56][22811-3-2-0.49]
[22811-3-3-1.00][22811-3-3-1.00][22976-3-2-0.89][22976-3-2-0.59][22976-3-1-1.00][22985-3-3-0.92][22985-3-3-0.97][22985-3-3-0.81][23014-0-0-0.56][23014-0-0-0.77]
[23014-0-0-0.81][23112-1-1-0.89][23112-1-1-0.88][23112-1-1-0.92][23144-3-3-0.98][23144-3-3-0.97][23144-3-3-0.99][23168-2-0-0.84][23168-2-0-0.61][23168-2-1-0.50]
[23219-0-1-0.65][23219-0-1-0.63][23219-0-3-0.56][23363-3-3-0.92][23363-3-3-0.97][23363-3-3-0.98][23470-0-1-0.76][23470-0-1-0.68][23470-0-1-0.68][23486-2-2-0.50]
[23486-2-3-0.48][23486-2-2-0.47][23497-0-3-0.99][23497-0-3-0.97][23497-0-3-0.82][23516-0-0-0.90][23516-0-0-0.96][23516-0-0-1.00][23690-1-1-0.96][23690-1-1-0.70]
[23690-1-1-0.86][23921-2-1-0.76][23921-2-2-0.59][23921-2-2-0.68][23936-1-2-1.00][23936-1-2-1.00][23936-1-2-0.99][24040-3-2-0.83][24040-3-2-0.82][24040-3-2-0.88]
[24111-1-1-0.99][24111-1-1-0.99][24111-1-1-0.99][24182-0-0-0.99][24182-0-0-0.81][24182-0-0-0.94][24238-3-3-0.98][24238-3-3-0.95][24238-3-3-0.97][24290-2-0-0.90]
[24290-2-0-0.62][24290-2-0-0.72][24345-0-0-0.97][24345-0-0-0.84][24345-0-2-0.58][24364-1-2-0.67][24364-1-2-0.93][24364-1-2-0.86][24427-3-3-0.91][24427-3-3-0.84]
[24427-3-3-0.57][24477-2-2-0.99][24477-2-2-1.00][24477-2-2-0.95][24495-2-1-0.98][24495-2-1-0.90][24495-2-1-0.99][24893-2-1-0.55][24893-2-2-0.55][24893-2-1-0.68]
[25012-1-1-0.94][25012-1-1-0.97][25012-1-1-0.96][25121-2-2-0.51][25121-2-2-0.57][25121-2-2-0.47][25165-3-3-0.73][25165-3-3-0.77][25165-3-3-0.70][25183-0-0-0.91]
[25183-0-0-0.94][25183-0-0-0.98][25297-3-3-0.87][25297-3-3-0.90][25297-3-3-0.87][25398-0-0-0.73][25398-0-0-0.95][25398-0-0-0.90][25574-2-1-0.70][25574-2-2-0.69]
[25574-2-2-0.83][25644-1-1-0.88][25644-1-1-0.71][25644-1-1-0.59][25718-1-1-0.99][25718-1-1-0.97][25718-1-1-0.60][25774-2-2-1.00][25774-2-2-1.00][25774-2-2-0.99]
[26032-3-3-0.98][26032-3-3-0.99][26032-3-3-0.94][26051-3-3-1.00][26051-3-3-1.00][26051-3-3-1.00][26120-0-1-0.61][26120-0-0-0.67][26120-0-0-1.00][26321-1-1-0.64]
[26321-1-1-0.69][26321-1-2-0.59][26732-1-1-0.98][26732-1-1-1.00][26732-1-1-0.98][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-0.93][26827-3-3-0.89]
[26827-3-3-0.58][26833-0-3-1.00][26833-0-3-0.99][26833-0-3-0.91][26838-2-2-0.51][26838-2-2-0.77][26838-2-2-0.60][26860-1-2-0.76][26860-1-2-0.62][26860-1-2-0.94]
[26948-0-0-0.94][26948-0-0-0.55][26948-0-1-0.43][27049-3-0-1.00][27049-3-0-1.00][27049-3-0-1.00][27098-1-1-0.68][27098-1-1-0.80][27098-1-1-0.82][27526-0-0-0.96]
[27526-0-0-0.96][27526-0-0-0.57][27639-3-3-0.54][27639-3-3-0.69][27639-3-3-0.56][27698-3-3-0.98][27698-3-3-0.96][27698-3-3-0.98][27772-0-0-0.90][27772-0-0-0.87]
[27772-0-3-0.71][27890-1-1-0.91][27890-1-1-0.87][27890-1-1-0.93][28040-0-0-0.66][28040-0-0-0.76][28040-0-0-1.00][28503-2-2-1.00][28503-2-2-1.00][28503-2-2-1.00]
[28577-1-1-0.90][28577-1-1-0.93][28577-1-1-0.90][28959-0-0-1.00][28959-0-0-0.92][28959-0-0-1.00][29198-3-1-0.89][29198-3-1-0.91][29198-3-1-0.92][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-1-0.92][29877-2-1-0.70][29877-2-1-0.59][30035-1-1-0.59][30035-1-2-0.82][30035-1-2-0.55][30098-0-3-0.56][30098-0-0-0.69]
[30098-0-0-0.91][30326-1-1-1.00][30326-1-1-1.00][30326-1-1-1.00][30572-2-2-0.67][30572-2-2-0.91][30572-2-2-0.85][30716-0-1-1.00][30716-0-1-1.00][30716-0-1-0.99]
[30806-2-2-0.81][30806-2-2-0.77][30806-2-2-0.75][30906-1-1-1.00][30906-1-1-1.00][30906-1-1-0.99][31007-0-0-0.85][31007-0-0-0.96][31007-0-2-0.75][31181-3-3-0.65]
[31181-3-2-0.45][31181-3-2-0.86][31238-0-3-0.83][31238-0-3-0.97][31238-0-3-0.95][31347-0-0-0.80][31347-0-0-0.61][31347-0-0-0.69][31422-2-2-0.53][31422-2-1-0.62]
[31422-2-2-0.64][31429-3-3-0.73][31429-3-3-0.81][31429-3-3-0.87][31431-0-0-0.68][31431-0-3-0.49][31431-0-1-0.62][31432-1-1-0.93][31432-1-1-0.90][31432-1-1-0.90]
[31477-0-0-0.79][31477-0-0-0.95][31477-0-0-0.52][31524-1-2-1.00][31524-1-2-0.75][31524-1-0-0.46][31597-1-2-1.00][31597-1-2-1.00][31597-1-2-0.58][31619-1-2-0.36]
[31619-1-0-0.42][31619-1-2-0.69][31701-0-0-0.99][31701-0-0-0.99][31701-0-0-0.98][31755-0-0-0.97][31755-0-0-0.81][31755-0-0-0.82][31854-3-3-0.92][31854-3-3-0.90]
[31854-3-3-0.71][32074-1-1-0.63][32074-1-1-0.65][32074-1-3-0.83][32078-3-2-0.58][32078-3-1-0.44][32078-3-1-0.43][32111-1-1-0.74][32111-1-1-0.83][32111-1-1-0.97]
[32127-1-2-1.00][32127-1-2-1.00][32127-1-2-1.00][32140-3-3-0.50][32140-3-2-0.61][32140-3-3-0.84][32263-2-2-0.49][32263-2-2-0.62][32263-2-2-0.59][32365-0-1-0.59]
[32365-0-1-0.57][32365-0-0-0.87][32411-2-3-0.76][32411-2-3-0.71][32411-2-3-0.77][32429-3-0-0.57][32429-3-0-0.62][32429-3-0-0.66][32473-3-0-0.83][32473-3-0-0.83]
[32473-3-0-0.79][32574-3-3-0.64][32574-3-3-0.55][32574-3-3-0.54][32584-0-1-0.37][32584-0-2-0.34][32584-0-2-0.35][32622-0-1-0.97][32622-0-1-0.95][32622-0-1-0.99]
[32858-3-0-0.90][32858-3-0-0.93][32858-3-0-0.93][32969-3-0-0.69][32969-3-0-0.66][32969-3-0-0.69][33016-2-1-0.85][33016-2-2-0.98][33016-2-2-0.99][33031-1-1-0.94]
[33031-1-1-0.97][33031-1-1-0.94][33035-2-2-1.00][33035-2-2-1.00][33035-2-2-1.00][33133-2-2-0.86][33133-2-2-0.93][33133-2-2-0.96][33173-2-1-0.55][33173-2-1-0.84]
[33173-2-1-0.68][33175-3-2-1.00][33175-3-2-1.00][33175-3-2-1.00][33306-3-1-0.61][33306-3-1-0.64][33306-3-1-0.58][33309-2-2-0.68][33309-2-2-0.51][33309-2-2-0.62]
[33474-0-1-0.83][33474-0-1-0.90][33474-0-3-0.63][33478-2-3-0.74][33478-2-2-0.49][33478-2-3-0.72][33618-1-1-0.95][33618-1-1-0.95][33618-1-1-0.95][33712-0-0-0.51]
[33712-0-3-0.53][33712-0-0-0.69][33782-2-1-0.90][33782-2-1-0.78][33782-2-1-0.79][33914-3-2-0.99][33914-3-3-0.88][33914-3-3-1.00][34076-3-2-0.72][34076-3-2-0.81]
[34076-3-3-0.72][34112-2-2-0.94][34112-2-2-0.77][34112-2-2-0.50][34138-2-2-1.00][34138-2-2-1.00][34138-2-2-1.00][34239-1-2-0.92][34239-1-2-0.86][34239-1-2-0.91]
[34364-2-2-0.79][34364-2-2-0.99][34364-2-2-0.99][34617-1-1-1.00][34617-1-1-1.00][34617-1-1-0.68][34751-3-3-1.00][34751-3-3-1.00][34751-3-3-1.00][34783-2-1-0.86]
[34783-2-1-0.81][34783-2-2-0.52][35015-3-2-0.96][35015-3-2-0.93][35015-3-2-0.97][35018-1-2-0.90][35018-1-1-0.73][35018-1-1-0.67][35288-2-3-0.83][35288-2-1-0.95]
[35288-2-1-0.84]
---------------------------
I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.073 | Acc: 68.250% | Wgt Acc: 67.763%
	I - Batch: 100 | Loss: 1.068 | Acc: 69.125% | Wgt Acc: 68.658%
	I - Batch: 150 | Loss: 1.065 | Acc: 69.708% | Wgt Acc: 69.143%
	I - Batch: 200 | Loss: 1.061 | Acc: 69.938% | Wgt Acc: 69.456%
	I - Batch: 250 | Loss: 1.068 | Acc: 69.175% | Wgt Acc: 68.761%
	I - Batch: 300 | Loss: 1.070 | Acc: 68.750% | Wgt Acc: 68.449%
	I - Batch: 350 | Loss: 1.070 | Acc: 68.839% | Wgt Acc: 68.431%
	I - Batch: 400 | Loss: 1.070 | Acc: 68.984% | Wgt Acc: 68.578%
	I - Batch: 450 | Loss: 1.070 | Acc: 68.986% | Wgt Acc: 68.574%
I - num batch: 478
I - Train -- Loss: 1.069 | Acc: 69.127% | Wgt Acc: 68.713% | LR: 1.000000e-03 | Dur: 307.76s
I - Confusion Matrix: [row->prediction - col->label]
[[1549.   74.   81.  310.]
 [ 128. 1173.  410.  145.]
 [ 111.  402. 1559.  158.]
 [ 303.   85.  152. 1001.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.181 | Acc: 55.375% | Wgt Acc: 54.414%
I - num batch: 62
I - Val -- Loss: 1.174 | Acc: 56.677% | Wgt Acc: 55.820% | Dur: 32.18s
I - Confusion Matrix: [row->prediction - col->label]
[[147.  11.  12.  51.]
 [ 21. 103.  30.  27.]
 [ 34. 118. 168.  42.]
 [ 62.   2.  15. 138.]]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.035 | Acc: 73.000% | Wgt Acc: 72.922%
	I - Batch: 100 | Loss: 1.030 | Acc: 72.688% | Wgt Acc: 72.334%
	I - Batch: 150 | Loss: 1.030 | Acc: 73.000% | Wgt Acc: 72.638%
	I - Batch: 200 | Loss: 1.030 | Acc: 72.781% | Wgt Acc: 72.446%
	I - Batch: 250 | Loss: 1.034 | Acc: 72.300% | Wgt Acc: 72.033%
	I - Batch: 300 | Loss: 1.039 | Acc: 71.625% | Wgt Acc: 71.380%
	I - Batch: 350 | Loss: 1.036 | Acc: 72.268% | Wgt Acc: 71.995%
	I - Batch: 400 | Loss: 1.034 | Acc: 72.391% | Wgt Acc: 72.135%
	I - Batch: 450 | Loss: 1.034 | Acc: 72.486% | Wgt Acc: 72.227%
I - num batch: 478
I - Train -- Loss: 1.033 | Acc: 72.569% | Wgt Acc: 72.299% | LR: 1.000000e-03 | Dur: 294.40s
I - Confusion Matrix: [row->prediction - col->label]
[[1574.   55.   66.  299.]
 [ 130. 1296.  347.  109.]
 [ 104.  304. 1633.  164.]
 [ 283.   79.  156. 1042.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.185 | Acc: 55.875% | Wgt Acc: 54.303%
I - num batch: 62
I - Val -- Loss: 1.187 | Acc: 55.454% | Wgt Acc: 53.940% | Dur: 30.33s
I - Confusion Matrix: [row->prediction - col->label]
[[202.  17.  20.  90.]
 [ 45. 138.  66.  65.]
 [  7.  76. 136.  35.]
 [ 10.   3.   3.  68.]]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 1.005 | Acc: 76.250% | Wgt Acc: 76.168%
	I - Batch: 100 | Loss: 1.010 | Acc: 75.875% | Wgt Acc: 75.769%
	I - Batch: 150 | Loss: 1.005 | Acc: 76.167% | Wgt Acc: 76.008%
	I - Batch: 200 | Loss: 1.009 | Acc: 75.625% | Wgt Acc: 75.492%
	I - Batch: 250 | Loss: 1.011 | Acc: 75.475% | Wgt Acc: 75.297%
	I - Batch: 300 | Loss: 1.011 | Acc: 75.417% | Wgt Acc: 75.248%
	I - Batch: 350 | Loss: 1.010 | Acc: 75.500% | Wgt Acc: 75.287%
	I - Batch: 400 | Loss: 1.008 | Acc: 75.844% | Wgt Acc: 75.616%
	I - Batch: 450 | Loss: 1.011 | Acc: 75.472% | Wgt Acc: 75.264%
I - num batch: 478
I - Train -- Loss: 1.011 | Acc: 75.448% | Wgt Acc: 75.227% | LR: 1.000000e-03 | Dur: 311.56s
I - Confusion Matrix: [row->prediction - col->label]
[[1604.   70.   63.  261.]
 [ 110. 1360.  297.   99.]
 [ 102.  248. 1710.  163.]
 [ 275.   56.  132. 1091.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.173 | Acc: 55.875% | Wgt Acc: 55.608%
I - num batch: 62
I - Val -- Loss: 1.172 | Acc: 55.861% | Wgt Acc: 55.571% | Dur: 36.35s
I - Confusion Matrix: [row->prediction - col->label]
[[117.   5.   7.  23.]
 [ 18. 102.  33.  18.]
 [ 44. 124. 169.  57.]
 [ 85.   3.  16. 160.]]

I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 1.016 | Acc: 74.625% | Wgt Acc: 74.231%
	I - Batch: 100 | Loss: 0.999 | Acc: 76.938% | Wgt Acc: 76.634%
	I - Batch: 150 | Loss: 0.995 | Acc: 77.250% | Wgt Acc: 76.987%
	I - Batch: 200 | Loss: 0.999 | Acc: 77.156% | Wgt Acc: 76.885%
	I - Batch: 250 | Loss: 0.998 | Acc: 76.925% | Wgt Acc: 76.672%
	I - Batch: 300 | Loss: 0.995 | Acc: 77.333% | Wgt Acc: 77.162%
	I - Batch: 350 | Loss: 0.996 | Acc: 77.411% | Wgt Acc: 77.210%
	I - Batch: 400 | Loss: 0.996 | Acc: 77.375% | Wgt Acc: 77.200%
	I - Batch: 450 | Loss: 0.996 | Acc: 77.208% | Wgt Acc: 77.064%
I - num batch: 478
I - Train -- Loss: 0.997 | Acc: 77.084% | Wgt Acc: 76.955% | LR: 1.000000e-03 | Dur: 352.76s
I - Confusion Matrix: [row->prediction - col->label]
[[1637.   54.   67.  246.]
 [ 109. 1400.  259.  101.]
 [  85.  207. 1716.  130.]
 [ 260.   73.  160. 1137.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.156 | Acc: 59.875% | Wgt Acc: 57.301%
I - num batch: 62
I - Val -- Loss: 1.161 | Acc: 59.837% | Wgt Acc: 57.337% | Dur: 38.79s
I - Confusion Matrix: [row->prediction - col->label]
[[235.  30.  33. 107.]
 [  1.  83.  19.  15.]
 [ 10. 118. 168.  35.]
 [ 18.   3.   5. 101.]]

I - Local maximum validation set accuracy:  59.84

I - Validation set results: 
[14-1-2-0.91][14-1-2-0.86][14-1-2-0.75][50-3-0-0.38][50-3-0-0.59][50-3-0-0.62][124-2-2-0.94][124-2-2-0.96][124-2-2-0.98][127-0-0-1.00]
[127-0-0-1.00][127-0-0-1.00][443-2-2-1.00][443-2-2-1.00][443-2-2-1.00][567-0-0-0.94][567-0-0-0.92][567-0-0-0.98][573-1-1-0.43][573-1-2-0.59]
[573-1-2-0.66][615-0-0-0.99][615-0-0-0.69][615-0-0-0.90][695-1-0-0.54][695-1-2-0.56][695-1-0-0.56][722-3-0-1.00][722-3-0-0.95][722-3-0-0.92]
[826-0-0-0.95][826-0-0-1.00][826-0-0-1.00][878-0-0-0.95][878-0-0-0.98][878-0-0-0.99][1103-0-0-0.87][1103-0-0-0.66][1103-0-0-0.69][1212-3-0-0.52]
[1212-3-3-0.49][1212-3-0-0.48][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-0-0.46][2181-2-0-0.45][2181-2-0-0.50][2476-2-2-0.76][2476-2-2-0.82]
[2476-2-2-0.90][2721-2-2-0.73][2721-2-2-0.82][2721-2-2-0.83][2818-1-2-0.46][2818-1-0-0.63][2818-1-0-0.97][2886-2-2-0.56][2886-2-2-0.61][2886-2-2-0.57]
[3231-2-2-1.00][3231-2-2-1.00][3231-2-2-1.00][3333-2-2-0.96][3333-2-2-0.95][3333-2-2-0.96][3482-2-2-1.00][3482-2-2-1.00][3482-2-2-1.00][3536-3-0-0.73]
[3536-3-0-0.54][3536-3-3-0.59][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-0-0.99][3909-0-0-0.91][3909-0-0-0.86][4035-0-0-0.87][4035-0-0-0.87]
[4035-0-0-0.83][4140-0-0-0.99][4140-0-0-0.98][4140-0-0-0.99][4214-1-0-0.66][4214-1-2-0.36][4214-1-1-0.56][4346-1-0-1.00][4346-1-0-1.00][4346-1-0-1.00]
[4581-2-2-1.00][4581-2-2-1.00][4581-2-2-0.92][4708-3-2-1.00][4708-3-2-0.99][4708-3-2-1.00][4838-3-0-0.76][4838-3-2-0.71][4838-3-0-0.50][4845-1-2-0.78]
[4845-1-2-0.58][4845-1-2-0.69][4868-0-0-1.00][4868-0-0-1.00][4868-0-0-1.00][4939-0-2-0.53][4939-0-2-0.87][4939-0-2-0.99][4984-2-2-0.98][4984-2-2-1.00]
[4984-2-2-0.99][5078-1-2-0.86][5078-1-2-0.91][5078-1-2-0.95][5396-0-0-1.00][5396-0-0-1.00][5396-0-0-1.00][5479-1-2-0.46][5479-1-1-0.58][5479-1-1-0.48]
[5717-0-0-0.77][5717-0-0-0.98][5717-0-0-0.62][5843-1-1-0.86][5843-1-2-0.78][5843-1-1-0.63][5949-3-0-0.54][5949-3-0-0.93][5949-3-0-0.62][5987-2-1-0.51]
[5987-2-1-0.45][5987-2-1-0.49][6014-3-1-0.59][6014-3-1-0.77][6014-3-3-0.46][6033-3-0-1.00][6033-3-0-0.59][6033-3-0-0.95][6313-0-0-1.00][6313-0-0-0.99]
[6313-0-0-0.92][6421-3-3-0.99][6421-3-3-0.97][6421-3-3-1.00][6500-1-1-0.84][6500-1-1-0.89][6500-1-1-0.88][6583-3-3-0.54][6583-3-3-0.58][6583-3-3-0.66]
[6683-3-3-0.61][6683-3-3-0.65][6683-3-3-0.83][6825-2-0-0.99][6825-2-0-0.98][6825-2-0-0.93][6998-3-2-0.48][6998-3-0-0.44][6998-3-2-0.42][7049-3-3-0.66]
[7049-3-3-0.52][7049-3-0-0.56][7517-1-2-0.71][7517-1-2-0.54][7517-1-1-0.77][7521-1-2-0.63][7521-1-2-0.71][7521-1-1-0.58][7528-1-3-0.79][7528-1-2-0.75]
[7528-1-2-0.90][7949-1-2-1.00][7949-1-2-1.00][7949-1-2-1.00][8135-1-0-0.99][8135-1-2-0.73][8135-1-0-0.91][8185-3-0-1.00][8185-3-0-0.98][8185-3-0-1.00]
[8269-3-2-0.66][8269-3-2-0.65][8269-3-1-0.54][8273-3-3-0.96][8273-3-3-0.89][8273-3-3-0.87][8543-3-0-0.90][8543-3-0-1.00][8543-3-0-1.00][8666-1-1-0.86]
[8666-1-1-0.87][8666-1-1-0.81][8672-0-0-1.00][8672-0-0-0.96][8672-0-0-0.74][8903-1-0-0.66][8903-1-2-0.54][8903-1-0-0.53][9001-2-0-0.87][9001-2-1-0.94]
[9001-2-1-0.93][9036-2-2-1.00][9036-2-2-1.00][9036-2-2-1.00][9281-3-1-0.38][9281-3-2-0.69][9281-3-2-0.56][9300-2-2-1.00][9300-2-2-1.00][9300-2-2-1.00]
[9571-0-3-0.61][9571-0-3-0.43][9571-0-0-0.40][9617-1-1-0.90][9617-1-1-0.50][9617-1-2-0.55][9644-2-2-0.99][9644-2-2-1.00][9644-2-2-0.98][9705-2-0-0.93]
[9705-2-0-0.84][9705-2-0-0.79][9801-0-0-0.77][9801-0-0-0.62][9801-0-0-0.51][9803-3-0-0.81][9803-3-3-0.67][9803-3-3-0.72][9865-3-3-0.76][9865-3-0-0.69]
[9865-3-0-0.95][9896-2-2-0.93][9896-2-2-0.94][9896-2-2-0.88][10314-1-2-0.68][10314-1-2-0.48][10314-1-2-0.66][10337-3-3-1.00][10337-3-3-0.98][10337-3-3-0.95]
[10403-0-0-0.81][10403-0-0-0.85][10403-0-0-0.89][10653-2-1-0.68][10653-2-2-0.57][10653-2-2-0.56][10704-2-2-1.00][10704-2-2-0.99][10704-2-2-0.99][10719-1-1-0.65]
[10719-1-1-0.83][10719-1-1-0.77][10727-1-1-0.55][10727-1-1-0.57][10727-1-1-0.65][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-3-0.88][10969-2-3-0.92]
[10969-2-3-0.87][11042-0-0-0.97][11042-0-0-0.94][11042-0-0-1.00][11088-1-2-0.62][11088-1-1-0.73][11088-1-1-0.88][11322-0-0-1.00][11322-0-0-1.00][11322-0-0-1.00]
[11398-2-2-1.00][11398-2-2-0.75][11398-2-0-0.71][11499-0-0-0.87][11499-0-0-0.86][11499-0-0-0.88][11502-3-0-0.83][11502-3-0-0.51][11502-3-0-0.98][11512-3-3-0.61]
[11512-3-3-0.56][11512-3-3-0.52][11608-1-2-0.98][11608-1-2-0.99][11608-1-2-0.80][11610-0-0-0.97][11610-0-0-0.99][11610-0-0-0.99][11692-0-0-1.00][11692-0-0-1.00]
[11692-0-0-1.00][11905-0-0-1.00][11905-0-0-0.97][11905-0-0-0.95][11993-1-2-0.81][11993-1-2-0.73][11993-1-2-0.69][12002-2-0-0.88][12002-2-2-0.56][12002-2-3-0.75]
[12052-0-0-0.75][12052-0-0-1.00][12052-0-0-0.98][12201-0-0-0.94][12201-0-0-0.70][12201-0-0-0.75][12235-2-2-1.00][12235-2-2-0.90][12235-2-2-0.99][12320-1-2-0.53]
[12320-1-2-0.63][12320-1-0-0.76][12377-2-2-0.45][12377-2-2-0.59][12377-2-2-0.89][12398-2-0-0.42][12398-2-1-0.45][12398-2-1-0.57][12503-1-2-0.96][12503-1-2-0.87]
[12503-1-2-0.73][12617-0-2-0.96][12617-0-2-0.98][12617-0-2-0.98][12685-3-1-0.58][12685-3-1-0.79][12685-3-1-0.69][12738-2-0-0.46][12738-2-0-0.68][12738-2-0-0.76]
[12742-2-2-1.00][12742-2-2-0.99][12742-2-2-1.00][12823-0-0-0.56][12823-0-0-0.70][12823-0-0-0.78][13110-1-1-0.87][13110-1-1-0.61][13110-1-2-0.87][13240-3-3-0.58]
[13240-3-3-0.53][13240-3-0-0.63][13253-1-2-0.58][13253-1-2-0.52][13253-1-1-0.52][13273-0-0-1.00][13273-0-0-1.00][13273-0-0-1.00][13634-1-2-0.88][13634-1-2-0.88]
[13634-1-2-0.88][13763-2-2-0.59][13763-2-2-0.92][13763-2-2-0.56][13905-3-0-0.91][13905-3-0-0.89][13905-3-0-0.77][14060-2-1-0.91][14060-2-1-0.90][14060-2-1-0.85]
[14065-3-0-0.81][14065-3-0-0.96][14065-3-0-0.85][14147-3-3-0.52][14147-3-3-0.63][14147-3-0-0.60][14595-2-2-0.84][14595-2-2-0.84][14595-2-2-0.87][14687-2-2-1.00]
[14687-2-2-1.00][14687-2-2-1.00][14788-2-2-0.95][14788-2-2-0.88][14788-2-2-0.99][14869-1-1-0.68][14869-1-1-0.67][14869-1-1-0.53][14872-3-0-0.57][14872-3-0-0.99]
[14872-3-0-0.72][14877-1-1-0.80][14877-1-1-0.86][14877-1-1-0.91][14927-0-0-0.91][14927-0-0-0.83][14927-0-0-0.69][15066-0-0-1.00][15066-0-0-1.00][15066-0-0-1.00]
[15175-1-2-0.61][15175-1-2-0.52][15175-1-2-0.64][15178-2-0-0.66][15178-2-2-0.63][15178-2-2-0.69][15375-3-0-0.97][15375-3-3-0.53][15375-3-1-0.49][15389-3-3-0.96]
[15389-3-3-0.99][15389-3-3-0.98][15568-2-1-0.56][15568-2-1-0.97][15568-2-1-0.99][15675-3-3-1.00][15675-3-3-1.00][15675-3-3-1.00][15869-1-2-0.99][15869-1-2-0.98]
[15869-1-2-0.88][16207-3-0-0.82][16207-3-0-0.82][16207-3-0-0.93][16236-0-0-0.64][16236-0-0-0.61][16236-0-0-0.80][16302-3-0-0.96][16302-3-0-0.93][16302-3-0-0.94]
[16331-2-2-1.00][16331-2-2-1.00][16331-2-2-1.00][16381-0-0-0.53][16381-0-0-0.46][16381-0-3-0.51][16488-1-1-0.89][16488-1-1-0.97][16488-1-1-0.90][16495-0-0-0.82]
[16495-0-0-1.00][16495-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16719-1-2-0.61][16719-1-2-0.81][16719-1-2-0.58][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.97][16828-0-0-0.99][16828-0-0-0.99][17137-3-0-0.99][17137-3-0-0.55][17137-3-0-0.99][17245-1-2-0.93][17245-1-2-0.86][17245-1-2-0.54]
[17278-3-0-0.57][17278-3-0-0.42][17278-3-0-0.93][17282-0-0-0.84][17282-0-0-0.86][17282-0-0-0.61][17311-2-2-1.00][17311-2-2-1.00][17311-2-2-1.00][17336-2-1-0.68]
[17336-2-2-0.79][17336-2-2-0.84][17608-3-3-0.67][17608-3-3-0.93][17608-3-3-0.88][17627-0-0-0.97][17627-0-0-0.58][17627-0-3-0.52][17877-3-2-0.62][17877-3-0-0.35]
[17877-3-0-0.52][17924-1-0-0.45][17924-1-0-0.68][17924-1-0-0.50][17984-3-0-1.00][17984-3-0-1.00][17984-3-0-1.00][18211-0-3-0.42][18211-0-3-0.76][18211-0-3-0.78]
[18276-3-3-0.59][18276-3-0-0.54][18276-3-3-0.82][18287-1-0-0.44][18287-1-1-0.55][18287-1-1-0.42][18394-0-0-1.00][18394-0-0-1.00][18394-0-0-1.00][18428-0-0-0.95]
[18428-0-0-0.79][18428-0-0-1.00][18442-0-3-0.97][18442-0-3-0.90][18442-0-3-0.94][18478-3-0-0.78][18478-3-0-0.68][18478-3-0-0.69][18607-0-0-1.00][18607-0-0-1.00]
[18607-0-0-1.00][18616-0-0-0.60][18616-0-0-0.71][18616-0-0-0.84][18663-0-0-0.64][18663-0-0-0.61][18663-0-0-0.72][18718-0-0-1.00][18718-0-0-1.00][18718-0-0-1.00]
[18766-2-2-0.96][18766-2-2-0.90][18766-2-2-0.93][18824-2-2-1.00][18824-2-2-0.97][18824-2-2-0.86][18890-3-2-0.57][18890-3-2-0.78][18890-3-2-0.82][18930-3-2-0.92]
[18930-3-2-0.86][18930-3-2-0.83][18938-3-3-0.49][18938-3-3-0.56][18938-3-2-0.47][19817-1-2-0.63][19817-1-2-0.53][19817-1-2-0.71][19839-0-0-1.00][19839-0-0-0.99]
[19839-0-0-1.00][19930-3-0-0.36][19930-3-3-0.51][19930-3-3-0.64][19944-0-0-0.64][19944-0-0-0.58][19944-0-0-0.85][20036-2-2-1.00][20036-2-2-1.00][20036-2-2-1.00]
[20101-3-1-0.80][20101-3-3-0.47][20101-3-0-0.45][20474-1-1-0.73][20474-1-1-0.56][20474-1-1-0.68][20547-3-2-0.62][20547-3-0-0.84][20547-3-0-0.99][20929-2-2-0.54]
[20929-2-2-1.00][20929-2-2-1.00][21245-1-1-0.78][21245-1-2-0.75][21245-1-2-0.73][21257-3-2-0.84][21257-3-2-0.80][21257-3-2-0.81][21293-1-2-0.65][21293-1-2-0.61]
[21293-1-2-0.64][21316-1-1-0.99][21316-1-1-0.99][21316-1-0-0.55][21384-1-2-0.90][21384-1-2-0.90][21384-1-2-0.88][21448-1-1-0.76][21448-1-2-0.58][21448-1-2-0.55]
[21483-0-0-1.00][21483-0-0-1.00][21483-0-0-1.00][21487-2-2-0.99][21487-2-2-0.95][21487-2-2-0.94][21714-0-0-1.00][21714-0-0-1.00][21714-0-0-1.00][21943-3-2-0.99]
[21943-3-2-1.00][21943-3-2-1.00][21947-0-0-1.00][21947-0-0-0.81][21947-0-0-0.80][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-1.00][21965-2-2-1.00]
[21965-2-2-0.75][21998-1-2-0.71][21998-1-1-0.76][21998-1-1-0.51][22025-0-2-0.98][22025-0-2-0.94][22025-0-2-0.91][22228-3-3-0.99][22228-3-3-0.99][22228-3-3-0.98]
[22446-1-1-0.97][22446-1-1-0.97][22446-1-1-0.98][22494-3-0-0.93][22494-3-0-0.71][22494-3-0-0.66][22757-0-0-0.83][22757-0-0-0.99][22757-0-0-0.99][22811-3-3-0.46]
[22811-3-3-1.00][22811-3-3-1.00][22976-3-2-0.91][22976-3-2-0.98][22976-3-1-0.74][22985-3-3-0.94][22985-3-3-0.99][22985-3-3-0.79][23014-0-0-0.65][23014-0-0-0.92]
[23014-0-0-0.90][23112-1-1-0.56][23112-1-2-0.56][23112-1-2-0.58][23144-3-3-0.87][23144-3-3-0.80][23144-3-0-0.74][23168-2-0-0.98][23168-2-0-0.91][23168-2-0-0.91]
[23219-0-0-0.50][23219-0-0-0.81][23219-0-0-0.78][23363-3-3-0.72][23363-3-3-0.89][23363-3-3-0.98][23470-0-0-0.88][23470-0-0-0.76][23470-0-0-0.60][23486-2-2-0.94]
[23486-2-2-0.74][23486-2-2-0.79][23497-0-0-0.51][23497-0-0-0.60][23497-0-0-0.61][23516-0-0-0.93][23516-0-0-1.00][23516-0-0-1.00][23690-1-1-0.94][23690-1-2-0.85]
[23690-1-2-0.93][23921-2-2-0.53][23921-2-2-0.81][23921-2-2-0.98][23936-1-2-0.99][23936-1-2-1.00][23936-1-2-0.98][24040-3-0-0.65][24040-3-0-0.75][24040-3-0-0.63]
[24111-1-2-0.82][24111-1-2-0.83][24111-1-2-0.82][24182-0-0-1.00][24182-0-0-1.00][24182-0-0-1.00][24238-3-3-0.60][24238-3-0-0.50][24238-3-0-0.52][24290-2-0-1.00]
[24290-2-0-0.96][24290-2-0-0.97][24345-0-0-0.99][24345-0-0-0.95][24345-0-0-0.60][24364-1-2-0.95][24364-1-2-0.98][24364-1-2-0.98][24427-3-0-0.93][24427-3-0-0.99]
[24427-3-0-0.98][24477-2-2-0.93][24477-2-2-0.99][24477-2-2-0.76][24495-2-1-0.69][24495-2-2-0.48][24495-2-1-0.75][24893-2-2-1.00][24893-2-2-0.98][24893-2-2-0.96]
[25012-1-2-0.95][25012-1-2-0.95][25012-1-2-0.88][25121-2-2-0.85][25121-2-2-0.86][25121-2-2-0.89][25165-3-3-0.93][25165-3-3-0.95][25165-3-3-0.93][25183-0-0-1.00]
[25183-0-0-1.00][25183-0-0-1.00][25297-3-3-0.99][25297-3-3-0.94][25297-3-3-0.97][25398-0-0-0.88][25398-0-0-1.00][25398-0-0-0.99][25574-2-2-0.95][25574-2-2-0.99]
[25574-2-2-1.00][25644-1-2-0.96][25644-1-2-0.87][25644-1-2-0.63][25718-1-1-0.68][25718-1-1-0.47][25718-1-0-0.36][25774-2-2-0.98][25774-2-2-0.96][25774-2-2-0.99]
[26032-3-0-0.93][26032-3-0-0.79][26032-3-0-0.89][26051-3-3-0.94][26051-3-3-0.99][26051-3-3-1.00][26120-0-0-0.66][26120-0-0-0.71][26120-0-0-1.00][26321-1-1-0.73]
[26321-1-1-0.69][26321-1-2-0.91][26732-1-1-0.75][26732-1-1-0.94][26732-1-1-0.80][26784-3-3-0.96][26784-3-3-0.99][26784-3-3-0.98][26827-3-3-0.99][26827-3-3-0.99]
[26827-3-3-0.99][26833-0-3-0.99][26833-0-3-0.81][26833-0-0-0.65][26838-2-1-0.39][26838-2-2-0.43][26838-2-2-0.52][26860-1-2-0.44][26860-1-2-0.49][26860-1-2-0.76]
[26948-0-0-1.00][26948-0-0-0.90][26948-0-0-0.97][27049-3-0-1.00][27049-3-0-0.98][27049-3-0-1.00][27098-1-0-0.59][27098-1-0-0.75][27098-1-0-0.89][27526-0-0-1.00]
[27526-0-0-1.00][27526-0-0-0.83][27639-3-3-0.72][27639-3-3-0.89][27639-3-3-0.83][27698-3-3-0.93][27698-3-3-0.97][27698-3-3-0.97][27772-0-0-1.00][27772-0-0-1.00]
[27772-0-0-0.75][27890-1-1-0.66][27890-1-1-0.72][27890-1-1-0.69][28040-0-2-0.71][28040-0-0-0.60][28040-0-0-0.96][28503-2-2-1.00][28503-2-2-1.00][28503-2-2-1.00]
[28577-1-2-0.81][28577-1-2-0.76][28577-1-2-0.75][28959-0-0-1.00][28959-0-0-1.00][28959-0-0-1.00][29198-3-2-0.41][29198-3-1-0.39][29198-3-1-0.40][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-2-0.90][29877-2-2-0.87][29877-2-2-0.51][30035-1-2-0.66][30035-1-2-0.97][30035-1-2-0.91][30098-0-0-0.93][30098-0-0-0.97]
[30098-0-0-0.95][30326-1-1-1.00][30326-1-1-0.99][30326-1-1-0.82][30572-2-2-0.89][30572-2-2-0.93][30572-2-2-0.94][30716-0-0-0.32][30716-0-0-0.41][30716-0-0-0.60]
[30806-2-2-0.65][30806-2-2-0.65][30806-2-2-0.77][30906-1-1-0.90][30906-1-1-0.76][30906-1-1-0.60][31007-0-0-0.99][31007-0-0-1.00][31007-0-0-0.87][31181-3-0-0.87]
[31181-3-0-0.62][31181-3-0-0.37][31238-0-3-0.76][31238-0-3-0.99][31238-0-3-0.88][31347-0-0-1.00][31347-0-0-1.00][31347-0-0-1.00][31422-2-2-0.91][31422-2-2-0.97]
[31422-2-2-0.72][31429-3-3-0.59][31429-3-3-0.53][31429-3-3-0.63][31431-0-0-0.96][31431-0-0-0.55][31431-0-0-0.80][31432-1-1-0.72][31432-1-1-0.81][31432-1-1-0.85]
[31477-0-0-0.98][31477-0-0-1.00][31477-0-0-0.92][31524-1-3-0.55][31524-1-2-0.47][31524-1-0-0.55][31597-1-2-1.00][31597-1-2-0.99][31597-1-2-1.00][31619-1-0-0.97]
[31619-1-0-1.00][31619-1-0-0.99][31701-0-0-1.00][31701-0-0-1.00][31701-0-0-1.00][31755-0-0-1.00][31755-0-0-1.00][31755-0-0-1.00][31854-3-3-0.75][31854-3-3-0.97]
[31854-3-3-0.66][32074-1-2-0.94][32074-1-2-0.53][32074-1-3-0.60][32078-3-0-0.77][32078-3-0-0.83][32078-3-0-0.94][32111-1-0-0.39][32111-1-1-0.61][32111-1-1-0.72]
[32127-1-2-1.00][32127-1-2-1.00][32127-1-2-1.00][32140-3-3-1.00][32140-3-3-1.00][32140-3-3-1.00][32263-2-0-0.53][32263-2-2-0.44][32263-2-0-0.62][32365-0-0-0.97]
[32365-0-0-0.96][32365-0-0-1.00][32411-2-0-0.98][32411-2-0-0.90][32411-2-0-0.93][32429-3-0-1.00][32429-3-0-0.99][32429-3-0-1.00][32473-3-0-0.85][32473-3-0-0.78]
[32473-3-0-0.77][32574-3-3-0.61][32574-3-0-0.61][32574-3-0-0.64][32584-0-3-0.78][32584-0-3-0.81][32584-0-3-0.77][32622-0-0-0.66][32622-0-0-0.44][32622-0-1-0.39]
[32858-3-0-1.00][32858-3-0-1.00][32858-3-0-1.00][32969-3-0-0.66][32969-3-0-0.62][32969-3-0-0.68][33016-2-2-0.89][33016-2-2-1.00][33016-2-2-1.00][33031-1-0-0.99]
[33031-1-0-0.95][33031-1-0-0.96][33035-2-2-0.99][33035-2-2-1.00][33035-2-2-1.00][33133-2-2-0.63][33133-2-2-0.69][33133-2-2-0.59][33173-2-2-0.60][33173-2-2-0.62]
[33173-2-2-0.83][33175-3-2-1.00][33175-3-2-1.00][33175-3-2-1.00][33306-3-1-0.72][33306-3-1-0.77][33306-3-1-0.73][33309-2-0-0.56][33309-2-0-0.64][33309-2-0-0.57]
[33474-0-0-0.60][33474-0-0-0.83][33474-0-0-0.94][33478-2-0-0.52][33478-2-2-0.61][33478-2-0-0.40][33618-1-1-0.86][33618-1-1-0.81][33618-1-1-0.80][33712-0-0-0.94]
[33712-0-0-0.91][33712-0-0-0.99][33782-2-2-0.62][33782-2-2-0.99][33782-2-2-0.93][33914-3-2-0.99][33914-3-3-0.91][33914-3-3-0.98][34076-3-3-0.36][34076-3-3-0.42]
[34076-3-3-0.46][34112-2-2-1.00][34112-2-2-0.97][34112-2-2-0.93][34138-2-2-0.75][34138-2-2-0.76][34138-2-2-0.81][34239-1-1-0.50][34239-1-2-0.59][34239-1-1-0.53]
[34364-2-2-0.75][34364-2-2-0.98][34364-2-2-0.99][34617-1-2-0.91][34617-1-2-0.98][34617-1-2-0.75][34751-3-3-0.99][34751-3-3-0.98][34751-3-3-0.99][34783-2-2-0.80]
[34783-2-2-0.91][34783-2-2-0.79][35015-3-2-0.64][35015-3-2-0.64][35015-3-2-0.55][35018-1-2-0.73][35018-1-1-0.63][35018-1-1-0.50][35288-2-3-0.45][35288-2-1-0.59]
[35288-2-2-0.49]
---------------------------
I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 1.000 | Acc: 76.625% | Wgt Acc: 76.126%
	I - Batch: 100 | Loss: 0.998 | Acc: 76.562% | Wgt Acc: 76.249%
	I - Batch: 150 | Loss: 0.986 | Acc: 77.792% | Wgt Acc: 77.545%
	I - Batch: 200 | Loss: 0.989 | Acc: 77.531% | Wgt Acc: 77.280%
	I - Batch: 250 | Loss: 0.989 | Acc: 77.800% | Wgt Acc: 77.531%
	I - Batch: 300 | Loss: 0.986 | Acc: 78.250% | Wgt Acc: 77.980%
	I - Batch: 350 | Loss: 0.982 | Acc: 78.768% | Wgt Acc: 78.514%
	I - Batch: 400 | Loss: 0.978 | Acc: 79.219% | Wgt Acc: 79.013%
	I - Batch: 450 | Loss: 0.980 | Acc: 78.806% | Wgt Acc: 78.588%
I - num batch: 478
I - Train -- Loss: 0.980 | Acc: 78.903% | Wgt Acc: 78.701% | LR: 1.000000e-03 | Dur: 349.83s
I - Confusion Matrix: [row->prediction - col->label]
[[1678.   59.   69.  252.]
 [  90. 1413.  225.   93.]
 [  94.  204. 1778.  109.]
 [ 229.   58.  130. 1160.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.110 | Acc: 63.000% | Wgt Acc: 63.659%
I - num batch: 62
I - Val -- Loss: 1.106 | Acc: 63.405% | Wgt Acc: 64.153% | Dur: 37.74s
I - Confusion Matrix: [row->prediction - col->label]
[[174.  12.  17.  39.]
 [ 22. 178.  82.  34.]
 [  9.  31. 103.  18.]
 [ 59.  13.  23. 167.]]

I - Local maximum validation set accuracy:  63.40

I - Validation set results: 
[14-1-2-0.55][14-1-1-0.78][14-1-1-0.79][50-3-1-0.73][50-3-1-0.93][50-3-1-0.82][124-2-2-0.49][124-2-2-0.51][124-2-1-0.51][127-0-0-1.00]
[127-0-0-1.00][127-0-0-1.00][443-2-2-1.00][443-2-2-0.99][443-2-2-0.98][567-0-0-0.96][567-0-0-0.95][567-0-0-0.98][573-1-1-0.45][573-1-1-0.94]
[573-1-1-0.83][615-0-0-0.67][615-0-0-0.58][615-0-0-0.64][695-1-2-0.94][695-1-2-0.97][695-1-2-0.94][722-3-0-0.81][722-3-3-0.54][722-3-0-0.68]
[826-0-0-0.92][826-0-0-1.00][826-0-0-0.99][878-0-0-0.83][878-0-0-0.97][878-0-0-1.00][1103-0-0-0.69][1103-0-0-0.52][1103-0-0-0.58][1212-3-3-0.72]
[1212-3-3-0.80][1212-3-3-0.55][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-3-0.92][2181-2-3-0.89][2181-2-3-0.91][2476-2-1-0.99][2476-2-1-0.99]
[2476-2-1-0.99][2721-2-2-0.76][2721-2-2-0.86][2721-2-2-0.84][2818-1-1-0.74][2818-1-1-0.91][2818-1-1-0.96][2886-2-1-0.77][2886-2-1-0.58][2886-2-1-0.70]
[3231-2-2-0.91][3231-2-2-0.87][3231-2-2-0.78][3333-2-1-0.96][3333-2-1-0.96][3333-2-1-0.72][3482-2-2-0.89][3482-2-2-0.86][3482-2-2-0.96][3536-3-3-0.92]
[3536-3-3-0.98][3536-3-3-1.00][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-1-0.66][3909-0-1-0.46][3909-0-3-0.58][4035-0-3-0.91][4035-0-3-0.92]
[4035-0-3-0.95][4140-0-0-0.96][4140-0-0-0.79][4140-0-0-0.78][4214-1-3-0.80][4214-1-1-0.56][4214-1-1-1.00][4346-1-0-0.93][4346-1-0-0.89][4346-1-0-0.90]
[4581-2-2-0.99][4581-2-2-0.96][4581-2-2-0.85][4708-3-1-0.57][4708-3-1-0.81][4708-3-1-0.88][4838-3-0-0.73][4838-3-0-0.90][4838-3-0-0.71][4845-1-1-0.72]
[4845-1-1-0.74][4845-1-1-0.81][4868-0-0-1.00][4868-0-0-1.00][4868-0-0-0.74][4939-0-2-0.35][4939-0-1-0.58][4939-0-2-0.81][4984-2-2-0.96][4984-2-2-0.98]
[4984-2-2-0.89][5078-1-1-0.62][5078-1-2-0.40][5078-1-2-0.58][5396-0-0-1.00][5396-0-0-1.00][5396-0-0-1.00][5479-1-1-1.00][5479-1-1-1.00][5479-1-1-0.99]
[5717-0-0-0.57][5717-0-0-0.79][5717-0-1-0.71][5843-1-1-0.89][5843-1-1-0.81][5843-1-1-0.99][5949-3-3-0.49][5949-3-0-0.75][5949-3-3-0.61][5987-2-1-0.84]
[5987-2-1-0.89][5987-2-1-0.90][6014-3-1-0.88][6014-3-1-0.90][6014-3-1-0.76][6033-3-0-0.99][6033-3-0-0.90][6033-3-0-0.98][6313-0-0-0.98][6313-0-0-0.81]
[6313-0-3-0.53][6421-3-3-1.00][6421-3-3-1.00][6421-3-3-1.00][6500-1-1-0.92][6500-1-1-0.94][6500-1-1-0.93][6583-3-3-0.97][6583-3-3-0.97][6583-3-3-0.97]
[6683-3-3-0.83][6683-3-3-0.92][6683-3-3-0.99][6825-2-1-0.92][6825-2-1-0.84][6825-2-1-0.84][6998-3-3-0.47][6998-3-3-0.88][6998-3-3-0.62][7049-3-3-0.75]
[7049-3-3-0.71][7049-3-3-0.93][7517-1-2-0.56][7517-1-1-0.62][7517-1-1-0.94][7521-1-1-0.98][7521-1-1-1.00][7521-1-1-1.00][7528-1-3-0.81][7528-1-2-0.72]
[7528-1-2-0.83][7949-1-2-0.95][7949-1-2-0.99][7949-1-2-0.80][8135-1-0-1.00][8135-1-0-0.97][8135-1-0-1.00][8185-3-0-0.62][8185-3-3-0.53][8185-3-0-0.82]
[8269-3-2-0.63][8269-3-1-0.50][8269-3-1-0.95][8273-3-3-1.00][8273-3-3-1.00][8273-3-3-1.00][8543-3-0-0.64][8543-3-0-1.00][8543-3-0-1.00][8666-1-1-0.90]
[8666-1-1-0.99][8666-1-1-0.98][8672-0-0-1.00][8672-0-0-0.97][8672-0-3-0.53][8903-1-1-0.57][8903-1-2-0.73][8903-1-2-0.74][9001-2-0-0.66][9001-2-1-1.00]
[9001-2-1-1.00][9036-2-2-1.00][9036-2-2-0.99][9036-2-2-0.97][9281-3-1-0.69][9281-3-1-0.83][9281-3-1-0.83][9300-2-1-0.73][9300-2-2-0.90][9300-2-1-0.79]
[9571-0-3-0.80][9571-0-3-0.57][9571-0-3-0.53][9617-1-1-1.00][9617-1-1-0.72][9617-1-1-0.76][9644-2-1-0.95][9644-2-1-0.50][9644-2-1-0.62][9705-2-1-0.78]
[9705-2-1-0.62][9705-2-1-0.70][9801-0-3-0.87][9801-0-3-0.93][9801-0-3-0.99][9803-3-0-0.77][9803-3-3-0.61][9803-3-3-0.58][9865-3-3-1.00][9865-3-3-0.99]
[9865-3-3-0.81][9896-2-2-0.67][9896-2-1-0.51][9896-2-2-0.67][10314-1-1-0.98][10314-1-1-0.95][10314-1-1-0.76][10337-3-3-1.00][10337-3-3-1.00][10337-3-3-1.00]
[10403-0-0-0.39][10403-0-0-0.49][10403-0-0-0.54][10653-2-1-0.94][10653-2-1-0.83][10653-2-1-0.65][10704-2-3-0.63][10704-2-3-0.58][10704-2-3-0.62][10719-1-1-0.87]
[10719-1-1-0.99][10719-1-1-1.00][10727-1-1-1.00][10727-1-1-1.00][10727-1-1-1.00][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-3-0.78][10969-2-3-0.96]
[10969-2-3-0.88][11042-0-0-0.88][11042-0-0-0.77][11042-0-0-0.99][11088-1-1-0.96][11088-1-1-0.96][11088-1-1-0.98][11322-0-0-1.00][11322-0-0-0.99][11322-0-0-0.95]
[11398-2-2-1.00][11398-2-2-0.85][11398-2-2-0.59][11499-0-3-0.59][11499-0-0-0.65][11499-0-0-0.45][11502-3-3-0.50][11502-3-3-0.54][11502-3-0-0.61][11512-3-3-0.53]
[11512-3-1-0.55][11512-3-1-0.64][11608-1-2-0.76][11608-1-2-0.64][11608-1-2-0.61][11610-0-3-0.66][11610-0-3-0.91][11610-0-3-0.48][11692-0-0-0.86][11692-0-0-0.75]
[11692-0-0-0.84][11905-0-0-0.92][11905-0-0-0.63][11905-0-0-0.68][11993-1-1-0.99][11993-1-1-0.99][11993-1-1-1.00][12002-2-0-0.78][12002-2-1-0.62][12002-2-3-0.76]
[12052-0-0-0.62][12052-0-0-0.90][12052-0-0-0.58][12201-0-3-0.51][12201-0-3-0.86][12201-0-3-0.59][12235-2-2-0.59][12235-2-1-0.73][12235-2-2-0.56][12320-1-0-0.73]
[12320-1-0-0.72][12320-1-0-0.64][12377-2-2-0.49][12377-2-1-0.54][12377-2-2-0.60][12398-2-3-0.98][12398-2-3-0.91][12398-2-3-0.88][12503-1-1-1.00][12503-1-1-0.76]
[12503-1-2-0.50][12617-0-2-0.50][12617-0-2-0.68][12617-0-3-0.44][12685-3-3-0.52][12685-3-3-0.76][12685-3-3-0.83][12738-2-1-0.53][12738-2-1-0.43][12738-2-1-0.60]
[12742-2-2-0.80][12742-2-2-0.71][12742-2-2-0.89][12823-0-3-0.82][12823-0-3-0.87][12823-0-3-0.74][13110-1-1-1.00][13110-1-1-0.87][13110-1-1-0.59][13240-3-3-0.98]
[13240-3-3-0.98][13240-3-3-0.98][13253-1-1-0.86][13253-1-1-0.82][13253-1-1-0.80][13273-0-0-1.00][13273-0-0-1.00][13273-0-0-1.00][13634-1-2-0.99][13634-1-2-0.99]
[13634-1-2-0.98][13763-2-3-0.76][13763-2-3-0.58][13763-2-3-0.62][13905-3-3-0.99][13905-3-3-0.99][13905-3-3-0.99][14060-2-1-1.00][14060-2-1-1.00][14060-2-1-1.00]
[14065-3-0-0.65][14065-3-0-0.85][14065-3-3-0.47][14147-3-3-0.71][14147-3-3-0.72][14147-3-3-0.61][14595-2-1-0.78][14595-2-1-0.73][14595-2-1-0.74][14687-2-2-1.00]
[14687-2-2-1.00][14687-2-2-1.00][14788-2-2-0.98][14788-2-2-0.50][14788-2-2-0.88][14869-1-1-0.88][14869-1-1-0.81][14869-1-1-0.71][14872-3-0-0.49][14872-3-0-0.90]
[14872-3-0-0.70][14877-1-1-0.94][14877-1-1-0.98][14877-1-1-1.00][14927-0-3-0.99][14927-0-3-0.99][14927-0-3-0.98][15066-0-0-0.55][15066-0-3-0.63][15066-0-3-0.64]
[15175-1-1-0.80][15175-1-1-0.82][15175-1-1-0.91][15178-2-0-0.89][15178-2-2-0.61][15178-2-2-0.61][15375-3-0-0.54][15375-3-3-0.93][15375-3-1-0.60][15389-3-3-0.81]
[15389-3-3-0.95][15389-3-3-0.83][15568-2-1-0.69][15568-2-1-1.00][15568-2-1-1.00][15675-3-3-0.90][15675-3-3-0.98][15675-3-3-0.75][15869-1-1-0.97][15869-1-1-0.96]
[15869-1-3-0.52][16207-3-0-0.41][16207-3-3-0.63][16207-3-0-0.45][16236-0-0-0.53][16236-0-0-0.49][16236-0-3-0.45][16302-3-0-0.85][16302-3-3-0.54][16302-3-0-0.52]
[16331-2-2-1.00][16331-2-2-1.00][16331-2-2-1.00][16381-0-0-0.90][16381-0-0-0.74][16381-0-0-0.63][16488-1-1-1.00][16488-1-1-1.00][16488-1-1-1.00][16495-0-0-0.54]
[16495-0-0-1.00][16495-0-0-0.89][16650-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16719-1-1-0.81][16719-1-1-0.87][16719-1-1-0.99][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.87][16828-0-0-0.95][16828-0-0-0.84][17137-3-0-0.69][17137-3-3-0.72][17137-3-3-0.58][17245-1-1-0.47][17245-1-1-0.57][17245-1-1-0.45]
[17278-3-0-0.51][17278-3-0-0.37][17278-3-0-0.57][17282-0-0-0.58][17282-0-1-0.35][17282-0-1-0.50][17311-2-2-0.81][17311-2-2-0.75][17311-2-2-0.80][17336-2-1-0.99]
[17336-2-1-0.96][17336-2-1-0.85][17608-3-3-1.00][17608-3-3-1.00][17608-3-3-0.99][17627-0-0-0.47][17627-0-1-0.61][17627-0-3-0.37][17877-3-1-0.99][17877-3-1-0.67]
[17877-3-0-0.53][17924-1-1-0.38][17924-1-3-0.55][17924-1-1-0.46][17984-3-3-0.99][17984-3-3-0.95][17984-3-3-0.98][18211-0-3-0.48][18211-0-3-0.95][18211-0-3-0.80]
[18276-3-3-0.61][18276-3-3-0.69][18276-3-3-0.93][18287-1-1-0.47][18287-1-1-0.98][18287-1-1-0.99][18394-0-0-0.99][18394-0-0-0.93][18394-0-0-0.92][18428-0-1-0.43]
[18428-0-0-0.49][18428-0-0-0.98][18442-0-3-1.00][18442-0-3-1.00][18442-0-3-1.00][18478-3-3-0.98][18478-3-3-0.99][18478-3-3-0.99][18607-0-0-1.00][18607-0-0-1.00]
[18607-0-0-1.00][18616-0-0-0.66][18616-0-0-0.87][18616-0-0-0.80][18663-0-3-0.51][18663-0-3-0.57][18663-0-0-0.44][18718-0-0-0.99][18718-0-0-0.99][18718-0-0-1.00]
[18766-2-2-0.65][18766-2-2-0.52][18766-2-1-0.71][18824-2-2-0.98][18824-2-2-0.65][18824-2-2-0.73][18890-3-1-0.58][18890-3-1-0.49][18890-3-1-0.34][18930-3-2-0.78]
[18930-3-2-0.77][18930-3-2-0.68][18938-3-3-0.99][18938-3-3-0.99][18938-3-3-0.99][19817-1-1-0.55][19817-1-1-0.85][19817-1-1-0.67][19839-0-1-0.91][19839-0-1-0.97]
[19839-0-1-0.92][19930-3-3-0.91][19930-3-3-0.91][19930-3-3-0.99][19944-0-0-0.63][19944-0-1-0.79][19944-0-1-0.81][20036-2-2-1.00][20036-2-2-0.98][20036-2-2-1.00]
[20101-3-1-0.91][20101-3-3-0.91][20101-3-3-0.96][20474-1-1-1.00][20474-1-1-1.00][20474-1-1-1.00][20547-3-2-0.40][20547-3-0-0.70][20547-3-3-0.65][20929-2-2-0.63]
[20929-2-2-0.93][20929-2-2-0.92][21245-1-1-1.00][21245-1-1-1.00][21245-1-1-0.96][21257-3-2-0.76][21257-3-3-0.60][21257-3-2-0.72][21293-1-1-0.88][21293-1-1-0.88]
[21293-1-1-0.85][21316-1-1-1.00][21316-1-1-1.00][21316-1-3-0.58][21384-1-2-0.88][21384-1-2-0.91][21384-1-2-0.87][21448-1-1-0.93][21448-1-1-0.86][21448-1-1-0.94]
[21483-0-0-0.98][21483-0-0-0.93][21483-0-0-0.98][21487-2-2-0.92][21487-2-2-0.90][21487-2-2-0.97][21714-0-0-0.89][21714-0-0-0.90][21714-0-0-0.89][21943-3-2-0.90]
[21943-3-2-0.98][21943-3-2-1.00][21947-0-0-0.89][21947-0-0-0.79][21947-0-3-0.47][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-0.96][21965-2-2-1.00]
[21965-2-1-0.92][21998-1-1-0.97][21998-1-1-1.00][21998-1-1-1.00][22025-0-2-0.55][22025-0-3-0.60][22025-0-2-0.32][22228-3-3-1.00][22228-3-3-1.00][22228-3-3-1.00]
[22446-1-1-1.00][22446-1-1-1.00][22446-1-1-1.00][22494-3-0-0.54][22494-3-3-0.85][22494-3-3-0.90][22757-0-3-1.00][22757-0-3-0.98][22757-0-3-0.94][22811-3-3-0.56]
[22811-3-3-1.00][22811-3-3-1.00][22976-3-2-0.58][22976-3-1-0.66][22976-3-1-1.00][22985-3-3-0.98][22985-3-3-0.98][22985-3-3-0.99][23014-0-0-0.98][23014-0-0-1.00]
[23014-0-0-1.00][23112-1-1-0.93][23112-1-1-0.80][23112-1-1-0.94][23144-3-3-1.00][23144-3-3-1.00][23144-3-3-1.00][23168-2-0-1.00][23168-2-0-0.97][23168-2-0-1.00]
[23219-0-3-0.59][23219-0-0-0.46][23219-0-3-0.63][23363-3-3-0.97][23363-3-3-0.99][23363-3-3-1.00][23470-0-0-0.59][23470-0-0-0.58][23470-0-1-0.64][23486-2-1-0.35]
[23486-2-3-0.77][23486-2-3-0.71][23497-0-3-1.00][23497-0-3-1.00][23497-0-3-0.99][23516-0-0-0.97][23516-0-0-1.00][23516-0-0-1.00][23690-1-1-0.94][23690-1-1-0.66]
[23690-1-1-0.91][23921-2-1-0.82][23921-2-1-0.73][23921-2-1-0.74][23936-1-2-0.86][23936-1-2-0.70][23936-1-3-0.43][24040-3-1-0.65][24040-3-1-0.75][24040-3-1-0.67]
[24111-1-1-0.83][24111-1-1-0.84][24111-1-1-0.89][24182-0-0-0.93][24182-0-3-0.51][24182-0-0-0.83][24238-3-3-0.99][24238-3-3-0.98][24238-3-3-0.99][24290-2-0-1.00]
[24290-2-0-0.98][24290-2-0-0.97][24345-0-0-0.97][24345-0-0-0.91][24345-0-2-0.62][24364-1-3-0.84][24364-1-2-0.70][24364-1-2-0.58][24427-3-3-0.47][24427-3-3-0.62]
[24427-3-3-0.60][24477-2-2-0.76][24477-2-2-0.75][24477-2-1-0.71][24495-2-1-0.95][24495-2-1-0.86][24495-2-1-0.97][24893-2-2-1.00][24893-2-2-0.97][24893-2-2-0.97]
[25012-1-1-0.64][25012-1-2-0.53][25012-1-1-0.46][25121-2-1-0.84][25121-2-1-0.82][25121-2-1-0.89][25165-3-3-0.68][25165-3-3-0.76][25165-3-3-0.68][25183-0-0-1.00]
[25183-0-0-1.00][25183-0-0-1.00][25297-3-3-1.00][25297-3-3-1.00][25297-3-3-1.00][25398-0-0-0.63][25398-0-0-0.90][25398-0-0-0.88][25574-2-1-0.64][25574-2-2-0.94]
[25574-2-2-0.97][25644-1-1-0.62][25644-1-1-0.74][25644-1-1-0.55][25718-1-1-0.91][25718-1-1-0.55][25718-1-1-0.66][25774-2-3-0.52][25774-2-2-0.49][25774-2-2-0.50]
[26032-3-3-0.96][26032-3-3-1.00][26032-3-3-0.96][26051-3-3-1.00][26051-3-3-1.00][26051-3-3-1.00][26120-0-0-0.74][26120-0-0-0.66][26120-0-0-1.00][26321-1-1-0.94]
[26321-1-1-0.93][26321-1-1-0.51][26732-1-1-0.88][26732-1-1-0.99][26732-1-1-0.90][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-1.00][26827-3-3-0.99]
[26827-3-3-0.96][26833-0-3-1.00][26833-0-3-1.00][26833-0-3-1.00][26838-2-1-0.56][26838-2-1-0.63][26838-2-1-0.67][26860-1-1-0.61][26860-1-1-0.74][26860-1-1-0.77]
[26948-0-0-0.74][26948-0-0-0.83][26948-0-0-0.89][27049-3-0-0.57][27049-3-0-0.64][27049-3-0-0.89][27098-1-1-0.98][27098-1-1-0.83][27098-1-1-0.73][27526-0-0-0.98]
[27526-0-0-0.96][27526-0-0-0.72][27639-3-0-0.41][27639-3-3-0.79][27639-3-3-0.66][27698-3-3-0.99][27698-3-3-0.98][27698-3-3-0.99][27772-0-0-0.98][27772-0-0-0.92]
[27772-0-3-0.58][27890-1-1-0.82][27890-1-1-0.87][27890-1-1-0.84][28040-0-2-0.61][28040-0-0-0.63][28040-0-0-0.96][28503-2-2-1.00][28503-2-2-1.00][28503-2-2-1.00]
[28577-1-2-0.60][28577-1-1-0.51][28577-1-1-0.55][28959-0-0-1.00][28959-0-0-0.97][28959-0-0-1.00][29198-3-1-0.75][29198-3-1-0.79][29198-3-1-0.87][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-1-0.78][29877-2-1-0.80][29877-2-1-0.83][30035-1-1-1.00][30035-1-1-0.88][30035-1-1-1.00][30098-0-0-0.62][30098-0-0-0.84]
[30098-0-0-0.91][30326-1-1-1.00][30326-1-1-1.00][30326-1-1-1.00][30572-2-2-0.73][30572-2-2-0.41][30572-2-2-0.82][30716-0-1-0.79][30716-0-1-0.78][30716-0-1-0.78]
[30806-2-2-0.69][30806-2-2-0.64][30806-2-2-0.80][30906-1-1-1.00][30906-1-1-1.00][30906-1-1-0.94][31007-0-0-0.75][31007-0-0-0.94][31007-0-2-0.71][31181-3-3-0.79]
[31181-3-3-0.49][31181-3-2-0.67][31238-0-3-1.00][31238-0-3-1.00][31238-0-3-0.99][31347-0-0-0.99][31347-0-0-0.94][31347-0-0-0.98][31422-2-1-0.69][31422-2-2-0.81]
[31422-2-1-0.78][31429-3-3-0.98][31429-3-3-0.95][31429-3-3-0.98][31431-0-0-1.00][31431-0-0-0.59][31431-0-3-0.47][31432-1-1-1.00][31432-1-1-1.00][31432-1-1-1.00]
[31477-0-0-0.82][31477-0-0-0.97][31477-0-0-0.89][31524-1-1-0.87][31524-1-3-0.37][31524-1-0-0.48][31597-1-2-0.56][31597-1-1-0.78][31597-1-1-0.56][31619-1-3-0.41]
[31619-1-0-0.70][31619-1-3-0.49][31701-0-0-0.97][31701-0-0-0.96][31701-0-0-0.81][31755-0-0-1.00][31755-0-0-1.00][31755-0-0-1.00][31854-3-3-0.99][31854-3-3-0.94]
[31854-3-3-0.76][32074-1-1-0.68][32074-1-1-0.64][32074-1-3-0.97][32078-3-3-0.92][32078-3-3-0.98][32078-3-3-0.97][32111-1-1-0.99][32111-1-1-1.00][32111-1-1-1.00]
[32127-1-1-0.85][32127-1-1-0.63][32127-1-1-0.69][32140-3-3-1.00][32140-3-3-0.99][32140-3-3-1.00][32263-2-0-0.52][32263-2-2-0.46][32263-2-0-0.55][32365-0-0-0.87]
[32365-0-0-0.82][32365-0-0-0.87][32411-2-0-0.99][32411-2-0-0.94][32411-2-0-0.96][32429-3-3-1.00][32429-3-3-1.00][32429-3-3-1.00][32473-3-3-0.48][32473-3-0-0.48]
[32473-3-0-0.51][32574-3-3-0.97][32574-3-3-0.98][32574-3-3-1.00][32584-0-0-0.95][32584-0-0-0.93][32584-0-0-0.91][32622-0-1-0.59][32622-0-1-0.38][32622-0-1-0.68]
[32858-3-3-0.87][32858-3-3-0.88][32858-3-3-0.91][32969-3-3-0.60][32969-3-3-0.61][32969-3-3-0.62][33016-2-2-0.76][33016-2-2-0.89][33016-2-2-0.90][33031-1-3-0.78]
[33031-1-3-0.62][33031-1-1-0.44][33035-2-2-0.84][33035-2-2-0.87][33035-2-2-0.92][33133-2-1-0.86][33133-2-1-0.52][33133-2-1-0.60][33173-2-2-0.46][33173-2-1-0.63]
[33173-2-2-0.60][33175-3-2-0.94][33175-3-2-0.96][33175-3-2-0.95][33306-3-1-0.91][33306-3-1-0.99][33306-3-1-0.94][33309-2-3-0.67][33309-2-3-0.73][33309-2-3-0.77]
[33474-0-1-0.57][33474-0-1-0.64][33474-0-3-0.69][33478-2-0-0.52][33478-2-0-0.55][33478-2-0-0.51][33618-1-1-0.97][33618-1-1-0.97][33618-1-1-0.96][33712-0-0-0.60]
[33712-0-3-0.53][33712-0-0-0.84][33782-2-1-0.53][33782-2-2-0.80][33782-2-2-0.59][33914-3-2-0.95][33914-3-3-0.94][33914-3-3-1.00][34076-3-3-0.59][34076-3-2-0.53]
[34076-3-3-0.66][34112-2-2-1.00][34112-2-2-0.90][34112-2-2-0.86][34138-2-2-1.00][34138-2-2-1.00][34138-2-2-1.00][34239-1-1-0.94][34239-1-1-0.92][34239-1-1-0.97]
[34364-2-1-0.96][34364-2-1-0.67][34364-2-1-0.54][34617-1-1-0.92][34617-1-1-0.73][34617-1-0-0.48][34751-3-3-1.00][34751-3-3-1.00][34751-3-3-1.00][34783-2-1-0.72]
[34783-2-2-0.51][34783-2-1-0.57][35015-3-3-0.55][35015-3-2-0.56][35015-3-3-0.76][35018-1-1-0.95][35018-1-1-0.88][35018-1-1-0.98][35288-2-3-0.97][35288-2-1-0.96]
[35288-2-1-0.80]
---------------------------
I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 0.955 | Acc: 81.375% | Wgt Acc: 81.297%
	I - Batch: 100 | Loss: 0.958 | Acc: 81.188% | Wgt Acc: 81.148%
	I - Batch: 150 | Loss: 0.967 | Acc: 79.875% | Wgt Acc: 79.912%
	I - Batch: 200 | Loss: 0.969 | Acc: 79.625% | Wgt Acc: 79.623%
	I - Batch: 250 | Loss: 0.968 | Acc: 80.025% | Wgt Acc: 80.050%
	I - Batch: 300 | Loss: 0.969 | Acc: 80.000% | Wgt Acc: 79.997%
	I - Batch: 350 | Loss: 0.967 | Acc: 80.321% | Wgt Acc: 80.316%
	I - Batch: 400 | Loss: 0.963 | Acc: 80.703% | Wgt Acc: 80.692%
	I - Batch: 450 | Loss: 0.962 | Acc: 80.931% | Wgt Acc: 80.927%
I - num batch: 478
I - Train -- Loss: 0.961 | Acc: 81.128% | Wgt Acc: 81.110% | LR: 1.000000e-03 | Dur: 350.03s
I - Confusion Matrix: [row->prediction - col->label]
[[1676.   44.   67.  193.]
 [  98. 1489.  217.   87.]
 [  93.  151. 1813.  113.]
 [ 224.   50.  105. 1221.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.120 | Acc: 61.750% | Wgt Acc: 61.383%
I - num batch: 62
I - Val -- Loss: 1.126 | Acc: 61.672% | Wgt Acc: 61.345% | Dur: 37.00s
I - Confusion Matrix: [row->prediction - col->label]
[[169.  13.   8.  32.]
 [ 11. 119.  49.  27.]
 [ 18.  94. 147.  29.]
 [ 66.   8.  21. 170.]]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 0.943 | Acc: 83.375% | Wgt Acc: 83.291%
	I - Batch: 100 | Loss: 0.946 | Acc: 83.250% | Wgt Acc: 83.124%
	I - Batch: 150 | Loss: 0.945 | Acc: 83.167% | Wgt Acc: 83.087%
	I - Batch: 200 | Loss: 0.945 | Acc: 83.188% | Wgt Acc: 83.112%
	I - Batch: 250 | Loss: 0.947 | Acc: 82.850% | Wgt Acc: 82.766%
	I - Batch: 300 | Loss: 0.946 | Acc: 82.958% | Wgt Acc: 82.914%
	I - Batch: 350 | Loss: 0.946 | Acc: 83.000% | Wgt Acc: 82.951%
	I - Batch: 400 | Loss: 0.944 | Acc: 83.250% | Wgt Acc: 83.206%
	I - Batch: 450 | Loss: 0.945 | Acc: 83.208% | Wgt Acc: 83.162%
I - num batch: 478
I - Train -- Loss: 0.946 | Acc: 83.144% | Wgt Acc: 83.103% | LR: 1.000000e-03 | Dur: 355.75s
I - Confusion Matrix: [row->prediction - col->label]
[[1698.   40.   52.  171.]
 [  75. 1509.  174.   77.]
 [  84.  139. 1885.  105.]
 [ 234.   46.   91. 1261.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.110 | Acc: 63.875% | Wgt Acc: 63.104%
I - num batch: 62
I - Val -- Loss: 1.110 | Acc: 64.016% | Wgt Acc: 63.293% | Dur: 37.53s
I - Confusion Matrix: [row->prediction - col->label]
[[218.  23.  32.  73.]
 [  6. 149.  52.  33.]
 [  8.  55. 127.  18.]
 [ 32.   7.  14. 134.]]

I - Local maximum validation set accuracy:  64.02

I - Validation set results: 
[14-1-2-0.77][14-1-2-0.62][14-1-2-0.45][50-3-1-0.46][50-3-1-0.57][50-3-0-0.61][124-2-2-0.50][124-2-2-0.81][124-2-2-0.85][127-0-0-0.99]
[127-0-0-1.00][127-0-0-1.00][443-2-2-1.00][443-2-2-1.00][443-2-2-1.00][567-0-0-0.87][567-0-0-0.88][567-0-0-0.99][573-1-1-0.44][573-1-1-0.73]
[573-1-1-0.64][615-0-3-0.58][615-0-3-0.57][615-0-3-0.60][695-1-2-0.87][695-1-2-0.97][695-1-2-0.90][722-3-3-0.54][722-3-3-0.83][722-3-3-0.62]
[826-0-0-0.63][826-0-0-0.99][826-0-0-1.00][878-0-0-0.79][878-0-0-0.93][878-0-0-1.00][1103-0-0-0.65][1103-0-0-0.59][1103-0-0-0.68][1212-3-3-0.73]
[1212-3-3-0.81][1212-3-3-0.68][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-3-0.53][2181-2-3-0.52][2181-2-3-0.50][2476-2-0-0.45][2476-2-0-0.53]
[2476-2-0-0.40][2721-2-2-0.90][2721-2-2-0.94][2721-2-2-0.98][2818-1-2-0.44][2818-1-3-0.62][2818-1-3-0.74][2886-2-1-0.92][2886-2-1-0.82][2886-2-1-0.86]
[3231-2-2-1.00][3231-2-2-1.00][3231-2-2-1.00][3333-2-2-0.49][3333-2-2-0.53][3333-2-2-0.86][3482-2-2-1.00][3482-2-2-1.00][3482-2-2-1.00][3536-3-0-0.58]
[3536-3-3-0.77][3536-3-3-0.81][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-0-1.00][3909-0-0-0.99][3909-0-0-1.00][4035-0-0-0.99][4035-0-0-0.99]
[4035-0-0-1.00][4140-0-0-1.00][4140-0-0-0.99][4140-0-0-0.99][4214-1-3-0.76][4214-1-1-0.56][4214-1-1-0.99][4346-1-0-1.00][4346-1-0-1.00][4346-1-0-1.00]
[4581-2-2-1.00][4581-2-2-1.00][4581-2-2-0.83][4708-3-0-0.61][4708-3-1-0.44][4708-3-0-0.41][4838-3-1-0.45][4838-3-0-0.32][4838-3-1-0.75][4845-1-1-0.79]
[4845-1-1-0.84][4845-1-1-0.85][4868-0-0-1.00][4868-0-0-1.00][4868-0-0-0.93][4939-0-2-0.50][4939-0-2-0.50][4939-0-2-0.85][4984-2-2-0.92][4984-2-2-0.96]
[4984-2-2-0.97][5078-1-2-0.52][5078-1-2-0.42][5078-1-2-0.68][5396-0-0-1.00][5396-0-0-1.00][5396-0-0-1.00][5479-1-1-0.90][5479-1-1-1.00][5479-1-1-0.90]
[5717-0-0-0.86][5717-0-0-0.99][5717-0-0-0.63][5843-1-1-0.97][5843-1-1-0.94][5843-1-1-1.00][5949-3-0-0.64][5949-3-0-0.84][5949-3-0-0.63][5987-2-1-0.70]
[5987-2-1-0.87][5987-2-1-0.86][6014-3-1-1.00][6014-3-1-1.00][6014-3-1-0.99][6033-3-0-0.51][6033-3-3-0.51][6033-3-0-0.75][6313-0-0-1.00][6313-0-0-0.99]
[6313-0-0-0.78][6421-3-3-1.00][6421-3-3-1.00][6421-3-3-1.00][6500-1-1-0.98][6500-1-1-0.99][6500-1-1-0.99][6583-3-3-0.89][6583-3-3-0.94][6583-3-3-0.89]
[6683-3-3-0.80][6683-3-3-0.84][6683-3-3-0.85][6825-2-0-0.72][6825-2-0-0.88][6825-2-0-0.94][6998-3-3-0.42][6998-3-3-0.72][6998-3-3-0.42][7049-3-3-0.96]
[7049-3-3-0.86][7049-3-3-0.93][7517-1-1-0.91][7517-1-1-0.84][7517-1-1-1.00][7521-1-1-0.90][7521-1-1-0.99][7521-1-1-0.99][7528-1-3-0.96][7528-1-2-0.53]
[7528-1-2-0.75][7949-1-2-1.00][7949-1-2-1.00][7949-1-2-0.99][8135-1-0-1.00][8135-1-0-0.98][8135-1-0-1.00][8185-3-0-0.96][8185-3-0-0.92][8185-3-0-0.99]
[8269-3-2-0.75][8269-3-2-0.64][8269-3-1-0.57][8273-3-3-1.00][8273-3-3-1.00][8273-3-3-0.99][8543-3-0-0.59][8543-3-0-1.00][8543-3-0-1.00][8666-1-1-0.93]
[8666-1-1-0.99][8666-1-1-1.00][8672-0-0-1.00][8672-0-0-0.99][8672-0-3-0.59][8903-1-2-0.73][8903-1-2-0.85][8903-1-2-0.97][9001-2-0-0.87][9001-2-1-1.00]
[9001-2-1-1.00][9036-2-2-0.97][9036-2-2-0.99][9036-2-2-0.98][9281-3-1-0.52][9281-3-1-0.47][9281-3-1-0.70][9300-2-2-0.93][9300-2-2-0.98][9300-2-2-0.95]
[9571-0-3-0.62][9571-0-0-0.47][9571-0-3-0.52][9617-1-1-1.00][9617-1-1-0.74][9617-1-1-0.76][9644-2-2-0.75][9644-2-2-0.99][9644-2-2-0.94][9705-2-0-0.63]
[9705-2-0-0.55][9705-2-2-0.50][9801-0-0-0.73][9801-0-0-0.87][9801-0-3-0.60][9803-3-0-0.88][9803-3-3-0.53][9803-3-3-0.66][9865-3-3-1.00][9865-3-3-1.00]
[9865-3-3-0.99][9896-2-2-0.87][9896-2-2-0.74][9896-2-2-0.82][10314-1-1-0.66][10314-1-1-0.79][10314-1-2-0.48][10337-3-3-1.00][10337-3-3-1.00][10337-3-3-1.00]
[10403-0-0-0.60][10403-0-0-0.62][10403-0-0-0.76][10653-2-1-0.64][10653-2-1-0.57][10653-2-0-0.46][10704-2-1-0.71][10704-2-1-0.57][10704-2-1-0.73][10719-1-1-0.72]
[10719-1-1-0.95][10719-1-1-0.98][10727-1-1-0.50][10727-1-1-0.73][10727-1-1-0.68][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-3-0.59][10969-2-3-0.65]
[10969-2-3-0.56][11042-0-0-1.00][11042-0-0-0.99][11042-0-0-1.00][11088-1-1-0.70][11088-1-1-0.87][11088-1-1-0.94][11322-0-0-1.00][11322-0-0-1.00][11322-0-0-1.00]
[11398-2-2-1.00][11398-2-2-0.76][11398-2-0-0.69][11499-0-0-0.92][11499-0-0-0.98][11499-0-0-0.95][11502-3-3-0.67][11502-3-3-0.55][11502-3-3-0.66][11512-3-3-0.73]
[11512-3-3-0.53][11512-3-1-0.54][11608-1-1-0.84][11608-1-1-0.51][11608-1-2-0.54][11610-0-0-1.00][11610-0-0-1.00][11610-0-0-1.00][11692-0-0-1.00][11692-0-0-0.98]
[11692-0-0-0.99][11905-0-0-0.96][11905-0-0-0.79][11905-0-0-0.67][11993-1-1-0.87][11993-1-1-0.89][11993-1-1-0.96][12002-2-0-0.97][12002-2-0-0.37][12002-2-3-0.79]
[12052-0-0-0.74][12052-0-0-0.99][12052-0-0-0.97][12201-0-0-0.67][12201-0-3-0.53][12201-0-0-0.61][12235-2-2-0.87][12235-2-1-0.66][12235-2-2-0.77][12320-1-0-0.98]
[12320-1-0-0.98][12320-1-0-0.98][12377-2-2-0.55][12377-2-2-0.48][12377-2-2-0.76][12398-2-1-0.54][12398-2-1-0.91][12398-2-1-0.98][12503-1-1-0.99][12503-1-1-0.85]
[12503-1-1-0.71][12617-0-2-0.47][12617-0-2-0.48][12617-0-2-0.44][12685-3-1-0.85][12685-3-1-0.82][12685-3-1-0.80][12738-2-0-0.50][12738-2-0-0.69][12738-2-0-0.73]
[12742-2-2-0.95][12742-2-2-0.99][12742-2-2-0.99][12823-0-0-0.76][12823-0-0-0.65][12823-0-3-0.51][13110-1-1-0.94][13110-1-1-0.65][13110-1-2-0.82][13240-3-0-0.98]
[13240-3-0-0.99][13240-3-0-0.99][13253-1-1-0.55][13253-1-2-0.58][13253-1-1-0.60][13273-0-0-1.00][13273-0-0-1.00][13273-0-0-0.99][13634-1-2-0.99][13634-1-2-0.99]
[13634-1-2-0.99][13763-2-2-0.56][13763-2-2-0.85][13763-2-2-0.54][13905-3-3-0.78][13905-3-3-0.71][13905-3-3-0.49][14060-2-1-1.00][14060-2-1-1.00][14060-2-1-1.00]
[14065-3-0-0.83][14065-3-0-0.93][14065-3-0-0.68][14147-3-0-0.76][14147-3-3-0.38][14147-3-0-0.97][14595-2-2-0.89][14595-2-2-0.87][14595-2-2-0.83][14687-2-2-0.99]
[14687-2-2-0.99][14687-2-2-0.96][14788-2-3-0.69][14788-2-3-0.78][14788-2-2-0.65][14869-1-1-0.89][14869-1-1-0.81][14869-1-1-0.75][14872-3-2-0.61][14872-3-0-0.96]
[14872-3-0-0.62][14877-1-1-0.74][14877-1-1-0.73][14877-1-1-0.88][14927-0-0-0.66][14927-0-0-0.58][14927-0-3-0.57][15066-0-0-0.93][15066-0-0-0.74][15066-0-0-0.78]
[15175-1-1-0.64][15175-1-1-0.69][15175-1-1-0.73][15178-2-2-0.67][15178-2-2-0.76][15178-2-2-0.83][15375-3-0-0.53][15375-3-3-0.62][15375-3-1-0.47][15389-3-3-0.93]
[15389-3-3-1.00][15389-3-3-0.98][15568-2-1-0.95][15568-2-1-1.00][15568-2-1-1.00][15675-3-3-0.73][15675-3-3-0.99][15675-3-3-0.75][15869-1-1-0.86][15869-1-1-0.80]
[15869-1-2-0.64][16207-3-0-0.43][16207-3-0-0.41][16207-3-0-0.59][16236-0-2-0.51][16236-0-0-0.50][16236-0-0-0.37][16302-3-0-0.84][16302-3-0-0.82][16302-3-0-0.63]
[16331-2-2-1.00][16331-2-2-1.00][16331-2-2-1.00][16381-0-0-0.64][16381-0-3-0.51][16381-0-3-0.62][16488-1-1-0.81][16488-1-1-0.94][16488-1-1-0.62][16495-0-0-0.75]
[16495-0-0-1.00][16495-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16719-1-2-0.47][16719-1-2-0.46][16719-1-1-0.86][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.85][16828-0-0-1.00][16828-0-0-1.00][17137-3-0-1.00][17137-3-0-0.60][17137-3-0-1.00][17245-1-2-0.67][17245-1-2-0.70][17245-1-2-0.56]
[17278-3-0-0.63][17278-3-0-0.46][17278-3-0-0.93][17282-0-0-0.80][17282-0-0-0.86][17282-0-0-0.61][17311-2-2-0.95][17311-2-2-0.95][17311-2-2-0.98][17336-2-1-0.87]
[17336-2-1-0.65][17336-2-1-0.84][17608-3-3-1.00][17608-3-3-1.00][17608-3-3-1.00][17627-0-0-0.99][17627-0-0-0.51][17627-0-0-0.90][17877-3-1-0.96][17877-3-0-0.49]
[17877-3-0-0.66][17924-1-2-0.54][17924-1-3-0.65][17924-1-2-0.63][17984-3-0-0.89][17984-3-0-0.93][17984-3-0-0.95][18211-0-3-0.46][18211-0-3-0.96][18211-0-3-0.71]
[18276-3-0-0.83][18276-3-0-0.95][18276-3-0-0.65][18287-1-1-0.47][18287-1-1-0.90][18287-1-1-0.98][18394-0-0-1.00][18394-0-0-0.97][18394-0-0-0.98][18428-0-0-1.00]
[18428-0-0-0.99][18428-0-0-0.96][18442-0-3-0.99][18442-0-3-0.90][18442-0-3-0.81][18478-3-3-0.92][18478-3-3-0.96][18478-3-3-0.97][18607-0-0-1.00][18607-0-0-1.00]
[18607-0-0-1.00][18616-0-1-0.50][18616-0-0-0.72][18616-0-0-0.78][18663-0-0-0.88][18663-0-0-0.95][18663-0-0-0.60][18718-0-0-1.00][18718-0-0-1.00][18718-0-0-1.00]
[18766-2-2-0.62][18766-2-2-0.51][18766-2-1-0.62][18824-2-2-0.95][18824-2-1-0.60][18824-2-2-0.81][18890-3-1-0.47][18890-3-2-0.44][18890-3-2-0.53][18930-3-2-0.63]
[18930-3-2-0.62][18930-3-2-0.64][18938-3-3-0.58][18938-3-3-0.63][18938-3-3-0.78][19817-1-2-0.60][19817-1-1-0.56][19817-1-2-0.63][19839-0-0-0.98][19839-0-0-0.93]
[19839-0-0-0.97][19930-3-3-0.64][19930-3-3-0.62][19930-3-3-0.52][19944-0-0-0.71][19944-0-0-0.45][19944-0-0-0.62][20036-2-2-0.99][20036-2-2-0.89][20036-2-2-0.98]
[20101-3-1-0.75][20101-3-0-0.57][20101-3-3-0.42][20474-1-1-0.96][20474-1-1-0.98][20474-1-1-0.99][20547-3-1-0.48][20547-3-0-0.72][20547-3-0-0.96][20929-2-0-0.48]
[20929-2-2-0.71][20929-2-2-0.74][21245-1-1-0.95][21245-1-1-0.87][21245-1-1-0.85][21257-3-3-0.68][21257-3-1-0.48][21257-3-1-0.54][21293-1-1-0.94][21293-1-1-0.94]
[21293-1-1-0.92][21316-1-1-1.00][21316-1-1-1.00][21316-1-3-0.40][21384-1-2-0.99][21384-1-2-0.99][21384-1-2-0.99][21448-1-1-0.99][21448-1-1-0.94][21448-1-1-0.92]
[21483-0-0-1.00][21483-0-0-1.00][21483-0-0-1.00][21487-2-2-0.96][21487-2-2-0.97][21487-2-2-0.94][21714-0-0-0.89][21714-0-0-0.93][21714-0-0-0.82][21943-3-2-0.65]
[21943-3-2-0.85][21943-3-2-1.00][21947-0-0-1.00][21947-0-0-0.79][21947-0-0-0.71][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-1.00][21965-2-2-1.00]
[21965-2-1-0.92][21998-1-1-0.43][21998-1-1-0.76][21998-1-1-0.66][22025-0-1-0.47][22025-0-1-0.50][22025-0-1-0.84][22228-3-3-0.87][22228-3-3-0.90][22228-3-3-0.84]
[22446-1-1-1.00][22446-1-1-1.00][22446-1-1-1.00][22494-3-3-0.51][22494-3-3-0.88][22494-3-3-0.91][22757-0-3-0.97][22757-0-3-0.99][22757-0-3-0.98][22811-3-3-0.51]
[22811-3-3-1.00][22811-3-3-1.00][22976-3-2-0.79][22976-3-1-0.67][22976-3-1-1.00][22985-3-3-0.93][22985-3-3-0.99][22985-3-3-1.00][23014-0-3-0.80][23014-0-0-0.63]
[23014-0-0-0.62][23112-1-1-0.90][23112-1-1-0.80][23112-1-1-0.81][23144-3-3-1.00][23144-3-3-1.00][23144-3-3-0.94][23168-2-0-0.77][23168-2-1-0.51][23168-2-1-0.66]
[23219-0-0-0.48][23219-0-0-0.87][23219-0-0-0.95][23363-3-3-0.98][23363-3-3-1.00][23363-3-3-1.00][23470-0-0-0.93][23470-0-0-0.77][23470-0-0-0.58][23486-2-1-0.42]
[23486-2-1-0.35][23486-2-1-0.39][23497-0-3-0.91][23497-0-3-0.85][23497-0-3-0.69][23516-0-0-1.00][23516-0-0-1.00][23516-0-0-1.00][23690-1-1-1.00][23690-1-1-0.77]
[23690-1-1-0.99][23921-2-1-0.95][23921-2-2-0.54][23921-2-1-0.63][23936-1-2-0.96][23936-1-2-0.99][23936-1-2-0.93][24040-3-0-0.58][24040-3-0-0.70][24040-3-0-0.62]
[24111-1-1-0.57][24111-1-1-0.68][24111-1-1-0.74][24182-0-0-1.00][24182-0-0-1.00][24182-0-0-1.00][24238-3-3-0.54][24238-3-0-0.57][24238-3-0-0.59][24290-2-0-1.00]
[24290-2-0-0.87][24290-2-0-0.84][24345-0-0-0.98][24345-0-0-0.99][24345-0-0-0.88][24364-1-2-0.80][24364-1-2-0.97][24364-1-2-0.96][24427-3-0-0.95][24427-3-0-1.00]
[24427-3-0-0.99][24477-2-2-0.91][24477-2-2-0.96][24477-2-2-0.62][24495-2-1-0.80][24495-2-1-0.66][24495-2-1-0.57][24893-2-2-0.66][24893-2-2-0.52][24893-2-2-0.68]
[25012-1-2-0.74][25012-1-2-0.75][25012-1-2-0.58][25121-2-2-0.59][25121-2-2-0.66][25121-2-2-0.60][25165-3-3-0.66][25165-3-3-0.72][25165-3-3-0.65][25183-0-0-1.00]
[25183-0-0-1.00][25183-0-0-1.00][25297-3-3-1.00][25297-3-3-1.00][25297-3-3-1.00][25398-0-0-0.75][25398-0-0-0.92][25398-0-0-0.94][25574-2-1-0.90][25574-2-2-0.75]
[25574-2-2-0.97][25644-1-1-0.94][25644-1-1-0.96][25644-1-1-0.56][25718-1-1-0.97][25718-1-1-0.78][25718-1-1-0.53][25774-2-2-0.69][25774-2-2-0.50][25774-2-2-0.68]
[26032-3-3-0.71][26032-3-3-0.93][26032-3-3-0.92][26051-3-3-1.00][26051-3-3-1.00][26051-3-3-1.00][26120-0-0-0.71][26120-0-0-0.79][26120-0-0-1.00][26321-1-1-0.98]
[26321-1-1-0.97][26321-1-2-0.59][26732-1-1-0.99][26732-1-1-1.00][26732-1-1-0.99][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-0.97][26827-3-3-0.97]
[26827-3-3-0.97][26833-0-3-1.00][26833-0-3-0.96][26833-0-3-0.64][26838-2-2-0.40][26838-2-3-0.43][26838-2-1-0.39][26860-1-0-0.64][26860-1-0-0.56][26860-1-2-0.63]
[26948-0-0-0.83][26948-0-0-0.80][26948-0-0-0.89][27049-3-0-1.00][27049-3-0-0.98][27049-3-0-0.99][27098-1-0-0.57][27098-1-0-0.78][27098-1-0-0.91][27526-0-0-0.68]
[27526-0-0-0.80][27526-0-1-0.62][27639-3-1-0.64][27639-3-3-0.62][27639-3-1-0.51][27698-3-3-0.99][27698-3-3-0.99][27698-3-3-0.99][27772-0-0-1.00][27772-0-0-1.00]
[27772-0-0-0.70][27890-1-1-0.88][27890-1-1-0.88][27890-1-1-0.90][28040-0-2-0.55][28040-0-0-0.60][28040-0-0-0.87][28503-2-2-1.00][28503-2-2-1.00][28503-2-2-1.00]
[28577-1-1-0.97][28577-1-1-0.98][28577-1-1-0.98][28959-0-0-1.00][28959-0-0-0.99][28959-0-0-1.00][29198-3-1-0.91][29198-3-1-0.91][29198-3-1-0.94][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-1-0.49][29877-2-1-0.61][29877-2-1-0.70][30035-1-1-0.99][30035-1-1-0.50][30035-1-1-0.88][30098-0-0-0.42][30098-0-0-0.83]
[30098-0-0-0.85][30326-1-1-1.00][30326-1-1-1.00][30326-1-1-1.00][30572-2-2-0.71][30572-2-2-0.50][30572-2-2-0.86][30716-0-0-0.67][30716-0-0-0.79][30716-0-0-0.81]
[30806-2-2-0.67][30806-2-2-0.61][30806-2-2-0.86][30906-1-1-0.99][30906-1-1-0.99][30906-1-1-0.90][31007-0-0-0.99][31007-0-0-1.00][31007-0-0-0.87][31181-3-0-0.62]
[31181-3-2-0.41][31181-3-2-0.91][31238-0-3-0.77][31238-0-3-0.95][31238-0-3-0.79][31347-0-0-1.00][31347-0-0-1.00][31347-0-0-1.00][31422-2-0-0.44][31422-2-0-0.64]
[31422-2-0-0.42][31429-3-3-0.86][31429-3-3-0.84][31429-3-3-0.76][31431-0-0-1.00][31431-0-0-0.88][31431-0-0-0.92][31432-1-1-0.89][31432-1-1-0.98][31432-1-1-0.99]
[31477-0-3-0.61][31477-0-0-0.69][31477-0-0-0.61][31524-1-2-0.46][31524-1-2-0.41][31524-1-0-0.52][31597-1-2-0.86][31597-1-2-0.64][31597-1-2-0.56][31619-1-0-0.91]
[31619-1-0-0.97][31619-1-0-0.97][31701-0-0-0.99][31701-0-0-0.97][31701-0-0-0.80][31755-0-0-1.00][31755-0-0-1.00][31755-0-0-1.00][31854-3-3-0.88][31854-3-3-0.84]
[31854-3-3-0.58][32074-1-1-0.46][32074-1-0-0.56][32074-1-3-0.74][32078-3-3-0.94][32078-3-3-0.88][32078-3-3-0.92][32111-1-0-0.70][32111-1-1-0.84][32111-1-1-0.99]
[32127-1-1-0.79][32127-1-1-0.55][32127-1-1-0.60][32140-3-3-1.00][32140-3-3-1.00][32140-3-3-1.00][32263-2-0-0.66][32263-2-2-0.52][32263-2-0-0.74][32365-0-0-1.00]
[32365-0-0-0.99][32365-0-0-1.00][32411-2-0-0.95][32411-2-0-0.82][32411-2-0-0.81][32429-3-3-0.91][32429-3-3-0.95][32429-3-3-0.91][32473-3-0-0.78][32473-3-0-0.72]
[32473-3-0-0.69][32574-3-0-0.69][32574-3-0-0.93][32574-3-0-1.00][32584-0-0-0.99][32584-0-0-0.99][32584-0-0-0.99][32622-0-0-0.45][32622-0-0-0.47][32622-0-1-0.62]
[32858-3-0-0.94][32858-3-0-0.96][32858-3-0-0.95][32969-3-3-0.69][32969-3-3-0.72][32969-3-3-0.69][33016-2-1-0.58][33016-2-2-1.00][33016-2-2-1.00][33031-1-0-0.64]
[33031-1-0-0.36][33031-1-0-0.67][33035-2-2-0.94][33035-2-2-0.89][33035-2-2-0.97][33133-2-2-0.74][33133-2-2-0.73][33133-2-2-0.82][33173-2-1-0.49][33173-2-1-0.66]
[33173-2-2-0.62][33175-3-2-0.68][33175-3-2-0.70][33175-3-2-0.70][33306-3-1-0.98][33306-3-1-0.95][33306-3-1-0.96][33309-2-3-0.56][33309-2-3-0.67][33309-2-3-0.81]
[33474-0-0-0.63][33474-0-0-0.72][33474-0-0-0.98][33478-2-0-0.40][33478-2-0-0.45][33478-2-0-0.37][33618-1-1-0.94][33618-1-1-0.90][33618-1-1-0.87][33712-0-0-0.98]
[33712-0-0-0.99][33712-0-0-1.00][33782-2-1-0.74][33782-2-2-0.51][33782-2-2-0.61][33914-3-2-0.88][33914-3-3-0.98][33914-3-3-1.00][34076-3-3-0.68][34076-3-3-0.64]
[34076-3-0-0.67][34112-2-2-1.00][34112-2-2-0.62][34112-2-2-0.72][34138-2-2-0.54][34138-2-2-0.47][34138-2-2-0.68][34239-1-1-0.73][34239-1-1-0.78][34239-1-1-0.94]
[34364-2-1-0.64][34364-2-2-0.80][34364-2-2-0.83][34617-1-1-0.90][34617-1-1-0.88][34617-1-2-0.41][34751-3-3-0.96][34751-3-3-0.95][34751-3-3-0.82][34783-2-1-0.72]
[34783-2-2-0.64][34783-2-2-0.56][35015-3-3-0.72][35015-3-3-0.55][35015-3-3-0.87][35018-1-1-0.74][35018-1-1-0.83][35018-1-1-0.82][35288-2-3-0.97][35288-2-1-0.72]
[35288-2-1-0.41]
---------------------------
I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 0.952 | Acc: 82.875% | Wgt Acc: 82.901%
	I - Batch: 100 | Loss: 0.947 | Acc: 83.625% | Wgt Acc: 83.611%
	I - Batch: 150 | Loss: 0.947 | Acc: 83.625% | Wgt Acc: 83.619%
	I - Batch: 200 | Loss: 0.942 | Acc: 84.125% | Wgt Acc: 84.078%
	I - Batch: 250 | Loss: 0.936 | Acc: 84.675% | Wgt Acc: 84.643%
	I - Batch: 300 | Loss: 0.935 | Acc: 84.562% | Wgt Acc: 84.540%
	I - Batch: 350 | Loss: 0.935 | Acc: 84.679% | Wgt Acc: 84.600%
	I - Batch: 400 | Loss: 0.935 | Acc: 84.469% | Wgt Acc: 84.398%
	I - Batch: 450 | Loss: 0.936 | Acc: 84.514% | Wgt Acc: 84.447%
I - num batch: 478
I - Train -- Loss: 0.934 | Acc: 84.662% | Wgt Acc: 84.581% | LR: 1.000000e-03 | Dur: 346.22s
I - Confusion Matrix: [row->prediction - col->label]
[[1739.   43.   48.  171.]
 [  90. 1546.  146.   73.]
 [  69.  109. 1923.  109.]
 [ 193.   36.   85. 1261.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.122 | Acc: 62.750% | Wgt Acc: 62.438%
I - num batch: 62
I - Val -- Loss: 1.120 | Acc: 63.201% | Wgt Acc: 62.976% | Dur: 36.20s
I - Confusion Matrix: [row->prediction - col->label]
[[177.  24.  19.  36.]
 [  1. 104.  24.   9.]
 [  9.  78. 142.  16.]
 [ 77.  28.  40. 197.]]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 0.906 | Acc: 88.125% | Wgt Acc: 88.116%
	I - Batch: 100 | Loss: 0.897 | Acc: 89.000% | Wgt Acc: 88.961%
	I - Batch: 150 | Loss: 0.895 | Acc: 88.792% | Wgt Acc: 88.788%
	I - Batch: 200 | Loss: 0.889 | Acc: 89.281% | Wgt Acc: 89.293%
	I - Batch: 250 | Loss: 0.886 | Acc: 89.525% | Wgt Acc: 89.536%
	I - Batch: 300 | Loss: 0.886 | Acc: 89.583% | Wgt Acc: 89.621%
	I - Batch: 350 | Loss: 0.886 | Acc: 89.554% | Wgt Acc: 89.549%
	I - Batch: 400 | Loss: 0.887 | Acc: 89.484% | Wgt Acc: 89.489%
	I - Batch: 450 | Loss: 0.886 | Acc: 89.681% | Wgt Acc: 89.702%
I - num batch: 478
I - Train -- Loss: 0.885 | Acc: 89.687% | Wgt Acc: 89.709% | LR: 5.000000e-04 | Dur: 346.73s
I - Confusion Matrix: [row->prediction - col->label]
[[1819.   42.   46.  116.]
 [  66. 1614.   71.   45.]
 [  66.   60. 2024.   57.]
 [ 140.   18.   61. 1396.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.111 | Acc: 63.000% | Wgt Acc: 62.743%
I - num batch: 62
I - Val -- Loss: 1.115 | Acc: 62.793% | Wgt Acc: 62.726% | Dur: 36.58s
I - Confusion Matrix: [row->prediction - col->label]
[[188.  13.  18.  52.]
 [ 16. 158.  72.  27.]
 [ 21.  49. 122.  31.]
 [ 39.  14.  13. 148.]]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 0.857 | Acc: 93.000% | Wgt Acc: 92.982%
	I - Batch: 100 | Loss: 0.869 | Acc: 91.500% | Wgt Acc: 91.482%
	I - Batch: 150 | Loss: 0.869 | Acc: 91.292% | Wgt Acc: 91.242%
	I - Batch: 200 | Loss: 0.868 | Acc: 91.375% | Wgt Acc: 91.309%
	I - Batch: 250 | Loss: 0.867 | Acc: 91.600% | Wgt Acc: 91.507%
	I - Batch: 300 | Loss: 0.866 | Acc: 91.667% | Wgt Acc: 91.610%
	I - Batch: 350 | Loss: 0.866 | Acc: 91.607% | Wgt Acc: 91.537%
	I - Batch: 400 | Loss: 0.867 | Acc: 91.547% | Wgt Acc: 91.493%
	I - Batch: 450 | Loss: 0.868 | Acc: 91.431% | Wgt Acc: 91.388%
I - num batch: 478
I - Train -- Loss: 0.868 | Acc: 91.506% | Wgt Acc: 91.460% | LR: 5.000000e-04 | Dur: 348.08s
I - Confusion Matrix: [row->prediction - col->label]
[[1879.   31.   35.   87.]
 [  53. 1629.   58.   36.]
 [  51.   49. 2065.   72.]
 [ 108.   25.   44. 1419.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.122 | Acc: 64.500% | Wgt Acc: 62.937%
I - num batch: 62
I - Val -- Loss: 1.121 | Acc: 64.118% | Wgt Acc: 62.636% | Dur: 38.22s
I - Confusion Matrix: [row->prediction - col->label]
[[215.  17.  24.  69.]
 [  9. 116.  26.  15.]
 [  8.  95. 164.  40.]
 [ 32.   6.  11. 134.]]

I - Local maximum validation set accuracy:  64.12

I - Validation set results: 
[14-1-2-0.92][14-1-2-0.79][14-1-2-0.73][50-3-3-0.58][50-3-1-0.44][50-3-0-0.36][124-2-2-0.68][124-2-2-0.96][124-2-2-0.88][127-0-0-1.00]
[127-0-0-1.00][127-0-0-1.00][443-2-2-1.00][443-2-2-1.00][443-2-2-1.00][567-0-0-0.98][567-0-0-0.96][567-0-0-0.99][573-1-1-0.44][573-1-1-0.76]
[573-1-1-0.63][615-0-0-0.61][615-0-0-0.56][615-0-0-0.67][695-1-2-0.93][695-1-2-0.99][695-1-2-0.96][722-3-0-0.59][722-3-3-0.56][722-3-0-0.50]
[826-0-0-0.96][826-0-0-1.00][826-0-0-1.00][878-0-0-0.99][878-0-0-1.00][878-0-0-1.00][1103-0-0-0.86][1103-0-0-0.63][1103-0-0-0.66][1212-3-3-0.68]
[1212-3-3-0.70][1212-3-3-0.37][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-0-0.62][2181-2-0-0.56][2181-2-0-0.60][2476-2-2-0.46][2476-2-2-0.42]
[2476-2-2-0.56][2721-2-2-1.00][2721-2-2-1.00][2721-2-2-1.00][2818-1-2-0.49][2818-1-1-0.53][2818-1-1-0.57][2886-2-2-0.59][2886-2-2-0.61][2886-2-1-0.50]
[3231-2-2-1.00][3231-2-2-1.00][3231-2-2-1.00][3333-2-2-0.95][3333-2-2-0.93][3333-2-2-0.99][3482-2-2-1.00][3482-2-2-1.00][3482-2-2-1.00][3536-3-0-0.53]
[3536-3-3-0.73][3536-3-3-0.75][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-0-0.99][3909-0-0-0.81][3909-0-0-0.86][4035-0-0-0.96][4035-0-0-0.94]
[4035-0-0-0.93][4140-0-0-0.92][4140-0-0-0.73][4140-0-0-0.92][4214-1-2-0.50][4214-1-2-0.53][4214-1-1-0.75][4346-1-0-1.00][4346-1-0-0.99][4346-1-0-0.99]
[4581-2-2-0.98][4581-2-2-0.99][4581-2-2-0.96][4708-3-2-1.00][4708-3-2-0.98][4708-3-2-0.99][4838-3-3-0.76][4838-3-3-0.63][4838-3-3-0.51][4845-1-2-0.82]
[4845-1-2-0.83][4845-1-2-0.85][4868-0-0-0.99][4868-0-0-1.00][4868-0-0-0.95][4939-0-0-0.42][4939-0-1-0.54][4939-0-2-0.79][4984-2-2-0.99][4984-2-2-1.00]
[4984-2-2-1.00][5078-1-2-0.60][5078-1-2-0.77][5078-1-2-0.67][5396-0-0-1.00][5396-0-0-1.00][5396-0-0-1.00][5479-1-1-0.63][5479-1-1-0.83][5479-1-1-0.65]
[5717-0-0-0.64][5717-0-0-0.91][5717-0-0-0.56][5843-1-1-0.88][5843-1-1-0.53][5843-1-1-0.97][5949-3-0-0.64][5949-3-0-0.90][5949-3-0-0.89][5987-2-1-0.57]
[5987-2-1-0.74][5987-2-1-0.58][6014-3-1-0.77][6014-3-1-0.87][6014-3-1-0.66][6033-3-0-0.98][6033-3-0-0.66][6033-3-0-0.96][6313-0-0-0.99][6313-0-0-0.98]
[6313-0-0-0.94][6421-3-3-1.00][6421-3-3-1.00][6421-3-3-0.99][6500-1-1-0.61][6500-1-1-0.69][6500-1-1-0.74][6583-3-3-0.47][6583-3-3-0.59][6583-3-2-0.57]
[6683-3-3-0.79][6683-3-3-0.80][6683-3-3-0.97][6825-2-0-0.46][6825-2-0-0.57][6825-2-0-0.57][6998-3-3-0.52][6998-3-3-0.75][6998-3-3-0.60][7049-3-3-0.53]
[7049-3-3-0.55][7049-3-3-0.90][7517-1-2-0.63][7517-1-1-0.57][7517-1-1-0.87][7521-1-1-0.42][7521-1-1-0.78][7521-1-1-0.72][7528-1-3-0.52][7528-1-2-0.98]
[7528-1-2-0.96][7949-1-2-1.00][7949-1-2-1.00][7949-1-2-0.99][8135-1-0-0.98][8135-1-0-0.68][8135-1-0-0.94][8185-3-0-0.95][8185-3-0-0.84][8185-3-0-0.99]
[8269-3-2-0.70][8269-3-2-0.73][8269-3-1-0.52][8273-3-3-1.00][8273-3-3-0.99][8273-3-3-0.77][8543-3-0-1.00][8543-3-0-1.00][8543-3-0-1.00][8666-1-1-0.89]
[8666-1-1-0.99][8666-1-1-0.95][8672-0-0-1.00][8672-0-0-1.00][8672-0-0-0.52][8903-1-1-0.71][8903-1-2-0.71][8903-1-2-0.94][9001-2-0-0.87][9001-2-1-0.98]
[9001-2-1-0.97][9036-2-2-1.00][9036-2-2-1.00][9036-2-2-1.00][9281-3-3-0.43][9281-3-2-0.44][9281-3-1-0.37][9300-2-2-0.95][9300-2-2-0.99][9300-2-2-0.84]
[9571-0-3-0.57][9571-0-0-0.39][9571-0-3-0.55][9617-1-1-0.98][9617-1-1-0.52][9617-1-1-0.49][9644-2-2-0.55][9644-2-2-0.92][9644-2-2-0.95][9705-2-0-0.55]
[9705-2-0-0.43][9705-2-2-0.46][9801-0-0-0.99][9801-0-0-0.91][9801-0-0-0.69][9803-3-0-0.77][9803-3-3-0.71][9803-3-3-0.73][9865-3-3-0.98][9865-3-3-0.93]
[9865-3-3-0.72][9896-2-2-0.99][9896-2-2-0.86][9896-2-2-0.95][10314-1-2-0.87][10314-1-2-0.69][10314-1-2-0.78][10337-3-3-1.00][10337-3-3-1.00][10337-3-3-1.00]
[10403-0-0-0.58][10403-0-0-0.63][10403-0-0-0.68][10653-2-1-0.57][10653-2-2-0.65][10653-2-2-0.45][10704-2-2-1.00][10704-2-2-0.99][10704-2-2-0.98][10719-1-1-0.57]
[10719-1-1-0.78][10719-1-1-0.80][10727-1-1-0.92][10727-1-1-0.96][10727-1-1-0.94][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-1-0.38][10969-2-3-0.47]
[10969-2-3-0.44][11042-0-0-0.97][11042-0-0-0.98][11042-0-0-1.00][11088-1-2-0.62][11088-1-1-0.66][11088-1-1-0.93][11322-0-0-1.00][11322-0-0-1.00][11322-0-0-1.00]
[11398-2-2-1.00][11398-2-2-0.96][11398-2-2-0.62][11499-0-0-0.77][11499-0-0-0.93][11499-0-0-0.89][11502-3-0-0.46][11502-3-2-0.51][11502-3-0-0.66][11512-3-3-0.96]
[11512-3-3-0.72][11512-3-3-0.74][11608-1-2-0.99][11608-1-2-0.99][11608-1-2-0.88][11610-0-0-0.84][11610-0-0-0.95][11610-0-0-0.99][11692-0-0-1.00][11692-0-0-0.99]
[11692-0-0-1.00][11905-0-0-0.92][11905-0-0-0.54][11905-0-3-0.52][11993-1-1-0.62][11993-1-1-0.62][11993-1-1-0.67][12002-2-0-0.76][12002-2-2-0.68][12002-2-3-0.75]
[12052-0-0-0.69][12052-0-0-0.97][12052-0-0-0.86][12201-0-0-0.79][12201-0-3-0.57][12201-0-3-0.50][12235-2-2-0.99][12235-2-2-0.57][12235-2-2-0.79][12320-1-0-0.65]
[12320-1-0-0.65][12320-1-0-0.80][12377-2-2-0.58][12377-2-1-0.53][12377-2-2-0.62][12398-2-3-0.90][12398-2-3-0.77][12398-2-3-0.65][12503-1-2-1.00][12503-1-2-0.81]
[12503-1-2-0.64][12617-0-3-0.60][12617-0-3-0.52][12617-0-3-0.61][12685-3-2-0.48][12685-3-2-0.42][12685-3-2-0.42][12738-2-3-0.52][12738-2-0-0.56][12738-2-0-0.52]
[12742-2-2-0.96][12742-2-2-0.99][12742-2-2-1.00][12823-0-0-0.73][12823-0-0-0.51][12823-0-0-0.78][13110-1-1-1.00][13110-1-1-0.90][13110-1-2-0.57][13240-3-0-0.51]
[13240-3-3-0.62][13240-3-0-0.59][13253-1-1-0.75][13253-1-2-0.59][13253-1-1-0.62][13273-0-0-1.00][13273-0-0-1.00][13273-0-0-1.00][13634-1-2-0.98][13634-1-2-0.99]
[13634-1-2-0.98][13763-2-2-0.76][13763-2-2-0.98][13763-2-2-0.79][13905-3-0-0.81][13905-3-0-0.81][13905-3-0-0.74][14060-2-1-0.94][14060-2-1-0.99][14060-2-1-0.91]
[14065-3-0-0.90][14065-3-0-0.91][14065-3-0-0.52][14147-3-0-0.79][14147-3-3-0.51][14147-3-0-0.99][14595-2-2-0.97][14595-2-2-0.96][14595-2-2-0.98][14687-2-2-1.00]
[14687-2-2-1.00][14687-2-2-1.00][14788-2-2-0.97][14788-2-2-0.55][14788-2-2-0.89][14869-1-1-0.97][14869-1-1-0.93][14869-1-1-0.87][14872-3-0-0.38][14872-3-0-0.93]
[14872-3-0-0.61][14877-1-1-0.95][14877-1-1-0.97][14877-1-1-1.00][14927-0-0-0.51][14927-0-3-0.60][14927-0-3-0.71][15066-0-0-0.99][15066-0-0-0.95][15066-0-0-0.96]
[15175-1-1-0.40][15175-1-1-0.32][15175-1-1-0.63][15178-2-0-0.75][15178-2-2-0.59][15178-2-2-0.63][15375-3-0-0.75][15375-3-3-0.73][15375-3-0-0.51][15389-3-0-0.63]
[15389-3-3-0.61][15389-3-3-0.51][15568-2-1-0.74][15568-2-1-0.99][15568-2-1-0.98][15675-3-3-0.97][15675-3-3-1.00][15675-3-3-1.00][15869-1-2-0.79][15869-1-2-0.70]
[15869-1-2-0.80][16207-3-0-0.65][16207-3-0-0.85][16207-3-0-0.80][16236-0-0-0.64][16236-0-0-0.69][16236-0-0-0.72][16302-3-0-0.81][16302-3-0-0.83][16302-3-0-0.84]
[16331-2-2-1.00][16331-2-2-1.00][16331-2-2-1.00][16381-0-0-0.57][16381-0-3-0.56][16381-0-3-0.64][16488-1-1-0.98][16488-1-1-1.00][16488-1-1-0.98][16495-0-0-0.73]
[16495-0-0-1.00][16495-0-0-0.99][16650-0-0-1.00][16650-0-0-0.98][16650-0-0-0.98][16719-1-1-0.61][16719-1-1-0.58][16719-1-1-0.67][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.82][16828-0-0-0.98][16828-0-0-0.99][17137-3-0-1.00][17137-3-0-0.42][17137-3-0-0.99][17245-1-2-0.77][17245-1-2-0.89][17245-1-2-0.68]
[17278-3-0-0.58][17278-3-0-0.44][17278-3-0-0.76][17282-0-0-0.83][17282-0-0-0.64][17282-0-0-0.48][17311-2-2-1.00][17311-2-2-0.99][17311-2-2-1.00][17336-2-2-0.70]
[17336-2-2-0.85][17336-2-2-0.99][17608-3-3-1.00][17608-3-3-1.00][17608-3-3-1.00][17627-0-0-0.83][17627-0-2-0.66][17627-0-0-0.67][17877-3-1-0.82][17877-3-0-0.49]
[17877-3-0-0.69][17924-1-2-0.88][17924-1-2-0.55][17924-1-2-0.87][17984-3-3-0.91][17984-3-3-0.57][17984-3-3-0.73][18211-0-1-0.51][18211-0-3-0.86][18211-0-3-0.63]
[18276-3-0-0.73][18276-3-0-0.86][18276-3-0-0.61][18287-1-1-0.52][18287-1-1-0.62][18287-1-1-0.72][18394-0-0-1.00][18394-0-0-0.97][18394-0-0-0.96][18428-0-0-0.60]
[18428-0-0-0.62][18428-0-0-1.00][18442-0-3-0.81][18442-0-0-0.51][18442-0-0-0.55][18478-3-3-0.71][18478-3-3-0.84][18478-3-3-0.86][18607-0-0-1.00][18607-0-0-1.00]
[18607-0-0-1.00][18616-0-0-0.74][18616-0-0-0.86][18616-0-0-0.80][18663-0-3-0.70][18663-0-0-0.78][18663-0-0-0.86][18718-0-0-1.00][18718-0-0-1.00][18718-0-0-1.00]
[18766-2-2-1.00][18766-2-2-0.99][18766-2-2-0.99][18824-2-2-0.97][18824-2-2-0.56][18824-2-2-0.61][18890-3-3-0.91][18890-3-3-0.75][18890-3-3-0.94][18930-3-2-0.73]
[18930-3-2-0.82][18930-3-2-0.58][18938-3-2-0.96][18938-3-2-0.97][18938-3-2-0.64][19817-1-2-0.55][19817-1-1-0.67][19817-1-2-0.57][19839-0-0-0.90][19839-0-0-0.61]
[19839-0-0-0.83][19930-3-3-0.62][19930-3-3-0.52][19930-3-3-0.64][19944-0-0-0.63][19944-0-2-0.81][19944-0-2-0.88][20036-2-2-1.00][20036-2-2-1.00][20036-2-2-1.00]
[20101-3-2-0.50][20101-3-0-0.59][20101-3-0-0.78][20474-1-1-0.71][20474-1-1-0.76][20474-1-1-0.84][20547-3-2-0.60][20547-3-0-0.44][20547-3-3-0.68][20929-2-2-0.69]
[20929-2-2-0.99][20929-2-2-1.00][21245-1-1-0.62][21245-1-2-0.55][21245-1-1-0.72][21257-3-2-0.97][21257-3-2-0.79][21257-3-2-0.97][21293-1-2-0.98][21293-1-2-0.98]
[21293-1-2-0.98][21316-1-1-1.00][21316-1-1-1.00][21316-1-3-0.63][21384-1-2-0.98][21384-1-2-0.98][21384-1-2-0.98][21448-1-1-0.91][21448-1-1-0.81][21448-1-1-0.82]
[21483-0-0-0.96][21483-0-0-0.98][21483-0-0-0.98][21487-2-2-1.00][21487-2-2-0.97][21487-2-2-0.84][21714-0-0-0.82][21714-0-0-0.95][21714-0-0-0.83][21943-3-2-0.98]
[21943-3-2-1.00][21943-3-2-1.00][21947-0-0-0.81][21947-0-0-0.60][21947-0-3-0.45][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-1.00][21965-2-2-1.00]
[21965-2-2-0.86][21998-1-1-0.66][21998-1-1-1.00][21998-1-1-0.83][22025-0-2-0.92][22025-0-2-0.80][22025-0-1-0.57][22228-3-3-0.98][22228-3-3-0.98][22228-3-3-0.97]
[22446-1-1-1.00][22446-1-1-1.00][22446-1-1-0.98][22494-3-0-0.85][22494-3-0-0.60][22494-3-3-0.86][22757-0-3-0.81][22757-0-3-0.80][22757-0-3-0.75][22811-3-3-0.46]
[22811-3-3-0.99][22811-3-3-0.98][22976-3-2-0.78][22976-3-1-0.54][22976-3-1-0.88][22985-3-3-0.98][22985-3-3-0.99][22985-3-3-0.99][23014-0-0-0.99][23014-0-0-1.00]
[23014-0-0-1.00][23112-1-1-0.88][23112-1-1-0.64][23112-1-1-0.66][23144-3-3-1.00][23144-3-3-1.00][23144-3-3-0.93][23168-2-0-0.96][23168-2-0-0.89][23168-2-0-0.90]
[23219-0-0-0.45][23219-0-0-0.73][23219-0-0-0.71][23363-3-3-0.91][23363-3-3-0.97][23363-3-3-1.00][23470-0-0-0.80][23470-0-0-0.78][23470-0-0-0.63][23486-2-2-0.64]
[23486-2-2-0.73][23486-2-2-0.79][23497-0-0-0.52][23497-0-3-0.72][23497-0-3-0.60][23516-0-0-0.96][23516-0-0-1.00][23516-0-0-1.00][23690-1-1-0.94][23690-1-2-0.71]
[23690-1-2-0.68][23921-2-1-0.54][23921-2-2-0.81][23921-2-2-0.85][23936-1-2-1.00][23936-1-2-1.00][23936-1-2-0.99][24040-3-2-0.68][24040-3-2-0.55][24040-3-2-0.69]
[24111-1-2-0.66][24111-1-2-0.64][24111-1-2-0.67][24182-0-0-1.00][24182-0-0-0.99][24182-0-0-1.00][24238-3-3-0.63][24238-3-3-0.60][24238-3-3-0.71][24290-2-0-1.00]
[24290-2-0-0.93][24290-2-0-0.92][24345-0-0-0.92][24345-0-0-0.98][24345-0-2-0.79][24364-1-2-0.89][24364-1-2-0.98][24364-1-2-0.98][24427-3-0-0.96][24427-3-0-0.95]
[24427-3-0-0.91][24477-2-2-0.99][24477-2-2-0.99][24477-2-2-0.99][24495-2-1-0.82][24495-2-1-0.74][24495-2-1-0.74][24893-2-2-0.99][24893-2-2-0.98][24893-2-2-0.99]
[25012-1-2-0.72][25012-1-2-0.69][25012-1-2-0.61][25121-2-1-0.59][25121-2-2-0.51][25121-2-1-0.53][25165-3-3-0.97][25165-3-3-0.98][25165-3-3-0.97][25183-0-0-1.00]
[25183-0-0-1.00][25183-0-0-1.00][25297-3-3-0.94][25297-3-2-0.67][25297-3-3-0.55][25398-0-0-0.70][25398-0-0-0.87][25398-0-0-0.88][25574-2-2-0.93][25574-2-2-0.97]
[25574-2-2-1.00][25644-1-2-0.89][25644-1-2-0.66][25644-1-1-0.77][25718-1-1-0.45][25718-1-0-0.73][25718-1-0-0.68][25774-2-2-0.93][25774-2-2-0.90][25774-2-2-0.96]
[26032-3-0-0.74][26032-3-3-0.69][26032-3-3-0.65][26051-3-3-0.99][26051-3-3-1.00][26051-3-3-0.99][26120-0-0-0.56][26120-0-2-0.69][26120-0-0-0.89][26321-1-1-0.50]
[26321-1-1-0.60][26321-1-2-0.89][26732-1-1-0.86][26732-1-1-0.99][26732-1-1-0.89][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-0.93][26827-3-3-0.93]
[26827-3-3-0.84][26833-0-3-1.00][26833-0-3-1.00][26833-0-3-0.99][26838-2-2-0.56][26838-2-2-0.71][26838-2-2-0.73][26860-1-2-0.64][26860-1-2-0.98][26860-1-2-0.89]
[26948-0-0-0.72][26948-0-0-0.56][26948-0-0-0.61][27049-3-0-0.99][27049-3-0-0.94][27049-3-0-0.99][27098-1-1-0.62][27098-1-0-0.37][27098-1-0-0.65][27526-0-0-0.85]
[27526-0-0-0.71][27526-0-0-0.45][27639-3-3-0.97][27639-3-3-0.97][27639-3-3-0.98][27698-3-3-0.93][27698-3-3-0.98][27698-3-3-0.97][27772-0-0-1.00][27772-0-0-1.00]
[27772-0-0-0.88][27890-1-1-0.80][27890-1-1-0.84][27890-1-1-0.80][28040-0-1-0.57][28040-0-0-0.51][28040-0-0-0.95][28503-2-2-1.00][28503-2-2-1.00][28503-2-2-1.00]
[28577-1-1-0.86][28577-1-1-0.89][28577-1-1-0.90][28959-0-0-1.00][28959-0-0-0.99][28959-0-0-1.00][29198-3-1-0.87][29198-3-1-0.90][29198-3-1-0.88][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-2-0.66][29877-2-2-0.43][29877-2-1-0.49][30035-1-1-0.60][30035-1-2-0.85][30035-1-2-0.80][30098-0-0-0.78][30098-0-0-0.95]
[30098-0-0-0.99][30326-1-1-0.99][30326-1-1-1.00][30326-1-1-0.98][30572-2-2-0.95][30572-2-2-0.63][30572-2-2-0.96][30716-0-1-0.68][30716-0-1-0.63][30716-0-0-0.57]
[30806-2-2-0.99][30806-2-2-0.98][30806-2-2-1.00][30906-1-1-0.99][30906-1-1-0.93][30906-1-1-0.70][31007-0-0-0.93][31007-0-0-0.99][31007-0-0-0.77][31181-3-0-0.61]
[31181-3-2-0.53][31181-3-2-0.89][31238-0-3-0.55][31238-0-3-0.89][31238-0-3-0.70][31347-0-0-1.00][31347-0-0-1.00][31347-0-0-1.00][31422-2-2-0.95][31422-2-2-0.97]
[31422-2-2-0.81][31429-3-3-0.99][31429-3-3-0.98][31429-3-3-0.99][31431-0-0-1.00][31431-0-0-0.68][31431-0-3-0.54][31432-1-1-0.95][31432-1-1-0.77][31432-1-1-0.85]
[31477-0-0-0.77][31477-0-0-0.87][31477-0-0-0.81][31524-1-2-0.65][31524-1-2-0.49][31524-1-0-0.48][31597-1-2-0.98][31597-1-2-0.89][31597-1-2-0.83][31619-1-0-0.91]
[31619-1-0-0.95][31619-1-0-0.98][31701-0-0-0.99][31701-0-0-0.97][31701-0-0-0.86][31755-0-0-1.00][31755-0-0-0.98][31755-0-0-0.96][31854-3-3-0.60][31854-3-0-0.37]
[31854-3-2-0.67][32074-1-2-0.54][32074-1-1-0.50][32074-1-3-0.82][32078-3-3-0.99][32078-3-3-0.99][32078-3-3-1.00][32111-1-1-0.69][32111-1-1-0.74][32111-1-1-0.79]
[32127-1-2-0.79][32127-1-2-0.95][32127-1-2-0.61][32140-3-3-0.80][32140-3-3-0.76][32140-3-3-0.97][32263-2-0-0.62][32263-2-2-0.50][32263-2-0-0.64][32365-0-0-0.97]
[32365-0-0-0.96][32365-0-0-0.99][32411-2-0-0.96][32411-2-0-0.96][32411-2-0-0.99][32429-3-3-0.96][32429-3-3-0.97][32429-3-3-0.96][32473-3-0-0.92][32473-3-0-0.92]
[32473-3-0-0.92][32574-3-3-0.97][32574-3-3-0.81][32574-3-3-0.79][32584-0-0-0.98][32584-0-0-0.98][32584-0-0-0.97][32622-0-1-0.42][32622-0-1-0.32][32622-0-1-0.52]
[32858-3-3-0.72][32858-3-3-0.77][32858-3-3-0.75][32969-3-3-0.60][32969-3-3-0.72][32969-3-3-0.63][33016-2-2-0.90][33016-2-2-0.99][33016-2-2-0.99][33031-1-3-0.89]
[33031-1-3-0.92][33031-1-3-0.86][33035-2-2-1.00][33035-2-2-1.00][33035-2-2-1.00][33133-2-2-0.87][33133-2-2-0.94][33133-2-2-0.68][33173-2-1-0.50][33173-2-2-0.55]
[33173-2-2-0.89][33175-3-2-1.00][33175-3-2-1.00][33175-3-2-1.00][33306-3-1-0.71][33306-3-1-0.68][33306-3-1-0.58][33309-2-3-0.69][33309-2-3-0.89][33309-2-3-0.86]
[33474-0-3-0.50][33474-0-3-0.48][33474-0-3-0.55][33478-2-3-0.32][33478-2-2-0.55][33478-2-2-0.31][33618-1-1-0.87][33618-1-1-0.85][33618-1-1-0.84][33712-0-0-0.83]
[33712-0-0-0.90][33712-0-0-0.99][33782-2-1-0.82][33782-2-2-0.60][33782-2-2-0.53][33914-3-2-0.90][33914-3-3-0.82][33914-3-3-0.90][34076-3-2-0.85][34076-3-2-0.82]
[34076-3-3-0.53][34112-2-2-1.00][34112-2-2-0.88][34112-2-2-0.94][34138-2-2-0.95][34138-2-2-0.96][34138-2-2-0.97][34239-1-2-0.69][34239-1-2-0.73][34239-1-2-0.93]
[34364-2-2-0.87][34364-2-2-1.00][34364-2-2-1.00][34617-1-1-0.55][34617-1-2-0.62][34617-1-2-0.57][34751-3-3-1.00][34751-3-3-0.98][34751-3-3-0.91][34783-2-2-0.54]
[34783-2-2-0.87][34783-2-2-0.64][35015-3-3-0.62][35015-3-2-0.53][35015-3-3-0.65][35018-1-2-0.61][35018-1-2-0.54][35018-1-2-0.60][35288-2-2-0.64][35288-2-1-0.69]
[35288-2-1-0.71]
---------------------------
I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 0.856 | Acc: 93.375% | Wgt Acc: 93.377%
	I - Batch: 100 | Loss: 0.850 | Acc: 93.375% | Wgt Acc: 93.342%
	I - Batch: 150 | Loss: 0.855 | Acc: 92.917% | Wgt Acc: 92.924%
	I - Batch: 200 | Loss: 0.856 | Acc: 92.781% | Wgt Acc: 92.747%
	I - Batch: 250 | Loss: 0.856 | Acc: 92.725% | Wgt Acc: 92.691%
	I - Batch: 300 | Loss: 0.857 | Acc: 92.375% | Wgt Acc: 92.320%
	I - Batch: 350 | Loss: 0.857 | Acc: 92.393% | Wgt Acc: 92.352%
	I - Batch: 400 | Loss: 0.857 | Acc: 92.375% | Wgt Acc: 92.339%
	I - Batch: 450 | Loss: 0.857 | Acc: 92.403% | Wgt Acc: 92.385%
I - num batch: 478
I - Train -- Loss: 0.857 | Acc: 92.422% | Wgt Acc: 92.413% | LR: 5.000000e-04 | Dur: 354.52s
I - Confusion Matrix: [row->prediction - col->label]
[[1900.   30.   33.   84.]
 [  48. 1652.   57.   32.]
 [  52.   36. 2071.   59.]
 [  91.   16.   41. 1439.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.108 | Acc: 63.875% | Wgt Acc: 63.409%
I - num batch: 62
I - Val -- Loss: 1.105 | Acc: 64.832% | Wgt Acc: 64.380% | Dur: 36.43s
I - Confusion Matrix: [row->prediction - col->label]
[[201.  20.  25.  50.]
 [  4. 139.  51.  21.]
 [  9.  61. 136.  27.]
 [ 50.  14.  13. 160.]]

I - Local maximum validation set accuracy:  64.83

I - Validation set results: 
[14-1-2-0.66][14-1-1-0.54][14-1-1-0.64][50-3-3-0.73][50-3-3-0.52][50-3-0-0.55][124-2-2-0.36][124-2-2-0.74][124-2-1-0.65][127-0-0-1.00]
[127-0-0-1.00][127-0-0-1.00][443-2-2-1.00][443-2-2-1.00][443-2-2-1.00][567-0-0-0.95][567-0-0-0.94][567-0-0-0.98][573-1-1-0.44][573-1-1-0.64]
[573-1-1-0.50][615-0-3-0.52][615-0-3-0.61][615-0-3-0.69][695-1-2-0.83][695-1-2-0.95][695-1-2-0.85][722-3-0-0.64][722-3-3-0.61][722-3-0-0.67]
[826-0-0-0.99][826-0-0-1.00][826-0-0-1.00][878-0-0-0.70][878-0-0-0.96][878-0-0-1.00][1103-0-0-0.90][1103-0-0-0.72][1103-0-0-0.67][1212-3-3-0.88]
[1212-3-3-0.86][1212-3-3-0.70][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-0-0.56][2181-2-0-0.49][2181-2-0-0.54][2476-2-1-0.50][2476-2-1-0.42]
[2476-2-1-0.55][2721-2-2-0.92][2721-2-2-0.98][2721-2-2-0.99][2818-1-2-0.44][2818-1-3-0.69][2818-1-3-0.80][2886-2-1-0.67][2886-2-1-0.55][2886-2-1-0.72]
[3231-2-2-0.88][3231-2-2-0.82][3231-2-2-0.73][3333-2-2-0.91][3333-2-2-0.90][3333-2-2-0.94][3482-2-2-0.99][3482-2-2-0.99][3482-2-2-1.00][3536-3-3-0.69]
[3536-3-3-0.84][3536-3-3-0.76][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-0-0.98][3909-0-0-0.87][3909-0-0-0.86][4035-0-0-0.97][4035-0-0-0.97]
[4035-0-0-0.96][4140-0-0-0.98][4140-0-0-0.93][4140-0-0-0.89][4214-1-3-0.93][4214-1-1-0.56][4214-1-1-1.00][4346-1-0-1.00][4346-1-0-1.00][4346-1-0-1.00]
[4581-2-2-0.63][4581-2-2-0.70][4581-2-2-0.55][4708-3-2-0.87][4708-3-2-0.81][4708-3-2-0.62][4838-3-0-0.67][4838-3-0-0.82][4838-3-0-0.57][4845-1-2-0.52]
[4845-1-1-0.57][4845-1-2-0.50][4868-0-0-1.00][4868-0-0-1.00][4868-0-0-0.98][4939-0-3-0.39][4939-0-2-0.46][4939-0-2-0.73][4984-2-2-0.60][4984-2-2-0.69]
[4984-2-2-0.85][5078-1-2-0.63][5078-1-2-0.74][5078-1-2-0.73][5396-0-0-0.99][5396-0-0-0.99][5396-0-0-0.99][5479-1-1-0.85][5479-1-1-0.96][5479-1-1-0.82]
[5717-0-0-0.88][5717-0-0-0.93][5717-0-0-0.68][5843-1-1-0.89][5843-1-1-0.75][5843-1-1-1.00][5949-3-3-0.61][5949-3-3-0.63][5949-3-3-0.86][5987-2-1-0.64]
[5987-2-1-0.73][5987-2-1-0.66][6014-3-1-0.62][6014-3-3-0.50][6014-3-3-0.50][6033-3-0-0.93][6033-3-1-0.53][6033-3-0-0.85][6313-0-0-1.00][6313-0-0-0.98]
[6313-0-0-0.92][6421-3-3-0.99][6421-3-3-0.99][6421-3-3-1.00][6500-1-3-0.61][6500-1-3-0.50][6500-1-3-0.56][6583-3-3-0.97][6583-3-3-0.99][6583-3-3-0.94]
[6683-3-3-0.79][6683-3-3-0.82][6683-3-3-0.96][6825-2-1-0.56][6825-2-0-0.54][6825-2-0-0.53][6998-3-3-0.44][6998-3-3-0.55][6998-3-3-0.50][7049-3-3-0.94]
[7049-3-3-0.80][7049-3-3-0.93][7517-1-1-0.93][7517-1-1-0.96][7517-1-1-1.00][7521-1-1-0.51][7521-1-1-0.92][7521-1-1-0.81][7528-1-3-0.59][7528-1-2-0.75]
[7528-1-2-0.82][7949-1-2-0.99][7949-1-2-1.00][7949-1-2-0.91][8135-1-0-1.00][8135-1-0-0.75][8135-1-0-0.98][8185-3-0-0.71][8185-3-3-0.84][8185-3-0-0.88]
[8269-3-2-0.67][8269-3-1-0.48][8269-3-1-1.00][8273-3-3-1.00][8273-3-3-0.99][8273-3-3-0.95][8543-3-0-0.94][8543-3-0-1.00][8543-3-0-1.00][8666-1-1-0.77]
[8666-1-1-0.77][8666-1-1-0.95][8672-0-0-0.93][8672-0-0-0.90][8672-0-3-0.56][8903-1-1-0.64][8903-1-2-0.65][8903-1-2-0.99][9001-2-0-0.87][9001-2-1-0.99]
[9001-2-1-0.99][9036-2-2-0.99][9036-2-2-0.99][9036-2-2-1.00][9281-3-1-0.55][9281-3-2-0.30][9281-3-1-0.47][9300-2-2-0.96][9300-2-2-0.98][9300-2-2-0.97]
[9571-0-3-0.71][9571-0-3-0.52][9571-0-3-0.51][9617-1-1-0.95][9617-1-1-0.52][9617-1-1-0.53][9644-2-2-0.67][9644-2-2-0.77][9644-2-2-0.91][9705-2-0-0.56]
[9705-2-0-0.55][9705-2-0-0.46][9801-0-0-0.79][9801-0-0-0.67][9801-0-3-0.72][9803-3-0-0.84][9803-3-3-0.62][9803-3-3-0.68][9865-3-3-0.90][9865-3-3-0.72]
[9865-3-3-0.53][9896-2-2-0.79][9896-2-1-0.65][9896-2-2-0.82][10314-1-1-0.45][10314-1-1-0.36][10314-1-2-0.42][10337-3-3-1.00][10337-3-3-1.00][10337-3-3-1.00]
[10403-0-0-0.67][10403-0-0-0.66][10403-0-0-0.66][10653-2-1-0.55][10653-2-2-0.76][10653-2-2-0.52][10704-2-2-0.86][10704-2-2-0.54][10704-2-1-0.50][10719-1-1-0.83]
[10719-1-1-1.00][10719-1-1-1.00][10727-1-1-0.70][10727-1-1-0.95][10727-1-1-0.94][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-3-0.82][10969-2-3-0.97]
[10969-2-3-0.94][11042-0-0-0.91][11042-0-0-0.81][11042-0-0-1.00][11088-1-1-0.99][11088-1-1-1.00][11088-1-1-1.00][11322-0-0-1.00][11322-0-0-0.99][11322-0-0-1.00]
[11398-2-2-0.94][11398-2-2-0.70][11398-2-0-0.55][11499-0-0-0.63][11499-0-0-0.88][11499-0-0-0.78][11502-3-3-0.64][11502-3-3-0.44][11502-3-3-0.57][11512-3-3-0.92]
[11512-3-3-0.71][11512-3-3-0.68][11608-1-2-0.91][11608-1-2-0.96][11608-1-2-0.64][11610-0-3-0.64][11610-0-3-0.86][11610-0-0-0.75][11692-0-0-0.76][11692-0-0-0.59]
[11692-0-0-0.53][11905-0-0-0.96][11905-0-0-0.77][11905-0-0-0.85][11993-1-1-0.98][11993-1-1-0.97][11993-1-1-0.97][12002-2-0-0.66][12002-2-2-0.59][12002-2-3-0.75]
[12052-0-0-0.70][12052-0-0-0.99][12052-0-0-0.83][12201-0-3-0.91][12201-0-3-0.97][12201-0-3-0.85][12235-2-2-0.98][12235-2-2-0.54][12235-2-2-0.59][12320-1-0-0.96]
[12320-1-0-0.95][12320-1-0-0.96][12377-2-2-0.64][12377-2-1-0.60][12377-2-1-0.55][12398-2-3-0.95][12398-2-3-0.96][12398-2-3-0.96][12503-1-1-0.84][12503-1-1-0.64]
[12503-1-1-0.53][12617-0-2-0.56][12617-0-2-0.58][12617-0-2-0.47][12685-3-1-0.56][12685-3-1-0.56][12685-3-1-0.44][12738-2-1-0.48][12738-2-0-0.44][12738-2-0-0.45]
[12742-2-2-0.99][12742-2-2-1.00][12742-2-2-1.00][12823-0-0-0.62][12823-0-3-0.70][12823-0-0-0.68][13110-1-1-0.99][13110-1-1-0.93][13110-1-2-0.54][13240-3-3-0.98]
[13240-3-3-0.98][13240-3-3-0.97][13253-1-1-0.92][13253-1-1-0.82][13253-1-1-0.96][13273-0-0-1.00][13273-0-0-1.00][13273-0-0-0.99][13634-1-2-0.81][13634-1-2-0.88]
[13634-1-2-0.87][13763-2-2-0.76][13763-2-2-0.94][13763-2-2-0.73][13905-3-3-0.54][13905-3-3-0.49][13905-3-3-0.48][14060-2-1-0.97][14060-2-1-1.00][14060-2-1-0.99]
[14065-3-0-0.48][14065-3-0-0.66][14065-3-3-0.57][14147-3-0-0.60][14147-3-3-0.76][14147-3-0-0.69][14595-2-1-0.76][14595-2-1-0.75][14595-2-1-0.67][14687-2-2-1.00]
[14687-2-2-1.00][14687-2-2-1.00][14788-2-2-0.72][14788-2-3-0.52][14788-2-1-0.58][14869-1-1-0.99][14869-1-1-0.97][14869-1-1-0.83][14872-3-0-0.56][14872-3-0-0.99]
[14872-3-0-0.70][14877-1-1-0.87][14877-1-1-0.91][14877-1-1-0.99][14927-0-3-0.79][14927-0-3-0.83][14927-0-3-0.84][15066-0-0-0.59][15066-0-3-0.51][15066-0-3-0.60]
[15175-1-1-0.79][15175-1-1-0.76][15175-1-1-0.94][15178-2-2-0.72][15178-2-2-0.94][15178-2-2-0.98][15375-3-3-0.50][15375-3-3-0.76][15375-3-0-0.76][15389-3-3-0.98]
[15389-3-3-0.99][15389-3-3-1.00][15568-2-1-0.58][15568-2-1-0.97][15568-2-1-0.98][15675-3-3-1.00][15675-3-3-1.00][15675-3-3-1.00][15869-1-2-0.70][15869-1-2-0.61]
[15869-1-3-0.52][16207-3-0-0.72][16207-3-0-0.84][16207-3-0-0.88][16236-0-2-0.53][16236-0-0-0.58][16236-0-0-0.51][16302-3-0-0.63][16302-3-0-0.53][16302-3-0-0.69]
[16331-2-2-1.00][16331-2-2-1.00][16331-2-2-1.00][16381-0-0-0.52][16381-0-3-0.68][16381-0-3-0.78][16488-1-1-0.98][16488-1-1-1.00][16488-1-1-1.00][16495-0-0-0.78]
[16495-0-0-1.00][16495-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16719-1-2-0.36][16719-1-2-0.62][16719-1-2-0.69][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.97][16828-0-0-1.00][16828-0-0-1.00][17137-3-0-0.98][17137-3-3-0.58][17137-3-0-0.95][17245-1-2-0.55][17245-1-2-0.70][17245-1-2-0.37]
[17278-3-0-0.55][17278-3-0-0.39][17278-3-0-0.51][17282-0-0-0.95][17282-0-0-0.80][17282-0-0-0.41][17311-2-2-0.98][17311-2-2-0.97][17311-2-2-0.99][17336-2-1-0.79]
[17336-2-1-0.64][17336-2-2-0.65][17608-3-3-1.00][17608-3-3-1.00][17608-3-3-1.00][17627-0-0-0.90][17627-0-0-0.40][17627-0-0-0.52][17877-3-1-1.00][17877-3-1-0.50]
[17877-3-0-0.61][17924-1-2-0.48][17924-1-3-0.50][17924-1-2-0.55][17984-3-3-0.89][17984-3-0-0.59][17984-3-3-0.80][18211-0-3-0.43][18211-0-3-0.74][18211-0-3-0.69]
[18276-3-0-0.62][18276-3-0-0.69][18276-3-3-0.54][18287-1-1-0.59][18287-1-1-0.73][18287-1-1-0.64][18394-0-0-0.98][18394-0-0-0.91][18394-0-0-0.90][18428-0-0-0.97]
[18428-0-0-0.93][18428-0-0-0.99][18442-0-3-0.99][18442-0-3-0.97][18442-0-3-0.88][18478-3-3-0.96][18478-3-3-0.98][18478-3-3-0.97][18607-0-0-1.00][18607-0-0-1.00]
[18607-0-0-1.00][18616-0-0-0.99][18616-0-0-1.00][18616-0-0-1.00][18663-0-3-0.70][18663-0-0-0.59][18663-0-0-0.77][18718-0-0-1.00][18718-0-0-1.00][18718-0-0-0.99]
[18766-2-2-0.81][18766-2-2-0.66][18766-2-2-0.63][18824-2-2-0.98][18824-2-2-0.56][18824-2-2-0.87][18890-3-1-0.44][18890-3-3-0.53][18890-3-3-0.74][18930-3-2-0.71]
[18930-3-2-0.78][18930-3-2-0.60][18938-3-2-0.81][18938-3-2-0.78][18938-3-2-0.52][19817-1-2-0.54][19817-1-1-0.60][19817-1-2-0.52][19839-0-0-0.97][19839-0-0-0.81]
[19839-0-0-0.96][19930-3-3-0.65][19930-3-3-0.76][19930-3-3-0.87][19944-0-0-0.64][19944-0-1-0.44][19944-0-1-0.42][20036-2-2-1.00][20036-2-2-0.98][20036-2-2-0.99]
[20101-3-1-0.50][20101-3-3-0.54][20101-3-0-0.47][20474-1-1-0.99][20474-1-1-0.99][20474-1-1-0.99][20547-3-0-0.32][20547-3-0-0.67][20547-3-0-0.58][20929-2-2-0.52]
[20929-2-2-0.99][20929-2-2-0.99][21245-1-1-0.98][21245-1-1-0.99][21245-1-1-0.99][21257-3-3-0.87][21257-3-3-0.57][21257-3-2-0.99][21293-1-1-0.76][21293-1-1-0.79]
[21293-1-1-0.74][21316-1-1-1.00][21316-1-1-1.00][21316-1-3-0.60][21384-1-2-0.99][21384-1-2-0.99][21384-1-2-0.99][21448-1-1-0.99][21448-1-1-0.97][21448-1-1-0.93]
[21483-0-0-1.00][21483-0-0-1.00][21483-0-0-1.00][21487-2-2-0.98][21487-2-2-0.99][21487-2-2-0.91][21714-0-0-0.51][21714-0-0-0.80][21714-0-0-0.71][21943-3-2-0.89]
[21943-3-2-0.79][21943-3-2-0.97][21947-0-0-0.96][21947-0-0-0.78][21947-0-3-0.45][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-1.00][21965-2-2-1.00]
[21965-2-2-0.62][21998-1-1-0.64][21998-1-1-0.99][21998-1-1-0.90][22025-0-2-1.00][22025-0-2-0.99][22025-0-2-0.78][22228-3-3-1.00][22228-3-3-1.00][22228-3-3-1.00]
[22446-1-1-1.00][22446-1-1-1.00][22446-1-1-1.00][22494-3-0-0.90][22494-3-0-0.69][22494-3-3-0.52][22757-0-3-0.82][22757-0-3-0.69][22757-0-3-0.61][22811-3-3-0.55]
[22811-3-3-1.00][22811-3-3-1.00][22976-3-2-0.74][22976-3-1-0.64][22976-3-1-0.96][22985-3-3-0.99][22985-3-3-0.98][22985-3-3-0.99][23014-0-3-0.61][23014-0-3-0.68]
[23014-0-3-0.62][23112-1-1-0.98][23112-1-1-0.92][23112-1-1-0.94][23144-3-3-1.00][23144-3-3-0.99][23144-3-3-0.61][23168-2-0-1.00][23168-2-0-1.00][23168-2-0-1.00]
[23219-0-0-0.42][23219-0-0-0.80][23219-0-0-0.71][23363-3-3-0.58][23363-3-3-0.87][23363-3-3-0.99][23470-0-0-0.91][23470-0-0-0.75][23470-0-1-0.40][23486-2-2-0.69]
[23486-2-2-0.63][23486-2-2-0.64][23497-0-3-0.97][23497-0-3-0.95][23497-0-3-0.95][23516-0-0-0.89][23516-0-0-1.00][23516-0-0-1.00][23690-1-1-1.00][23690-1-1-0.73]
[23690-1-1-0.99][23921-2-1-0.79][23921-2-2-0.61][23921-2-1-0.53][23936-1-2-0.91][23936-1-2-1.00][23936-1-2-0.99][24040-3-2-0.64][24040-3-2-0.60][24040-3-2-0.67]
[24111-1-1-0.85][24111-1-1-0.87][24111-1-1-0.85][24182-0-0-0.98][24182-0-0-0.94][24182-0-0-1.00][24238-3-3-0.97][24238-3-3-0.97][24238-3-3-0.99][24290-2-0-1.00]
[24290-2-0-0.96][24290-2-0-0.97][24345-0-0-0.79][24345-0-0-0.98][24345-0-0-0.72][24364-1-2-0.53][24364-1-2-0.91][24364-1-2-0.89][24427-3-0-0.88][24427-3-0-0.88]
[24427-3-0-0.57][24477-2-2-0.87][24477-2-2-0.81][24477-2-2-0.57][24495-2-1-0.89][24495-2-1-0.75][24495-2-1-0.72][24893-2-2-0.83][24893-2-2-0.65][24893-2-2-0.58]
[25012-1-2-0.74][25012-1-2-0.70][25012-1-2-0.63][25121-2-2-0.51][25121-2-2-0.48][25121-2-0-0.50][25165-3-3-0.66][25165-3-3-0.73][25165-3-3-0.71][25183-0-0-1.00]
[25183-0-0-1.00][25183-0-0-1.00][25297-3-3-0.99][25297-3-3-0.81][25297-3-3-0.91][25398-0-0-0.73][25398-0-0-0.94][25398-0-0-0.95][25574-2-1-0.58][25574-2-2-0.85]
[25574-2-2-0.99][25644-1-2-0.83][25644-1-2-0.72][25644-1-1-0.58][25718-1-1-0.76][25718-1-0-0.66][25718-1-0-0.65][25774-2-2-0.84][25774-2-2-0.66][25774-2-2-0.66]
[26032-3-3-0.98][26032-3-3-0.99][26032-3-3-0.97][26051-3-3-1.00][26051-3-3-1.00][26051-3-3-1.00][26120-0-0-0.74][26120-0-0-0.73][26120-0-0-0.97][26321-1-1-0.91]
[26321-1-1-0.89][26321-1-2-0.63][26732-1-1-0.90][26732-1-1-1.00][26732-1-1-0.98][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-0.96][26827-3-3-0.94]
[26827-3-3-0.67][26833-0-3-1.00][26833-0-3-1.00][26833-0-3-1.00][26838-2-1-0.61][26838-2-1-0.43][26838-2-2-0.43][26860-1-1-0.41][26860-1-2-0.73][26860-1-2-0.48]
[26948-0-0-0.87][26948-0-0-0.85][26948-0-0-0.95][27049-3-0-0.93][27049-3-0-0.81][27049-3-0-0.90][27098-1-1-0.83][27098-1-1-0.48][27098-1-0-0.63][27526-0-0-0.98]
[27526-0-0-0.87][27526-0-0-0.70][27639-3-3-0.95][27639-3-3-0.95][27639-3-3-0.98][27698-3-3-0.92][27698-3-3-0.87][27698-3-3-0.79][27772-0-0-0.99][27772-0-0-0.98]
[27772-0-3-0.47][27890-1-1-0.86][27890-1-1-0.85][27890-1-1-0.81][28040-0-1-0.39][28040-0-0-0.51][28040-0-0-0.76][28503-2-2-1.00][28503-2-2-1.00][28503-2-2-1.00]
[28577-1-1-0.96][28577-1-1-0.97][28577-1-1-0.98][28959-0-0-1.00][28959-0-0-0.99][28959-0-0-1.00][29198-3-1-0.65][29198-3-1-0.70][29198-3-1-0.66][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-1-0.53][29877-2-1-0.50][29877-2-1-0.71][30035-1-1-0.99][30035-1-1-0.68][30035-1-1-0.91][30098-0-3-0.70][30098-0-0-0.68]
[30098-0-0-0.82][30326-1-1-1.00][30326-1-1-1.00][30326-1-1-1.00][30572-2-2-0.88][30572-2-2-0.67][30572-2-2-0.99][30716-0-0-0.55][30716-0-0-0.61][30716-0-0-0.77]
[30806-2-2-0.93][30806-2-2-0.79][30806-2-2-0.95][30906-1-1-1.00][30906-1-1-1.00][30906-1-1-0.93][31007-0-0-0.97][31007-0-0-0.99][31007-0-0-0.72][31181-3-0-0.45]
[31181-3-2-0.42][31181-3-2-0.78][31238-0-3-0.75][31238-0-3-0.99][31238-0-3-0.92][31347-0-0-0.98][31347-0-0-0.78][31347-0-0-0.81][31422-2-1-0.80][31422-2-1-0.45]
[31422-2-1-0.39][31429-3-3-0.86][31429-3-3-0.79][31429-3-3-0.84][31431-0-0-0.98][31431-0-0-0.73][31431-0-0-0.64][31432-1-1-0.95][31432-1-1-0.99][31432-1-1-0.98]
[31477-0-3-0.73][31477-0-3-0.60][31477-0-3-0.66][31524-1-2-0.46][31524-1-3-0.41][31524-1-1-0.42][31597-1-2-0.86][31597-1-2-0.67][31597-1-2-0.58][31619-1-0-0.99]
[31619-1-0-0.99][31619-1-0-1.00][31701-0-0-0.99][31701-0-0-0.95][31701-0-0-0.62][31755-0-0-1.00][31755-0-0-0.98][31755-0-0-0.99][31854-3-3-0.95][31854-3-3-0.88]
[31854-3-3-0.49][32074-1-1-0.54][32074-1-1-0.44][32074-1-3-0.73][32078-3-3-1.00][32078-3-3-1.00][32078-3-3-1.00][32111-1-1-0.78][32111-1-1-1.00][32111-1-1-0.99]
[32127-1-1-0.94][32127-1-1-0.79][32127-1-1-0.84][32140-3-3-1.00][32140-3-3-1.00][32140-3-3-1.00][32263-2-0-0.71][32263-2-2-0.48][32263-2-0-0.42][32365-0-0-1.00]
[32365-0-0-0.99][32365-0-0-0.99][32411-2-0-0.94][32411-2-0-0.83][32411-2-0-0.82][32429-3-3-0.99][32429-3-3-0.99][32429-3-3-0.99][32473-3-3-0.88][32473-3-3-0.84]
[32473-3-3-0.80][32574-3-3-1.00][32574-3-3-1.00][32574-3-3-1.00][32584-0-0-0.94][32584-0-0-0.96][32584-0-0-0.93][32622-0-0-0.61][32622-0-0-0.66][32622-0-0-0.50]
[32858-3-3-0.89][32858-3-3-0.88][32858-3-3-0.88][32969-3-3-0.98][32969-3-3-0.99][32969-3-3-0.99][33016-2-2-0.65][33016-2-2-0.82][33016-2-2-0.62][33031-1-3-0.52]
[33031-1-3-0.67][33031-1-0-0.61][33035-2-2-0.74][33035-2-2-0.83][33035-2-2-0.92][33133-2-2-0.90][33133-2-2-0.94][33133-2-2-0.55][33173-2-3-0.55][33173-2-2-0.51]
[33173-2-2-0.94][33175-3-2-0.68][33175-3-2-0.72][33175-3-2-0.67][33306-3-1-0.90][33306-3-1-0.88][33306-3-1-0.86][33309-2-3-0.74][33309-2-3-0.82][33309-2-3-0.91]
[33474-0-0-0.71][33474-0-0-0.80][33474-0-0-0.97][33478-2-2-0.44][33478-2-2-0.64][33478-2-2-0.49][33618-1-0-0.49][33618-1-0-0.55][33618-1-0-0.53][33712-0-0-0.90]
[33712-0-0-0.87][33712-0-0-0.99][33782-2-1-0.71][33782-2-2-0.76][33782-2-2-0.64][33914-3-2-0.84][33914-3-3-0.86][33914-3-3-0.94][34076-3-2-0.85][34076-3-2-0.87]
[34076-3-3-0.77][34112-2-2-1.00][34112-2-2-0.80][34112-2-2-0.95][34138-2-2-0.67][34138-2-2-0.66][34138-2-2-0.72][34239-1-1-0.84][34239-1-1-0.50][34239-1-2-0.46]
[34364-2-1-0.83][34364-2-2-0.59][34364-2-2-0.96][34617-1-1-0.59][34617-1-2-0.63][34617-1-0-0.48][34751-3-3-1.00][34751-3-3-1.00][34751-3-3-0.99][34783-2-1-0.84]
[34783-2-2-0.60][34783-2-1-0.56][35015-3-3-0.94][35015-3-3-0.58][35015-3-3-0.80][35018-1-1-0.69][35018-1-1-0.53][35018-1-2-0.51][35288-2-3-0.95][35288-2-1-0.82]
[35288-2-1-0.56]
---------------------------
I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 0.842 | Acc: 94.125% | Wgt Acc: 94.119%
	I - Batch: 100 | Loss: 0.848 | Acc: 93.375% | Wgt Acc: 93.385%
	I - Batch: 150 | Loss: 0.845 | Acc: 93.708% | Wgt Acc: 93.645%
	I - Batch: 200 | Loss: 0.843 | Acc: 93.969% | Wgt Acc: 93.912%
	I - Batch: 250 | Loss: 0.846 | Acc: 93.575% | Wgt Acc: 93.539%
	I - Batch: 300 | Loss: 0.848 | Acc: 93.292% | Wgt Acc: 93.277%
	I - Batch: 350 | Loss: 0.847 | Acc: 93.393% | Wgt Acc: 93.388%
	I - Batch: 400 | Loss: 0.846 | Acc: 93.469% | Wgt Acc: 93.483%
	I - Batch: 450 | Loss: 0.846 | Acc: 93.542% | Wgt Acc: 93.548%
I - num batch: 478
I - Train -- Loss: 0.846 | Acc: 93.496% | Wgt Acc: 93.510% | LR: 5.000000e-04 | Dur: 349.20s
I - Confusion Matrix: [row->prediction - col->label]
[[1909.   23.   31.   73.]
 [  53. 1669.   39.   25.]
 [  45.   26. 2100.   50.]
 [  84.   16.   32. 1466.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.132 | Acc: 61.500% | Wgt Acc: 60.661%
I - num batch: 62
I - Val -- Loss: 1.129 | Acc: 61.672% | Wgt Acc: 60.892% | Dur: 38.81s
I - Confusion Matrix: [row->prediction - col->label]
[[172.  11.  18.  32.]
 [  4.  89.  23.   4.]
 [ 23. 121. 164.  42.]
 [ 65.  13.  20. 180.]]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 0.829 | Acc: 95.375% | Wgt Acc: 95.402%
	I - Batch: 100 | Loss: 0.831 | Acc: 94.812% | Wgt Acc: 94.831%
	I - Batch: 150 | Loss: 0.835 | Acc: 94.458% | Wgt Acc: 94.540%
	I - Batch: 200 | Loss: 0.835 | Acc: 94.469% | Wgt Acc: 94.535%
	I - Batch: 250 | Loss: 0.834 | Acc: 94.550% | Wgt Acc: 94.586%
	I - Batch: 300 | Loss: 0.833 | Acc: 94.583% | Wgt Acc: 94.622%
	I - Batch: 350 | Loss: 0.834 | Acc: 94.464% | Wgt Acc: 94.493%
	I - Batch: 400 | Loss: 0.833 | Acc: 94.578% | Wgt Acc: 94.613%
	I - Batch: 450 | Loss: 0.834 | Acc: 94.611% | Wgt Acc: 94.641%
I - num batch: 478
I - Train -- Loss: 0.835 | Acc: 94.530% | Wgt Acc: 94.556% | LR: 5.000000e-04 | Dur: 352.85s
I - Confusion Matrix: [row->prediction - col->label]
[[1931.   17.   20.   58.]
 [  40. 1679.   41.   19.]
 [  40.   30. 2118.   42.]
 [  80.    8.   23. 1495.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.103 | Acc: 63.875% | Wgt Acc: 63.770%
I - num batch: 62
I - Val -- Loss: 1.103 | Acc: 64.118% | Wgt Acc: 64.312% | Dur: 38.02s
I - Confusion Matrix: [row->prediction - col->label]
[[173.  11.  22.  37.]
 [ 17. 150.  37.  22.]
 [ 17.  59. 132.  25.]
 [ 57.  14.  34. 174.]]

I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 0.828 | Acc: 94.875% | Wgt Acc: 94.879%
	I - Batch: 100 | Loss: 0.831 | Acc: 94.500% | Wgt Acc: 94.491%
	I - Batch: 150 | Loss: 0.827 | Acc: 94.833% | Wgt Acc: 94.789%
	I - Batch: 200 | Loss: 0.829 | Acc: 94.719% | Wgt Acc: 94.698%
	I - Batch: 250 | Loss: 0.827 | Acc: 95.075% | Wgt Acc: 95.061%
	I - Batch: 300 | Loss: 0.828 | Acc: 95.104% | Wgt Acc: 95.095%
	I - Batch: 350 | Loss: 0.828 | Acc: 95.054% | Wgt Acc: 95.052%
	I - Batch: 400 | Loss: 0.829 | Acc: 95.000% | Wgt Acc: 95.004%
	I - Batch: 450 | Loss: 0.830 | Acc: 94.931% | Wgt Acc: 94.937%
I - num batch: 478
I - Train -- Loss: 0.831 | Acc: 94.831% | Wgt Acc: 94.825% | LR: 5.000000e-04 | Dur: 351.18s
I - Confusion Matrix: [row->prediction - col->label]
[[1947.   19.   21.   63.]
 [  44. 1676.   35.   22.]
 [  36.   33. 2126.   32.]
 [  64.    6.   20. 1497.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.108 | Acc: 64.625% | Wgt Acc: 63.631%
I - num batch: 62
I - Val -- Loss: 1.117 | Acc: 63.609% | Wgt Acc: 62.794% | Dur: 38.53s
I - Confusion Matrix: [row->prediction - col->label]
[[234.  21.  35. 101.]
 [  8. 157.  57.  24.]
 [  5.  39. 113.  13.]
 [ 17.  17.  20. 120.]]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 0.825 | Acc: 95.500% | Wgt Acc: 95.532%
	I - Batch: 100 | Loss: 0.818 | Acc: 95.875% | Wgt Acc: 95.906%
	I - Batch: 150 | Loss: 0.821 | Acc: 95.708% | Wgt Acc: 95.725%
	I - Batch: 200 | Loss: 0.821 | Acc: 95.781% | Wgt Acc: 95.770%
	I - Batch: 250 | Loss: 0.822 | Acc: 95.725% | Wgt Acc: 95.690%
	I - Batch: 300 | Loss: 0.822 | Acc: 95.833% | Wgt Acc: 95.818%
	I - Batch: 350 | Loss: 0.822 | Acc: 95.714% | Wgt Acc: 95.728%
	I - Batch: 400 | Loss: 0.822 | Acc: 95.750% | Wgt Acc: 95.759%
	I - Batch: 450 | Loss: 0.822 | Acc: 95.750% | Wgt Acc: 95.760%
I - num batch: 478
I - Train -- Loss: 0.821 | Acc: 95.747% | Wgt Acc: 95.760% | LR: 5.000000e-04 | Dur: 356.21s
I - Confusion Matrix: [row->prediction - col->label]
[[1966.   13.   17.   51.]
 [  33. 1692.   29.   18.]
 [  45.   25. 2140.   27.]
 [  47.    4.   16. 1518.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.100 | Acc: 65.375% | Wgt Acc: 64.603%
I - num batch: 62
I - Val -- Loss: 1.088 | Acc: 66.769% | Wgt Acc: 66.168% | Dur: 35.85s
I - Confusion Matrix: [row->prediction - col->label]
[[203.  21.  23.  50.]
 [  8. 135.  31.  16.]
 [ 11.  65. 150.  25.]
 [ 42.  13.  21. 167.]]

I - Local maximum validation set accuracy:  66.77

I - Validation set results: 
[14-1-2-0.76][14-1-2-0.78][14-1-2-0.63][50-3-1-0.70][50-3-1-0.79][50-3-0-0.40][124-2-3-0.37][124-2-2-0.50][124-2-1-0.58][127-0-0-1.00]
[127-0-0-1.00][127-0-0-1.00][443-2-2-1.00][443-2-2-1.00][443-2-2-1.00][567-0-0-0.98][567-0-0-0.97][567-0-0-0.99][573-1-1-0.44][573-1-1-0.85]
[573-1-1-0.76][615-0-0-0.55][615-0-3-0.58][615-0-3-0.66][695-1-2-1.00][695-1-2-1.00][695-1-2-1.00][722-3-0-0.67][722-3-3-0.52][722-3-3-0.61]
[826-0-0-0.87][826-0-0-0.99][826-0-0-0.99][878-0-0-0.76][878-0-0-0.93][878-0-0-1.00][1103-0-0-0.94][1103-0-0-0.71][1103-0-0-0.63][1212-3-3-0.78]
[1212-3-3-0.84][1212-3-3-0.45][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-3-0.95][2181-2-3-0.95][2181-2-3-0.95][2476-2-1-0.54][2476-2-1-0.46]
[2476-2-1-0.55][2721-2-2-0.99][2721-2-2-1.00][2721-2-2-1.00][2818-1-2-0.53][2818-1-1-0.53][2818-1-1-0.57][2886-2-1-0.60][2886-2-2-0.51][2886-2-2-0.50]
[3231-2-2-1.00][3231-2-2-1.00][3231-2-2-1.00][3333-2-2-0.95][3333-2-2-0.94][3333-2-2-0.93][3482-2-2-0.99][3482-2-2-0.98][3482-2-2-0.99][3536-3-0-0.57]
[3536-3-3-0.60][3536-3-3-0.79][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-0-0.98][3909-0-0-0.86][3909-0-0-0.82][4035-0-0-0.99][4035-0-0-0.99]
[4035-0-0-0.99][4140-0-0-0.98][4140-0-0-0.97][4140-0-0-0.90][4214-1-3-0.58][4214-1-1-0.42][4214-1-1-0.88][4346-1-0-0.70][4346-1-0-0.75][4346-1-0-0.75]
[4581-2-2-0.94][4581-2-2-0.96][4581-2-2-0.88][4708-3-2-0.98][4708-3-2-0.94][4708-3-2-0.82][4838-3-0-0.59][4838-3-0-0.94][4838-3-0-0.61][4845-1-1-0.60]
[4845-1-1-0.65][4845-1-1-0.61][4868-0-0-1.00][4868-0-0-1.00][4868-0-0-0.85][4939-0-3-0.45][4939-0-1-0.50][4939-0-2-0.81][4984-2-2-0.68][4984-2-2-0.98]
[4984-2-2-0.97][5078-1-3-0.61][5078-1-0-0.42][5078-1-3-0.60][5396-0-0-0.99][5396-0-0-1.00][5396-0-0-0.99][5479-1-1-0.80][5479-1-1-0.97][5479-1-1-0.77]
[5717-0-0-0.88][5717-0-0-0.98][5717-0-3-0.41][5843-1-1-0.98][5843-1-1-0.97][5843-1-1-0.97][5949-3-3-0.66][5949-3-3-0.68][5949-3-3-0.84][5987-2-1-0.71]
[5987-2-1-0.82][5987-2-1-0.71][6014-3-1-0.95][6014-3-1-0.84][6014-3-1-0.92][6033-3-0-0.93][6033-3-3-0.61][6033-3-0-0.93][6313-0-0-0.98][6313-0-0-1.00]
[6313-0-0-0.95][6421-3-3-1.00][6421-3-3-1.00][6421-3-3-1.00][6500-1-3-0.88][6500-1-3-0.77][6500-1-3-0.85][6583-3-3-0.96][6583-3-3-0.90][6583-3-3-0.82]
[6683-3-3-0.81][6683-3-3-0.84][6683-3-3-0.99][6825-2-1-0.50][6825-2-0-0.65][6825-2-0-0.62][6998-3-3-0.54][6998-3-3-0.66][6998-3-3-0.51][7049-3-3-0.80]
[7049-3-3-0.72][7049-3-3-0.95][7517-1-2-0.54][7517-1-1-0.54][7517-1-1-0.95][7521-1-1-0.70][7521-1-1-0.96][7521-1-1-0.86][7528-1-3-0.61][7528-1-2-0.95]
[7528-1-2-0.95][7949-1-2-0.99][7949-1-2-0.99][7949-1-2-0.80][8135-1-0-0.96][8135-1-0-0.97][8135-1-0-0.95][8185-3-0-0.95][8185-3-0-0.81][8185-3-0-0.99]
[8269-3-2-0.63][8269-3-1-0.48][8269-3-1-0.90][8273-3-3-1.00][8273-3-3-0.99][8273-3-3-0.98][8543-3-3-0.60][8543-3-0-0.98][8543-3-0-0.97][8666-1-1-0.76]
[8666-1-1-0.93][8666-1-1-0.96][8672-0-0-1.00][8672-0-0-0.98][8672-0-0-0.50][8903-1-2-0.65][8903-1-2-0.87][8903-1-2-0.99][9001-2-0-0.84][9001-2-1-0.95]
[9001-2-1-0.93][9036-2-2-0.99][9036-2-2-0.99][9036-2-2-0.98][9281-3-3-0.67][9281-3-3-0.70][9281-3-3-0.56][9300-2-2-1.00][9300-2-2-1.00][9300-2-2-1.00]
[9571-0-3-0.62][9571-0-3-0.56][9571-0-3-0.55][9617-1-1-1.00][9617-1-1-0.72][9617-1-1-0.53][9644-2-1-0.67][9644-2-2-0.70][9644-2-2-0.55][9705-2-2-0.40]
[9705-2-2-0.54][9705-2-2-0.67][9801-0-0-0.92][9801-0-0-0.88][9801-0-0-0.53][9803-3-0-0.80][9803-3-3-0.66][9803-3-3-0.74][9865-3-3-1.00][9865-3-3-1.00]
[9865-3-3-0.99][9896-2-2-0.98][9896-2-2-0.81][9896-2-2-0.90][10314-1-1-0.50][10314-1-1-0.66][10314-1-2-0.68][10337-3-3-1.00][10337-3-3-1.00][10337-3-3-1.00]
[10403-0-0-0.50][10403-0-0-0.62][10403-0-0-0.65][10653-2-2-0.57][10653-2-2-0.80][10653-2-2-0.69][10704-2-2-0.94][10704-2-2-0.80][10704-2-2-0.76][10719-1-1-0.66]
[10719-1-1-0.85][10719-1-1-0.83][10727-1-1-0.58][10727-1-1-0.94][10727-1-1-0.86][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-3-0.97][10969-2-3-1.00]
[10969-2-3-0.99][11042-0-0-0.91][11042-0-0-0.88][11042-0-0-0.98][11088-1-1-0.72][11088-1-1-0.95][11088-1-1-1.00][11322-0-0-1.00][11322-0-0-0.98][11322-0-0-0.95]
[11398-2-2-1.00][11398-2-2-0.98][11398-2-2-0.98][11499-0-0-0.89][11499-0-0-0.96][11499-0-0-0.92][11502-3-0-0.52][11502-3-0-0.37][11502-3-0-0.78][11512-3-3-0.96]
[11512-3-3-0.83][11512-3-3-0.61][11608-1-2-0.78][11608-1-2-0.84][11608-1-2-0.77][11610-0-3-0.51][11610-0-3-0.80][11610-0-0-0.79][11692-0-0-0.91][11692-0-0-0.87]
[11692-0-0-0.88][11905-0-0-1.00][11905-0-0-0.90][11905-0-0-0.82][11993-1-1-0.64][11993-1-1-0.74][11993-1-1-0.97][12002-2-0-0.52][12002-2-2-0.63][12002-2-3-0.76]
[12052-0-0-0.64][12052-0-0-0.95][12052-0-0-0.83][12201-0-3-0.63][12201-0-3-0.74][12201-0-3-0.73][12235-2-2-1.00][12235-2-2-0.69][12235-2-2-0.54][12320-1-0-0.99]
[12320-1-0-0.99][12320-1-0-0.95][12377-2-2-0.56][12377-2-1-0.64][12377-2-1-0.51][12398-2-3-1.00][12398-2-3-1.00][12398-2-3-1.00][12503-1-0-0.39][12503-1-2-0.48]
[12503-1-2-0.50][12617-0-0-0.43][12617-0-0-0.61][12617-0-0-0.40][12685-3-1-0.57][12685-3-3-0.52][12685-3-3-0.61][12738-2-2-0.52][12738-2-0-0.46][12738-2-0-0.54]
[12742-2-2-1.00][12742-2-2-1.00][12742-2-2-1.00][12823-0-0-0.94][12823-0-0-0.69][12823-0-0-0.71][13110-1-1-1.00][13110-1-1-0.83][13110-1-2-0.88][13240-3-0-0.67]
[13240-3-0-0.70][13240-3-0-0.77][13253-1-2-0.63][13253-1-2-0.77][13253-1-2-0.57][13273-0-0-1.00][13273-0-0-1.00][13273-0-0-1.00][13634-1-2-0.60][13634-1-2-0.71]
[13634-1-2-0.70][13763-2-2-0.51][13763-2-2-0.59][13763-2-2-0.50][13905-3-3-0.77][13905-3-3-0.78][13905-3-3-0.84][14060-2-1-0.88][14060-2-1-0.96][14060-2-1-0.91]
[14065-3-0-0.54][14065-3-0-0.91][14065-3-0-0.59][14147-3-0-0.61][14147-3-3-0.69][14147-3-0-0.94][14595-2-2-0.99][14595-2-2-0.98][14595-2-2-1.00][14687-2-2-1.00]
[14687-2-2-1.00][14687-2-2-1.00][14788-2-2-0.73][14788-2-3-0.53][14788-2-3-0.65][14869-1-1-0.89][14869-1-1-0.85][14869-1-1-0.74][14872-3-3-0.54][14872-3-0-0.77]
[14872-3-0-0.75][14877-1-1-0.99][14877-1-1-1.00][14877-1-1-1.00][14927-0-3-0.82][14927-0-3-0.84][14927-0-3-0.87][15066-0-3-0.84][15066-0-3-0.85][15066-0-3-0.85]
[15175-1-2-0.38][15175-1-1-0.37][15175-1-1-0.84][15178-2-0-0.75][15178-2-0-0.54][15178-2-0-0.50][15375-3-0-0.65][15375-3-3-0.74][15375-3-0-0.69][15389-3-0-0.59]
[15389-3-3-0.75][15389-3-3-0.92][15568-2-1-0.83][15568-2-1-0.99][15568-2-1-0.96][15675-3-3-0.82][15675-3-3-1.00][15675-3-3-1.00][15869-1-1-0.59][15869-1-1-0.56]
[15869-1-3-0.58][16207-3-0-0.78][16207-3-0-0.91][16207-3-0-0.92][16236-0-0-0.83][16236-0-0-0.81][16236-0-0-0.53][16302-3-0-0.81][16302-3-0-0.73][16302-3-0-0.93]
[16331-2-2-1.00][16331-2-2-1.00][16331-2-2-1.00][16381-0-3-0.65][16381-0-3-0.86][16381-0-3-0.91][16488-1-1-0.95][16488-1-1-0.99][16488-1-1-0.88][16495-0-0-0.82]
[16495-0-0-1.00][16495-0-0-0.99][16650-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16719-1-1-0.65][16719-1-1-0.71][16719-1-1-0.66][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.90][16828-0-0-0.98][16828-0-0-0.98][17137-3-0-0.84][17137-3-3-0.59][17137-3-0-0.87][17245-1-2-0.49][17245-1-2-0.77][17245-1-2-0.43]
[17278-3-0-0.63][17278-3-0-0.50][17278-3-0-0.67][17282-0-0-0.57][17282-0-2-0.70][17282-0-2-0.40][17311-2-2-1.00][17311-2-2-0.99][17311-2-2-0.99][17336-2-2-0.67]
[17336-2-2-0.82][17336-2-2-1.00][17608-3-3-1.00][17608-3-3-1.00][17608-3-3-1.00][17627-0-0-0.93][17627-0-2-0.70][17627-0-0-0.54][17877-3-1-0.56][17877-3-0-0.46]
[17877-3-0-0.64][17924-1-2-0.70][17924-1-3-0.66][17924-1-2-0.69][17984-3-3-1.00][17984-3-3-0.66][17984-3-3-0.96][18211-0-1-0.39][18211-0-3-0.80][18211-0-3-0.72]
[18276-3-3-0.82][18276-3-3-0.78][18276-3-3-0.83][18287-1-1-0.51][18287-1-1-0.66][18287-1-1-0.87][18394-0-0-1.00][18394-0-0-0.97][18394-0-0-0.97][18428-0-0-0.82]
[18428-0-0-0.75][18428-0-0-1.00][18442-0-3-0.98][18442-0-3-0.92][18442-0-3-0.83][18478-3-3-0.80][18478-3-3-0.90][18478-3-3-0.88][18607-0-0-1.00][18607-0-0-1.00]
[18607-0-0-1.00][18616-0-0-0.85][18616-0-0-0.92][18616-0-0-0.91][18663-0-0-0.63][18663-0-0-0.96][18663-0-0-0.94][18718-0-0-1.00][18718-0-0-1.00][18718-0-0-1.00]
[18766-2-2-1.00][18766-2-2-1.00][18766-2-2-1.00][18824-2-2-1.00][18824-2-2-0.93][18824-2-2-0.95][18890-3-3-0.95][18890-3-3-0.98][18890-3-3-0.99][18930-3-2-0.56]
[18930-3-2-0.70][18930-3-2-0.44][18938-3-2-0.49][18938-3-3-0.60][18938-3-3-0.89][19817-1-2-0.51][19817-1-1-0.72][19817-1-2-0.52][19839-0-0-0.58][19839-0-2-0.62]
[19839-0-0-0.58][19930-3-3-0.95][19930-3-3-0.92][19930-3-3-0.98][19944-0-0-0.63][19944-0-2-0.97][19944-0-2-1.00][20036-2-2-0.92][20036-2-2-0.74][20036-2-2-0.84]
[20101-3-1-0.50][20101-3-3-0.88][20101-3-3-0.77][20474-1-1-1.00][20474-1-1-1.00][20474-1-1-0.99][20547-3-2-0.50][20547-3-0-0.86][20547-3-3-0.67][20929-2-2-0.98]
[20929-2-2-1.00][20929-2-2-1.00][21245-1-1-0.81][21245-1-1-0.80][21245-1-1-0.94][21257-3-3-0.93][21257-3-3-0.95][21257-3-2-0.54][21293-1-2-0.55][21293-1-2-0.52]
[21293-1-2-0.56][21316-1-1-1.00][21316-1-1-0.99][21316-1-3-0.60][21384-1-2-1.00][21384-1-2-1.00][21384-1-2-1.00][21448-1-1-0.85][21448-1-1-0.80][21448-1-1-0.95]
[21483-0-0-1.00][21483-0-0-1.00][21483-0-0-1.00][21487-2-2-0.99][21487-2-2-1.00][21487-2-2-0.95][21714-0-0-0.79][21714-0-0-0.92][21714-0-0-0.92][21943-3-2-0.72]
[21943-3-2-0.75][21943-3-2-0.95][21947-0-0-0.99][21947-0-0-0.79][21947-0-3-0.62][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-1.00][21965-2-2-1.00]
[21965-2-1-0.62][21998-1-1-0.61][21998-1-1-1.00][21998-1-1-0.92][22025-0-2-0.68][22025-0-2-0.46][22025-0-1-0.67][22228-3-3-1.00][22228-3-3-1.00][22228-3-3-1.00]
[22446-1-1-0.99][22446-1-1-0.99][22446-1-1-0.91][22494-3-3-0.63][22494-3-3-0.80][22494-3-3-0.95][22757-0-3-0.79][22757-0-3-0.93][22757-0-3-0.89][22811-3-3-0.55]
[22811-3-3-1.00][22811-3-3-1.00][22976-3-3-0.56][22976-3-1-0.47][22976-3-1-0.70][22985-3-3-0.98][22985-3-3-1.00][22985-3-3-1.00][23014-0-0-0.98][23014-0-0-1.00]
[23014-0-0-0.99][23112-1-1-0.96][23112-1-1-0.89][23112-1-1-0.93][23144-3-3-1.00][23144-3-3-1.00][23144-3-3-0.98][23168-2-0-0.98][23168-2-0-0.81][23168-2-0-0.91]
[23219-0-0-0.59][23219-0-0-0.98][23219-0-0-0.82][23363-3-3-0.62][23363-3-3-0.91][23363-3-3-0.99][23470-0-0-0.89][23470-0-0-0.76][23470-0-0-0.55][23486-2-2-0.42]
[23486-2-3-0.61][23486-2-0-0.39][23497-0-3-0.99][23497-0-3-1.00][23497-0-3-1.00][23516-0-0-0.95][23516-0-0-1.00][23516-0-0-1.00][23690-1-1-0.99][23690-1-1-0.75]
[23690-1-1-0.97][23921-2-1-0.69][23921-2-2-0.77][23921-2-2-0.69][23936-1-2-0.99][23936-1-2-1.00][23936-1-2-0.99][24040-3-2-0.65][24040-3-2-0.62][24040-3-2-0.71]
[24111-1-1-0.50][24111-1-1-0.52][24111-1-2-0.52][24182-0-0-0.96][24182-0-0-0.94][24182-0-0-0.99][24238-3-3-0.87][24238-3-3-0.96][24238-3-3-0.97][24290-2-0-1.00]
[24290-2-0-0.96][24290-2-0-0.94][24345-0-0-0.70][24345-0-0-0.88][24345-0-2-0.95][24364-1-3-0.80][24364-1-2-0.59][24364-1-2-0.46][24427-3-0-0.98][24427-3-0-0.95]
[24427-3-0-0.97][24477-2-2-0.97][24477-2-2-1.00][24477-2-2-0.98][24495-2-1-0.81][24495-2-1-0.65][24495-2-1-0.65][24893-2-2-0.89][24893-2-2-0.92][24893-2-2-0.93]
[25012-1-2-0.58][25012-1-2-0.38][25012-1-1-0.57][25121-2-1-0.91][25121-2-1-0.88][25121-2-1-0.96][25165-3-3-0.82][25165-3-3-0.88][25165-3-3-0.81][25183-0-0-1.00]
[25183-0-0-1.00][25183-0-0-1.00][25297-3-3-1.00][25297-3-3-0.93][25297-3-3-0.99][25398-0-0-0.77][25398-0-0-0.97][25398-0-0-0.97][25574-2-2-0.87][25574-2-2-0.97]
[25574-2-2-0.99][25644-1-2-1.00][25644-1-2-0.94][25644-1-2-0.94][25718-1-0-0.57][25718-1-0-0.70][25718-1-0-0.70][25774-2-2-0.91][25774-2-2-0.66][25774-2-2-0.54]
[26032-3-3-0.67][26032-3-3-0.85][26032-3-3-0.83][26051-3-3-1.00][26051-3-3-1.00][26051-3-3-1.00][26120-0-0-0.78][26120-0-0-0.95][26120-0-0-1.00][26321-1-1-0.97]
[26321-1-1-0.98][26321-1-2-0.61][26732-1-1-0.95][26732-1-1-0.99][26732-1-1-1.00][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-0.77][26827-3-3-0.73]
[26827-3-3-0.61][26833-0-3-1.00][26833-0-3-1.00][26833-0-3-1.00][26838-2-2-0.63][26838-2-2-0.71][26838-2-2-0.58][26860-1-2-0.76][26860-1-2-0.99][26860-1-1-0.79]
[26948-0-0-0.76][26948-0-0-0.89][26948-0-0-0.83][27049-3-0-0.99][27049-3-0-0.95][27049-3-0-1.00][27098-1-1-0.47][27098-1-0-0.51][27098-1-0-0.96][27526-0-0-0.65]
[27526-0-3-0.51][27526-0-0-0.53][27639-3-3-0.82][27639-3-3-0.89][27639-3-3-0.91][27698-3-3-0.89][27698-3-3-0.97][27698-3-3-0.94][27772-0-0-0.97][27772-0-0-0.95]
[27772-0-0-0.55][27890-1-1-0.83][27890-1-1-0.85][27890-1-1-0.79][28040-0-0-0.67][28040-0-0-0.79][28040-0-0-0.99][28503-2-2-1.00][28503-2-2-1.00][28503-2-2-1.00]
[28577-1-2-0.63][28577-1-1-0.58][28577-1-2-0.50][28959-0-0-1.00][28959-0-0-1.00][28959-0-0-1.00][29198-3-1-0.76][29198-3-1-0.87][29198-3-1-0.90][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-3-0.45][29877-2-3-0.64][29877-2-2-0.49][30035-1-1-0.99][30035-1-1-0.70][30035-1-1-0.90][30098-0-0-0.54][30098-0-0-0.82]
[30098-0-0-0.90][30326-1-1-1.00][30326-1-1-1.00][30326-1-1-1.00][30572-2-2-0.98][30572-2-2-0.93][30572-2-2-1.00][30716-0-1-0.92][30716-0-1-0.93][30716-0-1-0.66]
[30806-2-2-0.97][30806-2-2-0.96][30806-2-2-0.99][30906-1-1-1.00][30906-1-1-1.00][30906-1-1-0.92][31007-0-0-0.99][31007-0-0-1.00][31007-0-2-0.88][31181-3-3-0.96]
[31181-3-2-0.48][31181-3-2-0.91][31238-0-0-0.68][31238-0-3-0.84][31238-0-3-0.70][31347-0-0-1.00][31347-0-0-0.99][31347-0-0-1.00][31422-2-2-0.62][31422-2-2-0.65]
[31422-2-2-0.65][31429-3-3-0.98][31429-3-3-0.93][31429-3-3-0.96][31431-0-0-1.00][31431-0-0-0.92][31431-0-0-0.52][31432-1-1-0.95][31432-1-1-0.96][31432-1-1-1.00]
[31477-0-3-0.80][31477-0-3-0.59][31477-0-3-0.75][31524-1-2-0.78][31524-1-2-0.47][31524-1-0-0.38][31597-1-2-0.94][31597-1-2-0.77][31597-1-2-0.74][31619-1-0-0.91]
[31619-1-0-0.93][31619-1-0-0.98][31701-0-0-1.00][31701-0-0-0.93][31701-0-0-0.72][31755-0-0-1.00][31755-0-0-1.00][31755-0-0-1.00][31854-3-3-0.99][31854-3-3-0.92]
[31854-3-3-0.62][32074-1-1-0.52][32074-1-1-0.57][32074-1-3-0.88][32078-3-3-0.98][32078-3-3-0.99][32078-3-3-0.91][32111-1-1-0.89][32111-1-1-0.96][32111-1-1-0.98]
[32127-1-1-0.94][32127-1-1-0.94][32127-1-1-0.84][32140-3-3-0.93][32140-3-3-0.97][32140-3-3-0.97][32263-2-0-0.52][32263-2-2-0.61][32263-2-3-0.52][32365-0-0-0.98]
[32365-0-0-0.99][32365-0-0-1.00][32411-2-0-0.67][32411-2-0-0.70][32411-2-0-0.77][32429-3-3-0.98][32429-3-3-0.98][32429-3-3-0.97][32473-3-3-0.99][32473-3-3-0.97]
[32473-3-3-0.97][32574-3-3-0.93][32574-3-3-0.88][32574-3-3-0.85][32584-0-0-0.86][32584-0-0-0.89][32584-0-0-0.87][32622-0-1-0.47][32622-0-0-0.47][32622-0-1-0.66]
[32858-3-3-0.84][32858-3-3-0.85][32858-3-3-0.85][32969-3-3-0.80][32969-3-3-0.86][32969-3-3-0.76][33016-2-2-0.78][33016-2-2-0.99][33016-2-2-0.81][33031-1-3-0.56]
[33031-1-1-0.66][33031-1-1-0.74][33035-2-2-0.98][33035-2-2-0.98][33035-2-2-1.00][33133-2-2-0.99][33133-2-2-1.00][33133-2-2-1.00][33173-2-3-0.80][33173-2-2-0.49]
[33173-2-2-0.94][33175-3-2-1.00][33175-3-2-1.00][33175-3-2-1.00][33306-3-1-0.53][33306-3-3-0.51][33306-3-3-0.42][33309-2-3-0.74][33309-2-3-0.92][33309-2-3-0.93]
[33474-0-0-0.50][33474-0-0-0.66][33474-0-0-0.95][33478-2-0-0.82][33478-2-0-0.79][33478-2-0-0.79][33618-1-1-0.60][33618-1-1-0.62][33618-1-1-0.61][33712-0-0-0.55]
[33712-0-0-0.65][33712-0-0-0.97][33782-2-1-0.68][33782-2-1-0.51][33782-2-2-0.79][33914-3-2-0.67][33914-3-3-0.98][33914-3-3-0.99][34076-3-2-0.71][34076-3-2-0.87]
[34076-3-2-0.45][34112-2-2-1.00][34112-2-2-0.96][34112-2-2-0.99][34138-2-2-0.92][34138-2-2-0.90][34138-2-2-0.94][34239-1-1-0.71][34239-1-1-0.72][34239-1-1-0.49]
[34364-2-2-0.86][34364-2-2-1.00][34364-2-2-1.00][34617-1-1-0.60][34617-1-2-0.53][34617-1-0-0.44][34751-3-3-0.80][34751-3-3-0.77][34751-3-3-0.75][34783-2-1-0.81]
[34783-2-2-0.72][34783-2-2-0.59][35015-3-3-0.98][35015-3-3-0.57][35015-3-3-0.72][35018-1-1-0.83][35018-1-1-0.66][35018-1-1-0.60][35288-2-2-0.52][35288-2-2-0.79]
[35288-2-2-0.70]
---------------------------
I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 0.811 | Acc: 96.750% | Wgt Acc: 96.774%
	I - Batch: 100 | Loss: 0.815 | Acc: 96.375% | Wgt Acc: 96.379%
	I - Batch: 150 | Loss: 0.815 | Acc: 96.375% | Wgt Acc: 96.346%
	I - Batch: 200 | Loss: 0.815 | Acc: 96.500% | Wgt Acc: 96.459%
	I - Batch: 250 | Loss: 0.815 | Acc: 96.650% | Wgt Acc: 96.606%
	I - Batch: 300 | Loss: 0.816 | Acc: 96.604% | Wgt Acc: 96.562%
	I - Batch: 350 | Loss: 0.818 | Acc: 96.339% | Wgt Acc: 96.312%
	I - Batch: 400 | Loss: 0.819 | Acc: 96.203% | Wgt Acc: 96.178%
	I - Batch: 450 | Loss: 0.819 | Acc: 96.208% | Wgt Acc: 96.202%
I - num batch: 478
I - Train -- Loss: 0.819 | Acc: 96.257% | Wgt Acc: 96.249% | LR: 5.000000e-04 | Dur: 349.92s
I - Confusion Matrix: [row->prediction - col->label]
[[1980.    7.   14.   49.]
 [  26. 1692.   18.   16.]
 [  42.   28. 2155.   21.]
 [  43.    7.   15. 1528.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.093 | Acc: 67.000% | Wgt Acc: 65.686%
I - num batch: 62
I - Val -- Loss: 1.095 | Acc: 66.972% | Wgt Acc: 65.625% | Dur: 38.84s
I - Confusion Matrix: [row->prediction - col->label]
[[225.  20.  18.  64.]
 [  1. 122.  25.  12.]
 [ 13.  78. 162.  34.]
 [ 25.  14.  20. 148.]]

I - Local maximum validation set accuracy:  66.97

I - Validation set results: 
[14-1-2-0.67][14-1-2-0.71][14-1-2-0.59][50-3-0-0.34][50-3-0-0.48][50-3-0-0.78][124-2-2-0.55][124-2-2-0.81][124-2-2-0.59][127-0-0-1.00]
[127-0-0-1.00][127-0-0-1.00][443-2-2-1.00][443-2-2-1.00][443-2-2-1.00][567-0-0-0.98][567-0-0-0.98][567-0-0-0.98][573-1-1-0.44][573-1-1-0.82]
[573-1-1-0.78][615-0-0-0.54][615-0-3-0.56][615-0-3-0.59][695-1-2-0.86][695-1-2-0.96][695-1-2-0.91][722-3-3-0.57][722-3-3-0.84][722-3-3-0.93]
[826-0-0-0.56][826-0-0-0.97][826-0-0-0.98][878-0-0-0.71][878-0-0-0.95][878-0-0-1.00][1103-0-0-0.91][1103-0-0-0.71][1103-0-0-0.47][1212-3-3-0.96]
[1212-3-3-0.92][1212-3-3-0.66][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-3-0.98][2181-2-3-0.98][2181-2-3-0.98][2476-2-2-0.92][2476-2-2-0.90]
[2476-2-2-0.92][2721-2-2-0.98][2721-2-2-0.98][2721-2-2-1.00][2818-1-2-0.45][2818-1-3-0.62][2818-1-3-0.89][2886-2-1-0.54][2886-2-2-0.62][2886-2-1-0.53]
[3231-2-2-1.00][3231-2-2-1.00][3231-2-2-1.00][3333-2-2-0.83][3333-2-2-0.77][3333-2-2-0.93][3482-2-2-0.98][3482-2-2-0.98][3482-2-2-0.99][3536-3-0-0.46]
[3536-3-2-0.50][3536-3-3-0.58][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-0-0.98][3909-0-0-0.94][3909-0-0-0.86][4035-0-0-0.90][4035-0-0-0.96]
[4035-0-0-0.95][4140-0-0-0.99][4140-0-0-0.99][4140-0-0-0.95][4214-1-3-0.95][4214-1-1-0.47][4214-1-1-0.88][4346-1-0-0.94][4346-1-0-0.95][4346-1-0-0.94]
[4581-2-2-0.84][4581-2-2-0.88][4581-2-2-0.78][4708-3-2-0.73][4708-3-2-0.72][4708-3-3-0.53][4838-3-0-0.52][4838-3-0-0.68][4838-3-0-0.38][4845-1-2-0.55]
[4845-1-2-0.51][4845-1-2-0.58][4868-0-0-1.00][4868-0-0-1.00][4868-0-0-0.83][4939-0-0-0.44][4939-0-1-0.49][4939-0-2-0.66][4984-2-2-0.62][4984-2-2-0.77]
[4984-2-2-0.87][5078-1-3-0.43][5078-1-0-0.52][5078-1-3-0.47][5396-0-0-1.00][5396-0-0-1.00][5396-0-0-1.00][5479-1-1-0.56][5479-1-1-0.76][5479-1-1-0.56]
[5717-0-0-0.88][5717-0-0-1.00][5717-0-0-0.74][5843-1-1-0.93][5843-1-1-0.84][5843-1-1-1.00][5949-3-3-0.44][5949-3-0-0.46][5949-3-3-0.72][5987-2-1-0.48]
[5987-2-1-0.55][5987-2-2-0.54][6014-3-1-0.66][6014-3-1-0.70][6014-3-1-0.65][6033-3-0-0.73][6033-3-3-0.70][6033-3-0-0.84][6313-0-0-0.94][6313-0-0-0.93]
[6313-0-0-0.83][6421-3-3-1.00][6421-3-3-1.00][6421-3-3-1.00][6500-1-1-0.66][6500-1-1-0.70][6500-1-1-0.69][6583-3-3-0.70][6583-3-3-0.41][6583-3-2-0.74]
[6683-3-3-0.76][6683-3-3-0.72][6683-3-3-0.85][6825-2-0-0.64][6825-2-0-0.83][6825-2-0-0.79][6998-3-3-0.54][6998-3-3-0.69][6998-3-3-0.53][7049-3-3-0.88]
[7049-3-3-0.83][7049-3-3-0.88][7517-1-1-0.95][7517-1-1-0.92][7517-1-1-0.99][7521-1-1-0.35][7521-1-1-0.83][7521-1-3-0.46][7528-1-3-0.55][7528-1-2-0.99]
[7528-1-2-0.97][7949-1-2-1.00][7949-1-2-1.00][7949-1-2-0.96][8135-1-0-0.96][8135-1-0-0.59][8135-1-0-0.75][8185-3-0-0.99][8185-3-0-0.90][8185-3-0-1.00]
[8269-3-2-0.69][8269-3-1-0.48][8269-3-1-0.99][8273-3-3-0.93][8273-3-3-0.71][8273-3-3-0.54][8543-3-3-0.76][8543-3-0-0.65][8543-3-3-0.74][8666-1-1-0.77]
[8666-1-1-0.76][8666-1-1-0.88][8672-0-0-1.00][8672-0-0-0.99][8672-0-0-0.55][8903-1-1-0.47][8903-1-2-0.72][8903-1-2-0.96][9001-2-0-0.76][9001-2-1-0.99]
[9001-2-1-0.98][9036-2-2-1.00][9036-2-2-1.00][9036-2-2-1.00][9281-3-3-0.65][9281-3-3-0.68][9281-3-0-0.51][9300-2-2-0.99][9300-2-2-1.00][9300-2-2-1.00]
[9571-0-0-0.67][9571-0-0-0.59][9571-0-0-0.53][9617-1-1-1.00][9617-1-1-0.56][9617-1-2-0.45][9644-2-2-0.73][9644-2-2-0.95][9644-2-2-0.97][9705-2-2-0.85]
[9705-2-2-0.82][9705-2-2-0.83][9801-0-0-0.83][9801-0-0-0.82][9801-0-0-0.54][9803-3-0-0.84][9803-3-3-0.60][9803-3-3-0.74][9865-3-3-1.00][9865-3-3-0.97]
[9865-3-3-0.94][9896-2-2-0.98][9896-2-2-0.89][9896-2-2-0.95][10314-1-2-0.73][10314-1-2-0.55][10314-1-2-0.78][10337-3-3-0.99][10337-3-3-1.00][10337-3-3-1.00]
[10403-0-0-0.80][10403-0-0-0.77][10403-0-0-0.95][10653-2-2-0.72][10653-2-2-0.57][10653-2-2-0.53][10704-2-2-0.74][10704-2-2-0.65][10704-2-2-0.57][10719-1-1-0.80]
[10719-1-1-0.94][10719-1-1-0.98][10727-1-2-0.48][10727-1-1-0.71][10727-1-1-0.74][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-1-0.42][10969-2-3-0.69]
[10969-2-3-0.70][11042-0-0-0.92][11042-0-2-0.54][11042-0-0-1.00][11088-1-1-0.89][11088-1-1-0.97][11088-1-1-1.00][11322-0-0-1.00][11322-0-0-1.00][11322-0-0-1.00]
[11398-2-2-1.00][11398-2-2-0.85][11398-2-2-0.49][11499-0-0-0.92][11499-0-0-0.98][11499-0-0-0.92][11502-3-0-0.70][11502-3-2-0.44][11502-3-0-0.88][11512-3-3-0.95]
[11512-3-3-0.91][11512-3-3-0.79][11608-1-1-0.66][11608-1-2-0.75][11608-1-2-0.84][11610-0-0-0.96][11610-0-0-0.99][11610-0-0-0.99][11692-0-0-0.76][11692-0-0-0.56]
[11692-0-0-0.55][11905-0-0-1.00][11905-0-0-0.99][11905-0-0-0.96][11993-1-1-0.95][11993-1-1-0.94][11993-1-1-0.94][12002-2-3-0.55][12002-2-2-0.66][12002-2-3-0.72]
[12052-0-0-0.73][12052-0-0-0.97][12052-0-0-0.87][12201-0-0-0.78][12201-0-0-0.66][12201-0-0-0.67][12235-2-2-1.00][12235-2-2-0.88][12235-2-2-0.73][12320-1-0-0.97]
[12320-1-0-0.95][12320-1-0-0.97][12377-2-2-0.61][12377-2-1-0.56][12377-2-1-0.49][12398-2-3-0.97][12398-2-3-0.99][12398-2-3-0.99][12503-1-1-0.66][12503-1-2-0.45]
[12503-1-2-0.80][12617-0-2-0.69][12617-0-2-0.62][12617-0-2-0.69][12685-3-3-0.44][12685-3-3-0.73][12685-3-3-0.77][12738-2-2-0.54][12738-2-0-0.54][12738-2-0-0.73]
[12742-2-2-1.00][12742-2-2-1.00][12742-2-2-1.00][12823-0-0-0.94][12823-0-0-0.68][12823-0-0-0.71][13110-1-1-0.83][13110-1-2-0.51][13110-1-2-0.84][13240-3-0-0.89]
[13240-3-0-0.92][13240-3-0-0.94][13253-1-1-0.57][13253-1-2-0.51][13253-1-1-0.68][13273-0-0-1.00][13273-0-0-1.00][13273-0-0-1.00][13634-1-3-0.53][13634-1-2-0.56]
[13634-1-2-0.60][13763-2-2-0.71][13763-2-2-0.86][13763-2-2-0.66][13905-3-0-0.85][13905-3-0-0.79][13905-3-0-0.69][14060-2-1-0.89][14060-2-1-0.94][14060-2-1-0.79]
[14065-3-0-0.42][14065-3-0-0.82][14065-3-0-0.55][14147-3-0-0.71][14147-3-3-0.78][14147-3-0-0.91][14595-2-2-0.97][14595-2-2-0.94][14595-2-2-0.98][14687-2-2-1.00]
[14687-2-2-1.00][14687-2-2-1.00][14788-2-3-0.91][14788-2-3-0.86][14788-2-3-0.88][14869-1-1-0.89][14869-1-1-0.79][14869-1-1-0.71][14872-3-2-0.60][14872-3-0-0.64]
[14872-3-0-0.65][14877-1-1-0.96][14877-1-1-0.98][14877-1-1-1.00][14927-0-0-0.64][14927-0-0-0.57][14927-0-3-0.53][15066-0-0-0.89][15066-0-0-0.82][15066-0-0-0.82]
[15175-1-2-0.57][15175-1-2-0.61][15175-1-2-0.53][15178-2-0-0.87][15178-2-2-0.52][15178-2-2-0.40][15375-3-0-0.54][15375-3-3-0.69][15375-3-0-0.55][15389-3-3-0.99]
[15389-3-3-1.00][15389-3-3-1.00][15568-2-1-0.58][15568-2-1-0.99][15568-2-1-0.96][15675-3-3-0.80][15675-3-3-0.95][15675-3-3-0.97][15869-1-1-0.41][15869-1-1-0.43]
[15869-1-2-0.62][16207-3-0-0.76][16207-3-0-0.86][16207-3-0-0.89][16236-0-0-0.74][16236-0-0-0.66][16236-0-2-0.63][16302-3-2-0.80][16302-3-2-0.66][16302-3-0-0.71]
[16331-2-2-1.00][16331-2-2-1.00][16331-2-2-1.00][16381-0-3-0.62][16381-0-3-0.76][16381-0-3-0.87][16488-1-1-0.88][16488-1-1-0.97][16488-1-1-0.83][16495-0-0-0.69]
[16495-0-0-1.00][16495-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16719-1-1-0.61][16719-1-2-0.51][16719-1-2-0.63][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.91][16828-0-0-0.99][16828-0-0-1.00][17137-3-0-0.93][17137-3-3-0.49][17137-3-0-0.97][17245-1-2-0.51][17245-1-2-0.84][17245-1-2-0.59]
[17278-3-0-0.68][17278-3-0-0.61][17278-3-0-0.85][17282-0-0-0.72][17282-0-0-0.52][17282-0-0-0.46][17311-2-2-0.98][17311-2-2-0.97][17311-2-2-0.98][17336-2-2-0.84]
[17336-2-2-0.66][17336-2-2-0.98][17608-3-3-0.99][17608-3-3-0.99][17608-3-3-0.99][17627-0-0-1.00][17627-0-0-0.72][17627-0-0-0.88][17877-3-1-0.55][17877-3-0-0.55]
[17877-3-0-0.89][17924-1-2-0.79][17924-1-2-0.70][17924-1-2-0.83][17984-3-3-0.97][17984-3-0-0.79][17984-3-3-0.60][18211-0-0-0.41][18211-0-3-0.70][18211-0-3-0.37]
[18276-3-0-0.57][18276-3-3-0.54][18276-3-3-0.89][18287-1-1-0.35][18287-1-1-0.96][18287-1-1-0.84][18394-0-0-1.00][18394-0-0-0.99][18394-0-0-0.99][18428-0-0-0.92]
[18428-0-0-0.95][18428-0-0-1.00][18442-0-3-0.65][18442-0-3-0.66][18442-0-3-0.50][18478-3-0-0.53][18478-3-3-0.58][18478-3-0-0.54][18607-0-0-0.99][18607-0-0-0.99]
[18607-0-0-0.99][18616-0-0-0.95][18616-0-0-0.90][18616-0-0-0.92][18663-0-0-0.81][18663-0-0-0.94][18663-0-0-0.68][18718-0-0-1.00][18718-0-0-1.00][18718-0-0-1.00]
[18766-2-2-1.00][18766-2-2-1.00][18766-2-2-1.00][18824-2-2-0.99][18824-2-2-0.97][18824-2-2-0.98][18890-3-3-0.91][18890-3-3-0.92][18890-3-3-0.97][18930-3-2-0.46]
[18930-3-2-0.63][18930-3-2-0.58][18938-3-2-0.87][18938-3-2-0.97][18938-3-2-0.61][19817-1-1-0.58][19817-1-1-0.81][19817-1-1-0.59][19839-0-0-0.96][19839-0-0-0.92]
[19839-0-0-0.96][19930-3-3-0.77][19930-3-3-0.81][19930-3-3-0.74][19944-0-0-0.63][19944-0-2-0.83][19944-0-2-0.78][20036-2-2-1.00][20036-2-2-0.90][20036-2-2-0.95]
[20101-3-2-0.78][20101-3-3-0.71][20101-3-3-0.47][20474-1-1-0.55][20474-1-1-0.49][20474-1-1-0.44][20547-3-2-0.52][20547-3-0-0.70][20547-3-0-0.87][20929-2-2-0.55]
[20929-2-2-0.90][20929-2-2-0.99][21245-1-1-0.62][21245-1-1-0.72][21245-1-1-0.92][21257-3-3-0.75][21257-3-3-0.76][21257-3-2-0.94][21293-1-2-0.57][21293-1-1-0.53]
[21293-1-2-0.58][21316-1-1-1.00][21316-1-1-1.00][21316-1-3-0.45][21384-1-2-1.00][21384-1-2-1.00][21384-1-2-1.00][21448-1-1-0.85][21448-1-1-0.73][21448-1-1-0.72]
[21483-0-0-0.95][21483-0-0-0.99][21483-0-0-0.99][21487-2-2-0.99][21487-2-2-1.00][21487-2-2-0.94][21714-0-0-0.87][21714-0-0-0.92][21714-0-0-0.95][21943-3-2-0.74]
[21943-3-3-0.52][21943-3-2-0.77][21947-0-0-1.00][21947-0-0-0.88][21947-0-0-0.86][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-1.00][21965-2-2-1.00]
[21965-2-2-0.85][21998-1-1-0.53][21998-1-1-0.99][21998-1-1-0.82][22025-0-2-1.00][22025-0-2-0.95][22025-0-2-0.95][22228-3-3-1.00][22228-3-3-1.00][22228-3-3-1.00]
[22446-1-1-1.00][22446-1-1-1.00][22446-1-1-0.98][22494-3-3-0.97][22494-3-3-0.99][22494-3-3-1.00][22757-0-3-0.61][22757-0-3-0.84][22757-0-3-0.75][22811-3-3-0.52]
[22811-3-3-1.00][22811-3-3-1.00][22976-3-2-0.72][22976-3-2-0.55][22976-3-1-0.77][22985-3-3-0.98][22985-3-3-0.99][22985-3-3-1.00][23014-0-0-0.89][23014-0-0-0.90]
[23014-0-0-0.89][23112-1-1-0.91][23112-1-1-0.71][23112-1-1-0.78][23144-3-3-1.00][23144-3-3-1.00][23144-3-3-1.00][23168-2-0-0.84][23168-2-0-0.60][23168-2-3-0.48]
[23219-0-0-0.91][23219-0-0-1.00][23219-0-0-0.99][23363-3-3-0.97][23363-3-3-1.00][23363-3-3-1.00][23470-0-0-0.83][23470-0-0-0.75][23470-0-0-0.57][23486-2-2-0.39]
[23486-2-3-0.51][23486-2-2-0.46][23497-0-3-0.92][23497-0-3-0.90][23497-0-3-0.83][23516-0-0-1.00][23516-0-0-1.00][23516-0-0-1.00][23690-1-1-0.95][23690-1-1-0.59]
[23690-1-1-0.90][23921-2-1-0.80][23921-2-2-0.94][23921-2-2-0.51][23936-1-2-0.99][23936-1-2-1.00][23936-1-2-1.00][24040-3-2-0.81][24040-3-2-0.75][24040-3-2-0.78]
[24111-1-2-0.60][24111-1-2-0.61][24111-1-2-0.59][24182-0-0-0.99][24182-0-0-0.96][24182-0-0-0.99][24238-3-3-0.91][24238-3-3-0.98][24238-3-3-0.98][24290-2-0-0.97]
[24290-2-0-0.78][24290-2-0-0.83][24345-0-2-0.55][24345-0-0-0.55][24345-0-0-0.64][24364-1-3-0.45][24364-1-2-0.78][24364-1-2-0.71][24427-3-0-0.99][24427-3-0-0.99]
[24427-3-0-0.97][24477-2-2-0.88][24477-2-2-1.00][24477-2-2-0.89][24495-2-1-0.94][24495-2-1-0.76][24495-2-1-0.92][24893-2-2-0.62][24893-2-2-0.55][24893-2-2-0.67]
[25012-1-2-0.56][25012-1-1-0.66][25012-1-2-0.60][25121-2-1-0.48][25121-2-1-0.41][25121-2-1-0.46][25165-3-3-0.81][25165-3-3-0.84][25165-3-3-0.82][25183-0-0-0.82]
[25183-0-0-0.81][25183-0-0-0.88][25297-3-3-0.99][25297-3-3-0.82][25297-3-3-0.94][25398-0-0-0.66][25398-0-0-0.78][25398-0-0-0.96][25574-2-2-0.99][25574-2-2-1.00]
[25574-2-2-1.00][25644-1-2-0.89][25644-1-2-0.64][25644-1-2-0.33][25718-1-0-0.50][25718-1-2-0.60][25718-1-0-0.60][25774-2-2-0.94][25774-2-2-0.77][25774-2-2-0.83]
[26032-3-0-0.76][26032-3-3-0.75][26032-3-3-0.85][26051-3-3-1.00][26051-3-3-1.00][26051-3-3-1.00][26120-0-0-0.81][26120-0-0-0.87][26120-0-0-1.00][26321-1-1-0.99]
[26321-1-1-1.00][26321-1-2-0.66][26732-1-1-0.86][26732-1-1-0.92][26732-1-1-0.99][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-0.67][26827-3-3-0.71]
[26827-3-3-0.66][26833-0-3-0.95][26833-0-3-1.00][26833-0-3-0.99][26838-2-2-0.56][26838-2-2-0.41][26838-2-2-0.51][26860-1-2-0.43][26860-1-2-0.65][26860-1-1-0.84]
[26948-0-0-0.94][26948-0-0-0.80][26948-0-0-0.86][27049-3-0-0.52][27049-3-0-0.73][27049-3-0-0.82][27098-1-1-0.69][27098-1-0-0.55][27098-1-0-0.93][27526-0-0-1.00]
[27526-0-0-0.93][27526-0-0-0.77][27639-3-3-0.93][27639-3-3-0.99][27639-3-3-0.98][27698-3-3-0.80][27698-3-3-0.87][27698-3-3-0.76][27772-0-0-1.00][27772-0-0-1.00]
[27772-0-0-0.69][27890-1-1-0.68][27890-1-1-0.77][27890-1-1-0.56][28040-0-0-0.86][28040-0-0-0.94][28040-0-0-0.99][28503-2-2-1.00][28503-2-2-0.99][28503-2-2-1.00]
[28577-1-1-0.92][28577-1-1-0.93][28577-1-1-0.93][28959-0-0-1.00][28959-0-0-1.00][28959-0-0-1.00][29198-3-3-0.33][29198-3-1-0.38][29198-3-1-0.44][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-3-0.56][29877-2-3-0.82][29877-2-2-0.43][30035-1-1-0.98][30035-1-1-0.69][30035-1-1-0.84][30098-0-0-0.85][30098-0-0-0.97]
[30098-0-0-0.97][30326-1-1-1.00][30326-1-1-1.00][30326-1-1-1.00][30572-2-2-0.98][30572-2-2-0.82][30572-2-2-0.99][30716-0-0-0.69][30716-0-0-0.72][30716-0-0-0.83]
[30806-2-2-0.93][30806-2-2-0.85][30806-2-2-0.99][30906-1-1-0.99][30906-1-1-0.96][30906-1-1-0.80][31007-0-0-0.77][31007-0-0-0.98][31007-0-2-0.68][31181-3-3-0.98]
[31181-3-3-0.90][31181-3-3-0.92][31238-0-0-0.70][31238-0-3-0.73][31238-0-0-0.49][31347-0-0-1.00][31347-0-0-0.97][31347-0-0-0.98][31422-2-0-0.49][31422-2-0-0.82]
[31422-2-2-0.41][31429-3-3-0.73][31429-3-3-0.61][31429-3-3-0.50][31431-0-0-1.00][31431-0-0-0.99][31431-0-0-0.62][31432-1-1-0.71][31432-1-1-0.87][31432-1-1-0.96]
[31477-0-3-0.74][31477-0-3-0.54][31477-0-3-0.69][31524-1-3-0.63][31524-1-2-0.42][31524-1-0-0.42][31597-1-2-0.71][31597-1-1-0.65][31597-1-2-0.52][31619-1-0-0.90]
[31619-1-0-0.98][31619-1-0-0.98][31701-0-0-0.96][31701-0-0-0.73][31701-0-3-0.51][31755-0-0-1.00][31755-0-0-1.00][31755-0-0-1.00][31854-3-3-0.87][31854-3-2-0.60]
[31854-3-2-0.74][32074-1-2-0.50][32074-1-1-0.65][32074-1-3-0.77][32078-3-3-0.97][32078-3-3-0.95][32078-3-3-0.92][32111-1-1-0.61][32111-1-1-0.63][32111-1-1-0.75]
[32127-1-1-0.69][32127-1-2-0.59][32127-1-1-0.54][32140-3-3-1.00][32140-3-3-1.00][32140-3-3-1.00][32263-2-0-0.47][32263-2-2-0.52][32263-2-2-0.35][32365-0-0-1.00]
[32365-0-0-0.99][32365-0-0-1.00][32411-2-0-0.94][32411-2-0-0.85][32411-2-0-0.99][32429-3-0-0.63][32429-3-3-0.54][32429-3-0-0.52][32473-3-0-0.83][32473-3-0-0.86]
[32473-3-0-0.87][32574-3-3-0.91][32574-3-3-0.51][32574-3-0-0.70][32584-0-0-0.85][32584-0-0-0.82][32584-0-0-0.76][32622-0-0-0.61][32622-0-0-0.72][32622-0-0-0.73]
[32858-3-3-0.72][32858-3-3-0.70][32858-3-3-0.71][32969-3-3-1.00][32969-3-3-1.00][32969-3-3-1.00][33016-2-2-0.95][33016-2-2-0.96][33016-2-2-0.97][33031-1-3-0.73]
[33031-1-3-0.52][33031-1-0-0.86][33035-2-2-0.99][33035-2-2-0.97][33035-2-2-1.00][33133-2-2-0.98][33133-2-2-0.99][33133-2-2-0.84][33173-2-1-0.41][33173-2-2-0.50]
[33173-2-2-0.72][33175-3-2-1.00][33175-3-2-1.00][33175-3-2-1.00][33306-3-1-0.73][33306-3-1-0.69][33306-3-1-0.73][33309-2-3-0.59][33309-2-3-0.71][33309-2-3-0.69]
[33474-0-0-0.57][33474-0-0-0.68][33474-0-0-0.91][33478-2-2-0.61][33478-2-2-0.69][33478-2-2-0.57][33618-1-1-0.77][33618-1-1-0.71][33618-1-1-0.69][33712-0-0-0.81]
[33712-0-0-0.92][33712-0-0-0.99][33782-2-1-0.66][33782-2-2-0.50][33782-2-2-0.78][33914-3-2-0.58][33914-3-3-0.94][33914-3-3-0.94][34076-3-2-0.97][34076-3-2-0.98]
[34076-3-2-0.74][34112-2-2-1.00][34112-2-2-0.68][34112-2-2-0.89][34138-2-2-0.64][34138-2-2-0.62][34138-2-2-0.70][34239-1-2-0.88][34239-1-2-0.84][34239-1-2-0.93]
[34364-2-2-0.83][34364-2-2-0.97][34364-2-2-1.00][34617-1-2-0.79][34617-1-2-0.67][34617-1-0-0.52][34751-3-3-0.61][34751-3-0-0.55][34751-3-3-0.60][34783-2-1-0.52]
[34783-2-2-0.78][34783-2-2-0.70][35015-3-3-0.95][35015-3-3-0.57][35015-3-3-0.72][35018-1-2-0.68][35018-1-2-0.71][35018-1-2-0.92][35288-2-2-0.70][35288-2-2-0.83]
[35288-2-2-0.92]
---------------------------
I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 0.816 | Acc: 96.000% | Wgt Acc: 95.965%
	I - Batch: 100 | Loss: 0.817 | Acc: 96.500% | Wgt Acc: 96.483%
	I - Batch: 150 | Loss: 0.816 | Acc: 96.542% | Wgt Acc: 96.510%
	I - Batch: 200 | Loss: 0.817 | Acc: 96.438% | Wgt Acc: 96.384%
	I - Batch: 250 | Loss: 0.818 | Acc: 96.250% | Wgt Acc: 96.196%
	I - Batch: 300 | Loss: 0.816 | Acc: 96.333% | Wgt Acc: 96.279%
	I - Batch: 350 | Loss: 0.817 | Acc: 96.268% | Wgt Acc: 96.232%
	I - Batch: 400 | Loss: 0.816 | Acc: 96.344% | Wgt Acc: 96.310%
	I - Batch: 450 | Loss: 0.815 | Acc: 96.389% | Wgt Acc: 96.364%
I - num batch: 478
I - Train -- Loss: 0.814 | Acc: 96.506% | Wgt Acc: 96.479% | LR: 5.000000e-04 | Dur: 352.52s
I - Confusion Matrix: [row->prediction - col->label]
[[2002.   11.   20.   48.]
 [  23. 1693.   18.   14.]
 [  29.   22. 2150.   23.]
 [  37.    8.   14. 1529.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.102 | Acc: 65.500% | Wgt Acc: 64.825%
I - num batch: 62
I - Val -- Loss: 1.098 | Acc: 65.953% | Wgt Acc: 65.444% | Dur: 35.91s
I - Confusion Matrix: [row->prediction - col->label]
[[195.  15.  14.  48.]
 [ 14. 155.  45.  29.]
 [ 12.  60. 150.  34.]
 [ 43.   4.  16. 147.]]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 0.810 | Acc: 97.375% | Wgt Acc: 97.354%
	I - Batch: 100 | Loss: 0.810 | Acc: 96.938% | Wgt Acc: 96.909%
	I - Batch: 150 | Loss: 0.806 | Acc: 97.125% | Wgt Acc: 97.101%
	I - Batch: 200 | Loss: 0.805 | Acc: 97.312% | Wgt Acc: 97.305%
	I - Batch: 250 | Loss: 0.806 | Acc: 97.200% | Wgt Acc: 97.186%
	I - Batch: 300 | Loss: 0.806 | Acc: 97.083% | Wgt Acc: 97.068%
	I - Batch: 350 | Loss: 0.807 | Acc: 96.946% | Wgt Acc: 96.921%
	I - Batch: 400 | Loss: 0.807 | Acc: 96.812% | Wgt Acc: 96.793%
	I - Batch: 450 | Loss: 0.808 | Acc: 96.764% | Wgt Acc: 96.739%
I - num batch: 478
I - Train -- Loss: 0.809 | Acc: 96.689% | Wgt Acc: 96.671% | LR: 5.000000e-04 | Dur: 350.22s
I - Confusion Matrix: [row->prediction - col->label]
[[2001.   12.   20.   47.]
 [  21. 1697.   16.   16.]
 [  35.   19. 2156.   17.]
 [  34.    6.   10. 1534.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.118 | Acc: 63.000% | Wgt Acc: 62.743%
I - num batch: 62
I - Val -- Loss: 1.116 | Acc: 63.405% | Wgt Acc: 63.383% | Dur: 37.57s
I - Confusion Matrix: [row->prediction - col->label]
[[166.  12.   9.  33.]
 [ 11. 114.  28.  12.]
 [ 31.  95. 145.  16.]
 [ 56.  13.  43. 197.]]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.794 | Acc: 98.250% | Wgt Acc: 98.254%
	I - Batch: 100 | Loss: 0.793 | Acc: 98.000% | Wgt Acc: 98.007%
	I - Batch: 150 | Loss: 0.791 | Acc: 98.208% | Wgt Acc: 98.219%
	I - Batch: 200 | Loss: 0.788 | Acc: 98.219% | Wgt Acc: 98.207%
	I - Batch: 250 | Loss: 0.788 | Acc: 98.100% | Wgt Acc: 98.075%
	I - Batch: 300 | Loss: 0.786 | Acc: 98.250% | Wgt Acc: 98.227%
	I - Batch: 350 | Loss: 0.785 | Acc: 98.304% | Wgt Acc: 98.288%
	I - Batch: 400 | Loss: 0.784 | Acc: 98.359% | Wgt Acc: 98.345%
	I - Batch: 450 | Loss: 0.783 | Acc: 98.361% | Wgt Acc: 98.347%
I - num batch: 478
I - Train -- Loss: 0.783 | Acc: 98.351% | Wgt Acc: 98.331% | LR: 2.500000e-04 | Dur: 356.60s
I - Confusion Matrix: [row->prediction - col->label]
[[2046.    8.   11.   26.]
 [   9. 1717.    4.    9.]
 [  14.    7. 2183.   10.]
 [  22.    2.    4. 1569.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.084 | Acc: 65.625% | Wgt Acc: 64.964%
I - num batch: 62
I - Val -- Loss: 1.087 | Acc: 65.851% | Wgt Acc: 65.376% | Dur: 36.83s
I - Confusion Matrix: [row->prediction - col->label]
[[222.  28.  29.  57.]
 [  5. 124.  33.   6.]
 [  4.  61. 121.  16.]
 [ 33.  21.  42. 179.]]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 0.779 | Acc: 98.125% | Wgt Acc: 98.090%
	I - Batch: 100 | Loss: 0.781 | Acc: 98.438% | Wgt Acc: 98.453%
	I - Batch: 150 | Loss: 0.778 | Acc: 98.625% | Wgt Acc: 98.639%
	I - Batch: 200 | Loss: 0.775 | Acc: 98.781% | Wgt Acc: 98.797%
	I - Batch: 250 | Loss: 0.775 | Acc: 98.775% | Wgt Acc: 98.791%
	I - Batch: 300 | Loss: 0.776 | Acc: 98.688% | Wgt Acc: 98.696%
	I - Batch: 350 | Loss: 0.776 | Acc: 98.768% | Wgt Acc: 98.768%
	I - Batch: 400 | Loss: 0.776 | Acc: 98.750% | Wgt Acc: 98.750%
	I - Batch: 450 | Loss: 0.775 | Acc: 98.792% | Wgt Acc: 98.789%
I - num batch: 478
I - Train -- Loss: 0.776 | Acc: 98.731% | Wgt Acc: 98.726% | LR: 2.500000e-04 | Dur: 353.00s
I - Confusion Matrix: [row->prediction - col->label]
[[2052.    6.    9.   19.]
 [   9. 1724.    2.    6.]
 [  13.    3. 2188.    9.]
 [  17.    1.    3. 1580.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.083 | Acc: 66.750% | Wgt Acc: 66.102%
I - num batch: 62
I - Val -- Loss: 1.083 | Acc: 66.871% | Wgt Acc: 66.350% | Dur: 37.91s
I - Confusion Matrix: [row->prediction - col->label]
[[216.  24.  26.  53.]
 [  0. 125.  26.   8.]
 [  7.  64. 134.  16.]
 [ 41.  21.  39. 181.]]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.772 | Acc: 98.750% | Wgt Acc: 98.708%
	I - Batch: 100 | Loss: 0.770 | Acc: 98.938% | Wgt Acc: 98.931%
	I - Batch: 150 | Loss: 0.769 | Acc: 99.000% | Wgt Acc: 99.007%
	I - Batch: 200 | Loss: 0.769 | Acc: 99.000% | Wgt Acc: 99.002%
	I - Batch: 250 | Loss: 0.771 | Acc: 98.950% | Wgt Acc: 98.959%
	I - Batch: 300 | Loss: 0.770 | Acc: 98.958% | Wgt Acc: 98.968%
	I - Batch: 350 | Loss: 0.771 | Acc: 98.964% | Wgt Acc: 98.971%
	I - Batch: 400 | Loss: 0.772 | Acc: 98.906% | Wgt Acc: 98.909%
	I - Batch: 450 | Loss: 0.772 | Acc: 98.972% | Wgt Acc: 98.973%
I - num batch: 478
I - Train -- Loss: 0.772 | Acc: 98.953% | Wgt Acc: 98.950% | LR: 2.500000e-04 | Dur: 349.79s
I - Confusion Matrix: [row->prediction - col->label]
[[2059.    4.   12.   14.]
 [   9. 1726.    0.    5.]
 [  11.    3. 2190.    9.]
 [  12.    1.    0. 1586.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.117 | Acc: 63.125% | Wgt Acc: 62.160%
I - num batch: 62
I - Val -- Loss: 1.119 | Acc: 63.507% | Wgt Acc: 62.568% | Dur: 37.24s
I - Confusion Matrix: [row->prediction - col->label]
[[241.  47.  44.  90.]
 [  0. 115.  28.   4.]
 [  3.  45. 111.   8.]
 [ 20.  27.  42. 156.]]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 0.773 | Acc: 98.875% | Wgt Acc: 98.872%
	I - Batch: 100 | Loss: 0.772 | Acc: 98.812% | Wgt Acc: 98.835%
	I - Batch: 150 | Loss: 0.770 | Acc: 99.125% | Wgt Acc: 99.139%
	I - Batch: 200 | Loss: 0.770 | Acc: 99.031% | Wgt Acc: 99.037%
	I - Batch: 250 | Loss: 0.770 | Acc: 99.025% | Wgt Acc: 99.031%
	I - Batch: 300 | Loss: 0.770 | Acc: 99.062% | Wgt Acc: 99.062%
	I - Batch: 350 | Loss: 0.771 | Acc: 99.089% | Wgt Acc: 99.091%
	I - Batch: 400 | Loss: 0.771 | Acc: 99.156% | Wgt Acc: 99.159%
	I - Batch: 450 | Loss: 0.771 | Acc: 99.194% | Wgt Acc: 99.199%
I - num batch: 478
I - Train -- Loss: 0.771 | Acc: 99.189% | Wgt Acc: 99.192% | LR: 2.500000e-04 | Dur: 355.56s
I - Confusion Matrix: [row->prediction - col->label]
[[2064.    0.    8.   11.]
 [   6. 1731.    0.    5.]
 [   8.    1. 2193.    7.]
 [  13.    2.    1. 1591.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.094 | Acc: 67.500% | Wgt Acc: 66.463%
I - num batch: 62
I - Val -- Loss: 1.100 | Acc: 66.463% | Wgt Acc: 65.557% | Dur: 39.59s
I - Confusion Matrix: [row->prediction - col->label]
[[230.  22.  32.  74.]
 [  5. 137.  37.  22.]
 [  6.  65. 135.  12.]
 [ 23.  10.  21. 150.]]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.760 | Acc: 99.750% | Wgt Acc: 99.774%
	I - Batch: 100 | Loss: 0.762 | Acc: 99.625% | Wgt Acc: 99.649%
	I - Batch: 150 | Loss: 0.764 | Acc: 99.500% | Wgt Acc: 99.513%
	I - Batch: 200 | Loss: 0.765 | Acc: 99.344% | Wgt Acc: 99.346%
	I - Batch: 250 | Loss: 0.765 | Acc: 99.350% | Wgt Acc: 99.353%
	I - Batch: 300 | Loss: 0.766 | Acc: 99.271% | Wgt Acc: 99.283%
	I - Batch: 350 | Loss: 0.766 | Acc: 99.286% | Wgt Acc: 99.293%
	I - Batch: 400 | Loss: 0.767 | Acc: 99.234% | Wgt Acc: 99.240%
	I - Batch: 450 | Loss: 0.768 | Acc: 99.167% | Wgt Acc: 99.174%
I - num batch: 478
I - Train -- Loss: 0.769 | Acc: 99.149% | Wgt Acc: 99.154% | LR: 2.500000e-04 | Dur: 352.43s
I - Confusion Matrix: [row->prediction - col->label]
[[2062.    3.    9.   14.]
 [  12. 1728.    0.    1.]
 [   9.    2. 2193.    6.]
 [   8.    1.    0. 1593.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.100 | Acc: 65.875% | Wgt Acc: 65.047%
I - num batch: 62
I - Val -- Loss: 1.099 | Acc: 65.953% | Wgt Acc: 65.217% | Dur: 35.77s
I - Confusion Matrix: [row->prediction - col->label]
[[221.  25.  24.  70.]
 [  4. 134.  35.  14.]
 [ 14.  65. 134.  16.]
 [ 25.  10.  32. 158.]]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.767 | Acc: 99.125% | Wgt Acc: 99.158%
	I - Batch: 100 | Loss: 0.766 | Acc: 99.125% | Wgt Acc: 99.129%
	I - Batch: 150 | Loss: 0.764 | Acc: 99.208% | Wgt Acc: 99.194%
	I - Batch: 200 | Loss: 0.765 | Acc: 99.188% | Wgt Acc: 99.183%
	I - Batch: 250 | Loss: 0.764 | Acc: 99.250% | Wgt Acc: 99.246%
	I - Batch: 300 | Loss: 0.763 | Acc: 99.333% | Wgt Acc: 99.329%
	I - Batch: 350 | Loss: 0.762 | Acc: 99.375% | Wgt Acc: 99.373%
	I - Batch: 400 | Loss: 0.762 | Acc: 99.391% | Wgt Acc: 99.391%
	I - Batch: 450 | Loss: 0.762 | Acc: 99.431% | Wgt Acc: 99.434%
I - num batch: 478
I - Train -- Loss: 0.763 | Acc: 99.333% | Wgt Acc: 99.342% | LR: 1.250000e-04 | Dur: 348.94s
I - Confusion Matrix: [row->prediction - col->label]
[[2068.    2.    8.   12.]
 [   8. 1730.    1.    1.]
 [   9.    2. 2193.    2.]
 [   6.    0.    0. 1599.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.086 | Acc: 66.000% | Wgt Acc: 65.602%
I - num batch: 62
I - Val -- Loss: 1.092 | Acc: 65.647% | Wgt Acc: 65.331% | Dur: 37.83s
I - Confusion Matrix: [row->prediction - col->label]
[[216.  34.  32.  58.]
 [  0. 127.  33.   8.]
 [  3.  47. 119.  10.]
 [ 45.  26.  41. 182.]]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.754 | Acc: 99.750% | Wgt Acc: 99.746%
	I - Batch: 100 | Loss: 0.757 | Acc: 99.500% | Wgt Acc: 99.506%
	I - Batch: 150 | Loss: 0.759 | Acc: 99.458% | Wgt Acc: 99.464%
	I - Batch: 200 | Loss: 0.758 | Acc: 99.562% | Wgt Acc: 99.570%
	I - Batch: 250 | Loss: 0.759 | Acc: 99.500% | Wgt Acc: 99.503%
	I - Batch: 300 | Loss: 0.759 | Acc: 99.500% | Wgt Acc: 99.507%
	I - Batch: 350 | Loss: 0.759 | Acc: 99.446% | Wgt Acc: 99.444%
	I - Batch: 400 | Loss: 0.759 | Acc: 99.484% | Wgt Acc: 99.482%
	I - Batch: 450 | Loss: 0.759 | Acc: 99.472% | Wgt Acc: 99.471%
I - num batch: 478
I - Train -- Loss: 0.759 | Acc: 99.490% | Wgt Acc: 99.490% | LR: 1.250000e-04 | Dur: 352.17s
I - Confusion Matrix: [row->prediction - col->label]
[[2077.    1.    8.   12.]
 [   1. 1732.    0.    2.]
 [   8.    1. 2194.    1.]
 [   5.    0.    0. 1599.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.084 | Acc: 67.500% | Wgt Acc: 66.713%
I - num batch: 62
I - Val -- Loss: 1.093 | Acc: 66.463% | Wgt Acc: 65.806% | Dur: 36.78s
I - Confusion Matrix: [row->prediction - col->label]
[[236.  28.  31.  74.]
 [  4. 138.  44.  15.]
 [  3.  52. 118.   9.]
 [ 21.  16.  32. 160.]]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.756 | Acc: 99.500% | Wgt Acc: 99.522%
	I - Batch: 100 | Loss: 0.756 | Acc: 99.688% | Wgt Acc: 99.705%
	I - Batch: 150 | Loss: 0.755 | Acc: 99.667% | Wgt Acc: 99.672%
	I - Batch: 200 | Loss: 0.755 | Acc: 99.688% | Wgt Acc: 99.691%
	I - Batch: 250 | Loss: 0.756 | Acc: 99.650% | Wgt Acc: 99.652%
	I - Batch: 300 | Loss: 0.756 | Acc: 99.625% | Wgt Acc: 99.634%
	I - Batch: 350 | Loss: 0.757 | Acc: 99.571% | Wgt Acc: 99.574%
	I - Batch: 400 | Loss: 0.757 | Acc: 99.594% | Wgt Acc: 99.595%
	I - Batch: 450 | Loss: 0.758 | Acc: 99.542% | Wgt Acc: 99.540%
I - num batch: 478
I - Train -- Loss: 0.758 | Acc: 99.555% | Wgt Acc: 99.552% | LR: 1.250000e-04 | Dur: 351.82s
I - Confusion Matrix: [row->prediction - col->label]
[[2079.    2.    5.   11.]
 [   2. 1732.    1.    2.]
 [   6.    0. 2196.    1.]
 [   4.    0.    0. 1600.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.077 | Acc: 68.625% | Wgt Acc: 68.129%
I - num batch: 62
I - Val -- Loss: 1.086 | Acc: 67.482% | Wgt Acc: 67.074% | Dur: 35.95s
I - Confusion Matrix: [row->prediction - col->label]
[[230.  26.  30.  62.]
 [  4. 148.  50.  15.]
 [  5.  45. 118.  15.]
 [ 25.  15.  27. 166.]]

I - Local maximum validation set accuracy:  67.48

I - Validation set results: 
[14-1-1-0.38][14-1-2-0.55][14-1-1-0.46][50-3-1-0.73][50-3-1-0.83][50-3-0-0.58][124-2-2-0.46][124-2-2-0.86][124-2-2-0.45][127-0-0-1.00]
[127-0-0-1.00][127-0-0-1.00][443-2-2-1.00][443-2-2-1.00][443-2-2-1.00][567-0-0-0.93][567-0-0-0.93][567-0-0-0.96][573-1-1-0.48][573-1-1-0.68]
[573-1-1-0.73][615-0-0-0.59][615-0-0-0.58][615-0-0-0.59][695-1-2-0.76][695-1-2-0.84][695-1-2-0.75][722-3-0-0.59][722-3-0-0.51][722-3-3-0.92]
[826-0-0-0.96][826-0-0-1.00][826-0-0-0.98][878-0-0-0.59][878-0-0-0.92][878-0-0-0.99][1103-0-0-0.97][1103-0-0-0.69][1103-0-0-0.61][1212-3-3-0.77]
[1212-3-3-0.76][1212-3-0-0.42][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-3-0.59][2181-2-3-0.71][2181-2-3-0.78][2476-2-1-0.58][2476-2-1-0.47]
[2476-2-1-0.57][2721-2-2-0.86][2721-2-2-0.84][2721-2-2-0.91][2818-1-1-0.47][2818-1-1-0.64][2818-1-1-0.51][2886-2-1-0.78][2886-2-1-0.65][2886-2-1-0.63]
[3231-2-2-1.00][3231-2-2-1.00][3231-2-2-1.00][3333-2-2-0.73][3333-2-2-0.72][3333-2-2-0.61][3482-2-2-0.99][3482-2-2-0.98][3482-2-2-1.00][3536-3-0-0.62]
[3536-3-3-0.66][3536-3-3-0.75][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-0-0.99][3909-0-0-0.91][3909-0-0-0.89][4035-0-0-1.00][4035-0-0-1.00]
[4035-0-0-1.00][4140-0-0-1.00][4140-0-0-0.98][4140-0-0-0.96][4214-1-3-0.75][4214-1-1-0.53][4214-1-1-0.98][4346-1-0-0.85][4346-1-0-0.65][4346-1-0-0.69]
[4581-2-1-0.57][4581-2-1-0.59][4581-2-1-0.49][4708-3-3-0.81][4708-3-3-0.79][4708-3-3-0.65][4838-3-3-0.71][4838-3-3-0.72][4838-3-3-0.56][4845-1-1-0.66]
[4845-1-1-0.61][4845-1-1-0.59][4868-0-0-1.00][4868-0-0-1.00][4868-0-0-0.88][4939-0-0-0.43][4939-0-1-0.77][4939-0-2-0.46][4984-2-3-0.73][4984-2-2-0.61]
[4984-2-2-0.58][5078-1-2-0.63][5078-1-2-0.76][5078-1-2-0.56][5396-0-0-0.98][5396-0-0-1.00][5396-0-0-0.97][5479-1-1-0.86][5479-1-1-1.00][5479-1-1-0.84]
[5717-0-0-0.88][5717-0-0-0.96][5717-0-0-0.48][5843-1-1-0.86][5843-1-1-0.95][5843-1-1-1.00][5949-3-3-0.66][5949-3-3-0.53][5949-3-3-0.89][5987-2-2-0.57]
[5987-2-1-0.45][5987-2-2-0.50][6014-3-1-0.85][6014-3-1-0.64][6014-3-1-0.77][6033-3-0-0.88][6033-3-3-0.60][6033-3-0-0.68][6313-0-0-0.99][6313-0-0-0.97]
[6313-0-0-0.96][6421-3-3-0.95][6421-3-3-0.96][6421-3-3-0.99][6500-1-3-0.60][6500-1-1-0.44][6500-1-1-0.52][6583-3-3-0.89][6583-3-3-0.85][6583-3-3-0.86]
[6683-3-3-0.75][6683-3-3-0.70][6683-3-3-0.82][6825-2-1-0.69][6825-2-1-0.63][6825-2-1-0.66][6998-3-3-0.51][6998-3-3-0.58][6998-3-3-0.82][7049-3-3-0.75]
[7049-3-3-0.51][7049-3-3-0.88][7517-1-1-0.84][7517-1-1-0.95][7517-1-1-1.00][7521-1-0-0.82][7521-1-0-0.87][7521-1-0-0.75][7528-1-3-0.94][7528-1-2-0.37]
[7528-1-3-0.44][7949-1-2-0.99][7949-1-2-0.98][7949-1-2-0.88][8135-1-0-0.75][8135-1-0-0.92][8135-1-0-0.89][8185-3-0-1.00][8185-3-0-1.00][8185-3-0-1.00]
[8269-3-2-0.52][8269-3-1-0.53][8269-3-1-1.00][8273-3-3-0.99][8273-3-3-0.98][8273-3-3-0.81][8543-3-3-0.64][8543-3-0-0.96][8543-3-0-0.94][8666-1-1-0.63]
[8666-1-1-0.79][8666-1-1-0.85][8672-0-0-1.00][8672-0-0-0.99][8672-0-3-0.51][8903-1-1-0.76][8903-1-2-0.60][8903-1-2-1.00][9001-2-0-0.87][9001-2-1-0.60]
[9001-2-1-0.67][9036-2-2-1.00][9036-2-2-0.98][9036-2-2-1.00][9281-3-0-0.76][9281-3-0-0.54][9281-3-0-0.65][9300-2-2-0.99][9300-2-2-1.00][9300-2-2-1.00]
[9571-0-0-0.53][9571-0-3-0.46][9571-0-3-0.50][9617-1-1-1.00][9617-1-1-0.52][9617-1-1-0.35][9644-2-2-0.81][9644-2-2-0.75][9644-2-2-0.91][9705-2-0-0.60]
[9705-2-0-0.63][9705-2-0-0.62][9801-0-0-0.93][9801-0-0-0.92][9801-0-0-0.70][9803-3-0-0.87][9803-3-3-0.57][9803-3-3-0.68][9865-3-3-0.94][9865-3-3-0.97]
[9865-3-3-0.99][9896-2-2-0.88][9896-2-1-0.53][9896-2-2-0.83][10314-1-2-0.55][10314-1-2-0.44][10314-1-2-0.55][10337-3-3-1.00][10337-3-3-1.00][10337-3-3-1.00]
[10403-0-0-0.97][10403-0-0-0.97][10403-0-0-0.99][10653-2-3-0.37][10653-2-2-0.42][10653-2-2-0.40][10704-2-1-0.96][10704-2-1-0.98][10704-2-1-0.99][10719-1-1-0.55]
[10719-1-1-0.80][10719-1-1-0.97][10727-1-1-0.58][10727-1-1-0.90][10727-1-1-0.93][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-3-0.81][10969-2-3-0.98]
[10969-2-3-0.96][11042-0-0-0.79][11042-0-0-0.77][11042-0-0-0.91][11088-1-1-1.00][11088-1-1-1.00][11088-1-1-1.00][11322-0-0-1.00][11322-0-0-0.99][11322-0-0-0.99]
[11398-2-2-1.00][11398-2-2-0.54][11398-2-0-0.66][11499-0-0-0.99][11499-0-0-1.00][11499-0-0-0.96][11502-3-0-0.69][11502-3-3-0.54][11502-3-0-0.94][11512-3-3-0.80]
[11512-3-3-0.64][11512-3-2-0.40][11608-1-2-0.91][11608-1-2-0.78][11608-1-2-0.67][11610-0-0-0.87][11610-0-0-0.86][11610-0-0-0.97][11692-0-3-0.52][11692-0-3-0.71]
[11692-0-0-0.53][11905-0-0-1.00][11905-0-0-0.80][11905-0-0-0.71][11993-1-1-0.97][11993-1-1-0.97][11993-1-1-0.95][12002-2-0-0.85][12002-2-2-0.62][12002-2-3-0.71]
[12052-0-0-0.85][12052-0-0-0.96][12052-0-0-0.87][12201-0-3-0.68][12201-0-0-0.50][12201-0-0-0.52][12235-2-2-0.99][12235-2-2-0.71][12235-2-1-0.70][12320-1-0-0.98]
[12320-1-0-0.98][12320-1-0-0.99][12377-2-2-0.41][12377-2-1-0.63][12377-2-1-0.55][12398-2-3-0.98][12398-2-3-0.98][12398-2-3-0.97][12503-1-1-0.98][12503-1-1-0.54]
[12503-1-2-0.55][12617-0-0-0.56][12617-0-0-0.69][12617-0-0-0.62][12685-3-3-0.47][12685-3-3-0.73][12685-3-3-0.73][12738-2-1-0.46][12738-2-0-0.50][12738-2-0-0.62]
[12742-2-2-1.00][12742-2-2-1.00][12742-2-2-1.00][12823-0-0-1.00][12823-0-0-0.85][12823-0-0-0.89][13110-1-1-0.86][13110-1-1-0.80][13110-1-1-0.89][13240-3-0-0.96]
[13240-3-0-0.96][13240-3-0-0.98][13253-1-1-0.97][13253-1-1-0.83][13253-1-1-0.98][13273-0-0-1.00][13273-0-0-1.00][13273-0-0-1.00][13634-1-3-0.44][13634-1-3-0.39]
[13634-1-2-0.39][13763-2-3-0.56][13763-2-2-0.65][13763-2-3-0.64][13905-3-0-0.99][13905-3-0-0.99][13905-3-0-0.91][14060-2-1-0.88][14060-2-1-0.95][14060-2-1-0.97]
[14065-3-3-0.53][14065-3-0-0.81][14065-3-0-0.62][14147-3-3-0.51][14147-3-3-0.98][14147-3-0-0.73][14595-2-2-0.89][14595-2-2-0.92][14595-2-2-0.95][14687-2-2-0.99]
[14687-2-2-1.00][14687-2-2-0.99][14788-2-1-0.89][14788-2-3-0.78][14788-2-3-0.93][14869-1-1-1.00][14869-1-1-1.00][14869-1-1-0.91][14872-3-2-0.52][14872-3-0-0.54]
[14872-3-0-0.69][14877-1-1-0.94][14877-1-1-0.99][14877-1-1-1.00][14927-0-3-0.73][14927-0-3-0.68][14927-0-3-0.70][15066-0-0-0.80][15066-0-0-0.77][15066-0-0-0.68]
[15175-1-1-0.49][15175-1-1-0.54][15175-1-1-0.80][15178-2-2-0.77][15178-2-2-0.96][15178-2-2-0.92][15375-3-0-0.70][15375-3-3-0.72][15375-3-0-0.50][15389-3-3-0.95]
[15389-3-3-0.92][15389-3-3-0.98][15568-2-1-0.68][15568-2-1-0.99][15568-2-1-0.94][15675-3-3-0.54][15675-3-3-0.99][15675-3-3-0.96][15869-1-0-0.64][15869-1-0-0.69]
[15869-1-3-0.61][16207-3-0-0.97][16207-3-0-0.98][16207-3-0-0.97][16236-0-0-1.00][16236-0-0-0.90][16236-0-0-0.81][16302-3-3-0.77][16302-3-3-0.80][16302-3-3-0.80]
[16331-2-2-0.99][16331-2-2-1.00][16331-2-2-1.00][16381-0-3-0.78][16381-0-3-0.87][16381-0-3-0.92][16488-1-1-0.97][16488-1-1-1.00][16488-1-1-0.99][16495-0-0-0.91]
[16495-0-0-1.00][16495-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16719-1-3-0.44][16719-1-3-0.47][16719-1-2-0.61][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.85][16828-0-0-0.98][16828-0-0-1.00][17137-3-3-0.66][17137-3-3-0.85][17137-3-0-0.66][17245-1-2-0.42][17245-1-2-0.69][17245-1-1-0.42]
[17278-3-0-0.65][17278-3-0-0.48][17278-3-0-0.49][17282-0-0-0.83][17282-0-0-0.85][17282-0-0-0.59][17311-2-2-0.81][17311-2-2-0.83][17311-2-2-0.82][17336-2-1-0.37]
[17336-2-1-0.91][17336-2-2-0.69][17608-3-3-1.00][17608-3-3-1.00][17608-3-3-1.00][17627-0-0-1.00][17627-0-0-0.90][17627-0-0-0.80][17877-3-0-0.65][17877-3-0-0.83]
[17877-3-0-0.93][17924-1-3-0.53][17924-1-3-0.82][17924-1-3-0.48][17984-3-3-0.99][17984-3-3-0.61][17984-3-3-0.95][18211-0-0-0.50][18211-0-3-0.74][18211-0-3-0.56]
[18276-3-3-0.88][18276-3-3-0.81][18276-3-3-0.98][18287-1-1-0.52][18287-1-1-0.88][18287-1-1-0.82][18394-0-0-1.00][18394-0-0-1.00][18394-0-0-1.00][18428-0-0-1.00]
[18428-0-0-0.99][18428-0-0-1.00][18442-0-3-0.53][18442-0-3-0.57][18442-0-0-0.56][18478-3-0-0.70][18478-3-0-0.59][18478-3-0-0.64][18607-0-0-1.00][18607-0-0-1.00]
[18607-0-0-1.00][18616-0-0-0.87][18616-0-0-0.95][18616-0-0-0.98][18663-0-0-0.81][18663-0-0-0.98][18663-0-0-0.76][18718-0-0-1.00][18718-0-0-1.00][18718-0-0-1.00]
[18766-2-2-1.00][18766-2-2-1.00][18766-2-2-1.00][18824-2-2-0.92][18824-2-2-0.68][18824-2-2-0.78][18890-3-3-0.62][18890-3-3-0.80][18890-3-3-0.82][18930-3-1-0.92]
[18930-3-1-0.69][18930-3-1-0.54][18938-3-3-0.98][18938-3-3-1.00][18938-3-3-0.99][19817-1-1-0.85][19817-1-1-0.91][19817-1-1-0.75][19839-0-0-0.96][19839-0-0-0.87]
[19839-0-0-0.90][19930-3-3-0.68][19930-3-3-0.70][19930-3-3-0.77][19944-0-0-0.77][19944-0-0-0.43][19944-0-0-0.66][20036-2-2-1.00][20036-2-2-1.00][20036-2-2-1.00]
[20101-3-3-0.33][20101-3-3-0.76][20101-3-3-0.62][20474-1-1-0.99][20474-1-1-0.99][20474-1-1-1.00][20547-3-1-0.54][20547-3-0-0.87][20547-3-3-0.63][20929-2-0-0.55]
[20929-2-2-0.98][20929-2-2-1.00][21245-1-1-0.81][21245-1-1-0.70][21245-1-1-0.96][21257-3-3-0.76][21257-3-3-0.78][21257-3-2-0.70][21293-1-2-0.52][21293-1-2-0.51]
[21293-1-2-0.54][21316-1-1-1.00][21316-1-1-1.00][21316-1-3-0.51][21384-1-2-0.69][21384-1-2-0.63][21384-1-2-0.77][21448-1-1-0.99][21448-1-1-0.97][21448-1-1-0.97]
[21483-0-0-0.99][21483-0-0-0.99][21483-0-0-1.00][21487-2-2-0.86][21487-2-2-0.64][21487-2-2-0.59][21714-0-0-0.70][21714-0-0-0.84][21714-0-0-0.89][21943-3-2-0.88]
[21943-3-2-0.50][21943-3-2-0.66][21947-0-0-0.98][21947-0-0-0.96][21947-0-0-0.92][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-0.99][21965-2-2-0.99]
[21965-2-2-0.55][21998-1-1-0.85][21998-1-1-0.95][21998-1-1-1.00][22025-0-2-0.99][22025-0-2-0.93][22025-0-2-0.88][22228-3-3-0.99][22228-3-3-0.99][22228-3-3-0.99]
[22446-1-1-1.00][22446-1-1-1.00][22446-1-1-0.53][22494-3-3-0.92][22494-3-3-0.98][22494-3-3-0.97][22757-0-0-0.64][22757-0-0-0.57][22757-0-0-0.66][22811-3-3-0.56]
[22811-3-3-1.00][22811-3-3-1.00][22976-3-3-0.70][22976-3-1-0.43][22976-3-1-0.73][22985-3-3-0.87][22985-3-3-0.96][22985-3-3-1.00][23014-0-0-0.62][23014-0-0-0.88]
[23014-0-0-0.89][23112-1-1-1.00][23112-1-1-0.98][23112-1-1-1.00][23144-3-3-1.00][23144-3-3-0.98][23144-3-3-1.00][23168-2-0-0.69][23168-2-0-0.49][23168-2-0-0.70]
[23219-0-0-0.92][23219-0-0-0.93][23219-0-0-0.99][23363-3-3-0.65][23363-3-3-0.87][23363-3-3-0.97][23470-0-0-0.80][23470-0-0-0.89][23470-0-0-0.70][23486-2-1-0.43]
[23486-2-3-0.68][23486-2-0-0.45][23497-0-3-0.71][23497-0-3-0.99][23497-0-3-0.99][23516-0-0-0.98][23516-0-0-0.99][23516-0-0-1.00][23690-1-1-1.00][23690-1-1-0.79]
[23690-1-1-0.98][23921-2-1-0.74][23921-2-2-0.47][23921-2-1-0.49][23936-1-2-0.90][23936-1-2-0.97][23936-1-2-0.73][24040-3-2-0.57][24040-3-0-0.49][24040-3-2-0.58]
[24111-1-1-0.72][24111-1-1-0.79][24111-1-1-0.67][24182-0-0-1.00][24182-0-0-1.00][24182-0-0-1.00][24238-3-3-0.99][24238-3-3-1.00][24238-3-3-1.00][24290-2-0-1.00]
[24290-2-0-1.00][24290-2-0-0.99][24345-0-0-0.72][24345-0-0-0.99][24345-0-0-0.80][24364-1-3-0.47][24364-1-2-0.43][24364-1-1-0.53][24427-3-0-0.88][24427-3-0-0.91]
[24427-3-0-0.99][24477-2-2-0.74][24477-2-2-0.74][24477-2-2-0.68][24495-2-1-0.88][24495-2-1-0.73][24495-2-0-0.49][24893-2-1-0.50][24893-2-2-0.48][24893-2-2-0.46]
[25012-1-1-0.67][25012-1-1-0.66][25012-1-1-0.83][25121-2-1-0.55][25121-2-1-0.61][25121-2-1-0.58][25165-3-3-0.51][25165-3-3-0.54][25165-3-3-0.53][25183-0-0-1.00]
[25183-0-0-1.00][25183-0-0-1.00][25297-3-3-0.97][25297-3-3-0.85][25297-3-3-0.93][25398-0-0-0.81][25398-0-0-1.00][25398-0-0-1.00][25574-2-2-0.55][25574-2-2-0.85]
[25574-2-2-0.87][25644-1-2-0.77][25644-1-1-0.64][25644-1-1-0.52][25718-1-0-0.80][25718-1-0-0.91][25718-1-0-0.66][25774-2-2-0.79][25774-2-0-0.43][25774-2-2-0.43]
[26032-3-3-0.72][26032-3-3-0.89][26032-3-3-0.93][26051-3-3-0.98][26051-3-3-1.00][26051-3-3-0.99][26120-0-0-0.94][26120-0-0-0.98][26120-0-0-1.00][26321-1-1-1.00]
[26321-1-1-1.00][26321-1-0-0.35][26732-1-1-0.96][26732-1-1-0.91][26732-1-1-0.98][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-0.90][26827-3-3-0.77]
[26827-3-3-0.90][26833-0-3-0.98][26833-0-3-0.98][26833-0-3-0.68][26838-2-2-0.54][26838-2-2-0.45][26838-2-3-0.44][26860-1-1-0.38][26860-1-0-0.52][26860-1-1-0.86]
[26948-0-0-0.65][26948-0-0-0.86][26948-0-0-0.87][27049-3-0-1.00][27049-3-0-1.00][27049-3-0-1.00][27098-1-1-0.54][27098-1-0-0.61][27098-1-0-0.99][27526-0-0-0.98]
[27526-0-0-0.81][27526-0-0-0.73][27639-3-3-0.79][27639-3-3-0.94][27639-3-3-0.84][27698-3-3-0.63][27698-3-3-0.92][27698-3-3-0.86][27772-0-0-0.98][27772-0-0-0.98]
[27772-0-0-0.78][27890-1-1-0.82][27890-1-1-0.78][27890-1-1-0.61][28040-0-0-0.75][28040-0-0-0.79][28040-0-0-0.84][28503-2-2-0.94][28503-2-2-1.00][28503-2-2-1.00]
[28577-1-1-0.92][28577-1-1-0.94][28577-1-1-0.95][28959-0-0-1.00][28959-0-0-1.00][28959-0-0-0.97][29198-3-3-0.62][29198-3-3-0.48][29198-3-3-0.56][29777-0-0-1.00]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-3-0.47][29877-2-3-0.72][29877-2-1-0.59][30035-1-1-0.99][30035-1-1-0.95][30035-1-1-0.96][30098-0-0-0.44][30098-0-0-0.82]
[30098-0-0-0.88][30326-1-1-1.00][30326-1-1-1.00][30326-1-1-1.00][30572-2-2-0.86][30572-2-2-0.72][30572-2-2-0.90][30716-0-1-0.93][30716-0-1-0.91][30716-0-1-0.60]
[30806-2-2-0.63][30806-2-0-0.53][30806-2-2-0.70][30906-1-1-1.00][30906-1-1-0.99][30906-1-1-0.92][31007-0-0-1.00][31007-0-0-1.00][31007-0-2-0.66][31181-3-3-0.65]
[31181-3-0-0.66][31181-3-0-0.75][31238-0-0-0.81][31238-0-0-0.58][31238-0-0-0.64][31347-0-0-0.96][31347-0-0-0.89][31347-0-0-0.87][31422-2-0-0.33][31422-2-0-0.49]
[31422-2-0-0.61][31429-3-3-0.55][31429-3-3-0.37][31429-3-0-0.51][31431-0-0-1.00][31431-0-0-1.00][31431-0-0-0.49][31432-1-1-0.99][31432-1-1-0.96][31432-1-1-1.00]
[31477-0-3-0.66][31477-0-3-0.62][31477-0-3-0.78][31524-1-1-0.39][31524-1-2-0.42][31524-1-0-0.47][31597-1-2-0.79][31597-1-2-0.57][31597-1-2-0.58][31619-1-0-0.95]
[31619-1-0-0.99][31619-1-0-0.99][31701-0-0-0.99][31701-0-0-0.96][31701-0-0-0.91][31755-0-0-1.00][31755-0-0-1.00][31755-0-0-1.00][31854-3-3-0.90][31854-3-3-0.48]
[31854-3-3-0.44][32074-1-1-0.77][32074-1-1-0.80][32074-1-3-0.84][32078-3-3-1.00][32078-3-3-1.00][32078-3-3-0.98][32111-1-1-0.97][32111-1-1-0.88][32111-1-1-0.94]
[32127-1-1-0.94][32127-1-1-0.84][32127-1-1-0.92][32140-3-3-1.00][32140-3-3-1.00][32140-3-3-1.00][32263-2-0-0.71][32263-2-0-0.42][32263-2-0-0.68][32365-0-0-0.97]
[32365-0-0-1.00][32365-0-0-1.00][32411-2-0-0.51][32411-2-0-0.72][32411-2-0-0.92][32429-3-0-0.58][32429-3-3-0.69][32429-3-0-0.58][32473-3-0-0.85][32473-3-0-0.90]
[32473-3-0-0.94][32574-3-3-1.00][32574-3-3-0.99][32574-3-3-0.99][32584-0-0-0.99][32584-0-0-0.99][32584-0-0-0.98][32622-0-0-0.59][32622-0-0-0.82][32622-0-0-0.70]
[32858-3-3-0.99][32858-3-3-0.98][32858-3-3-0.94][32969-3-0-0.51][32969-3-3-0.52][32969-3-0-0.55][33016-2-2-0.87][33016-2-2-0.92][33016-2-2-0.73][33031-1-1-0.65]
[33031-1-1-0.49][33031-1-0-0.73][33035-2-2-0.90][33035-2-2-0.91][33035-2-2-0.95][33133-2-2-0.95][33133-2-2-0.78][33133-2-2-0.55][33173-2-3-0.69][33173-2-2-0.41]
[33173-2-2-0.69][33175-3-2-0.78][33175-3-2-0.85][33175-3-2-0.87][33306-3-1-0.58][33306-3-1-0.50][33306-3-3-0.50][33309-2-3-0.70][33309-2-3-0.93][33309-2-3-0.85]
[33474-0-0-0.72][33474-0-0-0.73][33474-0-0-0.98][33478-2-0-0.78][33478-2-1-0.51][33478-2-0-0.76][33618-1-1-0.75][33618-1-1-0.74][33618-1-1-0.77][33712-0-0-0.96]
[33712-0-0-0.99][33712-0-0-1.00][33782-2-1-0.95][33782-2-1-0.91][33782-2-1-0.88][33914-3-3-0.53][33914-3-3-0.99][33914-3-3-0.99][34076-3-2-0.75][34076-3-2-0.91]
[34076-3-2-0.65][34112-2-2-0.99][34112-2-2-0.72][34112-2-2-0.80][34138-2-3-0.86][34138-2-3-0.89][34138-2-3-0.82][34239-1-1-0.41][34239-1-2-0.49][34239-1-2-0.45]
[34364-2-1-0.77][34364-2-2-0.74][34364-2-2-0.94][34617-1-2-0.84][34617-1-2-0.62][34617-1-2-0.68][34751-3-3-0.88][34751-3-3-0.47][34751-3-3-0.68][34783-2-1-0.83]
[34783-2-1-0.73][34783-2-2-0.50][35015-3-3-1.00][35015-3-3-0.78][35015-3-3-0.81][35018-1-1-0.90][35018-1-1-0.52][35018-1-2-0.76][35288-2-2-0.61][35288-2-1-0.47]
[35288-2-2-0.47]
---------------------------
I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.761 | Acc: 99.500% | Wgt Acc: 99.494%
	I - Batch: 100 | Loss: 0.758 | Acc: 99.625% | Wgt Acc: 99.633%
	I - Batch: 150 | Loss: 0.758 | Acc: 99.458% | Wgt Acc: 99.456%
	I - Batch: 200 | Loss: 0.758 | Acc: 99.500% | Wgt Acc: 99.493%
	I - Batch: 250 | Loss: 0.757 | Acc: 99.525% | Wgt Acc: 99.510%
	I - Batch: 300 | Loss: 0.757 | Acc: 99.542% | Wgt Acc: 99.531%
	I - Batch: 350 | Loss: 0.757 | Acc: 99.536% | Wgt Acc: 99.525%
	I - Batch: 400 | Loss: 0.757 | Acc: 99.562% | Wgt Acc: 99.556%
	I - Batch: 450 | Loss: 0.756 | Acc: 99.569% | Wgt Acc: 99.565%
I - num batch: 478
I - Train -- Loss: 0.756 | Acc: 99.594% | Wgt Acc: 99.590% | LR: 1.250000e-04 | Dur: 349.53s
I - Confusion Matrix: [row->prediction - col->label]
[[2082.    1.    5.    9.]
 [   0. 1731.    2.    0.]
 [   3.    2. 2195.    3.]
 [   6.    0.    0. 1602.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.091 | Acc: 65.625% | Wgt Acc: 64.964%
I - num batch: 62
I - Val -- Loss: 1.097 | Acc: 64.730% | Wgt Acc: 64.085% | Dur: 36.66s
I - Confusion Matrix: [row->prediction - col->label]
[[222.  33.  34.  69.]
 [  0. 119.  31.   4.]
 [  4.  63. 123.  14.]
 [ 38.  19.  37. 171.]]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.753 | Acc: 99.750% | Wgt Acc: 99.718%
	I - Batch: 100 | Loss: 0.757 | Acc: 99.375% | Wgt Acc: 99.338%
	I - Batch: 150 | Loss: 0.756 | Acc: 99.583% | Wgt Acc: 99.559%
	I - Batch: 200 | Loss: 0.756 | Acc: 99.562% | Wgt Acc: 99.535%
	I - Batch: 250 | Loss: 0.755 | Acc: 99.625% | Wgt Acc: 99.605%
	I - Batch: 300 | Loss: 0.756 | Acc: 99.583% | Wgt Acc: 99.563%
	I - Batch: 350 | Loss: 0.756 | Acc: 99.589% | Wgt Acc: 99.577%
	I - Batch: 400 | Loss: 0.756 | Acc: 99.609% | Wgt Acc: 99.599%
	I - Batch: 450 | Loss: 0.756 | Acc: 99.611% | Wgt Acc: 99.599%
I - num batch: 478
I - Train -- Loss: 0.756 | Acc: 99.607% | Wgt Acc: 99.596% | LR: 1.250000e-04 | Dur: 347.87s
I - Confusion Matrix: [row->prediction - col->label]
[[2085.    2.    6.   10.]
 [   0. 1731.    0.    2.]
 [   2.    1. 2195.    2.]
 [   4.    0.    1. 1600.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.096 | Acc: 67.250% | Wgt Acc: 65.908%
I - num batch: 62
I - Val -- Loss: 1.100 | Acc: 66.565% | Wgt Acc: 65.353% | Dur: 36.61s
I - Confusion Matrix: [row->prediction - col->label]
[[246.  29.  35.  85.]
 [  3. 131.  34.  16.]
 [  5.  61. 133.  14.]
 [ 10.  13.  23. 143.]]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.754 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.756 | Acc: 99.750% | Wgt Acc: 99.774%
	I - Batch: 150 | Loss: 0.754 | Acc: 99.833% | Wgt Acc: 99.849%
	I - Batch: 200 | Loss: 0.754 | Acc: 99.812% | Wgt Acc: 99.824%
	I - Batch: 250 | Loss: 0.754 | Acc: 99.800% | Wgt Acc: 99.814%
	I - Batch: 300 | Loss: 0.755 | Acc: 99.708% | Wgt Acc: 99.708%
	I - Batch: 350 | Loss: 0.755 | Acc: 99.696% | Wgt Acc: 99.698%
	I - Batch: 400 | Loss: 0.756 | Acc: 99.641% | Wgt Acc: 99.641%
	I - Batch: 450 | Loss: 0.756 | Acc: 99.639% | Wgt Acc: 99.637%
I - num batch: 478
I - Train -- Loss: 0.756 | Acc: 99.660% | Wgt Acc: 99.658% | LR: 1.250000e-04 | Dur: 351.24s
I - Confusion Matrix: [row->prediction - col->label]
[[2083.    1.    4.    8.]
 [   0. 1733.    0.    2.]
 [   3.    0. 2196.    1.]
 [   5.    0.    2. 1603.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.100 | Acc: 62.750% | Wgt Acc: 62.326%
I - num batch: 62
I - Val -- Loss: 1.109 | Acc: 61.774% | Wgt Acc: 61.458% | Dur: 39.72s
I - Confusion Matrix: [row->prediction - col->label]
[[225.  39.  43.  79.]
 [  4. 132.  43.  14.]
 [  3.  32.  91.   7.]
 [ 32.  31.  48. 158.]]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.754 | Acc: 99.625% | Wgt Acc: 99.635%
	I - Batch: 100 | Loss: 0.754 | Acc: 99.625% | Wgt Acc: 99.621%
	I - Batch: 150 | Loss: 0.754 | Acc: 99.667% | Wgt Acc: 99.672%
	I - Batch: 200 | Loss: 0.754 | Acc: 99.688% | Wgt Acc: 99.684%
	I - Batch: 250 | Loss: 0.755 | Acc: 99.675% | Wgt Acc: 99.679%
	I - Batch: 300 | Loss: 0.755 | Acc: 99.688% | Wgt Acc: 99.690%
	I - Batch: 350 | Loss: 0.755 | Acc: 99.661% | Wgt Acc: 99.658%
	I - Batch: 400 | Loss: 0.755 | Acc: 99.609% | Wgt Acc: 99.598%
	I - Batch: 450 | Loss: 0.755 | Acc: 99.653% | Wgt Acc: 99.643%
I - num batch: 478
I - Train -- Loss: 0.755 | Acc: 99.660% | Wgt Acc: 99.652% | LR: 1.250000e-04 | Dur: 355.93s
I - Confusion Matrix: [row->prediction - col->label]
[[2085.    0.    4.    7.]
 [   0. 1731.    2.    1.]
 [   1.    3. 2196.    3.]
 [   5.    0.    0. 1603.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.077 | Acc: 68.125% | Wgt Acc: 67.574%
I - num batch: 62
I - Val -- Loss: 1.085 | Acc: 67.278% | Wgt Acc: 66.757% | Dur: 38.39s
I - Confusion Matrix: [row->prediction - col->label]
[[235.  23.  23.  73.]
 [  7. 161.  61.  22.]
 [  4.  39. 117.  16.]
 [ 18.  11.  24. 147.]]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.753 | Acc: 99.625% | Wgt Acc: 99.663%
	I - Batch: 100 | Loss: 0.753 | Acc: 99.688% | Wgt Acc: 99.704%
	I - Batch: 150 | Loss: 0.753 | Acc: 99.708% | Wgt Acc: 99.728%
	I - Batch: 200 | Loss: 0.754 | Acc: 99.625% | Wgt Acc: 99.641%
	I - Batch: 250 | Loss: 0.754 | Acc: 99.650% | Wgt Acc: 99.663%
	I - Batch: 300 | Loss: 0.754 | Acc: 99.667% | Wgt Acc: 99.677%
	I - Batch: 350 | Loss: 0.754 | Acc: 99.661% | Wgt Acc: 99.666%
	I - Batch: 400 | Loss: 0.754 | Acc: 99.672% | Wgt Acc: 99.673%
	I - Batch: 450 | Loss: 0.753 | Acc: 99.681% | Wgt Acc: 99.678%
I - num batch: 478
I - Train -- Loss: 0.754 | Acc: 99.660% | Wgt Acc: 99.652% | LR: 1.250000e-04 | Dur: 350.95s
I - Confusion Matrix: [row->prediction - col->label]
[[2085.    2.    2.    8.]
 [   0. 1731.    2.    2.]
 [   2.    1. 2196.    1.]
 [   4.    0.    2. 1603.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.084 | Acc: 66.750% | Wgt Acc: 66.102%
I - num batch: 62
I - Val -- Loss: 1.090 | Acc: 65.647% | Wgt Acc: 65.104% | Dur: 38.67s
I - Confusion Matrix: [row->prediction - col->label]
[[220.  23.  29.  61.]
 [  5. 128.  36.  12.]
 [  6.  59. 125.  14.]
 [ 33.  24.  35. 171.]]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.753 | Acc: 99.875% | Wgt Acc: 99.888%
	I - Batch: 100 | Loss: 0.752 | Acc: 99.875% | Wgt Acc: 99.887%
	I - Batch: 150 | Loss: 0.753 | Acc: 99.708% | Wgt Acc: 99.718%
	I - Batch: 200 | Loss: 0.754 | Acc: 99.656% | Wgt Acc: 99.655%
	I - Batch: 250 | Loss: 0.753 | Acc: 99.650% | Wgt Acc: 99.650%
	I - Batch: 300 | Loss: 0.753 | Acc: 99.625% | Wgt Acc: 99.620%
	I - Batch: 350 | Loss: 0.754 | Acc: 99.607% | Wgt Acc: 99.597%
	I - Batch: 400 | Loss: 0.753 | Acc: 99.641% | Wgt Acc: 99.630%
	I - Batch: 450 | Loss: 0.753 | Acc: 99.653% | Wgt Acc: 99.643%
I - num batch: 478
I - Train -- Loss: 0.754 | Acc: 99.647% | Wgt Acc: 99.637% | LR: 1.250000e-04 | Dur: 352.64s
I - Confusion Matrix: [row->prediction - col->label]
[[2085.    1.    4.    7.]
 [   0. 1730.    0.    2.]
 [   1.    3. 2196.    2.]
 [   5.    0.    2. 1603.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.108 | Acc: 63.000% | Wgt Acc: 61.827%
I - num batch: 62
I - Val -- Loss: 1.118 | Acc: 62.080% | Wgt Acc: 61.096% | Dur: 35.47s
I - Confusion Matrix: [row->prediction - col->label]
[[252.  41.  53. 114.]
 [  0. 132.  41.   8.]
 [  3.  40.  95.   6.]
 [  9.  21.  36. 130.]]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.752 | Acc: 99.750% | Wgt Acc: 99.746%
	I - Batch: 100 | Loss: 0.754 | Acc: 99.500% | Wgt Acc: 99.507%
	I - Batch: 150 | Loss: 0.756 | Acc: 99.375% | Wgt Acc: 99.362%
	I - Batch: 200 | Loss: 0.756 | Acc: 99.500% | Wgt Acc: 99.486%
	I - Batch: 250 | Loss: 0.756 | Acc: 99.575% | Wgt Acc: 99.560%
	I - Batch: 300 | Loss: 0.755 | Acc: 99.646% | Wgt Acc: 99.634%
	I - Batch: 350 | Loss: 0.755 | Acc: 99.679% | Wgt Acc: 99.666%
	I - Batch: 400 | Loss: 0.755 | Acc: 99.656% | Wgt Acc: 99.648%
	I - Batch: 450 | Loss: 0.755 | Acc: 99.653% | Wgt Acc: 99.646%
I - num batch: 478
I - Train -- Loss: 0.756 | Acc: 99.647% | Wgt Acc: 99.640% | LR: 1.250000e-04 | Dur: 353.18s
I - Confusion Matrix: [row->prediction - col->label]
[[2084.    3.    4.    8.]
 [   0. 1731.    2.    2.]
 [   2.    0. 2196.    1.]
 [   5.    0.    0. 1603.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.096 | Acc: 65.750% | Wgt Acc: 64.770%
I - num batch: 62
I - Val -- Loss: 1.107 | Acc: 64.934% | Wgt Acc: 64.040% | Dur: 37.35s
I - Confusion Matrix: [row->prediction - col->label]
[[236.  33.  29.  88.]
 [  3. 129.  48.   7.]
 [  5.  53. 121.  12.]
 [ 20.  19.  27. 151.]]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.754 | Acc: 99.750% | Wgt Acc: 99.773%
	I - Batch: 100 | Loss: 0.753 | Acc: 99.875% | Wgt Acc: 99.887%
	I - Batch: 150 | Loss: 0.755 | Acc: 99.708% | Wgt Acc: 99.719%
	I - Batch: 200 | Loss: 0.755 | Acc: 99.656% | Wgt Acc: 99.662%
	I - Batch: 250 | Loss: 0.755 | Acc: 99.675% | Wgt Acc: 99.685%
	I - Batch: 300 | Loss: 0.754 | Acc: 99.688% | Wgt Acc: 99.699%
	I - Batch: 350 | Loss: 0.754 | Acc: 99.696% | Wgt Acc: 99.702%
	I - Batch: 400 | Loss: 0.755 | Acc: 99.672% | Wgt Acc: 99.669%
	I - Batch: 450 | Loss: 0.755 | Acc: 99.667% | Wgt Acc: 99.665%
I - num batch: 478
I - Train -- Loss: 0.755 | Acc: 99.673% | Wgt Acc: 99.670% | LR: 1.250000e-04 | Dur: 349.98s
I - Confusion Matrix: [row->prediction - col->label]
[[2084.    1.    3.    6.]
 [   0. 1731.    1.    1.]
 [   2.    1. 2196.    2.]
 [   5.    1.    2. 1605.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.105 | Acc: 66.125% | Wgt Acc: 64.853%
I - num batch: 62
I - Val -- Loss: 1.111 | Acc: 65.647% | Wgt Acc: 64.447% | Dur: 38.73s
I - Confusion Matrix: [row->prediction - col->label]
[[233.  31.  22.  80.]
 [  2. 116.  30.   6.]
 [  7.  68. 141.  18.]
 [ 22.  19.  32. 154.]]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.757 | Acc: 99.625% | Wgt Acc: 99.606%
	I - Batch: 100 | Loss: 0.756 | Acc: 99.688% | Wgt Acc: 99.691%
	I - Batch: 150 | Loss: 0.756 | Acc: 99.583% | Wgt Acc: 99.597%
	I - Batch: 200 | Loss: 0.756 | Acc: 99.562% | Wgt Acc: 99.570%
	I - Batch: 250 | Loss: 0.755 | Acc: 99.625% | Wgt Acc: 99.633%
	I - Batch: 300 | Loss: 0.755 | Acc: 99.646% | Wgt Acc: 99.652%
	I - Batch: 350 | Loss: 0.755 | Acc: 99.625% | Wgt Acc: 99.622%
	I - Batch: 400 | Loss: 0.755 | Acc: 99.656% | Wgt Acc: 99.651%
	I - Batch: 450 | Loss: 0.754 | Acc: 99.681% | Wgt Acc: 99.678%
I - num batch: 478
I - Train -- Loss: 0.754 | Acc: 99.686% | Wgt Acc: 99.682% | LR: 1.250000e-04 | Dur: 350.06s
I - Confusion Matrix: [row->prediction - col->label]
[[2085.    3.    5.    5.]
 [   0. 1730.    1.    2.]
 [   1.    1. 2196.    1.]
 [   5.    0.    0. 1606.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.093 | Acc: 64.875% | Wgt Acc: 64.464%
I - num batch: 62
I - Val -- Loss: 1.104 | Acc: 64.016% | Wgt Acc: 63.723% | Dur: 35.92s
I - Confusion Matrix: [row->prediction - col->label]
[[210.  27.  25.  58.]
 [  6. 126.  45.  11.]
 [  6.  53. 116.  13.]
 [ 42.  28.  39. 176.]]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.753 | Acc: 99.750% | Wgt Acc: 99.717%
	I - Batch: 100 | Loss: 0.755 | Acc: 99.688% | Wgt Acc: 99.676%
	I - Batch: 150 | Loss: 0.755 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 200 | Loss: 0.755 | Acc: 99.781% | Wgt Acc: 99.775%
	I - Batch: 250 | Loss: 0.755 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 300 | Loss: 0.755 | Acc: 99.750% | Wgt Acc: 99.742%
	I - Batch: 350 | Loss: 0.755 | Acc: 99.714% | Wgt Acc: 99.710%
	I - Batch: 400 | Loss: 0.754 | Acc: 99.734% | Wgt Acc: 99.729%
	I - Batch: 450 | Loss: 0.755 | Acc: 99.681% | Wgt Acc: 99.674%
I - num batch: 478
I - Train -- Loss: 0.755 | Acc: 99.699% | Wgt Acc: 99.693% | LR: 1.250000e-04 | Dur: 346.54s
I - Confusion Matrix: [row->prediction - col->label]
[[2086.    3.    4.    4.]
 [   1. 1730.    1.    1.]
 [   0.    1. 2196.    3.]
 [   4.    0.    1. 1606.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.097 | Acc: 64.000% | Wgt Acc: 63.909%
I - num batch: 62
I - Val -- Loss: 1.110 | Acc: 62.589% | Wgt Acc: 62.591% | Dur: 31.86s
I - Confusion Matrix: [row->prediction - col->label]
[[223.  32.  34.  78.]
 [  2. 146.  61.  17.]
 [  3.  24.  83.   1.]
 [ 36.  32.  47. 162.]]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.766 | Acc: 99.375% | Wgt Acc: 99.326%
	I - Batch: 100 | Loss: 0.763 | Acc: 99.500% | Wgt Acc: 99.480%
	I - Batch: 150 | Loss: 0.761 | Acc: 99.625% | Wgt Acc: 99.615%
	I - Batch: 200 | Loss: 0.762 | Acc: 99.562% | Wgt Acc: 99.557%
	I - Batch: 250 | Loss: 0.761 | Acc: 99.550% | Wgt Acc: 99.550%
	I - Batch: 300 | Loss: 0.760 | Acc: 99.604% | Wgt Acc: 99.601%
	I - Batch: 350 | Loss: 0.759 | Acc: 99.643% | Wgt Acc: 99.638%
	I - Batch: 400 | Loss: 0.759 | Acc: 99.672% | Wgt Acc: 99.669%
	I - Batch: 450 | Loss: 0.759 | Acc: 99.681% | Wgt Acc: 99.678%
I - num batch: 478
I - Train -- Loss: 0.758 | Acc: 99.699% | Wgt Acc: 99.696% | LR: 1.250000e-04 | Dur: 305.47s
I - Confusion Matrix: [row->prediction - col->label]
[[2085.    2.    5.    4.]
 [   0. 1731.    0.    1.]
 [   3.    1. 2196.    3.]
 [   3.    0.    1. 1606.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.089 | Acc: 64.750% | Wgt Acc: 64.159%
I - num batch: 62
I - Val -- Loss: 1.100 | Acc: 64.016% | Wgt Acc: 63.451% | Dur: 31.58s
I - Confusion Matrix: [row->prediction - col->label]
[[217.  21.  21.  69.]
 [  1. 131.  36.  15.]
 [  5.  59. 121.  15.]
 [ 41.  23.  47. 159.]]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.752 | Acc: 99.875% | Wgt Acc: 99.887%
	I - Batch: 100 | Loss: 0.754 | Acc: 99.750% | Wgt Acc: 99.761%
	I - Batch: 150 | Loss: 0.754 | Acc: 99.750% | Wgt Acc: 99.757%
	I - Batch: 200 | Loss: 0.753 | Acc: 99.750% | Wgt Acc: 99.754%
	I - Batch: 250 | Loss: 0.754 | Acc: 99.725% | Wgt Acc: 99.724%
	I - Batch: 300 | Loss: 0.754 | Acc: 99.750% | Wgt Acc: 99.752%
	I - Batch: 350 | Loss: 0.754 | Acc: 99.768% | Wgt Acc: 99.767%
	I - Batch: 400 | Loss: 0.754 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 450 | Loss: 0.754 | Acc: 99.764% | Wgt Acc: 99.762%
I - num batch: 478
I - Train -- Loss: 0.754 | Acc: 99.778% | Wgt Acc: 99.776% | LR: 1.250000e-04 | Dur: 301.96s
I - Confusion Matrix: [row->prediction - col->label]
[[2088.    1.    4.    5.]
 [   0. 1733.    1.    1.]
 [   0.    0. 2196.    1.]
 [   3.    0.    1. 1607.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.092 | Acc: 65.875% | Wgt Acc: 65.186%
I - num batch: 62
I - Val -- Loss: 1.098 | Acc: 65.341% | Wgt Acc: 64.832% | Dur: 31.57s
I - Confusion Matrix: [row->prediction - col->label]
[[225.  30.  26.  78.]
 [  4. 143.  45.  16.]
 [  5.  45. 117.   8.]
 [ 30.  16.  37. 156.]]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.756 | Acc: 99.375% | Wgt Acc: 99.383%
	I - Batch: 100 | Loss: 0.754 | Acc: 99.562% | Wgt Acc: 99.564%
	I - Batch: 150 | Loss: 0.754 | Acc: 99.625% | Wgt Acc: 99.634%
	I - Batch: 200 | Loss: 0.753 | Acc: 99.719% | Wgt Acc: 99.725%
	I - Batch: 250 | Loss: 0.753 | Acc: 99.700% | Wgt Acc: 99.701%
	I - Batch: 300 | Loss: 0.753 | Acc: 99.729% | Wgt Acc: 99.728%
	I - Batch: 350 | Loss: 0.753 | Acc: 99.768% | Wgt Acc: 99.767%
	I - Batch: 400 | Loss: 0.753 | Acc: 99.781% | Wgt Acc: 99.778%
	I - Batch: 450 | Loss: 0.752 | Acc: 99.792% | Wgt Acc: 99.787%
I - num batch: 478
I - Train -- Loss: 0.752 | Acc: 99.791% | Wgt Acc: 99.788% | LR: 1.250000e-04 | Dur: 303.30s
I - Confusion Matrix: [row->prediction - col->label]
[[2086.    2.    3.    4.]
 [   0. 1731.    0.    0.]
 [   2.    1. 2199.    1.]
 [   3.    0.    0. 1609.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.078 | Acc: 66.750% | Wgt Acc: 66.519%
I - num batch: 62
I - Val -- Loss: 1.091 | Acc: 65.953% | Wgt Acc: 65.738% | Dur: 30.97s
I - Confusion Matrix: [row->prediction - col->label]
[[202.  19.  16.  49.]
 [  6. 147.  48.  20.]
 [  8.  52. 130.  21.]
 [ 48.  16.  31. 168.]]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.752 | Acc: 99.625% | Wgt Acc: 99.605%
	I - Batch: 100 | Loss: 0.752 | Acc: 99.688% | Wgt Acc: 99.662%
	I - Batch: 150 | Loss: 0.751 | Acc: 99.708% | Wgt Acc: 99.689%
	I - Batch: 200 | Loss: 0.752 | Acc: 99.750% | Wgt Acc: 99.732%
	I - Batch: 250 | Loss: 0.751 | Acc: 99.800% | Wgt Acc: 99.786%
	I - Batch: 300 | Loss: 0.751 | Acc: 99.792% | Wgt Acc: 99.775%
	I - Batch: 350 | Loss: 0.751 | Acc: 99.804% | Wgt Acc: 99.791%
	I - Batch: 400 | Loss: 0.751 | Acc: 99.828% | Wgt Acc: 99.817%
	I - Batch: 450 | Loss: 0.751 | Acc: 99.819% | Wgt Acc: 99.809%
I - num batch: 478
I - Train -- Loss: 0.751 | Acc: 99.830% | Wgt Acc: 99.820% | LR: 1.250000e-04 | Dur: 306.38s
I - Confusion Matrix: [row->prediction - col->label]
[[2087.    2.    0.    5.]
 [   1. 1731.    0.    0.]
 [   0.    1. 2202.    1.]
 [   3.    0.    0. 1608.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.083 | Acc: 65.125% | Wgt Acc: 64.436%
I - num batch: 62
I - Val -- Loss: 1.087 | Acc: 65.138% | Wgt Acc: 64.583% | Dur: 30.00s
I - Confusion Matrix: [row->prediction - col->label]
[[202.  21.  16.  56.]
 [  9. 132.  40.  21.]
 [  9.  69. 141.  17.]
 [ 44.  12.  28. 164.]]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.753 | Acc: 99.750% | Wgt Acc: 99.746%
	I - Batch: 100 | Loss: 0.752 | Acc: 99.812% | Wgt Acc: 99.803%
	I - Batch: 150 | Loss: 0.752 | Acc: 99.833% | Wgt Acc: 99.821%
	I - Batch: 200 | Loss: 0.752 | Acc: 99.844% | Wgt Acc: 99.831%
	I - Batch: 250 | Loss: 0.752 | Acc: 99.800% | Wgt Acc: 99.791%
	I - Batch: 300 | Loss: 0.752 | Acc: 99.833% | Wgt Acc: 99.826%
	I - Batch: 350 | Loss: 0.752 | Acc: 99.857% | Wgt Acc: 99.851%
	I - Batch: 400 | Loss: 0.752 | Acc: 99.859% | Wgt Acc: 99.852%
	I - Batch: 450 | Loss: 0.751 | Acc: 99.847% | Wgt Acc: 99.840%
I - num batch: 478
I - Train -- Loss: 0.752 | Acc: 99.843% | Wgt Acc: 99.835% | LR: 1.250000e-04 | Dur: 298.75s
I - Confusion Matrix: [row->prediction - col->label]
[[2087.    1.    0.    4.]
 [   1. 1731.    0.    0.]
 [   0.    2. 2202.    1.]
 [   3.    0.    0. 1609.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.082 | Acc: 66.625% | Wgt Acc: 66.741%
I - num batch: 62
I - Val -- Loss: 1.091 | Acc: 65.341% | Wgt Acc: 65.602% | Dur: 30.83s
I - Confusion Matrix: [row->prediction - col->label]
[[186.  18.  15.  37.]
 [  6. 142.  46.  16.]
 [  7.  58. 122.  14.]
 [ 65.  16.  42. 191.]]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.757 | Acc: 99.875% | Wgt Acc: 99.887%
	I - Batch: 100 | Loss: 0.758 | Acc: 99.812% | Wgt Acc: 99.817%
	I - Batch: 150 | Loss: 0.759 | Acc: 99.750% | Wgt Acc: 99.756%
	I - Batch: 200 | Loss: 0.758 | Acc: 99.812% | Wgt Acc: 99.817%
	I - Batch: 250 | Loss: 0.757 | Acc: 99.850% | Wgt Acc: 99.854%
	I - Batch: 300 | Loss: 0.758 | Acc: 99.812% | Wgt Acc: 99.812%
	I - Batch: 350 | Loss: 0.758 | Acc: 99.821% | Wgt Acc: 99.819%
	I - Batch: 400 | Loss: 0.759 | Acc: 99.766% | Wgt Acc: 99.757%
	I - Batch: 450 | Loss: 0.760 | Acc: 99.736% | Wgt Acc: 99.734%
I - num batch: 478
I - Train -- Loss: 0.761 | Acc: 99.738% | Wgt Acc: 99.738% | LR: 1.250000e-04 | Dur: 303.86s
I - Confusion Matrix: [row->prediction - col->label]
[[2082.    1.    2.    4.]
 [   1. 1732.    0.    1.]
 [   0.    0. 2200.    2.]
 [   8.    1.    0. 1607.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.105 | Acc: 64.500% | Wgt Acc: 63.798%
I - num batch: 62
I - Val -- Loss: 1.111 | Acc: 64.220% | Wgt Acc: 63.655% | Dur: 30.72s
I - Confusion Matrix: [row->prediction - col->label]
[[212.  29.  30.  63.]
 [  0. 115.  24.   6.]
 [  2.  59. 127.  13.]
 [ 50.  31.  44. 176.]]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.769 | Acc: 99.250% | Wgt Acc: 99.238%
	I - Batch: 100 | Loss: 0.764 | Acc: 99.375% | Wgt Acc: 99.337%
	I - Batch: 150 | Loss: 0.762 | Acc: 99.500% | Wgt Acc: 99.465%
	I - Batch: 200 | Loss: 0.761 | Acc: 99.625% | Wgt Acc: 99.599%
	I - Batch: 250 | Loss: 0.761 | Acc: 99.700% | Wgt Acc: 99.679%
	I - Batch: 300 | Loss: 0.760 | Acc: 99.708% | Wgt Acc: 99.690%
	I - Batch: 350 | Loss: 0.759 | Acc: 99.732% | Wgt Acc: 99.714%
	I - Batch: 400 | Loss: 0.758 | Acc: 99.734% | Wgt Acc: 99.718%
	I - Batch: 450 | Loss: 0.757 | Acc: 99.736% | Wgt Acc: 99.722%
I - num batch: 478
I - Train -- Loss: 0.757 | Acc: 99.751% | Wgt Acc: 99.738% | LR: 1.250000e-04 | Dur: 300.59s
I - Confusion Matrix: [row->prediction - col->label]
[[2085.    3.    0.    8.]
 [   2. 1730.    0.    0.]
 [   0.    1. 2202.    1.]
 [   4.    0.    0. 1605.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.105 | Acc: 64.250% | Wgt Acc: 63.159%
I - num batch: 62
I - Val -- Loss: 1.115 | Acc: 63.405% | Wgt Acc: 62.432% | Dur: 26.06s
I - Confusion Matrix: [row->prediction - col->label]
[[236.  30.  37.  95.]
 [  4. 127.  41.  11.]
 [  5.  59. 117.  10.]
 [ 19.  18.  30. 142.]]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.755 | Acc: 99.500% | Wgt Acc: 99.465%
	I - Batch: 100 | Loss: 0.753 | Acc: 99.750% | Wgt Acc: 99.733%
	I - Batch: 150 | Loss: 0.751 | Acc: 99.833% | Wgt Acc: 99.822%
	I - Batch: 200 | Loss: 0.751 | Acc: 99.875% | Wgt Acc: 99.866%
	I - Batch: 250 | Loss: 0.752 | Acc: 99.750% | Wgt Acc: 99.746%
	I - Batch: 300 | Loss: 0.752 | Acc: 99.750% | Wgt Acc: 99.742%
	I - Batch: 350 | Loss: 0.752 | Acc: 99.768% | Wgt Acc: 99.759%
	I - Batch: 400 | Loss: 0.752 | Acc: 99.781% | Wgt Acc: 99.771%
	I - Batch: 450 | Loss: 0.751 | Acc: 99.806% | Wgt Acc: 99.797%
I - num batch: 478
I - Train -- Loss: 0.751 | Acc: 99.817% | Wgt Acc: 99.808% | LR: 1.250000e-04 | Dur: 250.87s
I - Confusion Matrix: [row->prediction - col->label]
[[2087.    1.    1.    5.]
 [   1. 1731.    0.    0.]
 [   0.    2. 2201.    1.]
 [   3.    0.    0. 1608.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.104 | Acc: 66.000% | Wgt Acc: 64.825%
I - num batch: 62
I - Val -- Loss: 1.114 | Acc: 64.628% | Wgt Acc: 63.632% | Dur: 26.04s
I - Confusion Matrix: [row->prediction - col->label]
[[247.  27.  35. 107.]
 [  6. 150.  52.  21.]
 [  4.  49. 113.   6.]
 [  7.   8.  25. 124.]]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.751 | Acc: 99.625% | Wgt Acc: 99.579%
	I - Batch: 100 | Loss: 0.750 | Acc: 99.750% | Wgt Acc: 99.719%
	I - Batch: 150 | Loss: 0.749 | Acc: 99.833% | Wgt Acc: 99.812%
	I - Batch: 200 | Loss: 0.750 | Acc: 99.844% | Wgt Acc: 99.831%
	I - Batch: 250 | Loss: 0.750 | Acc: 99.825% | Wgt Acc: 99.814%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.854% | Wgt Acc: 99.845%
	I - Batch: 350 | Loss: 0.749 | Acc: 99.857% | Wgt Acc: 99.847%
	I - Batch: 400 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.866%
	I - Batch: 450 | Loss: 0.750 | Acc: 99.819% | Wgt Acc: 99.803%
I - num batch: 478
I - Train -- Loss: 0.750 | Acc: 99.817% | Wgt Acc: 99.802% | LR: 1.250000e-04 | Dur: 250.87s
I - Confusion Matrix: [row->prediction - col->label]
[[2088.    3.    0.    6.]
 [   0. 1730.    0.    0.]
 [   0.    1. 2202.    1.]
 [   3.    0.    0. 1607.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.085 | Acc: 66.125% | Wgt Acc: 65.464%
I - num batch: 62
I - Val -- Loss: 1.091 | Acc: 65.545% | Wgt Acc: 64.923% | Dur: 26.04s
I - Confusion Matrix: [row->prediction - col->label]
[[205.  25.  22.  60.]
 [  6. 131.  37.  16.]
 [  8.  61. 143.  18.]
 [ 45.  17.  23. 164.]]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.750 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.749 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.860%
	I - Batch: 200 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.867%
	I - Batch: 250 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.865%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.869%
	I - Batch: 350 | Loss: 0.749 | Acc: 99.857% | Wgt Acc: 99.851%
	I - Batch: 400 | Loss: 0.750 | Acc: 99.844% | Wgt Acc: 99.838%
	I - Batch: 450 | Loss: 0.749 | Acc: 99.847% | Wgt Acc: 99.840%
I - num batch: 478
I - Train -- Loss: 0.749 | Acc: 99.856% | Wgt Acc: 99.850% | LR: 1.250000e-04 | Dur: 252.88s
I - Confusion Matrix: [row->prediction - col->label]
[[2087.    1.    0.    5.]
 [   1. 1733.    0.    0.]
 [   0.    0. 2202.    1.]
 [   3.    0.    0. 1608.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.092 | Acc: 66.250% | Wgt Acc: 65.158%
I - num batch: 62
I - Val -- Loss: 1.102 | Acc: 65.036% | Wgt Acc: 64.063% | Dur: 26.20s
I - Confusion Matrix: [row->prediction - col->label]
[[219.  24.  20.  62.]
 [  4. 126.  38.  18.]
 [  9.  74. 142.  27.]
 [ 32.  10.  25. 151.]]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.748 | Acc: 99.917% | Wgt Acc: 99.906%
	I - Batch: 200 | Loss: 0.748 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 250 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.870%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.869%
	I - Batch: 350 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.867%
	I - Batch: 400 | Loss: 0.749 | Acc: 99.859% | Wgt Acc: 99.852%
	I - Batch: 450 | Loss: 0.749 | Acc: 99.847% | Wgt Acc: 99.837%
I - num batch: 478
I - Train -- Loss: 0.749 | Acc: 99.856% | Wgt Acc: 99.847% | LR: 1.250000e-04 | Dur: 251.83s
I - Confusion Matrix: [row->prediction - col->label]
[[2088.    1.    0.    3.]
 [   0. 1732.    0.    1.]
 [   0.    1. 2202.    2.]
 [   3.    0.    0. 1608.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.084 | Acc: 65.750% | Wgt Acc: 64.964%
I - num batch: 62
I - Val -- Loss: 1.095 | Acc: 65.138% | Wgt Acc: 64.425% | Dur: 26.21s
I - Confusion Matrix: [row->prediction - col->label]
[[223.  29.  29.  68.]
 [  2. 124.  33.   9.]
 [  3.  61. 127.  16.]
 [ 36.  20.  36. 165.]]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.749 | Acc: 99.938% | Wgt Acc: 99.929%
	I - Batch: 150 | Loss: 0.748 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.749 | Acc: 99.844% | Wgt Acc: 99.831%
	I - Batch: 250 | Loss: 0.749 | Acc: 99.850% | Wgt Acc: 99.842%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.868%
	I - Batch: 350 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.871%
	I - Batch: 400 | Loss: 0.749 | Acc: 99.891% | Wgt Acc: 99.887%
	I - Batch: 450 | Loss: 0.749 | Acc: 99.903% | Wgt Acc: 99.900%
I - num batch: 478
I - Train -- Loss: 0.749 | Acc: 99.895% | Wgt Acc: 99.891% | LR: 1.250000e-04 | Dur: 252.84s
I - Confusion Matrix: [row->prediction - col->label]
[[2088.    1.    0.    2.]
 [   0. 1733.    0.    0.]
 [   0.    0. 2202.    2.]
 [   3.    0.    0. 1610.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.088 | Acc: 66.375% | Wgt Acc: 65.630%
I - num batch: 62
I - Val -- Loss: 1.097 | Acc: 66.055% | Wgt Acc: 65.444% | Dur: 26.19s
I - Confusion Matrix: [row->prediction - col->label]
[[219.  17.  19.  65.]
 [  5. 139.  49.  22.]
 [  8.  65. 131.  12.]
 [ 32.  13.  26. 159.]]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.748 | Acc: 99.958% | Wgt Acc: 99.962%
	I - Batch: 200 | Loss: 0.748 | Acc: 99.938% | Wgt Acc: 99.937%
	I - Batch: 250 | Loss: 0.748 | Acc: 99.925% | Wgt Acc: 99.927%
	I - Batch: 300 | Loss: 0.748 | Acc: 99.917% | Wgt Acc: 99.916%
	I - Batch: 350 | Loss: 0.749 | Acc: 99.893% | Wgt Acc: 99.891%
	I - Batch: 400 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.870%
	I - Batch: 450 | Loss: 0.749 | Acc: 99.889% | Wgt Acc: 99.884%
I - num batch: 478
I - Train -- Loss: 0.749 | Acc: 99.895% | Wgt Acc: 99.891% | LR: 1.250000e-04 | Dur: 255.54s
I - Confusion Matrix: [row->prediction - col->label]
[[2088.    1.    0.    2.]
 [   0. 1733.    0.    1.]
 [   0.    0. 2202.    1.]
 [   3.    0.    0. 1610.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.095 | Acc: 65.250% | Wgt Acc: 64.464%
I - num batch: 62
I - Val -- Loss: 1.111 | Acc: 63.710% | Wgt Acc: 62.998% | Dur: 26.82s
I - Confusion Matrix: [row->prediction - col->label]
[[237.  35.  29.  81.]
 [  2. 124.  44.  12.]
 [  3.  51. 106.   7.]
 [ 22.  24.  46. 158.]]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.750 | Acc: 99.625% | Wgt Acc: 99.633%
	I - Batch: 100 | Loss: 0.749 | Acc: 99.750% | Wgt Acc: 99.745%
	I - Batch: 150 | Loss: 0.749 | Acc: 99.833% | Wgt Acc: 99.830%
	I - Batch: 200 | Loss: 0.749 | Acc: 99.812% | Wgt Acc: 99.809%
	I - Batch: 250 | Loss: 0.749 | Acc: 99.850% | Wgt Acc: 99.848%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.875% | Wgt Acc: 99.873%
	I - Batch: 350 | Loss: 0.749 | Acc: 99.893% | Wgt Acc: 99.891%
	I - Batch: 400 | Loss: 0.749 | Acc: 99.906% | Wgt Acc: 99.905%
	I - Batch: 450 | Loss: 0.749 | Acc: 99.917% | Wgt Acc: 99.915%
I - num batch: 478
I - Train -- Loss: 0.749 | Acc: 99.921% | Wgt Acc: 99.920% | LR: 1.250000e-04 | Dur: 257.02s
I - Confusion Matrix: [row->prediction - col->label]
[[2088.    0.    0.    2.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   3.    0.    0. 1611.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.084 | Acc: 65.125% | Wgt Acc: 64.631%
I - num batch: 62
I - Val -- Loss: 1.096 | Acc: 63.710% | Wgt Acc: 63.247% | Dur: 26.84s
I - Confusion Matrix: [row->prediction - col->label]
[[201.  21.  16.  61.]
 [  8. 130.  47.  18.]
 [  8.  66. 131.  16.]
 [ 47.  17.  31. 163.]]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.749 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.749 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.749 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.749 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.749 | Acc: 99.975% | Wgt Acc: 99.977%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.958% | Wgt Acc: 99.958%
	I - Batch: 350 | Loss: 0.750 | Acc: 99.893% | Wgt Acc: 99.895%
	I - Batch: 400 | Loss: 0.753 | Acc: 99.828% | Wgt Acc: 99.824%
	I - Batch: 450 | Loss: 0.757 | Acc: 99.625% | Wgt Acc: 99.621%
I - num batch: 478
I - Train -- Loss: 0.761 | Acc: 99.503% | Wgt Acc: 99.490% | LR: 1.250000e-04 | Dur: 257.83s
I - Confusion Matrix: [row->prediction - col->label]
[[2080.    0.    1.    7.]
 [   2. 1724.    3.    1.]
 [   4.    6. 2196.    3.]
 [   5.    4.    2. 1603.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.089 | Acc: 65.750% | Wgt Acc: 66.047%
I - num batch: 62
I - Val -- Loss: 1.089 | Acc: 65.443% | Wgt Acc: 65.919% | Dur: 26.85s
I - Confusion Matrix: [row->prediction - col->label]
[[195.  22.  21.  45.]
 [ 20. 168.  64.  30.]
 [  4.  32. 104.   8.]
 [ 45.  12.  36. 175.]]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.795 | Acc: 98.500% | Wgt Acc: 98.508%
	I - Batch: 100 | Loss: 0.790 | Acc: 98.500% | Wgt Acc: 98.536%
	I - Batch: 150 | Loss: 0.786 | Acc: 98.792% | Wgt Acc: 98.818%
	I - Batch: 200 | Loss: 0.782 | Acc: 98.938% | Wgt Acc: 98.957%
	I - Batch: 250 | Loss: 0.778 | Acc: 99.125% | Wgt Acc: 99.138%
	I - Batch: 300 | Loss: 0.775 | Acc: 99.208% | Wgt Acc: 99.216%
	I - Batch: 350 | Loss: 0.772 | Acc: 99.304% | Wgt Acc: 99.312%
	I - Batch: 400 | Loss: 0.770 | Acc: 99.391% | Wgt Acc: 99.398%
	I - Batch: 450 | Loss: 0.768 | Acc: 99.431% | Wgt Acc: 99.440%
I - num batch: 478
I - Train -- Loss: 0.768 | Acc: 99.463% | Wgt Acc: 99.472% | LR: 1.250000e-04 | Dur: 255.17s
I - Confusion Matrix: [row->prediction - col->label]
[[2077.    0.    4.    8.]
 [   4. 1732.    4.    2.]
 [   3.    1. 2190.    3.]
 [   7.    1.    4. 1601.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.089 | Acc: 65.375% | Wgt Acc: 65.158%
I - num batch: 62
I - Val -- Loss: 1.095 | Acc: 65.036% | Wgt Acc: 64.968% | Dur: 26.62s
I - Confusion Matrix: [row->prediction - col->label]
[[205.  29.  24.  47.]
 [  9. 126.  40.  10.]
 [  4.  48. 116.  10.]
 [ 46.  31.  45. 191.]]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.754 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.752 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.752 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.752 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 250 | Loss: 0.751 | Acc: 99.950% | Wgt Acc: 99.944%
	I - Batch: 300 | Loss: 0.751 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 350 | Loss: 0.750 | Acc: 99.946% | Wgt Acc: 99.940%
	I - Batch: 400 | Loss: 0.750 | Acc: 99.953% | Wgt Acc: 99.947%
	I - Batch: 450 | Loss: 0.750 | Acc: 99.958% | Wgt Acc: 99.953%
I - num batch: 478
I - Train -- Loss: 0.750 | Acc: 99.948% | Wgt Acc: 99.944% | LR: 1.250000e-04 | Dur: 255.40s
I - Confusion Matrix: [row->prediction - col->label]
[[2090.    0.    0.    2.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   1.    0.    0. 1611.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.082 | Acc: 67.500% | Wgt Acc: 66.713%
I - num batch: 62
I - Val -- Loss: 1.093 | Acc: 66.259% | Wgt Acc: 65.557% | Dur: 26.28s
I - Confusion Matrix: [row->prediction - col->label]
[[209.  22.  21.  61.]
 [  6. 129.  28.  11.]
 [  9.  69. 146.  20.]
 [ 40.  14.  30. 166.]]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.749 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.748 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.748 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.748 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.748 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 350 | Loss: 0.748 | Acc: 99.964% | Wgt Acc: 99.960%
	I - Batch: 400 | Loss: 0.748 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.748 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.748 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 253.66s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.087 | Acc: 66.875% | Wgt Acc: 65.880%
I - num batch: 62
I - Val -- Loss: 1.100 | Acc: 65.749% | Wgt Acc: 64.832% | Dur: 26.35s
I - Confusion Matrix: [row->prediction - col->label]
[[231.  31.  27.  87.]
 [  7. 136.  35.  14.]
 [  5.  51. 131.  10.]
 [ 21.  16.  32. 147.]]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.747 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.747 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 250 | Loss: 0.747 | Acc: 99.950% | Wgt Acc: 99.944%
	I - Batch: 300 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 350 | Loss: 0.748 | Acc: 99.964% | Wgt Acc: 99.960%
	I - Batch: 400 | Loss: 0.747 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.747 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.747 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 255.44s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.087 | Acc: 67.375% | Wgt Acc: 66.297%
I - num batch: 62
I - Val -- Loss: 1.098 | Acc: 66.463% | Wgt Acc: 65.444% | Dur: 26.21s
I - Confusion Matrix: [row->prediction - col->label]
[[230.  25.  19.  78.]
 [  6. 134.  37.  13.]
 [  7.  62. 140.  19.]
 [ 21.  13.  29. 148.]]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.747 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.747 | Acc: 99.938% | Wgt Acc: 99.929%
	I - Batch: 150 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.747 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.747 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.747 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.747 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.747 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.747 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.747 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 260.69s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.078 | Acc: 66.375% | Wgt Acc: 66.185%
I - num batch: 62
I - Val -- Loss: 1.091 | Acc: 65.138% | Wgt Acc: 64.991% | Dur: 26.52s
I - Confusion Matrix: [row->prediction - col->label]
[[216.  23.  29.  68.]
 [  8. 146.  48.  15.]
 [  4.  47. 109.   7.]
 [ 36.  18.  39. 168.]]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.747 | Acc: 99.964% | Wgt Acc: 99.960%
	I - Batch: 400 | Loss: 0.747 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.747 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.747 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 258.22s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    2.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.085 | Acc: 66.500% | Wgt Acc: 66.047%
I - num batch: 62
I - Val -- Loss: 1.097 | Acc: 65.545% | Wgt Acc: 65.149% | Dur: 36.45s
I - Confusion Matrix: [row->prediction - col->label]
[[207.  20.  23.  57.]
 [  8. 131.  36.  13.]
 [  5.  66. 131.  14.]
 [ 44.  17.  35. 174.]]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.747 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 350 | Loss: 0.747 | Acc: 99.964% | Wgt Acc: 99.960%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 256.41s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.097 | Acc: 66.125% | Wgt Acc: 64.964%
I - num batch: 62
I - Val -- Loss: 1.107 | Acc: 65.138% | Wgt Acc: 63.995% | Dur: 27.26s
I - Confusion Matrix: [row->prediction - col->label]
[[234.  31.  32.  86.]
 [  5. 124.  34.  13.]
 [  5.  62. 135.  13.]
 [ 20.  17.  24. 146.]]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.964% | Wgt Acc: 99.960%
	I - Batch: 400 | Loss: 0.747 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 263.35s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.082 | Acc: 67.500% | Wgt Acc: 66.685%
I - num batch: 62
I - Val -- Loss: 1.092 | Acc: 66.157% | Wgt Acc: 65.421% | Dur: 26.79s
I - Confusion Matrix: [row->prediction - col->label]
[[230.  24.  29.  75.]
 [  7. 145.  44.  18.]
 [  4.  50. 126.  17.]
 [ 23.  15.  26. 148.]]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 271.90s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.078 | Acc: 65.875% | Wgt Acc: 65.297%
I - num batch: 62
I - Val -- Loss: 1.092 | Acc: 64.526% | Wgt Acc: 63.927% | Dur: 29.22s
I - Confusion Matrix: [row->prediction - col->label]
[[223.  25.  25.  76.]
 [  4. 135.  45.  15.]
 [  5.  55. 119.  11.]
 [ 32.  19.  36. 156.]]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 99.875% | Wgt Acc: 99.860%
	I - Batch: 100 | Loss: 0.746 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.964% | Wgt Acc: 99.960%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 262.56s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.093 | Acc: 65.000% | Wgt Acc: 64.076%
I - num batch: 62
I - Val -- Loss: 1.103 | Acc: 63.812% | Wgt Acc: 63.021% | Dur: 26.91s
I - Confusion Matrix: [row->prediction - col->label]
[[239.  32.  37.  88.]
 [  6. 130.  39.  13.]
 [  4.  48. 108.   8.]
 [ 15.  24.  41. 149.]]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.950% | Wgt Acc: 99.944%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.964% | Wgt Acc: 99.960%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.961% | Wgt Acc: 99.956% | LR: 1.250000e-04 | Dur: 266.78s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    2.]
 [   0.    0.    0. 1611.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.100 | Acc: 63.750% | Wgt Acc: 63.048%
I - num batch: 62
I - Val -- Loss: 1.111 | Acc: 62.181% | Wgt Acc: 61.594% | Dur: 26.93s
I - Confusion Matrix: [row->prediction - col->label]
[[218.  31.  32.  78.]
 [  5. 122.  39.  10.]
 [  6.  60. 112.  12.]
 [ 35.  21.  42. 158.]]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.748 | Acc: 99.875% | Wgt Acc: 99.873%
	I - Batch: 250 | Loss: 0.749 | Acc: 99.900% | Wgt Acc: 99.899%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.896% | Wgt Acc: 99.897%
	I - Batch: 350 | Loss: 0.750 | Acc: 99.893% | Wgt Acc: 99.891%
	I - Batch: 400 | Loss: 0.750 | Acc: 99.891% | Wgt Acc: 99.887%
	I - Batch: 450 | Loss: 0.750 | Acc: 99.889% | Wgt Acc: 99.884%
I - num batch: 478
I - Train -- Loss: 0.750 | Acc: 99.895% | Wgt Acc: 99.891% | LR: 1.250000e-04 | Dur: 267.46s
I - Confusion Matrix: [row->prediction - col->label]
[[2089.    0.    1.    1.]
 [   1. 1734.    0.    2.]
 [   1.    0. 2201.    2.]
 [   0.    0.    0. 1609.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.087 | Acc: 66.500% | Wgt Acc: 65.991%
I - num batch: 62
I - Val -- Loss: 1.098 | Acc: 65.036% | Wgt Acc: 64.606% | Dur: 26.78s
I - Confusion Matrix: [row->prediction - col->label]
[[208.  25.  20.  61.]
 [ 13. 136.  44.  10.]
 [  5.  60. 129.  22.]
 [ 38.  13.  32. 165.]]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.751 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.750 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.750 | Acc: 99.917% | Wgt Acc: 99.906%
	I - Batch: 200 | Loss: 0.749 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 250 | Loss: 0.749 | Acc: 99.925% | Wgt Acc: 99.915%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 350 | Loss: 0.749 | Acc: 99.946% | Wgt Acc: 99.940%
	I - Batch: 400 | Loss: 0.749 | Acc: 99.953% | Wgt Acc: 99.947%
	I - Batch: 450 | Loss: 0.749 | Acc: 99.958% | Wgt Acc: 99.953%
I - num batch: 478
I - Train -- Loss: 0.749 | Acc: 99.961% | Wgt Acc: 99.956% | LR: 1.250000e-04 | Dur: 268.80s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    1.    0.    0.]
 [   0. 1733.    0.    1.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.088 | Acc: 67.125% | Wgt Acc: 66.130%
I - num batch: 62
I - Val -- Loss: 1.095 | Acc: 65.953% | Wgt Acc: 65.014% | Dur: 28.33s
I - Confusion Matrix: [row->prediction - col->label]
[[210.  21.  18.  56.]
 [  5. 122.  32.  10.]
 [ 13.  77. 154.  31.]
 [ 36.  14.  21. 161.]]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.747 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.747 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.747 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.747 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.747 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.747 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.748 | Acc: 99.958% | Wgt Acc: 99.953%
I - num batch: 478
I - Train -- Loss: 0.748 | Acc: 99.961% | Wgt Acc: 99.956% | LR: 1.250000e-04 | Dur: 274.32s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    2.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1611.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.093 | Acc: 65.875% | Wgt Acc: 64.908%
I - num batch: 62
I - Val -- Loss: 1.096 | Acc: 65.545% | Wgt Acc: 64.674% | Dur: 27.10s
I - Confusion Matrix: [row->prediction - col->label]
[[224.  28.  28.  70.]
 [  7. 127.  27.  14.]
 [  7.  63. 135.  17.]
 [ 26.  16.  35. 157.]]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.748 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.748 | Acc: 99.938% | Wgt Acc: 99.929%
	I - Batch: 250 | Loss: 0.748 | Acc: 99.950% | Wgt Acc: 99.944%
	I - Batch: 300 | Loss: 0.748 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 350 | Loss: 0.748 | Acc: 99.946% | Wgt Acc: 99.940%
	I - Batch: 400 | Loss: 0.748 | Acc: 99.953% | Wgt Acc: 99.947%
	I - Batch: 450 | Loss: 0.748 | Acc: 99.958% | Wgt Acc: 99.953%
I - num batch: 478
I - Train -- Loss: 0.747 | Acc: 99.961% | Wgt Acc: 99.956% | LR: 1.250000e-04 | Dur: 254.71s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    2.]
 [   0. 1734.    0.    1.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1611.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.090 | Acc: 65.250% | Wgt Acc: 64.797%
I - num batch: 62
I - Val -- Loss: 1.102 | Acc: 64.628% | Wgt Acc: 64.176% | Dur: 26.40s
I - Confusion Matrix: [row->prediction - col->label]
[[234.  33.  36.  79.]
 [  3. 147.  54.  19.]
 [  5.  40. 102.   9.]
 [ 22.  14.  33. 151.]]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 99.875% | Wgt Acc: 99.860%
	I - Batch: 100 | Loss: 0.747 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.747 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.747 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 350 | Loss: 0.747 | Acc: 99.964% | Wgt Acc: 99.960%
	I - Batch: 400 | Loss: 0.747 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.748 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.748 | Acc: 99.948% | Wgt Acc: 99.944% | LR: 1.250000e-04 | Dur: 270.32s
I - Confusion Matrix: [row->prediction - col->label]
[[2090.    0.    0.    2.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   1.    0.    0. 1611.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.096 | Acc: 64.250% | Wgt Acc: 63.687%
I - num batch: 62
I - Val -- Loss: 1.097 | Acc: 64.628% | Wgt Acc: 64.063% | Dur: 27.51s
I - Confusion Matrix: [row->prediction - col->label]
[[225.  25.  30.  69.]
 [  6. 131.  40.  12.]
 [  5.  49. 116.  15.]
 [ 28.  29.  39. 162.]]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.758 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.761 | Acc: 99.875% | Wgt Acc: 99.874%
	I - Batch: 150 | Loss: 0.768 | Acc: 99.750% | Wgt Acc: 99.748%
	I - Batch: 200 | Loss: 0.772 | Acc: 99.531% | Wgt Acc: 99.530%
	I - Batch: 250 | Loss: 0.775 | Acc: 99.300% | Wgt Acc: 99.309%
	I - Batch: 300 | Loss: 0.779 | Acc: 99.167% | Wgt Acc: 99.189%
	I - Batch: 350 | Loss: 0.780 | Acc: 99.071% | Wgt Acc: 99.091%
	I - Batch: 400 | Loss: 0.779 | Acc: 99.047% | Wgt Acc: 99.064%
	I - Batch: 450 | Loss: 0.778 | Acc: 99.125% | Wgt Acc: 99.140%
I - num batch: 478
I - Train -- Loss: 0.777 | Acc: 99.176% | Wgt Acc: 99.189% | LR: 1.250000e-04 | Dur: 269.49s
I - Confusion Matrix: [row->prediction - col->label]
[[2060.    2.    4.    6.]
 [  12. 1725.    3.    2.]
 [   5.    5. 2193.    6.]
 [  14.    2.    2. 1600.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.108 | Acc: 65.875% | Wgt Acc: 64.353%
I - num batch: 62
I - Val -- Loss: 1.112 | Acc: 65.240% | Wgt Acc: 63.768% | Dur: 26.16s
I - Confusion Matrix: [row->prediction - col->label]
[[224.  26.  23.  66.]
 [  1.  98.  14.  10.]
 [  8.  92. 160.  24.]
 [ 31.  18.  28. 158.]]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.757 | Acc: 99.875% | Wgt Acc: 99.887%
	I - Batch: 100 | Loss: 0.756 | Acc: 99.938% | Wgt Acc: 99.944%
	I - Batch: 150 | Loss: 0.756 | Acc: 99.875% | Wgt Acc: 99.868%
	I - Batch: 200 | Loss: 0.755 | Acc: 99.906% | Wgt Acc: 99.901%
	I - Batch: 250 | Loss: 0.754 | Acc: 99.925% | Wgt Acc: 99.921%
	I - Batch: 300 | Loss: 0.754 | Acc: 99.917% | Wgt Acc: 99.911%
	I - Batch: 350 | Loss: 0.754 | Acc: 99.929% | Wgt Acc: 99.923%
	I - Batch: 400 | Loss: 0.755 | Acc: 99.906% | Wgt Acc: 99.901%
	I - Batch: 450 | Loss: 0.755 | Acc: 99.917% | Wgt Acc: 99.912%
I - num batch: 478
I - Train -- Loss: 0.755 | Acc: 99.921% | Wgt Acc: 99.917% | LR: 1.250000e-04 | Dur: 299.23s
I - Confusion Matrix: [row->prediction - col->label]
[[2090.    0.    0.    1.]
 [   1. 1732.    1.    0.]
 [   0.    1. 2201.    1.]
 [   0.    1.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.095 | Acc: 66.375% | Wgt Acc: 65.464%
I - num batch: 62
I - Val -- Loss: 1.105 | Acc: 65.036% | Wgt Acc: 64.312% | Dur: 39.50s
I - Confusion Matrix: [row->prediction - col->label]
[[219.  29.  26.  71.]
 [  9. 123.  27.  11.]
 [  6.  59. 131.  11.]
 [ 30.  23.  41. 165.]]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.750 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.749 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.750 | Acc: 99.917% | Wgt Acc: 99.916%
	I - Batch: 200 | Loss: 0.750 | Acc: 99.938% | Wgt Acc: 99.937%
	I - Batch: 250 | Loss: 0.749 | Acc: 99.950% | Wgt Acc: 99.949%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.938% | Wgt Acc: 99.934%
	I - Batch: 350 | Loss: 0.749 | Acc: 99.946% | Wgt Acc: 99.944%
	I - Batch: 400 | Loss: 0.749 | Acc: 99.938% | Wgt Acc: 99.933%
	I - Batch: 450 | Loss: 0.749 | Acc: 99.944% | Wgt Acc: 99.941%
I - num batch: 478
I - Train -- Loss: 0.749 | Acc: 99.948% | Wgt Acc: 99.944% | LR: 1.250000e-04 | Dur: 310.31s
I - Confusion Matrix: [row->prediction - col->label]
[[2090.    0.    0.    1.]
 [   1. 1734.    0.    1.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1611.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.072 | Acc: 67.125% | Wgt Acc: 66.907%
I - num batch: 62
I - Val -- Loss: 1.086 | Acc: 65.647% | Wgt Acc: 65.489% | Dur: 37.83s
I - Confusion Matrix: [row->prediction - col->label]
[[213.  20.  20.  67.]
 [  7. 154.  52.  17.]
 [  6.  40. 115.  12.]
 [ 38.  20.  38. 162.]]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.748 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 0.748 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.748 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.747 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.747 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.747 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.747 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.747 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.747 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.747 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 359.86s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.081 | Acc: 69.125% | Wgt Acc: 67.851%
I - num batch: 62
I - Val -- Loss: 1.091 | Acc: 67.482% | Wgt Acc: 66.350% | Dur: 37.49s
I - Confusion Matrix: [row->prediction - col->label]
[[234.  26.  18.  76.]
 [  6. 133.  34.  11.]
 [  5.  60. 146.  22.]
 [ 19.  15.  27. 149.]]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 355.46s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.078 | Acc: 67.250% | Wgt Acc: 66.713%
I - num batch: 62
I - Val -- Loss: 1.092 | Acc: 65.647% | Wgt Acc: 65.127% | Dur: 37.73s
I - Confusion Matrix: [row->prediction - col->label]
[[232.  25.  27.  77.]
 [  6. 144.  46.  12.]
 [  4.  46. 112.  13.]
 [ 22.  19.  40. 156.]]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 255.57s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    2.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.085 | Acc: 66.875% | Wgt Acc: 66.102%
I - num batch: 62
I - Val -- Loss: 1.097 | Acc: 65.341% | Wgt Acc: 64.697% | Dur: 26.45s
I - Confusion Matrix: [row->prediction - col->label]
[[217.  29.  28.  56.]
 [  2. 111.  22.   5.]
 [  3.  64. 131.  15.]
 [ 42.  30.  44. 182.]]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 99.938% | Wgt Acc: 99.929%
	I - Batch: 150 | Loss: 0.747 | Acc: 99.917% | Wgt Acc: 99.906%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.938% | Wgt Acc: 99.929%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.950% | Wgt Acc: 99.944%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.964% | Wgt Acc: 99.960%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 274.17s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    1.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.077 | Acc: 66.125% | Wgt Acc: 65.852%
I - num batch: 62
I - Val -- Loss: 1.086 | Acc: 65.240% | Wgt Acc: 65.036% | Dur: 31.02s
I - Confusion Matrix: [row->prediction - col->label]
[[206.  24.  23.  52.]
 [  6. 129.  39.  11.]
 [  4.  58. 122.  12.]
 [ 48.  23.  41. 183.]]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.747 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.747 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 270.51s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.082 | Acc: 66.750% | Wgt Acc: 65.963%
I - num batch: 62
I - Val -- Loss: 1.090 | Acc: 65.647% | Wgt Acc: 65.014% | Dur: 28.20s
I - Confusion Matrix: [row->prediction - col->label]
[[213.  29.  20.  64.]
 [  7. 128.  39.  14.]
 [  5.  58. 136.  13.]
 [ 39.  19.  30. 167.]]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 265.92s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.095 | Acc: 65.625% | Wgt Acc: 64.353%
I - num batch: 62
I - Val -- Loss: 1.102 | Acc: 64.934% | Wgt Acc: 63.700% | Dur: 27.38s
I - Confusion Matrix: [row->prediction - col->label]
[[240.  33.  35.  88.]
 [  1. 120.  29.  10.]
 [  5.  64. 132.  15.]
 [ 18.  17.  29. 145.]]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.972% | Wgt Acc: 99.969%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.974% | Wgt Acc: 99.971% | LR: 1.250000e-04 | Dur: 266.59s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    2.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1612.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.101 | Acc: 66.625% | Wgt Acc: 64.992%
I - num batch: 62
I - Val -- Loss: 1.115 | Acc: 64.628% | Wgt Acc: 63.089% | Dur: 27.87s
I - Confusion Matrix: [row->prediction - col->label]
[[240.  25.  25.  96.]
 [  5. 128.  35.  20.]
 [  8.  64. 144.  20.]
 [ 11.  17.  21. 122.]]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 279.57s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.082 | Acc: 68.250% | Wgt Acc: 67.407%
I - num batch: 62
I - Val -- Loss: 1.092 | Acc: 67.074% | Wgt Acc: 66.304% | Dur: 27.18s
I - Confusion Matrix: [row->prediction - col->label]
[[215.  25.  23.  69.]
 [  5. 133.  25.  10.]
 [  5.  59. 147.  16.]
 [ 39.  17.  30. 163.]]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 259.36s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.065 | Acc: 69.125% | Wgt Acc: 68.601%
I - num batch: 62
I - Val -- Loss: 1.075 | Acc: 67.788% | Wgt Acc: 67.323% | Dur: 27.12s
I - Confusion Matrix: [row->prediction - col->label]
[[217.  21.  19.  48.]
 [  7. 140.  43.  15.]
 [  6.  56. 135.  22.]
 [ 34.  17.  28. 173.]]

I - Local maximum validation set accuracy:  67.79

I - Validation set results: 
[14-1-2-0.59][14-1-2-0.73][14-1-2-0.46][50-3-1-0.80][50-3-1-0.95][50-3-1-0.54][124-2-2-0.63][124-2-2-0.90][124-2-2-0.71][127-0-0-0.99]
[127-0-0-1.00][127-0-0-0.99][443-2-2-1.00][443-2-2-0.99][443-2-2-1.00][567-0-0-0.96][567-0-0-0.97][567-0-0-0.98][573-1-1-0.48][573-1-1-0.55]
[573-1-1-0.77][615-0-0-0.88][615-0-0-0.69][615-0-0-0.73][695-1-2-0.97][695-1-2-0.97][695-1-2-0.97][722-3-3-0.78][722-3-3-0.55][722-3-3-0.85]
[826-0-0-0.96][826-0-0-0.98][826-0-0-0.97][878-0-3-0.50][878-0-0-0.88][878-0-0-0.84][1103-0-0-0.97][1103-0-0-0.70][1103-0-0-0.64][1212-3-3-0.56]
[1212-3-3-0.75][1212-3-0-0.48][1368-0-0-1.00][1368-0-0-1.00][1368-0-0-1.00][2181-2-3-0.90][2181-2-3-0.91][2181-2-3-0.93][2476-2-1-0.50][2476-2-2-0.52]
[2476-2-2-0.59][2721-2-2-0.88][2721-2-2-0.88][2721-2-2-0.98][2818-1-2-0.51][2818-1-3-0.54][2818-1-3-0.62][2886-2-1-0.56][2886-2-1-0.47][2886-2-1-0.47]
[3231-2-2-1.00][3231-2-2-1.00][3231-2-2-1.00][3333-2-2-0.63][3333-2-2-0.54][3333-2-2-0.57][3482-2-2-0.98][3482-2-2-0.99][3482-2-2-0.99][3536-3-3-0.67]
[3536-3-3-0.64][3536-3-3-0.48][3625-1-1-1.00][3625-1-1-1.00][3625-1-1-1.00][3909-0-0-0.99][3909-0-0-0.90][3909-0-0-0.93][4035-0-0-1.00][4035-0-0-1.00]
[4035-0-0-1.00][4140-0-0-1.00][4140-0-0-0.99][4140-0-0-0.90][4214-1-3-0.80][4214-1-1-0.32][4214-1-1-0.98][4346-1-0-0.62][4346-1-3-0.44][4346-1-3-0.49]
[4581-2-1-0.79][4581-2-1-0.73][4581-2-2-0.58][4708-3-2-0.89][4708-3-2-0.53][4708-3-2-0.70][4838-3-3-0.88][4838-3-3-0.77][4838-3-3-0.71][4845-1-1-0.36]
[4845-1-3-0.43][4845-1-2-0.50][4868-0-0-1.00][4868-0-0-1.00][4868-0-0-0.97][4939-0-3-0.51][4939-0-1-0.57][4939-0-1-0.49][4984-2-3-0.62][4984-2-2-0.75]
[4984-2-2-0.80][5078-1-2-0.35][5078-1-2-0.65][5078-1-3-0.39][5396-0-0-1.00][5396-0-0-1.00][5396-0-0-0.97][5479-1-1-0.72][5479-1-1-0.93][5479-1-1-0.79]
[5717-0-0-0.88][5717-0-0-0.99][5717-0-0-0.54][5843-1-1-0.96][5843-1-1-0.95][5843-1-1-1.00][5949-3-3-0.76][5949-3-3-0.75][5949-3-3-0.99][5987-2-2-0.46]
[5987-2-1-0.50][5987-2-2-0.56][6014-3-3-0.60][6014-3-3-0.72][6014-3-3-0.51][6033-3-0-0.56][6033-3-3-0.62][6033-3-3-0.42][6313-0-0-0.77][6313-0-0-0.69]
[6313-0-0-0.70][6421-3-3-0.97][6421-3-3-0.92][6421-3-3-0.99][6500-1-1-0.84][6500-1-1-0.93][6500-1-1-0.81][6583-3-3-0.89][6583-3-3-0.87][6583-3-3-0.81]
[6683-3-3-0.67][6683-3-3-0.72][6683-3-3-0.61][6825-2-1-0.85][6825-2-1-0.89][6825-2-1-0.85][6998-3-3-0.38][6998-3-3-0.59][6998-3-3-0.60][7049-3-3-0.55]
[7049-3-3-0.48][7049-3-3-0.70][7517-1-1-0.98][7517-1-1-0.96][7517-1-1-1.00][7521-1-0-0.59][7521-1-0-0.60][7521-1-0-0.69][7528-1-3-0.61][7528-1-2-0.65]
[7528-1-2-0.63][7949-1-2-0.98][7949-1-2-0.94][7949-1-2-0.74][8135-1-0-0.52][8135-1-0-0.50][8135-1-0-0.56][8185-3-0-1.00][8185-3-0-1.00][8185-3-0-1.00]
[8269-3-2-0.52][8269-3-1-0.55][8269-3-1-0.99][8273-3-3-1.00][8273-3-3-0.97][8273-3-3-0.90][8543-3-3-0.68][8543-3-0-0.94][8543-3-0-0.67][8666-1-1-0.92]
[8666-1-1-1.00][8666-1-1-0.98][8672-0-0-1.00][8672-0-0-0.94][8672-0-3-0.50][8903-1-1-0.74][8903-1-2-0.68][8903-1-2-0.85][9001-2-0-0.87][9001-2-1-1.00]
[9001-2-1-1.00][9036-2-2-1.00][9036-2-2-0.94][9036-2-2-0.58][9281-3-0-0.87][9281-3-0-0.75][9281-3-0-0.51][9300-2-2-1.00][9300-2-2-1.00][9300-2-2-1.00]
[9571-0-3-0.63][9571-0-3-0.66][9571-0-3-0.56][9617-1-1-1.00][9617-1-1-0.64][9617-1-1-0.43][9644-2-1-0.57][9644-2-2-0.72][9644-2-2-0.94][9705-2-1-0.38]
[9705-2-0-0.41][9705-2-1-0.39][9801-0-0-0.82][9801-0-0-0.56][9801-0-0-0.51][9803-3-0-0.58][9803-3-3-0.68][9803-3-3-0.68][9865-3-3-0.99][9865-3-3-1.00]
[9865-3-3-1.00][9896-2-2-0.89][9896-2-2-0.59][9896-2-2-0.81][10314-1-2-0.70][10314-1-2-0.61][10314-1-2-0.77][10337-3-3-0.99][10337-3-3-1.00][10337-3-3-0.99]
[10403-0-0-0.67][10403-0-2-0.50][10403-0-0-0.77][10653-2-2-0.55][10653-2-2-0.53][10653-2-2-0.51][10704-2-1-0.72][10704-2-1-0.82][10704-2-1-0.89][10719-1-1-0.79]
[10719-1-1-0.84][10719-1-1-0.91][10727-1-1-0.71][10727-1-1-0.96][10727-1-1-0.96][10836-0-0-1.00][10836-0-0-1.00][10836-0-0-1.00][10969-2-3-0.62][10969-2-3-0.83]
[10969-2-3-0.81][11042-0-0-0.91][11042-0-0-0.67][11042-0-0-0.98][11088-1-1-0.99][11088-1-1-1.00][11088-1-1-1.00][11322-0-0-1.00][11322-0-0-1.00][11322-0-0-0.99]
[11398-2-2-1.00][11398-2-2-0.53][11398-2-0-0.48][11499-0-0-0.98][11499-0-0-0.97][11499-0-0-0.92][11502-3-0-0.76][11502-3-0-0.45][11502-3-0-0.89][11512-3-3-0.88]
[11512-3-3-0.74][11512-3-3-0.47][11608-1-2-0.84][11608-1-2-0.87][11608-1-2-0.69][11610-0-0-0.78][11610-0-0-0.91][11610-0-0-0.93][11692-0-3-0.73][11692-0-3-0.66]
[11692-0-3-0.47][11905-0-0-0.95][11905-0-0-0.86][11905-0-0-0.55][11993-1-1-0.99][11993-1-1-0.96][11993-1-1-0.94][12002-2-0-0.79][12002-2-2-0.65][12002-2-3-0.74]
[12052-0-0-0.78][12052-0-0-0.86][12052-0-0-0.81][12201-0-3-0.71][12201-0-3-0.71][12201-0-3-0.61][12235-2-2-0.99][12235-2-2-0.75][12235-2-1-0.63][12320-1-0-0.99]
[12320-1-0-0.99][12320-1-0-0.99][12377-2-2-0.42][12377-2-1-0.57][12377-2-1-0.60][12398-2-3-0.99][12398-2-3-0.93][12398-2-3-0.89][12503-1-1-0.94][12503-1-1-0.47]
[12503-1-2-0.39][12617-0-0-0.50][12617-0-0-0.60][12617-0-0-0.51][12685-3-3-0.60][12685-3-3-0.86][12685-3-3-0.93][12738-2-2-0.65][12738-2-0-0.50][12738-2-0-0.66]
[12742-2-2-1.00][12742-2-2-1.00][12742-2-2-1.00][12823-0-0-1.00][12823-0-0-0.89][12823-0-0-0.78][13110-1-1-0.93][13110-1-1-0.82][13110-1-1-0.76][13240-3-0-0.76]
[13240-3-0-0.70][13240-3-0-0.78][13253-1-1-0.98][13253-1-1-0.96][13253-1-1-0.98][13273-0-0-1.00][13273-0-0-1.00][13273-0-0-1.00][13634-1-2-0.40][13634-1-3-0.42]
[13634-1-2-0.48][13763-2-3-0.52][13763-2-2-0.72][13763-2-2-0.51][13905-3-0-0.99][13905-3-0-0.97][13905-3-0-0.93][14060-2-1-0.80][14060-2-1-0.90][14060-2-1-0.77]
[14065-3-3-0.70][14065-3-0-0.85][14065-3-0-0.49][14147-3-3-0.63][14147-3-3-0.99][14147-3-3-0.71][14595-2-2-0.95][14595-2-2-0.95][14595-2-2-0.99][14687-2-2-0.99]
[14687-2-2-1.00][14687-2-2-1.00][14788-2-2-0.64][14788-2-3-0.89][14788-2-3-0.97][14869-1-1-0.89][14869-1-1-0.89][14869-1-1-0.73][14872-3-2-0.56][14872-3-0-0.53]
[14872-3-0-0.61][14877-1-1-1.00][14877-1-1-1.00][14877-1-1-1.00][14927-0-0-0.63][14927-0-0-0.56][14927-0-0-0.48][15066-0-3-0.51][15066-0-3-0.56][15066-0-0-0.63]
[15175-1-1-0.67][15175-1-1-0.66][15175-1-1-0.86][15178-2-2-0.79][15178-2-2-0.97][15178-2-2-0.97][15375-3-3-0.59][15375-3-3-0.73][15375-3-1-0.48][15389-3-3-0.84]
[15389-3-3-0.92][15389-3-3-0.95][15568-2-1-0.89][15568-2-1-0.99][15568-2-1-0.94][15675-3-3-0.60][15675-3-3-1.00][15675-3-3-0.99][15869-1-0-0.59][15869-1-0-0.43]
[15869-1-0-0.40][16207-3-0-0.96][16207-3-0-0.98][16207-3-0-0.96][16236-0-0-1.00][16236-0-0-1.00][16236-0-0-0.77][16302-3-3-0.92][16302-3-3-0.60][16302-3-2-0.77]
[16331-2-2-1.00][16331-2-2-1.00][16331-2-2-1.00][16381-0-3-0.62][16381-0-3-0.63][16381-0-3-0.53][16488-1-1-0.99][16488-1-1-1.00][16488-1-1-0.99][16495-0-0-0.96]
[16495-0-0-1.00][16495-0-0-0.98][16650-0-0-1.00][16650-0-0-1.00][16650-0-0-1.00][16719-1-1-0.76][16719-1-1-0.60][16719-1-2-0.70][16801-0-0-1.00][16801-0-0-1.00]
[16801-0-0-1.00][16828-0-0-0.83][16828-0-0-0.97][16828-0-0-0.97][17137-3-3-0.79][17137-3-3-0.91][17137-3-3-0.61][17245-1-2-0.52][17245-1-2-0.79][17245-1-2-0.38]
[17278-3-0-0.51][17278-3-1-0.54][17278-3-3-0.44][17282-0-0-0.85][17282-0-2-0.52][17282-0-0-0.55][17311-2-2-0.89][17311-2-2-0.90][17311-2-2-0.90][17336-2-2-0.54]
[17336-2-2-0.54][17336-2-2-0.77][17608-3-3-1.00][17608-3-3-1.00][17608-3-3-1.00][17627-0-0-0.99][17627-0-0-0.63][17627-0-0-0.86][17877-3-3-0.44][17877-3-0-0.48]
[17877-3-0-0.66][17924-1-3-0.58][17924-1-3-0.82][17924-1-3-0.57][17984-3-3-1.00][17984-3-3-0.98][17984-3-3-0.99][18211-0-3-0.72][18211-0-3-0.94][18211-0-3-0.80]
[18276-3-3-0.97][18276-3-3-0.92][18276-3-3-0.95][18287-1-1-0.53][18287-1-1-0.87][18287-1-1-0.95][18394-0-0-0.99][18394-0-0-1.00][18394-0-0-1.00][18428-0-0-0.91]
[18428-0-0-0.88][18428-0-0-1.00][18442-0-3-0.57][18442-0-3-0.71][18442-0-3-0.91][18478-3-3-0.65][18478-3-3-0.72][18478-3-3-0.74][18607-0-0-1.00][18607-0-0-1.00]
[18607-0-0-1.00][18616-0-0-0.98][18616-0-0-0.99][18616-0-0-0.99][18663-0-0-0.63][18663-0-0-0.93][18663-0-0-0.63][18718-0-0-1.00][18718-0-0-1.00][18718-0-0-1.00]
[18766-2-2-0.96][18766-2-2-0.91][18766-2-2-0.89][18824-2-2-0.96][18824-2-2-0.95][18824-2-2-0.96][18890-3-1-0.74][18890-3-3-0.71][18890-3-3-0.85][18930-3-1-0.67]
[18930-3-0-0.37][18930-3-2-0.41][18938-3-3-0.99][18938-3-3-0.99][18938-3-3-0.99][19817-1-1-0.51][19817-1-1-0.76][19817-1-1-0.64][19839-0-0-0.84][19839-0-0-0.85]
[19839-0-0-0.84][19930-3-3-0.83][19930-3-3-0.85][19930-3-3-0.84][19944-0-0-0.62][19944-0-1-0.60][19944-0-1-0.84][20036-2-2-0.97][20036-2-2-0.96][20036-2-2-0.97]
[20101-3-1-0.51][20101-3-3-0.84][20101-3-3-0.79][20474-1-1-1.00][20474-1-1-0.99][20474-1-1-1.00][20547-3-1-0.39][20547-3-3-0.70][20547-3-3-0.82][20929-2-2-0.51]
[20929-2-2-1.00][20929-2-2-1.00][21245-1-2-0.61][21245-1-1-0.71][21245-1-1-0.99][21257-3-3-0.99][21257-3-3-0.92][21257-3-3-0.42][21293-1-2-0.73][21293-1-2-0.70]
[21293-1-2-0.74][21316-1-1-0.99][21316-1-1-0.99][21316-1-3-0.49][21384-1-1-0.89][21384-1-1-0.90][21384-1-1-0.87][21448-1-1-0.84][21448-1-1-0.82][21448-1-1-0.86]
[21483-0-0-0.98][21483-0-0-0.97][21483-0-0-0.99][21487-2-2-0.95][21487-2-2-0.78][21487-2-2-0.76][21714-0-0-0.67][21714-0-0-0.64][21714-0-0-0.74][21943-3-2-0.90]
[21943-3-2-0.70][21943-3-2-0.60][21947-0-0-1.00][21947-0-0-0.94][21947-0-0-0.69][21948-0-0-1.00][21948-0-0-1.00][21948-0-0-1.00][21965-2-2-0.98][21965-2-2-0.95]
[21965-2-2-0.65][21998-1-1-0.85][21998-1-1-0.99][21998-1-1-0.98][22025-0-2-0.94][22025-0-2-0.91][22025-0-2-0.87][22228-3-3-1.00][22228-3-3-1.00][22228-3-3-1.00]
[22446-1-1-1.00][22446-1-1-1.00][22446-1-1-0.90][22494-3-3-0.62][22494-3-3-0.86][22494-3-3-0.92][22757-0-0-0.73][22757-0-0-0.63][22757-0-0-0.70][22811-3-3-0.57]
[22811-3-3-1.00][22811-3-3-0.99][22976-3-2-0.47][22976-3-2-0.64][22976-3-2-0.53][22985-3-3-0.98][22985-3-3-0.99][22985-3-3-0.97][23014-0-3-0.66][23014-0-0-0.76]
[23014-0-0-0.69][23112-1-1-1.00][23112-1-1-0.99][23112-1-1-1.00][23144-3-3-1.00][23144-3-3-0.98][23144-3-3-0.96][23168-2-0-0.63][23168-2-1-0.47][23168-2-3-0.47]
[23219-0-0-0.78][23219-0-0-0.78][23219-0-0-0.84][23363-3-3-0.53][23363-3-3-0.83][23363-3-3-0.95][23470-0-0-0.92][23470-0-0-0.91][23470-0-0-0.71][23486-2-1-0.48]
[23486-2-3-0.67][23486-2-2-0.39][23497-0-3-0.66][23497-0-3-0.99][23497-0-3-0.95][23516-0-0-0.90][23516-0-0-0.98][23516-0-0-1.00][23690-1-1-1.00][23690-1-1-0.90]
[23690-1-1-0.97][23921-2-1-0.73][23921-2-2-0.63][23921-2-2-0.64][23936-1-2-0.84][23936-1-2-0.96][23936-1-2-0.76][24040-3-2-0.76][24040-3-2-0.72][24040-3-2-0.71]
[24111-1-1-0.58][24111-1-1-0.61][24111-1-1-0.42][24182-0-0-1.00][24182-0-0-1.00][24182-0-0-1.00][24238-3-3-0.91][24238-3-3-0.94][24238-3-3-0.98][24290-2-0-0.99]
[24290-2-0-0.95][24290-2-0-0.95][24345-0-0-0.77][24345-0-0-0.94][24345-0-0-0.58][24364-1-2-0.51][24364-1-2-0.90][24364-1-2-0.81][24427-3-3-0.50][24427-3-0-0.82]
[24427-3-0-1.00][24477-2-2-0.92][24477-2-2-0.87][24477-2-2-0.67][24495-2-1-0.71][24495-2-1-0.67][24495-2-1-0.65][24893-2-2-0.52][24893-2-2-0.78][24893-2-2-0.53]
[25012-1-2-0.52][25012-1-1-0.64][25012-1-1-0.89][25121-2-2-0.67][25121-2-2-0.74][25121-2-2-0.65][25165-3-3-0.89][25165-3-3-0.88][25165-3-3-0.86][25183-0-0-0.98]
[25183-0-0-0.90][25183-0-0-1.00][25297-3-3-1.00][25297-3-3-1.00][25297-3-3-1.00][25398-0-0-0.76][25398-0-0-0.99][25398-0-0-0.97][25574-2-2-0.83][25574-2-2-0.85]
[25574-2-2-0.88][25644-1-2-0.88][25644-1-2-0.55][25644-1-2-0.41][25718-1-1-0.45][25718-1-0-0.61][25718-1-0-0.58][25774-2-2-0.86][25774-2-0-0.50][25774-2-0-0.45]
[26032-3-3-0.98][26032-3-3-0.99][26032-3-3-0.99][26051-3-3-0.98][26051-3-3-1.00][26051-3-3-1.00][26120-0-0-0.92][26120-0-0-0.95][26120-0-0-1.00][26321-1-1-1.00]
[26321-1-1-1.00][26321-1-2-0.52][26732-1-1-0.94][26732-1-1-0.86][26732-1-1-0.99][26784-3-3-1.00][26784-3-3-1.00][26784-3-3-1.00][26827-3-3-0.90][26827-3-3-0.87]
[26827-3-3-0.96][26833-0-3-0.89][26833-0-3-0.71][26833-0-0-0.65][26838-2-2-0.48][26838-2-3-0.39][26838-2-3-0.52][26860-1-2-0.46][26860-1-2-0.61][26860-1-1-0.72]
[26948-0-0-0.55][26948-0-0-0.68][26948-0-0-0.83][27049-3-0-0.98][27049-3-0-0.87][27049-3-0-0.97][27098-1-1-0.51][27098-1-0-0.65][27098-1-0-0.99][27526-0-0-0.87]
[27526-0-0-0.79][27526-0-0-0.59][27639-3-3-0.98][27639-3-3-0.99][27639-3-3-0.94][27698-3-3-0.78][27698-3-3-0.96][27698-3-3-0.95][27772-0-0-0.97][27772-0-0-0.98]
[27772-0-0-0.85][27890-1-1-0.87][27890-1-1-0.84][27890-1-1-0.86][28040-0-0-0.66][28040-0-0-0.79][28040-0-0-0.96][28503-2-2-0.99][28503-2-2-1.00][28503-2-2-1.00]
[28577-1-1-0.83][28577-1-1-0.83][28577-1-1-0.91][28959-0-0-0.99][28959-0-0-0.99][28959-0-0-0.98][29198-3-1-0.44][29198-3-1-0.53][29198-3-1-0.59][29777-0-0-0.99]
[29777-0-0-1.00][29777-0-0-1.00][29877-2-3-0.41][29877-2-3-0.69][29877-2-1-0.47][30035-1-1-1.00][30035-1-1-0.84][30035-1-1-0.92][30098-0-0-0.68][30098-0-0-0.71]
[30098-0-0-0.59][30326-1-1-1.00][30326-1-1-1.00][30326-1-1-1.00][30572-2-2-0.79][30572-2-2-0.77][30572-2-2-0.99][30716-0-1-0.43][30716-0-1-0.40][30716-0-0-0.30]
[30806-2-2-0.52][30806-2-0-0.71][30806-2-2-0.75][30906-1-1-1.00][30906-1-1-0.91][30906-1-1-0.84][31007-0-0-0.99][31007-0-0-1.00][31007-0-2-0.66][31181-3-3-0.84]
[31181-3-3-0.40][31181-3-0-0.54][31238-0-0-0.53][31238-0-3-0.72][31238-0-3-0.76][31347-0-0-0.89][31347-0-0-0.88][31347-0-0-0.87][31422-2-2-0.60][31422-2-2-0.80]
[31422-2-0-0.33][31429-3-3-0.66][31429-3-3-0.59][31429-3-0-0.51][31431-0-0-1.00][31431-0-0-0.96][31431-0-0-0.59][31432-1-1-0.96][31432-1-1-0.93][31432-1-1-0.99]
[31477-0-3-0.77][31477-0-3-0.70][31477-0-3-0.76][31524-1-1-0.63][31524-1-2-0.58][31524-1-1-0.45][31597-1-2-0.72][31597-1-2-0.50][31597-1-3-0.50][31619-1-0-0.75]
[31619-1-0-0.87][31619-1-0-0.91][31701-0-0-1.00][31701-0-0-0.96][31701-0-0-0.99][31755-0-0-1.00][31755-0-0-1.00][31755-0-0-1.00][31854-3-3-0.99][31854-3-3-0.77]
[31854-3-3-0.72][32074-1-1-0.61][32074-1-1-0.73][32074-1-3-0.84][32078-3-3-1.00][32078-3-3-0.99][32078-3-3-0.88][32111-1-1-0.96][32111-1-1-0.82][32111-1-1-0.87]
[32127-1-1-0.91][32127-1-1-0.88][32127-1-1-0.87][32140-3-3-1.00][32140-3-3-1.00][32140-3-3-1.00][32263-2-0-0.73][32263-2-2-0.54][32263-2-0-0.42][32365-0-0-0.99]
[32365-0-0-1.00][32365-0-0-1.00][32411-2-3-0.65][32411-2-0-0.63][32411-2-0-0.75][32429-3-3-0.61][32429-3-3-0.61][32429-3-3-0.78][32473-3-0-0.62][32473-3-0-0.71]
[32473-3-0-0.79][32574-3-3-1.00][32574-3-3-0.99][32574-3-3-0.99][32584-0-0-0.97][32584-0-0-0.99][32584-0-0-0.97][32622-0-0-0.42][32622-0-0-0.64][32622-0-0-0.61]
[32858-3-3-0.76][32858-3-3-0.68][32858-3-3-0.69][32969-3-0-0.59][32969-3-0-0.56][32969-3-0-0.60][33016-2-2-0.98][33016-2-2-1.00][33016-2-2-0.88][33031-1-3-0.84]
[33031-1-3-0.84][33031-1-0-0.52][33035-2-2-0.97][33035-2-2-0.95][33035-2-2-0.98][33133-2-2-0.99][33133-2-2-0.97][33133-2-2-0.66][33173-2-3-0.54][33173-2-1-0.48]
[33173-2-2-0.55][33175-3-2-1.00][33175-3-2-1.00][33175-3-2-1.00][33306-3-1-0.56][33306-3-3-0.53][33306-3-3-0.65][33309-2-3-0.49][33309-2-3-0.72][33309-2-2-0.49]
[33474-0-0-0.60][33474-0-1-0.70][33474-0-0-0.72][33478-2-0-0.48][33478-2-1-0.39][33478-2-2-0.39][33618-1-1-0.89][33618-1-1-0.92][33618-1-1-0.90][33712-0-0-0.90]
[33712-0-0-0.99][33712-0-0-0.97][33782-2-1-0.88][33782-2-1-0.90][33782-2-1-0.93][33914-3-2-0.46][33914-3-3-0.99][33914-3-3-0.98][34076-3-2-0.43][34076-3-2-0.47]
[34076-3-0-0.41][34112-2-2-0.64][34112-2-3-0.53][34112-2-1-0.87][34138-2-3-0.97][34138-2-3-0.97][34138-2-3-0.97][34239-1-1-0.38][34239-1-2-0.58][34239-1-2-0.43]
[34364-2-1-0.57][34364-2-2-0.93][34364-2-2-0.99][34617-1-2-0.88][34617-1-2-0.67][34617-1-2-0.84][34751-3-0-0.53][34751-3-0-0.76][34751-3-0-0.67][34783-2-1-0.56]
[34783-2-1-0.47][34783-2-2-0.48][35015-3-3-0.60][35015-3-3-0.64][35015-3-3-0.76][35018-1-1-0.86][35018-1-1-0.75][35018-1-2-0.60][35288-2-2-0.83][35288-2-2-0.68]
[35288-2-2-0.67]
---------------------------
I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 262.50s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.086 | Acc: 66.375% | Wgt Acc: 65.103%
I - num batch: 62
I - Val -- Loss: 1.096 | Acc: 65.138% | Wgt Acc: 63.927% | Dur: 27.16s
I - Confusion Matrix: [row->prediction - col->label]
[[238.  28.  27.  91.]
 [  4. 137.  43.  18.]
 [  5.  57. 134.  19.]
 [ 17.  12.  21. 130.]]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 257.92s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.078 | Acc: 65.875% | Wgt Acc: 65.575%
I - num batch: 62
I - Val -- Loss: 1.088 | Acc: 64.322% | Wgt Acc: 64.108% | Dur: 26.38s
I - Confusion Matrix: [row->prediction - col->label]
[[212.  29.  30.  62.]
 [  7. 136.  42.  13.]
 [  4.  44. 112.  12.]
 [ 41.  25.  41. 171.]]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 150 | Loss: 0.746 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.747 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 255.30s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.105 | Acc: 62.875% | Wgt Acc: 63.493%
I - num batch: 62
I - Val -- Loss: 1.109 | Acc: 62.283% | Wgt Acc: 63.066% | Dur: 26.49s
I - Confusion Matrix: [row->prediction - col->label]
[[148.  13.  10.  19.]
 [  1. 125.  37.   8.]
 [ 16.  64. 122.  15.]
 [ 99.  32.  56. 216.]]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.782 | Acc: 99.250% | Wgt Acc: 99.211%
	I - Batch: 100 | Loss: 0.796 | Acc: 98.312% | Wgt Acc: 98.296%
	I - Batch: 150 | Loss: 0.802 | Acc: 97.667% | Wgt Acc: 97.652%
	I - Batch: 200 | Loss: 0.801 | Acc: 97.625% | Wgt Acc: 97.612%
	I - Batch: 250 | Loss: 0.797 | Acc: 97.925% | Wgt Acc: 97.904%
	I - Batch: 300 | Loss: 0.793 | Acc: 98.167% | Wgt Acc: 98.144%
	I - Batch: 350 | Loss: 0.790 | Acc: 98.375% | Wgt Acc: 98.358%
	I - Batch: 400 | Loss: 0.787 | Acc: 98.562% | Wgt Acc: 98.548%
	I - Batch: 450 | Loss: 0.785 | Acc: 98.639% | Wgt Acc: 98.626%
I - num batch: 478
I - Train -- Loss: 0.783 | Acc: 98.717% | Wgt Acc: 98.705% | LR: 1.250000e-04 | Dur: 254.41s
I - Confusion Matrix: [row->prediction - col->label]
[[2063.    4.    6.   15.]
 [   9. 1716.   13.    5.]
 [   7.    7. 2179.    9.]
 [  12.    7.    4. 1585.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.093 | Acc: 66.250% | Wgt Acc: 65.380%
I - num batch: 62
I - Val -- Loss: 1.103 | Acc: 65.240% | Wgt Acc: 64.493% | Dur: 26.54s
I - Confusion Matrix: [row->prediction - col->label]
[[232.  25.  27.  82.]
 [  8. 147.  49.  18.]
 [  7.  53. 120.  17.]
 [ 17.   9.  29. 141.]]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.757 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.756 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.755 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.755 | Acc: 99.938% | Wgt Acc: 99.937%
	I - Batch: 250 | Loss: 0.754 | Acc: 99.950% | Wgt Acc: 99.949%
	I - Batch: 300 | Loss: 0.754 | Acc: 99.958% | Wgt Acc: 99.958%
	I - Batch: 350 | Loss: 0.753 | Acc: 99.946% | Wgt Acc: 99.944%
	I - Batch: 400 | Loss: 0.753 | Acc: 99.938% | Wgt Acc: 99.933%
	I - Batch: 450 | Loss: 0.752 | Acc: 99.944% | Wgt Acc: 99.941%
I - num batch: 478
I - Train -- Loss: 0.752 | Acc: 99.948% | Wgt Acc: 99.944% | LR: 1.250000e-04 | Dur: 255.85s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    2.]
 [   0. 1734.    0.    1.]
 [   0.    0. 2201.    0.]
 [   0.    0.    1. 1611.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.106 | Acc: 63.875% | Wgt Acc: 62.993%
I - num batch: 62
I - Val -- Loss: 1.110 | Acc: 63.405% | Wgt Acc: 62.545% | Dur: 26.81s
I - Confusion Matrix: [row->prediction - col->label]
[[230.  32.  34.  85.]
 [  2. 131.  38.  13.]
 [  4.  59. 118.  17.]
 [ 28.  12.  35. 143.]]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.750 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.750 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.749 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.749 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.749 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.749 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.749 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.749 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.749 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.749 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 255.90s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.108 | Acc: 65.125% | Wgt Acc: 63.687%
I - num batch: 62
I - Val -- Loss: 1.117 | Acc: 63.812% | Wgt Acc: 62.341% | Dur: 26.74s
I - Confusion Matrix: [row->prediction - col->label]
[[233.  20.  18.  86.]
 [  8. 134.  39.  21.]
 [  8.  67. 144.  36.]
 [ 15.  13.  24. 115.]]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.750 | Acc: 99.875% | Wgt Acc: 99.858%
	I - Batch: 100 | Loss: 0.748 | Acc: 99.938% | Wgt Acc: 99.929%
	I - Batch: 150 | Loss: 0.748 | Acc: 99.958% | Wgt Acc: 99.953%
	I - Batch: 200 | Loss: 0.747 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.747 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.747 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.747 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.747 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.747 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.747 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 256.09s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.100 | Acc: 65.125% | Wgt Acc: 64.048%
I - num batch: 62
I - Val -- Loss: 1.109 | Acc: 64.016% | Wgt Acc: 62.976% | Dur: 26.23s
I - Confusion Matrix: [row->prediction - col->label]
[[239.  34.  35.  94.]
 [  4. 132.  34.  13.]
 [  3.  48. 120.  14.]
 [ 18.  20.  36. 137.]]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 254.45s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.083 | Acc: 66.000% | Wgt Acc: 65.575%
I - num batch: 62
I - Val -- Loss: 1.094 | Acc: 64.934% | Wgt Acc: 64.583% | Dur: 27.22s
I - Confusion Matrix: [row->prediction - col->label]
[[217.  25.  27.  65.]
 [  8. 136.  42.  15.]
 [  4.  52. 116.  10.]
 [ 35.  21.  40. 168.]]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 268.81s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.090 | Acc: 66.500% | Wgt Acc: 65.297%
I - num batch: 62
I - Val -- Loss: 1.098 | Acc: 65.953% | Wgt Acc: 64.787% | Dur: 31.01s
I - Confusion Matrix: [row->prediction - col->label]
[[234.  32.  25.  75.]
 [  2. 111.  27.   7.]
 [  4.  73. 140.  14.]
 [ 24.  18.  33. 162.]]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 279.73s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.095 | Acc: 65.250% | Wgt Acc: 64.214%
I - num batch: 62
I - Val -- Loss: 1.105 | Acc: 64.016% | Wgt Acc: 62.953% | Dur: 26.44s
I - Confusion Matrix: [row->prediction - col->label]
[[236.  33.  35.  91.]
 [  0. 121.  30.   7.]
 [  3.  60. 124.  13.]
 [ 25.  20.  36. 147.]]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 254.69s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.080 | Acc: 65.625% | Wgt Acc: 65.464%
I - num batch: 62
I - Val -- Loss: 1.093 | Acc: 64.526% | Wgt Acc: 64.447% | Dur: 31.51s
I - Confusion Matrix: [row->prediction - col->label]
[[215.  27.  27.  68.]
 [  5. 145.  47.  11.]
 [  4.  31. 104.  10.]
 [ 40.  31.  47. 169.]]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 289.65s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.084 | Acc: 66.875% | Wgt Acc: 66.324%
I - num batch: 62
I - Val -- Loss: 1.093 | Acc: 65.749% | Wgt Acc: 65.285% | Dur: 28.09s
I - Confusion Matrix: [row->prediction - col->label]
[[210.  28.  24.  61.]
 [  3. 130.  34.  12.]
 [  8.  58. 132.  12.]
 [ 43.  18.  35. 173.]]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 99.969% | Wgt Acc: 99.965%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.972%
	I - Batch: 300 | Loss: 0.746 | Acc: 99.979% | Wgt Acc: 99.977%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 294.38s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    1.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.079 | Acc: 67.875% | Wgt Acc: 67.102%
I - num batch: 62
I - Val -- Loss: 1.087 | Acc: 66.871% | Wgt Acc: 66.146% | Dur: 25.91s
I - Confusion Matrix: [row->prediction - col->label]
[[232.  29.  26.  76.]
 [  6. 144.  39.  14.]
 [  4.  46. 127.  15.]
 [ 22.  15.  33. 153.]]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 269.99s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.080 | Acc: 67.125% | Wgt Acc: 66.519%
I - num batch: 62
I - Val -- Loss: 1.095 | Acc: 65.341% | Wgt Acc: 64.832% | Dur: 26.89s
I - Confusion Matrix: [row->prediction - col->label]
[[225.  26.  23.  70.]
 [  5. 138.  46.  12.]
 [  5.  55. 117.  15.]
 [ 29.  15.  39. 161.]]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 260.07s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.086 | Acc: 67.125% | Wgt Acc: 65.991%
I - num batch: 62
I - Val -- Loss: 1.095 | Acc: 66.769% | Wgt Acc: 65.716% | Dur: 27.62s
I - Confusion Matrix: [row->prediction - col->label]
[[238.  32.  29.  81.]
 [  1. 133.  34.  13.]
 [  4.  53. 135.  15.]
 [ 21.  16.  27. 149.]]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 99.982% | Wgt Acc: 99.980%
	I - Batch: 400 | Loss: 0.746 | Acc: 99.984% | Wgt Acc: 99.982%
	I - Batch: 450 | Loss: 0.746 | Acc: 99.986% | Wgt Acc: 99.984%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 99.987% | Wgt Acc: 99.985% | LR: 1.250000e-04 | Dur: 260.17s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    1.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.085 | Acc: 66.375% | Wgt Acc: 65.741%
I - num batch: 62
I - Val -- Loss: 1.091 | Acc: 65.443% | Wgt Acc: 64.900% | Dur: 27.70s
I - Confusion Matrix: [row->prediction - col->label]
[[216.  24.  19.  68.]
 [  7. 137.  42.  16.]
 [  7.  59. 128.  13.]
 [ 34.  14.  36. 161.]]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 258.53s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.089 | Acc: 67.125% | Wgt Acc: 66.102%
I - num batch: 62
I - Val -- Loss: 1.098 | Acc: 65.953% | Wgt Acc: 64.991% | Dur: 26.72s
I - Confusion Matrix: [row->prediction - col->label]
[[234.  29.  25.  88.]
 [  6. 146.  41.  18.]
 [  7.  50. 131.  16.]
 [ 17.   9.  28. 136.]]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 257.50s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.086 | Acc: 65.375% | Wgt Acc: 64.797%
I - num batch: 62
I - Val -- Loss: 1.098 | Acc: 64.118% | Wgt Acc: 63.587% | Dur: 26.68s
I - Confusion Matrix: [row->prediction - col->label]
[[228.  32.  34.  74.]
 [  6. 134.  42.  14.]
 [  4.  40. 109.  12.]
 [ 26.  28.  40. 158.]]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 263.04s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.090 | Acc: 64.875% | Wgt Acc: 63.853%
I - num batch: 62
I - Val -- Loss: 1.100 | Acc: 64.220% | Wgt Acc: 63.293% | Dur: 26.98s
I - Confusion Matrix: [row->prediction - col->label]
[[224.  29.  26.  82.]
 [  9. 131.  37.  17.]
 [  4.  57. 131.  15.]
 [ 27.  17.  31. 144.]]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 257.64s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.074 | Acc: 67.250% | Wgt Acc: 67.157%
I - num batch: 62
I - Val -- Loss: 1.085 | Acc: 66.259% | Wgt Acc: 66.214% | Dur: 26.38s
I - Confusion Matrix: [row->prediction - col->label]
[[215.  27.  26.  55.]
 [  4. 146.  47.  11.]
 [  5.  45. 111.  14.]
 [ 40.  16.  41. 178.]]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 254.05s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.075 | Acc: 67.875% | Wgt Acc: 67.046%
I - num batch: 62
I - Val -- Loss: 1.088 | Acc: 66.157% | Wgt Acc: 65.444% | Dur: 26.48s
I - Confusion Matrix: [row->prediction - col->label]
[[220.  30.  26.  72.]
 [  6. 138.  34.  12.]
 [  7.  52. 135.  18.]
 [ 31.  14.  30. 156.]]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 99.975% | Wgt Acc: 99.977%
	I - Batch: 300 | Loss: 0.747 | Acc: 99.958% | Wgt Acc: 99.962%
	I - Batch: 350 | Loss: 0.760 | Acc: 99.125% | Wgt Acc: 99.131%
	I - Batch: 400 | Loss: 0.774 | Acc: 98.219% | Wgt Acc: 98.228%
	I - Batch: 450 | Loss: 0.783 | Acc: 97.694% | Wgt Acc: 97.714%
I - num batch: 478
I - Train -- Loss: 0.785 | Acc: 97.697% | Wgt Acc: 97.718% | LR: 1.250000e-04 | Dur: 267.58s
I - Confusion Matrix: [row->prediction - col->label]
[[2039.    6.   16.   23.]
 [  10. 1703.   29.    7.]
 [  13.   19. 2148.    9.]
 [  29.    6.    9. 1575.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.085 | Acc: 65.250% | Wgt Acc: 65.214%
I - num batch: 62
I - Val -- Loss: 1.087 | Acc: 65.036% | Wgt Acc: 65.149% | Dur: 28.33s
I - Confusion Matrix: [row->prediction - col->label]
[[218.  22.  27.  54.]
 [  8. 143.  52.  13.]
 [  4.  37.  95.   9.]
 [ 34.  32.  51. 182.]]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.798 | Acc: 97.625% | Wgt Acc: 97.637%
	I - Batch: 100 | Loss: 0.794 | Acc: 98.125% | Wgt Acc: 98.109%
	I - Batch: 150 | Loss: 0.790 | Acc: 98.542% | Wgt Acc: 98.544%
	I - Batch: 200 | Loss: 0.786 | Acc: 98.719% | Wgt Acc: 98.726%
	I - Batch: 250 | Loss: 0.782 | Acc: 98.925% | Wgt Acc: 98.931%
	I - Batch: 300 | Loss: 0.779 | Acc: 99.104% | Wgt Acc: 99.108%
	I - Batch: 350 | Loss: 0.777 | Acc: 99.161% | Wgt Acc: 99.163%
	I - Batch: 400 | Loss: 0.775 | Acc: 99.266% | Wgt Acc: 99.267%
	I - Batch: 450 | Loss: 0.774 | Acc: 99.292% | Wgt Acc: 99.289%
I - num batch: 478
I - Train -- Loss: 0.773 | Acc: 99.333% | Wgt Acc: 99.331% | LR: 1.250000e-04 | Dur: 270.63s
I - Confusion Matrix: [row->prediction - col->label]
[[2073.    2.    6.    8.]
 [   5. 1727.    3.    2.]
 [   7.    4. 2192.    6.]
 [   6.    1.    1. 1598.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.111 | Acc: 64.625% | Wgt Acc: 63.326%
I - num batch: 62
I - Val -- Loss: 1.122 | Acc: 63.099% | Wgt Acc: 61.843% | Dur: 26.57s
I - Confusion Matrix: [row->prediction - col->label]
[[241.  28.  30. 101.]
 [  4. 132.  45.  16.]
 [  4.  55. 123.  18.]
 [ 15.  19.  27. 123.]]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.761 | Acc: 99.750% | Wgt Acc: 99.719%
	I - Batch: 100 | Loss: 0.758 | Acc: 99.875% | Wgt Acc: 99.860%
	I - Batch: 150 | Loss: 0.756 | Acc: 99.917% | Wgt Acc: 99.906%
	I - Batch: 200 | Loss: 0.754 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 250 | Loss: 0.754 | Acc: 99.925% | Wgt Acc: 99.916%
	I - Batch: 300 | Loss: 0.754 | Acc: 99.938% | Wgt Acc: 99.930%
	I - Batch: 350 | Loss: 0.753 | Acc: 99.946% | Wgt Acc: 99.940%
	I - Batch: 400 | Loss: 0.753 | Acc: 99.953% | Wgt Acc: 99.947%
	I - Batch: 450 | Loss: 0.752 | Acc: 99.958% | Wgt Acc: 99.953%
I - num batch: 478
I - Train -- Loss: 0.752 | Acc: 99.961% | Wgt Acc: 99.956% | LR: 1.250000e-04 | Dur: 314.37s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    1.    0.    0.]
 [   0. 1732.    0.    0.]
 [   0.    0. 2202.    1.]
 [   0.    1.    0. 1613.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.070 | Acc: 67.750% | Wgt Acc: 67.629%
I - num batch: 62
I - Val -- Loss: 1.083 | Acc: 66.463% | Wgt Acc: 66.327% | Dur: 38.08s
I - Confusion Matrix: [row->prediction - col->label]
[[206.  23.  29.  48.]
 [  7. 140.  31.  12.]
 [  3.  45. 125.  17.]
 [ 48.  26.  40. 181.]]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.749 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.749 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.749 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.748 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 358.38s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.082 | Acc: 68.000% | Wgt Acc: 67.601%
I - num batch: 62
I - Val -- Loss: 1.088 | Acc: 66.972% | Wgt Acc: 66.712% | Dur: 38.11s
I - Confusion Matrix: [row->prediction - col->label]
[[207.  24.  21.  48.]
 [  7. 137.  32.  15.]
 [  7.  55. 132.  14.]
 [ 43.  18.  40. 181.]]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 361.08s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.077 | Acc: 67.625% | Wgt Acc: 66.935%
I - num batch: 62
I - Val -- Loss: 1.087 | Acc: 66.871% | Wgt Acc: 66.214% | Dur: 38.34s
I - Confusion Matrix: [row->prediction - col->label]
[[222.  26.  23.  71.]
 [  8. 142.  33.  16.]
 [  3.  53. 134.  13.]
 [ 31.  13.  35. 158.]]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.747 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 360.82s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.092 | Acc: 66.250% | Wgt Acc: 65.713%
I - num batch: 62
I - Val -- Loss: 1.101 | Acc: 65.443% | Wgt Acc: 64.923% | Dur: 38.11s
I - Confusion Matrix: [row->prediction - col->label]
[[225.  26.  27.  76.]
 [  1. 140.  35.  13.]
 [  5.  51. 118.  10.]
 [ 33.  17.  45. 159.]]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 362.83s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.079 | Acc: 66.500% | Wgt Acc: 65.908%
I - num batch: 62
I - Val -- Loss: 1.087 | Acc: 65.953% | Wgt Acc: 65.376% | Dur: 38.91s
I - Confusion Matrix: [row->prediction - col->label]
[[221.  30.  24.  65.]
 [  1. 127.  29.  12.]
 [  3.  55. 127.   9.]
 [ 39.  22.  45. 172.]]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 356.60s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.071 | Acc: 68.375% | Wgt Acc: 67.990%
I - num batch: 62
I - Val -- Loss: 1.081 | Acc: 67.482% | Wgt Acc: 67.165% | Dur: 39.70s
I - Confusion Matrix: [row->prediction - col->label]
[[217.  26.  23.  53.]
 [  2. 138.  36.  12.]
 [  3.  53. 127.  13.]
 [ 42.  17.  39. 180.]]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 358.25s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.082 | Acc: 67.250% | Wgt Acc: 66.463%
I - num batch: 62
I - Val -- Loss: 1.090 | Acc: 66.361% | Wgt Acc: 65.602% | Dur: 37.58s
I - Confusion Matrix: [row->prediction - col->label]
[[225.  27.  24.  71.]
 [  4. 135.  33.  14.]
 [  6.  53. 133.  15.]
 [ 29.  19.  35. 158.]]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.746 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 357.44s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.078 | Acc: 66.375% | Wgt Acc: 65.908%
I - num batch: 62
I - Val -- Loss: 1.090 | Acc: 65.443% | Wgt Acc: 64.991% | Dur: 39.72s
I - Confusion Matrix: [row->prediction - col->label]
[[223.  31.  35.  71.]
 [  4. 140.  34.  12.]
 [  3.  47. 117.  13.]
 [ 34.  16.  39. 162.]]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 150 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 200 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 250 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 300 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 350 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 400 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 450 | Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000%
I - num batch: 478
I - Train -- Loss: 0.745 | Acc: 100.000% | Wgt Acc: 100.000% | LR: 1.250000e-04 | Dur: 357.35s
I - Confusion Matrix: [row->prediction - col->label]
[[2091.    0.    0.    0.]
 [   0. 1734.    0.    0.]
 [   0.    0. 2202.    0.]
 [   0.    0.    0. 1614.]]

I - Validation: 
	I - Batch: 50 | Loss: 1.083 | Acc: 65.875% | Wgt Acc: 65.353%
I - num batch: 62
I - Val -- Loss: 1.094 | Acc: 65.036% | Wgt Acc: 64.538% | Dur: 38.81s
I - Confusion Matrix: [row->prediction - col->label]
[[235.  33.  34.  72.]
 [  2. 141.  46.  17.]
 [  3.  40. 105.  12.]
 [ 24.  20.  40. 157.]]

I - Epoch: 113
I - Training: 
