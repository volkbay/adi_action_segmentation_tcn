Sun Oct 23 03:23:30 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.2, 0.25, 0.2, 0.25, 0.1], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'singleBackgroundPath': '/data_ssd/processed/kinetics400/pickles', 'singleBackgroundPickle': True, 'tossFirstLastFrames': True}, 'dataPath': '/data/processed/Kinetics/', 'dropoutRate': 0.5, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat', 'background'], 'lastLayerInitUniform': True, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 70.33639143730886, 'maxValidationTrainNo': 39, 'modelVersion': 10, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 46, 'validationAccThr': 70, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Train set length:  3547
I - Label distribution: [ 697.  578.  734.  538. 1000.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/test
I - Loading file: dataset_test_background00_no_samples72.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/test
I - Test set length:  399
I - Label distribution: [88. 78. 75. 86. 72.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(4) data number in dataset:  tensor([169824, 164763, 196171,   7673, 114001,  52277,   6873, 129907,   7517,
          7473, 203743,   9290,   7878, 111940, 165745,   7418])
I - Initializing model TCNv10
I - Number of Model Parameters: 94925
I - Model output shape:  torch.Size([16, 5])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv10                                   [16, 5]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 64, 64, 64]          6
│    └─ReLU: 2-1071                      [16, 64, 64, 64]          --
│    └─Conv2d: 2-2                       --                        3,136
│    └─BatchNorm2d: 2-1069               [16, 64, 64, 64]          --
│    └─OutputShiftSqueeze: 2-4           --                        --
│    └─One: 2-5                          [1]                       --
│    └─Scaler: 2-1070                    [16, 64, 64, 64]          --
│    └─OutputScale: 2-7                  --                        --
│    └─Empty: 2-8                        [64, 48, 1, 1]            --
│    └─Empty: 2-9                        [64, 48, 1, 1]            --
│    └─Empty: 2-10                       [64]                      --
│    └─Empty: 2-11                       [64]                      --
│    └─BatchNorm2d: 2-12                 [16, 64, 64, 64]          --
│    └─Scaler: 2-13                      [16, 64, 64, 64]          --
│    └─ReLU: 2-14                        [16, 64, 64, 64]          --
│    └─Empty: 2-15                       [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-80         [16, 64, 32, 32]          (recursive)
│    └─ReLU: 2-1086                      [16, 64, 32, 32]          --
│    └─MaxPool2d: 2-1074                 [16, 64, 32, 32]          --
│    └─Conv2d: 2-18                      --                        36,928
│    └─BatchNorm2d: 2-1084               [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-20                       [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-3          [16, 64, 32, 32]          36,934
│    └─Scaler: 2-1085                    [16, 64, 32, 32]          --
│    └─MaxPool2d: 2-22                   [16, 64, 32, 32]          --
│    └─Empty: 2-23                       [16, 64, 32, 32]          --
│    └─Empty: 2-24                       [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [64, 64, 3, 3]            --
│    └─Empty: 2-29                       [64, 64, 3, 3]            --
│    └─Empty: 2-30                       [64]                      --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 64, 16, 16]          (recursive)
│    └─ReLU: 2-1101                      [16, 64, 16, 16]          --
│    └─MaxPool2d: 2-1089                 [16, 64, 16, 16]          --
│    └─Conv2d: 2-33                      --                        36,928
│    └─BatchNorm2d: 2-1099               [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-35                       [64]                      --
│    └─BatchNorm2d: 2-36                 [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1100                    [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-38                      [16, 64, 32, 32]          --
│    └─ReLU: 2-39                        [16, 64, 32, 32]          --
│    └─Empty: 2-40                       [16, 64, 32, 32]          --
│    └─Clamp: 2-41                       [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-5          [16, 64, 16, 16]          36,674
│    └─MaxPool2d: 2-42                   [16, 64, 16, 16]          --
│    └─Empty: 2-43                       [16, 64, 16, 16]          --
│    └─Empty: 2-1090                     [16, 64, 16, 16]          --
│    └─Empty: 2-1091                     [16, 64, 16, 16]          --
│    └─Empty: 2-46                       [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1113                      [16, 4, 16, 16]           --
│    └─Conv2d: 2-48                      --                        260
│    └─BatchNorm2d: 2-1111               [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputShiftSqueeze: 2-50          --                        --
│    └─One: 2-51                         [1]                       --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1112                    [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-53                 --                        --
│    └─Empty: 2-54                       [64, 64, 3, 3]            --
│    └─Empty: 2-55                       [64, 64, 3, 3]            --
│    └─Empty: 2-56                       [64]                      --
│    └─Empty: 2-57                       [64]                      --
│    └─BatchNorm2d: 2-58                 [16, 64, 16, 16]          --
│    └─Scaler: 2-59                      [16, 64, 16, 16]          --
│    └─ReLU: 2-60                        [16, 64, 16, 16]          --
│    └─Empty: 2-61                       [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-83         [16, 4, 16, 16]           (recursive)
│    └─ReLU: 2-1128                      [16, 4, 16, 16]           --
│    └─MaxPool2d: 2-1116                 [16, 64, 16, 16]          --
│    └─Conv2d: 2-64                      --                        2,308
│    └─BatchNorm2d: 2-1126               [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Clamp: 2-66                       [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-7                 [16, 4, 16, 16]           266
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1127                    [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-68          --                        --
│    └─One: 2-69                         [1]                       --
│    └─OutputScale: 2-70                 --                        --
│    └─Empty: 2-71                       [4, 64, 1, 1]             --
│    └─Empty: 2-72                       [4, 64, 1, 1]             --
│    └─Empty: 2-73                       [4]                       --
│    └─Empty: 2-74                       [4]                       --
│    └─BatchNorm2d: 2-75                 [16, 4, 16, 16]           --
│    └─Scaler: 2-76                      [16, 4, 16, 16]           --
│    └─ReLU: 2-77                        [16, 4, 16, 16]           --
│    └─Empty: 2-78                       [16, 4, 16, 16]           --
│    └─Clamp: 2-79                       [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-8          [16, 4, 16, 16]           2,314
│    └─MaxPool2d: 2-80                   [16, 64, 16, 16]          --
├─Conv1d: 1                              --                        --
│    └─Scaler: 2-1138                    [16, 5, 14]               --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-82                       [16, 64, 16, 16]          --
│    └─Empty: 2-83                       [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-84          --                        --
│    └─One: 2-85                         [1]                       --
│    └─OutputScale: 2-86                 --                        --
│    └─Empty: 2-87                       [4, 64, 3, 3]             --
│    └─Empty: 2-88                       [4, 64, 3, 3]             --
│    └─Empty: 2-89                       [4]                       --
│    └─Empty: 2-90                       [4]                       --
│    └─BatchNorm2d: 2-91                 [16, 4, 16, 16]           --
│    └─Scaler: 2-92                      [16, 4, 16, 16]           --
│    └─ReLU: 2-93                        [16, 4, 16, 16]           --
│    └─Empty: 2-94                       [16, 4, 16, 16]           --
│    └─Clamp: 2-95                       [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-9                 [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-96          --                        --
│    └─One: 2-97                         [1]                       --
│    └─OutputScale: 2-98                 --                        --
│    └─Empty: 2-99                       [64, 48, 1, 1]            --
│    └─Empty: 2-100                      [64, 48, 1, 1]            --
│    └─Empty: 2-101                      [64]                      --
│    └─Empty: 2-102                      [64]                      --
│    └─BatchNorm2d: 2-103                [16, 64, 64, 64]          --
│    └─Scaler: 2-104                     [16, 64, 64, 64]          --
│    └─ReLU: 2-105                       [16, 64, 64, 64]          --
│    └─Empty: 2-106                      [16, 64, 64, 64]          --
│    └─Clamp: 2-107                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-10         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-108                  [16, 64, 32, 32]          --
│    └─Empty: 2-109                      [16, 64, 32, 32]          --
│    └─Empty: 2-110                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-111         --                        --
│    └─One: 2-112                        [1]                       --
│    └─OutputScale: 2-113                --                        --
│    └─Empty: 2-114                      [64, 64, 3, 3]            --
│    └─Empty: 2-115                      [64, 64, 3, 3]            --
│    └─Empty: 2-116                      [64]                      --
│    └─Empty: 2-117                      [64]                      --
│    └─BatchNorm2d: 2-118                [16, 64, 32, 32]          --
│    └─Scaler: 2-119                     [16, 64, 32, 32]          --
│    └─ReLU: 2-120                       [16, 64, 32, 32]          --
│    └─Empty: 2-121                      [16, 64, 32, 32]          --
│    └─Clamp: 2-122                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-11         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-123                  [16, 64, 16, 16]          --
│    └─Empty: 2-124                      [16, 64, 16, 16]          --
│    └─Empty: 2-125                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-126         --                        --
│    └─One: 2-127                        [1]                       --
│    └─OutputScale: 2-128                --                        --
│    └─Empty: 2-129                      [64, 64, 3, 3]            --
│    └─Empty: 2-130                      [64, 64, 3, 3]            --
│    └─Empty: 2-131                      [64]                      --
│    └─Empty: 2-132                      [64]                      --
│    └─BatchNorm2d: 2-133                [16, 64, 16, 16]          --
│    └─Scaler: 2-134                     [16, 64, 16, 16]          --
│    └─ReLU: 2-135                       [16, 64, 16, 16]          --
│    └─Empty: 2-136                      [16, 64, 16, 16]          --
│    └─Clamp: 2-137                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-12                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-138         --                        --
│    └─One: 2-139                        [1]                       --
│    └─OutputScale: 2-140                --                        --
│    └─Empty: 2-141                      [4, 64, 1, 1]             --
│    └─Empty: 2-142                      [4, 64, 1, 1]             --
│    └─Empty: 2-143                      [4]                       --
│    └─Empty: 2-144                      [4]                       --
│    └─BatchNorm2d: 2-145                [16, 4, 16, 16]           --
│    └─Scaler: 2-146                     [16, 4, 16, 16]           --
│    └─ReLU: 2-147                       [16, 4, 16, 16]           --
│    └─Empty: 2-148                      [16, 4, 16, 16]           --
│    └─Clamp: 2-149                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-13         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-150                  [16, 64, 16, 16]          --
│    └─Empty: 2-151                      [16, 64, 16, 16]          --
│    └─Empty: 2-152                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-153         --                        --
│    └─One: 2-154                        [1]                       --
│    └─OutputScale: 2-155                --                        --
│    └─Empty: 2-156                      [4, 64, 3, 3]             --
│    └─Empty: 2-157                      [4, 64, 3, 3]             --
│    └─Empty: 2-158                      [4]                       --
│    └─Empty: 2-159                      [4]                       --
│    └─BatchNorm2d: 2-160                [16, 4, 16, 16]           --
│    └─Scaler: 2-161                     [16, 4, 16, 16]           --
│    └─ReLU: 2-162                       [16, 4, 16, 16]           --
│    └─Empty: 2-163                      [16, 4, 16, 16]           --
│    └─Clamp: 2-164                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-14                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-165         --                        --
│    └─One: 2-166                        [1]                       --
│    └─OutputScale: 2-167                --                        --
│    └─Empty: 2-168                      [64, 48, 1, 1]            --
│    └─Empty: 2-169                      [64, 48, 1, 1]            --
│    └─Empty: 2-170                      [64]                      --
│    └─Empty: 2-171                      [64]                      --
│    └─BatchNorm2d: 2-172                [16, 64, 64, 64]          --
│    └─Scaler: 2-173                     [16, 64, 64, 64]          --
│    └─ReLU: 2-174                       [16, 64, 64, 64]          --
│    └─Empty: 2-175                      [16, 64, 64, 64]          --
│    └─Clamp: 2-176                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-177                  [16, 64, 32, 32]          --
│    └─Empty: 2-178                      [16, 64, 32, 32]          --
│    └─Empty: 2-179                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-180         --                        --
│    └─One: 2-181                        [1]                       --
│    └─OutputScale: 2-182                --                        --
│    └─Empty: 2-183                      [64, 64, 3, 3]            --
│    └─Empty: 2-184                      [64, 64, 3, 3]            --
│    └─Empty: 2-185                      [64]                      --
│    └─Empty: 2-186                      [64]                      --
│    └─BatchNorm2d: 2-187                [16, 64, 32, 32]          --
│    └─Scaler: 2-188                     [16, 64, 32, 32]          --
│    └─ReLU: 2-189                       [16, 64, 32, 32]          --
│    └─Empty: 2-190                      [16, 64, 32, 32]          --
│    └─Clamp: 2-191                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-16         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-192                  [16, 64, 16, 16]          --
│    └─Empty: 2-193                      [16, 64, 16, 16]          --
│    └─Empty: 2-194                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-195         --                        --
│    └─One: 2-196                        [1]                       --
│    └─OutputScale: 2-197                --                        --
│    └─Empty: 2-198                      [64, 64, 3, 3]            --
│    └─Empty: 2-199                      [64, 64, 3, 3]            --
│    └─Empty: 2-200                      [64]                      --
│    └─Empty: 2-201                      [64]                      --
│    └─BatchNorm2d: 2-202                [16, 64, 16, 16]          --
│    └─Scaler: 2-203                     [16, 64, 16, 16]          --
│    └─ReLU: 2-204                       [16, 64, 16, 16]          --
│    └─Empty: 2-205                      [16, 64, 16, 16]          --
│    └─Clamp: 2-206                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-17                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-207         --                        --
│    └─One: 2-208                        [1]                       --
│    └─OutputScale: 2-209                --                        --
│    └─Empty: 2-210                      [4, 64, 1, 1]             --
│    └─Empty: 2-211                      [4, 64, 1, 1]             --
│    └─Empty: 2-212                      [4]                       --
│    └─Empty: 2-213                      [4]                       --
│    └─BatchNorm2d: 2-214                [16, 4, 16, 16]           --
│    └─Scaler: 2-215                     [16, 4, 16, 16]           --
│    └─ReLU: 2-216                       [16, 4, 16, 16]           --
│    └─Empty: 2-217                      [16, 4, 16, 16]           --
│    └─Clamp: 2-218                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-18         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-219                  [16, 64, 16, 16]          --
│    └─Empty: 2-220                      [16, 64, 16, 16]          --
│    └─Empty: 2-221                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-222         --                        --
│    └─One: 2-223                        [1]                       --
│    └─OutputScale: 2-224                --                        --
│    └─Empty: 2-225                      [4, 64, 3, 3]             --
│    └─Empty: 2-226                      [4, 64, 3, 3]             --
│    └─Empty: 2-227                      [4]                       --
│    └─Empty: 2-228                      [4]                       --
│    └─BatchNorm2d: 2-229                [16, 4, 16, 16]           --
│    └─Scaler: 2-230                     [16, 4, 16, 16]           --
│    └─ReLU: 2-231                       [16, 4, 16, 16]           --
│    └─Empty: 2-232                      [16, 4, 16, 16]           --
│    └─Clamp: 2-233                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-19                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-234         --                        --
│    └─One: 2-235                        [1]                       --
│    └─OutputScale: 2-236                --                        --
│    └─Empty: 2-237                      [64, 48, 1, 1]            --
│    └─Empty: 2-238                      [64, 48, 1, 1]            --
│    └─Empty: 2-239                      [64]                      --
│    └─Empty: 2-240                      [64]                      --
│    └─BatchNorm2d: 2-241                [16, 64, 64, 64]          --
│    └─Scaler: 2-242                     [16, 64, 64, 64]          --
│    └─ReLU: 2-243                       [16, 64, 64, 64]          --
│    └─Empty: 2-244                      [16, 64, 64, 64]          --
│    └─Clamp: 2-245                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-20         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-246                  [16, 64, 32, 32]          --
│    └─Empty: 2-247                      [16, 64, 32, 32]          --
│    └─Empty: 2-248                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-249         --                        --
│    └─One: 2-250                        [1]                       --
│    └─OutputScale: 2-251                --                        --
│    └─Empty: 2-252                      [64, 64, 3, 3]            --
│    └─Empty: 2-253                      [64, 64, 3, 3]            --
│    └─Empty: 2-254                      [64]                      --
│    └─Empty: 2-255                      [64]                      --
│    └─BatchNorm2d: 2-256                [16, 64, 32, 32]          --
│    └─Scaler: 2-257                     [16, 64, 32, 32]          --
│    └─ReLU: 2-258                       [16, 64, 32, 32]          --
│    └─Empty: 2-259                      [16, 64, 32, 32]          --
│    └─Clamp: 2-260                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-21         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-261                  [16, 64, 16, 16]          --
│    └─Empty: 2-262                      [16, 64, 16, 16]          --
│    └─Empty: 2-263                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-264         --                        --
│    └─One: 2-265                        [1]                       --
│    └─OutputScale: 2-266                --                        --
│    └─Empty: 2-267                      [64, 64, 3, 3]            --
│    └─Empty: 2-268                      [64, 64, 3, 3]            --
│    └─Empty: 2-269                      [64]                      --
│    └─Empty: 2-270                      [64]                      --
│    └─BatchNorm2d: 2-271                [16, 64, 16, 16]          --
│    └─Scaler: 2-272                     [16, 64, 16, 16]          --
│    └─ReLU: 2-273                       [16, 64, 16, 16]          --
│    └─Empty: 2-274                      [16, 64, 16, 16]          --
│    └─Clamp: 2-275                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-22                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-276         --                        --
│    └─One: 2-277                        [1]                       --
│    └─OutputScale: 2-278                --                        --
│    └─Empty: 2-279                      [4, 64, 1, 1]             --
│    └─Empty: 2-280                      [4, 64, 1, 1]             --
│    └─Empty: 2-281                      [4]                       --
│    └─Empty: 2-282                      [4]                       --
│    └─BatchNorm2d: 2-283                [16, 4, 16, 16]           --
│    └─Scaler: 2-284                     [16, 4, 16, 16]           --
│    └─ReLU: 2-285                       [16, 4, 16, 16]           --
│    └─Empty: 2-286                      [16, 4, 16, 16]           --
│    └─Clamp: 2-287                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-23         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-288                  [16, 64, 16, 16]          --
│    └─Empty: 2-289                      [16, 64, 16, 16]          --
│    └─Empty: 2-290                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-291         --                        --
│    └─One: 2-292                        [1]                       --
│    └─OutputScale: 2-293                --                        --
│    └─Empty: 2-294                      [4, 64, 3, 3]             --
│    └─Empty: 2-295                      [4, 64, 3, 3]             --
│    └─Empty: 2-296                      [4]                       --
│    └─Empty: 2-297                      [4]                       --
│    └─BatchNorm2d: 2-298                [16, 4, 16, 16]           --
│    └─Scaler: 2-299                     [16, 4, 16, 16]           --
│    └─ReLU: 2-300                       [16, 4, 16, 16]           --
│    └─Empty: 2-301                      [16, 4, 16, 16]           --
│    └─Clamp: 2-302                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-24                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-303         --                        --
│    └─One: 2-304                        [1]                       --
│    └─OutputScale: 2-305                --                        --
│    └─Empty: 2-306                      [64, 48, 1, 1]            --
│    └─Empty: 2-307                      [64, 48, 1, 1]            --
│    └─Empty: 2-308                      [64]                      --
│    └─Empty: 2-309                      [64]                      --
│    └─BatchNorm2d: 2-310                [16, 64, 64, 64]          --
│    └─Scaler: 2-311                     [16, 64, 64, 64]          --
│    └─ReLU: 2-312                       [16, 64, 64, 64]          --
│    └─Empty: 2-313                      [16, 64, 64, 64]          --
│    └─Clamp: 2-314                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-25         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-315                  [16, 64, 32, 32]          --
│    └─Empty: 2-316                      [16, 64, 32, 32]          --
│    └─Empty: 2-317                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-318         --                        --
│    └─One: 2-319                        [1]                       --
│    └─OutputScale: 2-320                --                        --
│    └─Empty: 2-321                      [64, 64, 3, 3]            --
│    └─Empty: 2-322                      [64, 64, 3, 3]            --
│    └─Empty: 2-323                      [64]                      --
│    └─Empty: 2-324                      [64]                      --
│    └─BatchNorm2d: 2-325                [16, 64, 32, 32]          --
│    └─Scaler: 2-326                     [16, 64, 32, 32]          --
│    └─ReLU: 2-327                       [16, 64, 32, 32]          --
│    └─Empty: 2-328                      [16, 64, 32, 32]          --
│    └─Clamp: 2-329                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-330                  [16, 64, 16, 16]          --
│    └─Empty: 2-331                      [16, 64, 16, 16]          --
│    └─Empty: 2-332                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-333         --                        --
│    └─One: 2-334                        [1]                       --
│    └─OutputScale: 2-335                --                        --
│    └─Empty: 2-336                      [64, 64, 3, 3]            --
│    └─Empty: 2-337                      [64, 64, 3, 3]            --
│    └─Empty: 2-338                      [64]                      --
│    └─Empty: 2-339                      [64]                      --
│    └─BatchNorm2d: 2-340                [16, 64, 16, 16]          --
│    └─Scaler: 2-341                     [16, 64, 16, 16]          --
│    └─ReLU: 2-342                       [16, 64, 16, 16]          --
│    └─Empty: 2-343                      [16, 64, 16, 16]          --
│    └─Clamp: 2-344                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-27                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-345         --                        --
│    └─One: 2-346                        [1]                       --
│    └─OutputScale: 2-347                --                        --
│    └─Empty: 2-348                      [4, 64, 1, 1]             --
│    └─Empty: 2-349                      [4, 64, 1, 1]             --
│    └─Empty: 2-350                      [4]                       --
│    └─Empty: 2-351                      [4]                       --
│    └─BatchNorm2d: 2-352                [16, 4, 16, 16]           --
│    └─Scaler: 2-353                     [16, 4, 16, 16]           --
│    └─ReLU: 2-354                       [16, 4, 16, 16]           --
│    └─Empty: 2-355                      [16, 4, 16, 16]           --
│    └─Clamp: 2-356                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-28         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-357                  [16, 64, 16, 16]          --
│    └─Empty: 2-358                      [16, 64, 16, 16]          --
│    └─Empty: 2-359                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-360         --                        --
│    └─One: 2-361                        [1]                       --
│    └─OutputScale: 2-362                --                        --
│    └─Empty: 2-363                      [4, 64, 3, 3]             --
│    └─Empty: 2-364                      [4, 64, 3, 3]             --
│    └─Empty: 2-365                      [4]                       --
│    └─Empty: 2-366                      [4]                       --
│    └─BatchNorm2d: 2-367                [16, 4, 16, 16]           --
│    └─Scaler: 2-368                     [16, 4, 16, 16]           --
│    └─ReLU: 2-369                       [16, 4, 16, 16]           --
│    └─Empty: 2-370                      [16, 4, 16, 16]           --
│    └─Clamp: 2-371                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-29                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-372         --                        --
│    └─One: 2-373                        [1]                       --
│    └─OutputScale: 2-374                --                        --
│    └─Empty: 2-375                      [64, 48, 1, 1]            --
│    └─Empty: 2-376                      [64, 48, 1, 1]            --
│    └─Empty: 2-377                      [64]                      --
│    └─Empty: 2-378                      [64]                      --
│    └─BatchNorm2d: 2-379                [16, 64, 64, 64]          --
│    └─Scaler: 2-380                     [16, 64, 64, 64]          --
│    └─ReLU: 2-381                       [16, 64, 64, 64]          --
│    └─Empty: 2-382                      [16, 64, 64, 64]          --
│    └─Clamp: 2-383                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-30         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-384                  [16, 64, 32, 32]          --
│    └─Empty: 2-385                      [16, 64, 32, 32]          --
│    └─Empty: 2-386                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-387         --                        --
│    └─One: 2-388                        [1]                       --
│    └─OutputScale: 2-389                --                        --
│    └─Empty: 2-390                      [64, 64, 3, 3]            --
│    └─Empty: 2-391                      [64, 64, 3, 3]            --
│    └─Empty: 2-392                      [64]                      --
│    └─Empty: 2-393                      [64]                      --
│    └─BatchNorm2d: 2-394                [16, 64, 32, 32]          --
│    └─Scaler: 2-395                     [16, 64, 32, 32]          --
│    └─ReLU: 2-396                       [16, 64, 32, 32]          --
│    └─Empty: 2-397                      [16, 64, 32, 32]          --
│    └─Clamp: 2-398                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-399                  [16, 64, 16, 16]          --
│    └─Empty: 2-400                      [16, 64, 16, 16]          --
│    └─Empty: 2-401                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-402         --                        --
│    └─One: 2-403                        [1]                       --
│    └─OutputScale: 2-404                --                        --
│    └─Empty: 2-405                      [64, 64, 3, 3]            --
│    └─Empty: 2-406                      [64, 64, 3, 3]            --
│    └─Empty: 2-407                      [64]                      --
│    └─Empty: 2-408                      [64]                      --
│    └─BatchNorm2d: 2-409                [16, 64, 16, 16]          --
│    └─Scaler: 2-410                     [16, 64, 16, 16]          --
│    └─ReLU: 2-411                       [16, 64, 16, 16]          --
│    └─Empty: 2-412                      [16, 64, 16, 16]          --
│    └─Clamp: 2-413                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-32                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-414         --                        --
│    └─One: 2-415                        [1]                       --
│    └─OutputScale: 2-416                --                        --
│    └─Empty: 2-417                      [4, 64, 1, 1]             --
│    └─Empty: 2-418                      [4, 64, 1, 1]             --
│    └─Empty: 2-419                      [4]                       --
│    └─Empty: 2-420                      [4]                       --
│    └─BatchNorm2d: 2-421                [16, 4, 16, 16]           --
│    └─Scaler: 2-422                     [16, 4, 16, 16]           --
│    └─ReLU: 2-423                       [16, 4, 16, 16]           --
│    └─Empty: 2-424                      [16, 4, 16, 16]           --
│    └─Clamp: 2-425                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-33         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-426                  [16, 64, 16, 16]          --
│    └─Empty: 2-427                      [16, 64, 16, 16]          --
│    └─Empty: 2-428                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-429         --                        --
│    └─One: 2-430                        [1]                       --
│    └─OutputScale: 2-431                --                        --
│    └─Empty: 2-432                      [4, 64, 3, 3]             --
│    └─Empty: 2-433                      [4, 64, 3, 3]             --
│    └─Empty: 2-434                      [4]                       --
│    └─Empty: 2-435                      [4]                       --
│    └─BatchNorm2d: 2-436                [16, 4, 16, 16]           --
│    └─Scaler: 2-437                     [16, 4, 16, 16]           --
│    └─ReLU: 2-438                       [16, 4, 16, 16]           --
│    └─Empty: 2-439                      [16, 4, 16, 16]           --
│    └─Clamp: 2-440                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-34                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-441         --                        --
│    └─One: 2-442                        [1]                       --
│    └─OutputScale: 2-443                --                        --
│    └─Empty: 2-444                      [64, 48, 1, 1]            --
│    └─Empty: 2-445                      [64, 48, 1, 1]            --
│    └─Empty: 2-446                      [64]                      --
│    └─Empty: 2-447                      [64]                      --
│    └─BatchNorm2d: 2-448                [16, 64, 64, 64]          --
│    └─Scaler: 2-449                     [16, 64, 64, 64]          --
│    └─ReLU: 2-450                       [16, 64, 64, 64]          --
│    └─Empty: 2-451                      [16, 64, 64, 64]          --
│    └─Clamp: 2-452                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-35         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-453                  [16, 64, 32, 32]          --
│    └─Empty: 2-454                      [16, 64, 32, 32]          --
│    └─Empty: 2-455                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-456         --                        --
│    └─One: 2-457                        [1]                       --
│    └─OutputScale: 2-458                --                        --
│    └─Empty: 2-459                      [64, 64, 3, 3]            --
│    └─Empty: 2-460                      [64, 64, 3, 3]            --
│    └─Empty: 2-461                      [64]                      --
│    └─Empty: 2-462                      [64]                      --
│    └─BatchNorm2d: 2-463                [16, 64, 32, 32]          --
│    └─Scaler: 2-464                     [16, 64, 32, 32]          --
│    └─ReLU: 2-465                       [16, 64, 32, 32]          --
│    └─Empty: 2-466                      [16, 64, 32, 32]          --
│    └─Clamp: 2-467                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-36         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-468                  [16, 64, 16, 16]          --
│    └─Empty: 2-469                      [16, 64, 16, 16]          --
│    └─Empty: 2-470                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-471         --                        --
│    └─One: 2-472                        [1]                       --
│    └─OutputScale: 2-473                --                        --
│    └─Empty: 2-474                      [64, 64, 3, 3]            --
│    └─Empty: 2-475                      [64, 64, 3, 3]            --
│    └─Empty: 2-476                      [64]                      --
│    └─Empty: 2-477                      [64]                      --
│    └─BatchNorm2d: 2-478                [16, 64, 16, 16]          --
│    └─Scaler: 2-479                     [16, 64, 16, 16]          --
│    └─ReLU: 2-480                       [16, 64, 16, 16]          --
│    └─Empty: 2-481                      [16, 64, 16, 16]          --
│    └─Clamp: 2-482                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-37                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-483         --                        --
│    └─One: 2-484                        [1]                       --
│    └─OutputScale: 2-485                --                        --
│    └─Empty: 2-486                      [4, 64, 1, 1]             --
│    └─Empty: 2-487                      [4, 64, 1, 1]             --
│    └─Empty: 2-488                      [4]                       --
│    └─Empty: 2-489                      [4]                       --
│    └─BatchNorm2d: 2-490                [16, 4, 16, 16]           --
│    └─Scaler: 2-491                     [16, 4, 16, 16]           --
│    └─ReLU: 2-492                       [16, 4, 16, 16]           --
│    └─Empty: 2-493                      [16, 4, 16, 16]           --
│    └─Clamp: 2-494                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-38         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-495                  [16, 64, 16, 16]          --
│    └─Empty: 2-496                      [16, 64, 16, 16]          --
│    └─Empty: 2-497                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-498         --                        --
│    └─One: 2-499                        [1]                       --
│    └─OutputScale: 2-500                --                        --
│    └─Empty: 2-501                      [4, 64, 3, 3]             --
│    └─Empty: 2-502                      [4, 64, 3, 3]             --
│    └─Empty: 2-503                      [4]                       --
│    └─Empty: 2-504                      [4]                       --
│    └─BatchNorm2d: 2-505                [16, 4, 16, 16]           --
│    └─Scaler: 2-506                     [16, 4, 16, 16]           --
│    └─ReLU: 2-507                       [16, 4, 16, 16]           --
│    └─Empty: 2-508                      [16, 4, 16, 16]           --
│    └─Clamp: 2-509                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-39                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-510         --                        --
│    └─One: 2-511                        [1]                       --
│    └─OutputScale: 2-512                --                        --
│    └─Empty: 2-513                      [64, 48, 1, 1]            --
│    └─Empty: 2-514                      [64, 48, 1, 1]            --
│    └─Empty: 2-515                      [64]                      --
│    └─Empty: 2-516                      [64]                      --
│    └─BatchNorm2d: 2-517                [16, 64, 64, 64]          --
│    └─Scaler: 2-518                     [16, 64, 64, 64]          --
│    └─ReLU: 2-519                       [16, 64, 64, 64]          --
│    └─Empty: 2-520                      [16, 64, 64, 64]          --
│    └─Clamp: 2-521                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-40         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-522                  [16, 64, 32, 32]          --
│    └─Empty: 2-523                      [16, 64, 32, 32]          --
│    └─Empty: 2-524                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-525         --                        --
│    └─One: 2-526                        [1]                       --
│    └─OutputScale: 2-527                --                        --
│    └─Empty: 2-528                      [64, 64, 3, 3]            --
│    └─Empty: 2-529                      [64, 64, 3, 3]            --
│    └─Empty: 2-530                      [64]                      --
│    └─Empty: 2-531                      [64]                      --
│    └─BatchNorm2d: 2-532                [16, 64, 32, 32]          --
│    └─Scaler: 2-533                     [16, 64, 32, 32]          --
│    └─ReLU: 2-534                       [16, 64, 32, 32]          --
│    └─Empty: 2-535                      [16, 64, 32, 32]          --
│    └─Clamp: 2-536                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-41         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-537                  [16, 64, 16, 16]          --
│    └─Empty: 2-538                      [16, 64, 16, 16]          --
│    └─Empty: 2-539                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-540         --                        --
│    └─One: 2-541                        [1]                       --
│    └─OutputScale: 2-542                --                        --
│    └─Empty: 2-543                      [64, 64, 3, 3]            --
│    └─Empty: 2-544                      [64, 64, 3, 3]            --
│    └─Empty: 2-545                      [64]                      --
│    └─Empty: 2-546                      [64]                      --
│    └─BatchNorm2d: 2-547                [16, 64, 16, 16]          --
│    └─Scaler: 2-548                     [16, 64, 16, 16]          --
│    └─ReLU: 2-549                       [16, 64, 16, 16]          --
│    └─Empty: 2-550                      [16, 64, 16, 16]          --
│    └─Clamp: 2-551                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-42                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-552         --                        --
│    └─One: 2-553                        [1]                       --
│    └─OutputScale: 2-554                --                        --
│    └─Empty: 2-555                      [4, 64, 1, 1]             --
│    └─Empty: 2-556                      [4, 64, 1, 1]             --
│    └─Empty: 2-557                      [4]                       --
│    └─Empty: 2-558                      [4]                       --
│    └─BatchNorm2d: 2-559                [16, 4, 16, 16]           --
│    └─Scaler: 2-560                     [16, 4, 16, 16]           --
│    └─ReLU: 2-561                       [16, 4, 16, 16]           --
│    └─Empty: 2-562                      [16, 4, 16, 16]           --
│    └─Clamp: 2-563                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-43         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-564                  [16, 64, 16, 16]          --
│    └─Empty: 2-565                      [16, 64, 16, 16]          --
│    └─Empty: 2-566                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-567         --                        --
│    └─One: 2-568                        [1]                       --
│    └─OutputScale: 2-569                --                        --
│    └─Empty: 2-570                      [4, 64, 3, 3]             --
│    └─Empty: 2-571                      [4, 64, 3, 3]             --
│    └─Empty: 2-572                      [4]                       --
│    └─Empty: 2-573                      [4]                       --
│    └─BatchNorm2d: 2-574                [16, 4, 16, 16]           --
│    └─Scaler: 2-575                     [16, 4, 16, 16]           --
│    └─ReLU: 2-576                       [16, 4, 16, 16]           --
│    └─Empty: 2-577                      [16, 4, 16, 16]           --
│    └─Clamp: 2-578                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-44                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-579         --                        --
│    └─One: 2-580                        [1]                       --
│    └─OutputScale: 2-581                --                        --
│    └─Empty: 2-582                      [64, 48, 1, 1]            --
│    └─Empty: 2-583                      [64, 48, 1, 1]            --
│    └─Empty: 2-584                      [64]                      --
│    └─Empty: 2-585                      [64]                      --
│    └─BatchNorm2d: 2-586                [16, 64, 64, 64]          --
│    └─Scaler: 2-587                     [16, 64, 64, 64]          --
│    └─ReLU: 2-588                       [16, 64, 64, 64]          --
│    └─Empty: 2-589                      [16, 64, 64, 64]          --
│    └─Clamp: 2-590                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-45         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-591                  [16, 64, 32, 32]          --
│    └─Empty: 2-592                      [16, 64, 32, 32]          --
│    └─Empty: 2-593                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-594         --                        --
│    └─One: 2-595                        [1]                       --
│    └─OutputScale: 2-596                --                        --
│    └─Empty: 2-597                      [64, 64, 3, 3]            --
│    └─Empty: 2-598                      [64, 64, 3, 3]            --
│    └─Empty: 2-599                      [64]                      --
│    └─Empty: 2-600                      [64]                      --
│    └─BatchNorm2d: 2-601                [16, 64, 32, 32]          --
│    └─Scaler: 2-602                     [16, 64, 32, 32]          --
│    └─ReLU: 2-603                       [16, 64, 32, 32]          --
│    └─Empty: 2-604                      [16, 64, 32, 32]          --
│    └─Clamp: 2-605                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-606                  [16, 64, 16, 16]          --
│    └─Empty: 2-607                      [16, 64, 16, 16]          --
│    └─Empty: 2-608                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-609         --                        --
│    └─One: 2-610                        [1]                       --
│    └─OutputScale: 2-611                --                        --
│    └─Empty: 2-612                      [64, 64, 3, 3]            --
│    └─Empty: 2-613                      [64, 64, 3, 3]            --
│    └─Empty: 2-614                      [64]                      --
│    └─Empty: 2-615                      [64]                      --
│    └─BatchNorm2d: 2-616                [16, 64, 16, 16]          --
│    └─Scaler: 2-617                     [16, 64, 16, 16]          --
│    └─ReLU: 2-618                       [16, 64, 16, 16]          --
│    └─Empty: 2-619                      [16, 64, 16, 16]          --
│    └─Clamp: 2-620                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-47                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-621         --                        --
│    └─One: 2-622                        [1]                       --
│    └─OutputScale: 2-623                --                        --
│    └─Empty: 2-624                      [4, 64, 1, 1]             --
│    └─Empty: 2-625                      [4, 64, 1, 1]             --
│    └─Empty: 2-626                      [4]                       --
│    └─Empty: 2-627                      [4]                       --
│    └─BatchNorm2d: 2-628                [16, 4, 16, 16]           --
│    └─Scaler: 2-629                     [16, 4, 16, 16]           --
│    └─ReLU: 2-630                       [16, 4, 16, 16]           --
│    └─Empty: 2-631                      [16, 4, 16, 16]           --
│    └─Clamp: 2-632                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-633                  [16, 64, 16, 16]          --
│    └─Empty: 2-634                      [16, 64, 16, 16]          --
│    └─Empty: 2-635                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-636         --                        --
│    └─One: 2-637                        [1]                       --
│    └─OutputScale: 2-638                --                        --
│    └─Empty: 2-639                      [4, 64, 3, 3]             --
│    └─Empty: 2-640                      [4, 64, 3, 3]             --
│    └─Empty: 2-641                      [4]                       --
│    └─Empty: 2-642                      [4]                       --
│    └─BatchNorm2d: 2-643                [16, 4, 16, 16]           --
│    └─Scaler: 2-644                     [16, 4, 16, 16]           --
│    └─ReLU: 2-645                       [16, 4, 16, 16]           --
│    └─Empty: 2-646                      [16, 4, 16, 16]           --
│    └─Clamp: 2-647                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-49                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-648         --                        --
│    └─One: 2-649                        [1]                       --
│    └─OutputScale: 2-650                --                        --
│    └─Empty: 2-651                      [64, 48, 1, 1]            --
│    └─Empty: 2-652                      [64, 48, 1, 1]            --
│    └─Empty: 2-653                      [64]                      --
│    └─Empty: 2-654                      [64]                      --
│    └─BatchNorm2d: 2-655                [16, 64, 64, 64]          --
│    └─Scaler: 2-656                     [16, 64, 64, 64]          --
│    └─ReLU: 2-657                       [16, 64, 64, 64]          --
│    └─Empty: 2-658                      [16, 64, 64, 64]          --
│    └─Clamp: 2-659                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-50         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-660                  [16, 64, 32, 32]          --
│    └─Empty: 2-661                      [16, 64, 32, 32]          --
│    └─Empty: 2-662                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-663         --                        --
│    └─One: 2-664                        [1]                       --
│    └─OutputScale: 2-665                --                        --
│    └─Empty: 2-666                      [64, 64, 3, 3]            --
│    └─Empty: 2-667                      [64, 64, 3, 3]            --
│    └─Empty: 2-668                      [64]                      --
│    └─Empty: 2-669                      [64]                      --
│    └─BatchNorm2d: 2-670                [16, 64, 32, 32]          --
│    └─Scaler: 2-671                     [16, 64, 32, 32]          --
│    └─ReLU: 2-672                       [16, 64, 32, 32]          --
│    └─Empty: 2-673                      [16, 64, 32, 32]          --
│    └─Clamp: 2-674                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-675                  [16, 64, 16, 16]          --
│    └─Empty: 2-676                      [16, 64, 16, 16]          --
│    └─Empty: 2-677                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-678         --                        --
│    └─One: 2-679                        [1]                       --
│    └─OutputScale: 2-680                --                        --
│    └─Empty: 2-681                      [64, 64, 3, 3]            --
│    └─Empty: 2-682                      [64, 64, 3, 3]            --
│    └─Empty: 2-683                      [64]                      --
│    └─Empty: 2-684                      [64]                      --
│    └─BatchNorm2d: 2-685                [16, 64, 16, 16]          --
│    └─Scaler: 2-686                     [16, 64, 16, 16]          --
│    └─ReLU: 2-687                       [16, 64, 16, 16]          --
│    └─Empty: 2-688                      [16, 64, 16, 16]          --
│    └─Clamp: 2-689                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-52                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-690         --                        --
│    └─One: 2-691                        [1]                       --
│    └─OutputScale: 2-692                --                        --
│    └─Empty: 2-693                      [4, 64, 1, 1]             --
│    └─Empty: 2-694                      [4, 64, 1, 1]             --
│    └─Empty: 2-695                      [4]                       --
│    └─Empty: 2-696                      [4]                       --
│    └─BatchNorm2d: 2-697                [16, 4, 16, 16]           --
│    └─Scaler: 2-698                     [16, 4, 16, 16]           --
│    └─ReLU: 2-699                       [16, 4, 16, 16]           --
│    └─Empty: 2-700                      [16, 4, 16, 16]           --
│    └─Clamp: 2-701                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-53         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-702                  [16, 64, 16, 16]          --
│    └─Empty: 2-703                      [16, 64, 16, 16]          --
│    └─Empty: 2-704                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-705         --                        --
│    └─One: 2-706                        [1]                       --
│    └─OutputScale: 2-707                --                        --
│    └─Empty: 2-708                      [4, 64, 3, 3]             --
│    └─Empty: 2-709                      [4, 64, 3, 3]             --
│    └─Empty: 2-710                      [4]                       --
│    └─Empty: 2-711                      [4]                       --
│    └─BatchNorm2d: 2-712                [16, 4, 16, 16]           --
│    └─Scaler: 2-713                     [16, 4, 16, 16]           --
│    └─ReLU: 2-714                       [16, 4, 16, 16]           --
│    └─Empty: 2-715                      [16, 4, 16, 16]           --
│    └─Clamp: 2-716                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-54                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-717         --                        --
│    └─One: 2-718                        [1]                       --
│    └─OutputScale: 2-719                --                        --
│    └─Empty: 2-720                      [64, 48, 1, 1]            --
│    └─Empty: 2-721                      [64, 48, 1, 1]            --
│    └─Empty: 2-722                      [64]                      --
│    └─Empty: 2-723                      [64]                      --
│    └─BatchNorm2d: 2-724                [16, 64, 64, 64]          --
│    └─Scaler: 2-725                     [16, 64, 64, 64]          --
│    └─ReLU: 2-726                       [16, 64, 64, 64]          --
│    └─Empty: 2-727                      [16, 64, 64, 64]          --
│    └─Clamp: 2-728                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-55         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-729                  [16, 64, 32, 32]          --
│    └─Empty: 2-730                      [16, 64, 32, 32]          --
│    └─Empty: 2-731                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-732         --                        --
│    └─One: 2-733                        [1]                       --
│    └─OutputScale: 2-734                --                        --
│    └─Empty: 2-735                      [64, 64, 3, 3]            --
│    └─Empty: 2-736                      [64, 64, 3, 3]            --
│    └─Empty: 2-737                      [64]                      --
│    └─Empty: 2-738                      [64]                      --
│    └─BatchNorm2d: 2-739                [16, 64, 32, 32]          --
│    └─Scaler: 2-740                     [16, 64, 32, 32]          --
│    └─ReLU: 2-741                       [16, 64, 32, 32]          --
│    └─Empty: 2-742                      [16, 64, 32, 32]          --
│    └─Clamp: 2-743                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-56         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-744                  [16, 64, 16, 16]          --
│    └─Empty: 2-745                      [16, 64, 16, 16]          --
│    └─Empty: 2-746                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-747         --                        --
│    └─One: 2-748                        [1]                       --
│    └─OutputScale: 2-749                --                        --
│    └─Empty: 2-750                      [64, 64, 3, 3]            --
│    └─Empty: 2-751                      [64, 64, 3, 3]            --
│    └─Empty: 2-752                      [64]                      --
│    └─Empty: 2-753                      [64]                      --
│    └─BatchNorm2d: 2-754                [16, 64, 16, 16]          --
│    └─Scaler: 2-755                     [16, 64, 16, 16]          --
│    └─ReLU: 2-756                       [16, 64, 16, 16]          --
│    └─Empty: 2-757                      [16, 64, 16, 16]          --
│    └─Clamp: 2-758                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-57                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-759         --                        --
│    └─One: 2-760                        [1]                       --
│    └─OutputScale: 2-761                --                        --
│    └─Empty: 2-762                      [4, 64, 1, 1]             --
│    └─Empty: 2-763                      [4, 64, 1, 1]             --
│    └─Empty: 2-764                      [4]                       --
│    └─Empty: 2-765                      [4]                       --
│    └─BatchNorm2d: 2-766                [16, 4, 16, 16]           --
│    └─Scaler: 2-767                     [16, 4, 16, 16]           --
│    └─ReLU: 2-768                       [16, 4, 16, 16]           --
│    └─Empty: 2-769                      [16, 4, 16, 16]           --
│    └─Clamp: 2-770                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-58         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-771                  [16, 64, 16, 16]          --
│    └─Empty: 2-772                      [16, 64, 16, 16]          --
│    └─Empty: 2-773                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-774         --                        --
│    └─One: 2-775                        [1]                       --
│    └─OutputScale: 2-776                --                        --
│    └─Empty: 2-777                      [4, 64, 3, 3]             --
│    └─Empty: 2-778                      [4, 64, 3, 3]             --
│    └─Empty: 2-779                      [4]                       --
│    └─Empty: 2-780                      [4]                       --
│    └─BatchNorm2d: 2-781                [16, 4, 16, 16]           --
│    └─Scaler: 2-782                     [16, 4, 16, 16]           --
│    └─ReLU: 2-783                       [16, 4, 16, 16]           --
│    └─Empty: 2-784                      [16, 4, 16, 16]           --
│    └─Clamp: 2-785                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-59                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-786         --                        --
│    └─One: 2-787                        [1]                       --
│    └─OutputScale: 2-788                --                        --
│    └─Empty: 2-789                      [64, 48, 1, 1]            --
│    └─Empty: 2-790                      [64, 48, 1, 1]            --
│    └─Empty: 2-791                      [64]                      --
│    └─Empty: 2-792                      [64]                      --
│    └─BatchNorm2d: 2-793                [16, 64, 64, 64]          --
│    └─Scaler: 2-794                     [16, 64, 64, 64]          --
│    └─ReLU: 2-795                       [16, 64, 64, 64]          --
│    └─Empty: 2-796                      [16, 64, 64, 64]          --
│    └─Clamp: 2-797                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-60         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-798                  [16, 64, 32, 32]          --
│    └─Empty: 2-799                      [16, 64, 32, 32]          --
│    └─Empty: 2-800                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-801         --                        --
│    └─One: 2-802                        [1]                       --
│    └─OutputScale: 2-803                --                        --
│    └─Empty: 2-804                      [64, 64, 3, 3]            --
│    └─Empty: 2-805                      [64, 64, 3, 3]            --
│    └─Empty: 2-806                      [64]                      --
│    └─Empty: 2-807                      [64]                      --
│    └─BatchNorm2d: 2-808                [16, 64, 32, 32]          --
│    └─Scaler: 2-809                     [16, 64, 32, 32]          --
│    └─ReLU: 2-810                       [16, 64, 32, 32]          --
│    └─Empty: 2-811                      [16, 64, 32, 32]          --
│    └─Clamp: 2-812                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-61         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-813                  [16, 64, 16, 16]          --
│    └─Empty: 2-814                      [16, 64, 16, 16]          --
│    └─Empty: 2-815                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-816         --                        --
│    └─One: 2-817                        [1]                       --
│    └─OutputScale: 2-818                --                        --
│    └─Empty: 2-819                      [64, 64, 3, 3]            --
│    └─Empty: 2-820                      [64, 64, 3, 3]            --
│    └─Empty: 2-821                      [64]                      --
│    └─Empty: 2-822                      [64]                      --
│    └─BatchNorm2d: 2-823                [16, 64, 16, 16]          --
│    └─Scaler: 2-824                     [16, 64, 16, 16]          --
│    └─ReLU: 2-825                       [16, 64, 16, 16]          --
│    └─Empty: 2-826                      [16, 64, 16, 16]          --
│    └─Clamp: 2-827                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-62                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-828         --                        --
│    └─One: 2-829                        [1]                       --
│    └─OutputScale: 2-830                --                        --
│    └─Empty: 2-831                      [4, 64, 1, 1]             --
│    └─Empty: 2-832                      [4, 64, 1, 1]             --
│    └─Empty: 2-833                      [4]                       --
│    └─Empty: 2-834                      [4]                       --
│    └─BatchNorm2d: 2-835                [16, 4, 16, 16]           --
│    └─Scaler: 2-836                     [16, 4, 16, 16]           --
│    └─ReLU: 2-837                       [16, 4, 16, 16]           --
│    └─Empty: 2-838                      [16, 4, 16, 16]           --
│    └─Clamp: 2-839                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-63         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-840                  [16, 64, 16, 16]          --
│    └─Empty: 2-841                      [16, 64, 16, 16]          --
│    └─Empty: 2-842                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-843         --                        --
│    └─One: 2-844                        [1]                       --
│    └─OutputScale: 2-845                --                        --
│    └─Empty: 2-846                      [4, 64, 3, 3]             --
│    └─Empty: 2-847                      [4, 64, 3, 3]             --
│    └─Empty: 2-848                      [4]                       --
│    └─Empty: 2-849                      [4]                       --
│    └─BatchNorm2d: 2-850                [16, 4, 16, 16]           --
│    └─Scaler: 2-851                     [16, 4, 16, 16]           --
│    └─ReLU: 2-852                       [16, 4, 16, 16]           --
│    └─Empty: 2-853                      [16, 4, 16, 16]           --
│    └─Clamp: 2-854                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-64                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-855         --                        --
│    └─One: 2-856                        [1]                       --
│    └─OutputScale: 2-857                --                        --
│    └─Empty: 2-858                      [64, 48, 1, 1]            --
│    └─Empty: 2-859                      [64, 48, 1, 1]            --
│    └─Empty: 2-860                      [64]                      --
│    └─Empty: 2-861                      [64]                      --
│    └─BatchNorm2d: 2-862                [16, 64, 64, 64]          --
│    └─Scaler: 2-863                     [16, 64, 64, 64]          --
│    └─ReLU: 2-864                       [16, 64, 64, 64]          --
│    └─Empty: 2-865                      [16, 64, 64, 64]          --
│    └─Clamp: 2-866                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-65         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-867                  [16, 64, 32, 32]          --
│    └─Empty: 2-868                      [16, 64, 32, 32]          --
│    └─Empty: 2-869                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-870         --                        --
│    └─One: 2-871                        [1]                       --
│    └─OutputScale: 2-872                --                        --
│    └─Empty: 2-873                      [64, 64, 3, 3]            --
│    └─Empty: 2-874                      [64, 64, 3, 3]            --
│    └─Empty: 2-875                      [64]                      --
│    └─Empty: 2-876                      [64]                      --
│    └─BatchNorm2d: 2-877                [16, 64, 32, 32]          --
│    └─Scaler: 2-878                     [16, 64, 32, 32]          --
│    └─ReLU: 2-879                       [16, 64, 32, 32]          --
│    └─Empty: 2-880                      [16, 64, 32, 32]          --
│    └─Clamp: 2-881                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-882                  [16, 64, 16, 16]          --
│    └─Empty: 2-883                      [16, 64, 16, 16]          --
│    └─Empty: 2-884                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-885         --                        --
│    └─One: 2-886                        [1]                       --
│    └─OutputScale: 2-887                --                        --
│    └─Empty: 2-888                      [64, 64, 3, 3]            --
│    └─Empty: 2-889                      [64, 64, 3, 3]            --
│    └─Empty: 2-890                      [64]                      --
│    └─Empty: 2-891                      [64]                      --
│    └─BatchNorm2d: 2-892                [16, 64, 16, 16]          --
│    └─Scaler: 2-893                     [16, 64, 16, 16]          --
│    └─ReLU: 2-894                       [16, 64, 16, 16]          --
│    └─Empty: 2-895                      [16, 64, 16, 16]          --
│    └─Clamp: 2-896                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-67                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-897         --                        --
│    └─One: 2-898                        [1]                       --
│    └─OutputScale: 2-899                --                        --
│    └─Empty: 2-900                      [4, 64, 1, 1]             --
│    └─Empty: 2-901                      [4, 64, 1, 1]             --
│    └─Empty: 2-902                      [4]                       --
│    └─Empty: 2-903                      [4]                       --
│    └─BatchNorm2d: 2-904                [16, 4, 16, 16]           --
│    └─Scaler: 2-905                     [16, 4, 16, 16]           --
│    └─ReLU: 2-906                       [16, 4, 16, 16]           --
│    └─Empty: 2-907                      [16, 4, 16, 16]           --
│    └─Clamp: 2-908                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-909                  [16, 64, 16, 16]          --
│    └─Empty: 2-910                      [16, 64, 16, 16]          --
│    └─Empty: 2-911                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-912         --                        --
│    └─One: 2-913                        [1]                       --
│    └─OutputScale: 2-914                --                        --
│    └─Empty: 2-915                      [4, 64, 3, 3]             --
│    └─Empty: 2-916                      [4, 64, 3, 3]             --
│    └─Empty: 2-917                      [4]                       --
│    └─Empty: 2-918                      [4]                       --
│    └─BatchNorm2d: 2-919                [16, 4, 16, 16]           --
│    └─Scaler: 2-920                     [16, 4, 16, 16]           --
│    └─ReLU: 2-921                       [16, 4, 16, 16]           --
│    └─Empty: 2-922                      [16, 4, 16, 16]           --
│    └─Clamp: 2-923                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-69                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-924         --                        --
│    └─One: 2-925                        [1]                       --
│    └─OutputScale: 2-926                --                        --
│    └─Empty: 2-927                      [64, 48, 1, 1]            --
│    └─Empty: 2-928                      [64, 48, 1, 1]            --
│    └─Empty: 2-929                      [64]                      --
│    └─Empty: 2-930                      [64]                      --
│    └─BatchNorm2d: 2-931                [16, 64, 64, 64]          --
│    └─Scaler: 2-932                     [16, 64, 64, 64]          --
│    └─ReLU: 2-933                       [16, 64, 64, 64]          --
│    └─Empty: 2-934                      [16, 64, 64, 64]          --
│    └─Clamp: 2-935                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-70         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-936                  [16, 64, 32, 32]          --
│    └─Empty: 2-937                      [16, 64, 32, 32]          --
│    └─Empty: 2-938                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-939         --                        --
│    └─One: 2-940                        [1]                       --
│    └─OutputScale: 2-941                --                        --
│    └─Empty: 2-942                      [64, 64, 3, 3]            --
│    └─Empty: 2-943                      [64, 64, 3, 3]            --
│    └─Empty: 2-944                      [64]                      --
│    └─Empty: 2-945                      [64]                      --
│    └─BatchNorm2d: 2-946                [16, 64, 32, 32]          --
│    └─Scaler: 2-947                     [16, 64, 32, 32]          --
│    └─ReLU: 2-948                       [16, 64, 32, 32]          --
│    └─Empty: 2-949                      [16, 64, 32, 32]          --
│    └─Clamp: 2-950                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-71         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-951                  [16, 64, 16, 16]          --
│    └─Empty: 2-952                      [16, 64, 16, 16]          --
│    └─Empty: 2-953                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-954         --                        --
│    └─One: 2-955                        [1]                       --
│    └─OutputScale: 2-956                --                        --
│    └─Empty: 2-957                      [64, 64, 3, 3]            --
│    └─Empty: 2-958                      [64, 64, 3, 3]            --
│    └─Empty: 2-959                      [64]                      --
│    └─Empty: 2-960                      [64]                      --
│    └─BatchNorm2d: 2-961                [16, 64, 16, 16]          --
│    └─Scaler: 2-962                     [16, 64, 16, 16]          --
│    └─ReLU: 2-963                       [16, 64, 16, 16]          --
│    └─Empty: 2-964                      [16, 64, 16, 16]          --
│    └─Clamp: 2-965                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-72                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-966         --                        --
│    └─One: 2-967                        [1]                       --
│    └─OutputScale: 2-968                --                        --
│    └─Empty: 2-969                      [4, 64, 1, 1]             --
│    └─Empty: 2-970                      [4, 64, 1, 1]             --
│    └─Empty: 2-971                      [4]                       --
│    └─Empty: 2-972                      [4]                       --
│    └─BatchNorm2d: 2-973                [16, 4, 16, 16]           --
│    └─Scaler: 2-974                     [16, 4, 16, 16]           --
│    └─ReLU: 2-975                       [16, 4, 16, 16]           --
│    └─Empty: 2-976                      [16, 4, 16, 16]           --
│    └─Clamp: 2-977                      [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-978                  [16, 64, 16, 16]          --
│    └─Empty: 2-979                      [16, 64, 16, 16]          --
│    └─Empty: 2-980                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-981         --                        --
│    └─One: 2-982                        [1]                       --
│    └─OutputScale: 2-983                --                        --
│    └─Empty: 2-984                      [4, 64, 3, 3]             --
│    └─Empty: 2-985                      [4, 64, 3, 3]             --
│    └─Empty: 2-986                      [4]                       --
│    └─Empty: 2-987                      [4]                       --
│    └─BatchNorm2d: 2-988                [16, 4, 16, 16]           --
│    └─Scaler: 2-989                     [16, 4, 16, 16]           --
│    └─ReLU: 2-990                       [16, 4, 16, 16]           --
│    └─Empty: 2-991                      [16, 4, 16, 16]           --
│    └─Clamp: 2-992                      [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-74                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-993         --                        --
│    └─One: 2-994                        [1]                       --
│    └─OutputScale: 2-995                --                        --
│    └─Empty: 2-996                      [64, 48, 1, 1]            --
│    └─Empty: 2-997                      [64, 48, 1, 1]            --
│    └─Empty: 2-998                      [64]                      --
│    └─Empty: 2-999                      [64]                      --
│    └─BatchNorm2d: 2-1000               [16, 64, 64, 64]          --
│    └─Scaler: 2-1001                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1002                      [16, 64, 64, 64]          --
│    └─Empty: 2-1003                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1004                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1005                 [16, 64, 32, 32]          --
│    └─Empty: 2-1006                     [16, 64, 32, 32]          --
│    └─Empty: 2-1007                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1008        --                        --
│    └─One: 2-1009                       [1]                       --
│    └─OutputScale: 2-1010               --                        --
│    └─Empty: 2-1011                     [64, 64, 3, 3]            --
│    └─Empty: 2-1012                     [64, 64, 3, 3]            --
│    └─Empty: 2-1013                     [64]                      --
│    └─Empty: 2-1014                     [64]                      --
│    └─BatchNorm2d: 2-1015               [16, 64, 32, 32]          --
│    └─Scaler: 2-1016                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1017                      [16, 64, 32, 32]          --
│    └─Empty: 2-1018                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1019                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-76         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1020                 [16, 64, 16, 16]          --
│    └─Empty: 2-1021                     [16, 64, 16, 16]          --
│    └─Empty: 2-1022                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1023        --                        --
│    └─One: 2-1024                       [1]                       --
│    └─OutputScale: 2-1025               --                        --
│    └─Empty: 2-1026                     [64, 64, 3, 3]            --
│    └─Empty: 2-1027                     [64, 64, 3, 3]            --
│    └─Empty: 2-1028                     [64]                      --
│    └─Empty: 2-1029                     [64]                      --
│    └─BatchNorm2d: 2-1030               [16, 64, 16, 16]          --
│    └─Scaler: 2-1031                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1032                      [16, 64, 16, 16]          --
│    └─Empty: 2-1033                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1034                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-77                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1035        --                        --
│    └─One: 2-1036                       [1]                       --
│    └─OutputScale: 2-1037               --                        --
│    └─Empty: 2-1038                     [4, 64, 1, 1]             --
│    └─Empty: 2-1039                     [4, 64, 1, 1]             --
│    └─Empty: 2-1040                     [4]                       --
│    └─Empty: 2-1041                     [4]                       --
│    └─BatchNorm2d: 2-1042               [16, 4, 16, 16]           --
│    └─Scaler: 2-1043                    [16, 4, 16, 16]           --
│    └─ReLU: 2-1044                      [16, 4, 16, 16]           --
│    └─Empty: 2-1045                     [16, 4, 16, 16]           --
│    └─Clamp: 2-1046                     [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-78         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1047                 [16, 64, 16, 16]          --
│    └─Empty: 2-1048                     [16, 64, 16, 16]          --
│    └─Empty: 2-1049                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1050        --                        --
│    └─One: 2-1051                       [1]                       --
│    └─OutputScale: 2-1052               --                        --
│    └─Empty: 2-1053                     [4, 64, 3, 3]             --
│    └─Empty: 2-1054                     [4, 64, 3, 3]             --
│    └─Empty: 2-1055                     [4]                       --
│    └─Empty: 2-1056                     [4]                       --
│    └─BatchNorm2d: 2-1057               [16, 4, 16, 16]           --
│    └─Scaler: 2-1058                    [16, 4, 16, 16]           --
│    └─ReLU: 2-1059                      [16, 4, 16, 16]           --
│    └─Empty: 2-1060                     [16, 4, 16, 16]           --
│    └─Clamp: 2-1061                     [16, 4, 16, 16]           --
├─FusedConv2dBNReLU: 1-79                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1062        --                        --
│    └─One: 2-1063                       [1]                       --
│    └─OutputScale: 2-1064               --                        --
│    └─Empty: 2-1065                     [64, 48, 1, 1]            --
│    └─Empty: 2-1066                     [64, 48, 1, 1]            --
│    └─Empty: 2-1067                     [64]                      --
│    └─Empty: 2-1068                     [64]                      --
│    └─BatchNorm2d: 2-1069               [16, 64, 64, 64]          --
│    └─Scaler: 2-1070                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1071                      [16, 64, 64, 64]          --
│    └─Empty: 2-1072                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1073                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-80         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1074                 [16, 64, 32, 32]          --
│    └─Empty: 2-1075                     [16, 64, 32, 32]          --
│    └─Empty: 2-1076                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1077        --                        --
│    └─One: 2-1078                       [1]                       --
│    └─OutputScale: 2-1079               --                        --
│    └─Empty: 2-1080                     [64, 64, 3, 3]            --
│    └─Empty: 2-1081                     [64, 64, 3, 3]            --
│    └─Empty: 2-1082                     [64]                      --
│    └─Empty: 2-1083                     [64]                      --
│    └─BatchNorm2d: 2-1084               [16, 64, 32, 32]          --
│    └─Scaler: 2-1085                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1086                      [16, 64, 32, 32]          --
│    └─Empty: 2-1087                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1088                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1089                 [16, 64, 16, 16]          --
│    └─Empty: 2-1090                     [16, 64, 16, 16]          --
│    └─Empty: 2-1091                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1092        --                        --
│    └─One: 2-1093                       [1]                       --
│    └─OutputScale: 2-1094               --                        --
│    └─Empty: 2-1095                     [64, 64, 3, 3]            --
│    └─Empty: 2-1096                     [64, 64, 3, 3]            --
│    └─Empty: 2-1097                     [64]                      --
│    └─Empty: 2-1098                     [64]                      --
│    └─BatchNorm2d: 2-1099               [16, 64, 16, 16]          --
│    └─Scaler: 2-1100                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1101                      [16, 64, 16, 16]          --
│    └─Empty: 2-1102                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1103                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-82                [16, 4, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1104        --                        --
│    └─One: 2-1105                       [1]                       --
│    └─OutputScale: 2-1106               --                        --
│    └─Empty: 2-1107                     [4, 64, 1, 1]             --
│    └─Empty: 2-1108                     [4, 64, 1, 1]             --
│    └─Empty: 2-1109                     [4]                       --
│    └─Empty: 2-1110                     [4]                       --
│    └─BatchNorm2d: 2-1111               [16, 4, 16, 16]           --
│    └─Scaler: 2-1112                    [16, 4, 16, 16]           --
│    └─ReLU: 2-1113                      [16, 4, 16, 16]           --
│    └─Empty: 2-1114                     [16, 4, 16, 16]           --
│    └─Clamp: 2-1115                     [16, 4, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-83         [16, 4, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1116                 [16, 64, 16, 16]          --
│    └─Empty: 2-1117                     [16, 64, 16, 16]          --
│    └─Empty: 2-1118                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1119        --                        --
│    └─One: 2-1120                       [1]                       --
│    └─OutputScale: 2-1121               --                        --
│    └─Empty: 2-1122                     [4, 64, 3, 3]             --
│    └─Empty: 2-1123                     [4, 64, 3, 3]             --
│    └─Empty: 2-1124                     [4]                       --
│    └─Empty: 2-1125                     [4]                       --
│    └─BatchNorm2d: 2-1126               [16, 4, 16, 16]           --
│    └─Scaler: 2-1127                    [16, 4, 16, 16]           --
│    └─ReLU: 2-1128                      [16, 4, 16, 16]           --
│    └─Empty: 2-1129                     [16, 4, 16, 16]           --
│    └─Clamp: 2-1130                     [16, 4, 16, 16]           --
├─Conv1d: 1-84                           [16, 5, 14]               15,371
│    └─OutputShiftSqueeze: 2-1131        --                        --
│    └─One: 2-1132                       [1]                       --
│    └─OutputScale: 2-1133               --                        --
│    └─Empty: 2-1134                     [5, 1024, 3]              --
│    └─Empty: 2-1135                     [5, 1024, 3]              --
│    └─Empty: 2-1136                     [5]                       --
│    └─Empty: 2-1137                     [5]                       --
│    └─Scaler: 2-1138                    [16, 5, 14]               --
│    └─Empty: 2-1139                     [16, 5, 14]               --
│    └─Empty: 2-1140                     [16, 5, 14]               --
│    └─Clamp: 2-1141                     [16, 5, 14]               --
==========================================================================================
Total params: 94,961
Trainable params: 94,925
Non-trainable params: 36
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 0.00
Params size (MB): 0.32
Estimated Total Size (MB): 201.64
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.654 | Acc: 21.250% | Wgt Acc: 24.554%
	I - Batch: 100 | Loss: 1.568 | Acc: 25.125% | Wgt Acc: 29.364%
	I - Batch: 150 | Loss: 1.511 | Acc: 26.542% | Wgt Acc: 31.367%
	I - Batch: 200 | Loss: 1.482 | Acc: 28.531% | Wgt Acc: 33.737%
I - num batch: 222
I - Train -- Loss: 1.472 | Acc: 28.644% | Wgt Acc: 33.832% | LR: 1.000000e-03 | Dur: 127.43s
I - Confusion Matrix: [row->prediction - col->label]
[[347.  55. 116. 200. 142.]
 [120. 342. 376. 113. 516.]
 [111. 127. 157.  75. 198.]
 [109.  50.  77. 145. 119.]
 [ 10.   4.   8.   5.  25.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.410 | Acc: 35.088% | Wgt Acc: 38.490% | Dur: 12.89s
I - Confusion Matrix: [row->prediction - col->label]
[[68. 16. 21. 56. 30.]
 [ 9. 51. 38. 16. 37.]
 [ 3.  4.  7.  0.  1.]
 [ 8.  7.  8. 13.  3.]
 [ 0.  0.  1.  1.  1.]]

I - Local maximum validation set accuracy:  35.09

I - Validation set results: 
[14-1-1-0.99][50-3-1-0.93][124-2-1-0.46][127-0-0-0.99][443-2-1-0.99][567-0-0-0.99][573-1-1-0.99][615-0-0-0.99][695-1-2-0.97][722-3-0-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-1-0.99][1212-3-0-0.99][1368-0-0-0.99][2181-2-0-0.99][2476-2-0-0.99][2721-2-1-0.99][2818-1-0-0.99][2886-2-1-0.99]
[3231-2-1-0.99][3333-2-3-0.99][3482-2-1-0.99][3536-3-1-0.98][3625-1-1-0.99][3909-0-0-0.99][4035-0-0-0.99][4140-0-0-0.99][4214-1-3-0.99][4346-1-0-0.99]
[4581-2-1-0.99][4708-3-0-0.99][4838-3-0-0.99][4845-1-1-0.99][4868-0-1-0.97][4939-0-1-0.99][4984-2-0-0.99][5078-1-0-0.99][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.99][5843-1-1-0.99][5949-3-0-0.99][5987-2-1-0.99][6014-3-1-0.99][6033-3-0-0.99][6313-0-0-0.99][6421-3-0-0.99][6500-1-1-0.99][6583-3-3-0.99]
[6683-3-1-0.77][6825-2-0-0.99][6998-3-0-0.62][7049-3-3-0.99][7517-1-1-0.99][7521-1-3-0.94][7528-1-0-0.99][7949-1-0-0.99][8135-1-2-0.97][8185-3-0-0.99]
[8269-3-1-0.99][8273-3-3-0.99][8543-3-0-0.99][8666-1-3-0.99][8672-0-0-0.99][8903-1-0-0.99][9001-2-4-0.52][9036-2-2-0.99][9281-3-1-0.99][9300-2-0-0.99]
[9571-0-3-0.98][9617-1-1-0.99][9644-2-2-0.74][9705-2-1-0.74][9801-0-3-0.99][9803-3-0-0.99][9865-3-0-0.99][9896-2-1-0.99][10314-1-1-0.99][10337-3-0-0.99]
[10403-0-0-0.99][10653-2-1-0.99][10704-2-3-0.99][10719-1-1-0.99][10727-1-1-0.99][10836-0-0-0.99][10969-2-0-0.99][11042-0-0-0.99][11088-1-2-0.99][11322-0-0-0.99]
[11398-2-2-0.70][11499-0-0-0.77][11502-3-0-0.99][11512-3-3-0.99][11608-1-1-0.99][11610-0-3-0.68][11692-0-0-0.99][11905-0-0-0.99][11993-1-1-0.99][12002-2-0-0.99]
[12052-0-0-0.56][12201-0-0-0.99][12235-2-1-0.99][12320-1-0-0.97][12377-2-1-0.99][12398-2-1-0.99][12503-1-1-0.99][12617-0-1-0.99][12685-3-1-0.99][12738-2-3-0.73]
[12742-2-1-0.99][12823-0-0-0.99][13110-1-1-0.99][13240-3-0-0.94][13253-1-1-0.99][13273-0-0-0.99][13634-1-1-0.99][13763-2-3-0.92][13905-3-0-0.99][14060-2-1-0.99]
[14065-3-0-0.99][14147-3-3-0.99][14595-2-1-0.99][14687-2-0-0.94][14788-2-1-0.89][14869-1-1-0.99][14872-3-1-0.99][14877-1-0-0.87][14927-0-0-0.99][15066-0-0-0.99]
[15175-1-1-0.99][15178-2-0-0.99][15375-3-0-0.44][15389-3-0-0.99][15568-2-1-0.99][15675-3-0-0.99][15869-1-0-0.83][16207-3-0-0.99][16236-0-0-0.99][16302-3-0-0.99]
[16331-2-1-0.99][16381-0-0-0.99][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-1-0.99][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.99][17245-1-1-0.96]
[17278-3-1-0.99][17282-0-0-0.99][17311-2-0-0.99][17336-2-3-0.99][17608-3-0-0.99][17627-0-3-0.92][17877-3-1-0.99][17924-1-0-0.99][17984-3-0-0.99][18211-0-3-0.99]
[18276-3-0-0.99][18287-1-1-0.99][18394-0-0-0.99][18428-0-1-0.99][18442-0-0-0.99][18478-3-0-0.99][18607-0-0-0.99][18616-0-3-0.99][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-1-0.99][18824-2-1-0.99][18890-3-0-0.99][18930-3-0-0.99][18938-3-0-0.99][19817-1-1-0.99][19839-0-2-0.22][19930-3-0-0.99][19944-0-1-0.23][20036-2-0-0.99]
[20101-3-0-0.99][20474-1-1-0.99][20547-3-1-0.99][20929-2-1-0.99][21245-1-0-0.99][21257-3-3-0.99][21293-1-1-0.99][21316-1-1-0.81][21384-1-1-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-1-0.99][21714-0-0-0.99][21943-3-1-0.99][21947-0-0-0.99][21948-0-0-0.99][21965-2-1-0.99][21998-1-3-0.83][22025-0-3-0.99][22228-3-0-0.99]
[22446-1-1-0.99][22494-3-0-0.99][22757-0-0-0.99][22811-3-0-0.99][22976-3-1-0.99][22985-3-0-0.99][23014-0-0-0.99][23112-1-1-0.99][23144-3-0-0.99][23168-2-0-0.99]
[23219-0-0-0.99][23363-3-0-0.99][23470-0-1-0.99][23486-2-0-0.74][23497-0-0-0.99][23516-0-0-0.99][23690-1-1-0.99][23921-2-1-0.86][23936-1-3-0.99][24040-3-4-0.31]
[24111-1-1-0.99][24182-0-0-0.99][24238-3-0-0.99][24290-2-0-0.99][24345-0-0-0.99][24364-1-0-0.99][24427-3-0-0.99][24477-2-2-0.92][24495-2-1-0.99][24893-2-1-0.99]
[25012-1-1-0.99][25121-2-0-0.99][25165-3-0-0.99][25183-0-3-0.93][25297-3-3-0.99][25398-0-0-0.99][25574-2-1-0.99][25644-1-0-0.88][25718-1-1-0.99][25774-2-3-0.99]
[26032-3-0-0.89][26051-3-0-0.99][26120-0-0-0.99][26321-1-1-0.99][26732-1-1-0.99][26784-3-0-0.99][26827-3-0-0.99][26833-0-0-0.99][26838-2-0-0.95][26860-1-0-0.54]
[26948-0-2-0.99][27049-3-0-0.99][27098-1-0-0.98][27526-0-0-0.99][27639-3-0-0.99][27698-3-0-0.99][27772-0-0-0.99][27890-1-1-0.99][28040-0-0-0.99][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-1-0.99][29777-0-0-0.99][29877-2-1-0.99][30035-1-1-0.99][30098-0-0-0.99][30326-1-1-0.99][30572-2-3-0.99][30716-0-0-0.96]
[30806-2-0-0.87][30906-1-1-0.99][31007-0-0-0.49][31181-3-0-0.94][31238-0-0-0.99][31347-0-0-0.99][31422-2-1-0.99][31429-3-0-0.99][31431-0-0-0.99][31432-1-1-0.99]
[31477-0-0-0.99][31524-1-3-0.99][31597-1-1-0.99][31619-1-1-0.62][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.99][32074-1-3-0.39][32078-3-3-0.99][32111-1-1-0.99]
[32127-1-2-0.66][32140-3-0-0.99][32263-2-1-0.95][32365-0-1-0.99][32411-2-0-0.99][32429-3-0-0.99][32473-3-0-0.99][32574-3-0-0.99][32584-0-0-0.99][32622-0-1-0.99]
[32858-3-3-0.75][32969-3-0-0.99][33016-2-1-0.99][33031-1-0-0.99][33035-2-0-0.15][33133-2-2-0.99][33173-2-1-0.99][33175-3-1-0.99][33306-3-1-0.99][33309-2-1-0.99]
[33474-0-2-0.99][33478-2-0-0.99][33618-1-1-0.99][33712-0-0-0.98][33782-2-1-0.99][33914-3-3-0.99][34076-3-3-0.94][34112-2-1-0.99][34138-2-3-0.75][34239-1-1-0.99]
[34364-2-2-0.65][34617-1-1-0.99][34751-3-3-0.99][34783-2-1-0.99][35015-3-0-0.99][35018-1-1-0.99][35288-2-0-0.97][214490-4-1-0.99][214508-4-0-0.98][214565-4-3-0.99]
[214581-4-1-0.99][214585-4-1-0.99][214625-4-0-0.99][214627-4-1-0.99][214688-4-1-0.99][214694-4-1-0.99][214815-4-1-0.99][214882-4-1-0.99][214894-4-0-0.68][214979-4-1-0.99]
[215082-4-0-0.99][215160-4-0-0.99][215171-4-1-0.53][215210-4-0-0.73][215245-4-3-0.97][215274-4-1-0.99][215365-4-1-0.99][215412-4-1-0.89][215694-4-0-0.99][215718-4-0-0.99]
[215757-4-1-0.99][215778-4-0-0.97][215911-4-0-0.99][215913-4-1-0.99][215950-4-0-0.99][215974-4-0-0.99][215988-4-0-0.84][216086-4-1-0.99][216138-4-2-0.99][216343-4-0-0.99]
[216381-4-0-0.94][216394-4-1-0.99][216840-4-0-0.99][216895-4-1-0.99][217062-4-1-0.99][217108-4-0-0.99][217130-4-1-0.99][217143-4-1-0.99][217466-4-3-0.99][217504-4-1-0.99]
[217518-4-0-0.99][217644-4-1-0.99][217736-4-0-0.99][217752-4-1-0.99][217787-4-0-0.99][217797-4-0-0.99][218000-4-1-0.99][218030-4-1-0.99][218084-4-0-0.99][218157-4-1-0.99]
[218167-4-1-0.99][218290-4-1-0.99][218293-4-4-0.97][218478-4-1-0.99][218580-4-1-0.99][218649-4-1-0.99][218799-4-0-0.99][218966-4-0-0.99][219102-4-1-0.99][219166-4-0-0.99]
[219274-4-0-0.99][219568-4-1-0.99][219588-4-1-0.99][219635-4-0-0.75][219648-4-1-0.99][219661-4-0-0.99][219668-4-0-0.99][219740-4-0-0.99][219745-4-1-0.99]
---------------------------
I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.249 | Acc: 43.625% | Wgt Acc: 46.517%
	I - Batch: 100 | Loss: 1.237 | Acc: 44.000% | Wgt Acc: 46.639%
	I - Batch: 150 | Loss: 1.219 | Acc: 45.208% | Wgt Acc: 47.695%
I - num batch: 173
I - Train -- Loss: 1.223 | Acc: 44.826% | Wgt Acc: 47.478% | LR: 1.000000e-03 | Dur: 97.75s
I - Confusion Matrix: [row->prediction - col->label]
[[422.  24.  48. 186.  38.]
 [ 79. 400. 414.  81. 110.]
 [ 76. 116. 197.  52.  53.]
 [119.  38.  75. 219.  15.]
 [  1.   0.   0.   0.   1.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.488 | Acc: 38.095% | Wgt Acc: 44.121% | Dur: 13.14s
I - Confusion Matrix: [row->prediction - col->label]
[[33.  0.  4. 11.  2.]
 [16. 63. 53. 26. 53.]
 [12.  9. 14.  7. 11.]
 [27.  6.  4. 42.  6.]
 [ 0.  0.  0.  0.  0.]]

I - Local maximum validation set accuracy:  38.10

I - Validation set results: 
[14-1-1-0.99][50-3-1-0.99][124-2-1-0.92][127-0-0-0.85][443-2-1-0.99][567-0-0-0.99][573-1-1-0.99][615-0-3-0.85][695-1-1-0.99][722-3-3-0.99]
[826-0-3-0.99][878-0-0-0.99][1103-0-2-0.96][1212-3-1-0.61][1368-0-0-0.99][2181-2-3-0.99][2476-2-1-0.98][2721-2-2-0.99][2818-1-1-0.99][2886-2-1-0.99]
[3231-2-1-0.99][3333-2-1-0.99][3482-2-1-0.99][3536-3-1-0.99][3625-1-1-0.99][3909-0-1-0.67][4035-0-3-0.99][4140-0-0-0.67][4214-1-3-0.44][4346-1-3-0.98]
[4581-2-1-0.99][4708-3-1-0.86][4838-3-1-0.99][4845-1-1-0.99][4868-0-0-0.65][4939-0-1-0.99][4984-2-3-0.99][5078-1-1-0.99][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-1-0.53][5843-1-1-0.99][5949-3-3-0.96][5987-2-1-0.99][6014-3-1-0.99][6033-3-0-0.58][6313-0-3-0.99][6421-3-3-0.99][6500-1-1-0.99][6583-3-3-0.99]
[6683-3-3-0.99][6825-2-1-0.74][6998-3-1-0.33][7049-3-3-0.96][7517-1-1-0.99][7521-1-3-0.98][7528-1-1-0.99][7949-1-1-0.99][8135-1-3-0.81][8185-3-3-0.82]
[8269-3-1-0.99][8273-3-3-0.99][8543-3-0-0.92][8666-1-1-0.99][8672-0-3-0.99][8903-1-1-0.99][9001-2-1-0.99][9036-2-1-0.99][9281-3-1-0.99][9300-2-1-0.99]
[9571-0-3-0.36][9617-1-1-0.99][9644-2-1-0.99][9705-2-1-0.95][9801-0-3-0.99][9803-3-2-0.82][9865-3-0-0.99][9896-2-1-0.99][10314-1-1-0.99][10337-3-3-0.99]
[10403-0-2-0.99][10653-2-1-0.99][10704-2-1-0.99][10719-1-1-0.99][10727-1-1-0.99][10836-0-0-0.99][10969-2-3-0.99][11042-0-1-0.50][11088-1-1-0.99][11322-0-3-0.99]
[11398-2-2-0.77][11499-0-2-0.30][11502-3-3-0.99][11512-3-1-0.99][11608-1-1-0.99][11610-0-3-0.64][11692-0-3-0.99][11905-0-0-0.99][11993-1-1-0.99][12002-2-0-0.61]
[12052-0-2-0.25][12201-0-0-0.99][12235-2-1-0.99][12320-1-1-0.99][12377-2-1-0.99][12398-2-1-0.99][12503-1-1-0.99][12617-0-1-0.99][12685-3-1-0.99][12738-2-3-0.90]
[12742-2-1-0.99][12823-0-0-0.99][13110-1-1-0.99][13240-3-3-0.43][13253-1-1-0.99][13273-0-0-0.99][13634-1-1-0.99][13763-2-1-0.99][13905-3-2-0.98][14060-2-1-0.99]
[14065-3-3-0.99][14147-3-3-0.83][14595-2-1-0.99][14687-2-2-0.93][14788-2-1-0.99][14869-1-1-0.99][14872-3-1-0.99][14877-1-1-0.99][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-1-0.99][15178-2-0-0.29][15375-3-1-0.03][15389-3-3-0.99][15568-2-1-0.99][15675-3-3-0.80][15869-1-1-0.99][16207-3-1-0.76][16236-0-3-0.65][16302-3-2-0.61]
[16331-2-1-0.99][16381-0-0-0.47][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-1-0.99][16801-0-0-0.99][16828-0-0-0.85][17137-3-0-0.99][17245-1-2-0.97]
[17278-3-1-0.99][17282-0-1-0.45][17311-2-2-0.99][17336-2-2-0.99][17608-3-0-0.99][17627-0-1-0.17][17877-3-1-0.99][17924-1-2-0.77][17984-3-3-0.89][18211-0-3-0.84]
[18276-3-0-0.99][18287-1-1-0.99][18394-0-0-0.87][18428-0-0-0.99][18442-0-3-0.99][18478-3-0-0.99][18607-0-1-0.99][18616-0-1-0.99][18663-0-0-0.99][18718-0-0-0.62]
[18766-2-1-0.99][18824-2-1-0.99][18890-3-1-0.99][18930-3-1-0.99][18938-3-3-0.77][19817-1-1-0.99][19839-0-2-0.74][19930-3-0-0.99][19944-0-2-0.78][20036-2-1-0.99]
[20101-3-3-0.97][20474-1-1-0.99][20547-3-1-0.99][20929-2-2-0.95][21245-1-1-0.99][21257-3-1-0.99][21293-1-1-0.99][21316-1-2-0.98][21384-1-1-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-1-0.99][21714-0-2-0.76][21943-3-1-0.99][21947-0-0-0.88][21948-0-0-0.99][21965-2-1-0.99][21998-1-2-0.99][22025-0-2-0.99][22228-3-0-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-1-0.99][22985-3-3-0.99][23014-0-3-0.99][23112-1-1-0.99][23144-3-3-0.99][23168-2-1-0.99]
[23219-0-3-0.52][23363-3-3-0.99][23470-0-1-0.99][23486-2-1-0.99][23497-0-3-0.99][23516-0-0-0.99][23690-1-2-0.99][23921-2-1-0.99][23936-1-3-0.99][24040-3-2-0.70]
[24111-1-1-0.99][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.29][24345-0-0-0.99][24364-1-2-0.87][24427-3-3-0.97][24477-2-2-0.99][24495-2-1-0.99][24893-2-1-0.99]
[25012-1-1-0.99][25121-2-2-0.99][25165-3-3-0.99][25183-0-3-0.99][25297-3-3-0.99][25398-0-3-0.99][25574-2-1-0.99][25644-1-1-0.99][25718-1-1-0.99][25774-2-1-0.99]
[26032-3-3-0.96][26051-3-3-0.99][26120-0-1-0.99][26321-1-1-0.99][26732-1-1-0.99][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-2-0.93][26860-1-2-0.66]
[26948-0-2-0.97][27049-3-0-0.99][27098-1-2-0.46][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-3-0.91][27890-1-1-0.99][28040-0-0-0.41][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-1-0.99][29777-0-0-0.99][29877-2-1-0.99][30035-1-1-0.99][30098-0-3-0.99][30326-1-1-0.99][30572-2-2-0.93][30716-0-1-0.99]
[30806-2-1-0.85][30906-1-1-0.99][31007-0-2-0.79][31181-3-3-0.90][31238-0-3-0.85][31347-0-3-0.99][31422-2-1-0.99][31429-3-2-0.19][31431-0-3-0.89][31432-1-1-0.99]
[31477-0-3-0.99][31524-1-2-0.62][31597-1-1-0.99][31619-1-1-0.99][31701-0-2-0.47][31755-0-3-0.87][31854-3-3-0.99][32074-1-1-0.96][32078-3-3-0.99][32111-1-1-0.99]
[32127-1-1-0.99][32140-3-3-0.99][32263-2-1-0.99][32365-0-1-0.98][32411-2-0-0.99][32429-3-3-0.84][32473-3-2-0.99][32574-3-3-0.99][32584-0-2-0.69][32622-0-1-0.99]
[32858-3-3-0.46][32969-3-0-0.96][33016-2-1-0.99][33031-1-3-0.99][33035-2-2-0.99][33133-2-1-0.99][33173-2-1-0.99][33175-3-1-0.99][33306-3-1-0.99][33309-2-1-0.99]
[33474-0-1-0.96][33478-2-2-0.99][33618-1-1-0.99][33712-0-1-0.11][33782-2-1-0.99][33914-3-1-0.99][34076-3-3-0.63][34112-2-1-0.99][34138-2-2-0.94][34239-1-1-0.99]
[34364-2-1-0.99][34617-1-1-0.99][34751-3-3-0.99][34783-2-1-0.99][35015-3-2-0.82][35018-1-1-0.99][35288-2-1-0.65][214490-4-1-0.99][214508-4-1-0.99][214565-4-1-0.99]
[214581-4-1-0.99][214585-4-1-0.99][214625-4-1-0.99][214627-4-1-0.99][214688-4-1-0.99][214694-4-1-0.99][214815-4-1-0.99][214882-4-1-0.99][214894-4-2-0.66][214979-4-1-0.99]
[215082-4-1-0.99][215160-4-1-0.87][215171-4-2-0.99][215210-4-1-0.95][215245-4-2-0.92][215274-4-3-0.99][215365-4-1-0.99][215412-4-1-0.85][215694-4-1-0.99][215718-4-2-0.42]
[215757-4-1-0.99][215778-4-1-0.99][215911-4-0-0.41][215913-4-1-0.99][215950-4-3-0.76][215974-4-2-0.97][215988-4-1-0.53][216086-4-1-0.99][216138-4-1-0.99][216343-4-1-0.96]
[216381-4-1-0.57][216394-4-1-0.99][216840-4-0-0.46][216895-4-1-0.99][217062-4-1-0.99][217108-4-3-0.92][217130-4-1-0.99][217143-4-1-0.99][217466-4-2-0.73][217504-4-1-0.99]
[217518-4-1-0.99][217644-4-1-0.99][217736-4-3-0.93][217752-4-1-0.99][217787-4-1-0.99][217797-4-2-0.62][218000-4-1-0.99][218030-4-1-0.99][218084-4-3-0.99][218157-4-1-0.99]
[218167-4-1-0.99][218290-4-1-0.99][218293-4-1-0.99][218478-4-1-0.99][218580-4-1-0.99][218649-4-1-0.99][218799-4-1-0.99][218966-4-2-0.99][219102-4-1-0.99][219166-4-2-0.25]
[219274-4-1-0.99][219568-4-1-0.99][219588-4-1-0.99][219635-4-2-0.52][219648-4-1-0.99][219661-4-2-0.57][219668-4-1-0.99][219740-4-3-0.10][219745-4-1-0.99]
---------------------------
I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.353 | Acc: 37.875% | Wgt Acc: 44.738%
	I - Batch: 100 | Loss: 1.319 | Acc: 36.875% | Wgt Acc: 44.057%
	I - Batch: 150 | Loss: 1.306 | Acc: 38.667% | Wgt Acc: 45.033%
	I - Batch: 200 | Loss: 1.289 | Acc: 39.844% | Wgt Acc: 46.072%
I - num batch: 222
I - Train -- Loss: 1.283 | Acc: 40.062% | Wgt Acc: 46.294% | LR: 1.000000e-03 | Dur: 126.54s
I - Confusion Matrix: [row->prediction - col->label]
[[420.  27.  42. 139. 145.]
 [ 66. 401. 376.  52. 452.]
 [ 30.  97. 211.  53. 177.]
 [149.  36.  70. 284. 121.]
 [ 32.  17.  35.  10. 105.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.375 | Acc: 37.845% | Wgt Acc: 40.842% | Dur: 12.59s
I - Confusion Matrix: [row->prediction - col->label]
[[49.  1.  4. 25.  7.]
 [ 7. 41. 38.  9. 30.]
 [17. 31. 31. 25. 27.]
 [15.  3.  1. 25.  3.]
 [ 0.  2.  1.  2.  5.]]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.270 | Acc: 40.875% | Wgt Acc: 46.400%
	I - Batch: 100 | Loss: 1.233 | Acc: 43.375% | Wgt Acc: 48.901%
	I - Batch: 150 | Loss: 1.224 | Acc: 44.417% | Wgt Acc: 49.733%
	I - Batch: 200 | Loss: 1.225 | Acc: 44.750% | Wgt Acc: 50.104%
I - num batch: 222
I - Train -- Loss: 1.223 | Acc: 44.996% | Wgt Acc: 50.180% | LR: 1.000000e-03 | Dur: 124.67s
I - Confusion Matrix: [row->prediction - col->label]
[[429.  23.  18. 145. 119.]
 [ 46. 387. 278.  50. 337.]
 [ 46. 106. 320.  53. 261.]
 [135.  36.  69. 275.  98.]
 [ 41.  26.  49.  15. 185.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.343 | Acc: 45.614% | Wgt Acc: 46.906% | Dur: 12.77s
I - Confusion Matrix: [row->prediction - col->label]
[[70. 11.  8. 44. 23.]
 [ 1. 33. 14.  1. 13.]
 [ 2. 22. 31.  8. 11.]
 [11.  9. 13. 31.  8.]
 [ 4.  3.  9.  2. 17.]]

I - Local maximum validation set accuracy:  45.61

I - Validation set results: 
[14-1-1-0.20][50-3-0-0.95][124-2-2-0.58][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.93][615-0-0-0.99][695-1-2-0.99][722-3-0-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-0-0.83][1212-3-0-0.65][1368-0-0-0.99][2181-2-3-0.99][2476-2-0-0.99][2721-2-2-0.99][2818-1-0-0.40][2886-2-1-0.99]
[3231-2-1-0.96][3333-2-2-0.61][3482-2-2--0.38][3536-3-3-0.89][3625-1-1-0.99][3909-0-0-0.29][4035-0-0-0.99][4140-0-0-0.99][4214-1-3-0.65][4346-1-0-0.99]
[4581-2-2-0.99][4708-3-2-0.50][4838-3-0-0.99][4845-1-1--0.04][4868-0-0-0.99][4939-0-3--0.31][4984-2-3-0.99][5078-1-0--0.15][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.83][5843-1-1-0.99][5949-3-0-0.99][5987-2-4-0.99][6014-3-3-0.99][6033-3-0-0.99][6313-0-0-0.99][6421-3-3-0.99][6500-1-1-0.59][6583-3-3-0.99]
[6683-3-3-0.99][6825-2-3-0.43][6998-3-3-0.94][7049-3-3-0.99][7517-1-1-0.99][7521-1-3-0.99][7528-1-3-0.99][7949-1-2-0.94][8135-1-0-0.99][8185-3-0-0.99]
[8269-3-2-0.19][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.99][8903-1-3-0.77][9001-2-1-0.99][9036-2-4-0.42][9281-3-0-0.63][9300-2-2-0.65]
[9571-0-3-0.99][9617-1-0-0.84][9644-2-2-0.99][9705-2-4-0.99][9801-0-3-0.99][9803-3-3-0.99][9865-3-0-0.99][9896-2-4-0.99][10314-1-2-0.98][10337-3-0-0.99]
[10403-0-4-0.77][10653-2-1-0.24][10704-2-1-0.90][10719-1-2-0.91][10727-1-1-0.99][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.99][11088-1-2-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-3-0.67][11502-3-0-0.99][11512-3-3-0.98][11608-1-1-0.99][11610-0-3-0.61][11692-0-0-0.99][11905-0-0-0.99][11993-1-2-0.95][12002-2-0-0.65]
[12052-0-0-0.99][12201-0-0-0.99][12235-2-1-0.94][12320-1-4-0.99][12377-2-4-0.99][12398-2-4--0.43][12503-1-4-0.92][12617-0-1-0.99][12685-3-3-0.49][12738-2-3-0.99]
[12742-2-2-0.82][12823-0-0-0.99][13110-1-2-0.99][13240-3-0-0.99][13253-1-1-0.99][13273-0-0-0.99][13634-1-1-0.99][13763-2-3-0.99][13905-3-3-0.89][14060-2-1-0.99]
[14065-3-0-0.99][14147-3-3-0.88][14595-2-2-0.99][14687-2-2-0.99][14788-2-2-0.62][14869-1-1-0.21][14872-3-0-0.70][14877-1-1-0.93][14927-0-0-0.99][15066-0-0-0.99]
[15175-1-2-0.26][15178-2-0-0.99][15375-3-0-0.99][15389-3-0-0.99][15568-2-1-0.99][15675-3-3-0.99][15869-1-2-0.22][16207-3-0-0.99][16236-0-3-0.99][16302-3-0-0.99]
[16331-2-2-0.99][16381-0-0-0.99][16488-1-2-0.93][16495-0-0-0.99][16650-0-0-0.99][16719-1-0-0.94][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.99][17245-1-1--0.59]
[17278-3-0-0.65][17282-0-0-0.99][17311-2-2-0.89][17336-2-1-0.72][17608-3-0-0.99][17627-0-3-0.00][17877-3-0-0.77][17924-1-3--0.09][17984-3-0-0.99][18211-0-0-0.92]
[18276-3-0-0.99][18287-1-0-0.47][18394-0-0-0.99][18428-0-0-0.76][18442-0-0-0.99][18478-3-0-0.99][18607-0-0-0.99][18616-0-0-0.90][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-1-0.99][18824-2-2-0.99][18890-3-2-0.99][18930-3-0-0.98][18938-3-3-0.67][19817-1-4-0.36][19839-0-4-0.99][19930-3-0-0.99][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-3-0.99][20474-1-3--0.39][20547-3-0-0.99][20929-2-2-0.96][21245-1-2-0.01][21257-3-0-0.99][21293-1-1-0.99][21316-1-1-0.46][21384-1-2-0.64][21448-1-1-0.18]
[21483-0-0-0.99][21487-2-2-0.58][21714-0-0-0.99][21943-3-1-0.35][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.28][21998-1-2-0.87][22025-0-2-0.97][22228-3-0-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-4-0.99][22985-3-0-0.99][23014-0-0-0.99][23112-1-1-0.97][23144-3-0-0.99][23168-2-0-0.99]
[23219-0-0-0.99][23363-3-0-0.99][23470-0-0-0.48][23486-2-3--0.18][23497-0-0-0.99][23516-0-0-0.99][23690-1-3-0.99][23921-2-4-0.90][23936-1-3-0.99][24040-3-2-0.88]
[24111-1-1-0.99][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.99][24364-1-0--0.15][24427-3-0-0.99][24477-2-2-0.99][24495-2-1-0.64][24893-2-2-0.43]
[25012-1-0-0.38][25121-2-4-0.99][25165-3-0-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.44][25644-1-1-0.79][25718-1-1-0.99][25774-2-3-0.99]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-0-0.97][26321-1-2-0.77][26732-1-2--0.42][26784-3-3-0.99][26827-3-3-0.96][26833-0-0-0.99][26838-2-3-0.99][26860-1-2-0.73]
[26948-0-0-0.99][27049-3-0-0.99][27098-1-0-0.99][27526-0-4-0.76][27639-3-0-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.92][28040-0-0-0.99][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-0--0.06][29777-0-0-0.99][29877-2-0-0.27][30035-1-1-0.99][30098-0-0-0.99][30326-1-1-0.99][30572-2-2-0.86][30716-0-0-0.97]
[30806-2-3-0.09][30906-1-1-0.51][31007-0-0-0.99][31181-3-3-0.99][31238-0-0-0.94][31347-0-0-0.99][31422-2-1-0.99][31429-3-3-0.67][31431-0-3-0.99][31432-1-1-0.65]
[31477-0-0-0.99][31524-1-3-0.18][31597-1-2-0.99][31619-1-2-0.93][31701-0-3-0.97][31755-0-3-0.99][31854-3-0-0.99][32074-1-2--0.12][32078-3-3-0.99][32111-1-2-0.98]
[32127-1-2-0.99][32140-3-3-0.99][32263-2-3--0.11][32365-0-0-0.41][32411-2-0-0.99][32429-3-0-0.99][32473-3-0-0.99][32574-3-0-0.99][32584-0-0-0.00][32622-0-2-0.62]
[32858-3-4-0.59][32969-3-0-0.99][33016-2-2-0.55][33031-1-0-0.99][33035-2-2-0.99][33133-2-2-0.81][33173-2-1-0.99][33175-3-2-0.99][33306-3-2-0.55][33309-2-3-0.99]
[33474-0-0-0.99][33478-2-3-0.65][33618-1-1-0.99][33712-0-3-0.62][33782-2-2-0.99][33914-3-3-0.98][34076-3-2-0.50][34112-2-1-0.99][34138-2-2-0.99][34239-1-2-0.82]
[34364-2-2-0.49][34617-1-1-0.99][34751-3-3-0.99][34783-2-4-0.99][35015-3-2-0.76][35018-1-1-0.79][35288-2-0-0.61][214490-4-2-0.84][214508-4-0-0.50][214565-4-0-0.58]
[214581-4-0-0.97][214585-4-4-0.98][214625-4-4-0.84][214627-4-1-0.99][214688-4-2-0.90][214694-4-2-0.99][214815-4-1-0.99][214882-4-1-0.99][214894-4-0-0.06][214979-4-2-0.97]
[215082-4-2-0.58][215160-4-3-0.99][215171-4-4-0.99][215210-4-0-0.94][215245-4-3-0.99][215274-4-1-0.78][215365-4-1-0.99][215412-4-1-0.64][215694-4-0-0.99][215718-4-0-0.97]
[215757-4-4-0.99][215778-4-0-0.97][215911-4-0-0.99][215913-4-1-0.99][215950-4-0-0.66][215974-4-3-0.96][215988-4-0-0.43][216086-4-4-0.77][216138-4-2--0.02][216343-4-0-0.15]
[216381-4-4-0.99][216394-4-1-0.99][216840-4-0-0.90][216895-4-4-0.99][217062-4-4-0.96][217108-4-3-0.99][217130-4-4-0.99][217143-4-4-0.91][217466-4-0-0.48][217504-4-4-0.99]
[217518-4-0-0.85][217644-4-1-0.89][217736-4-0-0.98][217752-4-1-0.74][217787-4-0-0.05][217797-4-3-0.98][218000-4-1--0.02][218030-4-1-0.99][218084-4-0-0.99][218157-4-0-0.45]
[218167-4-4-0.96][218290-4-1-0.99][218293-4-4-0.95][218478-4-2-0.99][218580-4-4-0.99][218649-4-0-0.99][218799-4-0-0.99][218966-4-2-0.99][219102-4-4-0.99][219166-4-4-0.99]
[219274-4-2-0.99][219568-4-2-0.99][219588-4-3-0.33][219635-4-3-0.20][219648-4-4-0.99][219661-4-3-0.62][219668-4-0-0.99][219740-4-0-0.40][219745-4-2-0.58]
---------------------------
I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 1.227 | Acc: 43.250% | Wgt Acc: 48.804%
	I - Batch: 100 | Loss: 1.225 | Acc: 44.125% | Wgt Acc: 49.129%
	I - Batch: 150 | Loss: 1.213 | Acc: 46.875% | Wgt Acc: 51.636%
	I - Batch: 200 | Loss: 1.200 | Acc: 47.438% | Wgt Acc: 52.111%
I - num batch: 222
I - Train -- Loss: 1.200 | Acc: 47.730% | Wgt Acc: 52.285% | LR: 1.000000e-03 | Dur: 124.33s
I - Confusion Matrix: [row->prediction - col->label]
[[463.  27.  31. 161. 137.]
 [ 50. 382. 241.  46. 289.]
 [ 34. 119. 359.  47. 260.]
 [112.  27.  48. 260.  85.]
 [ 38.  23.  55.  24. 229.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.310 | Acc: 45.363% | Wgt Acc: 46.411% | Dur: 12.94s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  3.  6. 34. 16.]
 [ 1. 22.  9.  2.  7.]
 [ 5. 41. 41. 12. 25.]
 [16.  8. 10. 38.  7.]
 [ 3.  4.  9.  0. 17.]]

I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 1.203 | Acc: 45.750% | Wgt Acc: 50.693%
	I - Batch: 100 | Loss: 1.182 | Acc: 48.188% | Wgt Acc: 53.135%
	I - Batch: 150 | Loss: 1.142 | Acc: 50.042% | Wgt Acc: 54.679%
	I - Batch: 200 | Loss: 1.141 | Acc: 50.750% | Wgt Acc: 55.422%
I - num batch: 222
I - Train -- Loss: 1.139 | Acc: 51.537% | Wgt Acc: 55.998% | LR: 1.000000e-03 | Dur: 123.30s
I - Confusion Matrix: [row->prediction - col->label]
[[481.  23.  23. 145. 124.]
 [ 36. 414. 250.  44. 259.]
 [ 31.  93. 354.  41. 244.]
 [118.  26.  50. 294.  88.]
 [ 31.  22.  57.  14. 285.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.346 | Acc: 44.110% | Wgt Acc: 44.926% | Dur: 12.65s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  4.  5. 30. 14.]
 [ 1. 20.  8.  2.  5.]
 [ 4. 38. 39. 12. 28.]
 [17.  9. 12. 38.  7.]
 [ 5.  7. 11.  4. 18.]]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 1.119 | Acc: 52.500% | Wgt Acc: 57.830%
	I - Batch: 100 | Loss: 1.136 | Acc: 52.875% | Wgt Acc: 57.381%
	I - Batch: 150 | Loss: 1.141 | Acc: 52.917% | Wgt Acc: 57.401%
	I - Batch: 200 | Loss: 1.137 | Acc: 53.594% | Wgt Acc: 57.782%
I - num batch: 222
I - Train -- Loss: 1.138 | Acc: 53.623% | Wgt Acc: 57.577% | LR: 1.000000e-03 | Dur: 124.78s
I - Confusion Matrix: [row->prediction - col->label]
[[477.  26.  20. 114. 145.]
 [ 40. 395. 206.  39. 189.]
 [ 25.  91. 392.  51. 243.]
 [104.  29.  43. 311.  96.]
 [ 51.  37.  73.  23. 327.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.290 | Acc: 47.118% | Wgt Acc: 48.329% | Dur: 12.78s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  4.  7. 39. 16.]
 [ 2. 37. 21.  3. 13.]
 [ 2. 22. 26.  4. 17.]
 [13.  7. 13. 36.  4.]
 [ 4.  8.  8.  4. 22.]]

I - Local maximum validation set accuracy:  47.12

I - Validation set results: 
[14-1-1-0.84][50-3-4-0.95][124-2-2-0.25][127-0-3-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.99][615-0-3-0.99][695-1-2-0.99][722-3-3-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-0-0.59][1212-3-0-0.53][1368-0-0-0.99][2181-2-3-0.99][2476-2-0-0.58][2721-2-2-0.90][2818-1-2-0.86][2886-2-1-0.99]
[3231-2-1-0.99][3333-2-1-0.99][3482-2-2-0.85][3536-3-3-0.99][3625-1-1-0.99][3909-0-0-0.88][4035-0-0-0.99][4140-0-0-0.87][4214-1-3-0.80][4346-1-0-0.99]
[4581-2-2-0.99][4708-3-3--0.24][4838-3-2-0.18][4845-1-2--0.27][4868-0-0-0.99][4939-0-1-0.72][4984-2-3-0.99][5078-1-2-0.99][5396-0-0-0.99][5479-1-4-0.97]
[5717-0-0-0.30][5843-1-1-0.99][5949-3-0-0.99][5987-2-4-0.99][6014-3-3-0.83][6033-3-0-0.11][6313-0-0-0.99][6421-3-3-0.99][6500-1-1--0.81][6583-3-3-0.99]
[6683-3-3-0.78][6825-2-3-0.49][6998-3-3--0.11][7049-3-3-0.99][7517-1-1-0.99][7521-1-0-0.99][7528-1-3-0.99][7949-1-4-0.94][8135-1-0-0.99][8185-3-0-0.99]
[8269-3-2-0.46][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.46][8672-0-0-0.99][8903-1-3-0.64][9001-2-2-0.32][9036-2-4-0.95][9281-3-0-0.32][9300-2-2-0.89]
[9571-0-3-0.98][9617-1-1-0.68][9644-2-2-0.99][9705-2-4-0.99][9801-0-3-0.87][9803-3-3-0.91][9865-3-0-0.99][9896-2-4-0.92][10314-1-1-0.74][10337-3-0-0.99]
[10403-0-4-0.67][10653-2-1-0.17][10704-2-1-0.97][10719-1-2-0.56][10727-1-1-0.99][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.43][11088-1-2-0.92][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-3-0.63][11502-3-0-0.99][11512-3-3-0.14][11608-1-1-0.99][11610-0-0-0.89][11692-0-0-0.99][11905-0-0-0.99][11993-1-1-0.78][12002-2-0--0.01]
[12052-0-0-0.54][12201-0-0-0.99][12235-2-1-0.96][12320-1-4-0.99][12377-2-4-0.96][12398-2-2-0.52][12503-1-1-0.99][12617-0-1-0.99][12685-3-1-0.19][12738-2-3-0.76]
[12742-2-1-0.99][12823-0-0-0.99][13110-1-2-0.91][13240-3-0-0.99][13253-1-1-0.99][13273-0-0-0.99][13634-1-2-0.99][13763-2-3-0.87][13905-3-3-0.00][14060-2-1-0.96]
[14065-3-3-0.99][14147-3-0-0.69][14595-2-1-0.88][14687-2-2-0.99][14788-2-2-0.61][14869-1-1-0.87][14872-3-4-0.74][14877-1-4-0.28][14927-0-0-0.99][15066-0-0-0.99]
[15175-1-2-0.72][15178-2-0-0.99][15375-3-3-0.35][15389-3-0-0.99][15568-2-1-0.99][15675-3-3-0.99][15869-1-2-0.04][16207-3-0-0.97][16236-0-3-0.90][16302-3-0-0.99]
[16331-2-2-0.99][16381-0-0-0.99][16488-1-1-0.98][16495-0-0-0.98][16650-0-0-0.99][16719-1-2-0.30][16801-0-0-0.99][16828-0-0-0.64][17137-3-0-0.99][17245-1-4--0.22]
[17278-3-0--0.31][17282-0-0-0.99][17311-2-2-0.94][17336-2-1-0.99][17608-3-0-0.99][17627-0-0--0.27][17877-3-0-0.85][17924-1-2--0.63][17984-3-0-0.99][18211-0-3-0.56]
[18276-3-0-0.99][18287-1-1-0.56][18394-0-0-0.99][18428-0-0-0.99][18442-0-0-0.99][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.27][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-1-0.99][18824-2-4-0.99][18890-3-2-0.99][18930-3-0-0.99][18938-3-3-0.99][19817-1-2-0.97][19839-0-4-0.48][19930-3-3-0.99][19944-0-2-0.99][20036-2-2-0.92]
[20101-3-0-0.02][20474-1-1-0.99][20547-3-0-0.58][20929-2-2-0.99][21245-1-1-0.99][21257-3-0-0.66][21293-1-1-0.99][21316-1-1-0.99][21384-1-2-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.74][21714-0-0-0.97][21943-3-2-0.96][21947-0-0-0.58][21948-0-0-0.99][21965-2-1-0.99][21998-1-3-0.93][22025-0-3-0.94][22228-3-0-0.99]
[22446-1-1-0.99][22494-3-0-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-1-0.84][22985-3-0-0.99][23014-0-0-0.99][23112-1-1-0.99][23144-3-0-0.99][23168-2-0-0.94]
[23219-0-0-0.99][23363-3-0-0.99][23470-0-4--0.03][23486-2-1-0.59][23497-0-0-0.99][23516-0-0-0.99][23690-1-2-0.79][23921-2-2-0.71][23936-1-3-0.73][24040-3-4-0.80]
[24111-1-1-0.99][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.99][24364-1-2-0.01][24427-3-0-0.99][24477-2-2-0.85][24495-2-1-0.78][24893-2-2-0.77]
[25012-1-2-0.33][25121-2-4-0.99][25165-3-0-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.98][25574-2-3-0.51][25644-1-1-0.99][25718-1-1-0.77][25774-2-3-0.97]
[26032-3-3-0.89][26051-3-3-0.99][26120-0-0-0.34][26321-1-1-0.40][26732-1-1-0.17][26784-3-3-0.99][26827-3-0-0.99][26833-0-3-0.99][26838-2-1--0.09][26860-1-4-0.99]
[26948-0-0-0.23][27049-3-0-0.99][27098-1-0-0.22][27526-0-0-0.99][27639-3-0-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.79][28040-0-0-0.01][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-4-0.99][29777-0-0-0.99][29877-2-3-0.28][30035-1-1--0.04][30098-0-0-0.99][30326-1-1-0.99][30572-2-2-0.62][30716-0-0-0.99]
[30806-2-3--0.47][30906-1-1-0.99][31007-0-4-0.88][31181-3-3-0.99][31238-0-3-0.99][31347-0-0-0.99][31422-2-1-0.87][31429-3-0-0.03][31431-0-3-0.85][31432-1-1-0.98]
[31477-0-0-0.99][31524-1-3-0.08][31597-1-1-0.98][31619-1-2-0.71][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.59][32074-1-2-0.25][32078-3-3-0.99][32111-1-4-0.95]
[32127-1-2-0.99][32140-3-3-0.99][32263-2-0-0.55][32365-0-3-0.57][32411-2-0-0.99][32429-3-3-0.99][32473-3-0-0.99][32574-3-0-0.99][32584-0-0-0.99][32622-0-2-0.21]
[32858-3-0-0.95][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.99][33133-2-1-0.45][33173-2-1-0.82][33175-3-1-0.99][33306-3-3-0.98][33309-2-3-0.90]
[33474-0-0--0.66][33478-2-3--0.03][33618-1-1-0.97][33712-0-3-0.09][33782-2-2-0.99][33914-3-3-0.99][34076-3-3-0.80][34112-2-1-0.99][34138-2-2-0.99][34239-1-2--0.28]
[34364-2-1-0.99][34617-1-2-0.64][34751-3-3-0.99][34783-2-4-0.95][35015-3-3-0.85][35018-1-4-0.99][35288-2-3-0.61][214490-4-2-0.96][214508-4-4-0.14][214565-4-0--0.31]
[214581-4-2--0.03][214585-4-4-0.99][214625-4-4-0.03][214627-4-4-0.99][214688-4-2-0.99][214694-4-2-0.99][214815-4-1-0.99][214882-4-1--0.57][214894-4-0--0.31][214979-4-2-0.97]
[215082-4-4-0.54][215160-4-0-0.90][215171-4-4-0.99][215210-4-0-0.92][215245-4-3-0.99][215274-4-1-0.87][215365-4-1-0.99][215412-4-4-0.70][215694-4-2-0.15][215718-4-0-0.86]
[215757-4-4-0.99][215778-4-3-0.99][215911-4-3-0.82][215913-4-1-0.99][215950-4-0-0.81][215974-4-0-0.63][215988-4-0-0.17][216086-4-4-0.20][216138-4-2-0.61][216343-4-4-0.62]
[216381-4-4-0.99][216394-4-1-0.84][216840-4-4-0.31][216895-4-4-0.99][217062-4-4-0.99][217108-4-0-0.99][217130-4-2-0.99][217143-4-4-0.99][217466-4-2--0.02][217504-4-1-0.77]
[217518-4-0-0.80][217644-4-1-0.99][217736-4-0-0.97][217752-4-1-0.93][217787-4-4-0.99][217797-4-3-0.72][218000-4-1-0.77][218030-4-1-0.99][218084-4-0-0.99][218157-4-4-0.90]
[218167-4-2-0.99][218290-4-1-0.99][218293-4-4-0.99][218478-4-2-0.74][218580-4-4-0.99][218649-4-1--0.39][218799-4-0-0.88][218966-4-2-0.99][219102-4-4-0.65][219166-4-4-0.99]
[219274-4-2-0.99][219568-4-4-0.99][219588-4-2-0.24][219635-4-0-0.61][219648-4-2-0.28][219661-4-0-0.68][219668-4-2--0.25][219740-4-0-0.47][219745-4-2-0.76]
---------------------------
I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 1.104 | Acc: 54.000% | Wgt Acc: 58.814%
	I - Batch: 100 | Loss: 1.097 | Acc: 54.625% | Wgt Acc: 59.140%
	I - Batch: 150 | Loss: 1.105 | Acc: 53.667% | Wgt Acc: 58.446%
	I - Batch: 200 | Loss: 1.104 | Acc: 53.750% | Wgt Acc: 58.133%
I - num batch: 222
I - Train -- Loss: 1.108 | Acc: 53.454% | Wgt Acc: 57.810% | LR: 1.000000e-03 | Dur: 123.38s
I - Confusion Matrix: [row->prediction - col->label]
[[491.  28.  24. 141. 131.]
 [ 35. 397. 196.  35. 235.]
 [ 24.  82. 413.  42. 247.]
 [100.  34.  35. 300.  92.]
 [ 47.  37.  66.  20. 295.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.319 | Acc: 43.358% | Wgt Acc: 44.988% | Dur: 12.37s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  4.  7. 34. 14.]
 [ 5. 39. 24. 13. 18.]
 [ 5. 29. 35. 14. 24.]
 [12.  3.  1. 24.  2.]
 [ 5.  3.  8.  1. 14.]]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 1.094 | Acc: 56.875% | Wgt Acc: 60.632%
	I - Batch: 100 | Loss: 1.088 | Acc: 57.500% | Wgt Acc: 61.127%
	I - Batch: 150 | Loss: 1.083 | Acc: 57.417% | Wgt Acc: 61.112%
	I - Batch: 200 | Loss: 1.082 | Acc: 57.062% | Wgt Acc: 61.066%
I - num batch: 222
I - Train -- Loss: 1.082 | Acc: 57.203% | Wgt Acc: 61.192% | LR: 1.000000e-03 | Dur: 123.17s
I - Confusion Matrix: [row->prediction - col->label]
[[504.  22.  22. 128. 144.]
 [ 26. 422. 185.  32. 211.]
 [ 17.  72. 426.  37. 203.]
 [101.  24.  43. 319.  84.]
 [ 49.  38.  58.  22. 358.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.329 | Acc: 48.371% | Wgt Acc: 47.463% | Dur: 12.84s
I - Confusion Matrix: [row->prediction - col->label]
[[59. 10.  4. 18.  6.]
 [ 1. 20.  6.  0.  2.]
 [ 3. 20. 31. 11. 20.]
 [15. 10. 14. 47.  8.]
 [10. 18. 20. 10. 36.]]

I - Local maximum validation set accuracy:  48.37

I - Validation set results: 
[14-1-4-0.16][50-3-4-0.99][124-2-2-0.99][127-0-3-0.71][443-2-2-0.99][567-0-0-0.99][573-1-1-0.99][615-0-3-0.99][695-1-0-0.99][722-3-3-0.99]
[826-0-0-0.98][878-0-3-0.99][1103-0-0-0.87][1212-3-4-0.99][1368-0-0-0.97][2181-2-3-0.75][2476-2-0-0.76][2721-2-2-0.89][2818-1-4-0.86][2886-2-4-0.97]
[3231-2-2-0.99][3333-2-1-0.46][3482-2-2-0.99][3536-3-3-0.99][3625-1-1-0.99][3909-0-0-0.87][4035-0-0-0.99][4140-0-0-0.99][4214-1-3-0.99][4346-1-3-0.99]
[4581-2-2-0.99][4708-3-2-0.99][4838-3-2-0.95][4845-1-3-0.22][4868-0-0-0.85][4939-0-4-0.95][4984-2-2-0.99][5078-1-4-0.99][5396-0-0-0.99][5479-1-1-0.85]
[5717-0-0-0.85][5843-1-1-0.99][5949-3-3-0.99][5987-2-4-0.99][6014-3-3--0.02][6033-3-0--0.04][6313-0-3-0.99][6421-3-3-0.99][6500-1-2-0.21][6583-3-3-0.78]
[6683-3-3-0.99][6825-2-3-0.84][6998-3-3-0.63][7049-3-3-0.96][7517-1-1-0.99][7521-1-0-0.99][7528-1-4-0.51][7949-1-2-0.99][8135-1-4-0.99][8185-3-0-0.99]
[8269-3-4-0.69][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.59][8903-1-3-0.99][9001-2-4-0.91][9036-2-2-0.99][9281-3-4-0.99][9300-2-2-0.99]
[9571-0-3-0.95][9617-1-4-0.03][9644-2-2-0.99][9705-2-4-0.99][9801-0-4-0.54][9803-3-3-0.99][9865-3-3-0.99][9896-2-4-0.99][10314-1-0-0.77][10337-3-0-0.98]
[10403-0-4-0.68][10653-2-1-0.85][10704-2-2-0.99][10719-1-4-0.91][10727-1-1-0.97][10836-0-0-0.99][10969-2-3-0.99][11042-0-2--0.63][11088-1-1-0.75][11322-0-0-0.93]
[11398-2-2-0.85][11499-0-3-0.99][11502-3-3-0.99][11512-3-3-0.39][11608-1-1-0.99][11610-0-0-0.99][11692-0-0-0.99][11905-0-0-0.99][11993-1-0-0.98][12002-2-4-0.67]
[12052-0-0-0.85][12201-0-3-0.99][12235-2-4-0.78][12320-1-4-0.99][12377-2-4-0.99][12398-2-4-0.99][12503-1-2-0.99][12617-0-1-0.99][12685-3-3-0.91][12738-2-3-0.99]
[12742-2-3-0.88][12823-0-3-0.99][13110-1-2-0.91][13240-3-4-0.99][13253-1-4-0.99][13273-0-0-0.99][13634-1-2-0.99][13763-2-3-0.79][13905-3-3-0.99][14060-2-4-0.98]
[14065-3-3-0.99][14147-3-0-0.74][14595-2-4-0.20][14687-2-2-0.99][14788-2-2-0.99][14869-1-2-0.40][14872-3-4-0.99][14877-1-4--0.31][14927-0-0-0.99][15066-0-0-0.99]
[15175-1-4-0.99][15178-2-3--0.17][15375-3-0-0.96][15389-3-0-0.99][15568-2-4-0.99][15675-3-3-0.88][15869-1-2-0.83][16207-3-0-0.99][16236-0-3-0.59][16302-3-2-0.99]
[16331-2-2-0.91][16381-0-0-0.27][16488-1-0-0.17][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.99][16801-0-0-0.99][16828-0-0-0.84][17137-3-3-0.99][17245-1-4-0.47]
[17278-3-2--0.11][17282-0-0-0.99][17311-2-2-0.99][17336-2-2-0.55][17608-3-3-0.99][17627-0-0--0.06][17877-3-4-0.92][17924-1-3-0.77][17984-3-0-0.99][18211-0-3-0.65]
[18276-3-0-0.99][18287-1-0-0.51][18394-0-0-0.94][18428-0-0--0.13][18442-0-0-0.99][18478-3-3-0.99][18607-0-0-0.99][18616-0-4-0.83][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.99][18824-2-4-0.99][18890-3-2-0.99][18930-3-4-0.99][18938-3-3-0.99][19817-1-2-0.99][19839-0-4-0.73][19930-3-0-0.23][19944-0-2-0.99][20036-2-2-0.99]
[20101-3-3-0.64][20474-1-1--0.11][20547-3-0-0.93][20929-2-4-0.10][21245-1-0-0.23][21257-3-2-0.92][21293-1-1-0.95][21316-1-3-0.21][21384-1-2-0.99][21448-1-2-0.68]
[21483-0-0-0.99][21487-2-3-0.57][21714-0-0-0.52][21943-3-2-0.96][21947-0-4-0.90][21948-0-0-0.99][21965-2-2-0.82][21998-1-3-0.99][22025-0-2-0.99][22228-3-0-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-2-0.96][22985-3-3-0.99][23014-0-3-0.99][23112-1-2-0.88][23144-3-0-0.99][23168-2-0-0.94]
[23219-0-0-0.58][23363-3-3-0.99][23470-0-0-0.58][23486-2-1-0.26][23497-0-0-0.99][23516-0-0-0.99][23690-1-2-0.99][23921-2-4-0.61][23936-1-2-0.99][24040-3-4-0.44]
[24111-1-4-0.99][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.54][24364-1-2-0.99][24427-3-0-0.89][24477-2-4-0.99][24495-2-1-0.14][24893-2-2-0.99]
[25012-1-2-0.32][25121-2-4-0.99][25165-3-3-0.62][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.99][25644-1-1-0.81][25718-1-1-0.79][25774-2-3-0.61]
[26032-3-3-0.93][26051-3-3-0.99][26120-0-0-0.42][26321-1-1--0.06][26732-1-2--0.17][26784-3-3-0.99][26827-3-3-0.39][26833-0-3-0.99][26838-2-0-0.25][26860-1-4-0.99]
[26948-0-0-0.26][27049-3-0-0.99][27098-1-0-0.99][27526-0-0-0.99][27639-3-2-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.37][28040-0-4-0.22][28503-2-2-0.99]
[28577-1-1-0.68][28959-0-0-0.99][29198-3-4-0.99][29777-0-0-0.99][29877-2-2-0.66][30035-1-4-0.84][30098-0-0-0.99][30326-1-1-0.99][30572-2-2-0.99][30716-0-4-0.99]
[30806-2-3--0.03][30906-1-4-0.99][31007-0-0-0.29][31181-3-3-0.99][31238-0-0-0.99][31347-0-0-0.99][31422-2-4-0.97][31429-3-3-0.77][31431-0-3-0.88][31432-1-0-0.04]
[31477-0-3-0.99][31524-1-3-0.37][31597-1-2-0.98][31619-1-3-0.75][31701-0-0-0.95][31755-0-0-0.99][31854-3-3-0.44][32074-1-2-0.36][32078-3-3-0.99][32111-1-1-0.74]
[32127-1-2-0.99][32140-3-3-0.99][32263-2-3-0.34][32365-0-0-0.99][32411-2-3-0.99][32429-3-3-0.99][32473-3-3-0.99][32574-3-0-0.99][32584-0-0-0.60][32622-0-4-0.68]
[32858-3-0-0.99][32969-3-3-0.82][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.99][33133-2-2-0.40][33173-2-1-0.71][33175-3-2-0.99][33306-3-3-0.99][33309-2-3-0.99]
[33474-0-4--0.33][33478-2-3-0.80][33618-1-1-0.99][33712-0-3-0.57][33782-2-2-0.99][33914-3-2-0.90][34076-3-3-0.65][34112-2-2-0.99][34138-2-2-0.99][34239-1-0--0.15]
[34364-2-1-0.70][34617-1-2-0.98][34751-3-3-0.99][34783-2-4-0.96][35015-3-3-0.91][35018-1-4-0.42][35288-2-4-0.35][214490-4-2-0.98][214508-4-4-0.99][214565-4-4-0.74]
[214581-4-4--0.02][214585-4-4-0.99][214625-4-4-0.99][214627-4-4-0.99][214688-4-4-0.99][214694-4-2-0.99][214815-4-4-0.99][214882-4-4--0.32][214894-4-4-0.02][214979-4-2-0.54]
[215082-4-2-0.97][215160-4-0-0.99][215171-4-4-0.99][215210-4-3-0.34][215245-4-3-0.99][215274-4-3-0.99][215365-4-2-0.70][215412-4-4-0.97][215694-4-4-0.04][215718-4-0--0.17]
[215757-4-4-0.99][215778-4-3-0.07][215911-4-4-0.89][215913-4-4-0.99][215950-4-2-0.98][215974-4-4-0.69][215988-4-0-0.31][216086-4-4-0.99][216138-4-0-0.28][216343-4-4-0.99]
[216381-4-2-0.99][216394-4-4-0.46][216840-4-4-0.79][216895-4-4-0.99][217062-4-2-0.99][217108-4-2-0.00][217130-4-2-0.99][217143-4-4-0.99][217466-4-4-0.52][217504-4-4-0.99]
[217518-4-1-0.79][217644-4-4-0.99][217736-4-3-0.99][217752-4-2-0.99][217787-4-4-0.99][217797-4-3-0.99][218000-4-4-0.73][218030-4-1-0.76][218084-4-3-0.98][218157-4-4-0.98]
[218167-4-2-0.99][218290-4-4-0.99][218293-4-2-0.99][218478-4-2-0.42][218580-4-4-0.99][218649-4-2-0.63][218799-4-4-0.99][218966-4-2-0.99][219102-4-4-0.22][219166-4-2-0.99]
[219274-4-2-0.99][219568-4-4-0.66][219588-4-4-0.47][219635-4-3-0.99][219648-4-4-0.99][219661-4-0-0.43][219668-4-2-0.98][219740-4-0-0.32][219745-4-2-0.99]
---------------------------
I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 1.005 | Acc: 62.625% | Wgt Acc: 63.773%
	I - Batch: 100 | Loss: 0.991 | Acc: 63.750% | Wgt Acc: 65.139%
	I - Batch: 150 | Loss: 0.976 | Acc: 64.875% | Wgt Acc: 66.490%
I - num batch: 173
I - Train -- Loss: 0.976 | Acc: 64.834% | Wgt Acc: 66.672% | LR: 1.000000e-03 | Dur: 97.12s
I - Confusion Matrix: [row->prediction - col->label]
[[552.  22.  19. 148.  39.]
 [ 32. 428. 173.  37.  53.]
 [ 24.  93. 480.  53.  75.]
 [ 82.  28.  42. 298.  16.]
 [  7.   7.  20.   2.  34.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.297 | Acc: 46.617% | Wgt Acc: 50.557% | Dur: 13.60s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  2.  5. 35. 18.]
 [ 2. 48. 29.  3. 17.]
 [ 5. 24. 34. 13. 29.]
 [14.  3.  5. 35.  3.]
 [ 3.  1.  2.  0.  5.]]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 1.018 | Acc: 59.750% | Wgt Acc: 65.637%
	I - Batch: 100 | Loss: 1.010 | Acc: 60.562% | Wgt Acc: 65.920%
	I - Batch: 150 | Loss: 1.010 | Acc: 60.583% | Wgt Acc: 65.408%
	I - Batch: 200 | Loss: 1.012 | Acc: 60.938% | Wgt Acc: 65.269%
I - num batch: 222
I - Train -- Loss: 1.010 | Acc: 61.066% | Wgt Acc: 65.274% | LR: 5.000000e-04 | Dur: 125.69s
I - Confusion Matrix: [row->prediction - col->label]
[[516.  18.  13. 110. 147.]
 [ 19. 432. 140.  28. 184.]
 [ 21.  70. 481.  32. 203.]
 [105.  22.  40. 354.  83.]
 [ 36.  36.  60.  14. 383.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.263 | Acc: 49.624% | Wgt Acc: 49.691% | Dur: 12.45s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  4. 28.  6.]
 [ 4. 40. 19.  2. 10.]
 [ 0. 12. 19.  5.  9.]
 [13.  5. 12. 41. 12.]
 [ 8. 16. 21. 10. 35.]]

I - Local maximum validation set accuracy:  49.62

I - Validation set results: 
[14-1-1-0.56][50-3-4-0.87][124-2-2-0.98][127-0-3-0.87][443-2-2-0.99][567-0-0-0.92][573-1-1-0.99][615-0-3-0.98][695-1-0-0.91][722-3-3-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-4-0.45][1212-3-4-0.53][1368-0-0-0.99][2181-2-0-0.41][2476-2-0-0.99][2721-2-4-0.99][2818-1-4-0.60][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-1-0.99][3482-2-2-0.99][3536-3-3-0.99][3625-1-1-0.99][3909-0-0-0.96][4035-0-0-0.99][4140-0-0-0.94][4214-1-3-0.97][4346-1-3-0.99]
[4581-2-2-0.97][4708-3-2-0.37][4838-3-3-0.01][4845-1-1-0.08][4868-0-0-0.99][4939-0-4-0.68][4984-2-2-0.89][5078-1-4-0.96][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.91][5843-1-1-0.99][5949-3-3-0.99][5987-2-4-0.99][6014-3-3-0.55][6033-3-0-0.26][6313-0-3-0.99][6421-3-3-0.97][6500-1-1--0.35][6583-3-3--0.21]
[6683-3-3-0.83][6825-2-3-0.98][6998-3-0-0.43][7049-3-3-0.83][7517-1-1-0.99][7521-1-0-0.99][7528-1-3-0.78][7949-1-4-0.99][8135-1-0-0.99][8185-3-0-0.99]
[8269-3-4-0.46][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.99][8903-1-2-0.99][9001-2-4-0.98][9036-2-1-0.99][9281-3-4-0.68][9300-2-4-0.99]
[9571-0-3-0.30][9617-1-1-0.74][9644-2-3-0.73][9705-2-4-0.99][9801-0-0-0.58][9803-3-3-0.99][9865-3-3-0.99][9896-2-4-0.99][10314-1-4-0.98][10337-3-0-0.99]
[10403-0-4-0.97][10653-2-1-0.99][10704-2-1-0.99][10719-1-1-0.99][10727-1-4-0.99][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.49][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.84][11499-0-3-0.98][11502-3-3-0.99][11512-3-3-0.63][11608-1-1-0.99][11610-0-0-0.99][11692-0-0-0.99][11905-0-0-0.99][11993-1-1-0.79][12002-2-0-0.99]
[12052-0-0-0.94][12201-0-3-0.99][12235-2-1-0.99][12320-1-4-0.99][12377-2-4-0.99][12398-2-2-0.89][12503-1-2-0.99][12617-0-1-0.99][12685-3-3-0.84][12738-2-3-0.98]
[12742-2-1-0.89][12823-0-0-0.99][13110-1-2-0.68][13240-3-0-0.93][13253-1-4-0.99][13273-0-0-0.99][13634-1-2-0.71][13763-2-3-0.82][13905-3-3-0.56][14060-2-4-0.87]
[14065-3-0-0.99][14147-3-1--0.01][14595-2-4-0.99][14687-2-1-0.99][14788-2-2-0.51][14869-1-1-0.99][14872-3-4-0.99][14877-1-4-0.28][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-4-0.78][15178-2-4-0.43][15375-3-0-0.30][15389-3-3-0.99][15568-2-4-0.74][15675-3-3-0.99][15869-1-3-0.11][16207-3-0-0.99][16236-0-0-0.31][16302-3-0-0.99]
[16331-2-2-0.91][16381-0-0-0.81][16488-1-1-0.98][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.85][16801-0-0-0.99][16828-0-0-0.99][17137-3-3-0.89][17245-1-4--0.07]
[17278-3-4-0.51][17282-0-0-0.99][17311-2-2-0.96][17336-2-1-0.99][17608-3-0-0.99][17627-0-0-0.30][17877-3-4-0.80][17924-1-4--0.16][17984-3-0-0.99][18211-0-3-0.56]
[18276-3-0-0.99][18287-1-1-0.86][18394-0-0-0.99][18428-0-0-0.53][18442-0-0-0.98][18478-3-3-0.99][18607-0-0-0.90][18616-0-1--0.07][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-1-0.99][18824-2-4-0.99][18890-3-2-0.53][18930-3-4-0.99][18938-3-3-0.24][19817-1-2-0.99][19839-0-4-0.99][19930-3-0-0.69][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-3-0.99][20474-1-1-0.88][20547-3-0--0.05][20929-2-2-0.99][21245-1-1-0.99][21257-3-4--0.03][21293-1-1-0.99][21316-1-1--0.16][21384-1-4-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-4-0.99][21714-0-0-0.31][21943-3-2-0.80][21947-0-4-0.76][21948-0-0-0.99][21965-2-2-0.99][21998-1-0-0.19][22025-0-3-0.84][22228-3-0-0.99]
[22446-1-1-0.99][22494-3-0-0.99][22757-0-0-0.99][22811-3-0-0.99][22976-3-2-0.65][22985-3-0-0.99][23014-0-0-0.99][23112-1-1-0.99][23144-3-0-0.99][23168-2-3-0.92]
[23219-0-0-0.90][23363-3-3-0.99][23470-0-1-0.13][23486-2-3-0.69][23497-0-0-0.99][23516-0-0-0.99][23690-1-1-0.99][23921-2-1-0.94][23936-1-2-0.94][24040-3-4-0.99]
[24111-1-1-0.99][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.74][24364-1-2-0.07][24427-3-0-0.67][24477-2-4-0.99][24495-2-1-0.78][24893-2-1-0.87]
[25012-1-2-0.03][25121-2-4-0.99][25165-3-3-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-3-0.93][25574-2-3-0.83][25644-1-1-0.71][25718-1-1-0.19][25774-2-3--0.22]
[26032-3-3-0.69][26051-3-3-0.99][26120-0-4-0.97][26321-1-2-0.41][26732-1-1-0.48][26784-3-3-0.99][26827-3-3-0.85][26833-0-3-0.99][26838-2-1--0.01][26860-1-4-0.99]
[26948-0-0-0.95][27049-3-0-0.99][27098-1-0-0.79][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.81][28040-0-0-0.71][28503-2-4-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-0--0.11][29777-0-0-0.99][29877-2-2-0.72][30035-1-4-0.63][30098-0-0-0.99][30326-1-1-0.99][30572-2-2-0.69][30716-0-4-0.99]
[30806-2-4-0.83][30906-1-1-0.99][31007-0-0-0.97][31181-3-3-0.94][31238-0-0-0.96][31347-0-0-0.99][31422-2-4-0.99][31429-3-3--0.15][31431-0-3-0.88][31432-1-1--0.00]
[31477-0-0-0.99][31524-1-4-0.05][31597-1-2-0.95][31619-1-2-0.60][31701-0-0-0.99][31755-0-0-0.98][31854-3-3-0.52][32074-1-1--0.03][32078-3-3-0.99][32111-1-1-0.75]
[32127-1-2-0.91][32140-3-3-0.99][32263-2-4-0.01][32365-0-0-0.59][32411-2-3-0.99][32429-3-0-0.99][32473-3-0-0.99][32574-3-0-0.99][32584-0-0-0.99][32622-0-1-0.38]
[32858-3-0-0.99][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.99][33133-2-1-0.74][33173-2-1-0.09][33175-3-1-0.99][33306-3-2-0.68][33309-2-3-0.34]
[33474-0-0-0.99][33478-2-3-0.47][33618-1-1-0.99][33712-0-3-0.71][33782-2-1-0.99][33914-3-3-0.99][34076-3-3-0.55][34112-2-1-0.99][34138-2-2-0.99][34239-1-1-0.59]
[34364-2-1-0.69][34617-1-1-0.64][34751-3-3-0.99][34783-2-4-0.99][35015-3-3-0.96][35018-1-1-0.99][35288-2-4-0.28][214490-4-2-0.88][214508-4-4-0.67][214565-4-1-0.85]
[214581-4-4-0.20][214585-4-4-0.99][214625-4-4-0.99][214627-4-4-0.37][214688-4-4-0.99][214694-4-1-0.99][214815-4-4-0.99][214882-4-3--0.38][214894-4-4--0.52][214979-4-4-0.91]
[215082-4-2-0.94][215160-4-0-0.99][215171-4-4-0.99][215210-4-4-0.99][215245-4-3-0.99][215274-4-3-0.99][215365-4-4-0.99][215412-4-4-0.43][215694-4-4--0.21][215718-4-0-0.32]
[215757-4-4-0.99][215778-4-3-0.56][215911-4-4-0.86][215913-4-1-0.99][215950-4-3-0.83][215974-4-4-0.99][215988-4-0-0.48][216086-4-4-0.99][216138-4-1--0.04][216343-4-4-0.99]
[216381-4-4-0.99][216394-4-4-0.97][216840-4-4-0.73][216895-4-4-0.99][217062-4-2-0.92][217108-4-0-0.65][217130-4-2-0.99][217143-4-4-0.99][217466-4-4-0.65][217504-4-1-0.99]
[217518-4-1-0.87][217644-4-1-0.99][217736-4-3-0.68][217752-4-1-0.99][217787-4-4--0.03][217797-4-3-0.56][218000-4-4-0.89][218030-4-1-0.99][218084-4-3-0.99][218157-4-4-0.90]
[218167-4-2-0.99][218290-4-2-0.99][218293-4-4-0.93][218478-4-1-0.99][218580-4-4-0.99][218649-4-4--0.46][218799-4-4-0.76][218966-4-2-0.99][219102-4-3--0.61][219166-4-4-0.99]
[219274-4-2-0.99][219568-4-4-0.99][219588-4-3-0.58][219635-4-3-0.99][219648-4-4-0.99][219661-4-0-0.93][219668-4-3--0.51][219740-4-0-0.54][219745-4-2-0.50]
---------------------------
I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 0.981 | Acc: 64.875% | Wgt Acc: 68.579%
	I - Batch: 100 | Loss: 0.987 | Acc: 64.188% | Wgt Acc: 67.633%
	I - Batch: 150 | Loss: 0.995 | Acc: 63.125% | Wgt Acc: 66.449%
	I - Batch: 200 | Loss: 0.996 | Acc: 63.094% | Wgt Acc: 66.065%
I - num batch: 222
I - Train -- Loss: 0.992 | Acc: 63.378% | Wgt Acc: 66.416% | LR: 5.000000e-04 | Dur: 124.32s
I - Confusion Matrix: [row->prediction - col->label]
[[515.  12.  17. 122. 128.]
 [ 23. 448. 128.  17. 127.]
 [ 22.  57. 482.  38. 200.]
 [ 86.  21.  35. 334.  76.]
 [ 51.  40.  72.  27. 469.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.302 | Acc: 46.867% | Wgt Acc: 47.092% | Dur: 12.91s
I - Confusion Matrix: [row->prediction - col->label]
[[50.  3.  3. 20.  5.]
 [ 9. 47. 27. 14. 18.]
 [ 2. 11. 25.  8. 12.]
 [11.  4.  4. 32.  4.]
 [16. 13. 16. 12. 33.]]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 0.967 | Acc: 63.875% | Wgt Acc: 66.532%
	I - Batch: 100 | Loss: 0.983 | Acc: 62.688% | Wgt Acc: 65.673%
	I - Batch: 150 | Loss: 0.978 | Acc: 64.250% | Wgt Acc: 67.225%
	I - Batch: 200 | Loss: 0.981 | Acc: 64.125% | Wgt Acc: 67.092%
I - num batch: 222
I - Train -- Loss: 0.979 | Acc: 64.364% | Wgt Acc: 67.356% | LR: 5.000000e-04 | Dur: 123.65s
I - Confusion Matrix: [row->prediction - col->label]
[[519.  16.  12. 120. 138.]
 [ 21. 437. 122.  24. 136.]
 [ 25.  71. 504.  30. 178.]
 [ 80.  10.  28. 346.  71.]
 [ 52.  44.  68.  18. 477.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.224 | Acc: 52.130% | Wgt Acc: 53.899% | Dur: 12.24s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  6. 27. 15.]
 [ 3. 43. 21.  4. 11.]
 [ 1. 11. 25.  3. 11.]
 [14. 11. 11. 50.  8.]
 [ 7.  9. 12.  2. 27.]]

I - Local maximum validation set accuracy:  52.13

I - Validation set results: 
[14-1-1-0.39][50-3-3-0.37][124-2-2-0.01][127-0-0-0.89][443-2-2-0.99][567-0-0-0.99][573-1-1-0.99][615-0-3-0.98][695-1-0-0.07][722-3-3-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-0-0.60][1212-3-3--0.13][1368-0-0-0.99][2181-2-2--0.10][2476-2-0-0.62][2721-2-2-0.74][2818-1-3--0.38][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-1-0.98][3482-2-2-0.40][3536-3-3-0.99][3625-1-1-0.99][3909-0-0-0.72][4035-0-0-0.99][4140-0-0-0.99][4214-1-3-0.76][4346-1-4--0.01]
[4581-2-2-0.99][4708-3-3-0.08][4838-3-0--0.73][4845-1-1-0.50][4868-0-0-0.99][4939-0-2--0.26][4984-2-2-0.74][5078-1-4--0.24][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.51][5843-1-1-0.91][5949-3-3-0.84][5987-2-4-0.99][6014-3-3-0.75][6033-3-0-0.61][6313-0-3-0.91][6421-3-3-0.99][6500-1-3--0.55][6583-3-3-0.43]
[6683-3-3-0.99][6825-2-1-0.46][6998-3-3-0.53][7049-3-3-0.99][7517-1-1-0.99][7521-1-1-0.73][7528-1-3-0.99][7949-1-2-0.45][8135-1-0-0.99][8185-3-0-0.99]
[8269-3-2--0.47][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.99][8903-1-2-0.62][9001-2-4-0.91][9036-2-4-0.85][9281-3-1-0.39][9300-2-2-0.81]
[9571-0-3-0.85][9617-1-1-0.56][9644-2-2-0.20][9705-2-4-0.99][9801-0-3-0.44][9803-3-3-0.99][9865-3-3-0.99][9896-2-4-0.99][10314-1-1-0.99][10337-3-0-0.89]
[10403-0-4--0.19][10653-2-1-0.63][10704-2-1-0.96][10719-1-1-0.99][10727-1-1-0.98][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.96][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-3-0.98][11502-3-0-0.99][11512-3-3-0.26][11608-1-1-0.99][11610-0-0-0.97][11692-0-0-0.99][11905-0-0-0.99][11993-1-1-0.85][12002-2-0-0.66]
[12052-0-0-0.96][12201-0-0-0.99][12235-2-1-0.99][12320-1-4-0.99][12377-2-4-0.77][12398-2-2-0.06][12503-1-2-0.89][12617-0-1-0.99][12685-3-3-0.34][12738-2-3-0.57]
[12742-2-1-0.39][12823-0-0-0.99][13110-1-1-0.75][13240-3-0-0.95][13253-1-1-0.99][13273-0-0-0.99][13634-1-3-0.24][13763-2-3-0.11][13905-3-3-0.54][14060-2-1-0.93]
[14065-3-0-0.99][14147-3-3-0.48][14595-2-1-0.97][14687-2-2-0.96][14788-2-2-0.18][14869-1-1-0.99][14872-3-0-0.29][14877-1-1-0.43][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-4-0.22][15178-2-3-0.57][15375-3-3-0.98][15389-3-3-0.99][15568-2-1-0.99][15675-3-3-0.99][15869-1-1-0.25][16207-3-0-0.99][16236-0-3-0.51][16302-3-0-0.79]
[16331-2-2-0.99][16381-0-0-0.99][16488-1-1-0.86][16495-0-0-0.99][16650-0-0-0.99][16719-1-2--0.64][16801-0-0-0.99][16828-0-0-0.84][17137-3-0-0.99][17245-1-4--0.06]
[17278-3-0--0.14][17282-0-0-0.86][17311-2-2-0.99][17336-2-3-0.19][17608-3-0-0.99][17627-0-0-0.51][17877-3-4-0.60][17924-1-3--0.16][17984-3-0-0.99][18211-0-3-0.86]
[18276-3-0-0.99][18287-1-1-0.68][18394-0-0-0.99][18428-0-0-0.84][18442-0-0-0.99][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.16][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-1-0.99][18824-2-4-0.59][18890-3-2--0.07][18930-3-0--0.02][18938-3-3-0.99][19817-1-2-0.55][19839-0-4-0.98][19930-3-3-0.99][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-1-0.37][20474-1-1-0.99][20547-3-0-0.39][20929-2-2-0.81][21245-1-1-0.99][21257-3-3--0.09][21293-1-1-0.99][21316-1-3-0.10][21384-1-2-0.93][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.61][21714-0-0-0.82][21943-3-1-0.86][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.99][21998-1-3-0.98][22025-0-3--0.44][22228-3-0-0.99]
[22446-1-1-0.99][22494-3-3-0.98][22757-0-0-0.99][22811-3-3-0.99][22976-3-3--0.68][22985-3-3-0.99][23014-0-0-0.99][23112-1-1-0.90][23144-3-0-0.99][23168-2-0-0.47]
[23219-0-0-0.99][23363-3-3-0.99][23470-0-1-0.05][23486-2-3-0.72][23497-0-0-0.99][23516-0-0-0.99][23690-1-1--0.08][23921-2-4-0.25][23936-1-2-0.91][24040-3-4-0.99]
[24111-1-4-0.99][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.99][24364-1-2-0.06][24427-3-0-0.99][24477-2-4-0.99][24495-2-1-0.48][24893-2-1-0.99]
[25012-1-2--0.35][25121-2-4-0.99][25165-3-3-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-3-0.99][25574-2-3-0.90][25644-1-1-0.40][25718-1-0--0.38][25774-2-2-0.46]
[26032-3-3-0.96][26051-3-3-0.99][26120-0-4-0.47][26321-1-1-0.99][26732-1-1-0.25][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-1--0.32][26860-1-4-0.99]
[26948-0-0-0.03][27049-3-0-0.99][27098-1-3-0.21][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.69][28040-0-4-0.98][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-0-0.05][29777-0-0-0.99][29877-2-1-0.39][30035-1-1-0.99][30098-0-3-0.99][30326-1-1-0.99][30572-2-3-0.81][30716-0-4-0.99]
[30806-2-3-0.44][30906-1-1-0.94][31007-0-4-0.60][31181-3-3-0.99][31238-0-0-0.95][31347-0-0-0.99][31422-2-1-0.75][31429-3-3-0.28][31431-0-3-0.92][31432-1-1-0.96]
[31477-0-0-0.99][31524-1-3--0.14][31597-1-1-0.69][31619-1-0-0.04][31701-0-0-0.99][31755-0-1--0.18][31854-3-3-0.94][32074-1-2--0.08][32078-3-3-0.99][32111-1-1-0.99]
[32127-1-2-0.58][32140-3-3-0.99][32263-2-0--0.63][32365-0-0-0.99][32411-2-0-0.99][32429-3-0-0.99][32473-3-0-0.99][32574-3-0-0.99][32584-0-0-0.97][32622-0-0-0.07]
[32858-3-0-0.99][32969-3-3-0.99][33016-2-1-0.99][33031-1-3-0.99][33035-2-2-0.99][33133-2-2-0.15][33173-2-1-0.99][33175-3-2-0.99][33306-3-1-0.95][33309-2-3-0.77]
[33474-0-0-0.36][33478-2-3-0.56][33618-1-1-0.99][33712-0-3-0.81][33782-2-1-0.99][33914-3-3-0.99][34076-3-3-0.90][34112-2-1-0.99][34138-2-2-0.99][34239-1-1-0.85]
[34364-2-1-0.99][34617-1-4--0.46][34751-3-3-0.99][34783-2-4-0.98][35015-3-3-0.99][35018-1-4-0.95][35288-2-4--0.33][214490-4-4-0.76][214508-4-4-0.95][214565-4-1-0.39]
[214581-4-0--0.51][214585-4-4-0.80][214625-4-4-0.38][214627-4-4-0.48][214688-4-2-0.89][214694-4-2-0.99][214815-4-4-0.76][214882-4-1-0.04][214894-4-3--0.36][214979-4-2-0.04]
[215082-4-4-0.84][215160-4-3--0.25][215171-4-4-0.99][215210-4-4-0.34][215245-4-2-0.47][215274-4-3-0.99][215365-4-4-0.97][215412-4-4-0.43][215694-4-2--0.80][215718-4-0-0.57]
[215757-4-4-0.99][215778-4-0-0.52][215911-4-4-0.74][215913-4-1-0.99][215950-4-3-0.53][215974-4-0-0.85][215988-4-0--0.41][216086-4-1-0.97][216138-4-3-0.31][216343-4-4--0.16]
[216381-4-0-0.99][216394-4-1--0.28][216840-4-0-0.29][216895-4-4-0.98][217062-4-2--0.10][217108-4-3-0.21][217130-4-4-0.99][217143-4-1-0.64][217466-4-2--0.48][217504-4-1-0.99]
[217518-4-0--0.01][217644-4-1-0.99][217736-4-0-0.99][217752-4-1-0.99][217787-4-4--0.15][217797-4-0-0.48][218000-4-4-0.99][218030-4-1-0.99][218084-4-0-0.99][218157-4-4--0.05]
[218167-4-4-0.99][218290-4-4-0.99][218293-4-0-0.34][218478-4-1-0.99][218580-4-4-0.99][218649-4-4-0.73][218799-4-0-0.01][218966-4-2-0.88][219102-4-3--0.44][219166-4-4-0.99]
[219274-4-2-0.92][219568-4-4-0.49][219588-4-4--0.01][219635-4-3-0.99][219648-4-4-0.89][219661-4-0-0.72][219668-4-2--0.04][219740-4-0-0.21][219745-4-2-0.73]
---------------------------
I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 0.961 | Acc: 67.500% | Wgt Acc: 70.074%
	I - Batch: 100 | Loss: 0.949 | Acc: 67.562% | Wgt Acc: 70.418%
	I - Batch: 150 | Loss: 0.956 | Acc: 66.542% | Wgt Acc: 69.519%
	I - Batch: 200 | Loss: 0.960 | Acc: 66.531% | Wgt Acc: 69.611%
I - num batch: 222
I - Train -- Loss: 0.961 | Acc: 66.479% | Wgt Acc: 69.573% | LR: 5.000000e-04 | Dur: 124.30s
I - Confusion Matrix: [row->prediction - col->label]
[[535.   9.  13. 103. 121.]
 [ 14. 455. 107.  28. 145.]
 [ 24.  57. 508.  25. 162.]
 [ 69.  13.  33. 363.  75.]
 [ 55.  44.  73.  19. 497.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.239 | Acc: 50.877% | Wgt Acc: 53.218% | Dur: 12.69s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  2.  4. 18. 14.]
 [ 2. 42. 19.  6. 15.]
 [ 2. 18. 36. 10. 15.]
 [22. 10.  6. 48.  7.]
 [ 6.  6. 10.  4. 21.]]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 0.936 | Acc: 69.375% | Wgt Acc: 71.966%
	I - Batch: 100 | Loss: 0.952 | Acc: 67.750% | Wgt Acc: 70.755%
	I - Batch: 150 | Loss: 0.945 | Acc: 67.458% | Wgt Acc: 70.748%
	I - Batch: 200 | Loss: 0.944 | Acc: 67.125% | Wgt Acc: 70.275%
I - num batch: 222
I - Train -- Loss: 0.943 | Acc: 67.155% | Wgt Acc: 70.287% | LR: 5.000000e-04 | Dur: 125.44s
I - Confusion Matrix: [row->prediction - col->label]
[[538.  14.  16.  94. 133.]
 [ 19. 464. 105.  24. 113.]
 [ 17.  45. 503.  30. 174.]
 [ 77.  17.  36. 371.  74.]
 [ 46.  38.  74.  19. 506.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.247 | Acc: 52.882% | Wgt Acc: 51.918% | Dur: 13.21s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  3. 26.  8.]
 [ 5. 44. 25. 10.  8.]
 [ 2. 13. 28. 11. 10.]
 [10.  6.  3. 33.  5.]
 [ 6. 11. 16.  6. 41.]]

I - Local maximum validation set accuracy:  52.88

I - Validation set results: 
[14-1-1-0.59][50-3-4-0.61][124-2-2-0.99][127-0-0-0.76][443-2-2-0.99][567-0-0-0.99][573-1-1-0.99][615-0-3-0.89][695-1-0-0.84][722-3-3-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-0-0.21][1212-3-4-0.39][1368-0-0-0.99][2181-2-2-0.51][2476-2-1-0.93][2721-2-4-0.86][2818-1-4--0.22][2886-2-1-0.99]
[3231-2-1-0.99][3333-2-1-0.99][3482-2-2-0.96][3536-3-1-0.98][3625-1-1-0.99][3909-0-1-0.71][4035-0-0-0.99][4140-0-0-0.33][4214-1-3-0.57][4346-1-3-0.38]
[4581-2-2-0.99][4708-3-2-0.29][4838-3-2--0.32][4845-1-1-0.76][4868-0-0-0.99][4939-0-1-0.11][4984-2-2-0.99][5078-1-4-0.85][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.45][5843-1-1-0.99][5949-3-4-0.97][5987-2-4-0.99][6014-3-1-0.96][6033-3-0--0.05][6313-0-3-0.96][6421-3-3-0.71][6500-1-2--0.92][6583-3-2-0.54]
[6683-3-1--0.06][6825-2-1-0.87][6998-3-0--0.16][7049-3-3-0.53][7517-1-1-0.99][7521-1-0-0.71][7528-1-3-0.98][7949-1-4-0.99][8135-1-0-0.99][8185-3-3-0.95]
[8269-3-1-0.99][8273-3-3-0.99][8543-3-3-0.99][8666-1-1-0.61][8672-0-0-0.99][8903-1-2-0.99][9001-2-1-0.99][9036-2-4-0.99][9281-3-1-0.97][9300-2-2-0.99]
[9571-0-3--0.03][9617-1-4--0.15][9644-2-2-0.37][9705-2-4-0.99][9801-0-3-0.32][9803-3-3-0.96][9865-3-3-0.96][9896-2-4-0.99][10314-1-1-0.99][10337-3-0-0.99]
[10403-0-2-0.29][10653-2-1-0.80][10704-2-1-0.99][10719-1-1-0.99][10727-1-1-0.99][10836-0-0-0.99][10969-2-2-0.19][11042-0-0-0.67][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.97][11499-0-0--0.19][11502-3-0-0.50][11512-3-2-0.35][11608-1-1-0.99][11610-0-0-0.91][11692-0-0-0.99][11905-0-0-0.99][11993-1-1-0.99][12002-2-3--0.04]
[12052-0-0-0.51][12201-0-3-0.82][12235-2-1-0.99][12320-1-4-0.99][12377-2-4-0.99][12398-2-2-0.89][12503-1-2-0.99][12617-0-1-0.99][12685-3-1-0.00][12738-2-3--0.51]
[12742-2-1-0.99][12823-0-0-0.99][13110-1-2-0.72][13240-3-0-0.86][13253-1-1-0.99][13273-0-0-0.99][13634-1-2--0.01][13763-2-3--0.09][13905-3-3-0.36][14060-2-4-0.83]
[14065-3-0-0.94][14147-3-1-0.44][14595-2-2-0.70][14687-2-2-0.99][14788-2-2-0.86][14869-1-1-0.99][14872-3-0-0.89][14877-1-1-0.97][14927-0-0-0.60][15066-0-0-0.99]
[15175-1-4-0.87][15178-2-4-0.54][15375-3-0-0.40][15389-3-3-0.96][15568-2-1-0.99][15675-3-3-0.99][15869-1-1--0.35][16207-3-0-0.99][16236-0-0-0.02][16302-3-0-0.99]
[16331-2-2-0.99][16381-0-0-0.92][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.18][16801-0-0-0.99][16828-0-0-0.99][17137-3-3-0.61][17245-1-4--0.88]
[17278-3-0-0.15][17282-0-0-0.81][17311-2-2-0.99][17336-2-1-0.16][17608-3-0-0.99][17627-0-1--0.01][17877-3-4-0.62][17924-1-1--0.25][17984-3-0-0.99][18211-0-3--0.09]
[18276-3-0-0.99][18287-1-1-0.88][18394-0-0-0.99][18428-0-1-0.47][18442-0-0-0.99][18478-3-3-0.98][18607-0-0-0.99][18616-0-0-0.60][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.99][18824-2-4-0.99][18890-3-2-0.99][18930-3-4-0.77][18938-3-2-0.06][19817-1-2-0.83][19839-0-4-0.99][19930-3-0-0.20][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-3-0.33][20474-1-1-0.99][20547-3-0--0.47][20929-2-2-0.99][21245-1-1-0.99][21257-3-0-0.26][21293-1-1-0.99][21316-1-1--0.24][21384-1-2-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-1-0.94][21714-0-0-0.59][21943-3-2-0.99][21947-0-0-0.49][21948-0-0-0.99][21965-2-1-0.85][21998-1-3--0.06][22025-0-2-0.79][22228-3-0-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.41][22976-3-2-0.23][22985-3-3-0.97][23014-0-0-0.99][23112-1-1-0.88][23144-3-0-0.98][23168-2-0-0.45]
[23219-0-0-0.87][23363-3-3-0.98][23470-0-4-0.48][23486-2-2-0.83][23497-0-0-0.99][23516-0-0-0.99][23690-1-1-0.62][23921-2-1-0.59][23936-1-2-0.99][24040-3-4-0.99]
[24111-1-4-0.99][24182-0-3-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.86][24364-1-2-0.81][24427-3-0-0.69][24477-2-4-0.99][24495-2-1-0.97][24893-2-1-0.98]
[25012-1-1--0.28][25121-2-4-0.99][25165-3-3--0.17][25183-0-0-0.78][25297-3-3-0.97][25398-0-0-0.99][25574-2-2-0.21][25644-1-1-0.99][25718-1-1-0.22][25774-2-4--0.17]
[26032-3-3-0.72][26051-3-3-0.99][26120-0-4-0.96][26321-1-1-0.99][26732-1-1-0.24][26784-3-3-0.99][26827-3-3-0.07][26833-0-3-0.99][26838-2-1-0.39][26860-1-4-0.76]
[26948-0-0-0.28][27049-3-0-0.99][27098-1-0-0.95][27526-0-0-0.99][27639-3-0-0.75][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.95][28040-0-0-0.99][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-0--0.19][29777-0-0-0.99][29877-2-2-0.63][30035-1-1-0.66][30098-0-3-0.97][30326-1-1-0.99][30572-2-2-0.71][30716-0-4-0.99]
[30806-2-4-0.74][30906-1-4-0.99][31007-0-0-0.99][31181-3-3-0.90][31238-0-0-0.71][31347-0-0-0.98][31422-2-1-0.99][31429-3-1--0.15][31431-0-0-0.70][31432-1-1-0.99]
[31477-0-0-0.99][31524-1-1-0.97][31597-1-2-0.96][31619-1-2--0.11][31701-0-0-0.99][31755-0-0-0.54][31854-3-3-0.44][32074-1-3--0.40][32078-3-3-0.99][32111-1-1-0.99]
[32127-1-2-0.75][32140-3-3-0.99][32263-2-4--0.04][32365-0-0--0.21][32411-2-0-0.99][32429-3-3-0.99][32473-3-0-0.99][32574-3-0-0.99][32584-0-0-0.86][32622-0-4-0.55]
[32858-3-0-0.87][32969-3-3-0.99][33016-2-1-0.91][33031-1-3-0.69][33035-2-2-0.99][33133-2-2-0.32][33173-2-1-0.99][33175-3-2-0.99][33306-3-1-0.99][33309-2-1-0.41]
[33474-0-0--0.52][33478-2-2-0.15][33618-1-1-0.99][33712-0-3-0.65][33782-2-1-0.99][33914-3-2-0.85][34076-3-1-0.58][34112-2-1-0.99][34138-2-2-0.99][34239-1-1-0.96]
[34364-2-1-0.99][34617-1-2-0.54][34751-3-3-0.99][34783-2-4-0.58][35015-3-2-0.67][35018-1-1-0.99][35288-2-4-0.29][214490-4-4-0.99][214508-4-4-0.34][214565-4-1--0.18]
[214581-4-4--0.14][214585-4-4-0.99][214625-4-4-0.99][214627-4-4-0.99][214688-4-2-0.99][214694-4-2-0.99][214815-4-4-0.94][214882-4-1-0.02][214894-4-4-0.71][214979-4-2-0.03]
[215082-4-4-0.78][215160-4-0-0.71][215171-4-4-0.99][215210-4-4-0.38][215245-4-3-0.71][215274-4-3-0.65][215365-4-4-0.96][215412-4-4-0.88][215694-4-4-0.14][215718-4-0-0.17]
[215757-4-4-0.99][215778-4-1--0.57][215911-4-4-0.90][215913-4-4-0.99][215950-4-4-0.99][215974-4-4-0.45][215988-4-4-0.51][216086-4-4-0.99][216138-4-3--0.44][216343-4-4-0.88]
[216381-4-4-0.99][216394-4-4-0.53][216840-4-0--0.00][216895-4-4-0.99][217062-4-2-0.85][217108-4-0--0.51][217130-4-4-0.99][217143-4-4-0.97][217466-4-2--0.25][217504-4-4-0.99]
[217518-4-4-0.03][217644-4-1-0.99][217736-4-3-0.27][217752-4-1-0.96][217787-4-4-0.96][217797-4-0--0.70][218000-4-1-0.98][218030-4-1-0.99][218084-4-0-0.96][218157-4-4-0.36]
[218167-4-4-0.99][218290-4-4-0.99][218293-4-4-0.65][218478-4-1-0.99][218580-4-4-0.99][218649-4-0--0.52][218799-4-4-0.37][218966-4-2-0.99][219102-4-4-0.04][219166-4-2-0.99]
[219274-4-4-0.99][219568-4-4-0.99][219588-4-4-0.56][219635-4-3-0.17][219648-4-4-0.99][219661-4-2-0.15][219668-4-2-0.40][219740-4-0-0.86][219745-4-2--0.05]
---------------------------
I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 0.902 | Acc: 69.750% | Wgt Acc: 73.433%
	I - Batch: 100 | Loss: 0.935 | Acc: 67.438% | Wgt Acc: 71.073%
	I - Batch: 150 | Loss: 0.942 | Acc: 68.125% | Wgt Acc: 71.256%
	I - Batch: 200 | Loss: 0.946 | Acc: 67.312% | Wgt Acc: 70.672%
I - num batch: 222
I - Train -- Loss: 0.944 | Acc: 67.353% | Wgt Acc: 70.625% | LR: 5.000000e-04 | Dur: 126.13s
I - Confusion Matrix: [row->prediction - col->label]
[[543.  14.  18. 109. 106.]
 [ 20. 462. 101.  20. 136.]
 [ 20.  49. 524.  26. 173.]
 [ 80.  18.  22. 366.  91.]
 [ 34.  35.  69.  17. 494.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.255 | Acc: 52.381% | Wgt Acc: 50.866% | Dur: 12.62s
I - Confusion Matrix: [row->prediction - col->label]
[[51.  1.  3. 19.  2.]
 [ 6. 44. 25. 12. 11.]
 [ 2. 11. 31. 10.  8.]
 [17.  4.  2. 36.  4.]
 [12. 18. 14.  9. 47.]]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 0.890 | Acc: 72.875% | Wgt Acc: 75.637%
	I - Batch: 100 | Loss: 0.899 | Acc: 71.625% | Wgt Acc: 74.440%
	I - Batch: 150 | Loss: 0.912 | Acc: 70.333% | Wgt Acc: 72.984%
	I - Batch: 200 | Loss: 0.913 | Acc: 70.062% | Wgt Acc: 72.755%
I - num batch: 222
I - Train -- Loss: 0.915 | Acc: 69.834% | Wgt Acc: 72.647% | LR: 5.000000e-04 | Dur: 122.74s
I - Confusion Matrix: [row->prediction - col->label]
[[557.   4.  16. 106. 112.]
 [ 20. 470.  89.  20. 105.]
 [ 11.  54. 546.  30. 169.]
 [ 62.  12.  19. 365.  75.]
 [ 47.  38.  64.  17. 539.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.245 | Acc: 52.632% | Wgt Acc: 51.980% | Dur: 12.29s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  1.  3. 24.  5.]
 [ 2. 35. 11.  5.  9.]
 [ 4. 29. 44. 14. 18.]
 [16.  4.  3. 37.  4.]
 [ 8.  9. 14.  6. 36.]]

I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 0.817 | Acc: 76.125% | Wgt Acc: 78.085%
	I - Batch: 100 | Loss: 0.838 | Acc: 76.000% | Wgt Acc: 78.203%
	I - Batch: 150 | Loss: 0.844 | Acc: 75.292% | Wgt Acc: 77.517%
I - num batch: 173
I - Train -- Loss: 0.845 | Acc: 75.181% | Wgt Acc: 77.535% | LR: 5.000000e-04 | Dur: 97.32s
I - Confusion Matrix: [row->prediction - col->label]
[[571.  11.  12.  93.  33.]
 [ 15. 480.  89.  22.  47.]
 [ 32.  66. 589.  25.  79.]
 [ 71.  13.  27. 395.  15.]
 [  8.   8.  17.   3.  43.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.280 | Acc: 49.875% | Wgt Acc: 53.342% | Dur: 12.55s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  5. 28. 15.]
 [ 2. 38. 13.  3. 12.]
 [ 4. 24. 40. 10. 30.]
 [10.  7. 10. 44.  7.]
 [ 3.  3.  7.  1.  8.]]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 0.920 | Acc: 68.125% | Wgt Acc: 74.068%
	I - Batch: 100 | Loss: 0.917 | Acc: 68.500% | Wgt Acc: 73.712%
	I - Batch: 150 | Loss: 0.915 | Acc: 68.875% | Wgt Acc: 73.684%
	I - Batch: 200 | Loss: 0.905 | Acc: 70.094% | Wgt Acc: 74.349%
I - num batch: 222
I - Train -- Loss: 0.903 | Acc: 70.116% | Wgt Acc: 74.369% | LR: 5.000000e-04 | Dur: 122.93s
I - Confusion Matrix: [row->prediction - col->label]
[[569.   5.   9.  98. 141.]
 [ 14. 498.  78.  18. 119.]
 [ 21.  41. 568.  27. 189.]
 [ 59.  13.  26. 384.  83.]
 [ 34.  21.  53.  11. 468.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.206 | Acc: 56.140% | Wgt Acc: 55.693% | Dur: 12.54s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  3.  4. 16.  8.]
 [ 2. 40. 13.  3.  9.]
 [ 4. 14. 32.  4.  6.]
 [18.  5.  4. 52.  5.]
 [ 8. 16. 22. 11. 44.]]

I - Local maximum validation set accuracy:  56.14

I - Validation set results: 
[14-1-2-0.52][50-3-4-0.71][124-2-2-0.99][127-0-0-0.85][443-2-2-0.99][567-0-0-0.99][573-1-1-0.99][615-0-3-0.77][695-1-0-0.91][722-3-3-0.97]
[826-0-0-0.85][878-0-0-0.99][1103-0-0--0.01][1212-3-4-0.66][1368-0-0-0.99][2181-2-2-0.99][2476-2-0-0.31][2721-2-2-0.99][2818-1-4-0.61][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-1-0.40][3482-2-2-0.99][3536-3-3-0.95][3625-1-1-0.99][3909-0-0-0.97][4035-0-3-0.97][4140-0-0-0.39][4214-1-3-0.97][4346-1-3-0.49]
[4581-2-2-0.94][4708-3-3-0.36][4838-3-0--0.68][4845-1-1--0.06][4868-0-3-0.98][4939-0-2-0.45][4984-2-2-0.96][5078-1-2-0.38][5396-0-0-0.99][5479-1-4-0.18]
[5717-0-0-0.88][5843-1-1-0.95][5949-3-3-0.90][5987-2-4-0.92][6014-3-3-0.99][6033-3-1--0.49][6313-0-3-0.93][6421-3-3-0.99][6500-1-3--0.45][6583-3-2-0.88]
[6683-3-3--0.39][6825-2-3-0.59][6998-3-0--0.51][7049-3-3-0.97][7517-1-1-0.99][7521-1-1-0.99][7528-1-3-0.99][7949-1-4-0.99][8135-1-0-0.99][8185-3-3-0.99]
[8269-3-1-0.86][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.94][8672-0-0-0.99][8903-1-1-0.63][9001-2-2-0.68][9036-2-4-0.99][9281-3-4-0.67][9300-2-2-0.99]
[9571-0-0-0.54][9617-1-1-0.52][9644-2-1-0.45][9705-2-4-0.99][9801-0-3-0.74][9803-3-3-0.99][9865-3-3-0.99][9896-2-4-0.99][10314-1-1--0.02][10337-3-3--0.10]
[10403-0-4-0.99][10653-2-4-0.52][10704-2-1-0.69][10719-1-1-0.96][10727-1-4-0.99][10836-0-0-0.99][10969-2-2-0.83][11042-0-0-0.61][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-4-0.99][11499-0-3-0.76][11502-3-3-0.95][11512-3-4--0.18][11608-1-1-0.81][11610-0-0-0.99][11692-0-3-0.99][11905-0-0-0.99][11993-1-2-0.89][12002-2-2-0.38]
[12052-0-0-0.94][12201-0-3-0.98][12235-2-1-0.99][12320-1-4-0.99][12377-2-4-0.99][12398-2-2-0.99][12503-1-2-0.18][12617-0-1-0.99][12685-3-3--0.65][12738-2-4-0.48]
[12742-2-1-0.91][12823-0-0-0.99][13110-1-1-0.56][13240-3-0-0.35][13253-1-1-0.99][13273-0-0-0.99][13634-1-4-0.96][13763-2-2-0.57][13905-3-3--0.07][14060-2-4-0.22]
[14065-3-0-0.99][14147-3-3-0.48][14595-2-2-0.74][14687-2-2-0.92][14788-2-1-0.04][14869-1-1-0.99][14872-3-4-0.58][14877-1-1-0.99][14927-0-3-0.96][15066-0-3-0.99]
[15175-1-4-0.96][15178-2-4-0.89][15375-3-3-0.96][15389-3-3-0.99][15568-2-1-0.98][15675-3-3-0.91][15869-1-1--0.16][16207-3-0-0.99][16236-0-2--0.49][16302-3-4-0.82]
[16331-2-2-0.99][16381-0-0-0.44][16488-1-1-0.24][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.68][16801-0-0-0.99][16828-0-0-0.97][17137-3-0-0.67][17245-1-4--0.48]
[17278-3-0--0.16][17282-0-0-0.67][17311-2-2-0.99][17336-2-3-0.70][17608-3-3-0.99][17627-0-0-0.63][17877-3-4-0.99][17924-1-4-0.73][17984-3-0-0.99][18211-0-3-0.26]
[18276-3-0-0.99][18287-1-1-0.90][18394-0-0-0.99][18428-0-0-0.61][18442-0-3-0.94][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.96][18663-0-0-0.98][18718-0-0-0.99]
[18766-2-2-0.99][18824-2-4-0.99][18890-3-2-0.60][18930-3-4-0.99][18938-3-3-0.33][19817-1-2-0.96][19839-0-4-0.99][19930-3-3-0.85][19944-0-2-0.99][20036-2-2-0.99]
[20101-3-3-0.37][20474-1-1-0.99][20547-3-3-0.30][20929-2-2-0.99][21245-1-1--0.31][21257-3-4-0.87][21293-1-1-0.99][21316-1-1-0.90][21384-1-4-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-4-0.99][21714-0-0-0.41][21943-3-2-0.81][21947-0-0-0.57][21948-0-0-0.99][21965-2-1-0.50][21998-1-1-0.62][22025-0-2-0.01][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-4-0.51][22985-3-3-0.99][23014-0-0-0.74][23112-1-2-0.68][23144-3-3-0.99][23168-2-0-0.82]
[23219-0-0-0.70][23363-3-3-0.99][23470-0-4-0.07][23486-2-4-0.31][23497-0-0-0.99][23516-0-0-0.99][23690-1-1-0.31][23921-2-4-0.97][23936-1-2-0.99][24040-3-4-0.99]
[24111-1-1-0.99][24182-0-0-0.99][24238-3-3-0.98][24290-2-0-0.99][24345-0-0-0.93][24364-1-2-0.94][24427-3-0-0.59][24477-2-4-0.99][24495-2-1-0.33][24893-2-1-0.92]
[25012-1-4--0.47][25121-2-4-0.99][25165-3-3-0.99][25183-0-0-0.16][25297-3-3-0.92][25398-0-0-0.69][25574-2-2-0.29][25644-1-1-0.99][25718-1-4-0.35][25774-2-2-0.86]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.98][26321-1-1-0.97][26732-1-2--0.17][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-2-0.29][26860-1-4-0.99]
[26948-0-0-0.99][27049-3-0-0.99][27098-1-0--0.31][27526-0-0-0.99][27639-3-0-0.86][27698-3-3-0.84][27772-0-0-0.96][27890-1-1--0.04][28040-0-4-0.99][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3--0.74][29777-0-0-0.99][29877-2-1-0.40][30035-1-1-0.94][30098-0-0-0.81][30326-1-1-0.99][30572-2-3-0.99][30716-0-4-0.99]
[30806-2-4-0.57][30906-1-1-0.99][31007-0-4-0.99][31181-3-3-0.99][31238-0-3-0.70][31347-0-3-0.99][31422-2-4-0.77][31429-3-3--0.01][31431-0-3-0.42][31432-1-1-0.77]
[31477-0-0-0.99][31524-1-2-0.44][31597-1-2-0.56][31619-1-2-0.66][31701-0-0-0.82][31755-0-1--0.16][31854-3-3-0.32][32074-1-2--0.10][32078-3-3-0.99][32111-1-4-0.96]
[32127-1-2-0.47][32140-3-3-0.99][32263-2-4-0.48][32365-0-3-0.01][32411-2-0-0.99][32429-3-3-0.99][32473-3-0-0.98][32574-3-0-0.99][32584-0-0-0.79][32622-0-4-0.83]
[32858-3-0-0.98][32969-3-3-0.99][33016-2-1-0.99][33031-1-3-0.99][33035-2-2-0.99][33133-2-2-0.57][33173-2-2--0.11][33175-3-2-0.99][33306-3-1-0.96][33309-2-3-0.24]
[33474-0-0--0.05][33478-2-4-0.28][33618-1-1-0.99][33712-0-3-0.97][33782-2-4-0.99][33914-3-3-0.88][34076-3-3--0.59][34112-2-2-0.94][34138-2-2-0.99][34239-1-1-0.61]
[34364-2-2-0.99][34617-1-4--0.09][34751-3-3-0.99][34783-2-2-0.45][35015-3-3-0.69][35018-1-1-0.83][35288-2-4-0.17][214490-4-4-0.99][214508-4-4-0.51][214565-4-4-0.38]
[214581-4-4--0.24][214585-4-4-0.99][214625-4-4-0.99][214627-4-4-0.83][214688-4-2-0.99][214694-4-2-0.99][214815-4-1-0.82][214882-4-4-0.82][214894-4-4-0.57][214979-4-4--0.27]
[215082-4-4-0.99][215160-4-4-0.85][215171-4-4-0.99][215210-4-4-0.99][215245-4-3-0.13][215274-4-1-0.06][215365-4-4-0.92][215412-4-4-0.99][215694-4-4-0.56][215718-4-0-0.10]
[215757-4-4-0.99][215778-4-0-0.18][215911-4-4-0.99][215913-4-1-0.99][215950-4-4-0.99][215974-4-3-0.58][215988-4-0-0.20][216086-4-4-0.99][216138-4-4-0.17][216343-4-4-0.80]
[216381-4-4-0.99][216394-4-4-0.05][216840-4-4-0.19][216895-4-4-0.99][217062-4-4-0.85][217108-4-3-0.61][217130-4-4-0.99][217143-4-4-0.99][217466-4-4-0.16][217504-4-4-0.99]
[217518-4-0-0.55][217644-4-1-0.99][217736-4-3-0.89][217752-4-1-0.71][217787-4-4-0.73][217797-4-0--0.26][218000-4-4-0.99][218030-4-1-0.99][218084-4-0-0.99][218157-4-1-0.16]
[218167-4-4-0.99][218290-4-4-0.94][218293-4-4-0.99][218478-4-1-0.64][218580-4-4--0.49][218649-4-0--0.80][218799-4-4-0.99][218966-4-2-0.99][219102-4-4-0.78][219166-4-2-0.99]
[219274-4-4-0.99][219568-4-4-0.99][219588-4-4-0.67][219635-4-3-0.97][219648-4-4-0.99][219661-4-2-0.97][219668-4-2-0.67][219740-4-0--0.03][219745-4-1-0.06]
---------------------------
I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 0.900 | Acc: 71.000% | Wgt Acc: 73.438%
	I - Batch: 100 | Loss: 0.894 | Acc: 71.625% | Wgt Acc: 74.749%
	I - Batch: 150 | Loss: 0.896 | Acc: 70.500% | Wgt Acc: 73.873%
	I - Batch: 200 | Loss: 0.901 | Acc: 70.562% | Wgt Acc: 73.764%
I - num batch: 222
I - Train -- Loss: 0.900 | Acc: 70.651% | Wgt Acc: 73.835% | LR: 5.000000e-04 | Dur: 121.18s
I - Confusion Matrix: [row->prediction - col->label]
[[553.  10.  12.  87. 106.]
 [ 17. 484.  79.  16. 100.]
 [ 21.  44. 555.  28. 190.]
 [ 60.  10.  18. 381.  71.]
 [ 46.  30.  70.  26. 533.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.227 | Acc: 53.383% | Wgt Acc: 54.703% | Dur: 12.56s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  1.  3. 21.  7.]
 [ 6. 43. 18.  5. 12.]
 [ 1. 16. 38.  8. 21.]
 [12.  7.  6. 45.  4.]
 [10. 11. 10.  7. 28.]]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.884 | Acc: 72.750% | Wgt Acc: 75.852%
	I - Batch: 100 | Loss: 0.871 | Acc: 74.000% | Wgt Acc: 77.294%
	I - Batch: 150 | Loss: 0.873 | Acc: 73.333% | Wgt Acc: 76.805%
	I - Batch: 200 | Loss: 0.871 | Acc: 73.531% | Wgt Acc: 76.888%
I - num batch: 222
I - Train -- Loss: 0.872 | Acc: 73.527% | Wgt Acc: 76.902% | LR: 2.500000e-04 | Dur: 124.25s
I - Confusion Matrix: [row->prediction - col->label]
[[571.   4.  10.  68. 109.]
 [ 15. 496.  71.  16.  98.]
 [ 17.  41. 570.  20. 160.]
 [ 47.   8.  22. 415.  77.]
 [ 47.  29.  61.  19. 556.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.222 | Acc: 55.639% | Wgt Acc: 55.260% | Dur: 12.23s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  2.  3. 15.  2.]
 [ 4. 37. 11.  4.  8.]
 [ 4. 23. 47. 14. 20.]
 [18.  6.  7. 46.  3.]
 [ 9. 10.  7.  7. 39.]]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 0.843 | Acc: 76.250% | Wgt Acc: 78.910%
	I - Batch: 100 | Loss: 0.839 | Acc: 76.688% | Wgt Acc: 79.790%
	I - Batch: 150 | Loss: 0.851 | Acc: 76.000% | Wgt Acc: 79.084%
	I - Batch: 200 | Loss: 0.853 | Acc: 75.500% | Wgt Acc: 78.595%
I - num batch: 222
I - Train -- Loss: 0.851 | Acc: 75.500% | Wgt Acc: 78.683% | LR: 2.500000e-04 | Dur: 122.31s
I - Confusion Matrix: [row->prediction - col->label]
[[591.   5.   4.  80. 115.]
 [  9. 504.  63.  10.  94.]
 [ 14.  38. 591.  20. 146.]
 [ 33.   9.  22. 412.  65.]
 [ 50.  22.  54.  16. 580.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.204 | Acc: 54.887% | Wgt Acc: 55.136% | Dur: 13.46s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  3.  5. 18.  9.]
 [ 4. 38. 13.  5.  6.]
 [ 1. 19. 35.  7. 13.]
 [18.  7.  9. 51.  7.]
 [ 7. 11. 13.  5. 37.]]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.814 | Acc: 76.875% | Wgt Acc: 79.993%
	I - Batch: 100 | Loss: 0.837 | Acc: 75.562% | Wgt Acc: 78.903%
	I - Batch: 150 | Loss: 0.845 | Acc: 75.167% | Wgt Acc: 78.165%
	I - Batch: 200 | Loss: 0.842 | Acc: 75.375% | Wgt Acc: 78.571%
I - num batch: 222
I - Train -- Loss: 0.841 | Acc: 75.613% | Wgt Acc: 78.796% | LR: 2.500000e-04 | Dur: 125.43s
I - Confusion Matrix: [row->prediction - col->label]
[[591.  11.   6.  74. 108.]
 [ 10. 503.  64.  16.  90.]
 [ 15.  32. 593.  12. 148.]
 [ 42.   5.  15. 414.  73.]
 [ 39.  27.  56.  22. 581.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.210 | Acc: 52.632% | Wgt Acc: 53.342% | Dur: 12.57s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  3.  4. 14.  9.]
 [ 5. 38. 13.  4.  8.]
 [ 2. 17. 30.  5. 10.]
 [22. 10. 13. 54. 10.]
 [ 6. 10. 15.  9. 35.]]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 0.826 | Acc: 76.250% | Wgt Acc: 79.567%
	I - Batch: 100 | Loss: 0.830 | Acc: 77.312% | Wgt Acc: 80.401%
	I - Batch: 150 | Loss: 0.840 | Acc: 76.208% | Wgt Acc: 79.500%
	I - Batch: 200 | Loss: 0.841 | Acc: 75.906% | Wgt Acc: 79.068%
I - num batch: 222
I - Train -- Loss: 0.840 | Acc: 75.670% | Wgt Acc: 78.863% | LR: 2.500000e-04 | Dur: 123.22s
I - Confusion Matrix: [row->prediction - col->label]
[[574.   6.  10.  69. 108.]
 [ 14. 507.  62.  12. 100.]
 [ 13.  25. 596.  23. 142.]
 [ 45.   7.  13. 421.  64.]
 [ 51.  33.  53.  13. 586.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.253 | Acc: 51.378% | Wgt Acc: 50.866% | Dur: 13.26s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  4. 17.  5.]
 [ 4. 35. 13.  6. 12.]
 [ 4. 18. 38. 16. 16.]
 [10.  6.  5. 37.  4.]
 [10. 15. 15. 10. 35.]]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.801 | Acc: 77.750% | Wgt Acc: 80.922%
	I - Batch: 100 | Loss: 0.819 | Acc: 76.250% | Wgt Acc: 79.598%
	I - Batch: 150 | Loss: 0.823 | Acc: 76.208% | Wgt Acc: 79.547%
	I - Batch: 200 | Loss: 0.827 | Acc: 76.531% | Wgt Acc: 79.848%
I - num batch: 222
I - Train -- Loss: 0.829 | Acc: 76.149% | Wgt Acc: 79.578% | LR: 2.500000e-04 | Dur: 125.84s
I - Confusion Matrix: [row->prediction - col->label]
[[587.  10.   8.  60. 120.]
 [ 16. 513.  51.  13.  99.]
 [ 13.  31. 609.  29. 133.]
 [ 36.   6.  18. 418.  74.]
 [ 45.  18.  48.  18. 574.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.233 | Acc: 54.637% | Wgt Acc: 53.094% | Dur: 12.81s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  2. 18.  8.]
 [ 4. 42.  9.  7.  8.]
 [ 1. 11. 35.  8.  8.]
 [ 7.  4.  5. 36.  2.]
 [17. 17. 24. 17. 46.]]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.772 | Acc: 82.750% | Wgt Acc: 83.996%
	I - Batch: 100 | Loss: 0.747 | Acc: 83.000% | Wgt Acc: 84.744%
	I - Batch: 150 | Loss: 0.743 | Acc: 82.958% | Wgt Acc: 84.917%
I - num batch: 173
I - Train -- Loss: 0.741 | Acc: 82.959% | Wgt Acc: 84.946% | LR: 1.250000e-04 | Dur: 97.34s
I - Confusion Matrix: [row->prediction - col->label]
[[623.   4.   9.  64.  32.]
 [ 15. 525.  63.  12.  36.]
 [ 11.  31. 628.  20.  50.]
 [ 32.   7.  14. 436.  18.]
 [ 16.  11.  20.   6.  81.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.236 | Acc: 52.882% | Wgt Acc: 55.446% | Dur: 13.13s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  7.  5. 27. 19.]
 [ 3. 43. 14.  3. 12.]
 [ 1. 19. 39. 11. 19.]
 [12.  7. 11. 43.  5.]
 [ 3.  2.  6.  2. 17.]]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.824 | Acc: 73.250% | Wgt Acc: 79.481%
	I - Batch: 100 | Loss: 0.805 | Acc: 76.562% | Wgt Acc: 81.171%
	I - Batch: 150 | Loss: 0.808 | Acc: 77.125% | Wgt Acc: 81.203%
	I - Batch: 200 | Loss: 0.808 | Acc: 77.500% | Wgt Acc: 81.394%
I - num batch: 222
I - Train -- Loss: 0.812 | Acc: 77.305% | Wgt Acc: 81.231% | LR: 1.250000e-04 | Dur: 122.84s
I - Confusion Matrix: [row->prediction - col->label]
[[597.   4.  10.  61. 117.]
 [ 11. 525.  52.   9. 110.]
 [ 14.  28. 620.  17. 141.]
 [ 33.   5.   8. 438.  70.]
 [ 42.  16.  44.  13. 562.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.225 | Acc: 53.885% | Wgt Acc: 53.960% | Dur: 12.80s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  5. 22. 12.]
 [ 4. 38. 11.  8.  8.]
 [ 1. 17. 38. 10. 11.]
 [11.  7.  7. 42.  7.]
 [ 9. 11. 14.  4. 34.]]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.814 | Acc: 76.625% | Wgt Acc: 80.119%
	I - Batch: 100 | Loss: 0.801 | Acc: 77.000% | Wgt Acc: 80.373%
	I - Batch: 150 | Loss: 0.790 | Acc: 78.000% | Wgt Acc: 81.153%
	I - Batch: 200 | Loss: 0.792 | Acc: 78.125% | Wgt Acc: 81.221%
I - num batch: 222
I - Train -- Loss: 0.794 | Acc: 78.122% | Wgt Acc: 81.306% | LR: 1.250000e-04 | Dur: 125.73s
I - Confusion Matrix: [row->prediction - col->label]
[[595.   4.  12.  65. 105.]
 [ 12. 530.  52.  10.  84.]
 [  8.  23. 598.  12. 133.]
 [ 38.   2.  14. 433.  63.]
 [ 44.  19.  58.  18. 615.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.196 | Acc: 56.642% | Wgt Acc: 57.364% | Dur: 12.82s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  3.  4. 19.  9.]
 [ 4. 43.  9.  3.  9.]
 [ 2. 17. 34.  3. 10.]
 [18.  5. 11. 54.  7.]
 [ 6. 10. 17.  7. 37.]]

I - Local maximum validation set accuracy:  56.64

I - Validation set results: 
[14-1-2--0.11][50-3-4-0.05][124-2-2-0.99][127-0-0-0.96][443-2-2-0.99][567-0-0-0.86][573-1-1-0.99][615-0-3-0.97][695-1-0-0.92][722-3-3-0.93]
[826-0-0-0.64][878-0-0-0.99][1103-0-0-0.41][1212-3-3-0.45][1368-0-0-0.99][2181-2-2-0.99][2476-2-2-0.40][2721-2-2-0.99][2818-1-3-0.97][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-3-0.78][3482-2-2-0.98][3536-3-3-0.76][3625-1-1-0.99][3909-0-0-0.88][4035-0-0-0.99][4140-0-0--0.51][4214-1-3-0.70][4346-1-3-0.07]
[4581-2-2-0.99][4708-3-3-0.70][4838-3-3--0.01][4845-1-2--0.05][4868-0-0-0.66][4939-0-2--0.17][4984-2-2-0.85][5078-1-4-0.51][5396-0-0-0.99][5479-1-1--0.61]
[5717-0-0-0.61][5843-1-1-0.99][5949-3-3-0.99][5987-2-4-0.99][6014-3-3-0.99][6033-3-0-0.31][6313-0-3-0.99][6421-3-3-0.99][6500-1-2--0.29][6583-3-3-0.58]
[6683-3-3-0.52][6825-2-3-0.53][6998-3-0-0.08][7049-3-3-0.79][7517-1-2-0.97][7521-1-1-0.99][7528-1-3-0.99][7949-1-2-0.77][8135-1-0-0.98][8185-3-0-0.91]
[8269-3-1-0.80][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.84][8672-0-0-0.99][8903-1-1-0.20][9001-2-1-0.99][9036-2-4-0.96][9281-3-4--0.20][9300-2-2-0.99]
[9571-0-3-0.24][9617-1-1--0.44][9644-2-2-0.88][9705-2-4-0.99][9801-0-3-0.08][9803-3-3-0.99][9865-3-3-0.42][9896-2-4-0.99][10314-1-1-0.31][10337-3-3-0.98]
[10403-0-4-0.95][10653-2-1-0.36][10704-2-1-0.61][10719-1-1-0.99][10727-1-1-0.91][10836-0-0-0.99][10969-2-3-0.54][11042-0-0-0.35][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-4-0.99][11499-0-3-0.52][11502-3-3-0.89][11512-3-3-0.83][11608-1-2-0.88][11610-0-0-0.99][11692-0-0-0.87][11905-0-0-0.99][11993-1-2-0.51][12002-2-0-0.99]
[12052-0-0-0.95][12201-0-3-0.84][12235-2-4-0.83][12320-1-4-0.99][12377-2-4-0.93][12398-2-2-0.85][12503-1-4-0.91][12617-0-1-0.76][12685-3-3-0.78][12738-2-4--0.08]
[12742-2-2-0.27][12823-0-3-0.94][13110-1-1-0.66][13240-3-0-0.75][13253-1-1-0.99][13273-0-0-0.99][13634-1-4-0.80][13763-2-3-0.62][13905-3-3-0.82][14060-2-4-0.23]
[14065-3-0--0.14][14147-3-0-0.99][14595-2-2-0.85][14687-2-3-0.99][14788-2-2--0.01][14869-1-1-0.97][14872-3-4-0.54][14877-1-1-0.99][14927-0-3-0.95][15066-0-0-0.99]
[15175-1-4-0.95][15178-2-3-0.91][15375-3-3-0.38][15389-3-3-0.85][15568-2-1-0.75][15675-3-3-0.99][15869-1-1--0.61][16207-3-0-0.99][16236-0-0-0.47][16302-3-4-0.98]
[16331-2-2-0.99][16381-0-0-0.91][16488-1-1-0.49][16495-0-0-0.97][16650-0-0-0.99][16719-1-4-0.66][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.82][17245-1-4-0.23]
[17278-3-0--0.38][17282-0-0-0.95][17311-2-2-0.99][17336-2-1-0.22][17608-3-3-0.99][17627-0-1--0.08][17877-3-4-0.99][17924-1-4-0.81][17984-3-0-0.99][18211-0-3-0.86]
[18276-3-0-0.99][18287-1-1-0.87][18394-0-0-0.99][18428-0-0--0.25][18442-0-3-0.96][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.69][18663-0-0-0.93][18718-0-0-0.99]
[18766-2-2--0.07][18824-2-4-0.97][18890-3-3-0.03][18930-3-4-0.28][18938-3-3-0.63][19817-1-2-0.93][19839-0-4-0.99][19930-3-0-0.28][19944-0-4-0.66][20036-2-2-0.99]
[20101-3-3--0.08][20474-1-1-0.99][20547-3-3-0.57][20929-2-4-0.14][21245-1-1-0.82][21257-3-3-0.06][21293-1-1-0.99][21316-1-1-0.67][21384-1-4-0.83][21448-1-1-0.84]
[21483-0-0-0.99][21487-2-2-0.96][21714-0-0-0.23][21943-3-2-0.99][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.84][21998-1-1-0.73][22025-0-3-0.90][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-2-0.24][22985-3-0-0.94][23014-0-0-0.99][23112-1-1-0.98][23144-3-0-0.95][23168-2-0-0.91]
[23219-0-0-0.77][23363-3-3-0.99][23470-0-0--0.07][23486-2-3-0.49][23497-0-3-0.99][23516-0-0-0.99][23690-1-1-0.55][23921-2-4-0.79][23936-1-2-0.66][24040-3-4-0.99]
[24111-1-1-0.99][24182-0-3-0.99][24238-3-3-0.99][24290-2-0-0.96][24345-0-0-0.42][24364-1-2-0.99][24427-3-0-0.97][24477-2-4-0.99][24495-2-1-0.08][24893-2-1-0.39]
[25012-1-1--0.33][25121-2-4-0.98][25165-3-3-0.99][25183-0-3--0.34][25297-3-3-0.99][25398-0-0-0.68][25574-2-2-0.71][25644-1-1-0.99][25718-1-1--0.47][25774-2-2--0.44]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.65][26321-1-1-0.98][26732-1-1-0.72][26784-3-3-0.99][26827-3-3-0.97][26833-0-3-0.99][26838-2-2-0.07][26860-1-4-0.99]
[26948-0-0-0.99][27049-3-0-0.99][27098-1-0-0.40][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.96][27772-0-0-0.99][27890-1-1-0.09][28040-0-4-0.63][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3--0.69][29777-0-0-0.99][29877-2-3-0.85][30035-1-1-0.59][30098-0-3-0.88][30326-1-1-0.99][30572-2-2-0.94][30716-0-4-0.98]
[30806-2-3-0.38][30906-1-1-0.97][31007-0-0-0.80][31181-3-3-0.86][31238-0-3--0.07][31347-0-0-0.99][31422-2-4-0.61][31429-3-3-0.55][31431-0-0-0.56][31432-1-1-0.78]
[31477-0-0-0.99][31524-1-2--0.15][31597-1-2-0.50][31619-1-2-0.12][31701-0-0-0.95][31755-0-1--0.10][31854-3-3-0.88][32074-1-2--0.42][32078-3-3-0.99][32111-1-1-0.94]
[32127-1-2-0.80][32140-3-3-0.99][32263-2-2--0.32][32365-0-0-0.38][32411-2-0-0.99][32429-3-3-0.90][32473-3-0-0.90][32574-3-3-0.99][32584-0-0-0.57][32622-0-2-0.21]
[32858-3-0-0.93][32969-3-3-0.99][33016-2-2-0.94][33031-1-3-0.96][33035-2-2-0.98][33133-2-2-0.10][33173-2-2--0.47][33175-3-2-0.99][33306-3-1-0.99][33309-2-3-0.91]
[33474-0-1--0.25][33478-2-3-0.44][33618-1-1-0.99][33712-0-3-0.99][33782-2-4-0.99][33914-3-3-0.52][34076-3-1-0.39][34112-2-1-0.68][34138-2-2-0.97][34239-1-1-0.64]
[34364-2-2-0.90][34617-1-2-0.61][34751-3-3-0.99][34783-2-2-0.35][35015-3-3-0.56][35018-1-2-0.82][35288-2-4--0.43][214490-4-4-0.86][214508-4-4-0.80][214565-4-2--0.18]
[214581-4-4--0.49][214585-4-4-0.99][214625-4-4-0.99][214627-4-4-0.98][214688-4-2-0.77][214694-4-1-0.50][214815-4-4-0.99][214882-4-4-0.10][214894-4-4-0.48][214979-4-2-0.25]
[215082-4-4-0.71][215160-4-0-0.52][215171-4-4-0.99][215210-4-4-0.15][215245-4-2--0.36][215274-4-3-0.99][215365-4-1-0.75][215412-4-4-0.38][215694-4-2--0.66][215718-4-0-0.19]
[215757-4-4-0.99][215778-4-3--0.11][215911-4-4-0.93][215913-4-1-0.99][215950-4-3-0.24][215974-4-3-0.33][215988-4-0-0.48][216086-4-4-0.99][216138-4-0-0.32][216343-4-4-0.62]
[216381-4-4-0.99][216394-4-2-0.50][216840-4-4--0.24][216895-4-4-0.99][217062-4-4-0.57][217108-4-3--0.05][217130-4-4-0.88][217143-4-4-0.92][217466-4-1-0.06][217504-4-4-0.99]
[217518-4-0-0.02][217644-4-1-0.95][217736-4-3-0.68][217752-4-4-0.61][217787-4-4--0.07][217797-4-0--0.08][218000-4-1-0.60][218030-4-1-0.99][218084-4-0-0.97][218157-4-1-0.84]
[218167-4-4-0.99][218290-4-4-0.84][218293-4-4-0.69][218478-4-1-0.88][218580-4-4-0.99][218649-4-4-0.52][218799-4-0-0.17][218966-4-2-0.99][219102-4-4--0.45][219166-4-2-0.99]
[219274-4-2-0.98][219568-4-4-0.99][219588-4-4-0.96][219635-4-3-0.49][219648-4-4-0.99][219661-4-2-0.87][219668-4-4--0.31][219740-4-0-0.29][219745-4-4-0.62]
---------------------------
I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.791 | Acc: 79.500% | Wgt Acc: 82.735%
	I - Batch: 100 | Loss: 0.791 | Acc: 78.688% | Wgt Acc: 82.061%
	I - Batch: 150 | Loss: 0.783 | Acc: 79.542% | Wgt Acc: 83.032%
	I - Batch: 200 | Loss: 0.789 | Acc: 79.406% | Wgt Acc: 82.797%
I - num batch: 222
I - Train -- Loss: 0.789 | Acc: 79.335% | Wgt Acc: 82.667% | LR: 1.250000e-04 | Dur: 122.42s
I - Confusion Matrix: [row->prediction - col->label]
[[606.   2.   5.  52. 103.]
 [ 11. 524.  44.  11.  84.]
 [  5.  30. 630.  16. 128.]
 [ 31.   5.  12. 442.  73.]
 [ 44.  17.  43.  17. 612.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.215 | Acc: 55.388% | Wgt Acc: 55.569% | Dur: 12.51s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  6.  5. 24. 16.]
 [ 4. 43. 10.  4.  9.]
 [ 1. 11. 34.  5.  5.]
 [15.  8. 10. 45.  5.]
 [ 6. 10. 16.  8. 37.]]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.801 | Acc: 79.000% | Wgt Acc: 82.314%
	I - Batch: 100 | Loss: 0.788 | Acc: 79.375% | Wgt Acc: 82.380%
	I - Batch: 150 | Loss: 0.794 | Acc: 79.375% | Wgt Acc: 82.394%
	I - Batch: 200 | Loss: 0.792 | Acc: 79.312% | Wgt Acc: 82.203%
I - num batch: 222
I - Train -- Loss: 0.794 | Acc: 78.968% | Wgt Acc: 82.005% | LR: 1.250000e-04 | Dur: 127.78s
I - Confusion Matrix: [row->prediction - col->label]
[[602.   4.   7.  57. 104.]
 [  9. 524.  49.  13.  81.]
 [  8.  25. 615.  13. 121.]
 [ 34.   3.  16. 434.  68.]
 [ 44.  22.  47.  21. 626.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.186 | Acc: 56.391% | Wgt Acc: 57.983% | Dur: 13.52s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  2.  2. 14. 10.]
 [ 3. 47. 18.  4. 11.]
 [ 2. 11. 29.  4.  9.]
 [21.  8. 14. 58.  8.]
 [ 5. 10. 12.  6. 34.]]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.773 | Acc: 81.500% | Wgt Acc: 85.105%
	I - Batch: 100 | Loss: 0.773 | Acc: 81.000% | Wgt Acc: 84.021%
	I - Batch: 150 | Loss: 0.780 | Acc: 80.750% | Wgt Acc: 83.753%
	I - Batch: 200 | Loss: 0.778 | Acc: 80.875% | Wgt Acc: 83.809%
I - num batch: 222
I - Train -- Loss: 0.779 | Acc: 80.575% | Wgt Acc: 83.614% | LR: 1.250000e-04 | Dur: 122.92s
I - Confusion Matrix: [row->prediction - col->label]
[[610.   2.   6.  56.  87.]
 [ 13. 539.  43.   5.  91.]
 [  8.  21. 621.  16. 103.]
 [ 34.   3.  17. 443.  74.]
 [ 32.  13.  47.  18. 645.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.257 | Acc: 52.882% | Wgt Acc: 53.403% | Dur: 12.70s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  5.  7. 31. 18.]
 [ 4. 42. 14.  5. 10.]
 [ 0. 11. 26.  3.  6.]
 [ 9.  8. 12. 43.  5.]
 [ 8. 12. 16.  4. 33.]]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.769 | Acc: 80.625% | Wgt Acc: 83.751%
	I - Batch: 100 | Loss: 0.768 | Acc: 80.375% | Wgt Acc: 83.911%
	I - Batch: 150 | Loss: 0.772 | Acc: 80.542% | Wgt Acc: 84.070%
	I - Batch: 200 | Loss: 0.774 | Acc: 80.188% | Wgt Acc: 83.722%
I - num batch: 222
I - Train -- Loss: 0.774 | Acc: 80.406% | Wgt Acc: 83.794% | LR: 1.250000e-04 | Dur: 124.10s
I - Confusion Matrix: [row->prediction - col->label]
[[612.   0.   8.  56. 101.]
 [ 10. 540.  40.   7.  87.]
 [  8.  16. 631.  16. 118.]
 [ 31.   2.  13. 446.  71.]
 [ 36.  20.  42.  13. 623.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.250 | Acc: 52.381% | Wgt Acc: 51.423% | Dur: 12.84s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  4. 28. 17.]
 [ 3. 31.  8.  4.  3.]
 [ 2. 16. 29.  2.  9.]
 [14.  6. 14. 44.  3.]
 [ 4. 19. 20.  8. 40.]]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.750 | Acc: 80.875% | Wgt Acc: 84.102%
	I - Batch: 100 | Loss: 0.783 | Acc: 78.938% | Wgt Acc: 82.220%
	I - Batch: 150 | Loss: 0.780 | Acc: 79.167% | Wgt Acc: 82.508%
	I - Batch: 200 | Loss: 0.780 | Acc: 79.406% | Wgt Acc: 82.618%
I - num batch: 222
I - Train -- Loss: 0.781 | Acc: 79.363% | Wgt Acc: 82.547% | LR: 1.250000e-04 | Dur: 122.73s
I - Confusion Matrix: [row->prediction - col->label]
[[598.   5.  10.  54. 112.]
 [ 13. 528.  36.   9.  84.]
 [  8.  21. 629.  17. 113.]
 [ 36.   6.   8. 438.  69.]
 [ 42.  18.  51.  20. 622.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.244 | Acc: 54.135% | Wgt Acc: 54.703% | Dur: 13.04s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  6.  4. 23. 14.]
 [ 3. 41. 12.  4.  7.]
 [ 1. 11. 24.  3. 10.]
 [11.  6. 16. 49.  6.]
 [ 6. 14. 19.  7. 35.]]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.697 | Acc: 86.750% | Wgt Acc: 88.163%
	I - Batch: 100 | Loss: 0.706 | Acc: 86.000% | Wgt Acc: 87.898%
	I - Batch: 150 | Loss: 0.709 | Acc: 85.583% | Wgt Acc: 87.547%
I - num batch: 173
I - Train -- Loss: 0.708 | Acc: 85.637% | Wgt Acc: 87.638% | LR: 1.250000e-04 | Dur: 98.46s
I - Confusion Matrix: [row->prediction - col->label]
[[636.   6.   9.  50.  32.]
 [  9. 534.  43.  17.  38.]
 [  6.  26. 660.  14.  51.]
 [ 31.   5.   9. 453.  12.]
 [ 15.   7.  13.   4.  84.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.270 | Acc: 51.629% | Wgt Acc: 53.094% | Dur: 12.68s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  5.  5. 31. 17.]
 [ 2. 37. 10.  5.  9.]
 [ 3. 25. 41. 10. 24.]
 [ 9.  8.  9. 37.  2.]
 [ 3.  3. 10.  3. 20.]]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.792 | Acc: 75.875% | Wgt Acc: 81.479%
	I - Batch: 100 | Loss: 0.779 | Acc: 78.125% | Wgt Acc: 82.615%
	I - Batch: 150 | Loss: 0.774 | Acc: 79.375% | Wgt Acc: 83.266%
	I - Batch: 200 | Loss: 0.776 | Acc: 79.188% | Wgt Acc: 83.079%
I - num batch: 222
I - Train -- Loss: 0.774 | Acc: 79.447% | Wgt Acc: 83.238% | LR: 1.250000e-04 | Dur: 125.50s
I - Confusion Matrix: [row->prediction - col->label]
[[615.   0.   9.  57. 119.]
 [  9. 539.  41.  10.  94.]
 [  7.  22. 637.  14. 120.]
 [ 30.   2.  11. 439.  79.]
 [ 36.  15.  36.  18. 588.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.237 | Acc: 57.143% | Wgt Acc: 56.807% | Dur: 12.23s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  3.  3. 20.  8.]
 [ 3. 44. 15.  5. 10.]
 [ 1. 10. 34.  3.  7.]
 [13.  2.  8. 48.  4.]
 [12. 19. 15. 10. 43.]]

I - Local maximum validation set accuracy:  57.14

I - Validation set results: 
[14-1-2-0.27][50-3-4-0.75][124-2-2-0.78][127-0-0-0.94][443-2-2-0.99][567-0-0-0.96][573-1-1-0.99][615-0-0-0.25][695-1-0-0.95][722-3-0-0.56]
[826-0-0-0.59][878-0-0-0.99][1103-0-4-0.24][1212-3-3-0.44][1368-0-0-0.99][2181-2-2-0.41][2476-2-1-0.51][2721-2-2-0.91][2818-1-4-0.01][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-1-0.99][3482-2-2-0.73][3536-3-1-0.26][3625-1-1-0.99][3909-0-0-0.23][4035-0-3-0.99][4140-0-1--0.67][4214-1-4-0.89][4346-1-3--0.25]
[4581-2-1-0.48][4708-3-3-0.53][4838-3-3--0.15][4845-1-1--0.46][4868-0-0-0.96][4939-0-2--0.46][4984-2-2--0.25][5078-1-4-0.14][5396-0-0-0.99][5479-1-1--0.15]
[5717-0-0-0.90][5843-1-1-0.99][5949-3-3-0.90][5987-2-4-0.99][6014-3-3-0.99][6033-3-3--0.53][6313-0-3-0.13][6421-3-3-0.05][6500-1-2-0.25][6583-3-3-0.56]
[6683-3-4--0.72][6825-2-3-0.26][6998-3-0--0.48][7049-3-3-0.50][7517-1-1-0.90][7521-1-1-0.99][7528-1-3-0.97][7949-1-2-0.51][8135-1-0-0.97][8185-3-0-0.88]
[8269-3-1-0.99][8273-3-3-0.90][8543-3-0-0.99][8666-1-1-0.53][8672-0-0-0.99][8ai903-1-1-0.53][9001-2-1-0.99][9036-2-1-0.56][9281-3-4-0.90][9300-2-2-0.99]
[9571-0-4-0.46][9617-1-4--0.19][9644-2-2-0.68][9705-2-4-0.99][9801-0-3--0.12][9803-3-3-0.99][9865-3-0-0.85][9896-2-4-0.99][10314-1-1-0.61][10337-3-3-0.91]
[10403-0-4-0.98][10653-2-1-0.62][10704-2-1-0.37][10719-1-1-0.79][10727-1-4-0.99][10836-0-0-0.99][10969-2-2-0.36][11042-0-0--0.08][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.88][11502-3-0-0.98][11512-3-3-0.77][11608-1-2-0.54][11610-0-0-0.88][11692-0-0-0.79][11905-0-0-0.99][11993-1-2-0.50][12002-2-3-0.42]
[12052-0-0-0.95][12201-0-0-0.99][12235-2-4-0.85][12320-1-4-0.66][12377-2-4-0.99][12398-2-2-0.29][12503-1-4--0.11][12617-0-1-0.99][12685-3-3-0.66][12738-2-3-0.44]
[12742-2-2-0.48][12823-0-3-0.84][13110-1-1-0.73][13240-3-0-0.71][13253-1-1-0.99][13273-0-0-0.99][13634-1-4-0.88][13763-2-3--0.15][13905-3-3--0.24][14060-2-4-0.56]
[14065-3-0-0.95][14147-3-0-0.29][14595-2-2-0.64][14687-2-3-0.77][14788-2-1-0.77][14869-1-4-0.60][14872-3-4-0.54][14877-1-1-0.99][14927-0-3-0.55][15066-0-0-0.99]
[15175-1-4-0.90][15178-2-4-0.51][15375-3-0-0.38][15389-3-3-0.99][15568-2-1-0.80][15675-3-3-0.90][15869-1-1--0.37][16207-3-0-0.99][16236-0-0--0.17][16302-3-4-0.92]
[16331-2-2-0.54][16381-0-0-0.66][16488-1-1-0.48][16495-0-0--0.37][16650-0-0-0.99][16719-1-2-0.81][16801-0-0-0.99][16828-0-0-0.99][17137-3-3--0.09][17245-1-4-0.72]
[17278-3-2--0.47][17282-0-0-0.48][17311-2-2-0.89][17336-2-2-0.34][17608-3-3-0.99][17627-0-0--0.33][17877-3-4-0.62][17924-1-4-0.64][17984-3-0-0.99][18211-0-3-0.16]
[18276-3-0-0.99][18287-1-1-0.52][18394-0-0-0.99][18428-0-0-0.99][18442-0-3-0.72][18478-3-3-0.53][18607-0-0-0.99][18616-0-0--0.27][18663-0-0-0.88][18718-0-0-0.99]
[18766-2-2-0.95][18824-2-4-0.99][18890-3-3--0.19][18930-3-4-0.16][18938-3-3--0.30][19817-1-2-0.75][19839-0-4-0.99][19930-3-0-0.65][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-3-0.43][20474-1-1-0.75][20547-3-0-0.15][20929-2-2-0.97][21245-1-1-0.22][21257-3-4-0.21][21293-1-1-0.99][21316-1-1-0.94][21384-1-4-0.99][21448-1-1-0.70]
[21483-0-0-0.99][21487-2-2-0.79][21714-0-0-0.97][21943-3-4-0.67][21947-0-0-0.68][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.59][22025-0-4-0.50][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.94][22757-0-0-0.99][22811-3-3-0.99][22976-3-1-0.76][22985-3-3-0.77][23014-0-0-0.97][23112-1-1-0.97][23144-3-3-0.94][23168-2-0-0.01]
[23219-0-0-0.99][23363-3-3-0.99][23470-0-4-0.28][23486-2-2-0.38][23497-0-3-0.99][23516-0-0-0.99][23690-1-1-0.82][23921-2-1-0.67][23936-1-2-0.98][24040-3-4-0.99]
[24111-1-1-0.99][24182-0-3-0.99][24238-3-3-0.97][24290-2-0-0.93][24345-0-0-0.89][24364-1-2-0.99][24427-3-0-0.91][24477-2-4-0.99][24495-2-1-0.26][24893-2-2--0.20]
[25012-1-1-0.19][25121-2-4-0.90][25165-3-3-0.64][25183-0-0-0.17][25297-3-3--0.20][25398-0-0-0.96][25574-2-2-0.18][25644-1-1-0.99][25718-1-4-0.52][25774-2-2--0.17]
[26032-3-3-0.65][26051-3-3-0.72][26120-0-4-0.90][26321-1-1-0.91][26732-1-1-0.70][26784-3-3-0.96][26827-3-3-0.97][26833-0-3-0.99][26838-2-2-0.29][26860-1-4-0.99]
[26948-0-0--0.31][27049-3-0-0.99][27098-1-0--0.41][27526-0-0-0.38][27639-3-0-0.50][27698-3-3-0.54][27772-0-0-0.99][27890-1-1-0.82][28040-0-4-0.83][28503-2-2-0.97]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-2--0.81][29777-0-0-0.99][29877-2-2-0.05][30035-1-1-0.99][30098-0-3-0.35][30326-1-1-0.99][30572-2-3-0.86][30716-0-4-0.99]
[30806-2-3-0.76][30906-1-1-0.99][31007-0-0-0.98][31181-3-3-0.50][31238-0-3-0.94][31347-0-0-0.99][31422-2-4-0.58][31429-3-3-0.56][31431-0-4-0.13][31432-1-1-0.99]
[31477-0-0-0.99][31524-1-4-0.06][31597-1-2-0.47][31619-1-4-0.44][31701-0-0-0.88][31755-0-0--0.07][31854-3-3-0.47][32074-1-1-0.02][32078-3-3-0.99][32111-1-1-0.13]
[32127-1-1-0.46][32140-3-3-0.99][32263-2-4-0.42][32365-0-0-0.86][32411-2-0-0.99][32429-3-3-0.70][32473-3-0-0.99][32574-3-3-0.99][32584-0-0-0.56][32622-0-4-0.19]
[32858-3-0-0.99][32969-3-3-0.99][33016-2-2-0.97][33031-1-1-0.99][33035-2-4-0.95][33133-2-2-0.51][33173-2-1-0.18][33175-3-2-0.85][33306-3-1-0.99][33309-2-3-0.80]
[33474-0-1-0.43][33478-2-4-0.26][33618-1-1-0.99][33712-0-3-0.90][33782-2-4-0.99][33914-3-3-0.57][34076-3-1--0.23][34112-2-1-0.51][34138-2-2-0.98][34239-1-1--0.19]
[34364-2-1-0.98][34617-1-4-0.06][34751-3-3-0.99][34783-2-2-0.17][35015-3-3-0.64][35018-1-4-0.83][35288-2-2--0.52][214490-4-4-0.99][214508-4-4-0.89][214565-4-4--0.23]
[214581-4-4--0.44][214585-4-4-0.99][214625-4-4-0.99][214627-4-4-0.99][214688-4-2-0.99][214694-4-1-0.98][214815-4-4-0.93][214882-4-4-0.96][214894-4-4--0.05][214979-4-4-0.60]
[215082-4-4-0.46][215160-4-4-0.55][215171-4-4-0.99][215210-4-4-0.99][215245-4-3--0.44][215274-4-3-0.27][215365-4-1-0.90][215412-4-4-0.76][215694-4-4--0.21][215718-4-4-0.42]
[215757-4-4-0.99][215778-4-2--0.40][215911-4-4-0.94][215913-4-1-0.99][215950-4-4-0.36][215974-4-4--0.12][215988-4-0--0.13][216086-4-4-0.99][216138-4-2-0.22][216343-4-4-0.12]
[216381-4-4-0.99][216394-4-4-0.52][216840-4-0-0.54][216895-4-4-0.88][217062-4-4-0.95][217108-4-0-0.19][217130-4-4-0.70][217143-4-4-0.99][217466-4-4-0.55][217504-4-4-0.99]
[217518-4-0-0.21][217644-4-1-0.97][217736-4-3-0.78][217752-4-1-0.06][217787-4-4-0.31][217797-4-0-0.03][218000-4-1-0.34][218030-4-1-0.87][218084-4-0-0.99][218157-4-1-0.61]
[218167-4-4-0.99][218290-4-1-0.98][218293-4-4-0.99][218478-4-1-0.72][218580-4-2-0.39][218649-4-4-0.12][218799-4-0-0.16][218966-4-4-0.99][219102-4-4--0.23][219166-4-4-0.99]
[219274-4-2-0.99][219568-4-4-0.83][219588-4-4-0.92][219635-4-3-0.32][219648-4-4-0.99][219661-4-2-0.99][219668-4-2--0.22][219740-4-0-0.04][219745-4-4--0.34]
---------------------------
I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.756 | Acc: 82.500% | Wgt Acc: 85.184%
	I - Batch: 100 | Loss: 0.766 | Acc: 81.250% | Wgt Acc: 84.426%
	I - Batch: 150 | Loss: 0.761 | Acc: 81.000% | Wgt Acc: 84.306%
	I - Batch: 200 | Loss: 0.765 | Acc: 80.750% | Wgt Acc: 84.096%
I - num batch: 222
I - Train -- Loss: 0.767 | Acc: 80.772% | Wgt Acc: 83.945% | LR: 1.250000e-04 | Dur: 123.00s
I - Confusion Matrix: [row->prediction - col->label]
[[615.   4.   8.  50. 102.]
 [ 10. 535.  43.  10.  84.]
 [  8.  18. 625.   7. 114.]
 [ 28.   1.  12. 451.  61.]
 [ 36.  20.  46.  20. 639.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.220 | Acc: 53.885% | Wgt Acc: 53.094% | Dur: 13.39s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  3. 27. 13.]
 [ 2. 37. 11.  3.  6.]
 [ 0. 13. 31.  4.  8.]
 [12.  4. 12. 43.  4.]
 [11. 19. 18.  9. 41.]]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.755 | Acc: 82.125% | Wgt Acc: 85.243%
	I - Batch: 100 | Loss: 0.754 | Acc: 81.750% | Wgt Acc: 85.022%
	I - Batch: 150 | Loss: 0.755 | Acc: 81.750% | Wgt Acc: 85.149%
	I - Batch: 200 | Loss: 0.756 | Acc: 81.500% | Wgt Acc: 84.837%
I - num batch: 222
I - Train -- Loss: 0.757 | Acc: 81.195% | Wgt Acc: 84.681% | LR: 1.250000e-04 | Dur: 123.02s
I - Confusion Matrix: [row->prediction - col->label]
[[630.   4.   8.  50. 104.]
 [  6. 536.  33.  11.  93.]
 [  8.  17. 638.  11. 117.]
 [ 22.   4.   9. 454.  64.]
 [ 31.  17.  46.  12. 622.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.202 | Acc: 57.393% | Wgt Acc: 57.611% | Dur: 12.87s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  3.  3. 11.  9.]
 [ 4. 44. 15.  5.  7.]
 [ 0. 12. 31.  5.  8.]
 [19.  4. 10. 55.  6.]
 [ 8. 15. 16. 10. 42.]]

I - Local maximum validation set accuracy:  57.39

I - Validation set results: 
[14-1-2-0.21][50-3-4-0.16][124-2-2-0.70][127-0-0-0.99][443-2-2-0.99][567-0-0-0.77][573-1-1-0.99][615-0-3-0.67][695-1-0-0.69][722-3-3-0.99]
[826-0-0-0.38][878-0-0-0.99][1103-0-4-0.14][1212-3-3-0.23][1368-0-0-0.99][2181-2-2-0.52][2476-2-1-0.68][2721-2-2-0.93][2818-1-4--0.87][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-1-0.99][3482-2-2-0.93][3536-3-3-0.51][3625-1-1-0.99][3909-0-0-0.87][4035-0-3-0.99][4140-0-0--0.07][4214-1-3-0.32][4346-1-3-0.13]
[4581-2-2-0.76][4708-3-3-0.43][4838-3-3-0.15][4845-1-1-0.36][4868-0-0-0.82][4939-0-4--0.64][4984-2-2-0.14][5078-1-4-0.33][5396-0-0-0.99][5479-1-1-0.20]
[5717-0-0-0.77][5843-1-1-0.96][5949-3-3-0.99][5987-2-4-0.99][6014-3-3-0.97][6033-3-0--0.59][6313-0-3-0.35][6421-3-3-0.81][6500-1-2--0.37][6583-3-3-0.69]
[6683-3-3--0.30][6825-2-3-0.62][6998-3-1--0.39][7049-3-3-0.67][7517-1-1-0.99][7521-1-1-0.56][7528-1-3-0.97][7949-1-2-0.53][8135-1-0-0.61][8185-3-3-0.78]
[8269-3-4-0.51][8273-3-3-0.94][8543-3-0-0.99][8666-1-1-0.99][8672-0-3-0.48][8903-1-2-0.99][9001-2-1-0.99][9036-2-1-0.94][9281-3-4-0.72][9300-2-2-0.97]
[9571-0-3-0.33][9617-1-4-0.79][9644-2-2-0.91][9705-2-4-0.94][9801-0-3-0.56][9803-3-3-0.99][9865-3-3-0.78][9896-2-4-0.99][10314-1-1-0.77][10337-3-3-0.63]
[10403-0-4-0.98][10653-2-1-0.38][10704-2-1-0.87][10719-1-1-0.97][10727-1-4-0.56][10836-0-0-0.99][10969-2-2-0.25][11042-0-0-0.62][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-4-0.78][11499-0-3-0.29][11502-3-3-0.85][11512-3-3-0.50][11608-1-1-0.66][11610-0-0-0.69][11692-0-0-0.89][11905-0-0-0.99][11993-1-1-0.30][12002-2-0-0.29]
[12052-0-0-0.93][12201-0-3-0.99][12235-2-4-0.60][12320-1-4-0.99][12377-2-4-0.99][12398-2-2-0.81][12503-1-1-0.94][12617-0-1-0.98][12685-3-1--0.00][12738-2-4--0.06]
[12742-2-1-0.39][12823-0-0-0.99][13110-1-1-0.99][13240-3-4-0.17][13253-1-1-0.99][13273-0-0-0.99][13634-1-4-0.77][13763-2-3--0.09][13905-3-3-0.00][14060-2-4-0.86]
[14065-3-0-0.65][14147-3-3--0.17][14595-2-2-0.98][14687-2-3-0.93][14788-2-1-0.23][14869-1-1-0.57][14872-3-4-0.10][14877-1-1-0.98][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-4-0.28][15178-2-3-0.61][15375-3-3-0.43][15389-3-3-0.99][15568-2-1-0.89][15675-3-3-0.99][15869-1-1--0.26][16207-3-0-0.99][16236-0-0-0.05][16302-3-4-0.15]
[16331-2-2-0.67][16381-0-0-0.23][16488-1-1-0.88][16495-0-0-0.99][16650-0-0-0.99][16719-1-2-0.91][16801-0-0-0.99][16828-0-0-0.97][17137-3-3-0.70][17245-1-4--0.67]
[17278-3-4-0.52][17282-0-0-0.88][17311-2-2-0.97][17336-2-1-0.64][17608-3-3-0.99][17627-0-0--0.31][17877-3-4-0.59][17924-1-4-0.25][17984-3-3-0.99][18211-0-3-0.46]
[18276-3-0-0.96][18287-1-1-0.99][18394-0-0-0.99][18428-0-0-0.87][18442-0-3-0.95][18478-3-3-0.92][18607-0-0-0.99][18616-0-0-0.15][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.71][18824-2-4-0.99][18890-3-2--0.30][18930-3-4-0.56][18938-3-3-0.73][19817-1-2-0.72][19839-0-4-0.99][19930-3-3--0.09][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-3-0.46][20474-1-1-0.96][20547-3-0-0.27][20929-2-2-0.93][21245-1-1-0.85][21257-3-1--0.42][21293-1-1-0.99][21316-1-1-0.98][21384-1-4-0.91][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-1-0.95][21714-0-0-0.36][21943-3-2-0.99][21947-0-0-0.67][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.93][22025-0-3-0.60][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.94][22757-0-0-0.99][22811-3-3-0.99][22976-3-2-0.21][22985-3-3-0.90][23014-0-3-0.94][23112-1-1-0.99][23144-3-3-0.99][23168-2-0-0.80]
[23219-0-0-0.84][23363-3-3-0.99][23470-0-1-0.19][23486-2-2--0.03][23497-0-0-0.99][23516-0-0-0.91][23690-1-1--0.26][23921-2-4-0.76][23936-1-2-0.95][24040-3-4-0.99]
[24111-1-1-0.99][24182-0-3-0.99][24238-3-3-0.99][24290-2-3-0.53][24345-0-0-0.28][24364-1-2-0.99][24427-3-0-0.99][24477-2-2-0.68][24495-2-1-0.10][24893-2-1-0.04]
[25012-1-2--0.27][25121-2-4-0.99][25165-3-3-0.95][25183-0-3--0.83][25297-3-3-0.58][25398-0-0-0.87][25574-2-2-0.31][25644-1-1-0.71][25718-1-1-0.11][25774-2-4-0.45]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.96][26321-1-4-0.17][26732-1-1-0.99][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-2-0.57][26860-1-4-0.99]
[26948-0-0-0.43][27049-3-0-0.99][27098-1-0--0.47][27526-0-0-0.71][27639-3-0-0.19][27698-3-3-0.84][27772-0-0-0.89][27890-1-1-0.93][28040-0-0-0.46][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3--0.93][29777-0-0-0.99][29877-2-3-0.69][30035-1-1-0.76][30098-0-3-0.76][30326-1-1-0.99][30572-2-3-0.98][30716-0-4-0.97]
[30806-2-3-0.87][30906-1-1-0.68][31007-0-0-0.71][31181-3-3-0.99][31238-0-3-0.86][31347-0-0-0.94][31422-2-4-0.76][31429-3-3-0.31][31431-0-0--0.19][31432-1-1-0.89]
[31477-0-0-0.99][31524-1-4--0.52][31597-1-2-0.40][31619-1-4-0.35][31701-0-0-0.70][31755-0-1--0.08][31854-3-3-0.69][32074-1-3--0.06][32078-3-3-0.99][32111-1-1-0.56]
[32127-1-2-0.27][32140-3-3-0.99][32263-2-4-0.25][32365-0-0-0.18][32411-2-0-0.99][32429-3-3-0.92][32473-3-0-0.93][32574-3-3-0.99][32584-0-0-0.55][32622-0-4-0.22]
[32858-3-0-0.72][32969-3-3-0.99][33016-2-2-0.99][33031-1-1-0.98][33035-2-2-0.99][33133-2-2-0.64][33173-2-2--0.06][33175-3-2-0.99][33306-3-1-0.99][33309-2-3-0.72]
[33474-0-1-0.33][33478-2-3-0.09][33618-1-1-0.88][33712-0-3-0.74][33782-2-4-0.99][33914-3-3-0.37][34076-3-1-0.68][34112-2-1-0.78][34138-2-2-0.94][34239-1-1-0.82]
[34364-2-2-0.97][34617-1-4-0.83][34751-3-3-0.99][34783-2-4-0.99][35015-3-2-0.99][35018-1-2-0.50][35288-2-2--0.09][214490-4-4-0.93][214508-4-4-0.66][214565-4-2--0.09]
[214581-4-2--0.26][214585-4-4-0.99][214625-4-4-0.95][214627-4-4-0.99][214688-4-2-0.99][214694-4-4-0.49][214815-4-4-0.83][214882-4-2-0.20][214894-4-4--0.29][214979-4-4--0.00]
[215082-4-4-0.80][215160-4-0-0.53][215171-4-4-0.99][215210-4-4-0.47][215245-4-3-0.45][215274-4-3-0.89][215365-4-1-0.82][215412-4-4-0.93][215694-4-4--0.35][215718-4-0--0.21]
[215757-4-4-0.99][215778-4-2--0.30][215911-4-4-0.72][215913-4-1-0.99][215950-4-3--0.24][215974-4-3-0.29][215988-4-0--0.19][216086-4-4-0.99][216138-4-4-0.09][216343-4-4-0.16]
[216381-4-4-0.99][216394-4-4--0.00][216840-4-0-0.33][216895-4-4-0.98][217062-4-4-0.92][217108-4-0-0.25][217130-4-4-0.99][217143-4-4-0.94][217466-4-1-0.13][217504-4-4-0.99]
[217518-4-4--0.40][217644-4-1-0.99][217736-4-3-0.75][217752-4-4-0.76][217787-4-4-0.23][217797-4-4-0.62][218000-4-2-0.76][218030-4-1-0.99][218084-4-0-0.99][218157-4-4-0.64]
[218167-4-4-0.99][218290-4-1-0.80][218293-4-4-0.99][218478-4-1-0.84][218580-4-4-0.83][218649-4-4--0.07][218799-4-0--0.13][218966-4-4-0.99][219102-4-0--0.04][219166-4-4-0.99]
[219274-4-4-0.83][219568-4-4-0.98][219588-4-4-0.85][219635-4-3-0.59][219648-4-4-0.99][219661-4-2-0.74][219668-4-2--0.37][219740-4-0--0.05][219745-4-4-0.58]
---------------------------
I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.742 | Acc: 81.875% | Wgt Acc: 85.583%
	I - Batch: 100 | Loss: 0.756 | Acc: 81.750% | Wgt Acc: 85.081%
	I - Batch: 150 | Loss: 0.757 | Acc: 81.583% | Wgt Acc: 84.646%
	I - Batch: 200 | Loss: 0.764 | Acc: 80.969% | Wgt Acc: 84.072%
I - num batch: 222
I - Train -- Loss: 0.760 | Acc: 81.393% | Wgt Acc: 84.538% | LR: 1.250000e-04 | Dur: 123.74s
I - Confusion Matrix: [row->prediction - col->label]
[[616.   3.   7.  47.  97.]
 [  7. 532.  33.   9.  79.]
 [  8.  20. 643.  10. 111.]
 [ 33.   4.  14. 453.  70.]
 [ 33.  19.  37.  19. 643.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.250 | Acc: 53.634% | Wgt Acc: 52.166% | Dur: 12.96s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  4. 20.  9.]
 [ 6. 35. 13.  3.  8.]
 [ 0. 15. 27.  6.  6.]
 [13.  4.  7. 46.  2.]
 [10. 20. 24. 11. 47.]]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.733 | Acc: 82.875% | Wgt Acc: 85.595%
	I - Batch: 100 | Loss: 0.747 | Acc: 81.688% | Wgt Acc: 84.674%
	I - Batch: 150 | Loss: 0.748 | Acc: 81.208% | Wgt Acc: 84.732%
	I - Batch: 200 | Loss: 0.751 | Acc: 81.188% | Wgt Acc: 84.596%
I - num batch: 222
I - Train -- Loss: 0.749 | Acc: 81.336% | Wgt Acc: 84.651% | LR: 1.250000e-04 | Dur: 123.54s
I - Confusion Matrix: [row->prediction - col->label]
[[612.   1.   4.  47. 102.]
 [  7. 536.  25.   7.  82.]
 [  7.  24. 649.  12. 118.]
 [ 32.   3.  12. 454.  64.]
 [ 39.  14.  44.  18. 634.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.198 | Acc: 53.885% | Wgt Acc: 55.260% | Dur: 12.57s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  4. 19. 14.]
 [ 3. 46. 15.  8. 14.]
 [ 1.  9. 31.  4.  8.]
 [20.  7.  9. 49.  5.]
 [ 6. 12. 16.  6. 31.]]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.755 | Acc: 82.000% | Wgt Acc: 84.680%
	I - Batch: 100 | Loss: 0.751 | Acc: 82.188% | Wgt Acc: 85.153%
	I - Batch: 150 | Loss: 0.752 | Acc: 82.458% | Wgt Acc: 85.661%
	I - Batch: 200 | Loss: 0.748 | Acc: 82.438% | Wgt Acc: 85.744%
I - num batch: 222
I - Train -- Loss: 0.747 | Acc: 82.267% | Wgt Acc: 85.628% | LR: 1.250000e-04 | Dur: 122.70s
I - Confusion Matrix: [row->prediction - col->label]
[[630.   1.   7.  45.  97.]
 [  8. 544.  32.   8.  77.]
 [  6.  18. 645.  11. 119.]
 [ 24.   1.   7. 458.  66.]
 [ 29.  14.  43.  16. 641.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.219 | Acc: 55.138% | Wgt Acc: 53.527% | Dur: 12.58s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  3.  3. 14.  4.]
 [ 4. 40. 11.  4.  8.]
 [ 1. 13. 35. 10.  8.]
 [17.  6.  5. 43.  3.]
 [13. 16. 21. 15. 49.]]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 82.500% | Wgt Acc: 85.074%
	I - Batch: 100 | Loss: 0.751 | Acc: 82.312% | Wgt Acc: 85.608%
	I - Batch: 150 | Loss: 0.748 | Acc: 82.000% | Wgt Acc: 85.375%
	I - Batch: 200 | Loss: 0.749 | Acc: 81.719% | Wgt Acc: 85.099%
I - num batch: 222
I - Train -- Loss: 0.748 | Acc: 81.646% | Wgt Acc: 84.997% | LR: 1.250000e-04 | Dur: 121.12s
I - Confusion Matrix: [row->prediction - col->label]
[[622.   0.   5.  47. 118.]
 [  9. 548.  33.   8.  80.]
 [  6.  20. 648.  18. 101.]
 [ 28.   1.   9. 444.  67.]
 [ 32.   9.  39.  21. 634.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.225 | Acc: 54.386% | Wgt Acc: 53.899% | Dur: 13.23s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  5.  2. 15. 12.]
 [ 3. 35. 11.  3.  5.]
 [ 0. 16. 26.  7.  7.]
 [16.  7. 14. 52.  6.]
 [ 7. 15. 22.  9. 42.]]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.682 | Acc: 86.875% | Wgt Acc: 89.048%
	I - Batch: 100 | Loss: 0.672 | Acc: 88.562% | Wgt Acc: 90.501%
	I - Batch: 150 | Loss: 0.678 | Acc: 87.750% | Wgt Acc: 89.824%
I - num batch: 173
I - Train -- Loss: 0.677 | Acc: 87.735% | Wgt Acc: 89.751% | LR: 1.250000e-04 | Dur: 98.47s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   2.   5.  41.  32.]
 [ 10. 555.  37.  11.  40.]
 [  5.  14. 667.  15.  41.]
 [ 27.   2.   6. 466.  11.]
 [ 11.   5.  19.   5.  93.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.238 | Acc: 51.880% | Wgt Acc: 54.765% | Dur: 13.03s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  5.  4. 19. 16.]
 [ 6. 45. 18.  8. 11.]
 [ 1. 15. 31.  6. 14.]
 [18.  8. 12. 52. 11.]
 [ 4.  5. 10.  1. 20.]]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.776 | Acc: 78.250% | Wgt Acc: 82.838%
	I - Batch: 100 | Loss: 0.757 | Acc: 80.000% | Wgt Acc: 84.642%
	I - Batch: 150 | Loss: 0.755 | Acc: 80.667% | Wgt Acc: 84.813%
	I - Batch: 200 | Loss: 0.753 | Acc: 81.188% | Wgt Acc: 85.196%
I - num batch: 222
I - Train -- Loss: 0.752 | Acc: 81.167% | Wgt Acc: 85.102% | LR: 1.250000e-04 | Dur: 124.39s
I - Confusion Matrix: [row->prediction - col->label]
[[626.   4.   7.  39. 128.]
 [  7. 542.  30.  12. 101.]
 [  5.  14. 653.  11. 107.]
 [ 30.   2.   7. 460.  66.]
 [ 29.  16.  37.  16. 598.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.262 | Acc: 51.880% | Wgt Acc: 50.990% | Dur: 12.55s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  4.  3. 22. 10.]
 [ 2. 38. 12.  4. 13.]
 [ 0. 17. 30.  9. 10.]
 [10.  3.  8. 36.  0.]
 [12. 16. 22. 15. 39.]]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.734 | Acc: 82.000% | Wgt Acc: 85.575%
	I - Batch: 100 | Loss: 0.742 | Acc: 82.438% | Wgt Acc: 85.743%
	I - Batch: 150 | Loss: 0.741 | Acc: 83.000% | Wgt Acc: 85.979%
	I - Batch: 200 | Loss: 0.739 | Acc: 83.500% | Wgt Acc: 86.368%
I - num batch: 222
I - Train -- Loss: 0.740 | Acc: 83.225% | Wgt Acc: 86.260% | LR: 1.250000e-04 | Dur: 122.86s
I - Confusion Matrix: [row->prediction - col->label]
[[630.   3.   6.  41.  87.]
 [  3. 553.  30.  13.  64.]
 [  6.  10. 656.  12. 107.]
 [ 22.   1.   8. 447.  76.]
 [ 36.  11.  34.  25. 666.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.221 | Acc: 54.637% | Wgt Acc: 53.280% | Dur: 12.82s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  6.  1. 18.  8.]
 [ 3. 35.  9.  6.  6.]
 [ 1. 15. 26.  3.  5.]
 [15.  5. 16. 50.  5.]
 [10. 17. 23.  9. 48.]]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.718 | Acc: 83.875% | Wgt Acc: 87.145%
	I - Batch: 100 | Loss: 0.730 | Acc: 83.250% | Wgt Acc: 85.983%
	I - Batch: 150 | Loss: 0.733 | Acc: 82.833% | Wgt Acc: 85.884%
	I - Batch: 200 | Loss: 0.734 | Acc: 83.000% | Wgt Acc: 85.985%
I - num batch: 222
I - Train -- Loss: 0.737 | Acc: 82.718% | Wgt Acc: 85.726% | LR: 1.250000e-04 | Dur: 124.67s
I - Confusion Matrix: [row->prediction - col->label]
[[620.   2.   5.  45.  97.]
 [  6. 551.  33.  10.  73.]
 [  7.  10. 638.   8.  96.]
 [ 34.   1.   9. 456.  65.]
 [ 30.  14.  49.  19. 669.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.220 | Acc: 54.135% | Wgt Acc: 54.270% | Dur: 12.25s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  5.  5. 25. 13.]
 [ 2. 39. 12.  4.  5.]
 [ 1. 15. 34.  8. 16.]
 [13.  7. 11. 44.  3.]
 [ 8. 12. 13.  5. 35.]]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.700 | Acc: 85.875% | Wgt Acc: 88.442%
	I - Batch: 100 | Loss: 0.726 | Acc: 83.750% | Wgt Acc: 86.739%
	I - Batch: 150 | Loss: 0.726 | Acc: 83.500% | Wgt Acc: 86.564%
	I - Batch: 200 | Loss: 0.727 | Acc: 83.594% | Wgt Acc: 86.647%
I - num batch: 222
I - Train -- Loss: 0.729 | Acc: 83.394% | Wgt Acc: 86.425% | LR: 1.250000e-04 | Dur: 119.87s
I - Confusion Matrix: [row->prediction - col->label]
[[638.   0.   8.  42.  99.]
 [  7. 544.  28.   9.  74.]
 [  6.  14. 647.  11. 106.]
 [ 22.   0.   4. 460.  52.]
 [ 24.  20.  47.  16. 669.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.211 | Acc: 55.890% | Wgt Acc: 56.064% | Dur: 12.77s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  8.  2. 26. 13.]
 [ 2. 41. 11.  2.  5.]
 [ 1. 12. 33.  5. 11.]
 [13.  7. 12. 47.  6.]
 [ 7. 10. 17.  6. 37.]]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.737 | Acc: 82.875% | Wgt Acc: 85.505%
	I - Batch: 100 | Loss: 0.734 | Acc: 82.938% | Wgt Acc: 85.864%
	I - Batch: 150 | Loss: 0.730 | Acc: 83.250% | Wgt Acc: 86.299%
	I - Batch: 200 | Loss: 0.730 | Acc: 83.094% | Wgt Acc: 86.450%
I - num batch: 222
I - Train -- Loss: 0.731 | Acc: 82.887% | Wgt Acc: 86.237% | LR: 1.250000e-04 | Dur: 127.36s
I - Confusion Matrix: [row->prediction - col->label]
[[633.   1.   6.  39.  86.]
 [  8. 553.  25.   7.  76.]
 [  6.   9. 644.  14. 117.]
 [ 20.   1.   9. 460.  71.]
 [ 30.  14.  50.  18. 650.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.244 | Acc: 54.887% | Wgt Acc: 53.713% | Dur: 13.11s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  6.  5. 24. 10.]
 [ 4. 43. 18.  7.  9.]
 [ 0.  7. 27.  6.  5.]
 [ 9.  3.  4. 41.  2.]
 [13. 19. 21.  8. 46.]]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.714 | Acc: 83.000% | Wgt Acc: 86.426%
	I - Batch: 100 | Loss: 0.726 | Acc: 82.938% | Wgt Acc: 86.301%
	I - Batch: 150 | Loss: 0.726 | Acc: 83.000% | Wgt Acc: 86.416%
	I - Batch: 200 | Loss: 0.723 | Acc: 83.188% | Wgt Acc: 86.688%
I - num batch: 222
I - Train -- Loss: 0.722 | Acc: 83.338% | Wgt Acc: 86.651% | LR: 1.250000e-04 | Dur: 124.57s
I - Confusion Matrix: [row->prediction - col->label]
[[632.   0.   5.  37.  92.]
 [  9. 554.  27.  11.  93.]
 [  5.  13. 649.  11. 102.]
 [ 22.   0.  11. 464.  56.]
 [ 29.  11.  42.  15. 657.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.221 | Acc: 54.887% | Wgt Acc: 56.374% | Dur: 13.13s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  4.  3. 27. 15.]
 [ 1. 45. 14.  3. 12.]
 [ 0. 10. 30.  4. 10.]
 [11.  8. 13. 46.  7.]
 [ 6. 11. 15.  6. 28.]]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.743 | Acc: 82.625% | Wgt Acc: 85.545%
	I - Batch: 100 | Loss: 0.716 | Acc: 84.875% | Wgt Acc: 87.583%
	I - Batch: 150 | Loss: 0.718 | Acc: 84.792% | Wgt Acc: 87.546%
	I - Batch: 200 | Loss: 0.724 | Acc: 84.094% | Wgt Acc: 87.231%
I - num batch: 222
I - Train -- Loss: 0.727 | Acc: 83.789% | Wgt Acc: 86.981% | LR: 1.250000e-04 | Dur: 129.05s
I - Confusion Matrix: [row->prediction - col->label]
[[632.   1.   6.  39. 105.]
 [  7. 554.  28.  15.  76.]
 [  7.   7. 658.  12.  95.]
 [ 24.   1.  10. 462.  58.]
 [ 27.  15.  32.  10. 666.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.225 | Acc: 55.388% | Wgt Acc: 55.136% | Dur: 13.40s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  6.  3. 19. 12.]
 [ 2. 41. 15.  6.  7.]
 [ 0. 13. 27.  6. 11.]
 [ 6.  4. 10. 46.  2.]
 [13. 14. 20.  9. 40.]]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.652 | Acc: 89.000% | Wgt Acc: 90.164%
	I - Batch: 100 | Loss: 0.652 | Acc: 88.750% | Wgt Acc: 90.351%
	I - Batch: 150 | Loss: 0.654 | Acc: 88.792% | Wgt Acc: 90.520%
I - num batch: 173
I - Train -- Loss: 0.652 | Acc: 88.712% | Wgt Acc: 90.467% | LR: 1.250000e-04 | Dur: 100.45s
I - Confusion Matrix: [row->prediction - col->label]
[[648.   0.   8.  44.  31.]
 [  4. 559.  27.  12.  27.]
 [ 11.  10. 675.  12.  41.]
 [ 23.   2.   5. 464.  12.]
 [ 11.   7.  19.   6. 106.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.219 | Acc: 54.887% | Wgt Acc: 57.673% | Dur: 13.44s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  3.  4. 17. 15.]
 [ 5. 40.  9.  6.  9.]
 [ 4. 22. 45.  8. 24.]
 [16. 10.  9. 54.  5.]
 [ 2.  3.  8.  1. 19.]]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.723 | Acc: 81.375% | Wgt Acc: 86.094%
	I - Batch: 100 | Loss: 0.713 | Acc: 83.062% | Wgt Acc: 87.161%
	I - Batch: 150 | Loss: 0.710 | Acc: 83.708% | Wgt Acc: 87.429%
	I - Batch: 200 | Loss: 0.715 | Acc: 83.688% | Wgt Acc: 87.398%
I - num batch: 222
I - Train -- Loss: 0.716 | Acc: 83.648% | Wgt Acc: 87.350% | LR: 1.250000e-04 | Dur: 126.23s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   0.   4.  39. 109.]
 [  7. 550.  24.   9.  82.]
 [  5.   9. 671.   8. 112.]
 [ 17.   1.   4. 469.  64.]
 [ 24.  18.  31.  13. 633.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.231 | Acc: 53.133% | Wgt Acc: 52.413% | Dur: 13.10s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  5.  3. 25. 14.]
 [ 2. 43. 19.  4.  5.]
 [ 0.  7. 20.  3.  7.]
 [13.  4. 10. 42.  3.]
 [ 9. 19. 23. 12. 43.]]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.719 | Acc: 85.000% | Wgt Acc: 87.425%
	I - Batch: 100 | Loss: 0.715 | Acc: 84.688% | Wgt Acc: 87.496%
	I - Batch: 150 | Loss: 0.712 | Acc: 84.542% | Wgt Acc: 87.295%
	I - Batch: 200 | Loss: 0.715 | Acc: 84.344% | Wgt Acc: 87.130%
I - num batch: 222
I - Train -- Loss: 0.716 | Acc: 84.184% | Wgt Acc: 87.034% | LR: 1.250000e-04 | Dur: 125.57s
I - Confusion Matrix: [row->prediction - col->label]
[[628.   0.   7.  40.  93.]
 [  6. 550.  26.   8.  70.]
 [  5.   9. 659.  14.  95.]
 [ 22.   2.   9. 461.  54.]
 [ 36.  17.  33.  15. 688.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.222 | Acc: 55.890% | Wgt Acc: 55.507% | Dur: 11.88s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  4.  1. 18. 10.]
 [ 1. 37. 12.  3.  9.]
 [ 0. 17. 33.  4.  9.]
 [13.  7. 12. 50.  3.]
 [12. 13. 17. 11. 41.]]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.721 | Acc: 84.500% | Wgt Acc: 87.550%
	I - Batch: 100 | Loss: 0.700 | Acc: 85.625% | Wgt Acc: 88.584%
	I - Batch: 150 | Loss: 0.703 | Acc: 85.250% | Wgt Acc: 88.351%
	I - Batch: 200 | Loss: 0.709 | Acc: 84.375% | Wgt Acc: 87.529%
I - num batch: 222
I - Train -- Loss: 0.712 | Acc: 84.071% | Wgt Acc: 87.297% | LR: 1.250000e-04 | Dur: 123.24s
I - Confusion Matrix: [row->prediction - col->label]
[[641.   2.   7.  36. 100.]
 [  6. 551.  24.  11.  64.]
 [  8.  12. 660.  14.  97.]
 [ 15.   2.   5. 465.  74.]
 [ 27.  11.  38.  12. 665.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.219 | Acc: 54.637% | Wgt Acc: 54.208% | Dur: 12.74s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  9.  5. 25. 18.]
 [ 3. 37. 13.  2.  3.]
 [ 0. 14. 28.  5.  8.]
 [10.  3. 11. 45.  4.]
 [ 6. 15. 18.  9. 39.]]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.711 | Acc: 84.625% | Wgt Acc: 88.269%
	I - Batch: 100 | Loss: 0.712 | Acc: 84.562% | Wgt Acc: 87.768%
	I - Batch: 150 | Loss: 0.712 | Acc: 83.875% | Wgt Acc: 87.322%
	I - Batch: 200 | Loss: 0.712 | Acc: 84.031% | Wgt Acc: 87.422%
I - num batch: 222
I - Train -- Loss: 0.710 | Acc: 84.268% | Wgt Acc: 87.590% | LR: 1.250000e-04 | Dur: 122.89s
I - Confusion Matrix: [row->prediction - col->label]
[[636.   0.   5.  32.  94.]
 [  5. 558.  25.  11.  79.]
 [  9.   5. 658.   7. 102.]
 [ 21.   0.   5. 471.  59.]
 [ 26.  15.  41.  17. 666.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.227 | Acc: 54.386% | Wgt Acc: 53.094% | Dur: 12.41s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  4.  1. 19.  8.]
 [ 3. 41. 14. 10.  8.]
 [ 0. 14. 31.  6. 10.]
 [11.  0.  4. 39.  1.]
 [13. 19. 25. 12. 45.]]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.683 | Acc: 84.750% | Wgt Acc: 88.455%
	I - Batch: 100 | Loss: 0.691 | Acc: 84.875% | Wgt Acc: 88.493%
	I - Batch: 150 | Loss: 0.704 | Acc: 84.375% | Wgt Acc: 87.809%
	I - Batch: 200 | Loss: 0.705 | Acc: 84.469% | Wgt Acc: 87.659%
I - num batch: 222
I - Train -- Loss: 0.706 | Acc: 84.466% | Wgt Acc: 87.613% | LR: 1.250000e-04 | Dur: 123.22s
I - Confusion Matrix: [row->prediction - col->label]
[[641.   1.   4.  36.  97.]
 [  5. 552.  22.   9.  68.]
 [  4.   9. 664.  14.  95.]
 [ 22.   0.   4. 466.  67.]
 [ 25.  16.  40.  13. 673.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.229 | Acc: 55.138% | Wgt Acc: 53.527% | Dur: 12.59s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  4.  3. 17.  8.]
 [ 4. 39. 13.  5.  8.]
 [ 1. 18. 34. 12.  6.]
 [10.  4.  3. 42.  2.]
 [16. 13. 22. 10. 48.]]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.712 | Acc: 84.125% | Wgt Acc: 87.912%
	I - Batch: 100 | Loss: 0.720 | Acc: 83.375% | Wgt Acc: 87.115%
	I - Batch: 150 | Loss: 0.712 | Acc: 83.833% | Wgt Acc: 87.356%
	I - Batch: 200 | Loss: 0.709 | Acc: 84.031% | Wgt Acc: 87.657%
I - num batch: 222
I - Train -- Loss: 0.708 | Acc: 84.099% | Wgt Acc: 87.665% | LR: 1.250000e-04 | Dur: 122.09s
I - Confusion Matrix: [row->prediction - col->label]
[[643.   4.   3.  29.  95.]
 [  4. 556.  24.  10.  80.]
 [  4.   8. 659.   8. 106.]
 [ 18.   0.   7. 475.  69.]
 [ 28.  10.  41.  16. 650.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.203 | Acc: 55.890% | Wgt Acc: 54.332% | Dur: 12.40s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  3. 15. 10.]
 [ 2. 34. 10.  5.  7.]
 [ 0. 14. 32.  5.  4.]
 [ 9.  5.  7. 48.  3.]
 [16. 20. 23. 13. 48.]]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.708 | Acc: 82.750% | Wgt Acc: 86.766%
	I - Batch: 100 | Loss: 0.707 | Acc: 83.688% | Wgt Acc: 87.373%
	I - Batch: 150 | Loss: 0.704 | Acc: 84.000% | Wgt Acc: 87.517%
	I - Batch: 200 | Loss: 0.708 | Acc: 84.031% | Wgt Acc: 87.342%
I - num batch: 222
I - Train -- Loss: 0.708 | Acc: 84.099% | Wgt Acc: 87.410% | LR: 1.250000e-04 | Dur: 122.87s
I - Confusion Matrix: [row->prediction - col->label]
[[634.   0.   5.  28.  97.]
 [  6. 553.  28.   6.  80.]
 [  6.   9. 657.  12.  93.]
 [ 27.   1.   4. 474.  65.]
 [ 24.  15.  40.  18. 665.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.197 | Acc: 54.637% | Wgt Acc: 53.589% | Dur: 12.76s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  3.  2. 15.  6.]
 [ 4. 42. 18.  7.  8.]
 [ 1. 12. 26.  6.  7.]
 [16.  5.  8. 48.  3.]
 [13. 16. 21. 10. 48.]]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.635 | Acc: 89.500% | Wgt Acc: 91.215%
	I - Batch: 100 | Loss: 0.628 | Acc: 90.188% | Wgt Acc: 92.053%
	I - Batch: 150 | Loss: 0.633 | Acc: 89.625% | Wgt Acc: 91.550%
I - num batch: 173
I - Train -- Loss: 0.634 | Acc: 89.870% | Wgt Acc: 91.583% | LR: 1.250000e-04 | Dur: 95.87s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   3.   9.  36.  32.]
 [  5. 559.  17.   9.  28.]
 [ 11.   9. 688.  12.  34.]
 [ 19.   2.   4. 473.  14.]
 [  7.   5.  16.   8. 109.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.220 | Acc: 54.135% | Wgt Acc: 57.054% | Dur: 13.06s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  3.  4. 16.  9.]
 [ 6. 47. 18.  5. 17.]
 [ 1. 18. 39.  8. 18.]
 [21.  6.  9. 53.  7.]
 [ 4.  4.  5.  4. 21.]]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.717 | Acc: 82.500% | Wgt Acc: 87.074%
	I - Batch: 100 | Loss: 0.706 | Acc: 83.875% | Wgt Acc: 87.850%
	I - Batch: 150 | Loss: 0.708 | Acc: 83.667% | Wgt Acc: 87.601%
	I - Batch: 200 | Loss: 0.700 | Acc: 84.719% | Wgt Acc: 88.395%
I - num batch: 222
I - Train -- Loss: 0.700 | Acc: 84.748% | Wgt Acc: 88.327% | LR: 1.250000e-04 | Dur: 124.57s
I - Confusion Matrix: [row->prediction - col->label]
[[639.   0.   5.  31. 105.]
 [  7. 559.  23.   5.  72.]
 [  5.   3. 672.  13.  99.]
 [ 22.   1.   4. 480.  68.]
 [ 24.  15.  30.   9. 656.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.207 | Acc: 55.890% | Wgt Acc: 55.012% | Dur: 12.44s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  4.  2. 21. 12.]
 [ 4. 42. 16.  5.  5.]
 [ 0. 12. 29.  5.  7.]
 [10.  4. 10. 45.  3.]
 [12. 16. 18. 10. 45.]]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.700 | Acc: 84.875% | Wgt Acc: 87.546%
	I - Batch: 100 | Loss: 0.708 | Acc: 84.562% | Wgt Acc: 87.749%
	I - Batch: 150 | Loss: 0.701 | Acc: 85.125% | Wgt Acc: 88.263%
	I - Batch: 200 | Loss: 0.704 | Acc: 84.875% | Wgt Acc: 87.889%
I - num batch: 222
I - Train -- Loss: 0.699 | Acc: 85.030% | Wgt Acc: 88.154% | LR: 1.250000e-04 | Dur: 122.47s
I - Confusion Matrix: [row->prediction - col->label]
[[637.   1.   5.  31.  89.]
 [  7. 556.  18.   6.  66.]
 [  8.   7. 666.   8. 102.]
 [ 19.   2.   5. 474.  60.]
 [ 26.  12.  40.  19. 683.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.231 | Acc: 55.388% | Wgt Acc: 55.384% | Dur: 12.95s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  7.  3. 24. 13.]
 [ 3. 42. 12.  5.  7.]
 [ 0. 11. 29.  5.  9.]
 [11.  4.  9. 45.  5.]
 [ 7. 14. 22.  7. 38.]]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.685 | Acc: 86.875% | Wgt Acc: 90.010%
	I - Batch: 100 | Loss: 0.691 | Acc: 86.250% | Wgt Acc: 89.213%
	I - Batch: 150 | Loss: 0.699 | Acc: 85.583% | Wgt Acc: 88.776%
	I - Batch: 200 | Loss: 0.698 | Acc: 84.969% | Wgt Acc: 88.232%
I - num batch: 222
I - Train -- Loss: 0.697 | Acc: 85.030% | Wgt Acc: 88.252% | LR: 1.250000e-04 | Dur: 123.43s
I - Confusion Matrix: [row->prediction - col->label]
[[638.   0.   7.  28. 102.]
 [  6. 555.  19.   6.  64.]
 [  8.   7. 667.   9.  93.]
 [ 18.   1.   5. 478.  63.]
 [ 27.  15.  36.  17. 678.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.196 | Acc: 56.140% | Wgt Acc: 55.941% | Dur: 12.80s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  1. 16.  9.]
 [ 1. 41. 11.  2.  7.]
 [ 1. 16. 31.  7. 11.]
 [18.  6. 11. 51.  3.]
 [ 9. 11. 21. 10. 42.]]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.697 | Acc: 85.375% | Wgt Acc: 88.797%
	I - Batch: 100 | Loss: 0.697 | Acc: 84.562% | Wgt Acc: 88.049%
	I - Batch: 150 | Loss: 0.696 | Acc: 84.708% | Wgt Acc: 87.992%
	I - Batch: 200 | Loss: 0.697 | Acc: 84.656% | Wgt Acc: 87.938%
I - num batch: 222
I - Train -- Loss: 0.699 | Acc: 84.748% | Wgt Acc: 88.101% | LR: 1.250000e-04 | Dur: 123.50s
I - Confusion Matrix: [row->prediction - col->label]
[[634.   3.   6.  28. 106.]
 [  5. 552.  14.   5.  81.]
 [  2.   7. 674.   8.  96.]
 [ 19.   1.   3. 479.  50.]
 [ 37.  15.  37.  18. 667.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.218 | Acc: 55.138% | Wgt Acc: 54.517% | Dur: 12.67s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  5. 21.  8.]
 [ 4. 49. 21. 10. 12.]
 [ 1.  9. 25.  8.  5.]
 [ 9.  4.  7. 38.  4.]
 [ 9. 12. 17.  9. 43.]]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.699 | Acc: 83.750% | Wgt Acc: 87.247%
	I - Batch: 100 | Loss: 0.693 | Acc: 85.438% | Wgt Acc: 88.359%
	I - Batch: 150 | Loss: 0.697 | Acc: 84.958% | Wgt Acc: 88.084%
	I - Batch: 200 | Loss: 0.691 | Acc: 85.500% | Wgt Acc: 88.770%
I - num batch: 222
I - Train -- Loss: 0.695 | Acc: 85.424% | Wgt Acc: 88.635% | LR: 1.250000e-04 | Dur: 124.22s
I - Confusion Matrix: [row->prediction - col->label]
[[640.   0.   5.  27.  95.]
 [  8. 559.  21.   8.  68.]
 [  7.   5. 669.  10.  87.]
 [ 19.   0.   1. 479.  67.]
 [ 23.  14.  38.  14. 683.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.224 | Acc: 52.882% | Wgt Acc: 53.403% | Dur: 12.64s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  4. 17. 16.]
 [ 3. 39. 15.  3.  7.]
 [ 0. 10. 25.  6.  8.]
 [16.  9. 12. 52.  5.]
 [10. 16. 19.  8. 36.]]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.673 | Acc: 86.625% | Wgt Acc: 89.720%
	I - Batch: 100 | Loss: 0.692 | Acc: 85.375% | Wgt Acc: 88.539%
	I - Batch: 150 | Loss: 0.688 | Acc: 85.417% | Wgt Acc: 88.880%
	I - Batch: 200 | Loss: 0.693 | Acc: 85.406% | Wgt Acc: 88.561%
I - num batch: 222
I - Train -- Loss: 0.697 | Acc: 85.312% | Wgt Acc: 88.537% | LR: 1.250000e-04 | Dur: 124.03s
I - Confusion Matrix: [row->prediction - col->label]
[[638.   1.   5.  29.  92.]
 [  7. 557.  17.   5.  72.]
 [  8.   6. 667.   8.  97.]
 [ 17.   0.  10. 482.  57.]
 [ 27.  14.  35.  14. 682.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.177 | Acc: 57.393% | Wgt Acc: 56.436% | Dur: 12.76s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  4.  1. 13.  7.]
 [ 2. 40. 10.  6.  3.]
 [ 0. 11. 30.  4.  8.]
 [17.  6. 15. 54.  5.]
 [13. 17. 19.  9. 49.]]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.674 | Acc: 85.750% | Wgt Acc: 89.135%
	I - Batch: 100 | Loss: 0.685 | Acc: 85.438% | Wgt Acc: 89.240%
	I - Batch: 150 | Loss: 0.688 | Acc: 85.208% | Wgt Acc: 88.901%
	I - Batch: 200 | Loss: 0.689 | Acc: 85.312% | Wgt Acc: 88.728%
I - num batch: 222
I - Train -- Loss: 0.691 | Acc: 85.227% | Wgt Acc: 88.635% | LR: 1.250000e-04 | Dur: 125.36s
I - Confusion Matrix: [row->prediction - col->label]
[[648.   1.   5.  25. 102.]
 [  5. 554.  14.   7.  73.]
 [  3.   8. 671.   6.  93.]
 [ 15.   1.   3. 482.  64.]
 [ 26.  14.  41.  18. 668.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.199 | Acc: 56.391% | Wgt Acc: 56.498% | Dur: 12.89s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  3.  1. 17. 11.]
 [ 3. 46. 13.  5.  6.]
 [ 0. 10. 24.  3.  8.]
 [16.  5. 13. 51.  5.]
 [ 7. 14. 24. 10. 42.]]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.622 | Acc: 90.875% | Wgt Acc: 92.378%
	I - Batch: 100 | Loss: 0.616 | Acc: 91.688% | Wgt Acc: 93.214%
	I - Batch: 150 | Loss: 0.619 | Acc: 91.458% | Wgt Acc: 93.103%
I - num batch: 173
I - Train -- Loss: 0.619 | Acc: 91.281% | Wgt Acc: 92.963% | LR: 1.250000e-04 | Dur: 97.37s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   2.   9.  30.  31.]
 [  4. 571.  19.  10.  24.]
 [  5.   2. 692.   8.  31.]
 [ 20.   0.   1. 485.  13.]
 [ 11.   3.  13.   5. 118.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.230 | Acc: 53.383% | Wgt Acc: 55.384% | Dur: 13.05s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  4. 26. 19.]
 [ 2. 43. 12.  5.  7.]
 [ 1. 19. 35.  7. 19.]
 [13.  4. 13. 46.  4.]
 [ 6.  6. 11.  2. 23.]]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.676 | Acc: 85.375% | Wgt Acc: 89.683%
	I - Batch: 100 | Loss: 0.684 | Acc: 84.812% | Wgt Acc: 89.081%
	I - Batch: 150 | Loss: 0.679 | Acc: 85.625% | Wgt Acc: 89.512%
	I - Batch: 200 | Loss: 0.681 | Acc: 85.562% | Wgt Acc: 89.271%
I - num batch: 222
I - Train -- Loss: 0.681 | Acc: 85.650% | Wgt Acc: 89.334% | LR: 1.250000e-04 | Dur: 124.55s
I - Confusion Matrix: [row->prediction - col->label]
[[648.   1.   4.  26. 106.]
 [  4. 557.  14.   4.  66.]
 [  7.   8. 683.   6. 100.]
 [ 13.   0.   3. 492.  70.]
 [ 25.  12.  30.  10. 658.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.246 | Acc: 53.634% | Wgt Acc: 52.413% | Dur: 12.43s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  5.  3. 19.  7.]
 [ 6. 43. 11.  8.  6.]
 [ 1. 14. 36. 13. 12.]
 [13.  2.  6. 36.  3.]
 [13. 14. 19. 10. 44.]]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.685 | Acc: 86.000% | Wgt Acc: 88.422%
	I - Batch: 100 | Loss: 0.679 | Acc: 86.125% | Wgt Acc: 89.095%
	I - Batch: 150 | Loss: 0.678 | Acc: 86.458% | Wgt Acc: 89.511%
	I - Batch: 200 | Loss: 0.675 | Acc: 86.781% | Wgt Acc: 89.855%
I - num batch: 222
I - Train -- Loss: 0.674 | Acc: 86.806% | Wgt Acc: 89.958% | LR: 1.250000e-04 | Dur: 124.96s
I - Confusion Matrix: [row->prediction - col->label]
[[654.   0.   4.  26.  89.]
 [  4. 562.  11.   6.  63.]
 [  2.   5. 679.  11.  83.]
 [ 16.   0.   5. 486.  67.]
 [ 21.  11.  35.   9. 698.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.206 | Acc: 57.143% | Wgt Acc: 56.993% | Dur: 13.30s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  5.  3. 18. 13.]
 [ 2. 44. 14.  7.  6.]
 [ 1. 12. 33.  7.  6.]
 [10.  4.  9. 45.  7.]
 [ 9. 13. 16.  9. 40.]]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.688 | Acc: 84.625% | Wgt Acc: 87.430%
	I - Batch: 100 | Loss: 0.672 | Acc: 86.188% | Wgt Acc: 89.089%
	I - Batch: 150 | Loss: 0.680 | Acc: 85.958% | Wgt Acc: 88.912%
	I - Batch: 200 | Loss: 0.681 | Acc: 85.938% | Wgt Acc: 89.004%
I - num batch: 222
I - Train -- Loss: 0.680 | Acc: 85.847% | Wgt Acc: 88.906% | LR: 1.250000e-04 | Dur: 128.03s
I - Confusion Matrix: [row->prediction - col->label]
[[630.   1.   4.  29.  97.]
 [  6. 556.  18.   7.  59.]
 [  4.   6. 676.   6.  89.]
 [ 22.   1.   4. 486.  58.]
 [ 35.  14.  32.  10. 697.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.233 | Acc: 53.885% | Wgt Acc: 51.918% | Dur: 13.15s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  7.  3. 22. 13.]
 [ 1. 40. 18.  5.  4.]
 [ 0.  7. 18.  3.  4.]
 [ 8.  4.  9. 39.  1.]
 [11. 20. 27. 17. 50.]]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.661 | Acc: 88.250% | Wgt Acc: 91.067%
	I - Batch: 100 | Loss: 0.675 | Acc: 86.188% | Wgt Acc: 89.092%
	I - Batch: 150 | Loss: 0.671 | Acc: 86.833% | Wgt Acc: 89.782%
	I - Batch: 200 | Loss: 0.667 | Acc: 87.062% | Wgt Acc: 90.022%
I - num batch: 222
I - Train -- Loss: 0.666 | Acc: 87.200% | Wgt Acc: 90.146% | LR: 1.250000e-04 | Dur: 126.25s
I - Confusion Matrix: [row->prediction - col->label]
[[646.   2.   6.  20.  80.]
 [  5. 561.  13.   6.  66.]
 [  5.   4. 678.   7.  81.]
 [ 16.   1.   4. 492.  57.]
 [ 25.  10.  33.  13. 716.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.224 | Acc: 53.885% | Wgt Acc: 53.032% | Dur: 12.99s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  6.  3. 19. 15.]
 [ 2. 35. 12.  2.  2.]
 [ 0. 15. 27.  6.  8.]
 [17.  7. 13. 50.  3.]
 [10. 15. 20.  9. 44.]]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.694 | Acc: 86.000% | Wgt Acc: 88.133%
	I - Batch: 100 | Loss: 0.686 | Acc: 86.062% | Wgt Acc: 88.650%
	I - Batch: 150 | Loss: 0.679 | Acc: 86.542% | Wgt Acc: 89.245%
	I - Batch: 200 | Loss: 0.675 | Acc: 86.906% | Wgt Acc: 89.765%
I - num batch: 222
I - Train -- Loss: 0.675 | Acc: 86.862% | Wgt Acc: 89.755% | LR: 1.250000e-04 | Dur: 123.89s
I - Confusion Matrix: [row->prediction - col->label]
[[651.   1.   5.  25.  87.]
 [  3. 554.  15.   8.  62.]
 [  4.   2. 677.   8.  84.]
 [ 15.   1.   4. 487.  55.]
 [ 24.  20.  33.  10. 712.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.241 | Acc: 53.133% | Wgt Acc: 49.752% | Dur: 12.77s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  2.  3. 21.  6.]
 [ 3. 27.  6.  3.  4.]
 [ 0. 15. 31.  3.  4.]
 [11.  5.  9. 39.  3.]
 [14. 29. 26. 20. 55.]]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.673 | Acc: 86.625% | Wgt Acc: 90.512%
	I - Batch: 100 | Loss: 0.674 | Acc: 86.312% | Wgt Acc: 89.870%
	I - Batch: 150 | Loss: 0.677 | Acc: 86.583% | Wgt Acc: 89.948%
	I - Batch: 200 | Loss: 0.674 | Acc: 86.781% | Wgt Acc: 90.069%
I - num batch: 222
I - Train -- Loss: 0.674 | Acc: 86.693% | Wgt Acc: 89.875% | LR: 1.250000e-04 | Dur: 126.43s
I - Confusion Matrix: [row->prediction - col->label]
[[652.   0.   3.  24.  84.]
 [  7. 558.  10.   4.  57.]
 [  4.   5. 681.   3.  98.]
 [ 14.   2.   5. 489.  66.]
 [ 20.  13.  35.  18. 695.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.192 | Acc: 58.145% | Wgt Acc: 56.498% | Dur: 12.29s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  3. 12.  2.]
 [ 3. 41. 10.  6.  9.]
 [ 0. 12. 33.  4.  5.]
 [14.  4.  9. 48.  4.]
 [13. 17. 20. 16. 52.]]

I - Local maximum validation set accuracy:  58.15

I - Validation set results: 
[14-1-2-0.28][50-3-4-0.20][124-2-2-0.77][127-0-0-0.99][443-2-2-0.99][567-0-0-0.96][573-1-1-0.99][615-0-3-0.29][695-1-0--0.08][722-3-3-0.99]
[826-0-0-0.72][878-0-3-0.75][1103-0-0-0.54][1212-3-4-0.11][1368-0-0-0.99][2181-2-2-0.31][2476-2-1-0.95][2721-2-2-0.94][2818-1-3-0.99][2886-2-1-0.99]
[3231-2-2-0.92][3333-2-3-0.76][3482-2-4-0.76][3536-3-1--0.42][3625-1-1-0.99][3909-0-0-0.20][4035-0-3-0.99][4140-0-0--0.72][4214-1-3-0.14][4346-1-3--0.96]
[4581-2-4-0.50][4708-3-4-0.50][4838-3-3-0.51][4845-1-1-0.48][4868-0-0-0.96][4939-0-4-0.31][4984-2-1--0.04][5078-1-4-0.88][5396-0-0-0.99][5479-1-1-0.42]
[5717-0-0-0.63][5843-1-1-0.86][5949-3-3-0.99][5987-2-4-0.99][6014-3-3-0.78][6033-3-0--0.54][6313-0-0--0.67][6421-3-3--0.25][6500-1-2-0.49][6583-3-2-0.95]
[6683-3-3--0.35][6825-2-3-0.73][6998-3-0--0.33][7049-3-2-0.85][7517-1-1-0.84][7521-1-1-0.83][7528-1-3-0.96][7949-1-1-0.20][8135-1-0-0.48][8185-3-3-0.29]
[8269-3-4-0.81][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.88][8672-0-3-0.73][8903-1-2-0.82][9001-2-1-0.99][9036-2-2--0.12][9281-3-4-0.27][9300-2-2-0.85]
[9571-0-4-0.30][9617-1-1-0.77][9644-2-2-0.93][9705-2-4-0.98][9801-0-3-0.42][9803-3-3-0.91][9865-3-3-0.42][9896-2-4-0.99][10314-1-0-0.46][10337-3-3-0.99]
[10403-0-4-0.99][10653-2-4-0.14][10704-2-1-0.20][10719-1-1-0.92][10727-1-4-0.99][10836-0-0-0.99][10969-2-2-0.43][11042-0-0-0.36][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.36][11502-3-3-0.66][11512-3-3-0.79][11608-1-1-0.71][11610-0-0-0.98][11692-0-0-0.72][11905-0-0-0.99][11993-1-1-0.40][12002-2-0-0.99]
[12052-0-0-0.99][12201-0-3-0.93][12235-2-4-0.99][12320-1-4-0.90][12377-2-4-0.99][12398-2-3--0.31][12503-1-4-0.99][12617-0-1-0.99][12685-3-1-0.18][12738-2-4-0.04]
[12742-2-2-0.45][12823-0-0-0.98][13110-1-1-0.52][13240-3-4--0.13][13253-1-2-0.99][13273-0-0-0.99][13634-1-4-0.86][13763-2-2-0.78][13905-3-3-0.08][14060-2-4-0.46]
[14065-3-0-0.52][14147-3-3-0.23][14595-2-2-0.99][14687-2-3-0.96][14788-2-1-0.20][14869-1-4-0.84][14872-3-4-0.51][14877-1-1-0.99][14927-0-3-0.78][15066-0-0-0.74]
[15175-1-4-0.99][15178-2-3-0.32][15375-3-3-0.15][15389-3-3-0.71][15568-2-1-0.73][15675-3-3-0.99][15869-1-1-0.82][16207-3-0-0.99][16236-0-0-0.86][16302-3-4-0.23]
[16331-2-2-0.86][16381-0-3--0.07][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-2-0.99][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.15][17245-1-4--0.66]
[17278-3-4--0.43][17282-0-0-0.20][17311-2-2-0.91][17336-2-2-0.56][17608-3-3-0.99][17627-0-0-0.06][17877-3-4-0.90][17924-1-4-0.76][17984-3-3-0.99][18211-0-3-0.60]
[18276-3-0-0.52][18287-1-1-0.87][18394-0-0-0.99][18428-0-0-0.69][18442-0-3-0.39][18478-3-3--0.20][18607-0-0-0.99][18616-0-0-0.33][18663-0-0-0.75][18718-0-0-0.99]
[18766-2-1-0.23][18824-2-4-0.99][18890-3-1--0.24][18930-3-4-0.60][18938-3-3--0.13][19817-1-2-0.60][19839-0-4-0.98][19930-3-0-0.16][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-3-0.22][20474-1-1--0.15][20547-3-0-0.30][20929-2-2-0.12][21245-1-1-0.88][21257-3-4-0.48][21293-1-1-0.99][21316-1-1-0.83][21384-1-4-0.99][21448-1-2-0.40]
[21483-0-0-0.99][21487-2-2-0.96][21714-0-0--0.12][21943-3-4-0.86][21947-0-0-0.72][21948-0-0-0.99][21965-2-2-0.68][21998-1-1-0.44][22025-0-4-0.27][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.09][22757-0-0-0.99][22811-3-3-0.16][22976-3-4-0.50][22985-3-3-0.70][23014-0-4-0.26][23112-1-1-0.95][23144-3-3-0.91][23168-2-0--0.17]
[23219-0-4-0.51][23363-3-3-0.85][23470-0-0--0.37][23486-2-2--0.03][23497-0-0-0.78][23516-0-0-0.94][23690-1-1--0.35][23921-2-4-0.81][23936-1-2-0.97][24040-3-4-0.99]
[24111-1-4-0.99][24182-0-3-0.81][24238-3-3-0.97][24290-2-3-0.13][24345-0-0--0.10][24364-1-2-0.99][24427-3-3-0.58][24477-2-2-0.85][24495-2-4-0.98][24893-2-1--0.22]
[25012-1-4-0.20][25121-2-4-0.48][25165-3-1-0.55][25183-0-4-0.04][25297-3-3-0.51][25398-0-0-0.34][25574-2-2-0.22][25644-1-1-0.99][25718-1-4-0.67][25774-2-2-0.43]
[26032-3-3-0.95][26051-3-3--0.01][26120-0-4-0.84][26321-1-1-0.68][26732-1-1-0.99][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-2-0.54][26860-1-4-0.99]
[26948-0-0-0.89][27049-3-0-0.99][27098-1-0--0.02][27526-0-0-0.99][27639-3-3-0.43][27698-3-3-0.66][27772-0-0-0.99][27890-1-4--0.02][28040-0-0-0.78][28503-2-4-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-4--0.14][29777-0-0-0.99][29877-2-3-0.31][30035-1-1-0.99][30098-0-0--0.15][30326-1-1-0.99][30572-2-2-0.51][30716-0-4-0.99]
[30806-2-3-0.75][30906-1-1-0.99][31007-0-0-0.61][31181-3-3-0.34][31238-0-3-0.94][31347-0-0-0.77][31422-2-4-0.99][31429-3-3-0.42][31431-0-0-0.04][31432-1-1-0.96]
[31477-0-0-0.99][31524-1-2--0.53][31597-1-1-0.59][31619-1-4--0.44][31701-0-0-0.73][31755-0-1--0.35][31854-3-3-0.42][32074-1-2-0.35][32078-3-3-0.99][32111-1-1-0.96]
[32127-1-1-0.55][32140-3-3-0.99][32263-2-4--0.19][32365-0-0-0.60][32411-2-0-0.98][32429-3-3-0.73][32473-3-0-0.99][32574-3-3-0.99][32584-0-4-0.55][32622-0-4-0.33]
[32858-3-0-0.48][32969-3-3-0.99][33016-2-2-0.95][33031-1-1-0.99][33035-2-2-0.87][33133-2-2-0.83][33173-2-2-0.50][33175-3-2-0.98][33306-3-1-0.99][33309-2-3-0.73]
[33474-0-1-0.42][33478-2-4-0.55][33618-1-1-0.86][33712-0-3-0.77][33782-2-4-0.88][33914-3-3-0.48][34076-3-1-0.74][34112-2-1-0.61][34138-2-2-0.99][34239-1-1-0.23]
[34364-2-2-0.99][34617-1-2-0.33][34751-3-3-0.58][34783-2-4-0.55][35015-3-2-0.99][35018-1-4-0.99][35288-2-2-0.39][214490-4-4-0.99][214508-4-4-0.81][214565-4-4-0.22]
[214581-4-0--0.33][214585-4-4-0.99][214625-4-4-0.99][214627-4-4-0.99][214688-4-4-0.74][214694-4-1-0.99][214815-4-4-0.84][214882-4-4-0.38][214894-4-4--0.00][214979-4-4-0.04]
[215082-4-4-0.82][215160-4-4-0.09][215171-4-4-0.99][215210-4-2-0.18][215245-4-1--0.60][215274-4-3-0.75][215365-4-4-0.85][215412-4-4-0.60][215694-4-4--0.02][215718-4-4-0.79]
[215757-4-4-0.99][215778-4-3--0.28][215911-4-4-0.95][215913-4-1-0.99][215950-4-4-0.88][215974-4-4-0.07][215988-4-4-0.29][216086-4-4-0.99][216138-4-4-0.69][216343-4-4-0.62]
[216381-4-4-0.99][216394-4-2--0.03][216840-4-4-0.66][216895-4-4-0.99][217062-4-4-0.56][217108-4-1--0.53][217130-4-4-0.99][217143-4-4-0.99][217466-4-4-0.03][217504-4-4-0.99]
[217518-4-4--0.68][217644-4-1-0.99][217736-4-3-0.84][217752-4-1-0.52][217787-4-4-0.99][217797-4-4-0.63][218000-4-4-0.07][218030-4-1-0.77][218084-4-0-0.41][218157-4-4-0.93]
[218167-4-4-0.99][218290-4-1-0.96][218293-4-4-0.93][218478-4-2-0.98][218580-4-4-0.16][218649-4-4--0.12][218799-4-4--0.20][218966-4-4-0.99][219102-4-4--0.35][219166-4-4-0.99]
[219274-4-4-0.99][219568-4-4-0.87][219588-4-4-0.86][219635-4-1--0.32][219648-4-4-0.99][219661-4-2-0.87][219668-4-3--0.43][219740-4-4--0.30][219745-4-2-0.38]
---------------------------
I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.680 | Acc: 86.625% | Wgt Acc: 89.666%
	I - Batch: 100 | Loss: 0.669 | Acc: 86.875% | Wgt Acc: 89.896%
	I - Batch: 150 | Loss: 0.667 | Acc: 87.125% | Wgt Acc: 90.076%
	I - Batch: 200 | Loss: 0.667 | Acc: 87.125% | Wgt Acc: 90.122%
I - num batch: 222
I - Train -- Loss: 0.668 | Acc: 87.003% | Wgt Acc: 90.011% | LR: 1.250000e-04 | Dur: 122.17s
I - Confusion Matrix: [row->prediction - col->label]
[[656.   0.   4.  27.  83.]
 [  4. 560.  11.   2.  61.]
 [  5.   4. 675.   5.  82.]
 [ 12.   2.   1. 487.  66.]
 [ 20.  12.  43.  17. 708.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.218 | Acc: 54.887% | Wgt Acc: 54.022% | Dur: 12.81s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  5.  2. 16.  8.]
 [ 3. 36. 12.  5.  7.]
 [ 0. 10. 28.  4.  5.]
 [16.  4. 10. 51.  7.]
 [10. 23. 23. 10. 45.]]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.605 | Acc: 90.750% | Wgt Acc: 92.740%
	I - Batch: 100 | Loss: 0.605 | Acc: 90.938% | Wgt Acc: 92.773%
	I - Batch: 150 | Loss: 0.603 | Acc: 91.500% | Wgt Acc: 93.331%
I - num batch: 173
I - Train -- Loss: 0.602 | Acc: 91.860% | Wgt Acc: 93.593% | LR: 1.250000e-04 | Dur: 96.26s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   9.  25.  36.]
 [  5. 567.  13.   9.  18.]
 [  7.   5. 702.   9.  36.]
 [ 11.   0.   2. 489.  14.]
 [  6.   5.   8.   6. 113.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.229 | Acc: 56.642% | Wgt Acc: 57.859% | Dur: 12.94s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  4.  4. 24. 11.]
 [ 6. 47. 10.  6. 17.]
 [ 0. 12. 38.  5. 11.]
 [11.  6.  8. 46.  2.]
 [ 7.  9. 15.  5. 31.]]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.661 | Acc: 87.750% | Wgt Acc: 91.722%
	I - Batch: 100 | Loss: 0.675 | Acc: 87.188% | Wgt Acc: 90.627%
	I - Batch: 150 | Loss: 0.668 | Acc: 87.167% | Wgt Acc: 90.533%
	I - Batch: 200 | Loss: 0.671 | Acc: 86.594% | Wgt Acc: 90.041%
I - num batch: 222
I - Train -- Loss: 0.672 | Acc: 86.467% | Wgt Acc: 89.973% | LR: 1.250000e-04 | Dur: 123.55s
I - Confusion Matrix: [row->prediction - col->label]
[[649.   1.   4.  23. 101.]
 [  5. 564.  15.   3.  67.]
 [  2.   2. 688.   4.  80.]
 [ 16.   0.   4. 490.  76.]
 [ 25.  11.  23.  18. 676.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.216 | Acc: 55.639% | Wgt Acc: 55.074% | Dur: 13.40s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  6.  3. 14. 11.]
 [ 3. 39. 12.  5.  5.]
 [ 1. 16. 27.  5.  9.]
 [14.  3. 14. 51.  3.]
 [ 9. 14. 19. 11. 44.]]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.662 | Acc: 88.000% | Wgt Acc: 91.336%
	I - Batch: 100 | Loss: 0.659 | Acc: 88.000% | Wgt Acc: 91.144%
	I - Batch: 150 | Loss: 0.663 | Acc: 87.792% | Wgt Acc: 90.705%
	I - Batch: 200 | Loss: 0.663 | Acc: 87.812% | Wgt Acc: 90.830%
I - num batch: 222
I - Train -- Loss: 0.660 | Acc: 87.962% | Wgt Acc: 90.965% | LR: 1.250000e-04 | Dur: 125.35s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   0.   6.  17.  75.]
 [  3. 563.  10.   2.  59.]
 [  3.   6. 685.   4.  84.]
 [ 13.   0.   2. 493.  65.]
 [ 16.   9.  31.  22. 717.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.213 | Acc: 57.393% | Wgt Acc: 56.559% | Dur: 12.43s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  7.  2. 22. 15.]
 [ 2. 40. 10.  4.  4.]
 [ 0. 14. 32.  3.  7.]
 [11.  5. 14. 46.  2.]
 [ 8. 12. 17. 11. 44.]]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.643 | Acc: 88.375% | Wgt Acc: 91.440%
	I - Batch: 100 | Loss: 0.656 | Acc: 87.812% | Wgt Acc: 90.999%
	I - Batch: 150 | Loss: 0.662 | Acc: 87.417% | Wgt Acc: 90.636%
	I - Batch: 200 | Loss: 0.658 | Acc: 87.500% | Wgt Acc: 90.648%
I - num batch: 222
I - Train -- Loss: 0.658 | Acc: 87.595% | Wgt Acc: 90.695% | LR: 1.250000e-04 | Dur: 123.44s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   1.   2.  19.  98.]
 [  6. 561.  13.   7.  46.]
 [  1.   5. 685.   8.  79.]
 [  8.   0.   5. 495.  68.]
 [ 25.  11.  29.   9. 709.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.214 | Acc: 55.388% | Wgt Acc: 54.394% | Dur: 13.54s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  3.  1. 15.  4.]
 [ 5. 40.  9.  5.  4.]
 [ 0. 19. 32.  6. 12.]
 [16.  3. 12. 47.  6.]
 [11. 13. 21. 13. 46.]]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.634 | Acc: 90.375% | Wgt Acc: 92.445%
	I - Batch: 100 | Loss: 0.644 | Acc: 88.625% | Wgt Acc: 91.539%
	I - Batch: 150 | Loss: 0.652 | Acc: 88.208% | Wgt Acc: 91.085%
	I - Batch: 200 | Loss: 0.659 | Acc: 87.750% | Wgt Acc: 90.628%
I - num batch: 222
I - Train -- Loss: 0.659 | Acc: 87.764% | Wgt Acc: 90.717% | LR: 1.250000e-04 | Dur: 123.44s
I - Confusion Matrix: [row->prediction - col->label]
[[651.   1.   1.  18.  92.]
 [  3. 561.  13.   5.  52.]
 [  4.   4. 682.   6.  83.]
 [ 13.   0.   4. 498.  52.]
 [ 26.  12.  34.  11. 721.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.245 | Acc: 53.634% | Wgt Acc: 51.856% | Dur: 13.15s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  8.  4. 24. 13.]
 [ 3. 31.  9.  4.  4.]
 [ 0. 17. 28.  3.  6.]
 [ 9.  5. 10. 43.  3.]
 [10. 17. 24. 12. 46.]]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.641 | Acc: 89.625% | Wgt Acc: 92.177%
	I - Batch: 100 | Loss: 0.650 | Acc: 88.625% | Wgt Acc: 91.379%
	I - Batch: 150 | Loss: 0.654 | Acc: 88.292% | Wgt Acc: 91.165%
	I - Batch: 200 | Loss: 0.660 | Acc: 87.844% | Wgt Acc: 90.870%
I - num batch: 222
I - Train -- Loss: 0.659 | Acc: 87.877% | Wgt Acc: 90.920% | LR: 1.250000e-04 | Dur: 122.80s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   1.   6.  17.  82.]
 [  5. 559.  10.   7.  58.]
 [  4.   7. 689.   6.  84.]
 [ 14.   1.   4. 499.  61.]
 [ 19.  10.  25.   9. 715.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.231 | Acc: 54.135% | Wgt Acc: 51.918% | Dur: 12.38s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  5.  4. 16.  5.]
 [ 3. 33. 10.  4.  9.]
 [ 0. 19. 34.  6.  8.]
 [12.  1.  4. 40.  1.]
 [13. 20. 23. 20. 49.]]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.680 | Acc: 86.125% | Wgt Acc: 89.244%
	I - Batch: 100 | Loss: 0.659 | Acc: 87.625% | Wgt Acc: 90.770%
	I - Batch: 150 | Loss: 0.662 | Acc: 87.167% | Wgt Acc: 90.373%
	I - Batch: 200 | Loss: 0.661 | Acc: 87.219% | Wgt Acc: 90.416%
I - num batch: 222
I - Train -- Loss: 0.662 | Acc: 87.229% | Wgt Acc: 90.409% | LR: 1.250000e-04 | Dur: 125.31s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   2.   2.  21.  88.]
 [  5. 559.   9.   4.  63.]
 [  2.   5. 684.   3.  86.]
 [ 13.   0.   5. 495.  62.]
 [ 22.  12.  34.  15. 701.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.205 | Acc: 56.892% | Wgt Acc: 56.312% | Dur: 12.55s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  3. 16. 13.]
 [ 3. 41. 14.  4.  8.]
 [ 1. 12. 30.  7.  5.]
 [10.  3.  7. 49.  2.]
 [11. 17. 21. 10. 44.]]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.648 | Acc: 89.125% | Wgt Acc: 92.145%
	I - Batch: 100 | Loss: 0.645 | Acc: 88.312% | Wgt Acc: 91.320%
	I - Batch: 150 | Loss: 0.649 | Acc: 88.250% | Wgt Acc: 91.218%
	I - Batch: 200 | Loss: 0.655 | Acc: 87.781% | Wgt Acc: 90.851%
I - num batch: 222
I - Train -- Loss: 0.654 | Acc: 87.877% | Wgt Acc: 90.852% | LR: 1.250000e-04 | Dur: 125.61s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   1.   2.  24.  82.]
 [  4. 563.  14.   3.  66.]
 [  4.   5. 678.   4.  80.]
 [ 10.   1.   2. 498.  51.]
 [ 22.   8.  38.   9. 721.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.249 | Acc: 53.133% | Wgt Acc: 51.856% | Dur: 13.15s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  7.  4. 23. 15.]
 [ 2. 34.  8.  4.  3.]
 [ 1. 10. 24.  0.  3.]
 [ 9.  4. 10. 46.  6.]
 [13. 23. 29. 13. 45.]]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.609 | Acc: 92.375% | Wgt Acc: 93.881%
	I - Batch: 100 | Loss: 0.598 | Acc: 92.562% | Wgt Acc: 94.468%
	I - Batch: 150 | Loss: 0.593 | Acc: 92.667% | Wgt Acc: 94.520%
I - num batch: 173
I - Train -- Loss: 0.592 | Acc: 92.800% | Wgt Acc: 94.548% | LR: 1.250000e-04 | Dur: 99.23s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   1.   7.  13.  34.]
 [  6. 566.   9.   8.  20.]
 [  4.   6. 706.   7.  33.]
 [ 12.   1.   2. 506.  13.]
 [  5.   4.  10.   4. 117.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.174 | Acc: 58.145% | Wgt Acc: 58.973% | Dur: 13.08s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  6.  2. 20. 12.]
 [ 4. 40. 11.  4. 10.]
 [ 1. 16. 37.  4. 12.]
 [ 9.  7.  9. 53.  4.]
 [ 6.  9. 16.  5. 34.]]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.667 | Acc: 86.375% | Wgt Acc: 90.239%
	I - Batch: 100 | Loss: 0.652 | Acc: 87.688% | Wgt Acc: 91.052%
	I - Batch: 150 | Loss: 0.652 | Acc: 87.625% | Wgt Acc: 90.841%
	I - Batch: 200 | Loss: 0.649 | Acc: 88.031% | Wgt Acc: 91.164%
I - num batch: 222
I - Train -- Loss: 0.655 | Acc: 87.736% | Wgt Acc: 90.928% | LR: 1.250000e-04 | Dur: 122.86s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   0.   2.  17.  87.]
 [  5. 566.  12.   4.  57.]
 [  2.   4. 681.   3.  73.]
 [ 13.   0.   4. 499.  75.]
 [ 19.   8.  35.  15. 708.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.226 | Acc: 52.381% | Wgt Acc: 50.990% | Dur: 12.81s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  7.  4. 25. 11.]
 [ 1. 32. 10.  5.  7.]
 [ 1. 15. 27.  3.  8.]
 [10.  2. 13. 42.  3.]
 [11. 22. 21. 11. 43.]]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.648 | Acc: 88.625% | Wgt Acc: 91.831%
	I - Batch: 100 | Loss: 0.655 | Acc: 87.562% | Wgt Acc: 91.000%
	I - Batch: 150 | Loss: 0.649 | Acc: 88.167% | Wgt Acc: 91.338%
	I - Batch: 200 | Loss: 0.650 | Acc: 88.344% | Wgt Acc: 91.288%
I - num batch: 222
I - Train -- Loss: 0.652 | Acc: 88.159% | Wgt Acc: 91.085% | LR: 1.250000e-04 | Dur: 123.96s
I - Confusion Matrix: [row->prediction - col->label]
[[654.   0.   4.  13.  88.]
 [  5. 564.  10.   3.  62.]
 [  2.   2. 688.   7.  77.]
 [ 11.   0.   4. 496.  48.]
 [ 25.  12.  28.  19. 725.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.213 | Acc: 53.383% | Wgt Acc: 52.042% | Dur: 13.04s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  6.  4. 28. 16.]
 [ 1. 31.  8.  4.  5.]
 [ 0. 14. 24.  3.  4.]
 [ 7.  3. 13. 44.  4.]
 [ 9. 24. 26.  7. 43.]]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.644 | Acc: 87.750% | Wgt Acc: 91.066%
	I - Batch: 100 | Loss: 0.645 | Acc: 88.562% | Wgt Acc: 91.519%
	I - Batch: 150 | Loss: 0.651 | Acc: 87.792% | Wgt Acc: 90.918%
	I - Batch: 200 | Loss: 0.653 | Acc: 88.031% | Wgt Acc: 91.057%
I - num batch: 222
I - Train -- Loss: 0.651 | Acc: 88.244% | Wgt Acc: 91.206% | LR: 1.250000e-04 | Dur: 123.16s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   1.   6.  20.  89.]
 [  4. 564.   9.   3.  57.]
 [  2.   4. 692.   4.  70.]
 [  8.   1.   2. 494.  62.]
 [ 25.   8.  25.  17. 722.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.230 | Acc: 53.383% | Wgt Acc: 51.671% | Dur: 12.94s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  3. 20.  7.]
 [ 3. 35.  9.  5. 10.]
 [ 2. 18. 27.  3.  4.]
 [10.  3.  9. 44.  3.]
 [14. 18. 27. 14. 48.]]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.634 | Acc: 89.750% | Wgt Acc: 92.380%
	I - Batch: 100 | Loss: 0.644 | Acc: 89.062% | Wgt Acc: 91.858%
	I - Batch: 150 | Loss: 0.647 | Acc: 88.750% | Wgt Acc: 91.573%
	I - Batch: 200 | Loss: 0.644 | Acc: 89.000% | Wgt Acc: 91.792%
I - num batch: 222
I - Train -- Loss: 0.648 | Acc: 88.638% | Wgt Acc: 91.409% | LR: 1.250000e-04 | Dur: 126.09s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   0.   4.  19.  80.]
 [  6. 564.   8.   3.  63.]
 [  3.   2. 691.   4.  71.]
 [  9.   0.   8. 495.  49.]
 [ 22.  12.  23.  17. 737.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.195 | Acc: 58.897% | Wgt Acc: 57.983% | Dur: 12.81s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  3.  0. 11.  4.]
 [ 2. 38.  5.  6.  5.]
 [ 1. 16. 37.  2.  8.]
 [18.  6. 13. 55.  7.]
 [10. 15. 20. 12. 48.]]

I - Local maximum validation set accuracy:  58.90

I - Validation set results: 
[14-1-2-0.29][50-3-3-0.33][124-2-2-0.99][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.98][615-0-0-0.40][695-1-0--0.06][722-3-3-0.99]
[826-0-0-0.12][878-0-0-0.99][1103-0-4-0.38][1212-3-3-0.01][1368-0-0-0.99][2181-2-2-0.07][2476-2-2-0.36][2721-2-2-0.82][2818-1-3-0.99][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-1-0.92][3482-2-2-0.58][3536-3-3-0.41][3625-1-1-0.99][3909-0-0-0.77][4035-0-3-0.99][4140-0-0--0.38][4214-1-3-0.80][4346-1-3--0.57]
[4581-2-2-0.53][4708-3-4-0.63][4838-3-3-0.24][4845-1-1--0.04][4868-0-0-0.99][4939-0-4--0.40][4984-2-3--0.30][5078-1-4-0.51][5396-0-0-0.99][5479-1-1-0.53]
[5717-0-0-0.68][5843-1-1-0.49][5949-3-3-0.72][5987-2-4-0.97][6014-3-3-0.87][6033-3-4--0.52][6313-0-0--0.35][6421-3-3-0.90][6500-1-2--0.54][6583-3-3-0.89]
[6683-3-3--0.38][6825-2-3-0.99][6998-3-0-0.14][7049-3-3-0.99][7517-1-2-0.40][7521-1-1-0.99][7528-1-2-0.99][7949-1-1-0.47][8135-1-0-0.51][8185-3-3-0.98]
[8269-3-1-0.99][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.85][8672-0-0-0.99][8903-1-2-0.99][9001-2-4-0.39][9036-2-1-0.80][9281-3-4-0.85][9300-2-2-0.99]
[9571-0-3-0.20][9617-1-1-0.44][9644-2-2-0.79][9705-2-4-0.42][9801-0-3-0.95][9803-3-3-0.99][9865-3-3-0.02][9896-2-4-0.83][10314-1-0-0.58][10337-3-3-0.99]
[10403-0-4-0.71][10653-2-4-0.12][10704-2-2--0.61][10719-1-1-0.90][10727-1-4-0.81][10836-0-0-0.99][10969-2-2-0.41][11042-0-0--0.14][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.81][11502-3-3-0.89][11512-3-3-0.93][11608-1-1-0.51][11610-0-0-0.85][11692-0-0-0.53][11905-0-0-0.99][11993-1-1--0.06][12002-2-3-0.46]
[12052-0-0-0.96][12201-0-3-0.96][12235-2-4-0.99][12320-1-4-0.99][12377-2-4-0.97][12398-2-3-0.13][12503-1-4-0.11][12617-0-1-0.99][12685-3-1-0.59][12738-2-3-0.80]
[12742-2-2-0.69][12823-0-3-0.95][13110-1-1-0.85][13240-3-4--0.07][13253-1-2-0.79][13273-0-0-0.99][13634-1-4-0.46][13763-2-2-0.76][13905-3-3-0.67][14060-2-4-0.97]
[14065-3-0-0.10][14147-3-3-0.10][14595-2-2-0.88][14687-2-3-0.95][14788-2-2-0.21][14869-1-1-0.98][14872-3-4-0.82][14877-1-1-0.99][14927-0-3-0.93][15066-0-3-0.98]
[15175-1-4-0.98][15178-2-3-0.91][15375-3-1-0.58][15389-3-3-0.99][15568-2-4-0.26][15675-3-3-0.99][15869-1-1--0.16][16207-3-0-0.99][16236-0-0-0.08][16302-3-3--0.38]
[16331-2-2-0.89][16381-0-3-0.61][16488-1-1-0.57][16495-0-0-0.99][16650-0-0-0.99][16719-1-2-0.99][16801-0-0-0.99][16828-0-0-0.95][17137-3-0-0.54][17245-1-4-0.63]
[17278-3-2--0.43][17282-0-0-0.62][17311-2-2-0.87][17336-2-3-0.41][17608-3-3-0.99][17627-0-2--0.36][17877-3-4-0.99][17924-1-3-0.36][17984-3-0-0.99][18211-0-3-0.34]
[18276-3-3-0.87][18287-1-1-0.16][18394-0-0-0.99][18428-0-0-0.99][18442-0-3-0.91][18478-3-3-0.32][18607-0-0-0.99][18616-0-0-0.29][18663-0-0-0.92][18718-0-0-0.98]
[18766-2-2--0.35][18824-2-4-0.99][18890-3-4--0.40][18930-3-4-0.38][18938-3-3-0.52][19817-1-2-0.37][19839-0-4-0.68][19930-3-3-0.18][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-3-0.67][20474-1-2-0.10][20547-3-0-0.09][20929-2-2-0.29][21245-1-2-0.38][21257-3-4-0.73][21293-1-1-0.99][21316-1-1-0.97][21384-1-1-0.97][21448-1-1-0.39]
[21483-0-0-0.90][21487-2-2-0.97][21714-0-0-0.75][21943-3-2-0.97][21947-0-0-0.74][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.28][22025-0-4--0.29][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.88][22757-0-0-0.99][22811-3-3-0.88][22976-3-1-0.42][22985-3-3-0.68][23014-0-3-0.93][23112-1-1-0.85][23144-3-3-0.78][23168-2-4-0.10]
[23219-0-4-0.53][23363-3-3-0.90][23470-0-0--0.16][23486-2-2-0.55][23497-0-3-0.99][23516-0-0-0.99][23690-1-1-0.99][23921-2-4-0.74][23936-1-2-0.96][24040-3-4-0.99]
[24111-1-4-0.99][24182-0-3-0.98][24238-3-3-0.99][24290-2-3-0.80][24345-0-0-0.40][24364-1-2-0.99][24427-3-3-0.98][24477-2-2-0.56][24495-2-4-0.84][24893-2-1-0.11]
[25012-1-1-0.13][25121-2-4-0.64][25165-3-3-0.88][25183-0-4-0.93][25297-3-3-0.03][25398-0-0-0.63][25574-2-2-0.02][25644-1-1-0.99][25718-1-4--0.50][25774-2-2-0.09]
[26032-3-3-0.99][26051-3-3-0.86][26120-0-4-0.48][26321-1-2-0.04][26732-1-1-0.91][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-2-0.12][26860-1-4-0.99]
[26948-0-0-0.76][27049-3-0-0.99][27098-1-2--0.55][27526-0-0-0.99][27639-3-0--0.40][27698-3-3-0.81][27772-0-0-0.99][27890-1-1-0.41][28040-0-0-0.72][28503-2-4-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-4--0.11][29777-0-0-0.99][29877-2-2-0.04][30035-1-1-0.62][30098-0-3-0.99][30326-1-1-0.99][30572-2-3-0.53][30716-0-4-0.99]
[30806-2-3-0.87][30906-1-4-0.48][31007-0-0-0.73][31181-3-3-0.99][31238-0-3-0.71][31347-0-0-0.99][31422-2-4-0.78][31429-3-3-0.60][31431-0-0--0.44][31432-1-1-0.91]
[31477-0-0-0.49][31524-1-3--0.49][31597-1-2-0.35][31619-1-2--0.17][31701-0-0-0.61][31755-0-0-0.80][31854-3-3-0.68][32074-1-4-0.15][32078-3-3-0.99][32111-1-4--0.13]
[32127-1-1-0.68][32140-3-3-0.99][32263-2-4-0.08][32365-0-0-0.43][32411-2-3-0.75][32429-3-3-0.99][32473-3-0-0.97][32574-3-3-0.99][32584-0-0-0.51][32622-0-3-0.13]
[32858-3-0-0.54][32969-3-3-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.65][33133-2-2-0.26][33173-2-2--0.16][33175-3-4-0.95][33306-3-1-0.99][33309-2-3-0.95]
[33474-0-1-0.94][33478-2-4--0.32][33618-1-1-0.99][33712-0-3-0.25][33782-2-4-0.74][33914-3-3-0.92][34076-3-1-0.52][34112-2-2-0.40][34138-2-2-0.99][34239-1-1-0.53]
[34364-2-1-0.97][34617-1-4-0.10][34751-3-3-0.99][34783-2-4-0.32][35015-3-3-0.99][35018-1-4-0.81][35288-2-2-0.22][214490-4-4-0.99][214508-4-4-0.51][214565-4-2-0.27]
[214581-4-2-0.14][214585-4-4-0.99][214625-4-4-0.63][214627-4-4-0.99][214688-4-2-0.86][214694-4-4-0.29][214815-4-4-0.98][214882-4-2-0.12][214894-4-4-0.07][214979-4-4-0.57]
[215082-4-4-0.21][215160-4-3-0.76][215171-4-4-0.99][215210-4-4--0.07][215245-4-3-0.09][215274-4-3-0.95][215365-4-4-0.70][215412-4-4-0.65][215694-4-4-0.35][215718-4-4--0.10]
[215757-4-4-0.99][215778-4-4--0.28][215911-4-4-0.97][215913-4-1-0.99][215950-4-4-0.99][215974-4-4-0.33][215988-4-3--0.50][216086-4-4-0.99][216138-4-4-0.38][216343-4-4-0.18]
[216381-4-4-0.86][216394-4-4--0.14][216840-4-4-0.63][216895-4-4-0.99][217062-4-4-0.87][217108-4-3--0.33][217130-4-4-0.99][217143-4-4-0.99][217466-4-1--0.40][217504-4-4-0.64]
[217518-4-0--0.39][217644-4-4-0.99][217736-4-3-0.98][217752-4-1-0.53][217787-4-4-0.98][217797-4-4-0.34][218000-4-4-0.55][218030-4-1-0.72][218084-4-0-0.54][218157-4-4-0.71]
[218167-4-4-0.99][218290-4-4-0.71][218293-4-4-0.98][218478-4-2-0.99][218580-4-2-0.00][218649-4-1--0.75][218799-4-4--0.24][218966-4-4-0.89][219102-4-0-0.12][219166-4-4-0.82]
[219274-4-4-0.99][219568-4-4-0.84][219588-4-4-0.01][219635-4-3-0.83][219648-4-4-0.99][219661-4-2-0.58][219668-4-2-0.59][219740-4-0--0.22][219745-4-4-0.36]
---------------------------
I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.645 | Acc: 88.250% | Wgt Acc: 91.001%
	I - Batch: 100 | Loss: 0.642 | Acc: 88.625% | Wgt Acc: 91.535%
	I - Batch: 150 | Loss: 0.649 | Acc: 88.417% | Wgt Acc: 91.245%
	I - Batch: 200 | Loss: 0.646 | Acc: 88.625% | Wgt Acc: 91.588%
I - num batch: 222
I - Train -- Loss: 0.646 | Acc: 88.497% | Wgt Acc: 91.424% | LR: 1.250000e-04 | Dur: 124.65s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   0.   6.  20.  81.]
 [  2. 564.  10.   4.  49.]
 [  7.   5. 690.   6.  84.]
 [  7.   0.   0. 499.  58.]
 [ 23.   9.  28.   9. 728.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.196 | Acc: 55.890% | Wgt Acc: 55.136% | Dur: 12.52s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  6.  2. 20. 11.]
 [ 3. 37. 10.  4.  8.]
 [ 0. 15. 31.  7.  6.]
 [ 9.  4. 11. 46.  5.]
 [ 9. 16. 21.  9. 42.]]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.638 | Acc: 88.625% | Wgt Acc: 91.027%
	I - Batch: 100 | Loss: 0.641 | Acc: 88.625% | Wgt Acc: 91.593%
	I - Batch: 150 | Loss: 0.642 | Acc: 88.625% | Wgt Acc: 91.607%
	I - Batch: 200 | Loss: 0.644 | Acc: 88.688% | Wgt Acc: 91.715%
I - num batch: 222
I - Train -- Loss: 0.647 | Acc: 88.469% | Wgt Acc: 91.551% | LR: 1.250000e-04 | Dur: 126.00s
I - Confusion Matrix: [row->prediction - col->label]
[[651.   0.   4.  16.  92.]
 [  2. 569.  10.   5.  63.]
 [  4.   0. 693.   6.  71.]
 [ 10.   0.   3. 503.  52.]
 [ 30.   9.  24.   8. 722.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.202 | Acc: 56.391% | Wgt Acc: 55.136% | Dur: 12.68s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  7.  2. 16.  7.]
 [ 2. 38.  5.  3.  6.]
 [ 1. 12. 24.  0.  8.]
 [ 9.  3. 14. 51.  2.]
 [13. 18. 30. 16. 49.]]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.646 | Acc: 88.250% | Wgt Acc: 91.541%
	I - Batch: 100 | Loss: 0.637 | Acc: 88.750% | Wgt Acc: 91.961%
	I - Batch: 150 | Loss: 0.641 | Acc: 88.792% | Wgt Acc: 91.884%
	I - Batch: 200 | Loss: 0.639 | Acc: 88.938% | Wgt Acc: 91.926%
I - num batch: 222
I - Train -- Loss: 0.639 | Acc: 88.836% | Wgt Acc: 91.860% | LR: 1.250000e-04 | Dur: 124.98s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   0.   4.  16.  85.]
 [  3. 566.   9.   5.  56.]
 [  4.   5. 694.   2.  77.]
 [  9.   0.   4. 503.  56.]
 [ 19.   7.  23.  12. 726.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.200 | Acc: 55.639% | Wgt Acc: 56.436% | Dur: 12.56s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  2. 20. 14.]
 [ 2. 43. 13.  2.  6.]
 [ 0.  8. 22.  2.  6.]
 [14.  6. 19. 55.  9.]
 [ 7. 17. 19.  7. 37.]]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.572 | Acc: 93.625% | Wgt Acc: 95.012%
	I - Batch: 100 | Loss: 0.575 | Acc: 93.062% | Wgt Acc: 94.794%
	I - Batch: 150 | Loss: 0.581 | Acc: 92.792% | Wgt Acc: 94.522%
I - num batch: 173
I - Train -- Loss: 0.578 | Acc: 92.873% | Wgt Acc: 94.616% | LR: 1.250000e-04 | Dur: 97.42s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   0.   3.  19.  30.]
 [  4. 571.   8.   6.  24.]
 [  7.   3. 708.   5.  39.]
 [ 11.   0.   5. 503.   6.]
 [  8.   4.  10.   5. 118.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.194 | Acc: 56.140% | Wgt Acc: 56.869% | Dur: 13.46s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  8.  3. 28. 13.]
 [ 2. 46. 14.  5.  9.]
 [ 1.  8. 33.  5. 15.]
 [ 9.  4. 14. 43.  2.]
 [ 7. 12. 11.  5. 33.]]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.632 | Acc: 88.500% | Wgt Acc: 92.458%
	I - Batch: 100 | Loss: 0.645 | Acc: 88.688% | Wgt Acc: 92.239%
	I - Batch: 150 | Loss: 0.640 | Acc: 89.333% | Wgt Acc: 92.595%
	I - Batch: 200 | Loss: 0.643 | Acc: 89.000% | Wgt Acc: 92.146%
I - num batch: 222
I - Train -- Loss: 0.645 | Acc: 88.723% | Wgt Acc: 91.905% | LR: 1.250000e-04 | Dur: 127.11s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   0.   7.  18.  89.]
 [  3. 567.   7.   3.  48.]
 [  2.   3. 696.   4.  76.]
 [ 10.   0.   3. 504.  71.]
 [ 18.   8.  21.   9. 716.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.252 | Acc: 54.637% | Wgt Acc: 52.290% | Dur: 13.19s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  1. 16.  5.]
 [ 4. 38. 10.  5.  5.]
 [ 0.  9. 25.  3.  6.]
 [ 8.  3.  6. 43.  2.]
 [18. 24. 33. 19. 54.]]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.637 | Acc: 89.250% | Wgt Acc: 92.057%
	I - Batch: 100 | Loss: 0.645 | Acc: 88.625% | Wgt Acc: 91.555%
	I - Batch: 150 | Loss: 0.640 | Acc: 89.292% | Wgt Acc: 92.050%
	I - Batch: 200 | Loss: 0.638 | Acc: 89.438% | Wgt Acc: 92.181%
I - num batch: 222
I - Train -- Loss: 0.640 | Acc: 89.259% | Wgt Acc: 92.123% | LR: 1.250000e-04 | Dur: 125.23s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   0.   4.  17.  84.]
 [  3. 565.   5.   3.  51.]
 [  2.   3. 695.   3.  73.]
 [  8.   0.   2. 503.  54.]
 [ 19.  10.  28.  12. 738.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.211 | Acc: 56.642% | Wgt Acc: 55.322% | Dur: 12.89s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  2. 15. 10.]
 [ 3. 31.  5.  3.  5.]
 [ 0. 16. 35.  4.  9.]
 [10.  4. 12. 51.  2.]
 [12. 22. 21. 13. 46.]]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.649 | Acc: 87.000% | Wgt Acc: 90.416%
	I - Batch: 100 | Loss: 0.648 | Acc: 88.312% | Wgt Acc: 91.628%
	I - Batch: 150 | Loss: 0.641 | Acc: 88.750% | Wgt Acc: 91.900%
	I - Batch: 200 | Loss: 0.638 | Acc: 88.875% | Wgt Acc: 91.846%
I - num batch: 222
I - Train -- Loss: 0.638 | Acc: 88.892% | Wgt Acc: 91.927% | LR: 1.250000e-04 | Dur: 124.52s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   1.   4.  18.  92.]
 [  1. 572.   7.   5.  47.]
 [  5.   0. 697.   3.  77.]
 [ 11.   0.   4. 498.  58.]
 [ 20.   5.  22.  14. 726.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.195 | Acc: 57.644% | Wgt Acc: 55.631% | Dur: 13.18s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  1. 16.  9.]
 [ 3. 31.  5.  3.  2.]
 [ 0. 18. 30.  4.  4.]
 [11.  5. 14. 52.  5.]
 [ 9. 18. 25. 11. 52.]]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.645 | Acc: 88.875% | Wgt Acc: 91.452%
	I - Batch: 100 | Loss: 0.632 | Acc: 89.562% | Wgt Acc: 92.234%
	I - Batch: 150 | Loss: 0.637 | Acc: 88.958% | Wgt Acc: 91.996%
	I - Batch: 200 | Loss: 0.638 | Acc: 88.938% | Wgt Acc: 92.038%
I - num batch: 222
I - Train -- Loss: 0.639 | Acc: 89.089% | Wgt Acc: 92.085% | LR: 1.250000e-04 | Dur: 125.68s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   0.   4.  18.  79.]
 [  3. 566.   5.   4.  57.]
 [  3.   5. 699.   2.  75.]
 [  7.   0.   4. 507.  58.]
 [ 27.   7.  22.   7. 731.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.180 | Acc: 59.900% | Wgt Acc: 59.530% | Dur: 12.69s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  4.  2. 14.  8.]
 [ 3. 44.  8.  5.  7.]
 [ 2. 14. 39.  7.  8.]
 [13.  3.  7. 50.  5.]
 [ 8. 13. 19. 10. 44.]]

I - Local maximum validation set accuracy:  59.90

I - Validation set results: 
[14-1-2-0.18][50-3-3-0.88][124-2-2-0.90][127-0-0-0.99][443-2-2-0.99][567-0-0-0.80][573-1-1-0.99][615-0-0-0.15][695-1-0-0.76][722-3-3-0.93]
[826-0-0-0.34][878-0-0-0.99][1103-0-0-0.42][1212-3-4--0.37][1368-0-0-0.99][2181-2-2-0.90][2476-2-1-0.80][2721-2-2-0.99][2818-1-3-0.21][2886-2-1-0.91]
[3231-2-2-0.99][3333-2-1-0.94][3482-2-2-0.97][3536-3-3-0.13][3625-1-1-0.90][3909-0-0-0.95][4035-0-3-0.97][4140-0-0-0.93][4214-1-3-0.90][4346-1-4-0.20]
[4581-2-2-0.81][4708-3-2--0.09][4838-3-3-0.71][4845-1-1--0.25][4868-0-0-0.80][4939-0-2--0.59][4984-2-1--0.39][5078-1-2-0.68][5396-0-0-0.99][5479-1-1-0.88]
[5717-0-0-0.96][5843-1-1-0.52][5949-3-3-0.99][5987-2-4-0.99][6014-3-3-0.73][6033-3-3--0.53][6313-0-0-0.32][6421-3-3-0.87][6500-1-2--0.34][6583-3-2-0.99]
[6683-3-1--0.71][6825-2-3-0.90][6998-3-0--0.24][7049-3-3-0.95][7517-1-1-0.40][7521-1-1-0.46][7528-1-2-0.90][7949-1-1-0.08][8135-1-0-0.88][8185-3-3-0.61]
[8269-3-4-0.33][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.81][8672-0-3-0.62][8903-1-2-0.97][9001-2-1-0.95][9036-2-4-0.21][9281-3-0-0.61][9300-2-2-0.99]
[9571-0-3--0.10][9617-1-4-0.26][9644-2-2-0.72][9705-2-4-0.99][9801-0-3-0.48][9803-3-3-0.98][9865-3-3-0.91][9896-2-4-0.96][10314-1-4-0.57][10337-3-3-0.99]
[10403-0-4-0.66][10653-2-4-0.28][10704-2-2-0.08][10719-1-1-0.99][10727-1-4-0.95][10836-0-0-0.99][10969-2-2-0.64][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-4-0.99][11499-0-0-0.55][11502-3-3-0.59][11512-3-3-0.72][11608-1-1-0.44][11610-0-0-0.99][11692-0-0-0.85][11905-0-0-0.99][11993-1-1-0.75][12002-2-0-0.59]
[12052-0-0-0.99][12201-0-3-0.88][12235-2-4-0.57][12320-1-4-0.99][12377-2-4-0.85][12398-2-3--0.00][12503-1-1--0.44][12617-0-1-0.88][12685-3-1-0.18][12738-2-3--0.16]
[12742-2-2-0.69][12823-0-3-0.91][13110-1-1-0.89][13240-3-4--0.32][13253-1-4-0.74][13273-0-0-0.99][13634-1-4-0.64][13763-2-2-0.03][13905-3-3--0.51][14060-2-4-0.75]
[14065-3-0-0.80][14147-3-2-0.28][14595-2-2-0.90][14687-2-2-0.94][14788-2-2-0.29][14869-1-1-0.95][14872-3-4-0.15][14877-1-1-0.92][14927-0-1-0.59][15066-0-0-0.99]
[15175-1-4-0.96][15178-2-3-0.22][15375-3-3-0.56][15389-3-3-0.93][15568-2-1-0.29][15675-3-3-0.99][15869-1-1-0.37][16207-3-0-0.99][16236-0-0-0.94][16302-3-2--0.19]
[16331-2-2-0.97][16381-0-3-0.40][16488-1-1-0.98][16495-0-0-0.81][16650-0-0-0.99][16719-1-2-0.99][16801-0-0-0.99][16828-0-0-0.99][17137-3-3-0.12][17245-1-4-0.64]
[17278-3-2--0.09][17282-0-0-0.99][17311-2-2-0.87][17336-2-1-0.40][17608-3-3-0.99][17627-0-2--0.23][17877-3-4-0.90][17924-1-4-0.26][17984-3-0-0.63][18211-0-0-0.34]
[18276-3-0-0.80][18287-1-1-0.26][18394-0-0-0.99][18428-0-0-0.37][18442-0-3-0.68][18478-3-3-0.00][18607-0-0-0.99][18616-0-0-0.37][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2--0.05][18824-2-4-0.99][18890-3-3--0.48][18930-3-4--0.29][18938-3-3-0.19][19817-1-2-0.26][19839-0-4-0.99][19930-3-0--0.23][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-4-0.75][20474-1-2-0.99][20547-3-0--0.21][20929-2-2-0.99][21245-1-2-0.73][21257-3-4-0.20][21293-1-1-0.99][21316-1-1-0.58][21384-1-1-0.89][21448-1-1-0.74]
[21483-0-0-0.99][21487-2-2-0.51][21714-0-0-0.27][21943-3-2-0.95][21947-0-0-0.86][21948-0-0-0.99][21965-2-2-0.99][21998-1-1--0.28][22025-0-0--0.87][22228-3-3-0.99]
[22446-1-1-0.64][22494-3-3-0.48][22757-0-0-0.99][22811-3-3-0.99][22976-3-1-0.09][22985-3-3-0.43][23014-0-0-0.95][23112-1-1-0.78][23144-3-3-0.83][23168-2-3-0.38]
[23219-0-0-0.99][23363-3-3-0.51][23470-0-4-0.18][23486-2-2--0.31][23497-0-3-0.99][23516-0-0-0.99][23690-1-1--0.09][23921-2-4-0.34][23936-1-2-0.99][24040-3-4-0.98]
[24111-1-4-0.99][24182-0-3-0.88][24238-3-3-0.80][24290-2-3-0.42][24345-0-0-0.34][24364-1-2-0.99][24427-3-0-0.97][24477-2-4-0.95][24495-2-4-0.03][24893-2-2-0.11]
[25012-1-4--0.06][25121-2-4-0.99][25165-3-3-0.92][25183-0-4--0.19][25297-3-3-0.59][25398-0-0-0.89][25574-2-2-0.14][25644-1-1-0.99][25718-1-1-0.38][25774-2-2-0.99]
[26032-3-3-0.99][26051-3-3-0.76][26120-0-4-0.99][26321-1-2-0.13][26732-1-1-0.74][26784-3-3-0.99][26827-3-3-0.70][26833-0-3-0.99][26838-2-2-0.68][26860-1-4-0.99]
[26948-0-0-0.42][27049-3-0-0.99][27098-1-2--0.41][27526-0-0-0.99][27639-3-0-0.50][27698-3-3-0.30][27772-0-0-0.98][27890-1-1-0.43][28040-0-4-0.99][28503-2-4-0.93]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-2--0.83][29777-0-0-0.99][29877-2-1-0.28][30035-1-1-0.01][30098-0-0-0.65][30326-1-1-0.99][30572-2-2-0.96][30716-0-4-0.99]
[30806-2-2-0.82][30906-1-1-0.85][31007-0-0-0.96][31181-3-3-0.13][31238-0-3-0.57][31347-0-0-0.99][31422-2-4-0.42][31429-3-3--0.02][31431-0-0--0.22][31432-1-1-0.38]
[31477-0-0-0.99][31524-1-3-0.15][31597-1-1-0.36][31619-1-0--0.56][31701-0-0-0.93][31755-0-0--0.01][31854-3-3--0.22][32074-1-1-0.36][32078-3-3-0.99][32111-1-1-0.56]
[32127-1-1-0.49][32140-3-3-0.99][32263-2-2-0.50][32365-0-0-0.94][32411-2-0-0.99][32429-3-3-0.81][32473-3-0-0.99][32574-3-3-0.99][32584-0-0-0.54][32622-0-1--0.42]
[32858-3-0-0.77][32969-3-3-0.99][33016-2-2-0.96][33031-1-1-0.98][33035-2-2-0.91][33133-2-2-0.79][33173-2-2-0.27][33175-3-4-0.99][33306-3-1-0.99][33309-2-3-0.91]
[33474-0-0-0.05][33478-2-4--0.42][33618-1-1-0.99][33712-0-3-0.07][33782-2-4-0.59][33914-3-3-0.83][34076-3-1-0.31][34112-2-2-0.08][34138-2-2-0.99][34239-1-1--0.17]
[34364-2-2-0.98][34617-1-0--0.30][34751-3-3-0.93][34783-2-4-0.99][35015-3-3-0.97][35018-1-2-0.69][35288-2-2--0.04][214490-4-4-0.94][214508-4-4-0.59][214565-4-2-0.26]
[214581-4-0--0.76][214585-4-4-0.95][214625-4-4-0.50][214627-4-4-0.99][214688-4-1-0.95][214694-4-4--0.23][214815-4-4-0.64][214882-4-4--0.33][214894-4-3--0.44][214979-4-4-0.52]
[215082-4-4-0.54][215160-4-3--0.31][215171-4-4-0.99][215210-4-4-0.46][215245-4-0--0.35][215274-4-3-0.48][215365-4-4-0.69][215412-4-4-0.49][215694-4-4--0.44][215718-4-0-0.11]
[215757-4-4-0.99][215778-4-4--0.21][215911-4-4-0.99][215913-4-4-0.99][215950-4-4-0.99][215974-4-4--0.05][215988-4-4--0.69][216086-4-4-0.99][216138-4-1--0.61][216343-4-4-0.44]
[216381-4-4-0.99][216394-4-4-0.97][216840-4-0-0.42][216895-4-4-0.99][217062-4-4-0.33][217108-4-0-0.20][217130-4-4-0.99][217143-4-4-0.99][217466-4-4-0.54][217504-4-4-0.93]
[217518-4-1-0.11][217644-4-2-0.99][217736-4-3-0.84][217752-4-1-0.10][217787-4-2-0.74][217797-4-2--0.57][218000-4-4-0.99][218030-4-1-0.54][218084-4-0-0.99][218157-4-4--0.04]
[218167-4-4-0.99][218290-4-2-0.42][218293-4-4-0.99][218478-4-1-0.97][218580-4-4--0.27][218649-4-4-0.39][218799-4-0--0.13][218966-4-4-0.90][219102-4-1--0.83][219166-4-4-0.78]
[219274-4-4-0.85][219568-4-4-0.90][219588-4-2-0.14][219635-4-3--0.01][219648-4-4-0.99][219661-4-2-0.85][219668-4-2-0.29][219740-4-0-0.04][219745-4-4-0.28]
---------------------------
I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.626 | Acc: 89.500% | Wgt Acc: 92.031%
	I - Batch: 100 | Loss: 0.628 | Acc: 89.688% | Wgt Acc: 92.336%
	I - Batch: 150 | Loss: 0.627 | Acc: 89.625% | Wgt Acc: 92.207%
	I - Batch: 200 | Loss: 0.628 | Acc: 89.906% | Wgt Acc: 92.445%
I - num batch: 222
I - Train -- Loss: 0.632 | Acc: 89.456% | Wgt Acc: 92.070% | LR: 1.250000e-04 | Dur: 127.02s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   0.   3.  17.  83.]
 [  3. 568.  11.   4.  55.]
 [  3.   0. 691.   3.  58.]
 [ 10.   0.   2. 501.  48.]
 [ 24.  10.  27.  13. 756.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.213 | Acc: 56.140% | Wgt Acc: 53.651% | Dur: 12.87s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  3.  1. 18.  8.]
 [ 3. 34. 12.  6.  6.]
 [ 0. 19. 28.  4.  2.]
 [ 6.  4.  9. 43.  3.]
 [13. 18. 25. 15. 53.]]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.633 | Acc: 89.250% | Wgt Acc: 92.371%
	I - Batch: 100 | Loss: 0.630 | Acc: 89.000% | Wgt Acc: 91.941%
	I - Batch: 150 | Loss: 0.628 | Acc: 89.417% | Wgt Acc: 92.180%
	I - Batch: 200 | Loss: 0.631 | Acc: 89.281% | Wgt Acc: 92.153%
I - num batch: 222
I - Train -- Loss: 0.633 | Acc: 89.061% | Wgt Acc: 91.942% | LR: 1.250000e-04 | Dur: 123.43s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   0.   4.  17.  81.]
 [  6. 565.   6.   7.  54.]
 [  3.   1. 700.   1.  78.]
 [ 13.   0.   2. 503.  51.]
 [ 20.  12.  22.  10. 736.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.198 | Acc: 56.140% | Wgt Acc: 53.960% | Dur: 13.84s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  6.  2. 12.  4.]
 [ 1. 31.  5.  4.  1.]
 [ 0. 14. 29.  6.  6.]
 [13.  7. 15. 51.  8.]
 [14. 20. 24. 13. 53.]]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.637 | Acc: 87.875% | Wgt Acc: 90.924%
	I - Batch: 100 | Loss: 0.638 | Acc: 87.562% | Wgt Acc: 90.798%
	I - Batch: 150 | Loss: 0.640 | Acc: 87.917% | Wgt Acc: 91.209%
	I - Batch: 200 | Loss: 0.637 | Acc: 88.156% | Wgt Acc: 91.415%
I - num batch: 222
I - Train -- Loss: 0.640 | Acc: 87.933% | Wgt Acc: 91.273% | LR: 1.250000e-04 | Dur: 126.80s
I - Confusion Matrix: [row->prediction - col->label]
[[656.   0.   2.  16. 101.]
 [  3. 568.   9.   7.  70.]
 [  2.   1. 696.   3.  74.]
 [ 12.   0.   3. 499.  55.]
 [ 24.   9.  24.  13. 700.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.195 | Acc: 58.145% | Wgt Acc: 56.621% | Dur: 12.79s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  7.  1. 14.  8.]
 [ 2. 38.  7.  3.  6.]
 [ 1. 12. 32.  3.  6.]
 [11.  5.  9. 49.  2.]
 [11. 16. 26. 17. 50.]]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.571 | Acc: 93.875% | Wgt Acc: 95.453%
	I - Batch: 100 | Loss: 0.574 | Acc: 93.438% | Wgt Acc: 95.197%
	I - Batch: 150 | Loss: 0.574 | Acc: 93.292% | Wgt Acc: 95.068%
I - num batch: 173
I - Train -- Loss: 0.571 | Acc: 93.524% | Wgt Acc: 95.280% | LR: 1.250000e-04 | Dur: 95.74s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   4.  19.  29.]
 [  5. 575.   7.   5.  27.]
 [  2.   0. 714.   4.  32.]
 [  9.   0.   2. 505.  11.]
 [  8.   3.   7.   5. 118.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.209 | Acc: 57.644% | Wgt Acc: 57.921% | Dur: 12.85s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  7.  6. 24. 12.]
 [ 2. 46. 13.  6. 10.]
 [ 1. 14. 29.  1.  8.]
 [ 5.  3. 11. 44.  5.]
 [ 6.  8. 16. 11. 37.]]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.643 | Acc: 88.500% | Wgt Acc: 91.859%
	I - Batch: 100 | Loss: 0.640 | Acc: 88.250% | Wgt Acc: 91.572%
	I - Batch: 150 | Loss: 0.636 | Acc: 88.667% | Wgt Acc: 91.751%
	I - Batch: 200 | Loss: 0.633 | Acc: 89.094% | Wgt Acc: 92.121%
I - num batch: 222
I - Train -- Loss: 0.634 | Acc: 89.118% | Wgt Acc: 92.138% | LR: 1.250000e-04 | Dur: 126.37s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   0.   4.  16.  93.]
 [  1. 566.   5.   4.  50.]
 [  5.   4. 701.   3.  71.]
 [  6.   2.   3. 502.  59.]
 [ 20.   6.  21.  13. 727.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.195 | Acc: 56.892% | Wgt Acc: 56.683% | Dur: 12.91s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  3. 18.  9.]
 [ 2. 38. 10.  5.  3.]
 [ 0. 10. 28.  3.  7.]
 [13. 12. 20. 54. 11.]
 [ 8. 12. 14.  6. 42.]]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.650 | Acc: 88.750% | Wgt Acc: 91.697%
	I - Batch: 100 | Loss: 0.638 | Acc: 89.312% | Wgt Acc: 92.131%
	I - Batch: 150 | Loss: 0.635 | Acc: 89.542% | Wgt Acc: 92.337%
	I - Batch: 200 | Loss: 0.635 | Acc: 89.531% | Wgt Acc: 92.447%
I - num batch: 222
I - Train -- Loss: 0.632 | Acc: 89.625% | Wgt Acc: 92.536% | LR: 1.250000e-04 | Dur: 125.48s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   0.   5.  13.  79.]
 [  8. 573.   6.   3.  52.]
 [  3.   0. 695.   1.  68.]
 [  7.   1.   2. 508.  58.]
 [ 19.   4.  26.  13. 743.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.223 | Acc: 53.885% | Wgt Acc: 52.290% | Dur: 13.21s
I - Confusion Matrix: [row->prediction - col->label]
[[52.  3.  1. 12.  5.]
 [ 3. 37. 10.  5.  8.]
 [ 0.  9. 25.  1.  6.]
 [13.  6. 12. 50.  2.]
 [20. 23. 27. 18. 51.]]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.622 | Acc: 90.000% | Wgt Acc: 92.772%
	I - Batch: 100 | Loss: 0.624 | Acc: 89.875% | Wgt Acc: 92.619%
	I - Batch: 150 | Loss: 0.626 | Acc: 89.708% | Wgt Acc: 92.373%
	I - Batch: 200 | Loss: 0.625 | Acc: 90.000% | Wgt Acc: 92.581%
I - num batch: 222
I - Train -- Loss: 0.626 | Acc: 90.020% | Wgt Acc: 92.559% | LR: 1.250000e-04 | Dur: 124.14s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   0.   3.  14.  80.]
 [  3. 567.   8.   4.  43.]
 [  2.   4. 696.   4.  61.]
 [ 13.   0.   4. 503.  52.]
 [ 16.   7.  23.  13. 764.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.249 | Acc: 53.133% | Wgt Acc: 50.062% | Dur: 12.75s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  5.  3. 22.  6.]
 [ 2. 31.  9.  4.  6.]
 [ 1. 13. 27.  1.  3.]
 [10.  3.  8. 38.  3.]
 [13. 26. 28. 21. 54.]]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.612 | Acc: 89.500% | Wgt Acc: 92.428%
	I - Batch: 100 | Loss: 0.614 | Acc: 89.875% | Wgt Acc: 92.841%
	I - Batch: 150 | Loss: 0.625 | Acc: 89.167% | Wgt Acc: 92.273%
	I - Batch: 200 | Loss: 0.628 | Acc: 89.062% | Wgt Acc: 91.971%
I - num batch: 222
I - Train -- Loss: 0.630 | Acc: 89.146% | Wgt Acc: 92.048% | LR: 1.250000e-04 | Dur: 125.45s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   0.   3.  14.  88.]
 [  4. 567.   9.   4.  57.]
 [  3.   4. 699.   3.  72.]
 [  8.   0.   1. 503.  47.]
 [ 25.   7.  22.  14. 736.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.212 | Acc: 57.393% | Wgt Acc: 56.064% | Dur: 13.49s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  4.  3. 17.  8.]
 [ 2. 38.  8.  3.  4.]
 [ 1. 15. 25.  2.  7.]
 [12.  6. 18. 50.  4.]
 [ 6. 15. 21. 14. 49.]]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.616 | Acc: 91.125% | Wgt Acc: 93.692%
	I - Batch: 100 | Loss: 0.626 | Acc: 89.812% | Wgt Acc: 92.639%
	I - Batch: 150 | Loss: 0.630 | Acc: 89.458% | Wgt Acc: 92.220%
	I - Batch: 200 | Loss: 0.623 | Acc: 90.125% | Wgt Acc: 92.709%
I - num batch: 222
I - Train -- Loss: 0.625 | Acc: 90.020% | Wgt Acc: 92.656% | LR: 1.250000e-04 | Dur: 125.67s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   0.   3.  11.  85.]
 [  4. 570.   6.   4.  41.]
 [  4.   0. 697.   3.  70.]
 [  6.   1.   5. 507.  43.]
 [ 25.   7.  23.  13. 761.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.193 | Acc: 55.890% | Wgt Acc: 55.322% | Dur: 12.73s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  5.  3. 21. 13.]
 [ 2. 34.  4.  3.  6.]
 [ 0. 15. 28.  1.  4.]
 [ 9.  8. 16. 52.  7.]
 [10. 16. 24.  9. 42.]]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.625 | Acc: 91.125% | Wgt Acc: 93.744%
	I - Batch: 100 | Loss: 0.632 | Acc: 89.500% | Wgt Acc: 92.409%
	I - Batch: 150 | Loss: 0.626 | Acc: 89.750% | Wgt Acc: 92.488%
	I - Batch: 200 | Loss: 0.630 | Acc: 89.281% | Wgt Acc: 92.180%
I - num batch: 222
I - Train -- Loss: 0.630 | Acc: 89.484% | Wgt Acc: 92.333% | LR: 1.250000e-04 | Dur: 124.06s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   0.   3.  10.  72.]
 [  4. 569.   8.   4.  54.]
 [  3.   2. 688.   2.  69.]
 [  7.   1.   3. 509.  60.]
 [ 20.   6.  32.  13. 745.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.191 | Acc: 55.639% | Wgt Acc: 53.713% | Dur: 12.99s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  2. 20. 10.]
 [ 1. 35.  6.  3.  4.]
 [ 1. 15. 29.  3.  5.]
 [ 9.  4. 14. 43.  4.]
 [11. 18. 24. 17. 49.]]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.603 | Acc: 91.375% | Wgt Acc: 94.331%
	I - Batch: 100 | Loss: 0.622 | Acc: 90.375% | Wgt Acc: 93.175%
	I - Batch: 150 | Loss: 0.626 | Acc: 89.833% | Wgt Acc: 92.634%
	I - Batch: 200 | Loss: 0.628 | Acc: 89.500% | Wgt Acc: 92.555%
I - num batch: 222
I - Train -- Loss: 0.630 | Acc: 89.569% | Wgt Acc: 92.536% | LR: 1.250000e-04 | Dur: 126.86s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   0.   2.  11.  75.]
 [  2. 570.   5.   2.  61.]
 [  7.   2. 703.   0.  62.]
 [  8.   0.   5. 509.  64.]
 [ 23.   6.  19.  16. 738.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.209 | Acc: 56.892% | Wgt Acc: 54.703% | Dur: 12.23s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  7.  5. 25. 11.]
 [ 2. 33.  8.  3.  5.]
 [ 0. 15. 33.  3.  5.]
 [ 6.  5.  9. 41.  2.]
 [ 9. 18. 20. 14. 49.]]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.577 | Acc: 93.875% | Wgt Acc: 94.931%
	I - Batch: 100 | Loss: 0.569 | Acc: 94.562% | Wgt Acc: 95.645%
	I - Batch: 150 | Loss: 0.567 | Acc: 94.125% | Wgt Acc: 95.582%
I - num batch: 173
I - Train -- Loss: 0.565 | Acc: 94.428% | Wgt Acc: 95.800% | LR: 1.250000e-04 | Dur: 98.29s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   4.  16.  27.]
 [  2. 570.   7.   2.  20.]
 [  3.   3. 716.   5.  21.]
 [  8.   1.   1. 509.  12.]
 [  6.   4.   6.   6. 137.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.199 | Acc: 55.639% | Wgt Acc: 56.621% | Dur: 12.57s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  7.  3. 22. 10.]
 [ 3. 45. 12.  3. 10.]
 [ 2. 15. 34.  5. 13.]
 [10.  3. 10. 46.  7.]
 [ 8.  8. 16. 10. 32.]]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.617 | Acc: 91.250% | Wgt Acc: 94.082%
	I - Batch: 100 | Loss: 0.627 | Acc: 89.938% | Wgt Acc: 92.935%
	I - Batch: 150 | Loss: 0.628 | Acc: 89.708% | Wgt Acc: 92.708%
	I - Batch: 200 | Loss: 0.625 | Acc: 90.219% | Wgt Acc: 92.910%
I - num batch: 222
I - Train -- Loss: 0.627 | Acc: 89.992% | Wgt Acc: 92.777% | LR: 1.250000e-04 | Dur: 125.73s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   0.   3.  16.  88.]
 [  3. 569.   7.   5.  48.]
 [  2.   2. 707.   2.  55.]
 [  6.   0.   2. 502.  61.]
 [ 20.   7.  15.  13. 748.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.168 | Acc: 58.396% | Wgt Acc: 57.116% | Dur: 12.06s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  2. 13.  8.]
 [ 3. 38.  8.  5.  4.]
 [ 1. 12. 32.  3.  7.]
 [11.  4. 12. 51.  4.]
 [10. 20. 21. 14. 49.]]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.642 | Acc: 88.875% | Wgt Acc: 91.866%
	I - Batch: 100 | Loss: 0.627 | Acc: 89.625% | Wgt Acc: 92.422%
	I - Batch: 150 | Loss: 0.625 | Acc: 89.625% | Wgt Acc: 92.385%
	I - Batch: 200 | Loss: 0.624 | Acc: 89.438% | Wgt Acc: 92.365%
I - num batch: 222
I - Train -- Loss: 0.622 | Acc: 89.540% | Wgt Acc: 92.438% | LR: 1.250000e-04 | Dur: 123.50s
I - Confusion Matrix: [row->prediction - col->label]
[[659.   0.   2.  18.  82.]
 [  3. 569.   6.   4.  52.]
 [  3.   3. 703.   3.  67.]
 [  7.   0.   3. 505.  59.]
 [ 25.   6.  20.   8. 740.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.183 | Acc: 58.647% | Wgt Acc: 56.931% | Dur: 13.48s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  6.  3. 10.  7.]
 [ 4. 35.  9.  2.  3.]
 [ 0. 14. 33.  4.  8.]
 [12.  5.  8. 53.  2.]
 [11. 18. 22. 17. 52.]]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.633 | Acc: 89.875% | Wgt Acc: 92.735%
	I - Batch: 100 | Loss: 0.629 | Acc: 89.625% | Wgt Acc: 92.589%
	I - Batch: 150 | Loss: 0.618 | Acc: 90.292% | Wgt Acc: 93.181%
	I - Batch: 200 | Loss: 0.621 | Acc: 90.219% | Wgt Acc: 92.953%
I - num batch: 222
I - Train -- Loss: 0.621 | Acc: 90.133% | Wgt Acc: 92.912% | LR: 1.250000e-04 | Dur: 124.57s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   0.   1.   9.  84.]
 [  3. 568.   8.   5.  41.]
 [  0.   2. 704.   3.  68.]
 [ 11.   1.   1. 509.  55.]
 [ 19.   7.  20.  12. 752.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.195 | Acc: 56.642% | Wgt Acc: 55.631% | Dur: 12.59s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  2. 10.  6.]
 [ 6. 37. 10.  3. 10.]
 [ 2. 17. 30.  2.  5.]
 [10.  5.  8. 52.  4.]
 [10. 15. 25. 19. 47.]]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.598 | Acc: 91.625% | Wgt Acc: 94.092%
	I - Batch: 100 | Loss: 0.607 | Acc: 90.625% | Wgt Acc: 93.515%
	I - Batch: 150 | Loss: 0.620 | Acc: 90.000% | Wgt Acc: 92.715%
	I - Batch: 200 | Loss: 0.620 | Acc: 90.094% | Wgt Acc: 92.739%
I - num batch: 222
I - Train -- Loss: 0.618 | Acc: 90.133% | Wgt Acc: 92.844% | LR: 1.250000e-04 | Dur: 123.42s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   0.   3.  13.  83.]
 [  5. 571.   3.   3.  42.]
 [  3.   2. 700.   4.  68.]
 [  7.   1.   2. 503.  52.]
 [ 14.   4.  26.  15. 755.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.190 | Acc: 57.393% | Wgt Acc: 56.126% | Dur: 12.70s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  3. 15.  6.]
 [ 3. 36. 10.  3. 10.]
 [ 1. 19. 31.  3.  5.]
 [10.  3.  9. 53.  2.]
 [14. 16. 22. 12. 49.]]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.606 | Acc: 90.625% | Wgt Acc: 93.305%
	I - Batch: 100 | Loss: 0.612 | Acc: 89.938% | Wgt Acc: 92.974%
	I - Batch: 150 | Loss: 0.613 | Acc: 89.833% | Wgt Acc: 92.662%
	I - Batch: 200 | Loss: 0.615 | Acc: 89.875% | Wgt Acc: 92.633%
I - num batch: 222
I - Train -- Loss: 0.618 | Acc: 89.710% | Wgt Acc: 92.559% | LR: 1.250000e-04 | Dur: 124.96s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   0.   4.  12.  77.]
 [  3. 570.   8.   4.  48.]
 [  2.   1. 697.   2.  66.]
 [  7.   0.   3. 508.  63.]
 [ 24.   7.  22.  12. 746.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.232 | Acc: 53.133% | Wgt Acc: 51.918% | Dur: 12.53s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  1. 21.  9.]
 [ 1. 31.  5.  4.  5.]
 [ 0. 11. 24.  1.  6.]
 [10.  5. 17. 48.  8.]
 [12. 25. 28. 12. 44.]]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.620 | Acc: 89.250% | Wgt Acc: 92.495%
	I - Batch: 100 | Loss: 0.625 | Acc: 89.125% | Wgt Acc: 92.448%
	I - Batch: 150 | Loss: 0.622 | Acc: 89.375% | Wgt Acc: 92.646%
	I - Batch: 200 | Loss: 0.623 | Acc: 89.750% | Wgt Acc: 92.867%
I - num batch: 222
I - Train -- Loss: 0.625 | Acc: 89.653% | Wgt Acc: 92.724% | LR: 1.250000e-04 | Dur: 126.22s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   0.   3.  11.  76.]
 [  5. 567.   4.   3.  58.]
 [  2.   2. 706.   0.  72.]
 [  8.   1.   1. 513.  62.]
 [ 20.   8.  20.  11. 732.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.196 | Acc: 56.391% | Wgt Acc: 56.250% | Dur: 12.37s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  1. 14. 11.]
 [ 5. 41. 13.  5. 11.]
 [ 1. 12. 33.  5.  6.]
 [10.  7. 12. 48.  4.]
 [ 9. 14. 16. 14. 40.]]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.624 | Acc: 90.500% | Wgt Acc: 93.516%
	I - Batch: 100 | Loss: 0.618 | Acc: 90.438% | Wgt Acc: 93.394%
	I - Batch: 150 | Loss: 0.617 | Acc: 90.167% | Wgt Acc: 93.186%
	I - Batch: 200 | Loss: 0.621 | Acc: 89.875% | Wgt Acc: 92.777%
I - num batch: 222
I - Train -- Loss: 0.623 | Acc: 89.484% | Wgt Acc: 92.581% | LR: 1.250000e-04 | Dur: 122.08s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   1.   4.  13.  79.]
 [  3. 567.   1.   2.  63.]
 [  2.   1. 704.   0.  69.]
 [  9.   0.   4. 512.  60.]
 [ 21.   9.  21.  11. 729.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.216 | Acc: 55.890% | Wgt Acc: 53.527% | Dur: 12.39s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  2.  1. 14.  5.]
 [ 2. 38. 10.  1.  2.]
 [ 1.  9. 24.  3.  4.]
 [13.  3. 10. 49.  4.]
 [17. 26. 30. 19. 57.]]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 0.564 | Acc: 93.250% | Wgt Acc: 95.437%
	I - Batch: 100 | Loss: 0.564 | Acc: 94.062% | Wgt Acc: 95.694%
	I - Batch: 150 | Loss: 0.555 | Acc: 94.875% | Wgt Acc: 96.292%
I - num batch: 173
I - Train -- Loss: 0.556 | Acc: 94.863% | Wgt Acc: 96.320% | LR: 1.250000e-04 | Dur: 99.24s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   4.   8.  33.]
 [  2. 574.   4.   6.  20.]
 [  2.   1. 717.   4.  22.]
 [  8.   1.   2. 518.   5.]
 [  9.   2.   7.   2. 137.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.194 | Acc: 56.892% | Wgt Acc: 56.745% | Dur: 12.63s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  5.  4. 20. 16.]
 [ 2. 38. 10.  3.  8.]
 [ 2. 20. 34.  3.  7.]
 [ 7.  4.  9. 47.  3.]
 [ 7. 11. 18. 13. 38.]]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 0.611 | Acc: 90.500% | Wgt Acc: 93.811%
	I - Batch: 100 | Loss: 0.615 | Acc: 90.188% | Wgt Acc: 93.363%
	I - Batch: 150 | Loss: 0.615 | Acc: 90.458% | Wgt Acc: 93.336%
	I - Batch: 200 | Loss: 0.618 | Acc: 90.375% | Wgt Acc: 93.234%
I - num batch: 222
I - Train -- Loss: 0.618 | Acc: 90.273% | Wgt Acc: 93.205% | LR: 1.250000e-04 | Dur: 122.49s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   1.   9.  83.]
 [  2. 570.   6.   3.  52.]
 [  1.   0. 707.   3.  61.]
 [  7.   0.   3. 512.  59.]
 [ 19.   7.  17.  11. 745.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.180 | Acc: 58.647% | Wgt Acc: 57.673% | Dur: 12.18s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  7.  2. 17. 10.]
 [ 4. 41.  8.  4.  2.]
 [ 0. 11. 26.  2.  5.]
 [ 8.  4. 16. 53.  6.]
 [11. 15. 23. 10. 49.]]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 0.606 | Acc: 91.625% | Wgt Acc: 94.398%
	I - Batch: 100 | Loss: 0.606 | Acc: 91.125% | Wgt Acc: 94.098%
	I - Batch: 150 | Loss: 0.606 | Acc: 91.167% | Wgt Acc: 93.919%
	I - Batch: 200 | Loss: 0.611 | Acc: 90.625% | Wgt Acc: 93.409%
I - num batch: 222
I - Train -- Loss: 0.611 | Acc: 90.555% | Wgt Acc: 93.408% | LR: 1.250000e-04 | Dur: 123.63s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   0.   3.   5.  77.]
 [  2. 573.   6.   4.  43.]
 [  2.   0. 705.   2.  73.]
 [  4.   0.   3. 514.  53.]
 [ 23.   5.  17.  13. 754.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.229 | Acc: 52.882% | Wgt Acc: 51.609% | Dur: 12.42s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  1. 15. 10.]
 [ 4. 38. 12.  3.  6.]
 [ 2. 12. 22.  2.  6.]
 [11.  3. 14. 46.  3.]
 [13. 21. 26. 20. 47.]]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 0.620 | Acc: 91.125% | Wgt Acc: 93.640%
	I - Batch: 100 | Loss: 0.614 | Acc: 90.625% | Wgt Acc: 93.384%
	I - Batch: 150 | Loss: 0.613 | Acc: 90.750% | Wgt Acc: 93.546%
	I - Batch: 200 | Loss: 0.617 | Acc: 90.438% | Wgt Acc: 93.107%
I - num batch: 222
I - Train -- Loss: 0.612 | Acc: 90.668% | Wgt Acc: 93.325% | LR: 1.250000e-04 | Dur: 121.40s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   0.   4.   6.  82.]
 [  2. 572.   7.   3.  36.]
 [  2.   1. 703.   6.  67.]
 [  8.   0.   3. 510.  50.]
 [ 19.   5.  17.  13. 765.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.182 | Acc: 57.393% | Wgt Acc: 56.002% | Dur: 13.15s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  3.  2. 12.  7.]
 [ 1. 35.  7.  4.  4.]
 [ 0. 17. 27.  3.  6.]
 [13.  6. 16. 54.  5.]
 [11. 17. 23. 13. 50.]]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 0.608 | Acc: 90.875% | Wgt Acc: 93.869%
	I - Batch: 100 | Loss: 0.606 | Acc: 91.625% | Wgt Acc: 94.041%
	I - Batch: 150 | Loss: 0.610 | Acc: 91.417% | Wgt Acc: 93.833%
	I - Batch: 200 | Loss: 0.611 | Acc: 91.156% | Wgt Acc: 93.688%
I - num batch: 222
I - Train -- Loss: 0.610 | Acc: 91.147% | Wgt Acc: 93.694% | LR: 1.250000e-04 | Dur: 125.51s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   0.   2.   6.  71.]
 [  1. 572.   5.   5.  42.]
 [  4.   0. 702.   2.  71.]
 [  5.   0.   0. 515.  39.]
 [ 20.   6.  25.  10. 777.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.195 | Acc: 57.644% | Wgt Acc: 56.869% | Dur: 12.86s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  3. 15.  6.]
 [ 2. 43. 13.  5.  8.]
 [ 0. 11. 27.  3.  5.]
 [11.  4.  8. 50.  6.]
 [12. 16. 24. 13. 47.]]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 0.618 | Acc: 89.375% | Wgt Acc: 92.487%
	I - Batch: 100 | Loss: 0.624 | Acc: 89.312% | Wgt Acc: 92.407%
	I - Batch: 150 | Loss: 0.614 | Acc: 90.667% | Wgt Acc: 93.262%
	I - Batch: 200 | Loss: 0.615 | Acc: 90.750% | Wgt Acc: 93.184%
I - num batch: 222
I - Train -- Loss: 0.613 | Acc: 90.753% | Wgt Acc: 93.220% | LR: 1.250000e-04 | Dur: 125.50s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   1.   4.  11.  69.]
 [  2. 568.   6.   2.  46.]
 [  1.   1. 702.   3.  56.]
 [  5.   0.   1. 510.  53.]
 [ 26.   8.  21.  12. 776.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.190 | Acc: 56.642% | Wgt Acc: 55.074% | Dur: 12.55s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  2. 23.  7.]
 [ 4. 37.  7.  3.  9.]
 [ 1. 17. 36.  5.  6.]
 [ 9.  4. 10. 43.  3.]
 [11. 15. 20. 12. 47.]]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 0.626 | Acc: 88.750% | Wgt Acc: 91.698%
	I - Batch: 100 | Loss: 0.615 | Acc: 89.750% | Wgt Acc: 92.572%
	I - Batch: 150 | Loss: 0.616 | Acc: 89.875% | Wgt Acc: 92.621%
	I - Batch: 200 | Loss: 0.613 | Acc: 90.156% | Wgt Acc: 92.833%
I - num batch: 222
I - Train -- Loss: 0.612 | Acc: 90.302% | Wgt Acc: 92.987% | LR: 1.250000e-04 | Dur: 125.42s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   0.   4.  10.  70.]
 [  3. 566.   3.   3.  51.]
 [  1.   2. 698.   4.  66.]
 [  8.   0.   3. 515.  52.]
 [ 22.  10.  26.   6. 761.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.223 | Acc: 55.890% | Wgt Acc: 53.527% | Dur: 12.09s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  3.  3. 21.  4.]
 [ 3. 36. 10.  3.  5.]
 [ 1. 14. 31.  1.  8.]
 [ 7.  4.  6. 41.  3.]
 [14. 21. 25. 20. 52.]]

I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 0.596 | Acc: 92.125% | Wgt Acc: 94.030%
	I - Batch: 100 | Loss: 0.603 | Acc: 90.438% | Wgt Acc: 93.007%
	I - Batch: 150 | Loss: 0.604 | Acc: 90.792% | Wgt Acc: 93.344%
	I - Batch: 200 | Loss: 0.608 | Acc: 90.969% | Wgt Acc: 93.546%
I - num batch: 222
I - Train -- Loss: 0.613 | Acc: 90.640% | Wgt Acc: 93.318% | LR: 1.250000e-04 | Dur: 125.15s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   0.   5.  11.  84.]
 [  1. 571.   3.   6.  50.]
 [  1.   1. 699.   2.  49.]
 [  7.   0.   2. 512.  53.]
 [ 19.   6.  25.   7. 764.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.200 | Acc: 56.642% | Wgt Acc: 55.755% | Dur: 12.59s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  7.  3. 17.  9.]
 [ 2. 39.  9.  1.  7.]
 [ 1. 11. 21.  2.  5.]
 [ 8.  7. 18. 52.  4.]
 [10. 14. 24. 14. 47.]]

I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 0.556 | Acc: 95.375% | Wgt Acc: 96.571%
	I - Batch: 100 | Loss: 0.553 | Acc: 94.938% | Wgt Acc: 96.359%
	I - Batch: 150 | Loss: 0.550 | Acc: 94.917% | Wgt Acc: 96.318%
I - num batch: 173
I - Train -- Loss: 0.551 | Acc: 94.935% | Wgt Acc: 96.354% | LR: 1.250000e-04 | Dur: 97.46s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   7.   8.  26.]
 [  1. 574.   5.   8.  15.]
 [  1.   2. 713.   2.  29.]
 [  7.   0.   1. 516.   9.]
 [  5.   2.   8.   4. 138.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.196 | Acc: 56.391% | Wgt Acc: 57.178% | Dur: 13.31s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  6.  4. 18. 11.]
 [ 5. 40. 11.  3. 13.]
 [ 2. 19. 38.  6. 10.]
 [ 9.  4.  9. 48.  6.]
 [ 5.  9. 13. 11. 32.]]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 0.626 | Acc: 88.375% | Wgt Acc: 92.160%
	I - Batch: 100 | Loss: 0.614 | Acc: 89.812% | Wgt Acc: 92.817%
	I - Batch: 150 | Loss: 0.610 | Acc: 90.500% | Wgt Acc: 93.278%
	I - Batch: 200 | Loss: 0.609 | Acc: 90.344% | Wgt Acc: 93.215%
I - num batch: 222
I - Train -- Loss: 0.610 | Acc: 90.443% | Wgt Acc: 93.280% | LR: 1.250000e-04 | Dur: 125.20s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   1.   2.   6.  80.]
 [  3. 571.   2.   3.  44.]
 [  1.   2. 707.   2.  72.]
 [  6.   0.   2. 515.  50.]
 [ 26.   4.  21.  12. 754.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.173 | Acc: 57.895% | Wgt Acc: 56.683% | Dur: 13.33s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  3.  2. 17.  5.]
 [ 6. 40.  8.  4.  6.]
 [ 0. 13. 29.  1.  5.]
 [11.  7. 16. 52.  6.]
 [11. 15. 20. 12. 50.]]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 0.599 | Acc: 91.500% | Wgt Acc: 94.259%
	I - Batch: 100 | Loss: 0.602 | Acc: 91.125% | Wgt Acc: 93.834%
	I - Batch: 150 | Loss: 0.605 | Acc: 91.167% | Wgt Acc: 93.980%
	I - Batch: 200 | Loss: 0.608 | Acc: 90.844% | Wgt Acc: 93.691%
I - num batch: 222
I - Train -- Loss: 0.608 | Acc: 90.837% | Wgt Acc: 93.664% | LR: 1.250000e-04 | Dur: 124.59s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   0.   2.   3.  72.]
 [  2. 571.   3.   4.  47.]
 [  0.   2. 707.   3.  70.]
 [  4.   0.   3. 518.  53.]
 [ 23.   5.  19.  10. 758.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.244 | Acc: 55.639% | Wgt Acc: 53.094% | Dur: 12.84s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  6.  3. 18.  9.]
 [ 2. 30.  7.  3.  3.]
 [ 0. 10. 29.  3.  4.]
 [ 8.  5. 10. 46.  3.]
 [14. 27. 26. 16. 53.]]

I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 0.601 | Acc: 91.375% | Wgt Acc: 93.902%
	I - Batch: 100 | Loss: 0.607 | Acc: 91.438% | Wgt Acc: 93.585%
	I - Batch: 150 | Loss: 0.603 | Acc: 91.667% | Wgt Acc: 94.012%
	I - Batch: 200 | Loss: 0.603 | Acc: 91.875% | Wgt Acc: 94.039%
I - num batch: 222
I - Train -- Loss: 0.605 | Acc: 91.627% | Wgt Acc: 93.844% | LR: 1.250000e-04 | Dur: 125.49s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   0.   2.  14.  66.]
 [  1. 571.   4.   4.  28.]
 [  2.   1. 708.   3.  58.]
 [  6.   0.   3. 508.  51.]
 [ 22.   6.  17.   9. 797.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.170 | Acc: 58.145% | Wgt Acc: 57.240% | Dur: 12.41s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  2.  4. 14.  8.]
 [ 5. 42.  5.  2.  5.]
 [ 0. 10. 29.  2.  6.]
 [11.  4. 14. 53.  4.]
 [13. 20. 23. 15. 49.]]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 0.596 | Acc: 92.125% | Wgt Acc: 94.153%
	I - Batch: 100 | Loss: 0.598 | Acc: 90.938% | Wgt Acc: 93.369%
	I - Batch: 150 | Loss: 0.599 | Acc: 91.125% | Wgt Acc: 93.435%
	I - Batch: 200 | Loss: 0.602 | Acc: 90.875% | Wgt Acc: 93.334%
I - num batch: 222
I - Train -- Loss: 0.607 | Acc: 90.584% | Wgt Acc: 93.047% | LR: 1.250000e-04 | Dur: 122.19s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   1.   9.  83.]
 [  2. 568.   7.   2.  39.]
 [  2.   2. 689.   2.  60.]
 [  6.   0.   3. 509.  43.]
 [ 15.   8.  34.  16. 775.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.224 | Acc: 55.890% | Wgt Acc: 55.136% | Dur: 13.57s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  2. 20.  8.]
 [ 7. 46. 18.  6. 10.]
 [ 0.  7. 21.  3.  4.]
 [10.  4. 11. 45.  4.]
 [ 6. 16. 23. 12. 46.]]

I - Epoch: 126
I - Training: 
	I - Batch: 50 | Loss: 0.598 | Acc: 91.125% | Wgt Acc: 93.746%
	I - Batch: 100 | Loss: 0.598 | Acc: 91.312% | Wgt Acc: 93.915%
	I - Batch: 150 | Loss: 0.599 | Acc: 91.250% | Wgt Acc: 93.840%
	I - Batch: 200 | Loss: 0.602 | Acc: 91.156% | Wgt Acc: 93.811%
I - num batch: 222
I - Train -- Loss: 0.602 | Acc: 91.288% | Wgt Acc: 93.867% | LR: 1.250000e-04 | Dur: 127.10s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   0.   4.  12.  74.]
 [  3. 574.   1.   4.  38.]
 [  1.   2. 710.   0.  65.]
 [  5.   0.   3. 512.  48.]
 [ 21.   2.  16.  10. 775.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.231 | Acc: 56.391% | Wgt Acc: 54.455% | Dur: 11.99s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  3. 17.  6.]
 [ 2. 34.  4.  5.  4.]
 [ 1. 10. 32.  5.  8.]
 [ 6.  4. 10. 46.  4.]
 [16. 26. 26. 13. 50.]]

I - Epoch: 127
I - Training: 
	I - Batch: 50 | Loss: 0.609 | Acc: 90.750% | Wgt Acc: 93.043%
	I - Batch: 100 | Loss: 0.608 | Acc: 91.000% | Wgt Acc: 93.279%
	I - Batch: 150 | Loss: 0.601 | Acc: 91.042% | Wgt Acc: 93.518%
	I - Batch: 200 | Loss: 0.603 | Acc: 91.031% | Wgt Acc: 93.527%
I - num batch: 222
I - Train -- Loss: 0.604 | Acc: 90.922% | Wgt Acc: 93.416% | LR: 1.250000e-04 | Dur: 123.39s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   2.  10.  66.]
 [  1. 570.   4.   3.  45.]
 [  3.   1. 700.   1.  63.]
 [  2.   0.   3. 508.  51.]
 [ 19.   7.  25.  16. 775.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.193 | Acc: 55.890% | Wgt Acc: 54.455% | Dur: 13.39s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  2. 16.  7.]
 [ 4. 34. 10.  4.  6.]
 [ 0. 10. 30.  2.  7.]
 [15.  9. 12. 50.  4.]
 [ 8. 20. 21. 14. 48.]]

I - Epoch: 128
I - Training: 
	I - Batch: 50 | Loss: 0.589 | Acc: 91.125% | Wgt Acc: 93.816%
	I - Batch: 100 | Loss: 0.599 | Acc: 90.625% | Wgt Acc: 93.470%
	I - Batch: 150 | Loss: 0.599 | Acc: 90.958% | Wgt Acc: 93.762%
	I - Batch: 200 | Loss: 0.604 | Acc: 91.031% | Wgt Acc: 93.770%
I - num batch: 222
I - Train -- Loss: 0.603 | Acc: 91.091% | Wgt Acc: 93.724% | LR: 1.250000e-04 | Dur: 126.16s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   0.   3.  11.  63.]
 [  2. 572.   4.   4.  49.]
 [  0.   1. 706.   0.  63.]
 [  8.   0.   3. 513.  55.]
 [ 17.   5.  18.  10. 770.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.213 | Acc: 55.388% | Wgt Acc: 52.970% | Dur: 12.56s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  3.  3. 15.  7.]
 [ 4. 34. 10.  5.  5.]
 [ 0. 13. 26.  3.  1.]
 [10.  2. 13. 48.  4.]
 [16. 26. 23. 15. 55.]]

I - Epoch: 129
I - Training: 
	I - Batch: 50 | Loss: 0.541 | Acc: 95.375% | Wgt Acc: 96.709%
	I - Batch: 100 | Loss: 0.543 | Acc: 94.875% | Wgt Acc: 96.375%
	I - Batch: 150 | Loss: 0.544 | Acc: 94.917% | Wgt Acc: 96.518%
I - num batch: 173
I - Train -- Loss: 0.542 | Acc: 95.080% | Wgt Acc: 96.584% | LR: 1.250000e-04 | Dur: 100.08s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   2.   6.   5.  29.]
 [  2. 576.   4.   3.  11.]
 [  3.   0. 714.   5.  30.]
 [  5.   0.   2. 523.  10.]
 [  9.   0.   8.   2. 137.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.194 | Acc: 57.644% | Wgt Acc: 57.364% | Dur: 13.13s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  3.  4. 17.  9.]
 [ 3. 38. 10.  2. 10.]
 [ 1. 19. 34.  6.  7.]
 [11.  5. 11. 51.  5.]
 [ 7. 13. 16. 10. 41.]]

I - Epoch: 130
I - Training: 
	I - Batch: 50 | Loss: 0.596 | Acc: 91.250% | Wgt Acc: 94.102%
	I - Batch: 100 | Loss: 0.593 | Acc: 91.625% | Wgt Acc: 94.401%
	I - Batch: 150 | Loss: 0.597 | Acc: 91.208% | Wgt Acc: 94.100%
	I - Batch: 200 | Loss: 0.598 | Acc: 91.344% | Wgt Acc: 94.094%
I - num batch: 222
I - Train -- Loss: 0.598 | Acc: 91.429% | Wgt Acc: 94.152% | LR: 1.250000e-04 | Dur: 124.15s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   3.   5.  74.]
 [  0. 575.   6.   4.  58.]
 [  1.   2. 709.   4.  49.]
 [  2.   0.   2. 517.  50.]
 [ 21.   1.  14.   8. 769.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.220 | Acc: 55.639% | Wgt Acc: 53.713% | Dur: 12.97s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  6.  4. 13.  4.]
 [ 2. 31.  9.  1.  6.]
 [ 1. 12. 27.  2.  4.]
 [11.  8. 14. 51.  7.]
 [12. 21. 21. 19. 51.]]

I - Epoch: 131
I - Training: 
	I - Batch: 50 | Loss: 0.615 | Acc: 90.625% | Wgt Acc: 93.535%
	I - Batch: 100 | Loss: 0.599 | Acc: 91.625% | Wgt Acc: 94.315%
	I - Batch: 150 | Loss: 0.594 | Acc: 91.917% | Wgt Acc: 94.487%
	I - Batch: 200 | Loss: 0.597 | Acc: 91.719% | Wgt Acc: 94.263%
I - num batch: 222
I - Train -- Loss: 0.598 | Acc: 91.570% | Wgt Acc: 94.137% | LR: 1.250000e-04 | Dur: 124.35s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   0.   3.  10.  71.]
 [  0. 573.   5.   2.  40.]
 [  1.   1. 713.   2.  59.]
 [  5.   0.   1. 515.  52.]
 [ 22.   4.  12.   9. 778.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.218 | Acc: 55.639% | Wgt Acc: 52.908% | Dur: 12.57s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  3. 18.  9.]
 [ 4. 39. 10.  5.  4.]
 [ 0. 10. 28.  4.  2.]
 [10.  5. 11. 40.  1.]
 [15. 20. 23. 19. 56.]]

I - Epoch: 132
I - Training: 
	I - Batch: 50 | Loss: 0.601 | Acc: 90.375% | Wgt Acc: 93.547%
	I - Batch: 100 | Loss: 0.599 | Acc: 90.688% | Wgt Acc: 93.510%
	I - Batch: 150 | Loss: 0.596 | Acc: 91.167% | Wgt Acc: 93.856%
	I - Batch: 200 | Loss: 0.596 | Acc: 91.188% | Wgt Acc: 93.804%
I - num batch: 222
I - Train -- Loss: 0.597 | Acc: 91.063% | Wgt Acc: 93.633% | LR: 1.250000e-04 | Dur: 124.67s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   0.   4.   8.  75.]
 [  1. 573.   5.   3.  39.]
 [  2.   0. 706.   3.  58.]
 [  3.   0.   2. 512.  54.]
 [ 26.   5.  17.  12. 774.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.214 | Acc: 56.892% | Wgt Acc: 54.703% | Dur: 12.47s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  6.  2. 23.  7.]
 [ 2. 35.  7.  3.  5.]
 [ 0. 10. 24.  1.  4.]
 [ 5.  6. 15. 45.  4.]
 [10. 21. 27. 14. 52.]]

I - Epoch: 133
I - Training: 
	I - Batch: 50 | Loss: 0.586 | Acc: 91.750% | Wgt Acc: 94.639%
	I - Batch: 100 | Loss: 0.596 | Acc: 91.812% | Wgt Acc: 94.463%
	I - Batch: 150 | Loss: 0.600 | Acc: 91.250% | Wgt Acc: 93.893%
	I - Batch: 200 | Loss: 0.598 | Acc: 91.250% | Wgt Acc: 93.866%
I - num batch: 222
I - Train -- Loss: 0.596 | Acc: 91.373% | Wgt Acc: 93.964% | LR: 1.250000e-04 | Dur: 124.52s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   0.   3.   7.  72.]
 [  2. 570.   2.   4.  55.]
 [  1.   1. 709.   2.  59.]
 [  4.   1.   2. 519.  38.]
 [ 23.   6.  18.   6. 776.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.241 | Acc: 55.890% | Wgt Acc: 53.899% | Dur: 13.03s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  3.  3. 17.  5.]
 [ 4. 38. 10.  4.  8.]
 [ 0. 14. 31.  2.  6.]
 [ 6.  1. 10. 43.  2.]
 [18. 22. 21. 20. 51.]]

I - Epoch: 134
I - Training: 
	I - Batch: 50 | Loss: 0.587 | Acc: 92.125% | Wgt Acc: 93.952%
	I - Batch: 100 | Loss: 0.584 | Acc: 92.688% | Wgt Acc: 94.656%
	I - Batch: 150 | Loss: 0.587 | Acc: 92.167% | Wgt Acc: 94.358%
	I - Batch: 200 | Loss: 0.590 | Acc: 91.875% | Wgt Acc: 94.063%
I - num batch: 222
I - Train -- Loss: 0.594 | Acc: 91.711% | Wgt Acc: 94.024% | LR: 1.250000e-04 | Dur: 123.91s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   4.  11.  60.]
 [  5. 572.   4.   4.  44.]
 [  1.   1. 709.   2.  62.]
 [  4.   0.   2. 511.  41.]
 [ 19.   4.  15.  10. 793.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.206 | Acc: 56.642% | Wgt Acc: 54.084% | Dur: 12.09s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  6.  2. 15.  6.]
 [ 2. 35.  9.  4.  3.]
 [ 1. 14. 27.  2.  5.]
 [ 8.  3.  8. 49.  1.]
 [19. 20. 29. 16. 57.]]

I - Epoch: 135
I - Training: 
	I - Batch: 50 | Loss: 0.599 | Acc: 91.125% | Wgt Acc: 93.894%
	I - Batch: 100 | Loss: 0.600 | Acc: 91.750% | Wgt Acc: 94.305%
	I - Batch: 150 | Loss: 0.595 | Acc: 91.875% | Wgt Acc: 94.441%
	I - Batch: 200 | Loss: 0.599 | Acc: 91.781% | Wgt Acc: 94.303%
I - num batch: 222
I - Train -- Loss: 0.600 | Acc: 91.627% | Wgt Acc: 94.227% | LR: 1.250000e-04 | Dur: 123.75s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   4.   4.  87.]
 [  1. 572.   1.   2.  33.]
 [  0.   2. 710.   1.  50.]
 [  6.   0.   2. 518.  53.]
 [ 17.   4.  17.  13. 777.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.235 | Acc: 53.634% | Wgt Acc: 51.485% | Dur: 13.28s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  3.  2. 15.  1.]
 [ 2. 35. 10.  2.  9.]
 [ 0. 14. 27.  2.  8.]
 [ 8.  3.  7. 45.  2.]
 [23. 23. 29. 22. 52.]]

I - Epoch: 136
I - Training: 
	I - Batch: 50 | Loss: 0.607 | Acc: 90.125% | Wgt Acc: 93.036%
	I - Batch: 100 | Loss: 0.596 | Acc: 90.750% | Wgt Acc: 93.477%
	I - Batch: 150 | Loss: 0.595 | Acc: 91.167% | Wgt Acc: 93.708%
	I - Batch: 200 | Loss: 0.595 | Acc: 91.219% | Wgt Acc: 93.712%
I - num batch: 222
I - Train -- Loss: 0.596 | Acc: 91.288% | Wgt Acc: 93.776% | LR: 1.250000e-04 | Dur: 124.94s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   0.   3.  10.  69.]
 [  4. 570.   4.   4.  44.]
 [  1.   2. 710.   1.  52.]
 [  8.   0.   2. 512.  56.]
 [ 17.   6.  15.  11. 779.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.203 | Acc: 57.393% | Wgt Acc: 55.507% | Dur: 12.57s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  4. 11.  4.]
 [ 3. 29.  6.  2.  2.]
 [ 1. 16. 30.  2.  5.]
 [13.  5. 13. 58.  8.]
 [12. 24. 22. 13. 53.]]

I - Epoch: 137
I - Training: 
	I - Batch: 50 | Loss: 0.526 | Acc: 96.375% | Wgt Acc: 97.386%
	I - Batch: 100 | Loss: 0.533 | Acc: 96.000% | Wgt Acc: 97.210%
	I - Batch: 150 | Loss: 0.536 | Acc: 95.750% | Wgt Acc: 96.980%
I - num batch: 173
I - Train -- Loss: 0.538 | Acc: 95.478% | Wgt Acc: 96.831% | LR: 1.250000e-04 | Dur: 98.85s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   6.  10.  27.]
 [  1. 576.   3.   4.  16.]
 [  3.   1. 721.   1.  21.]
 [  4.   0.   0. 520.  10.]
 [ 10.   1.   4.   3. 143.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.210 | Acc: 56.391% | Wgt Acc: 54.827% | Dur: 13.48s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  8.  5. 19. 13.]
 [ 1. 29.  7.  3.  3.]
 [ 0. 10. 30.  2.  6.]
 [ 9. 13. 15. 51.  3.]
 [10. 18. 18. 11. 47.]]

I - Epoch: 138
I - Training: 
	I - Batch: 50 | Loss: 0.606 | Acc: 89.750% | Wgt Acc: 92.919%
	I - Batch: 100 | Loss: 0.590 | Acc: 91.438% | Wgt Acc: 94.176%
	I - Batch: 150 | Loss: 0.599 | Acc: 91.000% | Wgt Acc: 93.714%
	I - Batch: 200 | Loss: 0.599 | Acc: 91.000% | Wgt Acc: 93.644%
I - num batch: 222
I - Train -- Loss: 0.597 | Acc: 91.345% | Wgt Acc: 93.851% | LR: 1.250000e-04 | Dur: 124.72s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   0.   1.   7.  64.]
 [  1. 572.   5.   3.  50.]
 [  2.   1. 705.   1.  51.]
 [  7.   0.   5. 518.  53.]
 [ 24.   5.  18.   9. 782.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.257 | Acc: 52.632% | Wgt Acc: 50.928% | Dur: 12.68s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  2. 25. 12.]
 [ 1. 32.  5.  3.  3.]
 [ 2. 15. 32.  2. 10.]
 [ 8.  4.  9. 39.  3.]
 [14. 22. 27. 17. 44.]]

I - Epoch: 139
I - Training: 
	I - Batch: 50 | Loss: 0.598 | Acc: 91.750% | Wgt Acc: 94.090%
	I - Batch: 100 | Loss: 0.600 | Acc: 91.438% | Wgt Acc: 93.915%
	I - Batch: 150 | Loss: 0.602 | Acc: 91.167% | Wgt Acc: 93.792%
	I - Batch: 200 | Loss: 0.598 | Acc: 91.438% | Wgt Acc: 93.965%
I - num batch: 222
I - Train -- Loss: 0.596 | Acc: 91.429% | Wgt Acc: 93.882% | LR: 1.250000e-04 | Dur: 123.85s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   0.   0.   8.  66.]
 [  3. 568.   2.   5.  46.]
 [  3.   2. 707.   2.  57.]
 [  6.   1.   4. 516.  48.]
 [ 16.   7.  21.   7. 783.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.227 | Acc: 53.634% | Wgt Acc: 52.599% | Dur: 12.44s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  6. 23. 14.]
 [ 1. 35.  8.  4.  5.]
 [ 1. 14. 22.  3.  3.]
 [10. 11. 18. 47.  6.]
 [10. 12. 21.  9. 44.]]

I - Epoch: 140
I - Training: 
	I - Batch: 50 | Loss: 0.602 | Acc: 91.500% | Wgt Acc: 94.331%
	I - Batch: 100 | Loss: 0.594 | Acc: 92.125% | Wgt Acc: 94.636%
	I - Batch: 150 | Loss: 0.593 | Acc: 91.792% | Wgt Acc: 94.274%
	I - Batch: 200 | Loss: 0.592 | Acc: 92.125% | Wgt Acc: 94.579%
I - num batch: 222
I - Train -- Loss: 0.593 | Acc: 91.993% | Wgt Acc: 94.513% | LR: 1.250000e-04 | Dur: 124.82s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   0.   2.   6.  71.]
 [  0. 577.   3.   5.  42.]
 [  1.   0. 711.   1.  50.]
 [  4.   0.   1. 515.  52.]
 [ 17.   1.  17.  11. 785.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.235 | Acc: 56.140% | Wgt Acc: 52.290% | Dur: 12.43s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  2. 14.  2.]
 [ 2. 27.  7.  2.  1.]
 [ 0. 13. 23.  1.  4.]
 [ 8.  5. 10. 48.  2.]
 [15. 29. 33. 21. 63.]]

I - Epoch: 141
I - Training: 
	I - Batch: 50 | Loss: 0.605 | Acc: 90.750% | Wgt Acc: 93.298%
	I - Batch: 100 | Loss: 0.598 | Acc: 91.500% | Wgt Acc: 93.922%
	I - Batch: 150 | Loss: 0.594 | Acc: 91.958% | Wgt Acc: 94.325%
	I - Batch: 200 | Loss: 0.591 | Acc: 92.000% | Wgt Acc: 94.325%
I - num batch: 222
I - Train -- Loss: 0.590 | Acc: 92.021% | Wgt Acc: 94.302% | LR: 1.250000e-04 | Dur: 121.91s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   0.   3.  11.  59.]
 [  0. 572.   5.   3.  39.]
 [  0.   1. 710.   2.  61.]
 [  7.   0.   0. 514.  43.]
 [ 20.   5.  16.   8. 798.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.215 | Acc: 57.143% | Wgt Acc: 54.394% | Dur: 13.23s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  5.  4. 14.  5.]
 [ 3. 35.  9.  2.  2.]
 [ 0. 14. 28.  3.  5.]
 [11.  2.  7. 46.  3.]
 [12. 22. 27. 21. 57.]]

I - Epoch: 142
I - Training: 
	I - Batch: 50 | Loss: 0.592 | Acc: 91.125% | Wgt Acc: 94.220%
	I - Batch: 100 | Loss: 0.597 | Acc: 91.312% | Wgt Acc: 93.937%
	I - Batch: 150 | Loss: 0.591 | Acc: 91.667% | Wgt Acc: 94.179%
	I - Batch: 200 | Loss: 0.592 | Acc: 91.469% | Wgt Acc: 94.090%
I - num batch: 222
I - Train -- Loss: 0.590 | Acc: 91.655% | Wgt Acc: 94.190% | LR: 1.250000e-04 | Dur: 128.95s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   0.   2.   4.  71.]
 [  7. 572.   2.   2.  40.]
 [  1.   2. 714.   1.  57.]
 [  3.   0.   3. 517.  51.]
 [ 19.   4.  13.  14. 781.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.221 | Acc: 57.644% | Wgt Acc: 55.012% | Dur: 12.59s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  3. 18.  6.]
 [ 2. 38. 11.  6.  7.]
 [ 1. 11. 30.  3.  4.]
 [ 5.  4.  6. 39.  1.]
 [11. 19. 25. 20. 54.]]

I - Epoch: 143
I - Training: 
	I - Batch: 50 | Loss: 0.598 | Acc: 91.375% | Wgt Acc: 93.840%
	I - Batch: 100 | Loss: 0.595 | Acc: 91.312% | Wgt Acc: 93.802%
	I - Batch: 150 | Loss: 0.585 | Acc: 91.958% | Wgt Acc: 94.385%
	I - Batch: 200 | Loss: 0.590 | Acc: 91.688% | Wgt Acc: 94.109%
I - num batch: 222
I - Train -- Loss: 0.592 | Acc: 91.627% | Wgt Acc: 94.122% | LR: 1.250000e-04 | Dur: 127.41s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   2.   8.  67.]
 [  3. 573.   5.   4.  43.]
 [  1.   1. 708.   1.  71.]
 [  3.   0.   1. 513.  37.]
 [ 16.   4.  18.  12. 782.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.204 | Acc: 55.388% | Wgt Acc: 53.899% | Dur: 13.04s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  5.  4. 16.  7.]
 [ 3. 36.  7.  3.  4.]
 [ 1. 13. 28.  3.  5.]
 [12.  9. 13. 49.  7.]
 [13. 15. 23. 15. 49.]]

I - Epoch: 144
I - Training: 
	I - Batch: 50 | Loss: 0.603 | Acc: 90.375% | Wgt Acc: 92.778%
	I - Batch: 100 | Loss: 0.583 | Acc: 91.812% | Wgt Acc: 94.323%
	I - Batch: 150 | Loss: 0.585 | Acc: 91.958% | Wgt Acc: 94.537%
	I - Batch: 200 | Loss: 0.588 | Acc: 91.750% | Wgt Acc: 94.248%
I - num batch: 222
I - Train -- Loss: 0.590 | Acc: 91.711% | Wgt Acc: 94.190% | LR: 1.250000e-04 | Dur: 121.26s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   0.   5.   7.  64.]
 [  1. 571.   2.   3.  46.]
 [  0.   1. 707.   0.  52.]
 [  3.   0.   2. 516.  54.]
 [ 18.   6.  18.  12. 784.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.205 | Acc: 59.649% | Wgt Acc: 57.178% | Dur: 13.13s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  3.  3. 13.  6.]
 [ 2. 35.  7.  4.  5.]
 [ 0. 16. 30.  3.  4.]
 [ 5.  2.  8. 49.  1.]
 [13. 22. 27. 17. 56.]]

I - Epoch: 145
I - Training: 
	I - Batch: 50 | Loss: 0.536 | Acc: 96.000% | Wgt Acc: 96.816%
	I - Batch: 100 | Loss: 0.533 | Acc: 96.000% | Wgt Acc: 97.182%
	I - Batch: 150 | Loss: 0.534 | Acc: 95.875% | Wgt Acc: 97.071%
I - num batch: 173
I - Train -- Loss: 0.533 | Acc: 95.803% | Wgt Acc: 97.018% | LR: 1.250000e-04 | Dur: 99.03s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   4.   9.  29.]
 [  2. 574.   6.   5.  14.]
 [  0.   2. 717.   1.  16.]
 [  3.   0.   2. 522.   8.]
 [  7.   2.   5.   1. 150.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.159 | Acc: 59.900% | Wgt Acc: 58.973% | Dur: 13.34s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  7.  3. 14.  7.]
 [ 3. 35.  8.  4.  5.]
 [ 0. 21. 40.  5.  9.]
 [12.  6.  9. 54.  5.]
 [ 9.  9. 15.  9. 46.]]

I - Epoch: 146
I - Training: 
	I - Batch: 50 | Loss: 0.589 | Acc: 90.875% | Wgt Acc: 93.792%
	I - Batch: 100 | Loss: 0.595 | Acc: 90.812% | Wgt Acc: 93.629%
	I - Batch: 150 | Loss: 0.587 | Acc: 91.958% | Wgt Acc: 94.526%
	I - Batch: 200 | Loss: 0.590 | Acc: 91.719% | Wgt Acc: 94.400%
I - num batch: 222
I - Train -- Loss: 0.590 | Acc: 91.683% | Wgt Acc: 94.302% | LR: 1.250000e-04 | Dur: 124.19s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   3.   7.  70.]
 [  0. 572.   2.   2.  46.]
 [  1.   0. 714.   1.  56.]
 [  3.   0.   1. 514.  54.]
 [ 15.   6.  14.  14. 774.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.213 | Acc: 58.647% | Wgt Acc: 55.136% | Dur: 12.57s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  2. 13.  2.]
 [ 3. 34.  8.  2.  2.]
 [ 2. 18. 27.  2.  2.]
 [12.  2. 14. 49.  2.]
 [11. 20. 24. 20. 64.]]

I - Epoch: 147
I - Training: 
	I - Batch: 50 | Loss: 0.595 | Acc: 90.375% | Wgt Acc: 92.801%
	I - Batch: 100 | Loss: 0.595 | Acc: 90.812% | Wgt Acc: 93.448%
	I - Batch: 150 | Loss: 0.589 | Acc: 91.708% | Wgt Acc: 94.093%
	I - Batch: 200 | Loss: 0.589 | Acc: 92.000% | Wgt Acc: 94.351%
I - num batch: 222
I - Train -- Loss: 0.588 | Acc: 92.191% | Wgt Acc: 94.483% | LR: 1.250000e-04 | Dur: 124.81s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   5.  10.  63.]
 [  1. 575.   6.   4.  38.]
 [  1.   1. 707.   1.  51.]
 [  3.   0.   2. 515.  48.]
 [ 19.   2.  14.   8. 800.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.209 | Acc: 54.135% | Wgt Acc: 51.485% | Dur: 12.76s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  1. 18.  3.]
 [ 2. 27.  8.  2.  6.]
 [ 3. 14. 27.  2.  5.]
 [11.  4. 13. 49.  4.]
 [13. 29. 26. 15. 54.]]

I - Epoch: 148
I - Training: 
	I - Batch: 50 | Loss: 0.591 | Acc: 91.000% | Wgt Acc: 93.509%
	I - Batch: 100 | Loss: 0.593 | Acc: 91.938% | Wgt Acc: 94.123%
	I - Batch: 150 | Loss: 0.585 | Acc: 92.333% | Wgt Acc: 94.659%
	I - Batch: 200 | Loss: 0.583 | Acc: 92.438% | Wgt Acc: 94.670%
I - num batch: 222
I - Train -- Loss: 0.585 | Acc: 92.444% | Wgt Acc: 94.618% | LR: 1.250000e-04 | Dur: 126.68s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   1.   8.  63.]
 [  3. 570.   1.   3.  41.]
 [  2.   1. 713.   1.  52.]
 [  3.   0.   0. 516.  37.]
 [ 16.   6.  19.  10. 807.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.212 | Acc: 56.642% | Wgt Acc: 54.827% | Dur: 12.02s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  2.  2. 13.  4.]
 [ 5. 42. 13.  8.  7.]
 [ 1. 11. 27.  1.  5.]
 [10.  3.  7. 44.  4.]
 [11. 20. 26. 20. 52.]]

I - Epoch: 149
I - Training: 
	I - Batch: 50 | Loss: 0.587 | Acc: 91.625% | Wgt Acc: 94.410%
	I - Batch: 100 | Loss: 0.581 | Acc: 92.562% | Wgt Acc: 95.088%
	I - Batch: 150 | Loss: 0.586 | Acc: 92.083% | Wgt Acc: 94.712%
	I - Batch: 200 | Loss: 0.587 | Acc: 91.844% | Wgt Acc: 94.466%
I - num batch: 222
I - Train -- Loss: 0.586 | Acc: 91.796% | Wgt Acc: 94.445% | LR: 1.250000e-04 | Dur: 124.15s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   4.   6.  70.]
 [  2. 575.   4.   3.  46.]
 [  0.   0. 709.   1.  67.]
 [  4.   0.   0. 522.  39.]
 [ 19.   3.  17.   6. 778.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.200 | Acc: 58.396% | Wgt Acc: 55.012% | Dur: 12.28s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  1.  1. 17.  2.]
 [ 1. 34.  7.  4.  2.]
 [ 0. 14. 30.  2.  7.]
 [10.  3.  8. 45.  0.]
 [14. 26. 29. 18. 61.]]

I - Epoch: 150
I - Training: 
	I - Batch: 50 | Loss: 0.592 | Acc: 91.125% | Wgt Acc: 93.867%
	I - Batch: 100 | Loss: 0.594 | Acc: 91.250% | Wgt Acc: 93.929%
	I - Batch: 150 | Loss: 0.586 | Acc: 92.083% | Wgt Acc: 94.437%
	I - Batch: 200 | Loss: 0.590 | Acc: 91.750% | Wgt Acc: 94.201%
I - num batch: 222
I - Train -- Loss: 0.590 | Acc: 91.824% | Wgt Acc: 94.160% | LR: 1.250000e-04 | Dur: 122.63s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   0.   5.   7.  67.]
 [  1. 570.   2.   4.  46.]
 [  6.   4. 708.   2.  51.]
 [  4.   0.   1. 515.  43.]
 [ 15.   4.  18.  10. 793.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.170 | Acc: 61.153% | Wgt Acc: 58.663% | Dur: 12.75s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  4.  3. 14.  3.]
 [ 4. 39.  9.  3.  5.]
 [ 0. 11. 34.  3.  3.]
 [ 7.  4.  8. 47.  4.]
 [10. 20. 21. 19. 57.]]

I - Local maximum validation set accuracy:  61.15

I - Validation set results: 
[14-1-1--0.12][50-3-4-0.58][124-2-2-0.20][127-0-0-0.99][443-2-2-0.98][567-0-0-0.96][573-1-1-0.50][615-0-0-0.70][695-1-4--0.17][722-3-0-0.86]
[826-0-0-0.99][878-0-0-0.65][1103-0-4-0.59][1212-3-4--0.45][1368-0-0-0.99][2181-2-2-0.03][2476-2-1-0.68][2721-2-2-0.88][2818-1-3-0.52][2886-2-1-0.94]
[3231-2-2-0.99][3333-2-1-0.65][3482-2-4--0.05][3536-3-3-0.38][3625-1-1-0.99][3909-0-0-0.53][4035-0-0-0.99][4140-0-0-0.68][4214-1-3-0.75][4346-1-4--0.81]
[4581-2-4--0.31][4708-3-4-0.09][4838-3-3-0.60][4845-1-4-0.56][4868-0-0-0.99][4939-0-0--0.55][4984-2-2-0.03][5078-1-4-0.67][5396-0-0-0.99][5479-1-1-0.11]
[5717-0-0-0.80][5843-1-1-0.06][5949-3-3-0.99][5987-2-4-0.99][6014-3-3-0.08][6033-3-3--0.64][6313-0-0-0.50][6421-3-3-0.34][6500-1-2--0.42][6583-3-2-0.81]
[6683-3-3--0.40][6825-2-3-0.24][6998-3-0--0.16][7049-3-3-0.93][7517-1-2-0.39][7521-1-0-0.21][7528-1-2-0.66][7949-1-1-0.07][8135-1-0-0.16][8185-3-0-0.63]
[8269-3-1-0.99][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.99][8903-1-2-0.74][9001-2-1-0.99][9036-2-1-0.10][9281-3-4-0.69][9300-2-2-0.99]
[9571-0-4--0.16][9617-1-4-0.61][9644-2-2--0.22][9705-2-1--0.04][9801-0-3-0.30][9803-3-3-0.99][9865-3-3-0.04][9896-2-2-0.89][10314-1-4-0.17][10337-3-3-0.99]
[10403-0-4-0.90][10653-2-4-0.30][10704-2-2--0.51][10719-1-1-0.99][10727-1-4-0.68][10836-0-0-0.99][10969-2-3-0.12][11042-0-0-0.93][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.96][11499-0-0-0.26][11502-3-3-0.99][11512-3-3-0.68][11608-1-1-0.89][11610-0-0-0.95][11692-0-0-0.66][11905-0-0-0.99][11993-1-1-0.48][12002-2-0-0.81]
[12052-0-0-0.95][12201-0-3-0.85][12235-2-4-0.99][12320-1-4-0.99][12377-2-4-0.91][12398-2-3--0.39][12503-1-1--0.97][12617-0-1-0.99][12685-3-4--0.39][12738-2-0--0.51]
[12742-2-2-0.31][12823-0-0-0.95][13110-1-1-0.91][13240-3-3--0.44][13253-1-4-0.23][13273-0-0-0.99][13634-1-4-0.76][13763-2-2-0.40][13905-3-3--0.09][14060-2-4-0.86]
[14065-3-0-0.36][14147-3-2--0.10][14595-2-2-0.74][14687-2-2-0.93][14788-2-2--0.29][14869-1-1-0.98][14872-3-4-0.81][14877-1-1-0.94][14927-0-0-0.97][15066-0-3-0.65]
[15175-1-4-0.99][15178-2-3--0.79][15375-3-1-0.16][15389-3-3-0.81][15568-2-4--0.26][15675-3-3-0.75][15869-1-1-0.74][16207-3-0-0.99][16236-0-0-0.92][16302-3-4-0.67]
[16331-2-2-0.99][16381-0-0-0.11][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-2-0.99][16801-0-0-0.99][16828-0-0-0.99][17137-3-3--0.20][17245-1-4-0.41]
[17278-3-4-0.68][17282-0-0-0.97][17311-2-2-0.99][17336-2-1-0.17][17608-3-3-0.99][17627-0-1-0.29][17877-3-4-0.77][17924-1-4--0.17][17984-3-3-0.51][18211-0-1-0.16]
[18276-3-3-0.22][18287-1-1-0.21][18394-0-0-0.99][18428-0-0--0.00][18442-0-0-0.44][18478-3-3-0.09][18607-0-0-0.99][18616-0-4-0.80][18663-0-0-0.96][18718-0-0-0.99]
[18766-2-1-0.50][18824-2-4-0.95][18890-3-3--0.58][18930-3-4--0.10][18938-3-3--0.45][19817-1-4-0.84][19839-0-4-0.80][19930-3-0--0.41][19944-0-4-0.64][20036-2-2-0.92]
[20101-3-3--0.22][20474-1-2-0.37][20547-3-0--0.54][20929-2-2-0.91][21245-1-2-0.27][21257-3-4-0.55][21293-1-1-0.99][21316-1-1-0.99][21384-1-4-0.78][21448-1-1-0.81]
[21483-0-0-0.99][21487-2-2-0.72][21714-0-0-0.76][21943-3-2-0.96][21947-0-0-0.80][21948-0-0-0.99][21965-2-2-0.33][21998-1-4--0.42][22025-0-4-0.48][22228-3-3-0.99]
[22446-1-1-0.72][22494-3-3--0.07][22757-0-0-0.99][22811-3-3-0.99][22976-3-4-0.72][22985-3-3-0.41][23014-0-0-0.76][23112-1-1-0.81][23144-3-3-0.47][23168-2-3--0.41]
[23219-0-4-0.66][23363-3-3-0.65][23470-0-0--0.12][23486-2-2--0.44][23497-0-3-0.96][23516-0-0-0.49][23690-1-1--0.52][23921-2-4-0.44][23936-1-2-0.66][24040-3-4-0.99]
[24111-1-4-0.42][24182-0-0-0.92][24238-3-3-0.81][24290-2-3-0.20][24345-0-0-0.44][24364-1-2-0.99][24427-3-0-0.93][24477-2-4-0.69][24495-2-4-0.85][24893-2-1--0.15]
[25012-1-1-0.08][25121-2-4-0.92][25165-3-3--0.01][25183-0-0-0.07][25297-3-4--0.13][25398-0-0-0.54][25574-2-4--0.20][25644-1-1-0.99][25718-1-3--0.36][25774-2-4-0.08]
[26032-3-3-0.83][26051-3-3--0.30][26120-0-4-0.99][26321-1-1-0.99][26732-1-1-0.99][26784-3-3-0.97][26827-3-3-0.78][26833-0-3-0.85][26838-2-2-0.16][26860-1-4-0.99]
[26948-0-0-0.73][27049-3-0-0.99][27098-1-0--0.07][27526-0-0-0.75][27639-3-0-0.26][27698-3-3--0.08][27772-0-0-0.99][27890-1-1-0.45][28040-0-0-0.97][28503-2-4-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-4--0.89][29777-0-0-0.99][29877-2-3-0.57][30035-1-1-0.47][30098-0-0-0.60][30326-1-1-0.99][30572-2-2-0.82][30716-0-4-0.99]
[30806-2-2-0.24][30906-1-1-0.89][31007-0-0-0.22][31181-3-0-0.10][31238-0-3-0.82][31347-0-0-0.98][31422-2-4-0.87][31429-3-3-0.44][31431-0-0--0.07][31432-1-1-0.68]
[31477-0-0-0.99][31524-1-3--0.60][31597-1-2--0.29][31619-1-0-0.07][31701-0-0-0.88][31755-0-0-0.85][31854-3-3--0.10][32074-1-2-0.27][32078-3-3-0.93][32111-1-1-0.20]
[32127-1-1-0.87][32140-3-3-0.99][32263-2-2-0.05][32365-0-0-0.45][32411-2-0-0.99][32429-3-0-0.48][32473-3-4-0.99][32574-3-3-0.99][32584-0-0-0.94][32622-0-1-0.30]
[32858-3-0-0.50][32969-3-3-0.99][33016-2-2-0.45][33031-1-1-0.69][33035-2-4-0.35][33133-2-2-0.67][33173-2-2-0.37][33175-3-4-0.99][33306-3-1-0.99][33309-2-3-0.42]
[33474-0-0-0.35][33478-2-4--0.20][33618-1-1-0.94][33712-0-3--0.38][33782-2-4-0.99][33914-3-3-0.76][34076-3-4-0.86][34112-2-2-0.68][34138-2-2-0.99][34239-1-1-0.49]
[34364-2-2-0.76][34617-1-4-0.26][34751-3-3-0.66][34783-2-4-0.38][35015-3-4-0.88][35018-1-4-0.77][35288-2-2-0.06][214490-4-4-0.99][214508-4-4-0.99][214565-4-2--0.19]
[214581-4-2--0.27][214585-4-4-0.99][214625-4-4-0.99][214627-4-4-0.99][214688-4-4-0.93][214694-4-4-0.96][214815-4-1-0.40][214882-4-4-0.65][214894-4-4-0.37][214979-4-4-0.76]
[215082-4-4-0.78][215160-4-3-0.31][215171-4-4-0.99][215210-4-4-0.69][215245-4-1--0.63][215274-4-3-0.57][215365-4-4-0.66][215412-4-4-0.71][215694-4-4-0.25][215718-4-4-0.05]
[215757-4-4-0.99][215778-4-3--0.55][215911-4-4-0.99][215913-4-1-0.63][215950-4-4-0.99][215974-4-4-0.50][215988-4-4-0.40][216086-4-4-0.99][216138-4-4--0.16][216343-4-4-0.43]
[216381-4-4-0.99][216394-4-4-0.13][216840-4-4-0.08][216895-4-4-0.80][217062-4-4-0.50][217108-4-4-0.57][217130-4-4-0.99][217143-4-4-0.99][217466-4-4-0.53][217504-4-4-0.91]
[217518-4-4--0.80][217644-4-4-0.99][217736-4-3-0.75][217752-4-4-0.63][217787-4-4-0.74][217797-4-4-0.55][218000-4-4-0.78][218030-4-1-0.66][218084-4-4-0.70][218157-4-0--0.16]
[218167-4-4-0.98][218290-4-4-0.79][218293-4-4-0.92][218478-4-2-0.97][218580-4-4--0.53][218649-4-4-0.90][218799-4-4-0.01][218966-4-4-0.98][219102-4-4--0.44][219166-4-4-0.99]
[219274-4-4-0.99][219568-4-4-0.65][219588-4-4-0.51][219635-4-1--0.48][219648-4-4-0.99][219661-4-4-0.99][219668-4-0--0.46][219740-4-0--0.39][219745-4-4-0.98]
---------------------------
I - Epoch: 151
I - Training: 
	I - Batch: 50 | Loss: 0.594 | Acc: 90.750% | Wgt Acc: 93.022%
	I - Batch: 100 | Loss: 0.577 | Acc: 92.250% | Wgt Acc: 94.376%
	I - Batch: 150 | Loss: 0.578 | Acc: 91.958% | Wgt Acc: 94.448%
	I - Batch: 200 | Loss: 0.579 | Acc: 92.188% | Wgt Acc: 94.601%
I - num batch: 222
I - Train -- Loss: 0.585 | Acc: 91.852% | Wgt Acc: 94.295% | LR: 1.250000e-04 | Dur: 122.88s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   3.   6.  62.]
 [  1. 572.   1.   4.  39.]
 [  0.   1. 705.   0.  59.]
 [  2.   0.   3. 519.  51.]
 [ 21.   5.  22.   9. 789.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.212 | Acc: 57.644% | Wgt Acc: 55.507% | Dur: 11.78s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  4.  1. 19. 11.]
 [ 1. 36. 11.  1.  3.]
 [ 0. 10. 25.  2.  2.]
 [ 7.  5.  9. 47.  3.]
 [11. 23. 29. 17. 53.]]

I - Epoch: 152
I - Training: 
	I - Batch: 50 | Loss: 0.586 | Acc: 92.250% | Wgt Acc: 94.655%
	I - Batch: 100 | Loss: 0.577 | Acc: 92.375% | Wgt Acc: 94.794%
	I - Batch: 150 | Loss: 0.586 | Acc: 92.083% | Wgt Acc: 94.636%
	I - Batch: 200 | Loss: 0.590 | Acc: 91.625% | Wgt Acc: 94.344%
I - num batch: 222
I - Train -- Loss: 0.591 | Acc: 91.627% | Wgt Acc: 94.287% | LR: 1.250000e-04 | Dur: 125.23s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   0.   2.   9.  71.]
 [  2. 575.   5.   2.  41.]
 [  1.   0. 710.   2.  64.]
 [  5.   0.   3. 519.  49.]
 [ 18.   3.  14.   6. 775.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.226 | Acc: 56.391% | Wgt Acc: 53.032% | Dur: 12.50s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  4.  2. 20.  6.]
 [ 1. 35. 12.  5.  4.]
 [ 0. 14. 28.  5.  1.]
 [ 4.  3.  6. 36.  4.]
 [14. 22. 27. 20. 57.]]

I - Epoch: 153
I - Training: 
	I - Batch: 50 | Loss: 0.539 | Acc: 96.125% | Wgt Acc: 96.976%
	I - Batch: 100 | Loss: 0.532 | Acc: 96.000% | Wgt Acc: 97.034%
	I - Batch: 150 | Loss: 0.534 | Acc: 95.542% | Wgt Acc: 96.792%
I - num batch: 173
I - Train -- Loss: 0.531 | Acc: 95.803% | Wgt Acc: 97.018% | LR: 1.250000e-04 | Dur: 97.21s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   2.   7.  22.]
 [  1. 577.   4.   4.  12.]
 [  0.   1. 720.   2.  22.]
 [  4.   0.   4. 517.  12.]
 [  7.   0.   4.   8. 149.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.159 | Acc: 58.647% | Wgt Acc: 58.478% | Dur: 12.89s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  2. 17.  5.]
 [ 4. 43.  9.  3.  6.]
 [ 1. 10. 31.  2. 10.]
 [11.  9. 14. 52.  8.]
 [ 7. 11. 19. 12. 43.]]

I - Epoch: 154
I - Training: 
	I - Batch: 50 | Loss: 0.581 | Acc: 91.375% | Wgt Acc: 94.092%
	I - Batch: 100 | Loss: 0.572 | Acc: 92.812% | Wgt Acc: 95.117%
	I - Batch: 150 | Loss: 0.580 | Acc: 92.500% | Wgt Acc: 94.873%
	I - Batch: 200 | Loss: 0.583 | Acc: 92.438% | Wgt Acc: 94.794%
I - num batch: 222
I - Train -- Loss: 0.582 | Acc: 92.670% | Wgt Acc: 94.919% | LR: 1.250000e-04 | Dur: 126.62s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   5.   6.  57.]
 [  0. 572.   3.   5.  44.]
 [  1.   2. 715.   1.  36.]
 [  4.   0.   0. 518.  58.]
 [ 15.   4.  11.   8. 805.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.257 | Acc: 53.885% | Wgt Acc: 50.000% | Dur: 12.93s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  3.  3. 13.  4.]
 [ 4. 25.  4.  4.  3.]
 [ 1. 16. 27.  2.  5.]
 [ 7.  5.  7. 41.  1.]
 [13. 29. 34. 26. 59.]]

I - Epoch: 155
I - Training: 
	I - Batch: 50 | Loss: 0.589 | Acc: 92.500% | Wgt Acc: 94.963%
	I - Batch: 100 | Loss: 0.584 | Acc: 92.688% | Wgt Acc: 94.856%
	I - Batch: 150 | Loss: 0.585 | Acc: 92.375% | Wgt Acc: 94.549%
	I - Batch: 200 | Loss: 0.582 | Acc: 92.250% | Wgt Acc: 94.562%
I - num batch: 222
I - Train -- Loss: 0.583 | Acc: 92.162% | Wgt Acc: 94.483% | LR: 1.250000e-04 | Dur: 125.38s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   2.   4.  67.]
 [  2. 571.   2.   3.  36.]
 [  2.   0. 705.   3.  59.]
 [  2.   0.   3. 519.  40.]
 [ 15.   7.  22.   9. 798.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.255 | Acc: 55.639% | Wgt Acc: 52.723% | Dur: 12.76s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  2.  1. 15.  4.]
 [ 1. 28.  8.  4.  4.]
 [ 0. 19. 36.  9. 12.]
 [ 6.  1.  3. 38.  1.]
 [12. 28. 27. 20. 51.]]

I - Epoch: 156
I - Training: 
	I - Batch: 50 | Loss: 0.585 | Acc: 91.625% | Wgt Acc: 94.323%
	I - Batch: 100 | Loss: 0.583 | Acc: 92.562% | Wgt Acc: 94.920%
	I - Batch: 150 | Loss: 0.579 | Acc: 92.958% | Wgt Acc: 95.186%
	I - Batch: 200 | Loss: 0.578 | Acc: 93.031% | Wgt Acc: 95.102%
I - num batch: 222
I - Train -- Loss: 0.580 | Acc: 92.839% | Wgt Acc: 95.002% | LR: 1.250000e-04 | Dur: 122.87s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   2.   7.  66.]
 [  1. 572.   4.   4.  27.]
 [  1.   3. 716.   0.  49.]
 [  4.   0.   1. 519.  46.]
 [ 17.   3.  11.   8. 812.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.222 | Acc: 56.391% | Wgt Acc: 53.960% | Dur: 12.99s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  3.  2. 12.  4.]
 [ 3. 39.  8.  3.  6.]
 [ 1. 12. 24.  5.  4.]
 [11.  5. 11. 49.  0.]
 [18. 19. 30. 17. 58.]]

I - Epoch: 157
I - Training: 
	I - Batch: 50 | Loss: 0.570 | Acc: 93.500% | Wgt Acc: 95.767%
	I - Batch: 100 | Loss: 0.579 | Acc: 92.875% | Wgt Acc: 94.903%
	I - Batch: 150 | Loss: 0.578 | Acc: 92.833% | Wgt Acc: 94.982%
	I - Batch: 200 | Loss: 0.580 | Acc: 92.688% | Wgt Acc: 94.906%
I - num batch: 222
I - Train -- Loss: 0.581 | Acc: 92.698% | Wgt Acc: 94.896% | LR: 1.250000e-04 | Dur: 124.59s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   1.   4.  59.]
 [  2. 575.   3.   3.  44.]
 [  2.   0. 712.   1.  46.]
 [  6.   0.   0. 518.  41.]
 [ 14.   3.  18.  12. 810.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.195 | Acc: 56.391% | Wgt Acc: 55.879% | Dur: 12.26s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  6.  3. 21. 14.]
 [ 3. 37. 12.  2.  9.]
 [ 0. 14. 28.  4.  2.]
 [ 9.  4.  9. 50.  5.]
 [ 8. 17. 23.  9. 42.]]

I - Epoch: 158
I - Training: 
	I - Batch: 50 | Loss: 0.578 | Acc: 93.375% | Wgt Acc: 95.380%
	I - Batch: 100 | Loss: 0.578 | Acc: 93.312% | Wgt Acc: 95.062%
	I - Batch: 150 | Loss: 0.574 | Acc: 93.292% | Wgt Acc: 95.263%
	I - Batch: 200 | Loss: 0.577 | Acc: 93.062% | Wgt Acc: 95.181%
I - num batch: 222
I - Train -- Loss: 0.577 | Acc: 93.206% | Wgt Acc: 95.257% | LR: 1.250000e-04 | Dur: 123.44s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   2.   5.  58.]
 [  0. 574.   3.   3.  31.]
 [  0.   0. 711.   0.  46.]
 [  2.   0.   3. 521.  42.]
 [ 18.   4.  15.   9. 823.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.190 | Acc: 57.143% | Wgt Acc: 55.074% | Dur: 13.01s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  2.  4.  8.  4.]
 [ 1. 34.  6.  2.  5.]
 [ 1. 14. 31.  3.  4.]
 [16.  7. 10. 52.  5.]
 [13. 21. 24. 21. 54.]]

I - Epoch: 159
I - Training: 
	I - Batch: 50 | Loss: 0.584 | Acc: 91.500% | Wgt Acc: 93.588%
	I - Batch: 100 | Loss: 0.586 | Acc: 91.812% | Wgt Acc: 93.906%
	I - Batch: 150 | Loss: 0.579 | Acc: 92.375% | Wgt Acc: 94.414%
	I - Batch: 200 | Loss: 0.580 | Acc: 92.094% | Wgt Acc: 94.326%
I - num batch: 222
I - Train -- Loss: 0.580 | Acc: 92.134% | Wgt Acc: 94.385% | LR: 1.250000e-04 | Dur: 122.41s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   3.   7.  77.]
 [  1. 573.   2.   3.  38.]
 [  1.   1. 708.   1.  46.]
 [  1.   0.   2. 514.  38.]
 [ 22.   4.  19.  13. 801.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.207 | Acc: 56.391% | Wgt Acc: 54.827% | Dur: 13.04s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  7.  4. 17.  8.]
 [ 1. 35.  9.  3.  7.]
 [ 0. 13. 30.  6.  7.]
 [ 6.  3.  9. 45.  3.]
 [13. 20. 23. 15. 47.]]

I - Epoch: 160
I - Training: 
	I - Batch: 50 | Loss: 0.582 | Acc: 92.500% | Wgt Acc: 94.770%
	I - Batch: 100 | Loss: 0.586 | Acc: 92.250% | Wgt Acc: 94.534%
	I - Batch: 150 | Loss: 0.591 | Acc: 91.667% | Wgt Acc: 94.116%
	I - Batch: 200 | Loss: 0.588 | Acc: 91.812% | Wgt Acc: 94.193%
I - num batch: 222
I - Train -- Loss: 0.588 | Acc: 91.796% | Wgt Acc: 94.182% | LR: 1.250000e-04 | Dur: 127.53s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   1.   9.  59.]
 [  0. 572.   2.   6.  42.]
 [  4.   0. 706.   1.  62.]
 [  2.   0.   3. 514.  47.]
 [ 17.   6.  22.   8. 790.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.210 | Acc: 57.644% | Wgt Acc: 54.765% | Dur: 13.09s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  5.  5. 18.  5.]
 [ 2. 35.  6.  4.  0.]
 [ 1. 14. 29.  3.  7.]
 [ 7.  2.  9. 42.  4.]
 [10. 22. 26. 19. 56.]]

I - Epoch: 161
I - Training: 
	I - Batch: 50 | Loss: 0.529 | Acc: 95.750% | Wgt Acc: 97.206%
	I - Batch: 100 | Loss: 0.523 | Acc: 96.250% | Wgt Acc: 97.576%
	I - Batch: 150 | Loss: 0.526 | Acc: 95.917% | Wgt Acc: 97.257%
I - num batch: 173
I - Train -- Loss: 0.527 | Acc: 95.839% | Wgt Acc: 97.103% | LR: 1.250000e-04 | Dur: 97.43s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   4.  10.  22.]
 [  3. 576.   4.   2.  16.]
 [  2.   0. 721.   0.  23.]
 [  4.   1.   0. 522.   8.]
 [  6.   1.   5.   4. 148.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.204 | Acc: 56.642% | Wgt Acc: 55.817% | Dur: 12.79s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  6.  5. 17. 12.]
 [ 4. 37.  6.  2.  5.]
 [ 0. 16. 30.  2.  4.]
 [13.  8. 14. 51.  6.]
 [ 8. 11. 20. 14. 45.]]

I - Epoch: 162
I - Training: 
	I - Batch: 50 | Loss: 0.563 | Acc: 93.750% | Wgt Acc: 96.192%
	I - Batch: 100 | Loss: 0.570 | Acc: 93.375% | Wgt Acc: 95.674%
	I - Batch: 150 | Loss: 0.573 | Acc: 93.083% | Wgt Acc: 95.397%
	I - Batch: 200 | Loss: 0.577 | Acc: 92.812% | Wgt Acc: 95.037%
I - num batch: 222
I - Train -- Loss: 0.577 | Acc: 92.613% | Wgt Acc: 94.896% | LR: 1.250000e-04 | Dur: 123.22s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   2.  10.  62.]
 [  0. 576.   1.   2.  43.]
 [  1.   0. 710.   1.  38.]
 [  4.   0.   3. 517.  53.]
 [ 14.   2.  18.   8. 804.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.242 | Acc: 55.890% | Wgt Acc: 51.733% | Dur: 12.50s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  2. 19.  3.]
 [ 2. 32.  6.  2.  2.]
 [ 1. 15. 25.  2.  3.]
 [ 6.  2. 11. 38.  1.]
 [14. 23. 31. 25. 63.]]

I - Epoch: 163
I - Training: 
	I - Batch: 50 | Loss: 0.591 | Acc: 92.625% | Wgt Acc: 94.263%
	I - Batch: 100 | Loss: 0.584 | Acc: 92.562% | Wgt Acc: 94.571%
	I - Batch: 150 | Loss: 0.582 | Acc: 92.875% | Wgt Acc: 94.824%
	I - Batch: 200 | Loss: 0.581 | Acc: 92.969% | Wgt Acc: 95.013%
I - num batch: 222
I - Train -- Loss: 0.581 | Acc: 92.952% | Wgt Acc: 94.994% | LR: 1.250000e-04 | Dur: 125.47s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   4.   8.  61.]
 [  2. 570.   1.   4.  29.]
 [  2.   1. 717.   1.  44.]
 [  2.   1.   3. 518.  47.]
 [ 18.   5.   9.   7. 819.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.181 | Acc: 59.900% | Wgt Acc: 57.611% | Dur: 12.89s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  3.  1. 13.  1.]
 [ 3. 35. 10.  1.  6.]
 [ 0. 10. 33.  3.  4.]
 [ 9.  7.  9. 52.  5.]
 [13. 23. 22. 17. 56.]]

I - Epoch: 164
I - Training: 
	I - Batch: 50 | Loss: 0.580 | Acc: 93.125% | Wgt Acc: 95.244%
	I - Batch: 100 | Loss: 0.589 | Acc: 92.188% | Wgt Acc: 94.302%
	I - Batch: 150 | Loss: 0.581 | Acc: 92.917% | Wgt Acc: 94.767%
	I - Batch: 200 | Loss: 0.580 | Acc: 92.781% | Wgt Acc: 94.642%
I - num batch: 222
I - Train -- Loss: 0.581 | Acc: 92.811% | Wgt Acc: 94.663% | LR: 1.250000e-04 | Dur: 123.62s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   0.   4.   8.  51.]
 [  3. 573.   4.   3.  28.]
 [  0.   3. 709.   4.  47.]
 [  4.   0.   1. 511.  45.]
 [ 20.   2.  16.  12. 829.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.236 | Acc: 57.644% | Wgt Acc: 55.260% | Dur: 12.44s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  2. 19.  8.]
 [ 4. 28.  8.  3.  1.]
 [ 1. 14. 29.  3.  2.]
 [ 7.  6. 12. 51.  8.]
 [ 7. 24. 24. 10. 53.]]

I - Epoch: 165
I - Training: 
	I - Batch: 50 | Loss: 0.578 | Acc: 93.750% | Wgt Acc: 95.470%
	I - Batch: 100 | Loss: 0.582 | Acc: 92.812% | Wgt Acc: 94.949%
	I - Batch: 150 | Loss: 0.582 | Acc: 92.625% | Wgt Acc: 94.831%
	I - Batch: 200 | Loss: 0.578 | Acc: 92.906% | Wgt Acc: 95.028%
I - num batch: 222
I - Train -- Loss: 0.578 | Acc: 92.783% | Wgt Acc: 94.949% | LR: 1.250000e-04 | Dur: 123.45s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   4.   5.  60.]
 [  3. 571.   4.   2.  38.]
 [  0.   2. 714.   2.  52.]
 [  2.   0.   3. 517.  40.]
 [ 13.   5.   9.  12. 810.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.219 | Acc: 56.391% | Wgt Acc: 53.651% | Dur: 12.50s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  5. 15.  4.]
 [ 1. 34.  5.  5.  2.]
 [ 0.  7. 22.  1.  3.]
 [12.  8. 14. 47.  6.]
 [10. 25. 29. 18. 57.]]

I - Epoch: 166
I - Training: 
	I - Batch: 50 | Loss: 0.597 | Acc: 90.125% | Wgt Acc: 92.627%
	I - Batch: 100 | Loss: 0.584 | Acc: 92.125% | Wgt Acc: 94.024%
	I - Batch: 150 | Loss: 0.578 | Acc: 92.542% | Wgt Acc: 94.554%
	I - Batch: 200 | Loss: 0.580 | Acc: 92.469% | Wgt Acc: 94.617%
I - num batch: 222
I - Train -- Loss: 0.578 | Acc: 92.642% | Wgt Acc: 94.738% | LR: 1.250000e-04 | Dur: 123.72s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   4.   8.  60.]
 [  1. 571.   2.   3.  33.]
 [  1.   0. 712.   1.  49.]
 [  3.   1.   1. 517.  44.]
 [ 20.   6.  15.   9. 814.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.205 | Acc: 57.644% | Wgt Acc: 54.455% | Dur: 12.14s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  0.  2. 11.  3.]
 [ 3. 32.  2.  4.  2.]
 [ 0. 16. 32.  1.  5.]
 [ 9.  3.  4. 46.  3.]
 [15. 27. 35. 24. 59.]]

I - Epoch: 167
I - Training: 
	I - Batch: 50 | Loss: 0.568 | Acc: 92.875% | Wgt Acc: 95.236%
	I - Batch: 100 | Loss: 0.571 | Acc: 92.562% | Wgt Acc: 94.783%
	I - Batch: 150 | Loss: 0.571 | Acc: 92.542% | Wgt Acc: 94.861%
	I - Batch: 200 | Loss: 0.575 | Acc: 92.031% | Wgt Acc: 94.494%
I - num batch: 222
I - Train -- Loss: 0.574 | Acc: 92.021% | Wgt Acc: 94.445% | LR: 1.250000e-04 | Dur: 121.93s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   0.   5.   5.  63.]
 [  1. 575.   2.   4.  49.]
 [  0.   0. 705.   1.  50.]
 [  2.   0.   2. 520.  45.]
 [ 23.   3.  20.   8. 793.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.233 | Acc: 54.637% | Wgt Acc: 51.485% | Dur: 12.87s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  8.  3. 30. 10.]
 [ 1. 30. 10.  5.  4.]
 [ 0. 13. 30.  1.  5.]
 [ 6.  6.  7. 34.  1.]
 [ 9. 21. 25. 16. 52.]]

I - Epoch: 168
I - Training: 
	I - Batch: 50 | Loss: 0.578 | Acc: 93.500% | Wgt Acc: 95.632%
	I - Batch: 100 | Loss: 0.581 | Acc: 93.750% | Wgt Acc: 95.523%
	I - Batch: 150 | Loss: 0.584 | Acc: 92.708% | Wgt Acc: 94.739%
	I - Batch: 200 | Loss: 0.579 | Acc: 93.062% | Wgt Acc: 95.012%
I - num batch: 222
I - Train -- Loss: 0.581 | Acc: 92.811% | Wgt Acc: 94.874% | LR: 1.250000e-04 | Dur: 125.05s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   3.   8.  50.]
 [  0. 573.   2.   3.  40.]
 [  1.   1. 706.   2.  43.]
 [  6.   0.   4. 517.  49.]
 [ 12.   4.  19.   8. 818.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.246 | Acc: 55.890% | Wgt Acc: 53.651% | Dur: 12.92s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  5.  2. 16.  8.]
 [ 3. 38.  8.  3.  7.]
 [ 2. 14. 21.  2.  2.]
 [ 7.  3. 12. 47.  0.]
 [14. 18. 32. 18. 55.]]

I - Epoch: 169
I - Training: 
	I - Batch: 50 | Loss: 0.529 | Acc: 96.000% | Wgt Acc: 96.852%
	I - Batch: 100 | Loss: 0.528 | Acc: 96.312% | Wgt Acc: 97.343%
	I - Batch: 150 | Loss: 0.523 | Acc: 96.875% | Wgt Acc: 97.723%
I - num batch: 173
I - Train -- Loss: 0.522 | Acc: 96.816% | Wgt Acc: 97.751% | LR: 1.250000e-04 | Dur: 100.14s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   3.   4.  14.]
 [  0. 578.   4.   4.  13.]
 [  4.   0. 721.   0.  15.]
 [  2.   0.   1. 526.   8.]
 [  7.   0.   5.   4. 167.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.223 | Acc: 52.632% | Wgt Acc: 52.042% | Dur: 13.72s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  4. 27. 13.]
 [ 3. 33.  8.  2.  9.]
 [ 1. 13. 26.  2.  3.]
 [ 9.  9. 16. 44.  9.]
 [ 6. 17. 21. 11. 38.]]

I - Epoch: 170
I - Training: 
	I - Batch: 50 | Loss: 0.586 | Acc: 90.750% | Wgt Acc: 93.924%
	I - Batch: 100 | Loss: 0.579 | Acc: 92.312% | Wgt Acc: 94.784%
	I - Batch: 150 | Loss: 0.579 | Acc: 92.375% | Wgt Acc: 94.772%
	I - Batch: 200 | Loss: 0.579 | Acc: 92.719% | Wgt Acc: 95.027%
I - num batch: 222
I - Train -- Loss: 0.577 | Acc: 92.726% | Wgt Acc: 95.009% | LR: 1.250000e-04 | Dur: 125.51s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   3.   6.  75.]
 [  1. 573.   4.   3.  41.]
 [  0.   2. 719.   3.  39.]
 [  3.   0.   1. 519.  41.]
 [ 19.   3.   7.   7. 804.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.202 | Acc: 55.639% | Wgt Acc: 53.280% | Dur: 12.28s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  2. 12.  6.]
 [ 2. 31.  8.  2.  4.]
 [ 1. 15. 27.  4.  5.]
 [11.  7. 12. 50.  3.]
 [14. 21. 26. 18. 54.]]

I - Epoch: 171
I - Training: 
	I - Batch: 50 | Loss: 0.577 | Acc: 92.375% | Wgt Acc: 94.331%
	I - Batch: 100 | Loss: 0.575 | Acc: 92.625% | Wgt Acc: 94.879%
	I - Batch: 150 | Loss: 0.578 | Acc: 92.125% | Wgt Acc: 94.551%
	I - Batch: 200 | Loss: 0.572 | Acc: 92.562% | Wgt Acc: 94.822%
I - num batch: 222
I - Train -- Loss: 0.576 | Acc: 92.332% | Wgt Acc: 94.708% | LR: 1.250000e-04 | Dur: 124.24s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   4.   4.  67.]
 [  0. 573.   2.   3.  36.]
 [  2.   1. 715.   1.  57.]
 [  1.   0.   2. 519.  44.]
 [ 22.   4.  11.  11. 796.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.266 | Acc: 55.639% | Wgt Acc: 53.899% | Dur: 13.26s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  6.  3. 16. 10.]
 [ 2. 30.  4.  3.  4.]
 [ 0.  8. 15.  0.  3.]
 [ 9. 10. 23. 55.  4.]
 [ 6. 24. 30. 12. 51.]]

I - Epoch: 172
I - Training: 
	I - Batch: 50 | Loss: 0.572 | Acc: 93.500% | Wgt Acc: 95.357%
	I - Batch: 100 | Loss: 0.575 | Acc: 93.188% | Wgt Acc: 95.132%
	I - Batch: 150 | Loss: 0.578 | Acc: 92.917% | Wgt Acc: 94.978%
	I - Batch: 200 | Loss: 0.578 | Acc: 92.938% | Wgt Acc: 94.941%
I - num batch: 222
I - Train -- Loss: 0.579 | Acc: 92.980% | Wgt Acc: 94.926% | LR: 1.250000e-04 | Dur: 123.71s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   2.   8.  60.]
 [  1. 575.   3.   3.  32.]
 [  1.   0. 707.   1.  37.]
 [  4.   0.   3. 514.  45.]
 [ 15.   3.  19.  12. 826.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.265 | Acc: 53.383% | Wgt Acc: 49.814% | Dur: 12.19s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  3.  2. 18.  3.]
 [ 1. 24.  5.  2.  2.]
 [ 1. 10. 23.  0.  5.]
 [10. 10. 13. 47.  3.]
 [16. 31. 32. 19. 59.]]

I - Epoch: 173
I - Training: 
	I - Batch: 50 | Loss: 0.571 | Acc: 92.750% | Wgt Acc: 95.050%
	I - Batch: 100 | Loss: 0.571 | Acc: 93.250% | Wgt Acc: 95.449%
	I - Batch: 150 | Loss: 0.572 | Acc: 93.042% | Wgt Acc: 95.224%
	I - Batch: 200 | Loss: 0.573 | Acc: 93.062% | Wgt Acc: 95.196%
I - num batch: 222
I - Train -- Loss: 0.572 | Acc: 93.036% | Wgt Acc: 95.167% | LR: 1.250000e-04 | Dur: 123.74s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   1.   7.  69.]
 [  1. 577.   2.   4.  31.]
 [  0.   0. 714.   1.  56.]
 [  2.   0.   3. 516.  28.]
 [ 17.   1.  14.  10. 816.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.226 | Acc: 55.890% | Wgt Acc: 52.970% | Dur: 12.86s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  2.  1. 12.  2.]
 [ 2. 34.  4.  2.  6.]
 [ 1. 14. 29.  4.  4.]
 [11.  4. 13. 46.  2.]
 [18. 24. 28. 22. 58.]]

I - Epoch: 174
I - Training: 
	I - Batch: 50 | Loss: 0.564 | Acc: 94.125% | Wgt Acc: 95.757%
	I - Batch: 100 | Loss: 0.569 | Acc: 93.375% | Wgt Acc: 95.374%
	I - Batch: 150 | Loss: 0.569 | Acc: 93.125% | Wgt Acc: 95.328%
	I - Batch: 200 | Loss: 0.570 | Acc: 92.875% | Wgt Acc: 95.172%
I - num batch: 222
I - Train -- Loss: 0.572 | Acc: 92.811% | Wgt Acc: 95.114% | LR: 1.250000e-04 | Dur: 126.30s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   5.   6.  69.]
 [  2. 575.   1.   3.  37.]
 [  0.   0. 713.   1.  48.]
 [  2.   0.   1. 521.  41.]
 [ 15.   3.  14.   7. 805.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.235 | Acc: 54.637% | Wgt Acc: 51.238% | Dur: 12.82s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  5.  5. 15.  3.]
 [ 2. 26.  4.  2.  4.]
 [ 0. 10. 25.  0.  4.]
 [ 8.  9. 11. 48.  2.]
 [18. 28. 30. 21. 59.]]

I - Epoch: 175
I - Training: 
	I - Batch: 50 | Loss: 0.578 | Acc: 92.500% | Wgt Acc: 94.877%
	I - Batch: 100 | Loss: 0.587 | Acc: 91.812% | Wgt Acc: 94.122%
	I - Batch: 150 | Loss: 0.582 | Acc: 92.292% | Wgt Acc: 94.574%
	I - Batch: 200 | Loss: 0.581 | Acc: 92.438% | Wgt Acc: 94.619%
I - num batch: 222
I - Train -- Loss: 0.578 | Acc: 92.642% | Wgt Acc: 94.806% | LR: 1.250000e-04 | Dur: 124.93s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   4.   6.  68.]
 [  2. 573.   2.   3.  37.]
 [  0.   0. 707.   0.  43.]
 [  2.   0.   0. 518.  41.]
 [ 16.   5.  21.  11. 811.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.205 | Acc: 57.644% | Wgt Acc: 54.579% | Dur: 12.56s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  5.  2. 12.  3.]
 [ 1. 31.  8.  3.  1.]
 [ 0. 12. 30.  1.  5.]
 [14.  7. 12. 51.  3.]
 [15. 23. 23. 19. 60.]]

I - Epoch: 176
I - Training: 
	I - Batch: 50 | Loss: 0.574 | Acc: 93.000% | Wgt Acc: 95.039%
	I - Batch: 100 | Loss: 0.567 | Acc: 93.500% | Wgt Acc: 95.531%
	I - Batch: 150 | Loss: 0.569 | Acc: 93.458% | Wgt Acc: 95.457%
	I - Batch: 200 | Loss: 0.571 | Acc: 93.250% | Wgt Acc: 95.257%
I - num batch: 222
I - Train -- Loss: 0.571 | Acc: 93.206% | Wgt Acc: 95.280% | LR: 1.250000e-04 | Dur: 124.84s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   2.   5.  47.]
 [  0. 575.   2.   4.  38.]
 [  2.   0. 715.   0.  44.]
 [  3.   0.   0. 521.  49.]
 [ 19.   3.  15.   8. 822.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.205 | Acc: 57.895% | Wgt Acc: 54.950% | Dur: 12.38s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  4.  4. 19.  8.]
 [ 2. 35.  6.  5.  2.]
 [ 0. 12. 30.  2.  4.]
 [ 6.  7.  9. 41.  2.]
 [11. 20. 26. 19. 56.]]

I - Epoch: 177
I - Training: 
	I - Batch: 50 | Loss: 0.529 | Acc: 96.750% | Wgt Acc: 97.462%
	I - Batch: 100 | Loss: 0.519 | Acc: 96.562% | Wgt Acc: 97.373%
	I - Batch: 150 | Loss: 0.520 | Acc: 96.167% | Wgt Acc: 97.245%
I - num batch: 173
I - Train -- Loss: 0.519 | Acc: 96.382% | Wgt Acc: 97.436% | LR: 1.250000e-04 | Dur: 97.69s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   4.   6.  23.]
 [  5. 576.   4.   3.  10.]
 [  0.   1. 722.   3.  20.]
 [  2.   0.   0. 523.   5.]
 [  6.   1.   4.   3. 159.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.202 | Acc: 54.887% | Wgt Acc: 53.403% | Dur: 13.31s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  4. 16.  5.]
 [ 3. 31.  5.  4.  7.]
 [ 1. 23. 33.  7.  9.]
 [15.  8. 13. 50.  4.]
 [11. 12. 20.  9. 47.]]

I - Epoch: 178
I - Training: 
	I - Batch: 50 | Loss: 0.566 | Acc: 93.375% | Wgt Acc: 95.800%
	I - Batch: 100 | Loss: 0.566 | Acc: 93.375% | Wgt Acc: 95.485%
	I - Batch: 150 | Loss: 0.570 | Acc: 93.708% | Wgt Acc: 95.562%
	I - Batch: 200 | Loss: 0.570 | Acc: 93.594% | Wgt Acc: 95.498%
I - num batch: 222
I - Train -- Loss: 0.571 | Acc: 93.487% | Wgt Acc: 95.370% | LR: 1.250000e-04 | Dur: 127.20s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   1.   2.   5.  60.]
 [  1. 574.   3.   5.  33.]
 [  1.   1. 715.   1.  35.]
 [  2.   0.   2. 520.  37.]
 [ 21.   2.  12.   7. 835.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.192 | Acc: 58.897% | Wgt Acc: 55.941% | Dur: 12.47s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  4.  3. 16.  5.]
 [ 2. 34.  4.  3.  3.]
 [ 1. 11. 27.  3.  4.]
 [ 7.  8. 17. 46.  2.]
 [ 8. 21. 24. 18. 58.]]

I - Epoch: 179
I - Training: 
	I - Batch: 50 | Loss: 0.565 | Acc: 93.875% | Wgt Acc: 96.118%
	I - Batch: 100 | Loss: 0.569 | Acc: 93.375% | Wgt Acc: 95.603%
	I - Batch: 150 | Loss: 0.567 | Acc: 93.542% | Wgt Acc: 95.554%
	I - Batch: 200 | Loss: 0.570 | Acc: 93.219% | Wgt Acc: 95.213%
I - num batch: 222
I - Train -- Loss: 0.571 | Acc: 93.234% | Wgt Acc: 95.257% | LR: 1.250000e-04 | Dur: 128.67s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   3.   6.  59.]
 [  1. 572.   1.   2.  40.]
 [  3.   0. 717.   1.  36.]
 [  3.   0.   0. 519.  42.]
 [ 14.   6.  13.  10. 823.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.197 | Acc: 58.145% | Wgt Acc: 56.683% | Dur: 13.30s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  6.  5. 13.  7.]
 [ 2. 38. 12.  4.  8.]
 [ 0. 14. 27.  2.  5.]
 [11.  5.  7. 50.  2.]
 [ 8. 15. 24. 17. 50.]]

I - Epoch: 180
I - Training: 
	I - Batch: 50 | Loss: 0.558 | Acc: 93.500% | Wgt Acc: 95.690%
	I - Batch: 100 | Loss: 0.565 | Acc: 93.438% | Wgt Acc: 95.741%
	I - Batch: 150 | Loss: 0.567 | Acc: 93.542% | Wgt Acc: 95.777%
	I - Batch: 200 | Loss: 0.568 | Acc: 93.281% | Wgt Acc: 95.458%
I - num batch: 222
I - Train -- Loss: 0.568 | Acc: 93.262% | Wgt Acc: 95.498% | LR: 1.250000e-04 | Dur: 125.16s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   5.   6.  64.]
 [  0. 577.   3.   4.  25.]
 [  0.   0. 714.   1.  53.]
 [  2.   0.   1. 524.  44.]
 [ 16.   1.  11.   3. 814.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.222 | Acc: 56.140% | Wgt Acc: 51.300% | Dur: 12.45s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  2.  1.  8.  1.]
 [ 2. 29.  4.  2.  1.]
 [ 1. 14. 31.  1.  2.]
 [ 8.  5.  7. 40.  0.]
 [21. 28. 32. 35. 68.]]

I - Epoch: 181
I - Training: 
	I - Batch: 50 | Loss: 0.563 | Acc: 94.125% | Wgt Acc: 95.799%
	I - Batch: 100 | Loss: 0.562 | Acc: 94.438% | Wgt Acc: 95.993%
	I - Batch: 150 | Loss: 0.567 | Acc: 94.083% | Wgt Acc: 95.690%
	I - Batch: 200 | Loss: 0.567 | Acc: 93.969% | Wgt Acc: 95.725%
I - num batch: 222
I - Train -- Loss: 0.567 | Acc: 93.882% | Wgt Acc: 95.708% | LR: 1.250000e-04 | Dur: 123.33s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   4.   4.  56.]
 [  1. 575.   1.   4.  31.]
 [  3.   0. 715.   1.  38.]
 [  2.   0.   1. 522.  33.]
 [ 15.   3.  13.   7. 842.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.226 | Acc: 57.895% | Wgt Acc: 54.827% | Dur: 12.76s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  4.  2. 12.  4.]
 [ 1. 32.  6.  2.  2.]
 [ 0. 18. 27.  2.  3.]
 [ 9.  4. 10. 50.  3.]
 [16. 20. 30. 20. 60.]]

I - Epoch: 182
I - Training: 
	I - Batch: 50 | Loss: 0.563 | Acc: 93.750% | Wgt Acc: 95.951%
	I - Batch: 100 | Loss: 0.566 | Acc: 93.312% | Wgt Acc: 95.372%
	I - Batch: 150 | Loss: 0.568 | Acc: 93.333% | Wgt Acc: 95.287%
	I - Batch: 200 | Loss: 0.571 | Acc: 93.281% | Wgt Acc: 95.249%
I - num batch: 222
I - Train -- Loss: 0.570 | Acc: 93.318% | Wgt Acc: 95.340% | LR: 1.250000e-04 | Dur: 123.81s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   1.   5.   3.  49.]
 [  1. 572.   0.   4.  39.]
 [  0.   0. 716.   0.  43.]
 [  1.   0.   0. 522.  44.]
 [ 20.   5.  13.   9. 825.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.241 | Acc: 55.138% | Wgt Acc: 52.042% | Dur: 12.40s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  5. 18.  3.]
 [ 3. 36. 11.  4.  5.]
 [ 0. 15. 24.  2.  4.]
 [ 9.  3.  7. 43.  1.]
 [18. 20. 28. 19. 59.]]

I - Epoch: 183
I - Training: 
	I - Batch: 50 | Loss: 0.555 | Acc: 93.875% | Wgt Acc: 95.612%
	I - Batch: 100 | Loss: 0.574 | Acc: 92.750% | Wgt Acc: 94.850%
	I - Batch: 150 | Loss: 0.572 | Acc: 93.000% | Wgt Acc: 95.047%
	I - Batch: 200 | Loss: 0.572 | Acc: 93.094% | Wgt Acc: 95.120%
I - num batch: 222
I - Train -- Loss: 0.570 | Acc: 93.262% | Wgt Acc: 95.242% | LR: 1.250000e-04 | Dur: 122.24s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   1.   3.   8.  61.]
 [  1. 573.   1.   2.  30.]
 [  1.   0. 710.   3.  41.]
 [  0.   1.   1. 520.  41.]
 [ 17.   3.  19.   5. 827.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.230 | Acc: 56.140% | Wgt Acc: 52.970% | Dur: 12.62s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  3. 16.  6.]
 [ 6. 36.  6.  4.  2.]
 [ 0. 11. 26.  1.  3.]
 [ 8.  2. 10. 44.  1.]
 [16. 25. 30. 21. 60.]]

I - Epoch: 184
I - Training: 
	I - Batch: 50 | Loss: 0.566 | Acc: 93.500% | Wgt Acc: 95.604%
	I - Batch: 100 | Loss: 0.564 | Acc: 93.438% | Wgt Acc: 95.628%
	I - Batch: 150 | Loss: 0.569 | Acc: 93.417% | Wgt Acc: 95.493%
	I - Batch: 200 | Loss: 0.571 | Acc: 93.156% | Wgt Acc: 95.391%
I - num batch: 222
I - Train -- Loss: 0.571 | Acc: 93.121% | Wgt Acc: 95.287% | LR: 1.250000e-04 | Dur: 124.22s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   4.   6.  62.]
 [  2. 574.   2.   2.  33.]
 [  1.   1. 714.   0.  44.]
 [  4.   1.   3. 523.  45.]
 [ 14.   2.  11.   7. 816.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.234 | Acc: 57.143% | Wgt Acc: 53.403% | Dur: 12.91s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  3.  4. 18.  5.]
 [ 3. 34.  6.  4.  3.]
 [ 0. 13. 25.  1.  1.]
 [ 8.  5.  8. 41.  1.]
 [11. 23. 32. 22. 62.]]

I - Epoch: 185
I - Training: 
	I - Batch: 50 | Loss: 0.517 | Acc: 96.375% | Wgt Acc: 97.426%
	I - Batch: 100 | Loss: 0.519 | Acc: 96.812% | Wgt Acc: 97.743%
	I - Batch: 150 | Loss: 0.521 | Acc: 96.375% | Wgt Acc: 97.397%
I - num batch: 173
I - Train -- Loss: 0.518 | Acc: 96.599% | Wgt Acc: 97.572% | LR: 1.250000e-04 | Dur: 100.03s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   4.   5.  20.]
 [  5. 576.   4.   3.   9.]
 [  3.   0. 721.   4.  16.]
 [  3.   1.   1. 525.   8.]
 [  2.   1.   4.   1. 164.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.164 | Acc: 59.649% | Wgt Acc: 59.282% | Dur: 12.89s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  6. 12.  5.]
 [ 5. 42. 11.  4. 12.]
 [ 0. 12. 28.  4.  5.]
 [ 8.  7. 11. 54.  5.]
 [ 6. 12. 19. 12. 45.]]

I - Epoch: 186
I - Training: 
	I - Batch: 50 | Loss: 0.563 | Acc: 93.500% | Wgt Acc: 95.895%
	I - Batch: 100 | Loss: 0.563 | Acc: 94.312% | Wgt Acc: 96.344%
	I - Batch: 150 | Loss: 0.565 | Acc: 93.750% | Wgt Acc: 95.767%
	I - Batch: 200 | Loss: 0.562 | Acc: 93.938% | Wgt Acc: 95.911%
I - num batch: 222
I - Train -- Loss: 0.561 | Acc: 93.854% | Wgt Acc: 95.881% | LR: 1.250000e-04 | Dur: 124.88s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   2.   2.  57.]
 [  0. 578.   2.   2.  25.]
 [  2.   0. 719.   2.  45.]
 [  2.   0.   3. 526.  41.]
 [ 19.   0.   8.   6. 832.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.237 | Acc: 55.639% | Wgt Acc: 52.723% | Dur: 13.09s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  3.  4. 18.  8.]
 [ 4. 30.  7.  2.  5.]
 [ 0. 12. 25.  0.  3.]
 [ 7.  3. 11. 46.  0.]
 [12. 30. 28. 20. 56.]]

I - Epoch: 187
I - Training: 
	I - Batch: 50 | Loss: 0.571 | Acc: 94.125% | Wgt Acc: 95.539%
	I - Batch: 100 | Loss: 0.571 | Acc: 93.562% | Wgt Acc: 95.080%
	I - Batch: 150 | Loss: 0.568 | Acc: 93.417% | Wgt Acc: 95.307%
	I - Batch: 200 | Loss: 0.569 | Acc: 93.000% | Wgt Acc: 95.137%
I - num batch: 222
I - Train -- Loss: 0.567 | Acc: 93.234% | Wgt Acc: 95.242% | LR: 1.250000e-04 | Dur: 125.32s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   3.   6.  59.]
 [  1. 576.   3.   5.  35.]
 [  0.   0. 712.   0.  34.]
 [  4.   0.   3. 519.  46.]
 [ 18.   2.  13.   8. 826.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.227 | Acc: 54.637% | Wgt Acc: 52.413% | Dur: 12.46s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  3.  4. 15.  4.]
 [ 3. 27.  6.  2.  3.]
 [ 0. 15. 27.  1.  6.]
 [14.  9. 14. 52.  7.]
 [11. 24. 24. 16. 52.]]

I - Epoch: 188
I - Training: 
	I - Batch: 50 | Loss: 0.556 | Acc: 94.375% | Wgt Acc: 96.091%
	I - Batch: 100 | Loss: 0.565 | Acc: 93.562% | Wgt Acc: 95.455%
	I - Batch: 150 | Loss: 0.564 | Acc: 93.500% | Wgt Acc: 95.431%
	I - Batch: 200 | Loss: 0.565 | Acc: 93.438% | Wgt Acc: 95.403%
I - num batch: 222
I - Train -- Loss: 0.567 | Acc: 93.346% | Wgt Acc: 95.317% | LR: 1.250000e-04 | Dur: 123.24s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   0.   3.   5.  55.]
 [  1. 575.   2.   3.  29.]
 [  1.   0. 712.   1.  38.]
 [  4.   0.   2. 520.  49.]
 [ 16.   3.  15.   9. 829.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.231 | Acc: 56.642% | Wgt Acc: 53.960% | Dur: 12.69s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  4.  3. 14.  7.]
 [ 1. 31.  4.  1.  4.]
 [ 0. 16. 28.  3.  2.]
 [12.  2. 13. 49.  3.]
 [13. 25. 27. 19. 56.]]

I - Epoch: 189
I - Training: 
	I - Batch: 50 | Loss: 0.568 | Acc: 93.375% | Wgt Acc: 95.307%
	I - Batch: 100 | Loss: 0.563 | Acc: 94.062% | Wgt Acc: 95.613%
	I - Batch: 150 | Loss: 0.568 | Acc: 93.625% | Wgt Acc: 95.344%
	I - Batch: 200 | Loss: 0.569 | Acc: 93.375% | Wgt Acc: 95.327%
I - num batch: 222
I - Train -- Loss: 0.569 | Acc: 93.375% | Wgt Acc: 95.310% | LR: 1.250000e-04 | Dur: 124.93s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   3.   6.  58.]
 [  3. 575.   4.   3.  39.]
 [  0.   1. 714.   1.  40.]
 [  2.   0.   0. 519.  32.]
 [ 19.   1.  13.   9. 831.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.190 | Acc: 58.647% | Wgt Acc: 57.302% | Dur: 13.18s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  3.  4.  9.  1.]
 [ 2. 44.  9.  6.  7.]
 [ 1.  7. 26.  2.  6.]
 [14.  8. 13. 52.  5.]
 [12. 16. 23. 17. 53.]]

I - Epoch: 190
I - Training: 
	I - Batch: 50 | Loss: 0.576 | Acc: 93.250% | Wgt Acc: 95.553%
	I - Batch: 100 | Loss: 0.565 | Acc: 93.562% | Wgt Acc: 95.767%
	I - Batch: 150 | Loss: 0.564 | Acc: 93.417% | Wgt Acc: 95.672%
	I - Batch: 200 | Loss: 0.569 | Acc: 92.969% | Wgt Acc: 95.192%
I - num batch: 222
I - Train -- Loss: 0.570 | Acc: 92.811% | Wgt Acc: 95.062% | LR: 1.250000e-04 | Dur: 123.42s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   3.   5.  73.]
 [  2. 574.   2.   3.  31.]
 [  0.   0. 719.   1.  41.]
 [  1.   0.   1. 517.  49.]
 [ 18.   4.   9.  12. 806.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.237 | Acc: 56.892% | Wgt Acc: 53.342% | Dur: 12.40s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  3.  1. 11.  3.]
 [ 1. 33.  7.  3.  4.]
 [ 0. 12. 31.  4.  3.]
 [ 9.  6.  8. 45.  0.]
 [22. 24. 28. 23. 62.]]

I - Epoch: 191
I - Training: 
	I - Batch: 50 | Loss: 0.572 | Acc: 92.500% | Wgt Acc: 94.717%
	I - Batch: 100 | Loss: 0.572 | Acc: 92.875% | Wgt Acc: 94.964%
	I - Batch: 150 | Loss: 0.572 | Acc: 93.000% | Wgt Acc: 95.180%
	I - Batch: 200 | Loss: 0.570 | Acc: 93.188% | Wgt Acc: 95.384%
I - num batch: 222
I - Train -- Loss: 0.570 | Acc: 92.924% | Wgt Acc: 95.159% | LR: 1.250000e-04 | Dur: 122.76s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   5.   4.  69.]
 [  0. 576.   5.   2.  36.]
 [  0.   0. 714.   3.  47.]
 [  3.   1.   0. 522.  37.]
 [ 21.   1.  10.   7. 811.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.207 | Acc: 56.642% | Wgt Acc: 53.775% | Dur: 12.90s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  1.  2. 15.  4.]
 [ 1. 31.  6.  4.  5.]
 [ 0. 14. 28.  3.  3.]
 [ 9.  5. 11. 48.  3.]
 [16. 27. 28. 16. 57.]]

I - Epoch: 192
I - Training: 
	I - Batch: 50 | Loss: 0.569 | Acc: 93.875% | Wgt Acc: 95.796%
	I - Batch: 100 | Loss: 0.579 | Acc: 92.938% | Wgt Acc: 95.020%
	I - Batch: 150 | Loss: 0.573 | Acc: 93.458% | Wgt Acc: 95.437%
	I - Batch: 200 | Loss: 0.568 | Acc: 93.500% | Wgt Acc: 95.544%
I - num batch: 222
I - Train -- Loss: 0.568 | Acc: 93.544% | Wgt Acc: 95.535% | LR: 1.250000e-04 | Dur: 123.20s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   2.   5.  58.]
 [  3. 572.   3.   2.  32.]
 [  2.   1. 717.   1.  34.]
 [  0.   0.   0. 522.  48.]
 [ 13.   5.  12.   8. 828.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.212 | Acc: 55.639% | Wgt Acc: 53.837% | Dur: 13.24s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  3.  1. 14.  4.]
 [ 1. 33.  8.  2.  5.]
 [ 0.  9. 22.  2.  4.]
 [13. 11. 14. 55.  6.]
 [15. 22. 30. 13. 53.]]

I - Epoch: 193
I - Training: 
	I - Batch: 50 | Loss: 0.515 | Acc: 96.500% | Wgt Acc: 97.344%
	I - Batch: 100 | Loss: 0.516 | Acc: 96.250% | Wgt Acc: 97.265%
	I - Batch: 150 | Loss: 0.517 | Acc: 96.375% | Wgt Acc: 97.360%
I - num batch: 173
I - Train -- Loss: 0.515 | Acc: 96.491% | Wgt Acc: 97.427% | LR: 1.250000e-04 | Dur: 99.35s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   3.  10.  19.]
 [  0. 576.   2.   3.  11.]
 [  1.   1. 725.   2.  17.]
 [  4.   0.   3. 518.   7.]
 [  7.   1.   1.   5. 163.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.159 | Acc: 59.900% | Wgt Acc: 59.530% | Dur: 13.10s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  3. 13.  8.]
 [ 3. 39. 11.  5.  7.]
 [ 2. 16. 32.  4.  7.]
 [11.  8. 11. 55.  6.]
 [ 3. 10. 18.  9. 44.]]

I - Epoch: 194
I - Training: 
	I - Batch: 50 | Loss: 0.555 | Acc: 93.875% | Wgt Acc: 96.035%
	I - Batch: 100 | Loss: 0.558 | Acc: 93.875% | Wgt Acc: 96.053%
	I - Batch: 150 | Loss: 0.562 | Acc: 93.542% | Wgt Acc: 95.765%
	I - Batch: 200 | Loss: 0.560 | Acc: 93.938% | Wgt Acc: 95.948%
I - num batch: 222
I - Train -- Loss: 0.561 | Acc: 93.939% | Wgt Acc: 95.971% | LR: 1.250000e-04 | Dur: 122.70s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   1.   2.   4.  62.]
 [  0. 575.   2.   3.  33.]
 [  0.   0. 720.   0.  36.]
 [  3.   0.   1. 525.  39.]
 [ 12.   2.   9.   6. 830.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.218 | Acc: 56.892% | Wgt Acc: 53.218% | Dur: 12.71s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  1.  2. 13.  2.]
 [ 5. 38. 12.  3.  1.]
 [ 0. 11. 29.  3.  2.]
 [11.  6.  7. 42.  3.]
 [18. 22. 25. 25. 64.]]

I - Epoch: 195
I - Training: 
	I - Batch: 50 | Loss: 0.569 | Acc: 94.000% | Wgt Acc: 95.507%
	I - Batch: 100 | Loss: 0.564 | Acc: 93.875% | Wgt Acc: 95.726%
	I - Batch: 150 | Loss: 0.564 | Acc: 93.708% | Wgt Acc: 95.624%
	I - Batch: 200 | Loss: 0.568 | Acc: 93.188% | Wgt Acc: 95.258%
I - num batch: 222
I - Train -- Loss: 0.568 | Acc: 93.346% | Wgt Acc: 95.362% | LR: 1.250000e-04 | Dur: 124.42s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   0.   3.   3.  55.]
 [  4. 572.   2.   3.  35.]
 [  0.   0. 716.   0.  46.]
 [  4.   1.   4. 525.  37.]
 [ 18.   5.   9.   7. 827.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.225 | Acc: 57.143% | Wgt Acc: 54.827% | Dur: 12.90s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  4.  3. 12.  3.]
 [ 1. 33.  4.  2.  3.]
 [ 1. 13. 19.  0.  3.]
 [12.  7. 17. 55.  6.]
 [10. 21. 32. 17. 57.]]

I - Epoch: 196
I - Training: 
	I - Batch: 50 | Loss: 0.546 | Acc: 94.875% | Wgt Acc: 96.476%
	I - Batch: 100 | Loss: 0.563 | Acc: 93.625% | Wgt Acc: 95.624%
	I - Batch: 150 | Loss: 0.561 | Acc: 94.500% | Wgt Acc: 96.258%
	I - Batch: 200 | Loss: 0.562 | Acc: 94.219% | Wgt Acc: 96.093%
I - num batch: 222
I - Train -- Loss: 0.562 | Acc: 94.192% | Wgt Acc: 96.061% | LR: 1.250000e-04 | Dur: 124.91s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   0.   3.   4.  59.]
 [  0. 576.   0.   2.  20.]
 [  2.   1. 724.   2.  39.]
 [  0.   0.   1. 524.  40.]
 [ 20.   1.   6.   6. 842.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.213 | Acc: 55.388% | Wgt Acc: 52.847% | Dur: 13.30s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  3.  3. 16.  6.]
 [ 5. 36. 10.  1.  4.]
 [ 0.  7. 20.  0.  3.]
 [ 9.  7. 11. 48.  2.]
 [14. 25. 31. 21. 57.]]

I - Epoch: 197
I - Training: 
	I - Batch: 50 | Loss: 0.568 | Acc: 93.625% | Wgt Acc: 95.053%
	I - Batch: 100 | Loss: 0.568 | Acc: 93.188% | Wgt Acc: 94.804%
	I - Batch: 150 | Loss: 0.563 | Acc: 93.667% | Wgt Acc: 95.288%
	I - Batch: 200 | Loss: 0.564 | Acc: 93.688% | Wgt Acc: 95.392%
I - num batch: 222
I - Train -- Loss: 0.563 | Acc: 93.713% | Wgt Acc: 95.407% | LR: 1.250000e-04 | Dur: 126.45s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   3.   4.  57.]
 [  3. 575.   0.   3.  31.]
 [  1.   0. 714.   2.  39.]
 [  2.   0.   0. 516.  26.]
 [ 19.   3.  17.  13. 847.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.269 | Acc: 55.388% | Wgt Acc: 51.547% | Dur: 13.22s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  3.  1. 15.  2.]
 [ 4. 32.  7.  3.  3.]
 [ 0. 12. 29.  1.  4.]
 [ 6.  3.  6. 41.  1.]
 [21. 28. 32. 26. 62.]]

I - Epoch: 198
I - Training: 
	I - Batch: 50 | Loss: 0.570 | Acc: 93.250% | Wgt Acc: 95.450%
	I - Batch: 100 | Loss: 0.564 | Acc: 93.438% | Wgt Acc: 95.384%
	I - Batch: 150 | Loss: 0.565 | Acc: 93.417% | Wgt Acc: 95.380%
	I - Batch: 200 | Loss: 0.565 | Acc: 93.438% | Wgt Acc: 95.426%
I - num batch: 222
I - Train -- Loss: 0.566 | Acc: 93.318% | Wgt Acc: 95.370% | LR: 1.250000e-04 | Dur: 129.11s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   1.   4.  58.]
 [  0. 571.   2.   2.  37.]
 [  2.   0. 719.   0.  50.]
 [  3.   0.   4. 521.  33.]
 [ 15.   7.   8.  11. 822.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.191 | Acc: 58.897% | Wgt Acc: 56.869% | Dur: 12.75s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  2.  5. 13.  4.]
 [ 4. 42. 13.  5.  9.]
 [ 0. 16. 31.  5.  4.]
 [ 8.  3.  4. 45.  1.]
 [13. 15. 22. 18. 54.]]

I - Epoch: 199
I - Training: 
	I - Batch: 50 | Loss: 0.556 | Acc: 93.750% | Wgt Acc: 95.474%
	I - Batch: 100 | Loss: 0.562 | Acc: 93.438% | Wgt Acc: 95.259%
	I - Batch: 150 | Loss: 0.565 | Acc: 93.208% | Wgt Acc: 95.125%
	I - Batch: 200 | Loss: 0.563 | Acc: 93.375% | Wgt Acc: 95.323%
I - num batch: 222
I - Train -- Loss: 0.564 | Acc: 93.346% | Wgt Acc: 95.355% | LR: 1.250000e-04 | Dur: 124.57s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   3.   5.  64.]
 [  0. 576.   3.   2.  27.]
 [  1.   0. 716.   1.  44.]
 [  1.   0.   0. 520.  38.]
 [ 23.   2.  12.  10. 827.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.269 | Acc: 53.885% | Wgt Acc: 50.433% | Dur: 12.97s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  8.  9. 24.  9.]
 [ 3. 28.  1.  3.  2.]
 [ 0. 11. 17.  0.  1.]
 [ 5.  8. 12. 41.  3.]
 [ 8. 23. 36. 18. 57.]]

I - Epoch: 200
I - Training: 
	I - Batch: 50 | Loss: 0.567 | Acc: 92.750% | Wgt Acc: 95.331%
	I - Batch: 100 | Loss: 0.570 | Acc: 92.750% | Wgt Acc: 95.074%
	I - Batch: 150 | Loss: 0.565 | Acc: 93.125% | Wgt Acc: 95.359%
	I - Batch: 200 | Loss: 0.565 | Acc: 93.219% | Wgt Acc: 95.312%
I - num batch: 222
I - Train -- Loss: 0.564 | Acc: 93.177% | Wgt Acc: 95.310% | LR: 1.250000e-04 | Dur: 124.23s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   4.   6.  60.]
 [  1. 574.   2.   4.  49.]
 [  1.   1. 717.   0.  38.]
 [  3.   0.   0. 518.  37.]
 [ 12.   3.  11.  10. 816.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.193 | Acc: 57.895% | Wgt Acc: 55.322% | Dur: 13.05s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  5.  2. 14.  1.]
 [ 1. 37.  6.  1.  4.]
 [ 1. 14. 30.  2.  7.]
 [13.  6. 11. 47.  3.]
 [13. 16. 26. 22. 57.]]

I - Epoch: 201
I - Training: 
	I - Batch: 50 | Loss: 0.507 | Acc: 97.000% | Wgt Acc: 97.874%
	I - Batch: 100 | Loss: 0.509 | Acc: 97.125% | Wgt Acc: 98.028%
	I - Batch: 150 | Loss: 0.513 | Acc: 97.000% | Wgt Acc: 97.946%
I - num batch: 173
I - Train -- Loss: 0.514 | Acc: 96.961% | Wgt Acc: 97.904% | LR: 1.250000e-04 | Dur: 97.28s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   2.   6.  20.]
 [  3. 576.   3.   2.   7.]
 [  2.   1. 725.   2.  17.]
 [  0.   0.   1. 526.   8.]
 [  4.   1.   3.   2. 165.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.222 | Acc: 56.892% | Wgt Acc: 54.517% | Dur: 13.15s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  7.  4. 20.  8.]
 [ 3. 39. 10.  1.  4.]
 [ 1. 11. 26.  1.  3.]
 [ 8.  6. 10. 42.  3.]
 [10. 15. 25. 22. 54.]]

I - Epoch: 202
I - Training: 
	I - Batch: 50 | Loss: 0.550 | Acc: 94.750% | Wgt Acc: 96.606%
	I - Batch: 100 | Loss: 0.551 | Acc: 94.625% | Wgt Acc: 96.460%
	I - Batch: 150 | Loss: 0.555 | Acc: 94.083% | Wgt Acc: 96.000%
	I - Batch: 200 | Loss: 0.555 | Acc: 94.094% | Wgt Acc: 95.986%
I - num batch: 222
I - Train -- Loss: 0.554 | Acc: 94.249% | Wgt Acc: 96.129% | LR: 1.250000e-04 | Dur: 127.83s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   2.   4.  51.]
 [  0. 577.   0.   4.  35.]
 [  1.   0. 721.   1.  36.]
 [  2.   0.   1. 524.  36.]
 [ 15.   1.  10.   5. 842.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.251 | Acc: 52.632% | Wgt Acc: 49.876% | Dur: 12.68s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  4. 19. 10.]
 [ 2. 27.  4.  1.  2.]
 [ 0. 13. 17.  1.  3.]
 [10.  9. 16. 47.  3.]
 [11. 23. 34. 18. 54.]]

I - Epoch: 203
I - Training: 
	I - Batch: 50 | Loss: 0.558 | Acc: 93.625% | Wgt Acc: 95.539%
	I - Batch: 100 | Loss: 0.561 | Acc: 93.688% | Wgt Acc: 95.613%
	I - Batch: 150 | Loss: 0.563 | Acc: 93.792% | Wgt Acc: 95.539%
	I - Batch: 200 | Loss: 0.559 | Acc: 93.969% | Wgt Acc: 95.745%
I - num batch: 222
I - Train -- Loss: 0.559 | Acc: 93.939% | Wgt Acc: 95.716% | LR: 1.250000e-04 | Dur: 125.26s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   4.   4.  45.]
 [  0. 572.   0.   2.  28.]
 [  0.   1. 717.   2.  40.]
 [  2.   0.   2. 524.  42.]
 [ 21.   5.  11.   6. 845.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.252 | Acc: 52.130% | Wgt Acc: 48.515% | Dur: 12.44s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  7.  2. 18.  7.]
 [ 2. 24.  6.  1.  3.]
 [ 1. 13. 21.  1.  1.]
 [13.  7. 12. 46.  2.]
 [14. 27. 34. 20. 59.]]

I - Epoch: 204
I - Training: 
	I - Batch: 50 | Loss: 0.560 | Acc: 93.750% | Wgt Acc: 95.642%
	I - Batch: 100 | Loss: 0.559 | Acc: 94.188% | Wgt Acc: 95.903%
	I - Batch: 150 | Loss: 0.558 | Acc: 94.333% | Wgt Acc: 96.053%
	I - Batch: 200 | Loss: 0.557 | Acc: 94.344% | Wgt Acc: 96.012%
I - num batch: 222
I - Train -- Loss: 0.559 | Acc: 94.136% | Wgt Acc: 95.873% | LR: 1.250000e-04 | Dur: 126.81s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   0.   3.   4.  47.]
 [  2. 575.   2.   2.  20.]
 [  1.   1. 718.   3.  35.]
 [  5.   0.   1. 522.  49.]
 [ 14.   2.  10.   7. 849.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.231 | Acc: 57.644% | Wgt Acc: 53.713% | Dur: 12.66s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  5.  3. 12.  3.]
 [ 1. 30.  7.  2.  0.]
 [ 0.  9. 24.  0.  1.]
 [11.  7.  9. 50.  2.]
 [16. 27. 32. 22. 66.]]

I - Epoch: 205
I - Training: 
	I - Batch: 50 | Loss: 0.571 | Acc: 93.375% | Wgt Acc: 95.251%
	I - Batch: 100 | Loss: 0.573 | Acc: 92.875% | Wgt Acc: 94.888%
	I - Batch: 150 | Loss: 0.570 | Acc: 93.125% | Wgt Acc: 95.104%
	I - Batch: 200 | Loss: 0.567 | Acc: 93.406% | Wgt Acc: 95.327%
I - num batch: 222
I - Train -- Loss: 0.565 | Acc: 93.544% | Wgt Acc: 95.437% | LR: 1.250000e-04 | Dur: 123.05s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   1.   1.   4.  52.]
 [  2. 571.   1.   3.  37.]
 [  0.   1. 714.   0.  41.]
 [  2.   1.   5. 522.  36.]
 [ 16.   4.  13.   9. 834.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.232 | Acc: 54.637% | Wgt Acc: 50.866% | Dur: 12.88s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  3.  2. 15.  5.]
 [ 2. 23.  7.  3.  3.]
 [ 0. 12. 21.  2.  2.]
 [ 9.  6. 13. 47.  2.]
 [10. 34. 32. 19. 60.]]

I - Epoch: 206
I - Training: 
	I - Batch: 50 | Loss: 0.551 | Acc: 94.750% | Wgt Acc: 96.453%
	I - Batch: 100 | Loss: 0.556 | Acc: 94.125% | Wgt Acc: 95.906%
	I - Batch: 150 | Loss: 0.556 | Acc: 94.167% | Wgt Acc: 95.981%
	I - Batch: 200 | Loss: 0.558 | Acc: 93.938% | Wgt Acc: 95.809%
I - num batch: 222
I - Train -- Loss: 0.557 | Acc: 94.164% | Wgt Acc: 95.911% | LR: 1.250000e-04 | Dur: 126.30s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   4.   4.  50.]
 [  1. 574.   3.   4.  23.]
 [  0.   0. 717.   1.  35.]
 [  0.   0.   0. 522.  44.]
 [ 17.   4.  10.   7. 848.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.213 | Acc: 56.892% | Wgt Acc: 53.465% | Dur: 13.52s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  3.  5. 20.  4.]
 [ 4. 39.  8.  3.  6.]
 [ 0. 10. 27.  2.  2.]
 [ 5.  2.  7. 37.  0.]
 [15. 24. 28. 24. 60.]]

I - Epoch: 207
I - Training: 
	I - Batch: 50 | Loss: 0.559 | Acc: 93.375% | Wgt Acc: 95.593%
	I - Batch: 100 | Loss: 0.567 | Acc: 93.125% | Wgt Acc: 95.368%
	I - Batch: 150 | Loss: 0.560 | Acc: 93.875% | Wgt Acc: 95.755%
	I - Batch: 200 | Loss: 0.561 | Acc: 93.719% | Wgt Acc: 95.623%
I - num batch: 222
I - Train -- Loss: 0.565 | Acc: 93.516% | Wgt Acc: 95.505% | LR: 1.250000e-04 | Dur: 123.19s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   4.   5.  56.]
 [  2. 573.   2.   2.  30.]
 [  0.   1. 715.   1.  52.]
 [  2.   0.   0. 521.  34.]
 [ 13.   4.  13.   9. 828.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.206 | Acc: 57.393% | Wgt Acc: 54.827% | Dur: 12.97s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  3.  4. 19.  4.]
 [ 2. 35. 10.  1.  6.]
 [ 1.  9. 22.  0.  1.]
 [12.  8. 13. 51.  3.]
 [10. 23. 26. 15. 58.]]

I - Epoch: 208
I - Training: 
	I - Batch: 50 | Loss: 0.564 | Acc: 93.750% | Wgt Acc: 95.812%
	I - Batch: 100 | Loss: 0.565 | Acc: 93.562% | Wgt Acc: 95.570%
	I - Batch: 150 | Loss: 0.564 | Acc: 93.458% | Wgt Acc: 95.665%
	I - Batch: 200 | Loss: 0.566 | Acc: 93.312% | Wgt Acc: 95.377%
I - num batch: 222
I - Train -- Loss: 0.565 | Acc: 93.487% | Wgt Acc: 95.483% | LR: 1.250000e-04 | Dur: 126.92s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   0.   3.   3.  48.]
 [  3. 575.   2.   2.  31.]
 [  0.   1. 716.   2.  46.]
 [  4.   0.   3. 524.  45.]
 [ 19.   2.  10.   7. 830.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.208 | Acc: 56.892% | Wgt Acc: 55.136% | Dur: 13.23s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  2.  3. 13.  2.]
 [ 4. 39.  7.  3.  3.]
 [ 2. 12. 30.  2.  8.]
 [13.  6. 12. 48.  7.]
 [11. 19. 23. 20. 52.]]

I - Epoch: 209
I - Training: 
	I - Batch: 50 | Loss: 0.507 | Acc: 97.750% | Wgt Acc: 98.265%
	I - Batch: 100 | Loss: 0.510 | Acc: 97.062% | Wgt Acc: 97.972%
	I - Batch: 150 | Loss: 0.510 | Acc: 97.000% | Wgt Acc: 97.868%
I - num batch: 173
I - Train -- Loss: 0.511 | Acc: 96.816% | Wgt Acc: 97.776% | LR: 1.250000e-04 | Dur: 99.33s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   5.   3.  13.]
 [  0. 576.   1.   4.  12.]
 [  0.   1. 725.   1.  16.]
 [  2.   0.   2. 527.  11.]
 [ 12.   1.   1.   3. 165.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.222 | Acc: 58.396% | Wgt Acc: 56.250% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  1.  3. 14.  2.]
 [ 2. 37.  7.  1.  3.]
 [ 2. 13. 31.  2.  6.]
 [13.  6. 12. 50.  6.]
 [11. 21. 22. 19. 55.]]

I - Epoch: 210
I - Training: 
	I - Batch: 50 | Loss: 0.538 | Acc: 95.375% | Wgt Acc: 96.950%
	I - Batch: 100 | Loss: 0.551 | Acc: 94.688% | Wgt Acc: 96.479%
	I - Batch: 150 | Loss: 0.554 | Acc: 94.333% | Wgt Acc: 96.116%
	I - Batch: 200 | Loss: 0.555 | Acc: 94.281% | Wgt Acc: 96.134%
I - num batch: 222
I - Train -- Loss: 0.556 | Acc: 94.277% | Wgt Acc: 96.039% | LR: 1.250000e-04 | Dur: 127.33s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   2.   4.  57.]
 [  0. 576.   1.   2.  31.]
 [  0.   1. 721.   4.  36.]
 [  0.   0.   2. 517.  30.]
 [ 13.   1.   8.  11. 846.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.228 | Acc: 54.135% | Wgt Acc: 51.856% | Dur: 12.58s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  7.  6. 20. 11.]
 [ 1. 30.  8.  3.  2.]
 [ 0.  8. 25.  2.  3.]
 [14.  9. 12. 48.  4.]
 [12. 24. 24. 13. 52.]]

I - Epoch: 211
I - Training: 
	I - Batch: 50 | Loss: 0.563 | Acc: 93.250% | Wgt Acc: 95.251%
	I - Batch: 100 | Loss: 0.568 | Acc: 92.312% | Wgt Acc: 94.589%
	I - Batch: 150 | Loss: 0.567 | Acc: 92.542% | Wgt Acc: 94.768%
	I - Batch: 200 | Loss: 0.562 | Acc: 93.219% | Wgt Acc: 95.272%
I - num batch: 222
I - Train -- Loss: 0.562 | Acc: 93.206% | Wgt Acc: 95.235% | LR: 1.250000e-04 | Dur: 124.56s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   2.   5.  54.]
 [  0. 571.   0.   3.  35.]
 [  1.   0. 724.   1.  48.]
 [  3.   0.   1. 517.  42.]
 [ 20.   7.   7.  12. 821.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.265 | Acc: 52.882% | Wgt Acc: 48.762% | Dur: 12.98s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  4.  4. 17.  7.]
 [ 1. 24.  5.  2.  2.]
 [ 0. 14. 26.  3.  3.]
 [ 8.  2. 10. 40.  0.]
 [18. 34. 30. 24. 60.]]

I - Epoch: 212
I - Training: 
	I - Batch: 50 | Loss: 0.555 | Acc: 94.000% | Wgt Acc: 95.748%
	I - Batch: 100 | Loss: 0.555 | Acc: 94.500% | Wgt Acc: 95.970%
	I - Batch: 150 | Loss: 0.557 | Acc: 94.292% | Wgt Acc: 95.788%
	I - Batch: 200 | Loss: 0.556 | Acc: 94.281% | Wgt Acc: 95.825%
I - num batch: 222
I - Train -- Loss: 0.556 | Acc: 94.333% | Wgt Acc: 95.896% | LR: 1.250000e-04 | Dur: 125.26s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   4.   4.  43.]
 [  2. 572.   3.   3.  29.]
 [  0.   0. 718.   0.  33.]
 [  1.   1.   0. 518.  37.]
 [ 14.   5.   9.  13. 858.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.219 | Acc: 53.885% | Wgt Acc: 50.186% | Dur: 12.81s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  4.  3. 26.  4.]
 [ 4. 36. 10.  7.  5.]
 [ 0.  7. 23.  2.  2.]
 [ 7.  4.  8. 35.  1.]
 [16. 27. 31. 16. 60.]]

I - Epoch: 213
I - Training: 
	I - Batch: 50 | Loss: 0.551 | Acc: 94.000% | Wgt Acc: 96.215%
	I - Batch: 100 | Loss: 0.550 | Acc: 94.375% | Wgt Acc: 96.421%
	I - Batch: 150 | Loss: 0.553 | Acc: 94.292% | Wgt Acc: 96.218%
	I - Batch: 200 | Loss: 0.558 | Acc: 94.000% | Wgt Acc: 95.888%
I - num batch: 222
I - Train -- Loss: 0.560 | Acc: 93.854% | Wgt Acc: 95.723% | LR: 1.250000e-04 | Dur: 125.36s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   4.   5.  49.]
 [  0. 575.   0.   6.  35.]
 [  1.   0. 719.   0.  39.]
 [  3.   0.   2. 520.  39.]
 [ 16.   3.   9.   7. 838.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.228 | Acc: 57.393% | Wgt Acc: 54.208% | Dur: 12.51s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  4.  2. 13.  2.]
 [ 5. 43. 10.  2.  3.]
 [ 0.  8. 22.  2.  5.]
 [10.  2.  8. 41.  0.]
 [12. 21. 33. 28. 62.]]

I - Epoch: 214
I - Training: 
	I - Batch: 50 | Loss: 0.562 | Acc: 93.625% | Wgt Acc: 95.575%
	I - Batch: 100 | Loss: 0.565 | Acc: 94.000% | Wgt Acc: 95.760%
	I - Batch: 150 | Loss: 0.555 | Acc: 94.458% | Wgt Acc: 96.217%
	I - Batch: 200 | Loss: 0.554 | Acc: 94.469% | Wgt Acc: 96.322%
I - num batch: 222
I - Train -- Loss: 0.556 | Acc: 94.390% | Wgt Acc: 96.227% | LR: 1.250000e-04 | Dur: 126.17s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   3.   2.  58.]
 [  4. 577.   3.   3.  24.]
 [  0.   0. 717.   2.  35.]
 [  0.   0.   2. 523.  38.]
 [  7.   1.   9.   8. 845.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.239 | Acc: 55.639% | Wgt Acc: 51.300% | Dur: 12.10s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  4. 14.  3.]
 [ 4. 29.  5.  2.  0.]
 [ 0. 17. 30.  4.  4.]
 [ 8.  2.  3. 40.  1.]
 [17. 26. 33. 26. 64.]]

I - Epoch: 215
I - Training: 
	I - Batch: 50 | Loss: 0.555 | Acc: 94.625% | Wgt Acc: 95.944%
	I - Batch: 100 | Loss: 0.558 | Acc: 94.188% | Wgt Acc: 95.796%
	I - Batch: 150 | Loss: 0.560 | Acc: 94.042% | Wgt Acc: 95.632%
	I - Batch: 200 | Loss: 0.558 | Acc: 94.156% | Wgt Acc: 95.828%
I - num batch: 222
I - Train -- Loss: 0.558 | Acc: 94.164% | Wgt Acc: 95.851% | LR: 1.250000e-04 | Dur: 125.06s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   3.   8.  58.]
 [  1. 576.   2.   4.  22.]
 [  0.   0. 718.   1.  34.]
 [  3.   0.   2. 518.  35.]
 [ 16.   2.   9.   7. 851.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.222 | Acc: 56.892% | Wgt Acc: 53.837% | Dur: 13.17s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  5.  6. 23.  7.]
 [ 2. 38.  8.  5.  3.]
 [ 0.  8. 20.  0.  2.]
 [ 4.  6. 10. 42.  1.]
 [14. 21. 31. 16. 59.]]

I - Epoch: 216
I - Training: 
	I - Batch: 50 | Loss: 0.574 | Acc: 93.250% | Wgt Acc: 95.147%
	I - Batch: 100 | Loss: 0.563 | Acc: 93.625% | Wgt Acc: 95.554%
	I - Batch: 150 | Loss: 0.559 | Acc: 93.875% | Wgt Acc: 95.770%
	I - Batch: 200 | Loss: 0.558 | Acc: 93.875% | Wgt Acc: 95.749%
I - num batch: 222
I - Train -- Loss: 0.559 | Acc: 93.769% | Wgt Acc: 95.670% | LR: 1.250000e-04 | Dur: 125.68s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   0.   5.   3.  46.]
 [  2. 573.   2.   3.  34.]
 [  1.   1. 722.   0.  45.]
 [  1.   0.   0. 521.  40.]
 [ 18.   4.   5.  11. 835.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.252 | Acc: 55.639% | Wgt Acc: 52.104% | Dur: 13.11s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  3. 17.  4.]
 [ 5. 36.  8.  4.  2.]
 [ 0. 10. 22.  0.  3.]
 [ 6.  5.  9. 40.  2.]
 [14. 23. 33. 25. 61.]]

I - Epoch: 217
I - Training: 
	I - Batch: 50 | Loss: 0.517 | Acc: 96.875% | Wgt Acc: 97.894%
	I - Batch: 100 | Loss: 0.510 | Acc: 97.125% | Wgt Acc: 97.939%
	I - Batch: 150 | Loss: 0.508 | Acc: 97.250% | Wgt Acc: 98.097%
I - num batch: 173
I - Train -- Loss: 0.508 | Acc: 97.142% | Wgt Acc: 97.998% | LR: 1.250000e-04 | Dur: 97.54s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   4.   4.  15.]
 [  0. 576.   2.   4.  11.]
 [  0.   2. 722.   0.  15.]
 [  1.   0.   1. 527.   6.]
 [  6.   0.   5.   3. 170.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.211 | Acc: 56.892% | Wgt Acc: 54.394% | Dur: 13.61s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  3.  4. 15.  7.]
 [ 1. 31.  4.  2.  3.]
 [ 2. 18. 30.  1.  6.]
 [10.  6. 14. 48.  2.]
 [11. 20. 23. 20. 54.]]

I - Epoch: 218
I - Training: 
	I - Batch: 50 | Loss: 0.547 | Acc: 94.500% | Wgt Acc: 96.737%
	I - Batch: 100 | Loss: 0.547 | Acc: 94.312% | Wgt Acc: 96.331%
	I - Batch: 150 | Loss: 0.553 | Acc: 94.208% | Wgt Acc: 96.111%
	I - Batch: 200 | Loss: 0.557 | Acc: 93.844% | Wgt Acc: 95.823%
I - num batch: 222
I - Train -- Loss: 0.559 | Acc: 93.657% | Wgt Acc: 95.655% | LR: 1.250000e-04 | Dur: 125.88s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   3.  11.  66.]
 [  1. 575.   4.   3.  26.]
 [  1.   0. 717.   1.  43.]
 [  2.   0.   1. 521.  36.]
 [ 13.   3.   9.   2. 829.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.234 | Acc: 53.885% | Wgt Acc: 51.918% | Dur: 12.63s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  7.  5. 16.  8.]
 [ 4. 38.  8.  3.  6.]
 [ 0. 12. 23.  0.  5.]
 [ 6.  4. 12. 41.  3.]
 [15. 17. 27. 26. 50.]]

I - Epoch: 219
I - Training: 
	I - Batch: 50 | Loss: 0.542 | Acc: 95.375% | Wgt Acc: 96.875%
	I - Batch: 100 | Loss: 0.548 | Acc: 94.750% | Wgt Acc: 96.336%
	I - Batch: 150 | Loss: 0.552 | Acc: 94.500% | Wgt Acc: 96.098%
	I - Batch: 200 | Loss: 0.553 | Acc: 94.250% | Wgt Acc: 95.944%
I - num batch: 222
I - Train -- Loss: 0.555 | Acc: 94.136% | Wgt Acc: 95.836% | LR: 1.250000e-04 | Dur: 124.79s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   0.  10.  50.]
 [  2. 578.   2.   3.  34.]
 [  2.   0. 718.   0.  24.]
 [  2.   0.   1. 518.  41.]
 [ 17.   0.  13.   7. 851.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.203 | Acc: 54.386% | Wgt Acc: 51.733% | Dur: 12.63s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  3. 22.  4.]
 [ 4. 29.  2.  4.  1.]
 [ 0. 18. 27.  4.  6.]
 [16.  6. 16. 47.  7.]
 [ 8. 21. 27.  9. 54.]]

I - Epoch: 220
I - Training: 
	I - Batch: 50 | Loss: 0.549 | Acc: 95.375% | Wgt Acc: 96.858%
	I - Batch: 100 | Loss: 0.550 | Acc: 95.250% | Wgt Acc: 96.710%
	I - Batch: 150 | Loss: 0.555 | Acc: 94.750% | Wgt Acc: 96.350%
	I - Batch: 200 | Loss: 0.554 | Acc: 94.844% | Wgt Acc: 96.448%
I - num batch: 222
I - Train -- Loss: 0.554 | Acc: 94.813% | Wgt Acc: 96.422% | LR: 1.250000e-04 | Dur: 123.74s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   3.   5.  44.]
 [  1. 575.   2.   3.  21.]
 [  0.   0. 723.   0.  37.]
 [  1.   1.   2. 523.  37.]
 [ 14.   2.   4.   7. 861.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.225 | Acc: 54.135% | Wgt Acc: 50.990% | Dur: 12.57s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  5.  6. 18.  6.]
 [ 3. 36.  9.  2.  5.]
 [ 0.  9. 15.  1.  1.]
 [ 8.  4. 11. 42.  1.]
 [13. 24. 34. 23. 59.]]

I - Epoch: 221
I - Training: 
	I - Batch: 50 | Loss: 0.549 | Acc: 94.500% | Wgt Acc: 96.260%
	I - Batch: 100 | Loss: 0.550 | Acc: 94.500% | Wgt Acc: 96.159%
	I - Batch: 150 | Loss: 0.551 | Acc: 94.583% | Wgt Acc: 96.281%
	I - Batch: 200 | Loss: 0.550 | Acc: 94.750% | Wgt Acc: 96.395%
I - num batch: 222
I - Train -- Loss: 0.551 | Acc: 94.700% | Wgt Acc: 96.332% | LR: 1.250000e-04 | Dur: 124.23s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   2.   5.  56.]
 [  1. 575.   1.   2.  24.]
 [  0.   0. 722.   1.  33.]
 [  3.   0.   2. 523.  28.]
 [ 13.   3.   7.   7. 859.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.241 | Acc: 54.386% | Wgt Acc: 50.619% | Dur: 12.88s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  5. 12.  3.]
 [ 2. 27.  6.  2.  2.]
 [ 0. 11. 20.  0.  1.]
 [12.  6. 15. 49.  3.]
 [16. 30. 29. 23. 63.]]

I - Epoch: 222
I - Training: 
	I - Batch: 50 | Loss: 0.543 | Acc: 94.750% | Wgt Acc: 96.725%
	I - Batch: 100 | Loss: 0.547 | Acc: 94.375% | Wgt Acc: 96.362%
	I - Batch: 150 | Loss: 0.547 | Acc: 94.333% | Wgt Acc: 96.236%
	I - Batch: 200 | Loss: 0.547 | Acc: 94.594% | Wgt Acc: 96.395%
I - num batch: 222
I - Train -- Loss: 0.548 | Acc: 94.559% | Wgt Acc: 96.324% | LR: 1.250000e-04 | Dur: 127.97s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   4.   1.  56.]
 [  0. 577.   1.   4.  22.]
 [  0.   0. 716.   1.  36.]
 [  3.   0.   0. 526.  34.]
 [ 11.   1.  13.   6. 852.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.295 | Acc: 49.875% | Wgt Acc: 45.359% | Dur: 12.82s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  5.  5. 14.  2.]
 [ 1. 25.  5.  3.  1.]
 [ 0.  9. 17.  2.  2.]
 [11.  6.  9. 40.  3.]
 [23. 33. 39. 27. 64.]]

I - Epoch: 223
I - Training: 
	I - Batch: 50 | Loss: 0.567 | Acc: 92.375% | Wgt Acc: 94.508%
	I - Batch: 100 | Loss: 0.567 | Acc: 93.000% | Wgt Acc: 94.982%
	I - Batch: 150 | Loss: 0.561 | Acc: 93.667% | Wgt Acc: 95.467%
	I - Batch: 200 | Loss: 0.562 | Acc: 93.500% | Wgt Acc: 95.409%
I - num batch: 222
I - Train -- Loss: 0.559 | Acc: 93.741% | Wgt Acc: 95.640% | LR: 1.250000e-04 | Dur: 124.40s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   0.   1.   5.  54.]
 [  0. 576.   2.   2.  22.]
 [  2.   0. 712.   1.  47.]
 [  4.   0.   4. 524.  39.]
 [ 16.   2.  15.   6. 838.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.216 | Acc: 55.639% | Wgt Acc: 52.847% | Dur: 12.46s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  2.  3. 17.  5.]
 [ 2. 36.  8.  1.  2.]
 [ 0. 14. 23.  1.  2.]
 [12.  8. 10. 46.  5.]
 [15. 18. 31. 21. 58.]]

I - Epoch: 224
I - Training: 
	I - Batch: 50 | Loss: 0.580 | Acc: 91.500% | Wgt Acc: 93.926%
	I - Batch: 100 | Loss: 0.567 | Acc: 93.000% | Wgt Acc: 95.162%
	I - Batch: 150 | Loss: 0.565 | Acc: 93.292% | Wgt Acc: 95.343%
	I - Batch: 200 | Loss: 0.559 | Acc: 93.656% | Wgt Acc: 95.577%
I - num batch: 222
I - Train -- Loss: 0.559 | Acc: 93.685% | Wgt Acc: 95.588% | LR: 1.250000e-04 | Dur: 124.59s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   3.   7.  54.]
 [  1. 575.   0.   3.  25.]
 [  1.   0. 717.   0.  44.]
 [  1.   0.   1. 518.  43.]
 [ 15.   3.  13.  10. 834.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.182 | Acc: 58.396% | Wgt Acc: 55.569% | Dur: 12.96s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  2.  4.  9.  4.]
 [ 4. 36. 10.  8.  3.]
 [ 1. 13. 29.  4.  3.]
 [ 9.  8.  5. 48.  3.]
 [13. 19. 27. 17. 59.]]

I - Epoch: 225
I - Training: 
	I - Batch: 50 | Loss: 0.511 | Acc: 97.250% | Wgt Acc: 97.999%
	I - Batch: 100 | Loss: 0.504 | Acc: 97.438% | Wgt Acc: 98.167%
	I - Batch: 150 | Loss: 0.505 | Acc: 97.375% | Wgt Acc: 98.081%
I - num batch: 173
I - Train -- Loss: 0.507 | Acc: 97.178% | Wgt Acc: 98.006% | LR: 1.250000e-04 | Dur: 100.04s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   3.   3.  13.]
 [  2. 577.   2.   5.   7.]
 [  1.   1. 725.   1.  21.]
 [  1.   0.   1. 525.   5.]
 [  5.   0.   3.   4. 171.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.200 | Acc: 57.644% | Wgt Acc: 55.569% | Dur: 13.08s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  6.  4. 17.  7.]
 [ 2. 33.  6.  1.  4.]
 [ 1. 16. 32.  4.  6.]
 [11.  6. 10. 49.  3.]
 [10. 17. 23. 15. 52.]]

I - Epoch: 226
I - Training: 
	I - Batch: 50 | Loss: 0.553 | Acc: 92.750% | Wgt Acc: 95.392%
	I - Batch: 100 | Loss: 0.549 | Acc: 94.062% | Wgt Acc: 96.184%
	I - Batch: 150 | Loss: 0.548 | Acc: 94.458% | Wgt Acc: 96.302%
	I - Batch: 200 | Loss: 0.549 | Acc: 94.562% | Wgt Acc: 96.364%
I - num batch: 222
I - Train -- Loss: 0.551 | Acc: 94.587% | Wgt Acc: 96.377% | LR: 1.250000e-04 | Dur: 124.78s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   2.   4.  62.]
 [  1. 577.   1.   2.  18.]
 [  0.   0. 719.   1.  30.]
 [  2.   0.   0. 527.  39.]
 [ 13.   1.  12.   4. 851.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.250 | Acc: 55.639% | Wgt Acc: 51.547% | Dur: 12.78s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  4. 13.  0.]
 [ 1. 35.  7.  3.  4.]
 [ 1. 16. 27.  3.  3.]
 [11.  2.  5. 38.  1.]
 [17. 21. 32. 29. 64.]]

I - Epoch: 227
I - Training: 
	I - Batch: 50 | Loss: 0.537 | Acc: 95.750% | Wgt Acc: 97.139%
	I - Batch: 100 | Loss: 0.542 | Acc: 94.625% | Wgt Acc: 96.236%
	I - Batch: 150 | Loss: 0.549 | Acc: 94.208% | Wgt Acc: 96.024%
	I - Batch: 200 | Loss: 0.551 | Acc: 94.125% | Wgt Acc: 95.999%
I - num batch: 222
I - Train -- Loss: 0.551 | Acc: 94.220% | Wgt Acc: 96.031% | LR: 1.250000e-04 | Dur: 125.13s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   2.   1.  47.]
 [  0. 575.   2.   4.  27.]
 [  0.   0. 716.   1.  43.]
 [  1.   0.   0. 525.  37.]
 [ 16.   3.  14.   7. 846.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.241 | Acc: 54.386% | Wgt Acc: 51.547% | Dur: 12.73s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  2.  3. 14.  2.]
 [ 6. 33.  7.  4.  6.]
 [ 2. 13. 26.  2.  5.]
 [10.  4.  9. 46.  2.]
 [15. 26. 30. 20. 57.]]

I - Epoch: 228
I - Training: 
	I - Batch: 50 | Loss: 0.552 | Acc: 92.750% | Wgt Acc: 94.500%
	I - Batch: 100 | Loss: 0.547 | Acc: 93.812% | Wgt Acc: 95.468%
	I - Batch: 150 | Loss: 0.547 | Acc: 94.417% | Wgt Acc: 95.912%
	I - Batch: 200 | Loss: 0.546 | Acc: 94.656% | Wgt Acc: 96.100%
I - num batch: 222
I - Train -- Loss: 0.548 | Acc: 94.531% | Wgt Acc: 96.046% | LR: 1.250000e-04 | Dur: 125.85s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   3.   4.  50.]
 [  1. 574.   2.   2.  22.]
 [  0.   0. 717.   3.  32.]
 [  2.   0.   1. 520.  32.]
 [ 16.   4.  11.   9. 864.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.252 | Acc: 56.140% | Wgt Acc: 52.413% | Dur: 13.19s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  4.  6. 24.  4.]
 [ 2. 33.  6.  1.  2.]
 [ 1. 12. 22.  0.  2.]
 [ 6.  3.  5. 40.  3.]
 [11. 26. 36. 21. 61.]]

I - Epoch: 229
I - Training: 
	I - Batch: 50 | Loss: 0.546 | Acc: 95.375% | Wgt Acc: 96.701%
	I - Batch: 100 | Loss: 0.545 | Acc: 95.500% | Wgt Acc: 96.795%
	I - Batch: 150 | Loss: 0.548 | Acc: 94.917% | Wgt Acc: 96.420%
	I - Batch: 200 | Loss: 0.553 | Acc: 94.281% | Wgt Acc: 95.993%
I - num batch: 222
I - Train -- Loss: 0.553 | Acc: 94.390% | Wgt Acc: 96.106% | LR: 1.250000e-04 | Dur: 126.99s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   4.   5.  55.]
 [  1. 577.   1.   3.  17.]
 [  0.   1. 717.   1.  49.]
 [  3.   0.   1. 523.  26.]
 [ 15.   0.  11.   6. 853.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.217 | Acc: 58.145% | Wgt Acc: 55.198% | Dur: 12.69s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  3.  4. 15.  1.]
 [ 1. 36.  8.  2.  4.]
 [ 0. 10. 23.  0.  5.]
 [ 9.  5.  8. 48.  2.]
 [13. 24. 32. 21. 60.]]

I - Epoch: 230
I - Training: 
	I - Batch: 50 | Loss: 0.547 | Acc: 94.875% | Wgt Acc: 96.438%
	I - Batch: 100 | Loss: 0.547 | Acc: 94.875% | Wgt Acc: 96.374%
	I - Batch: 150 | Loss: 0.551 | Acc: 94.708% | Wgt Acc: 96.199%
	I - Batch: 200 | Loss: 0.553 | Acc: 94.438% | Wgt Acc: 96.111%
I - num batch: 222
I - Train -- Loss: 0.552 | Acc: 94.418% | Wgt Acc: 96.091% | LR: 1.250000e-04 | Dur: 124.86s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   4.   3.  43.]
 [  0. 575.   3.   3.  26.]
 [  2.   1. 715.   0.  46.]
 [  1.   0.   1. 523.  30.]
 [ 13.   2.  11.   9. 855.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.268 | Acc: 54.135% | Wgt Acc: 49.752% | Dur: 12.62s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  6.  3. 15.  2.]
 [ 2. 28.  5.  3.  0.]
 [ 0. 10. 18.  1.  4.]
 [ 8.  2.  7. 44.  0.]
 [18. 32. 42. 23. 66.]]

I - Epoch: 231
I - Training: 
	I - Batch: 50 | Loss: 0.555 | Acc: 94.625% | Wgt Acc: 96.056%
	I - Batch: 100 | Loss: 0.549 | Acc: 95.000% | Wgt Acc: 96.442%
	I - Batch: 150 | Loss: 0.549 | Acc: 94.708% | Wgt Acc: 96.309%
	I - Batch: 200 | Loss: 0.553 | Acc: 94.281% | Wgt Acc: 96.028%
I - num batch: 222
I - Train -- Loss: 0.554 | Acc: 94.136% | Wgt Acc: 95.903% | LR: 1.250000e-04 | Dur: 125.85s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   3.   3.  51.]
 [  0. 574.   0.   2.  24.]
 [  0.   0. 715.   1.  40.]
 [  2.   0.   2. 525.  37.]
 [ 18.   4.  14.   7. 848.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.225 | Acc: 56.391% | Wgt Acc: 54.084% | Dur: 12.62s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  5.  6. 15.  6.]
 [ 2. 35. 10.  2.  8.]
 [ 0. 10. 25.  1.  3.]
 [ 9.  8.  6. 47.  1.]
 [13. 20. 28. 21. 54.]]

I - Epoch: 232
I - Training: 
	I - Batch: 50 | Loss: 0.547 | Acc: 95.000% | Wgt Acc: 96.730%
	I - Batch: 100 | Loss: 0.542 | Acc: 95.250% | Wgt Acc: 96.873%
	I - Batch: 150 | Loss: 0.548 | Acc: 94.292% | Wgt Acc: 96.229%
	I - Batch: 200 | Loss: 0.550 | Acc: 94.188% | Wgt Acc: 96.090%
I - num batch: 222
I - Train -- Loss: 0.550 | Acc: 94.305% | Wgt Acc: 96.197% | LR: 1.250000e-04 | Dur: 125.32s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   4.   4.  55.]
 [  0. 577.   0.   2.  19.]
 [  0.   0. 718.   0.  39.]
 [  1.   0.   2. 529.  43.]
 [ 19.   1.  10.   3. 844.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.261 | Acc: 52.632% | Wgt Acc: 48.762% | Dur: 13.01s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  3.  3. 22.  4.]
 [ 2. 28.  5.  1.  2.]
 [ 0. 11. 14.  0.  3.]
 [ 6.  7.  8. 42.  2.]
 [15. 29. 45. 21. 61.]]

I - Epoch: 233
I - Training: 
	I - Batch: 50 | Loss: 0.505 | Acc: 97.375% | Wgt Acc: 97.989%
	I - Batch: 100 | Loss: 0.508 | Acc: 97.062% | Wgt Acc: 97.700%
	I - Batch: 150 | Loss: 0.504 | Acc: 97.208% | Wgt Acc: 97.931%
I - num batch: 173
I - Train -- Loss: 0.503 | Acc: 97.359% | Wgt Acc: 98.058% | LR: 1.250000e-04 | Dur: 97.86s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   5.   5.  11.]
 [  1. 578.   3.   4.   5.]
 [  0.   0. 724.   4.  15.]
 [  1.   0.   0. 524.   8.]
 [  8.   0.   2.   1. 178.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.231 | Acc: 53.885% | Wgt Acc: 53.032% | Dur: 13.31s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  7.  6. 16. 11.]
 [ 3. 31. 10.  4.  8.]
 [ 2. 16. 30.  2.  8.]
 [11.  7.  8. 48.  4.]
 [ 7. 17. 21. 16. 41.]]

I - Epoch: 234
I - Training: 
	I - Batch: 50 | Loss: 0.537 | Acc: 94.625% | Wgt Acc: 96.765%
	I - Batch: 100 | Loss: 0.545 | Acc: 94.938% | Wgt Acc: 96.701%
	I - Batch: 150 | Loss: 0.545 | Acc: 94.917% | Wgt Acc: 96.543%
	I - Batch: 200 | Loss: 0.548 | Acc: 94.906% | Wgt Acc: 96.471%
I - num batch: 222
I - Train -- Loss: 0.548 | Acc: 94.925% | Wgt Acc: 96.467% | LR: 1.250000e-04 | Dur: 125.69s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   3.   2.  53.]
 [  0. 578.   2.   4.  21.]
 [  0.   0. 720.   1.  25.]
 [  4.   0.   1. 522.  34.]
 [ 13.   0.   8.   9. 867.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.219 | Acc: 56.892% | Wgt Acc: 53.713% | Dur: 13.24s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  2.  4. 12.  2.]
 [ 1. 26.  5.  2.  2.]
 [ 1. 18. 32.  0.  6.]
 [10.  5.  8. 50.  4.]
 [15. 27. 26. 22. 58.]]

I - Epoch: 235
I - Training: 
	I - Batch: 50 | Loss: 0.541 | Acc: 95.250% | Wgt Acc: 96.760%
	I - Batch: 100 | Loss: 0.547 | Acc: 94.750% | Wgt Acc: 96.348%
	I - Batch: 150 | Loss: 0.549 | Acc: 94.708% | Wgt Acc: 96.367%
	I - Batch: 200 | Loss: 0.548 | Acc: 94.625% | Wgt Acc: 96.367%
I - num batch: 222
I - Train -- Loss: 0.547 | Acc: 94.672% | Wgt Acc: 96.430% | LR: 1.250000e-04 | Dur: 125.93s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   3.   0.  54.]
 [  2. 574.   1.   3.  20.]
 [  0.   1. 720.   0.  36.]
 [  1.   0.   2. 527.  38.]
 [  9.   3.   8.   8. 852.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.243 | Acc: 55.388% | Wgt Acc: 51.547% | Dur: 12.59s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  2.  4. 15.  2.]
 [ 1. 33.  6.  4.  4.]
 [ 0.  7. 18.  1.  0.]
 [11.  5. 13. 46.  1.]
 [17. 31. 34. 20. 65.]]

I - Epoch: 236
I - Training: 
	I - Batch: 50 | Loss: 0.547 | Acc: 94.625% | Wgt Acc: 96.421%
	I - Batch: 100 | Loss: 0.540 | Acc: 95.688% | Wgt Acc: 97.041%
	I - Batch: 150 | Loss: 0.542 | Acc: 95.542% | Wgt Acc: 96.901%
	I - Batch: 200 | Loss: 0.544 | Acc: 95.406% | Wgt Acc: 96.811%
I - num batch: 222
I - Train -- Loss: 0.544 | Acc: 95.405% | Wgt Acc: 96.790% | LR: 1.250000e-04 | Dur: 124.72s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   2.   4.  41.]
 [  0. 578.   1.   2.  22.]
 [  0.   0. 723.   0.  27.]
 [  1.   0.   2. 521.  31.]
 [ 13.   0.   6.  11. 879.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.262 | Acc: 54.386% | Wgt Acc: 50.248% | Dur: 12.54s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  2. 12.  5.]
 [ 1. 24.  3.  2.  0.]
 [ 0. 11. 18.  0.  1.]
 [12.  9. 14. 50.  1.]
 [15. 30. 38. 22. 65.]]

I - Epoch: 237
I - Training: 
	I - Batch: 50 | Loss: 0.554 | Acc: 95.375% | Wgt Acc: 97.004%
	I - Batch: 100 | Loss: 0.549 | Acc: 95.250% | Wgt Acc: 96.898%
	I - Batch: 150 | Loss: 0.546 | Acc: 95.208% | Wgt Acc: 96.813%
	I - Batch: 200 | Loss: 0.552 | Acc: 94.750% | Wgt Acc: 96.442%
I - num batch: 222
I - Train -- Loss: 0.551 | Acc: 94.813% | Wgt Acc: 96.452% | LR: 1.250000e-04 | Dur: 121.84s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   2.   3.  49.]
 [  1. 576.   3.   4.  20.]
 [  0.   1. 719.   1.  40.]
 [  2.   0.   0. 526.  30.]
 [ 13.   1.  10.   4. 861.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.211 | Acc: 57.393% | Wgt Acc: 54.332% | Dur: 12.53s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  2.  1. 11.  2.]
 [ 5. 34. 10.  4.  2.]
 [ 1. 14. 25.  2.  4.]
 [ 8.  6.  8. 50.  3.]
 [15. 22. 31. 19. 61.]]

I - Epoch: 238
I - Training: 
	I - Batch: 50 | Loss: 0.537 | Acc: 95.875% | Wgt Acc: 97.153%
	I - Batch: 100 | Loss: 0.544 | Acc: 95.188% | Wgt Acc: 96.694%
	I - Batch: 150 | Loss: 0.543 | Acc: 95.417% | Wgt Acc: 96.937%
	I - Batch: 200 | Loss: 0.547 | Acc: 95.031% | Wgt Acc: 96.599%
I - num batch: 222
I - Train -- Loss: 0.549 | Acc: 94.869% | Wgt Acc: 96.467% | LR: 1.250000e-04 | Dur: 123.11s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   2.   5.  46.]
 [  0. 576.   1.   4.  24.]
 [  0.   1. 722.   1.  29.]
 [  2.   0.   2. 522.  39.]
 [ 12.   1.   7.   6. 862.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.188 | Acc: 57.393% | Wgt Acc: 55.693% | Dur: 13.29s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  4.  3. 12.  5.]
 [ 3. 32.  7.  3.  7.]
 [ 0. 12. 28.  4.  4.]
 [14. 11. 11. 54.  5.]
 [ 7. 19. 26. 13. 51.]]

I - Epoch: 239
I - Training: 
	I - Batch: 50 | Loss: 0.556 | Acc: 94.875% | Wgt Acc: 96.611%
	I - Batch: 100 | Loss: 0.556 | Acc: 94.562% | Wgt Acc: 96.340%
	I - Batch: 150 | Loss: 0.551 | Acc: 94.500% | Wgt Acc: 96.326%
	I - Batch: 200 | Loss: 0.550 | Acc: 94.531% | Wgt Acc: 96.176%
I - num batch: 222
I - Train -- Loss: 0.550 | Acc: 94.446% | Wgt Acc: 96.167% | LR: 1.250000e-04 | Dur: 124.06s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   0.   6.  49.]
 [  0. 573.   2.   2.  32.]
 [  1.   0. 722.   1.  32.]
 [  1.   0.   1. 523.  36.]
 [ 14.   5.   9.   6. 851.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.244 | Acc: 55.388% | Wgt Acc: 51.547% | Dur: 13.06s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  8.  5. 15.  3.]
 [ 2. 29.  5.  3.  2.]
 [ 0. 10. 24.  1.  5.]
 [ 8.  5.  9. 42.  1.]
 [13. 26. 32. 25. 61.]]

I - Epoch: 240
I - Training: 
	I - Batch: 50 | Loss: 0.545 | Acc: 95.000% | Wgt Acc: 96.595%
	I - Batch: 100 | Loss: 0.546 | Acc: 94.562% | Wgt Acc: 96.227%
	I - Batch: 150 | Loss: 0.548 | Acc: 94.500% | Wgt Acc: 96.258%
	I - Batch: 200 | Loss: 0.547 | Acc: 94.781% | Wgt Acc: 96.445%
I - num batch: 222
I - Train -- Loss: 0.549 | Acc: 94.728% | Wgt Acc: 96.385% | LR: 1.250000e-04 | Dur: 125.30s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   1.   2.  47.]
 [  2. 577.   0.   2.  32.]
 [  0.   0. 717.   2.  30.]
 [  1.   0.   1. 522.  33.]
 [  8.   1.  15.  10. 858.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.215 | Acc: 56.391% | Wgt Acc: 52.785% | Dur: 12.84s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  4.  4. 13.  1.]
 [ 3. 31.  8.  3.  4.]
 [ 1. 10. 24.  1.  3.]
 [13.  6. 11. 48.  1.]
 [12. 27. 28. 21. 63.]]

I - Epoch: 241
I - Training: 
	I - Batch: 50 | Loss: 0.520 | Acc: 96.750% | Wgt Acc: 97.474%
	I - Batch: 100 | Loss: 0.507 | Acc: 97.250% | Wgt Acc: 98.017%
	I - Batch: 150 | Loss: 0.509 | Acc: 97.000% | Wgt Acc: 97.840%
I - num batch: 173
I - Train -- Loss: 0.509 | Acc: 97.033% | Wgt Acc: 97.879% | LR: 1.250000e-04 | Dur: 99.19s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   3.   3.  16.]
 [  2. 577.   1.   3.   8.]
 [  2.   0. 725.   2.  14.]
 [  3.   0.   0. 526.   8.]
 [  7.   1.   5.   4. 171.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.220 | Acc: 53.383% | Wgt Acc: 50.619% | Dur: 12.96s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  3. 22.  7.]
 [ 6. 33.  8.  6.  4.]
 [ 0. 18. 31.  5.  6.]
 [ 7.  5.  7. 35.  4.]
 [12. 18. 26. 18. 51.]]

I - Epoch: 242
I - Training: 
	I - Batch: 50 | Loss: 0.554 | Acc: 94.500% | Wgt Acc: 96.289%
	I - Batch: 100 | Loss: 0.549 | Acc: 95.188% | Wgt Acc: 96.658%
	I - Batch: 150 | Loss: 0.549 | Acc: 94.917% | Wgt Acc: 96.583%
	I - Batch: 200 | Loss: 0.545 | Acc: 95.094% | Wgt Acc: 96.756%
I - num batch: 222
I - Train -- Loss: 0.545 | Acc: 94.982% | Wgt Acc: 96.678% | LR: 1.250000e-04 | Dur: 125.87s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   1.   3.  52.]
 [  0. 577.   1.   2.  21.]
 [  0.   0. 723.   1.  35.]
 [  2.   0.   1. 525.  34.]
 [  9.   1.   8.   7. 858.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.206 | Acc: 57.393% | Wgt Acc: 54.827% | Dur: 12.43s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  3.  3. 17.  5.]
 [ 1. 31.  9.  3.  5.]
 [ 0. 18. 28.  4.  7.]
 [ 7.  7. 13. 47.  1.]
 [11. 19. 22. 15. 54.]]

I - Epoch: 243
I - Training: 
	I - Batch: 50 | Loss: 0.554 | Acc: 93.875% | Wgt Acc: 95.648%
	I - Batch: 100 | Loss: 0.558 | Acc: 93.438% | Wgt Acc: 95.216%
	I - Batch: 150 | Loss: 0.554 | Acc: 94.208% | Wgt Acc: 95.836%
	I - Batch: 200 | Loss: 0.550 | Acc: 94.656% | Wgt Acc: 96.252%
I - num batch: 222
I - Train -- Loss: 0.549 | Acc: 94.672% | Wgt Acc: 96.249% | LR: 1.250000e-04 | Dur: 124.44s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   4.   8.  44.]
 [  0. 576.   0.   2.  26.]
 [  1.   0. 723.   2.  35.]
 [  0.   0.   0. 519.  34.]
 [ 17.   2.   7.   7. 861.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.203 | Acc: 56.642% | Wgt Acc: 53.342% | Dur: 12.82s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  5. 15.  4.]
 [ 2. 30.  6.  3.  0.]
 [ 1. 12. 23.  2.  4.]
 [ 9.  6. 10. 48.  4.]
 [11. 25. 31. 18. 60.]]

I - Epoch: 244
I - Training: 
	I - Batch: 50 | Loss: 0.531 | Acc: 96.250% | Wgt Acc: 97.463%
	I - Batch: 100 | Loss: 0.540 | Acc: 95.688% | Wgt Acc: 97.016%
	I - Batch: 150 | Loss: 0.539 | Acc: 95.583% | Wgt Acc: 96.983%
	I - Batch: 200 | Loss: 0.543 | Acc: 95.125% | Wgt Acc: 96.654%
I - num batch: 222
I - Train -- Loss: 0.545 | Acc: 95.010% | Wgt Acc: 96.557% | LR: 1.250000e-04 | Dur: 123.09s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   2.   5.  44.]
 [  1. 575.   1.   3.  16.]
 [  0.   0. 720.   0.  32.]
 [  0.   0.   0. 525.  41.]
 [ 13.   3.  11.   5. 867.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.258 | Acc: 53.133% | Wgt Acc: 49.010% | Dur: 12.28s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  4.  3. 10.  3.]
 [ 4. 31.  5.  1.  3.]
 [ 0. 13. 24.  1.  3.]
 [ 6.  5.  7. 39.  0.]
 [23. 25. 36. 35. 63.]]

I - Epoch: 245
I - Training: 
	I - Batch: 50 | Loss: 0.548 | Acc: 94.875% | Wgt Acc: 96.044%
	I - Batch: 100 | Loss: 0.549 | Acc: 94.938% | Wgt Acc: 96.399%
	I - Batch: 150 | Loss: 0.546 | Acc: 95.125% | Wgt Acc: 96.595%
	I - Batch: 200 | Loss: 0.545 | Acc: 94.969% | Wgt Acc: 96.536%
I - num batch: 222
I - Train -- Loss: 0.546 | Acc: 94.925% | Wgt Acc: 96.475% | LR: 1.250000e-04 | Dur: 123.48s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   1.   5.  48.]
 [  1. 575.   0.   3.  27.]
 [  0.   0. 726.   2.  32.]
 [  1.   0.   0. 522.  28.]
 [ 16.   3.   7.   6. 865.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.264 | Acc: 53.634% | Wgt Acc: 49.752% | Dur: 13.15s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  3.  4. 13.  3.]
 [ 1. 24.  3.  2.  2.]
 [ 0. 17. 22.  1.  5.]
 [12.  6. 12. 46.  1.]
 [14. 28. 34. 24. 61.]]

I - Epoch: 246
I - Training: 
	I - Batch: 50 | Loss: 0.545 | Acc: 95.375% | Wgt Acc: 96.841%
	I - Batch: 100 | Loss: 0.543 | Acc: 95.438% | Wgt Acc: 96.891%
	I - Batch: 150 | Loss: 0.542 | Acc: 95.292% | Wgt Acc: 96.835%
	I - Batch: 200 | Loss: 0.546 | Acc: 95.094% | Wgt Acc: 96.571%
I - num batch: 222
I - Train -- Loss: 0.548 | Acc: 94.728% | Wgt Acc: 96.264% | LR: 1.250000e-04 | Dur: 127.32s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   2.   4.  54.]
 [  1. 575.   1.   2.  19.]
 [  0.   0. 722.   1.  32.]
 [  1.   0.   1. 520.  31.]
 [ 16.   3.   8.  11. 864.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.218 | Acc: 57.895% | Wgt Acc: 55.012% | Dur: 12.57s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  5.  5. 14.  4.]
 [ 2. 31.  6.  3.  2.]
 [ 0. 11. 26.  1.  3.]
 [ 8. 11. 14. 50.  5.]
 [12. 20. 24. 18. 58.]]

I - Epoch: 247
I - Training: 
	I - Batch: 50 | Loss: 0.540 | Acc: 95.125% | Wgt Acc: 96.659%
	I - Batch: 100 | Loss: 0.541 | Acc: 95.062% | Wgt Acc: 96.797%
	I - Batch: 150 | Loss: 0.541 | Acc: 95.208% | Wgt Acc: 96.813%
	I - Batch: 200 | Loss: 0.543 | Acc: 95.094% | Wgt Acc: 96.684%
I - num batch: 222
I - Train -- Loss: 0.547 | Acc: 94.841% | Wgt Acc: 96.452% | LR: 1.250000e-04 | Dur: 124.85s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   0.   4.  51.]
 [  1. 574.   1.   3.  23.]
 [  0.   0. 724.   1.  32.]
 [  1.   0.   2. 526.  32.]
 [ 17.   4.   7.   4. 862.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.210 | Acc: 57.393% | Wgt Acc: 54.765% | Dur: 13.01s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  3. 16.  4.]
 [ 3. 34.  9.  4.  3.]
 [ 0. 14. 29.  1.  5.]
 [11.  6.  9. 47.  4.]
 [11. 20. 25. 18. 56.]]

I - Epoch: 248
I - Training: 
	I - Batch: 50 | Loss: 0.527 | Acc: 95.875% | Wgt Acc: 97.270%
	I - Batch: 100 | Loss: 0.536 | Acc: 95.500% | Wgt Acc: 96.959%
	I - Batch: 150 | Loss: 0.543 | Acc: 95.000% | Wgt Acc: 96.731%
	I - Batch: 200 | Loss: 0.548 | Acc: 94.875% | Wgt Acc: 96.601%
I - num batch: 222
I - Train -- Loss: 0.547 | Acc: 94.925% | Wgt Acc: 96.685% | LR: 1.250000e-04 | Dur: 128.64s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   2.   4.  44.]
 [  1. 578.   0.   3.  35.]
 [  0.   0. 726.   1.  32.]
 [  2.   0.   0. 525.  35.]
 [ 10.   0.   6.   5. 854.]]

I - Loading file: dataset_cls4_background07_no_samples217.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [697. 578. 734. 538. 217.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.247 | Acc: 56.391% | Wgt Acc: 53.218% | Dur: 13.03s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  5.  4. 20.  3.]
 [ 2. 31.  7.  3.  7.]
 [ 0. 17. 26.  2.  5.]
 [ 6.  4. 10. 41.  1.]
 [ 9. 21. 28. 20. 56.]]

I - Epoch: 249
I - Training: 
	I - Batch: 50 | Loss: 0.504 | Acc: 97.375% | Wgt Acc: 98.292%
	I - Batch: 100 | Loss: 0.498 | Acc: 97.688% | Wgt Acc: 98.427%
	I - Batch: 150 | Loss: 0.498 | Acc: 97.833% | Wgt Acc: 98.481%
I - num batch: 173
I - Train -- Loss: 0.499 | Acc: 97.757% | Wgt Acc: 98.424% | LR: 1.250000e-04 | Dur: 100.42s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   3.   3.  17.]
 [  0. 578.   0.   3.   5.]
 [  1.   0. 727.   1.  11.]
 [  1.   0.   2. 527.   4.]
 [  5.   0.   2.   4. 180.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/pickles
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Validation: 
I - num batch: 25
I - Val -- Loss: 1.235 | Acc: 56.391% | Wgt Acc: 54.641% | Dur: 13.15s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  6.  5. 14.  5.]
 [ 4. 33.  7.  2.  7.]
 [ 1. 16. 26.  1.  3.]
 [12.  5. 17. 52.  6.]
 [ 8. 18. 20. 17. 51.]]

I - Maximum validation set accuracy in current training:  61.15
