Sun Oct 16 01:25:00 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.2, 0.25, 0.2, 0.25, 0.08], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'tossFirstLastFrames': True}, 'dataPath': '/data/processed/Kinetics/', 'dropoutRate': 0.5, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat'], 'lastLayerInitUniform': True, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 66.05504587155963, 'maxValidationTrainNo': 31, 'modelVersion': 6, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 33, 'validationAccThr': 65, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Train set length:  2547
I - Label distribution: [697. 578. 734. 538.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/test
I - Test set length:  327
I - Label distribution: [88. 78. 75. 86.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(2) data number in dataset:  tensor([ 62388,  79038, 143872, 212968,  37253,  61933, 159761,  58019,  95899,
         12337,   6568, 216831, 219725, 106404,   7975,  75841])
I - Initializing model TCNv6
I - Number of Model Parameters: 646156
I - Model output shape:  torch.Size([3, 16, 4, 16])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv6                                    [3, 16, 4, 16]            --
├─FusedConv2dBNReLU: 1-1                 [16, 64, 64, 64]          3,142
│    └─OutputShiftSqueeze: 2-1           --                        --
│    └─One: 2-2                          [1]                       --
│    └─OutputScale: 2-3                  --                        --
│    └─Empty: 2-4                        [64, 48, 1, 1]            --
│    └─Empty: 2-5                        [64, 48, 1, 1]            --
│    └─Empty: 2-6                        [64]                      --
│    └─Empty: 2-7                        [64]                      --
│    └─BatchNorm2d: 2-8                  [16, 64, 64, 64]          --
│    └─Scaler: 2-9                       [16, 64, 64, 64]          --
│    └─ReLU: 2-10                        [16, 64, 64, 64]          --
│    └─Empty: 2-11                       [16, 64, 64, 64]          --
│    └─Clamp: 2-12                       [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-2                 [16, 64, 64, 64]          36,934
│    └─OutputShiftSqueeze: 2-13          --                        --
│    └─One: 2-14                         [1]                       --
│    └─OutputScale: 2-15                 --                        --
│    └─Empty: 2-16                       [64, 64, 3, 3]            --
│    └─Empty: 2-17                       [64, 64, 3, 3]            --
│    └─Empty: 2-18                       [64]                      --
│    └─Empty: 2-19                       [64]                      --
│    └─BatchNorm2d: 2-20                 [16, 64, 64, 64]          --
│    └─Scaler: 2-21                      [16, 64, 64, 64]          --
│    └─ReLU: 2-22                        [16, 64, 64, 64]          --
│    └─Empty: 2-23                       [16, 64, 64, 64]          --
│    └─Clamp: 2-24                       [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-3                 [16, 64, 64, 64]          4,166
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [64, 64, 1, 1]            --
│    └─Empty: 2-29                       [64, 64, 1, 1]            --
│    └─Empty: 2-30                       [64]                      --
│    └─Empty: 2-31                       [64]                      --
│    └─BatchNorm2d: 2-32                 [16, 64, 64, 64]          --
│    └─Scaler: 2-33                      [16, 64, 64, 64]          --
│    └─ReLU: 2-34                        [16, 64, 64, 64]          --
│    └─Empty: 2-35                       [16, 64, 64, 64]          --
│    └─Clamp: 2-36                       [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-4                 [16, 64, 64, 64]          36,934
│    └─OutputShiftSqueeze: 2-37          --                        --
│    └─One: 2-38                         [1]                       --
│    └─OutputScale: 2-39                 --                        --
│    └─Empty: 2-40                       [64, 64, 3, 3]            --
│    └─Empty: 2-41                       [64, 64, 3, 3]            --
│    └─Empty: 2-42                       [64]                      --
│    └─Empty: 2-43                       [64]                      --
│    └─BatchNorm2d: 2-44                 [16, 64, 64, 64]          --
│    └─Scaler: 2-45                      [16, 64, 64, 64]          --
│    └─ReLU: 2-46                        [16, 64, 64, 64]          --
│    └─Empty: 2-47                       [16, 64, 64, 64]          --
│    └─Clamp: 2-48                       [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-5          [16, 64, 32, 32]          36,934
│    └─MaxPool2d: 2-49                   [16, 64, 32, 32]          --
│    └─Empty: 2-50                       [16, 64, 32, 32]          --
│    └─Empty: 2-51                       [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-52          --                        --
│    └─One: 2-53                         [1]                       --
│    └─OutputScale: 2-54                 --                        --
│    └─Empty: 2-55                       [64, 64, 3, 3]            --
│    └─Empty: 2-56                       [64, 64, 3, 3]            --
│    └─Empty: 2-57                       [64]                      --
│    └─Empty: 2-58                       [64]                      --
│    └─BatchNorm2d: 2-59                 [16, 64, 32, 32]          --
│    └─Scaler: 2-60                      [16, 64, 32, 32]          --
│    └─ReLU: 2-3489                      [16, 64, 32, 32]          --
│    └─ReLU: 2-62                        [16, 64, 32, 32]          --
│    └─Empty: 2-63                       [16, 64, 32, 32]          --
│    └─Clamp: 2-64                       [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-6                 [16, 64, 32, 32]          36,934
│    └─OutputShiftSqueeze: 2-65          --                        --
│    └─One: 2-66                         [1]                       --
│    └─OutputScale: 2-67                 --                        --
│    └─Empty: 2-68                       [64, 64, 3, 3]            --
│    └─Empty: 2-69                       [64, 64, 3, 3]            --
│    └─Empty: 2-70                       [64]                      --
│    └─Empty: 2-71                       [64]                      --
│    └─BatchNorm2d: 2-72                 [16, 64, 32, 32]          --
│    └─Scaler: 2-73                      [16, 64, 32, 32]          --
│    └─ReLU: 2-74                        [16, 64, 32, 32]          --
│    └─Empty: 2-75                       [16, 64, 32, 32]          --
│    └─Clamp: 2-76                       [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-7          [16, 64, 16, 16]          36,934
│    └─MaxPool2d: 2-77                   [16, 64, 16, 16]          --
│    └─Empty: 2-78                       [16, 64, 16, 16]          --
│    └─Empty: 2-79                       [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-80          --                        --
│    └─One: 2-81                         [1]                       --
│    └─OutputScale: 2-82                 --                        --
│    └─Empty: 2-83                       [64, 64, 3, 3]            --
│    └─Empty: 2-84                       [64, 64, 3, 3]            --
│    └─Empty: 2-85                       [64]                      --
│    └─Empty: 2-86                       [64]                      --
│    └─BatchNorm2d: 2-87                 [16, 64, 16, 16]          --
│    └─Scaler: 2-88                      [16, 64, 16, 16]          --
│    └─ReLU: 2-89                        [16, 64, 16, 16]          --
│    └─Empty: 2-90                       [16, 64, 16, 16]          --
│    └─Clamp: 2-91                       [16, 64, 16, 16]          --
│    └─ReLU: 2-3516                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-8                 [16, 64, 16, 16]          36,934
│    └─OutputShiftSqueeze: 2-93          --                        --
│    └─One: 2-94                         [1]                       --
│    └─OutputScale: 2-95                 --                        --
│    └─Empty: 2-96                       [64, 64, 3, 3]            --
│    └─Empty: 2-97                       [64, 64, 3, 3]            --
│    └─Empty: 2-98                       [64]                      --
│    └─Empty: 2-99                       [64]                      --
│    └─BatchNorm2d: 2-100                [16, 64, 16, 16]          --
│    └─Scaler: 2-101                     [16, 64, 16, 16]          --
│    └─ReLU: 2-102                       [16, 64, 16, 16]          --
│    └─Empty: 2-103                      [16, 64, 16, 16]          --
│    └─Clamp: 2-104                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-9          [16, 64, 8, 8]            36,934
│    └─MaxPool2d: 2-105                  [16, 64, 8, 8]            --
│    └─Empty: 2-106                      [16, 64, 8, 8]            --
│    └─Empty: 2-107                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-108         --                        --
│    └─One: 2-109                        [1]                       --
│    └─OutputScale: 2-110                --                        --
│    └─Empty: 2-111                      [64, 64, 3, 3]            --
│    └─Empty: 2-112                      [64, 64, 3, 3]            --
│    └─Empty: 2-113                      [64]                      --
│    └─Empty: 2-114                      [64]                      --
│    └─BatchNorm2d: 2-115                [16, 64, 8, 8]            --
│    └─Scaler: 2-116                     [16, 64, 8, 8]            --
│    └─ReLU: 2-117                       [16, 64, 8, 8]            --
│    └─Empty: 2-118                      [16, 64, 8, 8]            --
│    └─Clamp: 2-119                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-10                [16, 64, 8, 8]            4,166
│    └─OutputShiftSqueeze: 2-120         --                        --
│    └─One: 2-121                        [1]                       --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─ReLU: 2-3543                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputScale: 2-123                --                        --
│    └─Empty: 2-124                      [64, 64, 1, 1]            --
│    └─Empty: 2-125                      [64, 64, 1, 1]            --
│    └─Empty: 2-126                      [64]                      --
│    └─Empty: 2-127                      [64]                      --
│    └─BatchNorm2d: 2-128                [16, 64, 8, 8]            --
│    └─Scaler: 2-129                     [16, 64, 8, 8]            --
│    └─ReLU: 2-130                       [16, 64, 8, 8]            --
│    └─Empty: 2-131                      [16, 64, 8, 8]            --
│    └─Clamp: 2-132                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-11         [16, 64, 8, 8]            36,934
│    └─MaxPool2d: 2-133                  [16, 64, 8, 8]            --
│    └─Empty: 2-134                      [16, 64, 8, 8]            --
│    └─Empty: 2-135                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-136         --                        --
│    └─One: 2-137                        [1]                       --
│    └─OutputScale: 2-138                --                        --
│    └─Empty: 2-139                      [64, 64, 3, 3]            --
│    └─Empty: 2-140                      [64, 64, 3, 3]            --
│    └─Empty: 2-141                      [64]                      --
│    └─Empty: 2-142                      [64]                      --
│    └─BatchNorm2d: 2-143                [16, 64, 8, 8]            --
│    └─Scaler: 2-144                     [16, 64, 8, 8]            --
│    └─ReLU: 2-145                       [16, 64, 8, 8]            --
│    └─Empty: 2-146                      [16, 64, 8, 8]            --
│    └─Clamp: 2-147                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-12         [16, 64, 4, 4]            36,934
│    └─MaxPool2d: 2-148                  [16, 64, 4, 4]            --
│    └─Empty: 2-149                      [16, 64, 4, 4]            --
│    └─Empty: 2-150                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-151         --                        --
│    └─One: 2-152                        [1]                       --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─ReLU: 2-3570                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-154                --                        --
│    └─Empty: 2-155                      [64, 64, 3, 3]            --
│    └─Empty: 2-156                      [64, 64, 3, 3]            --
│    └─Empty: 2-157                      [64]                      --
│    └─Empty: 2-158                      [64]                      --
│    └─BatchNorm2d: 2-159                [16, 64, 4, 4]            --
│    └─Scaler: 2-160                     [16, 64, 4, 4]            --
│    └─ReLU: 2-161                       [16, 64, 4, 4]            --
│    └─Empty: 2-162                      [16, 64, 4, 4]            --
│    └─Clamp: 2-163                      [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-13                [16, 64, 4, 4]            4,166
│    └─OutputShiftSqueeze: 2-164         --                        --
│    └─One: 2-165                        [1]                       --
│    └─OutputScale: 2-166                --                        --
│    └─Empty: 2-167                      [64, 64, 1, 1]            --
│    └─Empty: 2-168                      [64, 64, 1, 1]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─ReLU: 2-3585                      [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-170                      [64]                      --
│    └─Empty: 2-171                      [64]                      --
│    └─BatchNorm2d: 2-172                [16, 64, 4, 4]            --
│    └─Scaler: 2-173                     [16, 64, 4, 4]            --
│    └─ReLU: 2-174                       [16, 64, 4, 4]            --
│    └─Empty: 2-175                      [16, 64, 4, 4]            --
│    └─Clamp: 2-176                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-14         [16, 64, 4, 4]            36,934
│    └─MaxPool2d: 2-177                  [16, 64, 4, 4]            --
│    └─Empty: 2-178                      [16, 64, 4, 4]            --
│    └─Empty: 2-179                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-180         --                        --
│    └─One: 2-181                        [1]                       --
│    └─OutputScale: 2-182                --                        --
│    └─Empty: 2-183                      [64, 64, 3, 3]            --
│    └─Empty: 2-184                      [64, 64, 3, 3]            --
│    └─Empty: 2-185                      [64]                      --
│    └─Empty: 2-186                      [64]                      --
│    └─BatchNorm2d: 2-187                [16, 64, 4, 4]            --
│    └─Scaler: 2-188                     [16, 64, 4, 4]            --
│    └─ReLU: 2-189                       [16, 64, 4, 4]            --
│    └─Empty: 2-190                      [16, 64, 4, 4]            --
│    └─Clamp: 2-191                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 64, 2, 2]            4,166
│    └─MaxPool2d: 2-192                  [16, 64, 2, 2]            --
│    └─Empty: 2-193                      [16, 64, 2, 2]            --
│    └─Empty: 2-194                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-195         --                        --
│    └─One: 2-196                        [1]                       --
│    └─OutputScale: 2-197                --                        --
│    └─Empty: 2-198                      [64, 64, 1, 1]            --
│    └─Empty: 2-199                      [64, 64, 1, 1]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─ReLU: 2-3612                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-201                      [64]                      --
│    └─Empty: 2-202                      [64]                      --
│    └─BatchNorm2d: 2-203                [16, 64, 2, 2]            --
│    └─Scaler: 2-204                     [16, 64, 2, 2]            --
│    └─ReLU: 2-205                       [16, 64, 2, 2]            --
│    └─Empty: 2-206                      [16, 64, 2, 2]            --
│    └─Clamp: 2-207                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-16                [16, 64, 2, 2]            4,166
│    └─OutputShiftSqueeze: 2-208         --                        --
│    └─One: 2-209                        [1]                       --
│    └─OutputScale: 2-210                --                        --
│    └─Empty: 2-211                      [64, 64, 1, 1]            --
│    └─Empty: 2-212                      [64, 64, 1, 1]            --
│    └─Empty: 2-213                      [64]                      --
│    └─Empty: 2-214                      [64]                      --
│    └─BatchNorm2d: 2-215                [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─ReLU: 2-3627                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-217                     [16, 64, 2, 2]            --
│    └─ReLU: 2-218                       [16, 64, 2, 2]            --
│    └─Empty: 2-219                      [16, 64, 2, 2]            --
│    └─Clamp: 2-220                      [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-17         [16, 64, 2, 2]            36,934
│    └─MaxPool2d: 2-221                  [16, 64, 2, 2]            --
│    └─Empty: 2-222                      [16, 64, 2, 2]            --
│    └─Empty: 2-223                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-224         --                        --
│    └─One: 2-225                        [1]                       --
│    └─OutputScale: 2-226                --                        --
│    └─Empty: 2-227                      [64, 64, 3, 3]            --
│    └─Empty: 2-228                      [64, 64, 3, 3]            --
│    └─Empty: 2-229                      [64]                      --
│    └─Empty: 2-230                      [64]                      --
│    └─BatchNorm2d: 2-231                [16, 64, 2, 2]            --
│    └─Scaler: 2-232                     [16, 64, 2, 2]            --
│    └─ReLU: 2-233                       [16, 64, 2, 2]            --
│    └─Empty: 2-234                      [16, 64, 2, 2]            --
│    └─Clamp: 2-235                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-18                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-236         --                        --
│    └─One: 2-237                        [1]                       --
│    └─OutputScale: 2-238                --                        --
│    └─Empty: 2-239                      [64, 48, 1, 1]            --
│    └─Empty: 2-240                      [64, 48, 1, 1]            --
│    └─Empty: 2-241                      [64]                      --
│    └─Empty: 2-242                      [64]                      --
│    └─BatchNorm2d: 2-243                [16, 64, 64, 64]          --
│    └─Scaler: 2-244                     [16, 64, 64, 64]          --
│    └─ReLU: 2-245                       [16, 64, 64, 64]          --
│    └─Empty: 2-246                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─ReLU: 2-3654                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-248                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-19                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-249         --                        --
│    └─One: 2-250                        [1]                       --
│    └─OutputScale: 2-251                --                        --
│    └─Empty: 2-252                      [64, 64, 3, 3]            --
│    └─Empty: 2-253                      [64, 64, 3, 3]            --
│    └─Empty: 2-254                      [64]                      --
│    └─Empty: 2-255                      [64]                      --
│    └─BatchNorm2d: 2-256                [16, 64, 64, 64]          --
│    └─Scaler: 2-257                     [16, 64, 64, 64]          --
│    └─ReLU: 2-258                       [16, 64, 64, 64]          --
│    └─Empty: 2-259                      [16, 64, 64, 64]          --
│    └─Clamp: 2-260                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-20                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-261         --                        --
│    └─One: 2-262                        [1]                       --
│    └─OutputScale: 2-263                --                        --
│    └─Empty: 2-264                      [64, 64, 1, 1]            --
│    └─Empty: 2-265                      [64, 64, 1, 1]            --
│    └─Empty: 2-266                      [64]                      --
│    └─Empty: 2-267                      [64]                      --
│    └─BatchNorm2d: 2-268                [16, 64, 64, 64]          --
│    └─Scaler: 2-269                     [16, 64, 64, 64]          --
│    └─ReLU: 2-270                       [16, 64, 64, 64]          --
│    └─Empty: 2-271                      [16, 64, 64, 64]          --
│    └─Clamp: 2-272                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-21                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-273         --                        --
│    └─One: 2-274                        [1]                       --
│    └─OutputScale: 2-275                --                        --
│    └─Empty: 2-276                      [64, 64, 3, 3]            --
│    └─Empty: 2-277                      [64, 64, 3, 3]            --
│    └─Empty: 2-278                      [64]                      --
│    └─Empty: 2-279                      [64]                      --
│    └─BatchNorm2d: 2-280                [16, 64, 64, 64]          --
│    └─Scaler: 2-281                     [16, 64, 64, 64]          --
│    └─ReLU: 2-282                       [16, 64, 64, 64]          --
│    └─Empty: 2-283                      [16, 64, 64, 64]          --
│    └─Clamp: 2-284                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-22         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-285                  [16, 64, 32, 32]          --
│    └─Empty: 2-286                      [16, 64, 32, 32]          --
│    └─Empty: 2-287                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-288         --                        --
│    └─One: 2-289                        [1]                       --
│    └─OutputScale: 2-290                --                        --
│    └─Empty: 2-291                      [64, 64, 3, 3]            --
│    └─Empty: 2-292                      [64, 64, 3, 3]            --
│    └─Empty: 2-293                      [64]                      --
│    └─Empty: 2-294                      [64]                      --
│    └─BatchNorm2d: 2-295                [16, 64, 32, 32]          --
│    └─Scaler: 2-296                     [16, 64, 32, 32]          --
│    └─ReLU: 2-297                       [16, 64, 32, 32]          --
│    └─Empty: 2-298                      [16, 64, 32, 32]          --
│    └─Clamp: 2-299                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-23                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-300         --                        --
│    └─One: 2-301                        [1]                       --
│    └─OutputScale: 2-302                --                        --
│    └─Empty: 2-303                      [64, 64, 3, 3]            --
│    └─Empty: 2-304                      [64, 64, 3, 3]            --
│    └─Empty: 2-305                      [64]                      --
│    └─Empty: 2-306                      [64]                      --
│    └─BatchNorm2d: 2-307                [16, 64, 32, 32]          --
│    └─Scaler: 2-308                     [16, 64, 32, 32]          --
│    └─ReLU: 2-309                       [16, 64, 32, 32]          --
│    └─Empty: 2-310                      [16, 64, 32, 32]          --
│    └─Clamp: 2-311                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-24         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-312                  [16, 64, 16, 16]          --
│    └─Empty: 2-313                      [16, 64, 16, 16]          --
│    └─Empty: 2-314                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-315         --                        --
│    └─One: 2-316                        [1]                       --
│    └─OutputScale: 2-317                --                        --
│    └─Empty: 2-318                      [64, 64, 3, 3]            --
│    └─Empty: 2-319                      [64, 64, 3, 3]            --
│    └─Empty: 2-320                      [64]                      --
│    └─Empty: 2-321                      [64]                      --
│    └─BatchNorm2d: 2-322                [16, 64, 16, 16]          --
│    └─Scaler: 2-323                     [16, 64, 16, 16]          --
│    └─ReLU: 2-324                       [16, 64, 16, 16]          --
│    └─Empty: 2-325                      [16, 64, 16, 16]          --
│    └─Clamp: 2-326                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-25                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-327         --                        --
│    └─One: 2-328                        [1]                       --
│    └─OutputScale: 2-329                --                        --
│    └─Empty: 2-330                      [64, 64, 3, 3]            --
│    └─Empty: 2-331                      [64, 64, 3, 3]            --
│    └─Empty: 2-332                      [64]                      --
│    └─Empty: 2-333                      [64]                      --
│    └─BatchNorm2d: 2-334                [16, 64, 16, 16]          --
│    └─Scaler: 2-335                     [16, 64, 16, 16]          --
│    └─ReLU: 2-336                       [16, 64, 16, 16]          --
│    └─Empty: 2-337                      [16, 64, 16, 16]          --
│    └─Clamp: 2-338                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-339                  [16, 64, 8, 8]            --
│    └─Empty: 2-340                      [16, 64, 8, 8]            --
│    └─Empty: 2-341                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-342         --                        --
│    └─One: 2-343                        [1]                       --
│    └─OutputScale: 2-344                --                        --
│    └─Empty: 2-345                      [64, 64, 3, 3]            --
│    └─Empty: 2-346                      [64, 64, 3, 3]            --
│    └─Empty: 2-347                      [64]                      --
│    └─Empty: 2-348                      [64]                      --
│    └─BatchNorm2d: 2-349                [16, 64, 8, 8]            --
│    └─Scaler: 2-350                     [16, 64, 8, 8]            --
│    └─ReLU: 2-351                       [16, 64, 8, 8]            --
│    └─Empty: 2-352                      [16, 64, 8, 8]            --
│    └─Clamp: 2-353                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-27                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-354         --                        --
│    └─One: 2-355                        [1]                       --
│    └─OutputScale: 2-356                --                        --
│    └─Empty: 2-357                      [64, 64, 1, 1]            --
│    └─Empty: 2-358                      [64, 64, 1, 1]            --
│    └─Empty: 2-359                      [64]                      --
│    └─Empty: 2-360                      [64]                      --
│    └─BatchNorm2d: 2-361                [16, 64, 8, 8]            --
│    └─Scaler: 2-362                     [16, 64, 8, 8]            --
│    └─ReLU: 2-363                       [16, 64, 8, 8]            --
│    └─Empty: 2-364                      [16, 64, 8, 8]            --
│    └─Clamp: 2-365                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-28         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-366                  [16, 64, 8, 8]            --
│    └─Empty: 2-367                      [16, 64, 8, 8]            --
│    └─Empty: 2-368                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-369         --                        --
│    └─One: 2-370                        [1]                       --
│    └─OutputScale: 2-371                --                        --
│    └─Empty: 2-372                      [64, 64, 3, 3]            --
│    └─Empty: 2-373                      [64, 64, 3, 3]            --
│    └─Empty: 2-374                      [64]                      --
│    └─Empty: 2-375                      [64]                      --
│    └─BatchNorm2d: 2-376                [16, 64, 8, 8]            --
│    └─Scaler: 2-377                     [16, 64, 8, 8]            --
│    └─ReLU: 2-378                       [16, 64, 8, 8]            --
│    └─Empty: 2-379                      [16, 64, 8, 8]            --
│    └─Clamp: 2-380                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-29         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-381                  [16, 64, 4, 4]            --
│    └─Empty: 2-382                      [16, 64, 4, 4]            --
│    └─Empty: 2-383                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-384         --                        --
│    └─One: 2-385                        [1]                       --
│    └─OutputScale: 2-386                --                        --
│    └─Empty: 2-387                      [64, 64, 3, 3]            --
│    └─Empty: 2-388                      [64, 64, 3, 3]            --
│    └─Empty: 2-389                      [64]                      --
│    └─Empty: 2-390                      [64]                      --
│    └─BatchNorm2d: 2-391                [16, 64, 4, 4]            --
│    └─Scaler: 2-392                     [16, 64, 4, 4]            --
│    └─ReLU: 2-393                       [16, 64, 4, 4]            --
│    └─Empty: 2-394                      [16, 64, 4, 4]            --
│    └─Clamp: 2-395                      [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-30                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-396         --                        --
│    └─One: 2-397                        [1]                       --
│    └─OutputScale: 2-398                --                        --
│    └─Empty: 2-399                      [64, 64, 1, 1]            --
│    └─Empty: 2-400                      [64, 64, 1, 1]            --
│    └─Empty: 2-401                      [64]                      --
│    └─Empty: 2-402                      [64]                      --
│    └─BatchNorm2d: 2-403                [16, 64, 4, 4]            --
│    └─Scaler: 2-404                     [16, 64, 4, 4]            --
│    └─ReLU: 2-405                       [16, 64, 4, 4]            --
│    └─Empty: 2-406                      [16, 64, 4, 4]            --
│    └─Clamp: 2-407                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-408                  [16, 64, 4, 4]            --
│    └─Empty: 2-409                      [16, 64, 4, 4]            --
│    └─Empty: 2-410                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-411         --                        --
│    └─One: 2-412                        [1]                       --
│    └─OutputScale: 2-413                --                        --
│    └─Empty: 2-414                      [64, 64, 3, 3]            --
│    └─Empty: 2-415                      [64, 64, 3, 3]            --
│    └─Empty: 2-416                      [64]                      --
│    └─Empty: 2-417                      [64]                      --
│    └─BatchNorm2d: 2-418                [16, 64, 4, 4]            --
│    └─Scaler: 2-419                     [16, 64, 4, 4]            --
│    └─ReLU: 2-420                       [16, 64, 4, 4]            --
│    └─Empty: 2-421                      [16, 64, 4, 4]            --
│    └─Clamp: 2-422                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-32         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-423                  [16, 64, 2, 2]            --
│    └─Empty: 2-424                      [16, 64, 2, 2]            --
│    └─Empty: 2-425                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-426         --                        --
│    └─One: 2-427                        [1]                       --
│    └─OutputScale: 2-428                --                        --
│    └─Empty: 2-429                      [64, 64, 1, 1]            --
│    └─Empty: 2-430                      [64, 64, 1, 1]            --
│    └─Empty: 2-431                      [64]                      --
│    └─Empty: 2-432                      [64]                      --
│    └─BatchNorm2d: 2-433                [16, 64, 2, 2]            --
│    └─Scaler: 2-434                     [16, 64, 2, 2]            --
│    └─ReLU: 2-435                       [16, 64, 2, 2]            --
│    └─Empty: 2-436                      [16, 64, 2, 2]            --
│    └─Clamp: 2-437                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-33                [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-438         --                        --
│    └─One: 2-439                        [1]                       --
│    └─OutputScale: 2-440                --                        --
│    └─Empty: 2-441                      [64, 64, 1, 1]            --
│    └─Empty: 2-442                      [64, 64, 1, 1]            --
│    └─Empty: 2-443                      [64]                      --
│    └─Empty: 2-444                      [64]                      --
│    └─BatchNorm2d: 2-445                [16, 64, 2, 2]            --
│    └─Scaler: 2-446                     [16, 64, 2, 2]            --
│    └─ReLU: 2-447                       [16, 64, 2, 2]            --
│    └─Empty: 2-448                      [16, 64, 2, 2]            --
│    └─Clamp: 2-449                      [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-34         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-450                  [16, 64, 2, 2]            --
│    └─Empty: 2-451                      [16, 64, 2, 2]            --
│    └─Empty: 2-452                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-453         --                        --
│    └─One: 2-454                        [1]                       --
│    └─OutputScale: 2-455                --                        --
│    └─Empty: 2-456                      [64, 64, 3, 3]            --
│    └─Empty: 2-457                      [64, 64, 3, 3]            --
│    └─Empty: 2-458                      [64]                      --
│    └─Empty: 2-459                      [64]                      --
│    └─BatchNorm2d: 2-460                [16, 64, 2, 2]            --
│    └─Scaler: 2-461                     [16, 64, 2, 2]            --
│    └─ReLU: 2-462                       [16, 64, 2, 2]            --
│    └─Empty: 2-463                      [16, 64, 2, 2]            --
│    └─Clamp: 2-464                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-35                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-465         --                        --
│    └─One: 2-466                        [1]                       --
│    └─OutputScale: 2-467                --                        --
│    └─Empty: 2-468                      [64, 48, 1, 1]            --
│    └─Empty: 2-469                      [64, 48, 1, 1]            --
│    └─Empty: 2-470                      [64]                      --
│    └─Empty: 2-471                      [64]                      --
│    └─BatchNorm2d: 2-472                [16, 64, 64, 64]          --
│    └─Scaler: 2-473                     [16, 64, 64, 64]          --
│    └─ReLU: 2-474                       [16, 64, 64, 64]          --
│    └─Empty: 2-475                      [16, 64, 64, 64]          --
│    └─Clamp: 2-476                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-36                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-477         --                        --
│    └─One: 2-478                        [1]                       --
│    └─OutputScale: 2-479                --                        --
│    └─Empty: 2-480                      [64, 64, 3, 3]            --
│    └─Empty: 2-481                      [64, 64, 3, 3]            --
│    └─Empty: 2-482                      [64]                      --
│    └─Empty: 2-483                      [64]                      --
│    └─BatchNorm2d: 2-484                [16, 64, 64, 64]          --
│    └─Scaler: 2-485                     [16, 64, 64, 64]          --
│    └─ReLU: 2-486                       [16, 64, 64, 64]          --
│    └─Empty: 2-487                      [16, 64, 64, 64]          --
│    └─Clamp: 2-488                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-37                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-489         --                        --
│    └─One: 2-490                        [1]                       --
│    └─OutputScale: 2-491                --                        --
│    └─Empty: 2-492                      [64, 64, 1, 1]            --
│    └─Empty: 2-493                      [64, 64, 1, 1]            --
│    └─Empty: 2-494                      [64]                      --
│    └─Empty: 2-495                      [64]                      --
│    └─BatchNorm2d: 2-496                [16, 64, 64, 64]          --
│    └─Scaler: 2-497                     [16, 64, 64, 64]          --
│    └─ReLU: 2-498                       [16, 64, 64, 64]          --
│    └─Empty: 2-499                      [16, 64, 64, 64]          --
│    └─Clamp: 2-500                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-38                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-501         --                        --
│    └─One: 2-502                        [1]                       --
│    └─OutputScale: 2-503                --                        --
│    └─Empty: 2-504                      [64, 64, 3, 3]            --
│    └─Empty: 2-505                      [64, 64, 3, 3]            --
│    └─Empty: 2-506                      [64]                      --
│    └─Empty: 2-507                      [64]                      --
│    └─BatchNorm2d: 2-508                [16, 64, 64, 64]          --
│    └─Scaler: 2-509                     [16, 64, 64, 64]          --
│    └─ReLU: 2-510                       [16, 64, 64, 64]          --
│    └─Empty: 2-511                      [16, 64, 64, 64]          --
│    └─Clamp: 2-512                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-39         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-513                  [16, 64, 32, 32]          --
│    └─Empty: 2-514                      [16, 64, 32, 32]          --
│    └─Empty: 2-515                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-516         --                        --
│    └─One: 2-517                        [1]                       --
│    └─OutputScale: 2-518                --                        --
│    └─Empty: 2-519                      [64, 64, 3, 3]            --
│    └─Empty: 2-520                      [64, 64, 3, 3]            --
│    └─Empty: 2-521                      [64]                      --
│    └─Empty: 2-522                      [64]                      --
│    └─BatchNorm2d: 2-523                [16, 64, 32, 32]          --
│    └─Scaler: 2-524                     [16, 64, 32, 32]          --
│    └─ReLU: 2-525                       [16, 64, 32, 32]          --
│    └─Empty: 2-526                      [16, 64, 32, 32]          --
│    └─Clamp: 2-527                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-40                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-528         --                        --
│    └─One: 2-529                        [1]                       --
│    └─OutputScale: 2-530                --                        --
│    └─Empty: 2-531                      [64, 64, 3, 3]            --
│    └─Empty: 2-532                      [64, 64, 3, 3]            --
│    └─Empty: 2-533                      [64]                      --
│    └─Empty: 2-534                      [64]                      --
│    └─BatchNorm2d: 2-535                [16, 64, 32, 32]          --
│    └─Scaler: 2-536                     [16, 64, 32, 32]          --
│    └─ReLU: 2-537                       [16, 64, 32, 32]          --
│    └─Empty: 2-538                      [16, 64, 32, 32]          --
│    └─Clamp: 2-539                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-41         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-540                  [16, 64, 16, 16]          --
│    └─Empty: 2-541                      [16, 64, 16, 16]          --
│    └─Empty: 2-542                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-543         --                        --
│    └─One: 2-544                        [1]                       --
│    └─OutputScale: 2-545                --                        --
│    └─Empty: 2-546                      [64, 64, 3, 3]            --
│    └─Empty: 2-547                      [64, 64, 3, 3]            --
│    └─Empty: 2-548                      [64]                      --
│    └─Empty: 2-549                      [64]                      --
│    └─BatchNorm2d: 2-550                [16, 64, 16, 16]          --
│    └─Scaler: 2-551                     [16, 64, 16, 16]          --
│    └─ReLU: 2-552                       [16, 64, 16, 16]          --
│    └─Empty: 2-553                      [16, 64, 16, 16]          --
│    └─Clamp: 2-554                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-42                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-555         --                        --
│    └─One: 2-556                        [1]                       --
│    └─OutputScale: 2-557                --                        --
│    └─Empty: 2-558                      [64, 64, 3, 3]            --
│    └─Empty: 2-559                      [64, 64, 3, 3]            --
│    └─Empty: 2-560                      [64]                      --
│    └─Empty: 2-561                      [64]                      --
│    └─BatchNorm2d: 2-562                [16, 64, 16, 16]          --
│    └─Scaler: 2-563                     [16, 64, 16, 16]          --
│    └─ReLU: 2-564                       [16, 64, 16, 16]          --
│    └─Empty: 2-565                      [16, 64, 16, 16]          --
│    └─Clamp: 2-566                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-43         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-567                  [16, 64, 8, 8]            --
│    └─Empty: 2-568                      [16, 64, 8, 8]            --
│    └─Empty: 2-569                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-570         --                        --
│    └─One: 2-571                        [1]                       --
│    └─OutputScale: 2-572                --                        --
│    └─Empty: 2-573                      [64, 64, 3, 3]            --
│    └─Empty: 2-574                      [64, 64, 3, 3]            --
│    └─Empty: 2-575                      [64]                      --
│    └─Empty: 2-576                      [64]                      --
│    └─BatchNorm2d: 2-577                [16, 64, 8, 8]            --
│    └─Scaler: 2-578                     [16, 64, 8, 8]            --
│    └─ReLU: 2-579                       [16, 64, 8, 8]            --
│    └─Empty: 2-580                      [16, 64, 8, 8]            --
│    └─Clamp: 2-581                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-44                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-582         --                        --
│    └─One: 2-583                        [1]                       --
│    └─OutputScale: 2-584                --                        --
│    └─Empty: 2-585                      [64, 64, 1, 1]            --
│    └─Empty: 2-586                      [64, 64, 1, 1]            --
│    └─Empty: 2-587                      [64]                      --
│    └─Empty: 2-588                      [64]                      --
│    └─BatchNorm2d: 2-589                [16, 64, 8, 8]            --
│    └─Scaler: 2-590                     [16, 64, 8, 8]            --
│    └─ReLU: 2-591                       [16, 64, 8, 8]            --
│    └─Empty: 2-592                      [16, 64, 8, 8]            --
│    └─Clamp: 2-593                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-45         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-594                  [16, 64, 8, 8]            --
│    └─Empty: 2-595                      [16, 64, 8, 8]            --
│    └─Empty: 2-596                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-597         --                        --
│    └─One: 2-598                        [1]                       --
│    └─OutputScale: 2-599                --                        --
│    └─Empty: 2-600                      [64, 64, 3, 3]            --
│    └─Empty: 2-601                      [64, 64, 3, 3]            --
│    └─Empty: 2-602                      [64]                      --
│    └─Empty: 2-603                      [64]                      --
│    └─BatchNorm2d: 2-604                [16, 64, 8, 8]            --
│    └─Scaler: 2-605                     [16, 64, 8, 8]            --
│    └─ReLU: 2-606                       [16, 64, 8, 8]            --
│    └─Empty: 2-607                      [16, 64, 8, 8]            --
│    └─Clamp: 2-608                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-609                  [16, 64, 4, 4]            --
│    └─Empty: 2-610                      [16, 64, 4, 4]            --
│    └─Empty: 2-611                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-612         --                        --
│    └─One: 2-613                        [1]                       --
│    └─OutputScale: 2-614                --                        --
│    └─Empty: 2-615                      [64, 64, 3, 3]            --
│    └─Empty: 2-616                      [64, 64, 3, 3]            --
│    └─Empty: 2-617                      [64]                      --
│    └─Empty: 2-618                      [64]                      --
│    └─BatchNorm2d: 2-619                [16, 64, 4, 4]            --
│    └─Scaler: 2-620                     [16, 64, 4, 4]            --
│    └─ReLU: 2-621                       [16, 64, 4, 4]            --
│    └─Empty: 2-622                      [16, 64, 4, 4]            --
│    └─Clamp: 2-623                      [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-47                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-624         --                        --
│    └─One: 2-625                        [1]                       --
│    └─OutputScale: 2-626                --                        --
│    └─Empty: 2-627                      [64, 64, 1, 1]            --
│    └─Empty: 2-628                      [64, 64, 1, 1]            --
│    └─Empty: 2-629                      [64]                      --
│    └─Empty: 2-630                      [64]                      --
│    └─BatchNorm2d: 2-631                [16, 64, 4, 4]            --
│    └─Scaler: 2-632                     [16, 64, 4, 4]            --
│    └─ReLU: 2-633                       [16, 64, 4, 4]            --
│    └─Empty: 2-634                      [16, 64, 4, 4]            --
│    └─Clamp: 2-635                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-636                  [16, 64, 4, 4]            --
│    └─Empty: 2-637                      [16, 64, 4, 4]            --
│    └─Empty: 2-638                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-639         --                        --
│    └─One: 2-640                        [1]                       --
│    └─OutputScale: 2-641                --                        --
│    └─Empty: 2-642                      [64, 64, 3, 3]            --
│    └─Empty: 2-643                      [64, 64, 3, 3]            --
│    └─Empty: 2-644                      [64]                      --
│    └─Empty: 2-645                      [64]                      --
│    └─BatchNorm2d: 2-646                [16, 64, 4, 4]            --
│    └─Scaler: 2-647                     [16, 64, 4, 4]            --
│    └─ReLU: 2-648                       [16, 64, 4, 4]            --
│    └─Empty: 2-649                      [16, 64, 4, 4]            --
│    └─Clamp: 2-650                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-49         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-651                  [16, 64, 2, 2]            --
│    └─Empty: 2-652                      [16, 64, 2, 2]            --
│    └─Empty: 2-653                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-654         --                        --
│    └─One: 2-655                        [1]                       --
│    └─OutputScale: 2-656                --                        --
│    └─Empty: 2-657                      [64, 64, 1, 1]            --
│    └─Empty: 2-658                      [64, 64, 1, 1]            --
│    └─Empty: 2-659                      [64]                      --
│    └─Empty: 2-660                      [64]                      --
│    └─BatchNorm2d: 2-661                [16, 64, 2, 2]            --
│    └─Scaler: 2-662                     [16, 64, 2, 2]            --
│    └─ReLU: 2-663                       [16, 64, 2, 2]            --
│    └─Empty: 2-664                      [16, 64, 2, 2]            --
│    └─Clamp: 2-665                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-50                [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-666         --                        --
│    └─One: 2-667                        [1]                       --
│    └─OutputScale: 2-668                --                        --
│    └─Empty: 2-669                      [64, 64, 1, 1]            --
│    └─Empty: 2-670                      [64, 64, 1, 1]            --
│    └─Empty: 2-671                      [64]                      --
│    └─Empty: 2-672                      [64]                      --
│    └─BatchNorm2d: 2-673                [16, 64, 2, 2]            --
│    └─Scaler: 2-674                     [16, 64, 2, 2]            --
│    └─ReLU: 2-675                       [16, 64, 2, 2]            --
│    └─Empty: 2-676                      [16, 64, 2, 2]            --
│    └─Clamp: 2-677                      [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-678                  [16, 64, 2, 2]            --
│    └─Empty: 2-679                      [16, 64, 2, 2]            --
│    └─Empty: 2-680                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-681         --                        --
│    └─One: 2-682                        [1]                       --
│    └─OutputScale: 2-683                --                        --
│    └─Empty: 2-684                      [64, 64, 3, 3]            --
│    └─Empty: 2-685                      [64, 64, 3, 3]            --
│    └─Empty: 2-686                      [64]                      --
│    └─Empty: 2-687                      [64]                      --
│    └─BatchNorm2d: 2-688                [16, 64, 2, 2]            --
│    └─Scaler: 2-689                     [16, 64, 2, 2]            --
│    └─ReLU: 2-690                       [16, 64, 2, 2]            --
│    └─Empty: 2-691                      [16, 64, 2, 2]            --
│    └─Clamp: 2-692                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-52                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-693         --                        --
│    └─One: 2-694                        [1]                       --
│    └─OutputScale: 2-695                --                        --
│    └─Empty: 2-696                      [64, 48, 1, 1]            --
│    └─Empty: 2-697                      [64, 48, 1, 1]            --
│    └─Empty: 2-698                      [64]                      --
│    └─Empty: 2-699                      [64]                      --
│    └─BatchNorm2d: 2-700                [16, 64, 64, 64]          --
│    └─Scaler: 2-701                     [16, 64, 64, 64]          --
│    └─ReLU: 2-702                       [16, 64, 64, 64]          --
│    └─Empty: 2-703                      [16, 64, 64, 64]          --
│    └─Clamp: 2-704                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-53                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-705         --                        --
│    └─One: 2-706                        [1]                       --
│    └─OutputScale: 2-707                --                        --
│    └─Empty: 2-708                      [64, 64, 3, 3]            --
│    └─Empty: 2-709                      [64, 64, 3, 3]            --
│    └─Empty: 2-710                      [64]                      --
│    └─Empty: 2-711                      [64]                      --
│    └─BatchNorm2d: 2-712                [16, 64, 64, 64]          --
│    └─Scaler: 2-713                     [16, 64, 64, 64]          --
│    └─ReLU: 2-714                       [16, 64, 64, 64]          --
│    └─Empty: 2-715                      [16, 64, 64, 64]          --
│    └─Clamp: 2-716                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-54                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-717         --                        --
│    └─One: 2-718                        [1]                       --
│    └─OutputScale: 2-719                --                        --
│    └─Empty: 2-720                      [64, 64, 1, 1]            --
│    └─Empty: 2-721                      [64, 64, 1, 1]            --
│    └─Empty: 2-722                      [64]                      --
│    └─Empty: 2-723                      [64]                      --
│    └─BatchNorm2d: 2-724                [16, 64, 64, 64]          --
│    └─Scaler: 2-725                     [16, 64, 64, 64]          --
│    └─ReLU: 2-726                       [16, 64, 64, 64]          --
│    └─Empty: 2-727                      [16, 64, 64, 64]          --
│    └─Clamp: 2-728                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-55                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-729         --                        --
│    └─One: 2-730                        [1]                       --
│    └─OutputScale: 2-731                --                        --
│    └─Empty: 2-732                      [64, 64, 3, 3]            --
│    └─Empty: 2-733                      [64, 64, 3, 3]            --
│    └─Empty: 2-734                      [64]                      --
│    └─Empty: 2-735                      [64]                      --
│    └─BatchNorm2d: 2-736                [16, 64, 64, 64]          --
│    └─Scaler: 2-737                     [16, 64, 64, 64]          --
│    └─ReLU: 2-738                       [16, 64, 64, 64]          --
│    └─Empty: 2-739                      [16, 64, 64, 64]          --
│    └─Clamp: 2-740                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-56         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-741                  [16, 64, 32, 32]          --
│    └─Empty: 2-742                      [16, 64, 32, 32]          --
│    └─Empty: 2-743                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-744         --                        --
│    └─One: 2-745                        [1]                       --
│    └─OutputScale: 2-746                --                        --
│    └─Empty: 2-747                      [64, 64, 3, 3]            --
│    └─Empty: 2-748                      [64, 64, 3, 3]            --
│    └─Empty: 2-749                      [64]                      --
│    └─Empty: 2-750                      [64]                      --
│    └─BatchNorm2d: 2-751                [16, 64, 32, 32]          --
│    └─Scaler: 2-752                     [16, 64, 32, 32]          --
│    └─ReLU: 2-753                       [16, 64, 32, 32]          --
│    └─Empty: 2-754                      [16, 64, 32, 32]          --
│    └─Clamp: 2-755                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-57                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-756         --                        --
│    └─One: 2-757                        [1]                       --
│    └─OutputScale: 2-758                --                        --
│    └─Empty: 2-759                      [64, 64, 3, 3]            --
│    └─Empty: 2-760                      [64, 64, 3, 3]            --
│    └─Empty: 2-761                      [64]                      --
│    └─Empty: 2-762                      [64]                      --
│    └─BatchNorm2d: 2-763                [16, 64, 32, 32]          --
│    └─Scaler: 2-764                     [16, 64, 32, 32]          --
│    └─ReLU: 2-765                       [16, 64, 32, 32]          --
│    └─Empty: 2-766                      [16, 64, 32, 32]          --
│    └─Clamp: 2-767                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-58         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-768                  [16, 64, 16, 16]          --
│    └─Empty: 2-769                      [16, 64, 16, 16]          --
│    └─Empty: 2-770                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-771         --                        --
│    └─One: 2-772                        [1]                       --
│    └─OutputScale: 2-773                --                        --
│    └─Empty: 2-774                      [64, 64, 3, 3]            --
│    └─Empty: 2-775                      [64, 64, 3, 3]            --
│    └─Empty: 2-776                      [64]                      --
│    └─Empty: 2-777                      [64]                      --
│    └─BatchNorm2d: 2-778                [16, 64, 16, 16]          --
│    └─Scaler: 2-779                     [16, 64, 16, 16]          --
│    └─ReLU: 2-780                       [16, 64, 16, 16]          --
│    └─Empty: 2-781                      [16, 64, 16, 16]          --
│    └─Clamp: 2-782                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-59                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-783         --                        --
│    └─One: 2-784                        [1]                       --
│    └─OutputScale: 2-785                --                        --
│    └─Empty: 2-786                      [64, 64, 3, 3]            --
│    └─Empty: 2-787                      [64, 64, 3, 3]            --
│    └─Empty: 2-788                      [64]                      --
│    └─Empty: 2-789                      [64]                      --
│    └─BatchNorm2d: 2-790                [16, 64, 16, 16]          --
│    └─Scaler: 2-791                     [16, 64, 16, 16]          --
│    └─ReLU: 2-792                       [16, 64, 16, 16]          --
│    └─Empty: 2-793                      [16, 64, 16, 16]          --
│    └─Clamp: 2-794                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-60         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-795                  [16, 64, 8, 8]            --
│    └─Empty: 2-796                      [16, 64, 8, 8]            --
│    └─Empty: 2-797                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-798         --                        --
│    └─One: 2-799                        [1]                       --
│    └─OutputScale: 2-800                --                        --
│    └─Empty: 2-801                      [64, 64, 3, 3]            --
│    └─Empty: 2-802                      [64, 64, 3, 3]            --
│    └─Empty: 2-803                      [64]                      --
│    └─Empty: 2-804                      [64]                      --
│    └─BatchNorm2d: 2-805                [16, 64, 8, 8]            --
│    └─Scaler: 2-806                     [16, 64, 8, 8]            --
│    └─ReLU: 2-807                       [16, 64, 8, 8]            --
│    └─Empty: 2-808                      [16, 64, 8, 8]            --
│    └─Clamp: 2-809                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-61                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-810         --                        --
│    └─One: 2-811                        [1]                       --
│    └─OutputScale: 2-812                --                        --
│    └─Empty: 2-813                      [64, 64, 1, 1]            --
│    └─Empty: 2-814                      [64, 64, 1, 1]            --
│    └─Empty: 2-815                      [64]                      --
│    └─Empty: 2-816                      [64]                      --
│    └─BatchNorm2d: 2-817                [16, 64, 8, 8]            --
│    └─Scaler: 2-818                     [16, 64, 8, 8]            --
│    └─ReLU: 2-819                       [16, 64, 8, 8]            --
│    └─Empty: 2-820                      [16, 64, 8, 8]            --
│    └─Clamp: 2-821                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-62         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-822                  [16, 64, 8, 8]            --
│    └─Empty: 2-823                      [16, 64, 8, 8]            --
│    └─Empty: 2-824                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-825         --                        --
│    └─One: 2-826                        [1]                       --
│    └─OutputScale: 2-827                --                        --
│    └─Empty: 2-828                      [64, 64, 3, 3]            --
│    └─Empty: 2-829                      [64, 64, 3, 3]            --
│    └─Empty: 2-830                      [64]                      --
│    └─Empty: 2-831                      [64]                      --
│    └─BatchNorm2d: 2-832                [16, 64, 8, 8]            --
│    └─Scaler: 2-833                     [16, 64, 8, 8]            --
│    └─ReLU: 2-834                       [16, 64, 8, 8]            --
│    └─Empty: 2-835                      [16, 64, 8, 8]            --
│    └─Clamp: 2-836                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-63         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-837                  [16, 64, 4, 4]            --
│    └─Empty: 2-838                      [16, 64, 4, 4]            --
│    └─Empty: 2-839                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-840         --                        --
│    └─One: 2-841                        [1]                       --
│    └─OutputScale: 2-842                --                        --
│    └─Empty: 2-843                      [64, 64, 3, 3]            --
│    └─Empty: 2-844                      [64, 64, 3, 3]            --
│    └─Empty: 2-845                      [64]                      --
│    └─Empty: 2-846                      [64]                      --
│    └─BatchNorm2d: 2-847                [16, 64, 4, 4]            --
│    └─Scaler: 2-848                     [16, 64, 4, 4]            --
│    └─ReLU: 2-849                       [16, 64, 4, 4]            --
│    └─Empty: 2-850                      [16, 64, 4, 4]            --
│    └─Clamp: 2-851                      [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-64                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-852         --                        --
│    └─One: 2-853                        [1]                       --
│    └─OutputScale: 2-854                --                        --
│    └─Empty: 2-855                      [64, 64, 1, 1]            --
│    └─Empty: 2-856                      [64, 64, 1, 1]            --
│    └─Empty: 2-857                      [64]                      --
│    └─Empty: 2-858                      [64]                      --
│    └─BatchNorm2d: 2-859                [16, 64, 4, 4]            --
│    └─Scaler: 2-860                     [16, 64, 4, 4]            --
│    └─ReLU: 2-861                       [16, 64, 4, 4]            --
│    └─Empty: 2-862                      [16, 64, 4, 4]            --
│    └─Clamp: 2-863                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-65         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-864                  [16, 64, 4, 4]            --
│    └─Empty: 2-865                      [16, 64, 4, 4]            --
│    └─Empty: 2-866                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-867         --                        --
│    └─One: 2-868                        [1]                       --
│    └─OutputScale: 2-869                --                        --
│    └─Empty: 2-870                      [64, 64, 3, 3]            --
│    └─Empty: 2-871                      [64, 64, 3, 3]            --
│    └─Empty: 2-872                      [64]                      --
│    └─Empty: 2-873                      [64]                      --
│    └─BatchNorm2d: 2-874                [16, 64, 4, 4]            --
│    └─Scaler: 2-875                     [16, 64, 4, 4]            --
│    └─ReLU: 2-876                       [16, 64, 4, 4]            --
│    └─Empty: 2-877                      [16, 64, 4, 4]            --
│    └─Clamp: 2-878                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-879                  [16, 64, 2, 2]            --
│    └─Empty: 2-880                      [16, 64, 2, 2]            --
│    └─Empty: 2-881                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-882         --                        --
│    └─One: 2-883                        [1]                       --
│    └─OutputScale: 2-884                --                        --
│    └─Empty: 2-885                      [64, 64, 1, 1]            --
│    └─Empty: 2-886                      [64, 64, 1, 1]            --
│    └─Empty: 2-887                      [64]                      --
│    └─Empty: 2-888                      [64]                      --
│    └─BatchNorm2d: 2-889                [16, 64, 2, 2]            --
│    └─Scaler: 2-890                     [16, 64, 2, 2]            --
│    └─ReLU: 2-891                       [16, 64, 2, 2]            --
│    └─Empty: 2-892                      [16, 64, 2, 2]            --
│    └─Clamp: 2-893                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-67                [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-894         --                        --
│    └─One: 2-895                        [1]                       --
│    └─OutputScale: 2-896                --                        --
│    └─Empty: 2-897                      [64, 64, 1, 1]            --
│    └─Empty: 2-898                      [64, 64, 1, 1]            --
│    └─Empty: 2-899                      [64]                      --
│    └─Empty: 2-900                      [64]                      --
│    └─BatchNorm2d: 2-901                [16, 64, 2, 2]            --
│    └─Scaler: 2-902                     [16, 64, 2, 2]            --
│    └─ReLU: 2-903                       [16, 64, 2, 2]            --
│    └─Empty: 2-904                      [16, 64, 2, 2]            --
│    └─Clamp: 2-905                      [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-906                  [16, 64, 2, 2]            --
│    └─Empty: 2-907                      [16, 64, 2, 2]            --
│    └─Empty: 2-908                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-909         --                        --
│    └─One: 2-910                        [1]                       --
│    └─OutputScale: 2-911                --                        --
│    └─Empty: 2-912                      [64, 64, 3, 3]            --
│    └─Empty: 2-913                      [64, 64, 3, 3]            --
│    └─Empty: 2-914                      [64]                      --
│    └─Empty: 2-915                      [64]                      --
│    └─BatchNorm2d: 2-916                [16, 64, 2, 2]            --
│    └─Scaler: 2-917                     [16, 64, 2, 2]            --
│    └─ReLU: 2-918                       [16, 64, 2, 2]            --
│    └─Empty: 2-919                      [16, 64, 2, 2]            --
│    └─Clamp: 2-920                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-69                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-921         --                        --
│    └─One: 2-922                        [1]                       --
│    └─OutputScale: 2-923                --                        --
│    └─Empty: 2-924                      [64, 48, 1, 1]            --
│    └─Empty: 2-925                      [64, 48, 1, 1]            --
│    └─Empty: 2-926                      [64]                      --
│    └─Empty: 2-927                      [64]                      --
│    └─BatchNorm2d: 2-928                [16, 64, 64, 64]          --
│    └─Scaler: 2-929                     [16, 64, 64, 64]          --
│    └─ReLU: 2-930                       [16, 64, 64, 64]          --
│    └─Empty: 2-931                      [16, 64, 64, 64]          --
│    └─Clamp: 2-932                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-70                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-933         --                        --
│    └─One: 2-934                        [1]                       --
│    └─OutputScale: 2-935                --                        --
│    └─Empty: 2-936                      [64, 64, 3, 3]            --
│    └─Empty: 2-937                      [64, 64, 3, 3]            --
│    └─Empty: 2-938                      [64]                      --
│    └─Empty: 2-939                      [64]                      --
│    └─BatchNorm2d: 2-940                [16, 64, 64, 64]          --
│    └─Scaler: 2-941                     [16, 64, 64, 64]          --
│    └─ReLU: 2-942                       [16, 64, 64, 64]          --
│    └─Empty: 2-943                      [16, 64, 64, 64]          --
│    └─Clamp: 2-944                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-71                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-945         --                        --
│    └─One: 2-946                        [1]                       --
│    └─OutputScale: 2-947                --                        --
│    └─Empty: 2-948                      [64, 64, 1, 1]            --
│    └─Empty: 2-949                      [64, 64, 1, 1]            --
│    └─Empty: 2-950                      [64]                      --
│    └─Empty: 2-951                      [64]                      --
│    └─BatchNorm2d: 2-952                [16, 64, 64, 64]          --
│    └─Scaler: 2-953                     [16, 64, 64, 64]          --
│    └─ReLU: 2-954                       [16, 64, 64, 64]          --
│    └─Empty: 2-955                      [16, 64, 64, 64]          --
│    └─Clamp: 2-956                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-72                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-957         --                        --
│    └─One: 2-958                        [1]                       --
│    └─OutputScale: 2-959                --                        --
│    └─Empty: 2-960                      [64, 64, 3, 3]            --
│    └─Empty: 2-961                      [64, 64, 3, 3]            --
│    └─Empty: 2-962                      [64]                      --
│    └─Empty: 2-963                      [64]                      --
│    └─BatchNorm2d: 2-964                [16, 64, 64, 64]          --
│    └─Scaler: 2-965                     [16, 64, 64, 64]          --
│    └─ReLU: 2-966                       [16, 64, 64, 64]          --
│    └─Empty: 2-967                      [16, 64, 64, 64]          --
│    └─Clamp: 2-968                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-969                  [16, 64, 32, 32]          --
│    └─Empty: 2-970                      [16, 64, 32, 32]          --
│    └─Empty: 2-971                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-972         --                        --
│    └─One: 2-973                        [1]                       --
│    └─OutputScale: 2-974                --                        --
│    └─Empty: 2-975                      [64, 64, 3, 3]            --
│    └─Empty: 2-976                      [64, 64, 3, 3]            --
│    └─Empty: 2-977                      [64]                      --
│    └─Empty: 2-978                      [64]                      --
│    └─BatchNorm2d: 2-979                [16, 64, 32, 32]          --
│    └─Scaler: 2-980                     [16, 64, 32, 32]          --
│    └─ReLU: 2-981                       [16, 64, 32, 32]          --
│    └─Empty: 2-982                      [16, 64, 32, 32]          --
│    └─Clamp: 2-983                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-74                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-984         --                        --
│    └─One: 2-985                        [1]                       --
│    └─OutputScale: 2-986                --                        --
│    └─Empty: 2-987                      [64, 64, 3, 3]            --
│    └─Empty: 2-988                      [64, 64, 3, 3]            --
│    └─Empty: 2-989                      [64]                      --
│    └─Empty: 2-990                      [64]                      --
│    └─BatchNorm2d: 2-991                [16, 64, 32, 32]          --
│    └─Scaler: 2-992                     [16, 64, 32, 32]          --
│    └─ReLU: 2-993                       [16, 64, 32, 32]          --
│    └─Empty: 2-994                      [16, 64, 32, 32]          --
│    └─Clamp: 2-995                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-996                  [16, 64, 16, 16]          --
│    └─Empty: 2-997                      [16, 64, 16, 16]          --
│    └─Empty: 2-998                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-999         --                        --
│    └─One: 2-1000                       [1]                       --
│    └─OutputScale: 2-1001               --                        --
│    └─Empty: 2-1002                     [64, 64, 3, 3]            --
│    └─Empty: 2-1003                     [64, 64, 3, 3]            --
│    └─Empty: 2-1004                     [64]                      --
│    └─Empty: 2-1005                     [64]                      --
│    └─BatchNorm2d: 2-1006               [16, 64, 16, 16]          --
│    └─Scaler: 2-1007                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1008                      [16, 64, 16, 16]          --
│    └─Empty: 2-1009                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1010                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-76                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1011        --                        --
│    └─One: 2-1012                       [1]                       --
│    └─OutputScale: 2-1013               --                        --
│    └─Empty: 2-1014                     [64, 64, 3, 3]            --
│    └─Empty: 2-1015                     [64, 64, 3, 3]            --
│    └─Empty: 2-1016                     [64]                      --
│    └─Empty: 2-1017                     [64]                      --
│    └─BatchNorm2d: 2-1018               [16, 64, 16, 16]          --
│    └─Scaler: 2-1019                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1020                      [16, 64, 16, 16]          --
│    └─Empty: 2-1021                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1022                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-77         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1023                 [16, 64, 8, 8]            --
│    └─Empty: 2-1024                     [16, 64, 8, 8]            --
│    └─Empty: 2-1025                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1026        --                        --
│    └─One: 2-1027                       [1]                       --
│    └─OutputScale: 2-1028               --                        --
│    └─Empty: 2-1029                     [64, 64, 3, 3]            --
│    └─Empty: 2-1030                     [64, 64, 3, 3]            --
│    └─Empty: 2-1031                     [64]                      --
│    └─Empty: 2-1032                     [64]                      --
│    └─BatchNorm2d: 2-1033               [16, 64, 8, 8]            --
│    └─Scaler: 2-1034                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1035                      [16, 64, 8, 8]            --
│    └─Empty: 2-1036                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1037                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-78                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1038        --                        --
│    └─One: 2-1039                       [1]                       --
│    └─OutputScale: 2-1040               --                        --
│    └─Empty: 2-1041                     [64, 64, 1, 1]            --
│    └─Empty: 2-1042                     [64, 64, 1, 1]            --
│    └─Empty: 2-1043                     [64]                      --
│    └─Empty: 2-1044                     [64]                      --
│    └─BatchNorm2d: 2-1045               [16, 64, 8, 8]            --
│    └─Scaler: 2-1046                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1047                      [16, 64, 8, 8]            --
│    └─Empty: 2-1048                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1049                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-79         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1050                 [16, 64, 8, 8]            --
│    └─Empty: 2-1051                     [16, 64, 8, 8]            --
│    └─Empty: 2-1052                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1053        --                        --
│    └─One: 2-1054                       [1]                       --
│    └─OutputScale: 2-1055               --                        --
│    └─Empty: 2-1056                     [64, 64, 3, 3]            --
│    └─Empty: 2-1057                     [64, 64, 3, 3]            --
│    └─Empty: 2-1058                     [64]                      --
│    └─Empty: 2-1059                     [64]                      --
│    └─BatchNorm2d: 2-1060               [16, 64, 8, 8]            --
│    └─Scaler: 2-1061                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1062                      [16, 64, 8, 8]            --
│    └─Empty: 2-1063                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1064                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-80         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1065                 [16, 64, 4, 4]            --
│    └─Empty: 2-1066                     [16, 64, 4, 4]            --
│    └─Empty: 2-1067                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1068        --                        --
│    └─One: 2-1069                       [1]                       --
│    └─OutputScale: 2-1070               --                        --
│    └─Empty: 2-1071                     [64, 64, 3, 3]            --
│    └─Empty: 2-1072                     [64, 64, 3, 3]            --
│    └─Empty: 2-1073                     [64]                      --
│    └─Empty: 2-1074                     [64]                      --
│    └─BatchNorm2d: 2-1075               [16, 64, 4, 4]            --
│    └─Scaler: 2-1076                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1077                      [16, 64, 4, 4]            --
│    └─Empty: 2-1078                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1079                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-81                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1080        --                        --
│    └─One: 2-1081                       [1]                       --
│    └─OutputScale: 2-1082               --                        --
│    └─Empty: 2-1083                     [64, 64, 1, 1]            --
│    └─Empty: 2-1084                     [64, 64, 1, 1]            --
│    └─Empty: 2-1085                     [64]                      --
│    └─Empty: 2-1086                     [64]                      --
│    └─BatchNorm2d: 2-1087               [16, 64, 4, 4]            --
│    └─Scaler: 2-1088                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1089                      [16, 64, 4, 4]            --
│    └─Empty: 2-1090                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1091                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-82         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1092                 [16, 64, 4, 4]            --
│    └─Empty: 2-1093                     [16, 64, 4, 4]            --
│    └─Empty: 2-1094                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1095        --                        --
│    └─One: 2-1096                       [1]                       --
│    └─OutputScale: 2-1097               --                        --
│    └─Empty: 2-1098                     [64, 64, 3, 3]            --
│    └─Empty: 2-1099                     [64, 64, 3, 3]            --
│    └─Empty: 2-1100                     [64]                      --
│    └─Empty: 2-1101                     [64]                      --
│    └─BatchNorm2d: 2-1102               [16, 64, 4, 4]            --
│    └─Scaler: 2-1103                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1104                      [16, 64, 4, 4]            --
│    └─Empty: 2-1105                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1106                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-83         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1107                 [16, 64, 2, 2]            --
│    └─Empty: 2-1108                     [16, 64, 2, 2]            --
│    └─Empty: 2-1109                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1110        --                        --
│    └─One: 2-1111                       [1]                       --
│    └─OutputScale: 2-1112               --                        --
│    └─Empty: 2-1113                     [64, 64, 1, 1]            --
│    └─Empty: 2-1114                     [64, 64, 1, 1]            --
│    └─Empty: 2-1115                     [64]                      --
│    └─Empty: 2-1116                     [64]                      --
│    └─BatchNorm2d: 2-1117               [16, 64, 2, 2]            --
│    └─Scaler: 2-1118                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1119                      [16, 64, 2, 2]            --
│    └─Empty: 2-1120                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1121                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-84                [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-1122        --                        --
│    └─One: 2-1123                       [1]                       --
│    └─OutputScale: 2-1124               --                        --
│    └─Empty: 2-1125                     [64, 64, 1, 1]            --
│    └─Empty: 2-1126                     [64, 64, 1, 1]            --
│    └─Empty: 2-1127                     [64]                      --
│    └─Empty: 2-1128                     [64]                      --
│    └─BatchNorm2d: 2-1129               [16, 64, 2, 2]            --
│    └─Scaler: 2-1130                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1131                      [16, 64, 2, 2]            --
│    └─Empty: 2-1132                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1133                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-85         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1134                 [16, 64, 2, 2]            --
│    └─Empty: 2-1135                     [16, 64, 2, 2]            --
│    └─Empty: 2-1136                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1137        --                        --
│    └─One: 2-1138                       [1]                       --
│    └─OutputScale: 2-1139               --                        --
│    └─Empty: 2-1140                     [64, 64, 3, 3]            --
│    └─Empty: 2-1141                     [64, 64, 3, 3]            --
│    └─Empty: 2-1142                     [64]                      --
│    └─Empty: 2-1143                     [64]                      --
│    └─BatchNorm2d: 2-1144               [16, 64, 2, 2]            --
│    └─Scaler: 2-1145                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1146                      [16, 64, 2, 2]            --
│    └─Empty: 2-1147                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1148                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-86                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1149        --                        --
│    └─One: 2-1150                       [1]                       --
│    └─OutputScale: 2-1151               --                        --
│    └─Empty: 2-1152                     [64, 48, 1, 1]            --
│    └─Empty: 2-1153                     [64, 48, 1, 1]            --
│    └─Empty: 2-1154                     [64]                      --
│    └─Empty: 2-1155                     [64]                      --
│    └─BatchNorm2d: 2-1156               [16, 64, 64, 64]          --
│    └─Scaler: 2-1157                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1158                      [16, 64, 64, 64]          --
│    └─Empty: 2-1159                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1160                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-87                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1161        --                        --
│    └─One: 2-1162                       [1]                       --
│    └─OutputScale: 2-1163               --                        --
│    └─Empty: 2-1164                     [64, 64, 3, 3]            --
│    └─Empty: 2-1165                     [64, 64, 3, 3]            --
│    └─Empty: 2-1166                     [64]                      --
│    └─Empty: 2-1167                     [64]                      --
│    └─BatchNorm2d: 2-1168               [16, 64, 64, 64]          --
│    └─Scaler: 2-1169                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1170                      [16, 64, 64, 64]          --
│    └─Empty: 2-1171                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1172                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-88                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1173        --                        --
│    └─One: 2-1174                       [1]                       --
│    └─OutputScale: 2-1175               --                        --
│    └─Empty: 2-1176                     [64, 64, 1, 1]            --
│    └─Empty: 2-1177                     [64, 64, 1, 1]            --
│    └─Empty: 2-1178                     [64]                      --
│    └─Empty: 2-1179                     [64]                      --
│    └─BatchNorm2d: 2-1180               [16, 64, 64, 64]          --
│    └─Scaler: 2-1181                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1182                      [16, 64, 64, 64]          --
│    └─Empty: 2-1183                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1184                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-89                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1185        --                        --
│    └─One: 2-1186                       [1]                       --
│    └─OutputScale: 2-1187               --                        --
│    └─Empty: 2-1188                     [64, 64, 3, 3]            --
│    └─Empty: 2-1189                     [64, 64, 3, 3]            --
│    └─Empty: 2-1190                     [64]                      --
│    └─Empty: 2-1191                     [64]                      --
│    └─BatchNorm2d: 2-1192               [16, 64, 64, 64]          --
│    └─Scaler: 2-1193                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1194                      [16, 64, 64, 64]          --
│    └─Empty: 2-1195                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1196                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-90         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1197                 [16, 64, 32, 32]          --
│    └─Empty: 2-1198                     [16, 64, 32, 32]          --
│    └─Empty: 2-1199                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1200        --                        --
│    └─One: 2-1201                       [1]                       --
│    └─OutputScale: 2-1202               --                        --
│    └─Empty: 2-1203                     [64, 64, 3, 3]            --
│    └─Empty: 2-1204                     [64, 64, 3, 3]            --
│    └─Empty: 2-1205                     [64]                      --
│    └─Empty: 2-1206                     [64]                      --
│    └─BatchNorm2d: 2-1207               [16, 64, 32, 32]          --
│    └─Scaler: 2-1208                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1209                      [16, 64, 32, 32]          --
│    └─Empty: 2-1210                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1211                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-91                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1212        --                        --
│    └─One: 2-1213                       [1]                       --
│    └─OutputScale: 2-1214               --                        --
│    └─Empty: 2-1215                     [64, 64, 3, 3]            --
│    └─Empty: 2-1216                     [64, 64, 3, 3]            --
│    └─Empty: 2-1217                     [64]                      --
│    └─Empty: 2-1218                     [64]                      --
│    └─BatchNorm2d: 2-1219               [16, 64, 32, 32]          --
│    └─Scaler: 2-1220                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1221                      [16, 64, 32, 32]          --
│    └─Empty: 2-1222                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1223                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-92         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1224                 [16, 64, 16, 16]          --
│    └─Empty: 2-1225                     [16, 64, 16, 16]          --
│    └─Empty: 2-1226                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1227        --                        --
│    └─One: 2-1228                       [1]                       --
│    └─OutputScale: 2-1229               --                        --
│    └─Empty: 2-1230                     [64, 64, 3, 3]            --
│    └─Empty: 2-1231                     [64, 64, 3, 3]            --
│    └─Empty: 2-1232                     [64]                      --
│    └─Empty: 2-1233                     [64]                      --
│    └─BatchNorm2d: 2-1234               [16, 64, 16, 16]          --
│    └─Scaler: 2-1235                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1236                      [16, 64, 16, 16]          --
│    └─Empty: 2-1237                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1238                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-93                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1239        --                        --
│    └─One: 2-1240                       [1]                       --
│    └─OutputScale: 2-1241               --                        --
│    └─Empty: 2-1242                     [64, 64, 3, 3]            --
│    └─Empty: 2-1243                     [64, 64, 3, 3]            --
│    └─Empty: 2-1244                     [64]                      --
│    └─Empty: 2-1245                     [64]                      --
│    └─BatchNorm2d: 2-1246               [16, 64, 16, 16]          --
│    └─Scaler: 2-1247                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1248                      [16, 64, 16, 16]          --
│    └─Empty: 2-1249                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1250                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-94         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1251                 [16, 64, 8, 8]            --
│    └─Empty: 2-1252                     [16, 64, 8, 8]            --
│    └─Empty: 2-1253                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1254        --                        --
│    └─One: 2-1255                       [1]                       --
│    └─OutputScale: 2-1256               --                        --
│    └─Empty: 2-1257                     [64, 64, 3, 3]            --
│    └─Empty: 2-1258                     [64, 64, 3, 3]            --
│    └─Empty: 2-1259                     [64]                      --
│    └─Empty: 2-1260                     [64]                      --
│    └─BatchNorm2d: 2-1261               [16, 64, 8, 8]            --
│    └─Scaler: 2-1262                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1263                      [16, 64, 8, 8]            --
│    └─Empty: 2-1264                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1265                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-95                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1266        --                        --
│    └─One: 2-1267                       [1]                       --
│    └─OutputScale: 2-1268               --                        --
│    └─Empty: 2-1269                     [64, 64, 1, 1]            --
│    └─Empty: 2-1270                     [64, 64, 1, 1]            --
│    └─Empty: 2-1271                     [64]                      --
│    └─Empty: 2-1272                     [64]                      --
│    └─BatchNorm2d: 2-1273               [16, 64, 8, 8]            --
│    └─Scaler: 2-1274                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1275                      [16, 64, 8, 8]            --
│    └─Empty: 2-1276                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1277                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-96         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1278                 [16, 64, 8, 8]            --
│    └─Empty: 2-1279                     [16, 64, 8, 8]            --
│    └─Empty: 2-1280                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1281        --                        --
│    └─One: 2-1282                       [1]                       --
│    └─OutputScale: 2-1283               --                        --
│    └─Empty: 2-1284                     [64, 64, 3, 3]            --
│    └─Empty: 2-1285                     [64, 64, 3, 3]            --
│    └─Empty: 2-1286                     [64]                      --
│    └─Empty: 2-1287                     [64]                      --
│    └─BatchNorm2d: 2-1288               [16, 64, 8, 8]            --
│    └─Scaler: 2-1289                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1290                      [16, 64, 8, 8]            --
│    └─Empty: 2-1291                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1292                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-97         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1293                 [16, 64, 4, 4]            --
│    └─Empty: 2-1294                     [16, 64, 4, 4]            --
│    └─Empty: 2-1295                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1296        --                        --
│    └─One: 2-1297                       [1]                       --
│    └─OutputScale: 2-1298               --                        --
│    └─Empty: 2-1299                     [64, 64, 3, 3]            --
│    └─Empty: 2-1300                     [64, 64, 3, 3]            --
│    └─Empty: 2-1301                     [64]                      --
│    └─Empty: 2-1302                     [64]                      --
│    └─BatchNorm2d: 2-1303               [16, 64, 4, 4]            --
│    └─Scaler: 2-1304                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1305                      [16, 64, 4, 4]            --
│    └─Empty: 2-1306                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1307                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-98                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1308        --                        --
│    └─One: 2-1309                       [1]                       --
│    └─OutputScale: 2-1310               --                        --
│    └─Empty: 2-1311                     [64, 64, 1, 1]            --
│    └─Empty: 2-1312                     [64, 64, 1, 1]            --
│    └─Empty: 2-1313                     [64]                      --
│    └─Empty: 2-1314                     [64]                      --
│    └─BatchNorm2d: 2-1315               [16, 64, 4, 4]            --
│    └─Scaler: 2-1316                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1317                      [16, 64, 4, 4]            --
│    └─Empty: 2-1318                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1319                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-99         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1320                 [16, 64, 4, 4]            --
│    └─Empty: 2-1321                     [16, 64, 4, 4]            --
│    └─Empty: 2-1322                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1323        --                        --
│    └─One: 2-1324                       [1]                       --
│    └─OutputScale: 2-1325               --                        --
│    └─Empty: 2-1326                     [64, 64, 3, 3]            --
│    └─Empty: 2-1327                     [64, 64, 3, 3]            --
│    └─Empty: 2-1328                     [64]                      --
│    └─Empty: 2-1329                     [64]                      --
│    └─BatchNorm2d: 2-1330               [16, 64, 4, 4]            --
│    └─Scaler: 2-1331                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1332                      [16, 64, 4, 4]            --
│    └─Empty: 2-1333                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1334                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-100        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1335                 [16, 64, 2, 2]            --
│    └─Empty: 2-1336                     [16, 64, 2, 2]            --
│    └─Empty: 2-1337                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1338        --                        --
│    └─One: 2-1339                       [1]                       --
│    └─OutputScale: 2-1340               --                        --
│    └─Empty: 2-1341                     [64, 64, 1, 1]            --
│    └─Empty: 2-1342                     [64, 64, 1, 1]            --
│    └─Empty: 2-1343                     [64]                      --
│    └─Empty: 2-1344                     [64]                      --
│    └─BatchNorm2d: 2-1345               [16, 64, 2, 2]            --
│    └─Scaler: 2-1346                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1347                      [16, 64, 2, 2]            --
│    └─Empty: 2-1348                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1349                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-101               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-1350        --                        --
│    └─One: 2-1351                       [1]                       --
│    └─OutputScale: 2-1352               --                        --
│    └─Empty: 2-1353                     [64, 64, 1, 1]            --
│    └─Empty: 2-1354                     [64, 64, 1, 1]            --
│    └─Empty: 2-1355                     [64]                      --
│    └─Empty: 2-1356                     [64]                      --
│    └─BatchNorm2d: 2-1357               [16, 64, 2, 2]            --
│    └─Scaler: 2-1358                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1359                      [16, 64, 2, 2]            --
│    └─Empty: 2-1360                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1361                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-102        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1362                 [16, 64, 2, 2]            --
│    └─Empty: 2-1363                     [16, 64, 2, 2]            --
│    └─Empty: 2-1364                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1365        --                        --
│    └─One: 2-1366                       [1]                       --
│    └─OutputScale: 2-1367               --                        --
│    └─Empty: 2-1368                     [64, 64, 3, 3]            --
│    └─Empty: 2-1369                     [64, 64, 3, 3]            --
│    └─Empty: 2-1370                     [64]                      --
│    └─Empty: 2-1371                     [64]                      --
│    └─BatchNorm2d: 2-1372               [16, 64, 2, 2]            --
│    └─Scaler: 2-1373                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1374                      [16, 64, 2, 2]            --
│    └─Empty: 2-1375                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1376                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-103               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1377        --                        --
│    └─One: 2-1378                       [1]                       --
│    └─OutputScale: 2-1379               --                        --
│    └─Empty: 2-1380                     [64, 48, 1, 1]            --
│    └─Empty: 2-1381                     [64, 48, 1, 1]            --
│    └─Empty: 2-1382                     [64]                      --
│    └─Empty: 2-1383                     [64]                      --
│    └─BatchNorm2d: 2-1384               [16, 64, 64, 64]          --
│    └─Scaler: 2-1385                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1386                      [16, 64, 64, 64]          --
│    └─Empty: 2-1387                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1388                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-104               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1389        --                        --
│    └─One: 2-1390                       [1]                       --
│    └─OutputScale: 2-1391               --                        --
│    └─Empty: 2-1392                     [64, 64, 3, 3]            --
│    └─Empty: 2-1393                     [64, 64, 3, 3]            --
│    └─Empty: 2-1394                     [64]                      --
│    └─Empty: 2-1395                     [64]                      --
│    └─BatchNorm2d: 2-1396               [16, 64, 64, 64]          --
│    └─Scaler: 2-1397                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1398                      [16, 64, 64, 64]          --
│    └─Empty: 2-1399                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1400                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-105               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1401        --                        --
│    └─One: 2-1402                       [1]                       --
│    └─OutputScale: 2-1403               --                        --
│    └─Empty: 2-1404                     [64, 64, 1, 1]            --
│    └─Empty: 2-1405                     [64, 64, 1, 1]            --
│    └─Empty: 2-1406                     [64]                      --
│    └─Empty: 2-1407                     [64]                      --
│    └─BatchNorm2d: 2-1408               [16, 64, 64, 64]          --
│    └─Scaler: 2-1409                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1410                      [16, 64, 64, 64]          --
│    └─Empty: 2-1411                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1412                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-106               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1413        --                        --
│    └─One: 2-1414                       [1]                       --
│    └─OutputScale: 2-1415               --                        --
│    └─Empty: 2-1416                     [64, 64, 3, 3]            --
│    └─Empty: 2-1417                     [64, 64, 3, 3]            --
│    └─Empty: 2-1418                     [64]                      --
│    └─Empty: 2-1419                     [64]                      --
│    └─BatchNorm2d: 2-1420               [16, 64, 64, 64]          --
│    └─Scaler: 2-1421                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1422                      [16, 64, 64, 64]          --
│    └─Empty: 2-1423                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1424                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-107        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1425                 [16, 64, 32, 32]          --
│    └─Empty: 2-1426                     [16, 64, 32, 32]          --
│    └─Empty: 2-1427                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1428        --                        --
│    └─One: 2-1429                       [1]                       --
│    └─OutputScale: 2-1430               --                        --
│    └─Empty: 2-1431                     [64, 64, 3, 3]            --
│    └─Empty: 2-1432                     [64, 64, 3, 3]            --
│    └─Empty: 2-1433                     [64]                      --
│    └─Empty: 2-1434                     [64]                      --
│    └─BatchNorm2d: 2-1435               [16, 64, 32, 32]          --
│    └─Scaler: 2-1436                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1437                      [16, 64, 32, 32]          --
│    └─Empty: 2-1438                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1439                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-108               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1440        --                        --
│    └─One: 2-1441                       [1]                       --
│    └─OutputScale: 2-1442               --                        --
│    └─Empty: 2-1443                     [64, 64, 3, 3]            --
│    └─Empty: 2-1444                     [64, 64, 3, 3]            --
│    └─Empty: 2-1445                     [64]                      --
│    └─Empty: 2-1446                     [64]                      --
│    └─BatchNorm2d: 2-1447               [16, 64, 32, 32]          --
│    └─Scaler: 2-1448                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1449                      [16, 64, 32, 32]          --
│    └─Empty: 2-1450                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1451                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-109        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1452                 [16, 64, 16, 16]          --
│    └─Empty: 2-1453                     [16, 64, 16, 16]          --
│    └─Empty: 2-1454                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1455        --                        --
│    └─One: 2-1456                       [1]                       --
│    └─OutputScale: 2-1457               --                        --
│    └─Empty: 2-1458                     [64, 64, 3, 3]            --
│    └─Empty: 2-1459                     [64, 64, 3, 3]            --
│    └─Empty: 2-1460                     [64]                      --
│    └─Empty: 2-1461                     [64]                      --
│    └─BatchNorm2d: 2-1462               [16, 64, 16, 16]          --
│    └─Scaler: 2-1463                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1464                      [16, 64, 16, 16]          --
│    └─Empty: 2-1465                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1466                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-110               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1467        --                        --
│    └─One: 2-1468                       [1]                       --
│    └─OutputScale: 2-1469               --                        --
│    └─Empty: 2-1470                     [64, 64, 3, 3]            --
│    └─Empty: 2-1471                     [64, 64, 3, 3]            --
│    └─Empty: 2-1472                     [64]                      --
│    └─Empty: 2-1473                     [64]                      --
│    └─BatchNorm2d: 2-1474               [16, 64, 16, 16]          --
│    └─Scaler: 2-1475                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1476                      [16, 64, 16, 16]          --
│    └─Empty: 2-1477                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1478                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-111        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1479                 [16, 64, 8, 8]            --
│    └─Empty: 2-1480                     [16, 64, 8, 8]            --
│    └─Empty: 2-1481                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1482        --                        --
│    └─One: 2-1483                       [1]                       --
│    └─OutputScale: 2-1484               --                        --
│    └─Empty: 2-1485                     [64, 64, 3, 3]            --
│    └─Empty: 2-1486                     [64, 64, 3, 3]            --
│    └─Empty: 2-1487                     [64]                      --
│    └─Empty: 2-1488                     [64]                      --
│    └─BatchNorm2d: 2-1489               [16, 64, 8, 8]            --
│    └─Scaler: 2-1490                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1491                      [16, 64, 8, 8]            --
│    └─Empty: 2-1492                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1493                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-112               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1494        --                        --
│    └─One: 2-1495                       [1]                       --
│    └─OutputScale: 2-1496               --                        --
│    └─Empty: 2-1497                     [64, 64, 1, 1]            --
│    └─Empty: 2-1498                     [64, 64, 1, 1]            --
│    └─Empty: 2-1499                     [64]                      --
│    └─Empty: 2-1500                     [64]                      --
│    └─BatchNorm2d: 2-1501               [16, 64, 8, 8]            --
│    └─Scaler: 2-1502                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1503                      [16, 64, 8, 8]            --
│    └─Empty: 2-1504                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1505                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-113        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1506                 [16, 64, 8, 8]            --
│    └─Empty: 2-1507                     [16, 64, 8, 8]            --
│    └─Empty: 2-1508                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1509        --                        --
│    └─One: 2-1510                       [1]                       --
│    └─OutputScale: 2-1511               --                        --
│    └─Empty: 2-1512                     [64, 64, 3, 3]            --
│    └─Empty: 2-1513                     [64, 64, 3, 3]            --
│    └─Empty: 2-1514                     [64]                      --
│    └─Empty: 2-1515                     [64]                      --
│    └─BatchNorm2d: 2-1516               [16, 64, 8, 8]            --
│    └─Scaler: 2-1517                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1518                      [16, 64, 8, 8]            --
│    └─Empty: 2-1519                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1520                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-114        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1521                 [16, 64, 4, 4]            --
│    └─Empty: 2-1522                     [16, 64, 4, 4]            --
│    └─Empty: 2-1523                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1524        --                        --
│    └─One: 2-1525                       [1]                       --
│    └─OutputScale: 2-1526               --                        --
│    └─Empty: 2-1527                     [64, 64, 3, 3]            --
│    └─Empty: 2-1528                     [64, 64, 3, 3]            --
│    └─Empty: 2-1529                     [64]                      --
│    └─Empty: 2-1530                     [64]                      --
│    └─BatchNorm2d: 2-1531               [16, 64, 4, 4]            --
│    └─Scaler: 2-1532                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1533                      [16, 64, 4, 4]            --
│    └─Empty: 2-1534                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1535                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-115               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1536        --                        --
│    └─One: 2-1537                       [1]                       --
│    └─OutputScale: 2-1538               --                        --
│    └─Empty: 2-1539                     [64, 64, 1, 1]            --
│    └─Empty: 2-1540                     [64, 64, 1, 1]            --
│    └─Empty: 2-1541                     [64]                      --
│    └─Empty: 2-1542                     [64]                      --
│    └─BatchNorm2d: 2-1543               [16, 64, 4, 4]            --
│    └─Scaler: 2-1544                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1545                      [16, 64, 4, 4]            --
│    └─Empty: 2-1546                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1547                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-116        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1548                 [16, 64, 4, 4]            --
│    └─Empty: 2-1549                     [16, 64, 4, 4]            --
│    └─Empty: 2-1550                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1551        --                        --
│    └─One: 2-1552                       [1]                       --
│    └─OutputScale: 2-1553               --                        --
│    └─Empty: 2-1554                     [64, 64, 3, 3]            --
│    └─Empty: 2-1555                     [64, 64, 3, 3]            --
│    └─Empty: 2-1556                     [64]                      --
│    └─Empty: 2-1557                     [64]                      --
│    └─BatchNorm2d: 2-1558               [16, 64, 4, 4]            --
│    └─Scaler: 2-1559                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1560                      [16, 64, 4, 4]            --
│    └─Empty: 2-1561                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1562                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-117        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1563                 [16, 64, 2, 2]            --
│    └─Empty: 2-1564                     [16, 64, 2, 2]            --
│    └─Empty: 2-1565                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1566        --                        --
│    └─One: 2-1567                       [1]                       --
│    └─OutputScale: 2-1568               --                        --
│    └─Empty: 2-1569                     [64, 64, 1, 1]            --
│    └─Empty: 2-1570                     [64, 64, 1, 1]            --
│    └─Empty: 2-1571                     [64]                      --
│    └─Empty: 2-1572                     [64]                      --
│    └─BatchNorm2d: 2-1573               [16, 64, 2, 2]            --
│    └─Scaler: 2-1574                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1575                      [16, 64, 2, 2]            --
│    └─Empty: 2-1576                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1577                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-118               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-1578        --                        --
│    └─One: 2-1579                       [1]                       --
│    └─OutputScale: 2-1580               --                        --
│    └─Empty: 2-1581                     [64, 64, 1, 1]            --
│    └─Empty: 2-1582                     [64, 64, 1, 1]            --
│    └─Empty: 2-1583                     [64]                      --
│    └─Empty: 2-1584                     [64]                      --
│    └─BatchNorm2d: 2-1585               [16, 64, 2, 2]            --
│    └─Scaler: 2-1586                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1587                      [16, 64, 2, 2]            --
│    └─Empty: 2-1588                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1589                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-119        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1590                 [16, 64, 2, 2]            --
│    └─Empty: 2-1591                     [16, 64, 2, 2]            --
│    └─Empty: 2-1592                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1593        --                        --
│    └─One: 2-1594                       [1]                       --
│    └─OutputScale: 2-1595               --                        --
│    └─Empty: 2-1596                     [64, 64, 3, 3]            --
│    └─Empty: 2-1597                     [64, 64, 3, 3]            --
│    └─Empty: 2-1598                     [64]                      --
│    └─Empty: 2-1599                     [64]                      --
│    └─BatchNorm2d: 2-1600               [16, 64, 2, 2]            --
│    └─Scaler: 2-1601                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1602                      [16, 64, 2, 2]            --
│    └─Empty: 2-1603                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1604                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-120               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1605        --                        --
│    └─One: 2-1606                       [1]                       --
│    └─OutputScale: 2-1607               --                        --
│    └─Empty: 2-1608                     [64, 48, 1, 1]            --
│    └─Empty: 2-1609                     [64, 48, 1, 1]            --
│    └─Empty: 2-1610                     [64]                      --
│    └─Empty: 2-1611                     [64]                      --
│    └─BatchNorm2d: 2-1612               [16, 64, 64, 64]          --
│    └─Scaler: 2-1613                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1614                      [16, 64, 64, 64]          --
│    └─Empty: 2-1615                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1616                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-121               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1617        --                        --
│    └─One: 2-1618                       [1]                       --
│    └─OutputScale: 2-1619               --                        --
│    └─Empty: 2-1620                     [64, 64, 3, 3]            --
│    └─Empty: 2-1621                     [64, 64, 3, 3]            --
│    └─Empty: 2-1622                     [64]                      --
│    └─Empty: 2-1623                     [64]                      --
│    └─BatchNorm2d: 2-1624               [16, 64, 64, 64]          --
│    └─Scaler: 2-1625                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1626                      [16, 64, 64, 64]          --
│    └─Empty: 2-1627                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1628                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-122               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1629        --                        --
│    └─One: 2-1630                       [1]                       --
│    └─OutputScale: 2-1631               --                        --
│    └─Empty: 2-1632                     [64, 64, 1, 1]            --
│    └─Empty: 2-1633                     [64, 64, 1, 1]            --
│    └─Empty: 2-1634                     [64]                      --
│    └─Empty: 2-1635                     [64]                      --
│    └─BatchNorm2d: 2-1636               [16, 64, 64, 64]          --
│    └─Scaler: 2-1637                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1638                      [16, 64, 64, 64]          --
│    └─Empty: 2-1639                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1640                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-123               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1641        --                        --
│    └─One: 2-1642                       [1]                       --
│    └─OutputScale: 2-1643               --                        --
│    └─Empty: 2-1644                     [64, 64, 3, 3]            --
│    └─Empty: 2-1645                     [64, 64, 3, 3]            --
│    └─Empty: 2-1646                     [64]                      --
│    └─Empty: 2-1647                     [64]                      --
│    └─BatchNorm2d: 2-1648               [16, 64, 64, 64]          --
│    └─Scaler: 2-1649                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1650                      [16, 64, 64, 64]          --
│    └─Empty: 2-1651                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1652                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-124        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1653                 [16, 64, 32, 32]          --
│    └─Empty: 2-1654                     [16, 64, 32, 32]          --
│    └─Empty: 2-1655                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1656        --                        --
│    └─One: 2-1657                       [1]                       --
│    └─OutputScale: 2-1658               --                        --
│    └─Empty: 2-1659                     [64, 64, 3, 3]            --
│    └─Empty: 2-1660                     [64, 64, 3, 3]            --
│    └─Empty: 2-1661                     [64]                      --
│    └─Empty: 2-1662                     [64]                      --
│    └─BatchNorm2d: 2-1663               [16, 64, 32, 32]          --
│    └─Scaler: 2-1664                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1665                      [16, 64, 32, 32]          --
│    └─Empty: 2-1666                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1667                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-125               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1668        --                        --
│    └─One: 2-1669                       [1]                       --
│    └─OutputScale: 2-1670               --                        --
│    └─Empty: 2-1671                     [64, 64, 3, 3]            --
│    └─Empty: 2-1672                     [64, 64, 3, 3]            --
│    └─Empty: 2-1673                     [64]                      --
│    └─Empty: 2-1674                     [64]                      --
│    └─BatchNorm2d: 2-1675               [16, 64, 32, 32]          --
│    └─Scaler: 2-1676                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1677                      [16, 64, 32, 32]          --
│    └─Empty: 2-1678                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1679                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-126        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1680                 [16, 64, 16, 16]          --
│    └─Empty: 2-1681                     [16, 64, 16, 16]          --
│    └─Empty: 2-1682                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1683        --                        --
│    └─One: 2-1684                       [1]                       --
│    └─OutputScale: 2-1685               --                        --
│    └─Empty: 2-1686                     [64, 64, 3, 3]            --
│    └─Empty: 2-1687                     [64, 64, 3, 3]            --
│    └─Empty: 2-1688                     [64]                      --
│    └─Empty: 2-1689                     [64]                      --
│    └─BatchNorm2d: 2-1690               [16, 64, 16, 16]          --
│    └─Scaler: 2-1691                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1692                      [16, 64, 16, 16]          --
│    └─Empty: 2-1693                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1694                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-127               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1695        --                        --
│    └─One: 2-1696                       [1]                       --
│    └─OutputScale: 2-1697               --                        --
│    └─Empty: 2-1698                     [64, 64, 3, 3]            --
│    └─Empty: 2-1699                     [64, 64, 3, 3]            --
│    └─Empty: 2-1700                     [64]                      --
│    └─Empty: 2-1701                     [64]                      --
│    └─BatchNorm2d: 2-1702               [16, 64, 16, 16]          --
│    └─Scaler: 2-1703                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1704                      [16, 64, 16, 16]          --
│    └─Empty: 2-1705                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1706                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-128        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1707                 [16, 64, 8, 8]            --
│    └─Empty: 2-1708                     [16, 64, 8, 8]            --
│    └─Empty: 2-1709                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1710        --                        --
│    └─One: 2-1711                       [1]                       --
│    └─OutputScale: 2-1712               --                        --
│    └─Empty: 2-1713                     [64, 64, 3, 3]            --
│    └─Empty: 2-1714                     [64, 64, 3, 3]            --
│    └─Empty: 2-1715                     [64]                      --
│    └─Empty: 2-1716                     [64]                      --
│    └─BatchNorm2d: 2-1717               [16, 64, 8, 8]            --
│    └─Scaler: 2-1718                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1719                      [16, 64, 8, 8]            --
│    └─Empty: 2-1720                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1721                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-129               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1722        --                        --
│    └─One: 2-1723                       [1]                       --
│    └─OutputScale: 2-1724               --                        --
│    └─Empty: 2-1725                     [64, 64, 1, 1]            --
│    └─Empty: 2-1726                     [64, 64, 1, 1]            --
│    └─Empty: 2-1727                     [64]                      --
│    └─Empty: 2-1728                     [64]                      --
│    └─BatchNorm2d: 2-1729               [16, 64, 8, 8]            --
│    └─Scaler: 2-1730                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1731                      [16, 64, 8, 8]            --
│    └─Empty: 2-1732                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1733                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-130        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1734                 [16, 64, 8, 8]            --
│    └─Empty: 2-1735                     [16, 64, 8, 8]            --
│    └─Empty: 2-1736                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1737        --                        --
│    └─One: 2-1738                       [1]                       --
│    └─OutputScale: 2-1739               --                        --
│    └─Empty: 2-1740                     [64, 64, 3, 3]            --
│    └─Empty: 2-1741                     [64, 64, 3, 3]            --
│    └─Empty: 2-1742                     [64]                      --
│    └─Empty: 2-1743                     [64]                      --
│    └─BatchNorm2d: 2-1744               [16, 64, 8, 8]            --
│    └─Scaler: 2-1745                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1746                      [16, 64, 8, 8]            --
│    └─Empty: 2-1747                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1748                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-131        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1749                 [16, 64, 4, 4]            --
│    └─Empty: 2-1750                     [16, 64, 4, 4]            --
│    └─Empty: 2-1751                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1752        --                        --
│    └─One: 2-1753                       [1]                       --
│    └─OutputScale: 2-1754               --                        --
│    └─Empty: 2-1755                     [64, 64, 3, 3]            --
│    └─Empty: 2-1756                     [64, 64, 3, 3]            --
│    └─Empty: 2-1757                     [64]                      --
│    └─Empty: 2-1758                     [64]                      --
│    └─BatchNorm2d: 2-1759               [16, 64, 4, 4]            --
│    └─Scaler: 2-1760                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1761                      [16, 64, 4, 4]            --
│    └─Empty: 2-1762                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1763                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-132               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1764        --                        --
│    └─One: 2-1765                       [1]                       --
│    └─OutputScale: 2-1766               --                        --
│    └─Empty: 2-1767                     [64, 64, 1, 1]            --
│    └─Empty: 2-1768                     [64, 64, 1, 1]            --
│    └─Empty: 2-1769                     [64]                      --
│    └─Empty: 2-1770                     [64]                      --
│    └─BatchNorm2d: 2-1771               [16, 64, 4, 4]            --
│    └─Scaler: 2-1772                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1773                      [16, 64, 4, 4]            --
│    └─Empty: 2-1774                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1775                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-133        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1776                 [16, 64, 4, 4]            --
│    └─Empty: 2-1777                     [16, 64, 4, 4]            --
│    └─Empty: 2-1778                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1779        --                        --
│    └─One: 2-1780                       [1]                       --
│    └─OutputScale: 2-1781               --                        --
│    └─Empty: 2-1782                     [64, 64, 3, 3]            --
│    └─Empty: 2-1783                     [64, 64, 3, 3]            --
│    └─Empty: 2-1784                     [64]                      --
│    └─Empty: 2-1785                     [64]                      --
│    └─BatchNorm2d: 2-1786               [16, 64, 4, 4]            --
│    └─Scaler: 2-1787                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1788                      [16, 64, 4, 4]            --
│    └─Empty: 2-1789                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1790                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-134        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1791                 [16, 64, 2, 2]            --
│    └─Empty: 2-1792                     [16, 64, 2, 2]            --
│    └─Empty: 2-1793                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1794        --                        --
│    └─One: 2-1795                       [1]                       --
│    └─OutputScale: 2-1796               --                        --
│    └─Empty: 2-1797                     [64, 64, 1, 1]            --
│    └─Empty: 2-1798                     [64, 64, 1, 1]            --
│    └─Empty: 2-1799                     [64]                      --
│    └─Empty: 2-1800                     [64]                      --
│    └─BatchNorm2d: 2-1801               [16, 64, 2, 2]            --
│    └─Scaler: 2-1802                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1803                      [16, 64, 2, 2]            --
│    └─Empty: 2-1804                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1805                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-135               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-1806        --                        --
│    └─One: 2-1807                       [1]                       --
│    └─OutputScale: 2-1808               --                        --
│    └─Empty: 2-1809                     [64, 64, 1, 1]            --
│    └─Empty: 2-1810                     [64, 64, 1, 1]            --
│    └─Empty: 2-1811                     [64]                      --
│    └─Empty: 2-1812                     [64]                      --
│    └─BatchNorm2d: 2-1813               [16, 64, 2, 2]            --
│    └─Scaler: 2-1814                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1815                      [16, 64, 2, 2]            --
│    └─Empty: 2-1816                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1817                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-136        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1818                 [16, 64, 2, 2]            --
│    └─Empty: 2-1819                     [16, 64, 2, 2]            --
│    └─Empty: 2-1820                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1821        --                        --
│    └─One: 2-1822                       [1]                       --
│    └─OutputScale: 2-1823               --                        --
│    └─Empty: 2-1824                     [64, 64, 3, 3]            --
│    └─Empty: 2-1825                     [64, 64, 3, 3]            --
│    └─Empty: 2-1826                     [64]                      --
│    └─Empty: 2-1827                     [64]                      --
│    └─BatchNorm2d: 2-1828               [16, 64, 2, 2]            --
│    └─Scaler: 2-1829                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1830                      [16, 64, 2, 2]            --
│    └─Empty: 2-1831                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1832                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-137               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1833        --                        --
│    └─One: 2-1834                       [1]                       --
│    └─OutputScale: 2-1835               --                        --
│    └─Empty: 2-1836                     [64, 48, 1, 1]            --
│    └─Empty: 2-1837                     [64, 48, 1, 1]            --
│    └─Empty: 2-1838                     [64]                      --
│    └─Empty: 2-1839                     [64]                      --
│    └─BatchNorm2d: 2-1840               [16, 64, 64, 64]          --
│    └─Scaler: 2-1841                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1842                      [16, 64, 64, 64]          --
│    └─Empty: 2-1843                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1844                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-138               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1845        --                        --
│    └─One: 2-1846                       [1]                       --
│    └─OutputScale: 2-1847               --                        --
│    └─Empty: 2-1848                     [64, 64, 3, 3]            --
│    └─Empty: 2-1849                     [64, 64, 3, 3]            --
│    └─Empty: 2-1850                     [64]                      --
│    └─Empty: 2-1851                     [64]                      --
│    └─BatchNorm2d: 2-1852               [16, 64, 64, 64]          --
│    └─Scaler: 2-1853                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1854                      [16, 64, 64, 64]          --
│    └─Empty: 2-1855                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1856                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-139               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1857        --                        --
│    └─One: 2-1858                       [1]                       --
│    └─OutputScale: 2-1859               --                        --
│    └─Empty: 2-1860                     [64, 64, 1, 1]            --
│    └─Empty: 2-1861                     [64, 64, 1, 1]            --
│    └─Empty: 2-1862                     [64]                      --
│    └─Empty: 2-1863                     [64]                      --
│    └─BatchNorm2d: 2-1864               [16, 64, 64, 64]          --
│    └─Scaler: 2-1865                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1866                      [16, 64, 64, 64]          --
│    └─Empty: 2-1867                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1868                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-140               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1869        --                        --
│    └─One: 2-1870                       [1]                       --
│    └─OutputScale: 2-1871               --                        --
│    └─Empty: 2-1872                     [64, 64, 3, 3]            --
│    └─Empty: 2-1873                     [64, 64, 3, 3]            --
│    └─Empty: 2-1874                     [64]                      --
│    └─Empty: 2-1875                     [64]                      --
│    └─BatchNorm2d: 2-1876               [16, 64, 64, 64]          --
│    └─Scaler: 2-1877                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1878                      [16, 64, 64, 64]          --
│    └─Empty: 2-1879                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1880                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-141        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1881                 [16, 64, 32, 32]          --
│    └─Empty: 2-1882                     [16, 64, 32, 32]          --
│    └─Empty: 2-1883                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1884        --                        --
│    └─One: 2-1885                       [1]                       --
│    └─OutputScale: 2-1886               --                        --
│    └─Empty: 2-1887                     [64, 64, 3, 3]            --
│    └─Empty: 2-1888                     [64, 64, 3, 3]            --
│    └─Empty: 2-1889                     [64]                      --
│    └─Empty: 2-1890                     [64]                      --
│    └─BatchNorm2d: 2-1891               [16, 64, 32, 32]          --
│    └─Scaler: 2-1892                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1893                      [16, 64, 32, 32]          --
│    └─Empty: 2-1894                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1895                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-142               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1896        --                        --
│    └─One: 2-1897                       [1]                       --
│    └─OutputScale: 2-1898               --                        --
│    └─Empty: 2-1899                     [64, 64, 3, 3]            --
│    └─Empty: 2-1900                     [64, 64, 3, 3]            --
│    └─Empty: 2-1901                     [64]                      --
│    └─Empty: 2-1902                     [64]                      --
│    └─BatchNorm2d: 2-1903               [16, 64, 32, 32]          --
│    └─Scaler: 2-1904                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1905                      [16, 64, 32, 32]          --
│    └─Empty: 2-1906                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1907                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-143        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1908                 [16, 64, 16, 16]          --
│    └─Empty: 2-1909                     [16, 64, 16, 16]          --
│    └─Empty: 2-1910                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1911        --                        --
│    └─One: 2-1912                       [1]                       --
│    └─OutputScale: 2-1913               --                        --
│    └─Empty: 2-1914                     [64, 64, 3, 3]            --
│    └─Empty: 2-1915                     [64, 64, 3, 3]            --
│    └─Empty: 2-1916                     [64]                      --
│    └─Empty: 2-1917                     [64]                      --
│    └─BatchNorm2d: 2-1918               [16, 64, 16, 16]          --
│    └─Scaler: 2-1919                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1920                      [16, 64, 16, 16]          --
│    └─Empty: 2-1921                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1922                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-144               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1923        --                        --
│    └─One: 2-1924                       [1]                       --
│    └─OutputScale: 2-1925               --                        --
│    └─Empty: 2-1926                     [64, 64, 3, 3]            --
│    └─Empty: 2-1927                     [64, 64, 3, 3]            --
│    └─Empty: 2-1928                     [64]                      --
│    └─Empty: 2-1929                     [64]                      --
│    └─BatchNorm2d: 2-1930               [16, 64, 16, 16]          --
│    └─Scaler: 2-1931                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1932                      [16, 64, 16, 16]          --
│    └─Empty: 2-1933                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1934                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-145        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1935                 [16, 64, 8, 8]            --
│    └─Empty: 2-1936                     [16, 64, 8, 8]            --
│    └─Empty: 2-1937                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1938        --                        --
│    └─One: 2-1939                       [1]                       --
│    └─OutputScale: 2-1940               --                        --
│    └─Empty: 2-1941                     [64, 64, 3, 3]            --
│    └─Empty: 2-1942                     [64, 64, 3, 3]            --
│    └─Empty: 2-1943                     [64]                      --
│    └─Empty: 2-1944                     [64]                      --
│    └─BatchNorm2d: 2-1945               [16, 64, 8, 8]            --
│    └─Scaler: 2-1946                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1947                      [16, 64, 8, 8]            --
│    └─Empty: 2-1948                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1949                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-146               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1950        --                        --
│    └─One: 2-1951                       [1]                       --
│    └─OutputScale: 2-1952               --                        --
│    └─Empty: 2-1953                     [64, 64, 1, 1]            --
│    └─Empty: 2-1954                     [64, 64, 1, 1]            --
│    └─Empty: 2-1955                     [64]                      --
│    └─Empty: 2-1956                     [64]                      --
│    └─BatchNorm2d: 2-1957               [16, 64, 8, 8]            --
│    └─Scaler: 2-1958                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1959                      [16, 64, 8, 8]            --
│    └─Empty: 2-1960                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1961                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-147        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1962                 [16, 64, 8, 8]            --
│    └─Empty: 2-1963                     [16, 64, 8, 8]            --
│    └─Empty: 2-1964                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1965        --                        --
│    └─One: 2-1966                       [1]                       --
│    └─OutputScale: 2-1967               --                        --
│    └─Empty: 2-1968                     [64, 64, 3, 3]            --
│    └─Empty: 2-1969                     [64, 64, 3, 3]            --
│    └─Empty: 2-1970                     [64]                      --
│    └─Empty: 2-1971                     [64]                      --
│    └─BatchNorm2d: 2-1972               [16, 64, 8, 8]            --
│    └─Scaler: 2-1973                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1974                      [16, 64, 8, 8]            --
│    └─Empty: 2-1975                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1976                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-148        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1977                 [16, 64, 4, 4]            --
│    └─Empty: 2-1978                     [16, 64, 4, 4]            --
│    └─Empty: 2-1979                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1980        --                        --
│    └─One: 2-1981                       [1]                       --
│    └─OutputScale: 2-1982               --                        --
│    └─Empty: 2-1983                     [64, 64, 3, 3]            --
│    └─Empty: 2-1984                     [64, 64, 3, 3]            --
│    └─Empty: 2-1985                     [64]                      --
│    └─Empty: 2-1986                     [64]                      --
│    └─BatchNorm2d: 2-1987               [16, 64, 4, 4]            --
│    └─Scaler: 2-1988                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1989                      [16, 64, 4, 4]            --
│    └─Empty: 2-1990                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1991                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-149               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1992        --                        --
│    └─One: 2-1993                       [1]                       --
│    └─OutputScale: 2-1994               --                        --
│    └─Empty: 2-1995                     [64, 64, 1, 1]            --
│    └─Empty: 2-1996                     [64, 64, 1, 1]            --
│    └─Empty: 2-1997                     [64]                      --
│    └─Empty: 2-1998                     [64]                      --
│    └─BatchNorm2d: 2-1999               [16, 64, 4, 4]            --
│    └─Scaler: 2-2000                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2001                      [16, 64, 4, 4]            --
│    └─Empty: 2-2002                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2003                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-150        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2004                 [16, 64, 4, 4]            --
│    └─Empty: 2-2005                     [16, 64, 4, 4]            --
│    └─Empty: 2-2006                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2007        --                        --
│    └─One: 2-2008                       [1]                       --
│    └─OutputScale: 2-2009               --                        --
│    └─Empty: 2-2010                     [64, 64, 3, 3]            --
│    └─Empty: 2-2011                     [64, 64, 3, 3]            --
│    └─Empty: 2-2012                     [64]                      --
│    └─Empty: 2-2013                     [64]                      --
│    └─BatchNorm2d: 2-2014               [16, 64, 4, 4]            --
│    └─Scaler: 2-2015                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2016                      [16, 64, 4, 4]            --
│    └─Empty: 2-2017                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2018                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-151        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2019                 [16, 64, 2, 2]            --
│    └─Empty: 2-2020                     [16, 64, 2, 2]            --
│    └─Empty: 2-2021                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2022        --                        --
│    └─One: 2-2023                       [1]                       --
│    └─OutputScale: 2-2024               --                        --
│    └─Empty: 2-2025                     [64, 64, 1, 1]            --
│    └─Empty: 2-2026                     [64, 64, 1, 1]            --
│    └─Empty: 2-2027                     [64]                      --
│    └─Empty: 2-2028                     [64]                      --
│    └─BatchNorm2d: 2-2029               [16, 64, 2, 2]            --
│    └─Scaler: 2-2030                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2031                      [16, 64, 2, 2]            --
│    └─Empty: 2-2032                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2033                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-152               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2034        --                        --
│    └─One: 2-2035                       [1]                       --
│    └─OutputScale: 2-2036               --                        --
│    └─Empty: 2-2037                     [64, 64, 1, 1]            --
│    └─Empty: 2-2038                     [64, 64, 1, 1]            --
│    └─Empty: 2-2039                     [64]                      --
│    └─Empty: 2-2040                     [64]                      --
│    └─BatchNorm2d: 2-2041               [16, 64, 2, 2]            --
│    └─Scaler: 2-2042                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2043                      [16, 64, 2, 2]            --
│    └─Empty: 2-2044                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2045                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-153        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2046                 [16, 64, 2, 2]            --
│    └─Empty: 2-2047                     [16, 64, 2, 2]            --
│    └─Empty: 2-2048                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2049        --                        --
│    └─One: 2-2050                       [1]                       --
│    └─OutputScale: 2-2051               --                        --
│    └─Empty: 2-2052                     [64, 64, 3, 3]            --
│    └─Empty: 2-2053                     [64, 64, 3, 3]            --
│    └─Empty: 2-2054                     [64]                      --
│    └─Empty: 2-2055                     [64]                      --
│    └─BatchNorm2d: 2-2056               [16, 64, 2, 2]            --
│    └─Scaler: 2-2057                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2058                      [16, 64, 2, 2]            --
│    └─Empty: 2-2059                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2060                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-154               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2061        --                        --
│    └─One: 2-2062                       [1]                       --
│    └─OutputScale: 2-2063               --                        --
│    └─Empty: 2-2064                     [64, 48, 1, 1]            --
│    └─Empty: 2-2065                     [64, 48, 1, 1]            --
│    └─Empty: 2-2066                     [64]                      --
│    └─Empty: 2-2067                     [64]                      --
│    └─BatchNorm2d: 2-2068               [16, 64, 64, 64]          --
│    └─Scaler: 2-2069                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2070                      [16, 64, 64, 64]          --
│    └─Empty: 2-2071                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2072                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-155               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2073        --                        --
│    └─One: 2-2074                       [1]                       --
│    └─OutputScale: 2-2075               --                        --
│    └─Empty: 2-2076                     [64, 64, 3, 3]            --
│    └─Empty: 2-2077                     [64, 64, 3, 3]            --
│    └─Empty: 2-2078                     [64]                      --
│    └─Empty: 2-2079                     [64]                      --
│    └─BatchNorm2d: 2-2080               [16, 64, 64, 64]          --
│    └─Scaler: 2-2081                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2082                      [16, 64, 64, 64]          --
│    └─Empty: 2-2083                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2084                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-156               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2085        --                        --
│    └─One: 2-2086                       [1]                       --
│    └─OutputScale: 2-2087               --                        --
│    └─Empty: 2-2088                     [64, 64, 1, 1]            --
│    └─Empty: 2-2089                     [64, 64, 1, 1]            --
│    └─Empty: 2-2090                     [64]                      --
│    └─Empty: 2-2091                     [64]                      --
│    └─BatchNorm2d: 2-2092               [16, 64, 64, 64]          --
│    └─Scaler: 2-2093                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2094                      [16, 64, 64, 64]          --
│    └─Empty: 2-2095                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2096                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-157               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2097        --                        --
│    └─One: 2-2098                       [1]                       --
│    └─OutputScale: 2-2099               --                        --
│    └─Empty: 2-2100                     [64, 64, 3, 3]            --
│    └─Empty: 2-2101                     [64, 64, 3, 3]            --
│    └─Empty: 2-2102                     [64]                      --
│    └─Empty: 2-2103                     [64]                      --
│    └─BatchNorm2d: 2-2104               [16, 64, 64, 64]          --
│    └─Scaler: 2-2105                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2106                      [16, 64, 64, 64]          --
│    └─Empty: 2-2107                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2108                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-158        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2109                 [16, 64, 32, 32]          --
│    └─Empty: 2-2110                     [16, 64, 32, 32]          --
│    └─Empty: 2-2111                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2112        --                        --
│    └─One: 2-2113                       [1]                       --
│    └─OutputScale: 2-2114               --                        --
│    └─Empty: 2-2115                     [64, 64, 3, 3]            --
│    └─Empty: 2-2116                     [64, 64, 3, 3]            --
│    └─Empty: 2-2117                     [64]                      --
│    └─Empty: 2-2118                     [64]                      --
│    └─BatchNorm2d: 2-2119               [16, 64, 32, 32]          --
│    └─Scaler: 2-2120                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2121                      [16, 64, 32, 32]          --
│    └─Empty: 2-2122                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2123                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-159               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2124        --                        --
│    └─One: 2-2125                       [1]                       --
│    └─OutputScale: 2-2126               --                        --
│    └─Empty: 2-2127                     [64, 64, 3, 3]            --
│    └─Empty: 2-2128                     [64, 64, 3, 3]            --
│    └─Empty: 2-2129                     [64]                      --
│    └─Empty: 2-2130                     [64]                      --
│    └─BatchNorm2d: 2-2131               [16, 64, 32, 32]          --
│    └─Scaler: 2-2132                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2133                      [16, 64, 32, 32]          --
│    └─Empty: 2-2134                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2135                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-160        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2136                 [16, 64, 16, 16]          --
│    └─Empty: 2-2137                     [16, 64, 16, 16]          --
│    └─Empty: 2-2138                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2139        --                        --
│    └─One: 2-2140                       [1]                       --
│    └─OutputScale: 2-2141               --                        --
│    └─Empty: 2-2142                     [64, 64, 3, 3]            --
│    └─Empty: 2-2143                     [64, 64, 3, 3]            --
│    └─Empty: 2-2144                     [64]                      --
│    └─Empty: 2-2145                     [64]                      --
│    └─BatchNorm2d: 2-2146               [16, 64, 16, 16]          --
│    └─Scaler: 2-2147                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2148                      [16, 64, 16, 16]          --
│    └─Empty: 2-2149                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2150                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-161               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2151        --                        --
│    └─One: 2-2152                       [1]                       --
│    └─OutputScale: 2-2153               --                        --
│    └─Empty: 2-2154                     [64, 64, 3, 3]            --
│    └─Empty: 2-2155                     [64, 64, 3, 3]            --
│    └─Empty: 2-2156                     [64]                      --
│    └─Empty: 2-2157                     [64]                      --
│    └─BatchNorm2d: 2-2158               [16, 64, 16, 16]          --
│    └─Scaler: 2-2159                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2160                      [16, 64, 16, 16]          --
│    └─Empty: 2-2161                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2162                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-162        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2163                 [16, 64, 8, 8]            --
│    └─Empty: 2-2164                     [16, 64, 8, 8]            --
│    └─Empty: 2-2165                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2166        --                        --
│    └─One: 2-2167                       [1]                       --
│    └─OutputScale: 2-2168               --                        --
│    └─Empty: 2-2169                     [64, 64, 3, 3]            --
│    └─Empty: 2-2170                     [64, 64, 3, 3]            --
│    └─Empty: 2-2171                     [64]                      --
│    └─Empty: 2-2172                     [64]                      --
│    └─BatchNorm2d: 2-2173               [16, 64, 8, 8]            --
│    └─Scaler: 2-2174                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2175                      [16, 64, 8, 8]            --
│    └─Empty: 2-2176                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2177                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-163               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2178        --                        --
│    └─One: 2-2179                       [1]                       --
│    └─OutputScale: 2-2180               --                        --
│    └─Empty: 2-2181                     [64, 64, 1, 1]            --
│    └─Empty: 2-2182                     [64, 64, 1, 1]            --
│    └─Empty: 2-2183                     [64]                      --
│    └─Empty: 2-2184                     [64]                      --
│    └─BatchNorm2d: 2-2185               [16, 64, 8, 8]            --
│    └─Scaler: 2-2186                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2187                      [16, 64, 8, 8]            --
│    └─Empty: 2-2188                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2189                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-164        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2190                 [16, 64, 8, 8]            --
│    └─Empty: 2-2191                     [16, 64, 8, 8]            --
│    └─Empty: 2-2192                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2193        --                        --
│    └─One: 2-2194                       [1]                       --
│    └─OutputScale: 2-2195               --                        --
│    └─Empty: 2-2196                     [64, 64, 3, 3]            --
│    └─Empty: 2-2197                     [64, 64, 3, 3]            --
│    └─Empty: 2-2198                     [64]                      --
│    └─Empty: 2-2199                     [64]                      --
│    └─BatchNorm2d: 2-2200               [16, 64, 8, 8]            --
│    └─Scaler: 2-2201                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2202                      [16, 64, 8, 8]            --
│    └─Empty: 2-2203                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2204                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-165        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2205                 [16, 64, 4, 4]            --
│    └─Empty: 2-2206                     [16, 64, 4, 4]            --
│    └─Empty: 2-2207                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2208        --                        --
│    └─One: 2-2209                       [1]                       --
│    └─OutputScale: 2-2210               --                        --
│    └─Empty: 2-2211                     [64, 64, 3, 3]            --
│    └─Empty: 2-2212                     [64, 64, 3, 3]            --
│    └─Empty: 2-2213                     [64]                      --
│    └─Empty: 2-2214                     [64]                      --
│    └─BatchNorm2d: 2-2215               [16, 64, 4, 4]            --
│    └─Scaler: 2-2216                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2217                      [16, 64, 4, 4]            --
│    └─Empty: 2-2218                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2219                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-166               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-2220        --                        --
│    └─One: 2-2221                       [1]                       --
│    └─OutputScale: 2-2222               --                        --
│    └─Empty: 2-2223                     [64, 64, 1, 1]            --
│    └─Empty: 2-2224                     [64, 64, 1, 1]            --
│    └─Empty: 2-2225                     [64]                      --
│    └─Empty: 2-2226                     [64]                      --
│    └─BatchNorm2d: 2-2227               [16, 64, 4, 4]            --
│    └─Scaler: 2-2228                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2229                      [16, 64, 4, 4]            --
│    └─Empty: 2-2230                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2231                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-167        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2232                 [16, 64, 4, 4]            --
│    └─Empty: 2-2233                     [16, 64, 4, 4]            --
│    └─Empty: 2-2234                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2235        --                        --
│    └─One: 2-2236                       [1]                       --
│    └─OutputScale: 2-2237               --                        --
│    └─Empty: 2-2238                     [64, 64, 3, 3]            --
│    └─Empty: 2-2239                     [64, 64, 3, 3]            --
│    └─Empty: 2-2240                     [64]                      --
│    └─Empty: 2-2241                     [64]                      --
│    └─BatchNorm2d: 2-2242               [16, 64, 4, 4]            --
│    └─Scaler: 2-2243                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2244                      [16, 64, 4, 4]            --
│    └─Empty: 2-2245                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2246                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-168        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2247                 [16, 64, 2, 2]            --
│    └─Empty: 2-2248                     [16, 64, 2, 2]            --
│    └─Empty: 2-2249                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2250        --                        --
│    └─One: 2-2251                       [1]                       --
│    └─OutputScale: 2-2252               --                        --
│    └─Empty: 2-2253                     [64, 64, 1, 1]            --
│    └─Empty: 2-2254                     [64, 64, 1, 1]            --
│    └─Empty: 2-2255                     [64]                      --
│    └─Empty: 2-2256                     [64]                      --
│    └─BatchNorm2d: 2-2257               [16, 64, 2, 2]            --
│    └─Scaler: 2-2258                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2259                      [16, 64, 2, 2]            --
│    └─Empty: 2-2260                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2261                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-169               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2262        --                        --
│    └─One: 2-2263                       [1]                       --
│    └─OutputScale: 2-2264               --                        --
│    └─Empty: 2-2265                     [64, 64, 1, 1]            --
│    └─Empty: 2-2266                     [64, 64, 1, 1]            --
│    └─Empty: 2-2267                     [64]                      --
│    └─Empty: 2-2268                     [64]                      --
│    └─BatchNorm2d: 2-2269               [16, 64, 2, 2]            --
│    └─Scaler: 2-2270                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2271                      [16, 64, 2, 2]            --
│    └─Empty: 2-2272                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2273                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-170        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2274                 [16, 64, 2, 2]            --
│    └─Empty: 2-2275                     [16, 64, 2, 2]            --
│    └─Empty: 2-2276                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2277        --                        --
│    └─One: 2-2278                       [1]                       --
│    └─OutputScale: 2-2279               --                        --
│    └─Empty: 2-2280                     [64, 64, 3, 3]            --
│    └─Empty: 2-2281                     [64, 64, 3, 3]            --
│    └─Empty: 2-2282                     [64]                      --
│    └─Empty: 2-2283                     [64]                      --
│    └─BatchNorm2d: 2-2284               [16, 64, 2, 2]            --
│    └─Scaler: 2-2285                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2286                      [16, 64, 2, 2]            --
│    └─Empty: 2-2287                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2288                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-171               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2289        --                        --
│    └─One: 2-2290                       [1]                       --
│    └─OutputScale: 2-2291               --                        --
│    └─Empty: 2-2292                     [64, 48, 1, 1]            --
│    └─Empty: 2-2293                     [64, 48, 1, 1]            --
│    └─Empty: 2-2294                     [64]                      --
│    └─Empty: 2-2295                     [64]                      --
│    └─BatchNorm2d: 2-2296               [16, 64, 64, 64]          --
│    └─Scaler: 2-2297                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2298                      [16, 64, 64, 64]          --
│    └─Empty: 2-2299                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2300                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-172               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2301        --                        --
│    └─One: 2-2302                       [1]                       --
│    └─OutputScale: 2-2303               --                        --
│    └─Empty: 2-2304                     [64, 64, 3, 3]            --
│    └─Empty: 2-2305                     [64, 64, 3, 3]            --
│    └─Empty: 2-2306                     [64]                      --
│    └─Empty: 2-2307                     [64]                      --
│    └─BatchNorm2d: 2-2308               [16, 64, 64, 64]          --
│    └─Scaler: 2-2309                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2310                      [16, 64, 64, 64]          --
│    └─Empty: 2-2311                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2312                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-173               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2313        --                        --
│    └─One: 2-2314                       [1]                       --
│    └─OutputScale: 2-2315               --                        --
│    └─Empty: 2-2316                     [64, 64, 1, 1]            --
│    └─Empty: 2-2317                     [64, 64, 1, 1]            --
│    └─Empty: 2-2318                     [64]                      --
│    └─Empty: 2-2319                     [64]                      --
│    └─BatchNorm2d: 2-2320               [16, 64, 64, 64]          --
│    └─Scaler: 2-2321                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2322                      [16, 64, 64, 64]          --
│    └─Empty: 2-2323                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2324                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-174               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2325        --                        --
│    └─One: 2-2326                       [1]                       --
│    └─OutputScale: 2-2327               --                        --
│    └─Empty: 2-2328                     [64, 64, 3, 3]            --
│    └─Empty: 2-2329                     [64, 64, 3, 3]            --
│    └─Empty: 2-2330                     [64]                      --
│    └─Empty: 2-2331                     [64]                      --
│    └─BatchNorm2d: 2-2332               [16, 64, 64, 64]          --
│    └─Scaler: 2-2333                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2334                      [16, 64, 64, 64]          --
│    └─Empty: 2-2335                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2336                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-175        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2337                 [16, 64, 32, 32]          --
│    └─Empty: 2-2338                     [16, 64, 32, 32]          --
│    └─Empty: 2-2339                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2340        --                        --
│    └─One: 2-2341                       [1]                       --
│    └─OutputScale: 2-2342               --                        --
│    └─Empty: 2-2343                     [64, 64, 3, 3]            --
│    └─Empty: 2-2344                     [64, 64, 3, 3]            --
│    └─Empty: 2-2345                     [64]                      --
│    └─Empty: 2-2346                     [64]                      --
│    └─BatchNorm2d: 2-2347               [16, 64, 32, 32]          --
│    └─Scaler: 2-2348                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2349                      [16, 64, 32, 32]          --
│    └─Empty: 2-2350                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2351                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-176               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2352        --                        --
│    └─One: 2-2353                       [1]                       --
│    └─OutputScale: 2-2354               --                        --
│    └─Empty: 2-2355                     [64, 64, 3, 3]            --
│    └─Empty: 2-2356                     [64, 64, 3, 3]            --
│    └─Empty: 2-2357                     [64]                      --
│    └─Empty: 2-2358                     [64]                      --
│    └─BatchNorm2d: 2-2359               [16, 64, 32, 32]          --
│    └─Scaler: 2-2360                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2361                      [16, 64, 32, 32]          --
│    └─Empty: 2-2362                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2363                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-177        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2364                 [16, 64, 16, 16]          --
│    └─Empty: 2-2365                     [16, 64, 16, 16]          --
│    └─Empty: 2-2366                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2367        --                        --
│    └─One: 2-2368                       [1]                       --
│    └─OutputScale: 2-2369               --                        --
│    └─Empty: 2-2370                     [64, 64, 3, 3]            --
│    └─Empty: 2-2371                     [64, 64, 3, 3]            --
│    └─Empty: 2-2372                     [64]                      --
│    └─Empty: 2-2373                     [64]                      --
│    └─BatchNorm2d: 2-2374               [16, 64, 16, 16]          --
│    └─Scaler: 2-2375                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2376                      [16, 64, 16, 16]          --
│    └─Empty: 2-2377                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2378                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-178               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2379        --                        --
│    └─One: 2-2380                       [1]                       --
│    └─OutputScale: 2-2381               --                        --
│    └─Empty: 2-2382                     [64, 64, 3, 3]            --
│    └─Empty: 2-2383                     [64, 64, 3, 3]            --
│    └─Empty: 2-2384                     [64]                      --
│    └─Empty: 2-2385                     [64]                      --
│    └─BatchNorm2d: 2-2386               [16, 64, 16, 16]          --
│    └─Scaler: 2-2387                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2388                      [16, 64, 16, 16]          --
│    └─Empty: 2-2389                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2390                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-179        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2391                 [16, 64, 8, 8]            --
│    └─Empty: 2-2392                     [16, 64, 8, 8]            --
│    └─Empty: 2-2393                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2394        --                        --
│    └─One: 2-2395                       [1]                       --
│    └─OutputScale: 2-2396               --                        --
│    └─Empty: 2-2397                     [64, 64, 3, 3]            --
│    └─Empty: 2-2398                     [64, 64, 3, 3]            --
│    └─Empty: 2-2399                     [64]                      --
│    └─Empty: 2-2400                     [64]                      --
│    └─BatchNorm2d: 2-2401               [16, 64, 8, 8]            --
│    └─Scaler: 2-2402                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2403                      [16, 64, 8, 8]            --
│    └─Empty: 2-2404                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2405                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-180               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2406        --                        --
│    └─One: 2-2407                       [1]                       --
│    └─OutputScale: 2-2408               --                        --
│    └─Empty: 2-2409                     [64, 64, 1, 1]            --
│    └─Empty: 2-2410                     [64, 64, 1, 1]            --
│    └─Empty: 2-2411                     [64]                      --
│    └─Empty: 2-2412                     [64]                      --
│    └─BatchNorm2d: 2-2413               [16, 64, 8, 8]            --
│    └─Scaler: 2-2414                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2415                      [16, 64, 8, 8]            --
│    └─Empty: 2-2416                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2417                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-181        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2418                 [16, 64, 8, 8]            --
│    └─Empty: 2-2419                     [16, 64, 8, 8]            --
│    └─Empty: 2-2420                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2421        --                        --
│    └─One: 2-2422                       [1]                       --
│    └─OutputScale: 2-2423               --                        --
│    └─Empty: 2-2424                     [64, 64, 3, 3]            --
│    └─Empty: 2-2425                     [64, 64, 3, 3]            --
│    └─Empty: 2-2426                     [64]                      --
│    └─Empty: 2-2427                     [64]                      --
│    └─BatchNorm2d: 2-2428               [16, 64, 8, 8]            --
│    └─Scaler: 2-2429                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2430                      [16, 64, 8, 8]            --
│    └─Empty: 2-2431                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2432                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-182        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2433                 [16, 64, 4, 4]            --
│    └─Empty: 2-2434                     [16, 64, 4, 4]            --
│    └─Empty: 2-2435                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2436        --                        --
│    └─One: 2-2437                       [1]                       --
│    └─OutputScale: 2-2438               --                        --
│    └─Empty: 2-2439                     [64, 64, 3, 3]            --
│    └─Empty: 2-2440                     [64, 64, 3, 3]            --
│    └─Empty: 2-2441                     [64]                      --
│    └─Empty: 2-2442                     [64]                      --
│    └─BatchNorm2d: 2-2443               [16, 64, 4, 4]            --
│    └─Scaler: 2-2444                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2445                      [16, 64, 4, 4]            --
│    └─Empty: 2-2446                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2447                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-183               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-2448        --                        --
│    └─One: 2-2449                       [1]                       --
│    └─OutputScale: 2-2450               --                        --
│    └─Empty: 2-2451                     [64, 64, 1, 1]            --
│    └─Empty: 2-2452                     [64, 64, 1, 1]            --
│    └─Empty: 2-2453                     [64]                      --
│    └─Empty: 2-2454                     [64]                      --
│    └─BatchNorm2d: 2-2455               [16, 64, 4, 4]            --
│    └─Scaler: 2-2456                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2457                      [16, 64, 4, 4]            --
│    └─Empty: 2-2458                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2459                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-184        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2460                 [16, 64, 4, 4]            --
│    └─Empty: 2-2461                     [16, 64, 4, 4]            --
│    └─Empty: 2-2462                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2463        --                        --
│    └─One: 2-2464                       [1]                       --
│    └─OutputScale: 2-2465               --                        --
│    └─Empty: 2-2466                     [64, 64, 3, 3]            --
│    └─Empty: 2-2467                     [64, 64, 3, 3]            --
│    └─Empty: 2-2468                     [64]                      --
│    └─Empty: 2-2469                     [64]                      --
│    └─BatchNorm2d: 2-2470               [16, 64, 4, 4]            --
│    └─Scaler: 2-2471                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2472                      [16, 64, 4, 4]            --
│    └─Empty: 2-2473                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2474                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-185        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2475                 [16, 64, 2, 2]            --
│    └─Empty: 2-2476                     [16, 64, 2, 2]            --
│    └─Empty: 2-2477                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2478        --                        --
│    └─One: 2-2479                       [1]                       --
│    └─OutputScale: 2-2480               --                        --
│    └─Empty: 2-2481                     [64, 64, 1, 1]            --
│    └─Empty: 2-2482                     [64, 64, 1, 1]            --
│    └─Empty: 2-2483                     [64]                      --
│    └─Empty: 2-2484                     [64]                      --
│    └─BatchNorm2d: 2-2485               [16, 64, 2, 2]            --
│    └─Scaler: 2-2486                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2487                      [16, 64, 2, 2]            --
│    └─Empty: 2-2488                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2489                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-186               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2490        --                        --
│    └─One: 2-2491                       [1]                       --
│    └─OutputScale: 2-2492               --                        --
│    └─Empty: 2-2493                     [64, 64, 1, 1]            --
│    └─Empty: 2-2494                     [64, 64, 1, 1]            --
│    └─Empty: 2-2495                     [64]                      --
│    └─Empty: 2-2496                     [64]                      --
│    └─BatchNorm2d: 2-2497               [16, 64, 2, 2]            --
│    └─Scaler: 2-2498                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2499                      [16, 64, 2, 2]            --
│    └─Empty: 2-2500                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2501                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-187        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2502                 [16, 64, 2, 2]            --
│    └─Empty: 2-2503                     [16, 64, 2, 2]            --
│    └─Empty: 2-2504                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2505        --                        --
│    └─One: 2-2506                       [1]                       --
│    └─OutputScale: 2-2507               --                        --
│    └─Empty: 2-2508                     [64, 64, 3, 3]            --
│    └─Empty: 2-2509                     [64, 64, 3, 3]            --
│    └─Empty: 2-2510                     [64]                      --
│    └─Empty: 2-2511                     [64]                      --
│    └─BatchNorm2d: 2-2512               [16, 64, 2, 2]            --
│    └─Scaler: 2-2513                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2514                      [16, 64, 2, 2]            --
│    └─Empty: 2-2515                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2516                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-188               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2517        --                        --
│    └─One: 2-2518                       [1]                       --
│    └─OutputScale: 2-2519               --                        --
│    └─Empty: 2-2520                     [64, 48, 1, 1]            --
│    └─Empty: 2-2521                     [64, 48, 1, 1]            --
│    └─Empty: 2-2522                     [64]                      --
│    └─Empty: 2-2523                     [64]                      --
│    └─BatchNorm2d: 2-2524               [16, 64, 64, 64]          --
│    └─Scaler: 2-2525                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2526                      [16, 64, 64, 64]          --
│    └─Empty: 2-2527                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2528                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-189               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2529        --                        --
│    └─One: 2-2530                       [1]                       --
│    └─OutputScale: 2-2531               --                        --
│    └─Empty: 2-2532                     [64, 64, 3, 3]            --
│    └─Empty: 2-2533                     [64, 64, 3, 3]            --
│    └─Empty: 2-2534                     [64]                      --
│    └─Empty: 2-2535                     [64]                      --
│    └─BatchNorm2d: 2-2536               [16, 64, 64, 64]          --
│    └─Scaler: 2-2537                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2538                      [16, 64, 64, 64]          --
│    └─Empty: 2-2539                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2540                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-190               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2541        --                        --
│    └─One: 2-2542                       [1]                       --
│    └─OutputScale: 2-2543               --                        --
│    └─Empty: 2-2544                     [64, 64, 1, 1]            --
│    └─Empty: 2-2545                     [64, 64, 1, 1]            --
│    └─Empty: 2-2546                     [64]                      --
│    └─Empty: 2-2547                     [64]                      --
│    └─BatchNorm2d: 2-2548               [16, 64, 64, 64]          --
│    └─Scaler: 2-2549                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2550                      [16, 64, 64, 64]          --
│    └─Empty: 2-2551                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2552                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-191               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2553        --                        --
│    └─One: 2-2554                       [1]                       --
│    └─OutputScale: 2-2555               --                        --
│    └─Empty: 2-2556                     [64, 64, 3, 3]            --
│    └─Empty: 2-2557                     [64, 64, 3, 3]            --
│    └─Empty: 2-2558                     [64]                      --
│    └─Empty: 2-2559                     [64]                      --
│    └─BatchNorm2d: 2-2560               [16, 64, 64, 64]          --
│    └─Scaler: 2-2561                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2562                      [16, 64, 64, 64]          --
│    └─Empty: 2-2563                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2564                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-192        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2565                 [16, 64, 32, 32]          --
│    └─Empty: 2-2566                     [16, 64, 32, 32]          --
│    └─Empty: 2-2567                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2568        --                        --
│    └─One: 2-2569                       [1]                       --
│    └─OutputScale: 2-2570               --                        --
│    └─Empty: 2-2571                     [64, 64, 3, 3]            --
│    └─Empty: 2-2572                     [64, 64, 3, 3]            --
│    └─Empty: 2-2573                     [64]                      --
│    └─Empty: 2-2574                     [64]                      --
│    └─BatchNorm2d: 2-2575               [16, 64, 32, 32]          --
│    └─Scaler: 2-2576                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2577                      [16, 64, 32, 32]          --
│    └─Empty: 2-2578                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2579                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-193               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2580        --                        --
│    └─One: 2-2581                       [1]                       --
│    └─OutputScale: 2-2582               --                        --
│    └─Empty: 2-2583                     [64, 64, 3, 3]            --
│    └─Empty: 2-2584                     [64, 64, 3, 3]            --
│    └─Empty: 2-2585                     [64]                      --
│    └─Empty: 2-2586                     [64]                      --
│    └─BatchNorm2d: 2-2587               [16, 64, 32, 32]          --
│    └─Scaler: 2-2588                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2589                      [16, 64, 32, 32]          --
│    └─Empty: 2-2590                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2591                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-194        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2592                 [16, 64, 16, 16]          --
│    └─Empty: 2-2593                     [16, 64, 16, 16]          --
│    └─Empty: 2-2594                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2595        --                        --
│    └─One: 2-2596                       [1]                       --
│    └─OutputScale: 2-2597               --                        --
│    └─Empty: 2-2598                     [64, 64, 3, 3]            --
│    └─Empty: 2-2599                     [64, 64, 3, 3]            --
│    └─Empty: 2-2600                     [64]                      --
│    └─Empty: 2-2601                     [64]                      --
│    └─BatchNorm2d: 2-2602               [16, 64, 16, 16]          --
│    └─Scaler: 2-2603                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2604                      [16, 64, 16, 16]          --
│    └─Empty: 2-2605                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2606                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-195               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2607        --                        --
│    └─One: 2-2608                       [1]                       --
│    └─OutputScale: 2-2609               --                        --
│    └─Empty: 2-2610                     [64, 64, 3, 3]            --
│    └─Empty: 2-2611                     [64, 64, 3, 3]            --
│    └─Empty: 2-2612                     [64]                      --
│    └─Empty: 2-2613                     [64]                      --
│    └─BatchNorm2d: 2-2614               [16, 64, 16, 16]          --
│    └─Scaler: 2-2615                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2616                      [16, 64, 16, 16]          --
│    └─Empty: 2-2617                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2618                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-196        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2619                 [16, 64, 8, 8]            --
│    └─Empty: 2-2620                     [16, 64, 8, 8]            --
│    └─Empty: 2-2621                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2622        --                        --
│    └─One: 2-2623                       [1]                       --
│    └─OutputScale: 2-2624               --                        --
│    └─Empty: 2-2625                     [64, 64, 3, 3]            --
│    └─Empty: 2-2626                     [64, 64, 3, 3]            --
│    └─Empty: 2-2627                     [64]                      --
│    └─Empty: 2-2628                     [64]                      --
│    └─BatchNorm2d: 2-2629               [16, 64, 8, 8]            --
│    └─Scaler: 2-2630                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2631                      [16, 64, 8, 8]            --
│    └─Empty: 2-2632                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2633                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-197               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2634        --                        --
│    └─One: 2-2635                       [1]                       --
│    └─OutputScale: 2-2636               --                        --
│    └─Empty: 2-2637                     [64, 64, 1, 1]            --
│    └─Empty: 2-2638                     [64, 64, 1, 1]            --
│    └─Empty: 2-2639                     [64]                      --
│    └─Empty: 2-2640                     [64]                      --
│    └─BatchNorm2d: 2-2641               [16, 64, 8, 8]            --
│    └─Scaler: 2-2642                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2643                      [16, 64, 8, 8]            --
│    └─Empty: 2-2644                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2645                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-198        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2646                 [16, 64, 8, 8]            --
│    └─Empty: 2-2647                     [16, 64, 8, 8]            --
│    └─Empty: 2-2648                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2649        --                        --
│    └─One: 2-2650                       [1]                       --
│    └─OutputScale: 2-2651               --                        --
│    └─Empty: 2-2652                     [64, 64, 3, 3]            --
│    └─Empty: 2-2653                     [64, 64, 3, 3]            --
│    └─Empty: 2-2654                     [64]                      --
│    └─Empty: 2-2655                     [64]                      --
│    └─BatchNorm2d: 2-2656               [16, 64, 8, 8]            --
│    └─Scaler: 2-2657                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2658                      [16, 64, 8, 8]            --
│    └─Empty: 2-2659                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2660                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-199        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2661                 [16, 64, 4, 4]            --
│    └─Empty: 2-2662                     [16, 64, 4, 4]            --
│    └─Empty: 2-2663                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2664        --                        --
│    └─One: 2-2665                       [1]                       --
│    └─OutputScale: 2-2666               --                        --
│    └─Empty: 2-2667                     [64, 64, 3, 3]            --
│    └─Empty: 2-2668                     [64, 64, 3, 3]            --
│    └─Empty: 2-2669                     [64]                      --
│    └─Empty: 2-2670                     [64]                      --
│    └─BatchNorm2d: 2-2671               [16, 64, 4, 4]            --
│    └─Scaler: 2-2672                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2673                      [16, 64, 4, 4]            --
│    └─Empty: 2-2674                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2675                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-200               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-2676        --                        --
│    └─One: 2-2677                       [1]                       --
│    └─OutputScale: 2-2678               --                        --
│    └─Empty: 2-2679                     [64, 64, 1, 1]            --
│    └─Empty: 2-2680                     [64, 64, 1, 1]            --
│    └─Empty: 2-2681                     [64]                      --
│    └─Empty: 2-2682                     [64]                      --
│    └─BatchNorm2d: 2-2683               [16, 64, 4, 4]            --
│    └─Scaler: 2-2684                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2685                      [16, 64, 4, 4]            --
│    └─Empty: 2-2686                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2687                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-201        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2688                 [16, 64, 4, 4]            --
│    └─Empty: 2-2689                     [16, 64, 4, 4]            --
│    └─Empty: 2-2690                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2691        --                        --
│    └─One: 2-2692                       [1]                       --
│    └─OutputScale: 2-2693               --                        --
│    └─Empty: 2-2694                     [64, 64, 3, 3]            --
│    └─Empty: 2-2695                     [64, 64, 3, 3]            --
│    └─Empty: 2-2696                     [64]                      --
│    └─Empty: 2-2697                     [64]                      --
│    └─BatchNorm2d: 2-2698               [16, 64, 4, 4]            --
│    └─Scaler: 2-2699                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2700                      [16, 64, 4, 4]            --
│    └─Empty: 2-2701                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2702                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-202        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2703                 [16, 64, 2, 2]            --
│    └─Empty: 2-2704                     [16, 64, 2, 2]            --
│    └─Empty: 2-2705                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2706        --                        --
│    └─One: 2-2707                       [1]                       --
│    └─OutputScale: 2-2708               --                        --
│    └─Empty: 2-2709                     [64, 64, 1, 1]            --
│    └─Empty: 2-2710                     [64, 64, 1, 1]            --
│    └─Empty: 2-2711                     [64]                      --
│    └─Empty: 2-2712                     [64]                      --
│    └─BatchNorm2d: 2-2713               [16, 64, 2, 2]            --
│    └─Scaler: 2-2714                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2715                      [16, 64, 2, 2]            --
│    └─Empty: 2-2716                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2717                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-203               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2718        --                        --
│    └─One: 2-2719                       [1]                       --
│    └─OutputScale: 2-2720               --                        --
│    └─Empty: 2-2721                     [64, 64, 1, 1]            --
│    └─Empty: 2-2722                     [64, 64, 1, 1]            --
│    └─Empty: 2-2723                     [64]                      --
│    └─Empty: 2-2724                     [64]                      --
│    └─BatchNorm2d: 2-2725               [16, 64, 2, 2]            --
│    └─Scaler: 2-2726                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2727                      [16, 64, 2, 2]            --
│    └─Empty: 2-2728                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2729                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-204        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2730                 [16, 64, 2, 2]            --
│    └─Empty: 2-2731                     [16, 64, 2, 2]            --
│    └─Empty: 2-2732                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2733        --                        --
│    └─One: 2-2734                       [1]                       --
│    └─OutputScale: 2-2735               --                        --
│    └─Empty: 2-2736                     [64, 64, 3, 3]            --
│    └─Empty: 2-2737                     [64, 64, 3, 3]            --
│    └─Empty: 2-2738                     [64]                      --
│    └─Empty: 2-2739                     [64]                      --
│    └─BatchNorm2d: 2-2740               [16, 64, 2, 2]            --
│    └─Scaler: 2-2741                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2742                      [16, 64, 2, 2]            --
│    └─Empty: 2-2743                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2744                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-205               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2745        --                        --
│    └─One: 2-2746                       [1]                       --
│    └─OutputScale: 2-2747               --                        --
│    └─Empty: 2-2748                     [64, 48, 1, 1]            --
│    └─Empty: 2-2749                     [64, 48, 1, 1]            --
│    └─Empty: 2-2750                     [64]                      --
│    └─Empty: 2-2751                     [64]                      --
│    └─BatchNorm2d: 2-2752               [16, 64, 64, 64]          --
│    └─Scaler: 2-2753                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2754                      [16, 64, 64, 64]          --
│    └─Empty: 2-2755                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2756                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-206               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2757        --                        --
│    └─One: 2-2758                       [1]                       --
│    └─OutputScale: 2-2759               --                        --
│    └─Empty: 2-2760                     [64, 64, 3, 3]            --
│    └─Empty: 2-2761                     [64, 64, 3, 3]            --
│    └─Empty: 2-2762                     [64]                      --
│    └─Empty: 2-2763                     [64]                      --
│    └─BatchNorm2d: 2-2764               [16, 64, 64, 64]          --
│    └─Scaler: 2-2765                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2766                      [16, 64, 64, 64]          --
│    └─Empty: 2-2767                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2768                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-207               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2769        --                        --
│    └─One: 2-2770                       [1]                       --
│    └─OutputScale: 2-2771               --                        --
│    └─Empty: 2-2772                     [64, 64, 1, 1]            --
│    └─Empty: 2-2773                     [64, 64, 1, 1]            --
│    └─Empty: 2-2774                     [64]                      --
│    └─Empty: 2-2775                     [64]                      --
│    └─BatchNorm2d: 2-2776               [16, 64, 64, 64]          --
│    └─Scaler: 2-2777                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2778                      [16, 64, 64, 64]          --
│    └─Empty: 2-2779                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2780                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-208               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2781        --                        --
│    └─One: 2-2782                       [1]                       --
│    └─OutputScale: 2-2783               --                        --
│    └─Empty: 2-2784                     [64, 64, 3, 3]            --
│    └─Empty: 2-2785                     [64, 64, 3, 3]            --
│    └─Empty: 2-2786                     [64]                      --
│    └─Empty: 2-2787                     [64]                      --
│    └─BatchNorm2d: 2-2788               [16, 64, 64, 64]          --
│    └─Scaler: 2-2789                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2790                      [16, 64, 64, 64]          --
│    └─Empty: 2-2791                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2792                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-209        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2793                 [16, 64, 32, 32]          --
│    └─Empty: 2-2794                     [16, 64, 32, 32]          --
│    └─Empty: 2-2795                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2796        --                        --
│    └─One: 2-2797                       [1]                       --
│    └─OutputScale: 2-2798               --                        --
│    └─Empty: 2-2799                     [64, 64, 3, 3]            --
│    └─Empty: 2-2800                     [64, 64, 3, 3]            --
│    └─Empty: 2-2801                     [64]                      --
│    └─Empty: 2-2802                     [64]                      --
│    └─BatchNorm2d: 2-2803               [16, 64, 32, 32]          --
│    └─Scaler: 2-2804                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2805                      [16, 64, 32, 32]          --
│    └─Empty: 2-2806                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2807                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-210               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2808        --                        --
│    └─One: 2-2809                       [1]                       --
│    └─OutputScale: 2-2810               --                        --
│    └─Empty: 2-2811                     [64, 64, 3, 3]            --
│    └─Empty: 2-2812                     [64, 64, 3, 3]            --
│    └─Empty: 2-2813                     [64]                      --
│    └─Empty: 2-2814                     [64]                      --
│    └─BatchNorm2d: 2-2815               [16, 64, 32, 32]          --
│    └─Scaler: 2-2816                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2817                      [16, 64, 32, 32]          --
│    └─Empty: 2-2818                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2819                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-211        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2820                 [16, 64, 16, 16]          --
│    └─Empty: 2-2821                     [16, 64, 16, 16]          --
│    └─Empty: 2-2822                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2823        --                        --
│    └─One: 2-2824                       [1]                       --
│    └─OutputScale: 2-2825               --                        --
│    └─Empty: 2-2826                     [64, 64, 3, 3]            --
│    └─Empty: 2-2827                     [64, 64, 3, 3]            --
│    └─Empty: 2-2828                     [64]                      --
│    └─Empty: 2-2829                     [64]                      --
│    └─BatchNorm2d: 2-2830               [16, 64, 16, 16]          --
│    └─Scaler: 2-2831                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2832                      [16, 64, 16, 16]          --
│    └─Empty: 2-2833                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2834                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-212               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2835        --                        --
│    └─One: 2-2836                       [1]                       --
│    └─OutputScale: 2-2837               --                        --
│    └─Empty: 2-2838                     [64, 64, 3, 3]            --
│    └─Empty: 2-2839                     [64, 64, 3, 3]            --
│    └─Empty: 2-2840                     [64]                      --
│    └─Empty: 2-2841                     [64]                      --
│    └─BatchNorm2d: 2-2842               [16, 64, 16, 16]          --
│    └─Scaler: 2-2843                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2844                      [16, 64, 16, 16]          --
│    └─Empty: 2-2845                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2846                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-213        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2847                 [16, 64, 8, 8]            --
│    └─Empty: 2-2848                     [16, 64, 8, 8]            --
│    └─Empty: 2-2849                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2850        --                        --
│    └─One: 2-2851                       [1]                       --
│    └─OutputScale: 2-2852               --                        --
│    └─Empty: 2-2853                     [64, 64, 3, 3]            --
│    └─Empty: 2-2854                     [64, 64, 3, 3]            --
│    └─Empty: 2-2855                     [64]                      --
│    └─Empty: 2-2856                     [64]                      --
│    └─BatchNorm2d: 2-2857               [16, 64, 8, 8]            --
│    └─Scaler: 2-2858                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2859                      [16, 64, 8, 8]            --
│    └─Empty: 2-2860                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2861                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-214               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2862        --                        --
│    └─One: 2-2863                       [1]                       --
│    └─OutputScale: 2-2864               --                        --
│    └─Empty: 2-2865                     [64, 64, 1, 1]            --
│    └─Empty: 2-2866                     [64, 64, 1, 1]            --
│    └─Empty: 2-2867                     [64]                      --
│    └─Empty: 2-2868                     [64]                      --
│    └─BatchNorm2d: 2-2869               [16, 64, 8, 8]            --
│    └─Scaler: 2-2870                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2871                      [16, 64, 8, 8]            --
│    └─Empty: 2-2872                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2873                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-215        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2874                 [16, 64, 8, 8]            --
│    └─Empty: 2-2875                     [16, 64, 8, 8]            --
│    └─Empty: 2-2876                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2877        --                        --
│    └─One: 2-2878                       [1]                       --
│    └─OutputScale: 2-2879               --                        --
│    └─Empty: 2-2880                     [64, 64, 3, 3]            --
│    └─Empty: 2-2881                     [64, 64, 3, 3]            --
│    └─Empty: 2-2882                     [64]                      --
│    └─Empty: 2-2883                     [64]                      --
│    └─BatchNorm2d: 2-2884               [16, 64, 8, 8]            --
│    └─Scaler: 2-2885                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2886                      [16, 64, 8, 8]            --
│    └─Empty: 2-2887                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2888                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-216        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2889                 [16, 64, 4, 4]            --
│    └─Empty: 2-2890                     [16, 64, 4, 4]            --
│    └─Empty: 2-2891                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2892        --                        --
│    └─One: 2-2893                       [1]                       --
│    └─OutputScale: 2-2894               --                        --
│    └─Empty: 2-2895                     [64, 64, 3, 3]            --
│    └─Empty: 2-2896                     [64, 64, 3, 3]            --
│    └─Empty: 2-2897                     [64]                      --
│    └─Empty: 2-2898                     [64]                      --
│    └─BatchNorm2d: 2-2899               [16, 64, 4, 4]            --
│    └─Scaler: 2-2900                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2901                      [16, 64, 4, 4]            --
│    └─Empty: 2-2902                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2903                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-217               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-2904        --                        --
│    └─One: 2-2905                       [1]                       --
│    └─OutputScale: 2-2906               --                        --
│    └─Empty: 2-2907                     [64, 64, 1, 1]            --
│    └─Empty: 2-2908                     [64, 64, 1, 1]            --
│    └─Empty: 2-2909                     [64]                      --
│    └─Empty: 2-2910                     [64]                      --
│    └─BatchNorm2d: 2-2911               [16, 64, 4, 4]            --
│    └─Scaler: 2-2912                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2913                      [16, 64, 4, 4]            --
│    └─Empty: 2-2914                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2915                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-218        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2916                 [16, 64, 4, 4]            --
│    └─Empty: 2-2917                     [16, 64, 4, 4]            --
│    └─Empty: 2-2918                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2919        --                        --
│    └─One: 2-2920                       [1]                       --
│    └─OutputScale: 2-2921               --                        --
│    └─Empty: 2-2922                     [64, 64, 3, 3]            --
│    └─Empty: 2-2923                     [64, 64, 3, 3]            --
│    └─Empty: 2-2924                     [64]                      --
│    └─Empty: 2-2925                     [64]                      --
│    └─BatchNorm2d: 2-2926               [16, 64, 4, 4]            --
│    └─Scaler: 2-2927                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2928                      [16, 64, 4, 4]            --
│    └─Empty: 2-2929                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2930                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-219        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2931                 [16, 64, 2, 2]            --
│    └─Empty: 2-2932                     [16, 64, 2, 2]            --
│    └─Empty: 2-2933                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2934        --                        --
│    └─One: 2-2935                       [1]                       --
│    └─OutputScale: 2-2936               --                        --
│    └─Empty: 2-2937                     [64, 64, 1, 1]            --
│    └─Empty: 2-2938                     [64, 64, 1, 1]            --
│    └─Empty: 2-2939                     [64]                      --
│    └─Empty: 2-2940                     [64]                      --
│    └─BatchNorm2d: 2-2941               [16, 64, 2, 2]            --
│    └─Scaler: 2-2942                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2943                      [16, 64, 2, 2]            --
│    └─Empty: 2-2944                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2945                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-220               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2946        --                        --
│    └─One: 2-2947                       [1]                       --
│    └─OutputScale: 2-2948               --                        --
│    └─Empty: 2-2949                     [64, 64, 1, 1]            --
│    └─Empty: 2-2950                     [64, 64, 1, 1]            --
│    └─Empty: 2-2951                     [64]                      --
│    └─Empty: 2-2952                     [64]                      --
│    └─BatchNorm2d: 2-2953               [16, 64, 2, 2]            --
│    └─Scaler: 2-2954                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2955                      [16, 64, 2, 2]            --
│    └─Empty: 2-2956                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2957                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-221        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2958                 [16, 64, 2, 2]            --
│    └─Empty: 2-2959                     [16, 64, 2, 2]            --
│    └─Empty: 2-2960                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2961        --                        --
│    └─One: 2-2962                       [1]                       --
│    └─OutputScale: 2-2963               --                        --
│    └─Empty: 2-2964                     [64, 64, 3, 3]            --
│    └─Empty: 2-2965                     [64, 64, 3, 3]            --
│    └─Empty: 2-2966                     [64]                      --
│    └─Empty: 2-2967                     [64]                      --
│    └─BatchNorm2d: 2-2968               [16, 64, 2, 2]            --
│    └─Scaler: 2-2969                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2970                      [16, 64, 2, 2]            --
│    └─Empty: 2-2971                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2972                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-222               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2973        --                        --
│    └─One: 2-2974                       [1]                       --
│    └─OutputScale: 2-2975               --                        --
│    └─Empty: 2-2976                     [64, 48, 1, 1]            --
│    └─Empty: 2-2977                     [64, 48, 1, 1]            --
│    └─Empty: 2-2978                     [64]                      --
│    └─Empty: 2-2979                     [64]                      --
│    └─BatchNorm2d: 2-2980               [16, 64, 64, 64]          --
│    └─Scaler: 2-2981                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2982                      [16, 64, 64, 64]          --
│    └─Empty: 2-2983                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2984                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-223               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2985        --                        --
│    └─One: 2-2986                       [1]                       --
│    └─OutputScale: 2-2987               --                        --
│    └─Empty: 2-2988                     [64, 64, 3, 3]            --
│    └─Empty: 2-2989                     [64, 64, 3, 3]            --
│    └─Empty: 2-2990                     [64]                      --
│    └─Empty: 2-2991                     [64]                      --
│    └─BatchNorm2d: 2-2992               [16, 64, 64, 64]          --
│    └─Scaler: 2-2993                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2994                      [16, 64, 64, 64]          --
│    └─Empty: 2-2995                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2996                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-224               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2997        --                        --
│    └─One: 2-2998                       [1]                       --
│    └─OutputScale: 2-2999               --                        --
│    └─Empty: 2-3000                     [64, 64, 1, 1]            --
│    └─Empty: 2-3001                     [64, 64, 1, 1]            --
│    └─Empty: 2-3002                     [64]                      --
│    └─Empty: 2-3003                     [64]                      --
│    └─BatchNorm2d: 2-3004               [16, 64, 64, 64]          --
│    └─Scaler: 2-3005                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3006                      [16, 64, 64, 64]          --
│    └─Empty: 2-3007                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3008                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-225               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3009        --                        --
│    └─One: 2-3010                       [1]                       --
│    └─OutputScale: 2-3011               --                        --
│    └─Empty: 2-3012                     [64, 64, 3, 3]            --
│    └─Empty: 2-3013                     [64, 64, 3, 3]            --
│    └─Empty: 2-3014                     [64]                      --
│    └─Empty: 2-3015                     [64]                      --
│    └─BatchNorm2d: 2-3016               [16, 64, 64, 64]          --
│    └─Scaler: 2-3017                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3018                      [16, 64, 64, 64]          --
│    └─Empty: 2-3019                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3020                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-226        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-3021                 [16, 64, 32, 32]          --
│    └─Empty: 2-3022                     [16, 64, 32, 32]          --
│    └─Empty: 2-3023                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-3024        --                        --
│    └─One: 2-3025                       [1]                       --
│    └─OutputScale: 2-3026               --                        --
│    └─Empty: 2-3027                     [64, 64, 3, 3]            --
│    └─Empty: 2-3028                     [64, 64, 3, 3]            --
│    └─Empty: 2-3029                     [64]                      --
│    └─Empty: 2-3030                     [64]                      --
│    └─BatchNorm2d: 2-3031               [16, 64, 32, 32]          --
│    └─Scaler: 2-3032                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3033                      [16, 64, 32, 32]          --
│    └─Empty: 2-3034                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3035                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-227               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-3036        --                        --
│    └─One: 2-3037                       [1]                       --
│    └─OutputScale: 2-3038               --                        --
│    └─Empty: 2-3039                     [64, 64, 3, 3]            --
│    └─Empty: 2-3040                     [64, 64, 3, 3]            --
│    └─Empty: 2-3041                     [64]                      --
│    └─Empty: 2-3042                     [64]                      --
│    └─BatchNorm2d: 2-3043               [16, 64, 32, 32]          --
│    └─Scaler: 2-3044                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3045                      [16, 64, 32, 32]          --
│    └─Empty: 2-3046                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3047                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-228        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-3048                 [16, 64, 16, 16]          --
│    └─Empty: 2-3049                     [16, 64, 16, 16]          --
│    └─Empty: 2-3050                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-3051        --                        --
│    └─One: 2-3052                       [1]                       --
│    └─OutputScale: 2-3053               --                        --
│    └─Empty: 2-3054                     [64, 64, 3, 3]            --
│    └─Empty: 2-3055                     [64, 64, 3, 3]            --
│    └─Empty: 2-3056                     [64]                      --
│    └─Empty: 2-3057                     [64]                      --
│    └─BatchNorm2d: 2-3058               [16, 64, 16, 16]          --
│    └─Scaler: 2-3059                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3060                      [16, 64, 16, 16]          --
│    └─Empty: 2-3061                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3062                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-229               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-3063        --                        --
│    └─One: 2-3064                       [1]                       --
│    └─OutputScale: 2-3065               --                        --
│    └─Empty: 2-3066                     [64, 64, 3, 3]            --
│    └─Empty: 2-3067                     [64, 64, 3, 3]            --
│    └─Empty: 2-3068                     [64]                      --
│    └─Empty: 2-3069                     [64]                      --
│    └─BatchNorm2d: 2-3070               [16, 64, 16, 16]          --
│    └─Scaler: 2-3071                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3072                      [16, 64, 16, 16]          --
│    └─Empty: 2-3073                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3074                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-230        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3075                 [16, 64, 8, 8]            --
│    └─Empty: 2-3076                     [16, 64, 8, 8]            --
│    └─Empty: 2-3077                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3078        --                        --
│    └─One: 2-3079                       [1]                       --
│    └─OutputScale: 2-3080               --                        --
│    └─Empty: 2-3081                     [64, 64, 3, 3]            --
│    └─Empty: 2-3082                     [64, 64, 3, 3]            --
│    └─Empty: 2-3083                     [64]                      --
│    └─Empty: 2-3084                     [64]                      --
│    └─BatchNorm2d: 2-3085               [16, 64, 8, 8]            --
│    └─Scaler: 2-3086                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3087                      [16, 64, 8, 8]            --
│    └─Empty: 2-3088                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3089                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-231               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-3090        --                        --
│    └─One: 2-3091                       [1]                       --
│    └─OutputScale: 2-3092               --                        --
│    └─Empty: 2-3093                     [64, 64, 1, 1]            --
│    └─Empty: 2-3094                     [64, 64, 1, 1]            --
│    └─Empty: 2-3095                     [64]                      --
│    └─Empty: 2-3096                     [64]                      --
│    └─BatchNorm2d: 2-3097               [16, 64, 8, 8]            --
│    └─Scaler: 2-3098                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3099                      [16, 64, 8, 8]            --
│    └─Empty: 2-3100                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3101                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-232        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3102                 [16, 64, 8, 8]            --
│    └─Empty: 2-3103                     [16, 64, 8, 8]            --
│    └─Empty: 2-3104                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3105        --                        --
│    └─One: 2-3106                       [1]                       --
│    └─OutputScale: 2-3107               --                        --
│    └─Empty: 2-3108                     [64, 64, 3, 3]            --
│    └─Empty: 2-3109                     [64, 64, 3, 3]            --
│    └─Empty: 2-3110                     [64]                      --
│    └─Empty: 2-3111                     [64]                      --
│    └─BatchNorm2d: 2-3112               [16, 64, 8, 8]            --
│    └─Scaler: 2-3113                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3114                      [16, 64, 8, 8]            --
│    └─Empty: 2-3115                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3116                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-233        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3117                 [16, 64, 4, 4]            --
│    └─Empty: 2-3118                     [16, 64, 4, 4]            --
│    └─Empty: 2-3119                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3120        --                        --
│    └─One: 2-3121                       [1]                       --
│    └─OutputScale: 2-3122               --                        --
│    └─Empty: 2-3123                     [64, 64, 3, 3]            --
│    └─Empty: 2-3124                     [64, 64, 3, 3]            --
│    └─Empty: 2-3125                     [64]                      --
│    └─Empty: 2-3126                     [64]                      --
│    └─BatchNorm2d: 2-3127               [16, 64, 4, 4]            --
│    └─Scaler: 2-3128                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3129                      [16, 64, 4, 4]            --
│    └─Empty: 2-3130                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3131                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-234               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-3132        --                        --
│    └─One: 2-3133                       [1]                       --
│    └─OutputScale: 2-3134               --                        --
│    └─Empty: 2-3135                     [64, 64, 1, 1]            --
│    └─Empty: 2-3136                     [64, 64, 1, 1]            --
│    └─Empty: 2-3137                     [64]                      --
│    └─Empty: 2-3138                     [64]                      --
│    └─BatchNorm2d: 2-3139               [16, 64, 4, 4]            --
│    └─Scaler: 2-3140                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3141                      [16, 64, 4, 4]            --
│    └─Empty: 2-3142                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3143                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-235        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3144                 [16, 64, 4, 4]            --
│    └─Empty: 2-3145                     [16, 64, 4, 4]            --
│    └─Empty: 2-3146                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3147        --                        --
│    └─One: 2-3148                       [1]                       --
│    └─OutputScale: 2-3149               --                        --
│    └─Empty: 2-3150                     [64, 64, 3, 3]            --
│    └─Empty: 2-3151                     [64, 64, 3, 3]            --
│    └─Empty: 2-3152                     [64]                      --
│    └─Empty: 2-3153                     [64]                      --
│    └─BatchNorm2d: 2-3154               [16, 64, 4, 4]            --
│    └─Scaler: 2-3155                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3156                      [16, 64, 4, 4]            --
│    └─Empty: 2-3157                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3158                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-236        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3159                 [16, 64, 2, 2]            --
│    └─Empty: 2-3160                     [16, 64, 2, 2]            --
│    └─Empty: 2-3161                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3162        --                        --
│    └─One: 2-3163                       [1]                       --
│    └─OutputScale: 2-3164               --                        --
│    └─Empty: 2-3165                     [64, 64, 1, 1]            --
│    └─Empty: 2-3166                     [64, 64, 1, 1]            --
│    └─Empty: 2-3167                     [64]                      --
│    └─Empty: 2-3168                     [64]                      --
│    └─BatchNorm2d: 2-3169               [16, 64, 2, 2]            --
│    └─Scaler: 2-3170                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3171                      [16, 64, 2, 2]            --
│    └─Empty: 2-3172                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3173                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-237               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-3174        --                        --
│    └─One: 2-3175                       [1]                       --
│    └─OutputScale: 2-3176               --                        --
│    └─Empty: 2-3177                     [64, 64, 1, 1]            --
│    └─Empty: 2-3178                     [64, 64, 1, 1]            --
│    └─Empty: 2-3179                     [64]                      --
│    └─Empty: 2-3180                     [64]                      --
│    └─BatchNorm2d: 2-3181               [16, 64, 2, 2]            --
│    └─Scaler: 2-3182                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3183                      [16, 64, 2, 2]            --
│    └─Empty: 2-3184                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3185                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-238        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3186                 [16, 64, 2, 2]            --
│    └─Empty: 2-3187                     [16, 64, 2, 2]            --
│    └─Empty: 2-3188                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3189        --                        --
│    └─One: 2-3190                       [1]                       --
│    └─OutputScale: 2-3191               --                        --
│    └─Empty: 2-3192                     [64, 64, 3, 3]            --
│    └─Empty: 2-3193                     [64, 64, 3, 3]            --
│    └─Empty: 2-3194                     [64]                      --
│    └─Empty: 2-3195                     [64]                      --
│    └─BatchNorm2d: 2-3196               [16, 64, 2, 2]            --
│    └─Scaler: 2-3197                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3198                      [16, 64, 2, 2]            --
│    └─Empty: 2-3199                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3200                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-239               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3201        --                        --
│    └─One: 2-3202                       [1]                       --
│    └─OutputScale: 2-3203               --                        --
│    └─Empty: 2-3204                     [64, 48, 1, 1]            --
│    └─Empty: 2-3205                     [64, 48, 1, 1]            --
│    └─Empty: 2-3206                     [64]                      --
│    └─Empty: 2-3207                     [64]                      --
│    └─BatchNorm2d: 2-3208               [16, 64, 64, 64]          --
│    └─Scaler: 2-3209                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3210                      [16, 64, 64, 64]          --
│    └─Empty: 2-3211                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3212                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-240               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3213        --                        --
│    └─One: 2-3214                       [1]                       --
│    └─OutputScale: 2-3215               --                        --
│    └─Empty: 2-3216                     [64, 64, 3, 3]            --
│    └─Empty: 2-3217                     [64, 64, 3, 3]            --
│    └─Empty: 2-3218                     [64]                      --
│    └─Empty: 2-3219                     [64]                      --
│    └─BatchNorm2d: 2-3220               [16, 64, 64, 64]          --
│    └─Scaler: 2-3221                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3222                      [16, 64, 64, 64]          --
│    └─Empty: 2-3223                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3224                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-241               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3225        --                        --
│    └─One: 2-3226                       [1]                       --
│    └─OutputScale: 2-3227               --                        --
│    └─Empty: 2-3228                     [64, 64, 1, 1]            --
│    └─Empty: 2-3229                     [64, 64, 1, 1]            --
│    └─Empty: 2-3230                     [64]                      --
│    └─Empty: 2-3231                     [64]                      --
│    └─BatchNorm2d: 2-3232               [16, 64, 64, 64]          --
│    └─Scaler: 2-3233                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3234                      [16, 64, 64, 64]          --
│    └─Empty: 2-3235                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3236                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-242               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3237        --                        --
│    └─One: 2-3238                       [1]                       --
│    └─OutputScale: 2-3239               --                        --
│    └─Empty: 2-3240                     [64, 64, 3, 3]            --
│    └─Empty: 2-3241                     [64, 64, 3, 3]            --
│    └─Empty: 2-3242                     [64]                      --
│    └─Empty: 2-3243                     [64]                      --
│    └─BatchNorm2d: 2-3244               [16, 64, 64, 64]          --
│    └─Scaler: 2-3245                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3246                      [16, 64, 64, 64]          --
│    └─Empty: 2-3247                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3248                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-243        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-3249                 [16, 64, 32, 32]          --
│    └─Empty: 2-3250                     [16, 64, 32, 32]          --
│    └─Empty: 2-3251                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-3252        --                        --
│    └─One: 2-3253                       [1]                       --
│    └─OutputScale: 2-3254               --                        --
│    └─Empty: 2-3255                     [64, 64, 3, 3]            --
│    └─Empty: 2-3256                     [64, 64, 3, 3]            --
│    └─Empty: 2-3257                     [64]                      --
│    └─Empty: 2-3258                     [64]                      --
│    └─BatchNorm2d: 2-3259               [16, 64, 32, 32]          --
│    └─Scaler: 2-3260                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3261                      [16, 64, 32, 32]          --
│    └─Empty: 2-3262                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3263                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-244               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-3264        --                        --
│    └─One: 2-3265                       [1]                       --
│    └─OutputScale: 2-3266               --                        --
│    └─Empty: 2-3267                     [64, 64, 3, 3]            --
│    └─Empty: 2-3268                     [64, 64, 3, 3]            --
│    └─Empty: 2-3269                     [64]                      --
│    └─Empty: 2-3270                     [64]                      --
│    └─BatchNorm2d: 2-3271               [16, 64, 32, 32]          --
│    └─Scaler: 2-3272                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3273                      [16, 64, 32, 32]          --
│    └─Empty: 2-3274                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3275                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-245        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-3276                 [16, 64, 16, 16]          --
│    └─Empty: 2-3277                     [16, 64, 16, 16]          --
│    └─Empty: 2-3278                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-3279        --                        --
│    └─One: 2-3280                       [1]                       --
│    └─OutputScale: 2-3281               --                        --
│    └─Empty: 2-3282                     [64, 64, 3, 3]            --
│    └─Empty: 2-3283                     [64, 64, 3, 3]            --
│    └─Empty: 2-3284                     [64]                      --
│    └─Empty: 2-3285                     [64]                      --
│    └─BatchNorm2d: 2-3286               [16, 64, 16, 16]          --
│    └─Scaler: 2-3287                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3288                      [16, 64, 16, 16]          --
│    └─Empty: 2-3289                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3290                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-246               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-3291        --                        --
│    └─One: 2-3292                       [1]                       --
│    └─OutputScale: 2-3293               --                        --
│    └─Empty: 2-3294                     [64, 64, 3, 3]            --
│    └─Empty: 2-3295                     [64, 64, 3, 3]            --
│    └─Empty: 2-3296                     [64]                      --
│    └─Empty: 2-3297                     [64]                      --
│    └─BatchNorm2d: 2-3298               [16, 64, 16, 16]          --
│    └─Scaler: 2-3299                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3300                      [16, 64, 16, 16]          --
│    └─Empty: 2-3301                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3302                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-247        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3303                 [16, 64, 8, 8]            --
│    └─Empty: 2-3304                     [16, 64, 8, 8]            --
│    └─Empty: 2-3305                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3306        --                        --
│    └─One: 2-3307                       [1]                       --
│    └─OutputScale: 2-3308               --                        --
│    └─Empty: 2-3309                     [64, 64, 3, 3]            --
│    └─Empty: 2-3310                     [64, 64, 3, 3]            --
│    └─Empty: 2-3311                     [64]                      --
│    └─Empty: 2-3312                     [64]                      --
│    └─BatchNorm2d: 2-3313               [16, 64, 8, 8]            --
│    └─Scaler: 2-3314                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3315                      [16, 64, 8, 8]            --
│    └─Empty: 2-3316                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3317                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-248               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-3318        --                        --
│    └─One: 2-3319                       [1]                       --
│    └─OutputScale: 2-3320               --                        --
│    └─Empty: 2-3321                     [64, 64, 1, 1]            --
│    └─Empty: 2-3322                     [64, 64, 1, 1]            --
│    └─Empty: 2-3323                     [64]                      --
│    └─Empty: 2-3324                     [64]                      --
│    └─BatchNorm2d: 2-3325               [16, 64, 8, 8]            --
│    └─Scaler: 2-3326                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3327                      [16, 64, 8, 8]            --
│    └─Empty: 2-3328                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3329                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-249        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3330                 [16, 64, 8, 8]            --
│    └─Empty: 2-3331                     [16, 64, 8, 8]            --
│    └─Empty: 2-3332                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3333        --                        --
│    └─One: 2-3334                       [1]                       --
│    └─OutputScale: 2-3335               --                        --
│    └─Empty: 2-3336                     [64, 64, 3, 3]            --
│    └─Empty: 2-3337                     [64, 64, 3, 3]            --
│    └─Empty: 2-3338                     [64]                      --
│    └─Empty: 2-3339                     [64]                      --
│    └─BatchNorm2d: 2-3340               [16, 64, 8, 8]            --
│    └─Scaler: 2-3341                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3342                      [16, 64, 8, 8]            --
│    └─Empty: 2-3343                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3344                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-250        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3345                 [16, 64, 4, 4]            --
│    └─Empty: 2-3346                     [16, 64, 4, 4]            --
│    └─Empty: 2-3347                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3348        --                        --
│    └─One: 2-3349                       [1]                       --
│    └─OutputScale: 2-3350               --                        --
│    └─Empty: 2-3351                     [64, 64, 3, 3]            --
│    └─Empty: 2-3352                     [64, 64, 3, 3]            --
│    └─Empty: 2-3353                     [64]                      --
│    └─Empty: 2-3354                     [64]                      --
│    └─BatchNorm2d: 2-3355               [16, 64, 4, 4]            --
│    └─Scaler: 2-3356                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3357                      [16, 64, 4, 4]            --
│    └─Empty: 2-3358                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3359                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-251               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-3360        --                        --
│    └─One: 2-3361                       [1]                       --
│    └─OutputScale: 2-3362               --                        --
│    └─Empty: 2-3363                     [64, 64, 1, 1]            --
│    └─Empty: 2-3364                     [64, 64, 1, 1]            --
│    └─Empty: 2-3365                     [64]                      --
│    └─Empty: 2-3366                     [64]                      --
│    └─BatchNorm2d: 2-3367               [16, 64, 4, 4]            --
│    └─Scaler: 2-3368                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3369                      [16, 64, 4, 4]            --
│    └─Empty: 2-3370                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3371                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-252        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3372                 [16, 64, 4, 4]            --
│    └─Empty: 2-3373                     [16, 64, 4, 4]            --
│    └─Empty: 2-3374                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3375        --                        --
│    └─One: 2-3376                       [1]                       --
│    └─OutputScale: 2-3377               --                        --
│    └─Empty: 2-3378                     [64, 64, 3, 3]            --
│    └─Empty: 2-3379                     [64, 64, 3, 3]            --
│    └─Empty: 2-3380                     [64]                      --
│    └─Empty: 2-3381                     [64]                      --
│    └─BatchNorm2d: 2-3382               [16, 64, 4, 4]            --
│    └─Scaler: 2-3383                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3384                      [16, 64, 4, 4]            --
│    └─Empty: 2-3385                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3386                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-253        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3387                 [16, 64, 2, 2]            --
│    └─Empty: 2-3388                     [16, 64, 2, 2]            --
│    └─Empty: 2-3389                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3390        --                        --
│    └─One: 2-3391                       [1]                       --
│    └─OutputScale: 2-3392               --                        --
│    └─Empty: 2-3393                     [64, 64, 1, 1]            --
│    └─Empty: 2-3394                     [64, 64, 1, 1]            --
│    └─Empty: 2-3395                     [64]                      --
│    └─Empty: 2-3396                     [64]                      --
│    └─BatchNorm2d: 2-3397               [16, 64, 2, 2]            --
│    └─Scaler: 2-3398                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3399                      [16, 64, 2, 2]            --
│    └─Empty: 2-3400                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3401                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-254               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-3402        --                        --
│    └─One: 2-3403                       [1]                       --
│    └─OutputScale: 2-3404               --                        --
│    └─Empty: 2-3405                     [64, 64, 1, 1]            --
│    └─Empty: 2-3406                     [64, 64, 1, 1]            --
│    └─Empty: 2-3407                     [64]                      --
│    └─Empty: 2-3408                     [64]                      --
│    └─BatchNorm2d: 2-3409               [16, 64, 2, 2]            --
│    └─Scaler: 2-3410                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3411                      [16, 64, 2, 2]            --
│    └─Empty: 2-3412                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3413                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-255        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3414                 [16, 64, 2, 2]            --
│    └─Empty: 2-3415                     [16, 64, 2, 2]            --
│    └─Empty: 2-3416                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3417        --                        --
│    └─One: 2-3418                       [1]                       --
│    └─OutputScale: 2-3419               --                        --
│    └─Empty: 2-3420                     [64, 64, 3, 3]            --
│    └─Empty: 2-3421                     [64, 64, 3, 3]            --
│    └─Empty: 2-3422                     [64]                      --
│    └─Empty: 2-3423                     [64]                      --
│    └─BatchNorm2d: 2-3424               [16, 64, 2, 2]            --
│    └─Scaler: 2-3425                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3426                      [16, 64, 2, 2]            --
│    └─Empty: 2-3427                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3428                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-256               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3429        --                        --
│    └─One: 2-3430                       [1]                       --
│    └─OutputScale: 2-3431               --                        --
│    └─Empty: 2-3432                     [64, 48, 1, 1]            --
│    └─Empty: 2-3433                     [64, 48, 1, 1]            --
│    └─Empty: 2-3434                     [64]                      --
│    └─Empty: 2-3435                     [64]                      --
│    └─BatchNorm2d: 2-3436               [16, 64, 64, 64]          --
│    └─Scaler: 2-3437                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3438                      [16, 64, 64, 64]          --
│    └─Empty: 2-3439                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3440                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-257               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3441        --                        --
│    └─One: 2-3442                       [1]                       --
│    └─OutputScale: 2-3443               --                        --
│    └─Empty: 2-3444                     [64, 64, 3, 3]            --
│    └─Empty: 2-3445                     [64, 64, 3, 3]            --
│    └─Empty: 2-3446                     [64]                      --
│    └─Empty: 2-3447                     [64]                      --
│    └─BatchNorm2d: 2-3448               [16, 64, 64, 64]          --
│    └─Scaler: 2-3449                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3450                      [16, 64, 64, 64]          --
│    └─Empty: 2-3451                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3452                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-258               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3453        --                        --
│    └─One: 2-3454                       [1]                       --
│    └─OutputScale: 2-3455               --                        --
│    └─Empty: 2-3456                     [64, 64, 1, 1]            --
│    └─Empty: 2-3457                     [64, 64, 1, 1]            --
│    └─Empty: 2-3458                     [64]                      --
│    └─Empty: 2-3459                     [64]                      --
│    └─BatchNorm2d: 2-3460               [16, 64, 64, 64]          --
│    └─Scaler: 2-3461                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3462                      [16, 64, 64, 64]          --
│    └─Empty: 2-3463                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3464                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-259               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3465        --                        --
│    └─One: 2-3466                       [1]                       --
│    └─OutputScale: 2-3467               --                        --
│    └─Empty: 2-3468                     [64, 64, 3, 3]            --
│    └─Empty: 2-3469                     [64, 64, 3, 3]            --
│    └─Empty: 2-3470                     [64]                      --
│    └─Empty: 2-3471                     [64]                      --
│    └─BatchNorm2d: 2-3472               [16, 64, 64, 64]          --
│    └─Scaler: 2-3473                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3474                      [16, 64, 64, 64]          --
│    └─Empty: 2-3475                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3476                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-260        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-3477                 [16, 64, 32, 32]          --
│    └─Empty: 2-3478                     [16, 64, 32, 32]          --
│    └─Empty: 2-3479                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-3480        --                        --
│    └─One: 2-3481                       [1]                       --
│    └─OutputScale: 2-3482               --                        --
│    └─Empty: 2-3483                     [64, 64, 3, 3]            --
│    └─Empty: 2-3484                     [64, 64, 3, 3]            --
│    └─Empty: 2-3485                     [64]                      --
│    └─Empty: 2-3486                     [64]                      --
│    └─BatchNorm2d: 2-3487               [16, 64, 32, 32]          --
│    └─Scaler: 2-3488                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3489                      [16, 64, 32, 32]          --
│    └─Empty: 2-3490                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3491                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-261               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-3492        --                        --
│    └─One: 2-3493                       [1]                       --
│    └─OutputScale: 2-3494               --                        --
│    └─Empty: 2-3495                     [64, 64, 3, 3]            --
│    └─Empty: 2-3496                     [64, 64, 3, 3]            --
│    └─Empty: 2-3497                     [64]                      --
│    └─Empty: 2-3498                     [64]                      --
│    └─BatchNorm2d: 2-3499               [16, 64, 32, 32]          --
│    └─Scaler: 2-3500                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3501                      [16, 64, 32, 32]          --
│    └─Empty: 2-3502                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3503                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-262        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-3504                 [16, 64, 16, 16]          --
│    └─Empty: 2-3505                     [16, 64, 16, 16]          --
│    └─Empty: 2-3506                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-3507        --                        --
│    └─One: 2-3508                       [1]                       --
│    └─OutputScale: 2-3509               --                        --
│    └─Empty: 2-3510                     [64, 64, 3, 3]            --
│    └─Empty: 2-3511                     [64, 64, 3, 3]            --
│    └─Empty: 2-3512                     [64]                      --
│    └─Empty: 2-3513                     [64]                      --
│    └─BatchNorm2d: 2-3514               [16, 64, 16, 16]          --
│    └─Scaler: 2-3515                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3516                      [16, 64, 16, 16]          --
│    └─Empty: 2-3517                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3518                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-263               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-3519        --                        --
│    └─One: 2-3520                       [1]                       --
│    └─OutputScale: 2-3521               --                        --
│    └─Empty: 2-3522                     [64, 64, 3, 3]            --
│    └─Empty: 2-3523                     [64, 64, 3, 3]            --
│    └─Empty: 2-3524                     [64]                      --
│    └─Empty: 2-3525                     [64]                      --
│    └─BatchNorm2d: 2-3526               [16, 64, 16, 16]          --
│    └─Scaler: 2-3527                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3528                      [16, 64, 16, 16]          --
│    └─Empty: 2-3529                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3530                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-264        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3531                 [16, 64, 8, 8]            --
│    └─Empty: 2-3532                     [16, 64, 8, 8]            --
│    └─Empty: 2-3533                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3534        --                        --
│    └─One: 2-3535                       [1]                       --
│    └─OutputScale: 2-3536               --                        --
│    └─Empty: 2-3537                     [64, 64, 3, 3]            --
│    └─Empty: 2-3538                     [64, 64, 3, 3]            --
│    └─Empty: 2-3539                     [64]                      --
│    └─Empty: 2-3540                     [64]                      --
│    └─BatchNorm2d: 2-3541               [16, 64, 8, 8]            --
│    └─Scaler: 2-3542                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3543                      [16, 64, 8, 8]            --
│    └─Empty: 2-3544                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3545                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-265               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-3546        --                        --
│    └─One: 2-3547                       [1]                       --
│    └─OutputScale: 2-3548               --                        --
│    └─Empty: 2-3549                     [64, 64, 1, 1]            --
│    └─Empty: 2-3550                     [64, 64, 1, 1]            --
│    └─Empty: 2-3551                     [64]                      --
│    └─Empty: 2-3552                     [64]                      --
│    └─BatchNorm2d: 2-3553               [16, 64, 8, 8]            --
│    └─Scaler: 2-3554                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3555                      [16, 64, 8, 8]            --
│    └─Empty: 2-3556                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3557                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-266        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3558                 [16, 64, 8, 8]            --
│    └─Empty: 2-3559                     [16, 64, 8, 8]            --
│    └─Empty: 2-3560                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3561        --                        --
│    └─One: 2-3562                       [1]                       --
│    └─OutputScale: 2-3563               --                        --
│    └─Empty: 2-3564                     [64, 64, 3, 3]            --
│    └─Empty: 2-3565                     [64, 64, 3, 3]            --
│    └─Empty: 2-3566                     [64]                      --
│    └─Empty: 2-3567                     [64]                      --
│    └─BatchNorm2d: 2-3568               [16, 64, 8, 8]            --
│    └─Scaler: 2-3569                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3570                      [16, 64, 8, 8]            --
│    └─Empty: 2-3571                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3572                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-267        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3573                 [16, 64, 4, 4]            --
│    └─Empty: 2-3574                     [16, 64, 4, 4]            --
│    └─Empty: 2-3575                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3576        --                        --
│    └─One: 2-3577                       [1]                       --
│    └─OutputScale: 2-3578               --                        --
│    └─Empty: 2-3579                     [64, 64, 3, 3]            --
│    └─Empty: 2-3580                     [64, 64, 3, 3]            --
│    └─Empty: 2-3581                     [64]                      --
│    └─Empty: 2-3582                     [64]                      --
│    └─BatchNorm2d: 2-3583               [16, 64, 4, 4]            --
│    └─Scaler: 2-3584                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3585                      [16, 64, 4, 4]            --
│    └─Empty: 2-3586                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3587                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-268               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-3588        --                        --
│    └─One: 2-3589                       [1]                       --
│    └─OutputScale: 2-3590               --                        --
│    └─Empty: 2-3591                     [64, 64, 1, 1]            --
│    └─Empty: 2-3592                     [64, 64, 1, 1]            --
│    └─Empty: 2-3593                     [64]                      --
│    └─Empty: 2-3594                     [64]                      --
│    └─BatchNorm2d: 2-3595               [16, 64, 4, 4]            --
│    └─Scaler: 2-3596                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3597                      [16, 64, 4, 4]            --
│    └─Empty: 2-3598                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3599                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-269        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3600                 [16, 64, 4, 4]            --
│    └─Empty: 2-3601                     [16, 64, 4, 4]            --
│    └─Empty: 2-3602                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3603        --                        --
│    └─One: 2-3604                       [1]                       --
│    └─OutputScale: 2-3605               --                        --
│    └─Empty: 2-3606                     [64, 64, 3, 3]            --
│    └─Empty: 2-3607                     [64, 64, 3, 3]            --
│    └─Empty: 2-3608                     [64]                      --
│    └─Empty: 2-3609                     [64]                      --
│    └─BatchNorm2d: 2-3610               [16, 64, 4, 4]            --
│    └─Scaler: 2-3611                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3612                      [16, 64, 4, 4]            --
│    └─Empty: 2-3613                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3614                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-270        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3615                 [16, 64, 2, 2]            --
│    └─Empty: 2-3616                     [16, 64, 2, 2]            --
│    └─Empty: 2-3617                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3618        --                        --
│    └─One: 2-3619                       [1]                       --
│    └─OutputScale: 2-3620               --                        --
│    └─Empty: 2-3621                     [64, 64, 1, 1]            --
│    └─Empty: 2-3622                     [64, 64, 1, 1]            --
│    └─Empty: 2-3623                     [64]                      --
│    └─Empty: 2-3624                     [64]                      --
│    └─BatchNorm2d: 2-3625               [16, 64, 2, 2]            --
│    └─Scaler: 2-3626                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3627                      [16, 64, 2, 2]            --
│    └─Empty: 2-3628                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3629                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-271               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-3630        --                        --
│    └─One: 2-3631                       [1]                       --
│    └─OutputScale: 2-3632               --                        --
│    └─Empty: 2-3633                     [64, 64, 1, 1]            --
│    └─Empty: 2-3634                     [64, 64, 1, 1]            --
│    └─Empty: 2-3635                     [64]                      --
│    └─Empty: 2-3636                     [64]                      --
│    └─BatchNorm2d: 2-3637               [16, 64, 2, 2]            --
│    └─Scaler: 2-3638                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3639                      [16, 64, 2, 2]            --
│    └─Empty: 2-3640                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3641                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-272        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3642                 [16, 64, 2, 2]            --
│    └─Empty: 2-3643                     [16, 64, 2, 2]            --
│    └─Empty: 2-3644                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3645        --                        --
│    └─One: 2-3646                       [1]                       --
│    └─OutputScale: 2-3647               --                        --
│    └─Empty: 2-3648                     [64, 64, 3, 3]            --
│    └─Empty: 2-3649                     [64, 64, 3, 3]            --
│    └─Empty: 2-3650                     [64]                      --
│    └─Empty: 2-3651                     [64]                      --
│    └─BatchNorm2d: 2-3652               [16, 64, 2, 2]            --
│    └─Scaler: 2-3653                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3654                      [16, 64, 2, 2]            --
│    └─Empty: 2-3655                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3656                     [16, 64, 2, 2]            --
├─Conv1d: 1-273                          [16, 64, 16]              16,454
│    └─OutputShiftSqueeze: 2-3657        --                        --
│    └─One: 2-3658                       [1]                       --
│    └─OutputScale: 2-3659               --                        --
│    └─Empty: 2-3660                     [64, 256, 1]              --
│    └─Empty: 2-3661                     [64, 256, 1]              --
│    └─Empty: 2-3662                     [64]                      --
│    └─Empty: 2-3663                     [64]                      --
│    └─Scaler: 2-3664                    [16, 64, 16]              --
│    └─Empty: 2-3665                     [16, 64, 16]              --
│    └─Empty: 2-3666                     [16, 64, 16]              --
│    └─Clamp: 2-3667                     [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-274               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3668        --                        --
│    └─One: 2-3669                       [1]                       --
│    └─OutputScale: 2-3670               --                        --
│    └─Empty: 2-3671                     [64, 64, 3]               --
│    └─Empty: 2-3672                     [64, 64, 3]               --
│    └─Empty: 2-3673                     [64]                      --
│    └─Empty: 2-3674                     [64]                      --
│    └─BatchNorm1d: 2-3675               [16, 64, 16]              --
│    └─Scaler: 2-3676                    [16, 64, 16]              --
│    └─ReLU: 2-3677                      [16, 64, 16]              --
│    └─Empty: 2-3678                     [16, 64, 16]              --
│    └─Clamp: 2-3679                     [16, 64, 16]              --
├─Conv1d: 1-275                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3680        --                        --
│    └─One: 2-3681                       [1]                       --
│    └─OutputScale: 2-3682               --                        --
│    └─Empty: 2-3683                     [64, 64, 1]               --
│    └─Empty: 2-3684                     [64, 64, 1]               --
│    └─Empty: 2-3685                     [64]                      --
│    └─Empty: 2-3686                     [64]                      --
│    └─Scaler: 2-3687                    [16, 64, 16]              --
│    └─Empty: 2-3688                     [16, 64, 16]              --
│    └─Empty: 2-3689                     [16, 64, 16]              --
│    └─Clamp: 2-3690                     [16, 64, 16]              --
├─Dropout: 1-276                         [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-277               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3691        --                        --
│    └─One: 2-3692                       [1]                       --
│    └─OutputScale: 2-3693               --                        --
│    └─Empty: 2-3694                     [64, 64, 3]               --
│    └─Empty: 2-3695                     [64, 64, 3]               --
│    └─Empty: 2-3696                     [64]                      --
│    └─Empty: 2-3697                     [64]                      --
│    └─BatchNorm1d: 2-3698               [16, 64, 16]              --
│    └─Scaler: 2-3699                    [16, 64, 16]              --
│    └─ReLU: 2-3700                      [16, 64, 16]              --
│    └─Empty: 2-3701                     [16, 64, 16]              --
│    └─Clamp: 2-3702                     [16, 64, 16]              --
├─Conv1d: 1-278                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3703        --                        --
│    └─One: 2-3704                       [1]                       --
│    └─OutputScale: 2-3705               --                        --
│    └─Empty: 2-3706                     [64, 64, 1]               --
│    └─Empty: 2-3707                     [64, 64, 1]               --
│    └─Empty: 2-3708                     [64]                      --
│    └─Empty: 2-3709                     [64]                      --
│    └─Scaler: 2-3710                    [16, 64, 16]              --
│    └─Empty: 2-3711                     [16, 64, 16]              --
│    └─Empty: 2-3712                     [16, 64, 16]              --
│    └─Clamp: 2-3713                     [16, 64, 16]              --
├─Dropout: 1-279                         [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-280               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3714        --                        --
│    └─One: 2-3715                       [1]                       --
│    └─OutputScale: 2-3716               --                        --
│    └─Empty: 2-3717                     [64, 64, 3]               --
│    └─Empty: 2-3718                     [64, 64, 3]               --
│    └─Empty: 2-3719                     [64]                      --
│    └─Empty: 2-3720                     [64]                      --
│    └─BatchNorm1d: 2-3721               [16, 64, 16]              --
│    └─Scaler: 2-3722                    [16, 64, 16]              --
│    └─ReLU: 2-3723                      [16, 64, 16]              --
│    └─Empty: 2-3724                     [16, 64, 16]              --
│    └─Clamp: 2-3725                     [16, 64, 16]              --
├─Conv1d: 1-281                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3726        --                        --
│    └─One: 2-3727                       [1]                       --
│    └─OutputScale: 2-3728               --                        --
│    └─Empty: 2-3729                     [64, 64, 1]               --
│    └─Empty: 2-3730                     [64, 64, 1]               --
│    └─Empty: 2-3731                     [64]                      --
│    └─Empty: 2-3732                     [64]                      --
│    └─Scaler: 2-3733                    [16, 64, 16]              --
│    └─Empty: 2-3734                     [16, 64, 16]              --
│    └─Empty: 2-3735                     [16, 64, 16]              --
│    └─Clamp: 2-3736                     [16, 64, 16]              --
├─Dropout: 1-282                         [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-283               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3737        --                        --
│    └─One: 2-3738                       [1]                       --
│    └─OutputScale: 2-3739               --                        --
│    └─Empty: 2-3740                     [64, 64, 3]               --
│    └─Empty: 2-3741                     [64, 64, 3]               --
│    └─Empty: 2-3742                     [64]                      --
│    └─Empty: 2-3743                     [64]                      --
│    └─BatchNorm1d: 2-3744               [16, 64, 16]              --
│    └─Scaler: 2-3745                    [16, 64, 16]              --
│    └─ReLU: 2-3746                      [16, 64, 16]              --
│    └─Empty: 2-3747                     [16, 64, 16]              --
│    └─Clamp: 2-3748                     [16, 64, 16]              --
├─Conv1d: 1-284                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3749        --                        --
│    └─One: 2-3750                       [1]                       --
│    └─OutputScale: 2-3751               --                        --
│    └─Empty: 2-3752                     [64, 64, 1]               --
│    └─Empty: 2-3753                     [64, 64, 1]               --
│    └─Empty: 2-3754                     [64]                      --
│    └─Empty: 2-3755                     [64]                      --
│    └─Scaler: 2-3756                    [16, 64, 16]              --
│    └─Empty: 2-3757                     [16, 64, 16]              --
│    └─Empty: 2-3758                     [16, 64, 16]              --
│    └─Clamp: 2-3759                     [16, 64, 16]              --
├─Dropout: 1-285                         [16, 64, 16]              --
├─Conv1d: 1-286                          [16, 4, 16]               266
│    └─OutputShiftSqueeze: 2-3760        --                        --
│    └─One: 2-3761                       [1]                       --
│    └─OutputScale: 2-3762               --                        --
│    └─Empty: 2-3763                     [4, 64, 1]                --
│    └─Empty: 2-3764                     [4, 64, 1]                --
│    └─Empty: 2-3765                     [4]                       --
│    └─Empty: 2-3766                     [4]                       --
│    └─Scaler: 2-3767                    [16, 4, 16]               --
│    └─Empty: 2-3768                     [16, 4, 16]               --
│    └─Empty: 2-3769                     [16, 4, 16]               --
│    └─Clamp: 2-3770                     [16, 4, 16]               --
├─Conv1d: 1-287                          [16, 64, 16]              326
│    └─OutputShiftSqueeze: 2-3771        --                        --
│    └─One: 2-3772                       [1]                       --
│    └─OutputScale: 2-3773               --                        --
│    └─Empty: 2-3774                     [64, 4, 1]                --
│    └─Empty: 2-3775                     [64, 4, 1]                --
│    └─Empty: 2-3776                     [64]                      --
│    └─Empty: 2-3777                     [64]                      --
│    └─Scaler: 2-3778                    [16, 64, 16]              --
│    └─Empty: 2-3779                     [16, 64, 16]              --
│    └─Empty: 2-3780                     [16, 64, 16]              --
│    └─Clamp: 2-3781                     [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-288               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3782        --                        --
│    └─One: 2-3783                       [1]                       --
│    └─OutputScale: 2-3784               --                        --
│    └─Empty: 2-3785                     [64, 64, 3]               --
│    └─Empty: 2-3786                     [64, 64, 3]               --
│    └─Empty: 2-3787                     [64]                      --
│    └─Empty: 2-3788                     [64]                      --
│    └─BatchNorm1d: 2-3789               [16, 64, 16]              --
│    └─Scaler: 2-3790                    [16, 64, 16]              --
│    └─ReLU: 2-3791                      [16, 64, 16]              --
│    └─Empty: 2-3792                     [16, 64, 16]              --
│    └─Clamp: 2-3793                     [16, 64, 16]              --
├─Conv1d: 1-289                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3794        --                        --
│    └─One: 2-3795                       [1]                       --
│    └─OutputScale: 2-3796               --                        --
│    └─Empty: 2-3797                     [64, 64, 1]               --
│    └─Empty: 2-3798                     [64, 64, 1]               --
│    └─Empty: 2-3799                     [64]                      --
│    └─Empty: 2-3800                     [64]                      --
│    └─Scaler: 2-3801                    [16, 64, 16]              --
│    └─Empty: 2-3802                     [16, 64, 16]              --
│    └─Empty: 2-3803                     [16, 64, 16]              --
│    └─Clamp: 2-3804                     [16, 64, 16]              --
├─Dropout: 1-290                         [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-291               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3805        --                        --
│    └─One: 2-3806                       [1]                       --
│    └─OutputScale: 2-3807               --                        --
│    └─Empty: 2-3808                     [64, 64, 3]               --
│    └─Empty: 2-3809                     [64, 64, 3]               --
│    └─Empty: 2-3810                     [64]                      --
│    └─Empty: 2-3811                     [64]                      --
│    └─BatchNorm1d: 2-3812               [16, 64, 16]              --
│    └─Scaler: 2-3813                    [16, 64, 16]              --
│    └─ReLU: 2-3814                      [16, 64, 16]              --
│    └─Empty: 2-3815                     [16, 64, 16]              --
│    └─Clamp: 2-3816                     [16, 64, 16]              --
├─Conv1d: 1-292                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3817        --                        --
│    └─One: 2-3818                       [1]                       --
│    └─OutputScale: 2-3819               --                        --
│    └─Empty: 2-3820                     [64, 64, 1]               --
│    └─Empty: 2-3821                     [64, 64, 1]               --
│    └─Empty: 2-3822                     [64]                      --
│    └─Empty: 2-3823                     [64]                      --
│    └─Scaler: 2-3824                    [16, 64, 16]              --
│    └─Empty: 2-3825                     [16, 64, 16]              --
│    └─Empty: 2-3826                     [16, 64, 16]              --
│    └─Clamp: 2-3827                     [16, 64, 16]              --
├─Dropout: 1-293                         [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-294               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3828        --                        --
│    └─One: 2-3829                       [1]                       --
│    └─OutputScale: 2-3830               --                        --
│    └─Empty: 2-3831                     [64, 64, 3]               --
│    └─Empty: 2-3832                     [64, 64, 3]               --
│    └─Empty: 2-3833                     [64]                      --
│    └─Empty: 2-3834                     [64]                      --
│    └─BatchNorm1d: 2-3835               [16, 64, 16]              --
│    └─Scaler: 2-3836                    [16, 64, 16]              --
│    └─ReLU: 2-3837                      [16, 64, 16]              --
│    └─Empty: 2-3838                     [16, 64, 16]              --
│    └─Clamp: 2-3839                     [16, 64, 16]              --
├─Conv1d: 1-295                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3840        --                        --
│    └─One: 2-3841                       [1]                       --
│    └─OutputScale: 2-3842               --                        --
│    └─Empty: 2-3843                     [64, 64, 1]               --
│    └─Empty: 2-3844                     [64, 64, 1]               --
│    └─Empty: 2-3845                     [64]                      --
│    └─Empty: 2-3846                     [64]                      --
│    └─Scaler: 2-3847                    [16, 64, 16]              --
│    └─Empty: 2-3848                     [16, 64, 16]              --
│    └─Empty: 2-3849                     [16, 64, 16]              --
│    └─Clamp: 2-3850                     [16, 64, 16]              --
├─Dropout: 1-296                         [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-297               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3851        --                        --
│    └─One: 2-3852                       [1]                       --
│    └─OutputScale: 2-3853               --                        --
│    └─Empty: 2-3854                     [64, 64, 3]               --
│    └─Empty: 2-3855                     [64, 64, 3]               --
│    └─Empty: 2-3856                     [64]                      --
│    └─Empty: 2-3857                     [64]                      --
│    └─BatchNorm1d: 2-3858               [16, 64, 16]              --
│    └─Scaler: 2-3859                    [16, 64, 16]              --
│    └─ReLU: 2-3860                      [16, 64, 16]              --
│    └─Empty: 2-3861                     [16, 64, 16]              --
│    └─Clamp: 2-3862                     [16, 64, 16]              --
├─Conv1d: 1-298                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3863        --                        --
│    └─One: 2-3864                       [1]                       --
│    └─OutputScale: 2-3865               --                        --
│    └─Empty: 2-3866                     [64, 64, 1]               --
│    └─Empty: 2-3867                     [64, 64, 1]               --
│    └─Empty: 2-3868                     [64]                      --
│    └─Empty: 2-3869                     [64]                      --
│    └─Scaler: 2-3870                    [16, 64, 16]              --
│    └─Empty: 2-3871                     [16, 64, 16]              --
│    └─Empty: 2-3872                     [16, 64, 16]              --
│    └─Clamp: 2-3873                     [16, 64, 16]              --
├─Dropout: 1-299                         [16, 64, 16]              --
├─Conv1d: 1-300                          [16, 4, 16]               266
│    └─OutputShiftSqueeze: 2-3874        --                        --
│    └─One: 2-3875                       [1]                       --
│    └─OutputScale: 2-3876               --                        --
│    └─Empty: 2-3877                     [4, 64, 1]                --
│    └─Empty: 2-3878                     [4, 64, 1]                --
│    └─Empty: 2-3879                     [4]                       --
│    └─Empty: 2-3880                     [4]                       --
│    └─Scaler: 2-3881                    [16, 4, 16]               --
│    └─Empty: 2-3882                     [16, 4, 16]               --
│    └─Empty: 2-3883                     [16, 4, 16]               --
│    └─Clamp: 2-3884                     [16, 4, 16]               --
├─Conv1d: 1-301                          [16, 64, 16]              326
│    └─OutputShiftSqueeze: 2-3885        --                        --
│    └─One: 2-3886                       [1]                       --
│    └─OutputScale: 2-3887               --                        --
│    └─Empty: 2-3888                     [64, 4, 1]                --
│    └─Empty: 2-3889                     [64, 4, 1]                --
│    └─Empty: 2-3890                     [64]                      --
│    └─Empty: 2-3891                     [64]                      --
│    └─Scaler: 2-3892                    [16, 64, 16]              --
│    └─Empty: 2-3893                     [16, 64, 16]              --
│    └─Empty: 2-3894                     [16, 64, 16]              --
│    └─Clamp: 2-3895                     [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-302               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3896        --                        --
│    └─One: 2-3897                       [1]                       --
│    └─OutputScale: 2-3898               --                        --
│    └─Empty: 2-3899                     [64, 64, 3]               --
│    └─Empty: 2-3900                     [64, 64, 3]               --
│    └─Empty: 2-3901                     [64]                      --
│    └─Empty: 2-3902                     [64]                      --
│    └─BatchNorm1d: 2-3903               [16, 64, 16]              --
│    └─Scaler: 2-3904                    [16, 64, 16]              --
│    └─ReLU: 2-3905                      [16, 64, 16]              --
│    └─Empty: 2-3906                     [16, 64, 16]              --
│    └─Clamp: 2-3907                     [16, 64, 16]              --
├─Conv1d: 1-303                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3908        --                        --
│    └─One: 2-3909                       [1]                       --
│    └─OutputScale: 2-3910               --                        --
│    └─Empty: 2-3911                     [64, 64, 1]               --
│    └─Empty: 2-3912                     [64, 64, 1]               --
│    └─Empty: 2-3913                     [64]                      --
│    └─Empty: 2-3914                     [64]                      --
│    └─Scaler: 2-3915                    [16, 64, 16]              --
│    └─Empty: 2-3916                     [16, 64, 16]              --
│    └─Empty: 2-3917                     [16, 64, 16]              --
│    └─Clamp: 2-3918                     [16, 64, 16]              --
├─Dropout: 1-304                         [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-305               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3919        --                        --
│    └─One: 2-3920                       [1]                       --
│    └─OutputScale: 2-3921               --                        --
│    └─Empty: 2-3922                     [64, 64, 3]               --
│    └─Empty: 2-3923                     [64, 64, 3]               --
│    └─Empty: 2-3924                     [64]                      --
│    └─Empty: 2-3925                     [64]                      --
│    └─BatchNorm1d: 2-3926               [16, 64, 16]              --
│    └─Scaler: 2-3927                    [16, 64, 16]              --
│    └─ReLU: 2-3928                      [16, 64, 16]              --
│    └─Empty: 2-3929                     [16, 64, 16]              --
│    └─Clamp: 2-3930                     [16, 64, 16]              --
├─Conv1d: 1-306                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3931        --                        --
│    └─One: 2-3932                       [1]                       --
│    └─OutputScale: 2-3933               --                        --
│    └─Empty: 2-3934                     [64, 64, 1]               --
│    └─Empty: 2-3935                     [64, 64, 1]               --
│    └─Empty: 2-3936                     [64]                      --
│    └─Empty: 2-3937                     [64]                      --
│    └─Scaler: 2-3938                    [16, 64, 16]              --
│    └─Empty: 2-3939                     [16, 64, 16]              --
│    └─Empty: 2-3940                     [16, 64, 16]              --
│    └─Clamp: 2-3941                     [16, 64, 16]              --
├─Dropout: 1-307                         [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-308               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3942        --                        --
│    └─One: 2-3943                       [1]                       --
│    └─OutputScale: 2-3944               --                        --
│    └─Empty: 2-3945                     [64, 64, 3]               --
│    └─Empty: 2-3946                     [64, 64, 3]               --
│    └─Empty: 2-3947                     [64]                      --
│    └─Empty: 2-3948                     [64]                      --
│    └─BatchNorm1d: 2-3949               [16, 64, 16]              --
│    └─Scaler: 2-3950                    [16, 64, 16]              --
│    └─ReLU: 2-3951                      [16, 64, 16]              --
│    └─Empty: 2-3952                     [16, 64, 16]              --
│    └─Clamp: 2-3953                     [16, 64, 16]              --
├─Conv1d: 1-309                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3954        --                        --
│    └─One: 2-3955                       [1]                       --
│    └─OutputScale: 2-3956               --                        --
│    └─Empty: 2-3957                     [64, 64, 1]               --
│    └─Empty: 2-3958                     [64, 64, 1]               --
│    └─Empty: 2-3959                     [64]                      --
│    └─Empty: 2-3960                     [64]                      --
│    └─Scaler: 2-3961                    [16, 64, 16]              --
│    └─Empty: 2-3962                     [16, 64, 16]              --
│    └─Empty: 2-3963                     [16, 64, 16]              --
│    └─Clamp: 2-3964                     [16, 64, 16]              --
├─Dropout: 1-310                         [16, 64, 16]              --
├─FusedConv1dBNReLU: 1-311               [16, 64, 16]              12,358
│    └─OutputShiftSqueeze: 2-3965        --                        --
│    └─One: 2-3966                       [1]                       --
│    └─OutputScale: 2-3967               --                        --
│    └─Empty: 2-3968                     [64, 64, 3]               --
│    └─Empty: 2-3969                     [64, 64, 3]               --
│    └─Empty: 2-3970                     [64]                      --
│    └─Empty: 2-3971                     [64]                      --
│    └─BatchNorm1d: 2-3972               [16, 64, 16]              --
│    └─Scaler: 2-3973                    [16, 64, 16]              --
│    └─ReLU: 2-3974                      [16, 64, 16]              --
│    └─Empty: 2-3975                     [16, 64, 16]              --
│    └─Clamp: 2-3976                     [16, 64, 16]              --
├─Conv1d: 1-312                          [16, 64, 16]              4,166
│    └─OutputShiftSqueeze: 2-3977        --                        --
│    └─One: 2-3978                       [1]                       --
│    └─OutputScale: 2-3979               --                        --
│    └─Empty: 2-3980                     [64, 64, 1]               --
│    └─Empty: 2-3981                     [64, 64, 1]               --
│    └─Empty: 2-3982                     [64]                      --
│    └─Empty: 2-3983                     [64]                      --
│    └─Scaler: 2-3984                    [16, 64, 16]              --
│    └─Empty: 2-3985                     [16, 64, 16]              --
│    └─Empty: 2-3986                     [16, 64, 16]              --
│    └─Clamp: 2-3987                     [16, 64, 16]              --
├─Dropout: 1-313                         [16, 64, 16]              --
├─Conv1d: 1-314                          [16, 4, 16]               266
│    └─OutputShiftSqueeze: 2-3988        --                        --
│    └─One: 2-3989                       [1]                       --
│    └─OutputScale: 2-3990               --                        --
│    └─Empty: 2-3991                     [4, 64, 1]                --
│    └─Empty: 2-3992                     [4, 64, 1]                --
│    └─Empty: 2-3993                     [4]                       --
│    └─Empty: 2-3994                     [4]                       --
│    └─Scaler: 2-3995                    [16, 4, 16]               --
│    └─Empty: 2-3996                     [16, 4, 16]               --
│    └─Empty: 2-3997                     [16, 4, 16]               --
│    └─Clamp: 2-3998                     [16, 4, 16]               --
==========================================================================================
Total params: 646,438
Trainable params: 646,156
Non-trainable params: 282
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 201.33
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 3.839 | Acc: 31.000% | Wgt Acc: 31.443%
	I - Batch: 100 | Loss: 3.679 | Acc: 39.125% | Wgt Acc: 37.266%
	I - Batch: 150 | Loss: 3.577 | Acc: 38.875% | Wgt Acc: 37.948%
I - num batch: 160
I - Train -- Loss: 3.572 | Acc: 38.712% | Wgt Acc: 37.995% | LR: 1.000000e-03 | Dur: 155.16s
I - Confusion Matrix: [row->prediction - col->label]
[[355.  58.  71. 268.]
 [138. 272. 366. 117.]
 [ 99. 234. 280.  74.]
 [105.  14.  17.  79.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 4.793 | Acc: 41.284% | Wgt Acc: 37.432% | Dur: 16.18s
I - Confusion Matrix: [row->prediction - col->label]
[[79. 17. 21. 74.]
 [ 2. 11.  9.  2.]
 [ 7. 50. 45. 10.]
 [ 0.  0.  0.  0.]]

I - Local maximum validation set accuracy:  41.28

I - Validation set results: 
[14.         1.         2.        12.9332943]-
[50.         3.         0.         6.9598484]-[124.           2.           2.           7.97794676]-[127.           0.           0.          10.68066311]-[443.      2.      1.     15.875]-[567.           0.           0.          10.66407585]-[573.           1.           0.           4.12235832]-[615.           0.           0.          10.67014599]-[695.          1.          2.         14.9662838]-[722.           3.           0.          10.68097019]-[826.           0.           0.          10.67791176]-
[878.           0.           0.          10.55412292]-[1103.            0.            0.            6.14238358]-[1212.           3.           0.           6.6261282]-[1368.            0.            0.           10.50652122]-[2181.            2.            0.            5.80056763]-[2476.            2.            2.            4.39220047]-[2721.       2.       1.      15.875]-[2818.            1.            2.            9.39403343]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.            2.            2.            4.12468052]-[3482.            2.            0.            1.20398402]-[3536.           3.           0.           9.0631609]-[3625.            1.            2.           15.84633446]-[3909.            0.            0.           10.28323364]-[4035.            0.            0.           10.66070747]-[4140.            0.            0.            9.44865036]-[4214.            1.            0.            0.79305053]-[4346.            1.            0.            6.78594875]-[4581.            2.            2.           15.78307629]-
[4708.           3.           0.           4.2609601]-[4838.            3.            0.            3.13355088]-[4845.            1.            0.            1.46523261]-[4868.            0.            0.           10.58952045]-[4939.       0.       2.      15.875]-[4984.            2.            0.            6.11901808]-[5078.            1.            2.           12.78953171]-[5396.            0.            0.           10.68064976]-[5479.            1.            2.            5.34958363]-[5717.            0.            0.            3.18957758]-
[5843.            1.            2.            9.29438496]-[5949.            3.            0.           10.68101692]-[5987.            2.            2.           15.64199257]-[6014.            3.            2.            0.82505018]-[6033.            3.            0.           10.21983719]-[6313.            0.            0.           10.67860126]-[6421.            3.            0.           10.68057442]-[6500.            1.            1.            0.21937174]-[6583.            3.            0.           10.65190125]-[6683.            3.            0.            0.63623393]-
[6825.            2.            0.           10.21313477]-[6998.           3.           0.           6.8163991]-[7049.            3.            0.            9.11534691]-[7517.            1.            2.           15.69358158]-[7521.            1.            0.            1.21387076]-[7528.            1.            0.            3.02801776]-[7949.            1.            2.           10.88204193]-[8135.            1.            0.            8.06795597]-[8185.            3.            0.           10.64908981]-[8269.           3.           2.           9.8589983]-
[8273.            3.            0.           10.67984581]-[8543.            3.            0.           10.68075943]-[8666.           1.           0.           4.9325881]-[8672.            0.            0.           10.67889595]-[8903.           1.           2.          14.5755806]-[9001.            2.            2.           13.61947632]-[9036.            2.            2.           11.64601994]-[9281.            3.            2.            1.98538506]-[9300.            2.            2.            2.29671192]-[9571.            0.            0.            7.68200254]-
[9617.            1.            0.            2.32911015]-[9644.            2.            2.           11.38774204]-[9705.            2.            2.            5.92661715]-[9801.            0.            0.           10.63503456]-[9803.           3.           0.           4.9639492]-[9865.            3.            0.           10.68066883]-[9896.       2.       2.      15.875]-[10314.        1.        2.       15.875]-[10337.             3.             0.            10.68076229]-[10403.             0.             2.             5.18807411]-
[10653.             2.             2.            11.63720989]-[10704.             2.             0.            10.34293747]-[10719.        1.        1.       15.875]-[10727.        1.        1.       15.875]-[10836.             0.             0.            10.68092918]-[10969.             2.             0.             9.44213867]-[11042.             0.             0.             6.03464556]-[11088.             1.             2.             9.01594543]-[11322.             0.             0.            10.67963982]-[11398.        2.        1.       15.875]-
[11499.             0.             0.             2.02688742]-[11502.             3.             0.            10.67448807]-[11512.             3.             1.             0.36257774]-[11608.             1.             2.            15.84446812]-[11610.             0.             0.            10.60722733]-[11692.             0.             0.            10.67297459]-[11905.             0.             0.            10.68076706]-[11993.             1.             2.            14.93569374]-[12002.             2.             0.             1.40978479]-[12052.             0.             0.            10.67999172]-
[12201.             0.             0.            10.68060684]-[12235.        2.        2.       15.875]-[12320.        1.        2.       15.875]-[12377.        2.        2.       15.875]-[12398.             2.             0.            -0.18880191]-[12503.        1.        2.       15.875]-[12617.             0.             0.            -0.25682321]-[12685.             3.             0.            10.09544945]-[12738.             2.             0.             4.05772972]-[12742.             2.             2.            14.32561874]-
[12823.             0.             0.            10.68078709]-[13110.             1.             2.             2.37354136]-[13240.             3.             0.             9.99965382]-[13253.        1.        1.       15.875]-[13273.             0.             0.            10.67552757]-[13634.             1.             2.            13.53977013]-[13763.             2.             0.             5.98205853]-[13905.             3.             0.             6.89717293]-[14060.             2.             2.             4.89629507]-[14065.            3.            0.           10.6808939]-
[14147.             3.             0.             7.05804253]-[14595.        2.        2.       15.875]-[14687.             2.             2.             5.45892143]-[14788.             2.             2.             1.02851069]-[14869.             1.             2.            15.31650543]-[14872.             3.             0.             0.72437149]-[14877.             1.             1.             0.14464769]-[14927.             0.             0.             8.00965214]-[15066.             0.             0.            10.68066597]-[15175.             1.             2.            15.26538277]-
[15178.             2.             0.            10.47884941]-[15375.             3.             0.             5.34118748]-[15389.             3.             0.            10.68068886]-[15568.            2.            2.           12.9583931]-[15675.             3.             0.            10.00339031]-[15869.             1.             0.            -0.14239374]-[16207.             3.             0.             9.34526157]-[16236.             0.             0.             9.84762955]-[16302.             3.             0.            10.55338478]-[16331.             2.             2.            15.33344269]-
[16381.             0.             0.            10.59383869]-[16488.             1.             2.            15.72779942]-[16495.             0.             0.            10.68027496]-[16650.             0.             0.            10.68062782]-[16719.             1.             2.             3.70417404]-[16801.             0.             0.            10.68060303]-[16828.             0.             0.            10.68082619]-[17137.             3.             0.            10.68043709]-[17245.             1.             0.             0.71332574]-[17278.             3.             2.             4.32663393]-
[17282.             0.             0.             6.77272511]-[17311.             2.             2.            15.75266552]-[17336.             2.             1.             0.57453728]-[17608.             3.             0.            10.68077278]-[17627.             0.             0.             5.95432281]-[17877.             3.             0.             3.65795493]-[17924.             1.             0.             6.00243187]-[17984.             3.             0.            10.66195774]-[18211.             0.             0.            10.59233093]-[18276.             3.             0.            10.68054581]-
[18287.             1.             2.             7.65845013]-[18394.             0.             0.            10.62112427]-[18428.             0.             0.             9.51811409]-[18442.             0.             0.            10.67956352]-[18478.             3.             0.            10.67944908]-[18607.             0.             0.             9.77194595]-[18616.             0.             0.             8.95414162]-[18663.             0.             0.            10.68032169]-[18718.             0.             0.            10.65145588]-[18766.             2.             2.            15.81343079]-
[18824.             2.             2.            15.84325218]-[18890.             3.             2.            15.30935955]-[18930.        3.        2.       15.875]-[18938.             3.             0.            10.58451271]-[19817.             1.             2.            15.76292801]-[19839.             0.             2.             6.20624924]-[19930.             3.             0.            10.68057823]-[19944.        0.        1.       15.875]-[20036.             2.             2.            11.34267902]-[20101.             3.             0.             2.51767612]-
[20474.            1.            1.           -0.2274653]-[20547.             3.             2.            14.61603355]-[20929.        2.        2.       15.875]-[21245.             1.             2.            13.16679955]-[21257.             3.             0.             5.27612257]-[21293.            1.            2.            9.7742157]-[21316.             1.             0.             1.62607074]-[21384.        1.        2.       15.875]-[21448.        1.        2.       15.875]-[21483.             0.             0.            10.61433983]-
[21487.             2.             2.            15.39151382]-[21714.             0.             0.             2.23173738]-[21943.             3.             0.             3.83742166]-[21947.            0.            0.           10.5625782]-[21948.             0.             0.            10.68070984]-[21965.             2.             2.            11.27526665]-[21998.             1.             0.             1.18414295]-[22025.             0.             0.             1.85685432]-[22228.             3.             0.            10.63155937]-[22446.             1.             2.            14.44190216]-
[22494.             3.             0.            10.68085098]-[22757.             0.             0.            10.68107224]-[22811.             3.             0.            10.68100262]-[22976.             3.             1.             0.32736093]-[22985.             3.             0.            10.68087482]-[23014.             0.             0.            10.68071938]-[23112.        1.        2.       15.875]-[23144.             3.             0.            10.68084526]-[23168.             2.             0.             6.34216738]-[23219.             0.             0.             9.70263863]-
[23363.             3.             0.            10.68083191]-[23470.             0.             2.             5.41100454]-[23486.             2.             0.             4.26219416]-[23497.             0.             0.            10.68082809]-[23516.             0.             0.            10.68084335]-[23690.        1.        1.       15.875]-[23921.             2.             2.            15.52106953]-[23936.             1.             0.            10.35939217]-[24040.             3.             2.             5.56384277]-[24111.        1.        1.       15.875]-
[24182.             0.             0.            10.68080902]-[24238.             3.             0.            10.68057823]-[24290.             2.             0.            10.55081177]-[24345.             0.             0.             8.37719345]-[24364.             1.             2.             2.46446848]-[24427.             3.             0.            10.07005215]-[24477.             2.             2.            13.24694824]-[24495.             2.             2.             7.21857595]-[24893.             2.             2.            14.25164032]-[25012.             1.             2.             4.04694414]-
[25121.             2.             2.            10.22489357]-[25165.            3.            0.           10.6712513]-[25183.             0.             0.            10.59002686]-[25297.             3.             0.            10.49526882]-[25398.             0.             0.            10.67987823]-[25574.             2.             0.             5.14185143]-[25644.        1.        2.       15.875]-[25718.             1.             2.             3.94539261]-[25774.            2.            0.           10.6695261]-[26032.             3.             0.             9.84350395]-
[26051.             3.             0.            10.68086338]-[26120.             0.             0.             8.68855381]-[26321.             1.             2.             1.51257217]-[26732.            1.            2.            7.6561532]-[26784.             3.             0.            10.68100166]-[26827.             3.             0.            10.46352863]-[26833.             0.             0.            10.68075371]-[26838.             2.             0.             2.20725965]-[26860.        1.        2.       15.875]-[26948.             0.             1.            -0.49076051]-
[27049.             3.             0.            10.66986752]-[27098.             1.             2.            15.37413025]-[27526.             0.             0.             6.80942059]-[27639.             3.             0.            10.68066216]-[27698.             3.             0.            10.68070602]-[27772.            0.            0.           10.6809597]-[27890.             1.             1.             0.06682089]-[28040.             0.             0.            10.41742706]-[28503.        2.        1.       15.875]-[28577.             1.             2.            15.85585403]-
[28959.             0.             0.            10.68074322]-[29198.            3.            0.           10.0811224]-[29777.             0.             0.            10.68062401]-[29877.             2.             2.             3.09104657]-[30035.             1.             2.            11.22277927]-[30098.             0.             0.            10.57552338]-[30326.             1.             2.            13.75010872]-[30572.             2.             0.             3.65422678]-[30716.             0.             2.             4.78733063]-[30806.           2.           0.           4.470294]-
[30906.        1.        2.       15.875]-[31007.             0.             0.             0.04683399]-[31181.             3.             0.             9.34428215]-[31238.             0.             0.            10.68053246]-[31347.             0.             0.            10.68061733]-[31422.             2.             2.             8.94808006]-[31429.             3.             0.             6.42179966]-[31431.             0.             0.            10.68005371]-[31432.             1.             2.             5.86112213]-[31477.             0.             0.            10.68074036]-
[31524.            1.            0.            0.4002507]-[31597.             1.             2.            11.65319824]-[31619.            1.            1.           -0.3245042]-[31701.             0.             0.            10.67708397]-[31755.             0.             0.            10.65115547]-[31854.             3.             0.             1.52090621]-[32074.             1.             2.             9.27263832]-[32078.           3.           0.           0.928298]-[32111.        1.        1.       15.875]-[32127.             1.             2.            14.84768295]-
[32140.             3.             0.            10.67668915]-[32263.             2.             2.             0.88627744]-[32365.             0.             2.             3.72295237]-[32411.             2.             0.            10.68081284]-[32429.             3.             0.            10.63636684]-[32473.             3.             0.            10.64972115]-[32574.             3.             0.            10.68087196]-[32584.             0.             0.            10.55681133]-[32622.             0.             2.            13.09584045]-[32858.             3.             0.            10.64225197]-
[32969.             3.             0.            10.68036842]-[33016.             2.             2.            15.70075989]-[33031.             1.             0.            10.27472019]-[33035.             2.             2.            10.19894409]-[33133.            2.            2.           15.8201704]-[33173.             2.             2.             1.60957301]-[33175.        3.        2.       15.875]-[33306.             3.             2.             2.32829666]-[33309.             2.             2.             0.80949348]-[33474.             0.             0.            10.66734791]-
[33478.            2.            0.            6.4760623]-[33618.             1.             2.             1.58430982]-[33712.             0.             0.             6.86107302]-[33782.        2.        1.       15.875]-[33914.             3.             0.             3.78350186]-[34076.             3.             0.            10.67846298]-[34112.             2.             2.             6.68262434]-[34138.             2.             2.            12.99444008]-[34239.            1.            2.            8.0043993]-[34364.        2.        1.       15.875]-
[34617.             1.             2.             6.62318897]-[34751.             3.             0.            10.67433739]-[34783.        2.        2.       15.875]-[35015.             3.             0.             3.14981389]-[35018.        1.        2.       15.875]-[35288.             2.             2.             3.10857964]-
---------------------------
I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 3.269 | Acc: 46.375% | Wgt Acc: 43.132%
	I - Batch: 100 | Loss: 3.284 | Acc: 45.938% | Wgt Acc: 44.289%
	I - Batch: 150 | Loss: 3.262 | Acc: 44.375% | Wgt Acc: 43.776%
I - num batch: 160
I - Train -- Loss: 3.247 | Acc: 43.973% | Wgt Acc: 43.604% | LR: 1.000000e-03 | Dur: 158.52s
I - Confusion Matrix: [row->prediction - col->label]
[[347.  35.  55. 256.]
 [ 46. 282. 312.  63.]
 [ 67. 230. 324.  52.]
 [237.  31.  43. 167.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.027 | Acc: 42.508% | Wgt Acc: 42.323% | Dur: 16.39s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  4.  4. 54.]
 [ 5. 46. 46.  3.]
 [ 5. 19. 15.  8.]
 [21.  9. 10. 21.]]

I - Local maximum validation set accuracy:  42.51

I - Validation set results: 
[14.     1.     1.    15.875]-
[50.     3.     0.    15.875]-[124.      2.      1.     15.875]-[127.      0.      0.     15.875]-[443.      2.      1.     15.875]-[567.      0.      0.     15.875]-[573.           1.           3.           9.11378956]-[615.      0.      0.     15.875]-[695.           1.           2.          15.76601219]-[722.      3.      0.     15.875]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.       0.       3.      15.875]-[1212.       3.       0.      15.875]-[1368.       0.       0.      15.875]-[2181.       2.       0.      15.875]-[2476.       2.       1.      15.875]-[2721.       2.       1.      15.875]-[2818.           1.           1.           7.2580862]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.            2.            2.           13.27427483]-[3482.           2.           1.           3.8140533]-[3536.            3.            3.           15.82424736]-[3625.       1.       1.      15.875]-[3909.            0.            3.           13.86530304]-[4035.       0.       0.      15.875]-[4140.       0.       0.      15.875]-[4214.            1.            3.           15.83764458]-[4346.       1.       0.      15.875]-[4581.       2.       1.      15.875]-
[4708.            3.            1.            0.99111485]-[4838.            3.            3.            2.27649403]-[4845.            1.            1.            4.07334709]-[4868.       0.       0.      15.875]-[4939.            0.            2.           15.87117004]-[4984.            2.            3.           13.59877205]-[5078.            1.            2.           14.42815971]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.            1.            2.           15.25795269]-[5949.       3.       0.      15.875]-[5987.       2.       1.      15.875]-[6014.           3.           3.           6.1471138]-[6033.       3.       0.      15.875]-[6313.       0.       0.      15.875]-[6421.       3.       0.      15.875]-[6500.            1.            1.            1.56840169]-[6583.       3.       0.      15.875]-[6683.            3.            3.            5.08381319]-
[6825.            2.            3.           13.62284851]-[6998.            3.            3.           13.60346413]-[7049.            3.            3.           15.85350037]-[7517.       1.       1.      15.875]-[7521.            1.            3.            9.19471931]-[7528.            1.            1.            2.64698648]-[7949.            1.            2.           15.43702126]-[8135.            1.            3.            0.57225049]-[8185.       3.       0.      15.875]-[8269.           3.           2.           6.9406786]-
[8273.       3.       0.      15.875]-[8543.       3.       0.      15.875]-[8666.            1.            2.           11.54345036]-[8672.       0.       0.      15.875]-[8903.            1.            2.           14.66522789]-[9001.       2.       1.      15.875]-[9036.       2.       1.      15.875]-[9281.            3.            3.            4.13389921]-[9300.            2.            2.           11.82734299]-[9571.       0.       3.      15.875]-
[9617.            1.            1.            3.66498303]-[9644.       2.       1.      15.875]-[9705.       2.       1.      15.875]-[9801.       0.       0.      15.875]-[9803.            3.            3.           14.71339035]-[9865.       3.       0.      15.875]-[9896.       2.       1.      15.875]-[10314.        1.        1.       15.875]-[10337.        3.        0.       15.875]-[10403.             0.             1.             0.28344083]-
[10653.             2.             2.            13.79364204]-[10704.             2.             3.            11.60779953]-[10719.        1.        1.       15.875]-[10727.        1.        1.       15.875]-[10836.        0.        0.       15.875]-[10969.        2.        0.       15.875]-[11042.             0.             3.            14.22229958]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        1.       15.875]-
[11499.             0.             3.             3.97173381]-[11502.        3.        0.       15.875]-[11512.             3.             3.             0.66923112]-[11608.        1.        1.       15.875]-[11610.             0.             3.            13.74565697]-[11692.        0.        0.       15.875]-[11905.        0.        0.       15.875]-[11993.        1.        1.       15.875]-[12002.             2.             3.            10.59675026]-[12052.        0.        3.       15.875]-
[12201.        0.        0.       15.875]-[12235.        2.        1.       15.875]-[12320.             1.             2.            13.73332024]-[12377.             2.             2.            15.11256313]-[12398.             2.             3.            14.38177109]-[12503.        1.        1.       15.875]-[12617.             0.             1.             2.93593121]-[12685.        3.        0.       15.875]-[12738.             2.             1.             2.14259219]-[12742.        2.        1.       15.875]-
[12823.        0.        0.       15.875]-[13110.             1.             1.             0.30797994]-[13240.        3.        0.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.             1.             2.            15.73548508]-[13763.             2.             3.             9.86255074]-[13905.        3.        0.       15.875]-[14060.        2.        1.       15.875]-[14065.        3.        0.       15.875]-
[14147.             3.             2.             9.45873928]-[14595.        2.        1.       15.875]-[14687.             2.             1.             3.70463133]-[14788.             2.             2.            15.15420723]-[14869.             1.             2.            10.35428429]-[14872.             3.             3.            13.37539768]-[14877.             1.             1.             2.54542589]-[14927.             0.             3.            15.71308899]-[15066.        0.        0.       15.875]-[15175.        1.        1.       15.875]-
[15178.        2.        0.       15.875]-[15375.        3.        0.       15.875]-[15389.        3.        0.       15.875]-[15568.             2.             2.            12.02121639]-[15675.        3.        0.       15.875]-[15869.             1.             1.             4.09186363]-[16207.            3.            3.            1.0637008]-[16236.             0.             3.            15.37782192]-[16302.        3.        0.       15.875]-[16331.        2.        1.       15.875]-
[16381.        0.        0.       15.875]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.        1.        1.       15.875]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.             1.             2.            11.84446907]-[17278.            3.            3.           14.8191967]-
[17282.             0.             3.            15.53574944]-[17311.             2.             2.            15.87078285]-[17336.             2.             2.            15.12632179]-[17608.        3.        0.       15.875]-[17627.            0.            3.            7.9168911]-[17877.        3.        3.       15.875]-[17924.             1.             1.             0.96365952]-[17984.        3.        0.       15.875]-[18211.        0.        0.       15.875]-[18276.        3.        0.       15.875]-
[18287.             1.             2.            12.60961437]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        0.       15.875]-[18478.        3.        0.       15.875]-[18607.             0.             3.             0.23025417]-[18616.        0.        0.       15.875]-[18663.        0.        0.       15.875]-[18718.        0.        0.       15.875]-[18766.        2.        1.       15.875]-
[18824.        2.        1.       15.875]-[18890.             3.             1.             3.28189039]-[18930.             3.             2.             8.65280628]-[18938.             3.             3.            15.76487255]-[19817.             1.             2.            15.28836823]-[19839.        0.        1.       15.875]-[19930.        3.        0.       15.875]-[19944.        0.        1.       15.875]-[20036.        2.        1.       15.875]-[20101.             3.             3.            15.08615589]-
[20474.             1.             2.            14.27188015]-[20547.             3.             3.             4.73586369]-[20929.        2.        1.       15.875]-[21245.        1.        1.       15.875]-[21257.             3.             3.            14.14659977]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        1.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        1.       15.875]-[21714.             0.             3.            15.66738701]-[21943.             3.             2.             9.84470081]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        1.       15.875]-[21998.             1.             1.             7.84454536]-[22025.             0.             3.            14.16394997]-[22228.        3.        0.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.        3.        0.       15.875]-[22976.            3.            2.           10.8037405]-[22985.        3.        0.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.        3.        0.       15.875]-[23168.            2.            3.            6.3065691]-[23219.        0.        0.       15.875]-
[23363.        3.        0.       15.875]-[23470.             0.             2.            10.52988243]-[23486.        2.        3.       15.875]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.        1.        1.       15.875]-[23921.        2.        1.       15.875]-[23936.             1.             3.             9.38174057]-[24040.             3.             2.            13.62101173]-[24111.        1.        1.       15.875]-
[24182.        0.        0.       15.875]-[24238.        3.        0.       15.875]-[24290.        2.        3.       15.875]-[24345.            0.            3.           15.6126976]-[24364.             1.             3.             7.20724297]-[24427.        3.        0.       15.875]-[24477.        2.        1.       15.875]-[24495.             2.             2.            12.58810616]-[24893.        2.        1.       15.875]-[25012.             1.             2.            11.36560059]-
[25121.        2.        1.       15.875]-[25165.        3.        0.       15.875]-[25183.             0.             3.            14.45916462]-[25297.        3.        0.       15.875]-[25398.        0.        0.       15.875]-[25574.             2.             1.             2.84763002]-[25644.        1.        1.       15.875]-[25718.             1.             3.            10.68678761]-[25774.        2.        1.       15.875]-[26032.        3.        0.       15.875]-
[26051.        3.        0.       15.875]-[26120.             0.             2.            11.89235878]-[26321.        1.        1.       15.875]-[26732.             1.             2.            13.48474503]-[26784.        3.        0.       15.875]-[26827.        3.        0.       15.875]-[26833.        0.        0.       15.875]-[26838.             2.             1.             0.90063858]-[26860.        1.        1.       15.875]-[26948.             0.             3.            14.07071877]-
[27049.        3.        0.       15.875]-[27098.             1.             2.            15.53512096]-[27526.             0.             3.            15.53482723]-[27639.        3.        0.       15.875]-[27698.        3.        0.       15.875]-[27772.        0.        0.       15.875]-[27890.             1.             1.             7.85585308]-[28040.             0.             3.             1.52661169]-[28503.        2.        1.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             3.             0.91645432]-[29777.        0.        0.       15.875]-[29877.             2.             2.             4.34792328]-[30035.        1.        1.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.             2.             1.             7.24816895]-[30716.             0.             2.            14.17958164]-[30806.             2.             2.             7.22674322]-
[30906.        1.        1.       15.875]-[31007.            0.            1.            4.3627615]-[31181.        3.        0.       15.875]-[31238.        0.        0.       15.875]-[31347.        0.        0.       15.875]-[31422.        2.        1.       15.875]-[31429.             3.             3.            15.03396988]-[31431.        0.        0.       15.875]-[31432.            1.            2.           14.1703043]-[31477.        0.        0.       15.875]-
[31524.             1.             3.             3.34457111]-[31597.        1.        1.       15.875]-[31619.             1.             3.            15.64936066]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.            3.            3.           15.6836319]-[32074.        1.        0.       15.875]-[32078.            3.            2.           15.8708477]-[32111.        1.        1.       15.875]-[32127.        1.        2.       15.875]-
[32140.        3.        0.       15.875]-[32263.             2.             2.             5.34686279]-[32365.             0.             3.             6.89790535]-[32411.        2.        0.       15.875]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        0.       15.875]-[32584.        0.        0.       15.875]-[32622.             0.             2.            13.73453903]-[32858.        3.        0.       15.875]-
[32969.        3.        0.       15.875]-[33016.        2.        1.       15.875]-[33031.        1.        0.       15.875]-[33035.        2.        1.       15.875]-[33133.        2.        1.       15.875]-[33173.             2.             2.            15.57020569]-[33175.        3.        1.       15.875]-[33306.             3.             2.            12.49245358]-[33309.             2.             2.            11.29443169]-[33474.             0.             3.            12.34995079]-
[33478.             2.             3.            15.86982346]-[33618.        1.        0.       15.875]-[33712.        0.        0.       15.875]-[33782.        2.        1.       15.875]-[33914.             3.             3.            13.15934372]-[34076.        3.        0.       15.875]-[34112.             2.             2.            12.63693619]-[34138.        2.        1.       15.875]-[34239.        1.        1.       15.875]-[34364.        2.        1.       15.875]-
[34617.             1.             2.            14.53027916]-[34751.        3.        0.       15.875]-[34783.        2.        1.       15.875]-[35015.        3.        0.       15.875]-[35018.        1.        1.       15.875]-[35288.            2.            1.            0.2749877]-
---------------------------
I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 3.208 | Acc: 48.500% | Wgt Acc: 45.578%
	I - Batch: 100 | Loss: 3.155 | Acc: 44.750% | Wgt Acc: 43.744%
	I - Batch: 150 | Loss: 3.172 | Acc: 43.417% | Wgt Acc: 42.730%
I - num batch: 160
I - Train -- Loss: 3.183 | Acc: 43.384% | Wgt Acc: 42.932% | LR: 1.000000e-03 | Dur: 158.35s
I - Confusion Matrix: [row->prediction - col->label]
[[318.  19.  28. 221.]
 [ 23. 213. 286.  29.]
 [ 68. 308. 354.  68.]
 [288.  38.  66. 220.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 4.836 | Acc: 38.838% | Wgt Acc: 43.071% | Dur: 15.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 1. 45. 39.  4.]
 [ 5.  8.  1.  1.]
 [82. 25. 35. 81.]]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 3.177 | Acc: 42.500% | Wgt Acc: 44.714%
	I - Batch: 100 | Loss: 3.133 | Acc: 45.375% | Wgt Acc: 45.126%
	I - Batch: 150 | Loss: 3.158 | Acc: 44.792% | Wgt Acc: 44.366%
I - num batch: 160
I - Train -- Loss: 3.175 | Acc: 44.209% | Wgt Acc: 43.914% | LR: 1.000000e-03 | Dur: 154.35s
I - Confusion Matrix: [row->prediction - col->label]
[[337.  11.  24. 237.]
 [ 23. 256. 312.  31.]
 [ 70. 259. 329.  66.]
 [267.  52.  69. 204.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.085 | Acc: 37.309% | Wgt Acc: 40.829% | Dur: 16.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 1. 31. 34.  1.]
 [ 3. 16.  9.  3.]
 [84. 31. 32. 82.]]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 3.113 | Acc: 43.625% | Wgt Acc: 45.128%
	I - Batch: 100 | Loss: 3.077 | Acc: 44.000% | Wgt Acc: 45.114%
	I - Batch: 150 | Loss: 3.091 | Acc: 45.875% | Wgt Acc: 45.389%
I - num batch: 160
I - Train -- Loss: 3.078 | Acc: 46.093% | Wgt Acc: 45.435% | LR: 1.000000e-03 | Dur: 155.98s
I - Confusion Matrix: [row->prediction - col->label]
[[289.   6.  13. 190.]
 [ 11. 177. 215.  11.]
 [ 62. 349. 445.  74.]
 [335.  46.  61. 263.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.104 | Acc: 44.037% | Wgt Acc: 44.633% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[51.  3.  4. 50.]
 [ 9. 64. 49.  8.]
 [ 6.  6. 12. 11.]
 [22.  5. 10. 17.]]

I - Local maximum validation set accuracy:  44.04

I - Validation set results: 
[14.     1.     1.    15.875]-
[50.          3.          2.          9.62761211]-[124.      2.      1.     15.875]-[127.      0.      0.     15.875]-[443.      2.      1.     15.875]-[567.      0.      0.     15.875]-[573.           1.           2.           9.28495598]-[615.      0.      0.     15.875]-[695.      1.      1.     15.875]-[722.      3.      0.     15.875]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.            0.            3.           11.87285423]-[1212.       3.       0.      15.875]-[1368.       0.       0.      15.875]-[2181.       2.       0.      15.875]-[2476.       2.       1.      15.875]-[2721.       2.       1.      15.875]-[2818.       1.       1.      15.875]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.       2.       1.      15.875]-[3482.            2.            2.            1.41633976]-[3536.            3.            3.           15.72214508]-[3625.       1.       1.      15.875]-[3909.            0.            1.           13.77302361]-[4035.       0.       0.      15.875]-[4140.            0.            3.           15.77589417]-[4214.            1.            3.            4.34917259]-[4346.       1.       0.      15.875]-[4581.       2.       1.      15.875]-
[4708.          3.          1.         15.788064]-[4838.            3.            2.            7.53350687]-[4845.       1.       1.      15.875]-[4868.       0.       0.      15.875]-[4939.       0.       1.      15.875]-[4984.            2.            3.            3.29140806]-[5078.       1.       1.      15.875]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.            0.            3.           14.07707882]-
[5843.       1.       1.      15.875]-[5949.       3.       0.      15.875]-[5987.       2.       1.      15.875]-[6014.            3.            3.            7.23635101]-[6033.       3.       0.      15.875]-[6313.       0.       0.      15.875]-[6421.            3.            3.           13.04015827]-[6500.            1.            1.           15.20004082]-[6583.       3.       0.      15.875]-[6683.            3.            2.            0.68019676]-
[6825.       2.       0.      15.875]-[6998.            3.            3.            5.96468258]-[7049.            3.            3.           12.56003666]-[7517.       1.       1.      15.875]-[7521.       1.       0.      15.875]-[7528.            1.            2.            9.96212673]-[7949.       1.       1.      15.875]-[8135.          1.          3.         14.749506]-[8185.       3.       0.      15.875]-[8269.            3.            2.            9.54144573]-
[8273.       3.       0.      15.875]-[8543.       3.       0.      15.875]-[8666.           1.           1.          15.7858448]-[8672.       0.       0.      15.875]-[8903.       1.       1.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       1.      15.875]-[9281.       3.       1.      15.875]-[9300.       2.       1.      15.875]-[9571.            0.            3.           11.03960228]-
[9617.            1.            2.            2.07767868]-[9644.       2.       1.      15.875]-[9705.            2.            3.            3.00749183]-[9801.       0.       0.      15.875]-[9803.           3.           3.           9.5465126]-[9865.       3.       0.      15.875]-[9896.       2.       1.      15.875]-[10314.        1.        1.       15.875]-[10337.        3.        0.       15.875]-[10403.             0.             1.            14.53148556]-
[10653.             2.             2.            15.21033287]-[10704.        2.        1.       15.875]-[10719.        1.        1.       15.875]-[10727.        1.        1.       15.875]-[10836.        0.        0.       15.875]-[10969.             2.             3.            15.17344666]-[11042.             0.             3.            14.10778522]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        1.       15.875]-
[11499.             0.             2.             9.65210819]-[11502.        3.        0.       15.875]-[11512.        3.        1.       15.875]-[11608.        1.        1.       15.875]-[11610.        0.        0.       15.875]-[11692.        0.        0.       15.875]-[11905.        0.        0.       15.875]-[11993.        1.        1.       15.875]-[12002.        2.        0.       15.875]-[12052.        0.        0.       15.875]-
[12201.        0.        0.       15.875]-[12235.        2.        1.       15.875]-[12320.        1.        1.       15.875]-[12377.        2.        1.       15.875]-[12398.             2.             3.            15.75173759]-[12503.        1.        1.       15.875]-[12617.        0.        1.       15.875]-[12685.             3.             3.             8.99981785]-[12738.             2.             3.            14.83584976]-[12742.        2.        1.       15.875]-
[12823.        0.        0.       15.875]-[13110.        1.        1.       15.875]-[13240.        3.        0.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.             2.             2.             7.74254274]-[13905.             3.             3.            15.22393703]-[14060.        2.        1.       15.875]-[14065.        3.        0.       15.875]-
[14147.        3.        0.       15.875]-[14595.        2.        1.       15.875]-[14687.        2.        1.       15.875]-[14788.        2.        1.       15.875]-[14869.        1.        1.       15.875]-[14872.             3.             0.            15.60153389]-[14877.             1.             1.            14.20860481]-[14927.        0.        0.       15.875]-[15066.        0.        0.       15.875]-[15175.        1.        1.       15.875]-
[15178.            2.            3.            7.6679945]-[15375.        3.        0.       15.875]-[15389.        3.        0.       15.875]-[15568.             2.             2.             9.33247948]-[15675.        3.        0.       15.875]-[15869.            1.            1.           14.3198185]-[16207.             3.             3.             5.18247366]-[16236.            0.            3.           12.2107811]-[16302.        3.        0.       15.875]-[16331.        2.        1.       15.875]-
[16381.             0.             3.            14.02466106]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.        1.        1.       15.875]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.             1.             1.            15.72774696]-[17278.             3.             3.            10.45279598]-
[17282.             0.             3.             5.51490831]-[17311.        2.        1.       15.875]-[17336.        2.        1.       15.875]-[17608.        3.        0.       15.875]-[17627.             0.             3.             7.58639812]-[17877.             3.             2.             5.62560081]-[17924.             1.             2.             8.99218845]-[17984.        3.        0.       15.875]-[18211.             0.             3.             7.09042454]-[18276.        3.        0.       15.875]-
[18287.             1.             1.            15.83815765]-[18394.        0.        0.       15.875]-[18428.             0.             2.            10.71586704]-[18442.        0.        0.       15.875]-[18478.        3.        0.       15.875]-[18607.           0.           2.           2.857903]-[18616.             0.             3.            14.17952538]-[18663.        0.        0.       15.875]-[18718.        0.        0.       15.875]-[18766.        2.        1.       15.875]-
[18824.        2.        1.       15.875]-[18890.             3.             3.             5.47653103]-[18930.        3.        1.       15.875]-[18938.        3.        0.       15.875]-[19817.        1.        1.       15.875]-[19839.             0.             3.             2.61537433]-[19930.        3.        0.       15.875]-[19944.        0.        1.       15.875]-[20036.        2.        1.       15.875]-[20101.             3.             3.            11.16657829]-
[20474.        1.        1.       15.875]-[20547.             3.             2.             8.33026886]-[20929.        2.        1.       15.875]-[21245.        1.        1.       15.875]-[21257.             3.             3.             7.21352911]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        1.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        1.       15.875]-[21714.             0.             3.            15.10148335]-[21943.        3.        1.       15.875]-[21947.             0.             3.             5.58106804]-[21948.        0.        0.       15.875]-[21965.        2.        1.       15.875]-[21998.            1.            2.            7.7524457]-[22025.        0.        1.       15.875]-[22228.        3.        0.       15.875]-[22446.             1.             1.            15.86150932]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.        3.        0.       15.875]-[22976.        3.        1.       15.875]-[22985.        3.        0.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.        3.        0.       15.875]-[23168.             2.             3.             5.76713943]-[23219.             0.             2.             8.10050201]-
[23363.        3.        0.       15.875]-[23470.            0.            2.            4.8032136]-[23486.             2.             2.             6.47419119]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.        1.        1.       15.875]-[23921.        2.        1.       15.875]-[23936.             1.             1.            15.24530411]-[24040.             3.             2.             3.13365245]-[24111.        1.        1.       15.875]-
[24182.        0.        0.       15.875]-[24238.        3.        0.       15.875]-[24290.             2.             3.             5.42313099]-[24345.             0.             3.            13.62513733]-[24364.        1.        1.       15.875]-[24427.             3.             2.            10.82428265]-[24477.        2.        1.       15.875]-[24495.        2.        1.       15.875]-[24893.        2.        1.       15.875]-[25012.        1.        1.       15.875]-
[25121.        2.        1.       15.875]-[25165.             3.             2.             1.44465923]-[25183.        0.        0.       15.875]-[25297.             3.             3.             5.05111504]-[25398.        0.        0.       15.875]-[25574.             2.             2.            12.87297249]-[25644.        1.        1.       15.875]-[25718.        1.        1.       15.875]-[25774.            2.            2.            7.0909214]-[26032.        3.        0.       15.875]-
[26051.        3.        0.       15.875]-[26120.             0.             3.             9.41399002]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        0.       15.875]-[26827.             3.             2.             3.30972719]-[26833.        0.        0.       15.875]-[26838.             2.             2.            12.39975452]-[26860.        1.        1.       15.875]-[26948.             0.             2.             4.03628016]-
[27049.        3.        0.       15.875]-[27098.             1.             2.             3.68112326]-[27526.        0.        0.       15.875]-[27639.        3.        0.       15.875]-[27698.        3.        0.       15.875]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.             0.             3.            12.34880543]-[28503.        2.        1.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             3.             3.16325521]-[29777.        0.        0.       15.875]-[29877.             2.             1.            15.68922234]-[30035.        1.        1.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        1.       15.875]-[30716.        0.        1.       15.875]-[30806.             2.             2.             0.54140872]-
[30906.        1.        1.       15.875]-[31007.             0.             3.             4.78149414]-[31181.        3.        0.       15.875]-[31238.        0.        0.       15.875]-[31347.        0.        0.       15.875]-[31422.             2.             1.            15.24975395]-[31429.             3.             2.             5.66407776]-[31431.             0.             3.             9.09596157]-[31432.        1.        1.       15.875]-[31477.        0.        0.       15.875]-
[31524.             1.             3.            11.06692696]-[31597.        1.        1.       15.875]-[31619.             1.             3.            11.00531578]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        0.       15.875]-[32074.             1.             1.            15.63510513]-[32078.             3.             3.            13.92275238]-[32111.             1.             1.            15.72646904]-[32127.        1.        1.       15.875]-
[32140.        3.        0.       15.875]-[32263.             2.             2.            10.29373932]-[32365.             0.             3.            12.91686726]-[32411.        2.        0.       15.875]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        0.       15.875]-[32584.        0.        1.       15.875]-[32622.        0.        1.       15.875]-[32858.        3.        0.       15.875]-
[32969.        3.        0.       15.875]-[33016.        2.        1.       15.875]-[33031.             1.             3.             0.66159332]-[33035.        2.        1.       15.875]-[33133.        2.        1.       15.875]-[33173.             2.             2.             7.43305683]-[33175.        3.        1.       15.875]-[33306.        3.        1.       15.875]-[33309.             2.             2.             2.27819467]-[33474.             0.             3.             8.33625412]-
[33478.            2.            3.           15.1007576]-[33618.        1.        0.       15.875]-[33712.             0.             3.            14.75038815]-[33782.        2.        1.       15.875]-[33914.        3.        0.       15.875]-[34076.        3.        0.       15.875]-[34112.        2.        1.       15.875]-[34138.        2.        1.       15.875]-[34239.             1.             1.            14.36344719]-[34364.        2.        1.       15.875]-
[34617.        1.        1.       15.875]-[34751.        3.        0.       15.875]-[34783.        2.        1.       15.875]-[35015.             3.             3.            10.06591797]-[35018.        1.        1.       15.875]-[35288.             2.             3.             3.53740788]-
---------------------------
I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 3.069 | Acc: 43.500% | Wgt Acc: 46.139%
	I - Batch: 100 | Loss: 3.037 | Acc: 46.438% | Wgt Acc: 47.506%
	I - Batch: 150 | Loss: 3.048 | Acc: 44.667% | Wgt Acc: 45.275%
I - num batch: 160
I - Train -- Loss: 3.081 | Acc: 44.641% | Wgt Acc: 45.108% | LR: 1.000000e-03 | Dur: 155.59s
I - Confusion Matrix: [row->prediction - col->label]
[[339.   5.   8. 221.]
 [ 18. 330. 410.  30.]
 [ 46. 201. 247.  66.]
 [294.  42.  69. 221.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 4.964 | Acc: 38.838% | Wgt Acc: 35.394% | Dur: 15.95s
I - Confusion Matrix: [row->prediction - col->label]
[[74. 10. 22. 69.]
 [ 0.  0.  0.  0.]
 [ 4. 49. 40.  4.]
 [10. 19. 13. 13.]]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 2.997 | Acc: 48.500% | Wgt Acc: 47.754%
	I - Batch: 100 | Loss: 2.986 | Acc: 47.875% | Wgt Acc: 47.689%
	I - Batch: 150 | Loss: 3.000 | Acc: 46.750% | Wgt Acc: 46.741%
I - num batch: 160
I - Train -- Loss: 3.008 | Acc: 46.761% | Wgt Acc: 46.718% | LR: 1.000000e-03 | Dur: 153.82s
I - Confusion Matrix: [row->prediction - col->label]
[[293.   3.   8. 174.]
 [  3. 233. 288.  11.]
 [ 66. 302. 381.  69.]
 [335.  40.  57. 284.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 4.997 | Acc: 41.896% | Wgt Acc: 38.315% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7. 14. 63.]
 [ 0.  0.  0.  0.]
 [ 6. 57. 51.  7.]
 [12. 14. 10. 16.]]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 3.088 | Acc: 46.000% | Wgt Acc: 44.681%
	I - Batch: 100 | Loss: 3.006 | Acc: 46.625% | Wgt Acc: 45.094%
	I - Batch: 150 | Loss: 3.013 | Acc: 46.958% | Wgt Acc: 45.813%
I - num batch: 160
I - Train -- Loss: 3.016 | Acc: 46.839% | Wgt Acc: 45.807% | LR: 1.000000e-03 | Dur: 156.28s
I - Confusion Matrix: [row->prediction - col->label]
[[382.   6.  17. 272.]
 [  9. 226. 256.  10.]
 [ 52. 310. 405.  76.]
 [254.  36.  56. 180.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.004 | Acc: 43.119% | Wgt Acc: 41.712% | Dur: 16.10s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  0.  6. 51.]
 [ 2. 26. 23.  2.]
 [ 6. 39. 32.  9.]
 [21. 13. 14. 24.]]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 3.024 | Acc: 46.000% | Wgt Acc: 47.336%
	I - Batch: 100 | Loss: 3.049 | Acc: 45.125% | Wgt Acc: 46.327%
	I - Batch: 150 | Loss: 3.012 | Acc: 47.000% | Wgt Acc: 47.266%
I - num batch: 160
I - Train -- Loss: 3.005 | Acc: 46.957% | Wgt Acc: 47.045% | LR: 1.000000e-03 | Dur: 153.10s
I - Confusion Matrix: [row->prediction - col->label]
[[314.   4.  11. 188.]
 [ 17. 259. 315.  11.]
 [ 42. 273. 348.  64.]
 [324.  42.  60. 275.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.026 | Acc: 43.425% | Wgt Acc: 43.207% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[46.  2.  4. 36.]
 [ 3. 44. 38.  5.]
 [13. 29. 28. 21.]
 [26.  3.  5. 24.]]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 2.943 | Acc: 48.625% | Wgt Acc: 49.535%
	I - Batch: 100 | Loss: 2.963 | Acc: 48.375% | Wgt Acc: 48.547%
	I - Batch: 150 | Loss: 2.970 | Acc: 48.542% | Wgt Acc: 47.991%
I - num batch: 160
I - Train -- Loss: 2.955 | Acc: 48.528% | Wgt Acc: 48.036% | LR: 1.000000e-03 | Dur: 158.28s
I - Confusion Matrix: [row->prediction - col->label]
[[393.   4.  10. 251.]
 [  7. 272. 304.   9.]
 [ 47. 267. 357.  64.]
 [250.  35.  63. 214.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.017 | Acc: 46.483% | Wgt Acc: 46.671% | Dur: 16.44s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  1.  4. 39.]
 [ 5. 52. 49.  6.]
 [11. 18. 17. 14.]
 [16.  7.  5. 27.]]

I - Local maximum validation set accuracy:  46.48

I - Validation set results: 
[14.     1.     1.    15.875]-
[50.          3.          3.          4.51955032]-[124.           2.           2.           5.41516209]-[127.      0.      0.     15.875]-[443.      2.      1.     15.875]-[567.      0.      0.     15.875]-[573.           1.           2.           9.78849983]-[615.      0.      0.     15.875]-[695.           1.           2.           1.73654675]-[722.      3.      0.     15.875]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.            0.            2.            5.76104546]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.       2.       0.      15.875]-[2476.       2.       1.      15.875]-[2721.       2.       1.      15.875]-[2818.       1.       1.      15.875]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.       2.       1.      15.875]-[3482.       2.       1.      15.875]-[3536.            3.            3.           14.36774826]-[3625.       1.       1.      15.875]-[3909.            0.            2.           12.23518372]-[4035.       0.       0.      15.875]-[4140.       0.       0.      15.875]-[4214.       1.       1.      15.875]-[4346.            1.            2.            2.37765145]-[4581.       2.       1.      15.875]-
[4708.       3.       1.      15.875]-[4838.            3.            2.            9.19025421]-[4845.       1.       1.      15.875]-[4868.       0.       0.      15.875]-[4939.       0.       1.      15.875]-[4984.            2.            2.            8.91694832]-[5078.       1.       1.      15.875]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       3.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       0.      15.875]-[5987.       2.       1.      15.875]-[6014.            3.            3.           12.72939873]-[6033.            3.            3.           13.59575176]-[6313.       0.       0.      15.875]-[6421.            3.            3.            1.85934949]-[6500.       1.       1.      15.875]-[6583.            3.            3.           15.64845657]-[6683.            3.            3.           11.60835266]-
[6825.       2.       3.      15.875]-[6998.            3.            2.           10.18369675]-[7049.            3.            2.            3.38115001]-[7517.       1.       1.      15.875]-[7521.       1.       0.      15.875]-[7528.            1.            3.            9.87703323]-[7949.       1.       1.      15.875]-[8135.            1.            3.           15.70402908]-[8185.       3.       0.      15.875]-[8269.            3.            2.           13.92561817]-
[8273.       3.       0.      15.875]-[8543.       3.       0.      15.875]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       1.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       1.      15.875]-[9281.            3.            2.            5.31024933]-[9300.       2.       1.      15.875]-[9571.            0.            3.            1.14997578]-
[9617.            1.            2.           15.73681831]-[9644.       2.       1.      15.875]-[9705.            2.            2.            3.45840454]-[9801.       0.       0.      15.875]-[9803.            3.            3.           13.34032059]-[9865.       3.       0.      15.875]-[9896.       2.       1.      15.875]-[10314.        1.        1.       15.875]-[10337.        3.        0.       15.875]-[10403.        0.        1.       15.875]-
[10653.             2.             2.            15.43635368]-[10704.        2.        1.       15.875]-[10719.        1.        1.       15.875]-[10727.             1.             2.            15.86739254]-[10836.        0.        0.       15.875]-[10969.        2.        0.       15.875]-[11042.          0.          3.         14.96243]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        1.       15.875]-
[11499.             0.             3.            15.80108643]-[11502.        3.        0.       15.875]-[11512.             3.             3.             9.67933464]-[11608.        1.        1.       15.875]-[11610.        0.        0.       15.875]-[11692.        0.        0.       15.875]-[11905.        0.        0.       15.875]-[11993.             1.             2.            13.14467907]-[12002.             2.             3.            11.35025597]-[12052.        0.        0.       15.875]-
[12201.        0.        0.       15.875]-[12235.        2.        1.       15.875]-[12320.             1.             2.             3.64005756]-[12377.        2.        1.       15.875]-[12398.             2.             2.             5.50130653]-[12503.             1.             2.            15.86667633]-[12617.             0.             2.            14.33209419]-[12685.             3.             2.            15.74236488]-[12738.        2.        0.       15.875]-[12742.        2.        1.       15.875]-
[12823.        0.        0.       15.875]-[13110.        1.        1.       15.875]-[13240.        3.        0.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.             2.             2.            10.86925888]-[13905.             3.             2.             3.07716227]-[14060.        2.        1.       15.875]-[14065.        3.        0.       15.875]-
[14147.        3.        0.       15.875]-[14595.        2.        1.       15.875]-[14687.        2.        1.       15.875]-[14788.        2.        1.       15.875]-[14869.        1.        1.       15.875]-[14872.             3.             3.             9.89290142]-[14877.        1.        1.       15.875]-[14927.             0.             3.            15.27653122]-[15066.        0.        0.       15.875]-[15175.        1.        1.       15.875]-
[15178.             2.             3.            15.60210419]-[15375.        3.        0.       15.875]-[15389.        3.        0.       15.875]-[15568.        2.        1.       15.875]-[15675.             3.             3.            14.65656567]-[15869.        1.        1.       15.875]-[16207.        3.        3.       15.875]-[16236.             0.             3.            15.38803864]-[16302.            3.            3.           14.3557663]-[16331.        2.        1.       15.875]-
[16381.             0.             3.            15.73149776]-[16488.             1.             2.            15.40958595]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.        1.        1.       15.875]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.        1.        2.       15.875]-[17278.             3.             3.            12.60043716]-
[17282.        0.        3.       15.875]-[17311.        2.        1.       15.875]-[17336.        2.        2.       15.875]-[17608.        3.        0.       15.875]-[17627.            0.            3.            3.7044692]-[17877.             3.             2.            15.83163738]-[17924.        1.        2.       15.875]-[17984.        3.        0.       15.875]-[18211.        0.        0.       15.875]-[18276.        3.        0.       15.875]-
[18287.        1.        1.       15.875]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        0.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.             0.             2.             5.38570595]-[18663.        0.        0.       15.875]-[18718.        0.        0.       15.875]-[18766.            2.            2.           15.5204134]-
[18824.        2.        1.       15.875]-[18890.             3.             2.            14.45139027]-[18930.        3.        2.       15.875]-[18938.        3.        0.       15.875]-[19817.        1.        1.       15.875]-[19839.             0.             2.            15.00105476]-[19930.        3.        0.       15.875]-[19944.             0.             2.            13.46063805]-[20036.        2.        1.       15.875]-[20101.             3.             2.             1.35529137]-
[20474.        1.        1.       15.875]-[20547.             3.             3.             1.29837513]-[20929.        2.        1.       15.875]-[21245.        1.        1.       15.875]-[21257.             3.             2.             6.38430786]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        1.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        1.       15.875]-[21714.             0.             3.             9.00099182]-[21943.             3.             2.            15.69778824]-[21947.             0.             2.             2.65342093]-[21948.        0.        0.       15.875]-[21965.        2.        1.       15.875]-[21998.        1.        2.       15.875]-[22025.        0.        1.       15.875]-[22228.        3.        0.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.        3.        0.       15.875]-[22976.        3.        1.       15.875]-[22985.        3.        0.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.        3.        0.       15.875]-[23168.             2.             2.             6.28866386]-[23219.             0.             2.            13.23906326]-
[23363.        3.        0.       15.875]-[23470.             0.             3.             9.00543594]-[23486.             2.             2.             5.41804218]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.        1.        1.       15.875]-[23921.        2.        1.       15.875]-[23936.             1.             2.             8.57713318]-[24040.             3.             3.             2.92655921]-[24111.        1.        1.       15.875]-
[24182.        0.        0.       15.875]-[24238.        3.        3.       15.875]-[24290.             2.             2.             4.50489092]-[24345.             0.             2.            12.09493065]-[24364.        1.        2.       15.875]-[24427.        3.        0.       15.875]-[24477.             2.             2.            15.66187859]-[24495.             2.             2.            14.91262436]-[24893.        2.        1.       15.875]-[25012.             1.             2.             4.02896976]-
[25121.        2.        1.       15.875]-[25165.             3.             3.             9.38101768]-[25183.        0.        0.       15.875]-[25297.             3.             3.            15.83928108]-[25398.        0.        0.       15.875]-[25574.        2.        1.       15.875]-[25644.        1.        1.       15.875]-[25718.             1.             2.             5.92433548]-[25774.        2.        1.       15.875]-[26032.             3.             3.            12.54325294]-
[26051.        3.        0.       15.875]-[26120.             0.             3.            14.93898773]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        0.       15.875]-[26827.             3.             3.            15.81576443]-[26833.        0.        0.       15.875]-[26838.             2.             2.            14.42277908]-[26860.             1.             2.            15.72439384]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.             1.             3.             2.72046089]-[27526.             0.             3.            15.41799164]-[27639.        3.        3.       15.875]-[27698.        3.        0.       15.875]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.             0.             2.            11.39295769]-[28503.        2.        1.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.        3.        1.       15.875]-[29777.        0.        0.       15.875]-[29877.        2.        1.       15.875]-[30035.        1.        1.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        1.       15.875]-[30716.        0.        1.       15.875]-[30806.        2.        1.       15.875]-
[30906.        1.        1.       15.875]-[31007.        0.        3.       15.875]-[31181.             3.             3.            15.80667019]-[31238.        0.        0.       15.875]-[31347.        0.        0.       15.875]-[31422.        2.        1.       15.875]-[31429.             3.             2.            13.31933403]-[31431.             0.             3.            15.69701099]-[31432.        1.        1.       15.875]-[31477.        0.        0.       15.875]-
[31524.             1.             3.            15.35096169]-[31597.        1.        1.       15.875]-[31619.             1.             2.             8.20536709]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        0.       15.875]-[32074.             1.             3.            15.84234524]-[32078.        3.        0.       15.875]-[32111.        1.        1.       15.875]-[32127.        1.        1.       15.875]-
[32140.        3.        0.       15.875]-[32263.             2.             2.            13.11139488]-[32365.        0.        0.       15.875]-[32411.        2.        0.       15.875]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        0.       15.875]-[32584.             0.             2.             5.00828171]-[32622.        0.        1.       15.875]-[32858.        3.        0.       15.875]-
[32969.        3.        0.       15.875]-[33016.        2.        1.       15.875]-[33031.             1.             3.            13.73856735]-[33035.        2.        1.       15.875]-[33133.        2.        1.       15.875]-[33173.             2.             3.            13.18348408]-[33175.        3.        1.       15.875]-[33306.        3.        1.       15.875]-[33309.             2.             2.             2.82225633]-[33474.             0.             3.            15.55390167]-
[33478.             2.             3.            15.68874741]-[33618.           1.           3.          15.314394]-[33712.        0.        0.       15.875]-[33782.        2.        1.       15.875]-[33914.            3.            3.           14.9779911]-[34076.        3.        1.       15.875]-[34112.        2.        1.       15.875]-[34138.             2.             2.            13.29761219]-[34239.        1.        1.       15.875]-[34364.        2.        1.       15.875]-
[34617.        1.        1.       15.875]-[34751.             3.             3.            11.22392178]-[34783.        2.        1.       15.875]-[35015.             3.             3.             3.37302542]-[35018.        1.        1.       15.875]-[35288.        2.        1.       15.875]-
---------------------------
I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 2.867 | Acc: 49.375% | Wgt Acc: 49.689%
	I - Batch: 100 | Loss: 2.864 | Acc: 49.562% | Wgt Acc: 49.958%
	I - Batch: 150 | Loss: 2.826 | Acc: 49.708% | Wgt Acc: 50.530%
I - num batch: 160
I - Train -- Loss: 2.831 | Acc: 49.548% | Wgt Acc: 50.310% | LR: 5.000000e-04 | Dur: 158.35s
I - Confusion Matrix: [row->prediction - col->label]
[[285.   4.   5. 163.]
 [  6. 311. 334.   5.]
 [ 41. 239. 338.  42.]
 [365.  24.  57. 328.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 4.985 | Acc: 45.872% | Wgt Acc: 43.954% | Dur: 15.73s
I - Confusion Matrix: [row->prediction - col->label]
[[41.  0.  1. 23.]
 [ 0.  0.  0.  0.]
 [ 9. 68. 62. 16.]
 [38. 10. 12. 47.]]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 2.720 | Acc: 50.625% | Wgt Acc: 51.070%
	I - Batch: 100 | Loss: 2.756 | Acc: 49.500% | Wgt Acc: 49.979%
	I - Batch: 150 | Loss: 2.793 | Acc: 50.417% | Wgt Acc: 50.357%
I - num batch: 160
I - Train -- Loss: 2.807 | Acc: 50.609% | Wgt Acc: 50.593% | LR: 5.000000e-04 | Dur: 155.58s
I - Confusion Matrix: [row->prediction - col->label]
[[291.   2.   4. 145.]
 [  2. 222. 244.   6.]
 [ 32. 335. 435.  46.]
 [372.  19.  51. 341.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 4.971 | Acc: 37.309% | Wgt Acc: 40.421% | Dur: 15.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 1. 25. 34.  1.]
 [ 3. 38. 15.  3.]
 [84. 15. 26. 82.]]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 2.700 | Acc: 49.875% | Wgt Acc: 51.783%
	I - Batch: 100 | Loss: 2.752 | Acc: 48.875% | Wgt Acc: 50.613%
	I - Batch: 150 | Loss: 2.762 | Acc: 49.292% | Wgt Acc: 51.009%
I - num batch: 160
I - Train -- Loss: 2.759 | Acc: 49.863% | Wgt Acc: 51.548% | LR: 5.000000e-04 | Dur: 155.85s
I - Confusion Matrix: [row->prediction - col->label]
[[263.   1.   4. 116.]
 [ 10. 373. 429.   5.]
 [ 20. 177. 260.  43.]
 [404.  27.  41. 374.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.042 | Acc: 47.401% | Wgt Acc: 47.554% | Dur: 15.53s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  3.  5. 43.]
 [ 2. 51. 45.  4.]
 [ 5. 12. 15. 10.]
 [21. 12. 10. 29.]]

I - Local maximum validation set accuracy:  47.40

I - Validation set results: 
[14.     1.     1.    15.875]-
[50.          3.          3.         13.41719818]-[124.      2.      1.     15.875]-[127.      0.      0.     15.875]-[443.      2.      1.     15.875]-[567.      0.      0.     15.875]-[573.      1.      1.     15.875]-[615.      0.      0.     15.875]-[695.           1.           3.          14.02194786]-[722.      3.      0.     15.875]-[826.      0.      3.     15.875]-
[878.      0.      0.     15.875]-[1103.            0.            3.           14.11779404]-[1212.       3.       0.      15.875]-[1368.       0.       0.      15.875]-[2181.       2.       0.      15.875]-[2476.       2.       1.      15.875]-[2721.       2.       1.      15.875]-[2818.       1.       1.      15.875]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.       2.       1.      15.875]-[3482.       2.       1.      15.875]-[3536.       3.       0.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.       0.       0.      15.875]-[4140.       0.       3.      15.875]-[4214.            1.            3.            2.08927822]-[4346.            1.            3.            2.18287659]-[4581.       2.       1.      15.875]-
[4708.            3.            2.           10.33685112]-[4838.            3.            3.            9.97585869]-[4845.       1.       1.      15.875]-[4868.       0.       0.      15.875]-[4939.       0.       1.      15.875]-[4984.       2.       1.      15.875]-[5078.            1.            3.            4.20225048]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       0.      15.875]-[5987.       2.       1.      15.875]-[6014.            3.            3.           14.71232796]-[6033.            3.            2.           12.41007996]-[6313.       0.       0.      15.875]-[6421.            3.            2.            5.80768061]-[6500.            1.            3.            5.14552593]-[6583.       3.       3.      15.875]-[6683.            3.            3.           15.17218018]-
[6825.       2.       0.      15.875]-[6998.            3.            3.            3.63770151]-[7049.           3.           3.          12.6312542]-[7517.       1.       1.      15.875]-[7521.       1.       0.      15.875]-[7528.            1.            3.           13.48473549]-[7949.       1.       1.      15.875]-[8135.       1.       0.      15.875]-[8185.       3.       0.      15.875]-[8269.            3.            2.           12.38362694]-
[8273.       3.       0.      15.875]-[8543.       3.       0.      15.875]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       1.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       1.      15.875]-[9281.            3.            3.            9.63084221]-[9300.       2.       1.      15.875]-[9571.       0.       0.      15.875]-
[9617.            1.            3.           14.54464531]-[9644.       2.       1.      15.875]-[9705.            2.            3.            9.40931511]-[9801.       0.       0.      15.875]-[9803.            3.            3.            2.66008973]-[9865.       3.       0.      15.875]-[9896.       2.       1.      15.875]-[10314.        1.        1.       15.875]-[10337.        3.        0.       15.875]-[10403.             0.             2.            15.83132839]-
[10653.             2.             2.             6.93839359]-[10704.        2.        1.       15.875]-[10719.        1.        1.       15.875]-[10727.        1.        1.       15.875]-[10836.        0.        0.       15.875]-[10969.        2.        0.       15.875]-[11042.        0.        3.       15.875]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        1.       15.875]-
[11499.        0.        0.       15.875]-[11502.             3.             3.            15.87206841]-[11512.            3.            3.            3.7531004]-[11608.        1.        1.       15.875]-[11610.        0.        0.       15.875]-[11692.        0.        0.       15.875]-[11905.        0.        0.       15.875]-[11993.        1.        1.       15.875]-[12002.             2.             3.            12.96275425]-[12052.             0.             3.            15.82366657]-
[12201.        0.        0.       15.875]-[12235.        2.        1.       15.875]-[12320.             1.             3.            11.33969498]-[12377.        2.        2.       15.875]-[12398.             2.             3.            14.51896858]-[12503.        1.        1.       15.875]-[12617.             0.             3.            13.94378757]-[12685.        3.        3.       15.875]-[12738.        2.        0.       15.875]-[12742.        2.        1.       15.875]-
[12823.        0.        0.       15.875]-[13110.        1.        1.       15.875]-[13240.        3.        0.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.             2.             3.            11.62142944]-[13905.             3.             2.            15.66090679]-[14060.        2.        1.       15.875]-[14065.        3.        0.       15.875]-
[14147.        3.        0.       15.875]-[14595.        2.        1.       15.875]-[14687.        2.        1.       15.875]-[14788.        2.        1.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        3.       15.875]-[14877.        1.        1.       15.875]-[14927.        0.        3.       15.875]-[15066.        0.        0.       15.875]-[15175.        1.        1.       15.875]-
[15178.             2.             2.             5.22557259]-[15375.        3.        0.       15.875]-[15389.        3.        0.       15.875]-[15568.        2.        1.       15.875]-[15675.        3.        3.       15.875]-[15869.             1.             2.             4.55667257]-[16207.             3.             3.            15.52209568]-[16236.             0.             3.             8.13968468]-[16302.        3.        3.       15.875]-[16331.        2.        1.       15.875]-
[16381.        0.        0.       15.875]-[16488.             1.             2.            15.42797279]-[16495.        0.        3.       15.875]-[16650.        0.        0.       15.875]-[16719.             1.             2.             9.91775036]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.        1.        1.       15.875]-[17278.        3.        0.       15.875]-
[17282.        0.        0.       15.875]-[17311.        2.        1.       15.875]-[17336.             2.             2.            13.47031498]-[17608.        3.        0.       15.875]-[17627.             0.             3.            12.35743618]-[17877.             3.             3.            15.62218761]-[17924.        1.        1.       15.875]-[17984.        3.        0.       15.875]-[18211.        0.        0.       15.875]-[18276.        3.        0.       15.875]-
[18287.             1.             2.            12.03928375]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        0.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.             0.             3.            15.27110481]-[18663.        0.        0.       15.875]-[18718.        0.        0.       15.875]-[18766.        2.        1.       15.875]-
[18824.        2.        1.       15.875]-[18890.             3.             2.             5.97045517]-[18930.        3.        2.       15.875]-[18938.        3.        0.       15.875]-[19817.        1.        1.       15.875]-[19839.             0.             3.            15.44513512]-[19930.        3.        0.       15.875]-[19944.        0.        1.       15.875]-[20036.        2.        1.       15.875]-[20101.        3.        3.       15.875]-
[20474.             1.             2.            15.42365074]-[20547.             3.             3.             7.36365318]-[20929.        2.        1.       15.875]-[21245.        1.        1.       15.875]-[21257.             3.             3.            13.09588051]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        1.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        1.       15.875]-[21714.        0.        0.       15.875]-[21943.             3.             2.             8.74851704]-[21947.             0.             3.            11.16720963]-[21948.        0.        0.       15.875]-[21965.        2.        1.       15.875]-[21998.             1.             2.            14.86121178]-[22025.             0.             2.             4.40294313]-[22228.        3.        0.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.        3.        0.       15.875]-[22976.             3.             2.            14.77958298]-[22985.        3.        0.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.        3.        0.       15.875]-[23168.            2.            3.           15.0539093]-[23219.        0.        0.       15.875]-
[23363.        3.        0.       15.875]-[23470.            0.            3.           13.9256258]-[23486.           2.           3.           3.709903]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.             1.             2.            15.79776478]-[23921.        2.        1.       15.875]-[23936.             1.             3.            12.63909721]-[24040.             3.             3.            12.48262024]-[24111.        1.        1.       15.875]-
[24182.        0.        0.       15.875]-[24238.        3.        3.       15.875]-[24290.             2.             3.             3.04433537]-[24345.             0.             3.            15.77825356]-[24364.        1.        1.       15.875]-[24427.             3.             3.             9.23944664]-[24477.        2.        1.       15.875]-[24495.             2.             2.            15.14715862]-[24893.        2.        1.       15.875]-[25012.             1.             2.             3.31640482]-
[25121.        2.        1.       15.875]-[25165.        3.        3.       15.875]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.        2.        1.       15.875]-[25644.        1.        1.       15.875]-[25718.             1.             2.             3.69923067]-[25774.            2.            2.           14.4164362]-[26032.        3.        0.       15.875]-
[26051.        3.        0.       15.875]-[26120.             0.             3.             3.45660973]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        0.       15.875]-[26827.        3.        0.       15.875]-[26833.        0.        0.       15.875]-[26838.             2.             2.            11.89051437]-[26860.        1.        1.       15.875]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.             1.             3.             3.22376585]-[27526.        0.        0.       15.875]-[27639.        3.        0.       15.875]-[27698.        3.        0.       15.875]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.        0.        0.       15.875]-[28503.        2.        1.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             3.            14.06045914]-[29777.        0.        0.       15.875]-[29877.             2.             2.            14.79599094]-[30035.        1.        1.       15.875]-[30098.        0.        0.       15.875]-[30326.             1.             2.            15.81165504]-[30572.        2.        1.       15.875]-[30716.             0.             2.             5.99828386]-[30806.        2.        2.       15.875]-
[30906.        1.        1.       15.875]-[31007.        0.        3.       15.875]-[31181.        3.        3.       15.875]-[31238.        0.        0.       15.875]-[31347.        0.        0.       15.875]-[31422.             2.             2.            15.85574436]-[31429.            3.            2.           14.6872282]-[31431.        0.        3.       15.875]-[31432.           1.           2.           8.611763]-[31477.        0.        0.       15.875]-
[31524.        1.        1.       15.875]-[31597.        1.        1.       15.875]-[31619.             1.             2.            10.18417835]-[31701.        0.        3.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        0.       15.875]-[32074.             1.             3.            15.42355919]-[32078.        3.        0.       15.875]-[32111.        1.        1.       15.875]-[32127.        1.        1.       15.875]-
[32140.        3.        0.       15.875]-[32263.             2.             2.             9.60717869]-[32365.        0.        0.       15.875]-[32411.        2.        0.       15.875]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        0.       15.875]-[32584.            0.            2.            9.2374773]-[32622.             0.             2.             9.45795345]-[32858.        3.        0.       15.875]-
[32969.        3.        0.       15.875]-[33016.        2.        2.       15.875]-[33031.        1.        0.       15.875]-[33035.        2.        1.       15.875]-[33133.        2.        1.       15.875]-[33173.             2.             2.            14.60661888]-[33175.        3.        1.       15.875]-[33306.        3.        1.       15.875]-[33309.        2.        3.       15.875]-[33474.        0.        3.       15.875]-
[33478.             2.             3.             6.02963543]-[33618.             1.             3.            14.14945602]-[33712.        0.        3.       15.875]-[33782.        2.        1.       15.875]-[33914.             3.             3.            14.17892838]-[34076.        3.        1.       15.875]-[34112.             2.             3.             3.58024693]-[34138.             2.             2.             8.59302807]-[34239.        1.        1.       15.875]-[34364.        2.        1.       15.875]-
[34617.        1.        1.       15.875]-[34751.        3.        3.       15.875]-[34783.        2.        1.       15.875]-[35015.        3.        1.       15.875]-[35018.        1.        1.       15.875]-[35288.             2.             2.            15.85072327]-
---------------------------
I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 2.719 | Acc: 50.375% | Wgt Acc: 50.507%
	I - Batch: 100 | Loss: 2.707 | Acc: 50.625% | Wgt Acc: 51.492%
	I - Batch: 150 | Loss: 2.727 | Acc: 51.250% | Wgt Acc: 51.888%
I - num batch: 160
I - Train -- Loss: 2.738 | Acc: 50.726% | Wgt Acc: 51.451% | LR: 5.000000e-04 | Dur: 158.31s
I - Confusion Matrix: [row->prediction - col->label]
[[292.   1.   4. 142.]
 [  6. 291. 337.   8.]
 [ 24. 267. 352.  31.]
 [375.  19.  41. 357.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.008 | Acc: 40.979% | Wgt Acc: 38.791% | Dur: 16.10s
I - Confusion Matrix: [row->prediction - col->label]
[[41.  3.  4. 39.]
 [ 0.  0.  0.  0.]
 [ 8. 62. 58. 12.]
 [39. 13. 13. 35.]]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 2.724 | Acc: 54.125% | Wgt Acc: 55.530%
	I - Batch: 100 | Loss: 2.683 | Acc: 56.062% | Wgt Acc: 57.571%
	I - Batch: 150 | Loss: 2.674 | Acc: 55.667% | Wgt Acc: 57.418%
I - num batch: 160
I - Train -- Loss: 2.687 | Acc: 55.634% | Wgt Acc: 57.360% | LR: 5.000000e-04 | Dur: 156.27s
I - Confusion Matrix: [row->prediction - col->label]
[[400.   4.  12. 132.]
 [ 11. 446. 478.   7.]
 [ 20. 106. 201.  29.]
 [266.  22.  43. 370.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.025 | Acc: 48.624% | Wgt Acc: 47.418% | Dur: 16.50s
I - Confusion Matrix: [row->prediction - col->label]
[[78.  5. 15. 53.]
 [ 1. 38. 37.  3.]
 [ 6. 24. 19.  6.]
 [ 3. 11.  4. 24.]]

I - Local maximum validation set accuracy:  48.62

I - Validation set results: 
[14.          1.          2.         15.06114578]-
[50.          3.          3.         13.83432865]-[124.           2.           0.           5.02127647]-[127.      0.      0.     15.875]-[443.      2.      2.     15.875]-[567.      0.      0.     15.875]-[573.      1.      2.     15.875]-[615.           0.           0.          13.50439548]-[695.      1.      0.     15.875]-[722.           3.           0.          15.73637009]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.            0.            0.           15.67687798]-[1212.            3.            0.           15.29974842]-[1368.       0.       0.      15.875]-[2181.       2.       0.      15.875]-[2476.       2.       1.      15.875]-[2721.       2.       2.      15.875]-[2818.            1.            0.            4.75043297]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.       2.       1.      15.875]-[3482.           2.           2.          15.3132019]-[3536.       3.       0.      15.875]-[3625.       1.       1.      15.875]-[3909.            0.            0.           15.80847359]-[4035.       0.       0.      15.875]-[4140.       0.       0.      15.875]-[4214.           1.           3.          13.4781599]-[4346.       1.       0.      15.875]-[4581.       2.       1.      15.875]-
[4708.            3.            3.            9.73160553]-[4838.            3.            0.            8.19877434]-[4845.            1.            2.           14.92853355]-[4868.       0.       0.      15.875]-[4939.            0.            2.            8.70535851]-[4984.       2.       1.      15.875]-[5078.            1.            3.            8.10082817]-[5396.       0.       0.      15.875]-[5479.            1.            2.           12.73075867]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.            3.            0.           15.86735725]-[5987.       2.       1.      15.875]-[6014.            3.            3.           15.75006294]-[6033.       3.       0.      15.875]-[6313.       0.       0.      15.875]-[6421.            3.            3.           13.34263229]-[6500.           1.           3.          13.3316021]-[6583.       3.       3.      15.875]-[6683.            3.            3.            8.36944103]-
[6825.            2.            0.           12.65010452]-[6998.            3.            0.            6.72034693]-[7049.            3.            0.           15.49596786]-[7517.       1.       1.      15.875]-[7521.       1.       3.      15.875]-[7528.            1.            3.           12.78499794]-[7949.       1.       1.      15.875]-[8135.            1.            3.           15.05156136]-[8185.       3.       0.      15.875]-[8269.       3.       1.      15.875]-
[8273.            3.            0.           12.74490833]-[8543.       3.       0.      15.875]-[8666.            1.            2.           11.57538605]-[8672.       0.       0.      15.875]-[8903.       1.       1.      15.875]-[9001.            2.            2.           12.91105938]-[9036.       2.       1.      15.875]-[9281.            3.            3.            9.53741074]-[9300.       2.       1.      15.875]-[9571.            0.            0.           10.82664871]-
[9617.            1.            2.            7.23786259]-[9644.       2.       2.      15.875]-[9705.       2.       0.      15.875]-[9801.           0.           0.          15.6731987]-[9803.       3.       0.      15.875]-[9865.       3.       0.      15.875]-[9896.       2.       1.      15.875]-[10314.        1.        1.       15.875]-[10337.             3.             3.            13.61355972]-[10403.             0.             2.             9.37926865]-
[10653.             2.             2.            15.18009853]-[10704.        2.        2.       15.875]-[10719.        1.        1.       15.875]-[10727.        1.        1.       15.875]-[10836.        0.        0.       15.875]-[10969.        2.        0.       15.875]-[11042.        0.        0.       15.875]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.             2.             2.            15.55447006]-
[11499.        0.        0.       15.875]-[11502.        3.        0.       15.875]-[11512.             3.             2.            11.89532948]-[11608.        1.        1.       15.875]-[11610.             0.             3.            15.75102806]-[11692.             0.             0.            13.81055069]-[11905.        0.        0.       15.875]-[11993.        1.        2.       15.875]-[12002.        2.        0.       15.875]-[12052.        0.        0.       15.875]-
[12201.        0.        0.       15.875]-[12235.        2.        1.       15.875]-[12320.            1.            0.           15.3650341]-[12377.        2.        2.       15.875]-[12398.             2.             0.            15.35865402]-[12503.        1.        1.       15.875]-[12617.             0.             2.            15.29443645]-[12685.             3.             3.            13.12022591]-[12738.             2.             0.            13.45926189]-[12742.        2.        1.       15.875]-
[12823.        0.        0.       15.875]-[13110.        1.        1.       15.875]-[13240.        3.        0.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        2.       15.875]-[13763.             2.             3.            12.30654144]-[13905.             3.             0.            13.88335609]-[14060.        2.        1.       15.875]-[14065.        3.        3.       15.875]-
[14147.             3.             0.            13.67302513]-[14595.        2.        1.       15.875]-[14687.        2.        1.       15.875]-[14788.        2.        1.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        0.       15.875]-[14877.        1.        1.       15.875]-[14927.             0.             3.            15.33108139]-[15066.        0.        0.       15.875]-[15175.        1.        2.       15.875]-
[15178.        2.        0.       15.875]-[15375.        3.        0.       15.875]-[15389.        3.        0.       15.875]-[15568.             2.             2.            15.76266861]-[15675.             3.             0.            14.95545578]-[15869.             1.             3.             9.71137047]-[16207.        3.        0.       15.875]-[16236.             0.             0.            14.45933914]-[16302.             3.             3.            13.58296967]-[16331.        2.        1.       15.875]-
[16381.        0.        0.       15.875]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.             1.             2.             7.31213951]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.             1.             3.             5.89163208]-[17278.            3.            0.           15.5588522]-
[17282.        0.        0.       15.875]-[17311.        2.        1.       15.875]-[17336.        2.        1.       15.875]-[17608.             3.             3.            14.46793365]-[17627.            0.            0.           14.1358242]-[17877.             3.             0.            11.12783813]-[17924.             1.             2.             9.02953911]-[17984.        3.        0.       15.875]-[18211.        0.        0.       15.875]-[18276.        3.        0.       15.875]-
[18287.             1.             2.            12.24959373]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        0.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.        0.        0.       15.875]-[18663.        0.        0.       15.875]-[18718.        0.        0.       15.875]-[18766.        2.        1.       15.875]-
[18824.        2.        1.       15.875]-[18890.             3.             0.            13.56111526]-[18930.             3.             0.            15.05247021]-[18938.             3.             0.            15.15556049]-[19817.        1.        1.       15.875]-[19839.             0.             0.            10.64986229]-[19930.        3.        0.       15.875]-[19944.        0.        2.       15.875]-[20036.        2.        1.       15.875]-[20101.             3.             0.            14.93519402]-
[20474.             1.             2.            12.78772449]-[20547.             3.             0.             7.18086815]-[20929.        2.        1.       15.875]-[21245.        1.        1.       15.875]-[21257.            3.            0.           15.5126667]-[21293.        1.        1.       15.875]-[21316.        1.        2.       15.875]-[21384.        1.        1.       15.875]-[21448.             1.             2.            15.83089256]-[21483.        0.        0.       15.875]-
[21487.        2.        1.       15.875]-[21714.        0.        0.       15.875]-[21943.            3.            2.           14.0819931]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        1.       15.875]-[21998.             1.             2.             8.45312023]-[22025.             0.             3.            10.41797638]-[22228.        3.        0.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.            3.            3.           15.4148922]-[22976.        3.        1.       15.875]-[22985.        3.        0.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.        3.        0.       15.875]-[23168.             2.             0.            13.08261776]-[23219.        0.        0.       15.875]-
[23363.             3.             3.            15.80804157]-[23470.             0.             0.            15.70960426]-[23486.             2.             2.             4.66998816]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.             1.             3.             7.13611984]-[23921.        2.        1.       15.875]-[23936.        1.        2.       15.875]-[24040.             3.             3.            14.57286644]-[24111.             1.             2.            15.53005791]-
[24182.        0.        0.       15.875]-[24238.            3.            3.           15.5141449]-[24290.        2.        0.       15.875]-[24345.             0.             2.             8.63890076]-[24364.             1.             3.             3.38472676]-[24427.            3.            3.           15.6349926]-[24477.        2.        1.       15.875]-[24495.             2.             0.             3.57216358]-[24893.        2.        1.       15.875]-[25012.        1.        1.       15.875]-
[25121.        2.        1.       15.875]-[25165.        3.        0.       15.875]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.             2.             2.            10.37610722]-[25644.             1.             2.            15.70336914]-[25718.             1.             2.             9.46276379]-[25774.             2.             3.            13.91632652]-[26032.             3.             0.            15.84766197]-
[26051.        3.        0.       15.875]-[26120.        0.        0.       15.875]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.             3.             3.            15.86802769]-[26827.             3.             3.            15.58523941]-[26833.        0.        0.       15.875]-[26838.             2.             2.            10.55684566]-[26860.        1.        1.       15.875]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.           1.           2.          13.639328]-[27526.        0.        0.       15.875]-[27639.             3.             0.            14.47310829]-[27698.             3.             3.            15.32025337]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.        0.        0.       15.875]-[28503.        2.        1.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             2.            12.75603199]-[29777.        0.        0.       15.875]-[29877.        2.        2.       15.875]-[30035.        1.        1.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        1.       15.875]-[30716.        0.        1.       15.875]-[30806.             2.             2.             8.76686478]-
[30906.        1.        1.       15.875]-[31007.        0.        0.       15.875]-[31181.             3.             0.            14.49560356]-[31238.             0.             0.            14.61467743]-[31347.        0.        0.       15.875]-[31422.        2.        1.       15.875]-[31429.             3.             2.             2.24493313]-[31431.             0.             0.            15.01670647]-[31432.        1.        1.       15.875]-[31477.        0.        0.       15.875]-
[31524.        1.        1.       15.875]-[31597.             1.             2.            14.34257507]-[31619.             1.             2.            13.45899105]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        0.       15.875]-[32074.        1.        1.       15.875]-[32078.             3.             0.            15.22755623]-[32111.        1.        1.       15.875]-[32127.        1.        1.       15.875]-
[32140.        3.        0.       15.875]-[32263.             2.             3.            13.13795567]-[32365.        0.        0.       15.875]-[32411.        2.        0.       15.875]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        0.       15.875]-[32584.             0.             0.            15.83256817]-[32622.        0.        2.       15.875]-[32858.        3.        0.       15.875]-
[32969.        3.        0.       15.875]-[33016.             2.             2.            15.82619286]-[33031.        1.        0.       15.875]-[33035.        2.        1.       15.875]-[33133.        2.        1.       15.875]-[33173.        2.        2.       15.875]-[33175.        3.        1.       15.875]-[33306.             3.             3.             5.33691883]-[33309.        2.        0.       15.875]-[33474.        0.        0.       15.875]-
[33478.        2.        0.       15.875]-[33618.             1.             3.            15.86864567]-[33712.        0.        0.       15.875]-[33782.        2.        1.       15.875]-[33914.             3.             3.            12.99631691]-[34076.             3.             2.             5.97106647]-[34112.             2.             2.            11.29902267]-[34138.            2.            3.           14.4943161]-[34239.             1.             2.            15.09362221]-[34364.        2.        1.       15.875]-
[34617.        1.        1.       15.875]-[34751.            3.            3.           15.7295332]-[34783.        2.        1.       15.875]-[35015.             3.             2.             7.75856543]-[35018.        1.        1.       15.875]-[35288.             2.             2.            15.70625401]-
---------------------------
I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 2.590 | Acc: 63.000% | Wgt Acc: 63.836%
	I - Batch: 100 | Loss: 2.625 | Acc: 60.312% | Wgt Acc: 61.285%
	I - Batch: 150 | Loss: 2.669 | Acc: 57.375% | Wgt Acc: 58.750%
I - num batch: 160
I - Train -- Loss: 2.668 | Acc: 57.636% | Wgt Acc: 59.032% | LR: 5.000000e-04 | Dur: 157.15s
I - Confusion Matrix: [row->prediction - col->label]
[[483.   7.  15. 136.]
 [ 13. 438. 489.   9.]
 [ 23. 111. 184.  30.]
 [178.  22.  46. 363.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.007 | Acc: 47.095% | Wgt Acc: 48.302% | Dur: 15.92s
I - Confusion Matrix: [row->prediction - col->label]
[[47.  9.  8. 22.]
 [ 2. 41. 42.  2.]
 [ 4. 18. 12.  8.]
 [35. 10. 13. 54.]]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 2.631 | Acc: 58.750% | Wgt Acc: 59.395%
	I - Batch: 100 | Loss: 2.599 | Acc: 59.688% | Wgt Acc: 60.150%
	I - Batch: 150 | Loss: 2.595 | Acc: 59.792% | Wgt Acc: 60.770%
I - num batch: 160
I - Train -- Loss: 2.590 | Acc: 59.442% | Wgt Acc: 60.492% | LR: 5.000000e-04 | Dur: 154.32s
I - Confusion Matrix: [row->prediction - col->label]
[[497.  10.  24. 122.]
 [  8. 399. 438.   5.]
 [ 19. 154. 235.  28.]
 [173.  15.  37. 383.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.029 | Acc: 51.070% | Wgt Acc: 51.698% | Dur: 16.47s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  2.  8. 34.]
 [ 5. 59. 50.  4.]
 [ 7. 10. 10. 14.]
 [12.  7.  7. 34.]]

I - Local maximum validation set accuracy:  51.07

I - Validation set results: 
[14.     1.     1.    15.875]-
[50.     3.     1.    15.875]-[124.      2.      1.     15.875]-[127.      0.      0.     15.875]-[443.      2.      1.     15.875]-[567.           0.           0.          15.84994698]-[573.           1.           2.          13.72556973]-[615.      0.      0.     15.875]-[695.      1.      1.     15.875]-[722.      3.      0.     15.875]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.            0.            3.            1.47589684]-[1212.            3.            0.           15.11386585]-[1368.       0.       0.      15.875]-[2181.       2.       0.      15.875]-[2476.       2.       1.      15.875]-[2721.       2.       1.      15.875]-[2818.       1.       1.      15.875]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.       2.       1.      15.875]-[3482.            2.            2.            9.80056953]-[3536.            3.            0.           15.86478043]-[3625.       1.       1.      15.875]-[3909.            0.            3.            6.81456709]-[4035.            0.            0.           15.21094704]-[4140.       0.       0.      15.875]-[4214.       1.       1.      15.875]-[4346.            1.            3.           12.81941891]-[4581.       2.       1.      15.875]-
[4708.       3.       1.      15.875]-[4838.            3.            3.           12.08761024]-[4845.       1.       1.      15.875]-[4868.       0.       0.      15.875]-[4939.            0.            3.           10.32835674]-[4984.            2.            3.            9.50146675]-[5078.            1.            2.            4.60504627]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.            0.            0.           12.21325588]-
[5843.       1.       1.      15.875]-[5949.       3.       0.      15.875]-[5987.       2.       1.      15.875]-[6014.            3.            3.           15.68288708]-[6033.            3.            0.            8.25338173]-[6313.       0.       0.      15.875]-[6421.            3.            2.           15.29730034]-[6500.            1.            2.           13.01181412]-[6583.            3.            3.            8.59119129]-[6683.           3.           3.           1.1493727]-
[6825.            2.            0.           14.97053814]-[6998.            3.            0.            1.89913654]-[7049.            3.            3.           15.54073906]-[7517.       1.       1.      15.875]-[7521.            1.            3.            5.56463146]-[7528.            1.            3.           12.08213711]-[7949.       1.       1.      15.875]-[8135.            1.            3.            6.56189251]-[8185.       3.       0.      15.875]-[8269.            3.            2.            6.63439131]-
[8273.       3.       0.      15.875]-[8543.       3.       0.      15.875]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       1.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       1.      15.875]-[9281.            3.            3.            9.76324558]-[9300.       2.       1.      15.875]-[9571.            0.            3.           15.27662659]-
[9617.            1.            2.            7.87678909]-[9644.       2.       1.      15.875]-[9705.            2.            2.           11.48421383]-[9801.           0.           3.          14.4282589]-[9803.            3.            3.            7.31012583]-[9865.       3.       0.      15.875]-[9896.       2.       1.      15.875]-[10314.        1.        1.       15.875]-[10337.        3.        3.       15.875]-[10403.        0.        2.       15.875]-
[10653.             2.             2.            12.26859093]-[10704.        2.        1.       15.875]-[10719.        1.        1.       15.875]-[10727.        1.        1.       15.875]-[10836.        0.        0.       15.875]-[10969.             2.             3.            10.12758255]-[11042.             0.             3.             9.00477409]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        1.       15.875]-
[11499.             0.             0.            13.59736443]-[11502.             3.             3.            15.47257233]-[11512.             3.             2.            14.10224056]-[11608.        1.        1.       15.875]-[11610.             0.             0.             8.65869045]-[11692.        0.        0.       15.875]-[11905.             0.             0.            15.86940861]-[11993.        1.        1.       15.875]-[12002.             2.             0.             9.51736355]-[12052.             0.             0.            15.16539955]-
[12201.        0.        0.       15.875]-[12235.        2.        1.       15.875]-[12320.             1.             2.            11.96372128]-[12377.        2.        1.       15.875]-[12398.             2.             3.            11.39071655]-[12503.        1.        1.       15.875]-[12617.        0.        1.       15.875]-[12685.             3.             3.            14.25985336]-[12738.        2.        0.       15.875]-[12742.        2.        1.       15.875]-
[12823.        0.        0.       15.875]-[13110.        1.        1.       15.875]-[13240.        3.        0.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.             2.             2.            14.21844578]-[13905.        3.        0.       15.875]-[14060.        2.        1.       15.875]-[14065.             3.             3.            14.76522732]-
[14147.             3.             3.            12.27763557]-[14595.        2.        1.       15.875]-[14687.        2.        1.       15.875]-[14788.        2.        1.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        0.       15.875]-[14877.        1.        1.       15.875]-[14927.             0.             2.            10.39798737]-[15066.        0.        0.       15.875]-[15175.             1.             2.            13.16212845]-
[15178.             2.             3.            14.38542843]-[15375.        3.        0.       15.875]-[15389.        3.        0.       15.875]-[15568.        2.        1.       15.875]-[15675.             3.             2.            12.70049858]-[15869.        1.        1.       15.875]-[16207.             3.             3.            11.51770973]-[16236.             0.             3.             6.86902523]-[16302.             3.             2.             9.74269676]-[16331.        2.        1.       15.875]-
[16381.             0.             0.            15.64368343]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.             1.             2.            15.81837177]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.            3.            0.           13.3771801]-[17245.        1.        1.       15.875]-[17278.             3.             3.             1.12098169]-
[17282.             0.             0.            14.17012978]-[17311.        2.        1.       15.875]-[17336.        2.        1.       15.875]-[17608.        3.        0.       15.875]-[17627.             0.             0.             6.56865835]-[17877.             3.             2.            15.51766205]-[17924.        1.        1.       15.875]-[17984.        3.        0.       15.875]-[18211.             0.             3.            14.21126938]-[18276.        3.        0.       15.875]-
[18287.        1.        1.       15.875]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        3.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.             0.             2.             5.47156811]-[18663.             0.             3.            15.78055477]-[18718.        0.        0.       15.875]-[18766.             2.             2.             9.32236958]-
[18824.        2.        1.       15.875]-[18890.             3.             3.            13.91793156]-[18930.        3.        1.       15.875]-[18938.        3.        3.       15.875]-[19817.        1.        1.       15.875]-[19839.        0.        1.       15.875]-[19930.             3.             3.            12.92558002]-[19944.             0.             2.             2.78928804]-[20036.        2.        1.       15.875]-[20101.             3.             3.             4.57813215]-
[20474.        1.        1.       15.875]-[20547.             3.             2.             2.00585437]-[20929.        2.        1.       15.875]-[21245.        1.        1.       15.875]-[21257.             3.             3.            11.64838982]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        1.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        1.       15.875]-[21714.        0.        0.       15.875]-[21943.             3.             3.            10.16318226]-[21947.             0.             0.            15.34609699]-[21948.        0.        0.       15.875]-[21965.        2.        1.       15.875]-[21998.        1.        1.       15.875]-[22025.        0.        1.       15.875]-[22228.        3.        0.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.             3.             0.            15.82363415]-[22976.             3.             2.            10.95837402]-[22985.        3.        0.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.        3.        0.       15.875]-[23168.        2.        2.       15.875]-[23219.             0.             0.            15.04161263]-
[23363.        3.        3.       15.875]-[23470.             0.             3.             1.34099841]-[23486.             2.             3.             7.78388262]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.        1.        2.       15.875]-[23921.        2.        1.       15.875]-[23936.             1.             3.             4.67617035]-[24040.             3.             2.             9.30536461]-[24111.        1.        1.       15.875]-
[24182.             0.             0.            15.65254784]-[24238.             3.             3.            15.57987213]-[24290.             2.             0.            14.43408966]-[24345.             0.             0.            15.47689724]-[24364.        1.        1.       15.875]-[24427.             3.             3.             9.89713764]-[24477.        2.        1.       15.875]-[24495.             2.             2.            14.65142345]-[24893.        2.        1.       15.875]-[25012.        1.        1.       15.875]-
[25121.        2.        1.       15.875]-[25165.             3.             2.            10.68953133]-[25183.             0.             0.            14.95255661]-[25297.            3.            3.            9.2896843]-[25398.        0.        0.       15.875]-[25574.            2.            2.           12.1697216]-[25644.        1.        1.       15.875]-[25718.             1.             2.            14.14548397]-[25774.        2.        1.       15.875]-[26032.             3.             3.            14.79808044]-
[26051.        3.        0.       15.875]-[26120.             0.             0.            15.33796787]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.             3.             3.            15.14585304]-[26827.             3.             3.            15.69772339]-[26833.        0.        0.       15.875]-[26838.             2.             3.             7.65890169]-[26860.        1.        1.       15.875]-[26948.             0.             0.            15.63167381]-
[27049.        3.        0.       15.875]-[27098.        1.        1.       15.875]-[27526.             0.             0.            15.82240486]-[27639.        3.        3.       15.875]-[27698.             3.             3.            15.61834431]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.             0.             3.            15.52205372]-[28503.        2.        1.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             2.            13.95064163]-[29777.        0.        0.       15.875]-[29877.             2.             2.            15.53970242]-[30035.        1.        1.       15.875]-[30098.            0.            0.           15.3875351]-[30326.        1.        1.       15.875]-[30572.        2.        1.       15.875]-[30716.        0.        1.       15.875]-[30806.        2.        1.       15.875]-
[30906.        1.        1.       15.875]-[31007.             0.             2.            10.16037083]-[31181.             3.             3.            13.66981792]-[31238.        0.        0.       15.875]-[31347.        0.        0.       15.875]-[31422.             2.             2.            14.46980476]-[31429.             3.             2.             7.16625452]-[31431.             0.             0.            11.49479485]-[31432.        1.        1.       15.875]-[31477.        0.        0.       15.875]-
[31524.             1.             0.             6.03543901]-[31597.        1.        1.       15.875]-[31619.        1.        1.       15.875]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.             3.             0.            14.87364197]-[32074.             1.             3.            10.70828629]-[32078.        3.        3.       15.875]-[32111.             1.             2.            15.74497318]-[32127.        1.        1.       15.875]-
[32140.             3.             0.            15.84396076]-[32263.             2.             0.            15.32266045]-[32365.             0.             0.            13.98456764]-[32411.        2.        0.       15.875]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        0.       15.875]-[32584.        0.        2.       15.875]-[32622.             0.             2.            11.63820744]-[32858.        3.        0.       15.875]-
[32969.        3.        0.       15.875]-[33016.        2.        1.       15.875]-[33031.        1.        0.       15.875]-[33035.        2.        1.       15.875]-[33133.        2.        1.       15.875]-[33173.        2.        1.       15.875]-[33175.        3.        1.       15.875]-[33306.             3.             2.            12.29941368]-[33309.             2.             3.            15.02772331]-[33474.        0.        1.       15.875]-
[33478.            2.            0.            8.9108305]-[33618.        1.        3.       15.875]-[33712.           0.           0.          15.591012]-[33782.        2.        1.       15.875]-[33914.             3.             3.            15.77625179]-[34076.             3.             2.             9.67917919]-[34112.        2.        1.       15.875]-[34138.        2.        1.       15.875]-[34239.        1.        1.       15.875]-[34364.        2.        1.       15.875]-
[34617.        1.        1.       15.875]-[34751.             3.             3.            12.48288536]-[34783.        2.        1.       15.875]-[35015.             3.             3.            13.92536926]-[35018.        1.        1.       15.875]-[35288.        2.        1.       15.875]-
---------------------------
I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 2.523 | Acc: 61.250% | Wgt Acc: 62.910%
	I - Batch: 100 | Loss: 2.503 | Acc: 60.188% | Wgt Acc: 61.873%
	I - Batch: 150 | Loss: 2.529 | Acc: 60.167% | Wgt Acc: 61.713%
I - num batch: 160
I - Train -- Loss: 2.534 | Acc: 60.110% | Wgt Acc: 61.660% | LR: 5.000000e-04 | Dur: 157.86s
I - Confusion Matrix: [row->prediction - col->label]
[[532.   7.  12. 118.]
 [  7. 456. 527.   7.]
 [ 16.  97. 153.  23.]
 [142.  18.  42. 390.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.023 | Acc: 47.095% | Wgt Acc: 47.894% | Dur: 16.30s
I - Confusion Matrix: [row->prediction - col->label]
[[41.  3.  6. 19.]
 [ 1. 25. 29.  1.]
 [ 4. 26. 24.  2.]
 [42. 24. 16. 64.]]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 2.436 | Acc: 64.375% | Wgt Acc: 65.209%
	I - Batch: 100 | Loss: 2.547 | Acc: 60.312% | Wgt Acc: 60.234%
	I - Batch: 150 | Loss: 2.560 | Acc: 59.958% | Wgt Acc: 60.293%
I - num batch: 160
I - Train -- Loss: 2.566 | Acc: 60.031% | Wgt Acc: 60.465% | LR: 5.000000e-04 | Dur: 154.04s
I - Confusion Matrix: [row->prediction - col->label]
[[539.  16.  16. 113.]
 [  6. 335. 409.   8.]
 [ 20. 212. 271.  33.]
 [132.  15.  38. 384.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.153 | Acc: 48.012% | Wgt Acc: 50.272% | Dur: 15.92s
I - Confusion Matrix: [row->prediction - col->label]
[[37.  4.  2.  8.]
 [ 8. 62. 56. 10.]
 [ 5. 10.  8. 18.]
 [38.  2.  9. 50.]]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 2.431 | Acc: 60.875% | Wgt Acc: 62.711%
	I - Batch: 100 | Loss: 2.457 | Acc: 61.125% | Wgt Acc: 62.403%
	I - Batch: 150 | Loss: 2.490 | Acc: 60.250% | Wgt Acc: 61.680%
I - num batch: 160
I - Train -- Loss: 2.490 | Acc: 60.581% | Wgt Acc: 62.013% | LR: 5.000000e-04 | Dur: 156.48s
I - Confusion Matrix: [row->prediction - col->label]
[[548.  14.  16. 105.]
 [  9. 436. 520.   6.]
 [ 19. 117. 157.  25.]
 [121.  11.  41. 402.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.107 | Acc: 47.401% | Wgt Acc: 48.913% | Dur: 15.88s
I - Confusion Matrix: [row->prediction - col->label]
[[49.  3.  2. 18.]
 [10. 59. 57. 10.]
 [ 8. 11.  6. 17.]
 [21.  5. 10. 41.]]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 2.464 | Acc: 63.625% | Wgt Acc: 65.672%
	I - Batch: 100 | Loss: 2.374 | Acc: 63.312% | Wgt Acc: 65.310%
	I - Batch: 150 | Loss: 2.352 | Acc: 63.208% | Wgt Acc: 65.199%
I - num batch: 160
I - Train -- Loss: 2.335 | Acc: 63.486% | Wgt Acc: 65.499% | LR: 2.500000e-04 | Dur: 155.06s
I - Confusion Matrix: [row->prediction - col->label]
[[555.   8.  11.  67.]
 [ 10. 482. 568.   4.]
 [ 16.  75. 126.  13.]
 [116.  13.  29. 454.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.121 | Acc: 52.599% | Wgt Acc: 53.940% | Dur: 16.00s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  2.  5. 17.]
 [ 4. 55. 54.  6.]
 [ 6. 14.  9. 12.]
 [21.  7.  7. 51.]]

I - Local maximum validation set accuracy:  52.60

I - Validation set results: 
[14.     1.     1.    15.875]-
[50.          3.          2.          9.92131138]-[124.      2.      1.     15.875]-[127.      0.      0.     15.875]-[443.      2.      1.     15.875]-[567.      0.      0.     15.875]-[573.      1.      2.     15.875]-[615.      0.      3.     15.875]-[695.      1.      1.     15.875]-[722.      3.      3.     15.875]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.            0.            3.           12.96743393]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.            2.            3.           15.38465977]-[2476.       2.       1.      15.875]-[2721.       2.       1.      15.875]-[2818.            1.            2.            8.25190067]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.       2.       1.      15.875]-[3482.       2.       2.      15.875]-[3536.            3.            0.           12.83123302]-[3625.       1.       1.      15.875]-[3909.            0.            0.            3.07429814]-[4035.       0.       3.      15.875]-[4140.       0.       0.      15.875]-[4214.       1.       1.      15.875]-[4346.            1.            2.            1.18250561]-[4581.       2.       1.      15.875]-
[4708.            3.            3.            7.40905809]-[4838.       3.       3.      15.875]-[4845.       1.       1.      15.875]-[4868.       0.       0.      15.875]-[4939.       0.       1.      15.875]-[4984.          2.          2.         12.030756]-[5078.            1.            3.           11.70588303]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       3.      15.875]-[5987.       2.       1.      15.875]-[6014.            3.            2.           11.02715015]-[6033.       3.       0.      15.875]-[6313.       0.       0.      15.875]-[6421.       3.       1.      15.875]-[6500.       1.       1.      15.875]-[6583.            3.            3.           15.59747982]-[6683.            3.            3.           14.67188454]-
[6825.           2.           0.          12.5555172]-[6998.            3.            3.            4.09813547]-[7049.            3.            2.            8.47860241]-[7517.       1.       1.      15.875]-[7521.            1.            3.           15.74607086]-[7528.       1.       3.      15.875]-[7949.       1.       1.      15.875]-[8135.            1.            0.           15.82824135]-[8185.       3.       0.      15.875]-[8269.            3.            2.           10.78443813]-
[8273.            3.            3.            9.58972549]-[8543.       3.       0.      15.875]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       1.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       1.      15.875]-[9281.            3.            3.           10.98395252]-[9300.       2.       1.      15.875]-[9571.            0.            0.           13.36624527]-
[9617.       1.       1.      15.875]-[9644.       2.       1.      15.875]-[9705.       2.       0.      15.875]-[9801.            0.            0.            2.57288456]-[9803.            3.            3.            9.55975533]-[9865.            3.            3.           13.75687122]-[9896.       2.       1.      15.875]-[10314.        1.        1.       15.875]-[10337.        3.        3.       15.875]-[10403.             0.             2.            11.10079479]-
[10653.             2.             2.            12.94607735]-[10704.        2.        1.       15.875]-[10719.        1.        1.       15.875]-[10727.        1.        1.       15.875]-[10836.        0.        0.       15.875]-[10969.             2.             2.            10.22986221]-[11042.        0.        0.       15.875]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        2.       15.875]-
[11499.             0.             0.            15.05826187]-[11502.             3.             3.            15.84897423]-[11512.             3.             2.            14.95396233]-[11608.        1.        1.       15.875]-[11610.             0.             0.             4.14364815]-[11692.             0.             3.            12.18434429]-[11905.             0.             3.            15.19416428]-[11993.        1.        1.       15.875]-[12002.             2.             2.             4.28940773]-[12052.        0.        0.       15.875]-
[12201.             0.             3.            15.67848969]-[12235.        2.        1.       15.875]-[12320.             1.             0.            15.65791035]-[12377.        2.        1.       15.875]-[12398.        2.        3.       15.875]-[12503.        1.        2.       15.875]-[12617.             0.             2.            15.74807262]-[12685.             3.             3.             9.95761013]-[12738.        2.        3.       15.875]-[12742.        2.        1.       15.875]-
[12823.            0.            0.           15.2999382]-[13110.        1.        1.       15.875]-[13240.        3.        3.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.            2.            2.           15.2645998]-[13905.             3.             0.            15.42053604]-[14060.        2.        1.       15.875]-[14065.             3.             3.            14.65651131]-
[14147.        3.        3.       15.875]-[14595.        2.        1.       15.875]-[14687.        2.        1.       15.875]-[14788.        2.        1.       15.875]-[14869.        1.        1.       15.875]-[14872.             3.             3.            14.85860443]-[14877.        1.        1.       15.875]-[14927.        0.        0.       15.875]-[15066.        0.        0.       15.875]-[15175.        1.        1.       15.875]-
[15178.             2.             3.            11.97039795]-[15375.             3.             3.            10.47671127]-[15389.             3.             3.            15.39819908]-[15568.        2.        1.       15.875]-[15675.             3.             2.            14.56556416]-[15869.            1.            3.            7.3680172]-[16207.        3.        1.       15.875]-[16236.             0.             3.            13.24503136]-[16302.             3.             2.            12.45106506]-[16331.        2.        1.       15.875]-
[16381.        0.        3.       15.875]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.             1.             3.            12.69760895]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.             3.             0.            10.75786972]-[17245.             1.             2.            15.29806709]-[17278.             3.             0.             3.66230774]-
[17282.             0.             0.            15.57165718]-[17311.        2.        1.       15.875]-[17336.        2.        1.       15.875]-[17608.        3.        3.       15.875]-[17627.            0.            0.           14.0868845]-[17877.        3.        1.       15.875]-[17924.        1.        1.       15.875]-[17984.        3.        3.       15.875]-[18211.             0.             3.            15.79899406]-[18276.             3.             3.            15.72024536]-
[18287.             1.             2.            14.67937088]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        3.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.             0.             2.            14.38159657]-[18663.             0.             3.            13.83257103]-[18718.        0.        0.       15.875]-[18766.        2.        1.       15.875]-
[18824.        2.        1.       15.875]-[18890.             3.             3.            15.36582279]-[18930.        3.        1.       15.875]-[18938.        3.        3.       15.875]-[19817.        1.        1.       15.875]-[19839.        0.        1.       15.875]-[19930.             3.             0.            15.67299557]-[19944.        0.        1.       15.875]-[20036.        2.        1.       15.875]-[20101.            3.            3.           15.8293848]-
[20474.        1.        1.       15.875]-[20547.             3.             0.            14.42770004]-[20929.        2.        1.       15.875]-[21245.        1.        1.       15.875]-[21257.             3.             3.            15.25903511]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        1.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        1.       15.875]-[21714.        0.        3.       15.875]-[21943.             3.             2.            11.48500633]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        1.       15.875]-[21998.             1.             2.            14.53141975]-[22025.        0.        2.       15.875]-[22228.        3.        3.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        3.       15.875]-[22811.        3.        3.       15.875]-[22976.        3.        1.       15.875]-[22985.        3.        3.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.             3.             3.            15.25305748]-[23168.        2.        2.       15.875]-[23219.             0.             0.            15.68138504]-
[23363.        3.        3.       15.875]-[23470.             0.             0.            10.97227764]-[23486.             2.             3.            13.69396973]-[23497.        0.        3.       15.875]-[23516.        0.        3.       15.875]-[23690.             1.             2.            14.61541939]-[23921.        2.        1.       15.875]-[23936.             1.             2.            12.18329525]-[24040.             3.             3.            10.97194672]-[24111.        1.        1.       15.875]-
[24182.        0.        3.       15.875]-[24238.        3.        3.       15.875]-[24290.             2.             0.            14.51292992]-[24345.             0.             0.            13.64844513]-[24364.             1.             2.            14.85761356]-[24427.        3.        3.       15.875]-[24477.        2.        1.       15.875]-[24495.        2.        1.       15.875]-[24893.        2.        1.       15.875]-[25012.             1.             3.            12.12599754]-
[25121.        2.        1.       15.875]-[25165.             3.             0.            15.31139183]-[25183.        0.        0.       15.875]-[25297.             3.             3.            11.78997612]-[25398.        0.        0.       15.875]-[25574.        2.        1.       15.875]-[25644.        1.        1.       15.875]-[25718.        1.        1.       15.875]-[25774.        2.        1.       15.875]-[26032.        3.        3.       15.875]-
[26051.        3.        3.       15.875]-[26120.        0.        0.       15.875]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.             0.             3.            11.97138977]-[26838.        2.        1.       15.875]-[26860.             1.             2.            15.15235996]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.             1.             2.            11.18104362]-[27526.        0.        0.       15.875]-[27639.             3.             3.             9.88179588]-[27698.        3.        3.       15.875]-[27772.             0.             0.            12.70573044]-[27890.        1.        1.       15.875]-[28040.             0.             3.            15.55917931]-[28503.        2.        1.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             3.            11.60279083]-[29777.        0.        0.       15.875]-[29877.        2.        1.       15.875]-[30035.        1.        1.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        1.       15.875]-[30716.        0.        1.       15.875]-[30806.        2.        1.       15.875]-
[30906.        1.        1.       15.875]-[31007.        0.        3.       15.875]-[31181.             3.             3.            13.17865944]-[31238.             0.             3.            15.58136559]-[31347.        0.        0.       15.875]-[31422.        2.        1.       15.875]-[31429.             3.             2.            10.65412235]-[31431.             0.             3.             9.86700153]-[31432.        1.        1.       15.875]-[31477.        0.        0.       15.875]-
[31524.        1.        1.       15.875]-[31597.        1.        1.       15.875]-[31619.        1.        2.       15.875]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.             3.             3.            15.75222969]-[32074.        1.        1.       15.875]-[32078.        3.        3.       15.875]-[32111.        1.        1.       15.875]-[32127.        1.        1.       15.875]-
[32140.             3.             3.            14.72432232]-[32263.             2.             0.             8.52805519]-[32365.        0.        0.       15.875]-[32411.             2.             0.            15.18241501]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        3.       15.875]-[32584.             0.             2.             7.85876131]-[32622.             0.             2.             2.70568752]-[32858.             3.             0.            15.74600601]-
[32969.        3.        0.       15.875]-[33016.        2.        1.       15.875]-[33031.             1.             2.             6.69581461]-[33035.        2.        1.       15.875]-[33133.        2.        1.       15.875]-[33173.        2.        1.       15.875]-[33175.        3.        1.       15.875]-[33306.        3.        2.       15.875]-[33309.             2.             3.            15.72419357]-[33474.             0.             0.            13.18579865]-
[33478.             2.             3.            12.25434303]-[33618.        1.        3.       15.875]-[33712.        0.        0.       15.875]-[33782.        2.        1.       15.875]-[33914.        3.        3.       15.875]-[34076.        3.        2.       15.875]-[34112.             2.             2.             7.03472662]-[34138.        2.        1.       15.875]-[34239.        1.        1.       15.875]-[34364.        2.        1.       15.875]-
[34617.        1.        1.       15.875]-[34751.        3.        3.       15.875]-[34783.        2.        1.       15.875]-[35015.        3.        2.       15.875]-[35018.        1.        1.       15.875]-[35288.        2.        1.       15.875]-
---------------------------
I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 2.216 | Acc: 66.750% | Wgt Acc: 68.964%
	I - Batch: 100 | Loss: 2.236 | Acc: 66.625% | Wgt Acc: 68.885%
	I - Batch: 150 | Loss: 2.228 | Acc: 67.042% | Wgt Acc: 69.131%
I - num batch: 160
I - Train -- Loss: 2.240 | Acc: 66.667% | Wgt Acc: 68.719% | LR: 2.500000e-04 | Dur: 157.18s
I - Confusion Matrix: [row->prediction - col->label]
[[587.   6.  15.  58.]
 [  4. 509. 568.   3.]
 [ 13.  57. 135.  10.]
 [ 93.   6.  16. 467.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.144 | Acc: 53.211% | Wgt Acc: 53.533% | Dur: 15.78s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  8.  7. 33.]
 [ 3. 51. 50.  4.]
 [ 5. 11. 12.  8.]
 [10.  8.  6. 41.]]

I - Local maximum validation set accuracy:  53.21

I - Validation set results: 
[14.     1.     1.    15.875]-
[50.          3.          3.         14.77446461]-[124.      2.      2.     15.875]-[127.      0.      0.     15.875]-[443.      2.      1.     15.875]-[567.      0.      0.     15.875]-[573.      1.      1.     15.875]-[615.           0.           3.          13.24362087]-[695.      1.      1.     15.875]-[722.           3.           3.          15.50320435]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.            0.            0.           15.02927399]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.            2.            0.           14.74667358]-[2476.       2.       1.      15.875]-[2721.       2.       1.      15.875]-[2818.            1.            2.            1.31169033]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.       2.       1.      15.875]-[3482.            2.            2.           15.26346874]-[3536.       3.       0.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.       0.       0.      15.875]-[4140.       0.       0.      15.875]-[4214.            1.            3.            7.98237896]-[4346.            1.            0.           12.21162701]-[4581.       2.       1.      15.875]-
[4708.            3.            2.            7.03243542]-[4838.            3.            0.           15.08736801]-[4845.       1.       1.      15.875]-[4868.       0.       0.      15.875]-[4939.            0.            2.            9.73567772]-[4984.       2.       1.      15.875]-[5078.       1.       0.      15.875]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       0.      15.875]-[5987.       2.       1.      15.875]-[6014.       3.       3.      15.875]-[6033.       3.       1.      15.875]-[6313.       0.       0.      15.875]-[6421.       3.       2.      15.875]-[6500.            1.            3.            7.73421097]-[6583.       3.       3.      15.875]-[6683.            3.            3.            1.84571838]-
[6825.       2.       0.      15.875]-[6998.           3.           3.           2.3510437]-[7049.            3.            3.           15.71381378]-[7517.       1.       1.      15.875]-[7521.       1.       0.      15.875]-[7528.       1.       3.      15.875]-[7949.       1.       1.      15.875]-[8135.       1.       0.      15.875]-[8185.       3.       0.      15.875]-[8269.            3.            2.           12.40601444]-
[8273.       3.       0.      15.875]-[8543.       3.       0.      15.875]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       1.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       1.      15.875]-[9281.           3.           3.           6.5254302]-[9300.       2.       1.      15.875]-[9571.            0.            0.           11.86999702]-
[9617.            1.            2.            6.70937824]-[9644.       2.       1.      15.875]-[9705.            2.            0.           15.72045135]-[9801.       0.       0.      15.875]-[9803.            3.            0.            6.20249271]-[9865.            3.            3.           15.85313606]-[9896.       2.       1.      15.875]-[10314.        1.        1.       15.875]-[10337.        3.        0.       15.875]-[10403.             0.             2.            15.34475708]-
[10653.             2.             3.            15.24176979]-[10704.        2.        1.       15.875]-[10719.        1.        1.       15.875]-[10727.        1.        1.       15.875]-[10836.        0.        0.       15.875]-[10969.             2.             2.             8.96601295]-[11042.        0.        0.       15.875]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        1.       15.875]-
[11499.        0.        0.       15.875]-[11502.        3.        0.       15.875]-[11512.        3.        3.       15.875]-[11608.        1.        1.       15.875]-[11610.        0.        0.       15.875]-[11692.        0.        0.       15.875]-[11905.        0.        3.       15.875]-[11993.        1.        1.       15.875]-[12002.        2.        1.       15.875]-[12052.        0.        0.       15.875]-
[12201.        0.        0.       15.875]-[12235.        2.        1.       15.875]-[12320.        1.        0.       15.875]-[12377.        2.        1.       15.875]-[12398.        2.        3.       15.875]-[12503.        1.        1.       15.875]-[12617.        0.        1.       15.875]-[12685.             3.             2.            15.76046658]-[12738.        2.        0.       15.875]-[12742.        2.        1.       15.875]-
[12823.        0.        3.       15.875]-[13110.        1.        1.       15.875]-[13240.        3.        0.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.             2.             3.            10.13023949]-[13905.            3.            3.           12.3294611]-[14060.        2.        1.       15.875]-[14065.             3.             3.            14.73410034]-
[14147.        3.        3.       15.875]-[14595.        2.        1.       15.875]-[14687.        2.        1.       15.875]-[14788.        2.        1.       15.875]-[14869.        1.        1.       15.875]-[14872.             3.             0.            10.94930553]-[14877.        1.        1.       15.875]-[14927.        0.        0.       15.875]-[15066.        0.        0.       15.875]-[15175.             1.             2.            15.82919312]-
[15178.            2.            3.           15.5914278]-[15375.        3.        0.       15.875]-[15389.        3.        0.       15.875]-[15568.        2.        1.       15.875]-[15675.             3.             3.            15.50807762]-[15869.        1.        2.       15.875]-[16207.        3.        0.       15.875]-[16236.             0.             3.            13.61593246]-[16302.             3.             2.            10.64985275]-[16331.        2.        1.       15.875]-
[16381.        0.        3.       15.875]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.            1.            3.           12.0510025]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.             3.             0.             5.26486206]-[17245.             1.             3.             2.91258526]-[17278.        3.        0.       15.875]-
[17282.        0.        0.       15.875]-[17311.        2.        1.       15.875]-[17336.        2.        1.       15.875]-[17608.        3.        3.       15.875]-[17627.        0.        0.       15.875]-[17877.             3.             2.            15.85090065]-[17924.        1.        1.       15.875]-[17984.             3.             3.            15.04444408]-[18211.             0.             3.            14.55053329]-[18276.             3.             0.            15.70249939]-
[18287.             1.             0.            10.35712624]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.             0.             3.            15.54317284]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.        0.        0.       15.875]-[18663.        0.        0.       15.875]-[18718.        0.        0.       15.875]-[18766.             2.             2.            13.42901516]-
[18824.        2.        1.       15.875]-[18890.        3.        3.       15.875]-[18930.        3.        1.       15.875]-[18938.        3.        3.       15.875]-[19817.        1.        1.       15.875]-[19839.        0.        2.       15.875]-[19930.        3.        0.       15.875]-[19944.             0.             0.            10.31583595]-[20036.        2.        1.       15.875]-[20101.        3.        0.       15.875]-
[20474.        1.        1.       15.875]-[20547.        3.        0.       15.875]-[20929.        2.        1.       15.875]-[21245.        1.        1.       15.875]-[21257.             3.             3.            15.58563423]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        1.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        1.       15.875]-[21714.        0.        3.       15.875]-[21943.             3.             3.            13.74858761]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        1.       15.875]-[21998.            1.            2.           15.5056839]-[22025.        0.        1.       15.875]-[22228.        3.        3.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.             3.             0.            15.56698704]-[22976.            3.            3.            7.1358881]-[22985.             3.             3.            15.80877876]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.             3.             3.            15.60367966]-[23168.        2.        1.       15.875]-[23219.        0.        0.       15.875]-
[23363.        3.        3.       15.875]-[23470.            0.            0.           14.0025692]-[23486.             2.             3.             7.78325129]-[23497.        0.        0.       15.875]-[23516.             0.             0.            14.69023323]-[23690.        1.        1.       15.875]-[23921.        2.        1.       15.875]-[23936.        1.        1.       15.875]-[24040.             3.             0.            14.12882042]-[24111.        1.        2.       15.875]-
[24182.        0.        0.       15.875]-[24238.        3.        0.       15.875]-[24290.        2.        0.       15.875]-[24345.        0.        0.       15.875]-[24364.        1.        1.       15.875]-[24427.             3.             3.            15.81769753]-[24477.        2.        1.       15.875]-[24495.             2.             2.            12.95334244]-[24893.        2.        2.       15.875]-[25012.            1.            2.           15.8194952]-
[25121.        2.        1.       15.875]-[25165.        3.        0.       15.875]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.        2.        2.       15.875]-[25644.        1.        1.       15.875]-[25718.        1.        1.       15.875]-[25774.             2.             2.             7.95017338]-[26032.        3.        0.       15.875]-
[26051.        3.        3.       15.875]-[26120.             0.             0.            15.68257713]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.        0.        3.       15.875]-[26838.             2.             2.            14.21977806]-[26860.        1.        1.       15.875]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.        1.        0.       15.875]-[27526.        0.        0.       15.875]-[27639.             3.             2.            11.94937897]-[27698.             3.             3.            15.41295433]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.             0.             0.            13.93735886]-[28503.        2.        1.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             2.            13.77985954]-[29777.        0.        0.       15.875]-[29877.        2.        1.       15.875]-[30035.             1.             2.            15.20114899]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        1.       15.875]-[30716.        0.        1.       15.875]-[30806.        2.        1.       15.875]-
[30906.        1.        1.       15.875]-[31007.             0.             3.            13.77625179]-[31181.        3.        3.       15.875]-[31238.             0.             0.            14.66478252]-[31347.        0.        0.       15.875]-[31422.        2.        0.       15.875]-[31429.        3.        1.       15.875]-[31431.             0.             0.            15.47444248]-[31432.        1.        1.       15.875]-[31477.        0.        0.       15.875]-
[31524.             1.             2.            13.91315937]-[31597.        1.        1.       15.875]-[31619.        1.        2.       15.875]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        3.       15.875]-[32074.             1.             0.             4.01610088]-[32078.        3.        0.       15.875]-[32111.        1.        1.       15.875]-[32127.        1.        1.       15.875]-
[32140.             3.             3.            14.07070255]-[32263.             2.             2.             5.65109348]-[32365.        0.        0.       15.875]-[32411.        2.        0.       15.875]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        3.       15.875]-[32584.             0.             2.             9.08019829]-[32622.             0.             2.            12.50458527]-[32858.        3.        0.       15.875]-
[32969.        3.        0.       15.875]-[33016.        2.        1.       15.875]-[33031.             1.             3.            15.83807182]-[33035.        2.        1.       15.875]-[33133.        2.        1.       15.875]-[33173.        2.        1.       15.875]-[33175.        3.        1.       15.875]-[33306.             3.             3.             6.68870449]-[33309.        2.        3.       15.875]-[33474.        0.        0.       15.875]-
[33478.             2.             2.            13.08781338]-[33618.             1.             3.            10.20665932]-[33712.        0.        0.       15.875]-[33782.        2.        1.       15.875]-[33914.             3.             3.            11.32172489]-[34076.        3.        3.       15.875]-[34112.        2.        1.       15.875]-[34138.        2.        2.       15.875]-[34239.             1.             3.            10.35627556]-[34364.        2.        1.       15.875]-
[34617.        1.        2.       15.875]-[34751.        3.        3.       15.875]-[34783.        2.        1.       15.875]-[35015.             3.             3.             6.27429438]-[35018.        1.        1.       15.875]-[35288.        2.        1.       15.875]-
---------------------------
I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 2.145 | Acc: 70.125% | Wgt Acc: 71.791%
	I - Batch: 100 | Loss: 2.187 | Acc: 69.125% | Wgt Acc: 70.596%
	I - Batch: 150 | Loss: 2.180 | Acc: 68.417% | Wgt Acc: 69.936%
I - num batch: 160
I - Train -- Loss: 2.182 | Acc: 68.198% | Wgt Acc: 69.754% | LR: 2.500000e-04 | Dur: 154.37s
I - Confusion Matrix: [row->prediction - col->label]
[[611.   5.   9.  52.]
 [  4. 461. 517.   2.]
 [ 13. 106. 189.   8.]
 [ 69.   6.  19. 476.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.148 | Acc: 53.211% | Wgt Acc: 54.823% | Dur: 16.30s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  5.  7. 12.]
 [ 1. 54. 50.  2.]
 [ 7. 13.  8. 15.]
 [25.  6. 10. 57.]]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 2.163 | Acc: 66.125% | Wgt Acc: 68.479%
	I - Batch: 100 | Loss: 2.152 | Acc: 66.688% | Wgt Acc: 68.574%
	I - Batch: 150 | Loss: 2.153 | Acc: 68.083% | Wgt Acc: 69.774%
I - num batch: 160
I - Train -- Loss: 2.154 | Acc: 68.159% | Wgt Acc: 69.825% | LR: 2.500000e-04 | Dur: 154.23s
I - Confusion Matrix: [row->prediction - col->label]
[[611.   5.   8.  37.]
 [  3. 463. 534.   3.]
 [ 13. 107. 176.  12.]
 [ 70.   3.  16. 486.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.161 | Acc: 52.294% | Wgt Acc: 54.144% | Dur: 16.18s
I - Confusion Matrix: [row->prediction - col->label]
[[46.  4.  3. 11.]
 [ 6. 57. 50.  4.]
 [ 3. 11. 12. 15.]
 [33.  6. 10. 56.]]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 2.169 | Acc: 68.750% | Wgt Acc: 69.558%
	I - Batch: 100 | Loss: 2.123 | Acc: 70.250% | Wgt Acc: 70.793%
	I - Batch: 150 | Loss: 2.119 | Acc: 70.458% | Wgt Acc: 70.694%
I - num batch: 160
I - Train -- Loss: 2.119 | Acc: 70.318% | Wgt Acc: 70.586% | LR: 2.500000e-04 | Dur: 157.85s
I - Confusion Matrix: [row->prediction - col->label]
[[618.   4.   7.  38.]
 [  2. 330. 356.   1.]
 [  8. 237. 358.  14.]
 [ 69.   7.  13. 485.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.168 | Acc: 52.599% | Wgt Acc: 53.261% | Dur: 16.59s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  5. 21.]
 [ 0. 39. 37.  1.]
 [ 3. 23. 16.  7.]
 [25. 12. 17. 57.]]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 2.072 | Acc: 72.125% | Wgt Acc: 72.185%
	I - Batch: 100 | Loss: 2.057 | Acc: 73.188% | Wgt Acc: 73.154%
	I - Batch: 150 | Loss: 2.020 | Acc: 73.542% | Wgt Acc: 73.241%
I - num batch: 160
I - Train -- Loss: 2.018 | Acc: 73.616% | Wgt Acc: 73.186% | LR: 1.250000e-04 | Dur: 151.85s
I - Confusion Matrix: [row->prediction - col->label]
[[648.   6.   6.  27.]
 [  2. 268. 262.   2.]
 [  6. 301. 454.   4.]
 [ 41.   3.  12. 505.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.123 | Acc: 53.517% | Wgt Acc: 53.940% | Dur: 16.42s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  4.  3. 15.]
 [ 0. 36. 33.  1.]
 [ 9. 29. 28. 12.]
 [26.  9. 11. 58.]]

I - Local maximum validation set accuracy:  53.52

I - Validation set results: 
[14.          1.          2.         14.48990822]-
[50.         3.         2.         6.1899724]-[124.      2.      2.     15.875]-[127.      0.      0.     15.875]-[443.      2.      1.     15.875]-[567.      0.      0.     15.875]-[573.      1.      1.     15.875]-[615.           0.           0.          15.53481007]-[695.      1.      2.     15.875]-[722.      3.      3.     15.875]-[826.      0.      0.     15.875]-
[878.           0.           0.           7.45254803]-[1103.            0.            3.           15.00402737]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.            2.            3.           14.76004028]-[2476.       2.       1.      15.875]-[2721.       2.       2.      15.875]-[2818.       1.       2.      15.875]-[2886.       2.       1.      15.875]-[3231.       2.       1.      15.875]-
[3333.       2.       1.      15.875]-[3482.       2.       2.      15.875]-[3536.       3.       3.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.       0.       3.      15.875]-[4140.       0.       0.      15.875]-[4214.       1.       2.      15.875]-[4346.       1.       3.      15.875]-[4581.       2.       1.      15.875]-
[4708.            3.            2.            5.55214548]-[4838.            3.            3.           15.47334576]-[4845.       1.       1.      15.875]-[4868.       0.       0.      15.875]-[4939.       0.       2.      15.875]-[4984.       2.       2.      15.875]-[5078.            1.            3.           15.78607178]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.            3.            3.           15.77396393]-[5987.       2.       2.      15.875]-[6014.       3.       3.      15.875]-[6033.       3.       2.      15.875]-[6313.       0.       3.      15.875]-[6421.            3.            2.           12.48109341]-[6500.            1.            3.            8.35652065]-[6583.       3.       3.      15.875]-[6683.           3.           3.          14.8499918]-
[6825.            2.            0.           13.57787132]-[6998.            3.            3.            4.35628891]-[7049.            3.            3.            4.91818237]-[7517.       1.       1.      15.875]-[7521.            1.            3.           15.22651863]-[7528.       1.       3.      15.875]-[7949.       1.       1.      15.875]-[8135.            1.            0.           14.98833466]-[8185.       3.       0.      15.875]-[8269.           3.           2.          13.2259903]-
[8273.       3.       3.      15.875]-[8543.           3.           0.          14.7854166]-[8666.       1.       1.      15.875]-[8672.            0.            3.           15.78478718]-[8903.       1.       1.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       1.      15.875]-[9281.            3.            3.           12.78515434]-[9300.       2.       1.      15.875]-[9571.           0.           3.          15.6310339]-
[9617.       1.       2.      15.875]-[9644.       2.       2.      15.875]-[9705.            2.            2.            5.99960136]-[9801.            0.            3.           14.58828926]-[9803.            3.            3.           14.36849499]-[9865.       3.       3.      15.875]-[9896.       2.       2.      15.875]-[10314.        1.        1.       15.875]-[10337.        3.        3.       15.875]-[10403.             0.             2.            15.49061298]-
[10653.             2.             2.            15.18245125]-[10704.        2.        1.       15.875]-[10719.        1.        1.       15.875]-[10727.        1.        2.       15.875]-[10836.        0.        0.       15.875]-[10969.        2.        3.       15.875]-[11042.        0.        3.       15.875]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        2.       15.875]-
[11499.        0.        0.       15.875]-[11502.             3.             3.            15.50631142]-[11512.        3.        3.       15.875]-[11608.        1.        1.       15.875]-[11610.        0.        0.       15.875]-[11692.             0.             3.            15.50949478]-[11905.        0.        3.       15.875]-[11993.        1.        2.       15.875]-[12002.        2.        2.       15.875]-[12052.        0.        0.       15.875]-
[12201.             0.             3.            15.70126915]-[12235.        2.        1.       15.875]-[12320.        1.        0.       15.875]-[12377.        2.        1.       15.875]-[12398.        2.        3.       15.875]-[12503.        1.        2.       15.875]-[12617.        0.        2.       15.875]-[12685.             3.             2.             8.32453632]-[12738.             2.             3.            14.87582588]-[12742.        2.        1.       15.875]-
[12823.        0.        3.       15.875]-[13110.        1.        1.       15.875]-[13240.        3.        3.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.             2.             3.            10.38440323]-[13905.        3.        3.       15.875]-[14060.        2.        1.       15.875]-[14065.        3.        3.       15.875]-
[14147.        3.        3.       15.875]-[14595.        2.        1.       15.875]-[14687.        2.        1.       15.875]-[14788.        2.        2.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        3.       15.875]-[14877.        1.        2.       15.875]-[14927.           0.           0.          10.454566]-[15066.        0.        3.       15.875]-[15175.        1.        2.       15.875]-
[15178.        2.        3.       15.875]-[15375.        3.        3.       15.875]-[15389.             3.             0.            12.22114944]-[15568.        2.        2.       15.875]-[15675.        3.        3.       15.875]-[15869.             1.             2.             8.29302406]-[16207.             3.             0.            11.57551384]-[16236.             0.             3.            13.76173592]-[16302.        3.        0.       15.875]-[16331.        2.        1.       15.875]-
[16381.        0.        3.       15.875]-[16488.        1.        2.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.             1.             2.            14.88916874]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.             3.             3.            12.73028946]-[17245.             1.             2.            14.97203827]-[17278.            3.            0.           15.3014822]-
[17282.             0.             0.            14.97252083]-[17311.        2.        1.       15.875]-[17336.        2.        2.       15.875]-[17608.        3.        3.       15.875]-[17627.             0.             0.             4.30973673]-[17877.             3.             3.             3.25742531]-[17924.        1.        1.       15.875]-[17984.        3.        3.       15.875]-[18211.             0.             3.            15.81083202]-[18276.        3.        3.       15.875]-
[18287.        1.        1.       15.875]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        3.       15.875]-[18478.             3.             0.             7.68493509]-[18607.        0.        0.       15.875]-[18616.             0.             0.            15.23072243]-[18663.             0.             3.             9.34760571]-[18718.        0.        0.       15.875]-[18766.             2.             2.             9.65479469]-
[18824.        2.        2.       15.875]-[18890.             3.             3.            14.80567265]-[18930.        3.        2.       15.875]-[18938.        3.        3.       15.875]-[19817.        1.        1.       15.875]-[19839.        0.        2.       15.875]-[19930.             3.             3.            14.97285461]-[19944.            0.            2.           14.8460741]-[20036.        2.        1.       15.875]-[20101.            3.            3.           10.2168026]-
[20474.             1.             2.            12.60022068]-[20547.            3.            3.           10.0720787]-[20929.        2.        1.       15.875]-[21245.        1.        2.       15.875]-[21257.             3.             3.            15.77967262]-[21293.        1.        1.       15.875]-[21316.        1.        2.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        1.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        1.       15.875]-[21714.        0.        3.       15.875]-[21943.             3.             3.            12.19857502]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        1.       15.875]-[21998.        1.        2.       15.875]-[22025.             0.             2.             6.06731129]-[22228.        3.        3.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        3.       15.875]-[22757.        0.        3.       15.875]-[22811.             3.             0.            12.68791771]-[22976.             3.             2.             6.70773411]-[22985.        3.        3.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.             3.             3.            12.73371983]-[23168.             2.             2.             7.74769115]-[23219.            0.            3.           11.7679081]-
[23363.        3.        3.       15.875]-[23470.             0.             0.            10.92475605]-[23486.            2.            3.           10.7686367]-[23497.        0.        0.       15.875]-[23516.            0.            3.           15.7713623]-[23690.             1.             2.             9.54169941]-[23921.        2.        2.       15.875]-[23936.             1.             3.             4.49589539]-[24040.             3.             0.            10.50082207]-[24111.        1.        1.       15.875]-
[24182.        0.        0.       15.875]-[24238.             3.             3.            15.68354225]-[24290.        2.        0.       15.875]-[24345.             0.             0.            15.66985035]-[24364.        1.        1.       15.875]-[24427.        3.        3.       15.875]-[24477.        2.        1.       15.875]-[24495.             2.             2.             8.44194508]-[24893.        2.        1.       15.875]-[25012.        1.        2.       15.875]-
[25121.        2.        1.       15.875]-[25165.        3.        0.       15.875]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.        2.        1.       15.875]-[25644.        1.        1.       15.875]-[25718.        1.        2.       15.875]-[25774.        2.        2.       15.875]-[26032.             3.             3.            15.70225143]-
[26051.        3.        3.       15.875]-[26120.             0.             0.            13.63538647]-[26321.        1.        2.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.        0.        3.       15.875]-[26838.        2.        2.       15.875]-[26860.        1.        2.       15.875]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.             1.             0.            13.87897682]-[27526.        0.        0.       15.875]-[27639.             3.             2.            15.71507359]-[27698.        3.        3.       15.875]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.            0.            3.           15.2532692]-[28503.        2.        1.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             2.             5.50630093]-[29777.        0.        0.       15.875]-[29877.             2.             2.             7.90678883]-[30035.        1.        1.       15.875]-[30098.        0.        3.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        1.       15.875]-[30716.        0.        2.       15.875]-[30806.        2.        2.       15.875]-
[30906.        1.        1.       15.875]-[31007.             0.             3.             1.60028315]-[31181.        3.        3.       15.875]-[31238.        0.        3.       15.875]-[31347.        0.        0.       15.875]-[31422.        2.        0.       15.875]-[31429.             3.             2.            12.82196999]-[31431.        0.        0.       15.875]-[31432.        1.        2.       15.875]-[31477.             0.             0.            15.67254829]-
[31524.             1.             0.             2.72029114]-[31597.        1.        2.       15.875]-[31619.             1.             2.            12.66094494]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        3.       15.875]-[32074.             1.             3.             9.54733467]-[32078.        3.        3.       15.875]-[32111.             1.             2.             9.21255112]-[32127.        1.        1.       15.875]-
[32140.             3.             3.            15.87085915]-[32263.             2.             3.            15.79638577]-[32365.        0.        0.       15.875]-[32411.        2.        3.       15.875]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        3.       15.875]-[32584.            0.            2.           11.0521698]-[32622.             0.             2.             2.39196682]-[32858.        3.        0.       15.875]-
[32969.        3.        0.       15.875]-[33016.        2.        1.       15.875]-[33031.             1.             3.            15.52991486]-[33035.        2.        1.       15.875]-[33133.        2.        2.       15.875]-[33173.        2.        2.       15.875]-[33175.        3.        1.       15.875]-[33306.             3.             2.            11.86131096]-[33309.        2.        3.       15.875]-[33474.        0.        0.       15.875]-
[33478.             2.             3.             9.87466335]-[33618.             1.             3.            15.70298767]-[33712.        0.        0.       15.875]-[33782.        2.        1.       15.875]-[33914.             3.             3.             9.90078545]-[34076.             3.             3.            15.78462887]-[34112.        2.        1.       15.875]-[34138.             2.             2.             5.60354662]-[34239.             1.             2.             5.87104034]-[34364.        2.        1.       15.875]-
[34617.        1.        2.       15.875]-[34751.            3.            3.           14.6430397]-[34783.        2.        2.       15.875]-[35015.             3.             3.            15.01930428]-[35018.        1.        1.       15.875]-[35288.        2.        2.       15.875]-
---------------------------
I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 2.000 | Acc: 76.125% | Wgt Acc: 74.365%
	I - Batch: 100 | Loss: 1.978 | Acc: 75.562% | Wgt Acc: 74.694%
	I - Batch: 150 | Loss: 2.017 | Acc: 73.708% | Wgt Acc: 72.961%
I - num batch: 160
I - Train -- Loss: 2.017 | Acc: 73.616% | Wgt Acc: 72.841% | LR: 1.250000e-04 | Dur: 154.64s
I - Confusion Matrix: [row->prediction - col->label]
[[650.   3.   8.  27.]
 [  0. 233. 222.   2.]
 [  7. 336. 491.   8.]
 [ 40.   6.  13. 501.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.213 | Acc: 46.789% | Wgt Acc: 46.535% | Dur: 16.14s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  2.  5. 29.]
 [ 4. 48. 51.  4.]
 [12. 26. 15. 28.]
 [ 7.  2.  4. 25.]]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 2.020 | Acc: 74.000% | Wgt Acc: 71.777%
	I - Batch: 100 | Loss: 2.000 | Acc: 73.500% | Wgt Acc: 71.457%
	I - Batch: 150 | Loss: 1.983 | Acc: 74.167% | Wgt Acc: 72.717%
I - num batch: 160
I - Train -- Loss: 1.985 | Acc: 73.930% | Wgt Acc: 72.390% | LR: 1.250000e-04 | Dur: 152.84s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   2.   6.  29.]
 [  0. 146. 144.   1.]
 [  6. 426. 577.   3.]
 [ 36.   4.   7. 505.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.117 | Acc: 52.294% | Wgt Acc: 51.359% | Dur: 15.76s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  9. 10. 25.]
 [ 0. 14. 15.  0.]
 [ 0. 37. 33.  3.]
 [22. 18. 17. 58.]]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 1.901 | Acc: 75.125% | Wgt Acc: 75.133%
	I - Batch: 100 | Loss: 1.929 | Acc: 75.750% | Wgt Acc: 74.887%
	I - Batch: 150 | Loss: 1.952 | Acc: 75.542% | Wgt Acc: 74.557%
I - num batch: 160
I - Train -- Loss: 1.950 | Acc: 75.658% | Wgt Acc: 74.646% | LR: 1.250000e-04 | Dur: 157.72s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   1.   7.  20.]
 [  1. 218. 182.   1.]
 [  7. 356. 536.   5.]
 [ 28.   3.   9. 512.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.147 | Acc: 52.905% | Wgt Acc: 52.106% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  6. 27.]
 [ 1. 29. 29.  1.]
 [ 7. 38. 33. 12.]
 [15.  5.  7. 46.]]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 1.987 | Acc: 73.375% | Wgt Acc: 71.705%
	I - Batch: 100 | Loss: 1.970 | Acc: 74.250% | Wgt Acc: 72.578%
	I - Batch: 150 | Loss: 1.941 | Acc: 75.292% | Wgt Acc: 73.901%
I - num batch: 160
I - Train -- Loss: 1.941 | Acc: 75.265% | Wgt Acc: 73.797% | LR: 1.250000e-04 | Dur: 156.16s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   4.   8.  18.]
 [  2. 161. 138.   2.]
 [  7. 410. 578.   5.]
 [ 23.   3.  10. 513.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.141 | Acc: 55.657% | Wgt Acc: 53.804% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  7. 10. 27.]
 [ 0.  9.  5.  0.]
 [ 3. 55. 47.  4.]
 [14.  7. 13. 55.]]

I - Local maximum validation set accuracy:  55.66

I - Validation set results: 
[14.          1.          2.         14.99650097]-
[50.          3.          3.         15.24172211]-[124.           2.           0.           9.36441231]-[127.      0.      0.     15.875]-[443.      2.      2.     15.875]-[567.      0.      0.     15.875]-[573.      1.      2.     15.875]-[615.      0.      3.     15.875]-[695.           1.           0.           8.07560349]-[722.      3.      3.     15.875]-[826.      0.      0.     15.875]-
[878.           0.           0.          15.80776215]-[1103.       0.       0.      15.875]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.       2.       3.      15.875]-[2476.       2.       2.      15.875]-[2721.       2.       2.      15.875]-[2818.            1.            0.           15.82718277]-[2886.       2.       2.      15.875]-[3231.       2.       2.      15.875]-
[3333.       2.       2.      15.875]-[3482.            2.            2.            7.21698952]-[3536.       3.       3.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.       0.       3.      15.875]-[4140.       0.       0.      15.875]-[4214.       1.       2.      15.875]-[4346.            1.            3.           10.91227531]-[4581.       2.       2.      15.875]-
[4708.            3.            2.            5.03788042]-[4838.            3.            3.            4.73062468]-[4845.       1.       2.      15.875]-[4868.       0.       0.      15.875]-[4939.            0.            2.           11.09012413]-[4984.            2.            3.           13.21769905]-[5078.           1.           0.          15.3212471]-[5396.       0.       0.      15.875]-[5479.       1.       2.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       3.      15.875]-[5987.       2.       2.      15.875]-[6014.            3.            3.           11.96096516]-[6033.       3.       0.      15.875]-[6313.       0.       3.      15.875]-[6421.            3.            2.            4.31591082]-[6500.            1.            3.           14.82395554]-[6583.       3.       3.      15.875]-[6683.            3.            3.            7.68800354]-
[6825.       2.       0.      15.875]-[6998.            3.            3.           14.97391224]-[7049.            3.            3.           11.81455994]-[7517.       1.       2.      15.875]-[7521.            1.            0.           11.13344574]-[7528.       1.       3.      15.875]-[7949.       1.       2.      15.875]-[8135.       1.       0.      15.875]-[8185.       3.       0.      15.875]-[8269.       3.       2.      15.875]-
[8273.       3.       0.      15.875]-[8543.       3.       0.      15.875]-[8666.       1.       2.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       2.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       2.      15.875]-[9281.       3.       0.      15.875]-[9300.       2.       2.      15.875]-[9571.            0.            0.           14.91331482]-
[9617.            1.            0.            4.50424337]-[9644.       2.       2.      15.875]-[9705.       2.       0.      15.875]-[9801.       0.       3.      15.875]-[9803.       3.       0.      15.875]-[9865.       3.       3.      15.875]-[9896.       2.       2.      15.875]-[10314.        1.        2.       15.875]-[10337.        3.        3.       15.875]-[10403.        0.        0.       15.875]-
[10653.             2.             2.             8.84033203]-[10704.        2.        1.       15.875]-[10719.        1.        2.       15.875]-[10727.             1.             2.            15.53170013]-[10836.        0.        0.       15.875]-[10969.        2.        0.       15.875]-[11042.             0.             0.            11.25051117]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        2.       15.875]-
[11499.        0.        0.       15.875]-[11502.        3.        0.       15.875]-[11512.        3.        3.       15.875]-[11608.        1.        2.       15.875]-[11610.        0.        0.       15.875]-[11692.        0.        3.       15.875]-[11905.        0.        0.       15.875]-[11993.        1.        2.       15.875]-[12002.        2.        0.       15.875]-[12052.        0.        0.       15.875]-
[12201.        0.        0.       15.875]-[12235.        2.        2.       15.875]-[12320.        1.        0.       15.875]-[12377.            2.            2.           12.8371191]-[12398.        2.        3.       15.875]-[12503.        1.        2.       15.875]-[12617.             0.             2.             3.41964936]-[12685.             3.             3.            11.45135307]-[12738.        2.        3.       15.875]-[12742.        2.        2.       15.875]-
[12823.        0.        3.       15.875]-[13110.        1.        2.       15.875]-[13240.             3.             3.            15.75205612]-[13253.        1.        2.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        2.       15.875]-[13763.             2.             3.            15.28246403]-[13905.             3.             3.            14.65063858]-[14060.        2.        2.       15.875]-[14065.            3.            3.           12.6869173]-
[14147.        3.        3.       15.875]-[14595.             2.             2.            15.82842827]-[14687.        2.        2.       15.875]-[14788.        2.        2.       15.875]-[14869.        1.        2.       15.875]-[14872.             3.             3.            13.38887405]-[14877.             1.             2.            12.96896362]-[14927.        0.        0.       15.875]-[15066.        0.        0.       15.875]-[15175.        1.        2.       15.875]-
[15178.        2.        0.       15.875]-[15375.             3.             3.            15.86569786]-[15389.        3.        0.       15.875]-[15568.        2.        2.       15.875]-[15675.            3.            3.           15.8138113]-[15869.             1.             2.             1.79909086]-[16207.        3.        0.       15.875]-[16236.        0.        0.       15.875]-[16302.        3.        0.       15.875]-[16331.        2.        1.       15.875]-
[16381.        0.        0.       15.875]-[16488.             1.             2.            15.84732533]-[16495.             0.             0.            15.01693058]-[16650.        0.        0.       15.875]-[16719.             1.             2.            11.46039009]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.        1.        2.       15.875]-[17278.        3.        0.       15.875]-
[17282.        0.        0.       15.875]-[17311.        2.        2.       15.875]-[17336.             2.             2.            11.96206284]-[17608.        3.        3.       15.875]-[17627.        0.        0.       15.875]-[17877.        3.        0.       15.875]-[17924.        1.        2.       15.875]-[17984.        3.        3.       15.875]-[18211.             0.             3.            14.82972145]-[18276.        3.        3.       15.875]-
[18287.        1.        2.       15.875]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        3.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.        0.        0.       15.875]-[18663.             0.             3.             2.38262415]-[18718.        0.        0.       15.875]-[18766.        2.        2.       15.875]-
[18824.             2.             2.            13.97665977]-[18890.             3.             3.            15.67917824]-[18930.             3.             0.            15.17587662]-[18938.        3.        3.       15.875]-[19817.        1.        2.       15.875]-[19839.            0.            0.           12.0322237]-[19930.             3.             3.            15.78959465]-[19944.        0.        0.       15.875]-[20036.        2.        1.       15.875]-[20101.             3.             3.            15.06727123]-
[20474.             1.             3.             4.84067297]-[20547.        3.        0.       15.875]-[20929.        2.        1.       15.875]-[21245.        1.        2.       15.875]-[21257.        3.        3.       15.875]-[21293.        1.        1.       15.875]-[21316.             1.             2.            15.51243401]-[21384.        1.        2.       15.875]-[21448.        1.        2.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        2.       15.875]-[21714.        0.        3.       15.875]-[21943.             3.             3.             7.02440357]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        2.       15.875]-[21998.        1.        2.       15.875]-[22025.             0.             0.            12.65909386]-[22228.        3.        3.       15.875]-[22446.        1.        2.       15.875]-
[22494.            3.            3.           15.5939827]-[22757.        0.        0.       15.875]-[22811.             3.             3.             9.10191727]-[22976.             3.             3.             5.78686047]-[22985.        3.        0.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        2.       15.875]-[23144.        3.        3.       15.875]-[23168.        2.        0.       15.875]-[23219.        0.        0.       15.875]-
[23363.        3.        3.       15.875]-[23470.        0.        0.       15.875]-[23486.             2.             3.            15.76743698]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.        1.        2.       15.875]-[23921.             2.             2.            14.78582764]-[23936.             1.             2.            12.00489426]-[24040.             3.             3.            13.57377625]-[24111.        1.        2.       15.875]-
[24182.        0.        3.       15.875]-[24238.        3.        0.       15.875]-[24290.        2.        0.       15.875]-[24345.             0.             0.             5.28069496]-[24364.        1.        2.       15.875]-[24427.        3.        3.       15.875]-[24477.        2.        2.       15.875]-[24495.        2.        2.       15.875]-[24893.        2.        2.       15.875]-[25012.        1.        2.       15.875]-
[25121.        2.        2.       15.875]-[25165.        3.        0.       15.875]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.             2.             2.            11.76059818]-[25644.        1.        1.       15.875]-[25718.        1.        2.       15.875]-[25774.        2.        2.       15.875]-[26032.             3.             3.            15.70310116]-
[26051.        3.        3.       15.875]-[26120.        0.        0.       15.875]-[26321.        1.        2.       15.875]-[26732.        1.        2.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.             0.             3.            15.15794945]-[26838.             2.             3.            15.04528713]-[26860.        1.        2.       15.875]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.        1.        2.       15.875]-[27526.        0.        0.       15.875]-[27639.             3.             3.             2.76213908]-[27698.        3.        3.       15.875]-[27772.             0.             3.            14.27179623]-[27890.        1.        2.       15.875]-[28040.        0.        0.       15.875]-[28503.        2.        2.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             3.             9.55772495]-[29777.        0.        0.       15.875]-[29877.             2.             3.             3.22987413]-[30035.        1.        2.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        2.       15.875]-[30716.            0.            0.            9.7431736]-[30806.             2.             3.             4.86872578]-
[30906.        1.        1.       15.875]-[31007.             0.             0.            -1.26000977]-[31181.        3.        3.       15.875]-[31238.        0.        3.       15.875]-[31347.        0.        0.       15.875]-[31422.             2.             2.             6.56353045]-[31429.             3.             3.             0.34084368]-[31431.        0.        0.       15.875]-[31432.             1.             2.            15.48549938]-[31477.        0.        0.       15.875]-
[31524.        1.        2.       15.875]-[31597.        1.        2.       15.875]-[31619.        1.        3.       15.875]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.             3.             0.             8.85261726]-[32074.        1.        2.       15.875]-[32078.        3.        3.       15.875]-[32111.        1.        2.       15.875]-[32127.        1.        1.       15.875]-
[32140.             3.             0.            12.94468689]-[32263.        2.        0.       15.875]-[32365.        0.        0.       15.875]-[32411.             2.             3.             7.61692047]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        3.       15.875]-[32584.             0.             0.            14.81838226]-[32622.        0.        2.       15.875]-[32858.             3.             0.            13.72867775]-
[32969.        3.        0.       15.875]-[33016.        2.        2.       15.875]-[33031.        1.        3.       15.875]-[33035.        2.        2.       15.875]-[33133.        2.        2.       15.875]-[33173.        2.        2.       15.875]-[33175.        3.        2.       15.875]-[33306.             3.             3.            11.13156319]-[33309.             2.             3.            15.47696686]-[33474.        0.        0.       15.875]-
[33478.             2.             0.             3.15331078]-[33618.             1.             3.            15.82996559]-[33712.        0.        0.       15.875]-[33782.        2.        2.       15.875]-[33914.             3.             0.             7.04598713]-[34076.        3.        3.       15.875]-[34112.            2.            3.            4.0316205]-[34138.             2.             3.             6.32006025]-[34239.             1.             2.            11.34038544]-[34364.        2.        2.       15.875]-
[34617.        1.        2.       15.875]-[34751.        3.        3.       15.875]-[34783.        2.        2.       15.875]-[35015.             3.             3.            15.26744843]-[35018.        1.        2.       15.875]-[35288.             2.             2.            11.30337906]-
---------------------------
I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 1.927 | Acc: 77.750% | Wgt Acc: 76.578%
	I - Batch: 100 | Loss: 1.942 | Acc: 76.375% | Wgt Acc: 74.842%
	I - Batch: 150 | Loss: 1.942 | Acc: 76.208% | Wgt Acc: 74.761%
I - num batch: 160
I - Train -- Loss: 1.940 | Acc: 76.404% | Wgt Acc: 74.912% | LR: 1.250000e-04 | Dur: 161.00s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   4.  12.  18.]
 [  0. 171. 108.   2.]
 [  6. 400. 601.   5.]
 [ 30.   3.  13. 513.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.097 | Acc: 58.716% | Wgt Acc: 57.337% | Dur: 16.38s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  6. 10. 21.]
 [ 0. 20. 11.  0.]
 [ 5. 43. 45.  9.]
 [12.  9.  9. 56.]]

I - Local maximum validation set accuracy:  58.72

I - Validation set results: 
[14.     1.     2.    15.875]-
[50.          3.          3.          8.33586311]-[124.           2.           0.          12.23515701]-[127.      0.      0.     15.875]-[443.      2.      2.     15.875]-[567.      0.      0.     15.875]-[573.      1.      2.     15.875]-[615.           0.           3.          13.25569344]-[695.           1.           2.           9.64207649]-[722.      3.      3.     15.875]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.            0.            0.            6.63895702]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.            2.            3.           12.94846725]-[2476.       2.       1.      15.875]-[2721.       2.       2.      15.875]-[2818.            1.            2.           15.24504089]-[2886.       2.       2.      15.875]-[3231.       2.       2.      15.875]-
[3333.       2.       1.      15.875]-[3482.       2.       2.      15.875]-[3536.       3.       3.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.       0.       3.      15.875]-[4140.       0.       0.      15.875]-[4214.            1.            3.           10.87927246]-[4346.            1.            0.            2.46658611]-[4581.       2.       1.      15.875]-
[4708.            3.            3.            4.45013905]-[4838.            3.            3.           12.97094059]-[4845.       1.       2.      15.875]-[4868.       0.       0.      15.875]-[4939.            0.            3.           13.35540581]-[4984.       2.       2.      15.875]-[5078.            1.            3.           14.04722118]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       3.      15.875]-[5987.       2.       2.      15.875]-[6014.            3.            3.           15.54013062]-[6033.            3.            2.           14.01766968]-[6313.       0.       3.      15.875]-[6421.            3.            2.           15.83481026]-[6500.            1.            2.           14.15078163]-[6583.       3.       3.      15.875]-[6683.            3.            2.           11.32901382]-
[6825.            2.            0.           12.38005733]-[6998.            3.            3.            6.30552053]-[7049.            3.            3.           10.38895893]-[7517.       1.       1.      15.875]-[7521.       1.       0.      15.875]-[7528.       1.       3.      15.875]-[7949.       1.       1.      15.875]-[8135.       1.       0.      15.875]-[8185.       3.       0.      15.875]-[8269.            3.            2.            8.55376625]-
[8273.           3.           3.           9.8825531]-[8543.       3.       0.      15.875]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       2.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       2.      15.875]-[9281.            3.            2.           12.10349274]-[9300.            2.            2.           15.66322327]-[9571.            0.            0.           12.03043747]-
[9617.            1.            3.            7.98120022]-[9644.       2.       2.      15.875]-[9705.       2.       0.      15.875]-[9801.       0.       0.      15.875]-[9803.            3.            0.           13.36446667]-[9865.       3.       3.      15.875]-[9896.       2.       2.      15.875]-[10314.        1.        2.       15.875]-[10337.        3.        3.       15.875]-[10403.             0.             0.            -0.90236616]-
[10653.             2.             3.             7.37940598]-[10704.        2.        1.       15.875]-[10719.        1.        2.       15.875]-[10727.             1.             0.             5.81472397]-[10836.        0.        0.       15.875]-[10969.        2.        0.       15.875]-[11042.             0.             3.             9.86438942]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        2.       15.875]-
[11499.        0.        0.       15.875]-[11502.             3.             3.             7.16871738]-[11512.             3.             3.             6.45514393]-[11608.        1.        2.       15.875]-[11610.        0.        0.       15.875]-[11692.        0.        0.       15.875]-[11905.             0.             0.            12.88620186]-[11993.        1.        2.       15.875]-[12002.        2.        0.       15.875]-[12052.        0.        0.       15.875]-
[12201.        0.        0.       15.875]-[12235.        2.        2.       15.875]-[12320.        1.        0.       15.875]-[12377.        2.        2.       15.875]-[12398.        2.        3.       15.875]-[12503.        1.        2.       15.875]-[12617.             0.             2.            15.71003342]-[12685.             3.             3.             9.48506451]-[12738.             2.             3.             4.94865894]-[12742.        2.        2.       15.875]-
[12823.        0.        0.       15.875]-[13110.        1.        1.       15.875]-[13240.             3.             3.            14.09352112]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        2.       15.875]-[13763.             2.             2.            15.74960232]-[13905.        3.        3.       15.875]-[14060.        2.        1.       15.875]-[14065.             3.             3.            14.52772903]-
[14147.             3.             3.            15.70935822]-[14595.        2.        2.       15.875]-[14687.        2.        2.       15.875]-[14788.        2.        2.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        3.       15.875]-[14877.             1.             2.            15.70393372]-[14927.             0.             0.             4.32146549]-[15066.             0.             0.            15.52611732]-[15175.        1.        2.       15.875]-
[15178.             2.             0.             1.58788157]-[15375.             3.             3.            13.71769905]-[15389.        3.        0.       15.875]-[15568.        2.        2.       15.875]-[15675.             3.             3.            11.58616447]-[15869.             1.             2.            15.84142685]-[16207.        3.        0.       15.875]-[16236.             0.             0.             9.31597519]-[16302.        3.        0.       15.875]-[16331.        2.        1.       15.875]-
[16381.        0.        3.       15.875]-[16488.        1.        2.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.             1.             3.            -0.64297056]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.        1.        2.       15.875]-[17278.             3.             0.            15.78244209]-
[17282.             0.             0.            15.72539902]-[17311.        2.        2.       15.875]-[17336.        2.        2.       15.875]-[17608.        3.        3.       15.875]-[17627.             0.             0.            11.46111965]-[17877.             3.             0.             9.94417953]-[17924.        1.        2.       15.875]-[17984.             3.             3.            14.68279076]-[18211.             0.             3.            10.29537964]-[18276.        3.        3.       15.875]-
[18287.        1.        2.       15.875]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        3.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.        0.        0.       15.875]-[18663.             0.             0.             6.62223244]-[18718.        0.        0.       15.875]-[18766.             2.             2.             5.80275726]-
[18824.             2.             2.            15.85715961]-[18890.        3.        3.       15.875]-[18930.             3.             2.             6.30759859]-[18938.             3.             3.            15.57183647]-[19817.        1.        2.       15.875]-[19839.        0.        2.       15.875]-[19930.             3.             3.             9.29824638]-[19944.             0.             3.             7.28850985]-[20036.        2.        1.       15.875]-[20101.             3.             3.            14.15414047]-
[20474.             1.             2.             8.22091103]-[20547.        3.        0.       15.875]-[20929.        2.        2.       15.875]-[21245.        1.        2.       15.875]-[21257.             3.             3.             6.64403677]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        2.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        2.       15.875]-[21714.        0.        3.       15.875]-[21943.        3.        3.       15.875]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        2.       15.875]-[21998.        1.        2.       15.875]-[22025.             0.             2.             8.90235138]-[22228.        3.        3.       15.875]-[22446.        1.        2.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.        3.        0.       15.875]-[22976.             3.             3.            13.31899261]-[22985.        3.        0.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        2.       15.875]-[23144.             3.             3.            15.87404537]-[23168.        2.        2.       15.875]-[23219.             0.             0.            15.18986416]-
[23363.        3.        3.       15.875]-[23470.        0.        0.       15.875]-[23486.             2.             2.             5.34038305]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.        1.        2.       15.875]-[23921.        2.        2.       15.875]-[23936.            1.            3.            3.5856719]-[24040.             3.             3.            14.45172787]-[24111.        1.        2.       15.875]-
[24182.        0.        0.       15.875]-[24238.             3.             3.             7.14296913]-[24290.        2.        0.       15.875]-[24345.             0.             0.            -2.21673059]-[24364.        1.        1.       15.875]-[24427.        3.        3.       15.875]-[24477.        2.        2.       15.875]-[24495.             2.             2.             4.59608746]-[24893.        2.        2.       15.875]-[25012.             1.             2.             8.76079941]-
[25121.        2.        2.       15.875]-[25165.        3.        0.       15.875]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.             2.             2.            14.75545025]-[25644.        1.        1.       15.875]-[25718.        1.        2.       15.875]-[25774.        2.        2.       15.875]-[26032.             3.             3.            13.00283241]-
[26051.        3.        3.       15.875]-[26120.        0.        0.       15.875]-[26321.        1.        2.       15.875]-[26732.        1.        2.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.        0.        0.       15.875]-[26838.             2.             2.            14.99217224]-[26860.             1.             2.            13.62589359]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.        1.        2.       15.875]-[27526.        0.        0.       15.875]-[27639.             3.             3.            15.78369522]-[27698.           3.           3.          14.430583]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.             0.             0.            15.48158455]-[28503.        2.        2.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             3.             3.91646504]-[29777.        0.        0.       15.875]-[29877.        2.        2.       15.875]-[30035.        1.        2.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        1.       15.875]-[30716.        0.        2.       15.875]-[30806.            2.            3.            7.2480545]-
[30906.        1.        1.       15.875]-[31007.             0.             3.            15.77695274]-[31181.        3.        3.       15.875]-[31238.        0.        0.       15.875]-[31347.        0.        0.       15.875]-[31422.        2.        0.       15.875]-[31429.        3.        2.       15.875]-[31431.        0.        0.       15.875]-[31432.        1.        2.       15.875]-[31477.        0.        0.       15.875]-
[31524.        1.        0.       15.875]-[31597.        1.        2.       15.875]-[31619.            1.            3.            4.1085968]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.             3.             3.            15.80577564]-[32074.        1.        2.       15.875]-[32078.        3.        3.       15.875]-[32111.             1.             2.            15.51838017]-[32127.        1.        1.       15.875]-
[32140.            3.            0.           15.6160183]-[32263.        2.        0.       15.875]-[32365.        0.        0.       15.875]-[32411.        2.        0.       15.875]-[32429.        3.        0.       15.875]-[32473.        3.        0.       15.875]-[32574.        3.        3.       15.875]-[32584.             0.             3.             7.03119373]-[32622.             0.             2.             1.33033895]-[32858.        3.        0.       15.875]-
[32969.        3.        0.       15.875]-[33016.        2.        2.       15.875]-[33031.        1.        3.       15.875]-[33035.        2.        2.       15.875]-[33133.        2.        1.       15.875]-[33173.        2.        2.       15.875]-[33175.        3.        2.       15.875]-[33306.             3.             2.            11.85896206]-[33309.             2.             3.            15.78896999]-[33474.        0.        0.       15.875]-
[33478.             2.             3.            -0.16471004]-[33618.             1.             3.            15.59691429]-[33712.        0.        0.       15.875]-[33782.        2.        2.       15.875]-[33914.             3.             3.            15.69172955]-[34076.        3.        3.       15.875]-[34112.             2.             3.             6.56308889]-[34138.             2.             3.             3.18888998]-[34239.             1.             2.             4.88548994]-[34364.        2.        1.       15.875]-
[34617.             1.             2.            12.91285133]-[34751.           3.           3.          14.855443]-[34783.        2.        2.       15.875]-[35015.             3.             3.            15.08718681]-[35018.        1.        2.       15.875]-[35288.        2.        2.       15.875]-
---------------------------
I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 1.894 | Acc: 77.875% | Wgt Acc: 76.519%
	I - Batch: 100 | Loss: 1.912 | Acc: 77.250% | Wgt Acc: 75.740%
	I - Batch: 150 | Loss: 1.884 | Acc: 77.708% | Wgt Acc: 76.448%
I - num batch: 160
I - Train -- Loss: 1.888 | Acc: 77.856% | Wgt Acc: 76.672% | LR: 1.250000e-04 | Dur: 155.53s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   2.   3.  16.]
 [  0. 217. 138.   0.]
 [  6. 355. 583.   4.]
 [ 26.   4.  10. 518.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.208 | Acc: 56.575% | Wgt Acc: 56.046% | Dur: 16.04s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7. 11. 32.]
 [ 1. 38. 30.  1.]
 [ 3. 26. 25.  6.]
 [ 9.  7.  9. 47.]]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 1.934 | Acc: 75.750% | Wgt Acc: 74.289%
	I - Batch: 100 | Loss: 1.886 | Acc: 77.438% | Wgt Acc: 76.584%
	I - Batch: 150 | Loss: 1.859 | Acc: 78.458% | Wgt Acc: 77.479%
I - num batch: 160
I - Train -- Loss: 1.858 | Acc: 78.877% | Wgt Acc: 77.964% | LR: 1.250000e-04 | Dur: 154.71s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   2.   9.  16.]
 [  0. 261. 157.   2.]
 [  9. 313. 562.   4.]
 [ 18.   2.   6. 516.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.155 | Acc: 56.269% | Wgt Acc: 55.503% | Dur: 16.68s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  6.  7. 24.]
 [ 0. 27. 24.  1.]
 [ 6. 37. 35.  7.]
 [14.  8.  9. 54.]]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 1.766 | Acc: 81.375% | Wgt Acc: 80.236%
	I - Batch: 100 | Loss: 1.836 | Acc: 80.250% | Wgt Acc: 79.338%
	I - Batch: 150 | Loss: 1.850 | Acc: 80.667% | Wgt Acc: 79.741%
I - num batch: 160
I - Train -- Loss: 1.855 | Acc: 80.644% | Wgt Acc: 79.697% | LR: 1.250000e-04 | Dur: 156.19s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   2.   5.  17.]
 [  1. 281. 125.   1.]
 [  6. 293. 594.   8.]
 [ 23.   2.  10. 512.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.180 | Acc: 52.294% | Wgt Acc: 53.261% | Dur: 16.32s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  3.  2. 15.]
 [ 0. 44. 43.  3.]
 [ 5. 24. 15. 12.]
 [27.  7. 15. 56.]]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 1.782 | Acc: 83.500% | Wgt Acc: 83.197%
	I - Batch: 100 | Loss: 1.791 | Acc: 84.000% | Wgt Acc: 83.584%
	I - Batch: 150 | Loss: 1.800 | Acc: 84.167% | Wgt Acc: 83.840%
I - num batch: 160
I - Train -- Loss: 1.802 | Acc: 83.706% | Wgt Acc: 83.289% | LR: 1.250000e-04 | Dur: 154.73s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   2.   7.  14.]
 [  0. 370. 153.   2.]
 [  6. 204. 565.   5.]
 [ 11.   2.   9. 517.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.224 | Acc: 55.657% | Wgt Acc: 55.095% | Dur: 15.75s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  5.  8. 24.]
 [ 2. 34. 30.  1.]
 [ 6. 31. 31. 12.]
 [12.  8.  6. 49.]]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 1.783 | Acc: 84.375% | Wgt Acc: 83.516%
	I - Batch: 100 | Loss: 1.802 | Acc: 84.062% | Wgt Acc: 83.668%
	I - Batch: 150 | Loss: 1.790 | Acc: 84.208% | Wgt Acc: 83.818%
I - num batch: 160
I - Train -- Loss: 1.788 | Acc: 84.335% | Wgt Acc: 83.908% | LR: 1.250000e-04 | Dur: 154.90s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   2.   5.  17.]
 [  0. 383. 140.   1.]
 [  8. 191. 580.  10.]
 [ 14.   2.   9. 510.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.146 | Acc: 58.104% | Wgt Acc: 57.269% | Dur: 15.65s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  5.  3. 18.]
 [ 0. 26. 13.  2.]
 [ 8. 38. 45.  9.]
 [18.  9. 14. 57.]]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 1.761 | Acc: 84.125% | Wgt Acc: 84.026%
	I - Batch: 100 | Loss: 1.787 | Acc: 84.312% | Wgt Acc: 84.272%
	I - Batch: 150 | Loss: 1.769 | Acc: 84.667% | Wgt Acc: 84.474%
I - num batch: 160
I - Train -- Loss: 1.772 | Acc: 84.531% | Wgt Acc: 84.351% | LR: 1.250000e-04 | Dur: 154.58s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   1.   4.  13.]
 [  1. 405. 161.   2.]
 [  4. 170. 560.   5.]
 [ 22.   2.   9. 518.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.220 | Acc: 52.599% | Wgt Acc: 52.310% | Dur: 15.93s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  3.  3. 19.]
 [ 2. 46. 39.  7.]
 [11. 24. 28. 24.]
 [13.  5.  5. 36.]]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 1.751 | Acc: 84.250% | Wgt Acc: 84.025%
	I - Batch: 100 | Loss: 1.763 | Acc: 83.750% | Wgt Acc: 83.704%
	I - Batch: 150 | Loss: 1.766 | Acc: 84.000% | Wgt Acc: 83.685%
I - num batch: 160
I - Train -- Loss: 1.761 | Acc: 84.256% | Wgt Acc: 83.961% | LR: 1.250000e-04 | Dur: 155.23s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   7.  10.]
 [  0. 388. 157.   1.]
 [  9. 188. 566.   8.]
 [ 15.   2.   4. 519.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.220 | Acc: 54.740% | Wgt Acc: 54.823% | Dur: 15.97s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  6.  7. 21.]
 [ 0. 37. 32.  2.]
 [ 3. 27. 24.  9.]
 [21.  8. 12. 54.]]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 1.723 | Acc: 86.625% | Wgt Acc: 86.443%
	I - Batch: 100 | Loss: 1.749 | Acc: 85.625% | Wgt Acc: 85.256%
	I - Batch: 150 | Loss: 1.747 | Acc: 86.000% | Wgt Acc: 85.561%
I - num batch: 160
I - Train -- Loss: 1.744 | Acc: 85.866% | Wgt Acc: 85.421% | LR: 1.250000e-04 | Dur: 158.29s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   2.   3.  13.]
 [  1. 394. 121.   2.]
 [  7. 180. 602.   9.]
 [ 12.   2.   8. 514.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.233 | Acc: 55.657% | Wgt Acc: 53.940% | Dur: 16.10s
I - Confusion Matrix: [row->prediction - col->label]
[[76.  7. 13. 26.]
 [ 0. 23. 15.  3.]
 [ 5. 43. 40. 14.]
 [ 7.  5.  7. 43.]]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 1.628 | Acc: 89.375% | Wgt Acc: 88.920%
	I - Batch: 100 | Loss: 1.684 | Acc: 87.938% | Wgt Acc: 87.519%
	I - Batch: 150 | Loss: 1.694 | Acc: 88.375% | Wgt Acc: 88.029%
I - num batch: 160
I - Train -- Loss: 1.695 | Acc: 88.221% | Wgt Acc: 87.907% | LR: 1.250000e-04 | Dur: 154.42s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   1.   4.   6.]
 [  0. 422. 104.   1.]
 [ 11. 153. 623.   4.]
 [ 11.   2.   3. 527.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.188 | Acc: 55.352% | Wgt Acc: 54.484% | Dur: 16.16s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  4.  7. 21.]
 [ 0. 32. 29.  2.]
 [ 8. 35. 34. 17.]
 [11.  7.  5. 46.]]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 1.664 | Acc: 89.750% | Wgt Acc: 90.121%
	I - Batch: 100 | Loss: 1.668 | Acc: 89.812% | Wgt Acc: 89.923%
	I - Batch: 150 | Loss: 1.683 | Acc: 88.833% | Wgt Acc: 88.841%
I - num batch: 160
I - Train -- Loss: 1.696 | Acc: 88.771% | Wgt Acc: 88.809% | LR: 1.250000e-04 | Dur: 154.07s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   4.   9.]
 [  1. 472. 133.   2.]
 [ 10. 104. 589.   4.]
 [  9.   2.   8. 523.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.187 | Acc: 56.881% | Wgt Acc: 55.910% | Dur: 15.91s
I - Confusion Matrix: [row->prediction - col->label]
[[67. 11. 12. 28.]
 [ 0. 26. 12.  1.]
 [ 3. 27. 40.  4.]
 [18. 14. 11. 53.]]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 1.631 | Acc: 90.125% | Wgt Acc: 89.715%
	I - Batch: 100 | Loss: 1.673 | Acc: 88.500% | Wgt Acc: 88.144%
	I - Batch: 150 | Loss: 1.670 | Acc: 89.292% | Wgt Acc: 88.974%
I - num batch: 160
I - Train -- Loss: 1.675 | Acc: 89.478% | Wgt Acc: 89.181% | LR: 1.250000e-04 | Dur: 156.32s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   1.   3.   8.]
 [  0. 440.  95.   2.]
 [  4. 135. 631.   3.]
 [ 10.   2.   5. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.203 | Acc: 60.550% | Wgt Acc: 59.443% | Dur: 15.97s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  3. 13.]
 [ 0. 24.  5.  2.]
 [ 7. 43. 54. 12.]
 [20.  6. 13. 59.]]

I - Local maximum validation set accuracy:  60.55

I - Validation set results: 
[14.     1.     2.    15.875]-
[50.          3.          2.          7.90130234]-[124.           2.           0.          15.20731926]-[127.      0.      0.     15.875]-[443.      2.      2.     15.875]-[567.      0.      0.     15.875]-[573.           1.           2.          15.81979942]-[615.      0.      3.     15.875]-[695.          1.          3.         13.5713129]-[722.      3.      3.     15.875]-[826.      0.      0.     15.875]-
[878.           0.           3.          12.05288219]-[1103.            0.            0.           15.59416389]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.       2.       3.      15.875]-[2476.       2.       2.      15.875]-[2721.       2.       2.      15.875]-[2818.           1.           2.          15.8656311]-[2886.       2.       2.      15.875]-[3231.       2.       2.      15.875]-
[3333.       2.       2.      15.875]-[3482.       2.       2.      15.875]-[3536.       3.       3.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.            0.            3.           15.87017632]-[4140.       0.       0.      15.875]-[4214.       1.       2.      15.875]-[4346.            1.            3.           15.79655457]-[4581.       2.       2.      15.875]-
[4708.       3.       3.      15.875]-[4838.       3.       3.      15.875]-[4845.       1.       2.      15.875]-[4868.       0.       0.      15.875]-[4939.       0.       2.      15.875]-[4984.            2.            3.            7.90188599]-[5078.            1.            0.            6.74303818]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       3.      15.875]-[5987.       2.       2.      15.875]-[6014.       3.       2.      15.875]-[6033.       3.       1.      15.875]-[6313.            0.            3.           14.82201576]-[6421.       3.       2.      15.875]-[6500.       1.       2.      15.875]-[6583.       3.       3.      15.875]-[6683.            3.            3.            4.11776257]-
[6825.            2.            3.           11.86464977]-[6998.            3.            3.            8.26496601]-[7049.            3.            2.            4.80520201]-[7517.       1.       1.      15.875]-[7521.           1.           0.          14.3335495]-[7528.       1.       3.      15.875]-[7949.       1.       1.      15.875]-[8135.           1.           0.          15.8636322]-[8185.            3.            3.           11.84891129]-[8269.       3.       2.      15.875]-
[8273.       3.       3.      15.875]-[8543.            3.            0.            7.63623619]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       2.      15.875]-[9001.       2.       2.      15.875]-[9036.       2.       2.      15.875]-[9281.            3.            2.           15.63172531]-[9300.       2.       2.      15.875]-[9571.       0.       3.      15.875]-
[9617.       1.       1.      15.875]-[9644.       2.       2.      15.875]-[9705.           2.           3.          15.3043375]-[9801.       0.       0.      15.875]-[9803.       3.       3.      15.875]-[9865.       3.       3.      15.875]-[9896.       2.       2.      15.875]-[10314.        1.        2.       15.875]-[10337.        3.        3.       15.875]-[10403.        0.        0.       15.875]-
[10653.             2.             2.            15.65497303]-[10704.        2.        1.       15.875]-[10719.        1.        2.       15.875]-[10727.        1.        2.       15.875]-[10836.        0.        0.       15.875]-[10969.             2.             3.            13.32701397]-[11042.             0.             3.            14.65206051]-[11088.        1.        2.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        2.       15.875]-
[11499.        0.        0.       15.875]-[11502.             3.             3.            15.47872734]-[11512.             3.             3.             4.70294666]-[11608.        1.        2.       15.875]-[11610.             0.             3.             9.07384014]-[11692.             0.             0.            14.99825859]-[11905.             0.             3.            15.27290535]-[11993.        1.        2.       15.875]-[12002.        2.        2.       15.875]-[12052.        0.        0.       15.875]-
[12201.             0.             0.            10.72139359]-[12235.        2.        2.       15.875]-[12320.        1.        0.       15.875]-[12377.        2.        2.       15.875]-[12398.        2.        3.       15.875]-[12503.             1.             2.            10.59747696]-[12617.        0.        2.       15.875]-[12685.             3.             3.            10.19706535]-[12738.        2.        3.       15.875]-[12742.        2.        2.       15.875]-
[12823.             0.             3.            14.87034893]-[13110.        1.        1.       15.875]-[13240.        3.        3.       15.875]-[13253.        1.        2.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.        2.        2.       15.875]-[13905.        3.        3.       15.875]-[14060.        2.        1.       15.875]-[14065.        3.        3.       15.875]-
[14147.        3.        3.       15.875]-[14595.        2.        2.       15.875]-[14687.        2.        2.       15.875]-[14788.        2.        2.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        3.       15.875]-[14877.             1.             2.            15.10217762]-[14927.        0.        3.       15.875]-[15066.             0.             3.            14.38636112]-[15175.        1.        2.       15.875]-
[15178.        2.        0.       15.875]-[15375.             3.             0.            10.78093243]-[15389.        3.        3.       15.875]-[15568.        2.        1.       15.875]-[15675.        3.        3.       15.875]-[15869.             1.             3.            15.45724392]-[16207.        3.        0.       15.875]-[16236.             0.             0.            14.53226185]-[16302.            3.            0.           12.7340374]-[16331.        2.        1.       15.875]-
[16381.             0.             0.            15.38591766]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.             1.             2.            11.20311737]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.        1.        2.       15.875]-[17278.             3.             0.            14.13415623]-
[17282.            0.            0.           15.7971344]-[17311.        2.        2.       15.875]-[17336.        2.        2.       15.875]-[17608.        3.        3.       15.875]-[17627.        0.        0.       15.875]-[17877.        3.        2.       15.875]-[17924.        1.        2.       15.875]-[17984.        3.        3.       15.875]-[18211.             0.             3.            15.36663437]-[18276.        3.        3.       15.875]-
[18287.        1.        2.       15.875]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        3.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.        0.        0.       15.875]-[18663.        0.        3.       15.875]-[18718.        0.        0.       15.875]-[18766.             2.             2.            15.55731201]-
[18824.        2.        2.       15.875]-[18890.             3.             3.            11.53990936]-[18930.        3.        2.       15.875]-[18938.        3.        3.       15.875]-[19817.        1.        2.       15.875]-[19839.        0.        0.       15.875]-[19930.             3.             0.            13.23641586]-[19944.        0.        2.       15.875]-[20036.        2.        2.       15.875]-[20101.             3.             3.            15.61089611]-
[20474.        1.        2.       15.875]-[20547.             3.             0.            15.73209953]-[20929.        2.        1.       15.875]-[21245.        1.        2.       15.875]-[21257.             3.             3.            10.31572914]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        2.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        2.       15.875]-[21714.             0.             3.             3.60846901]-[21943.             3.             3.            13.94117069]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        2.       15.875]-[21998.        1.        1.       15.875]-[22025.             0.             2.            14.55531216]-[22228.        3.        3.       15.875]-[22446.        1.        1.       15.875]-
[22494.           3.           0.          12.049366]-[22757.        0.        0.       15.875]-[22811.             3.             3.            15.50898457]-[22976.        3.        2.       15.875]-[22985.        3.        3.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.        3.        3.       15.875]-[23168.             2.             2.            15.70640755]-[23219.        0.        0.       15.875]-
[23363.        3.        3.       15.875]-[23470.        0.        0.       15.875]-[23486.             2.             2.            10.13149834]-[23497.        0.        0.       15.875]-[23516.             0.             3.            15.62640762]-[23690.        1.        2.       15.875]-[23921.        2.        2.       15.875]-[23936.             1.             2.             9.17162323]-[24040.             3.             3.             2.90644979]-[24111.        1.        2.       15.875]-
[24182.        0.        3.       15.875]-[24238.        3.        3.       15.875]-[24290.        2.        2.       15.875]-[24345.        0.        0.       15.875]-[24364.        1.        2.       15.875]-[24427.        3.        3.       15.875]-[24477.        2.        2.       15.875]-[24495.        2.        2.       15.875]-[24893.        2.        2.       15.875]-[25012.             1.             2.            15.15467739]-
[25121.        2.        2.       15.875]-[25165.             3.             0.            15.81165504]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.        2.        2.       15.875]-[25644.        1.        1.       15.875]-[25718.        1.        0.       15.875]-[25774.             2.             2.            15.49746037]-[26032.        3.        3.       15.875]-
[26051.        3.        3.       15.875]-[26120.        0.        0.       15.875]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.        0.        3.       15.875]-[26838.             2.             3.            11.71350288]-[26860.        1.        2.       15.875]-[26948.        0.        0.       15.875]-
[27049.             3.             3.            15.80545235]-[27098.        1.        2.       15.875]-[27526.        0.        0.       15.875]-[27639.             3.             2.            10.63155174]-[27698.        3.        3.       15.875]-[27772.            0.            0.           14.0886116]-[27890.        1.        1.       15.875]-[28040.        0.        0.       15.875]-[28503.        2.        2.       15.875]-[28577.        1.        2.       15.875]-
[28959.        0.        0.       15.875]-[29198.             3.             3.            11.04141808]-[29777.        0.        0.       15.875]-[29877.        2.        2.       15.875]-[30035.        1.        2.       15.875]-[30098.        0.        3.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        2.       15.875]-[30716.             0.             2.            14.04984379]-[30806.            2.            3.           15.0328331]-
[30906.        1.        1.       15.875]-[31007.             0.             0.            11.36837482]-[31181.        3.        3.       15.875]-[31238.        0.        3.       15.875]-[31347.        0.        0.       15.875]-[31422.        2.        0.       15.875]-[31429.             3.             2.            15.83834934]-[31431.        0.        0.       15.875]-[31432.        1.        2.       15.875]-[31477.        0.        0.       15.875]-
[31524.        1.        2.       15.875]-[31597.        1.        2.       15.875]-[31619.        1.        3.       15.875]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.           3.           3.          15.528512]-[32074.        1.        1.       15.875]-[32078.        3.        3.       15.875]-[32111.        1.        2.       15.875]-[32127.        1.        2.       15.875]-
[32140.        3.        3.       15.875]-[32263.             2.             3.             3.06057453]-[32365.        0.        0.       15.875]-[32411.        2.        3.       15.875]-[32429.        3.        0.       15.875]-[32473.            3.            3.           12.6599102]-[32574.        3.        3.       15.875]-[32584.            0.            2.           15.6772356]-[32622.        0.        2.       15.875]-[32858.            3.            3.           15.3681736]-
[32969.        3.        0.       15.875]-[33016.        2.        2.       15.875]-[33031.             1.             2.             5.11961222]-[33035.        2.        2.       15.875]-[33133.        2.        2.       15.875]-[33173.        2.        2.       15.875]-[33175.        3.        2.       15.875]-[33306.        3.        1.       15.875]-[33309.        2.        3.       15.875]-[33474.        0.        0.       15.875]-
[33478.        2.        3.       15.875]-[33618.             1.             3.            14.27871418]-[33712.             0.             0.            15.47135544]-[33782.        2.        2.       15.875]-[33914.        3.        3.       15.875]-[34076.        3.        3.       15.875]-[34112.        2.        2.       15.875]-[34138.             2.             2.            15.31637192]-[34239.        1.        2.       15.875]-[34364.        2.        2.       15.875]-
[34617.             1.             2.            10.52640629]-[34751.        3.        3.       15.875]-[34783.        2.        2.       15.875]-[35015.             3.             3.            15.39999199]-[35018.        1.        2.       15.875]-[35288.        2.        2.       15.875]-
---------------------------
I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 1.718 | Acc: 89.625% | Wgt Acc: 89.420%
	I - Batch: 100 | Loss: 1.699 | Acc: 89.312% | Wgt Acc: 88.976%
	I - Batch: 150 | Loss: 1.674 | Acc: 89.583% | Wgt Acc: 89.285%
I - num batch: 160
I - Train -- Loss: 1.673 | Acc: 89.831% | Wgt Acc: 89.561% | LR: 1.250000e-04 | Dur: 156.74s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   1.   5.  12.]
 [  0. 451.  87.   1.]
 [  6. 124. 637.   4.]
 [ 12.   2.   5. 521.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.183 | Acc: 61.162% | Wgt Acc: 60.598% | Dur: 15.86s
I - Confusion Matrix: [row->prediction - col->label]
[[65. 11.  7. 17.]
 [ 1. 35. 15.  1.]
 [ 3. 27. 43. 11.]
 [19.  5. 10. 57.]]

I - Local maximum validation set accuracy:  61.16

I - Validation set results: 
[14.          1.          2.         15.84932327]-
[50.     3.     3.    15.875]-[124.           2.           3.          15.46575451]-[127.      0.      0.     15.875]-[443.      2.      2.     15.875]-[567.      0.      0.     15.875]-[573.      1.      2.     15.875]-[615.      0.      3.     15.875]-[695.           1.           0.          12.20465851]-[722.           3.           3.          15.83012676]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.            0.            0.           15.46186256]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.            2.            3.           14.35400486]-[2476.       2.       1.      15.875]-[2721.       2.       2.      15.875]-[2818.            1.            0.           15.35233974]-[2886.       2.       2.      15.875]-[3231.       2.       2.      15.875]-
[3333.       2.       2.      15.875]-[3482.            2.            2.           13.23464012]-[3536.       3.       3.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.       0.       3.      15.875]-[4140.       0.       0.      15.875]-[4214.       1.       1.      15.875]-[4346.            1.            0.            3.04334879]-[4581.       2.       1.      15.875]-
[4708.           3.           3.           8.1783371]-[4838.       3.       3.      15.875]-[4845.       1.       2.      15.875]-[4868.       0.       0.      15.875]-[4939.            0.            2.            7.99605751]-[4984.       2.       2.      15.875]-[5078.            1.            0.           15.21068668]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       3.      15.875]-[5987.       2.       1.      15.875]-[6014.            3.            2.           10.27690792]-[6033.       3.       2.      15.875]-[6313.       0.       3.      15.875]-[6421.       3.       2.      15.875]-[6500.            1.            2.           15.78303337]-[6583.       3.       3.      15.875]-[6683.            3.            3.           11.28417301]-
[6825.            2.            3.           10.71515083]-[6998.            3.            3.           14.22999573]-[7049.            3.            3.            9.14329529]-[7517.       1.       1.      15.875]-[7521.       1.       0.      15.875]-[7528.       1.       3.      15.875]-[7949.       1.       2.      15.875]-[8135.       1.       0.      15.875]-[8185.            3.            3.           15.84251976]-[8269.       3.       2.      15.875]-
[8273.           3.           3.          13.1202898]-[8543.       3.       0.      15.875]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       2.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       2.      15.875]-[9281.       3.       2.      15.875]-[9300.            2.            2.           14.02185917]-[9571.            0.            0.           14.57666969]-
[9617.       1.       2.      15.875]-[9644.       2.       1.      15.875]-[9705.       2.       0.      15.875]-[9801.       0.       3.      15.875]-[9803.       3.       0.      15.875]-[9865.       3.       3.      15.875]-[9896.       2.       2.      15.875]-[10314.        1.        2.       15.875]-[10337.        3.        3.       15.875]-[10403.        0.        0.       15.875]-
[10653.        2.        2.       15.875]-[10704.        2.        1.       15.875]-[10719.        1.        1.       15.875]-[10727.        1.        2.       15.875]-[10836.        0.        0.       15.875]-[10969.        2.        3.       15.875]-[11042.             0.             3.            15.29572487]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.             2.             2.            14.46734428]-
[11499.        0.        0.       15.875]-[11502.            3.            0.            3.8237052]-[11512.        3.        3.       15.875]-[11608.        1.        1.       15.875]-[11610.             0.             0.            11.00251961]-[11692.             0.             3.            12.61962891]-[11905.             0.             0.            12.92118359]-[11993.        1.        2.       15.875]-[12002.             2.             2.             7.23940945]-[12052.        0.        0.       15.875]-
[12201.             0.             0.            15.81025887]-[12235.        2.        1.       15.875]-[12320.        1.        0.       15.875]-[12377.        2.        2.       15.875]-[12398.        2.        3.       15.875]-[12503.        1.        2.       15.875]-[12617.        0.        2.       15.875]-[12685.        3.        3.       15.875]-[12738.             2.             0.            14.94238949]-[12742.        2.        2.       15.875]-
[12823.        0.        3.       15.875]-[13110.        1.        1.       15.875]-[13240.        3.        3.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.        2.        2.       15.875]-[13905.             3.             3.            15.85756588]-[14060.        2.        1.       15.875]-[14065.             3.             0.            15.83535576]-
[14147.        3.        3.       15.875]-[14595.        2.        2.       15.875]-[14687.        2.        2.       15.875]-[14788.        2.        2.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        3.       15.875]-[14877.        1.        1.       15.875]-[14927.        0.        3.       15.875]-[15066.             0.             3.            15.71331787]-[15175.        1.        2.       15.875]-
[15178.        2.        0.       15.875]-[15375.             3.             3.            14.91880035]-[15389.        3.        3.       15.875]-[15568.        2.        2.       15.875]-[15675.        3.        3.       15.875]-[15869.             1.             3.            15.24837112]-[16207.        3.        0.       15.875]-[16236.        0.        0.       15.875]-[16302.        3.        0.       15.875]-[16331.        2.        1.       15.875]-
[16381.             0.             0.            15.81777382]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.             1.             2.            15.58627605]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.             3.             0.            15.54551411]-[17245.        1.        2.       15.875]-[17278.        3.        0.       15.875]-
[17282.             0.             0.            12.57018948]-[17311.        2.        2.       15.875]-[17336.        2.        1.       15.875]-[17608.        3.        3.       15.875]-[17627.             0.             0.            15.27491951]-[17877.             3.             2.            15.81103611]-[17924.             1.             2.            10.59777164]-[17984.        3.        3.       15.875]-[18211.             0.             0.            15.36769581]-[18276.        3.        3.       15.875]-
[18287.             1.             0.             3.53961611]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        3.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.        0.        0.       15.875]-[18663.        0.        3.       15.875]-[18718.        0.        0.       15.875]-[18766.        2.        2.       15.875]-
[18824.        2.        2.       15.875]-[18890.        3.        3.       15.875]-[18930.             3.             0.            15.72290325]-[18938.        3.        3.       15.875]-[19817.        1.        2.       15.875]-[19839.        0.        1.       15.875]-[19930.             3.             3.             4.34324408]-[19944.             0.             2.             2.70986557]-[20036.        2.        2.       15.875]-[20101.        3.        3.       15.875]-
[20474.        1.        2.       15.875]-[20547.        3.        0.       15.875]-[20929.        2.        1.       15.875]-[21245.        1.        1.       15.875]-[21257.            3.            3.            7.8824625]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        1.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        2.       15.875]-[21714.        0.        3.       15.875]-[21943.             3.             2.            15.84987164]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        2.       15.875]-[21998.        1.        1.       15.875]-[22025.             0.             3.            15.37343597]-[22228.        3.        3.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.        3.        0.       15.875]-[22976.        3.        2.       15.875]-[22985.        3.        0.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        1.       15.875]-[23144.        3.        3.       15.875]-[23168.        2.        0.       15.875]-[23219.        0.        0.       15.875]-
[23363.        3.        3.       15.875]-[23470.        0.        0.       15.875]-[23486.             2.             2.            11.76581669]-[23497.        0.        0.       15.875]-[23516.             0.             0.            15.64172268]-[23690.        1.        1.       15.875]-[23921.        2.        2.       15.875]-[23936.        1.        2.       15.875]-[24040.             3.             3.            14.21681595]-[24111.        1.        2.       15.875]-
[24182.             0.             3.            15.87054539]-[24238.        3.        3.       15.875]-[24290.        2.        0.       15.875]-[24345.             0.             0.            14.90797424]-[24364.        1.        2.       15.875]-[24427.             3.             3.            15.01969528]-[24477.        2.        2.       15.875]-[24495.        2.        2.       15.875]-[24893.        2.        1.       15.875]-[25012.        1.        1.       15.875]-
[25121.        2.        2.       15.875]-[25165.        3.        0.       15.875]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.        2.        2.       15.875]-[25644.        1.        1.       15.875]-[25718.             1.             0.            15.38882923]-[25774.             2.             2.            15.19862938]-[26032.             3.             3.             9.99543762]-
[26051.        3.        3.       15.875]-[26120.             0.             0.            13.53941727]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.        0.        3.       15.875]-[26838.        2.        3.       15.875]-[26860.        1.        2.       15.875]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.             1.             0.            15.09935951]-[27526.        0.        0.       15.875]-[27639.             3.             3.            15.48208904]-[27698.        3.        3.       15.875]-[27772.             0.             0.            10.71207047]-[27890.        1.        1.       15.875]-[28040.             0.             0.            12.10184002]-[28503.        2.        2.       15.875]-[28577.        1.        1.       15.875]-
[28959.        0.        0.       15.875]-[29198.        3.        2.       15.875]-[29777.        0.        0.       15.875]-[29877.             2.             2.            14.48124409]-[30035.        1.        1.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        1.       15.875]-[30716.        0.        0.       15.875]-[30806.             2.             3.             7.43893719]-
[30906.        1.        1.       15.875]-[31007.             0.             0.             8.87837505]-[31181.        3.        3.       15.875]-[31238.        0.        3.       15.875]-[31347.        0.        0.       15.875]-[31422.             2.             0.            15.09549904]-[31429.             3.             2.            11.02351952]-[31431.           0.           3.          10.999753]-[31432.        1.        1.       15.875]-[31477.        0.        3.       15.875]-
[31524.             1.             0.            -2.24615908]-[31597.        1.        2.       15.875]-[31619.             1.             3.            15.58610916]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        3.       15.875]-[32074.             1.             2.            15.73823357]-[32078.        3.        3.       15.875]-[32111.             1.             2.            15.49340248]-[32127.        1.        1.       15.875]-
[32140.           3.           3.          15.836483]-[32263.             2.             0.            13.93915176]-[32365.        0.        0.       15.875]-[32411.             2.             3.            15.30276871]-[32429.        3.        0.       15.875]-[32473.             3.             3.            12.73298836]-[32574.        3.        3.       15.875]-[32584.            0.            0.           15.7705164]-[32622.             0.             3.             5.17478371]-[32858.        3.        3.       15.875]-
[32969.             3.             3.            15.82582188]-[33016.        2.        2.       15.875]-[33031.        1.        3.       15.875]-[33035.        2.        1.       15.875]-[33133.        2.        2.       15.875]-[33173.        2.        2.       15.875]-[33175.        3.        1.       15.875]-[33306.        3.        2.       15.875]-[33309.             2.             3.            15.81656361]-[33474.        0.        0.       15.875]-
[33478.             2.             3.            14.13933182]-[33618.        1.        3.       15.875]-[33712.        0.        0.       15.875]-[33782.        2.        2.       15.875]-[33914.        3.        3.       15.875]-[34076.        3.        3.       15.875]-[34112.             2.             2.            12.61045074]-[34138.        2.        2.       15.875]-[34239.             1.             2.            10.81175137]-[34364.        2.        2.       15.875]-
[34617.             1.             2.            14.89591217]-[34751.        3.        3.       15.875]-[34783.        2.        1.       15.875]-[35015.             3.             3.            15.85963726]-[35018.        1.        2.       15.875]-[35288.        2.        2.       15.875]-
---------------------------
I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 1.622 | Acc: 91.500% | Wgt Acc: 91.325%
	I - Batch: 100 | Loss: 1.628 | Acc: 91.125% | Wgt Acc: 90.784%
	I - Batch: 150 | Loss: 1.639 | Acc: 91.083% | Wgt Acc: 90.860%
I - num batch: 160
I - Train -- Loss: 1.635 | Acc: 91.088% | Wgt Acc: 90.853% | LR: 1.250000e-04 | Dur: 156.65s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   5.   4.]
 [  0. 464.  81.   1.]
 [  7. 112. 645.   7.]
 [  5.   2.   3. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.241 | Acc: 55.963% | Wgt Acc: 54.212% | Dur: 16.12s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  7.  6. 23.]
 [ 0. 29. 14.  1.]
 [10. 37. 50. 25.]
 [11.  5.  5. 37.]]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 1.609 | Acc: 90.250% | Wgt Acc: 89.807%
	I - Batch: 100 | Loss: 1.622 | Acc: 91.562% | Wgt Acc: 91.119%
	I - Batch: 150 | Loss: 1.604 | Acc: 92.292% | Wgt Acc: 91.985%
I - num batch: 160
I - Train -- Loss: 1.604 | Acc: 92.226% | Wgt Acc: 91.914% | LR: 1.250000e-04 | Dur: 156.00s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   1.   2.   3.]
 [  0. 465.  57.   1.]
 [  7. 110. 672.   5.]
 [  7.   2.   3. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.273 | Acc: 55.046% | Wgt Acc: 53.465% | Dur: 15.76s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  3.  3. 19.]
 [ 1. 27. 12.  3.]
 [13. 41. 53. 24.]
 [14.  7.  7. 40.]]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 1.623 | Acc: 91.375% | Wgt Acc: 90.937%
	I - Batch: 100 | Loss: 1.656 | Acc: 90.875% | Wgt Acc: 90.464%
	I - Batch: 150 | Loss: 1.634 | Acc: 91.083% | Wgt Acc: 90.630%
I - num batch: 160
I - Train -- Loss: 1.634 | Acc: 91.284% | Wgt Acc: 90.870% | LR: 1.250000e-04 | Dur: 155.77s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   6.   7.]
 [  1. 447.  54.   1.]
 [  7. 129. 669.   5.]
 [  5.   2.   5. 525.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.216 | Acc: 55.657% | Wgt Acc: 56.250% | Dur: 16.23s
I - Confusion Matrix: [row->prediction - col->label]
[[50.  4.  3.  9.]
 [ 2. 46. 32.  6.]
 [ 8. 21. 32. 17.]
 [28.  7.  8. 54.]]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 1.597 | Acc: 92.875% | Wgt Acc: 92.665%
	I - Batch: 100 | Loss: 1.639 | Acc: 90.875% | Wgt Acc: 90.508%
	I - Batch: 150 | Loss: 1.643 | Acc: 91.083% | Wgt Acc: 90.838%
I - num batch: 160
I - Train -- Loss: 1.640 | Acc: 91.323% | Wgt Acc: 91.100% | LR: 1.250000e-04 | Dur: 155.77s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   1.   5.  12.]
 [  1. 476.  75.   1.]
 [  5.  99. 652.   7.]
 [ 11.   2.   2. 518.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.206 | Acc: 60.245% | Wgt Acc: 58.696% | Dur: 15.85s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  9.  9. 27.]
 [ 1. 31. 11.  1.]
 [ 3. 34. 49. 13.]
 [12.  4.  6. 45.]]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 1.613 | Acc: 91.250% | Wgt Acc: 90.840%
	I - Batch: 100 | Loss: 1.641 | Acc: 92.125% | Wgt Acc: 91.824%
	I - Batch: 150 | Loss: 1.641 | Acc: 91.625% | Wgt Acc: 91.259%
I - num batch: 160
I - Train -- Loss: 1.641 | Acc: 91.598% | Wgt Acc: 91.215% | LR: 1.250000e-04 | Dur: 153.70s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   6.  13.]
 [  1. 460.  46.   1.]
 [  5. 116. 676.   5.]
 [ 13.   2.   6. 519.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.268 | Acc: 55.963% | Wgt Acc: 53.804% | Dur: 15.50s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  5.  6. 26.]
 [ 0. 19.  6.  0.]
 [ 9. 48. 56. 19.]
 [12.  6.  7. 41.]]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 1.618 | Acc: 92.250% | Wgt Acc: 92.029%
	I - Batch: 100 | Loss: 1.610 | Acc: 92.812% | Wgt Acc: 92.484%
	I - Batch: 150 | Loss: 1.617 | Acc: 92.875% | Wgt Acc: 92.529%
I - num batch: 160
I - Train -- Loss: 1.627 | Acc: 92.894% | Wgt Acc: 92.551% | LR: 1.250000e-04 | Dur: 157.06s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   2.   8.   6.]
 [  1. 476.  38.   1.]
 [  5.  98. 682.   9.]
 [  5.   2.   6. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.241 | Acc: 55.657% | Wgt Acc: 54.212% | Dur: 15.65s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  9. 11. 32.]
 [ 0. 28. 12.  1.]
 [ 3. 33. 41. 11.]
 [14.  8. 11. 42.]]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 1.566 | Acc: 93.000% | Wgt Acc: 92.620%
	I - Batch: 100 | Loss: 1.577 | Acc: 92.812% | Wgt Acc: 92.477%
	I - Batch: 150 | Loss: 1.577 | Acc: 93.583% | Wgt Acc: 93.363%
I - num batch: 160
I - Train -- Loss: 1.577 | Acc: 93.483% | Wgt Acc: 93.241% | LR: 1.250000e-04 | Dur: 155.16s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   1.   3.   8.]
 [  0. 490.  50.   1.]
 [  7.  84. 679.   3.]
 [  4.   3.   2. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.232 | Acc: 56.575% | Wgt Acc: 55.231% | Dur: 16.11s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  5.  3. 19.]
 [ 1. 29.  9.  2.]
 [11. 38. 53. 21.]
 [17.  6. 10. 44.]]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 1.514 | Acc: 94.250% | Wgt Acc: 93.985%
	I - Batch: 100 | Loss: 1.558 | Acc: 94.125% | Wgt Acc: 93.775%
	I - Batch: 150 | Loss: 1.554 | Acc: 94.125% | Wgt Acc: 93.804%
I - num batch: 160
I - Train -- Loss: 1.562 | Acc: 93.914% | Wgt Acc: 93.613% | LR: 1.250000e-04 | Dur: 156.66s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   3.   7.]
 [  0. 488.  36.   1.]
 [  6.  88. 692.   4.]
 [  5.   2.   3. 526.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.245 | Acc: 56.881% | Wgt Acc: 56.182% | Dur: 15.77s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  5.  5. 18.]
 [ 1. 31. 12.  1.]
 [ 8. 38. 47. 15.]
 [23.  4. 11. 52.]]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 1.545 | Acc: 95.375% | Wgt Acc: 95.268%
	I - Batch: 100 | Loss: 1.541 | Acc: 95.250% | Wgt Acc: 95.090%
	I - Batch: 150 | Loss: 1.557 | Acc: 94.458% | Wgt Acc: 94.215%
I - num batch: 160
I - Train -- Loss: 1.555 | Acc: 94.700% | Wgt Acc: 94.471% | LR: 1.250000e-04 | Dur: 156.66s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   4.   4.]
 [  0. 502.  38.   1.]
 [  2.  74. 692.   4.]
 [  6.   2.   0. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.255 | Acc: 59.939% | Wgt Acc: 58.628% | Dur: 16.02s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  9.  7. 20.]
 [ 0. 26. 11.  0.]
 [ 4. 38. 47. 13.]
 [14.  5. 10. 53.]]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 1.579 | Acc: 94.625% | Wgt Acc: 94.405%
	I - Batch: 100 | Loss: 1.547 | Acc: 95.000% | Wgt Acc: 94.853%
	I - Batch: 150 | Loss: 1.540 | Acc: 94.792% | Wgt Acc: 94.591%
I - num batch: 160
I - Train -- Loss: 1.546 | Acc: 94.739% | Wgt Acc: 94.506% | LR: 1.250000e-04 | Dur: 155.52s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   3.   6.]
 [  0. 502.  33.   1.]
 [  3.  74. 694.   2.]
 [  6.   2.   4. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.241 | Acc: 48.930% | Wgt Acc: 49.049% | Dur: 16.14s
I - Confusion Matrix: [row->prediction - col->label]
[[23.  3.  1.  4.]
 [ 0. 10.  2.  0.]
 [ 7. 53. 55. 10.]
 [58. 12. 17. 72.]]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 1.598 | Acc: 95.250% | Wgt Acc: 95.189%
	I - Batch: 100 | Loss: 1.576 | Acc: 95.062% | Wgt Acc: 94.892%
	I - Batch: 150 | Loss: 1.567 | Acc: 94.458% | Wgt Acc: 94.182%
I - num batch: 160
I - Train -- Loss: 1.568 | Acc: 94.543% | Wgt Acc: 94.268% | LR: 1.250000e-04 | Dur: 155.16s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   3.   5.]
 [  0. 496.  27.   1.]
 [  7.  80. 702.   4.]
 [  8.   2.   2. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.268 | Acc: 56.881% | Wgt Acc: 55.027% | Dur: 15.89s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  6.  5. 25.]
 [ 2. 25. 11.  1.]
 [ 9. 43. 53. 19.]
 [10.  4.  6. 41.]]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 1.519 | Acc: 95.250% | Wgt Acc: 95.048%
	I - Batch: 100 | Loss: 1.520 | Acc: 95.875% | Wgt Acc: 95.732%
	I - Batch: 150 | Loss: 1.518 | Acc: 95.667% | Wgt Acc: 95.452%
I - num batch: 160
I - Train -- Loss: 1.539 | Acc: 95.603% | Wgt Acc: 95.391% | LR: 1.250000e-04 | Dur: 154.23s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   4.   4.]
 [  0. 515.  26.   0.]
 [  4.  61. 702.   6.]
 [  3.   2.   2. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.244 | Acc: 59.021% | Wgt Acc: 57.745% | Dur: 16.14s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  5. 15.]
 [ 0. 23.  7.  0.]
 [ 7. 47. 54. 16.]
 [20.  3.  9. 55.]]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 1.517 | Acc: 95.625% | Wgt Acc: 95.363%
	I - Batch: 100 | Loss: 1.514 | Acc: 94.938% | Wgt Acc: 94.666%
	I - Batch: 150 | Loss: 1.524 | Acc: 95.250% | Wgt Acc: 94.988%
I - num batch: 160
I - Train -- Loss: 1.525 | Acc: 95.406% | Wgt Acc: 95.152% | LR: 1.250000e-04 | Dur: 155.54s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   2.   5.]
 [  0. 507.  25.   1.]
 [  2.  69. 704.   3.]
 [  5.   2.   3. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.254 | Acc: 58.716% | Wgt Acc: 56.997% | Dur: 15.99s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  7.  5. 19.]
 [ 1. 24.  7.  2.]
 [ 5. 43. 56. 18.]
 [17.  4.  7. 47.]]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 1.533 | Acc: 95.875% | Wgt Acc: 95.663%
	I - Batch: 100 | Loss: 1.541 | Acc: 95.062% | Wgt Acc: 94.727%
	I - Batch: 150 | Loss: 1.535 | Acc: 95.375% | Wgt Acc: 95.118%
I - num batch: 160
I - Train -- Loss: 1.533 | Acc: 95.485% | Wgt Acc: 95.249% | LR: 1.250000e-04 | Dur: 155.88s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   2.   5.]
 [  0. 511.  22.   1.]
 [  5.  65. 709.   4.]
 [  8.   2.   1. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.241 | Acc: 56.881% | Wgt Acc: 55.910% | Dur: 16.76s
I - Confusion Matrix: [row->prediction - col->label]
[[51.  5.  2. 10.]
 [ 1. 23.  8.  1.]
 [ 7. 45. 56. 19.]
 [29.  5.  9. 56.]]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 1.514 | Acc: 96.125% | Wgt Acc: 95.832%
	I - Batch: 100 | Loss: 1.504 | Acc: 96.000% | Wgt Acc: 95.691%
	I - Batch: 150 | Loss: 1.503 | Acc: 95.958% | Wgt Acc: 95.706%
I - num batch: 160
I - Train -- Loss: 1.517 | Acc: 95.878% | Wgt Acc: 95.612% | LR: 1.250000e-04 | Dur: 156.44s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   4.   6.]
 [  0. 509.  18.   1.]
 [  2.  67. 712.   0.]
 [  5.   2.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.267 | Acc: 57.187% | Wgt Acc: 55.707% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  5.  3. 18.]
 [ 0. 32. 12.  5.]
 [11. 37. 55. 23.]
 [17.  4.  5. 40.]]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 1.486 | Acc: 96.625% | Wgt Acc: 96.507%
	I - Batch: 100 | Loss: 1.487 | Acc: 96.875% | Wgt Acc: 96.705%
	I - Batch: 150 | Loss: 1.510 | Acc: 95.917% | Wgt Acc: 95.746%
I - num batch: 160
I - Train -- Loss: 1.509 | Acc: 95.956% | Wgt Acc: 95.789% | LR: 1.250000e-04 | Dur: 154.08s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   4.   5.]
 [  0. 520.  26.   1.]
 [  5.  56. 703.   0.]
 [  3.   2.   1. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.268 | Acc: 56.881% | Wgt Acc: 55.571% | Dur: 15.71s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  6.  3. 18.]
 [ 0. 27.  9.  2.]
 [ 9. 39. 55. 19.]
 [22.  6.  8. 47.]]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 1.617 | Acc: 94.375% | Wgt Acc: 94.279%
	I - Batch: 100 | Loss: 1.554 | Acc: 95.438% | Wgt Acc: 95.339%
	I - Batch: 150 | Loss: 1.548 | Acc: 95.458% | Wgt Acc: 95.321%
I - num batch: 160
I - Train -- Loss: 1.552 | Acc: 95.563% | Wgt Acc: 95.426% | LR: 1.250000e-04 | Dur: 153.95s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   4.   5.]
 [  1. 522.  28.   1.]
 [  1.  54. 699.   3.]
 [ 11.   2.   3. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.274 | Acc: 59.327% | Wgt Acc: 57.337% | Dur: 15.98s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  6.  6. 18.]
 [ 1. 24.  5.  2.]
 [ 7. 44. 58. 22.]
 [12.  4.  6. 44.]]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 1.529 | Acc: 95.625% | Wgt Acc: 95.428%
	I - Batch: 100 | Loss: 1.530 | Acc: 95.938% | Wgt Acc: 95.797%
	I - Batch: 150 | Loss: 1.539 | Acc: 96.000% | Wgt Acc: 95.784%
I - num batch: 160
I - Train -- Loss: 1.535 | Acc: 96.192% | Wgt Acc: 95.993% | LR: 1.250000e-04 | Dur: 156.06s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   2.   8.]
 [  0. 524.  20.   0.]
 [  3.  52. 707.   3.]
 [  2.   2.   5. 527.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.238 | Acc: 58.104% | Wgt Acc: 57.065% | Dur: 16.17s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  6. 10. 19.]
 [ 1. 26. 12.  1.]
 [ 6. 41. 47. 12.]
 [18.  5.  6. 54.]]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 1.497 | Acc: 95.625% | Wgt Acc: 95.442%
	I - Batch: 100 | Loss: 1.521 | Acc: 96.438% | Wgt Acc: 96.360%
	I - Batch: 150 | Loss: 1.525 | Acc: 95.833% | Wgt Acc: 95.674%
I - num batch: 160
I - Train -- Loss: 1.526 | Acc: 95.720% | Wgt Acc: 95.524% | LR: 1.250000e-04 | Dur: 155.04s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   6.   6.]
 [  0. 517.  21.   0.]
 [  5.  59. 706.   3.]
 [  6.   2.   1. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.268 | Acc: 58.104% | Wgt Acc: 56.250% | Dur: 15.75s
I - Confusion Matrix: [row->prediction - col->label]
[[77.  6.  9. 31.]
 [ 0. 28. 14.  2.]
 [ 4. 40. 45. 13.]
 [ 7.  4.  7. 40.]]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 1.491 | Acc: 96.625% | Wgt Acc: 96.609%
	I - Batch: 100 | Loss: 1.490 | Acc: 96.625% | Wgt Acc: 96.518%
	I - Batch: 150 | Loss: 1.504 | Acc: 96.708% | Wgt Acc: 96.593%
I - num batch: 160
I - Train -- Loss: 1.506 | Acc: 96.702% | Wgt Acc: 96.594% | LR: 1.250000e-04 | Dur: 154.90s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   2.   4.]
 [  0. 534.  21.   1.]
 [  6.  42. 709.   0.]
 [  4.   2.   2. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.280 | Acc: 58.410% | Wgt Acc: 57.133% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  3. 12.]
 [ 0. 24.  8.  2.]
 [ 8. 45. 56. 19.]
 [22.  5.  8. 53.]]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 1.487 | Acc: 96.750% | Wgt Acc: 96.618%
	I - Batch: 100 | Loss: 1.483 | Acc: 97.125% | Wgt Acc: 96.962%
	I - Batch: 150 | Loss: 1.491 | Acc: 97.125% | Wgt Acc: 96.959%
I - num batch: 160
I - Train -- Loss: 1.488 | Acc: 97.252% | Wgt Acc: 97.098% | LR: 1.250000e-04 | Dur: 152.95s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   1.   2.   4.]
 [  0. 537.  14.   0.]
 [  4.  38. 718.   3.]
 [  2.   2.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.245 | Acc: 58.410% | Wgt Acc: 56.250% | Dur: 15.71s
I - Confusion Matrix: [row->prediction - col->label]
[[75. 10. 11. 34.]
 [ 0. 19.  4.  1.]
 [ 2. 43. 52.  6.]
 [11.  6.  8. 45.]]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 1.463 | Acc: 97.125% | Wgt Acc: 97.023%
	I - Batch: 100 | Loss: 1.474 | Acc: 97.125% | Wgt Acc: 97.036%
	I - Batch: 150 | Loss: 1.489 | Acc: 97.083% | Wgt Acc: 96.967%
I - num batch: 160
I - Train -- Loss: 1.498 | Acc: 97.055% | Wgt Acc: 96.939% | LR: 1.250000e-04 | Dur: 155.22s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   3.   6.]
 [  0. 540.  13.   1.]
 [  7.  36. 718.   1.]
 [  6.   2.   0. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.282 | Acc: 55.963% | Wgt Acc: 54.755% | Dur: 15.85s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  4.  4. 15.]
 [ 1. 21. 10.  1.]
 [10. 45. 53. 17.]
 [21.  8.  8. 53.]]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 1.521 | Acc: 95.875% | Wgt Acc: 95.749%
	I - Batch: 100 | Loss: 1.514 | Acc: 95.500% | Wgt Acc: 95.374%
	I - Batch: 150 | Loss: 1.514 | Acc: 96.000% | Wgt Acc: 95.876%
I - num batch: 160
I - Train -- Loss: 1.519 | Acc: 95.878% | Wgt Acc: 95.736% | LR: 1.250000e-04 | Dur: 154.79s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   4.   5.]
 [  0. 524.  21.   1.]
 [  4.  52. 705.   2.]
 [ 10.   2.   4. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.235 | Acc: 54.740% | Wgt Acc: 54.348% | Dur: 16.11s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  4.  3. 22.]
 [ 2. 40. 34.  5.]
 [ 6. 29. 29. 15.]
 [14.  5.  9. 44.]]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 1.488 | Acc: 96.625% | Wgt Acc: 96.569%
	I - Batch: 100 | Loss: 1.489 | Acc: 96.750% | Wgt Acc: 96.578%
	I - Batch: 150 | Loss: 1.485 | Acc: 96.750% | Wgt Acc: 96.603%
I - num batch: 160
I - Train -- Loss: 1.484 | Acc: 96.820% | Wgt Acc: 96.683% | LR: 1.250000e-04 | Dur: 154.45s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   1.   2.   2.]
 [  0. 530.  18.   0.]
 [  6.  45. 713.   1.]
 [  3.   2.   1. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.229 | Acc: 58.104% | Wgt Acc: 56.318% | Dur: 16.02s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  9.  9. 28.]
 [ 0. 21.  8.  1.]
 [ 3. 43. 49.  9.]
 [13.  5.  9. 48.]]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 1.502 | Acc: 96.250% | Wgt Acc: 95.960%
	I - Batch: 100 | Loss: 1.474 | Acc: 97.188% | Wgt Acc: 97.006%
	I - Batch: 150 | Loss: 1.489 | Acc: 97.000% | Wgt Acc: 96.830%
I - num batch: 160
I - Train -- Loss: 1.487 | Acc: 97.095% | Wgt Acc: 96.939% | LR: 1.250000e-04 | Dur: 156.77s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   3.   2.]
 [  0. 535.  15.   0.]
 [  2.  41. 714.   5.]
 [  2.   2.   2. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.254 | Acc: 57.798% | Wgt Acc: 55.842% | Dur: 15.99s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  7.  8. 26.]
 [ 0. 20.  5.  0.]
 [ 4. 46. 56. 14.]
 [17.  5.  6. 46.]]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 1.486 | Acc: 95.500% | Wgt Acc: 95.152%
	I - Batch: 100 | Loss: 1.492 | Acc: 96.312% | Wgt Acc: 96.081%
	I - Batch: 150 | Loss: 1.499 | Acc: 96.458% | Wgt Acc: 96.253%
I - num batch: 160
I - Train -- Loss: 1.505 | Acc: 96.349% | Wgt Acc: 96.152% | LR: 1.250000e-04 | Dur: 153.87s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   1.   5.]
 [  0. 523.  13.   0.]
 [  8.  53. 717.   3.]
 [  5.   2.   3. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.283 | Acc: 59.327% | Wgt Acc: 57.473% | Dur: 16.11s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  7. 22.]
 [ 0. 25.  7.  0.]
 [10. 43. 55. 19.]
 [ 9.  4.  6. 45.]]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 1.480 | Acc: 97.000% | Wgt Acc: 96.846%
	I - Batch: 100 | Loss: 1.481 | Acc: 97.062% | Wgt Acc: 96.896%
	I - Batch: 150 | Loss: 1.492 | Acc: 96.750% | Wgt Acc: 96.572%
I - num batch: 160
I - Train -- Loss: 1.501 | Acc: 96.623% | Wgt Acc: 96.426% | LR: 1.250000e-04 | Dur: 157.48s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   1.   2.   6.]
 [  0. 529.  12.   0.]
 [  4.  46. 717.   5.]
 [  5.   2.   3. 527.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.273 | Acc: 59.021% | Wgt Acc: 57.269% | Dur: 15.81s
I - Confusion Matrix: [row->prediction - col->label]
[[71. 11. 10. 24.]
 [ 0. 20.  7.  1.]
 [ 5. 45. 51. 10.]
 [12.  2.  7. 51.]]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 1.548 | Acc: 96.250% | Wgt Acc: 96.146%
	I - Batch: 100 | Loss: 1.498 | Acc: 96.750% | Wgt Acc: 96.587%
	I - Batch: 150 | Loss: 1.499 | Acc: 96.542% | Wgt Acc: 96.403%
I - num batch: 160
I - Train -- Loss: 1.499 | Acc: 96.584% | Wgt Acc: 96.444% | LR: 1.250000e-04 | Dur: 151.18s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   3.   4.]
 [  0. 531.  17.   0.]
 [  3.  45. 712.   3.]
 [  8.   2.   2. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.247 | Acc: 55.963% | Wgt Acc: 53.940% | Dur: 12.06s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  7. 20.]
 [ 0. 13.  5.  0.]
 [ 7. 56. 56. 17.]
 [16.  5.  7. 49.]]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 1.450 | Acc: 97.875% | Wgt Acc: 97.795%
	I - Batch: 100 | Loss: 1.461 | Acc: 97.625% | Wgt Acc: 97.587%
	I - Batch: 150 | Loss: 1.476 | Acc: 97.333% | Wgt Acc: 97.213%
I - num batch: 160
I - Train -- Loss: 1.475 | Acc: 97.487% | Wgt Acc: 97.373% | LR: 1.250000e-04 | Dur: 119.93s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   1.   2.   2.]
 [  0. 542.  15.   0.]
 [  2.  33. 717.   3.]
 [  4.   2.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.218 | Acc: 59.939% | Wgt Acc: 58.424% | Dur: 12.22s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  8.  8. 21.]
 [ 0. 27. 10.  3.]
 [ 2. 38. 51. 13.]
 [17.  5.  6. 49.]]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 1.473 | Acc: 97.250% | Wgt Acc: 97.150%
	I - Batch: 100 | Loss: 1.480 | Acc: 97.562% | Wgt Acc: 97.494%
	I - Batch: 150 | Loss: 1.476 | Acc: 97.750% | Wgt Acc: 97.670%
I - num batch: 160
I - Train -- Loss: 1.474 | Acc: 97.644% | Wgt Acc: 97.550% | LR: 1.250000e-04 | Dur: 119.43s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   2.   3.   4.]
 [  0. 546.  13.   1.]
 [  3.  29. 717.   0.]
 [  3.   1.   1. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.242 | Acc: 61.774% | Wgt Acc: 60.054% | Dur: 12.38s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  8. 10. 21.]
 [ 0. 22.  3.  2.]
 [ 4. 42. 55.  9.]
 [13.  6.  7. 54.]]

I - Local maximum validation set accuracy:  61.77

I - Validation set results: 
[14.     1.     2.    15.875]-
[50.          3.          3.          3.72426534]-[124.           2.           3.          14.39571571]-[127.      0.      0.     15.875]-[443.      2.      2.     15.875]-[567.      0.      0.     15.875]-[573.      1.      2.     15.875]-[615.           0.           3.          15.08421135]-[695.          1.          2.          5.8332181]-[722.      3.      3.     15.875]-[826.      0.      0.     15.875]-
[878.          0.          0.          6.2567215]-[1103.       0.       0.      15.875]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.            2.            3.           12.92072487]-[2476.       2.       2.      15.875]-[2721.       2.       2.      15.875]-[2818.       1.       2.      15.875]-[2886.       2.       2.      15.875]-[3231.       2.       2.      15.875]-
[3333.       2.       2.      15.875]-[3482.       2.       2.      15.875]-[3536.       3.       3.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.            0.            0.            9.99318695]-[4140.       0.       0.      15.875]-[4214.            1.            3.           11.37326908]-[4346.       1.       3.      15.875]-[4581.       2.       2.      15.875]-
[4708.       3.       3.      15.875]-[4838.            3.            3.            1.76679468]-[4845.       1.       2.      15.875]-[4868.            0.            0.           15.57041168]-[4939.            0.            3.           13.97620296]-[4984.       2.       2.      15.875]-[5078.       1.       3.      15.875]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.            3.            3.           15.38981247]-[5987.       2.       2.      15.875]-[6014.       3.       2.      15.875]-[6033.       3.       1.      15.875]-[6313.       0.       3.      15.875]-[6421.       3.       2.      15.875]-[6500.            1.            2.           13.07661915]-[6583.       3.       3.      15.875]-[6683.            3.            3.           10.07053757]-
[6825.            2.            0.           13.75046539]-[6998.       3.       3.      15.875]-[7049.           3.           3.          15.8555584]-[7517.       1.       1.      15.875]-[7521.           1.           0.          14.6639328]-[7528.       1.       0.      15.875]-[7949.       1.       2.      15.875]-[8135.       1.       0.      15.875]-[8185.       3.       0.      15.875]-[8269.       3.       1.      15.875]-
[8273.            3.            0.            4.93873215]-[8543.       3.       0.      15.875]-[8666.       1.       2.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       2.      15.875]-[9001.       2.       2.      15.875]-[9036.       2.       2.      15.875]-[9281.            3.            2.           12.42236328]-[9300.       2.       2.      15.875]-[9571.            0.            0.           10.42221451]-
[9617.            1.            0.           10.28175068]-[9644.       2.       2.      15.875]-[9705.       2.       0.      15.875]-[9801.            0.            3.           14.87234402]-[9803.            3.            3.            9.70478725]-[9865.       3.       3.      15.875]-[9896.       2.       2.      15.875]-[10314.        1.        2.       15.875]-[10337.        3.        3.       15.875]-[10403.             0.             2.            15.36230946]-
[10653.        2.        2.       15.875]-[10704.        2.        1.       15.875]-[10719.        1.        2.       15.875]-[10727.        1.        2.       15.875]-[10836.        0.        0.       15.875]-[10969.        2.        3.       15.875]-[11042.             0.             3.            13.65644932]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.             2.             2.            13.98635197]-
[11499.        0.        0.       15.875]-[11502.        3.        0.       15.875]-[11512.        3.        3.       15.875]-[11608.        1.        2.       15.875]-[11610.        0.        0.       15.875]-[11692.        0.        0.       15.875]-[11905.             0.             0.            11.38885117]-[11993.        1.        2.       15.875]-[12002.             2.             0.            14.38247681]-[12052.        0.        0.       15.875]-
[12201.        0.        0.       15.875]-[12235.        2.        2.       15.875]-[12320.        1.        0.       15.875]-[12377.        2.        2.       15.875]-[12398.        2.        3.       15.875]-[12503.             1.             2.            15.75118351]-[12617.        0.        2.       15.875]-[12685.             3.             3.            14.60260105]-[12738.             2.             3.            10.26624298]-[12742.        2.        2.       15.875]-
[12823.        0.        0.       15.875]-[13110.        1.        2.       15.875]-[13240.        3.        3.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.        2.        2.       15.875]-[13905.        3.        3.       15.875]-[14060.        2.        1.       15.875]-[14065.             3.             0.            12.57574844]-
[14147.        3.        3.       15.875]-[14595.        2.        2.       15.875]-[14687.        2.        2.       15.875]-[14788.        2.        2.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        3.       15.875]-[14877.        1.        2.       15.875]-[14927.             0.             3.            15.43871784]-[15066.             0.             3.            12.48351479]-[15175.        1.        2.       15.875]-
[15178.        2.        0.       15.875]-[15375.             3.             3.            15.81876564]-[15389.             3.             3.            15.25200081]-[15568.        2.        2.       15.875]-[15675.            3.            3.           14.7972908]-[15869.             1.             3.            11.94726276]-[16207.        3.        0.       15.875]-[16236.        0.        0.       15.875]-[16302.        3.        0.       15.875]-[16331.        2.        2.       15.875]-
[16381.            0.            0.           15.8288517]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.        1.        2.       15.875]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.        1.        2.       15.875]-[17278.             3.             0.            15.56540298]-
[17282.        0.        0.       15.875]-[17311.        2.        2.       15.875]-[17336.             2.             2.            15.70162773]-[17608.        3.        3.       15.875]-[17627.        0.        0.       15.875]-[17877.             3.             2.            12.29202747]-[17924.        1.        2.       15.875]-[17984.        3.        3.       15.875]-[18211.        0.        3.       15.875]-[18276.        3.        3.       15.875]-
[18287.        1.        2.       15.875]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.             0.             3.             5.57900524]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.        0.        0.       15.875]-[18663.        0.        3.       15.875]-[18718.        0.        0.       15.875]-[18766.        2.        2.       15.875]-
[18824.        2.        2.       15.875]-[18890.             3.             3.            15.27107906]-[18930.             3.             0.             6.98952866]-[18938.        3.        3.       15.875]-[19817.        1.        2.       15.875]-[19839.        0.        0.       15.875]-[19930.            3.            3.           10.2718029]-[19944.             0.             2.             4.73891878]-[20036.        2.        2.       15.875]-[20101.        3.        3.       15.875]-
[20474.        1.        2.       15.875]-[20547.            3.            0.           13.9633522]-[20929.        2.        2.       15.875]-[21245.        1.        2.       15.875]-[21257.        3.        3.       15.875]-[21293.        1.        1.       15.875]-[21316.             1.             2.            11.12720776]-[21384.        1.        1.       15.875]-[21448.        1.        2.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        2.       15.875]-[21714.        0.        3.       15.875]-[21943.        3.        3.       15.875]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        2.       15.875]-[21998.        1.        1.       15.875]-[22025.        0.        0.       15.875]-[22228.        3.        3.       15.875]-[22446.        1.        1.       15.875]-
[22494.             3.             0.            15.66537571]-[22757.        0.        0.       15.875]-[22811.        3.        0.       15.875]-[22976.        3.        2.       15.875]-[22985.        3.        3.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        2.       15.875]-[23144.        3.        3.       15.875]-[23168.             2.             0.             2.94005919]-[23219.        0.        0.       15.875]-
[23363.             3.             3.             8.85274506]-[23470.             0.             0.            12.15683365]-[23486.             2.             2.            15.35025883]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.        1.        1.       15.875]-[23921.        2.        2.       15.875]-[23936.        1.        2.       15.875]-[24040.             3.             0.             1.38692808]-[24111.             1.             2.            15.19915771]-
[24182.        0.        0.       15.875]-[24238.             3.             0.             5.15679455]-[24290.        2.        0.       15.875]-[24345.             0.             0.            15.80120277]-[24364.        1.        2.       15.875]-[24427.             3.             3.            15.65312386]-[24477.        2.        2.       15.875]-[24495.        2.        2.       15.875]-[24893.        2.        2.       15.875]-[25012.             1.             2.             6.11522055]-
[25121.        2.        2.       15.875]-[25165.             3.             3.            11.94640541]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.        2.        2.       15.875]-[25644.        1.        1.       15.875]-[25718.        1.        0.       15.875]-[25774.        2.        2.       15.875]-[26032.        3.        0.       15.875]-
[26051.        3.        3.       15.875]-[26120.        0.        0.       15.875]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.             0.             3.            10.10756493]-[26838.        2.        2.       15.875]-[26860.             1.             2.             5.38315296]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.           1.           0.          15.634305]-[27526.        0.        0.       15.875]-[27639.        3.        3.       15.875]-[27698.             3.             3.             7.84496784]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.        0.        0.       15.875]-[28503.        2.        2.       15.875]-[28577.        1.        2.       15.875]-
[28959.        0.        0.       15.875]-[29198.        3.        2.       15.875]-[29777.        0.        0.       15.875]-[29877.        2.        2.       15.875]-[30035.        1.        1.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        2.       15.875]-[30716.        0.        0.       15.875]-[30806.             2.             0.             2.69633245]-
[30906.        1.        1.       15.875]-[31007.            0.            3.           15.7741375]-[31181.             3.             3.             7.24569321]-[31238.        0.        0.       15.875]-[31347.        0.        0.       15.875]-[31422.        2.        0.       15.875]-[31429.             3.             2.            10.52249336]-[31431.        0.        0.       15.875]-[31432.        1.        2.       15.875]-[31477.        0.        0.       15.875]-
[31524.             1.             0.            15.13024616]-[31597.        1.        2.       15.875]-[31619.        1.        2.       15.875]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        3.       15.875]-[32074.        1.        2.       15.875]-[32078.        3.        3.       15.875]-[32111.        1.        2.       15.875]-[32127.        1.        1.       15.875]-
[32140.             3.             3.             4.43917465]-[32263.             2.             0.            11.85978317]-[32365.        0.        0.       15.875]-[32411.        2.        3.       15.875]-[32429.        3.        0.       15.875]-[32473.             3.             0.            15.44526482]-[32574.        3.        3.       15.875]-[32584.             0.             0.             6.13066721]-[32622.        0.        2.       15.875]-[32858.             3.             3.            12.66309547]-
[32969.        3.        0.       15.875]-[33016.        2.        2.       15.875]-[33031.             1.             3.            13.48983288]-[33035.        2.        2.       15.875]-[33133.        2.        2.       15.875]-[33173.        2.        2.       15.875]-[33175.        3.        2.       15.875]-[33306.        3.        2.       15.875]-[33309.        2.        3.       15.875]-[33474.        0.        0.       15.875]-
[33478.             2.             0.            11.55868149]-[33618.        1.        3.       15.875]-[33712.        0.        0.       15.875]-[33782.        2.        2.       15.875]-[33914.        3.        3.       15.875]-[34076.        3.        3.       15.875]-[34112.        2.        2.       15.875]-[34138.        2.        2.       15.875]-[34239.        1.        2.       15.875]-[34364.        2.        1.       15.875]-
[34617.        1.        2.       15.875]-[34751.        3.        3.       15.875]-[34783.        2.        2.       15.875]-[35015.        3.        3.       15.875]-[35018.        1.        2.       15.875]-[35288.        2.        2.       15.875]-
---------------------------
I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 1.439 | Acc: 99.000% | Wgt Acc: 98.933%
	I - Batch: 100 | Loss: 1.450 | Acc: 98.375% | Wgt Acc: 98.341%
	I - Batch: 150 | Loss: 1.460 | Acc: 98.250% | Wgt Acc: 98.188%
I - num batch: 160
I - Train -- Loss: 1.465 | Acc: 98.233% | Wgt Acc: 98.160% | LR: 1.250000e-04 | Dur: 122.99s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   1.   2.   1.]
 [  0. 553.   9.   0.]
 [  2.  23. 722.   2.]
 [  3.   1.   1. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.235 | Acc: 57.492% | Wgt Acc: 55.774% | Dur: 11.97s
I - Confusion Matrix: [row->prediction - col->label]
[[66. 10.  6. 24.]
 [ 1. 17.  3.  0.]
 [ 5. 45. 53. 10.]
 [16.  6. 13. 52.]]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 1.440 | Acc: 98.000% | Wgt Acc: 97.938%
	I - Batch: 100 | Loss: 1.454 | Acc: 97.375% | Wgt Acc: 97.223%
	I - Batch: 150 | Loss: 1.466 | Acc: 97.708% | Wgt Acc: 97.588%
I - num batch: 160
I - Train -- Loss: 1.464 | Acc: 97.762% | Wgt Acc: 97.656% | LR: 1.250000e-04 | Dur: 123.20s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   2.   2.]
 [  0. 545.  12.   0.]
 [  2.  31. 720.   2.]
 [  4.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.240 | Acc: 58.716% | Wgt Acc: 56.454% | Dur: 12.62s
I - Confusion Matrix: [row->prediction - col->label]
[[74. 10. 10. 33.]
 [ 0. 21.  3.  0.]
 [ 2. 42. 55. 11.]
 [12.  5.  7. 42.]]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 1.454 | Acc: 97.625% | Wgt Acc: 97.568%
	I - Batch: 100 | Loss: 1.476 | Acc: 97.438% | Wgt Acc: 97.333%
	I - Batch: 150 | Loss: 1.481 | Acc: 97.458% | Wgt Acc: 97.334%
I - num batch: 160
I - Train -- Loss: 1.480 | Acc: 97.566% | Wgt Acc: 97.443% | LR: 1.250000e-04 | Dur: 118.79s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   2.   5.]
 [  0. 546.  12.   0.]
 [  4.  30. 720.   4.]
 [  3.   2.   0. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.268 | Acc: 59.021% | Wgt Acc: 56.861% | Dur: 11.82s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  8.  7. 20.]
 [ 0. 15.  3.  0.]
 [ 7. 50. 58. 16.]
 [11.  5.  7. 50.]]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 1.453 | Acc: 97.000% | Wgt Acc: 96.865%
	I - Batch: 100 | Loss: 1.464 | Acc: 97.875% | Wgt Acc: 97.786%
	I - Batch: 150 | Loss: 1.477 | Acc: 97.500% | Wgt Acc: 97.400%
I - num batch: 160
I - Train -- Loss: 1.476 | Acc: 97.566% | Wgt Acc: 97.470% | LR: 1.250000e-04 | Dur: 147.85s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   3.   2.]
 [  0. 544.  15.   0.]
 [  4.  32. 716.   2.]
 [  2.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.241 | Acc: 58.716% | Wgt Acc: 57.541% | Dur: 16.85s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  5.  5. 14.]
 [ 0. 19.  4.  0.]
 [ 8. 45. 55. 12.]
 [22.  9. 11. 60.]]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 1.491 | Acc: 97.125% | Wgt Acc: 97.012%
	I - Batch: 100 | Loss: 1.494 | Acc: 97.125% | Wgt Acc: 96.986%
	I - Batch: 150 | Loss: 1.484 | Acc: 97.000% | Wgt Acc: 96.827%
I - num batch: 160
I - Train -- Loss: 1.479 | Acc: 97.134% | Wgt Acc: 96.966% | LR: 1.250000e-04 | Dur: 156.33s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   1.   4.   3.]
 [  0. 534.  11.   0.]
 [  2.  41. 718.   4.]
 [  4.   2.   1. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.266 | Acc: 57.187% | Wgt Acc: 55.027% | Dur: 15.92s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  7. 23.]
 [ 0. 11.  3.  0.]
 [ 6. 55. 59. 12.]
 [16.  6.  6. 51.]]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 1.444 | Acc: 97.625% | Wgt Acc: 97.541%
	I - Batch: 100 | Loss: 1.471 | Acc: 97.438% | Wgt Acc: 97.293%
	I - Batch: 150 | Loss: 1.474 | Acc: 97.292% | Wgt Acc: 97.138%
I - num batch: 160
I - Train -- Loss: 1.470 | Acc: 97.369% | Wgt Acc: 97.231% | LR: 1.250000e-04 | Dur: 156.56s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   2.   1.]
 [  0. 538.  15.   0.]
 [  2.  38. 716.   4.]
 [  2.   2.   1. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.220 | Acc: 58.104% | Wgt Acc: 56.318% | Dur: 15.76s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  4.  8. 20.]
 [ 1. 18.  4.  0.]
 [ 6. 48. 55. 15.]
 [15.  8.  8. 51.]]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 1.518 | Acc: 97.125% | Wgt Acc: 96.997%
	I - Batch: 100 | Loss: 1.493 | Acc: 97.438% | Wgt Acc: 97.325%
	I - Batch: 150 | Loss: 1.477 | Acc: 97.667% | Wgt Acc: 97.578%
I - num batch: 160
I - Train -- Loss: 1.477 | Acc: 97.527% | Wgt Acc: 97.426% | LR: 1.250000e-04 | Dur: 153.91s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   2.   6.]
 [  0. 548.  19.   0.]
 [  2.  28. 713.   3.]
 [  1.   2.   0. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.262 | Acc: 57.492% | Wgt Acc: 55.978% | Dur: 16.25s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  8.  8. 16.]
 [ 0. 13.  5.  0.]
 [ 4. 52. 54. 11.]
 [22.  5.  8. 59.]]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 1.483 | Acc: 97.125% | Wgt Acc: 97.095%
	I - Batch: 100 | Loss: 1.487 | Acc: 97.062% | Wgt Acc: 96.961%
	I - Batch: 150 | Loss: 1.471 | Acc: 97.625% | Wgt Acc: 97.549%
I - num batch: 160
I - Train -- Loss: 1.472 | Acc: 97.684% | Wgt Acc: 97.603% | LR: 1.250000e-04 | Dur: 155.93s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   3.   1.]
 [  0. 547.  15.   0.]
 [  2.  28. 715.   3.]
 [  3.   3.   1. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.246 | Acc: 59.021% | Wgt Acc: 57.677% | Dur: 15.78s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  6.  3. 18.]
 [ 1. 26.  9.  0.]
 [ 7. 41. 54. 17.]
 [18.  5.  9. 51.]]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 1.606 | Acc: 92.125% | Wgt Acc: 92.048%
	I - Batch: 100 | Loss: 1.556 | Acc: 93.875% | Wgt Acc: 93.703%
	I - Batch: 150 | Loss: 1.538 | Acc: 94.875% | Wgt Acc: 94.692%
I - num batch: 160
I - Train -- Loss: 1.537 | Acc: 95.014% | Wgt Acc: 94.825% | LR: 1.250000e-04 | Dur: 156.49s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   2.   7.]
 [  2. 509.  32.   0.]
 [  3.  67. 696.   1.]
 [  7.   2.   4. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.233 | Acc: 56.575% | Wgt Acc: 55.978% | Dur: 15.79s
I - Confusion Matrix: [row->prediction - col->label]
[[50.  3.  4.  9.]
 [ 1. 31. 12.  1.]
 [12. 42. 51. 23.]
 [25.  2.  8. 53.]]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 1.528 | Acc: 97.750% | Wgt Acc: 97.729%
	I - Batch: 100 | Loss: 1.491 | Acc: 97.062% | Wgt Acc: 97.043%
	I - Batch: 150 | Loss: 1.490 | Acc: 97.208% | Wgt Acc: 97.157%
I - num batch: 160
I - Train -- Loss: 1.487 | Acc: 97.291% | Wgt Acc: 97.231% | LR: 1.250000e-04 | Dur: 154.58s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   0.   4.]
 [  1. 547.  21.   0.]
 [  3.  29. 711.   2.]
 [  5.   2.   2. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.251 | Acc: 54.434% | Wgt Acc: 52.310% | Dur: 15.90s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  8. 11. 39.]
 [ 1. 27. 12.  1.]
 [ 4. 41. 48. 15.]
 [11.  2.  4. 31.]]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 1.480 | Acc: 98.000% | Wgt Acc: 97.913%
	I - Batch: 100 | Loss: 1.460 | Acc: 98.062% | Wgt Acc: 98.024%
	I - Batch: 150 | Loss: 1.462 | Acc: 97.833% | Wgt Acc: 97.757%
I - num batch: 160
I - Train -- Loss: 1.458 | Acc: 97.919% | Wgt Acc: 97.841% | LR: 1.250000e-04 | Dur: 154.12s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   1.   2.]
 [  0. 550.  17.   0.]
 [  2.  26. 716.   2.]
 [  1.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.214 | Acc: 56.575% | Wgt Acc: 55.639% | Dur: 16.26s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  6.  7. 15.]
 [ 0. 19.  7.  0.]
 [ 4. 47. 51. 11.]
 [29.  6. 10. 60.]]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 1.507 | Acc: 97.750% | Wgt Acc: 97.676%
	I - Batch: 100 | Loss: 1.467 | Acc: 97.562% | Wgt Acc: 97.401%
	I - Batch: 150 | Loss: 1.469 | Acc: 97.292% | Wgt Acc: 97.104%
I - num batch: 160
I - Train -- Loss: 1.474 | Acc: 97.095% | Wgt Acc: 96.886% | LR: 1.250000e-04 | Dur: 155.51s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   3.   5.]
 [  0. 529.  10.   0.]
 [  1.  47. 721.   2.]
 [  4.   2.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.201 | Acc: 51.376% | Wgt Acc: 51.427% | Dur: 16.72s
I - Confusion Matrix: [row->prediction - col->label]
[[31.  7.  2.  3.]
 [ 0.  9.  2.  0.]
 [ 3. 52. 52.  7.]
 [54. 10. 19. 76.]]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 1.499 | Acc: 97.375% | Wgt Acc: 97.149%
	I - Batch: 100 | Loss: 1.468 | Acc: 97.938% | Wgt Acc: 97.802%
	I - Batch: 150 | Loss: 1.464 | Acc: 98.042% | Wgt Acc: 97.934%
I - num batch: 160
I - Train -- Loss: 1.468 | Acc: 98.037% | Wgt Acc: 97.930% | LR: 1.250000e-04 | Dur: 155.31s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   1.   3.]
 [  0. 548.   7.   0.]
 [  2.  28. 723.   1.]
 [  3.   2.   3. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.233 | Acc: 59.327% | Wgt Acc: 58.152% | Dur: 16.25s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  7.  4.  9.]
 [ 0. 18.  3.  0.]
 [ 9. 51. 59. 15.]
 [24.  2.  9. 62.]]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 1.477 | Acc: 96.000% | Wgt Acc: 95.684%
	I - Batch: 100 | Loss: 1.469 | Acc: 96.875% | Wgt Acc: 96.643%
	I - Batch: 150 | Loss: 1.475 | Acc: 96.833% | Wgt Acc: 96.632%
I - num batch: 160
I - Train -- Loss: 1.478 | Acc: 96.859% | Wgt Acc: 96.665% | LR: 1.250000e-04 | Dur: 153.97s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   3.]
 [  0. 527.  12.   0.]
 [  5.  49. 721.   3.]
 [  5.   2.   1. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.255 | Acc: 57.187% | Wgt Acc: 54.959% | Dur: 16.37s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  9. 12. 30.]
 [ 0. 18.  6.  0.]
 [ 5. 44. 53. 13.]
 [10.  7.  4. 43.]]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 1.519 | Acc: 97.000% | Wgt Acc: 96.953%
	I - Batch: 100 | Loss: 1.493 | Acc: 97.688% | Wgt Acc: 97.662%
	I - Batch: 150 | Loss: 1.473 | Acc: 98.042% | Wgt Acc: 97.991%
I - num batch: 160
I - Train -- Loss: 1.470 | Acc: 97.958% | Wgt Acc: 97.903% | LR: 1.250000e-04 | Dur: 154.74s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   4.]
 [  0. 555.  15.   0.]
 [  4.  21. 717.   2.]
 [  2.   2.   2. 532.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.262 | Acc: 57.798% | Wgt Acc: 55.367% | Dur: 15.55s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  6.  8. 25.]
 [ 0. 14.  4.  0.]
 [ 4. 53. 57. 16.]
 [11.  5.  6. 45.]]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 1.464 | Acc: 98.500% | Wgt Acc: 98.452%
	I - Batch: 100 | Loss: 1.443 | Acc: 98.438% | Wgt Acc: 98.324%
	I - Batch: 150 | Loss: 1.455 | Acc: 98.000% | Wgt Acc: 97.878%
I - num batch: 160
I - Train -- Loss: 1.454 | Acc: 98.076% | Wgt Acc: 97.965% | LR: 1.250000e-04 | Dur: 155.98s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   4.]
 [  0. 551.  10.   0.]
 [  2.  25. 723.   3.]
 [  2.   2.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.253 | Acc: 61.162% | Wgt Acc: 59.307% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  8.  4. 16.]
 [ 1. 16.  2.  0.]
 [ 4. 49. 59. 13.]
 [15.  5. 10. 57.]]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 1.459 | Acc: 97.375% | Wgt Acc: 97.175%
	I - Batch: 100 | Loss: 1.461 | Acc: 97.500% | Wgt Acc: 97.374%
	I - Batch: 150 | Loss: 1.459 | Acc: 97.750% | Wgt Acc: 97.615%
I - num batch: 160
I - Train -- Loss: 1.457 | Acc: 97.841% | Wgt Acc: 97.709% | LR: 1.250000e-04 | Dur: 156.10s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   5.]
 [  0. 546.   9.   0.]
 [  2.  30. 725.   2.]
 [  5.   2.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.217 | Acc: 59.939% | Wgt Acc: 58.832% | Dur: 15.97s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  9.  7. 20.]
 [ 1. 23.  9.  1.]
 [ 2. 39. 51.  6.]
 [22.  7.  8. 59.]]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 1.485 | Acc: 98.375% | Wgt Acc: 98.312%
	I - Batch: 100 | Loss: 1.456 | Acc: 98.000% | Wgt Acc: 97.898%
	I - Batch: 150 | Loss: 1.455 | Acc: 98.292% | Wgt Acc: 98.206%
I - num batch: 160
I - Train -- Loss: 1.456 | Acc: 98.351% | Wgt Acc: 98.266% | LR: 1.250000e-04 | Dur: 155.20s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   1.   1.   3.]
 [  0. 555.   9.   0.]
 [  2.  20. 724.   2.]
 [  2.   2.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.251 | Acc: 59.939% | Wgt Acc: 58.628% | Dur: 16.18s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  3.  5. 16.]
 [ 1. 24.  5.  1.]
 [10. 46. 57. 14.]
 [17.  5.  8. 55.]]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 1.435 | Acc: 98.250% | Wgt Acc: 98.202%
	I - Batch: 100 | Loss: 1.458 | Acc: 97.688% | Wgt Acc: 97.554%
	I - Batch: 150 | Loss: 1.455 | Acc: 97.833% | Wgt Acc: 97.701%
I - num batch: 160
I - Train -- Loss: 1.454 | Acc: 97.919% | Wgt Acc: 97.788% | LR: 1.250000e-04 | Dur: 155.54s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   4.]
 [  0. 547.  10.   0.]
 [  3.  29. 724.   3.]
 [  2.   2.   0. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.216 | Acc: 60.550% | Wgt Acc: 59.783% | Dur: 16.19s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  6.  7. 10.]
 [ 1. 30. 13.  1.]
 [ 9. 37. 49. 17.]
 [17.  5.  6. 58.]]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 1.622 | Acc: 94.875% | Wgt Acc: 94.775%
	I - Batch: 100 | Loss: 1.593 | Acc: 95.062% | Wgt Acc: 94.842%
	I - Batch: 150 | Loss: 1.556 | Acc: 95.750% | Wgt Acc: 95.561%
I - num batch: 160
I - Train -- Loss: 1.549 | Acc: 95.917% | Wgt Acc: 95.727% | LR: 1.250000e-04 | Dur: 157.66s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   1.   0.  11.]
 [  0. 529.  15.   0.]
 [  2.  46. 714.   7.]
 [ 15.   2.   5. 520.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.224 | Acc: 59.021% | Wgt Acc: 59.103% | Dur: 16.86s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  4.  9. 10.]
 [ 0. 38. 24.  1.]
 [ 5. 28. 34. 15.]
 [22.  8.  8. 60.]]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 1.504 | Acc: 96.875% | Wgt Acc: 96.781%
	I - Batch: 100 | Loss: 1.520 | Acc: 96.312% | Wgt Acc: 96.137%
	I - Batch: 150 | Loss: 1.498 | Acc: 96.792% | Wgt Acc: 96.621%
I - num batch: 160
I - Train -- Loss: 1.497 | Acc: 96.898% | Wgt Acc: 96.727% | LR: 1.250000e-04 | Dur: 155.85s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   1.   5.]
 [  0. 532.  13.   0.]
 [  2.  44. 720.   3.]
 [  9.   2.   0. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.217 | Acc: 56.575% | Wgt Acc: 54.687% | Dur: 15.81s
I - Confusion Matrix: [row->prediction - col->label]
[[71. 10. 10. 29.]
 [ 0. 13.  4.  1.]
 [ 0. 47. 49.  4.]
 [17.  8. 12. 52.]]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 1.460 | Acc: 97.250% | Wgt Acc: 97.066%
	I - Batch: 100 | Loss: 1.452 | Acc: 97.875% | Wgt Acc: 97.787%
	I - Batch: 150 | Loss: 1.454 | Acc: 98.042% | Wgt Acc: 97.971%
I - num batch: 160
I - Train -- Loss: 1.454 | Acc: 98.155% | Wgt Acc: 98.089% | LR: 1.250000e-04 | Dur: 152.54s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.]
 [  0. 554.  11.   0.]
 [  1.  22. 720.   2.]
 [  4.   2.   3. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.242 | Acc: 58.716% | Wgt Acc: 56.997% | Dur: 16.80s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  8.  7. 17.]
 [ 0. 15.  7.  1.]
 [ 7. 49. 54. 12.]
 [14.  6.  7. 56.]]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 1.471 | Acc: 98.500% | Wgt Acc: 98.418%
	I - Batch: 100 | Loss: 1.466 | Acc: 97.812% | Wgt Acc: 97.746%
	I - Batch: 150 | Loss: 1.461 | Acc: 97.583% | Wgt Acc: 97.464%
I - num batch: 160
I - Train -- Loss: 1.459 | Acc: 97.684% | Wgt Acc: 97.567% | LR: 1.250000e-04 | Dur: 155.07s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   3.]
 [  0. 544.  13.   0.]
 [  1.  32. 721.   2.]
 [  6.   2.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.254 | Acc: 57.798% | Wgt Acc: 56.929% | Dur: 15.81s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  2.  6. 15.]
 [ 2. 33. 11.  2.]
 [13. 40. 52. 20.]
 [18.  3.  6. 49.]]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 1.444 | Acc: 98.125% | Wgt Acc: 98.062%
	I - Batch: 100 | Loss: 1.452 | Acc: 98.375% | Wgt Acc: 98.298%
	I - Batch: 150 | Loss: 1.460 | Acc: 98.042% | Wgt Acc: 97.917%
I - num batch: 160
I - Train -- Loss: 1.454 | Acc: 98.076% | Wgt Acc: 97.965% | LR: 1.250000e-04 | Dur: 153.71s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   2.]
 [  0. 548.   9.   0.]
 [  1.  28. 723.   2.]
 [  3.   2.   1. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.250 | Acc: 61.468% | Wgt Acc: 59.579% | Dur: 16.02s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  7. 23.]
 [ 1. 22.  4.  0.]
 [ 3. 44. 58. 12.]
 [14.  5.  6. 51.]]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 1.456 | Acc: 98.625% | Wgt Acc: 98.543%
	I - Batch: 100 | Loss: 1.444 | Acc: 98.375% | Wgt Acc: 98.284%
	I - Batch: 150 | Loss: 1.448 | Acc: 98.000% | Wgt Acc: 97.943%
I - num batch: 160
I - Train -- Loss: 1.457 | Acc: 97.919% | Wgt Acc: 97.850% | LR: 1.250000e-04 | Dur: 157.51s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   2.]
 [  0. 552.  14.   0.]
 [  2.  24. 718.   3.]
 [  4.   2.   2. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.245 | Acc: 55.963% | Wgt Acc: 53.872% | Dur: 16.78s
I - Confusion Matrix: [row->prediction - col->label]
[[74. 10. 13. 28.]
 [ 0. 15.  7.  0.]
 [ 4. 47. 48. 12.]
 [10.  6.  7. 46.]]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 1.396 | Acc: 99.000% | Wgt Acc: 98.952%
	I - Batch: 100 | Loss: 1.445 | Acc: 97.875% | Wgt Acc: 97.774%
	I - Batch: 150 | Loss: 1.455 | Acc: 98.000% | Wgt Acc: 97.917%
I - num batch: 160
I - Train -- Loss: 1.453 | Acc: 98.037% | Wgt Acc: 97.948% | LR: 1.250000e-04 | Dur: 155.36s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   5.]
 [  0. 554.  13.   0.]
 [  2.  22. 721.   3.]
 [  3.   2.   0. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.231 | Acc: 59.021% | Wgt Acc: 57.201% | Dur: 16.23s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  5.  8. 17.]
 [ 1. 14.  2.  0.]
 [ 6. 53. 57. 13.]
 [15.  6.  8. 56.]]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 1.533 | Acc: 96.250% | Wgt Acc: 96.192%
	I - Batch: 100 | Loss: 1.498 | Acc: 96.812% | Wgt Acc: 96.694%
	I - Batch: 150 | Loss: 1.473 | Acc: 97.458% | Wgt Acc: 97.334%
I - num batch: 160
I - Train -- Loss: 1.471 | Acc: 97.527% | Wgt Acc: 97.408% | LR: 1.250000e-04 | Dur: 155.75s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   1.]
 [  0. 541.  13.   0.]
 [  4.  35. 718.   3.]
 [  2.   2.   3. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.227 | Acc: 61.468% | Wgt Acc: 60.598% | Dur: 15.94s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  7. 11. 21.]
 [ 0. 32. 14.  1.]
 [ 4. 28. 39.  8.]
 [10. 11. 11. 56.]]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 1.432 | Acc: 97.750% | Wgt Acc: 97.544%
	I - Batch: 100 | Loss: 1.450 | Acc: 98.188% | Wgt Acc: 98.086%
	I - Batch: 150 | Loss: 1.450 | Acc: 98.417% | Wgt Acc: 98.329%
I - num batch: 160
I - Train -- Loss: 1.444 | Acc: 98.390% | Wgt Acc: 98.293% | LR: 1.250000e-04 | Dur: 155.02s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   4.]
 [  0. 554.   8.   0.]
 [  1.  22. 726.   1.]
 [  3.   2.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.231 | Acc: 59.939% | Wgt Acc: 58.220% | Dur: 16.08s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  8.  5. 16.]
 [ 1. 17.  4.  0.]
 [ 6. 51. 59. 14.]
 [17.  2.  7. 56.]]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 1.454 | Acc: 98.500% | Wgt Acc: 98.367%
	I - Batch: 100 | Loss: 1.436 | Acc: 98.125% | Wgt Acc: 97.984%
	I - Batch: 150 | Loss: 1.440 | Acc: 98.500% | Wgt Acc: 98.404%
I - num batch: 160
I - Train -- Loss: 1.438 | Acc: 98.469% | Wgt Acc: 98.363% | LR: 1.250000e-04 | Dur: 157.67s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 552.   6.   0.]
 [  1.  24. 728.   1.]
 [  3.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.247 | Acc: 55.657% | Wgt Acc: 53.872% | Dur: 15.94s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6. 10. 21.]
 [ 2. 23.  6.  2.]
 [ 7. 44. 52. 21.]
 [14.  5.  7. 42.]]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 1.425 | Acc: 98.750% | Wgt Acc: 98.673%
	I - Batch: 100 | Loss: 1.436 | Acc: 98.750% | Wgt Acc: 98.660%
	I - Batch: 150 | Loss: 1.443 | Acc: 98.792% | Wgt Acc: 98.724%
I - num batch: 160
I - Train -- Loss: 1.438 | Acc: 98.822% | Wgt Acc: 98.762% | LR: 1.250000e-04 | Dur: 154.58s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 561.   7.   0.]
 [  1.  15. 727.   2.]
 [  2.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.241 | Acc: 60.550% | Wgt Acc: 58.560% | Dur: 16.08s
I - Confusion Matrix: [row->prediction - col->label]
[[76.  9. 11. 29.]
 [ 0. 23.  6.  1.]
 [ 4. 43. 52.  9.]
 [ 8.  3.  6. 47.]]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 1.450 | Acc: 98.875% | Wgt Acc: 98.817%
	I - Batch: 100 | Loss: 1.437 | Acc: 98.812% | Wgt Acc: 98.788%
	I - Batch: 150 | Loss: 1.438 | Acc: 98.708% | Wgt Acc: 98.657%
I - num batch: 160
I - Train -- Loss: 1.438 | Acc: 98.783% | Wgt Acc: 98.735% | LR: 1.250000e-04 | Dur: 156.89s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   1.]
 [  0. 562.   6.   0.]
 [  1.  14. 728.   2.]
 [  5.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.229 | Acc: 59.327% | Wgt Acc: 57.609% | Dur: 16.68s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  6. 10. 24.]
 [ 0. 23.  7.  1.]
 [ 6. 43. 51. 12.]
 [11.  6.  7. 49.]]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 1.454 | Acc: 98.250% | Wgt Acc: 98.114%
	I - Batch: 100 | Loss: 1.462 | Acc: 98.625% | Wgt Acc: 98.539%
	I - Batch: 150 | Loss: 1.450 | Acc: 98.708% | Wgt Acc: 98.631%
I - num batch: 160
I - Train -- Loss: 1.446 | Acc: 98.744% | Wgt Acc: 98.664% | LR: 1.250000e-04 | Dur: 155.76s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 560.   5.   0.]
 [  3.  16. 729.   3.]
 [  1.   2.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.212 | Acc: 62.385% | Wgt Acc: 61.209% | Dur: 15.79s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  9. 14.]
 [ 0. 24.  4.  1.]
 [ 4. 41. 53. 10.]
 [18.  7.  9. 61.]]

I - Local maximum validation set accuracy:  62.39

I - Validation set results: 
[14.     1.     2.    15.875]-
[50.          3.          3.         15.73414612]-[124.      2.      0.     15.875]-[127.      0.      0.     15.875]-[443.      2.      1.     15.875]-[567.      0.      0.     15.875]-[573.      1.      2.     15.875]-[615.           0.           3.          14.26126194]-[695.           1.           3.           5.69882202]-[722.      3.      3.     15.875]-[826.      0.      0.     15.875]-
[878.          0.          3.         15.5952425]-[1103.       0.       0.      15.875]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.            2.            3.            5.54927921]-[2476.       2.       2.      15.875]-[2721.       2.       2.      15.875]-[2818.       1.       2.      15.875]-[2886.       2.       2.      15.875]-[3231.       2.       2.      15.875]-
[3333.       2.       2.      15.875]-[3482.       2.       2.      15.875]-[3536.       3.       3.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.       0.       3.      15.875]-[4140.       0.       0.      15.875]-[4214.            1.            3.            3.80081964]-[4346.       1.       3.      15.875]-[4581.       2.       2.      15.875]-
[4708.            3.            2.           11.64687538]-[4838.            3.            3.            5.61126232]-[4845.       1.       2.      15.875]-[4868.       0.       0.      15.875]-[4939.            0.            3.            3.90697217]-[4984.       2.       2.      15.875]-[5078.            1.            3.            8.35741711]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       3.      15.875]-[5987.       2.       2.      15.875]-[6014.           3.           2.          12.2573185]-[6033.       3.       2.      15.875]-[6313.       0.       3.      15.875]-[6421.       3.       2.      15.875]-[6500.       1.       2.      15.875]-[6583.       3.       3.      15.875]-[6683.            3.            3.           15.32549286]-
[6825.            2.            3.           14.86713123]-[6998.            3.            3.            3.30484295]-[7049.            3.            2.           13.76230335]-[7517.       1.       1.      15.875]-[7521.            1.            0.           10.93298054]-[7528.       1.       3.      15.875]-[7949.       1.       2.      15.875]-[8135.       1.       0.      15.875]-[8185.       3.       3.      15.875]-[8269.       3.       2.      15.875]-
[8273.            3.            3.           15.69991207]-[8543.            3.            0.           14.78456688]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       2.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       2.      15.875]-[9281.            3.            2.           14.83113289]-[9300.       2.       2.      15.875]-[9571.            0.            3.           10.54946804]-
[9617.       1.       1.      15.875]-[9644.       2.       2.      15.875]-[9705.       2.       0.      15.875]-[9801.       0.       0.      15.875]-[9803.            3.            3.           14.12679482]-[9865.       3.       3.      15.875]-[9896.       2.       2.      15.875]-[10314.        1.        2.       15.875]-[10337.        3.        3.       15.875]-[10403.             0.             0.            12.65464211]-
[10653.        2.        2.       15.875]-[10704.        2.        1.       15.875]-[10719.        1.        2.       15.875]-[10727.        1.        1.       15.875]-[10836.        0.        0.       15.875]-[10969.        2.        3.       15.875]-[11042.        0.        0.       15.875]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.             2.             0.            10.53593922]-
[11499.        0.        0.       15.875]-[11502.             3.             0.            13.59633064]-[11512.        3.        3.       15.875]-[11608.        1.        2.       15.875]-[11610.        0.        0.       15.875]-[11692.        0.        0.       15.875]-[11905.        0.        0.       15.875]-[11993.        1.        2.       15.875]-[12002.        2.        0.       15.875]-[12052.        0.        0.       15.875]-
[12201.             0.             0.             9.43379116]-[12235.        2.        2.       15.875]-[12320.        1.        0.       15.875]-[12377.        2.        2.       15.875]-[12398.        2.        3.       15.875]-[12503.        1.        2.       15.875]-[12617.        0.        2.       15.875]-[12685.             3.             3.            14.44619179]-[12738.             2.             3.            15.66754341]-[12742.        2.        2.       15.875]-
[12823.        0.        0.       15.875]-[13110.        1.        2.       15.875]-[13240.        3.        3.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.             2.             2.            14.11204147]-[13905.        3.        3.       15.875]-[14060.        2.        1.       15.875]-[14065.        3.        3.       15.875]-
[14147.        3.        3.       15.875]-[14595.        2.        2.       15.875]-[14687.        2.        2.       15.875]-[14788.        2.        2.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        3.       15.875]-[14877.        1.        2.       15.875]-[14927.             0.             3.             9.74081898]-[15066.             0.             0.            14.99143314]-[15175.        1.        2.       15.875]-
[15178.             2.             0.             7.22579718]-[15375.             3.             3.            14.84316921]-[15389.        3.        3.       15.875]-[15568.        2.        2.       15.875]-[15675.        3.        3.       15.875]-[15869.             1.             3.            11.69542694]-[16207.        3.        0.       15.875]-[16236.        0.        0.       15.875]-[16302.        3.        0.       15.875]-[16331.        2.        2.       15.875]-
[16381.        0.        3.       15.875]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.        1.        2.       15.875]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.             3.             3.            13.25310326]-[17245.        1.        2.       15.875]-[17278.        3.        0.       15.875]-
[17282.        0.        0.       15.875]-[17311.        2.        2.       15.875]-[17336.        2.        2.       15.875]-[17608.        3.        3.       15.875]-[17627.             0.             0.             4.14979267]-[17877.        3.        0.       15.875]-[17924.            1.            2.           15.0201664]-[17984.        3.        3.       15.875]-[18211.        0.        3.       15.875]-[18276.        3.        3.       15.875]-
[18287.        1.        2.       15.875]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.             0.             3.            15.76923943]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.             0.             2.             8.10836411]-[18663.        0.        3.       15.875]-[18718.        0.        0.       15.875]-[18766.        2.        2.       15.875]-
[18824.        2.        2.       15.875]-[18890.             3.             3.            15.79795551]-[18930.             3.             0.            15.60875034]-[18938.        3.        3.       15.875]-[19817.        1.        2.       15.875]-[19839.             0.             2.             7.67841721]-[19930.        3.        3.       15.875]-[19944.             0.             0.            15.03260612]-[20036.        2.        2.       15.875]-[20101.             3.             3.            12.74288464]-
[20474.        1.        2.       15.875]-[20547.             3.             3.            15.17304993]-[20929.        2.        2.       15.875]-[21245.        1.        1.       15.875]-[21257.        3.        3.       15.875]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        2.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        2.       15.875]-[21714.        0.        3.       15.875]-[21943.        3.        3.       15.875]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        2.       15.875]-[21998.        1.        1.       15.875]-[22025.             0.             0.            14.96378899]-[22228.        3.        3.       15.875]-[22446.        1.        1.       15.875]-
[22494.             3.             0.            10.09712982]-[22757.        0.        0.       15.875]-[22811.        3.        3.       15.875]-[22976.             3.             3.             3.33414221]-[22985.        3.        3.       15.875]-[23014.        0.        0.       15.875]-[23112.        1.        2.       15.875]-[23144.        3.        3.       15.875]-[23168.             2.             0.             6.97840881]-[23219.        0.        0.       15.875]-
[23363.        3.        3.       15.875]-[23470.        0.        0.       15.875]-[23486.             2.             3.            10.06673622]-[23497.        0.        0.       15.875]-[23516.             0.             3.             7.59300041]-[23690.        1.        2.       15.875]-[23921.        2.        2.       15.875]-[23936.        1.        2.       15.875]-[24040.             3.             0.             6.48141003]-[24111.        1.        2.       15.875]-
[24182.        0.        3.       15.875]-[24238.             3.             3.             3.38437319]-[24290.        2.        0.       15.875]-[24345.             0.             0.            15.45427418]-[24364.        1.        2.       15.875]-[24427.        3.        3.       15.875]-[24477.        2.        2.       15.875]-[24495.        2.        2.       15.875]-[24893.        2.        2.       15.875]-[25012.             1.             2.            10.25850391]-
[25121.        2.        2.       15.875]-[25165.             3.             0.            13.65584946]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.        2.        2.       15.875]-[25644.        1.        2.       15.875]-[25718.        1.        0.       15.875]-[25774.        2.        2.       15.875]-[26032.             3.             0.            15.75208759]-
[26051.        3.        3.       15.875]-[26120.             0.             0.            15.69237709]-[26321.        1.        1.       15.875]-[26732.        1.        2.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.        0.        3.       15.875]-[26838.        2.        2.       15.875]-[26860.        1.        2.       15.875]-[26948.        0.        0.       15.875]-
[27049.        3.        3.       15.875]-[27098.             1.             0.             9.41778374]-[27526.        0.        0.       15.875]-[27639.        3.        3.       15.875]-[27698.        3.        3.       15.875]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.        0.        0.       15.875]-[28503.        2.        2.       15.875]-[28577.        1.        2.       15.875]-
[28959.        0.        0.       15.875]-[29198.        3.        2.       15.875]-[29777.        0.        0.       15.875]-[29877.        2.        2.       15.875]-[30035.        1.        2.       15.875]-[30098.             0.             3.             9.05008411]-[30326.        1.        1.       15.875]-[30572.        2.        2.       15.875]-[30716.             0.             2.            13.12348175]-[30806.        2.        2.       15.875]-
[30906.        1.        1.       15.875]-[31007.        0.        0.       15.875]-[31181.        3.        3.       15.875]-[31238.        0.        0.       15.875]-[31347.        0.        0.       15.875]-[31422.             2.             0.            15.24166965]-[31429.             3.             2.            14.99640274]-[31431.             0.             3.             7.06310511]-[31432.        1.        1.       15.875]-[31477.            0.            0.           15.8723793]-
[31524.             1.             0.            11.73512077]-[31597.        1.        2.       15.875]-[31619.        1.        2.       15.875]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        3.       15.875]-[32074.        1.        2.       15.875]-[32078.        3.        3.       15.875]-[32111.        1.        2.       15.875]-[32127.        1.        2.       15.875]-
[32140.        3.        3.       15.875]-[32263.        2.        0.       15.875]-[32365.        0.        0.       15.875]-[32411.        2.        3.       15.875]-[32429.        3.        0.       15.875]-[32473.             3.             3.            14.08866405]-[32574.        3.        3.       15.875]-[32584.             0.             0.             1.64501345]-[32622.             0.             3.             3.14723587]-[32858.             3.             3.            15.67974854]-
[32969.             3.             0.            15.61681843]-[33016.        2.        2.       15.875]-[33031.             1.             2.            15.07094669]-[33035.        2.        2.       15.875]-[33133.        2.        2.       15.875]-[33173.        2.        2.       15.875]-[33175.        3.        1.       15.875]-[33306.        3.        2.       15.875]-[33309.        2.        3.       15.875]-[33474.             0.             0.            15.64604187]-
[33478.             2.             3.            14.56438065]-[33618.        1.        3.       15.875]-[33712.             0.             0.             9.06910133]-[33782.        2.        2.       15.875]-[33914.        3.        3.       15.875]-[34076.        3.        3.       15.875]-[34112.        2.        2.       15.875]-[34138.        2.        2.       15.875]-[34239.             1.             2.            15.72582436]-[34364.        2.        2.       15.875]-
[34617.        1.        2.       15.875]-[34751.        3.        3.       15.875]-[34783.        2.        2.       15.875]-[35015.        3.        3.       15.875]-[35018.        1.        1.       15.875]-[35288.        2.        2.       15.875]-
---------------------------
I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 1.469 | Acc: 98.125% | Wgt Acc: 98.054%
	I - Batch: 100 | Loss: 1.450 | Acc: 98.375% | Wgt Acc: 98.298%
	I - Batch: 150 | Loss: 1.433 | Acc: 98.708% | Wgt Acc: 98.648%
I - num batch: 160
I - Train -- Loss: 1.438 | Acc: 98.626% | Wgt Acc: 98.549% | LR: 1.250000e-04 | Dur: 157.15s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 557.   8.   0.]
 [  3.  19. 726.   2.]
 [  0.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.229 | Acc: 58.716% | Wgt Acc: 57.473% | Dur: 16.11s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  7. 20.]
 [ 0. 23. 14.  1.]
 [ 3. 43. 45. 10.]
 [16.  6.  9. 55.]]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 1.454 | Acc: 99.000% | Wgt Acc: 98.964%
	I - Batch: 100 | Loss: 1.443 | Acc: 98.625% | Wgt Acc: 98.537%
	I - Batch: 150 | Loss: 1.435 | Acc: 98.667% | Wgt Acc: 98.583%
I - num batch: 160
I - Train -- Loss: 1.435 | Acc: 98.744% | Wgt Acc: 98.664% | LR: 1.250000e-04 | Dur: 158.27s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   2.]
 [  0. 559.   6.   0.]
 [  2.  17. 728.   2.]
 [  1.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.222 | Acc: 60.550% | Wgt Acc: 59.171% | Dur: 15.78s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  8.  6. 16.]
 [ 0. 18.  7.  0.]
 [ 3. 44. 50.  9.]
 [16.  8. 12. 61.]]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 1.453 | Acc: 98.125% | Wgt Acc: 97.980%
	I - Batch: 100 | Loss: 1.421 | Acc: 98.938% | Wgt Acc: 98.859%
	I - Batch: 150 | Loss: 1.431 | Acc: 98.667% | Wgt Acc: 98.563%
I - num batch: 160
I - Train -- Loss: 1.431 | Acc: 98.626% | Wgt Acc: 98.531% | LR: 1.250000e-04 | Dur: 153.82s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  1. 555.   6.   0.]
 [  0.  21. 728.   2.]
 [  2.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.269 | Acc: 59.633% | Wgt Acc: 57.880% | Dur: 15.99s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  7. 17.]
 [ 0. 19.  7.  0.]
 [11. 49. 57. 16.]
 [11.  4.  4. 53.]]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 1.412 | Acc: 99.375% | Wgt Acc: 99.349%
	I - Batch: 100 | Loss: 1.434 | Acc: 99.000% | Wgt Acc: 98.930%
	I - Batch: 150 | Loss: 1.440 | Acc: 98.875% | Wgt Acc: 98.826%
I - num batch: 160
I - Train -- Loss: 1.436 | Acc: 98.901% | Wgt Acc: 98.850% | LR: 1.250000e-04 | Dur: 155.73s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   3.]
 [  0. 565.   5.   0.]
 [  2.  11. 729.   2.]
 [  3.   2.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.237 | Acc: 59.939% | Wgt Acc: 58.424% | Dur: 15.97s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  9. 17.]
 [ 0. 21.  6.  2.]
 [ 4. 46. 51. 12.]
 [15.  5.  9. 55.]]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 1.456 | Acc: 98.125% | Wgt Acc: 97.946%
	I - Batch: 100 | Loss: 1.444 | Acc: 98.188% | Wgt Acc: 98.070%
	I - Batch: 150 | Loss: 1.437 | Acc: 98.250% | Wgt Acc: 98.131%
I - num batch: 160
I - Train -- Loss: 1.437 | Acc: 98.272% | Wgt Acc: 98.151% | LR: 1.250000e-04 | Dur: 156.29s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   2.]
 [  0. 549.   8.   0.]
 [  1.  27. 726.   2.]
 [  2.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.239 | Acc: 61.468% | Wgt Acc: 59.511% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  8.  8. 22.]
 [ 0. 21.  4.  0.]
 [ 3. 45. 55. 13.]
 [11.  4.  8. 51.]]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 1.449 | Acc: 98.625% | Wgt Acc: 98.540%
	I - Batch: 100 | Loss: 1.449 | Acc: 98.625% | Wgt Acc: 98.578%
	I - Batch: 150 | Loss: 1.443 | Acc: 98.833% | Wgt Acc: 98.789%
I - num batch: 160
I - Train -- Loss: 1.436 | Acc: 98.430% | Wgt Acc: 98.337% | LR: 1.250000e-04 | Dur: 155.58s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.]
 [  0. 554.   7.   0.]
 [  1.  22. 727.   2.]
 [  4.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.243 | Acc: 59.939% | Wgt Acc: 58.492% | Dur: 16.22s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  8. 22.]
 [ 0. 23.  7.  2.]
 [ 4. 42. 49.  8.]
 [14.  6. 11. 54.]]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 1.443 | Acc: 98.125% | Wgt Acc: 98.007%
	I - Batch: 100 | Loss: 1.434 | Acc: 98.750% | Wgt Acc: 98.666%
	I - Batch: 150 | Loss: 1.424 | Acc: 98.792% | Wgt Acc: 98.723%
I - num batch: 160
I - Train -- Loss: 1.444 | Acc: 98.704% | Wgt Acc: 98.620% | LR: 1.250000e-04 | Dur: 157.90s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 556.   6.   0.]
 [  1.  20. 728.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.231 | Acc: 60.245% | Wgt Acc: 58.560% | Dur: 15.93s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  9.  3. 20.]
 [ 0. 18.  5.  1.]
 [ 3. 47. 55.  9.]
 [17.  4. 12. 56.]]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 1.443 | Acc: 99.250% | Wgt Acc: 99.239%
	I - Batch: 100 | Loss: 1.449 | Acc: 98.500% | Wgt Acc: 98.435%
	I - Batch: 150 | Loss: 1.444 | Acc: 98.292% | Wgt Acc: 98.179%
I - num batch: 160
I - Train -- Loss: 1.447 | Acc: 98.272% | Wgt Acc: 98.160% | LR: 1.250000e-04 | Dur: 155.11s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   3.]
 [  0. 551.   6.   0.]
 [  2.  25. 728.   2.]
 [  4.   2.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.243 | Acc: 59.633% | Wgt Acc: 58.560% | Dur: 15.80s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  9.  7. 19.]
 [ 0. 26. 11.  0.]
 [ 3. 36. 47. 11.]
 [19.  7. 10. 56.]]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 1.443 | Acc: 97.875% | Wgt Acc: 97.746%
	I - Batch: 100 | Loss: 1.434 | Acc: 98.562% | Wgt Acc: 98.476%
	I - Batch: 150 | Loss: 1.437 | Acc: 98.792% | Wgt Acc: 98.723%
I - num batch: 160
I - Train -- Loss: 1.435 | Acc: 98.861% | Wgt Acc: 98.797% | LR: 1.250000e-04 | Dur: 154.45s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 561.   6.   0.]
 [  1.  15. 728.   2.]
 [  2.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.242 | Acc: 59.939% | Wgt Acc: 58.560% | Dur: 16.16s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  9. 11. 28.]
 [ 0. 30. 10.  0.]
 [ 3. 32. 46. 10.]
 [13.  7.  8. 48.]]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 1.423 | Acc: 99.125% | Wgt Acc: 99.070%
	I - Batch: 100 | Loss: 1.408 | Acc: 99.375% | Wgt Acc: 99.351%
	I - Batch: 150 | Loss: 1.429 | Acc: 99.042% | Wgt Acc: 99.005%
I - num batch: 160
I - Train -- Loss: 1.431 | Acc: 99.018% | Wgt Acc: 98.983% | LR: 1.250000e-04 | Dur: 158.59s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.]
 [  0. 566.   4.   0.]
 [  3.  10. 729.   1.]
 [  2.   2.   1. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.250 | Acc: 55.963% | Wgt Acc: 53.804% | Dur: 16.30s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  7.  6. 18.]
 [ 0. 14.  5.  0.]
 [ 8. 51. 58. 22.]
 [15.  6.  6. 46.]]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 1.407 | Acc: 99.000% | Wgt Acc: 98.898%
	I - Batch: 100 | Loss: 1.430 | Acc: 99.000% | Wgt Acc: 98.958%
	I - Batch: 150 | Loss: 1.423 | Acc: 99.042% | Wgt Acc: 98.986%
I - num batch: 160
I - Train -- Loss: 1.429 | Acc: 99.058% | Wgt Acc: 99.000% | LR: 1.250000e-04 | Dur: 160.24s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 564.   3.   0.]
 [  1.  12. 731.   1.]
 [  3.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.255 | Acc: 59.633% | Wgt Acc: 57.609% | Dur: 15.93s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7.  8. 28.]
 [ 0. 21.  4.  0.]
 [ 3. 41. 52. 11.]
 [10.  9. 11. 47.]]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 1.472 | Acc: 98.500% | Wgt Acc: 98.430%
	I - Batch: 100 | Loss: 1.444 | Acc: 98.250% | Wgt Acc: 98.140%
	I - Batch: 150 | Loss: 1.433 | Acc: 98.542% | Wgt Acc: 98.441%
I - num batch: 160
I - Train -- Loss: 1.438 | Acc: 98.547% | Wgt Acc: 98.443% | LR: 1.250000e-04 | Dur: 156.47s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 552.   6.   0.]
 [  1.  24. 728.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.231 | Acc: 58.410% | Wgt Acc: 56.726% | Dur: 16.38s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  6.  8. 24.]
 [ 0. 19.  5.  0.]
 [ 3. 47. 52. 10.]
 [17.  6. 10. 52.]]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 1.408 | Acc: 99.500% | Wgt Acc: 99.494%
	I - Batch: 100 | Loss: 1.442 | Acc: 99.062% | Wgt Acc: 99.003%
	I - Batch: 150 | Loss: 1.428 | Acc: 99.000% | Wgt Acc: 98.949%
I - num batch: 160
I - Train -- Loss: 1.431 | Acc: 99.018% | Wgt Acc: 98.965% | LR: 1.250000e-04 | Dur: 154.32s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   3.]
 [  0. 566.   4.   0.]
 [  1.  10. 730.   2.]
 [  3.   2.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.199 | Acc: 58.716% | Wgt Acc: 57.405% | Dur: 15.85s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  6.  5. 11.]
 [ 0. 22.  8.  0.]
 [11. 46. 55. 20.]
 [17.  4.  7. 55.]]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 1.411 | Acc: 99.000% | Wgt Acc: 98.931%
	I - Batch: 100 | Loss: 1.438 | Acc: 99.000% | Wgt Acc: 98.916%
	I - Batch: 150 | Loss: 1.433 | Acc: 98.917% | Wgt Acc: 98.845%
I - num batch: 160
I - Train -- Loss: 1.431 | Acc: 98.979% | Wgt Acc: 98.912% | LR: 1.250000e-04 | Dur: 155.82s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 561.   4.   0.]
 [  1.  15. 730.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.282 | Acc: 55.963% | Wgt Acc: 53.668% | Dur: 16.72s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7. 14. 36.]
 [ 1. 20.  6.  0.]
 [ 4. 48. 50. 12.]
 [ 8.  3.  5. 38.]]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 1.421 | Acc: 99.250% | Wgt Acc: 99.237%
	I - Batch: 100 | Loss: 1.425 | Acc: 99.250% | Wgt Acc: 99.196%
	I - Batch: 150 | Loss: 1.430 | Acc: 99.167% | Wgt Acc: 99.118%
I - num batch: 160
I - Train -- Loss: 1.421 | Acc: 99.136% | Wgt Acc: 99.098% | LR: 1.250000e-04 | Dur: 156.79s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 567.   4.   0.]
 [  2.   9. 730.   1.]
 [  2.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.235 | Acc: 63.609% | Wgt Acc: 62.092% | Dur: 15.82s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  5.  6. 20.]
 [ 0. 30.  7.  1.]
 [ 5. 38. 55. 13.]
 [12.  5.  7. 52.]]

I - Local maximum validation set accuracy:  63.61

I - Validation set results: 
[14.     1.     2.    15.875]-
[50.     3.     1.    15.875]-[124.      2.      0.     15.875]-[127.      0.      0.     15.875]-[443.      2.      2.     15.875]-[567.      0.      0.     15.875]-[573.          1.          2.         15.7981081]-[615.      0.      3.     15.875]-[695.           1.           0.          12.65705872]-[722.      3.      3.     15.875]-[826.      0.      0.     15.875]-
[878.      0.      0.     15.875]-[1103.       0.       0.      15.875]-[1212.       3.       3.      15.875]-[1368.       0.       0.      15.875]-[2181.       2.       3.      15.875]-[2476.       2.       2.      15.875]-[2721.       2.       2.      15.875]-[2818.       1.       1.      15.875]-[2886.       2.       2.      15.875]-[3231.       2.       2.      15.875]-
[3333.       2.       2.      15.875]-[3482.       2.       2.      15.875]-[3536.       3.       3.      15.875]-[3625.       1.       1.      15.875]-[3909.       0.       0.      15.875]-[4035.       0.       3.      15.875]-[4140.       0.       0.      15.875]-[4214.            1.            3.            3.36540937]-[4346.            1.            0.           11.10191631]-[4581.       2.       2.      15.875]-
[4708.            3.            3.           15.46240616]-[4838.            3.            3.            2.28540182]-[4845.       1.       2.      15.875]-[4868.            0.            0.           14.96968269]-[4939.            0.            2.            8.75041771]-[4984.       2.       2.      15.875]-[5078.           1.           0.           3.6874795]-[5396.       0.       0.      15.875]-[5479.       1.       1.      15.875]-[5717.       0.       0.      15.875]-
[5843.       1.       1.      15.875]-[5949.       3.       3.      15.875]-[5987.       2.       2.      15.875]-[6014.            3.            2.            9.73005772]-[6033.       3.       2.      15.875]-[6313.       0.       3.      15.875]-[6421.       3.       2.      15.875]-[6500.            1.            2.           15.69100475]-[6583.       3.       3.      15.875]-[6683.            3.            3.           10.71099091]-
[6825.            2.            0.           -1.23225617]-[6998.       3.       3.      15.875]-[7049.           3.           2.           2.9622035]-[7517.       1.       1.      15.875]-[7521.            1.            2.            7.71005058]-[7528.       1.       3.      15.875]-[7949.       1.       2.      15.875]-[8135.            1.            3.            8.35935974]-[8185.       3.       0.      15.875]-[8269.            3.            3.            1.28753185]-
[8273.            3.            3.           15.76296806]-[8543.       3.       0.      15.875]-[8666.       1.       1.      15.875]-[8672.       0.       0.      15.875]-[8903.       1.       2.      15.875]-[9001.       2.       1.      15.875]-[9036.       2.       2.      15.875]-[9281.       3.       2.      15.875]-[9300.       2.       2.      15.875]-[9571.            0.            3.            8.37968445]-
[9617.       1.       1.      15.875]-[9644.       2.       2.      15.875]-[9705.       2.       0.      15.875]-[9801.       0.       0.      15.875]-[9803.            3.            3.           13.00076103]-[9865.       3.       3.      15.875]-[9896.       2.       2.      15.875]-[10314.        1.        2.       15.875]-[10337.        3.        3.       15.875]-[10403.             0.             2.            15.74869919]-
[10653.        2.        2.       15.875]-[10704.        2.        1.       15.875]-[10719.        1.        2.       15.875]-[10727.        1.        2.       15.875]-[10836.        0.        0.       15.875]-[10969.             2.             3.            11.40983582]-[11042.             0.             0.            13.87670231]-[11088.        1.        1.       15.875]-[11322.        0.        0.       15.875]-[11398.        2.        2.       15.875]-
[11499.        0.        0.       15.875]-[11502.        3.        0.       15.875]-[11512.             3.             3.             8.04471779]-[11608.        1.        1.       15.875]-[11610.             0.             3.             2.85540342]-[11692.        0.        0.       15.875]-[11905.             0.             0.             8.51624393]-[11993.        1.        2.       15.875]-[12002.            2.            2.           14.2243185]-[12052.        0.        0.       15.875]-
[12201.        0.        0.       15.875]-[12235.        2.        2.       15.875]-[12320.        1.        0.       15.875]-[12377.        2.        1.       15.875]-[12398.        2.        3.       15.875]-[12503.        1.        2.       15.875]-[12617.        0.        2.       15.875]-[12685.            3.            2.            3.5566535]-[12738.             2.             3.            11.95718575]-[12742.        2.        2.       15.875]-
[12823.        0.        0.       15.875]-[13110.        1.        2.       15.875]-[13240.        3.        3.       15.875]-[13253.        1.        1.       15.875]-[13273.        0.        0.       15.875]-[13634.        1.        1.       15.875]-[13763.        2.        2.       15.875]-[13905.             3.             3.            13.51697731]-[14060.        2.        1.       15.875]-[14065.        3.        3.       15.875]-
[14147.        3.        3.       15.875]-[14595.        2.        2.       15.875]-[14687.        2.        2.       15.875]-[14788.        2.        2.       15.875]-[14869.        1.        1.       15.875]-[14872.        3.        3.       15.875]-[14877.        1.        2.       15.875]-[14927.        0.        3.       15.875]-[15066.        0.        0.       15.875]-[15175.        1.        2.       15.875]-
[15178.             2.             3.            15.03456306]-[15375.             3.             0.            10.92749786]-[15389.        3.        3.       15.875]-[15568.        2.        1.       15.875]-[15675.        3.        3.       15.875]-[15869.        1.        2.       15.875]-[16207.        3.        0.       15.875]-[16236.        0.        0.       15.875]-[16302.        3.        0.       15.875]-[16331.        2.        2.       15.875]-
[16381.        0.        3.       15.875]-[16488.        1.        1.       15.875]-[16495.        0.        0.       15.875]-[16650.        0.        0.       15.875]-[16719.        1.        2.       15.875]-[16801.        0.        0.       15.875]-[16828.        0.        0.       15.875]-[17137.        3.        0.       15.875]-[17245.        1.        2.       15.875]-[17278.             3.             0.             4.70960236]-
[17282.             0.             0.            11.53316498]-[17311.        2.        2.       15.875]-[17336.        2.        2.       15.875]-[17608.        3.        3.       15.875]-[17627.        0.        0.       15.875]-[17877.             3.             2.             5.79939127]-[17924.        1.        2.       15.875]-[17984.        3.        3.       15.875]-[18211.             0.             3.             9.74697113]-[18276.        3.        3.       15.875]-
[18287.        1.        1.       15.875]-[18394.        0.        0.       15.875]-[18428.        0.        0.       15.875]-[18442.        0.        3.       15.875]-[18478.        3.        0.       15.875]-[18607.        0.        0.       15.875]-[18616.             0.             0.            15.12866974]-[18663.        0.        0.       15.875]-[18718.        0.        0.       15.875]-[18766.        2.        2.       15.875]-
[18824.        2.        2.       15.875]-[18890.             3.             3.            10.02202797]-[18930.        3.        2.       15.875]-[18938.        3.        3.       15.875]-[19817.        1.        2.       15.875]-[19839.        0.        0.       15.875]-[19930.        3.        3.       15.875]-[19944.             0.             0.            12.88613033]-[20036.        2.        2.       15.875]-[20101.        3.        3.       15.875]-
[20474.        1.        2.       15.875]-[20547.             3.             0.             7.88344765]-[20929.        2.        2.       15.875]-[21245.        1.        1.       15.875]-[21257.        3.        3.       15.875]-[21293.        1.        1.       15.875]-[21316.        1.        1.       15.875]-[21384.        1.        1.       15.875]-[21448.        1.        2.       15.875]-[21483.        0.        0.       15.875]-
[21487.        2.        2.       15.875]-[21714.        0.        3.       15.875]-[21943.             3.             3.             8.55919838]-[21947.        0.        0.       15.875]-[21948.        0.        0.       15.875]-[21965.        2.        2.       15.875]-[21998.        1.        1.       15.875]-[22025.             0.             0.             8.85009861]-[22228.        3.        3.       15.875]-[22446.        1.        1.       15.875]-
[22494.        3.        0.       15.875]-[22757.        0.        0.       15.875]-[22811.        3.        0.       15.875]-[22976.        3.        2.       15.875]-[22985.            3.            0.           15.7266655]-[23014.        0.        0.       15.875]-[23112.        1.        2.       15.875]-[23144.        3.        3.       15.875]-[23168.            2.            2.           11.7578907]-[23219.             0.             0.            15.79771996]-
[23363.        3.        3.       15.875]-[23470.        0.        0.       15.875]-[23486.            2.            2.           15.8633709]-[23497.        0.        0.       15.875]-[23516.        0.        0.       15.875]-[23690.        1.        2.       15.875]-[23921.        2.        2.       15.875]-[23936.        1.        2.       15.875]-[24040.            3.            2.           13.0153656]-[24111.        1.        2.       15.875]-
[24182.        0.        0.       15.875]-[24238.             3.             3.            15.81826782]-[24290.        2.        0.       15.875]-[24345.             0.             0.            15.03734589]-[24364.        1.        2.       15.875]-[24427.        3.        3.       15.875]-[24477.        2.        2.       15.875]-[24495.        2.        1.       15.875]-[24893.        2.        2.       15.875]-[25012.        1.        1.       15.875]-
[25121.        2.        2.       15.875]-[25165.             3.             0.            10.67754555]-[25183.        0.        0.       15.875]-[25297.        3.        3.       15.875]-[25398.        0.        0.       15.875]-[25574.        2.        2.       15.875]-[25644.        1.        1.       15.875]-[25718.             1.             2.            14.89020538]-[25774.        2.        2.       15.875]-[26032.        3.        3.       15.875]-
[26051.        3.        3.       15.875]-[26120.        0.        0.       15.875]-[26321.        1.        1.       15.875]-[26732.        1.        1.       15.875]-[26784.        3.        3.       15.875]-[26827.        3.        3.       15.875]-[26833.        0.        3.       15.875]-[26838.        2.        2.       15.875]-[26860.        1.        2.       15.875]-[26948.        0.        0.       15.875]-
[27049.        3.        0.       15.875]-[27098.        1.        0.       15.875]-[27526.        0.        0.       15.875]-[27639.             3.             3.             6.63198042]-[27698.        3.        3.       15.875]-[27772.        0.        0.       15.875]-[27890.        1.        1.       15.875]-[28040.             0.             0.            15.78222084]-[28503.        2.        2.       15.875]-[28577.        1.        2.       15.875]-
[28959.        0.        0.       15.875]-[29198.        3.        2.       15.875]-[29777.        0.        0.       15.875]-[29877.        2.        2.       15.875]-[30035.        1.        1.       15.875]-[30098.        0.        0.       15.875]-[30326.        1.        1.       15.875]-[30572.        2.        2.       15.875]-[30716.             0.             2.            15.38430595]-[30806.        2.        2.       15.875]-
[30906.        1.        1.       15.875]-[31007.        0.        0.       15.875]-[31181.        3.        3.       15.875]-[31238.        0.        0.       15.875]-[31347.        0.        0.       15.875]-[31422.        2.        2.       15.875]-[31429.             3.             0.            14.04870701]-[31431.             0.             0.             8.30671597]-[31432.        1.        1.       15.875]-[31477.        0.        0.       15.875]-
[31524.        1.        2.       15.875]-[31597.        1.        2.       15.875]-[31619.        1.        2.       15.875]-[31701.        0.        0.       15.875]-[31755.        0.        0.       15.875]-[31854.        3.        3.       15.875]-[32074.        1.        2.       15.875]-[32078.        3.        3.       15.875]-[32111.        1.        2.       15.875]-[32127.        1.        2.       15.875]-
[32140.        3.        3.       15.875]-[32263.             2.             0.            13.93524456]-[32365.        0.        0.       15.875]-[32411.        2.        0.       15.875]-[32429.        3.        0.       15.875]-[32473.             3.             0.            15.59115124]-[32574.        3.        3.       15.875]-[32584.        0.        2.       15.875]-[32622.             0.             3.             4.63917637]-[32858.             3.             0.             9.89662552]-
[32969.        3.        0.       15.875]-[33016.        2.        1.       15.875]-[33031.        1.        3.       15.875]-[33035.        2.        2.       15.875]-[33133.        2.        2.       15.875]-[33173.        2.        2.       15.875]-[33175.        3.        2.       15.875]-[33306.        3.        2.       15.875]-[33309.        2.        3.       15.875]-[33474.        0.        0.       15.875]-
[33478.             2.             3.            12.70238876]-[33618.           1.           3.           2.682477]-[33712.        0.        0.       15.875]-[33782.        2.        2.       15.875]-[33914.             3.             3.             2.07243156]-[34076.        3.        3.       15.875]-[34112.        2.        2.       15.875]-[34138.        2.        2.       15.875]-[34239.             1.             2.            15.51986122]-[34364.        2.        2.       15.875]-
[34617.        1.        2.       15.875]-[34751.        3.        3.       15.875]-[34783.        2.        2.       15.875]-[35015.             3.             3.            10.12185478]-[35018.        1.        1.       15.875]-[35288.        2.        2.       15.875]-
---------------------------
I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 1.413 | Acc: 99.500% | Wgt Acc: 99.438%
	I - Batch: 100 | Loss: 1.425 | Acc: 98.625% | Wgt Acc: 98.478%
	I - Batch: 150 | Loss: 1.431 | Acc: 98.458% | Wgt Acc: 98.321%
I - num batch: 160
I - Train -- Loss: 1.429 | Acc: 98.508% | Wgt Acc: 98.372% | LR: 1.250000e-04 | Dur: 155.39s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 549.   3.   0.]
 [  1.  27. 731.   2.]
 [  2.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.190 | Acc: 55.963% | Wgt Acc: 54.416% | Dur: 16.00s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  7.  7. 26.]
 [ 0. 16.  8.  0.]
 [ 2. 45. 43.  7.]
 [15. 10. 17. 53.]]

I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 1.421 | Acc: 99.500% | Wgt Acc: 99.466%
	I - Batch: 100 | Loss: 1.440 | Acc: 98.812% | Wgt Acc: 98.707%
	I - Batch: 150 | Loss: 1.430 | Acc: 99.000% | Wgt Acc: 98.940%
I - num batch: 160
I - Train -- Loss: 1.424 | Acc: 99.018% | Wgt Acc: 98.965% | LR: 1.250000e-04 | Dur: 155.63s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   1.]
 [  0. 563.   3.   0.]
 [  1.  13. 730.   1.]
 [  3.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.245 | Acc: 60.245% | Wgt Acc: 58.560% | Dur: 16.18s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7.  9. 30.]
 [ 0. 29. 10.  0.]
 [ 5. 37. 48. 11.]
 [ 8.  5.  8. 45.]]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 1.453 | Acc: 98.000% | Wgt Acc: 97.838%
	I - Batch: 100 | Loss: 1.432 | Acc: 98.625% | Wgt Acc: 98.512%
	I - Batch: 150 | Loss: 1.429 | Acc: 98.750% | Wgt Acc: 98.647%
I - num batch: 160
I - Train -- Loss: 1.425 | Acc: 98.822% | Wgt Acc: 98.726% | LR: 1.250000e-04 | Dur: 154.14s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   2.]
 [  0. 557.   3.   0.]
 [  1.  19. 731.   1.]
 [  2.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.214 | Acc: 59.021% | Wgt Acc: 57.677% | Dur: 15.76s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  7.  8. 18.]
 [ 0. 19.  7.  0.]
 [ 4. 47. 48. 10.]
 [16.  5. 12. 58.]]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 1.405 | Acc: 98.625% | Wgt Acc: 98.478%
	I - Batch: 100 | Loss: 1.430 | Acc: 98.625% | Wgt Acc: 98.522%
	I - Batch: 150 | Loss: 1.423 | Acc: 99.000% | Wgt Acc: 98.921%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 99.018% | Wgt Acc: 98.947% | LR: 1.250000e-04 | Dur: 156.08s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 561.   3.   0.]
 [  1.  15. 731.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.230 | Acc: 58.104% | Wgt Acc: 56.250% | Dur: 15.85s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  5. 18.]
 [ 0. 17.  3.  0.]
 [ 6. 50. 57. 17.]
 [17.  5. 10. 51.]]

I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 1.451 | Acc: 98.250% | Wgt Acc: 98.121%
	I - Batch: 100 | Loss: 1.424 | Acc: 98.625% | Wgt Acc: 98.523%
	I - Batch: 150 | Loss: 1.424 | Acc: 98.833% | Wgt Acc: 98.759%
I - num batch: 160
I - Train -- Loss: 1.436 | Acc: 98.783% | Wgt Acc: 98.700% | LR: 1.250000e-04 | Dur: 155.17s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.]
 [  0. 558.   4.   0.]
 [  3.  18. 730.   2.]
 [  1.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.254 | Acc: 56.881% | Wgt Acc: 55.027% | Dur: 16.90s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  5. 10. 24.]
 [ 0. 20.  6.  0.]
 [ 5. 49. 52. 16.]
 [15.  4.  7. 46.]]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 1.459 | Acc: 97.000% | Wgt Acc: 96.786%
	I - Batch: 100 | Loss: 1.444 | Acc: 97.375% | Wgt Acc: 97.169%
	I - Batch: 150 | Loss: 1.443 | Acc: 97.333% | Wgt Acc: 97.099%
I - num batch: 160
I - Train -- Loss: 1.450 | Acc: 97.330% | Wgt Acc: 97.098% | LR: 1.250000e-04 | Dur: 157.07s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 527.   7.   0.]
 [  2.  49. 726.   3.]
 [  2.   2.   1. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.212 | Acc: 59.327% | Wgt Acc: 57.813% | Dur: 15.99s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  8. 12. 22.]
 [ 0. 21.  4.  0.]
 [ 3. 44. 51. 10.]
 [17.  5.  8. 54.]]

I - Epoch: 126
I - Training: 
	I - Batch: 50 | Loss: 1.456 | Acc: 99.125% | Wgt Acc: 99.131%
	I - Batch: 100 | Loss: 1.442 | Acc: 99.062% | Wgt Acc: 99.000%
	I - Batch: 150 | Loss: 1.434 | Acc: 98.958% | Wgt Acc: 98.892%
I - num batch: 160
I - Train -- Loss: 1.431 | Acc: 98.940% | Wgt Acc: 98.877% | LR: 1.250000e-04 | Dur: 155.63s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   2.]
 [  0. 562.   5.   0.]
 [  1.  14. 729.   1.]
 [  2.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.242 | Acc: 58.104% | Wgt Acc: 56.861% | Dur: 15.57s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  3.  5. 16.]
 [ 0. 30. 13.  2.]
 [10. 39. 52. 21.]
 [17.  6.  5. 47.]]

I - Epoch: 127
I - Training: 
	I - Batch: 50 | Loss: 1.418 | Acc: 98.750% | Wgt Acc: 98.700%
	I - Batch: 100 | Loss: 1.439 | Acc: 98.688% | Wgt Acc: 98.633%
	I - Batch: 150 | Loss: 1.435 | Acc: 98.917% | Wgt Acc: 98.865%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 98.940% | Wgt Acc: 98.894% | LR: 1.250000e-04 | Dur: 158.08s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.]
 [  0. 564.   5.   0.]
 [  2.  12. 729.   1.]
 [  3.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.271 | Acc: 58.104% | Wgt Acc: 56.658% | Dur: 16.65s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  2.  5. 18.]
 [ 0. 32.  9.  1.]
 [13. 40. 55. 25.]
 [14.  4.  6. 42.]]

I - Epoch: 128
I - Training: 
	I - Batch: 50 | Loss: 1.387 | Acc: 99.500% | Wgt Acc: 99.462%
	I - Batch: 100 | Loss: 1.404 | Acc: 99.188% | Wgt Acc: 99.182%
	I - Batch: 150 | Loss: 1.421 | Acc: 98.833% | Wgt Acc: 98.750%
I - num batch: 160
I - Train -- Loss: 1.430 | Acc: 98.901% | Wgt Acc: 98.823% | LR: 1.250000e-04 | Dur: 156.23s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.]
 [  0. 560.   3.   0.]
 [  1.  16. 731.   2.]
 [  3.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.200 | Acc: 58.410% | Wgt Acc: 56.793% | Dur: 15.86s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  8. 20.]
 [ 0. 18.  5.  0.]
 [ 4. 48. 54. 12.]
 [19.  8.  8. 54.]]

I - Epoch: 129
I - Training: 
	I - Batch: 50 | Loss: 1.417 | Acc: 99.250% | Wgt Acc: 99.207%
	I - Batch: 100 | Loss: 1.413 | Acc: 99.438% | Wgt Acc: 99.421%
	I - Batch: 150 | Loss: 1.419 | Acc: 99.417% | Wgt Acc: 99.409%
I - num batch: 160
I - Train -- Loss: 1.420 | Acc: 99.450% | Wgt Acc: 99.443% | LR: 1.250000e-04 | Dur: 155.82s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   0.   1.]
 [  0. 573.   4.   0.]
 [  1.   3. 730.   1.]
 [  2.   1.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.241 | Acc: 57.187% | Wgt Acc: 55.842% | Dur: 15.85s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  3.  7. 13.]
 [ 1. 20.  4.  0.]
 [ 9. 48. 56. 19.]
 [21.  7.  8. 54.]]

I - Epoch: 130
I - Training: 
	I - Batch: 50 | Loss: 1.376 | Acc: 98.500% | Wgt Acc: 98.359%
	I - Batch: 100 | Loss: 1.401 | Acc: 98.938% | Wgt Acc: 98.841%
	I - Batch: 150 | Loss: 1.416 | Acc: 99.083% | Wgt Acc: 99.023%
I - num batch: 160
I - Train -- Loss: 1.420 | Acc: 99.136% | Wgt Acc: 99.080% | LR: 1.250000e-04 | Dur: 154.08s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   0.   1.]
 [  0. 565.   3.   0.]
 [  1.  11. 731.   2.]
 [  2.   1.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.192 | Acc: 58.104% | Wgt Acc: 57.065% | Dur: 16.07s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  6.  7. 12.]
 [ 0. 17.  6.  0.]
 [ 5. 46. 49. 11.]
 [22.  9. 13. 63.]]

I - Epoch: 131
I - Training: 
	I - Batch: 50 | Loss: 1.420 | Acc: 99.375% | Wgt Acc: 99.353%
	I - Batch: 100 | Loss: 1.420 | Acc: 99.312% | Wgt Acc: 99.297%
	I - Batch: 150 | Loss: 1.418 | Acc: 99.292% | Wgt Acc: 99.268%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 99.254% | Wgt Acc: 99.222% | LR: 1.250000e-04 | Dur: 156.10s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   0.   1.]
 [  0. 568.   4.   0.]
 [  1.   8. 730.   1.]
 [  2.   1.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.253 | Acc: 61.162% | Wgt Acc: 59.579% | Dur: 16.78s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  5.  8. 25.]
 [ 0. 29. 10.  1.]
 [ 5. 38. 51. 12.]
 [11.  6.  6. 48.]]

I - Epoch: 132
I - Training: 
	I - Batch: 50 | Loss: 1.469 | Acc: 97.750% | Wgt Acc: 97.570%
	I - Batch: 100 | Loss: 1.450 | Acc: 98.500% | Wgt Acc: 98.378%
	I - Batch: 150 | Loss: 1.436 | Acc: 98.542% | Wgt Acc: 98.450%
I - num batch: 160
I - Train -- Loss: 1.432 | Acc: 98.626% | Wgt Acc: 98.540% | LR: 1.250000e-04 | Dur: 154.37s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   1.   0.   1.]
 [  0. 557.   4.   0.]
 [  1.  19. 728.   3.]
 [  3.   1.   2. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.198 | Acc: 59.327% | Wgt Acc: 57.948% | Dur: 15.78s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  9.  8. 20.]
 [ 0. 22.  7.  0.]
 [ 3. 42. 53. 11.]
 [21.  5.  7. 55.]]

I - Epoch: 133
I - Training: 
	I - Batch: 50 | Loss: 1.465 | Acc: 99.125% | Wgt Acc: 99.097%
	I - Batch: 100 | Loss: 1.433 | Acc: 99.188% | Wgt Acc: 99.170%
	I - Batch: 150 | Loss: 1.427 | Acc: 99.167% | Wgt Acc: 99.127%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 99.215% | Wgt Acc: 99.177% | LR: 1.250000e-04 | Dur: 153.59s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   2.]
 [  0. 568.   4.   0.]
 [  2.   8. 730.   1.]
 [  1.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.264 | Acc: 57.492% | Wgt Acc: 55.639% | Dur: 17.03s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6. 13. 28.]
 [ 0. 24.  7.  0.]
 [ 5. 41. 52. 15.]
 [14.  7.  3. 43.]]

I - Epoch: 134
I - Training: 
	I - Batch: 50 | Loss: 1.429 | Acc: 99.125% | Wgt Acc: 99.068%
	I - Batch: 100 | Loss: 1.429 | Acc: 98.750% | Wgt Acc: 98.634%
	I - Batch: 150 | Loss: 1.422 | Acc: 98.917% | Wgt Acc: 98.816%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 98.940% | Wgt Acc: 98.850% | LR: 1.250000e-04 | Dur: 156.08s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   2.]
 [  0. 560.   2.   0.]
 [  1.  16. 732.   2.]
 [  2.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.209 | Acc: 58.410% | Wgt Acc: 56.522% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  6.  8. 20.]
 [ 0. 13.  2.  0.]
 [ 6. 52. 60. 11.]
 [19.  7.  5. 55.]]

I - Epoch: 135
I - Training: 
	I - Batch: 50 | Loss: 1.425 | Acc: 99.500% | Wgt Acc: 99.436%
	I - Batch: 100 | Loss: 1.418 | Acc: 99.062% | Wgt Acc: 98.985%
	I - Batch: 150 | Loss: 1.424 | Acc: 99.125% | Wgt Acc: 99.071%
I - num batch: 160
I - Train -- Loss: 1.417 | Acc: 99.176% | Wgt Acc: 99.124% | LR: 1.250000e-04 | Dur: 154.93s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 565.   3.   0.]
 [  1.  11. 731.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.217 | Acc: 60.856% | Wgt Acc: 59.307% | Dur: 16.11s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  8. 21.]
 [ 1. 25.  7.  0.]
 [ 5. 43. 56. 13.]
 [16.  4.  4. 52.]]

I - Epoch: 136
I - Training: 
	I - Batch: 50 | Loss: 1.416 | Acc: 99.375% | Wgt Acc: 99.324%
	I - Batch: 100 | Loss: 1.415 | Acc: 99.312% | Wgt Acc: 99.268%
	I - Batch: 150 | Loss: 1.422 | Acc: 99.167% | Wgt Acc: 99.118%
I - num batch: 160
I - Train -- Loss: 1.421 | Acc: 99.136% | Wgt Acc: 99.098% | LR: 1.250000e-04 | Dur: 156.64s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 567.   4.   0.]
 [  1.   9. 730.   1.]
 [  3.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.264 | Acc: 61.468% | Wgt Acc: 59.443% | Dur: 15.77s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  7.  9. 20.]
 [ 1. 22.  4.  0.]
 [ 6. 45. 58. 17.]
 [ 9.  4.  4. 49.]]

I - Epoch: 137
I - Training: 
	I - Batch: 50 | Loss: 1.440 | Acc: 99.000% | Wgt Acc: 98.957%
	I - Batch: 100 | Loss: 1.437 | Acc: 99.000% | Wgt Acc: 98.972%
	I - Batch: 150 | Loss: 1.425 | Acc: 99.167% | Wgt Acc: 99.136%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 99.176% | Wgt Acc: 99.142% | LR: 1.250000e-04 | Dur: 155.28s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.]
 [  0. 567.   4.   0.]
 [  2.   9. 730.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.248 | Acc: 59.633% | Wgt Acc: 57.677% | Dur: 16.78s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  4. 11. 27.]
 [ 1. 23.  6.  0.]
 [ 4. 46. 53. 13.]
 [10.  5.  5. 46.]]

I - Epoch: 138
I - Training: 
	I - Batch: 50 | Loss: 1.407 | Acc: 99.500% | Wgt Acc: 99.464%
	I - Batch: 100 | Loss: 1.407 | Acc: 99.000% | Wgt Acc: 98.929%
	I - Batch: 150 | Loss: 1.415 | Acc: 99.125% | Wgt Acc: 99.070%
I - num batch: 160
I - Train -- Loss: 1.420 | Acc: 99.136% | Wgt Acc: 99.080% | LR: 1.250000e-04 | Dur: 159.02s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 564.   3.   0.]
 [  1.  12. 731.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.195 | Acc: 58.716% | Wgt Acc: 57.133% | Dur: 16.50s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  5.  6. 20.]
 [ 0. 19.  4.  0.]
 [ 4. 47. 53. 12.]
 [18.  7. 12. 54.]]

I - Epoch: 139
I - Training: 
	I - Batch: 50 | Loss: 1.421 | Acc: 98.625% | Wgt Acc: 98.533%
	I - Batch: 100 | Loss: 1.424 | Acc: 98.438% | Wgt Acc: 98.281%
	I - Batch: 150 | Loss: 1.425 | Acc: 98.667% | Wgt Acc: 98.565%
I - num batch: 160
I - Train -- Loss: 1.433 | Acc: 98.547% | Wgt Acc: 98.434% | LR: 1.250000e-04 | Dur: 156.51s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.]
 [  0. 552.   4.   0.]
 [  2.  24. 730.   2.]
 [  2.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.186 | Acc: 60.856% | Wgt Acc: 59.647% | Dur: 16.03s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  7.  4. 17.]
 [ 0. 21.  6.  0.]
 [ 2. 41. 51.  8.]
 [20.  9. 14. 61.]]

I - Epoch: 140
I - Training: 
	I - Batch: 50 | Loss: 1.413 | Acc: 99.250% | Wgt Acc: 99.243%
	I - Batch: 100 | Loss: 1.407 | Acc: 99.188% | Wgt Acc: 99.170%
	I - Batch: 150 | Loss: 1.422 | Acc: 98.958% | Wgt Acc: 98.920%
I - num batch: 160
I - Train -- Loss: 1.428 | Acc: 98.940% | Wgt Acc: 98.894% | LR: 1.250000e-04 | Dur: 158.58s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   1.   0.   0.]
 [  0. 562.   6.   0.]
 [  2.  14. 728.   1.]
 [  2.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.222 | Acc: 60.550% | Wgt Acc: 58.967% | Dur: 17.05s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  8.  8. 18.]
 [ 0. 18.  3.  0.]
 [ 4. 45. 54. 10.]
 [16.  7. 10. 58.]]

I - Epoch: 141
I - Training: 
	I - Batch: 50 | Loss: 1.412 | Acc: 99.375% | Wgt Acc: 99.350%
	I - Batch: 100 | Loss: 1.418 | Acc: 99.375% | Wgt Acc: 99.367%
	I - Batch: 150 | Loss: 1.420 | Acc: 98.958% | Wgt Acc: 98.883%
I - num batch: 160
I - Train -- Loss: 1.418 | Acc: 99.018% | Wgt Acc: 98.947% | LR: 1.250000e-04 | Dur: 155.45s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 561.   3.   0.]
 [  1.  15. 731.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.240 | Acc: 60.856% | Wgt Acc: 59.103% | Dur: 16.21s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  3. 19.]
 [ 0. 24.  6.  3.]
 [ 8. 44. 56. 14.]
 [11.  5. 10. 50.]]

I - Epoch: 142
I - Training: 
	I - Batch: 50 | Loss: 1.454 | Acc: 98.875% | Wgt Acc: 98.823%
	I - Batch: 100 | Loss: 1.430 | Acc: 99.125% | Wgt Acc: 99.073%
	I - Batch: 150 | Loss: 1.420 | Acc: 99.250% | Wgt Acc: 99.220%
I - num batch: 160
I - Train -- Loss: 1.420 | Acc: 99.215% | Wgt Acc: 99.177% | LR: 1.250000e-04 | Dur: 155.34s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 567.   4.   0.]
 [  1.   9. 730.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.254 | Acc: 57.798% | Wgt Acc: 56.046% | Dur: 15.97s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  6. 12. 23.]
 [ 0. 18.  2.  0.]
 [ 6. 50. 53. 12.]
 [15.  4.  8. 51.]]

I - Epoch: 143
I - Training: 
	I - Batch: 50 | Loss: 1.403 | Acc: 99.250% | Wgt Acc: 99.266%
	I - Batch: 100 | Loss: 1.408 | Acc: 99.500% | Wgt Acc: 99.494%
	I - Batch: 150 | Loss: 1.415 | Acc: 99.417% | Wgt Acc: 99.399%
I - num batch: 160
I - Train -- Loss: 1.421 | Acc: 99.333% | Wgt Acc: 99.310% | LR: 1.250000e-04 | Dur: 153.77s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.]
 [  0. 569.   4.   0.]
 [  1.   7. 730.   1.]
 [  2.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.274 | Acc: 59.939% | Wgt Acc: 57.745% | Dur: 15.48s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  7.  7. 28.]
 [ 0. 25.  3.  0.]
 [ 6. 42. 57. 17.]
 [ 9.  4.  8. 41.]]

I - Epoch: 144
I - Training: 
	I - Batch: 50 | Loss: 1.446 | Acc: 98.875% | Wgt Acc: 98.810%
	I - Batch: 100 | Loss: 1.434 | Acc: 99.250% | Wgt Acc: 99.224%
	I - Batch: 150 | Loss: 1.425 | Acc: 98.958% | Wgt Acc: 98.901%
I - num batch: 160
I - Train -- Loss: 1.428 | Acc: 98.979% | Wgt Acc: 98.921% | LR: 1.250000e-04 | Dur: 156.36s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.]
 [  0. 562.   4.   0.]
 [  1.  14. 730.   1.]
 [  3.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.254 | Acc: 57.187% | Wgt Acc: 55.231% | Dur: 16.95s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  6. 10. 21.]
 [ 0. 12.  4.  0.]
 [ 3. 55. 52. 12.]
 [15.  5.  9. 53.]]

I - Epoch: 145
I - Training: 
	I - Batch: 50 | Loss: 1.406 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 100 | Loss: 1.409 | Acc: 99.438% | Wgt Acc: 99.409%
	I - Batch: 150 | Loss: 1.413 | Acc: 99.292% | Wgt Acc: 99.268%
I - num batch: 160
I - Train -- Loss: 1.419 | Acc: 99.333% | Wgt Acc: 99.310% | LR: 1.250000e-04 | Dur: 157.41s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   2.]
 [  0. 571.   4.   0.]
 [  2.   5. 730.   1.]
 [  1.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.246 | Acc: 60.550% | Wgt Acc: 58.696% | Dur: 15.96s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  7.  5. 15.]
 [ 0. 17.  5.  0.]
 [ 5. 50. 57. 16.]
 [14.  4.  8. 55.]]

I - Epoch: 146
I - Training: 
	I - Batch: 50 | Loss: 1.394 | Acc: 99.250% | Wgt Acc: 99.236%
	I - Batch: 100 | Loss: 1.418 | Acc: 99.125% | Wgt Acc: 99.098%
	I - Batch: 150 | Loss: 1.422 | Acc: 99.208% | Wgt Acc: 99.173%
I - num batch: 160
I - Train -- Loss: 1.422 | Acc: 99.215% | Wgt Acc: 99.177% | LR: 1.250000e-04 | Dur: 155.10s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 568.   4.   0.]
 [  2.   8. 730.   2.]
 [  1.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.288 | Acc: 59.021% | Wgt Acc: 57.065% | Dur: 16.44s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  3.  6. 16.]
 [ 0. 20.  3.  0.]
 [ 8. 50. 59. 22.]
 [14.  5.  7. 48.]]

I - Epoch: 147
I - Training: 
	I - Batch: 50 | Loss: 1.443 | Acc: 99.250% | Wgt Acc: 99.187%
	I - Batch: 100 | Loss: 1.424 | Acc: 99.250% | Wgt Acc: 99.213%
	I - Batch: 150 | Loss: 1.423 | Acc: 99.167% | Wgt Acc: 99.118%
I - num batch: 160
I - Train -- Loss: 1.417 | Acc: 99.176% | Wgt Acc: 99.124% | LR: 1.250000e-04 | Dur: 156.45s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 565.   3.   0.]
 [  1.  11. 731.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.234 | Acc: 62.080% | Wgt Acc: 60.870% | Dur: 15.83s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  4.  7. 14.]
 [ 0. 28.  9.  0.]
 [ 8. 44. 55. 16.]
 [16.  2.  4. 56.]]

I - Epoch: 148
I - Training: 
	I - Batch: 50 | Loss: 1.395 | Acc: 98.875% | Wgt Acc: 98.789%
	I - Batch: 100 | Loss: 1.429 | Acc: 98.938% | Wgt Acc: 98.876%
	I - Batch: 150 | Loss: 1.419 | Acc: 99.042% | Wgt Acc: 98.977%
I - num batch: 160
I - Train -- Loss: 1.421 | Acc: 98.940% | Wgt Acc: 98.868% | LR: 1.250000e-04 | Dur: 157.32s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.]
 [  0. 559.   4.   0.]
 [  1.  17. 730.   1.]
 [  2.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.230 | Acc: 58.104% | Wgt Acc: 55.978% | Dur: 15.82s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  5.  8. 26.]
 [ 0. 17.  4.  0.]
 [ 3. 51. 55. 13.]
 [14.  5.  8. 47.]]

I - Epoch: 149
I - Training: 
	I - Batch: 50 | Loss: 1.398 | Acc: 99.000% | Wgt Acc: 98.926%
	I - Batch: 100 | Loss: 1.402 | Acc: 99.062% | Wgt Acc: 98.984%
	I - Batch: 150 | Loss: 1.415 | Acc: 98.792% | Wgt Acc: 98.694%
I - num batch: 160
I - Train -- Loss: 1.417 | Acc: 98.822% | Wgt Acc: 98.726% | LR: 1.250000e-04 | Dur: 154.07s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 556.   3.   0.]
 [  2.  20. 731.   1.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.265 | Acc: 58.104% | Wgt Acc: 56.250% | Dur: 16.49s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  7. 21.]
 [ 0. 20.  8.  1.]
 [ 6. 48. 52. 16.]
 [12.  3.  8. 48.]]

I - Epoch: 150
I - Training: 
	I - Batch: 50 | Loss: 1.392 | Acc: 99.250% | Wgt Acc: 99.239%
	I - Batch: 100 | Loss: 1.415 | Acc: 99.250% | Wgt Acc: 99.224%
	I - Batch: 150 | Loss: 1.428 | Acc: 98.875% | Wgt Acc: 98.808%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 98.861% | Wgt Acc: 98.788% | LR: 1.250000e-04 | Dur: 157.51s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   3.]
 [  0. 561.   3.   0.]
 [  1.  15. 731.   1.]
 [  4.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.251 | Acc: 59.021% | Wgt Acc: 57.745% | Dur: 15.76s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  2.  2. 13.]
 [ 0. 19.  7.  0.]
 [ 8. 49. 57. 14.]
 [22.  8.  9. 59.]]

I - Epoch: 151
I - Training: 
	I - Batch: 50 | Loss: 1.427 | Acc: 99.250% | Wgt Acc: 99.209%
	I - Batch: 100 | Loss: 1.401 | Acc: 99.188% | Wgt Acc: 99.139%
	I - Batch: 150 | Loss: 1.414 | Acc: 99.208% | Wgt Acc: 99.174%
I - num batch: 160
I - Train -- Loss: 1.417 | Acc: 99.215% | Wgt Acc: 99.186% | LR: 1.250000e-04 | Dur: 157.34s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.]
 [  0. 567.   4.   0.]
 [  2.   9. 729.   1.]
 [  1.   2.   1. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.271 | Acc: 57.798% | Wgt Acc: 56.182% | Dur: 16.21s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  8. 22.]
 [ 1. 23.  8.  2.]
 [ 6. 47. 53. 14.]
 [16.  3.  6. 48.]]

I - Epoch: 152
I - Training: 
	I - Batch: 50 | Loss: 1.404 | Acc: 99.375% | Wgt Acc: 99.351%
	I - Batch: 100 | Loss: 1.414 | Acc: 99.438% | Wgt Acc: 99.423%
	I - Batch: 150 | Loss: 1.425 | Acc: 98.875% | Wgt Acc: 98.818%
I - num batch: 160
I - Train -- Loss: 1.422 | Acc: 98.940% | Wgt Acc: 98.885% | LR: 1.250000e-04 | Dur: 153.69s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 562.   4.   0.]
 [  2.  14. 728.   1.]
 [  1.   2.   2. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.262 | Acc: 61.468% | Wgt Acc: 59.715% | Dur: 16.09s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  4.  7. 18.]
 [ 0. 23.  8.  0.]
 [ 6. 46. 57. 16.]
 [13.  5.  3. 52.]]

I - Epoch: 153
I - Training: 
	I - Batch: 50 | Loss: 1.422 | Acc: 99.375% | Wgt Acc: 99.351%
	I - Batch: 100 | Loss: 1.419 | Acc: 99.312% | Wgt Acc: 99.268%
	I - Batch: 150 | Loss: 1.419 | Acc: 99.250% | Wgt Acc: 99.211%
I - num batch: 160
I - Train -- Loss: 1.414 | Acc: 99.293% | Wgt Acc: 99.257% | LR: 1.250000e-04 | Dur: 154.55s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 568.   4.   0.]
 [  2.   8. 730.   1.]
 [  0.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.263 | Acc: 58.716% | Wgt Acc: 56.318% | Dur: 15.78s
I - Confusion Matrix: [row->prediction - col->label]
[[74. 10.  9. 31.]
 [ 0. 19.  2.  0.]
 [ 3. 45. 57. 13.]
 [11.  4.  7. 42.]]

I - Epoch: 154
I - Training: 
	I - Batch: 50 | Loss: 1.395 | Acc: 98.375% | Wgt Acc: 98.197%
	I - Batch: 100 | Loss: 1.398 | Acc: 98.812% | Wgt Acc: 98.730%
	I - Batch: 150 | Loss: 1.423 | Acc: 98.917% | Wgt Acc: 98.827%
I - num batch: 160
I - Train -- Loss: 1.427 | Acc: 98.940% | Wgt Acc: 98.859% | LR: 1.250000e-04 | Dur: 156.59s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 559.   4.   0.]
 [  1.  17. 730.   1.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.262 | Acc: 58.410% | Wgt Acc: 56.590% | Dur: 15.93s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  9. 24.]
 [ 0. 22.  6.  1.]
 [ 6. 45. 53. 14.]
 [13.  5.  7. 47.]]

I - Epoch: 155
I - Training: 
	I - Batch: 50 | Loss: 1.381 | Acc: 99.000% | Wgt Acc: 98.869%
	I - Batch: 100 | Loss: 1.433 | Acc: 98.750% | Wgt Acc: 98.661%
	I - Batch: 150 | Loss: 1.433 | Acc: 98.250% | Wgt Acc: 98.104%
I - num batch: 160
I - Train -- Loss: 1.433 | Acc: 98.312% | Wgt Acc: 98.169% | LR: 1.250000e-04 | Dur: 157.32s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   2.]
 [  0. 546.   5.   0.]
 [  2.  30. 729.   1.]
 [  1.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.204 | Acc: 60.245% | Wgt Acc: 58.967% | Dur: 16.23s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  6. 21.]
 [ 0. 22.  5.  0.]
 [ 3. 43. 54.  7.]
 [22.  9. 10. 58.]]

I - Epoch: 156
I - Training: 
	I - Batch: 50 | Loss: 1.406 | Acc: 99.250% | Wgt Acc: 99.236%
	I - Batch: 100 | Loss: 1.422 | Acc: 98.312% | Wgt Acc: 98.221%
	I - Batch: 150 | Loss: 1.433 | Acc: 98.458% | Wgt Acc: 98.366%
I - num batch: 160
I - Train -- Loss: 1.436 | Acc: 98.469% | Wgt Acc: 98.372% | LR: 1.250000e-04 | Dur: 155.45s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   2.]
 [  0. 554.   5.   0.]
 [  2.  22. 727.   2.]
 [  2.   2.   1. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.221 | Acc: 57.187% | Wgt Acc: 55.367% | Dur: 15.99s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6. 10. 19.]
 [ 0. 14.  2.  1.]
 [ 6. 54. 54. 13.]
 [16.  4.  9. 53.]]

I - Epoch: 157
I - Training: 
	I - Batch: 50 | Loss: 1.450 | Acc: 98.750% | Wgt Acc: 98.680%
	I - Batch: 100 | Loss: 1.443 | Acc: 99.000% | Wgt Acc: 98.933%
	I - Batch: 150 | Loss: 1.429 | Acc: 99.042% | Wgt Acc: 98.987%
I - num batch: 160
I - Train -- Loss: 1.427 | Acc: 99.097% | Wgt Acc: 99.045% | LR: 1.250000e-04 | Dur: 155.49s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   1.   0.]
 [  0. 563.   4.   0.]
 [  1.  13. 729.   1.]
 [  1.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.225 | Acc: 58.716% | Wgt Acc: 56.929% | Dur: 15.93s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  6. 19.]
 [ 0. 12.  1.  0.]
 [ 5. 54. 57.  9.]
 [18.  7. 11. 58.]]

I - Epoch: 158
I - Training: 
	I - Batch: 50 | Loss: 1.424 | Acc: 98.625% | Wgt Acc: 98.455%
	I - Batch: 100 | Loss: 1.411 | Acc: 99.062% | Wgt Acc: 98.957%
	I - Batch: 150 | Loss: 1.420 | Acc: 99.042% | Wgt Acc: 98.939%
I - num batch: 160
I - Train -- Loss: 1.420 | Acc: 99.058% | Wgt Acc: 98.965% | LR: 1.250000e-04 | Dur: 154.41s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 559.   1.   0.]
 [  1.  17. 733.   2.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.236 | Acc: 57.798% | Wgt Acc: 56.726% | Dur: 15.61s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  4.  5. 13.]
 [ 1. 22.  8.  2.]
 [ 7. 46. 55. 14.]
 [25.  6.  7. 57.]]

I - Epoch: 159
I - Training: 
	I - Batch: 50 | Loss: 1.425 | Acc: 98.500% | Wgt Acc: 98.396%
	I - Batch: 100 | Loss: 1.426 | Acc: 98.812% | Wgt Acc: 98.723%
	I - Batch: 150 | Loss: 1.420 | Acc: 99.000% | Wgt Acc: 98.921%
I - num batch: 160
I - Train -- Loss: 1.420 | Acc: 99.018% | Wgt Acc: 98.938% | LR: 1.250000e-04 | Dur: 131.58s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.]
 [  0. 559.   2.   0.]
 [  1.  17. 732.   1.]
 [  2.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.237 | Acc: 60.550% | Wgt Acc: 59.035% | Dur: 12.30s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  5.  3. 16.]
 [ 1. 27.  7.  0.]
 [ 7. 41. 57. 20.]
 [16.  5.  8. 50.]]

I - Epoch: 160
I - Training: 
	I - Batch: 50 | Loss: 1.466 | Acc: 97.750% | Wgt Acc: 97.566%
	I - Batch: 100 | Loss: 1.428 | Acc: 98.438% | Wgt Acc: 98.311%
	I - Batch: 150 | Loss: 1.430 | Acc: 98.667% | Wgt Acc: 98.555%
I - num batch: 160
I - Train -- Loss: 1.431 | Acc: 98.626% | Wgt Acc: 98.514% | LR: 1.250000e-04 | Dur: 122.09s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 552.   5.   0.]
 [  1.  24. 729.   2.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.258 | Acc: 59.633% | Wgt Acc: 57.541% | Dur: 11.58s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  6.  8. 24.]
 [ 0. 19.  5.  0.]
 [ 3. 46. 54. 14.]
 [11.  7.  8. 48.]]

I - Epoch: 161
I - Training: 
	I - Batch: 50 | Loss: 1.382 | Acc: 99.500% | Wgt Acc: 99.489%
	I - Batch: 100 | Loss: 1.418 | Acc: 99.500% | Wgt Acc: 99.480%
	I - Batch: 150 | Loss: 1.428 | Acc: 99.208% | Wgt Acc: 99.156%
I - num batch: 160
I - Train -- Loss: 1.420 | Acc: 99.254% | Wgt Acc: 99.204% | LR: 1.250000e-04 | Dur: 119.20s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 566.   3.   0.]
 [  1.  10. 731.   1.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.238 | Acc: 59.939% | Wgt Acc: 58.492% | Dur: 11.65s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  7. 19.]
 [ 0. 24.  7.  1.]
 [ 7. 42. 54. 13.]
 [16.  7.  7. 53.]]

I - Epoch: 162
I - Training: 
	I - Batch: 50 | Loss: 1.410 | Acc: 99.500% | Wgt Acc: 99.437%
	I - Batch: 100 | Loss: 1.412 | Acc: 99.500% | Wgt Acc: 99.465%
	I - Batch: 150 | Loss: 1.418 | Acc: 99.208% | Wgt Acc: 99.164%
I - num batch: 160
I - Train -- Loss: 1.416 | Acc: 99.215% | Wgt Acc: 99.168% | LR: 1.250000e-04 | Dur: 121.03s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 565.   4.   0.]
 [  1.  11. 730.   1.]
 [  1.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.271 | Acc: 56.269% | Wgt Acc: 53.804% | Dur: 12.08s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  7.  6. 27.]
 [ 1. 21.  9.  1.]
 [ 7. 48. 55. 23.]
 [ 7.  2.  5. 35.]]

I - Epoch: 163
I - Training: 
	I - Batch: 50 | Loss: 1.439 | Acc: 98.875% | Wgt Acc: 98.820%
	I - Batch: 100 | Loss: 1.432 | Acc: 98.438% | Wgt Acc: 98.312%
	I - Batch: 150 | Loss: 1.426 | Acc: 98.667% | Wgt Acc: 98.572%
I - num batch: 160
I - Train -- Loss: 1.424 | Acc: 98.704% | Wgt Acc: 98.611% | LR: 1.250000e-04 | Dur: 123.76s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.]
 [  0. 554.   3.   0.]
 [  1.  22. 731.   1.]
 [  4.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.303 | Acc: 54.434% | Wgt Acc: 51.495% | Dur: 13.03s
I - Confusion Matrix: [row->prediction - col->label]
[[77.  9. 10. 37.]
 [ 0. 12.  5.  0.]
 [ 6. 54. 55. 15.]
 [ 5.  3.  5. 34.]]

I - Epoch: 164
I - Training: 
	I - Batch: 50 | Loss: 1.410 | Acc: 99.375% | Wgt Acc: 99.350%
	I - Batch: 100 | Loss: 1.437 | Acc: 98.688% | Wgt Acc: 98.608%
	I - Batch: 150 | Loss: 1.424 | Acc: 99.083% | Wgt Acc: 99.024%
I - num batch: 160
I - Train -- Loss: 1.421 | Acc: 98.901% | Wgt Acc: 98.841% | LR: 1.250000e-04 | Dur: 129.99s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   1.   0.]
 [  0. 560.   3.   0.]
 [  1.  16. 730.   1.]
 [  4.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.240 | Acc: 58.716% | Wgt Acc: 57.065% | Dur: 15.90s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  6. 18.]
 [ 0. 22.  9.  3.]
 [ 7. 47. 51. 15.]
 [12.  3.  9. 50.]]

I - Epoch: 165
I - Training: 
	I - Batch: 50 | Loss: 1.431 | Acc: 98.125% | Wgt Acc: 97.944%
	I - Batch: 100 | Loss: 1.422 | Acc: 98.562% | Wgt Acc: 98.419%
	I - Batch: 150 | Loss: 1.444 | Acc: 97.917% | Wgt Acc: 97.726%
I - num batch: 160
I - Train -- Loss: 1.449 | Acc: 97.998% | Wgt Acc: 97.815% | LR: 1.250000e-04 | Dur: 154.27s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   3.]
 [  0. 540.   1.   0.]
 [  2.  36. 733.   2.]
 [  5.   2.   0. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.270 | Acc: 53.823% | Wgt Acc: 51.563% | Dur: 15.99s
I - Confusion Matrix: [row->prediction - col->label]
[[72. 10. 13. 37.]
 [ 1. 23.  9.  2.]
 [ 4. 40. 49. 15.]
 [11.  5.  4. 32.]]

I - Epoch: 166
I - Training: 
	I - Batch: 50 | Loss: 1.452 | Acc: 98.750% | Wgt Acc: 98.709%
	I - Batch: 100 | Loss: 1.438 | Acc: 99.125% | Wgt Acc: 99.085%
	I - Batch: 150 | Loss: 1.441 | Acc: 98.792% | Wgt Acc: 98.723%
I - num batch: 160
I - Train -- Loss: 1.442 | Acc: 98.587% | Wgt Acc: 98.487% | LR: 1.250000e-04 | Dur: 154.13s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 554.   4.   0.]
 [  2.  22. 729.   1.]
 [  2.   2.   1. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.224 | Acc: 59.633% | Wgt Acc: 57.880% | Dur: 15.97s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  4.  8. 23.]
 [ 0. 23.  9.  2.]
 [ 7. 45. 52. 12.]
 [10.  6.  6. 49.]]

I - Epoch: 167
I - Training: 
	I - Batch: 50 | Loss: 1.433 | Acc: 98.250% | Wgt Acc: 98.141%
	I - Batch: 100 | Loss: 1.425 | Acc: 98.562% | Wgt Acc: 98.450%
	I - Batch: 150 | Loss: 1.441 | Acc: 98.333% | Wgt Acc: 98.206%
I - num batch: 160
I - Train -- Loss: 1.436 | Acc: 98.430% | Wgt Acc: 98.310% | LR: 1.250000e-04 | Dur: 153.41s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 550.   4.   0.]
 [  1.  26. 727.   2.]
 [  1.   2.   3. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.289 | Acc: 58.716% | Wgt Acc: 56.590% | Dur: 16.31s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7. 11. 33.]
 [ 1. 22.  6.  1.]
 [ 1. 45. 52.  9.]
 [11.  4.  6. 43.]]

I - Epoch: 168
I - Training: 
	I - Batch: 50 | Loss: 1.449 | Acc: 97.875% | Wgt Acc: 97.644%
	I - Batch: 100 | Loss: 1.421 | Acc: 98.812% | Wgt Acc: 98.679%
	I - Batch: 150 | Loss: 1.427 | Acc: 98.167% | Wgt Acc: 97.972%
I - num batch: 160
I - Train -- Loss: 1.424 | Acc: 98.233% | Wgt Acc: 98.045% | LR: 1.250000e-04 | Dur: 158.59s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 539.   1.   0.]
 [  3.  37. 733.   1.]
 [  0.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.261 | Acc: 58.104% | Wgt Acc: 56.046% | Dur: 16.87s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  8.  7. 22.]
 [ 0. 10.  2.  0.]
 [ 4. 56. 58.  9.]
 [17.  4.  8. 55.]]

I - Epoch: 169
I - Training: 
	I - Batch: 50 | Loss: 1.373 | Acc: 98.875% | Wgt Acc: 98.786%
	I - Batch: 100 | Loss: 1.395 | Acc: 99.188% | Wgt Acc: 99.112%
	I - Batch: 150 | Loss: 1.422 | Acc: 99.167% | Wgt Acc: 99.108%
I - num batch: 160
I - Train -- Loss: 1.419 | Acc: 99.176% | Wgt Acc: 99.115% | LR: 1.250000e-04 | Dur: 158.41s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 565.   2.   0.]
 [  2.  11. 731.   2.]
 [  0.   2.   1. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.222 | Acc: 58.104% | Wgt Acc: 57.065% | Dur: 15.46s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  5.  4. 11.]
 [ 0. 19.  5.  1.]
 [ 6. 47. 53. 13.]
 [25.  7. 13. 61.]]

I - Epoch: 170
I - Training: 
	I - Batch: 50 | Loss: 1.400 | Acc: 99.000% | Wgt Acc: 98.896%
	I - Batch: 100 | Loss: 1.439 | Acc: 99.000% | Wgt Acc: 98.900%
	I - Batch: 150 | Loss: 1.420 | Acc: 99.167% | Wgt Acc: 99.099%
I - num batch: 160
I - Train -- Loss: 1.418 | Acc: 99.215% | Wgt Acc: 99.151% | LR: 1.250000e-04 | Dur: 154.68s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 564.   2.   0.]
 [  2.  12. 732.   2.]
 [  0.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.284 | Acc: 57.492% | Wgt Acc: 55.231% | Dur: 16.37s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  5. 24.]
 [ 0. 22.  7.  2.]
 [ 8. 46. 58. 21.]
 [11.  4.  5. 39.]]

I - Epoch: 171
I - Training: 
	I - Batch: 50 | Loss: 1.438 | Acc: 99.750% | Wgt Acc: 99.719%
	I - Batch: 100 | Loss: 1.454 | Acc: 98.375% | Wgt Acc: 98.270%
	I - Batch: 150 | Loss: 1.448 | Acc: 98.583% | Wgt Acc: 98.507%
I - num batch: 160
I - Train -- Loss: 1.440 | Acc: 98.626% | Wgt Acc: 98.549% | LR: 1.250000e-04 | Dur: 153.57s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   0.   3.]
 [  0. 558.   2.   0.]
 [  2.  18. 732.   1.]
 [  7.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.255 | Acc: 57.798% | Wgt Acc: 55.910% | Dur: 16.04s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  8.  8. 22.]
 [ 0. 17.  9.  1.]
 [ 5. 53. 50. 13.]
 [11.  0.  8. 50.]]

I - Epoch: 172
I - Training: 
	I - Batch: 50 | Loss: 1.437 | Acc: 98.625% | Wgt Acc: 98.527%
	I - Batch: 100 | Loss: 1.440 | Acc: 98.750% | Wgt Acc: 98.630%
	I - Batch: 150 | Loss: 1.444 | Acc: 98.583% | Wgt Acc: 98.460%
I - num batch: 160
I - Train -- Loss: 1.458 | Acc: 98.469% | Wgt Acc: 98.337% | LR: 1.250000e-04 | Dur: 158.57s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   5.]
 [  0. 554.   3.   0.]
 [  1.  22. 731.   3.]
 [  3.   2.   0. 530.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.269 | Acc: 56.881% | Wgt Acc: 54.688% | Dur: 16.08s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  6. 11. 29.]
 [ 0. 16.  6.  0.]
 [ 4. 49. 51. 12.]
 [10.  7.  7. 45.]]

I - Epoch: 173
I - Training: 
	I - Batch: 50 | Loss: 1.460 | Acc: 98.625% | Wgt Acc: 98.541%
	I - Batch: 100 | Loss: 1.448 | Acc: 98.875% | Wgt Acc: 98.776%
	I - Batch: 150 | Loss: 1.440 | Acc: 98.917% | Wgt Acc: 98.835%
I - num batch: 160
I - Train -- Loss: 1.444 | Acc: 98.861% | Wgt Acc: 98.770% | LR: 1.250000e-04 | Dur: 155.78s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 558.   1.   0.]
 [  2.  18. 731.   2.]
 [  1.   2.   2. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.231 | Acc: 58.104% | Wgt Acc: 56.997% | Dur: 15.92s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  9.  8. 18.]
 [ 0. 21.  4.  0.]
 [ 5. 43. 53. 10.]
 [25.  5. 10. 58.]]

I - Epoch: 174
I - Training: 
	I - Batch: 50 | Loss: 1.440 | Acc: 99.000% | Wgt Acc: 98.904%
	I - Batch: 100 | Loss: 1.414 | Acc: 98.812% | Wgt Acc: 98.672%
	I - Batch: 150 | Loss: 1.423 | Acc: 98.792% | Wgt Acc: 98.676%
I - num batch: 160
I - Train -- Loss: 1.430 | Acc: 98.783% | Wgt Acc: 98.682% | LR: 1.250000e-04 | Dur: 155.32s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   0.   3.]
 [  0. 557.   2.   0.]
 [  1.  18. 731.   1.]
 [  2.   2.   1. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.239 | Acc: 61.162% | Wgt Acc: 59.579% | Dur: 16.13s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  9. 23.]
 [ 0. 23.  3.  0.]
 [ 2. 42. 53.  9.]
 [16.  6. 10. 54.]]

I - Epoch: 175
I - Training: 
	I - Batch: 50 | Loss: 1.399 | Acc: 99.250% | Wgt Acc: 99.183%
	I - Batch: 100 | Loss: 1.415 | Acc: 99.375% | Wgt Acc: 99.338%
	I - Batch: 150 | Loss: 1.415 | Acc: 99.125% | Wgt Acc: 99.089%
I - num batch: 160
I - Train -- Loss: 1.419 | Acc: 99.176% | Wgt Acc: 99.142% | LR: 1.250000e-04 | Dur: 155.32s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   1.   0.]
 [  0. 566.   5.   0.]
 [  1.  10. 728.   1.]
 [  1.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.226 | Acc: 56.269% | Wgt Acc: 54.348% | Dur: 16.22s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  3.  6. 11.]
 [ 1. 10.  3.  0.]
 [ 8. 58. 58. 21.]
 [17.  7.  8. 54.]]

I - Epoch: 176
I - Training: 
	I - Batch: 50 | Loss: 1.369 | Acc: 99.625% | Wgt Acc: 99.603%
	I - Batch: 100 | Loss: 1.388 | Acc: 99.438% | Wgt Acc: 99.392%
	I - Batch: 150 | Loss: 1.416 | Acc: 99.375% | Wgt Acc: 99.314%
I - num batch: 160
I - Train -- Loss: 1.421 | Acc: 99.372% | Wgt Acc: 99.310% | LR: 1.250000e-04 | Dur: 157.80s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   2.]
 [  0. 568.   0.   0.]
 [  1.   8. 734.   2.]
 [  1.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.223 | Acc: 57.798% | Wgt Acc: 55.707% | Dur: 16.72s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  5.  9. 22.]
 [ 0. 14.  5.  0.]
 [ 9. 53. 54. 14.]
 [ 8.  6.  7. 50.]]

I - Epoch: 177
I - Training: 
	I - Batch: 50 | Loss: 1.402 | Acc: 99.625% | Wgt Acc: 99.576%
	I - Batch: 100 | Loss: 1.420 | Acc: 99.250% | Wgt Acc: 99.183%
	I - Batch: 150 | Loss: 1.417 | Acc: 99.042% | Wgt Acc: 98.948%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 98.979% | Wgt Acc: 98.885% | LR: 1.250000e-04 | Dur: 155.47s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   2.   0.   0.]
 [  0. 557.   1.   0.]
 [  2.  19. 733.   1.]
 [  1.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.253 | Acc: 57.798% | Wgt Acc: 55.910% | Dur: 16.14s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  8.  6. 17.]
 [ 0.  9.  4.  0.]
 [ 4. 55. 54. 11.]
 [16.  6. 11. 58.]]

I - Epoch: 178
I - Training: 
	I - Batch: 50 | Loss: 1.392 | Acc: 99.625% | Wgt Acc: 99.576%
	I - Batch: 100 | Loss: 1.423 | Acc: 99.438% | Wgt Acc: 99.380%
	I - Batch: 150 | Loss: 1.417 | Acc: 99.375% | Wgt Acc: 99.333%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 99.372% | Wgt Acc: 99.328% | LR: 1.250000e-04 | Dur: 155.12s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 568.   2.   0.]
 [  1.   8. 732.   2.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.279 | Acc: 56.881% | Wgt Acc: 55.299% | Dur: 15.82s
I - Confusion Matrix: [row->prediction - col->label]
[[76.  8. 10. 35.]
 [ 0. 30. 18.  1.]
 [ 3. 37. 40. 10.]
 [ 9.  3.  7. 40.]]

I - Epoch: 179
I - Training: 
	I - Batch: 50 | Loss: 1.418 | Acc: 98.750% | Wgt Acc: 98.649%
	I - Batch: 100 | Loss: 1.431 | Acc: 98.875% | Wgt Acc: 98.761%
	I - Batch: 150 | Loss: 1.434 | Acc: 99.042% | Wgt Acc: 98.958%
I - num batch: 160
I - Train -- Loss: 1.433 | Acc: 99.058% | Wgt Acc: 98.974% | LR: 1.250000e-04 | Dur: 156.07s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   2.]
 [  0. 562.   0.   0.]
 [  1.  14. 732.   2.]
 [  1.   2.   2. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.232 | Acc: 58.716% | Wgt Acc: 57.065% | Dur: 16.37s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  7.  7. 16.]
 [ 0. 13.  3.  0.]
 [ 7. 55. 58. 11.]
 [19.  3.  7. 59.]]

I - Epoch: 180
I - Training: 
	I - Batch: 50 | Loss: 1.419 | Acc: 99.875% | Wgt Acc: 99.888%
	I - Batch: 100 | Loss: 1.436 | Acc: 99.438% | Wgt Acc: 99.382%
	I - Batch: 150 | Loss: 1.423 | Acc: 99.458% | Wgt Acc: 99.418%
I - num batch: 160
I - Train -- Loss: 1.422 | Acc: 99.490% | Wgt Acc: 99.452% | LR: 1.250000e-04 | Dur: 161.91s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 571.   1.   0.]
 [  1.   5. 733.   2.]
 [  1.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.269 | Acc: 57.492% | Wgt Acc: 55.435% | Dur: 16.06s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  3.  7. 22.]
 [ 2. 19.  5.  2.]
 [ 9. 51. 58. 17.]
 [11.  5.  5. 45.]]

I - Epoch: 181
I - Training: 
	I - Batch: 50 | Loss: 1.435 | Acc: 99.375% | Wgt Acc: 99.326%
	I - Batch: 100 | Loss: 1.454 | Acc: 99.188% | Wgt Acc: 99.143%
	I - Batch: 150 | Loss: 1.416 | Acc: 99.333% | Wgt Acc: 99.286%
I - num batch: 160
I - Train -- Loss: 1.424 | Acc: 99.293% | Wgt Acc: 99.239% | LR: 1.250000e-04 | Dur: 153.59s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 567.   2.   0.]
 [  1.   9. 732.   3.]
 [  1.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.196 | Acc: 62.080% | Wgt Acc: 61.005% | Dur: 15.95s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  5. 11.]
 [ 0. 24.  7.  1.]
 [ 4. 42. 52. 12.]
 [19.  7. 11. 62.]]

I - Epoch: 182
I - Training: 
	I - Batch: 50 | Loss: 1.427 | Acc: 99.750% | Wgt Acc: 99.718%
	I - Batch: 100 | Loss: 1.420 | Acc: 99.562% | Wgt Acc: 99.548%
	I - Batch: 150 | Loss: 1.420 | Acc: 99.500% | Wgt Acc: 99.465%
I - num batch: 160
I - Train -- Loss: 1.418 | Acc: 99.490% | Wgt Acc: 99.452% | LR: 1.250000e-04 | Dur: 154.20s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 569.   1.   0.]
 [  1.   7. 733.   1.]
 [  1.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.236 | Acc: 59.939% | Wgt Acc: 58.424% | Dur: 16.51s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  6.  6. 19.]
 [ 0. 21.  3.  0.]
 [ 6. 47. 56. 12.]
 [18.  4. 10. 55.]]

I - Epoch: 183
I - Training: 
	I - Batch: 50 | Loss: 1.383 | Acc: 99.750% | Wgt Acc: 99.719%
	I - Batch: 100 | Loss: 1.401 | Acc: 99.312% | Wgt Acc: 99.253%
	I - Batch: 150 | Loss: 1.411 | Acc: 99.375% | Wgt Acc: 99.334%
I - num batch: 160
I - Train -- Loss: 1.413 | Acc: 99.333% | Wgt Acc: 99.283% | LR: 1.250000e-04 | Dur: 155.34s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 567.   2.   0.]
 [  1.   9. 732.   1.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.277 | Acc: 58.104% | Wgt Acc: 55.978% | Dur: 16.01s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  7.  9. 31.]
 [ 0. 22.  6.  0.]
 [ 3. 45. 51. 13.]
 [10.  4.  9. 42.]]

I - Epoch: 184
I - Training: 
	I - Batch: 50 | Loss: 1.420 | Acc: 99.125% | Wgt Acc: 99.098%
	I - Batch: 100 | Loss: 1.416 | Acc: 99.188% | Wgt Acc: 99.128%
	I - Batch: 150 | Loss: 1.413 | Acc: 99.208% | Wgt Acc: 99.155%
I - num batch: 160
I - Train -- Loss: 1.413 | Acc: 99.215% | Wgt Acc: 99.160% | LR: 1.250000e-04 | Dur: 156.06s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 564.   2.   0.]
 [  1.  12. 731.   1.]
 [  1.   2.   1. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.214 | Acc: 62.385% | Wgt Acc: 61.073% | Dur: 16.34s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  5.  3.  9.]
 [ 0. 18.  3.  0.]
 [ 4. 51. 59. 12.]
 [22.  4. 10. 65.]]

I - Epoch: 185
I - Training: 
	I - Batch: 50 | Loss: 1.452 | Acc: 99.500% | Wgt Acc: 99.465%
	I - Batch: 100 | Loss: 1.417 | Acc: 99.438% | Wgt Acc: 99.422%
	I - Batch: 150 | Loss: 1.423 | Acc: 99.417% | Wgt Acc: 99.399%
I - num batch: 160
I - Train -- Loss: 1.419 | Acc: 99.450% | Wgt Acc: 99.434% | LR: 1.250000e-04 | Dur: 154.82s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.]
 [  0. 572.   2.   0.]
 [  1.   4. 732.   1.]
 [  3.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.237 | Acc: 58.410% | Wgt Acc: 56.522% | Dur: 16.08s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  3.  7. 14.]
 [ 2. 14.  4.  1.]
 [ 5. 55. 59. 17.]
 [17.  6.  5. 54.]]

I - Epoch: 186
I - Training: 
	I - Batch: 50 | Loss: 1.431 | Acc: 97.750% | Wgt Acc: 97.532%
	I - Batch: 100 | Loss: 1.454 | Acc: 98.125% | Wgt Acc: 97.967%
	I - Batch: 150 | Loss: 1.439 | Acc: 98.500% | Wgt Acc: 98.385%
I - num batch: 160
I - Train -- Loss: 1.438 | Acc: 98.508% | Wgt Acc: 98.390% | LR: 1.250000e-04 | Dur: 153.67s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 552.   4.   0.]
 [  1.  24. 730.   2.]
 [  3.   2.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.224 | Acc: 59.021% | Wgt Acc: 57.337% | Dur: 15.73s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  7. 21.]
 [ 0. 20.  5.  1.]
 [ 5. 47. 52. 12.]
 [14.  6. 11. 52.]]

I - Epoch: 187
I - Training: 
	I - Batch: 50 | Loss: 1.453 | Acc: 99.000% | Wgt Acc: 98.901%
	I - Batch: 100 | Loss: 1.414 | Acc: 99.312% | Wgt Acc: 99.254%
	I - Batch: 150 | Loss: 1.425 | Acc: 98.792% | Wgt Acc: 98.667%
I - num batch: 160
I - Train -- Loss: 1.424 | Acc: 98.861% | Wgt Acc: 98.744% | LR: 1.250000e-04 | Dur: 154.02s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 554.   0.   0.]
 [  2.  22. 734.   1.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.291 | Acc: 55.963% | Wgt Acc: 53.601% | Dur: 15.58s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  8.  8. 27.]
 [ 0. 16.  3.  0.]
 [ 4. 51. 58. 18.]
 [16.  3.  6. 41.]]

I - Epoch: 188
I - Training: 
	I - Batch: 50 | Loss: 1.406 | Acc: 98.500% | Wgt Acc: 98.309%
	I - Batch: 100 | Loss: 1.417 | Acc: 98.812% | Wgt Acc: 98.691%
	I - Batch: 150 | Loss: 1.413 | Acc: 99.125% | Wgt Acc: 99.033%
I - num batch: 160
I - Train -- Loss: 1.413 | Acc: 99.136% | Wgt Acc: 99.045% | LR: 1.250000e-04 | Dur: 156.79s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 559.   0.   0.]
 [  1.  17. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.271 | Acc: 60.550% | Wgt Acc: 59.035% | Dur: 15.68s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  5. 10. 23.]
 [ 1. 24.  7.  0.]
 [ 3. 44. 50. 10.]
 [13.  5.  8. 53.]]

I - Epoch: 189
I - Training: 
	I - Batch: 50 | Loss: 1.425 | Acc: 99.375% | Wgt Acc: 99.352%
	I - Batch: 100 | Loss: 1.415 | Acc: 99.375% | Wgt Acc: 99.366%
	I - Batch: 150 | Loss: 1.427 | Acc: 99.083% | Wgt Acc: 99.043%
I - num batch: 160
I - Train -- Loss: 1.426 | Acc: 99.097% | Wgt Acc: 99.053% | LR: 1.250000e-04 | Dur: 159.13s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   1.   1.   0.]
 [  0. 564.   1.   0.]
 [  2.  13. 731.   1.]
 [  3.   0.   1. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.227 | Acc: 56.881% | Wgt Acc: 54.755% | Dur: 17.08s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  5. 19.]
 [ 0. 19.  2.  0.]
 [ 5. 48. 59. 24.]
 [18.  7.  9. 43.]]

I - Epoch: 190
I - Training: 
	I - Batch: 50 | Loss: 1.403 | Acc: 99.000% | Wgt Acc: 98.959%
	I - Batch: 100 | Loss: 1.416 | Acc: 98.688% | Wgt Acc: 98.592%
	I - Batch: 150 | Loss: 1.432 | Acc: 98.250% | Wgt Acc: 98.087%
I - num batch: 160
I - Train -- Loss: 1.429 | Acc: 98.312% | Wgt Acc: 98.160% | LR: 1.250000e-04 | Dur: 155.33s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   0.   1.]
 [  0. 544.   4.   0.]
 [  1.  32. 730.   1.]
 [  2.   1.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.221 | Acc: 60.856% | Wgt Acc: 59.375% | Dur: 15.76s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  3.  7. 20.]
 [ 0. 30.  7.  2.]
 [ 7. 39. 55. 16.]
 [15.  6.  6. 48.]]

I - Epoch: 191
I - Training: 
	I - Batch: 50 | Loss: 1.437 | Acc: 99.250% | Wgt Acc: 99.214%
	I - Batch: 100 | Loss: 1.435 | Acc: 99.250% | Wgt Acc: 99.185%
	I - Batch: 150 | Loss: 1.423 | Acc: 99.292% | Wgt Acc: 99.250%
I - num batch: 160
I - Train -- Loss: 1.423 | Acc: 99.254% | Wgt Acc: 99.204% | LR: 1.250000e-04 | Dur: 157.16s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   2.]
 [  0. 568.   2.   0.]
 [  1.   8. 731.   2.]
 [  1.   1.   1. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.237 | Acc: 56.881% | Wgt Acc: 55.095% | Dur: 16.24s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  3.  0. 14.]
 [ 1. 21. 10.  2.]
 [ 7. 52. 60. 24.]
 [21.  2.  5. 46.]]

I - Epoch: 192
I - Training: 
	I - Batch: 50 | Loss: 1.491 | Acc: 98.750% | Wgt Acc: 98.603%
	I - Batch: 100 | Loss: 1.453 | Acc: 98.750% | Wgt Acc: 98.648%
	I - Batch: 150 | Loss: 1.441 | Acc: 98.792% | Wgt Acc: 98.694%
I - num batch: 160
I - Train -- Loss: 1.441 | Acc: 98.861% | Wgt Acc: 98.770% | LR: 1.250000e-04 | Dur: 156.14s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   1.   4.]
 [  0. 560.   1.   0.]
 [  2.  16. 731.   1.]
 [  1.   2.   1. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.238 | Acc: 59.327% | Wgt Acc: 57.677% | Dur: 16.87s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  6. 21.]
 [ 0. 19.  4.  0.]
 [ 5. 48. 52. 11.]
 [14.  6. 13. 54.]]

I - Epoch: 193
I - Training: 
	I - Batch: 50 | Loss: 1.422 | Acc: 99.125% | Wgt Acc: 99.125%
	I - Batch: 100 | Loss: 1.440 | Acc: 99.000% | Wgt Acc: 98.944%
	I - Batch: 150 | Loss: 1.433 | Acc: 99.000% | Wgt Acc: 98.948%
I - num batch: 160
I - Train -- Loss: 1.438 | Acc: 98.901% | Wgt Acc: 98.832% | LR: 1.250000e-04 | Dur: 154.72s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.]
 [  0. 561.   2.   0.]
 [  2.  15. 731.   1.]
 [  3.   2.   1. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.182 | Acc: 55.963% | Wgt Acc: 55.503% | Dur: 15.79s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  8.  6.  9.]
 [ 0. 13.  3.  0.]
 [ 1. 46. 45.  5.]
 [34. 11. 21. 72.]]

I - Epoch: 194
I - Training: 
	I - Batch: 50 | Loss: 1.482 | Acc: 97.000% | Wgt Acc: 96.865%
	I - Batch: 100 | Loss: 1.480 | Acc: 96.438% | Wgt Acc: 96.192%
	I - Batch: 150 | Loss: 1.479 | Acc: 96.625% | Wgt Acc: 96.357%
I - num batch: 160
I - Train -- Loss: 1.479 | Acc: 96.781% | Wgt Acc: 96.532% | LR: 1.250000e-04 | Dur: 154.48s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.]
 [  1. 523.   9.   0.]
 [  2.  53. 721.   7.]
 [  2.   2.   4. 529.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.265 | Acc: 57.798% | Wgt Acc: 57.065% | Dur: 16.91s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  3.  6. 18.]
 [ 2. 43. 24.  5.]
 [10. 29. 42. 22.]
 [13.  3.  3. 41.]]

I - Epoch: 195
I - Training: 
	I - Batch: 50 | Loss: 1.451 | Acc: 98.500% | Wgt Acc: 98.422%
	I - Batch: 100 | Loss: 1.433 | Acc: 98.750% | Wgt Acc: 98.659%
	I - Batch: 150 | Loss: 1.432 | Acc: 98.792% | Wgt Acc: 98.713%
I - num batch: 160
I - Train -- Loss: 1.444 | Acc: 98.783% | Wgt Acc: 98.700% | LR: 1.250000e-04 | Dur: 157.11s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   1.   0.]
 [  0. 557.   4.   0.]
 [  2.  19. 729.   2.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.233 | Acc: 59.633% | Wgt Acc: 58.492% | Dur: 16.06s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  7. 20.]
 [ 0. 29. 13.  2.]
 [ 3. 38. 45. 12.]
 [16.  6. 10. 52.]]

I - Epoch: 196
I - Training: 
	I - Batch: 50 | Loss: 1.722 | Acc: 90.125% | Wgt Acc: 89.961%
	I - Batch: 100 | Loss: 1.618 | Acc: 93.625% | Wgt Acc: 93.480%
	I - Batch: 150 | Loss: 1.579 | Acc: 94.875% | Wgt Acc: 94.751%
I - num batch: 160
I - Train -- Loss: 1.581 | Acc: 94.817% | Wgt Acc: 94.683% | LR: 1.250000e-04 | Dur: 154.93s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   1.   0.   6.]
 [  0. 521.  37.   0.]
 [  5.  54. 688.  10.]
 [  8.   2.   9. 522.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.225 | Acc: 59.633% | Wgt Acc: 58.152% | Dur: 15.98s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  4.  3. 13.]
 [ 0. 15.  5.  1.]
 [ 6. 52. 58. 11.]
 [21.  7.  9. 61.]]

I - Epoch: 197
I - Training: 
	I - Batch: 50 | Loss: 1.498 | Acc: 97.875% | Wgt Acc: 97.730%
	I - Batch: 100 | Loss: 1.464 | Acc: 98.312% | Wgt Acc: 98.215%
	I - Batch: 150 | Loss: 1.443 | Acc: 98.625% | Wgt Acc: 98.554%
I - num batch: 160
I - Train -- Loss: 1.448 | Acc: 98.547% | Wgt Acc: 98.461% | LR: 1.250000e-04 | Dur: 153.87s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   3.]
 [  0. 557.   4.   0.]
 [  2.  19. 729.   2.]
 [  4.   2.   1. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.225 | Acc: 60.245% | Wgt Acc: 58.696% | Dur: 15.95s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  7. 11. 28.]
 [ 2. 31. 11.  2.]
 [ 6. 35. 47. 11.]
 [ 6.  5.  6. 45.]]

I - Epoch: 198
I - Training: 
	I - Batch: 50 | Loss: 1.462 | Acc: 98.625% | Wgt Acc: 98.562%
	I - Batch: 100 | Loss: 1.450 | Acc: 98.500% | Wgt Acc: 98.395%
	I - Batch: 150 | Loss: 1.442 | Acc: 98.583% | Wgt Acc: 98.497%
I - num batch: 160
I - Train -- Loss: 1.443 | Acc: 98.547% | Wgt Acc: 98.470% | LR: 1.250000e-04 | Dur: 153.94s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   2.]
 [  0. 556.   4.   0.]
 [  2.  20. 729.   1.]
 [  5.   2.   1. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.253 | Acc: 58.410% | Wgt Acc: 56.658% | Dur: 15.60s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  8.  8. 26.]
 [ 1. 21.  7.  2.]
 [ 2. 45. 52.  9.]
 [16.  4.  8. 49.]]

I - Epoch: 199
I - Training: 
	I - Batch: 50 | Loss: 1.430 | Acc: 98.750% | Wgt Acc: 98.648%
	I - Batch: 100 | Loss: 1.425 | Acc: 98.875% | Wgt Acc: 98.788%
	I - Batch: 150 | Loss: 1.449 | Acc: 98.083% | Wgt Acc: 97.998%
I - num batch: 160
I - Train -- Loss: 1.452 | Acc: 97.998% | Wgt Acc: 97.903% | LR: 1.250000e-04 | Dur: 158.19s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 550.  13.   0.]
 [  2.  27. 720.   3.]
 [  2.   1.   1. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.213 | Acc: 59.327% | Wgt Acc: 58.424% | Dur: 15.75s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  7.  4.  8.]
 [ 0. 12.  1.  0.]
 [ 5. 52. 57.  6.]
 [30.  7. 13. 72.]]

I - Epoch: 200
I - Training: 
	I - Batch: 50 | Loss: 1.497 | Acc: 97.500% | Wgt Acc: 97.321%
	I - Batch: 100 | Loss: 1.462 | Acc: 98.312% | Wgt Acc: 98.213%
	I - Batch: 150 | Loss: 1.453 | Acc: 98.500% | Wgt Acc: 98.394%
I - num batch: 160
I - Train -- Loss: 1.456 | Acc: 98.508% | Wgt Acc: 98.408% | LR: 1.250000e-04 | Dur: 156.26s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   1.   1.   4.]
 [  0. 557.   3.   0.]
 [  2.  18. 729.   3.]
 [  3.   2.   1. 531.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.231 | Acc: 59.327% | Wgt Acc: 58.696% | Dur: 16.37s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  2.  3.  3.]
 [ 1. 32. 12.  2.]
 [ 8. 39. 53. 25.]
 [26.  5.  7. 56.]]

I - Epoch: 201
I - Training: 
	I - Batch: 50 | Loss: 1.481 | Acc: 98.250% | Wgt Acc: 98.168%
	I - Batch: 100 | Loss: 1.481 | Acc: 98.125% | Wgt Acc: 97.998%
	I - Batch: 150 | Loss: 1.464 | Acc: 98.042% | Wgt Acc: 97.897%
I - num batch: 160
I - Train -- Loss: 1.465 | Acc: 98.115% | Wgt Acc: 97.974% | LR: 1.250000e-04 | Dur: 154.68s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   4.]
 [  0. 546.   2.   0.]
 [  2.  30. 730.   1.]
 [  5.   2.   2. 533.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.224 | Acc: 59.021% | Wgt Acc: 57.541% | Dur: 15.80s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  6. 20.]
 [ 0. 17.  2.  0.]
 [ 4. 49. 55.  8.]
 [21.  8. 12. 58.]]

I - Epoch: 202
I - Training: 
	I - Batch: 50 | Loss: 1.426 | Acc: 99.375% | Wgt Acc: 99.354%
	I - Batch: 100 | Loss: 1.439 | Acc: 99.125% | Wgt Acc: 99.072%
	I - Batch: 150 | Loss: 1.429 | Acc: 99.083% | Wgt Acc: 99.024%
I - num batch: 160
I - Train -- Loss: 1.433 | Acc: 99.018% | Wgt Acc: 98.965% | LR: 1.250000e-04 | Dur: 153.96s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   2.]
 [  0. 564.   1.   0.]
 [  2.  12. 732.   1.]
 [  4.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.254 | Acc: 56.881% | Wgt Acc: 55.163% | Dur: 15.50s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  4. 26.]
 [ 0. 23.  7.  1.]
 [ 8. 45. 55. 14.]
 [17.  5.  9. 45.]]

I - Epoch: 203
I - Training: 
	I - Batch: 50 | Loss: 1.415 | Acc: 99.250% | Wgt Acc: 99.239%
	I - Batch: 100 | Loss: 1.433 | Acc: 99.125% | Wgt Acc: 99.072%
	I - Batch: 150 | Loss: 1.435 | Acc: 99.000% | Wgt Acc: 98.939%
I - num batch: 160
I - Train -- Loss: 1.435 | Acc: 99.018% | Wgt Acc: 98.956% | LR: 1.250000e-04 | Dur: 155.24s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.]
 [  0. 563.   2.   0.]
 [  1.  13. 732.   1.]
 [  4.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.263 | Acc: 58.104% | Wgt Acc: 56.590% | Dur: 15.58s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  4.  7. 21.]
 [ 1. 26. 10.  2.]
 [ 9. 45. 51. 16.]
 [12.  3.  7. 47.]]

I - Epoch: 204
I - Training: 
	I - Batch: 50 | Loss: 1.387 | Acc: 99.375% | Wgt Acc: 99.353%
	I - Batch: 100 | Loss: 1.412 | Acc: 99.438% | Wgt Acc: 99.394%
	I - Batch: 150 | Loss: 1.420 | Acc: 99.292% | Wgt Acc: 99.249%
I - num batch: 160
I - Train -- Loss: 1.418 | Acc: 99.333% | Wgt Acc: 99.292% | LR: 1.250000e-04 | Dur: 155.80s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 567.   2.   0.]
 [  1.   9. 731.   1.]
 [  1.   2.   1. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.298 | Acc: 58.716% | Wgt Acc: 56.046% | Dur: 15.59s
I - Confusion Matrix: [row->prediction - col->label]
[[78.  4.  7. 32.]
 [ 0. 18.  5.  2.]
 [ 2. 54. 57. 13.]
 [ 8.  2.  6. 39.]]

I - Epoch: 205
I - Training: 
	I - Batch: 50 | Loss: 1.437 | Acc: 99.000% | Wgt Acc: 98.932%
	I - Batch: 100 | Loss: 1.433 | Acc: 99.125% | Wgt Acc: 99.056%
	I - Batch: 150 | Loss: 1.414 | Acc: 99.375% | Wgt Acc: 99.324%
I - num batch: 160
I - Train -- Loss: 1.432 | Acc: 99.293% | Wgt Acc: 99.257% | LR: 1.250000e-04 | Dur: 157.24s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.]
 [  0. 567.   2.   0.]
 [  3.   9. 732.   1.]
 [  1.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.216 | Acc: 61.774% | Wgt Acc: 60.054% | Dur: 15.55s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  6. 18.]
 [ 1. 20.  5.  1.]
 [ 6. 48. 57. 11.]
 [12.  5.  7. 56.]]

I - Epoch: 206
I - Training: 
	I - Batch: 50 | Loss: 1.561 | Acc: 94.750% | Wgt Acc: 94.487%
	I - Batch: 100 | Loss: 1.515 | Acc: 96.250% | Wgt Acc: 96.063%
	I - Batch: 150 | Loss: 1.503 | Acc: 96.542% | Wgt Acc: 96.348%
I - num batch: 160
I - Train -- Loss: 1.499 | Acc: 96.741% | Wgt Acc: 96.559% | LR: 1.250000e-04 | Dur: 125.64s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   2.   5.   6.]
 [  0. 531.   8.   0.]
 [  3.  43. 717.   4.]
 [  6.   2.   4. 528.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.233 | Acc: 58.716% | Wgt Acc: 57.133% | Dur: 15.97s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  8.  5. 18.]
 [ 0. 21.  3.  0.]
 [ 2. 43. 56. 16.]
 [23.  6. 11. 52.]]

I - Epoch: 207
I - Training: 
	I - Batch: 50 | Loss: 1.460 | Acc: 98.375% | Wgt Acc: 98.279%
	I - Batch: 100 | Loss: 1.458 | Acc: 98.312% | Wgt Acc: 98.228%
	I - Batch: 150 | Loss: 1.449 | Acc: 98.458% | Wgt Acc: 98.385%
I - num batch: 160
I - Train -- Loss: 1.447 | Acc: 98.508% | Wgt Acc: 98.443% | LR: 1.250000e-04 | Dur: 148.76s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   1.   1.   2.]
 [  0. 557.   7.   0.]
 [  1.  18. 726.   1.]
 [  5.   2.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.234 | Acc: 57.492% | Wgt Acc: 55.910% | Dur: 10.13s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  5.  3. 12.]
 [ 1. 14.  4.  0.]
 [11. 57. 59. 17.]
 [18.  2.  9. 57.]]

I - Epoch: 208
I - Training: 
	I - Batch: 50 | Loss: 1.424 | Acc: 99.125% | Wgt Acc: 99.045%
	I - Batch: 100 | Loss: 1.425 | Acc: 99.188% | Wgt Acc: 99.114%
	I - Batch: 150 | Loss: 1.428 | Acc: 99.333% | Wgt Acc: 99.277%
I - num batch: 160
I - Train -- Loss: 1.426 | Acc: 99.293% | Wgt Acc: 99.230% | LR: 1.250000e-04 | Dur: 101.76s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 565.   1.   0.]
 [  1.  11. 733.   1.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.182 | Acc: 63.609% | Wgt Acc: 61.753% | Dur: 9.96s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  8.  7. 21.]
 [ 0. 15.  0.  0.]
 [ 0. 46. 57.  3.]
 [14.  9. 11. 62.]]

I - Epoch: 209
I - Training: 
	I - Batch: 50 | Loss: 1.418 | Acc: 99.750% | Wgt Acc: 99.720%
	I - Batch: 100 | Loss: 1.425 | Acc: 99.438% | Wgt Acc: 99.380%
	I - Batch: 150 | Loss: 1.424 | Acc: 99.458% | Wgt Acc: 99.427%
I - num batch: 160
I - Train -- Loss: 1.420 | Acc: 99.490% | Wgt Acc: 99.460% | LR: 1.250000e-04 | Dur: 100.78s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.]
 [  0. 571.   1.   0.]
 [  1.   5. 733.   2.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.238 | Acc: 58.716% | Wgt Acc: 57.473% | Dur: 9.95s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  7.  5. 16.]
 [ 1. 21.  9.  1.]
 [ 7. 48. 54. 12.]
 [20.  2.  7. 57.]]

I - Epoch: 210
I - Training: 
	I - Batch: 50 | Loss: 1.398 | Acc: 99.000% | Wgt Acc: 98.985%
	I - Batch: 100 | Loss: 1.407 | Acc: 99.000% | Wgt Acc: 98.942%
	I - Batch: 150 | Loss: 1.430 | Acc: 99.042% | Wgt Acc: 98.986%
I - num batch: 160
I - Train -- Loss: 1.428 | Acc: 99.097% | Wgt Acc: 99.045% | LR: 1.250000e-04 | Dur: 104.05s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   1.   1.]
 [  0. 565.   3.   0.]
 [  1.  11. 730.   2.]
 [  2.   1.   0. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.201 | Acc: 59.021% | Wgt Acc: 58.492% | Dur: 10.51s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  3.  3.  5.]
 [ 1. 20.  9.  1.]
 [ 4. 49. 51. 11.]
 [30.  6. 12. 69.]]

I - Epoch: 211
I - Training: 
	I - Batch: 50 | Loss: 1.423 | Acc: 99.500% | Wgt Acc: 99.465%
	I - Batch: 100 | Loss: 1.418 | Acc: 99.250% | Wgt Acc: 99.183%
	I - Batch: 150 | Loss: 1.420 | Acc: 99.333% | Wgt Acc: 99.267%
I - num batch: 160
I - Train -- Loss: 1.421 | Acc: 99.333% | Wgt Acc: 99.266% | LR: 1.250000e-04 | Dur: 102.08s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 565.   0.   0.]
 [  1.  11. 734.   1.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.241 | Acc: 61.468% | Wgt Acc: 60.122% | Dur: 10.16s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  5.  5. 15.]
 [ 0. 21.  6.  0.]
 [ 3. 47. 53. 11.]
 [18.  5. 11. 60.]]

I - Epoch: 212
I - Training: 
	I - Batch: 50 | Loss: 1.444 | Acc: 99.500% | Wgt Acc: 99.497%
	I - Batch: 100 | Loss: 1.421 | Acc: 99.500% | Wgt Acc: 99.479%
	I - Batch: 150 | Loss: 1.423 | Acc: 99.417% | Wgt Acc: 99.380%
I - num batch: 160
I - Train -- Loss: 1.425 | Acc: 99.372% | Wgt Acc: 99.337% | LR: 1.250000e-04 | Dur: 102.58s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.]
 [  0. 570.   0.   0.]
 [  2.   6. 733.   1.]
 [  2.   2.   1. 535.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.209 | Acc: 59.939% | Wgt Acc: 59.103% | Dur: 10.12s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  2.  3. 12.]
 [ 0. 23.  6.  1.]
 [ 3. 47. 52. 10.]
 [27.  6. 14. 63.]]

I - Epoch: 213
I - Training: 
	I - Batch: 50 | Loss: 1.403 | Acc: 99.125% | Wgt Acc: 99.068%
	I - Batch: 100 | Loss: 1.403 | Acc: 99.312% | Wgt Acc: 99.253%
	I - Batch: 150 | Loss: 1.420 | Acc: 99.292% | Wgt Acc: 99.230%
I - num batch: 160
I - Train -- Loss: 1.418 | Acc: 99.333% | Wgt Acc: 99.275% | LR: 1.250000e-04 | Dur: 102.25s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.]
 [  0. 566.   1.   0.]
 [  1.  10. 733.   1.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.272 | Acc: 59.939% | Wgt Acc: 57.948% | Dur: 10.18s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  7.  3. 20.]
 [ 0. 20.  6.  0.]
 [ 6. 47. 59. 17.]
 [14.  4.  7. 49.]]

I - Epoch: 214
I - Training: 
	I - Batch: 50 | Loss: 1.453 | Acc: 99.750% | Wgt Acc: 99.719%
	I - Batch: 100 | Loss: 1.429 | Acc: 99.438% | Wgt Acc: 99.394%
	I - Batch: 150 | Loss: 1.415 | Acc: 99.542% | Wgt Acc: 99.502%
I - num batch: 160
I - Train -- Loss: 1.414 | Acc: 99.568% | Wgt Acc: 99.531% | LR: 1.250000e-04 | Dur: 102.93s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 570.   0.   0.]
 [  1.   6. 734.   1.]
 [  1.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.182 | Acc: 56.575% | Wgt Acc: 55.299% | Dur: 10.66s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  6.  1. 13.]
 [ 0. 14.  4.  0.]
 [ 5. 53. 54. 13.]
 [26.  5. 16. 60.]]

I - Epoch: 215
I - Training: 
	I - Batch: 50 | Loss: 1.440 | Acc: 98.125% | Wgt Acc: 97.958%
	I - Batch: 100 | Loss: 1.418 | Acc: 98.875% | Wgt Acc: 98.779%
	I - Batch: 150 | Loss: 1.416 | Acc: 99.042% | Wgt Acc: 98.950%
I - num batch: 160
I - Train -- Loss: 1.417 | Acc: 99.058% | Wgt Acc: 98.965% | LR: 1.250000e-04 | Dur: 102.90s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 559.   0.   0.]
 [  1.  17. 734.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.256 | Acc: 58.716% | Wgt Acc: 57.065% | Dur: 10.14s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  6.  3. 15.]
 [ 0. 19.  5.  1.]
 [10. 50. 57. 17.]
 [15.  3. 10. 53.]]

I - Epoch: 216
I - Training: 
	I - Batch: 50 | Loss: 1.398 | Acc: 99.750% | Wgt Acc: 99.744%
	I - Batch: 100 | Loss: 1.402 | Acc: 99.562% | Wgt Acc: 99.534%
	I - Batch: 150 | Loss: 1.422 | Acc: 99.500% | Wgt Acc: 99.465%
I - num batch: 160
I - Train -- Loss: 1.419 | Acc: 99.490% | Wgt Acc: 99.452% | LR: 1.250000e-04 | Dur: 111.14s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  1. 570.   0.   0.]
 [  1.   6. 734.   1.]
 [  1.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.253 | Acc: 55.657% | Wgt Acc: 53.397% | Dur: 11.37s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  4.  6. 25.]
 [ 1. 20. 10.  0.]
 [ 4. 50. 52. 23.]
 [11.  4.  7. 38.]]

I - Epoch: 217
I - Training: 
	I - Batch: 50 | Loss: 1.401 | Acc: 99.500% | Wgt Acc: 99.492%
	I - Batch: 100 | Loss: 1.406 | Acc: 99.000% | Wgt Acc: 98.924%
	I - Batch: 150 | Loss: 1.425 | Acc: 98.917% | Wgt Acc: 98.826%
I - num batch: 160
I - Train -- Loss: 1.437 | Acc: 98.940% | Wgt Acc: 98.850% | LR: 1.250000e-04 | Dur: 107.45s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.]
 [  0. 558.   1.   0.]
 [  1.  19. 733.   2.]
 [  3.   1.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.222 | Acc: 62.691% | Wgt Acc: 61.617% | Dur: 11.33s
I - Confusion Matrix: [row->prediction - col->label]
[[72. 12.  9. 22.]
 [ 0. 30. 13.  1.]
 [ 3. 34. 46.  6.]
 [13.  2.  7. 57.]]

I - Epoch: 218
I - Training: 
	I - Batch: 50 | Loss: 1.412 | Acc: 98.375% | Wgt Acc: 98.195%
	I - Batch: 100 | Loss: 1.422 | Acc: 99.062% | Wgt Acc: 98.975%
	I - Batch: 150 | Loss: 1.410 | Acc: 99.125% | Wgt Acc: 99.043%
I - num batch: 160
I - Train -- Loss: 1.418 | Acc: 99.097% | Wgt Acc: 99.009% | LR: 1.250000e-04 | Dur: 109.79s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 559.   1.   0.]
 [  1.  18. 733.   1.]
 [  1.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.231 | Acc: 57.492% | Wgt Acc: 56.046% | Dur: 10.40s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  7.  6. 17.]
 [ 0. 17.  7.  0.]
 [ 6. 48. 52. 13.]
 [19.  6. 10. 56.]]

I - Epoch: 219
I - Training: 
	I - Batch: 50 | Loss: 1.477 | Acc: 97.250% | Wgt Acc: 97.038%
	I - Batch: 100 | Loss: 1.475 | Acc: 98.125% | Wgt Acc: 98.003%
	I - Batch: 150 | Loss: 1.447 | Acc: 98.417% | Wgt Acc: 98.301%
I - num batch: 160
I - Train -- Loss: 1.442 | Acc: 98.508% | Wgt Acc: 98.399% | LR: 1.250000e-04 | Dur: 108.36s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   1.   0.   3.]
 [  0. 553.   1.   0.]
 [  2.  23. 733.   1.]
 [  6.   1.   0. 534.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.196 | Acc: 57.798% | Wgt Acc: 56.590% | Dur: 11.68s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  5.  8. 23.]
 [ 0. 19.  8.  0.]
 [ 2. 45. 46.  5.]
 [20.  9. 13. 58.]]

I - Epoch: 220
I - Training: 
	I - Batch: 50 | Loss: 1.432 | Acc: 98.375% | Wgt Acc: 98.201%
	I - Batch: 100 | Loss: 1.415 | Acc: 99.062% | Wgt Acc: 98.958%
	I - Batch: 150 | Loss: 1.418 | Acc: 99.000% | Wgt Acc: 98.892%
I - num batch: 160
I - Train -- Loss: 1.413 | Acc: 99.018% | Wgt Acc: 98.912% | LR: 1.250000e-04 | Dur: 101.41s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 556.   0.   0.]
 [  1.  21. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.248 | Acc: 58.716% | Wgt Acc: 57.201% | Dur: 10.02s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  8.  2. 17.]
 [ 1. 24.  7.  0.]
 [ 6. 43. 57. 19.]
 [20.  3.  9. 50.]]

I - Epoch: 221
I - Training: 
	I - Batch: 50 | Loss: 1.411 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 1.431 | Acc: 99.438% | Wgt Acc: 99.395%
	I - Batch: 150 | Loss: 1.422 | Acc: 99.500% | Wgt Acc: 99.465%
I - num batch: 160
I - Train -- Loss: 1.415 | Acc: 99.529% | Wgt Acc: 99.496% | LR: 1.250000e-04 | Dur: 102.87s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 571.   0.   0.]
 [  1.   5. 734.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.191 | Acc: 59.633% | Wgt Acc: 58.356% | Dur: 9.90s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  2.  3. 19.]
 [ 1. 27.  9.  0.]
 [ 7. 43. 54. 15.]
 [18.  6.  9. 52.]]

I - Epoch: 222
I - Training: 
	I - Batch: 50 | Loss: 1.375 | Acc: 99.750% | Wgt Acc: 99.717%
	I - Batch: 100 | Loss: 1.393 | Acc: 99.500% | Wgt Acc: 99.463%
	I - Batch: 150 | Loss: 1.410 | Acc: 99.458% | Wgt Acc: 99.408%
I - num batch: 160
I - Train -- Loss: 1.410 | Acc: 99.450% | Wgt Acc: 99.398% | LR: 1.250000e-04 | Dur: 109.78s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 567.   0.   0.]
 [  1.  11. 734.   1.]
 [  1.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.208 | Acc: 58.104% | Wgt Acc: 56.454% | Dur: 12.60s
I - Confusion Matrix: [row->prediction - col->label]
[[75. 10. 10. 29.]
 [ 0. 24. 11.  0.]
 [ 5. 35. 44. 10.]
 [ 8.  9. 10. 47.]]

I - Epoch: 223
I - Training: 
	I - Batch: 50 | Loss: 1.395 | Acc: 99.750% | Wgt Acc: 99.745%
	I - Batch: 100 | Loss: 1.412 | Acc: 99.500% | Wgt Acc: 99.465%
	I - Batch: 150 | Loss: 1.413 | Acc: 99.458% | Wgt Acc: 99.427%
I - num batch: 160
I - Train -- Loss: 1.413 | Acc: 99.450% | Wgt Acc: 99.416% | LR: 1.250000e-04 | Dur: 113.49s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.]
 [  0. 570.   1.   0.]
 [  1.   6. 733.   1.]
 [  2.   2.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.264 | Acc: 58.716% | Wgt Acc: 56.861% | Dur: 10.02s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  3. 26.]
 [ 1. 25.  9.  1.]
 [ 7. 41. 53. 15.]
 [10.  5. 10. 44.]]

I - Epoch: 224
I - Training: 
	I - Batch: 50 | Loss: 1.399 | Acc: 99.625% | Wgt Acc: 99.577%
	I - Batch: 100 | Loss: 1.388 | Acc: 99.750% | Wgt Acc: 99.718%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.542% | Wgt Acc: 99.502%
I - num batch: 160
I - Train -- Loss: 1.412 | Acc: 99.490% | Wgt Acc: 99.443% | LR: 1.250000e-04 | Dur: 100.63s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 568.   0.   0.]
 [  1.   9. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.238 | Acc: 58.104% | Wgt Acc: 56.114% | Dur: 10.05s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  7. 24.]
 [ 0. 18.  4.  0.]
 [ 5. 50. 54. 14.]
 [13.  3. 10. 48.]]

I - Epoch: 225
I - Training: 
	I - Batch: 50 | Loss: 1.434 | Acc: 99.500% | Wgt Acc: 99.493%
	I - Batch: 100 | Loss: 1.419 | Acc: 99.688% | Wgt Acc: 99.676%
	I - Batch: 150 | Loss: 1.403 | Acc: 99.792% | Wgt Acc: 99.784%
I - num batch: 160
I - Train -- Loss: 1.410 | Acc: 99.764% | Wgt Acc: 99.752% | LR: 1.250000e-04 | Dur: 100.77s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 575.   0.   0.]
 [  1.   1. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.219 | Acc: 56.881% | Wgt Acc: 55.503% | Dur: 10.00s
I - Confusion Matrix: [row->prediction - col->label]
[[70. 10.  8. 28.]
 [ 0. 22. 10.  1.]
 [ 1. 35. 43.  6.]
 [17. 11. 14. 51.]]

I - Epoch: 226
I - Training: 
	I - Batch: 50 | Loss: 1.446 | Acc: 99.250% | Wgt Acc: 99.215%
	I - Batch: 100 | Loss: 1.413 | Acc: 99.625% | Wgt Acc: 99.606%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.750% | Wgt Acc: 99.737%
I - num batch: 160
I - Train -- Loss: 1.406 | Acc: 99.764% | Wgt Acc: 99.752% | LR: 1.250000e-04 | Dur: 100.83s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   2.   0.   0.]
 [  0. 575.   0.   0.]
 [  1.   1. 734.   1.]
 [  1.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.235 | Acc: 57.798% | Wgt Acc: 56.454% | Dur: 10.04s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  6.  5. 15.]
 [ 0. 15.  5.  0.]
 [ 7. 50. 53. 11.]
 [20.  7. 12. 60.]]

I - Epoch: 227
I - Training: 
	I - Batch: 50 | Loss: 1.408 | Acc: 99.750% | Wgt Acc: 99.718%
	I - Batch: 100 | Loss: 1.411 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.708% | Wgt Acc: 99.700%
I - num batch: 160
I - Train -- Loss: 1.412 | Acc: 99.529% | Wgt Acc: 99.496% | LR: 1.250000e-04 | Dur: 100.91s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 570.   0.   0.]
 [  2.   6. 733.   1.]
 [  0.   1.   1. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.202 | Acc: 58.410% | Wgt Acc: 56.590% | Dur: 9.96s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  7.  4. 22.]
 [ 0. 18.  4.  0.]
 [ 2. 46. 52. 13.]
 [16.  7. 15. 51.]]

I - Epoch: 228
I - Training: 
	I - Batch: 50 | Loss: 1.408 | Acc: 99.500% | Wgt Acc: 99.462%
	I - Batch: 100 | Loss: 1.395 | Acc: 99.562% | Wgt Acc: 99.519%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.542% | Wgt Acc: 99.502%
I - num batch: 160
I - Train -- Loss: 1.411 | Acc: 99.568% | Wgt Acc: 99.531% | LR: 1.250000e-04 | Dur: 100.28s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   2.   0.   0.]
 [  0. 570.   0.   0.]
 [  1.   6. 734.   1.]
 [  1.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.221 | Acc: 58.716% | Wgt Acc: 57.541% | Dur: 9.93s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  7.  8. 13.]
 [ 0. 17.  5.  0.]
 [ 4. 47. 52. 11.]
 [23.  7. 10. 62.]]

I - Epoch: 229
I - Training: 
	I - Batch: 50 | Loss: 1.408 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 100 | Loss: 1.420 | Acc: 99.375% | Wgt Acc: 99.324%
	I - Batch: 150 | Loss: 1.409 | Acc: 99.542% | Wgt Acc: 99.502%
I - num batch: 160
I - Train -- Loss: 1.408 | Acc: 99.529% | Wgt Acc: 99.487% | LR: 1.250000e-04 | Dur: 100.01s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 569.   0.   0.]
 [  1.   7. 734.   1.]
 [  1.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.203 | Acc: 57.492% | Wgt Acc: 56.114% | Dur: 9.92s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  5.  5. 15.]
 [ 0. 19.  6.  1.]
 [10. 49. 55. 15.]
 [19.  5.  9. 55.]]

I - Epoch: 230
I - Training: 
	I - Batch: 50 | Loss: 1.412 | Acc: 99.500% | Wgt Acc: 99.464%
	I - Batch: 100 | Loss: 1.414 | Acc: 99.625% | Wgt Acc: 99.606%
	I - Batch: 150 | Loss: 1.409 | Acc: 99.708% | Wgt Acc: 99.690%
I - num batch: 160
I - Train -- Loss: 1.409 | Acc: 99.725% | Wgt Acc: 99.708% | LR: 1.250000e-04 | Dur: 100.21s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 574.   0.   0.]
 [  2.   3. 734.   1.]
 [  0.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.213 | Acc: 60.550% | Wgt Acc: 58.832% | Dur: 10.05s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  8.  7. 26.]
 [ 0. 25. 10.  0.]
 [ 4. 38. 50. 11.]
 [10.  7.  8. 49.]]

I - Epoch: 231
I - Training: 
	I - Batch: 50 | Loss: 1.382 | Acc: 98.875% | Wgt Acc: 98.755%
	I - Batch: 100 | Loss: 1.405 | Acc: 99.000% | Wgt Acc: 98.901%
	I - Batch: 150 | Loss: 1.410 | Acc: 99.250% | Wgt Acc: 99.174%
I - num batch: 160
I - Train -- Loss: 1.407 | Acc: 99.293% | Wgt Acc: 99.222% | LR: 1.250000e-04 | Dur: 100.80s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 563.   0.   0.]
 [  1.  14. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.203 | Acc: 59.327% | Wgt Acc: 57.337% | Dur: 10.02s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  7.  7. 29.]
 [ 1. 22.  7.  1.]
 [ 3. 45. 52. 10.]
 [10.  4.  9. 46.]]

I - Epoch: 232
I - Training: 
	I - Batch: 50 | Loss: 1.380 | Acc: 100.000% | Wgt Acc: 100.000%
	I - Batch: 100 | Loss: 1.409 | Acc: 99.312% | Wgt Acc: 99.253%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.000% | Wgt Acc: 98.893%
I - num batch: 160
I - Train -- Loss: 1.406 | Acc: 98.979% | Wgt Acc: 98.868% | LR: 1.250000e-04 | Dur: 116.70s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 555.   0.   0.]
 [  1.  22. 734.   1.]
 [  1.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.251 | Acc: 58.410% | Wgt Acc: 56.182% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[74. 10.  7. 26.]
 [ 0. 18.  7.  0.]
 [ 5. 45. 54. 15.]
 [ 9.  5.  7. 45.]]

I - Epoch: 233
I - Training: 
	I - Batch: 50 | Loss: 1.406 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 1.411 | Acc: 99.188% | Wgt Acc: 99.112%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.292% | Wgt Acc: 99.221%
I - num batch: 160
I - Train -- Loss: 1.406 | Acc: 99.333% | Wgt Acc: 99.266% | LR: 1.250000e-04 | Dur: 120.29s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   2.   0.   0.]
 [  0. 564.   0.   0.]
 [  1.  12. 734.   1.]
 [  1.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.241 | Acc: 56.269% | Wgt Acc: 54.348% | Dur: 10.12s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  2. 20.]
 [ 1. 17.  7.  0.]
 [ 7. 51. 55. 19.]
 [15.  5. 11. 47.]]

I - Epoch: 234
I - Training: 
	I - Batch: 50 | Loss: 1.367 | Acc: 99.625% | Wgt Acc: 99.576%
	I - Batch: 100 | Loss: 1.413 | Acc: 99.500% | Wgt Acc: 99.450%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.375% | Wgt Acc: 99.314%
I - num batch: 160
I - Train -- Loss: 1.411 | Acc: 99.411% | Wgt Acc: 99.354% | LR: 1.250000e-04 | Dur: 101.32s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 566.   0.   0.]
 [  1.  10. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.223 | Acc: 61.468% | Wgt Acc: 59.986% | Dur: 10.07s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  7.  4. 16.]
 [ 1. 25.  8.  0.]
 [ 7. 43. 57. 16.]
 [15.  3.  6. 54.]]

I - Epoch: 235
I - Training: 
	I - Batch: 50 | Loss: 1.379 | Acc: 99.750% | Wgt Acc: 99.745%
	I - Batch: 100 | Loss: 1.411 | Acc: 99.500% | Wgt Acc: 99.464%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.500% | Wgt Acc: 99.456%
I - num batch: 160
I - Train -- Loss: 1.410 | Acc: 99.450% | Wgt Acc: 99.398% | LR: 1.250000e-04 | Dur: 101.27s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   2.   0.   0.]
 [  0. 567.   0.   0.]
 [  1.   9. 734.   1.]
 [  1.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.210 | Acc: 59.327% | Wgt Acc: 58.084% | Dur: 10.09s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  7.  7. 18.]
 [ 0. 22.  8.  0.]
 [ 4. 43. 48. 11.]
 [17.  6. 12. 57.]]

I - Epoch: 236
I - Training: 
	I - Batch: 50 | Loss: 1.407 | Acc: 99.750% | Wgt Acc: 99.748%
	I - Batch: 100 | Loss: 1.418 | Acc: 99.688% | Wgt Acc: 99.677%
	I - Batch: 150 | Loss: 1.407 | Acc: 99.750% | Wgt Acc: 99.737%
I - num batch: 160
I - Train -- Loss: 1.409 | Acc: 99.686% | Wgt Acc: 99.664% | LR: 1.250000e-04 | Dur: 141.97s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 573.   0.   0.]
 [  2.   3. 734.   1.]
 [  0.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.229 | Acc: 58.410% | Wgt Acc: 56.590% | Dur: 17.00s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  4.  6. 23.]
 [ 0. 17.  7.  0.]
 [ 3. 49. 53. 11.]
 [16.  8.  9. 52.]]

I - Epoch: 237
I - Training: 
	I - Batch: 50 | Loss: 1.398 | Acc: 99.750% | Wgt Acc: 99.747%
	I - Batch: 100 | Loss: 1.434 | Acc: 99.625% | Wgt Acc: 99.592%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.708% | Wgt Acc: 99.681%
I - num batch: 160
I - Train -- Loss: 1.412 | Acc: 99.607% | Wgt Acc: 99.575% | LR: 1.250000e-04 | Dur: 150.10s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 571.   0.   0.]
 [  1.   7. 734.   1.]
 [  1.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.215 | Acc: 60.245% | Wgt Acc: 58.356% | Dur: 9.87s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  7.  7. 24.]
 [ 0. 14.  2.  0.]
 [ 2. 51. 55.  5.]
 [15.  6. 11. 57.]]

I - Epoch: 238
I - Training: 
	I - Batch: 50 | Loss: 1.402 | Acc: 98.750% | Wgt Acc: 98.618%
	I - Batch: 100 | Loss: 1.411 | Acc: 99.125% | Wgt Acc: 99.040%
	I - Batch: 150 | Loss: 1.405 | Acc: 99.333% | Wgt Acc: 99.267%
I - num batch: 160
I - Train -- Loss: 1.408 | Acc: 99.097% | Wgt Acc: 99.000% | LR: 1.250000e-04 | Dur: 124.57s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 558.   0.   0.]
 [  2.  19. 734.   1.]
 [  0.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.204 | Acc: 57.492% | Wgt Acc: 55.910% | Dur: 15.96s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  5.  7. 21.]
 [ 1. 17.  7.  0.]
 [ 5. 48. 54. 11.]
 [19.  8.  7. 54.]]

I - Epoch: 239
I - Training: 
	I - Batch: 50 | Loss: 1.384 | Acc: 99.875% | Wgt Acc: 99.859%
	I - Batch: 100 | Loss: 1.401 | Acc: 99.875% | Wgt Acc: 99.860%
	I - Batch: 150 | Loss: 1.410 | Acc: 99.708% | Wgt Acc: 99.690%
I - num batch: 160
I - Train -- Loss: 1.407 | Acc: 99.725% | Wgt Acc: 99.708% | LR: 1.250000e-04 | Dur: 140.93s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 574.   0.   0.]
 [  2.   2. 734.   1.]
 [  0.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.211 | Acc: 57.187% | Wgt Acc: 55.910% | Dur: 9.69s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  5. 10. 26.]
 [ 0. 24.  9.  0.]
 [ 2. 41. 42.  9.]
 [16.  8. 14. 51.]]

I - Epoch: 240
I - Training: 
	I - Batch: 50 | Loss: 1.378 | Acc: 99.625% | Wgt Acc: 99.576%
	I - Batch: 100 | Loss: 1.408 | Acc: 99.000% | Wgt Acc: 98.888%
	I - Batch: 150 | Loss: 1.410 | Acc: 99.208% | Wgt Acc: 99.127%
I - num batch: 160
I - Train -- Loss: 1.407 | Acc: 99.254% | Wgt Acc: 99.177% | LR: 1.250000e-04 | Dur: 108.91s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   2.   0.   0.]
 [  0. 562.   0.   0.]
 [  2.  14. 734.   1.]
 [  0.   0.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.188 | Acc: 61.774% | Wgt Acc: 60.122% | Dur: 9.89s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  3.  8. 17.]
 [ 0. 20.  7.  0.]
 [ 5. 48. 54. 12.]
 [12.  7.  6. 57.]]

I - Epoch: 241
I - Training: 
	I - Batch: 50 | Loss: 1.424 | Acc: 99.625% | Wgt Acc: 99.579%
	I - Batch: 100 | Loss: 1.411 | Acc: 99.625% | Wgt Acc: 99.578%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.625% | Wgt Acc: 99.596%
I - num batch: 160
I - Train -- Loss: 1.409 | Acc: 99.647% | Wgt Acc: 99.620% | LR: 1.250000e-04 | Dur: 103.64s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   2.   0.   1.]
 [  0. 573.   0.   0.]
 [  1.   3. 734.   1.]
 [  1.   0.   0. 536.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.190 | Acc: 61.162% | Wgt Acc: 60.462% | Dur: 10.18s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  7.  7. 22.]
 [ 1. 36. 22.  2.]
 [ 1. 30. 36.  8.]
 [12.  5. 10. 54.]]

I - Epoch: 242
I - Training: 
	I - Batch: 50 | Loss: 1.404 | Acc: 99.500% | Wgt Acc: 99.461%
	I - Batch: 100 | Loss: 1.389 | Acc: 99.625% | Wgt Acc: 99.591%
	I - Batch: 150 | Loss: 1.397 | Acc: 99.708% | Wgt Acc: 99.681%
I - num batch: 160
I - Train -- Loss: 1.409 | Acc: 99.333% | Wgt Acc: 99.266% | LR: 1.250000e-04 | Dur: 102.10s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 564.   0.   0.]
 [  2.  13. 734.   1.]
 [  0.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.195 | Acc: 58.410% | Wgt Acc: 56.929% | Dur: 10.23s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  5.  3. 14.]
 [ 0. 13.  7.  0.]
 [ 4. 51. 51. 11.]
 [18.  9. 14. 61.]]

I - Epoch: 243
I - Training: 
	I - Batch: 50 | Loss: 1.403 | Acc: 99.750% | Wgt Acc: 99.746%
	I - Batch: 100 | Loss: 1.420 | Acc: 99.562% | Wgt Acc: 99.522%
	I - Batch: 150 | Loss: 1.409 | Acc: 99.583% | Wgt Acc: 99.549%
I - num batch: 160
I - Train -- Loss: 1.405 | Acc: 99.607% | Wgt Acc: 99.575% | LR: 1.250000e-04 | Dur: 122.68s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 571.   0.   0.]
 [  1.   5. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.175 | Acc: 59.939% | Wgt Acc: 58.288% | Dur: 15.79s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  4. 16.]
 [ 1. 20.  6.  0.]
 [ 5. 46. 56. 16.]
 [16.  6.  9. 54.]]

I - Epoch: 244
I - Training: 
	I - Batch: 50 | Loss: 1.415 | Acc: 99.625% | Wgt Acc: 99.577%
	I - Batch: 100 | Loss: 1.418 | Acc: 99.562% | Wgt Acc: 99.521%
	I - Batch: 150 | Loss: 1.408 | Acc: 99.625% | Wgt Acc: 99.596%
I - num batch: 160
I - Train -- Loss: 1.410 | Acc: 99.568% | Wgt Acc: 99.531% | LR: 1.250000e-04 | Dur: 156.00s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 570.   0.   0.]
 [  1.   6. 734.   1.]
 [  1.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.225 | Acc: 57.492% | Wgt Acc: 55.707% | Dur: 16.28s
I - Confusion Matrix: [row->prediction - col->label]
[[68. 10.  9. 25.]
 [ 0. 15.  4.  0.]
 [ 2. 45. 52.  8.]
 [18.  8. 10. 53.]]

I - Epoch: 245
I - Training: 
	I - Batch: 50 | Loss: 1.439 | Acc: 99.500% | Wgt Acc: 99.492%
	I - Batch: 100 | Loss: 1.405 | Acc: 99.688% | Wgt Acc: 99.676%
	I - Batch: 150 | Loss: 1.410 | Acc: 99.708% | Wgt Acc: 99.700%
I - num batch: 160
I - Train -- Loss: 1.406 | Acc: 99.725% | Wgt Acc: 99.717% | LR: 1.250000e-04 | Dur: 152.75s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 575.   0.   0.]
 [  1.   3. 733.   1.]
 [  1.   0.   1. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.223 | Acc: 56.881% | Wgt Acc: 54.755% | Dur: 16.51s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  4.  8. 25.]
 [ 1. 19.  7.  0.]
 [ 8. 51. 56. 18.]
 [11.  4.  4. 43.]]

I - Epoch: 246
I - Training: 
	I - Batch: 50 | Loss: 1.444 | Acc: 99.250% | Wgt Acc: 99.186%
	I - Batch: 100 | Loss: 1.402 | Acc: 99.625% | Wgt Acc: 99.591%
	I - Batch: 150 | Loss: 1.402 | Acc: 99.667% | Wgt Acc: 99.634%
I - num batch: 160
I - Train -- Loss: 1.405 | Acc: 99.647% | Wgt Acc: 99.620% | LR: 1.250000e-04 | Dur: 155.29s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 572.   0.   0.]
 [  1.   4. 734.   1.]
 [  1.   2.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.289 | Acc: 58.716% | Wgt Acc: 56.590% | Dur: 15.91s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  4.  5. 19.]
 [ 0. 16.  7.  0.]
 [ 8. 55. 58. 18.]
 [11.  3.  5. 49.]]

I - Epoch: 247
I - Training: 
	I - Batch: 50 | Loss: 1.418 | Acc: 98.750% | Wgt Acc: 98.591%
	I - Batch: 100 | Loss: 1.417 | Acc: 99.188% | Wgt Acc: 99.084%
	I - Batch: 150 | Loss: 1.410 | Acc: 99.375% | Wgt Acc: 99.315%
I - num batch: 160
I - Train -- Loss: 1.405 | Acc: 99.411% | Wgt Acc: 99.354% | LR: 1.250000e-04 | Dur: 154.14s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 566.   0.   0.]
 [  1.  11. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.161 | Acc: 61.774% | Wgt Acc: 60.870% | Dur: 16.01s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  5.  3. 12.]
 [ 1. 20.  7.  0.]
 [ 3. 43. 52.  6.]
 [22. 10. 13. 68.]]

I - Epoch: 248
I - Training: 
	I - Batch: 50 | Loss: 1.406 | Acc: 99.250% | Wgt Acc: 99.153%
	I - Batch: 100 | Loss: 1.416 | Acc: 99.500% | Wgt Acc: 99.452%
	I - Batch: 150 | Loss: 1.405 | Acc: 99.625% | Wgt Acc: 99.596%
I - num batch: 160
I - Train -- Loss: 1.407 | Acc: 99.607% | Wgt Acc: 99.575% | LR: 1.250000e-04 | Dur: 151.40s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   0.   0.]
 [  0. 571.   0.   0.]
 [  1.   5. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.215 | Acc: 57.492% | Wgt Acc: 55.842% | Dur: 17.01s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  7.  7. 23.]
 [ 0. 13.  6.  0.]
 [ 3. 49. 50.  6.]
 [17.  9. 12. 57.]]

I - Epoch: 249
I - Training: 
	I - Batch: 50 | Loss: 1.370 | Acc: 99.625% | Wgt Acc: 99.631%
	I - Batch: 100 | Loss: 1.394 | Acc: 99.438% | Wgt Acc: 99.393%
	I - Batch: 150 | Loss: 1.406 | Acc: 99.542% | Wgt Acc: 99.502%
I - num batch: 160
I - Train -- Loss: 1.407 | Acc: 99.568% | Wgt Acc: 99.531% | LR: 1.250000e-04 | Dur: 156.72s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.]
 [  0. 570.   0.   0.]
 [  1.   7. 734.   1.]
 [  1.   1.   0. 537.]]

I - Validation: 
I - num batch: 21
I - Val -- Loss: 5.185 | Acc: 57.187% | Wgt Acc: 55.095% | Dur: 16.15s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  6.  8. 22.]
 [ 0. 15.  5.  0.]
 [ 6. 53. 55. 16.]
 [13.  4.  7. 48.]]

I - Maximum validation set accuracy in current training:  63.61
