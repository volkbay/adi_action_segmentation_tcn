Sun Oct 30 17:48:47 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.23, 0.24, 0.23, 0.25, 0.05], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'singleBackgroundPath': 'new_background', 'singleBackgroundPickle': True, 'tossFirstLastFrames': True}, 'dataPath': '/data_ssd/processed/kinetics400/', 'dropoutRate': 0.5, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat', 'background'], 'lastLayerInitUniform': False, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 71.20315581854044, 'maxValidationTrainNo': 64, 'modelVersion': 17, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 69, 'validationAccThr': 75, 'warmStartConfig': {'checkpointFile': './sav/model17_trainNo60_at_epoch_197_with_acc_71_60_checkpoint.pth.tar', 'checkpointModelNo': 17, 'freezeSpatialCNN': False, 'warmStartFlag': False}, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Train set length:  4550
I - Label distribution: [ 806.  780. 1124.  840. 1000.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test
I - Loading file: dataset_test_background00_no_samples180.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test/new_background
I - New label distribution: [ 88.  78.  75.  86. 180.]

I - Test set length:  507
I - Label distribution: [ 88.  78.  75.  86. 180.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(4) data number in dataset:  tensor([  5924,  48670,   7019, 209276,  25863,  21467, 177107,    598, 164763,
        139557,    198, 149252,  52045, 158969, 117511, 119571])
I - Initializing model TCNv17
I - Number of Model Parameters: 640096
I - Model output shape:  torch.Size([16, 5])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv17                                   [16, 5]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 128, 64, 64]         6
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Conv2d: 2-2                       --                        6,272
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─OutputShiftSqueeze: 2-4           --                        --
│    └─One: 2-5                          [1]                       --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─OutputScale: 2-7                  --                        --
│    └─Empty: 2-8                        [128, 48, 1, 1]           --
│    └─Empty: 2-9                        [128, 48, 1, 1]           --
│    └─Empty: 2-10                       [128]                     --
│    └─Empty: 2-11                       [128]                     --
│    └─BatchNorm2d: 2-12                 [16, 128, 64, 64]         --
│    └─Scaler: 2-13                      [16, 128, 64, 64]         --
│    └─ReLU: 2-14                        [16, 128, 64, 64]         --
│    └─Empty: 2-15                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Conv2d: 2-18                      --                        147,584
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         (recursive)
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-20                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-3          [16, 128, 32, 32]         147,846
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-22                   [16, 128, 32, 32]         --
│    └─Empty: 2-23                       [16, 128, 32, 32]         --
│    └─Empty: 2-24                       [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [128, 128, 3, 3]          --
│    └─Empty: 2-29                       [128, 128, 3, 3]          --
│    └─Empty: 2-30                       [128]                     --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-33                      --                        147,584
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         (recursive)
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-35                       [128]                     --
│    └─BatchNorm2d: 2-36                 [16, 128, 32, 32]         256
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-38                      [16, 128, 32, 32]         --
│    └─ReLU: 2-39                        [16, 128, 32, 32]         --
│    └─Empty: 2-40                       [16, 128, 32, 32]         --
│    └─Clamp: 2-41                       [16, 128, 32, 32]         --
├─Dropout2d: 1-5                         [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-6          [16, 128, 16, 16]         131,078
│    └─MaxPool2d: 2-42                   [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─Empty: 2-45                       [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Conv2d: 2-47                      --                        16,512
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         (recursive)
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-49                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-50          --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-52                         [1]                       --
│    └─OutputScale: 2-53                 --                        --
│    └─Empty: 2-54                       [128, 128, 3, 3]          --
│    └─Empty: 2-55                       [128, 128, 3, 3]          --
│    └─Empty: 2-56                       [128]                     --
│    └─Empty: 2-57                       [128]                     --
│    └─BatchNorm2d: 2-58                 [16, 128, 16, 16]         256
│    └─Scaler: 2-59                      [16, 128, 16, 16]         --
│    └─ReLU: 2-60                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-63                      --                        147,584
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         (recursive)
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-65                       [16, 128, 16, 16]         --
│    └─Clamp: 2-66                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-8                 [16, 128, 16, 16]         16,518
│    └─OutputShiftSqueeze: 2-68          --                        --
│    └─One: 2-69                         [1]                       --
│    └─OutputScale: 2-70                 --                        --
│    └─Empty: 2-71                       [128, 128, 1, 1]          --
│    └─Empty: 2-72                       [128, 128, 1, 1]          --
│    └─Empty: 2-73                       [128]                     --
│    └─Empty: 2-74                       [128]                     --
│    └─BatchNorm2d: 2-75                 [16, 128, 16, 16]         256
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-78                      --                        147,584
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           (recursive)
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-80                      [16, 128, 16, 16]         --
│    └─ReLU: 2-81                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-83                       [16, 128, 16, 16]         --
│    └─Clamp: 2-84                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-10         [16, 128, 16, 16]         145,526
│    └─MaxPool2d: 2-85                   [16, 128, 16, 16]         --
│    └─Empty: 2-86                       [16, 128, 16, 16]         --
│    └─Empty: 2-87                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-88          --                        --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-91                         [1]                       --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Conv2d: 2-93                      --                        2,064
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            (recursive)
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-95                 --                        --
│    └─Empty: 2-96                       [128, 128, 3, 3]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-98                       [128, 128, 3, 3]          --
│    └─Empty: 2-99                       [128]                     --
│    └─Empty: 2-100                      [128]                     --
│    └─BatchNorm2d: 2-101                [16, 128, 16, 16]         256
│    └─Scaler: 2-102                     [16, 128, 16, 16]         --
│    └─ReLU: 2-103                       [16, 128, 16, 16]         --
│    └─Empty: 2-104                      [16, 128, 16, 16]         --
│    └─Clamp: 2-105                      [16, 128, 16, 16]         --
├─Dropout2d: 1-11                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-108                     --                        18,448
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            (recursive)
├─FusedMaxPoolConv2dBNReLU: 1-13         [16, 128, 8, 8]           147,590
│    └─MaxPool2d: 2-110                  [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-112                      [16, 128, 8, 8]           --
│    └─Empty: 2-113                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-114         --                        --
│    └─One: 2-115                        [1]                       --
│    └─OutputScale: 2-116                --                        --
│    └─Empty: 2-117                      [128, 128, 3, 3]          --
│    └─Empty: 2-118                      [128, 128, 3, 3]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-121                      [128]                     --
│    └─Empty: 2-122                      [128]                     --
│    └─BatchNorm2d: 2-123                [16, 128, 8, 8]           256
│    └─Scaler: 2-124                     [16, 128, 8, 8]           --
│    └─ReLU: 2-125                       [16, 128, 8, 8]           --
│    └─Empty: 2-126                      [16, 128, 8, 8]           --
│    └─Clamp: 2-127                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-14                [16, 16, 8, 8]            2,070
├─Linear: 1                              --                        --
│    └─Scaler: 2-128                     --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-129         --                        --
│    └─One: 2-130                        [1]                       --
│    └─OutputScale: 2-131                --                        --
│    └─Empty: 2-132                      [16, 128, 1, 1]           --
│    └─Empty: 2-133                      [16, 128, 1, 1]           --
│    └─Empty: 2-134                      [16]                      --
│    └─Empty: 2-135                      [16]                      --
│    └─BatchNorm2d: 2-136                [16, 16, 8, 8]            32
│    └─Scaler: 2-137                     [16, 16, 8, 8]            --
│    └─ReLU: 2-138                       [16, 16, 8, 8]            --
│    └─Empty: 2-139                      [16, 16, 8, 8]            --
│    └─Clamp: 2-140                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 16, 8, 8]            18,454
│    └─MaxPool2d: 2-141                  [16, 128, 8, 8]           --
│    └─Empty: 2-142                      [16, 128, 8, 8]           --
│    └─Empty: 2-143                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-144         --                        --
│    └─One: 2-145                        [1]                       --
│    └─OutputScale: 2-146                --                        --
│    └─Empty: 2-147                      [16, 128, 3, 3]           --
│    └─Empty: 2-148                      [16, 128, 3, 3]           --
│    └─Empty: 2-149                      [16]                      --
│    └─Empty: 2-150                      [16]                      --
│    └─BatchNorm2d: 2-151                [16, 16, 8, 8]            32
│    └─Scaler: 2-152                     [16, 16, 8, 8]            --
│    └─ReLU: 2-153                       [16, 16, 8, 8]            --
│    └─Empty: 2-154                      [16, 16, 8, 8]            --
│    └─Clamp: 2-155                      [16, 16, 8, 8]            --
├─Dropout2d: 1-16                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-17                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-156         --                        --
│    └─One: 2-157                        [1]                       --
│    └─OutputScale: 2-158                --                        --
│    └─Empty: 2-159                      [128, 48, 1, 1]           --
│    └─Empty: 2-160                      [128, 48, 1, 1]           --
│    └─Empty: 2-161                      [128]                     --
│    └─Empty: 2-162                      [128]                     --
│    └─BatchNorm2d: 2-163                [16, 128, 64, 64]         --
│    └─Scaler: 2-164                     [16, 128, 64, 64]         --
│    └─ReLU: 2-165                       [16, 128, 64, 64]         --
│    └─Empty: 2-166                      [16, 128, 64, 64]         --
│    └─Clamp: 2-167                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-18         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-168                  [16, 128, 32, 32]         --
│    └─Empty: 2-169                      [16, 128, 32, 32]         --
│    └─Empty: 2-170                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-171         --                        --
│    └─One: 2-172                        [1]                       --
│    └─OutputScale: 2-173                --                        --
│    └─Empty: 2-174                      [128, 128, 3, 3]          --
│    └─Empty: 2-175                      [128, 128, 3, 3]          --
│    └─Empty: 2-176                      [128]                     --
│    └─Empty: 2-177                      [128]                     --
│    └─BatchNorm2d: 2-178                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-179                     [16, 128, 32, 32]         --
│    └─ReLU: 2-180                       [16, 128, 32, 32]         --
│    └─Empty: 2-181                      [16, 128, 32, 32]         --
│    └─Clamp: 2-182                      [16, 128, 32, 32]         --
├─Dropout2d: 1-19                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-20         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-183                  [16, 128, 16, 16]         --
│    └─Empty: 2-184                      [16, 128, 16, 16]         --
│    └─Empty: 2-185                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-186         --                        --
│    └─One: 2-187                        [1]                       --
│    └─OutputScale: 2-188                --                        --
│    └─Empty: 2-189                      [128, 128, 3, 3]          --
│    └─Empty: 2-190                      [128, 128, 3, 3]          --
│    └─Empty: 2-191                      [128]                     --
│    └─Empty: 2-192                      [128]                     --
│    └─BatchNorm2d: 2-193                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-194                     [16, 128, 16, 16]         --
│    └─ReLU: 2-195                       [16, 128, 16, 16]         --
│    └─Empty: 2-196                      [16, 128, 16, 16]         --
│    └─Clamp: 2-197                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-21                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-198         --                        --
│    └─One: 2-199                        [1]                       --
│    └─OutputScale: 2-200                --                        --
│    └─Empty: 2-201                      [128, 128, 1, 1]          --
│    └─Empty: 2-202                      [128, 128, 1, 1]          --
│    └─Empty: 2-203                      [128]                     --
│    └─Empty: 2-204                      [128]                     --
│    └─BatchNorm2d: 2-205                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-206                     [16, 128, 16, 16]         --
│    └─ReLU: 2-207                       [16, 128, 16, 16]         --
│    └─Empty: 2-208                      [16, 128, 16, 16]         --
│    └─Clamp: 2-209                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-22         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-210                  [16, 128, 16, 16]         --
│    └─Empty: 2-211                      [16, 128, 16, 16]         --
│    └─Empty: 2-212                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-213         --                        --
│    └─One: 2-214                        [1]                       --
│    └─OutputScale: 2-215                --                        --
│    └─Empty: 2-216                      [128, 128, 3, 3]          --
│    └─Empty: 2-217                      [128, 128, 3, 3]          --
│    └─Empty: 2-218                      [128]                     --
│    └─Empty: 2-219                      [128]                     --
│    └─BatchNorm2d: 2-220                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-221                     [16, 128, 16, 16]         --
│    └─ReLU: 2-222                       [16, 128, 16, 16]         --
│    └─Empty: 2-223                      [16, 128, 16, 16]         --
│    └─Clamp: 2-224                      [16, 128, 16, 16]         --
├─Dropout2d: 1-23                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-24         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-225                  [16, 128, 8, 8]           --
│    └─Empty: 2-226                      [16, 128, 8, 8]           --
│    └─Empty: 2-227                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-228         --                        --
│    └─One: 2-229                        [1]                       --
│    └─OutputScale: 2-230                --                        --
│    └─Empty: 2-231                      [128, 128, 3, 3]          --
│    └─Empty: 2-232                      [128, 128, 3, 3]          --
│    └─Empty: 2-233                      [128]                     --
│    └─Empty: 2-234                      [128]                     --
│    └─BatchNorm2d: 2-235                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-236                     [16, 128, 8, 8]           --
│    └─ReLU: 2-237                       [16, 128, 8, 8]           --
│    └─Empty: 2-238                      [16, 128, 8, 8]           --
│    └─Clamp: 2-239                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-25                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-240         --                        --
│    └─One: 2-241                        [1]                       --
│    └─OutputScale: 2-242                --                        --
│    └─Empty: 2-243                      [16, 128, 1, 1]           --
│    └─Empty: 2-244                      [16, 128, 1, 1]           --
│    └─Empty: 2-245                      [16]                      --
│    └─Empty: 2-246                      [16]                      --
│    └─BatchNorm2d: 2-247                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-248                     [16, 16, 8, 8]            --
│    └─ReLU: 2-249                       [16, 16, 8, 8]            --
│    └─Empty: 2-250                      [16, 16, 8, 8]            --
│    └─Clamp: 2-251                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-252                  [16, 128, 8, 8]           --
│    └─Empty: 2-253                      [16, 128, 8, 8]           --
│    └─Empty: 2-254                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-255         --                        --
│    └─One: 2-256                        [1]                       --
│    └─OutputScale: 2-257                --                        --
│    └─Empty: 2-258                      [16, 128, 3, 3]           --
│    └─Empty: 2-259                      [16, 128, 3, 3]           --
│    └─Empty: 2-260                      [16]                      --
│    └─Empty: 2-261                      [16]                      --
│    └─BatchNorm2d: 2-262                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-263                     [16, 16, 8, 8]            --
│    └─ReLU: 2-264                       [16, 16, 8, 8]            --
│    └─Empty: 2-265                      [16, 16, 8, 8]            --
│    └─Clamp: 2-266                      [16, 16, 8, 8]            --
├─Dropout2d: 1-27                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-28                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-267         --                        --
│    └─One: 2-268                        [1]                       --
│    └─OutputScale: 2-269                --                        --
│    └─Empty: 2-270                      [128, 48, 1, 1]           --
│    └─Empty: 2-271                      [128, 48, 1, 1]           --
│    └─Empty: 2-272                      [128]                     --
│    └─Empty: 2-273                      [128]                     --
│    └─BatchNorm2d: 2-274                [16, 128, 64, 64]         --
│    └─Scaler: 2-275                     [16, 128, 64, 64]         --
│    └─ReLU: 2-276                       [16, 128, 64, 64]         --
│    └─Empty: 2-277                      [16, 128, 64, 64]         --
│    └─Clamp: 2-278                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-29         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-279                  [16, 128, 32, 32]         --
│    └─Empty: 2-280                      [16, 128, 32, 32]         --
│    └─Empty: 2-281                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-282         --                        --
│    └─One: 2-283                        [1]                       --
│    └─OutputScale: 2-284                --                        --
│    └─Empty: 2-285                      [128, 128, 3, 3]          --
│    └─Empty: 2-286                      [128, 128, 3, 3]          --
│    └─Empty: 2-287                      [128]                     --
│    └─Empty: 2-288                      [128]                     --
│    └─BatchNorm2d: 2-289                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-290                     [16, 128, 32, 32]         --
│    └─ReLU: 2-291                       [16, 128, 32, 32]         --
│    └─Empty: 2-292                      [16, 128, 32, 32]         --
│    └─Clamp: 2-293                      [16, 128, 32, 32]         --
├─Dropout2d: 1-30                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-294                  [16, 128, 16, 16]         --
│    └─Empty: 2-295                      [16, 128, 16, 16]         --
│    └─Empty: 2-296                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-297         --                        --
│    └─One: 2-298                        [1]                       --
│    └─OutputScale: 2-299                --                        --
│    └─Empty: 2-300                      [128, 128, 3, 3]          --
│    └─Empty: 2-301                      [128, 128, 3, 3]          --
│    └─Empty: 2-302                      [128]                     --
│    └─Empty: 2-303                      [128]                     --
│    └─BatchNorm2d: 2-304                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-305                     [16, 128, 16, 16]         --
│    └─ReLU: 2-306                       [16, 128, 16, 16]         --
│    └─Empty: 2-307                      [16, 128, 16, 16]         --
│    └─Clamp: 2-308                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-32                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-309         --                        --
│    └─One: 2-310                        [1]                       --
│    └─OutputScale: 2-311                --                        --
│    └─Empty: 2-312                      [128, 128, 1, 1]          --
│    └─Empty: 2-313                      [128, 128, 1, 1]          --
│    └─Empty: 2-314                      [128]                     --
│    └─Empty: 2-315                      [128]                     --
│    └─BatchNorm2d: 2-316                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-317                     [16, 128, 16, 16]         --
│    └─ReLU: 2-318                       [16, 128, 16, 16]         --
│    └─Empty: 2-319                      [16, 128, 16, 16]         --
│    └─Clamp: 2-320                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-33         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-321                  [16, 128, 16, 16]         --
│    └─Empty: 2-322                      [16, 128, 16, 16]         --
│    └─Empty: 2-323                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-324         --                        --
│    └─One: 2-325                        [1]                       --
│    └─OutputScale: 2-326                --                        --
│    └─Empty: 2-327                      [128, 128, 3, 3]          --
│    └─Empty: 2-328                      [128, 128, 3, 3]          --
│    └─Empty: 2-329                      [128]                     --
│    └─Empty: 2-330                      [128]                     --
│    └─BatchNorm2d: 2-331                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-332                     [16, 128, 16, 16]         --
│    └─ReLU: 2-333                       [16, 128, 16, 16]         --
│    └─Empty: 2-334                      [16, 128, 16, 16]         --
│    └─Clamp: 2-335                      [16, 128, 16, 16]         --
├─Dropout2d: 1-34                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-35         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-336                  [16, 128, 8, 8]           --
│    └─Empty: 2-337                      [16, 128, 8, 8]           --
│    └─Empty: 2-338                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-339         --                        --
│    └─One: 2-340                        [1]                       --
│    └─OutputScale: 2-341                --                        --
│    └─Empty: 2-342                      [128, 128, 3, 3]          --
│    └─Empty: 2-343                      [128, 128, 3, 3]          --
│    └─Empty: 2-344                      [128]                     --
│    └─Empty: 2-345                      [128]                     --
│    └─BatchNorm2d: 2-346                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-347                     [16, 128, 8, 8]           --
│    └─ReLU: 2-348                       [16, 128, 8, 8]           --
│    └─Empty: 2-349                      [16, 128, 8, 8]           --
│    └─Clamp: 2-350                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-36                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-351         --                        --
│    └─One: 2-352                        [1]                       --
│    └─OutputScale: 2-353                --                        --
│    └─Empty: 2-354                      [16, 128, 1, 1]           --
│    └─Empty: 2-355                      [16, 128, 1, 1]           --
│    └─Empty: 2-356                      [16]                      --
│    └─Empty: 2-357                      [16]                      --
│    └─BatchNorm2d: 2-358                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-359                     [16, 16, 8, 8]            --
│    └─ReLU: 2-360                       [16, 16, 8, 8]            --
│    └─Empty: 2-361                      [16, 16, 8, 8]            --
│    └─Clamp: 2-362                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-37         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-363                  [16, 128, 8, 8]           --
│    └─Empty: 2-364                      [16, 128, 8, 8]           --
│    └─Empty: 2-365                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-366         --                        --
│    └─One: 2-367                        [1]                       --
│    └─OutputScale: 2-368                --                        --
│    └─Empty: 2-369                      [16, 128, 3, 3]           --
│    └─Empty: 2-370                      [16, 128, 3, 3]           --
│    └─Empty: 2-371                      [16]                      --
│    └─Empty: 2-372                      [16]                      --
│    └─BatchNorm2d: 2-373                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-374                     [16, 16, 8, 8]            --
│    └─ReLU: 2-375                       [16, 16, 8, 8]            --
│    └─Empty: 2-376                      [16, 16, 8, 8]            --
│    └─Clamp: 2-377                      [16, 16, 8, 8]            --
├─Dropout2d: 1-38                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-39                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-378         --                        --
│    └─One: 2-379                        [1]                       --
│    └─OutputScale: 2-380                --                        --
│    └─Empty: 2-381                      [128, 48, 1, 1]           --
│    └─Empty: 2-382                      [128, 48, 1, 1]           --
│    └─Empty: 2-383                      [128]                     --
│    └─Empty: 2-384                      [128]                     --
│    └─BatchNorm2d: 2-385                [16, 128, 64, 64]         --
│    └─Scaler: 2-386                     [16, 128, 64, 64]         --
│    └─ReLU: 2-387                       [16, 128, 64, 64]         --
│    └─Empty: 2-388                      [16, 128, 64, 64]         --
│    └─Clamp: 2-389                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-40         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-390                  [16, 128, 32, 32]         --
│    └─Empty: 2-391                      [16, 128, 32, 32]         --
│    └─Empty: 2-392                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-393         --                        --
│    └─One: 2-394                        [1]                       --
│    └─OutputScale: 2-395                --                        --
│    └─Empty: 2-396                      [128, 128, 3, 3]          --
│    └─Empty: 2-397                      [128, 128, 3, 3]          --
│    └─Empty: 2-398                      [128]                     --
│    └─Empty: 2-399                      [128]                     --
│    └─BatchNorm2d: 2-400                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-401                     [16, 128, 32, 32]         --
│    └─ReLU: 2-402                       [16, 128, 32, 32]         --
│    └─Empty: 2-403                      [16, 128, 32, 32]         --
│    └─Clamp: 2-404                      [16, 128, 32, 32]         --
├─Dropout2d: 1-41                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-42         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-405                  [16, 128, 16, 16]         --
│    └─Empty: 2-406                      [16, 128, 16, 16]         --
│    └─Empty: 2-407                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-408         --                        --
│    └─One: 2-409                        [1]                       --
│    └─OutputScale: 2-410                --                        --
│    └─Empty: 2-411                      [128, 128, 3, 3]          --
│    └─Empty: 2-412                      [128, 128, 3, 3]          --
│    └─Empty: 2-413                      [128]                     --
│    └─Empty: 2-414                      [128]                     --
│    └─BatchNorm2d: 2-415                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-416                     [16, 128, 16, 16]         --
│    └─ReLU: 2-417                       [16, 128, 16, 16]         --
│    └─Empty: 2-418                      [16, 128, 16, 16]         --
│    └─Clamp: 2-419                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-43                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-420         --                        --
│    └─One: 2-421                        [1]                       --
│    └─OutputScale: 2-422                --                        --
│    └─Empty: 2-423                      [128, 128, 1, 1]          --
│    └─Empty: 2-424                      [128, 128, 1, 1]          --
│    └─Empty: 2-425                      [128]                     --
│    └─Empty: 2-426                      [128]                     --
│    └─BatchNorm2d: 2-427                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-428                     [16, 128, 16, 16]         --
│    └─ReLU: 2-429                       [16, 128, 16, 16]         --
│    └─Empty: 2-430                      [16, 128, 16, 16]         --
│    └─Clamp: 2-431                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-44         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-432                  [16, 128, 16, 16]         --
│    └─Empty: 2-433                      [16, 128, 16, 16]         --
│    └─Empty: 2-434                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-435         --                        --
│    └─One: 2-436                        [1]                       --
│    └─OutputScale: 2-437                --                        --
│    └─Empty: 2-438                      [128, 128, 3, 3]          --
│    └─Empty: 2-439                      [128, 128, 3, 3]          --
│    └─Empty: 2-440                      [128]                     --
│    └─Empty: 2-441                      [128]                     --
│    └─BatchNorm2d: 2-442                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-443                     [16, 128, 16, 16]         --
│    └─ReLU: 2-444                       [16, 128, 16, 16]         --
│    └─Empty: 2-445                      [16, 128, 16, 16]         --
│    └─Clamp: 2-446                      [16, 128, 16, 16]         --
├─Dropout2d: 1-45                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-447                  [16, 128, 8, 8]           --
│    └─Empty: 2-448                      [16, 128, 8, 8]           --
│    └─Empty: 2-449                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-450         --                        --
│    └─One: 2-451                        [1]                       --
│    └─OutputScale: 2-452                --                        --
│    └─Empty: 2-453                      [128, 128, 3, 3]          --
│    └─Empty: 2-454                      [128, 128, 3, 3]          --
│    └─Empty: 2-455                      [128]                     --
│    └─Empty: 2-456                      [128]                     --
│    └─BatchNorm2d: 2-457                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-458                     [16, 128, 8, 8]           --
│    └─ReLU: 2-459                       [16, 128, 8, 8]           --
│    └─Empty: 2-460                      [16, 128, 8, 8]           --
│    └─Clamp: 2-461                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-47                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-462         --                        --
│    └─One: 2-463                        [1]                       --
│    └─OutputScale: 2-464                --                        --
│    └─Empty: 2-465                      [16, 128, 1, 1]           --
│    └─Empty: 2-466                      [16, 128, 1, 1]           --
│    └─Empty: 2-467                      [16]                      --
│    └─Empty: 2-468                      [16]                      --
│    └─BatchNorm2d: 2-469                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-470                     [16, 16, 8, 8]            --
│    └─ReLU: 2-471                       [16, 16, 8, 8]            --
│    └─Empty: 2-472                      [16, 16, 8, 8]            --
│    └─Clamp: 2-473                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-474                  [16, 128, 8, 8]           --
│    └─Empty: 2-475                      [16, 128, 8, 8]           --
│    └─Empty: 2-476                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-477         --                        --
│    └─One: 2-478                        [1]                       --
│    └─OutputScale: 2-479                --                        --
│    └─Empty: 2-480                      [16, 128, 3, 3]           --
│    └─Empty: 2-481                      [16, 128, 3, 3]           --
│    └─Empty: 2-482                      [16]                      --
│    └─Empty: 2-483                      [16]                      --
│    └─BatchNorm2d: 2-484                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-485                     [16, 16, 8, 8]            --
│    └─ReLU: 2-486                       [16, 16, 8, 8]            --
│    └─Empty: 2-487                      [16, 16, 8, 8]            --
│    └─Clamp: 2-488                      [16, 16, 8, 8]            --
├─Dropout2d: 1-49                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-50                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-489         --                        --
│    └─One: 2-490                        [1]                       --
│    └─OutputScale: 2-491                --                        --
│    └─Empty: 2-492                      [128, 48, 1, 1]           --
│    └─Empty: 2-493                      [128, 48, 1, 1]           --
│    └─Empty: 2-494                      [128]                     --
│    └─Empty: 2-495                      [128]                     --
│    └─BatchNorm2d: 2-496                [16, 128, 64, 64]         --
│    └─Scaler: 2-497                     [16, 128, 64, 64]         --
│    └─ReLU: 2-498                       [16, 128, 64, 64]         --
│    └─Empty: 2-499                      [16, 128, 64, 64]         --
│    └─Clamp: 2-500                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-501                  [16, 128, 32, 32]         --
│    └─Empty: 2-502                      [16, 128, 32, 32]         --
│    └─Empty: 2-503                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-504         --                        --
│    └─One: 2-505                        [1]                       --
│    └─OutputScale: 2-506                --                        --
│    └─Empty: 2-507                      [128, 128, 3, 3]          --
│    └─Empty: 2-508                      [128, 128, 3, 3]          --
│    └─Empty: 2-509                      [128]                     --
│    └─Empty: 2-510                      [128]                     --
│    └─BatchNorm2d: 2-511                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-512                     [16, 128, 32, 32]         --
│    └─ReLU: 2-513                       [16, 128, 32, 32]         --
│    └─Empty: 2-514                      [16, 128, 32, 32]         --
│    └─Clamp: 2-515                      [16, 128, 32, 32]         --
├─Dropout2d: 1-52                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-53         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-516                  [16, 128, 16, 16]         --
│    └─Empty: 2-517                      [16, 128, 16, 16]         --
│    └─Empty: 2-518                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-519         --                        --
│    └─One: 2-520                        [1]                       --
│    └─OutputScale: 2-521                --                        --
│    └─Empty: 2-522                      [128, 128, 3, 3]          --
│    └─Empty: 2-523                      [128, 128, 3, 3]          --
│    └─Empty: 2-524                      [128]                     --
│    └─Empty: 2-525                      [128]                     --
│    └─BatchNorm2d: 2-526                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-527                     [16, 128, 16, 16]         --
│    └─ReLU: 2-528                       [16, 128, 16, 16]         --
│    └─Empty: 2-529                      [16, 128, 16, 16]         --
│    └─Clamp: 2-530                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-54                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-531         --                        --
│    └─One: 2-532                        [1]                       --
│    └─OutputScale: 2-533                --                        --
│    └─Empty: 2-534                      [128, 128, 1, 1]          --
│    └─Empty: 2-535                      [128, 128, 1, 1]          --
│    └─Empty: 2-536                      [128]                     --
│    └─Empty: 2-537                      [128]                     --
│    └─BatchNorm2d: 2-538                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-539                     [16, 128, 16, 16]         --
│    └─ReLU: 2-540                       [16, 128, 16, 16]         --
│    └─Empty: 2-541                      [16, 128, 16, 16]         --
│    └─Clamp: 2-542                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-55         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-543                  [16, 128, 16, 16]         --
│    └─Empty: 2-544                      [16, 128, 16, 16]         --
│    └─Empty: 2-545                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-546         --                        --
│    └─One: 2-547                        [1]                       --
│    └─OutputScale: 2-548                --                        --
│    └─Empty: 2-549                      [128, 128, 3, 3]          --
│    └─Empty: 2-550                      [128, 128, 3, 3]          --
│    └─Empty: 2-551                      [128]                     --
│    └─Empty: 2-552                      [128]                     --
│    └─BatchNorm2d: 2-553                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-554                     [16, 128, 16, 16]         --
│    └─ReLU: 2-555                       [16, 128, 16, 16]         --
│    └─Empty: 2-556                      [16, 128, 16, 16]         --
│    └─Clamp: 2-557                      [16, 128, 16, 16]         --
├─Dropout2d: 1-56                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-57         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-558                  [16, 128, 8, 8]           --
│    └─Empty: 2-559                      [16, 128, 8, 8]           --
│    └─Empty: 2-560                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-561         --                        --
│    └─One: 2-562                        [1]                       --
│    └─OutputScale: 2-563                --                        --
│    └─Empty: 2-564                      [128, 128, 3, 3]          --
│    └─Empty: 2-565                      [128, 128, 3, 3]          --
│    └─Empty: 2-566                      [128]                     --
│    └─Empty: 2-567                      [128]                     --
│    └─BatchNorm2d: 2-568                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-569                     [16, 128, 8, 8]           --
│    └─ReLU: 2-570                       [16, 128, 8, 8]           --
│    └─Empty: 2-571                      [16, 128, 8, 8]           --
│    └─Clamp: 2-572                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-58                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-573         --                        --
│    └─One: 2-574                        [1]                       --
│    └─OutputScale: 2-575                --                        --
│    └─Empty: 2-576                      [16, 128, 1, 1]           --
│    └─Empty: 2-577                      [16, 128, 1, 1]           --
│    └─Empty: 2-578                      [16]                      --
│    └─Empty: 2-579                      [16]                      --
│    └─BatchNorm2d: 2-580                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-581                     [16, 16, 8, 8]            --
│    └─ReLU: 2-582                       [16, 16, 8, 8]            --
│    └─Empty: 2-583                      [16, 16, 8, 8]            --
│    └─Clamp: 2-584                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-59         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-585                  [16, 128, 8, 8]           --
│    └─Empty: 2-586                      [16, 128, 8, 8]           --
│    └─Empty: 2-587                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-588         --                        --
│    └─One: 2-589                        [1]                       --
│    └─OutputScale: 2-590                --                        --
│    └─Empty: 2-591                      [16, 128, 3, 3]           --
│    └─Empty: 2-592                      [16, 128, 3, 3]           --
│    └─Empty: 2-593                      [16]                      --
│    └─Empty: 2-594                      [16]                      --
│    └─BatchNorm2d: 2-595                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-596                     [16, 16, 8, 8]            --
│    └─ReLU: 2-597                       [16, 16, 8, 8]            --
│    └─Empty: 2-598                      [16, 16, 8, 8]            --
│    └─Clamp: 2-599                      [16, 16, 8, 8]            --
├─Dropout2d: 1-60                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-61                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-600         --                        --
│    └─One: 2-601                        [1]                       --
│    └─OutputScale: 2-602                --                        --
│    └─Empty: 2-603                      [128, 48, 1, 1]           --
│    └─Empty: 2-604                      [128, 48, 1, 1]           --
│    └─Empty: 2-605                      [128]                     --
│    └─Empty: 2-606                      [128]                     --
│    └─BatchNorm2d: 2-607                [16, 128, 64, 64]         --
│    └─Scaler: 2-608                     [16, 128, 64, 64]         --
│    └─ReLU: 2-609                       [16, 128, 64, 64]         --
│    └─Empty: 2-610                      [16, 128, 64, 64]         --
│    └─Clamp: 2-611                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-62         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-612                  [16, 128, 32, 32]         --
│    └─Empty: 2-613                      [16, 128, 32, 32]         --
│    └─Empty: 2-614                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-615         --                        --
│    └─One: 2-616                        [1]                       --
│    └─OutputScale: 2-617                --                        --
│    └─Empty: 2-618                      [128, 128, 3, 3]          --
│    └─Empty: 2-619                      [128, 128, 3, 3]          --
│    └─Empty: 2-620                      [128]                     --
│    └─Empty: 2-621                      [128]                     --
│    └─BatchNorm2d: 2-622                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-623                     [16, 128, 32, 32]         --
│    └─ReLU: 2-624                       [16, 128, 32, 32]         --
│    └─Empty: 2-625                      [16, 128, 32, 32]         --
│    └─Clamp: 2-626                      [16, 128, 32, 32]         --
├─Dropout2d: 1-63                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-64         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-627                  [16, 128, 16, 16]         --
│    └─Empty: 2-628                      [16, 128, 16, 16]         --
│    └─Empty: 2-629                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-630         --                        --
│    └─One: 2-631                        [1]                       --
│    └─OutputScale: 2-632                --                        --
│    └─Empty: 2-633                      [128, 128, 3, 3]          --
│    └─Empty: 2-634                      [128, 128, 3, 3]          --
│    └─Empty: 2-635                      [128]                     --
│    └─Empty: 2-636                      [128]                     --
│    └─BatchNorm2d: 2-637                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-638                     [16, 128, 16, 16]         --
│    └─ReLU: 2-639                       [16, 128, 16, 16]         --
│    └─Empty: 2-640                      [16, 128, 16, 16]         --
│    └─Clamp: 2-641                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-65                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-642         --                        --
│    └─One: 2-643                        [1]                       --
│    └─OutputScale: 2-644                --                        --
│    └─Empty: 2-645                      [128, 128, 1, 1]          --
│    └─Empty: 2-646                      [128, 128, 1, 1]          --
│    └─Empty: 2-647                      [128]                     --
│    └─Empty: 2-648                      [128]                     --
│    └─BatchNorm2d: 2-649                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-650                     [16, 128, 16, 16]         --
│    └─ReLU: 2-651                       [16, 128, 16, 16]         --
│    └─Empty: 2-652                      [16, 128, 16, 16]         --
│    └─Clamp: 2-653                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-654                  [16, 128, 16, 16]         --
│    └─Empty: 2-655                      [16, 128, 16, 16]         --
│    └─Empty: 2-656                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-657         --                        --
│    └─One: 2-658                        [1]                       --
│    └─OutputScale: 2-659                --                        --
│    └─Empty: 2-660                      [128, 128, 3, 3]          --
│    └─Empty: 2-661                      [128, 128, 3, 3]          --
│    └─Empty: 2-662                      [128]                     --
│    └─Empty: 2-663                      [128]                     --
│    └─BatchNorm2d: 2-664                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-665                     [16, 128, 16, 16]         --
│    └─ReLU: 2-666                       [16, 128, 16, 16]         --
│    └─Empty: 2-667                      [16, 128, 16, 16]         --
│    └─Clamp: 2-668                      [16, 128, 16, 16]         --
├─Dropout2d: 1-67                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-669                  [16, 128, 8, 8]           --
│    └─Empty: 2-670                      [16, 128, 8, 8]           --
│    └─Empty: 2-671                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-672         --                        --
│    └─One: 2-673                        [1]                       --
│    └─OutputScale: 2-674                --                        --
│    └─Empty: 2-675                      [128, 128, 3, 3]          --
│    └─Empty: 2-676                      [128, 128, 3, 3]          --
│    └─Empty: 2-677                      [128]                     --
│    └─Empty: 2-678                      [128]                     --
│    └─BatchNorm2d: 2-679                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-680                     [16, 128, 8, 8]           --
│    └─ReLU: 2-681                       [16, 128, 8, 8]           --
│    └─Empty: 2-682                      [16, 128, 8, 8]           --
│    └─Clamp: 2-683                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-69                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-684         --                        --
│    └─One: 2-685                        [1]                       --
│    └─OutputScale: 2-686                --                        --
│    └─Empty: 2-687                      [16, 128, 1, 1]           --
│    └─Empty: 2-688                      [16, 128, 1, 1]           --
│    └─Empty: 2-689                      [16]                      --
│    └─Empty: 2-690                      [16]                      --
│    └─BatchNorm2d: 2-691                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-692                     [16, 16, 8, 8]            --
│    └─ReLU: 2-693                       [16, 16, 8, 8]            --
│    └─Empty: 2-694                      [16, 16, 8, 8]            --
│    └─Clamp: 2-695                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-70         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-696                  [16, 128, 8, 8]           --
│    └─Empty: 2-697                      [16, 128, 8, 8]           --
│    └─Empty: 2-698                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-699         --                        --
│    └─One: 2-700                        [1]                       --
│    └─OutputScale: 2-701                --                        --
│    └─Empty: 2-702                      [16, 128, 3, 3]           --
│    └─Empty: 2-703                      [16, 128, 3, 3]           --
│    └─Empty: 2-704                      [16]                      --
│    └─Empty: 2-705                      [16]                      --
│    └─BatchNorm2d: 2-706                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-707                     [16, 16, 8, 8]            --
│    └─ReLU: 2-708                       [16, 16, 8, 8]            --
│    └─Empty: 2-709                      [16, 16, 8, 8]            --
│    └─Clamp: 2-710                      [16, 16, 8, 8]            --
├─Dropout2d: 1-71                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-72                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-711         --                        --
│    └─One: 2-712                        [1]                       --
│    └─OutputScale: 2-713                --                        --
│    └─Empty: 2-714                      [128, 48, 1, 1]           --
│    └─Empty: 2-715                      [128, 48, 1, 1]           --
│    └─Empty: 2-716                      [128]                     --
│    └─Empty: 2-717                      [128]                     --
│    └─BatchNorm2d: 2-718                [16, 128, 64, 64]         --
│    └─Scaler: 2-719                     [16, 128, 64, 64]         --
│    └─ReLU: 2-720                       [16, 128, 64, 64]         --
│    └─Empty: 2-721                      [16, 128, 64, 64]         --
│    └─Clamp: 2-722                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-723                  [16, 128, 32, 32]         --
│    └─Empty: 2-724                      [16, 128, 32, 32]         --
│    └─Empty: 2-725                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-726         --                        --
│    └─One: 2-727                        [1]                       --
│    └─OutputScale: 2-728                --                        --
│    └─Empty: 2-729                      [128, 128, 3, 3]          --
│    └─Empty: 2-730                      [128, 128, 3, 3]          --
│    └─Empty: 2-731                      [128]                     --
│    └─Empty: 2-732                      [128]                     --
│    └─BatchNorm2d: 2-733                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-734                     [16, 128, 32, 32]         --
│    └─ReLU: 2-735                       [16, 128, 32, 32]         --
│    └─Empty: 2-736                      [16, 128, 32, 32]         --
│    └─Clamp: 2-737                      [16, 128, 32, 32]         --
├─Dropout2d: 1-74                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-738                  [16, 128, 16, 16]         --
│    └─Empty: 2-739                      [16, 128, 16, 16]         --
│    └─Empty: 2-740                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-741         --                        --
│    └─One: 2-742                        [1]                       --
│    └─OutputScale: 2-743                --                        --
│    └─Empty: 2-744                      [128, 128, 3, 3]          --
│    └─Empty: 2-745                      [128, 128, 3, 3]          --
│    └─Empty: 2-746                      [128]                     --
│    └─Empty: 2-747                      [128]                     --
│    └─BatchNorm2d: 2-748                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-749                     [16, 128, 16, 16]         --
│    └─ReLU: 2-750                       [16, 128, 16, 16]         --
│    └─Empty: 2-751                      [16, 128, 16, 16]         --
│    └─Clamp: 2-752                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-76                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-753         --                        --
│    └─One: 2-754                        [1]                       --
│    └─OutputScale: 2-755                --                        --
│    └─Empty: 2-756                      [128, 128, 1, 1]          --
│    └─Empty: 2-757                      [128, 128, 1, 1]          --
│    └─Empty: 2-758                      [128]                     --
│    └─Empty: 2-759                      [128]                     --
│    └─BatchNorm2d: 2-760                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-761                     [16, 128, 16, 16]         --
│    └─ReLU: 2-762                       [16, 128, 16, 16]         --
│    └─Empty: 2-763                      [16, 128, 16, 16]         --
│    └─Clamp: 2-764                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-77         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-765                  [16, 128, 16, 16]         --
│    └─Empty: 2-766                      [16, 128, 16, 16]         --
│    └─Empty: 2-767                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-768         --                        --
│    └─One: 2-769                        [1]                       --
│    └─OutputScale: 2-770                --                        --
│    └─Empty: 2-771                      [128, 128, 3, 3]          --
│    └─Empty: 2-772                      [128, 128, 3, 3]          --
│    └─Empty: 2-773                      [128]                     --
│    └─Empty: 2-774                      [128]                     --
│    └─BatchNorm2d: 2-775                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-776                     [16, 128, 16, 16]         --
│    └─ReLU: 2-777                       [16, 128, 16, 16]         --
│    └─Empty: 2-778                      [16, 128, 16, 16]         --
│    └─Clamp: 2-779                      [16, 128, 16, 16]         --
├─Dropout2d: 1-78                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-79         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-780                  [16, 128, 8, 8]           --
│    └─Empty: 2-781                      [16, 128, 8, 8]           --
│    └─Empty: 2-782                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-783         --                        --
│    └─One: 2-784                        [1]                       --
│    └─OutputScale: 2-785                --                        --
│    └─Empty: 2-786                      [128, 128, 3, 3]          --
│    └─Empty: 2-787                      [128, 128, 3, 3]          --
│    └─Empty: 2-788                      [128]                     --
│    └─Empty: 2-789                      [128]                     --
│    └─BatchNorm2d: 2-790                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-791                     [16, 128, 8, 8]           --
│    └─ReLU: 2-792                       [16, 128, 8, 8]           --
│    └─Empty: 2-793                      [16, 128, 8, 8]           --
│    └─Clamp: 2-794                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-80                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-795         --                        --
│    └─One: 2-796                        [1]                       --
│    └─OutputScale: 2-797                --                        --
│    └─Empty: 2-798                      [16, 128, 1, 1]           --
│    └─Empty: 2-799                      [16, 128, 1, 1]           --
│    └─Empty: 2-800                      [16]                      --
│    └─Empty: 2-801                      [16]                      --
│    └─BatchNorm2d: 2-802                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-803                     [16, 16, 8, 8]            --
│    └─ReLU: 2-804                       [16, 16, 8, 8]            --
│    └─Empty: 2-805                      [16, 16, 8, 8]            --
│    └─Clamp: 2-806                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-807                  [16, 128, 8, 8]           --
│    └─Empty: 2-808                      [16, 128, 8, 8]           --
│    └─Empty: 2-809                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-810         --                        --
│    └─One: 2-811                        [1]                       --
│    └─OutputScale: 2-812                --                        --
│    └─Empty: 2-813                      [16, 128, 3, 3]           --
│    └─Empty: 2-814                      [16, 128, 3, 3]           --
│    └─Empty: 2-815                      [16]                      --
│    └─Empty: 2-816                      [16]                      --
│    └─BatchNorm2d: 2-817                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-818                     [16, 16, 8, 8]            --
│    └─ReLU: 2-819                       [16, 16, 8, 8]            --
│    └─Empty: 2-820                      [16, 16, 8, 8]            --
│    └─Clamp: 2-821                      [16, 16, 8, 8]            --
├─Dropout2d: 1-82                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-83                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-822         --                        --
│    └─One: 2-823                        [1]                       --
│    └─OutputScale: 2-824                --                        --
│    └─Empty: 2-825                      [128, 48, 1, 1]           --
│    └─Empty: 2-826                      [128, 48, 1, 1]           --
│    └─Empty: 2-827                      [128]                     --
│    └─Empty: 2-828                      [128]                     --
│    └─BatchNorm2d: 2-829                [16, 128, 64, 64]         --
│    └─Scaler: 2-830                     [16, 128, 64, 64]         --
│    └─ReLU: 2-831                       [16, 128, 64, 64]         --
│    └─Empty: 2-832                      [16, 128, 64, 64]         --
│    └─Clamp: 2-833                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-84         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-834                  [16, 128, 32, 32]         --
│    └─Empty: 2-835                      [16, 128, 32, 32]         --
│    └─Empty: 2-836                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-837         --                        --
│    └─One: 2-838                        [1]                       --
│    └─OutputScale: 2-839                --                        --
│    └─Empty: 2-840                      [128, 128, 3, 3]          --
│    └─Empty: 2-841                      [128, 128, 3, 3]          --
│    └─Empty: 2-842                      [128]                     --
│    └─Empty: 2-843                      [128]                     --
│    └─BatchNorm2d: 2-844                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-845                     [16, 128, 32, 32]         --
│    └─ReLU: 2-846                       [16, 128, 32, 32]         --
│    └─Empty: 2-847                      [16, 128, 32, 32]         --
│    └─Clamp: 2-848                      [16, 128, 32, 32]         --
├─Dropout2d: 1-85                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-86         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-849                  [16, 128, 16, 16]         --
│    └─Empty: 2-850                      [16, 128, 16, 16]         --
│    └─Empty: 2-851                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-852         --                        --
│    └─One: 2-853                        [1]                       --
│    └─OutputScale: 2-854                --                        --
│    └─Empty: 2-855                      [128, 128, 3, 3]          --
│    └─Empty: 2-856                      [128, 128, 3, 3]          --
│    └─Empty: 2-857                      [128]                     --
│    └─Empty: 2-858                      [128]                     --
│    └─BatchNorm2d: 2-859                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-860                     [16, 128, 16, 16]         --
│    └─ReLU: 2-861                       [16, 128, 16, 16]         --
│    └─Empty: 2-862                      [16, 128, 16, 16]         --
│    └─Clamp: 2-863                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-87                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-864         --                        --
│    └─One: 2-865                        [1]                       --
│    └─OutputScale: 2-866                --                        --
│    └─Empty: 2-867                      [128, 128, 1, 1]          --
│    └─Empty: 2-868                      [128, 128, 1, 1]          --
│    └─Empty: 2-869                      [128]                     --
│    └─Empty: 2-870                      [128]                     --
│    └─BatchNorm2d: 2-871                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-872                     [16, 128, 16, 16]         --
│    └─ReLU: 2-873                       [16, 128, 16, 16]         --
│    └─Empty: 2-874                      [16, 128, 16, 16]         --
│    └─Clamp: 2-875                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-88         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-876                  [16, 128, 16, 16]         --
│    └─Empty: 2-877                      [16, 128, 16, 16]         --
│    └─Empty: 2-878                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-879         --                        --
│    └─One: 2-880                        [1]                       --
│    └─OutputScale: 2-881                --                        --
│    └─Empty: 2-882                      [128, 128, 3, 3]          --
│    └─Empty: 2-883                      [128, 128, 3, 3]          --
│    └─Empty: 2-884                      [128]                     --
│    └─Empty: 2-885                      [128]                     --
│    └─BatchNorm2d: 2-886                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-887                     [16, 128, 16, 16]         --
│    └─ReLU: 2-888                       [16, 128, 16, 16]         --
│    └─Empty: 2-889                      [16, 128, 16, 16]         --
│    └─Clamp: 2-890                      [16, 128, 16, 16]         --
├─Dropout2d: 1-89                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-90         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-891                  [16, 128, 8, 8]           --
│    └─Empty: 2-892                      [16, 128, 8, 8]           --
│    └─Empty: 2-893                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-894         --                        --
│    └─One: 2-895                        [1]                       --
│    └─OutputScale: 2-896                --                        --
│    └─Empty: 2-897                      [128, 128, 3, 3]          --
│    └─Empty: 2-898                      [128, 128, 3, 3]          --
│    └─Empty: 2-899                      [128]                     --
│    └─Empty: 2-900                      [128]                     --
│    └─BatchNorm2d: 2-901                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-902                     [16, 128, 8, 8]           --
│    └─ReLU: 2-903                       [16, 128, 8, 8]           --
│    └─Empty: 2-904                      [16, 128, 8, 8]           --
│    └─Clamp: 2-905                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-91                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-906         --                        --
│    └─One: 2-907                        [1]                       --
│    └─OutputScale: 2-908                --                        --
│    └─Empty: 2-909                      [16, 128, 1, 1]           --
│    └─Empty: 2-910                      [16, 128, 1, 1]           --
│    └─Empty: 2-911                      [16]                      --
│    └─Empty: 2-912                      [16]                      --
│    └─BatchNorm2d: 2-913                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-914                     [16, 16, 8, 8]            --
│    └─ReLU: 2-915                       [16, 16, 8, 8]            --
│    └─Empty: 2-916                      [16, 16, 8, 8]            --
│    └─Clamp: 2-917                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-92         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-918                  [16, 128, 8, 8]           --
│    └─Empty: 2-919                      [16, 128, 8, 8]           --
│    └─Empty: 2-920                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-921         --                        --
│    └─One: 2-922                        [1]                       --
│    └─OutputScale: 2-923                --                        --
│    └─Empty: 2-924                      [16, 128, 3, 3]           --
│    └─Empty: 2-925                      [16, 128, 3, 3]           --
│    └─Empty: 2-926                      [16]                      --
│    └─Empty: 2-927                      [16]                      --
│    └─BatchNorm2d: 2-928                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-929                     [16, 16, 8, 8]            --
│    └─ReLU: 2-930                       [16, 16, 8, 8]            --
│    └─Empty: 2-931                      [16, 16, 8, 8]            --
│    └─Clamp: 2-932                      [16, 16, 8, 8]            --
├─Dropout2d: 1-93                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-94                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-933         --                        --
│    └─One: 2-934                        [1]                       --
│    └─OutputScale: 2-935                --                        --
│    └─Empty: 2-936                      [128, 48, 1, 1]           --
│    └─Empty: 2-937                      [128, 48, 1, 1]           --
│    └─Empty: 2-938                      [128]                     --
│    └─Empty: 2-939                      [128]                     --
│    └─BatchNorm2d: 2-940                [16, 128, 64, 64]         --
│    └─Scaler: 2-941                     [16, 128, 64, 64]         --
│    └─ReLU: 2-942                       [16, 128, 64, 64]         --
│    └─Empty: 2-943                      [16, 128, 64, 64]         --
│    └─Clamp: 2-944                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-95         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-945                  [16, 128, 32, 32]         --
│    └─Empty: 2-946                      [16, 128, 32, 32]         --
│    └─Empty: 2-947                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-948         --                        --
│    └─One: 2-949                        [1]                       --
│    └─OutputScale: 2-950                --                        --
│    └─Empty: 2-951                      [128, 128, 3, 3]          --
│    └─Empty: 2-952                      [128, 128, 3, 3]          --
│    └─Empty: 2-953                      [128]                     --
│    └─Empty: 2-954                      [128]                     --
│    └─BatchNorm2d: 2-955                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-956                     [16, 128, 32, 32]         --
│    └─ReLU: 2-957                       [16, 128, 32, 32]         --
│    └─Empty: 2-958                      [16, 128, 32, 32]         --
│    └─Clamp: 2-959                      [16, 128, 32, 32]         --
├─Dropout2d: 1-96                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-97         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-960                  [16, 128, 16, 16]         --
│    └─Empty: 2-961                      [16, 128, 16, 16]         --
│    └─Empty: 2-962                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-963         --                        --
│    └─One: 2-964                        [1]                       --
│    └─OutputScale: 2-965                --                        --
│    └─Empty: 2-966                      [128, 128, 3, 3]          --
│    └─Empty: 2-967                      [128, 128, 3, 3]          --
│    └─Empty: 2-968                      [128]                     --
│    └─Empty: 2-969                      [128]                     --
│    └─BatchNorm2d: 2-970                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-971                     [16, 128, 16, 16]         --
│    └─ReLU: 2-972                       [16, 128, 16, 16]         --
│    └─Empty: 2-973                      [16, 128, 16, 16]         --
│    └─Clamp: 2-974                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-98                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-975         --                        --
│    └─One: 2-976                        [1]                       --
│    └─OutputScale: 2-977                --                        --
│    └─Empty: 2-978                      [128, 128, 1, 1]          --
│    └─Empty: 2-979                      [128, 128, 1, 1]          --
│    └─Empty: 2-980                      [128]                     --
│    └─Empty: 2-981                      [128]                     --
│    └─BatchNorm2d: 2-982                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-983                     [16, 128, 16, 16]         --
│    └─ReLU: 2-984                       [16, 128, 16, 16]         --
│    └─Empty: 2-985                      [16, 128, 16, 16]         --
│    └─Clamp: 2-986                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-99         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-987                  [16, 128, 16, 16]         --
│    └─Empty: 2-988                      [16, 128, 16, 16]         --
│    └─Empty: 2-989                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-990         --                        --
│    └─One: 2-991                        [1]                       --
│    └─OutputScale: 2-992                --                        --
│    └─Empty: 2-993                      [128, 128, 3, 3]          --
│    └─Empty: 2-994                      [128, 128, 3, 3]          --
│    └─Empty: 2-995                      [128]                     --
│    └─Empty: 2-996                      [128]                     --
│    └─BatchNorm2d: 2-997                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-998                     [16, 128, 16, 16]         --
│    └─ReLU: 2-999                       [16, 128, 16, 16]         --
│    └─Empty: 2-1000                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1001                     [16, 128, 16, 16]         --
├─Dropout2d: 1-100                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-101        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1002                 [16, 128, 8, 8]           --
│    └─Empty: 2-1003                     [16, 128, 8, 8]           --
│    └─Empty: 2-1004                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1005        --                        --
│    └─One: 2-1006                       [1]                       --
│    └─OutputScale: 2-1007               --                        --
│    └─Empty: 2-1008                     [128, 128, 3, 3]          --
│    └─Empty: 2-1009                     [128, 128, 3, 3]          --
│    └─Empty: 2-1010                     [128]                     --
│    └─Empty: 2-1011                     [128]                     --
│    └─BatchNorm2d: 2-1012               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1013                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1014                      [16, 128, 8, 8]           --
│    └─Empty: 2-1015                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1016                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-102               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1017        --                        --
│    └─One: 2-1018                       [1]                       --
│    └─OutputScale: 2-1019               --                        --
│    └─Empty: 2-1020                     [16, 128, 1, 1]           --
│    └─Empty: 2-1021                     [16, 128, 1, 1]           --
│    └─Empty: 2-1022                     [16]                      --
│    └─Empty: 2-1023                     [16]                      --
│    └─BatchNorm2d: 2-1024               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1025                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1026                      [16, 16, 8, 8]            --
│    └─Empty: 2-1027                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1028                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-103        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1029                 [16, 128, 8, 8]           --
│    └─Empty: 2-1030                     [16, 128, 8, 8]           --
│    └─Empty: 2-1031                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1032        --                        --
│    └─One: 2-1033                       [1]                       --
│    └─OutputScale: 2-1034               --                        --
│    └─Empty: 2-1035                     [16, 128, 3, 3]           --
│    └─Empty: 2-1036                     [16, 128, 3, 3]           --
│    └─Empty: 2-1037                     [16]                      --
│    └─Empty: 2-1038                     [16]                      --
│    └─BatchNorm2d: 2-1039               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1040                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1041                      [16, 16, 8, 8]            --
│    └─Empty: 2-1042                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1043                     [16, 16, 8, 8]            --
├─Dropout2d: 1-104                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-105               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1044        --                        --
│    └─One: 2-1045                       [1]                       --
│    └─OutputScale: 2-1046               --                        --
│    └─Empty: 2-1047                     [128, 48, 1, 1]           --
│    └─Empty: 2-1048                     [128, 48, 1, 1]           --
│    └─Empty: 2-1049                     [128]                     --
│    └─Empty: 2-1050                     [128]                     --
│    └─BatchNorm2d: 2-1051               [16, 128, 64, 64]         --
│    └─Scaler: 2-1052                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1053                      [16, 128, 64, 64]         --
│    └─Empty: 2-1054                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1055                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-106        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1056                 [16, 128, 32, 32]         --
│    └─Empty: 2-1057                     [16, 128, 32, 32]         --
│    └─Empty: 2-1058                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1059        --                        --
│    └─One: 2-1060                       [1]                       --
│    └─OutputScale: 2-1061               --                        --
│    └─Empty: 2-1062                     [128, 128, 3, 3]          --
│    └─Empty: 2-1063                     [128, 128, 3, 3]          --
│    └─Empty: 2-1064                     [128]                     --
│    └─Empty: 2-1065                     [128]                     --
│    └─BatchNorm2d: 2-1066               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1067                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1068                      [16, 128, 32, 32]         --
│    └─Empty: 2-1069                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1070                     [16, 128, 32, 32]         --
├─Dropout2d: 1-107                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-108        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1071                 [16, 128, 16, 16]         --
│    └─Empty: 2-1072                     [16, 128, 16, 16]         --
│    └─Empty: 2-1073                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1074        --                        --
│    └─One: 2-1075                       [1]                       --
│    └─OutputScale: 2-1076               --                        --
│    └─Empty: 2-1077                     [128, 128, 3, 3]          --
│    └─Empty: 2-1078                     [128, 128, 3, 3]          --
│    └─Empty: 2-1079                     [128]                     --
│    └─Empty: 2-1080                     [128]                     --
│    └─BatchNorm2d: 2-1081               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1082                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1083                      [16, 128, 16, 16]         --
│    └─Empty: 2-1084                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1085                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-109               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1086        --                        --
│    └─One: 2-1087                       [1]                       --
│    └─OutputScale: 2-1088               --                        --
│    └─Empty: 2-1089                     [128, 128, 1, 1]          --
│    └─Empty: 2-1090                     [128, 128, 1, 1]          --
│    └─Empty: 2-1091                     [128]                     --
│    └─Empty: 2-1092                     [128]                     --
│    └─BatchNorm2d: 2-1093               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1094                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1095                      [16, 128, 16, 16]         --
│    └─Empty: 2-1096                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1097                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-110        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1098                 [16, 128, 16, 16]         --
│    └─Empty: 2-1099                     [16, 128, 16, 16]         --
│    └─Empty: 2-1100                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1101        --                        --
│    └─One: 2-1102                       [1]                       --
│    └─OutputScale: 2-1103               --                        --
│    └─Empty: 2-1104                     [128, 128, 3, 3]          --
│    └─Empty: 2-1105                     [128, 128, 3, 3]          --
│    └─Empty: 2-1106                     [128]                     --
│    └─Empty: 2-1107                     [128]                     --
│    └─BatchNorm2d: 2-1108               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1109                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1110                      [16, 128, 16, 16]         --
│    └─Empty: 2-1111                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1112                     [16, 128, 16, 16]         --
├─Dropout2d: 1-111                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-112        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1113                 [16, 128, 8, 8]           --
│    └─Empty: 2-1114                     [16, 128, 8, 8]           --
│    └─Empty: 2-1115                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1116        --                        --
│    └─One: 2-1117                       [1]                       --
│    └─OutputScale: 2-1118               --                        --
│    └─Empty: 2-1119                     [128, 128, 3, 3]          --
│    └─Empty: 2-1120                     [128, 128, 3, 3]          --
│    └─Empty: 2-1121                     [128]                     --
│    └─Empty: 2-1122                     [128]                     --
│    └─BatchNorm2d: 2-1123               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1124                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1125                      [16, 128, 8, 8]           --
│    └─Empty: 2-1126                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1127                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-113               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1128        --                        --
│    └─One: 2-1129                       [1]                       --
│    └─OutputScale: 2-1130               --                        --
│    └─Empty: 2-1131                     [16, 128, 1, 1]           --
│    └─Empty: 2-1132                     [16, 128, 1, 1]           --
│    └─Empty: 2-1133                     [16]                      --
│    └─Empty: 2-1134                     [16]                      --
│    └─BatchNorm2d: 2-1135               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1136                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1137                      [16, 16, 8, 8]            --
│    └─Empty: 2-1138                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1139                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-114        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1140                 [16, 128, 8, 8]           --
│    └─Empty: 2-1141                     [16, 128, 8, 8]           --
│    └─Empty: 2-1142                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1143        --                        --
│    └─One: 2-1144                       [1]                       --
│    └─OutputScale: 2-1145               --                        --
│    └─Empty: 2-1146                     [16, 128, 3, 3]           --
│    └─Empty: 2-1147                     [16, 128, 3, 3]           --
│    └─Empty: 2-1148                     [16]                      --
│    └─Empty: 2-1149                     [16]                      --
│    └─BatchNorm2d: 2-1150               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1151                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1152                      [16, 16, 8, 8]            --
│    └─Empty: 2-1153                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1154                     [16, 16, 8, 8]            --
├─Dropout2d: 1-115                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-116               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1155        --                        --
│    └─One: 2-1156                       [1]                       --
│    └─OutputScale: 2-1157               --                        --
│    └─Empty: 2-1158                     [128, 48, 1, 1]           --
│    └─Empty: 2-1159                     [128, 48, 1, 1]           --
│    └─Empty: 2-1160                     [128]                     --
│    └─Empty: 2-1161                     [128]                     --
│    └─BatchNorm2d: 2-1162               [16, 128, 64, 64]         --
│    └─Scaler: 2-1163                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1164                      [16, 128, 64, 64]         --
│    └─Empty: 2-1165                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1166                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-117        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1167                 [16, 128, 32, 32]         --
│    └─Empty: 2-1168                     [16, 128, 32, 32]         --
│    └─Empty: 2-1169                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1170        --                        --
│    └─One: 2-1171                       [1]                       --
│    └─OutputScale: 2-1172               --                        --
│    └─Empty: 2-1173                     [128, 128, 3, 3]          --
│    └─Empty: 2-1174                     [128, 128, 3, 3]          --
│    └─Empty: 2-1175                     [128]                     --
│    └─Empty: 2-1176                     [128]                     --
│    └─BatchNorm2d: 2-1177               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1178                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1179                      [16, 128, 32, 32]         --
│    └─Empty: 2-1180                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1181                     [16, 128, 32, 32]         --
├─Dropout2d: 1-118                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-119        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1182                 [16, 128, 16, 16]         --
│    └─Empty: 2-1183                     [16, 128, 16, 16]         --
│    └─Empty: 2-1184                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1185        --                        --
│    └─One: 2-1186                       [1]                       --
│    └─OutputScale: 2-1187               --                        --
│    └─Empty: 2-1188                     [128, 128, 3, 3]          --
│    └─Empty: 2-1189                     [128, 128, 3, 3]          --
│    └─Empty: 2-1190                     [128]                     --
│    └─Empty: 2-1191                     [128]                     --
│    └─BatchNorm2d: 2-1192               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1193                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1194                      [16, 128, 16, 16]         --
│    └─Empty: 2-1195                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1196                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-120               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1197        --                        --
│    └─One: 2-1198                       [1]                       --
│    └─OutputScale: 2-1199               --                        --
│    └─Empty: 2-1200                     [128, 128, 1, 1]          --
│    └─Empty: 2-1201                     [128, 128, 1, 1]          --
│    └─Empty: 2-1202                     [128]                     --
│    └─Empty: 2-1203                     [128]                     --
│    └─BatchNorm2d: 2-1204               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1205                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1206                      [16, 128, 16, 16]         --
│    └─Empty: 2-1207                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1208                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-121        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1209                 [16, 128, 16, 16]         --
│    └─Empty: 2-1210                     [16, 128, 16, 16]         --
│    └─Empty: 2-1211                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1212        --                        --
│    └─One: 2-1213                       [1]                       --
│    └─OutputScale: 2-1214               --                        --
│    └─Empty: 2-1215                     [128, 128, 3, 3]          --
│    └─Empty: 2-1216                     [128, 128, 3, 3]          --
│    └─Empty: 2-1217                     [128]                     --
│    └─Empty: 2-1218                     [128]                     --
│    └─BatchNorm2d: 2-1219               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1220                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1221                      [16, 128, 16, 16]         --
│    └─Empty: 2-1222                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1223                     [16, 128, 16, 16]         --
├─Dropout2d: 1-122                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-123        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1224                 [16, 128, 8, 8]           --
│    └─Empty: 2-1225                     [16, 128, 8, 8]           --
│    └─Empty: 2-1226                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1227        --                        --
│    └─One: 2-1228                       [1]                       --
│    └─OutputScale: 2-1229               --                        --
│    └─Empty: 2-1230                     [128, 128, 3, 3]          --
│    └─Empty: 2-1231                     [128, 128, 3, 3]          --
│    └─Empty: 2-1232                     [128]                     --
│    └─Empty: 2-1233                     [128]                     --
│    └─BatchNorm2d: 2-1234               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1235                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1236                      [16, 128, 8, 8]           --
│    └─Empty: 2-1237                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1238                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-124               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1239        --                        --
│    └─One: 2-1240                       [1]                       --
│    └─OutputScale: 2-1241               --                        --
│    └─Empty: 2-1242                     [16, 128, 1, 1]           --
│    └─Empty: 2-1243                     [16, 128, 1, 1]           --
│    └─Empty: 2-1244                     [16]                      --
│    └─Empty: 2-1245                     [16]                      --
│    └─BatchNorm2d: 2-1246               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1247                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1248                      [16, 16, 8, 8]            --
│    └─Empty: 2-1249                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1250                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-125        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1251                 [16, 128, 8, 8]           --
│    └─Empty: 2-1252                     [16, 128, 8, 8]           --
│    └─Empty: 2-1253                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1254        --                        --
│    └─One: 2-1255                       [1]                       --
│    └─OutputScale: 2-1256               --                        --
│    └─Empty: 2-1257                     [16, 128, 3, 3]           --
│    └─Empty: 2-1258                     [16, 128, 3, 3]           --
│    └─Empty: 2-1259                     [16]                      --
│    └─Empty: 2-1260                     [16]                      --
│    └─BatchNorm2d: 2-1261               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1262                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1263                      [16, 16, 8, 8]            --
│    └─Empty: 2-1264                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1265                     [16, 16, 8, 8]            --
├─Dropout2d: 1-126                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-127               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1266        --                        --
│    └─One: 2-1267                       [1]                       --
│    └─OutputScale: 2-1268               --                        --
│    └─Empty: 2-1269                     [128, 48, 1, 1]           --
│    └─Empty: 2-1270                     [128, 48, 1, 1]           --
│    └─Empty: 2-1271                     [128]                     --
│    └─Empty: 2-1272                     [128]                     --
│    └─BatchNorm2d: 2-1273               [16, 128, 64, 64]         --
│    └─Scaler: 2-1274                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1275                      [16, 128, 64, 64]         --
│    └─Empty: 2-1276                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1277                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-128        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1278                 [16, 128, 32, 32]         --
│    └─Empty: 2-1279                     [16, 128, 32, 32]         --
│    └─Empty: 2-1280                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1281        --                        --
│    └─One: 2-1282                       [1]                       --
│    └─OutputScale: 2-1283               --                        --
│    └─Empty: 2-1284                     [128, 128, 3, 3]          --
│    └─Empty: 2-1285                     [128, 128, 3, 3]          --
│    └─Empty: 2-1286                     [128]                     --
│    └─Empty: 2-1287                     [128]                     --
│    └─BatchNorm2d: 2-1288               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1289                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1290                      [16, 128, 32, 32]         --
│    └─Empty: 2-1291                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1292                     [16, 128, 32, 32]         --
├─Dropout2d: 1-129                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-130        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1293                 [16, 128, 16, 16]         --
│    └─Empty: 2-1294                     [16, 128, 16, 16]         --
│    └─Empty: 2-1295                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1296        --                        --
│    └─One: 2-1297                       [1]                       --
│    └─OutputScale: 2-1298               --                        --
│    └─Empty: 2-1299                     [128, 128, 3, 3]          --
│    └─Empty: 2-1300                     [128, 128, 3, 3]          --
│    └─Empty: 2-1301                     [128]                     --
│    └─Empty: 2-1302                     [128]                     --
│    └─BatchNorm2d: 2-1303               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1304                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1305                      [16, 128, 16, 16]         --
│    └─Empty: 2-1306                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1307                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-131               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1308        --                        --
│    └─One: 2-1309                       [1]                       --
│    └─OutputScale: 2-1310               --                        --
│    └─Empty: 2-1311                     [128, 128, 1, 1]          --
│    └─Empty: 2-1312                     [128, 128, 1, 1]          --
│    └─Empty: 2-1313                     [128]                     --
│    └─Empty: 2-1314                     [128]                     --
│    └─BatchNorm2d: 2-1315               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1316                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1317                      [16, 128, 16, 16]         --
│    └─Empty: 2-1318                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1319                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-132        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1320                 [16, 128, 16, 16]         --
│    └─Empty: 2-1321                     [16, 128, 16, 16]         --
│    └─Empty: 2-1322                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1323        --                        --
│    └─One: 2-1324                       [1]                       --
│    └─OutputScale: 2-1325               --                        --
│    └─Empty: 2-1326                     [128, 128, 3, 3]          --
│    └─Empty: 2-1327                     [128, 128, 3, 3]          --
│    └─Empty: 2-1328                     [128]                     --
│    └─Empty: 2-1329                     [128]                     --
│    └─BatchNorm2d: 2-1330               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1331                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1332                      [16, 128, 16, 16]         --
│    └─Empty: 2-1333                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1334                     [16, 128, 16, 16]         --
├─Dropout2d: 1-133                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-134        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1335                 [16, 128, 8, 8]           --
│    └─Empty: 2-1336                     [16, 128, 8, 8]           --
│    └─Empty: 2-1337                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1338        --                        --
│    └─One: 2-1339                       [1]                       --
│    └─OutputScale: 2-1340               --                        --
│    └─Empty: 2-1341                     [128, 128, 3, 3]          --
│    └─Empty: 2-1342                     [128, 128, 3, 3]          --
│    └─Empty: 2-1343                     [128]                     --
│    └─Empty: 2-1344                     [128]                     --
│    └─BatchNorm2d: 2-1345               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1346                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1347                      [16, 128, 8, 8]           --
│    └─Empty: 2-1348                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1349                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-135               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1350        --                        --
│    └─One: 2-1351                       [1]                       --
│    └─OutputScale: 2-1352               --                        --
│    └─Empty: 2-1353                     [16, 128, 1, 1]           --
│    └─Empty: 2-1354                     [16, 128, 1, 1]           --
│    └─Empty: 2-1355                     [16]                      --
│    └─Empty: 2-1356                     [16]                      --
│    └─BatchNorm2d: 2-1357               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1358                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1359                      [16, 16, 8, 8]            --
│    └─Empty: 2-1360                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1361                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-136        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1362                 [16, 128, 8, 8]           --
│    └─Empty: 2-1363                     [16, 128, 8, 8]           --
│    └─Empty: 2-1364                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1365        --                        --
│    └─One: 2-1366                       [1]                       --
│    └─OutputScale: 2-1367               --                        --
│    └─Empty: 2-1368                     [16, 128, 3, 3]           --
│    └─Empty: 2-1369                     [16, 128, 3, 3]           --
│    └─Empty: 2-1370                     [16]                      --
│    └─Empty: 2-1371                     [16]                      --
│    └─BatchNorm2d: 2-1372               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1373                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1374                      [16, 16, 8, 8]            --
│    └─Empty: 2-1375                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1376                     [16, 16, 8, 8]            --
├─Dropout2d: 1-137                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-138               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1377        --                        --
│    └─One: 2-1378                       [1]                       --
│    └─OutputScale: 2-1379               --                        --
│    └─Empty: 2-1380                     [128, 48, 1, 1]           --
│    └─Empty: 2-1381                     [128, 48, 1, 1]           --
│    └─Empty: 2-1382                     [128]                     --
│    └─Empty: 2-1383                     [128]                     --
│    └─BatchNorm2d: 2-1384               [16, 128, 64, 64]         --
│    └─Scaler: 2-1385                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1386                      [16, 128, 64, 64]         --
│    └─Empty: 2-1387                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1388                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-139        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1389                 [16, 128, 32, 32]         --
│    └─Empty: 2-1390                     [16, 128, 32, 32]         --
│    └─Empty: 2-1391                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1392        --                        --
│    └─One: 2-1393                       [1]                       --
│    └─OutputScale: 2-1394               --                        --
│    └─Empty: 2-1395                     [128, 128, 3, 3]          --
│    └─Empty: 2-1396                     [128, 128, 3, 3]          --
│    └─Empty: 2-1397                     [128]                     --
│    └─Empty: 2-1398                     [128]                     --
│    └─BatchNorm2d: 2-1399               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1400                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1401                      [16, 128, 32, 32]         --
│    └─Empty: 2-1402                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1403                     [16, 128, 32, 32]         --
├─Dropout2d: 1-140                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-141        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1404                 [16, 128, 16, 16]         --
│    └─Empty: 2-1405                     [16, 128, 16, 16]         --
│    └─Empty: 2-1406                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1407        --                        --
│    └─One: 2-1408                       [1]                       --
│    └─OutputScale: 2-1409               --                        --
│    └─Empty: 2-1410                     [128, 128, 3, 3]          --
│    └─Empty: 2-1411                     [128, 128, 3, 3]          --
│    └─Empty: 2-1412                     [128]                     --
│    └─Empty: 2-1413                     [128]                     --
│    └─BatchNorm2d: 2-1414               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1415                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1416                      [16, 128, 16, 16]         --
│    └─Empty: 2-1417                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1418                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-142               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1419        --                        --
│    └─One: 2-1420                       [1]                       --
│    └─OutputScale: 2-1421               --                        --
│    └─Empty: 2-1422                     [128, 128, 1, 1]          --
│    └─Empty: 2-1423                     [128, 128, 1, 1]          --
│    └─Empty: 2-1424                     [128]                     --
│    └─Empty: 2-1425                     [128]                     --
│    └─BatchNorm2d: 2-1426               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1427                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1428                      [16, 128, 16, 16]         --
│    └─Empty: 2-1429                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1430                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-143        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1431                 [16, 128, 16, 16]         --
│    └─Empty: 2-1432                     [16, 128, 16, 16]         --
│    └─Empty: 2-1433                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1434        --                        --
│    └─One: 2-1435                       [1]                       --
│    └─OutputScale: 2-1436               --                        --
│    └─Empty: 2-1437                     [128, 128, 3, 3]          --
│    └─Empty: 2-1438                     [128, 128, 3, 3]          --
│    └─Empty: 2-1439                     [128]                     --
│    └─Empty: 2-1440                     [128]                     --
│    └─BatchNorm2d: 2-1441               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1442                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1443                      [16, 128, 16, 16]         --
│    └─Empty: 2-1444                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1445                     [16, 128, 16, 16]         --
├─Dropout2d: 1-144                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-145        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1446                 [16, 128, 8, 8]           --
│    └─Empty: 2-1447                     [16, 128, 8, 8]           --
│    └─Empty: 2-1448                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1449        --                        --
│    └─One: 2-1450                       [1]                       --
│    └─OutputScale: 2-1451               --                        --
│    └─Empty: 2-1452                     [128, 128, 3, 3]          --
│    └─Empty: 2-1453                     [128, 128, 3, 3]          --
│    └─Empty: 2-1454                     [128]                     --
│    └─Empty: 2-1455                     [128]                     --
│    └─BatchNorm2d: 2-1456               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1457                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1458                      [16, 128, 8, 8]           --
│    └─Empty: 2-1459                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1460                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-146               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1461        --                        --
│    └─One: 2-1462                       [1]                       --
│    └─OutputScale: 2-1463               --                        --
│    └─Empty: 2-1464                     [16, 128, 1, 1]           --
│    └─Empty: 2-1465                     [16, 128, 1, 1]           --
│    └─Empty: 2-1466                     [16]                      --
│    └─Empty: 2-1467                     [16]                      --
│    └─BatchNorm2d: 2-1468               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1469                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1470                      [16, 16, 8, 8]            --
│    └─Empty: 2-1471                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1472                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-147        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1473                 [16, 128, 8, 8]           --
│    └─Empty: 2-1474                     [16, 128, 8, 8]           --
│    └─Empty: 2-1475                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1476        --                        --
│    └─One: 2-1477                       [1]                       --
│    └─OutputScale: 2-1478               --                        --
│    └─Empty: 2-1479                     [16, 128, 3, 3]           --
│    └─Empty: 2-1480                     [16, 128, 3, 3]           --
│    └─Empty: 2-1481                     [16]                      --
│    └─Empty: 2-1482                     [16]                      --
│    └─BatchNorm2d: 2-1483               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1484                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1485                      [16, 16, 8, 8]            --
│    └─Empty: 2-1486                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1487                     [16, 16, 8, 8]            --
├─Dropout2d: 1-148                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-149               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1488        --                        --
│    └─One: 2-1489                       [1]                       --
│    └─OutputScale: 2-1490               --                        --
│    └─Empty: 2-1491                     [128, 48, 1, 1]           --
│    └─Empty: 2-1492                     [128, 48, 1, 1]           --
│    └─Empty: 2-1493                     [128]                     --
│    └─Empty: 2-1494                     [128]                     --
│    └─BatchNorm2d: 2-1495               [16, 128, 64, 64]         --
│    └─Scaler: 2-1496                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1497                      [16, 128, 64, 64]         --
│    └─Empty: 2-1498                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1499                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-150        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1500                 [16, 128, 32, 32]         --
│    └─Empty: 2-1501                     [16, 128, 32, 32]         --
│    └─Empty: 2-1502                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1503        --                        --
│    └─One: 2-1504                       [1]                       --
│    └─OutputScale: 2-1505               --                        --
│    └─Empty: 2-1506                     [128, 128, 3, 3]          --
│    └─Empty: 2-1507                     [128, 128, 3, 3]          --
│    └─Empty: 2-1508                     [128]                     --
│    └─Empty: 2-1509                     [128]                     --
│    └─BatchNorm2d: 2-1510               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1511                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1512                      [16, 128, 32, 32]         --
│    └─Empty: 2-1513                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1514                     [16, 128, 32, 32]         --
├─Dropout2d: 1-151                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-152        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1515                 [16, 128, 16, 16]         --
│    └─Empty: 2-1516                     [16, 128, 16, 16]         --
│    └─Empty: 2-1517                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1518        --                        --
│    └─One: 2-1519                       [1]                       --
│    └─OutputScale: 2-1520               --                        --
│    └─Empty: 2-1521                     [128, 128, 3, 3]          --
│    └─Empty: 2-1522                     [128, 128, 3, 3]          --
│    └─Empty: 2-1523                     [128]                     --
│    └─Empty: 2-1524                     [128]                     --
│    └─BatchNorm2d: 2-1525               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1526                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1527                      [16, 128, 16, 16]         --
│    └─Empty: 2-1528                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1529                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-153               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1530        --                        --
│    └─One: 2-1531                       [1]                       --
│    └─OutputScale: 2-1532               --                        --
│    └─Empty: 2-1533                     [128, 128, 1, 1]          --
│    └─Empty: 2-1534                     [128, 128, 1, 1]          --
│    └─Empty: 2-1535                     [128]                     --
│    └─Empty: 2-1536                     [128]                     --
│    └─BatchNorm2d: 2-1537               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1538                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1539                      [16, 128, 16, 16]         --
│    └─Empty: 2-1540                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1541                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-154        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1542                 [16, 128, 16, 16]         --
│    └─Empty: 2-1543                     [16, 128, 16, 16]         --
│    └─Empty: 2-1544                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1545        --                        --
│    └─One: 2-1546                       [1]                       --
│    └─OutputScale: 2-1547               --                        --
│    └─Empty: 2-1548                     [128, 128, 3, 3]          --
│    └─Empty: 2-1549                     [128, 128, 3, 3]          --
│    └─Empty: 2-1550                     [128]                     --
│    └─Empty: 2-1551                     [128]                     --
│    └─BatchNorm2d: 2-1552               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1553                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1554                      [16, 128, 16, 16]         --
│    └─Empty: 2-1555                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1556                     [16, 128, 16, 16]         --
├─Dropout2d: 1-155                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-156        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1557                 [16, 128, 8, 8]           --
│    └─Empty: 2-1558                     [16, 128, 8, 8]           --
│    └─Empty: 2-1559                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1560        --                        --
│    └─One: 2-1561                       [1]                       --
│    └─OutputScale: 2-1562               --                        --
│    └─Empty: 2-1563                     [128, 128, 3, 3]          --
│    └─Empty: 2-1564                     [128, 128, 3, 3]          --
│    └─Empty: 2-1565                     [128]                     --
│    └─Empty: 2-1566                     [128]                     --
│    └─BatchNorm2d: 2-1567               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1568                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1569                      [16, 128, 8, 8]           --
│    └─Empty: 2-1570                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1571                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-157               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1572        --                        --
│    └─One: 2-1573                       [1]                       --
│    └─OutputScale: 2-1574               --                        --
│    └─Empty: 2-1575                     [16, 128, 1, 1]           --
│    └─Empty: 2-1576                     [16, 128, 1, 1]           --
│    └─Empty: 2-1577                     [16]                      --
│    └─Empty: 2-1578                     [16]                      --
│    └─BatchNorm2d: 2-1579               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1580                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1581                      [16, 16, 8, 8]            --
│    └─Empty: 2-1582                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1583                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-158        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1584                 [16, 128, 8, 8]           --
│    └─Empty: 2-1585                     [16, 128, 8, 8]           --
│    └─Empty: 2-1586                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1587        --                        --
│    └─One: 2-1588                       [1]                       --
│    └─OutputScale: 2-1589               --                        --
│    └─Empty: 2-1590                     [16, 128, 3, 3]           --
│    └─Empty: 2-1591                     [16, 128, 3, 3]           --
│    └─Empty: 2-1592                     [16]                      --
│    └─Empty: 2-1593                     [16]                      --
│    └─BatchNorm2d: 2-1594               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1595                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1596                      [16, 16, 8, 8]            --
│    └─Empty: 2-1597                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1598                     [16, 16, 8, 8]            --
├─Dropout2d: 1-159                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-160               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1599        --                        --
│    └─One: 2-1600                       [1]                       --
│    └─OutputScale: 2-1601               --                        --
│    └─Empty: 2-1602                     [128, 48, 1, 1]           --
│    └─Empty: 2-1603                     [128, 48, 1, 1]           --
│    └─Empty: 2-1604                     [128]                     --
│    └─Empty: 2-1605                     [128]                     --
│    └─BatchNorm2d: 2-1606               [16, 128, 64, 64]         --
│    └─Scaler: 2-1607                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1608                      [16, 128, 64, 64]         --
│    └─Empty: 2-1609                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1610                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-161        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1611                 [16, 128, 32, 32]         --
│    └─Empty: 2-1612                     [16, 128, 32, 32]         --
│    └─Empty: 2-1613                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1614        --                        --
│    └─One: 2-1615                       [1]                       --
│    └─OutputScale: 2-1616               --                        --
│    └─Empty: 2-1617                     [128, 128, 3, 3]          --
│    └─Empty: 2-1618                     [128, 128, 3, 3]          --
│    └─Empty: 2-1619                     [128]                     --
│    └─Empty: 2-1620                     [128]                     --
│    └─BatchNorm2d: 2-1621               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1622                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1623                      [16, 128, 32, 32]         --
│    └─Empty: 2-1624                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1625                     [16, 128, 32, 32]         --
├─Dropout2d: 1-162                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-163        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1626                 [16, 128, 16, 16]         --
│    └─Empty: 2-1627                     [16, 128, 16, 16]         --
│    └─Empty: 2-1628                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1629        --                        --
│    └─One: 2-1630                       [1]                       --
│    └─OutputScale: 2-1631               --                        --
│    └─Empty: 2-1632                     [128, 128, 3, 3]          --
│    └─Empty: 2-1633                     [128, 128, 3, 3]          --
│    └─Empty: 2-1634                     [128]                     --
│    └─Empty: 2-1635                     [128]                     --
│    └─BatchNorm2d: 2-1636               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1637                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1638                      [16, 128, 16, 16]         --
│    └─Empty: 2-1639                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1640                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-164               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1641        --                        --
│    └─One: 2-1642                       [1]                       --
│    └─OutputScale: 2-1643               --                        --
│    └─Empty: 2-1644                     [128, 128, 1, 1]          --
│    └─Empty: 2-1645                     [128, 128, 1, 1]          --
│    └─Empty: 2-1646                     [128]                     --
│    └─Empty: 2-1647                     [128]                     --
│    └─BatchNorm2d: 2-1648               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1649                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1650                      [16, 128, 16, 16]         --
│    └─Empty: 2-1651                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1652                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-165        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1653                 [16, 128, 16, 16]         --
│    └─Empty: 2-1654                     [16, 128, 16, 16]         --
│    └─Empty: 2-1655                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1656        --                        --
│    └─One: 2-1657                       [1]                       --
│    └─OutputScale: 2-1658               --                        --
│    └─Empty: 2-1659                     [128, 128, 3, 3]          --
│    └─Empty: 2-1660                     [128, 128, 3, 3]          --
│    └─Empty: 2-1661                     [128]                     --
│    └─Empty: 2-1662                     [128]                     --
│    └─BatchNorm2d: 2-1663               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1664                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1665                      [16, 128, 16, 16]         --
│    └─Empty: 2-1666                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1667                     [16, 128, 16, 16]         --
├─Dropout2d: 1-166                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-167        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1668                 [16, 128, 8, 8]           --
│    └─Empty: 2-1669                     [16, 128, 8, 8]           --
│    └─Empty: 2-1670                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1671        --                        --
│    └─One: 2-1672                       [1]                       --
│    └─OutputScale: 2-1673               --                        --
│    └─Empty: 2-1674                     [128, 128, 3, 3]          --
│    └─Empty: 2-1675                     [128, 128, 3, 3]          --
│    └─Empty: 2-1676                     [128]                     --
│    └─Empty: 2-1677                     [128]                     --
│    └─BatchNorm2d: 2-1678               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1679                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1680                      [16, 128, 8, 8]           --
│    └─Empty: 2-1681                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1682                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-168               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1683        --                        --
│    └─One: 2-1684                       [1]                       --
│    └─OutputScale: 2-1685               --                        --
│    └─Empty: 2-1686                     [16, 128, 1, 1]           --
│    └─Empty: 2-1687                     [16, 128, 1, 1]           --
│    └─Empty: 2-1688                     [16]                      --
│    └─Empty: 2-1689                     [16]                      --
│    └─BatchNorm2d: 2-1690               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1691                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1692                      [16, 16, 8, 8]            --
│    └─Empty: 2-1693                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1694                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-169        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1695                 [16, 128, 8, 8]           --
│    └─Empty: 2-1696                     [16, 128, 8, 8]           --
│    └─Empty: 2-1697                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1698        --                        --
│    └─One: 2-1699                       [1]                       --
│    └─OutputScale: 2-1700               --                        --
│    └─Empty: 2-1701                     [16, 128, 3, 3]           --
│    └─Empty: 2-1702                     [16, 128, 3, 3]           --
│    └─Empty: 2-1703                     [16]                      --
│    └─Empty: 2-1704                     [16]                      --
│    └─BatchNorm2d: 2-1705               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1706                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1707                      [16, 16, 8, 8]            --
│    └─Empty: 2-1708                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1709                     [16, 16, 8, 8]            --
├─Dropout2d: 1-170                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-171               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1710        --                        --
│    └─One: 2-1711                       [1]                       --
│    └─OutputScale: 2-1712               --                        --
│    └─Empty: 2-1713                     [128, 48, 1, 1]           --
│    └─Empty: 2-1714                     [128, 48, 1, 1]           --
│    └─Empty: 2-1715                     [128]                     --
│    └─Empty: 2-1716                     [128]                     --
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Empty: 2-1720                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1721                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Empty: 2-1723                     [16, 128, 32, 32]         --
│    └─Empty: 2-1724                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1725        --                        --
│    └─One: 2-1726                       [1]                       --
│    └─OutputScale: 2-1727               --                        --
│    └─Empty: 2-1728                     [128, 128, 3, 3]          --
│    └─Empty: 2-1729                     [128, 128, 3, 3]          --
│    └─Empty: 2-1730                     [128]                     --
│    └─Empty: 2-1731                     [128]                     --
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─Empty: 2-1735                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1736                     [16, 128, 32, 32]         --
├─Dropout2d: 1-173                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1740        --                        --
│    └─One: 2-1741                       [1]                       --
│    └─OutputScale: 2-1742               --                        --
│    └─Empty: 2-1743                     [128, 128, 3, 3]          --
│    └─Empty: 2-1744                     [128, 128, 3, 3]          --
│    └─Empty: 2-1745                     [128]                     --
│    └─Empty: 2-1746                     [128]                     --
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─Empty: 2-1750                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1751                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-175               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1752        --                        --
│    └─One: 2-1753                       [1]                       --
│    └─OutputScale: 2-1754               --                        --
│    └─Empty: 2-1755                     [128, 128, 1, 1]          --
│    └─Empty: 2-1756                     [128, 128, 1, 1]          --
│    └─Empty: 2-1757                     [128]                     --
│    └─Empty: 2-1758                     [128]                     --
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Empty: 2-1762                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1763                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Empty: 2-1765                     [16, 128, 16, 16]         --
│    └─Empty: 2-1766                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1767        --                        --
│    └─One: 2-1768                       [1]                       --
│    └─OutputScale: 2-1769               --                        --
│    └─Empty: 2-1770                     [128, 128, 3, 3]          --
│    └─Empty: 2-1771                     [128, 128, 3, 3]          --
│    └─Empty: 2-1772                     [128]                     --
│    └─Empty: 2-1773                     [128]                     --
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─Empty: 2-1777                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1778                     [16, 128, 16, 16]         --
├─Dropout2d: 1-177                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1782        --                        --
│    └─One: 2-1783                       [1]                       --
│    └─OutputScale: 2-1784               --                        --
│    └─Empty: 2-1785                     [128, 128, 3, 3]          --
│    └─Empty: 2-1786                     [128, 128, 3, 3]          --
│    └─Empty: 2-1787                     [128]                     --
│    └─Empty: 2-1788                     [128]                     --
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─Empty: 2-1792                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1793                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-179               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1794        --                        --
│    └─One: 2-1795                       [1]                       --
│    └─OutputScale: 2-1796               --                        --
│    └─Empty: 2-1797                     [16, 128, 1, 1]           --
│    └─Empty: 2-1798                     [16, 128, 1, 1]           --
│    └─Empty: 2-1799                     [16]                      --
│    └─Empty: 2-1800                     [16]                      --
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Empty: 2-1804                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1805                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1809        --                        --
│    └─One: 2-1810                       [1]                       --
│    └─OutputScale: 2-1811               --                        --
│    └─Empty: 2-1812                     [16, 128, 3, 3]           --
│    └─Empty: 2-1813                     [16, 128, 3, 3]           --
│    └─Empty: 2-1814                     [16]                      --
│    └─Empty: 2-1815                     [16]                      --
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─Empty: 2-1819                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1820                     [16, 16, 8, 8]            --
├─Dropout2d: 1-181                       [16, 16, 8, 8]            --
├─Linear: 1-182                          [16, 5]                   5,126
│    └─OutputShiftSqueeze: 2-1821        --                        --
│    └─One: 2-1822                       [1]                       --
│    └─OutputScale: 2-1823               --                        --
│    └─Empty: 2-1824                     [5, 1024]                 --
│    └─Empty: 2-1825                     [5, 1024]                 --
│    └─Empty: 2-1826                     [16, 5]                   --
│    └─Empty: 2-1827                     [16, 5]                   --
│    └─Clamp: 2-1828                     [16, 5]                   --
├─Linear: 1-183                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1829        --                        --
│    └─One: 2-1830                       [1]                       --
│    └─OutputScale: 2-1831               --                        --
│    └─Empty: 2-1832                     [5, 1024]                 --
│    └─Empty: 2-1833                     [5, 1024]                 --
│    └─Empty: 2-1834                     [16, 5]                   --
│    └─Empty: 2-1835                     [16, 5]                   --
│    └─Clamp: 2-1836                     [16, 5]                   --
├─Linear: 1-184                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1837        --                        --
│    └─One: 2-1838                       [1]                       --
│    └─OutputScale: 2-1839               --                        --
│    └─Empty: 2-1840                     [5, 1024]                 --
│    └─Empty: 2-1841                     [5, 1024]                 --
│    └─Empty: 2-1842                     [16, 5]                   --
│    └─Empty: 2-1843                     [16, 5]                   --
│    └─Clamp: 2-1844                     [16, 5]                   --
├─Linear: 1-185                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1845        --                        --
│    └─One: 2-1846                       [1]                       --
│    └─OutputScale: 2-1847               --                        --
│    └─Empty: 2-1848                     [5, 1024]                 --
│    └─Empty: 2-1849                     [5, 1024]                 --
│    └─Empty: 2-1850                     [16, 5]                   --
│    └─Empty: 2-1851                     [16, 5]                   --
│    └─Clamp: 2-1852                     [16, 5]                   --
├─Linear: 1-186                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1853        --                        --
│    └─One: 2-1854                       [1]                       --
│    └─OutputScale: 2-1855               --                        --
│    └─Empty: 2-1856                     [5, 1024]                 --
│    └─Empty: 2-1857                     [5, 1024]                 --
│    └─Empty: 2-1858                     [16, 5]                   --
│    └─Empty: 2-1859                     [16, 5]                   --
│    └─Clamp: 2-1860                     [16, 5]                   --
├─Linear: 1-187                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1861        --                        --
│    └─One: 2-1862                       [1]                       --
│    └─OutputScale: 2-1863               --                        --
│    └─Empty: 2-1864                     [5, 1024]                 --
│    └─Empty: 2-1865                     [5, 1024]                 --
│    └─Empty: 2-1866                     [16, 5]                   --
│    └─Empty: 2-1867                     [16, 5]                   --
│    └─Clamp: 2-1868                     [16, 5]                   --
├─Linear: 1-188                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1869        --                        --
│    └─One: 2-1870                       [1]                       --
│    └─OutputScale: 2-1871               --                        --
│    └─Empty: 2-1872                     [5, 1024]                 --
│    └─Empty: 2-1873                     [5, 1024]                 --
│    └─Empty: 2-1874                     [16, 5]                   --
│    └─Empty: 2-1875                     [16, 5]                   --
│    └─Clamp: 2-1876                     [16, 5]                   --
├─Linear: 1-189                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1877        --                        --
│    └─One: 2-1878                       [1]                       --
│    └─OutputScale: 2-1879               --                        --
│    └─Empty: 2-1880                     [5, 1024]                 --
│    └─Empty: 2-1881                     [5, 1024]                 --
│    └─Empty: 2-1882                     [16, 5]                   --
│    └─Empty: 2-1883                     [16, 5]                   --
│    └─Clamp: 2-1884                     [16, 5]                   --
├─Linear: 1-190                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1885        --                        --
│    └─One: 2-1886                       [1]                       --
│    └─OutputScale: 2-1887               --                        --
│    └─Empty: 2-1888                     [5, 1024]                 --
│    └─Empty: 2-1889                     [5, 1024]                 --
│    └─Empty: 2-1890                     [16, 5]                   --
│    └─Empty: 2-1891                     [16, 5]                   --
│    └─Clamp: 2-1892                     [16, 5]                   --
├─Linear: 1-191                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1893        --                        --
│    └─One: 2-1894                       [1]                       --
│    └─OutputScale: 2-1895               --                        --
│    └─Empty: 2-1896                     [5, 1024]                 --
│    └─Empty: 2-1897                     [5, 1024]                 --
│    └─Empty: 2-1898                     [16, 5]                   --
│    └─Empty: 2-1899                     [16, 5]                   --
│    └─Clamp: 2-1900                     [16, 5]                   --
├─Linear: 1-192                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1901        --                        --
│    └─One: 2-1902                       [1]                       --
│    └─OutputScale: 2-1903               --                        --
│    └─Empty: 2-1904                     [5, 1024]                 --
│    └─Empty: 2-1905                     [5, 1024]                 --
│    └─Empty: 2-1906                     [16, 5]                   --
│    └─Empty: 2-1907                     [16, 5]                   --
│    └─Clamp: 2-1908                     [16, 5]                   --
├─Linear: 1-193                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1909        --                        --
│    └─One: 2-1910                       [1]                       --
│    └─OutputScale: 2-1911               --                        --
│    └─Empty: 2-1912                     [5, 1024]                 --
│    └─Empty: 2-1913                     [5, 1024]                 --
│    └─Empty: 2-1914                     [16, 5]                   --
│    └─Empty: 2-1915                     [16, 5]                   --
│    └─Clamp: 2-1916                     [16, 5]                   --
├─Linear: 1-194                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1917        --                        --
│    └─One: 2-1918                       [1]                       --
│    └─OutputScale: 2-1919               --                        --
│    └─Empty: 2-1920                     [5, 1024]                 --
│    └─Empty: 2-1921                     [5, 1024]                 --
│    └─Empty: 2-1922                     [16, 5]                   --
│    └─Empty: 2-1923                     [16, 5]                   --
│    └─Clamp: 2-1924                     [16, 5]                   --
├─Linear: 1-195                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1925        --                        --
│    └─One: 2-1926                       [1]                       --
│    └─OutputScale: 2-1927               --                        --
│    └─Empty: 2-1928                     [5, 1024]                 --
│    └─Empty: 2-1929                     [5, 1024]                 --
│    └─Empty: 2-1930                     [16, 5]                   --
│    └─Empty: 2-1931                     [16, 5]                   --
│    └─Clamp: 2-1932                     [16, 5]                   --
├─Linear: 1-196                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1933        --                        --
│    └─One: 2-1934                       [1]                       --
│    └─OutputScale: 2-1935               --                        --
│    └─Empty: 2-1936                     [5, 1024]                 --
│    └─Empty: 2-1937                     [5, 1024]                 --
│    └─Empty: 2-1938                     [16, 5]                   --
│    └─Empty: 2-1939                     [16, 5]                   --
│    └─Clamp: 2-1940                     [16, 5]                   --
├─Linear: 1-197                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1941        --                        --
│    └─One: 2-1942                       [1]                       --
│    └─OutputScale: 2-1943               --                        --
│    └─Empty: 2-1944                     [5, 1024]                 --
│    └─Empty: 2-1945                     [5, 1024]                 --
│    └─Empty: 2-1946                     [16, 5]                   --
│    └─Empty: 2-1947                     [16, 5]                   --
│    └─Clamp: 2-1948                     [16, 5]                   --
==========================================================================================
Total params: 640,150
Trainable params: 640,096
Non-trainable params: 54
Total mult-adds (M): 0.37
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 30.67
Params size (MB): 2.54
Estimated Total Size (MB): 234.54
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.512 | Acc: 26.125% | Wgt Acc: 31.418%
	I - Batch: 100 | Loss: 1.434 | Acc: 31.688% | Wgt Acc: 38.157%
	I - Batch: 150 | Loss: 1.389 | Acc: 32.625% | Wgt Acc: 39.119%
	I - Batch: 200 | Loss: 1.361 | Acc: 33.594% | Wgt Acc: 40.472%
	I - Batch: 250 | Loss: 1.343 | Acc: 34.600% | Wgt Acc: 41.640%
I - num batch: 285
I - Train -- Loss: 1.324 | Acc: 35.736% | Wgt Acc: 43.039% | LR: 1.000000e-03 | Dur: 171.38s
I - Confusion Matrix: [row->prediction - col->label]
[[329.  29.  73. 175. 133.]
 [ 36. 204. 176.  69. 177.]
 [218. 437. 718. 221. 540.]
 [223. 110. 157. 375. 150.]
 [  0.   0.   0.   0.   0.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.692 | Acc: 31.755% | Wgt Acc: 43.790% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 42.   1.   3.  13.  14.]
 [  5.  24.   8.   4.  32.]
 [ 19.  50.  60.  34. 125.]
 [ 22.   3.   4.  35.   9.]
 [  0.   0.   0.   0.   0.]]

I - Local maximum validation set accuracy:  31.76

I - Validation set results: 
[14-1-2-0.83][50-3-3-0.13][124-2-2-1.58][127-0-0-2.33][443-2-2-1.98][567-0-0-0.93][573-1-1-0.66][615-0-2-1.37][695-1-2-1.61][722-3-3-1.57]
[826-0-0-0.39][878-0-0-1.21][1103-0-2-0.41][1212-3-2-1.55][1368-0-0-1.24][2181-2-2-1.10][2476-2-2-0.40][2721-2-2-1.88][2818-1-2-0.75][2886-2-2-0.97]
[3231-2-2-1.86][3333-2-2-0.91][3482-2-2-1.98][3536-3-3-0.87][3625-1-1-1.40][3909-0-3-0.26][4035-0-3-1.76][4140-0-0-1.86][4214-1-2-2.13][4346-1-2-0.58]
[4581-2-2-1.49][4708-3-1-0.45][4838-3-3-0.37][4845-1-2-1.57][4868-0-3-0.20][4939-0-2-1.10][4984-2-2-0.87][5078-1-2-0.67][5396-0-0-2.08][5479-1-1-0.85]
[5717-0-0-1.56][5843-1-1-1.17][5949-3-3-0.58][5987-2-2-1.29][6014-3-1-0.24][6033-3-2-0.21][6313-0-0-0.62][6421-3-3-0.52][6500-1-2-1.03][6583-3-2-0.76]
[6683-3-2-0.25][6825-2-0-0.32][6998-3-2-1.16][7049-3-2-0.74][7517-1-1-1.22][7521-1-1-0.61][7528-1-2-0.80][7949-1-2-1.69][8135-1-0-0.71][8185-3-3-1.05]
[8269-3-2-1.32][8273-3-3-0.40][8543-3-0-1.62][8666-1-3-0.45][8672-0-0-1.61][8903-1-2-1.98][9001-2-1-1.06][9036-2-2-1.61][9281-3-2-0.31][9300-2-2-0.87]
[9571-0-2-0.32][9617-1-1-0.40][9644-2-2-2.34][9705-2-2-1.19][9801-0-3-0.28][9803-3-3-0.61][9865-3-3-0.60][9896-2-2-1.66][10314-1-2-1.06][10337-3-3-1.30]
[10403-0-2-1.66][10653-2-1-1.28][10704-2-1-0.88][10719-1-2-1.11][10727-1-1-0.91][10836-0-0-2.17][10969-2-2-0.73][11042-0-0-0.39][11088-1-2-2.16][11322-0-0-1.61]
[11398-2-2-2.22][11499-0-0-0.37][11502-3-2-0.34][11512-3-2-0.59][11608-1-1-1.38][11610-0-3-0.35][11692-0-3-0.37][11905-0-0-1.60][11993-1-2-0.93][12002-2-3-1.60]
[12052-0-0-1.10][12201-0-3-0.30][12235-2-2-0.63][12320-1-2-0.70][12377-2-2-1.09][12398-2-3-0.43][12503-1-2-2.34][12617-0-3-0.90][12685-3-2-1.11][12738-2-2-1.18]
[12742-2-2-2.24][12823-0-3-1.30][13110-1-3-0.77][13240-3-3-0.79][13253-1-1-0.99][13273-0-0-2.03][13634-1-2-2.41][13763-2-3-0.71][13905-3-3-0.50][14060-2-1-0.70]
[14065-3-3-0.26][14147-3-2-0.73][14595-2-2-1.46][14687-2-2-0.87][14788-2-2-1.66][14869-1-2-2.04][14872-3-0-0.75][14877-1-2-1.42][14927-0-2-0.73][15066-0-0-2.61]
[15175-1-2-0.88][15178-2-2-0.71][15375-3-3-0.94][15389-3-2-0.60][15568-2-1-0.85][15675-3-3-0.64][15869-1-2-1.31][16207-3-0-0.31][16236-0-2-1.98][16302-3-2-1.44]
[16331-2-2-1.46][16381-0-0-0.88][16488-1-1-1.91][16495-0-0-1.14][16650-0-0-2.04][16719-1-2-0.43][16801-0-0-3.00][16828-0-0-0.84][17137-3-0-0.43][17245-1-2-0.92]
[17278-3-2-0.36][17282-0-0-0.06][17311-2-2-1.74][17336-2-2-1.19][17608-3-3-1.47][17627-0-1-0.48][17877-3-2-1.19][17924-1-2-1.16][17984-3-0-1.71][18211-0-3-1.17]
[18276-3-0-1.21][18287-1-2-0.50][18394-0-0-0.74][18428-0-3-0.47][18442-0-3-0.68][18478-3-0-0.56][18607-0-1-0.21][18616-0-1-0.07][18663-0-0-0.68][18718-0-0-0.87]
[18766-2-2-2.05][18824-2-2-0.56][18890-3-2-0.78][18930-3-2-0.50][18938-3-2-0.16][19817-1-2-1.23][19839-0-2-1.42][19930-3-3-0.36][19944-0-2-1.97][20036-2-2-2.12]
[20101-3-3-0.52][20474-1-2-1.52][20547-3-0-0.31][20929-2-2-1.82][21245-1-1-1.25][21257-3-2-1.60][21293-1-1-1.18][21316-1-2-1.20][21384-1-2-3.47][21448-1-2-0.73]
[21483-0-0-0.72][21487-2-2-1.89][21714-0-2-0.70][21943-3-2-1.17][21947-0-0-0.76][21948-0-0-2.79][21965-2-2-1.15][21998-1-1-0.85][22025-0-3-0.94][22228-3-3-1.29]
[22446-1-1-1.19][22494-3-0-0.44][22757-0-0-2.11][22811-3-2-1.29][22976-3-1-0.98][22985-3-3-0.69][23014-0-2-0.65][23112-1-1-1.21][23144-3-0-1.63][23168-2-0-1.17]
[23219-0-0-0.94][23363-3-3-1.85][23470-0-2-0.70][23486-2-2-1.23][23497-0-3-1.75][23516-0-0-3.08][23690-1-2-1.99][23921-2-2-1.04][23936-1-2-0.64][24040-3-2-0.87]
[24111-1-2-0.76][24182-0-3-1.85][24238-3-3-0.71][24290-2-0-1.73][24345-0-0-1.78][24364-1-2-1.45][24427-3-3-1.02][24477-2-2-2.06][24495-2-1-0.22][24893-2-2-1.92]
[25012-1-2-0.08][25121-2-2-1.63][25165-3-3-0.86][25183-0-0-0.51][25297-3-3-0.76][25398-0-0-0.42][25574-2-2-1.49][25644-1-1-0.84][25718-1-2-1.01][25774-2-2-1.32]
[26032-3-3-0.63][26051-3-3-0.47][26120-0-2-0.89][26321-1-1-0.76][26732-1-1-0.83][26784-3-3-1.09][26827-3-2-0.82][26833-0-3-1.45][26838-2-2-0.74][26860-1-2-1.82]
[26948-0-0-0.81][27049-3-0-1.10][27098-1-2-0.46][27526-0-0-1.13][27639-3-3-1.13][27698-3-3-0.96][27772-0-3-1.68][27890-1-1-0.56][28040-0-2-0.75][28503-2-2-2.64]
[28577-1-2-1.37][28959-0-0-2.65][29198-3-3-0.20][29777-0-0-1.33][29877-2-1-0.76][30035-1-2-1.16][30098-0-3-0.96][30326-1-1-0.88][30572-2-2-1.40][30716-0-1-0.47]
[30806-2-2-0.76][30906-1-2-0.64][31007-0-0-1.16][31181-3-2-1.49][31238-0-3-0.46][31347-0-2-1.32][31422-2-2-0.61][31429-3-0-0.37][31431-0-2-0.42][31432-1-1-0.81]
[31477-0-3-0.87][31524-1-2-0.18][31597-1-2-1.76][31619-1-2-1.29][31701-0-2-0.42][31755-0-2-1.10][31854-3-1-0.85][32074-1-2-0.72][32078-3-2-1.22][32111-1-1-0.95]
[32127-1-2-2.07][32140-3-2-1.05][32263-2-2-0.59][32365-0-0-0.67][32411-2-3-1.75][32429-3-0-1.22][32473-3-2-0.23][32574-3-3-1.98][32584-0-3-0.16][32622-0-1-0.38]
[32858-3-2-0.87][32969-3-3-1.17][33016-2-2-2.28][33031-1-3-1.25][33035-2-2-2.43][33133-2-2-1.44][33173-2-1-0.78][33175-3-2-2.11][33306-3-2-1.00][33309-2-2-0.47]
[33474-0-3-0.39][33478-2-2-0.66][33618-1-1-0.75][33712-0-2-0.10][33782-2-2-1.93][33914-3-2-0.62][34076-3-2-1.76][34112-2-2-1.29][34138-2-2-1.93][34239-1-2-1.39]
[34364-2-2-1.94][34617-1-2-0.41][34751-3-3-1.01][34783-2-2-0.95][35015-3-2-1.39][35018-1-2-1.43][35288-2-2-0.96][0-4-2-0.57][1-4-2-1.86][2-4-2-0.22]
[3-4-2-1.31][4-4-2-0.77][5-4-1-1.22][6-4-0-1.74][7-4-2-0.34][8-4-2-1.58][9-4-2-1.02][10-4-0-0.44][11-4-2-2.22][12-4-2-2.06]
[14-4-1-0.31][15-4-3-0.88][16-4-1-0.68][17-4-2-0.87][18-4-2-1.16][19-4-3-0.73][20-4-1-0.17][21-4-2-1.47][22-4-2-0.56][23-4-1-0.49]
[24-4-2-2.23][25-4-2-0.40][26-4-3-0.24][27-4-0-0.87][28-4-1-1.20][29-4-2-0.96][30-4-2-1.31][31-4-2-2.24][32-4-2-1.19][33-4-2-0.53]
[34-4-2-0.55][35-4-3-1.22][37-4-2-0.76][39-4-3-1.28][40-4-2-1.10][41-4-3-0.20][42-4-2-1.36][43-4-1-0.73][45-4-2-0.78][46-4-2-0.97]
[47-4-2-0.63][48-4-2-1.52][51-4-2-0.74][52-4-2-1.10][53-4-2-1.19][54-4-2-0.21][55-4-2-1.32][56-4-1-1.23][57-4-3-0.38][58-4-2-1.82]
[59-4-3-0.15][60-4-1-0.53][61-4-1-0.84][62-4-2-1.67][63-4-2-2.28][64-4-2-0.10][65-4-1-0.82][66-4-1-1.20][67-4-2-1.48][68-4-2-1.39]
[69-4-0-1.52][70-4-2-1.03][72-4-2-0.75][73-4-1-0.90][74-4-2-1.65][75-4-0-0.19][77-4-1-0.93][78-4-2-1.70][79-4-2-1.15][80-4-1-0.91]
[81-4-2-1.43][82-4-2-0.65][83-4-2-1.25][84-4-2-1.54][85-4-2-1.82][86-4-2-0.95][87-4-1-0.46][88-4-2-0.38][89-4-2-1.80][90-4-2-0.64]
[91-4-2-1.32][92-4-3-0.26][93-4-0-0.34][94-4-2-0.34][95-4-2-1.24][96-4-2-1.08][97-4-2-0.60][98-4-2-2.07][99-4-2-0.63][100-4-2-1.00]
[101-4-2-1.68][102-4-2-1.20][103-4-2-1.01][104-4-2-1.10][105-4-2-0.80][106-4-1-1.32][107-4-1-0.65][108-4-2-0.66][109-4-1-0.63][110-4-2-0.28]
[111-4-0-2.06][112-4-0-0.33][113-4-2-1.28][114-4-2-1.00][115-4-2-0.44][116-4-0-0.06][117-4-2-1.55][119-4-2-2.66][121-4-2-0.58][122-4-1-0.77]
[124-4-2-1.09][125-4-2-0.69][126-4-2-1.80][127-4-2-1.69][128-4-2-1.03][129-4-1-0.31][130-4-2-0.81][131-4-2-2.19][132-4-2-1.57][133-4-0-2.23]
[135-4-2-1.31][136-4-2-0.35][137-4-2-0.71][138-4-2-1.33][139-4-2-0.82][140-4-2-0.70][141-4-2-1.87][142-4-2-1.05][143-4-2-1.35][144-4-2-0.98]
[145-4-2-1.50][148-4-0-1.04][149-4-2-1.39][150-4-2-1.09][151-4-2-1.41][152-4-1-1.10][153-4-2-1.56][154-4-2-2.02][155-4-1-1.39][156-4-0-0.04]
[157-4-2-0.65][158-4-1-0.71][160-4-2-0.94][161-4-2-0.94][162-4-2-1.45][164-4-2-0.73][165-4-1-0.27][167-4-0-0.58][168-4-2-0.37][170-4-2-0.40]
[171-4-1-1.27][172-4-1-1.49][173-4-0-0.51][174-4-2-0.54][175-4-1-0.68][177-4-2-0.14][178-4-2-1.00][179-4-1-0.82][180-4-2-0.66][181-4-2-1.84]
[182-4-2-2.41][183-4-2-1.01][184-4-2-2.14][186-4-2-0.61][187-4-2-1.60][188-4-2-0.84][189-4-2-1.27][190-4-1-0.34][191-4-2-1.30][192-4-1-0.77]
[193-4-2-2.49][194-4-2-1.40][195-4-2-1.14][196-4-2-0.69][197-4-1-0.76][198-4-2-1.47][199-4-2-1.07]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.185 | Acc: 43.250% | Wgt Acc: 52.222%
	I - Batch: 100 | Loss: 1.177 | Acc: 43.812% | Wgt Acc: 53.020%
	I - Batch: 150 | Loss: 1.182 | Acc: 43.292% | Wgt Acc: 52.353%
	I - Batch: 200 | Loss: 1.177 | Acc: 43.312% | Wgt Acc: 52.298%
	I - Batch: 250 | Loss: 1.171 | Acc: 43.525% | Wgt Acc: 52.505%
I - num batch: 285
I - Train -- Loss: 1.165 | Acc: 43.736% | Wgt Acc: 52.628% | LR: 1.000000e-03 | Dur: 174.40s
I - Confusion Matrix: [row->prediction - col->label]
[[483.  26.  46. 211. 138.]
 [ 29. 347. 179.  76. 211.]
 [ 98. 310. 770. 163. 533.]
 [196.  97. 128. 390. 118.]
 [  0.   0.   1.   0.   0.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.669 | Acc: 31.755% | Wgt Acc: 43.225% | Dur: 15.88s
I - Confusion Matrix: [row->prediction - col->label]
[[38.  1.  4. 14. 22.]
 [ 4. 39. 23. 13. 54.]
 [13. 31. 41. 20. 75.]
 [33.  7.  7. 39. 25.]
 [ 0.  0.  0.  0.  4.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.163 | Acc: 42.125% | Wgt Acc: 51.743%
	I - Batch: 100 | Loss: 1.111 | Acc: 46.000% | Wgt Acc: 55.540%
	I - Batch: 150 | Loss: 1.124 | Acc: 45.875% | Wgt Acc: 55.323%
	I - Batch: 200 | Loss: 1.121 | Acc: 45.375% | Wgt Acc: 54.564%
	I - Batch: 250 | Loss: 1.106 | Acc: 46.225% | Wgt Acc: 55.135%
I - num batch: 285
I - Train -- Loss: 1.106 | Acc: 46.264% | Wgt Acc: 55.115% | LR: 1.000000e-03 | Dur: 171.26s
I - Confusion Matrix: [row->prediction - col->label]
[[486.  27.  45. 181. 138.]
 [ 20. 378. 169.  63. 150.]
 [ 96. 289. 771. 157. 513.]
 [202.  85. 133. 439. 168.]
 [  2.   1.   6.   0.  31.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.465 | Acc: 35.897% | Wgt Acc: 47.422% | Dur: 15.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   3.   3.  27.  26.]
 [  1.  22.   9.   4.  16.]
 [ 11.  46.  54.  13. 113.]
 [ 21.   5.   9.  42.  15.]
 [  1.   2.   0.   0.  10.]]

I - Local maximum validation set accuracy:  35.90

I - Validation set results: 
[14-1-2-0.72][50-3-2-0.21][124-2-2-1.34][127-0-0-2.81][443-2-2-1.64][567-0-0-1.55][573-1-1-0.63][615-0-0-2.04][695-1-2-0.31][722-3-3-2.02]
[826-0-0-2.04][878-0-0-1.89][1103-0-2-0.09][1212-3-0-0.60][1368-0-0-3.48][2181-2-3-0.43][2476-2-2-0.41][2721-2-2-1.43][2818-1-2-0.08][2886-2-2-1.05]
[3231-2-2-1.67][3333-2-1-0.72][3482-2-2-1.63][3536-3-3-0.92][3625-1-1-1.59][3909-0-0-0.05][4035-0-3-1.65][4140-0-0-1.44][4214-1-3-1.10][4346-1-0-0.38]
[4581-2-2-1.93][4708-3-3-0.54][4838-3-0-0.47][4845-1-2-0.82][4868-0-0-2.10][4939-0-2-0.80][4984-2-2-0.55][5078-1-2-0.16][5396-0-0-3.82][5479-1-1-0.99]
[5717-0-0-1.22][5843-1-2-0.86][5949-3-0-1.84][5987-2-1-0.54][6014-3-3-0.37][6033-3-0-1.14][6313-0-3-1.14][6421-3-3-1.63][6500-1-2-0.61][6583-3-2-0.67]
[6683-3-3-0.23][6825-2-0-1.01][6998-3-3-0.24][7049-3-3-0.92][7517-1-1-1.62][7521-1-1-0.72][7528-1-2-0.55][7949-1-2-1.07][8135-1-0-0.79][8185-3-0-1.61]
[8269-3-2-1.55][8273-3-3-0.74][8543-3-0-3.79][8666-1-1-0.39][8672-0-0-1.95][8903-1-3--0.14][9001-2-1-1.27][9036-2-2-1.32][9281-3-3-0.46][9300-2-2-0.71]
[9571-0-0-0.45][9617-1-2-0.61][9644-2-2-2.04][9705-2-2-0.69][9801-0-3-0.59][9803-3-3-0.88][9865-3-0-2.44][9896-2-2-1.27][10314-1-2-1.14][10337-3-3-1.43]
[10403-0-2-1.25][10653-2-1-0.62][10704-2-3-0.59][10719-1-2-1.24][10727-1-2-0.93][10836-0-3-2.02][10969-2-3-0.26][11042-0-0-1.50][11088-1-2-2.28][11322-0-0-1.42]
[11398-2-2-1.42][11499-0-0-0.11][11502-3-3-1.14][11512-3-2-0.38][11608-1-1-1.48][11610-0-0-0.38][11692-0-3-1.49][11905-0-0-2.66][11993-1-2-1.18][12002-2-3-0.88]
[12052-0-0-1.56][12201-0-0-1.23][12235-2-2-1.71][12320-1-2-0.56][12377-2-2-0.68][12398-2-3-0.36][12503-1-2-1.29][12617-0-1-0.86][12685-3-3-0.52][12738-2-2-0.67]
[12742-2-2-1.99][12823-0-3-1.59][13110-1-2-0.91][13240-3-3-0.83][13253-1-2-0.79][13273-0-0-2.89][13634-1-2-1.67][13763-2-3-0.50][13905-3-3-0.29][14060-2-2-0.73]
[14065-3-0-1.96][14147-3-1--0.05][14595-2-2-0.94][14687-2-2-1.29][14788-2-2-1.20][14869-1-1-0.73][14872-3-2--0.04][14877-1-2-1.53][14927-0-3-1.64][15066-0-0-3.11]
[15175-1-2-1.07][15178-2-3-0.58][15375-3-3-1.05][15389-3-3-2.01][15568-2-1-0.74][15675-3-3-0.81][15869-1-2-0.83][16207-3-0-0.59][16236-0-2-0.80][16302-3-0-0.90]
[16331-2-2-1.62][16381-0-0-1.60][16488-1-1-2.04][16495-0-0-0.34][16650-0-0-2.72][16719-1-2-0.58][16801-0-0-3.06][16828-0-0-1.10][17137-3-0-0.47][17245-1-2-0.78]
[17278-3-3-0.48][17282-0-0-0.46][17311-2-2-1.62][17336-2-1-0.76][17608-3-0-1.66][17627-0-0-0.55][17877-3-2-0.96][17924-1-2-0.43][17984-3-0-1.99][18211-0-3-0.98]
[18276-3-0-1.74][18287-1-1-0.35][18394-0-0-2.10][18428-0-0-0.84][18442-0-3-1.07][18478-3-0-1.81][18607-0-2-0.46][18616-0-3-0.24][18663-0-0-0.37][18718-0-0-2.24]
[18766-2-2-1.29][18824-2-2-0.94][18890-3-2-0.76][18930-3-2-0.81][18938-3-0-0.60][19817-1-2-1.43][19839-0-2-0.99][19930-3-3-1.21][19944-0-2-1.37][20036-2-2-2.39]
[20101-3-3-0.84][20474-1-2-1.82][20547-3-3-0.71][20929-2-2-1.77][21245-1-2-0.70][21257-3-2-0.15][21293-1-1-1.14][21316-1-1-2.27][21384-1-2-0.90][21448-1-2-0.87]
[21483-0-0-1.66][21487-2-2-1.13][21714-0-0-0.52][21943-3-2-0.98][21947-0-0-2.91][21948-0-0-2.42][21965-2-2-1.12][21998-1-2-0.38][22025-0-3-1.04][22228-3-3-1.16]
[22446-1-2-0.67][22494-3-0-1.79][22757-0-3-1.68][22811-3-3-0.99][22976-3-1-0.89][22985-3-0-1.61][23014-0-0-1.31][23112-1-1-1.40][23144-3-0-2.10][23168-2-3-0.57]
[23219-0-0-1.18][23363-3-3-1.27][23470-0-2-0.88][23486-2-2-1.09][23497-0-0-1.84][23516-0-0-3.79][23690-1-1-0.78][23921-2-2-0.92][23936-1-3-0.52][24040-3-2-0.67]
[24111-1-4-1.19][24182-0-3-1.74][24238-3-3-1.48][24290-2-0-0.71][24345-0-0-1.25][24364-1-2-0.53][24427-3-3-1.25][24477-2-2-1.91][24495-2-2-0.70][24893-2-2-1.88]
[25012-1-2-0.54][25121-2-2-1.31][25165-3-3-1.12][25183-0-3-0.05][25297-3-3-1.19][25398-0-0-0.82][25574-2-2-0.73][25644-1-1-1.50][25718-1-1--0.10][25774-2-2-1.04]
[26032-3-3-1.19][26051-3-3-1.47][26120-0-2-0.56][26321-1-2-0.28][26732-1-2-0.20][26784-3-3-2.08][26827-3-3-1.02][26833-0-3-1.92][26838-2-2-0.70][26860-1-2-2.26]
[26948-0-0-1.34][27049-3-0-1.06][27098-1-2-0.95][27526-0-0-2.76][27639-3-3-1.07][27698-3-0-0.92][27772-0-3-1.77][27890-1-1-0.68][28040-0-0-1.24][28503-2-2-2.53]
[28577-1-1-0.94][28959-0-0-4.77][29198-3-1-0.25][29777-0-0-3.45][29877-2-2-1.11][30035-1-2-1.36][30098-0-3-0.72][30326-1-1-0.88][30572-2-2-1.32][30716-0-4-0.38]
[30806-2-1-0.26][30906-1-4-0.98][31007-0-0-0.50][31181-3-3-0.81][31238-0-3-0.69][31347-0-3-1.13][31422-2-2-0.35][31429-3-0--0.02][31431-0-3-1.65][31432-1-1-0.85]
[31477-0-0-2.28][31524-1-2-0.78][31597-1-2-1.38][31619-1-0-1.05][31701-0-0-1.74][31755-0-0-0.91][31854-3-3-0.14][32074-1-3-0.94][32078-3-1--0.02][32111-1-2-0.76]
[32127-1-2-1.64][32140-3-3-1.50][32263-2-2-0.80][32365-0-0-0.81][32411-2-3-2.01][32429-3-0-2.69][32473-3-0-1.17][32574-3-0-2.10][32584-0-2-0.39][32622-0-2-0.10]
[32858-3-0-1.00][32969-3-0-2.28][33016-2-2-2.03][33031-1-3-0.91][33035-2-2-2.17][33133-2-2-0.87][33173-2-2-1.13][33175-3-2-1.93][33306-3-2-0.97][33309-2-1-0.30]
[33474-0-3-0.20][33478-2-0-0.48][33618-1-1-0.89][33712-0-0-0.80][33782-2-2-1.70][33914-3-3-0.85][34076-3-3-0.39][34112-2-2-0.73][34138-2-2-1.11][34239-1-2-1.13]
[34364-2-1-1.01][34617-1-1-0.31][34751-3-3-2.21][34783-2-2-0.95][35015-3-3-0.33][35018-1-2-1.36][35288-2-2-0.64][0-4-2-0.90][1-4-0-1.28][2-4-0-0.96]
[3-4-2-0.64][4-4-2-0.48][5-4-2-0.20][6-4-2-0.62][7-4-0-1.47][8-4-2-1.21][9-4-2-0.61][10-4-0-0.66][11-4-2-1.66][12-4-3-1.03]
[14-4-3--0.10][15-4-0-1.48][16-4-2-0.30][17-4-2-0.77][18-4-2-0.63][19-4-3-1.16][20-4-2-0.73][21-4-1-0.97][22-4-4-1.05][23-4-2-0.33]
[24-4-4-0.96][25-4-3-0.53][26-4-3-0.66][27-4-2-0.92][28-4-2-0.26][29-4-2-1.34][30-4-0-1.17][31-4-2-1.10][32-4-2-1.17][33-4-2-0.42]
[34-4-2-0.57][35-4-3-1.11][37-4-0-0.94][39-4-3-1.32][40-4-0-0.62][41-4-2-0.92][42-4-2-0.62][43-4-2-0.92][45-4-2-0.70][46-4-2-1.13]
[47-4-2-1.05][48-4-2-0.80][51-4-2-0.86][52-4-2-1.27][53-4-2-0.07][54-4-3-0.36][55-4-2-1.00][56-4-1-1.18][57-4-0-1.22][58-4-2-1.22]
[59-4-0-0.70][60-4-4--0.03][61-4-2-0.88][62-4-2-0.76][63-4-2-1.90][64-4-2-0.27][65-4-2-0.61][66-4-4-0.89][67-4-2-0.33][68-4-1-0.72]
[69-4-0-2.28][70-4-2-0.68][72-4-2-1.07][73-4-2-0.90][74-4-2-1.66][75-4-0-0.65][77-4-4-0.98][78-4-2-0.30][79-4-2-0.75][80-4-2-0.75]
[81-4-2-1.95][82-4-2-0.74][83-4-2-0.69][84-4-0-1.65][85-4-0-0.12][86-4-2-0.96][87-4-2-0.48][88-4-1-0.79][89-4-0-0.26][90-4-2-0.08]
[91-4-2-1.14][92-4-2-0.52][93-4-4-0.71][94-4-4-0.47][95-4-2-0.46][96-4-1-0.59][97-4-2-1.65][98-4-2-1.79][99-4-2-0.72][100-4-2-1.00]
[101-4-0-0.23][102-4-2-1.04][103-4-2-0.41][104-4-2-1.39][105-4-2-0.78][106-4-1-0.76][107-4-0-0.62][108-4-2-1.10][109-4-1--0.08][110-4-2-0.84]
[111-4-0-1.43][112-4-2-0.57][113-4-3-0.42][114-4-3-0.35][115-4-2-0.29][116-4-2-0.89][117-4-1-1.11][119-4-2-2.33][121-4-2-0.80][122-4-3-0.34]
[124-4-2-0.47][125-4-2-1.03][126-4-2-0.27][127-4-2-1.27][128-4-2-0.17][129-4-1-0.78][130-4-2-0.66][131-4-2-0.81][132-4-2--0.00][133-4-0-1.26]
[135-4-2-1.39][136-4-1-0.34][137-4-2-1.31][138-4-1-0.78][139-4-3-0.94][140-4-2-0.46][141-4-2-0.35][142-4-2-0.79][143-4-2-0.62][144-4-2-1.34]
[145-4-2-0.89][148-4-0-1.76][149-4-2-1.20][150-4-2-0.79][151-4-2-0.79][152-4-3-0.28][153-4-2-1.12][154-4-2-1.00][155-4-1--0.04][156-4-0-0.62]
[157-4-0-0.20][158-4-2-0.46][160-4-2-0.57][161-4-1-0.74][162-4-2-0.39][164-4-2-1.11][165-4-2-0.57][167-4-0-1.02][168-4-4-0.79][170-4-3-0.97]
[171-4-1-0.49][172-4-2-0.20][173-4-0-0.86][174-4-0-0.47][175-4-2-0.55][177-4-3-0.90][178-4-2-0.89][179-4-1-1.06][180-4-4-0.94][181-4-2-0.97]
[182-4-2-0.60][183-4-2-0.57][184-4-2-1.20][186-4-2-0.63][187-4-2-0.63][188-4-2-0.90][189-4-2-0.83][190-4-2-0.25][191-4-2-0.52][192-4-2-0.71]
[193-4-2-2.58][194-4-0-0.76][195-4-2-1.24][196-4-2-0.58][197-4-1-0.70][198-4-4-1.07][199-4-2-0.32]
---------------------------
I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.043 | Acc: 47.875% | Wgt Acc: 57.332%
	I - Batch: 100 | Loss: 1.073 | Acc: 48.062% | Wgt Acc: 57.612%
	I - Batch: 150 | Loss: 1.069 | Acc: 49.167% | Wgt Acc: 58.684%
	I - Batch: 200 | Loss: 1.070 | Acc: 48.781% | Wgt Acc: 58.009%
	I - Batch: 250 | Loss: 1.068 | Acc: 48.950% | Wgt Acc: 57.924%
I - num batch: 285
I - Train -- Loss: 1.063 | Acc: 49.121% | Wgt Acc: 57.996% | LR: 1.000000e-03 | Dur: 182.77s
I - Confusion Matrix: [row->prediction - col->label]
[[512.  16.  43. 188. 142.]
 [ 31. 423. 163.  66. 181.]
 [ 72. 259. 794. 138. 486.]
 [184.  78. 118. 448. 133.]
 [  7.   4.   6.   0.  58.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.481 | Acc: 40.237% | Wgt Acc: 52.036% | Dur: 15.85s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  9.  9. 39. 45.]
 [ 0. 30.  7.  0. 14.]
 [ 6. 31. 48.  8. 90.]
 [11.  8. 10. 39. 15.]
 [ 0.  0.  1.  0. 16.]]

I - Local maximum validation set accuracy:  40.24

I - Validation set results: 
[14-1-2-0.88][50-3-3-0.62][124-2-2-1.54][127-0-0-3.36][443-2-2-1.46][567-0-0-2.46][573-1-1-0.21][615-0-0-2.31][695-1-2-0.47][722-3-3-2.08]
[826-0-0-2.61][878-0-0-3.00][1103-0-0-0.32][1212-3-3-0.84][1368-0-0-2.59][2181-2-0-0.71][2476-2-0-0.16][2721-2-2-2.02][2818-1-0--0.03][2886-2-2-1.31]
[3231-2-2-2.07][3333-2-1-0.55][3482-2-2-1.03][3536-3-3-1.29][3625-1-1-1.29][3909-0-0-1.24][4035-0-3-1.88][4140-0-0-1.84][4214-1-3-0.27][4346-1-0-0.96]
[4581-2-2-2.09][4708-3-3-0.76][4838-3-3-0.93][4845-1-2-0.74][4868-0-0-3.05][4939-0-2-0.73][4984-2-2-1.27][5078-1-1-0.34][5396-0-0-4.74][5479-1-1-0.57]
[5717-0-0-1.94][5843-1-2-0.61][5949-3-0-1.09][5987-2-4-0.80][6014-3-3-0.97][6033-3-0-2.31][6313-0-3-1.61][6421-3-3-1.66][6500-1-1-0.13][6583-3-3-0.81]
[6683-3-3-0.67][6825-2-0-1.30][6998-3-3-0.49][7049-3-3-1.14][7517-1-1-1.45][7521-1-1-0.48][7528-1-3-1.29][7949-1-2-0.90][8135-1-0-1.77][8185-3-0-2.14]
[8269-3-2-1.93][8273-3-3-1.28][8543-3-0-4.55][8666-1-3-0.53][8672-0-0-4.16][8903-1-2-0.29][9001-2-1-0.67][9036-2-2-1.11][9281-3-3-0.78][9300-2-2-1.02]
[9571-0-0-1.35][9617-1-0-0.49][9644-2-2-1.22][9705-2-2-0.65][9801-0-3-1.22][9803-3-0-1.44][9865-3-0-2.34][9896-2-2-2.30][10314-1-2-1.18][10337-3-0-2.48]
[10403-0-2-0.86][10653-2-2-0.16][10704-2-3-0.31][10719-1-1-1.53][10727-1-2-1.31][10836-0-0-3.50][10969-2-3-1.09][11042-0-0-1.88][11088-1-1-1.67][11322-0-0-2.74]
[11398-2-2-1.82][11499-0-0-0.86][11502-3-3-1.13][11512-3-3-0.76][11608-1-1-1.60][11610-0-0-1.49][11692-0-3-0.90][11905-0-0-3.34][11993-1-2-1.37][12002-2-3-0.57]
[12052-0-0-2.34][12201-0-0-2.55][12235-2-1-1.06][12320-1-0-1.09][12377-2-2-0.73][12398-2-3-0.58][12503-1-2-2.30][12617-0-3-0.63][12685-3-3-0.72][12738-2-2-0.34]
[12742-2-2-2.25][12823-0-0-2.13][13110-1-3-0.60][13240-3-3-0.98][13253-1-1-1.10][13273-0-0-4.39][13634-1-2-1.04][13763-2-3-1.08][13905-3-3-0.10][14060-2-1-0.07]
[14065-3-0-0.98][14147-3-0-1.27][14595-2-2-1.43][14687-2-2-1.09][14788-2-2-0.47][14869-1-2-1.11][14872-3-0-0.74][14877-1-2-0.33][14927-0-3-1.41][15066-0-0-4.30]
[15175-1-1-0.67][15178-2-0-1.39][15375-3-0-2.23][15389-3-0-2.01][15568-2-3-0.25][15675-3-3-1.37][15869-1-2-0.03][16207-3-0-0.32][16236-0-3-0.60][16302-3-0-0.79]
[16331-2-2-1.95][16381-0-0-2.57][16488-1-1-1.97][16495-0-0-1.53][16650-0-0-4.09][16719-1-2-0.09][16801-0-0-3.95][16828-0-0-2.21][17137-3-0-1.09][17245-1-2-0.23]
[17278-3-3-0.77][17282-0-0-0.92][17311-2-2-1.91][17336-2-2-0.77][17608-3-0-2.74][17627-0-0-0.89][17877-3-2-1.65][17924-1-0-0.37][17984-3-0-1.76][18211-0-3-1.62]
[18276-3-0-2.97][18287-1-1-0.18][18394-0-0-2.83][18428-0-0-3.65][18442-0-0-2.14][18478-3-0-2.16][18607-0-0-1.29][18616-0-0-1.20][18663-0-0-1.20][18718-0-0-2.86]
[18766-2-2-1.73][18824-2-2-0.96][18890-3-3-0.77][18930-3-0-0.70][18938-3-0-1.09][19817-1-2-1.51][19839-0-2-1.05][19930-3-3-1.34][19944-0-2-1.12][20036-2-2-1.88]
[20101-3-0-1.12][20474-1-1-1.52][20547-3-0-0.22][20929-2-2-1.51][21245-1-2-1.10][21257-3-3-0.55][21293-1-1-0.68][21316-1-1-2.09][21384-1-1-1.02][21448-1-1-1.53]
[21483-0-0-2.55][21487-2-2-1.49][21714-0-0-0.30][21943-3-2-0.15][21947-0-0-2.58][21948-0-0-3.60][21965-2-2-1.67][21998-1-1--0.01][22025-0-3-1.36][22228-3-0-2.31]
[22446-1-1-0.61][22494-3-0-2.23][22757-0-0-3.13][22811-3-3-0.44][22976-3-2-0.29][22985-3-0-2.34][23014-0-0-2.66][23112-1-1-1.22][23144-3-0-3.23][23168-2-0-1.17]
[23219-0-0-2.28][23363-3-3-1.36][23470-0-2-0.20][23486-2-2-0.84][23497-0-0-2.95][23516-0-0-3.89][23690-1-2-0.09][23921-2-1-0.33][23936-1-3-0.70][24040-3-2-0.01]
[24111-1-2-0.81][24182-0-0-3.36][24238-3-3-1.54][24290-2-0-2.74][24345-0-0-1.56][24364-1-2-0.19][24427-3-0-2.30][24477-2-2-1.75][24495-2-2-0.11][24893-2-2-0.79]
[25012-1-0-0.46][25121-2-2-0.94][25165-3-3-1.30][25183-0-0-1.51][25297-3-3-1.58][25398-0-0-2.21][25574-2-3-0.99][25644-1-2-0.77][25718-1-0-0.66][25774-2-3-0.72]
[26032-3-3-0.77][26051-3-0-2.22][26120-0-0-0.53][26321-1-1-0.70][26732-1-2-0.37][26784-3-0-2.14][26827-3-3-1.33][26833-0-3-2.07][26838-2-2-0.44][26860-1-2-0.94]
[26948-0-0-1.48][27049-3-0-1.85][27098-1-2-0.49][27526-0-0-2.60][27639-3-0-1.56][27698-3-3-1.20][27772-0-0-2.99][27890-1-1-0.61][28040-0-0-2.06][28503-2-2-2.70]
[28577-1-2-1.11][28959-0-0-4.68][29198-3-3-1.34][29777-0-0-4.08][29877-2-2-1.13][30035-1-2-1.21][30098-0-0-1.76][30326-1-1-1.50][30572-2-2-1.28][30716-0-0-0.78]
[30806-2-3-0.47][30906-1-1-0.52][31007-0-0-1.69][31181-3-2-0.16][31238-0-3-1.22][31347-0-0-1.59][31422-2-2-0.36][31429-3-0-0.53][31431-0-0-0.80][31432-1-1-0.58]
[31477-0-0-3.81][31524-1-3-0.44][31597-1-2-1.52][31619-1-0-0.40][31701-0-0-2.23][31755-0-0-1.26][31854-3-0-1.91][32074-1-3-0.20][32078-3-3-0.48][32111-1-1-1.00]
[32127-1-2-2.05][32140-3-3-1.16][32263-2-0-0.76][32365-0-0-1.24][32411-2-0-3.42][32429-3-0-2.83][32473-3-0-0.90][32574-3-0-3.11][32584-0-0-1.33][32622-0-2-0.14]
[32858-3-0-1.74][32969-3-0-2.72][33016-2-2-2.49][33031-1-3-1.31][33035-2-2-1.71][33133-2-2-0.92][33173-2-2-0.71][33175-3-2-1.54][33306-3-2-0.78][33309-2-3-0.02]
[33474-0-0-0.68][33478-2-0-0.91][33618-1-1-1.00][33712-0-0-1.42][33782-2-1-1.61][33914-3-3-1.49][34076-3-3-0.52][34112-2-2-0.79][34138-2-2-1.32][34239-1-1-0.78]
[34364-2-1-1.29][34617-1-2-0.48][34751-3-3-1.98][34783-2-2-0.94][35015-3-3-0.40][35018-1-2-1.37][35288-2-2-0.74][0-4-2-0.84][1-4-0-1.08][2-4-0-0.52]
[3-4-2-0.64][4-4-3-0.37][5-4-3-0.16][6-4-0-2.72][7-4-0-1.88][8-4-2-1.46][9-4-2-1.22][10-4-2-0.58][11-4-2-2.94][12-4-2-0.64]
[14-4-0-0.95][15-4-0-2.56][16-4-3-0.92][17-4-1-0.35][18-4-2-0.57][19-4-0-1.99][20-4-3-0.03][21-4-2-1.02][22-4-4-0.34][23-4-2--0.08]
[24-4-4-1.31][25-4-3-0.97][26-4-0-0.94][27-4-0-1.72][28-4-4-0.64][29-4-2-1.21][30-4-2-0.27][31-4-2-1.03][32-4-1-0.93][33-4-3-0.99]
[34-4-2-0.32][35-4-0-1.61][37-4-2-0.74][39-4-0-1.37][40-4-0-1.04][41-4-0-0.43][42-4-2-0.31][43-4-2-1.77][45-4-2-1.72][46-4-2-0.41]
[47-4-2-0.65][48-4-2-0.83][51-4-2-0.89][52-4-2-1.20][53-4-2-0.35][54-4-0-0.50][55-4-2-0.40][56-4-1-0.29][57-4-3-0.95][58-4-2-2.00]
[59-4-0-1.46][60-4-0-0.31][61-4-4-0.57][62-4-2-0.76][63-4-2-2.35][64-4-0-0.19][65-4-2-0.83][66-4-1-0.90][67-4-2-0.70][68-4-1-0.90]
[69-4-0-2.20][70-4-2-0.85][72-4-2-1.20][73-4-2-0.45][74-4-2-1.60][75-4-0-1.41][77-4-2-1.00][78-4-2-0.42][79-4-2-0.88][80-4-2-0.49]
[81-4-2-1.93][82-4-1-0.39][83-4-2-1.01][84-4-2-0.20][85-4-2-0.45][86-4-2-0.66][87-4-4-0.96][88-4-1-0.32][89-4-2-1.51][90-4-0-0.16]
[91-4-2-0.80][92-4-2-0.18][93-4-0-1.55][94-4-4-0.13][95-4-2-0.12][96-4-1-0.77][97-4-2-0.95][98-4-2-1.66][99-4-4-0.46][100-4-2-0.57]
[101-4-4-0.36][102-4-2-1.00][103-4-3-0.67][104-4-2-0.42][105-4-2-0.88][106-4-1-0.57][107-4-0-0.43][108-4-2-0.83][109-4-3-0.13][110-4-2-0.23]
[111-4-0-3.11][112-4-0-0.92][113-4-3-0.73][114-4-3-0.77][115-4-0-0.21][116-4-2-0.66][117-4-2-0.29][119-4-2-2.33][121-4-1-0.42][122-4-0-1.03]
[124-4-2-0.81][125-4-1-1.10][126-4-4-0.30][127-4-2-0.89][128-4-0-0.33][129-4-0-0.66][130-4-2-1.65][131-4-2-0.79][132-4-2-0.18][133-4-0-3.60]
[135-4-2-1.65][136-4-0-0.26][137-4-0--0.03][138-4-1-0.73][139-4-0-1.35][140-4-2-0.75][141-4-3-1.06][142-4-4-0.96][143-4-0-0.27][144-4-4-1.17]
[145-4-2-0.76][148-4-0-2.17][149-4-2-0.39][150-4-2-0.50][151-4-2-1.13][152-4-3-0.10][153-4-2-1.92][154-4-2-1.46][155-4-0-0.08][156-4-0-0.97]
[157-4-0-2.26][158-4-3-0.25][160-4-2-0.58][161-4-2-0.47][162-4-0-0.28][164-4-2-0.96][165-4-4-0.64][167-4-0-2.90][168-4-4-0.48][170-4-0-1.39]
[171-4-2-0.56][172-4-2-0.71][173-4-0-1.93][174-4-0-2.33][175-4-4-0.15][177-4-0-2.19][178-4-2-0.93][179-4-3-0.09][180-4-4-0.70][181-4-2-0.82]
[182-4-2-1.21][183-4-2-0.57][184-4-2-1.13][186-4-0-0.89][187-4-2-0.60][188-4-2-0.50][189-4-2-1.21][190-4-2-0.41][191-4-2-1.48][192-4-2-0.68]
[193-4-2-2.18][194-4-1-1.19][195-4-0-1.59][196-4-2-0.44][197-4-1-0.72][198-4-4-1.13][199-4-2-0.38]
---------------------------
I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 1.069 | Acc: 47.500% | Wgt Acc: 57.224%
	I - Batch: 100 | Loss: 1.054 | Acc: 49.188% | Wgt Acc: 58.640%
	I - Batch: 150 | Loss: 1.043 | Acc: 49.625% | Wgt Acc: 58.647%
	I - Batch: 200 | Loss: 1.043 | Acc: 49.938% | Wgt Acc: 59.180%
	I - Batch: 250 | Loss: 1.049 | Acc: 49.250% | Wgt Acc: 58.395%
I - num batch: 285
I - Train -- Loss: 1.044 | Acc: 49.626% | Wgt Acc: 58.686% | LR: 1.000000e-03 | Dur: 173.04s
I - Confusion Matrix: [row->prediction - col->label]
[[519.  30.  49. 180. 179.]
 [ 27. 401. 146.  59. 136.]
 [ 80. 273. 808. 124. 492.]
 [177.  74. 116. 475. 138.]
 [  3.   2.   5.   2.  55.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.398 | Acc: 41.223% | Wgt Acc: 53.327% | Dur: 14.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   2.   2.   7.  11.]
 [  0.  29.   3.   2.  12.]
 [ 10.  43.  63.  21. 114.]
 [ 34.   3.   7.  56.  25.]
 [  1.   1.   0.   0.  18.]]

I - Local maximum validation set accuracy:  41.22

I - Validation set results: 
[14-1-2-1.88][50-3-2-0.46][124-2-2-1.77][127-0-0-2.41][443-2-2-1.69][567-0-0-0.93][573-1-1-1.47][615-0-0-1.51][695-1-2-0.40][722-3-3-2.80]
[826-0-3-1.78][878-0-3-1.65][1103-0-0-0.29][1212-3-2-0.54][1368-0-0-2.18][2181-2-2-0.76][2476-2-2-0.84][2721-2-2-1.39][2818-1-1-0.43][2886-2-2-1.63]
[3231-2-2-2.21][3333-2-2-1.81][3482-2-2-2.10][3536-3-3-1.56][3625-1-1-2.31][3909-0-0-0.69][4035-0-3-2.43][4140-0-0-0.05][4214-1-1-0.84][4346-1-0-0.35]
[4581-2-2-2.61][4708-3-2-0.63][4838-3-3-0.57][4845-1-2-0.72][4868-0-0-2.41][4939-0-2-0.37][4984-2-2-2.27][5078-1-2-0.62][5396-0-0-3.22][5479-1-1-2.03]
[5717-0-0-0.78][5843-1-1-2.36][5949-3-3-0.82][5987-2-2-0.63][6014-3-1-0.38][6033-3-0-0.84][6313-0-3-2.31][6421-3-3-1.59][6500-1-2-0.78][6583-3-2-1.07]
[6683-3-3-0.88][6825-2-3-0.91][6998-3-3-0.30][7049-3-3-1.01][7517-1-2-2.07][7521-1-2-0.43][7528-1-2-0.76][7949-1-2-1.30][8135-1-3-0.66][8185-3-3-1.62]
[8269-3-2-1.41][8273-3-3-1.66][8543-3-0-2.97][8666-1-1-1.19][8672-0-0-2.33][8903-1-2-0.71][9001-2-2-1.69][9036-2-2-1.79][9281-3-2-0.41][9300-2-2-1.91]
[9571-0-3-1.26][9617-1-1-1.38][9644-2-2-1.26][9705-2-2-0.02][9801-0-3-0.95][9803-3-3-1.27][9865-3-3-1.92][9896-2-2-1.51][10314-1-2-1.57][10337-3-3-2.66]
[10403-0-2-0.97][10653-2-1-0.42][10704-2-2-1.43][10719-1-1-1.82][10727-1-2-1.29][10836-0-0-2.85][10969-2-3-1.13][11042-0-0-1.55][11088-1-1-1.81][11322-0-3-1.41]
[11398-2-2-0.80][11499-0-3-0.29][11502-3-3-0.50][11512-3-2-0.35][11608-1-2-2.25][11610-0-0-1.39][11692-0-3-0.93][11905-0-0-1.43][11993-1-2-1.76][12002-2-3-0.61]
[12052-0-0-1.81][12201-0-3-1.85][12235-2-2-1.89][12320-1-2-0.70][12377-2-2-1.39][12398-2-2-0.57][12503-1-2-1.33][12617-0-2-0.96][12685-3-3-0.85][12738-2-2-0.27]
[12742-2-2-2.51][12823-0-3-1.86][13110-1-1-0.34][13240-3-3-1.64][13253-1-1-1.49][13273-0-0-3.37][13634-1-2-1.24][13763-2-2-1.41][13905-3-3-0.09][14060-2-2-1.43]
[14065-3-3-1.77][14147-3-0-0.04][14595-2-2-1.37][14687-2-2-2.27][14788-2-2-1.59][14869-1-1-1.45][14872-3-0-0.03][14877-1-1-1.13][14927-0-3-1.83][15066-0-0-2.75]
[15175-1-1-1.04][15178-2-3-0.76][15375-3-3-1.29][15389-3-3-1.79][15568-2-1-1.80][15675-3-3-1.15][15869-1-2-0.89][16207-3-3-0.47][16236-0-3-0.39][16302-3-2-0.50]
[16331-2-2-1.72][16381-0-0-0.80][16488-1-1-2.69][16495-0-0-1.38][16650-0-0-2.26][16719-1-2-0.84][16801-0-0-3.22][16828-0-0-1.64][17137-3-3-0.95][17245-1-2-1.18]
[17278-3-3-0.57][17282-0-0-1.19][17311-2-2-1.85][17336-2-1-1.56][17608-3-3-2.29][17627-0-2-0.42][17877-3-2-1.83][17924-1-3-0.46][17984-3-3-2.73][18211-0-3-0.98]
[18276-3-3-0.85][18287-1-1-1.38][18394-0-0-1.94][18428-0-3-0.36][18442-0-3-1.44][18478-3-3-1.10][18607-0-0-0.37][18616-0-3-0.60][18663-0-3-0.22][18718-0-0-2.10]
[18766-2-2-2.32][18824-2-2-1.61][18890-3-2-1.24][18930-3-2-0.65][18938-3-3-0.94][19817-1-2-2.29][19839-0-4-0.67][19930-3-3-1.62][19944-0-2-0.68][20036-2-2-2.79]
[20101-3-1-0.40][20474-1-1-1.57][20547-3-3-0.12][20929-2-2-2.63][21245-1-2-1.80][21257-3-3-0.66][21293-1-2-1.78][21316-1-1-3.19][21384-1-2-0.79][21448-1-2-1.93]
[21483-0-3-1.12][21487-2-2-1.18][21714-0-2-0.55][21943-3-2-1.13][21947-0-0-2.13][21948-0-0-3.04][21965-2-2-2.39][21998-1-1-0.45][22025-0-2-0.98][22228-3-3-1.55]
[22446-1-1-1.38][22494-3-0-0.64][22757-0-3-2.33][22811-3-3-1.83][22976-3-2-1.33][22985-3-3-1.87][23014-0-3-1.95][23112-1-1-1.90][23144-3-3-2.17][23168-2-3-0.60]
[23219-0-2-0.64][23363-3-3-1.12][23470-0-0-0.25][23486-2-2-1.05][23497-0-3-2.79][23516-0-3-2.03][23690-1-2-1.25][23921-2-2-2.34][23936-1-2-1.04][24040-3-2-0.31]
[24111-1-4-1.38][24182-0-3-2.00][24238-3-3-1.52][24290-2-0-1.50][24345-0-0-1.42][24364-1-2-0.45][24427-3-3-0.69][24477-2-2-1.70][24495-2-2-1.05][24893-2-2-1.33]
[25012-1-2-0.70][25121-2-2-1.03][25165-3-3-0.61][25183-0-3-0.73][25297-3-3-1.25][25398-0-0-1.25][25574-2-2-1.10][25644-1-2-2.51][25718-1-2-0.63][25774-2-2-1.50]
[26032-3-3-1.36][26051-3-3-2.24][26120-0-0-0.41][26321-1-2-0.48][26732-1-1-0.82][26784-3-3-2.03][26827-3-3-0.92][26833-0-3-2.43][26838-2-2-0.72][26860-1-2-0.54]
[26948-0-0-0.78][27049-3-0-1.66][27098-1-2-0.18][27526-0-0-1.59][27639-3-3-0.46][27698-3-3-1.64][27772-0-3-1.22][27890-1-1-1.02][28040-0-3-0.28][28503-2-2-2.94]
[28577-1-2-2.22][28959-0-0-3.77][29198-3-3-0.74][29777-0-0-3.20][29877-2-2-1.48][30035-1-2-1.74][30098-0-0-0.76][30326-1-1-2.40][30572-2-2-1.77][30716-0-2-0.52]
[30806-2-2-1.12][30906-1-1-1.26][31007-0-0-0.82][31181-3-2-0.73][31238-0-3-1.44][31347-0-3-1.48][31422-2-2-1.33][31429-3-2-0.24][31431-0-3-0.30][31432-1-1-1.60]
[31477-0-0-2.03][31524-1-2-1.28][31597-1-2-1.65][31619-1-0-0.33][31701-0-3-0.91][31755-0-0-0.67][31854-3-3-0.87][32074-1-2-0.21][32078-3-3-0.58][32111-1-1-1.27]
[32127-1-2-2.10][32140-3-3-1.71][32263-2-2-0.60][32365-0-0-1.14][32411-2-0-2.18][32429-3-0-2.02][32473-3-2-0.37][32574-3-3-2.20][32584-0-0-0.11][32622-0-2-0.53]
[32858-3-3-0.44][32969-3-3-1.60][33016-2-2-1.37][33031-1-3-1.38][33035-2-2-2.04][33133-2-2-1.28][33173-2-2-1.22][33175-3-2-2.01][33306-3-2-1.33][33309-2-3-0.29]
[33474-0-3-0.04][33478-2-3-0.43][33618-1-1-1.38][33712-0-3-0.79][33782-2-2-1.60][33914-3-3-1.02][34076-3-2-0.54][34112-2-2-2.21][34138-2-2-1.03][34239-1-2-0.90]
[34364-2-2-2.07][34617-1-2-1.79][34751-3-3-2.29][34783-2-2-1.37][35015-3-2-0.74][35018-1-2-2.36][35288-2-2-1.42][0-4-2-1.51][1-4-3-0.36][2-4-3--0.27]
[3-4-2-0.77][4-4-2-0.95][5-4-3-0.35][6-4-2-0.24][7-4-2-0.27][8-4-2-0.77][9-4-2-1.40][10-4-4-1.21][11-4-2-3.26][12-4-2-1.01]
[14-4-3-0.90][15-4-3-1.50][16-4-3-0.21][17-4-1-1.00][18-4-2-0.95][19-4-3-1.35][20-4-2-0.45][21-4-1-1.28][22-4-4-0.73][23-4-2-0.77]
[24-4-4-0.70][25-4-3-0.68][26-4-2-0.91][27-4-3-1.28][28-4-2-0.64][29-4-2-1.73][30-4-0-0.58][31-4-2-1.58][32-4-1-1.58][33-4-2-0.71]
[34-4-2-0.95][35-4-3-1.24][37-4-2-0.69][39-4-3-0.99][40-4-2-0.13][41-4-2-0.97][42-4-2-1.35][43-4-2-2.09][45-4-2-2.04][46-4-2-1.68]
[47-4-2-1.68][48-4-2-0.85][51-4-4-1.56][52-4-2-1.41][53-4-2-0.86][54-4-3-0.54][55-4-2-0.94][56-4-2-0.97][57-4-3-1.33][58-4-2-2.30]
[59-4-0-0.36][60-4-2--0.01][61-4-2-0.95][62-4-2-0.16][63-4-2-1.99][64-4-0--0.02][65-4-2-1.62][66-4-2-0.29][67-4-2-0.69][68-4-2-0.84]
[69-4-0-1.00][70-4-2-1.81][72-4-2-1.51][73-4-2-0.90][74-4-2-1.24][75-4-0-0.53][77-4-2-1.07][78-4-2-1.19][79-4-2-1.29][80-4-4-0.94]
[81-4-2-1.41][82-4-1-0.59][83-4-2-0.39][84-4-2-0.27][85-4-4-1.18][86-4-2-0.65][87-4-4-0.82][88-4-4--0.03][89-4-2-1.75][90-4-2-0.07]
[91-4-2-1.72][92-4-2-1.02][93-4-0-0.98][94-4-4-0.74][95-4-2-0.68][96-4-1-1.07][97-4-2-1.60][98-4-2-1.72][99-4-4-0.80][100-4-2-1.42]
[101-4-4-0.69][102-4-2-2.07][103-4-3-0.96][104-4-4-0.67][105-4-2-1.82][106-4-1-0.78][107-4-1-0.31][108-4-2-1.55][109-4-2-0.60][110-4-2-1.24]
[111-4-3-1.11][112-4-0-0.62][113-4-2-0.55][114-4-3-0.70][115-4-0--0.01][116-4-2-0.29][117-4-2-0.86][119-4-2-1.75][121-4-2-1.83][122-4-3-0.75]
[124-4-2-1.53][125-4-2-1.61][126-4-3-0.68][127-4-2-1.33][128-4-2-0.05][129-4-2-0.35][130-4-2-1.07][131-4-2-1.04][132-4-2-0.99][133-4-0-1.22]
[135-4-2-1.53][136-4-1-0.80][137-4-1-0.30][138-4-2-0.37][139-4-2-1.29][140-4-2-0.91][141-4-3-1.30][142-4-4-1.70][143-4-2-0.87][144-4-4-1.42]
[145-4-1-0.82][148-4-0-1.54][149-4-2-0.94][150-4-2-1.49][151-4-2-1.61][152-4-2-1.41][153-4-2-1.94][154-4-2-1.02][155-4-2-0.07][156-4-3-0.73]
[157-4-0-0.62][158-4-2-1.05][160-4-4-0.87][161-4-1-0.67][162-4-2-1.06][164-4-2-1.41][165-4-2-0.16][167-4-3-0.75][168-4-4-0.60][170-4-3-1.07]
[171-4-2-1.49][172-4-2-0.90][173-4-2-0.42][174-4-3-1.05][175-4-2-0.96][177-4-3-0.50][178-4-2-1.03][179-4-2-0.66][180-4-4-0.92][181-4-2-1.04]
[182-4-2-0.81][183-4-2-0.78][184-4-2-0.87][186-4-3-0.46][187-4-1-0.63][188-4-2-0.93][189-4-2-0.81][190-4-2-0.53][191-4-2-1.46][192-4-2-0.95]
[193-4-2-2.06][194-4-2-0.49][195-4-2--0.02][196-4-2-1.26][197-4-2-2.16][198-4-4-0.94][199-4-2-0.40]
---------------------------
I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 0.994 | Acc: 51.375% | Wgt Acc: 60.312%
	I - Batch: 100 | Loss: 1.008 | Acc: 51.062% | Wgt Acc: 60.147%
	I - Batch: 150 | Loss: 1.017 | Acc: 50.292% | Wgt Acc: 59.226%
	I - Batch: 200 | Loss: 1.010 | Acc: 50.594% | Wgt Acc: 59.433%
	I - Batch: 250 | Loss: 1.013 | Acc: 50.575% | Wgt Acc: 59.390%
I - num batch: 285
I - Train -- Loss: 1.009 | Acc: 51.077% | Wgt Acc: 59.917% | LR: 1.000000e-03 | Dur: 172.58s
I - Confusion Matrix: [row->prediction - col->label]
[[519.  21.  36. 163. 156.]
 [ 25. 418. 144.  60. 155.]
 [ 78. 266. 826. 133. 488.]
 [180.  69. 108. 481. 121.]
 [  4.   6.  10.   3.  80.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.437 | Acc: 42.406% | Wgt Acc: 54.573% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  2.  4. 22. 24.]
 [ 2. 39. 12.  2. 23.]
 [ 6. 23. 47.  8. 84.]
 [24. 14. 12. 54. 29.]
 [ 1.  0.  0.  0. 20.]]

I - Local maximum validation set accuracy:  42.41

I - Validation set results: 
[14-1-2-1.62][50-3-1-0.48][124-2-2-2.48][127-0-0-3.64][443-2-2-2.07][567-0-0-1.56][573-1-1-1.44][615-0-3-2.09][695-1-2-0.16][722-3-3-2.75]
[826-0-0-2.92][878-0-0-2.05][1103-0-0-1.22][1212-3-3-1.31][1368-0-0-2.64][2181-2-3-1.26][2476-2-2-0.28][2721-2-2-1.83][2818-1-3-0.60][2886-2-1-1.20]
[3231-2-1-2.58][3333-2-2-1.08][3482-2-2-1.43][3536-3-3-1.40][3625-1-1-2.54][3909-0-0-1.30][4035-0-3-2.28][4140-0-0-0.70][4214-1-3-0.47][4346-1-3-1.01]
[4581-2-2-2.78][4708-3-3-0.98][4838-3-3-0.76][4845-1-2-0.77][4868-0-0-2.66][4939-0-2-1.42][4984-2-2-1.17][5078-1-2-0.08][5396-0-0-4.83][5479-1-1-1.82]
[5717-0-0-1.88][5843-1-1-1.13][5949-3-0-2.01][5987-2-1-1.22][6014-3-3-1.30][6033-3-0-1.02][6313-0-3-2.05][6421-3-3-1.84][6500-1-3-0.22][6583-3-2-0.69]
[6683-3-3-1.19][6825-2-0-1.10][6998-3-3-1.40][7049-3-3-1.54][7517-1-2-1.39][7521-1-1-0.55][7528-1-3-1.32][7949-1-2-1.20][8135-1-0-0.67][8185-3-0-2.74]
[8269-3-2-1.95][8273-3-3-1.83][8543-3-0-4.61][8666-1-1-2.15][8672-0-0-4.34][8903-1-2-1.21][9001-2-1-0.40][9036-2-2-1.91][9281-3-3-0.52][9300-2-2-2.30]
[9571-0-3-0.81][9617-1-1-0.54][9644-2-2-1.68][9705-2-2-0.07][9801-0-3-1.53][9803-3-3-1.87][9865-3-0-2.51][9896-2-2-1.63][10314-1-2-0.88][10337-3-3-2.99]
[10403-0-2-1.36][10653-2-1-0.59][10704-2-3-0.13][10719-1-1-2.03][10727-1-2-1.06][10836-0-0-4.44][10969-2-3-1.42][11042-0-0-1.16][11088-1-1-1.75][11322-0-0-2.34]
[11398-2-2-2.17][11499-0-0-1.23][11502-3-3-1.48][11512-3-3-0.95][11608-1-1-3.11][11610-0-0-2.81][11692-0-3-1.52][11905-0-0-3.27][11993-1-2-1.82][12002-2-3-1.18]
[12052-0-0-3.19][12201-0-0-2.38][12235-2-1-1.91][12320-1-2-0.01][12377-2-2-1.03][12398-2-3-1.01][12503-1-1-1.55][12617-0-3-0.84][12685-3-3-0.75][12738-2-2-1.02]
[12742-2-2-2.86][12823-0-3-2.47][13110-1-1-0.77][13240-3-3-1.63][13253-1-1-2.22][13273-0-0-4.15][13634-1-2-1.17][13763-2-3-0.62][13905-3-3-0.16][14060-2-1-0.48]
[14065-3-0-1.96][14147-3-3-1.08][14595-2-2-1.36][14687-2-2-2.10][14788-2-2-1.48][14869-1-1-2.98][14872-3-0-0.45][14877-1-1-1.38][14927-0-3-2.45][15066-0-0-3.39]
[15175-1-1-1.43][15178-2-3-1.16][15375-3-3-0.90][15389-3-0-2.79][15568-2-1-0.37][15675-3-3-1.88][15869-1-1-0.29][16207-3-3-0.38][16236-0-3-0.71][16302-3-3-1.02]
[16331-2-2-2.47][16381-0-0-2.17][16488-1-1-2.94][16495-0-0-2.01][16650-0-0-3.32][16719-1-2-0.94][16801-0-0-3.93][16828-0-0-2.01][17137-3-0-1.74][17245-1-3-0.19]
[17278-3-0-0.19][17282-0-0-1.17][17311-2-2-2.06][17336-2-1-0.88][17608-3-3-2.25][17627-0-1-0.16][17877-3-2-2.11][17924-1-3-0.37][17984-3-3-2.25][18211-0-3-1.85]
[18276-3-0-1.73][18287-1-1-1.11][18394-0-0-3.39][18428-0-0-1.20][18442-0-3-1.80][18478-3-0-1.49][18607-0-0-0.59][18616-0-3-0.24][18663-0-3-1.54][18718-0-0-2.52]
[18766-2-2-2.11][18824-2-2-1.38][18890-3-3-1.04][18930-3-2-0.63][18938-3-3-1.08][19817-1-2-1.91][19839-0-2-0.34][19930-3-3-1.31][19944-0-2-0.79][20036-2-2-2.52]
[20101-3-3-1.55][20474-1-1-1.99][20547-3-3-0.91][20929-2-2-2.25][21245-1-1-0.93][21257-3-3-1.00][21293-1-2-1.20][21316-1-1-3.15][21384-1-2-1.87][21448-1-1-1.68]
[21483-0-0-2.26][21487-2-2-1.13][21714-0-3-0.48][21943-3-2-0.81][21947-0-0-0.50][21948-0-0-3.87][21965-2-2-2.70][21998-1-1-0.61][22025-0-3-0.95][22228-3-3-1.83]
[22446-1-1-2.00][22494-3-0-2.41][22757-0-3-2.59][22811-3-3-2.61][22976-3-1-0.86][22985-3-3-2.42][23014-0-0-2.43][23112-1-1-2.23][23144-3-0-2.81][23168-2-0-1.30]
[23219-0-3-0.36][23363-3-3-1.95][23470-0-2--0.10][23486-2-3-0.77][23497-0-0-2.97][23516-0-0-3.76][23690-1-2-0.51][23921-2-2-1.15][23936-1-3-0.59][24040-3-0--0.09]
[24111-1-1-0.92][24182-0-0-3.34][24238-3-3-2.37][24290-2-0-2.88][24345-0-0-2.51][24364-1-3-1.16][24427-3-3-1.44][24477-2-2-1.98][24495-2-2-0.86][24893-2-2-2.09]
[25012-1-3-0.20][25121-2-2-1.24][25165-3-3-1.37][25183-0-0-0.97][25297-3-3-1.93][25398-0-0-2.42][25574-2-2-0.31][25644-1-1-3.26][25718-1-3-0.52][25774-2-3-0.75]
[26032-3-3-1.43][26051-3-3-2.48][26120-0-2-0.96][26321-1-1-1.05][26732-1-1-0.67][26784-3-3-2.40][26827-3-3-1.46][26833-0-3-2.67][26838-2-2-0.34][26860-1-2-0.49]
[26948-0-0-0.85][27049-3-0-1.88][27098-1-3-1.09][27526-0-0-3.02][27639-3-3-0.99][27698-3-3-1.79][27772-0-3-1.71][27890-1-1-1.49][28040-0-0-0.30][28503-2-2-3.42]
[28577-1-1-2.03][28959-0-0-5.07][29198-3-3-1.27][29777-0-0-4.49][29877-2-2-1.16][30035-1-1-1.69][30098-0-0-1.58][30326-1-1-2.61][30572-2-2-1.93][30716-0-4-0.38]
[30806-2-3-1.09][30906-1-2-1.53][31007-0-0-0.67][31181-3-0-1.92][31238-0-3-1.60][31347-0-3-2.16][31422-2-2--0.02][31429-3-3--0.02][31431-0-3-0.72][31432-1-1-2.49]
[31477-0-0-2.72][31524-1-3-0.58][31597-1-2-1.31][31619-1-0-0.44][31701-0-0-1.96][31755-0-0-2.19][31854-3-3-1.35][32074-1-2-0.26][32078-3-3-1.50][32111-1-1-3.41]
[32127-1-2-3.01][32140-3-3-1.96][32263-2-2-0.94][32365-0-0-0.86][32411-2-0-4.07][32429-3-0-2.11][32473-3-0-0.89][32574-3-0-3.09][32584-0-0-0.83][32622-0-1--0.09]
[32858-3-0-1.29][32969-3-0-2.36][33016-2-1-1.17][33031-1-3-1.60][33035-2-2-2.47][33133-2-2-0.74][33173-2-2-1.08][33175-3-2-1.60][33306-3-2-1.16][33309-2-3-0.58]
[33474-0-3-0.99][33478-2-3-0.25][33618-1-1-2.50][33712-0-0-1.14][33782-2-2-1.47][33914-3-3-1.11][34076-3-3-1.30][34112-2-2-0.84][34138-2-2-0.75][34239-1-1-0.68]
[34364-2-1-1.92][34617-1-2-0.54][34751-3-3-2.81][34783-2-1-1.20][35015-3-2-0.66][35018-1-1-1.80][35288-2-2-1.32][0-4-2-1.57][1-4-0-1.64][2-4-0-1.14]
[3-4-2-0.72][4-4-2-0.66][5-4-3-0.59][6-4-0-2.05][7-4-4-0.89][8-4-2-0.99][9-4-2-0.27][10-4-4-0.26][11-4-2-3.04][12-4-3-0.89]
[14-4-3-0.80][15-4-0-2.58][16-4-3-0.49][17-4-2-0.57][18-4-4-0.97][19-4-0-1.97][20-4-2-0.11][21-4-2-0.59][22-4-1-0.60][23-4-1--0.06]
[24-4-2-0.66][25-4-3-1.41][26-4-3-0.58][27-4-0-1.02][28-4-4-0.94][29-4-2-1.53][30-4-0-0.24][31-4-2-1.37][32-4-4-0.76][33-4-3-0.81]
[34-4-2-0.23][35-4-0-1.79][37-4-0-0.94][39-4-0-3.10][40-4-3-0.28][41-4-2-0.44][42-4-2-0.61][43-4-2-2.06][45-4-2-2.38][46-4-2-0.92]
[47-4-2-1.20][48-4-2-1.45][51-4-2-1.03][52-4-2-0.83][53-4-2-0.05][54-4-3-0.63][55-4-3-0.30][56-4-1-0.99][57-4-3-1.40][58-4-2-1.88]
[59-4-2-0.34][60-4-3-0.48][61-4-4-0.75][62-4-3-0.50][63-4-2-2.23][64-4-2-0.52][65-4-1-0.60][66-4-4-1.65][67-4-3-0.45][68-4-1-1.65]
[69-4-0-1.84][70-4-2-2.25][72-4-2-0.44][73-4-1-0.51][74-4-2-2.20][75-4-2-0.91][77-4-4-1.46][78-4-2-0.76][79-4-2-0.91][80-4-4-1.22]
[81-4-2-1.41][82-4-1-1.02][83-4-1-0.52][84-4-0-0.26][85-4-2-1.16][86-4-2-0.23][87-4-4-0.92][88-4-4-0.49][89-4-3-0.40][90-4-0-0.07]
[91-4-2-1.37][92-4-3-0.29][93-4-0-0.35][94-4-4-0.74][95-4-2-0.92][96-4-1-1.69][97-4-2-1.24][98-4-2-2.02][99-4-3-0.01][100-4-2-1.64]
[101-4-2-0.98][102-4-2-0.76][103-4-3-0.98][104-4-2-0.41][105-4-4-1.06][106-4-2-0.77][107-4-4-0.61][108-4-2-0.96][109-4-1-0.60][110-4-2-0.46]
[111-4-0-2.58][112-4-2-0.53][113-4-3-1.02][114-4-3-0.59][115-4-3-0.06][116-4-2-1.20][117-4-1-1.21][119-4-2-2.74][121-4-1-1.38][122-4-3-0.69]
[124-4-2-0.72][125-4-1-2.00][126-4-3-0.23][127-4-2-1.97][128-4-3-0.16][129-4-2-0.02][130-4-2-0.58][131-4-2-1.12][132-4-1-0.60][133-4-0-2.34]
[135-4-2-1.70][136-4-2-0.16][137-4-2-0.68][138-4-1--0.15][139-4-3-0.42][140-4-1-0.25][141-4-3-1.73][142-4-4-0.92][143-4-4-0.45][144-4-2-1.13]
[145-4-1-1.31][148-4-0-2.40][149-4-2-0.67][150-4-1-1.53][151-4-2-1.04][152-4-2-0.86][153-4-2-1.32][154-4-2-1.87][155-4-4-0.22][156-4-3-1.12]
[157-4-0-1.44][158-4-2-0.50][160-4-1-0.36][161-4-2-1.02][162-4-2-0.73][164-4-2-1.42][165-4-2-0.15][167-4-0-0.77][168-4-4-0.45][170-4-0-2.38]
[171-4-0--0.09][172-4-4-0.77][173-4-2--0.05][174-4-0-1.76][175-4-2-0.59][177-4-0-1.53][178-4-2-0.81][179-4-2-0.21][180-4-4-1.20][181-4-3-1.30]
[182-4-3-1.01][183-4-2-0.38][184-4-2-0.92][186-4-2-0.48][187-4-1-1.12][188-4-2-1.48][189-4-2-0.76][190-4-1-0.67][191-4-2-1.53][192-4-2-1.31]
[193-4-1-1.73][194-4-2-0.46][195-4-0-1.35][196-4-2-1.53][197-4-1-1.73][198-4-2-1.18][199-4-2--0.06]
---------------------------
I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 0.989 | Acc: 53.625% | Wgt Acc: 62.292%
	I - Batch: 100 | Loss: 0.978 | Acc: 53.688% | Wgt Acc: 62.606%
	I - Batch: 150 | Loss: 0.981 | Acc: 53.375% | Wgt Acc: 62.124%
	I - Batch: 200 | Loss: 0.981 | Acc: 53.031% | Wgt Acc: 62.190%
	I - Batch: 250 | Loss: 0.973 | Acc: 53.300% | Wgt Acc: 62.643%
I - num batch: 285
I - Train -- Loss: 0.972 | Acc: 53.253% | Wgt Acc: 62.564% | LR: 1.000000e-03 | Dur: 178.47s
I - Confusion Matrix: [row->prediction - col->label]
[[548.  25.  38. 157. 146.]
 [ 24. 470. 135.  66. 143.]
 [ 62. 223. 829. 120. 459.]
 [166.  59. 109. 496. 172.]
 [  6.   3.  13.   1.  80.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.356 | Acc: 46.746% | Wgt Acc: 54.158% | Dur: 26.24s
I - Confusion Matrix: [row->prediction - col->label]
[[40.  1.  1.  4.  5.]
 [ 2. 41. 10.  7. 22.]
 [10. 31. 54. 22. 89.]
 [32.  3.  6. 52. 14.]
 [ 4.  2.  4.  1. 50.]]

I - Local maximum validation set accuracy:  46.75

I - Validation set results: 
[14-1-2-1.39][50-3-1-0.49][124-2-2-2.19][127-0-0-2.01][443-2-2-1.88][567-0-0-0.65][573-1-1-1.10][615-0-3-1.08][695-1-2-1.54][722-3-3-2.65]
[826-0-3-0.84][878-0-3-1.34][1103-0-4-0.11][1212-3-2-1.29][1368-0-0-2.37][2181-2-2-1.43][2476-2-2-0.76][2721-2-2-2.36][2818-1-1-0.39][2886-2-2-0.99]
[3231-2-1-2.33][3333-2-1-1.17][3482-2-2-2.18][3536-3-3-1.73][3625-1-1-2.06][3909-0-0-1.03][4035-0-3-1.24][4140-0-0-0.27][4214-1-2-2.01][4346-1-3-0.43]
[4581-2-2-1.85][4708-3-2-0.18][4838-3-2-0.22][4845-1-2-1.14][4868-0-0-0.64][4939-0-4-0.17][4984-2-2-1.56][5078-1-1-0.39][5396-0-0-3.14][5479-1-1-2.26]
[5717-0-0-1.45][5843-1-1-1.84][5949-3-3-1.47][5987-2-4-1.52][6014-3-1-1.07][6033-3-3-0.05][6313-0-3-1.26][6421-3-2-1.02][6500-1-2-0.62][6583-3-2-1.30]
[6683-3-3-0.84][6825-2-3-0.66][6998-3-2-0.92][7049-3-3-0.83][7517-1-1-1.82][7521-1-1-0.88][7528-1-3-0.74][7949-1-2-1.84][8135-1-0--0.09][8185-3-0-1.24]
[8269-3-2-2.13][8273-3-3-1.59][8543-3-0-2.23][8666-1-1-1.81][8672-0-3-1.27][8903-1-2-2.02][9001-2-2-1.17][9036-2-2-2.65][9281-3-3-0.61][9300-2-1-1.72]
[9571-0-3-0.62][9617-1-1-1.44][9644-2-2-1.64][9705-2-2-0.04][9801-0-3-0.44][9803-3-3-0.90][9865-3-3-2.14][9896-2-2-1.25][10314-1-2-1.03][10337-3-3-2.87]
[10403-0-2-1.04][10653-2-1-0.82][10704-2-2-1.53][10719-1-1-2.44][10727-1-1-0.75][10836-0-0-2.71][10969-2-2-0.90][11042-0-0-0.19][11088-1-2-2.24][11322-0-0-1.27]
[11398-2-2-2.57][11499-0-0-0.81][11502-3-3-0.80][11512-3-3-0.36][11608-1-1-2.72][11610-0-0-1.56][11692-0-3-1.07][11905-0-0-1.68][11993-1-2-0.85][12002-2-3-2.10]
[12052-0-0-2.00][12201-0-3-1.42][12235-2-1-2.18][12320-1-2-0.75][12377-2-4-1.66][12398-2-1-0.83][12503-1-1-1.44][12617-0-1-1.13][12685-3-1-0.40][12738-2-2-0.63]
[12742-2-2-2.66][12823-0-3-1.47][13110-1-1-1.53][13240-3-3-1.25][13253-1-1-1.20][13273-0-0-1.87][13634-1-2-1.30][13763-2-2-1.13][13905-3-2-0.66][14060-2-1-1.51]
[14065-3-3-0.98][14147-3-2-0.33][14595-2-2-1.76][14687-2-2-2.11][14788-2-2-1.33][14869-1-1-2.30][14872-3-4--0.07][14877-1-1-1.36][14927-0-3-1.39][15066-0-0-1.82]
[15175-1-1-1.55][15178-2-3-0.67][15375-3-3-0.51][15389-3-3-1.73][15568-2-1-1.37][15675-3-3-0.96][15869-1-1-1.20][16207-3-3--0.06][16236-0-2-0.72][16302-3-2-1.02]
[16331-2-2-1.99][16381-0-3-0.23][16488-1-1-2.46][16495-0-0-1.02][16650-0-0-1.81][16719-1-1-0.85][16801-0-0-3.11][16828-0-0-1.19][17137-3-2-0.36][17245-1-2-1.73]
[17278-3-1-0.29][17282-0-0-0.63][17311-2-2-2.44][17336-2-2-1.01][17608-3-3-1.93][17627-0-3-0.16][17877-3-2-1.12][17924-1-2-0.88][17984-3-3-1.96][18211-0-3-1.34]
[18276-3-3-0.24][18287-1-1-1.41][18394-0-0-1.99][18428-0-0-1.65][18442-0-3-1.29][18478-3-3-0.93][18607-0-0-0.43][18616-0-3-0.04][18663-0-3-0.98][18718-0-0-1.22]
[18766-2-2-2.47][18824-2-2-0.62][18890-3-3-1.22][18930-3-2-0.91][18938-3-3-0.74][19817-1-2-1.76][19839-0-2-0.84][19930-3-3-1.09][19944-0-2-1.18][20036-2-2-2.65]
[20101-3-3-1.21][20474-1-1-2.27][20547-3-3-0.92][20929-2-2-1.72][21245-1-2-1.53][21257-3-3-0.30][21293-1-1-1.35][21316-1-1-2.41][21384-1-2-1.67][21448-1-1-1.46]
[21483-0-0-1.37][21487-2-2-1.64][21714-0-2-1.01][21943-3-2-1.35][21947-0-0-0.39][21948-0-0-3.01][21965-2-1-0.92][21998-1-1-0.14][22025-0-2-0.82][22228-3-3-1.17]
[22446-1-1-1.93][22494-3-3-1.03][22757-0-3-2.48][22811-3-3-1.92][22976-3-1-1.40][22985-3-3-1.92][23014-0-3-1.17][23112-1-1-1.50][23144-3-3-2.11][23168-2-3-0.01]
[23219-0-3-0.46][23363-3-3-0.96][23470-0-2-0.80][23486-2-2-1.15][23497-0-3-2.91][23516-0-0-2.47][23690-1-2-1.40][23921-2-2-1.64][23936-1-2-1.40][24040-3-2--0.14]
[24111-1-4-2.14][24182-0-3-2.06][24238-3-3-1.36][24290-2-0-0.63][24345-0-0-1.17][24364-1-2-1.23][24427-3-3-1.16][24477-2-2-2.53][24495-2-1-0.60][24893-2-2-1.51]
[25012-1-2-0.59][25121-2-2-1.42][25165-3-3-1.34][25183-0-0-0.55][25297-3-3-1.15][25398-0-0-1.02][25574-2-2-2.09][25644-1-1-2.87][25718-1-1-0.41][25774-2-2-1.52]
[26032-3-3-0.95][26051-3-3-2.58][26120-0-4-0.32][26321-1-2-0.51][26732-1-1-1.39][26784-3-3-1.96][26827-3-2-0.68][26833-0-3-2.69][26838-2-2-0.54][26860-1-2-0.95]
[26948-0-0-0.18][27049-3-0-1.37][27098-1-2-0.17][27526-0-0-1.23][27639-3-3-0.49][27698-3-3-1.40][27772-0-0-1.49][27890-1-1-1.29][28040-0-2-0.77][28503-2-2-3.28]
[28577-1-1-1.63][28959-0-0-3.08][29198-3-3-1.09][29777-0-0-2.26][29877-2-2-1.47][30035-1-2-1.65][30098-0-0-0.35][30326-1-1-2.81][30572-2-2-1.87][30716-0-4-1.19]
[30806-2-2-1.57][30906-1-1-1.37][31007-0-3-0.10][31181-3-2-0.65][31238-0-3-1.64][31347-0-3-0.74][31422-2-2-0.71][31429-3-1-0.44][31431-0-3-0.22][31432-1-1-2.67]
[31477-0-3-1.90][31524-1-2-0.58][31597-1-2-1.69][31619-1-2-1.18][31701-0-3-0.28][31755-0-2-0.85][31854-3-1-0.40][32074-1-1-1.07][32078-3-3-0.58][32111-1-4-1.04]
[32127-1-2-2.20][32140-3-3-1.27][32263-2-4-0.61][32365-0-0-0.72][32411-2-3-1.65][32429-3-3-1.53][32473-3-2-0.57][32574-3-3-1.58][32584-0-2-0.17][32622-0-1-0.32]
[32858-3-0-0.51][32969-3-3-1.29][33016-2-2-2.04][33031-1-3-0.44][33035-2-2-2.77][33133-2-2-0.93][33173-2-2-0.32][33175-3-2-2.02][33306-3-2-0.94][33309-2-3-0.42]
[33474-0-3--0.31][33478-2-2-0.66][33618-1-1-2.23][33712-0-3-0.48][33782-2-2-1.63][33914-3-3-0.71][34076-3-2-1.54][34112-2-2-1.99][34138-2-2-1.35][34239-1-1-0.42]
[34364-2-2-2.60][34617-1-2-0.87][34751-3-3-1.44][34783-2-4-1.19][35015-3-2-1.64][35018-1-2-1.51][35288-2-2-1.54][0-4-2-0.40][1-4-4-1.17][2-4-4-0.35]
[3-4-2-1.19][4-4-2-1.23][5-4-1-1.23][6-4-4-1.25][7-4-1-0.57][8-4-2-1.48][9-4-2-1.10][10-4-4-0.79][11-4-2-3.51][12-4-2-1.27]
[14-4-3-0.45][15-4-3-1.18][16-4-4-1.14][17-4-1-1.65][18-4-4-1.10][19-4-3-1.10][20-4-2-0.19][21-4-1-1.43][22-4-4-1.78][23-4-2-0.43]
[24-4-4-0.75][25-4-2-0.87][26-4-2-0.42][27-4-2-0.98][28-4-4-0.69][29-4-2-0.97][30-4-2-0.77][31-4-2-1.28][32-4-4-1.20][33-4-2-1.64]
[34-4-2-0.58][35-4-1-0.70][37-4-2-1.71][39-4-3-1.09][40-4-4-0.40][41-4-2-0.61][42-4-2-0.53][43-4-2-0.78][45-4-2-1.71][46-4-2-1.16]
[47-4-4-1.21][48-4-2-0.98][51-4-4-1.73][52-4-2-1.09][53-4-2-0.44][54-4-3-0.20][55-4-2-0.75][56-4-1-1.41][57-4-3-1.14][58-4-2-2.01]
[59-4-2-0.74][60-4-1-0.71][61-4-4-1.50][62-4-2-1.12][63-4-2-2.65][64-4-1-0.65][65-4-4-1.76][66-4-4-2.26][67-4-2-0.94][68-4-2-1.46]
[69-4-3-0.46][70-4-4-0.77][72-4-2-1.08][73-4-2-0.60][74-4-2-2.29][75-4-2-0.80][77-4-4-1.42][78-4-2-1.77][79-4-2-1.44][80-4-4-1.62]
[81-4-1-1.08][82-4-1-1.01][83-4-1-1.17][84-4-2-0.88][85-4-2-0.79][86-4-4-0.70][87-4-4-1.10][88-4-4-0.66][89-4-2-1.27][90-4-2-0.56]
[91-4-2-1.45][92-4-2-0.46][93-4-4-0.23][94-4-4-1.32][95-4-2-0.58][96-4-1-1.97][97-4-2-1.25][98-4-2-1.75][99-4-4-1.54][100-4-2-1.26]
[101-4-4-1.11][102-4-2-1.13][103-4-3-1.06][104-4-4-1.16][105-4-4-2.34][106-4-1-1.06][107-4-4-0.45][108-4-2-0.77][109-4-1-0.41][110-4-2-1.00]
[111-4-0-1.42][112-4-4-0.47][113-4-2-0.77][114-4-3-0.67][115-4-4-0.30][116-4-1--0.18][117-4-1-2.25][119-4-2-2.92][121-4-1-1.49][122-4-2-0.66]
[124-4-2-1.11][125-4-4-1.91][126-4-4-0.49][127-4-2-1.55][128-4-2-1.02][129-4-2-0.14][130-4-2-1.50][131-4-2-2.22][132-4-2-0.87][133-4-4-1.01]
[135-4-2-1.16][136-4-1-0.51][137-4-4-0.56][138-4-1-1.79][139-4-2-1.52][140-4-2-0.18][141-4-3-1.00][142-4-4-1.74][143-4-4-1.36][144-4-4-2.26]
[145-4-1-1.67][148-4-0-0.76][149-4-2-0.81][150-4-1-0.64][151-4-2-0.95][152-4-2-0.68][153-4-2-1.19][154-4-2-1.10][155-4-4-0.73][156-4-3-0.56]
[157-4-0-0.26][158-4-2-0.27][160-4-4-1.03][161-4-2-1.12][162-4-2-1.42][164-4-2-1.17][165-4-4-0.52][167-4-0-0.66][168-4-4-1.07][170-4-3-0.88]
[171-4-2-0.42][172-4-4-0.98][173-4-4-0.34][174-4-0-0.43][175-4-4-0.99][177-4-3-0.30][178-4-4-1.80][179-4-2-0.03][180-4-4-1.95][181-4-3-0.67]
[182-4-2-1.72][183-4-4-0.85][184-4-2-2.13][186-4-2-0.58][187-4-2-1.43][188-4-2-1.01][189-4-2-1.11][190-4-1-0.56][191-4-2-1.49][192-4-2-1.74]
[193-4-2-3.06][194-4-2-0.83][195-4-2-1.17][196-4-2-1.06][197-4-4-1.32][198-4-4-1.75][199-4-2-0.57]
---------------------------
I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 1.010 | Acc: 51.750% | Wgt Acc: 62.041%
	I - Batch: 100 | Loss: 0.970 | Acc: 52.750% | Wgt Acc: 62.845%
	I - Batch: 150 | Loss: 0.957 | Acc: 54.000% | Wgt Acc: 64.158%
	I - Batch: 200 | Loss: 0.947 | Acc: 54.031% | Wgt Acc: 64.019%
	I - Batch: 250 | Loss: 0.949 | Acc: 54.275% | Wgt Acc: 64.144%
I - num batch: 285
I - Train -- Loss: 0.952 | Acc: 53.978% | Wgt Acc: 63.709% | LR: 1.000000e-03 | Dur: 171.68s
I - Confusion Matrix: [row->prediction - col->label]
[[545.  20.  44. 164. 119.]
 [ 29. 477. 113.  47. 137.]
 [ 69. 222. 851. 112. 509.]
 [159.  57. 106. 515. 167.]
 [  4.   4.  10.   2.  68.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.554 | Acc: 40.039% | Wgt Acc: 53.719% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[44.  3.  4.  6. 26.]
 [ 2. 31.  8.  2. 20.]
 [ 4. 33. 49.  9. 82.]
 [38. 10. 14. 69. 42.]
 [ 0.  1.  0.  0. 10.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 0.960 | Acc: 53.125% | Wgt Acc: 63.519%
	I - Batch: 100 | Loss: 0.947 | Acc: 54.375% | Wgt Acc: 64.220%
	I - Batch: 150 | Loss: 0.931 | Acc: 54.958% | Wgt Acc: 64.557%
	I - Batch: 200 | Loss: 0.922 | Acc: 55.000% | Wgt Acc: 64.880%
	I - Batch: 250 | Loss: 0.930 | Acc: 54.525% | Wgt Acc: 64.212%
I - num batch: 285
I - Train -- Loss: 0.932 | Acc: 54.308% | Wgt Acc: 64.116% | LR: 1.000000e-03 | Dur: 172.49s
I - Confusion Matrix: [row->prediction - col->label]
[[552.  25.  44. 156. 143.]
 [ 33. 483. 122.  49. 136.]
 [ 65. 222. 847. 112. 496.]
 [153.  47.  97. 521. 157.]
 [  3.   3.  14.   2.  68.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.321 | Acc: 46.351% | Wgt Acc: 58.736% | Dur: 14.12s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  3.  3.  9. 20.]
 [ 3. 40. 11.  3. 23.]
 [ 5. 26. 48.  7. 80.]
 [26.  6. 11. 67. 30.]
 [ 1.  3.  2.  0. 27.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 0.879 | Acc: 58.000% | Wgt Acc: 67.495%
	I - Batch: 100 | Loss: 0.905 | Acc: 55.812% | Wgt Acc: 65.437%
	I - Batch: 150 | Loss: 0.919 | Acc: 55.125% | Wgt Acc: 64.990%
	I - Batch: 200 | Loss: 0.910 | Acc: 55.781% | Wgt Acc: 65.541%
	I - Batch: 250 | Loss: 0.905 | Acc: 55.750% | Wgt Acc: 65.445%
I - num batch: 285
I - Train -- Loss: 0.907 | Acc: 55.582% | Wgt Acc: 65.380% | LR: 1.000000e-03 | Dur: 178.12s
I - Confusion Matrix: [row->prediction - col->label]
[[549.  22.  38. 139. 144.]
 [ 38. 505. 108.  60. 158.]
 [ 54. 186. 861. 104. 456.]
 [163.  59. 106. 532. 160.]
 [  2.   8.  11.   5.  82.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.358 | Acc: 45.562% | Wgt Acc: 57.248% | Dur: 14.68s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  3.  4. 11. 22.]
 [ 0. 33.  7.  2. 16.]
 [ 6. 32. 52. 11. 91.]
 [24.  5. 10. 61. 23.]
 [ 1.  5.  2.  1. 28.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 0.829 | Acc: 60.375% | Wgt Acc: 70.075%
	I - Batch: 100 | Loss: 0.842 | Acc: 58.688% | Wgt Acc: 68.876%
	I - Batch: 150 | Loss: 0.837 | Acc: 58.500% | Wgt Acc: 68.592%
	I - Batch: 200 | Loss: 0.832 | Acc: 58.844% | Wgt Acc: 68.857%
	I - Batch: 250 | Loss: 0.832 | Acc: 58.775% | Wgt Acc: 68.789%
I - num batch: 285
I - Train -- Loss: 0.833 | Acc: 58.967% | Wgt Acc: 68.879% | LR: 5.000000e-04 | Dur: 173.30s
I - Confusion Matrix: [row->prediction - col->label]
[[580.  18.  40. 136. 152.]
 [ 20. 551.  88.  54. 122.]
 [ 54. 147. 893.  97. 443.]
 [148.  55.  89. 549. 173.]
 [  4.   9.  14.   4. 110.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.369 | Acc: 48.126% | Wgt Acc: 60.789% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  6. 12. 21.]
 [ 3. 39.  7.  4. 16.]
 [ 1. 25. 55.  8. 83.]
 [23.  9.  7. 62. 32.]
 [ 1.  1.  0.  0. 28.]]

I - Local maximum validation set accuracy:  48.13

I - Validation set results: 
[14-1-2-1.41][50-3-3--0.26][124-2-2-2.04][127-0-0-3.25][443-2-2-1.62][567-0-0-2.03][573-1-1-1.47][615-0-0-2.36][695-1-2-0.64][722-3-0-3.17]
[826-0-0-2.60][878-0-0-3.23][1103-0-0-0.42][1212-3-2-0.51][1368-0-0-3.52][2181-2-0-0.95][2476-2-2-0.64][2721-2-2-2.37][2818-1-3-1.25][2886-2-1-1.22]
[3231-2-2-2.25][3333-2-2-1.51][3482-2-2-2.65][3536-3-3-1.61][3625-1-1-3.86][3909-0-0-2.79][4035-0-0-2.64][4140-0-0-2.10][4214-1-3-1.46][4346-1-0-1.07]
[4581-2-2-2.06][4708-3-3-0.76][4838-3-3-1.48][4845-1-2-0.43][4868-0-0-2.41][4939-0-1-0.16][4984-2-2-2.19][5078-1-2-0.20][5396-0-0-4.32][5479-1-1-1.65]
[5717-0-0-3.88][5843-1-1-1.06][5949-3-3-1.52][5987-2-2-0.89][6014-3-3-1.08][6033-3-0-0.58][6313-0-3-2.21][6421-3-3-2.76][6500-1-2-0.30][6583-3-2-1.63]
[6683-3-3-1.96][6825-2-3-0.99][6998-3-3-0.87][7049-3-3-1.26][7517-1-2-1.78][7521-1-1-1.80][7528-1-3-1.69][7949-1-2-0.92][8135-1-0-1.18][8185-3-0-2.28]
[8269-3-1-2.98][8273-3-3-1.79][8543-3-0-3.39][8666-1-1-1.40][8672-0-0-3.90][8903-1-1-1.69][9001-2-2-1.49][9036-2-2-2.03][9281-3-3-0.69][9300-2-2-3.19]
[9571-0-3-1.18][9617-1-1-0.83][9644-2-2-1.21][9705-2-2-0.16][9801-0-3-0.98][9803-3-3-2.21][9865-3-3-2.91][9896-2-2-3.44][10314-1-2-0.66][10337-3-3-4.02]
[10403-0-0-0.63][10653-2-1-2.02][10704-2-2-0.69][10719-1-1-2.74][10727-1-2-0.52][10836-0-0-6.15][10969-2-3-1.92][11042-0-0-1.33][11088-1-1-1.77][11322-0-0-2.25]
[11398-2-2-1.45][11499-0-0-2.20][11502-3-3-1.25][11512-3-3-1.41][11608-1-1-2.82][11610-0-0-3.45][11692-0-3-1.30][11905-0-0-3.51][11993-1-1-1.91][12002-2-3-0.35]
[12052-0-0-3.18][12201-0-3-2.21][12235-2-1-1.95][12320-1-0-0.15][12377-2-2-0.37][12398-2-3-0.92][12503-1-2-0.95][12617-0-2-0.30][12685-3-3-1.56][12738-2-0--0.01]
[12742-2-2-3.87][12823-0-3-2.20][13110-1-1-1.04][13240-3-3-1.81][13253-1-1-1.95][13273-0-0-4.89][13634-1-2-0.93][13763-2-2-1.41][13905-3-3-0.27][14060-2-1-0.74]
[14065-3-0-1.51][14147-3-3-1.34][14595-2-2-1.39][14687-2-2-3.04][14788-2-2-2.22][14869-1-1-4.02][14872-3-3-0.13][14877-1-1-0.69][14927-0-3-1.58][15066-0-0-3.36]
[15175-1-1-2.19][15178-2-3-0.80][15375-3-3-0.89][15389-3-3-1.73][15568-2-1-1.73][15675-3-3-3.09][15869-1-2-0.57][16207-3-3-0.34][16236-0-3-0.66][16302-3-3-1.19]
[16331-2-2-2.75][16381-0-0-0.92][16488-1-1-3.12][16495-0-0-3.44][16650-0-0-4.50][16719-1-2-0.79][16801-0-0-4.86][16828-0-0-2.33][17137-3-0-0.99][17245-1-2-0.73]
[17278-3-0-0.60][17282-0-0-1.55][17311-2-2-1.95][17336-2-2-1.13][17608-3-3-2.82][17627-0-1-0.33][17877-3-1-0.27][17924-1-3-0.81][17984-3-3-3.18][18211-0-3-1.93]
[18276-3-3-0.89][18287-1-1-1.84][18394-0-0-3.49][18428-0-0-2.04][18442-0-3-1.85][18478-3-3-2.13][18607-0-0-2.08][18616-0-3-0.05][18663-0-3-0.87][18718-0-0-3.42]
[18766-2-2-2.21][18824-2-2-1.18][18890-3-3-2.39][18930-3-3-0.52][18938-3-3-0.96][19817-1-1-2.09][19839-0-0-0.24][19930-3-3-2.40][19944-0-0-0.70][20036-2-2-3.58]
[20101-3-3-1.25][20474-1-1-1.63][20547-3-3-1.85][20929-2-2-2.60][21245-1-2-1.92][21257-3-3-1.20][21293-1-1-1.09][21316-1-1-0.82][21384-1-2-1.95][21448-1-2-1.42]
[21483-0-0-2.87][21487-2-2-2.50][21714-0-3-0.38][21943-3-2-1.08][21947-0-0-2.91][21948-0-0-4.77][21965-2-2-0.91][21998-1-1-1.57][22025-0-3-0.60][22228-3-3-2.18]
[22446-1-1-0.65][22494-3-0-1.22][22757-0-3-2.68][22811-3-3-2.54][22976-3-1-1.38][22985-3-3-2.67][23014-0-3-2.06][23112-1-1-1.79][23144-3-3-2.80][23168-2-0-1.44]
[23219-0-0-1.53][23363-3-3-2.45][23470-0-0-0.89][23486-2-2-2.31][23497-0-3-3.62][23516-0-0-4.06][23690-1-1-0.96][23921-2-2-2.76][23936-1-2-0.80][24040-3-2-0.14]
[24111-1-4-1.03][24182-0-0-4.93][24238-3-3-2.00][24290-2-0-2.77][24345-0-0-0.41][24364-1-3-0.89][24427-3-0-1.88][24477-2-2-2.31][24495-2-1-0.33][24893-2-1-1.06]
[25012-1-3-0.42][25121-2-2-1.92][25165-3-3-1.17][25183-0-0-1.86][25297-3-3-2.10][25398-0-0-2.68][25574-2-2-1.85][25644-1-1-2.40][25718-1-2-0.48][25774-2-2-1.37]
[26032-3-3-1.81][26051-3-3-2.56][26120-0-0-1.33][26321-1-1-4.49][26732-1-1-1.14][26784-3-3-2.48][26827-3-3-1.09][26833-0-3-2.97][26838-2-2-0.50][26860-1-2-0.80]
[26948-0-0-1.94][27049-3-0-3.07][27098-1-3-0.90][27526-0-0-2.15][27639-3-3-0.37][27698-3-3-2.15][27772-0-0-3.08][27890-1-1-3.27][28040-0-0-2.83][28503-2-2-3.85]
[28577-1-1-3.42][28959-0-0-5.04][29198-3-3-2.01][29777-0-0-4.29][29877-2-2-1.65][30035-1-2-2.31][30098-0-0-1.40][30326-1-1-2.77][30572-2-2-2.83][30716-0-4-0.52]
[30806-2-2-1.66][30906-1-1-2.14][31007-0-3-1.53][31181-3-2-0.42][31238-0-3-1.87][31347-0-0-2.01][31422-2-2-0.71][31429-3-3-1.24][31431-0-3-1.11][31432-1-1-2.39]
[31477-0-0-2.78][31524-1-2-1.15][31597-1-2-1.73][31619-1-0-1.44][31701-0-0-2.22][31755-0-0-1.60][31854-3-3-2.49][32074-1-3-1.19][32078-3-3-1.23][32111-1-1-0.56]
[32127-1-2-2.98][32140-3-3-2.67][32263-2-0-0.54][32365-0-0-2.14][32411-2-0-4.01][32429-3-0-2.15][32473-3-2-0.97][32574-3-3-2.86][32584-0-0-0.92][32622-0-1-0.33]
[32858-3-0-1.61][32969-3-3-2.42][33016-2-2-1.85][33031-1-3-1.76][33035-2-2-3.21][33133-2-2-2.04][33173-2-2-1.11][33175-3-1-1.06][33306-3-3-0.81][33309-2-3-0.83]
[33474-0-3-1.01][33478-2-3-0.14][33618-1-1-3.17][33712-0-3-1.68][33782-2-2-2.47][33914-3-3-1.91][34076-3-2-1.47][34112-2-2-2.36][34138-2-2-1.17][34239-1-1-0.61]
[34364-2-2-2.65][34617-1-1-1.00][34751-3-3-2.98][34783-2-2-0.99][35015-3-2-1.93][35018-1-2-1.77][35288-2-2-1.19][0-4-2-2.05][1-4-4-1.14][2-4-3-0.62]
[3-4-2-0.62][4-4-2-0.12][5-4-3-0.90][6-4-0-0.59][7-4-4-1.12][8-4-2-1.40][9-4-2-0.65][10-4-4-0.46][11-4-2-3.44][12-4-2-0.60]
[14-4-3-0.70][15-4-3-1.76][16-4-4-0.16][17-4-2-0.08][18-4-2-0.82][19-4-3-1.41][20-4-3-0.21][21-4-2-0.56][22-4-4-0.69][23-4-2-0.59]
[24-4-4-1.04][25-4-3-1.36][26-4-3-0.76][27-4-2-0.35][28-4-2-0.71][29-4-1-1.63][30-4-3-1.21][31-4-2-1.14][32-4-1-0.45][33-4-2-1.23]
[34-4-2-0.14][35-4-3-1.38][37-4-2-2.40][39-4-0-3.58][40-4-0-0.33][41-4-3-0.30][42-4-2-0.21][43-4-1-2.69][45-4-2-1.93][46-4-4-1.30]
[47-4-2-1.30][48-4-2-2.04][51-4-4-1.17][52-4-2-0.55][53-4-2-1.03][54-4-3-1.33][55-4-3-1.51][56-4-1-0.77][57-4-3-1.69][58-4-2-2.15]
[59-4-0-2.08][60-4-3-0.19][61-4-4-0.95][62-4-2-2.07][63-4-2-4.03][64-4-0-0.06][65-4-2-0.99][66-4-1-1.59][67-4-2-0.37][68-4-3-0.72]
[69-4-3-0.69][70-4-2-0.13][72-4-4-0.91][73-4-3-0.14][74-4-2-2.00][75-4-0-0.78][77-4-4-1.84][78-4-2-0.79][79-4-2-1.07][80-4-4-0.99]
[81-4-2-2.02][82-4-1-1.55][83-4-1-0.61][84-4-3-0.36][85-4-4-0.25][86-4-2-0.57][87-4-4-0.73][88-4-1-0.78][89-4-2-2.16][90-4-0-0.20]
[91-4-2-0.78][92-4-3--0.11][93-4-0-1.96][94-4-4-0.89][95-4-2-0.10][96-4-1-1.93][97-4-2-0.24][98-4-2-1.64][99-4-4-0.43][100-4-2-0.84]
[101-4-4-1.54][102-4-2-0.79][103-4-2-1.13][104-4-2-0.82][105-4-4-1.66][106-4-1-1.58][107-4-0-0.76][108-4-3-0.15][109-4-2-0.34][110-4-1-0.08]
[111-4-0-2.52][112-4-0-0.11][113-4-2-1.13][114-4-3-1.30][115-4-3-0.31][116-4-2-0.06][117-4-2-1.48][119-4-2-3.34][121-4-1-1.01][122-4-4-0.21]
[124-4-2-0.69][125-4-2-1.50][126-4-4-0.20][127-4-2-1.11][128-4-3-0.98][129-4-3--0.14][130-4-2-0.84][131-4-2-2.53][132-4-2-0.32][133-4-4-0.92]
[135-4-2-1.02][136-4-0-0.10][137-4-2-0.89][138-4-2-1.00][139-4-3-0.51][140-4-1-0.60][141-4-3-0.92][142-4-4-0.69][143-4-2-0.26][144-4-4-2.03]
[145-4-4-0.75][148-4-0-1.78][149-4-2-0.69][150-4-2-1.78][151-4-2-1.02][152-4-3-0.29][153-4-2-1.54][154-4-1-2.89][155-4-4-1.55][156-4-3-1.35]
[157-4-0-0.69][158-4-2-0.30][160-4-2-0.58][161-4-2-1.02][162-4-2-0.13][164-4-2-1.25][165-4-2-0.36][167-4-0-1.15][168-4-0-0.25][170-4-0-1.24]
[171-4-2-0.84][172-4-4-0.61][173-4-4-0.27][174-4-0-1.43][175-4-2-0.63][177-4-0-0.58][178-4-2-0.95][179-4-2--0.01][180-4-4-1.39][181-4-3-1.31]
[182-4-2-0.39][183-4-2-0.96][184-4-2-1.58][186-4-2-0.15][187-4-2-1.72][188-4-2-1.23][189-4-2-1.38][190-4-2-0.64][191-4-2-2.50][192-4-0-0.28]
[193-4-2-2.48][194-4-1-1.63][195-4-0-2.28][196-4-3-1.07][197-4-1-0.88][198-4-2-1.30][199-4-3-0.89]
---------------------------
I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 0.812 | Acc: 60.125% | Wgt Acc: 71.158%
	I - Batch: 100 | Loss: 0.818 | Acc: 60.812% | Wgt Acc: 72.101%
	I - Batch: 150 | Loss: 0.795 | Acc: 61.333% | Wgt Acc: 72.119%
	I - Batch: 200 | Loss: 0.791 | Acc: 61.562% | Wgt Acc: 72.252%
	I - Batch: 250 | Loss: 0.790 | Acc: 61.425% | Wgt Acc: 72.051%
I - num batch: 285
I - Train -- Loss: 0.790 | Acc: 61.121% | Wgt Acc: 71.853% | LR: 5.000000e-04 | Dur: 176.56s
I - Confusion Matrix: [row->prediction - col->label]
[[589.  16.  38. 110. 144.]
 [ 25. 587.  75.  50. 126.]
 [ 58. 130. 926.  94. 465.]
 [128.  40.  78. 585. 171.]
 [  6.   7.   7.   1.  94.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.279 | Acc: 47.337% | Wgt Acc: 56.602% | Dur: 14.63s
I - Confusion Matrix: [row->prediction - col->label]
[[52.  0.  4. 10. 12.]
 [ 2. 40.  8.  3. 17.]
 [ 9. 30. 54. 18. 93.]
 [24.  5.  7. 52. 16.]
 [ 1.  3.  2.  3. 42.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 0.739 | Acc: 65.125% | Wgt Acc: 75.323%
	I - Batch: 100 | Loss: 0.730 | Acc: 64.750% | Wgt Acc: 74.998%
	I - Batch: 150 | Loss: 0.750 | Acc: 64.250% | Wgt Acc: 74.335%
	I - Batch: 200 | Loss: 0.749 | Acc: 64.156% | Wgt Acc: 74.354%
	I - Batch: 250 | Loss: 0.753 | Acc: 63.525% | Wgt Acc: 73.900%
I - num batch: 285
I - Train -- Loss: 0.756 | Acc: 63.319% | Wgt Acc: 73.703% | LR: 5.000000e-04 | Dur: 180.71s
I - Confusion Matrix: [row->prediction - col->label]
[[603.  19.  31. 115. 152.]
 [ 17. 598.  76.  35. 126.]
 [ 51. 126. 932.  70. 435.]
 [129.  29.  69. 614. 153.]
 [  6.   8.  16.   6. 134.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.271 | Acc: 49.507% | Wgt Acc: 59.520% | Dur: 16.94s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  1.  4. 19. 12.]
 [ 2. 42. 10.  3. 17.]
 [ 9. 28. 56. 12. 93.]
 [16.  5.  4. 51. 16.]
 [ 1.  2.  1.  1. 42.]]

I - Local maximum validation set accuracy:  49.51

I - Validation set results: 
[14-1-2-3.22][50-3-1-0.20][124-2-2-3.50][127-0-0-3.91][443-2-2-2.13][567-0-0-1.64][573-1-1-3.00][615-0-0-1.42][695-1-2-2.73][722-3-0-2.18]
[826-0-0-2.64][878-0-0-2.22][1103-0-0-0.82][1212-3-2-0.79][1368-0-0-3.29][2181-2-3-1.01][2476-2-2-1.61][2721-2-2-3.15][2818-1-3-0.42][2886-2-2-2.11]
[3231-2-2-3.14][3333-2-2-1.67][3482-2-2-2.60][3536-3-3-1.06][3625-1-1-3.82][3909-0-0-3.05][4035-0-0-1.71][4140-0-0-1.35][4214-1-2-0.59][4346-1-3-0.66]
[4581-2-2-3.10][4708-3-3-1.34][4838-3-3-0.81][4845-1-2-0.88][4868-0-0-2.38][4939-0-1--0.24][4984-2-2-2.74][5078-1-2-0.70][5396-0-0-4.75][5479-1-1-2.10]
[5717-0-0-3.08][5843-1-1-1.52][5949-3-0-1.25][5987-2-4-1.46][6014-3-3-0.62][6033-3-0-0.77][6313-0-3-2.17][6421-3-3-2.25][6500-1-1-0.31][6583-3-2-1.53]
[6683-3-3-1.20][6825-2-1-1.16][6998-3-3-0.70][7049-3-2-0.98][7517-1-2-3.66][7521-1-1-1.89][7528-1-3-1.02][7949-1-2-1.75][8135-1-0-1.47][8185-3-0-2.07]
[8269-3-2-0.78][8273-3-3-1.93][8543-3-0-4.25][8666-1-1-3.17][8672-0-0-3.92][8903-1-1-1.37][9001-2-2-1.51][9036-2-2-3.40][9281-3-3-1.30][9300-2-2-2.78]
[9571-0-3-0.58][9617-1-1-1.18][9644-2-1-1.86][9705-2-1-0.66][9801-0-0-0.51][9803-3-3-1.63][9865-3-3-3.08][9896-2-2-3.05][10314-1-2-0.79][10337-3-3-3.06]
[10403-0-0-0.28][10653-2-1-2.24][10704-2-2-1.72][10719-1-1-2.94][10727-1-1-0.19][10836-0-0-5.96][10969-2-3-0.88][11042-0-0-0.55][11088-1-1-2.55][11322-0-0-2.47]
[11398-2-2-1.45][11499-0-3-1.16][11502-3-0-0.79][11512-3-3-1.14][11608-1-1-3.27][11610-0-0-3.08][11692-0-0-1.12][11905-0-0-2.36][11993-1-1-1.68][12002-2-0-2.15]
[12052-0-0-3.24][12201-0-3-1.99][12235-2-2-1.80][12320-1-2-0.34][12377-2-2-1.62][12398-2-2-0.66][12503-1-2-1.56][12617-0-2-1.44][12685-3-2-0.39][12738-2-2-0.09]
[12742-2-2-4.79][12823-0-3-1.69][13110-1-1-0.97][13240-3-3-1.65][13253-1-1-2.74][13273-0-0-4.26][13634-1-1-2.37][13763-2-2-1.81][13905-3-2--0.46][14060-2-2-1.51]
[14065-3-0-1.57][14147-3-3-0.67][14595-2-2-2.18][14687-2-2-2.80][14788-2-2-2.25][14869-1-1-5.18][14872-3-0--0.01][14877-1-1-2.28][14927-0-3-0.75][15066-0-0-3.63]
[15175-1-1-3.08][15178-2-3-0.26][15375-3-0-0.97][15389-3-0-0.71][15568-2-1-2.31][15675-3-3-2.57][15869-1-2-0.52][16207-3-0--0.29][16236-0-2-1.04][16302-3-0-1.17]
[16331-2-2-2.52][16381-0-0-1.40][16488-1-1-4.32][16495-0-0-2.87][16650-0-0-3.97][16719-1-1-0.78][16801-0-0-4.81][16828-0-0-1.97][17137-3-0-0.29][17245-1-2-0.95]
[17278-3-3--0.41][17282-0-0-2.01][17311-2-2-2.23][17336-2-2-1.96][17608-3-3-2.10][17627-0-2-0.76][17877-3-0-0.62][17924-1-3-1.12][17984-3-3-2.25][18211-0-3-2.46]
[18276-3-0-1.82][18287-1-1-1.80][18394-0-0-3.08][18428-0-0-4.77][18442-0-0-2.21][18478-3-3-2.01][18607-0-0-2.23][18616-0-3-0.14][18663-0-0-1.24][18718-0-0-2.93]
[18766-2-2-3.81][18824-2-2-1.80][18890-3-3-2.40][18930-3-4-0.59][18938-3-3-1.13][19817-1-2-2.53][19839-0-2-0.95][19930-3-3-1.86][19944-0-2-1.25][20036-2-2-4.15]
[20101-3-3-0.86][20474-1-1-2.40][20547-3-3-1.41][20929-2-2-2.81][21245-1-2-2.83][21257-3-3-0.26][21293-1-2-2.95][21316-1-1-1.87][21384-1-2-2.03][21448-1-2-2.10]
[21483-0-0-1.51][21487-2-2-2.72][21714-0-2-0.21][21943-3-2-0.93][21947-0-0-1.41][21948-0-0-4.98][21965-2-2-1.69][21998-1-1-2.32][22025-0-2-0.76][22228-3-3-2.20]
[22446-1-1-1.50][22494-3-0-1.38][22757-0-3-2.34][22811-3-3-2.28][22976-3-1-2.07][22985-3-3-2.71][23014-0-0-2.78][23112-1-1-2.41][23144-3-3-2.86][23168-2-0-0.44]
[23219-0-2-0.48][23363-3-3-2.54][23470-0-1--0.11][23486-2-2-3.05][23497-0-3-3.32][23516-0-0-3.19][23690-1-4-1.18][23921-2-2-1.38][23936-1-2-1.78][24040-3-2-0.19]
[24111-1-4-2.03][24182-0-0-3.29][24238-3-3-0.75][24290-2-0-2.89][24345-0-0-1.36][24364-1-2-1.14][24427-3-3-1.27][24477-2-2-2.25][24495-2-2-0.97][24893-2-1-1.37]
[25012-1-2-1.34][25121-2-2-1.86][25165-3-3-0.96][25183-0-0-2.19][25297-3-3-1.47][25398-0-0-2.74][25574-2-2-2.57][25644-1-2-3.02][25718-1-1-0.96][25774-2-2-1.22]
[26032-3-3-1.10][26051-3-3-2.55][26120-0-0-1.12][26321-1-1-4.81][26732-1-1-2.16][26784-3-3-2.18][26827-3-3-0.08][26833-0-3-2.29][26838-2-2-0.34][26860-1-2-1.00]
[26948-0-0-2.21][27049-3-0-2.87][27098-1-1-1.04][27526-0-0-2.70][27639-3-3-0.94][27698-3-3-2.04][27772-0-0-2.03][27890-1-1-3.49][28040-0-0-1.50][28503-2-2-5.19]
[28577-1-1-3.20][28959-0-0-5.13][29198-3-3-2.40][29777-0-0-4.59][29877-2-2-2.51][30035-1-1-3.53][30098-0-0-1.12][30326-1-1-2.94][30572-2-2-2.17][30716-0-4-1.10]
[30806-2-2-1.74][30906-1-1-2.39][31007-0-3-1.37][31181-3-3-1.72][31238-0-3-1.15][31347-0-0-1.54][31422-2-2-2.11][31429-3-3-0.99][31431-0-3-1.58][31432-1-1-3.13]
[31477-0-0-2.87][31524-1-2-0.95][31597-1-2-1.58][31619-1-2-1.20][31701-0-0-2.65][31755-0-0-1.38][31854-3-3-1.88][32074-1-1-0.89][32078-3-3-0.85][32111-1-1-1.83]
[32127-1-2-3.43][32140-3-3-2.20][32263-2-2-0.36][32365-0-0-2.01][32411-2-0-4.08][32429-3-3-2.06][32473-3-2-0.88][32574-3-3-2.42][32584-0-0-0.83][32622-0-2-0.15]
[32858-3-0-1.76][32969-3-0-2.11][33016-2-2-2.50][33031-1-3-2.20][33035-2-2-3.23][33133-2-2-0.90][33173-2-1-0.96][33175-3-1-1.06][33306-3-2-1.18][33309-2-3-0.63]
[33474-0-3-1.65][33478-2-1--0.19][33618-1-1-4.15][33712-0-3-1.40][33782-2-2-1.81][33914-3-3-1.52][34076-3-2-1.15][34112-2-2-3.70][34138-2-2-1.02][34239-1-1-1.12]
[34364-2-2-4.30][34617-1-2-2.49][34751-3-3-2.50][34783-2-1-1.58][35015-3-2-1.99][35018-1-2-2.19][35288-2-1-0.85][0-4-2-1.33][1-4-4-1.56][2-4-3-0.79]
[3-4-2-1.33][4-4-2-1.42][5-4-1-0.31][6-4-4-2.06][7-4-4-2.03][8-4-2-1.68][9-4-2-1.16][10-4-3-0.39][11-4-2-4.18][12-4-2-1.63]
[14-4-3-0.48][15-4-0-1.98][16-4-4-1.06][17-4-2--0.17][18-4-4-1.86][19-4-0-1.28][20-4-2-0.12][21-4-2-2.55][22-4-4-1.11][23-4-2-0.65]
[24-4-4-2.13][25-4-3-1.02][26-4-1-0.55][27-4-2-0.32][28-4-4-2.03][29-4-2-1.75][30-4-3-0.29][31-4-1-0.63][32-4-4-1.01][33-4-2-1.71]
[34-4-2-0.47][35-4-3-0.29][37-4-3-0.62][39-4-0-2.11][40-4-1-0.15][41-4-2-0.56][42-4-2-1.33][43-4-2-1.43][45-4-2-2.22][46-4-2-1.06]
[47-4-2-1.49][48-4-2-2.27][51-4-4-1.51][52-4-2-0.29][53-4-2-0.72][54-4-3-1.07][55-4-3-1.41][56-4-2-1.61][57-4-3-1.11][58-4-2-2.61]
[59-4-0-0.70][60-4-3-0.22][61-4-4-1.47][62-4-3-0.93][63-4-2-2.55][64-4-2-0.04][65-4-4-1.69][66-4-4-1.06][67-4-2-1.22][68-4-2-0.49]
[69-4-2-1.77][70-4-2-1.64][72-4-2-2.19][73-4-1-0.76][74-4-2-2.28][75-4-0-0.88][77-4-4-2.24][78-4-2-0.87][79-4-2-2.00][80-4-4-2.01]
[81-4-1-1.41][82-4-1-1.46][83-4-2-0.55][84-4-4-1.50][85-4-4-1.01][86-4-2-1.54][87-4-4-0.93][88-4-4-0.94][89-4-2-1.80][90-4-4-1.26]
[91-4-2-0.76][92-4-3--0.32][93-4-0-1.74][94-4-4-1.34][95-4-2-0.80][96-4-1-0.88][97-4-4-0.82][98-4-2-1.73][99-4-4-1.09][100-4-2-2.13]
[101-4-4-2.51][102-4-2-1.22][103-4-2-0.84][104-4-4-1.19][105-4-2-1.96][106-4-1-1.10][107-4-1-1.02][108-4-2-1.03][109-4-1-0.96][110-4-2-1.07]
[111-4-0-2.76][112-4-2-0.88][113-4-2-1.56][114-4-2-0.36][115-4-4-0.23][116-4-2-0.93][117-4-2-1.28][119-4-2-1.76][121-4-2-1.25][122-4-4-0.98]
[124-4-2-1.23][125-4-4-2.66][126-4-4-1.74][127-4-2-1.95][128-4-2-0.77][129-4-2-0.13][130-4-2-1.17][131-4-2-1.93][132-4-2-0.85][133-4-4-2.04]
[135-4-2-1.70][136-4-1-0.50][137-4-2-1.54][138-4-1-0.62][139-4-4-0.13][140-4-1-1.24][141-4-3-0.41][142-4-4-1.58][143-4-4-1.73][144-4-4-2.61]
[145-4-1-0.93][148-4-0-1.94][149-4-2-0.16][150-4-2-1.35][151-4-2-1.37][152-4-2-0.99][153-4-2-3.37][154-4-4-0.92][155-4-4-1.30][156-4-3-1.50]
[157-4-2-0.68][158-4-2-1.06][160-4-2-1.50][161-4-2-2.25][162-4-2-1.14][164-4-2-1.38][165-4-4-0.57][167-4-0-0.76][168-4-4-0.42][170-4-0-0.73]
[171-4-2-0.77][172-4-4-1.34][173-4-0-0.82][174-4-0-1.28][175-4-2-0.96][177-4-4-1.00][178-4-2-0.84][179-4-1-0.50][180-4-4-1.91][181-4-3-1.69]
[182-4-2-1.99][183-4-2-0.89][184-4-2-1.93][186-4-2-0.25][187-4-2-3.05][188-4-2-1.71][189-4-2-1.57][190-4-2-0.74][191-4-2-1.11][192-4-2-1.27]
[193-4-2-2.92][194-4-1-1.57][195-4-2-0.90][196-4-2-1.70][197-4-2-1.97][198-4-4-2.29][199-4-2-1.23]
---------------------------
I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 0.669 | Acc: 66.750% | Wgt Acc: 77.686%
	I - Batch: 100 | Loss: 0.685 | Acc: 65.562% | Wgt Acc: 76.772%
	I - Batch: 150 | Loss: 0.703 | Acc: 65.542% | Wgt Acc: 76.337%
	I - Batch: 200 | Loss: 0.714 | Acc: 65.344% | Wgt Acc: 75.691%
	I - Batch: 250 | Loss: 0.719 | Acc: 64.475% | Wgt Acc: 74.995%
I - num batch: 285
I - Train -- Loss: 0.722 | Acc: 64.527% | Wgt Acc: 74.938% | LR: 5.000000e-04 | Dur: 173.72s
I - Confusion Matrix: [row->prediction - col->label]
[[616.  16.  35. 105. 129.]
 [ 19. 611.  59.  31. 149.]
 [ 58. 111. 950.  85. 413.]
 [108.  30.  69. 615. 165.]
 [  5.  12.  11.   4. 144.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.254 | Acc: 49.112% | Wgt Acc: 58.967% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  2.  5. 12. 15.]
 [ 3. 38.  7.  4. 21.]
 [ 4. 29. 53.  8. 78.]
 [25.  5.  9. 61. 23.]
 [ 2.  4.  1.  1. 43.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 0.684 | Acc: 66.000% | Wgt Acc: 76.724%
	I - Batch: 100 | Loss: 0.692 | Acc: 65.938% | Wgt Acc: 76.846%
	I - Batch: 150 | Loss: 0.701 | Acc: 65.708% | Wgt Acc: 76.116%
	I - Batch: 200 | Loss: 0.690 | Acc: 66.219% | Wgt Acc: 76.361%
	I - Batch: 250 | Loss: 0.693 | Acc: 66.075% | Wgt Acc: 76.302%
I - num batch: 285
I - Train -- Loss: 0.700 | Acc: 65.604% | Wgt Acc: 75.949% | LR: 5.000000e-04 | Dur: 174.33s
I - Confusion Matrix: [row->prediction - col->label]
[[619.   9.  29. 119. 144.]
 [ 22. 635.  66.  20. 131.]
 [ 51. 100. 947.  72. 403.]
 [107.  28.  73. 625. 163.]
 [  7.   8.   9.   4. 159.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.375 | Acc: 46.351% | Wgt Acc: 57.214% | Dur: 17.33s
I - Confusion Matrix: [row->prediction - col->label]
[[46.  1.  2.  8. 11.]
 [ 0. 34.  4.  1. 17.]
 [ 7. 31. 54.  9. 88.]
 [34. 10. 13. 67. 30.]
 [ 1.  2.  2.  1. 34.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 0.672 | Acc: 65.250% | Wgt Acc: 76.179%
	I - Batch: 100 | Loss: 0.671 | Acc: 65.625% | Wgt Acc: 76.127%
	I - Batch: 150 | Loss: 0.670 | Acc: 65.750% | Wgt Acc: 76.394%
	I - Batch: 200 | Loss: 0.663 | Acc: 66.188% | Wgt Acc: 76.555%
	I - Batch: 250 | Loss: 0.665 | Acc: 66.225% | Wgt Acc: 76.525%
I - num batch: 285
I - Train -- Loss: 0.673 | Acc: 65.780% | Wgt Acc: 76.121% | LR: 5.000000e-04 | Dur: 178.44s
I - Confusion Matrix: [row->prediction - col->label]
[[629.   9.  28. 107. 150.]
 [ 18. 640.  63.  35. 126.]
 [ 42.  89. 938.  68. 396.]
 [110.  31.  79. 625. 167.]
 [  7.  11.  16.   5. 161.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.344 | Acc: 48.915% | Wgt Acc: 60.650% | Dur: 16.42s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  1.  5.  8. 18.]
 [ 3. 35.  7.  2. 23.]
 [ 7. 28. 56. 10. 80.]
 [20. 10.  6. 66. 25.]
 [ 1.  4.  1.  0. 34.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 0.617 | Acc: 68.000% | Wgt Acc: 78.642%
	I - Batch: 100 | Loss: 0.630 | Acc: 66.875% | Wgt Acc: 77.744%
	I - Batch: 150 | Loss: 0.632 | Acc: 67.417% | Wgt Acc: 77.904%
	I - Batch: 200 | Loss: 0.637 | Acc: 67.125% | Wgt Acc: 77.594%
	I - Batch: 250 | Loss: 0.631 | Acc: 67.175% | Wgt Acc: 77.793%
I - num batch: 285
I - Train -- Loss: 0.626 | Acc: 67.341% | Wgt Acc: 77.832% | LR: 5.000000e-04 | Dur: 176.56s
I - Confusion Matrix: [row->prediction - col->label]
[[624.  10.  26.  94. 153.]
 [ 16. 662.  60.  33. 123.]
 [ 42.  71. 977.  75. 414.]
 [116.  29.  50. 632. 141.]
 [  8.   8.  11.   6. 169.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.359 | Acc: 50.099% | Wgt Acc: 61.469% | Dur: 18.55s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  2.  3.  6. 22.]
 [ 1. 40. 14.  2. 29.]
 [ 3. 16. 41.  2. 37.]
 [24. 16. 16. 75. 53.]
 [ 1.  4.  1.  1. 39.]]

I - Local maximum validation set accuracy:  50.10

I - Validation set results: 
[14-1-2-1.19][50-3-3-0.60][124-2-2-0.75][127-0-0-4.97][443-2-2-1.52][567-0-0-3.19][573-1-1-4.25][615-0-3-3.38][695-1-2-0.57][722-3-3-4.04]
[826-0-3-2.87][878-0-0-2.93][1103-0-0-1.05][1212-3-3-0.48][1368-0-0-5.12][2181-2-3-1.09][2476-2-2--0.16][2721-2-2-1.49][2818-1-3-1.39][2886-2-1-3.77]
[3231-2-2-2.83][3333-2-2-0.63][3482-2-2-2.27][3536-3-3-3.08][3625-1-1-5.18][3909-0-0-3.57][4035-0-0-1.79][4140-0-0-2.22][4214-1-3-0.93][4346-1-3-1.58]
[4581-2-2-1.87][4708-3-3-1.83][4838-3-3-2.66][4845-1-3-1.77][4868-0-0-2.27][4939-0-1-1.07][4984-2-2-2.05][5078-1-3-0.65][5396-0-0-5.41][5479-1-1-4.31]
[5717-0-0-3.12][5843-1-1-0.67][5949-3-3-2.40][5987-2-4-1.37][6014-3-3-1.00][6033-3-3-1.40][6313-0-3-3.42][6421-3-3-2.72][6500-1-1-0.35][6583-3-2-1.23]
[6683-3-3-3.11][6825-2-1-3.39][6998-3-3-1.82][7049-3-3-1.76][7517-1-1-1.76][7521-1-2--0.00][7528-1-3-2.18][7949-1-2-1.04][8135-1-0-1.06][8185-3-0-4.23]
[8269-3-1-3.20][8273-3-3-2.75][8543-3-0-4.34][8666-1-1-2.25][8672-0-0-5.01][8903-1-1-1.46][9001-2-2-1.67][9036-2-2-2.59][9281-3-3-2.05][9300-2-2-3.97]
[9571-0-3-1.78][9617-1-4-0.59][9644-2-1-0.98][9705-2-1-0.94][9801-0-3-1.91][9803-3-3-1.84][9865-3-3-5.95][9896-2-1-1.40][10314-1-1-0.78][10337-3-3-4.79]
[10403-0-0-1.43][10653-2-1-0.06][10704-2-2-1.41][10719-1-1-4.06][10727-1-2-0.10][10836-0-0-7.08][10969-2-3-2.25][11042-0-0-2.21][11088-1-1-2.34][11322-0-0-2.58]
[11398-2-2-1.03][11499-0-0-1.74][11502-3-3-1.55][11512-3-3-1.75][11608-1-1-2.50][11610-0-0-1.63][11692-0-3-2.53][11905-0-0-4.29][11993-1-1-5.55][12002-2-3-0.64]
[12052-0-0-3.53][12201-0-3-3.77][12235-2-2-1.13][12320-1-4-0.37][12377-2-2-0.30][12398-2-3-2.01][12503-1-1-0.51][12617-0-2-0.77][12685-3-3-0.52][12738-2-3-1.10]
[12742-2-2-3.78][12823-0-3-3.99][13110-1-3-1.79][13240-3-3-3.05][13253-1-1-6.08][13273-0-0-5.46][13634-1-1-2.50][13763-2-3-0.89][13905-3-3-1.03][14060-2-3-0.34]
[14065-3-3-2.25][14147-3-3-2.13][14595-2-1-2.19][14687-2-2-1.80][14788-2-2-0.95][14869-1-1-5.20][14872-3-3-0.75][14877-1-1-3.64][14927-0-3-1.66][15066-0-0-4.56]
[15175-1-1-3.69][15178-2-3-1.48][15375-3-3-2.28][15389-3-3-3.11][15568-2-1-1.16][15675-3-3-2.85][15869-1-3-0.61][16207-3-0-0.50][16236-0-2-0.76][16302-3-3-1.58]
[16331-2-2-1.64][16381-0-3-1.82][16488-1-1-5.20][16495-0-0-3.91][16650-0-0-4.35][16719-1-1-1.26][16801-0-0-4.91][16828-0-0-2.58][17137-3-0-1.15][17245-1-3-0.75]
[17278-3-0-0.51][17282-0-0-1.87][17311-2-2-0.67][17336-2-2-1.66][17608-3-3-3.71][17627-0-0-1.26][17877-3-4-0.55][17924-1-3-1.78][17984-3-3-3.07][18211-0-3-3.67]
[18276-3-3-2.19][18287-1-1-3.23][18394-0-0-4.25][18428-0-0-5.04][18442-0-3-3.17][18478-3-3-3.26][18607-0-0-2.81][18616-0-3-1.11][18663-0-0-1.80][18718-0-0-4.94]
[18766-2-2-3.92][18824-2-2-0.32][18890-3-3-2.25][18930-3-3-0.17][18938-3-3-3.06][19817-1-1-2.48][19839-0-0-0.20][19930-3-3-3.93][19944-0-2-0.34][20036-2-2-3.47]
[20101-3-3-3.73][20474-1-1-2.12][20547-3-3-1.66][20929-2-2-4.49][21245-1-2-2.04][21257-3-3-1.71][21293-1-2-1.72][21316-1-1-3.46][21384-1-4-1.00][21448-1-1-3.28]
[21483-0-0-2.88][21487-2-2-1.68][21714-0-3-0.95][21943-3-3-1.42][21947-0-0-2.85][21948-0-0-5.65][21965-2-2-3.90][21998-1-1-2.76][22025-0-3-1.03][22228-3-3-2.39]
[22446-1-1-1.55][22494-3-3-2.07][22757-0-0-3.33][22811-3-3-4.07][22976-3-3-0.59][22985-3-3-4.34][23014-0-3-3.35][23112-1-1-2.98][23144-3-3-4.93][23168-2-3-2.28]
[23219-0-0-2.75][23363-3-3-3.80][23470-0-0-1.00][23486-2-3-0.82][23497-0-3-4.42][23516-0-0-3.85][23690-1-2-0.77][23921-2-1-0.73][23936-1-2-1.42][24040-3-3-0.49]
[24111-1-4-1.11][24182-0-0-5.01][24238-3-3-2.93][24290-2-0-3.45][24345-0-0-3.48][24364-1-2-0.45][24427-3-3-2.55][24477-2-2-3.12][24495-2-1-0.49][24893-2-1-1.61]
[25012-1-3-0.32][25121-2-2-1.57][25165-3-3-1.72][25183-0-0-2.52][25297-3-3-3.92][25398-0-0-3.54][25574-2-2-1.53][25644-1-2-1.95][25718-1-3-0.13][25774-2-3-1.51]
[26032-3-3-2.78][26051-3-3-4.81][26120-0-0-1.76][26321-1-3--0.06][26732-1-1-1.92][26784-3-3-3.63][26827-3-3-3.06][26833-0-3-3.46][26838-2-3-1.01][26860-1-2-0.69]
[26948-0-0-2.53][27049-3-0-3.59][27098-1-0-0.44][27526-0-0-3.46][27639-3-3-3.18][27698-3-3-3.04][27772-0-0-3.18][27890-1-1-2.66][28040-0-0-2.16][28503-2-2-4.01]
[28577-1-1-3.62][28959-0-0-5.86][29198-3-3-3.13][29777-0-0-6.04][29877-2-2-0.66][30035-1-1-4.06][30098-0-3-1.85][30326-1-1-3.80][30572-2-2-2.51][30716-0-4-0.30]
[30806-2-3-1.37][30906-1-1-4.17][31007-0-0-2.23][31181-3-3-2.35][31238-0-3-2.22][31347-0-0-2.69][31422-2-2-0.53][31429-3-3-1.26][31431-0-3-0.90][31432-1-1-4.09]
[31477-0-0-4.21][31524-1-2-1.15][31597-1-2-1.58][31619-1-3-2.02][31701-0-0-3.08][31755-0-0-2.87][31854-3-3-3.32][32074-1-1-1.82][32078-3-3-4.00][32111-1-1-4.10]
[32127-1-2-1.89][32140-3-3-4.53][32263-2-0-1.57][32365-0-0-2.11][32411-2-0-4.86][32429-3-3-3.20][32473-3-2-0.73][32574-3-3-3.29][32584-0-0-1.00][32622-0-3-0.50]
[32858-3-3-2.94][32969-3-3-3.66][33016-2-2-2.25][33031-1-3-2.73][33035-2-2-3.28][33133-2-2-0.90][33173-2-1-1.15][33175-3-1-0.42][33306-3-3-2.68][33309-2-3-1.60]
[33474-0-3-1.04][33478-2-3-1.15][33618-1-1-4.53][33712-0-3-2.90][33782-2-2-1.10][33914-3-3-2.39][34076-3-3-2.05][34112-2-2-3.00][34138-2-3-1.82][34239-1-3-0.33]
[34364-2-2-3.99][34617-1-2-0.72][34751-3-3-3.04][34783-2-1-1.40][35015-3-3-2.04][35018-1-1-2.23][35288-2-1-0.84][0-4-2-0.87][1-4-0-1.59][2-4-0-0.28]
[3-4-0-0.43][4-4-3-0.55][5-4-3-1.74][6-4-4-1.69][7-4-3-0.76][8-4-3-0.52][9-4-1-0.30][10-4-4-1.84][11-4-2-2.53][12-4-2-0.40]
[14-4-3-1.44][15-4-3-3.32][16-4-4-0.17][17-4-1-1.12][18-4-4-2.16][19-4-3-2.43][20-4-3-0.99][21-4-2-0.45][22-4-1-1.20][23-4-3-0.84]
[24-4-4-1.14][25-4-3-1.43][26-4-3-1.49][27-4-2-0.38][28-4-4-0.61][29-4-1-1.55][30-4-0-1.45][31-4-2-0.02][32-4-1-1.46][33-4-3-1.24]
[34-4-3--0.04][35-4-0-1.79][37-4-3-1.71][39-4-0-3.66][40-4-3-0.13][41-4-3-1.02][42-4-3-1.09][43-4-2-1.34][45-4-1-1.73][46-4-4-1.74]
[47-4-2-1.88][48-4-2-1.33][51-4-2-0.55][52-4-1-0.16][53-4-2--0.06][54-4-3-1.59][55-4-3-1.90][56-4-1-0.69][57-4-3-2.43][58-4-2-3.01]
[59-4-0-3.03][60-4-3-0.47][61-4-4-1.45][62-4-3-1.50][63-4-2-2.73][64-4-0-0.69][65-4-4-0.99][66-4-4-0.66][67-4-3-1.16][68-4-1-1.52]
[69-4-0-1.69][70-4-4-0.12][72-4-1-2.20][73-4-1-0.77][74-4-2-1.93][75-4-0-2.46][77-4-4-1.73][78-4-2-0.58][79-4-2-0.39][80-4-1-1.04]
[81-4-1-1.90][82-4-1-0.88][83-4-1-1.27][84-4-0-1.63][85-4-4-1.19][86-4-1-0.81][87-4-4-1.00][88-4-4-0.01][89-4-2-0.86][90-4-0-0.40]
[91-4-3-1.01][92-4-3-0.29][93-4-0-2.33][94-4-4-1.07][95-4-3-0.16][96-4-1-1.57][97-4-4-0.44][98-4-2-0.85][99-4-4-0.75][100-4-1-1.38]
[101-4-4-2.00][102-4-1-0.17][103-4-3-1.56][104-4-4-0.66][105-4-4-1.84][106-4-1-0.29][107-4-1-2.31][108-4-3-0.59][109-4-1-1.78][110-4-1--0.26]
[111-4-0-3.36][112-4-0-0.86][113-4-2-0.86][114-4-3-1.18][115-4-3-0.38][116-4-3-0.18][117-4-1-0.42][119-4-2-2.20][121-4-4-0.45][122-4-3-0.68]
[124-4-3-0.75][125-4-4-1.43][126-4-4-1.13][127-4-1-0.48][128-4-3-0.44][129-4-3-0.65][130-4-2--0.16][131-4-3-1.23][132-4-3--0.12][133-4-0-3.97]
[135-4-3-2.24][136-4-0-0.14][137-4-4--0.13][138-4-1-0.44][139-4-3-1.48][140-4-1-0.07][141-4-3-1.61][142-4-4-2.67][143-4-2-1.27][144-4-4-2.03]
[145-4-2-0.73][148-4-0-2.26][149-4-3-1.33][150-4-4-2.37][151-4-2-1.60][152-4-3-1.13][153-4-2-0.77][154-4-2-0.50][155-4-4-0.91][156-4-3-2.69]
[157-4-0-0.59][158-4-3-0.96][160-4-1-0.13][161-4-2-1.24][162-4-4--0.33][164-4-3-1.25][165-4-2--0.11][167-4-3-2.47][168-4-4-0.55][170-4-3-0.94]
[171-4-4-0.91][172-4-4-1.15][173-4-4-0.26][174-4-0-1.21][175-4-2-0.55][177-4-0-1.71][178-4-2-0.45][179-4-2-0.22][180-4-4-1.17][181-4-3-0.94]
[182-4-2-1.72][183-4-4-0.31][184-4-2-1.90][186-4-0-1.36][187-4-2-1.57][188-4-2-1.36][189-4-2-0.54][190-4-3-0.36][191-4-4-0.31][192-4-3-0.57]
[193-4-1-0.94][194-4-3-2.47][195-4-3-0.12][196-4-2-0.71][197-4-4-0.95][198-4-4-1.66][199-4-3-1.26]
---------------------------
I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 0.605 | Acc: 68.875% | Wgt Acc: 79.702%
	I - Batch: 100 | Loss: 0.636 | Acc: 67.750% | Wgt Acc: 78.444%
	I - Batch: 150 | Loss: 0.640 | Acc: 67.583% | Wgt Acc: 78.527%
	I - Batch: 200 | Loss: 0.631 | Acc: 68.438% | Wgt Acc: 78.924%
	I - Batch: 250 | Loss: 0.630 | Acc: 68.500% | Wgt Acc: 78.953%
I - num batch: 285
I - Train -- Loss: 0.629 | Acc: 68.527% | Wgt Acc: 79.046% | LR: 5.000000e-04 | Dur: 176.80s
I - Confusion Matrix: [row->prediction - col->label]
[[629.  15.  28.  91. 145.]
 [ 22. 656.  47.  27. 138.]
 [ 53.  76. 989.  53. 388.]
 [ 93.  26.  50. 663. 148.]
 [  9.   7.  10.   6. 181.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.302 | Acc: 51.677% | Wgt Acc: 62.484% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  0.  1. 11. 18.]
 [ 4. 48. 12.  4. 32.]
 [ 4. 15. 44.  3. 55.]
 [21.  9. 17. 68. 31.]
 [ 1.  6.  1.  0. 44.]]

I - Local maximum validation set accuracy:  51.68

I - Validation set results: 
[14-1-1-1.15][50-3-1-1.26][124-2-2-1.35][127-0-0-3.05][443-2-2-1.01][567-0-0-1.48][573-1-1-1.04][615-0-0-1.85][695-1-2-1.27][722-3-3-1.80]
[826-0-0-4.42][878-0-0-2.22][1103-0-0-1.92][1212-3-2-0.29][1368-0-0-3.22][2181-2-3-0.96][2476-2-2-0.55][2721-2-2-1.88][2818-1-3-1.55][2886-2-1-1.09]
[3231-2-2-1.40][3333-2-1-2.85][3482-2-2-3.12][3536-3-3-1.87][3625-1-1-3.37][3909-0-0-1.54][4035-0-0-1.29][4140-0-0-0.96][4214-1-1-1.15][4346-1-3-0.41]
[4581-2-2-2.17][4708-3-3-2.13][4838-3-3-1.79][4845-1-3-0.60][4868-0-0-2.59][4939-0-1-0.76][4984-2-3-0.79][5078-1-2-0.48][5396-0-0-4.72][5479-1-1-3.23]
[5717-0-0-1.30][5843-1-1-0.80][5949-3-0-1.18][5987-2-4-1.44][6014-3-3-1.61][6033-3-0--0.46][6313-0-3-3.05][6421-3-3-1.13][6500-1-2-1.02][6583-3-3-0.63]
[6683-3-3-1.71][6825-2-1-2.35][6998-3-3-1.30][7049-3-3-2.16][7517-1-1-1.11][7521-1-1-2.40][7528-1-3-1.69][7949-1-1-1.78][8135-1-2-0.39][8185-3-0-1.97]
[8269-3-1-0.41][8273-3-3-2.26][8543-3-0-4.03][8666-1-1-2.19][8672-0-0-3.58][8903-1-2-0.54][9001-2-2-1.08][9036-2-2-2.27][9281-3-3-2.16][9300-2-2-4.04]
[9571-0-3-1.34][9617-1-4-0.91][9644-2-2-1.59][9705-2-1-0.17][9801-0-3-2.13][9803-3-3-1.96][9865-3-3-5.03][9896-2-2-3.03][10314-1-4-0.75][10337-3-3-4.01]
[10403-0-0-0.08][10653-2-1-0.71][10704-2-2-1.77][10719-1-1-1.56][10727-1-1-0.52][10836-0-0-4.66][10969-2-3-1.83][11042-0-0-0.54][11088-1-2-1.77][11322-0-0-2.36]
[11398-2-2-0.64][11499-0-0-1.37][11502-3-3-0.51][11512-3-3-2.12][11608-1-1-2.78][11610-0-0-1.65][11692-0-3-1.06][11905-0-0-1.90][11993-1-1-2.58][12002-2-3-2.35]
[12052-0-0-1.79][12201-0-3-3.47][12235-2-2-2.05][12320-1-4-0.37][12377-2-2-0.92][12398-2-3-1.39][12503-1-2-2.76][12617-0-3-0.88][12685-3-3-0.89][12738-2-3-0.20]
[12742-2-2-3.67][12823-0-3-2.65][13110-1-1-2.46][13240-3-3-3.00][13253-1-1-0.66][13273-0-0-3.98][13634-1-1-2.68][13763-2-3-0.64][13905-3-0--0.36][14060-2-1-1.00]
[14065-3-0-0.92][14147-3-3-2.06][14595-2-2-1.16][14687-2-2-2.84][14788-2-2-1.96][14869-1-1-4.14][14872-3-1-1.59][14877-1-1-1.63][14927-0-3-1.67][15066-0-0-4.49]
[15175-1-1-4.15][15178-2-3-0.95][15375-3-3-0.62][15389-3-3-4.10][15568-2-1-1.85][15675-3-3-3.83][15869-1-1-0.65][16207-3-3-0.40][16236-0-0-0.95][16302-3-0-1.10]
[16331-2-2-2.74][16381-0-0-1.94][16488-1-1-3.20][16495-0-0-0.04][16650-0-0-3.45][16719-1-2-1.79][16801-0-0-3.21][16828-0-0-1.75][17137-3-0-0.83][17245-1-2-0.60]
[17278-3-3--0.10][17282-0-0-0.21][17311-2-2-1.45][17336-2-3-0.87][17608-3-3-3.21][17627-0-3-0.43][17877-3-0-1.48][17924-1-3-0.34][17984-3-3-2.92][18211-0-3-2.76]
[18276-3-3-0.59][18287-1-1-2.57][18394-0-0-3.23][18428-0-3-1.66][18442-0-3-2.68][18478-3-3-2.59][18607-0-0-1.82][18616-0-1-0.13][18663-0-0-0.61][18718-0-0-3.86]
[18766-2-2-2.61][18824-2-2-0.69][18890-3-3-2.94][18930-3-3-0.47][18938-3-3-1.95][19817-1-1-3.36][19839-0-2-0.00][19930-3-3-3.09][19944-0-1-0.75][20036-2-2-4.91]
[20101-3-3-2.95][20474-1-1-2.35][20547-3-3-0.97][20929-2-2-1.96][21245-1-1-2.67][21257-3-3-0.34][21293-1-1-2.09][21316-1-1-4.81][21384-1-4-1.89][21448-1-1-2.68]
[21483-0-0-0.92][21487-2-2-2.31][21714-0-2-0.53][21943-3-3-1.11][21947-0-0-2.56][21948-0-0-3.10][21965-2-1-3.26][21998-1-1-5.05][22025-0-2-0.78][22228-3-3-2.48]
[22446-1-1-2.79][22494-3-3-1.34][22757-0-3-2.84][22811-3-3-3.64][22976-3-2-1.85][22985-3-3-4.15][23014-0-3-3.15][23112-1-1-1.48][23144-3-3-3.82][23168-2-3-1.10]
[23219-0-2-2.19][23363-3-3-4.88][23470-0-0-0.39][23486-2-2-0.88][23497-0-3-3.70][23516-0-0-2.45][23690-1-4-1.23][23921-2-2-3.07][23936-1-3-0.38][24040-3-0-0.22]
[24111-1-4-1.90][24182-0-0-4.38][24238-3-3-2.67][24290-2-0-2.82][24345-0-0-3.45][24364-1-2-1.20][24427-3-3-2.22][24477-2-2-1.92][24495-2-1-0.72][24893-2-2-1.72]
[25012-1-3-0.71][25121-2-2-1.86][25165-3-3-2.57][25183-0-0-0.97][25297-3-3-3.66][25398-0-0-1.98][25574-2-2-0.80][25644-1-1-2.21][25718-1-1-0.05][25774-2-3-0.51]
[26032-3-3-1.92][26051-3-3-4.06][26120-0-0-0.03][26321-1-1-4.88][26732-1-1-2.14][26784-3-3-3.51][26827-3-3-1.68][26833-0-3-3.27][26838-2-3-1.03][26860-1-2-0.31]
[26948-0-0-1.57][27049-3-0-1.01][27098-1-1-0.31][27526-0-0-3.08][27639-3-3-1.96][27698-3-3-2.86][27772-0-0-0.86][27890-1-1-6.42][28040-0-0--0.01][28503-2-2-4.11]
[28577-1-1-3.91][28959-0-0-4.36][29198-3-3-3.41][29777-0-0-4.69][29877-2-2-1.42][30035-1-1-2.87][30098-0-0-1.31][30326-1-1-2.97][30572-2-2-2.58][30716-0-4-1.12]
[30806-2-3-0.84][30906-1-1-3.16][31007-0-3-1.38][31181-3-3-1.28][31238-0-3-1.87][31347-0-0-1.97][31422-2-1-1.63][31429-3-3-1.35][31431-0-3--0.19][31432-1-1-3.36]
[31477-0-0-2.95][31524-1-3-0.64][31597-1-2-1.00][31619-1-2-0.93][31701-0-0-1.87][31755-0-0-2.20][31854-3-3-2.22][32074-1-1-0.05][32078-3-3-2.23][32111-1-1-1.89]
[32127-1-2-2.44][32140-3-3-4.06][32263-2-2-0.22][32365-0-0-0.84][32411-2-3-3.66][32429-3-3-2.51][32473-3-2-1.60][32574-3-3-3.74][32584-0-0-0.49][32622-0-1--0.15]
[32858-3-3-2.01][32969-3-3-2.61][33016-2-2-2.68][33031-1-3-1.35][33035-2-2-2.54][33133-2-2-0.50][33173-2-2-1.30][33175-3-1-0.23][33306-3-3-1.96][33309-2-3-1.78]
[33474-0-3-0.19][33478-2-3-0.12][33618-1-1-4.66][33712-0-3-1.77][33782-2-2-1.11][33914-3-3-0.79][34076-3-3-0.84][34112-2-2-3.70][34138-2-3-0.74][34239-1-1-0.94]
[34364-2-2-3.32][34617-1-2-1.25][34751-3-3-3.08][34783-2-1-1.20][35015-3-3-1.48][35018-1-1-1.26][35288-2-1-0.76][0-4-2-2.87][1-4-0-1.17][2-4-0-0.92]
[3-4-2-1.28][4-4-1-1.17][5-4-3-1.02][6-4-4-2.22][7-4-2-0.11][8-4-2-0.95][9-4-2--0.04][10-4-4-1.54][11-4-2-4.20][12-4-1-0.75]
[14-4-3-0.57][15-4-3-2.97][16-4-4-0.59][17-4-2-0.74][18-4-4-1.24][19-4-0-1.25][20-4-3-1.31][21-4-2-1.86][22-4-4-1.27][23-4-3-0.56]
[24-4-4-3.18][25-4-3-1.91][26-4-2-0.15][27-4-0-0.90][28-4-4-2.36][29-4-1-1.52][30-4-3-0.77][31-4-1-1.39][32-4-4-0.60][33-4-2-2.67]
[34-4-2-0.06][35-4-1-0.68][37-4-3-0.29][39-4-0-1.90][40-4-2-0.50][41-4-1-1.05][42-4-3-1.06][43-4-1-3.58][45-4-2-0.93][46-4-4-1.56]
[47-4-2-1.74][48-4-2-1.86][51-4-4-1.66][52-4-1-0.12][53-4-2-0.85][54-4-3-1.07][55-4-3-1.50][56-4-1-1.72][57-4-3-2.18][58-4-1-0.64]
[59-4-3-0.61][60-4-3-0.02][61-4-4-1.63][62-4-2-3.44][63-4-2-2.56][64-4-1-0.98][65-4-4-1.39][66-4-4-1.17][67-4-3-1.22][68-4-1-1.97]
[69-4-0-0.34][70-4-2-1.64][72-4-4-1.22][73-4-1-1.34][74-4-2-0.73][75-4-2-0.61][77-4-4-2.51][78-4-3-0.14][79-4-4-0.51][80-4-4-1.46]
[81-4-4-1.25][82-4-1-1.55][83-4-1-1.40][84-4-0-0.51][85-4-2-0.78][86-4-2-0.20][87-4-4-0.72][88-4-4-0.51][89-4-2--0.28][90-4-3--0.08]
[91-4-3-0.45][92-4-3-0.72][93-4-0-1.06][94-4-4-1.78][95-4-2-0.58][96-4-1--0.18][97-4-2-1.49][98-4-2-1.82][99-4-4-0.92][100-4-2-1.13]
[101-4-4-2.62][102-4-1-1.11][103-4-3-0.49][104-4-4-1.24][105-4-4-2.53][106-4-4-1.60][107-4-1-1.09][108-4-2-0.43][109-4-1-1.43][110-4-1-2.13]
[111-4-0-3.24][112-4-2-0.81][113-4-2-1.09][114-4-3-1.21][115-4-4-0.49][116-4-2-0.74][117-4-2-1.72][119-4-2-2.19][121-4-1-0.96][122-4-3-0.85]
[124-4-3-0.48][125-4-4-1.79][126-4-4-2.46][127-4-2-0.94][128-4-3-0.95][129-4-1--0.19][130-4-2-0.64][131-4-2-1.20][132-4-0-0.42][133-4-0-4.02]
[135-4-3-1.35][136-4-3-0.07][137-4-2-2.54][138-4-1-1.87][139-4-3-0.67][140-4-1-3.04][141-4-3-1.17][142-4-4-1.75][143-4-0-0.28][144-4-4-2.97]
[145-4-2-1.39][148-4-0-1.25][149-4-2-0.44][150-4-2-1.56][151-4-4-1.25][152-4-2-0.90][153-4-2-1.38][154-4-4-2.31][155-4-4-1.68][156-4-0-0.72]
[157-4-0-0.48][158-4-2-0.62][160-4-1-0.82][161-4-1-0.89][162-4-2-0.59][164-4-2-0.35][165-4-1-0.97][167-4-0-0.22][168-4-4-0.28][170-4-0-0.73]
[171-4-4-0.30][172-4-4-1.56][173-4-4-0.40][174-4-3-1.40][175-4-4-0.25][177-4-3-0.31][178-4-2-0.30][179-4-1-0.51][180-4-4-2.10][181-4-3-0.48]
[182-4-2-1.07][183-4-4-0.85][184-4-2-1.09][186-4-2-0.76][187-4-2-2.71][188-4-1-0.16][189-4-2-0.62][190-4-1-0.81][191-4-2-1.62][192-4-1-0.64]
[193-4-2-1.04][194-4-1-0.82][195-4-0--0.17][196-4-2-2.70][197-4-4-1.06][198-4-4-2.38][199-4-4-0.67]
---------------------------
I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 0.587 | Acc: 71.125% | Wgt Acc: 80.791%
	I - Batch: 100 | Loss: 0.611 | Acc: 69.312% | Wgt Acc: 78.968%
	I - Batch: 150 | Loss: 0.592 | Acc: 69.125% | Wgt Acc: 79.685%
	I - Batch: 200 | Loss: 0.595 | Acc: 69.625% | Wgt Acc: 79.641%
	I - Batch: 250 | Loss: 0.597 | Acc: 69.425% | Wgt Acc: 79.617%
I - num batch: 285
I - Train -- Loss: 0.605 | Acc: 68.901% | Wgt Acc: 79.216% | LR: 5.000000e-04 | Dur: 171.55s
I - Confusion Matrix: [row->prediction - col->label]
[[639.  14.  36.  89. 122.]
 [ 19. 660.  53.  28. 143.]
 [ 46.  76. 974.  51. 378.]
 [ 94.  20.  54. 667. 162.]
 [  8.  10.   7.   5. 195.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.188 | Acc: 56.213% | Wgt Acc: 58.990% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  1.  6. 13. 13.]
 [ 4. 34.  7.  4. 11.]
 [ 5. 26. 51.  9. 55.]
 [13.  5.  7. 55. 13.]
 [ 9. 12.  4.  5. 88.]]

I - Local maximum validation set accuracy:  56.21

I - Validation set results: 
[14-1-2-3.71][50-3-4-1.86][124-2-2-2.68][127-0-0-3.77][443-2-2-3.57][567-0-4-1.45][573-1-1-3.18][615-0-3-1.73][695-1-2-1.95][722-3-3-2.84]
[826-0-0-4.58][878-0-0-3.02][1103-0-0-1.51][1212-3-2-0.70][1368-0-0-3.27][2181-2-0-0.39][2476-2-2-2.26][2721-2-2-3.22][2818-1-3-0.93][2886-2-2-2.24]
[3231-2-2-4.14][3333-2-1-1.73][3482-2-2-3.23][3536-3-3--0.39][3625-1-1-3.41][3909-0-0-2.22][4035-0-0-1.41][4140-0-0-1.03][4214-1-1-1.09][4346-1-3-1.36]
[4581-2-2-2.22][4708-3-3-0.86][4838-3-4-0.57][4845-1-2-0.37][4868-0-0-2.15][4939-0-2-1.62][4984-2-2-2.09][5078-1-4-0.74][5396-0-0-5.37][5479-1-1-4.15]
[5717-0-0-1.73][5843-1-4-1.79][5949-3-3-1.24][5987-2-4-2.85][6014-3-3-1.30][6033-3-3-0.12][6313-0-3-2.67][6421-3-3-1.31][6500-1-2-1.03][6583-3-2-1.22]
[6683-3-3-2.74][6825-2-1-2.19][6998-3-2-0.94][7049-3-3-1.18][7517-1-2-3.68][7521-1-1-0.35][7528-1-3-2.47][7949-1-2-3.61][8135-1-0-0.38][8185-3-0-4.47]
[8269-3-1-1.25][8273-3-3-2.63][8543-3-0-3.62][8666-1-1-4.27][8672-0-0-2.54][8903-1-2-1.42][9001-2-0-0.26][9036-2-2-5.40][9281-3-3-1.12][9300-2-2-5.41]
[9571-0-4-0.55][9617-1-1-1.72][9644-2-2-1.63][9705-2-0-0.47][9801-0-0-2.39][9803-3-3-1.94][9865-3-3-3.96][9896-2-2-3.23][10314-1-1-0.11][10337-3-3-3.58]
[10403-0-4-1.31][10653-2-2-1.10][10704-2-2-3.09][10719-1-1-3.83][10727-1-4-0.37][10836-0-0-6.60][10969-2-3-1.05][11042-0-4-0.59][11088-1-1-3.21][11322-0-0-1.73]
[11398-2-2-2.15][11499-0-0-2.88][11502-3-0-0.10][11512-3-3-1.05][11608-1-1-4.26][11610-0-0-1.53][11692-0-0-1.60][11905-0-0-2.16][11993-1-1-3.41][12002-2-0-2.39]
[12052-0-0-3.09][12201-0-3-2.97][12235-2-2-3.32][12320-1-4-1.86][12377-2-4-2.56][12398-2-1-0.62][12503-1-2-1.21][12617-0-1-0.46][12685-3-1--0.37][12738-2-3-1.44]
[12742-2-2-4.85][12823-0-0-3.54][13110-1-1-2.99][13240-3-3-2.62][13253-1-1-1.74][13273-0-0-4.82][13634-1-1-2.58][13763-2-2-1.70][13905-3-0--0.12][14060-2-1-1.26]
[14065-3-3-0.68][14147-3-3-1.93][14595-2-2-2.60][14687-2-2-3.56][14788-2-2-3.28][14869-1-1-6.43][14872-3-4-0.02][14877-1-1-2.06][14927-0-3-1.84][15066-0-0-2.79]
[15175-1-1-3.70][15178-2-3-0.77][15375-3-2--0.15][15389-3-3-3.12][15568-2-1-1.97][15675-3-3-2.58][15869-1-2-1.59][16207-3-2--0.35][16236-0-3-0.27][16302-3-0-2.18]
[16331-2-2-4.06][16381-0-4-0.95][16488-1-1-2.02][16495-0-0-2.55][16650-0-0-3.39][16719-1-4-1.81][16801-0-0-4.15][16828-0-0-2.39][17137-3-3-0.91][17245-1-4-0.92]
[17278-3-1-0.29][17282-0-0-1.38][17311-2-2-2.20][17336-2-2-2.25][17608-3-3-4.26][17627-0-0--0.22][17877-3-0-0.90][17924-1-3-0.62][17984-3-3-3.05][18211-0-3-2.17]
[18276-3-0-0.57][18287-1-1-0.77][18394-0-0-4.16][18428-0-0-3.76][18442-0-3-3.55][18478-3-3-3.76][18607-0-0-2.42][18616-0-4-0.71][18663-0-0-1.02][18718-0-0-3.52]
[18766-2-2-3.66][18824-2-2-1.76][18890-3-3-1.41][18930-3-4-1.15][18938-3-3-2.76][19817-1-2-3.10][19839-0-2-0.48][19930-3-3-1.49][19944-0-0-0.76][20036-2-2-6.88]
[20101-3-3-2.57][20474-1-1-3.98][20547-3-3-0.96][20929-2-2-4.88][21245-1-2-2.60][21257-3-2-0.30][21293-1-2-5.61][21316-1-1-5.30][21384-1-4-2.06][21448-1-1-2.46]
[21483-0-0-3.77][21487-2-2-2.96][21714-0-2-0.69][21943-3-2-1.27][21947-0-0-1.65][21948-0-0-4.72][21965-2-2-4.61][21998-1-1-3.63][22025-0-2-2.07][22228-3-3-1.77]
[22446-1-1-3.46][22494-3-3-2.03][22757-0-3-3.29][22811-3-3-5.06][22976-3-2-2.25][22985-3-3-3.90][23014-0-0-3.44][23112-1-2-1.97][23144-3-3-3.97][23168-2-4-0.35]
[23219-0-2-1.91][23363-3-3-3.49][23470-0-0--0.42][23486-2-2-0.51][23497-0-3-4.93][23516-0-0-3.01][23690-1-4-1.88][23921-2-2-3.25][23936-1-2-0.65][24040-3-0-0.72]
[24111-1-4-2.40][24182-0-0-4.27][24238-3-3-1.84][24290-2-0-1.37][24345-0-0-2.06][24364-1-2-1.58][24427-3-0-1.25][24477-2-2-3.29][24495-2-2-1.15][24893-2-2-3.33]
[25012-1-2-1.69][25121-2-2-2.33][25165-3-3-0.82][25183-0-4-1.68][25297-3-3-3.01][25398-0-0-2.68][25574-2-2-1.46][25644-1-2-3.46][25718-1-4-0.83][25774-2-3-0.62]
[26032-3-3-2.54][26051-3-3-4.16][26120-0-0-1.21][26321-1-1-5.04][26732-1-1-1.34][26784-3-3-3.92][26827-3-3-1.45][26833-0-3-2.58][26838-2-3-0.29][26860-1-2--0.30]
[26948-0-0-0.65][27049-3-0-3.26][27098-1-1-0.30][27526-0-0-1.62][27639-3-1-0.73][27698-3-3-4.38][27772-0-0-2.09][27890-1-1-3.34][28040-0-4-1.43][28503-2-2-5.60]
[28577-1-1-1.57][28959-0-0-4.66][29198-3-3-1.79][29777-0-0-5.95][29877-2-2-2.73][30035-1-2-3.84][30098-0-0-0.18][30326-1-1-2.13][30572-2-2-1.49][30716-0-4-2.97]
[30806-2-2-1.60][30906-1-2-2.56][31007-0-3-1.67][31181-3-0-2.04][31238-0-3-1.96][31347-0-0-4.38][31422-2-2-1.97][31429-3-3-1.27][31431-0-0-1.58][31432-1-1-4.48]
[31477-0-0-3.13][31524-1-4-0.24][31597-1-2-1.14][31619-1-2-1.29][31701-0-0-2.86][31755-0-0-3.31][31854-3-3-2.05][32074-1-2-1.99][32078-3-3-4.04][32111-1-4-1.05]
[32127-1-2-4.39][32140-3-3-3.39][32263-2-2-1.57][32365-0-0-1.59][32411-2-0-3.34][32429-3-0-3.19][32473-3-2-1.72][32574-3-3-3.82][32584-0-1-0.52][32622-0-1-1.45]
[32858-3-0-2.84][32969-3-3-2.35][33016-2-2-2.45][33031-1-3-0.59][33035-2-2-4.68][33133-2-2-1.98][33173-2-2-1.11][33175-3-4-0.52][33306-3-3-2.14][33309-2-3-0.93]
[33474-0-1-0.33][33478-2-1-0.08][33618-1-1-4.96][33712-0-3-0.94][33782-2-1-1.47][33914-3-3-2.23][34076-3-3-2.35][34112-2-2-3.51][34138-2-3-2.80][34239-1-1-2.11]
[34364-2-2-4.66][34617-1-2-3.49][34751-3-3-2.67][34783-2-4-2.68][35015-3-3-1.40][35018-1-2-2.57][35288-2-2-0.28][0-4-2-3.19][1-4-4-1.60][2-4-4-1.34]
[3-4-4-1.89][4-4-4-0.65][5-4-1-0.82][6-4-4-2.46][7-4-4-1.78][8-4-0-0.21][9-4-4-0.42][10-4-4-3.59][11-4-2-2.95][12-4-1-2.40]
[14-4-3-1.07][15-4-0-1.96][16-4-4-1.64][17-4-2-1.09][18-4-4-2.80][19-4-3-2.15][20-4-0-0.14][21-4-4-1.15][22-4-4-1.95][23-4-4-1.22]
[24-4-4-2.51][25-4-3-0.64][26-4-2-1.41][27-4-4-1.03][28-4-4-2.02][29-4-2-1.56][30-4-4-1.01][31-4-4-2.26][32-4-4-2.16][33-4-2-2.40]
[34-4-4--0.04][35-4-1-0.77][37-4-2-1.86][39-4-0-3.33][40-4-2-0.29][41-4-4-0.77][42-4-4-0.12][43-4-2-0.70][45-4-2-2.49][46-4-4-2.93]
[47-4-4-3.24][48-4-2-2.68][51-4-4-3.29][52-4-4-0.40][53-4-4-1.61][54-4-3-0.40][55-4-3-2.60][56-4-1-1.01][57-4-3-1.85][58-4-1-1.66]
[59-4-4-1.42][60-4-4-0.34][61-4-4-2.22][62-4-3-0.17][63-4-2-2.63][64-4-2-1.51][65-4-4-3.03][66-4-4-2.22][67-4-3-0.11][68-4-1-1.38]
[69-4-4-0.65][70-4-4-1.52][72-4-2-1.48][73-4-4-1.68][74-4-2-1.81][75-4-4-1.60][77-4-4-2.60][78-4-2-0.79][79-4-2-2.89][80-4-4-2.59]
[81-4-1-1.96][82-4-1-1.63][83-4-4-1.12][84-4-4-1.82][85-4-4-3.51][86-4-2-0.89][87-4-4-2.59][88-4-4-2.25][89-4-2-1.19][90-4-4-1.65]
[91-4-4-1.08][92-4-0--0.01][93-4-4-1.05][94-4-4-3.04][95-4-4-0.75][96-4-1-1.71][97-4-4-1.95][98-4-2-1.68][99-4-4-2.09][100-4-2-1.78]
[101-4-4-4.29][102-4-4-1.64][103-4-0-0.89][104-4-2-1.98][105-4-2-1.68][106-4-4-2.93][107-4-4-0.70][108-4-2-1.82][109-4-4-1.24][110-4-4-1.45]
[111-4-0-2.61][112-4-2-2.71][113-4-2-0.42][114-4-3-0.12][115-4-4-1.01][116-4-2-0.23][117-4-4-1.60][119-4-2-1.91][121-4-4-1.86][122-4-4-1.66]
[124-4-2-0.86][125-4-4-2.25][126-4-4-1.33][127-4-2-3.71][128-4-3-0.14][129-4-4-0.34][130-4-2-1.69][131-4-2-1.14][132-4-0-0.93][133-4-4-2.20]
[135-4-3-0.34][136-4-4-0.27][137-4-2-2.93][138-4-1--0.02][139-4-4-1.87][140-4-2-1.61][141-4-3-1.48][142-4-4-3.67][143-4-4-1.48][144-4-4-3.68]
[145-4-2-3.72][148-4-0-2.80][149-4-4-0.11][150-4-4-1.92][151-4-4-1.97][152-4-4-1.77][153-4-2-2.07][154-4-4-1.60][155-4-4-3.00][156-4-0-0.41]
[157-4-2-0.96][158-4-4-0.87][160-4-2-1.02][161-4-2-1.97][162-4-2-1.14][164-4-2-1.55][165-4-2-1.16][167-4-4-1.52][168-4-4-1.49][170-4-0-1.69]
[171-4-4-0.42][172-4-4-2.29][173-4-4-2.99][174-4-0-2.00][175-4-2-1.62][177-4-4-1.93][178-4-4-2.11][179-4-4-0.74][180-4-4-3.00][181-4-3-1.84]
[182-4-1-2.09][183-4-4-1.95][184-4-2-1.67][186-4-2-1.40][187-4-2-2.73][188-4-2-1.95][189-4-2-0.22][190-4-2-2.30][191-4-2-2.52][192-4-0-0.12]
[193-4-2-2.34][194-4-2-0.17][195-4-2-1.15][196-4-2-4.18][197-4-4-1.35][198-4-4-3.33][199-4-2-0.56]
---------------------------
I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 0.550 | Acc: 71.375% | Wgt Acc: 81.119%
	I - Batch: 100 | Loss: 0.552 | Acc: 71.250% | Wgt Acc: 81.516%
	I - Batch: 150 | Loss: 0.555 | Acc: 71.583% | Wgt Acc: 81.288%
	I - Batch: 200 | Loss: 0.561 | Acc: 71.031% | Wgt Acc: 81.025%
	I - Batch: 250 | Loss: 0.563 | Acc: 70.850% | Wgt Acc: 80.855%
I - num batch: 285
I - Train -- Loss: 0.567 | Acc: 70.747% | Wgt Acc: 80.662% | LR: 5.000000e-04 | Dur: 177.40s
I - Confusion Matrix: [row->prediction - col->label]
[[651.  11.  33.  95. 154.]
 [ 19. 679.  39.  24. 121.]
 [ 39.  66. 998.  54. 327.]
 [ 86.  17.  48. 660. 167.]
 [ 11.   7.   6.   7. 231.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.228 | Acc: 54.241% | Wgt Acc: 63.153% | Dur: 16.77s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  2.  6. 17. 21.]
 [ 2. 41.  4.  4. 17.]
 [ 8. 25. 55.  3. 66.]
 [12.  7.  9. 59. 20.]
 [ 2.  3.  1.  3. 56.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.509 | Acc: 70.625% | Wgt Acc: 82.939%
	I - Batch: 100 | Loss: 0.511 | Acc: 71.438% | Wgt Acc: 83.437%
	I - Batch: 150 | Loss: 0.493 | Acc: 72.875% | Wgt Acc: 84.136%
	I - Batch: 200 | Loss: 0.486 | Acc: 73.406% | Wgt Acc: 84.615%
	I - Batch: 250 | Loss: 0.478 | Acc: 74.125% | Wgt Acc: 85.142%
I - num batch: 285
I - Train -- Loss: 0.476 | Acc: 74.198% | Wgt Acc: 85.152% | LR: 2.500000e-04 | Dur: 178.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 681.    8.   16.   57.  139.]
 [  19.  717.   23.   19.  111.]
 [  25.   32. 1041.   42.  368.]
 [  71.   12.   32.  719.  164.]
 [  10.   11.   12.    3.  218.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.265 | Acc: 53.846% | Wgt Acc: 64.987% | Dur: 22.51s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  2.  6.  8. 22.]
 [ 0. 40.  6.  2. 11.]
 [ 5. 21. 53.  3. 69.]
 [19. 13.  9. 72. 32.]
 [ 2.  2.  1.  1. 46.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 0.490 | Acc: 72.250% | Wgt Acc: 84.290%
	I - Batch: 100 | Loss: 0.467 | Acc: 74.562% | Wgt Acc: 85.578%
	I - Batch: 150 | Loss: 0.462 | Acc: 74.917% | Wgt Acc: 85.727%
	I - Batch: 200 | Loss: 0.466 | Acc: 75.031% | Wgt Acc: 85.571%
	I - Batch: 250 | Loss: 0.464 | Acc: 75.475% | Wgt Acc: 85.870%
I - num batch: 285
I - Train -- Loss: 0.468 | Acc: 75.473% | Wgt Acc: 85.959% | LR: 2.500000e-04 | Dur: 177.89s
I - Confusion Matrix: [row->prediction - col->label]
[[ 691.   12.   17.   53.  125.]
 [   7.  712.   23.   12.   97.]
 [  37.   37. 1039.   30.  343.]
 [  66.   15.   31.  738.  181.]
 [   5.    4.   14.    7.  254.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.257 | Acc: 54.043% | Wgt Acc: 61.492% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[77.  5.  9. 28. 37.]
 [ 0. 44. 11.  3. 17.]
 [ 1. 13. 40.  0. 42.]
 [ 6.  9. 11. 51. 22.]
 [ 4.  7.  4.  4. 62.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.458 | Acc: 75.875% | Wgt Acc: 86.591%
	I - Batch: 100 | Loss: 0.433 | Acc: 76.562% | Wgt Acc: 87.244%
	I - Batch: 150 | Loss: 0.445 | Acc: 76.125% | Wgt Acc: 86.984%
	I - Batch: 200 | Loss: 0.453 | Acc: 75.781% | Wgt Acc: 86.489%
	I - Batch: 250 | Loss: 0.447 | Acc: 76.100% | Wgt Acc: 86.724%
I - num batch: 285
I - Train -- Loss: 0.447 | Acc: 76.088% | Wgt Acc: 86.657% | LR: 2.500000e-04 | Dur: 173.63s
I - Confusion Matrix: [row->prediction - col->label]
[[ 692.    8.   17.   46.  152.]
 [   9.  722.   17.   19.  103.]
 [  30.   35. 1062.   37.  331.]
 [  64.    6.   20.  731.  159.]
 [  11.    9.    8.    7.  255.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.358 | Acc: 53.649% | Wgt Acc: 63.649% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  3.  4. 17. 29.]
 [ 1. 37.  2.  0.  9.]
 [ 3. 21. 53.  6. 59.]
 [14. 13. 14. 63. 33.]
 [ 1.  4.  2.  0. 50.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 0.431 | Acc: 76.375% | Wgt Acc: 86.636%
	I - Batch: 100 | Loss: 0.420 | Acc: 77.562% | Wgt Acc: 87.475%
	I - Batch: 150 | Loss: 0.418 | Acc: 78.083% | Wgt Acc: 88.020%
	I - Batch: 200 | Loss: 0.419 | Acc: 77.562% | Wgt Acc: 87.756%
	I - Batch: 250 | Loss: 0.422 | Acc: 77.125% | Wgt Acc: 87.254%
I - num batch: 285
I - Train -- Loss: 0.420 | Acc: 77.253% | Wgt Acc: 87.497% | LR: 2.500000e-04 | Dur: 177.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 699.    5.   14.   53.  143.]
 [   7.  740.   19.   12.  115.]
 [  27.   25. 1054.   30.  307.]
 [  63.    4.   25.  739.  152.]
 [  10.    6.   12.    6.  283.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.276 | Acc: 55.030% | Wgt Acc: 63.833% | Dur: 14.71s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  3.  6. 14. 22.]
 [ 1. 38.  7.  4. 10.]
 [ 6. 24. 54.  6. 75.]
 [11. 11.  8. 62. 15.]
 [ 3.  2.  0.  0. 58.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.391 | Acc: 78.500% | Wgt Acc: 89.130%
	I - Batch: 100 | Loss: 0.393 | Acc: 77.500% | Wgt Acc: 88.289%
	I - Batch: 150 | Loss: 0.393 | Acc: 78.083% | Wgt Acc: 88.520%
	I - Batch: 200 | Loss: 0.390 | Acc: 77.844% | Wgt Acc: 88.724%
	I - Batch: 250 | Loss: 0.405 | Acc: 77.425% | Wgt Acc: 88.111%
I - num batch: 285
I - Train -- Loss: 0.402 | Acc: 77.451% | Wgt Acc: 88.056% | LR: 2.500000e-04 | Dur: 177.59s
I - Confusion Matrix: [row->prediction - col->label]
[[ 709.    5.    9.   48.  159.]
 [  12.  733.   15.   12.  105.]
 [  22.   29. 1071.   27.  316.]
 [  50.    9.   16.  744.  153.]
 [  13.    4.   13.    9.  267.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.072 | Acc: 60.750% | Wgt Acc: 63.257% | Dur: 15.02s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  2.  5. 15. 14.]
 [ 1. 41.  8.  2. 12.]
 [ 4. 20. 45.  3. 42.]
 [16.  8. 10. 62. 14.]
 [ 5.  7.  7.  4. 98.]]

I - Local maximum validation set accuracy:  60.75

I - Validation set results: 
[14-1-2-2.69][50-3-3-0.14][124-2-2-1.57][127-0-0-4.72][443-2-4-2.37][567-0-0-2.56][573-1-1-2.81][615-0-0-1.94][695-1-2-1.31][722-3-3-2.52]
[826-0-0-3.03][878-0-0-3.35][1103-0-4-0.77][1212-3-4-0.54][1368-0-0-3.62][2181-2-3--0.12][2476-2-2-1.24][2721-2-2-3.63][2818-1-3-1.70][2886-2-1-2.35]
[3231-2-1-2.38][3333-2-2-0.51][3482-2-2-3.14][3536-3-3-2.22][3625-1-1-2.51][3909-0-0-3.08][4035-0-0-2.64][4140-0-0-2.38][4214-1-1-2.19][4346-1-3-0.69]
[4581-2-2-2.29][4708-3-3-0.82][4838-3-3-2.18][4845-1-3-0.20][4868-0-0-2.35][4939-0-4-0.53][4984-2-2-1.21][5078-1-2-1.10][5396-0-0-5.29][5479-1-1-4.46]
[5717-0-0-2.01][5843-1-2-0.72][5949-3-3-2.29][5987-2-4-2.47][6014-3-3-0.95][6033-3-3--0.07][6313-0-3-3.02][6421-3-3-2.32][6500-1-4-0.85][6583-3-2-0.57]
[6683-3-3-2.23][6825-2-1-3.78][6998-3-2--0.24][7049-3-3-1.13][7517-1-2-2.04][7521-1-1-0.51][7528-1-3-1.59][7949-1-2-3.03][8135-1-0-0.78][8185-3-0-4.30]
[8269-3-1-2.87][8273-3-3-2.53][8543-3-0-4.95][8666-1-1-1.62][8672-0-0-3.81][8903-1-1-1.09][9001-2-2-2.02][9036-2-2-4.18][9281-3-3-1.15][9300-2-2-6.29]
[9571-0-3-0.74][9617-1-1-1.64][9644-2-2-0.41][9705-2-1--0.02][9801-0-3-2.79][9803-3-3-2.29][9865-3-3-5.66][9896-2-2-1.77][10314-1-1-1.29][10337-3-3-3.58]
[10403-0-0-1.27][10653-2-1-1.94][10704-2-2-1.62][10719-1-1-2.64][10727-1-4-0.69][10836-0-0-6.95][10969-2-3-0.96][11042-0-0-2.08][11088-1-1-4.73][11322-0-0-2.95]
[11398-2-2-2.03][11499-0-0-2.80][11502-3-0--0.25][11512-3-3-1.25][11608-1-1-5.65][11610-0-0-3.73][11692-0-0-2.28][11905-0-0-4.11][11993-1-1-5.42][12002-2-0-3.15]
[12052-0-0-3.52][12201-0-3-3.38][12235-2-2-2.65][12320-1-4-2.79][12377-2-4-2.96][12398-2-3-1.13][12503-1-1-1.38][12617-0-2--0.22][12685-3-3-0.14][12738-2-0-1.51]
[12742-2-2-4.49][12823-0-3-2.28][13110-1-3-0.92][13240-3-3-3.37][13253-1-1-2.63][13273-0-0-6.05][13634-1-1-4.50][13763-2-3-0.45][13905-3-0--0.17][14060-2-2-0.70]
[14065-3-3-1.08][14147-3-3-2.62][14595-2-1-2.25][14687-2-2-3.57][14788-2-2-1.80][14869-1-1-4.17][14872-3-3--0.05][14877-1-1-2.27][14927-0-3-0.55][15066-0-0-5.67]
[15175-1-1-3.74][15178-2-3-0.70][15375-3-0-2.63][15389-3-3-3.06][15568-2-4-0.14][15675-3-3-5.67][15869-1-3--0.10][16207-3-1-0.14][16236-0-0-1.25][16302-3-0-1.36]
[16331-2-2-3.20][16381-0-0-2.00][16488-1-1-4.50][16495-0-0-4.08][16650-0-0-4.06][16719-1-2-1.78][16801-0-0-5.41][16828-0-0-3.79][17137-3-3-0.54][17245-1-2-0.32]
[17278-3-0-1.34][17282-0-0-3.35][17311-2-2-1.35][17336-2-2-0.77][17608-3-3-3.46][17627-0-0-1.10][17877-3-4-1.63][17924-1-3-0.72][17984-3-0-2.48][18211-0-3-2.29]
[18276-3-0-1.00][18287-1-1-1.09][18394-0-0-4.97][18428-0-3-0.58][18442-0-3-2.59][18478-3-3-3.31][18607-0-0-2.59][18616-0-4-0.62][18663-0-0-2.47][18718-0-0-4.40]
[18766-2-2-4.63][18824-2-2-3.11][18890-3-3-2.72][18930-3-4-1.79][18938-3-3-3.35][19817-1-2-1.85][19839-0-2-0.90][19930-3-3-2.14][19944-0-1-1.52][20036-2-2-5.73]
[20101-3-3-3.30][20474-1-1-2.19][20547-3-3-2.21][20929-2-2-1.68][21245-1-2-2.08][21257-3-0-1.29][21293-1-2-3.73][21316-1-1-4.87][21384-1-4-2.11][21448-1-1-1.38]
[21483-0-0-3.17][21487-2-2-2.64][21714-0-3-0.53][21943-3-3-1.20][21947-0-0-1.65][21948-0-0-6.36][21965-2-2-4.63][21998-1-1-1.27][22025-0-2-1.12][22228-3-3-3.25]
[22446-1-1-2.53][22494-3-3-1.97][22757-0-0-4.30][22811-3-3-2.12][22976-3-2-0.42][22985-3-3-4.44][23014-0-3-3.69][23112-1-1-2.89][23144-3-3-5.05][23168-2-3-1.41]
[23219-0-0-0.75][23363-3-3-3.49][23470-0-0-1.98][23486-2-2-2.24][23497-0-3-4.83][23516-0-0-4.39][23690-1-4-0.67][23921-2-2-2.91][23936-1-2-2.61][24040-3-3-0.17]
[24111-1-4-1.68][24182-0-0-5.27][24238-3-3-1.21][24290-2-0-3.43][24345-0-0-2.56][24364-1-2-0.15][24427-3-0-2.92][24477-2-2-2.12][24495-2-4-0.24][24893-2-1-2.03]
[25012-1-2-0.44][25121-2-4-1.57][25165-3-3-1.42][25183-0-0-3.54][25297-3-3-3.61][25398-0-0-4.25][25574-2-2-1.46][25644-1-2-1.07][25718-1-4-0.17][25774-2-2-0.99]
[26032-3-3-2.99][26051-3-3-5.40][26120-0-0-1.84][26321-1-1-8.32][26732-1-1-1.75][26784-3-3-4.26][26827-3-3-2.68][26833-0-3-3.12][26838-2-3-1.23][26860-1-1-1.63]
[26948-0-0-2.13][27049-3-0-3.06][27098-1-0-0.35][27526-0-0-3.35][27639-3-3-1.54][27698-3-3-1.42][27772-0-0-3.56][27890-1-1-4.13][28040-0-0-1.76][28503-2-2-5.66]
[28577-1-1-2.57][28959-0-0-5.38][29198-3-3-0.83][29777-0-0-6.03][29877-2-2-0.95][30035-1-1-4.44][30098-0-3-2.32][30326-1-1-3.45][30572-2-1-0.33][30716-0-4-2.21]
[30806-2-2-1.57][30906-1-1-3.50][31007-0-3-2.11][31181-3-3-0.55][31238-0-0-2.73][31347-0-0-3.27][31422-2-4-0.47][31429-3-3-1.83][31431-0-0-2.77][31432-1-1-3.63]
[31477-0-0-4.51][31524-1-2-0.46][31597-1-2-0.96][31619-1-2-0.14][31701-0-0-2.44][31755-0-0-3.45][31854-3-3-3.18][32074-1-1-0.09][32078-3-3-4.43][32111-1-1-4.12]
[32127-1-1-1.54][32140-3-3-4.31][32263-2-0-0.55][32365-0-0-2.74][32411-2-0-5.10][32429-3-0-2.92][32473-3-0-1.62][32574-3-3-3.80][32584-0-4-1.97][32622-0-2--0.49]
[32858-3-3-1.01][32969-3-0-3.15][33016-2-2-4.03][33031-1-3-0.30][33035-2-2-5.28][33133-2-2-0.70][33173-2-2-1.64][33175-3-4-1.00][33306-3-3-3.20][33309-2-3-1.42]
[33474-0-0-0.33][33478-2-3-0.63][33618-1-1-3.46][33712-0-3-1.72][33782-2-2-2.44][33914-3-3-1.94][34076-3-3-1.75][34112-2-2-4.77][34138-2-3-1.98][34239-1-1-2.22]
[34364-2-2-3.84][34617-1-2-2.03][34751-3-3-2.31][34783-2-2-0.61][35015-3-3-1.96][35018-1-2-2.08][35288-2-2-0.11][0-4-4-1.24][1-4-4-1.53][2-4-0-1.39]
[3-4-2-1.58][4-4-1-0.51][5-4-1-1.60][6-4-4-3.20][7-4-4-1.24][8-4-2-0.50][9-4-4-0.92][10-4-4-3.34][11-4-2-2.94][12-4-1-0.81]
[14-4-0-0.58][15-4-0-3.10][16-4-4-0.54][17-4-2-1.14][18-4-4-3.83][19-4-0-2.43][20-4-0-0.80][21-4-4-0.58][22-4-4-1.03][23-4-4-1.37]
[24-4-4-4.70][25-4-3-0.49][26-4-4-0.76][27-4-4-0.66][28-4-4-2.39][29-4-2-1.79][30-4-3-0.15][31-4-4-0.92][32-4-4-1.60][33-4-2-3.32]
[34-4-4-0.83][35-4-0-0.91][37-4-2-1.29][39-4-0-2.03][40-4-4-0.19][41-4-3-0.37][42-4-4-1.14][43-4-1-4.05][45-4-4-3.48][46-4-4-4.25]
[47-4-4-3.72][48-4-4-2.43][51-4-4-2.07][52-4-4-0.70][53-4-2-1.16][54-4-3-1.61][55-4-4-1.38][56-4-4-0.94][57-4-3-2.54][58-4-2-2.92]
[59-4-0-2.28][60-4-4-0.50][61-4-4-2.47][62-4-4-0.64][63-4-2-2.56][64-4-4-0.25][65-4-4-3.37][66-4-4-2.65][67-4-2-0.35][68-4-3-0.40]
[69-4-2-1.94][70-4-4-2.05][72-4-4-0.96][73-4-1-2.00][74-4-2-2.04][75-4-0-0.71][77-4-4-3.74][78-4-4--0.20][79-4-2-2.62][80-4-4-2.95]
[81-4-2-2.60][82-4-4-0.20][83-4-1-1.28][84-4-4-3.13][85-4-4-2.83][86-4-2-1.42][87-4-4-2.08][88-4-4-1.38][89-4-4-1.23][90-4-4-0.00]
[91-4-4-0.99][92-4-3--0.13][93-4-0-1.60][94-4-4-1.44][95-4-4--0.01][96-4-1-0.58][97-4-4-3.23][98-4-2-1.75][99-4-2-1.35][100-4-1-1.30]
[101-4-4-4.10][102-4-4-1.71][103-4-2-1.52][104-4-4-1.41][105-4-4-1.78][106-4-1-2.48][107-4-1-1.41][108-4-4-1.43][109-4-4-1.58][110-4-1-0.56]
[111-4-0-5.23][112-4-2-0.49][113-4-2-0.94][114-4-3-0.43][115-4-4-0.44][116-4-4-0.57][117-4-4-1.53][119-4-2-2.29][121-4-4-1.56][122-4-4-1.99]
[124-4-4--0.24][125-4-2-2.53][126-4-4-2.39][127-4-2-1.56][128-4-3-0.73][129-4-4-1.46][130-4-4-0.86][131-4-2-1.23][132-4-3-0.73][133-4-4-3.68]
[135-4-4-1.18][136-4-2-1.32][137-4-2-2.02][138-4-1-0.75][139-4-4-1.89][140-4-2-1.10][141-4-0-1.65][142-4-4-2.62][143-4-2-2.38][144-4-4-3.71]
[145-4-2-1.83][148-4-0-2.46][149-4-4-1.23][150-4-4-3.05][151-4-4-2.43][152-4-4-1.09][153-4-4-1.50][154-4-4-2.81][155-4-4-2.26][156-4-3-1.47]
[157-4-4-0.22][158-4-4-1.98][160-4-4-0.19][161-4-2-1.80][162-4-4-0.48][164-4-2-1.59][165-4-2-1.10][167-4-4-0.88][168-4-4-1.02][170-4-4-1.16]
[171-4-4-2.52][172-4-4-3.35][173-4-4-2.68][174-4-0-1.31][175-4-4-1.65][177-4-4-2.42][178-4-4-1.58][179-4-4-0.90][180-4-4-2.47][181-4-3-1.89]
[182-4-2-1.31][183-4-4-1.59][184-4-2-2.97][186-4-4-0.65][187-4-2-2.08][188-4-2-1.72][189-4-2-1.05][190-4-2-1.55][191-4-2-2.33][192-4-4-0.29]
[193-4-2-2.20][194-4-3-0.14][195-4-4-0.37][196-4-2-2.25][197-4-4-1.20][198-4-4-4.18][199-4-3-0.60]
---------------------------
I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.356 | Acc: 80.125% | Wgt Acc: 90.537%
	I - Batch: 100 | Loss: 0.351 | Acc: 80.250% | Wgt Acc: 90.567%
	I - Batch: 150 | Loss: 0.348 | Acc: 79.833% | Wgt Acc: 90.272%
	I - Batch: 200 | Loss: 0.349 | Acc: 80.062% | Wgt Acc: 90.369%
	I - Batch: 250 | Loss: 0.350 | Acc: 79.950% | Wgt Acc: 90.226%
I - num batch: 285
I - Train -- Loss: 0.351 | Acc: 79.780% | Wgt Acc: 90.136% | LR: 1.250000e-04 | Dur: 174.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 720.    4.   12.   31.  154.]
 [   7.  746.   13.    9.   96.]
 [  19.   18. 1082.   19.  303.]
 [  49.    3.   11.  778.  143.]
 [  11.    9.    6.    3.  304.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.238 | Acc: 56.805% | Wgt Acc: 65.806% | Dur: 14.55s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  3.  6. 14. 27.]
 [ 0. 40.  9.  1.  8.]
 [ 1. 19. 47.  1. 60.]
 [16. 13. 12. 70. 24.]
 [ 1.  3.  1.  0. 61.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840.  781.]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.307 | Acc: 83.125% | Wgt Acc: 92.326%
	I - Batch: 100 | Loss: 0.306 | Acc: 83.312% | Wgt Acc: 92.290%
	I - Batch: 150 | Loss: 0.315 | Acc: 83.000% | Wgt Acc: 91.738%
	I - Batch: 200 | Loss: 0.313 | Acc: 83.000% | Wgt Acc: 91.662%
	I - Batch: 250 | Loss: 0.313 | Acc: 82.650% | Wgt Acc: 91.582%
I - num batch: 271
I - Train -- Loss: 0.313 | Acc: 82.752% | Wgt Acc: 91.641% | LR: 1.250000e-04 | Dur: 163.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 743.    5.    3.   37.  111.]
 [   7.  746.    9.    9.   84.]
 [  21.   18. 1095.   17.  238.]
 [  29.    5.   12.  774.  122.]
 [   6.    6.    5.    3.  226.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.293 | Acc: 53.057% | Wgt Acc: 64.053% | Dur: 14.10s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  4.  4. 12. 23.]
 [ 1. 43. 10.  4. 24.]
 [ 2. 22. 46.  2. 69.]
 [15.  8. 14. 67. 19.]
 [ 2.  1.  1.  1. 45.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.310 | Acc: 82.750% | Wgt Acc: 92.449%
	I - Batch: 100 | Loss: 0.320 | Acc: 81.188% | Wgt Acc: 91.329%
	I - Batch: 150 | Loss: 0.317 | Acc: 81.542% | Wgt Acc: 91.260%
	I - Batch: 200 | Loss: 0.319 | Acc: 81.062% | Wgt Acc: 91.220%
	I - Batch: 250 | Loss: 0.319 | Acc: 81.250% | Wgt Acc: 91.318%
I - num batch: 285
I - Train -- Loss: 0.324 | Acc: 81.099% | Wgt Acc: 91.262% | LR: 1.250000e-04 | Dur: 178.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 736.    3.   13.   31.  141.]
 [   5.  752.    8.    6.   94.]
 [  21.   19. 1091.   14.  286.]
 [  33.    2.    6.  785.  153.]
 [  11.    4.    6.    4.  326.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.138 | Acc: 58.383% | Wgt Acc: 62.922% | Dur: 19.54s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  3.  3. 17. 18.]
 [ 0. 38. 10.  1. 13.]
 [ 4. 19. 47.  1. 53.]
 [13.  7. 12. 61. 12.]
 [ 5. 11.  3.  6. 84.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.303 | Acc: 79.750% | Wgt Acc: 91.008%
	I - Batch: 100 | Loss: 0.304 | Acc: 81.000% | Wgt Acc: 91.387%
	I - Batch: 150 | Loss: 0.313 | Acc: 81.083% | Wgt Acc: 91.373%
	I - Batch: 200 | Loss: 0.310 | Acc: 81.125% | Wgt Acc: 91.269%
	I - Batch: 250 | Loss: 0.315 | Acc: 81.250% | Wgt Acc: 91.123%
I - num batch: 285
I - Train -- Loss: 0.318 | Acc: 80.989% | Wgt Acc: 90.995% | LR: 1.250000e-04 | Dur: 175.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 737.    2.    8.   26.  135.]
 [   4.  753.    7.   10.   97.]
 [  17.   12. 1086.   18.  291.]
 [  33.    4.   12.  777.  145.]
 [  15.    9.   11.    9.  332.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.105 | Acc: 60.355% | Wgt Acc: 62.334% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   0.   1.   7.  12.]
 [  0.  35.   4.   1.   8.]
 [  4.  23.  53.   7.  48.]
 [ 24.  12.  10.  66.  12.]
 [  8.   8.   7.   5. 100.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.288 | Acc: 81.750% | Wgt Acc: 91.781%
	I - Batch: 100 | Loss: 0.293 | Acc: 82.188% | Wgt Acc: 92.359%
	I - Batch: 150 | Loss: 0.295 | Acc: 82.542% | Wgt Acc: 92.507%
	I - Batch: 200 | Loss: 0.301 | Acc: 82.281% | Wgt Acc: 92.337%
	I - Batch: 250 | Loss: 0.297 | Acc: 82.625% | Wgt Acc: 92.497%
I - num batch: 285
I - Train -- Loss: 0.296 | Acc: 82.659% | Wgt Acc: 92.517% | LR: 1.250000e-04 | Dur: 173.97s
I - Confusion Matrix: [row->prediction - col->label]
[[ 755.    0.   12.   17.  151.]
 [   4.  759.    7.    6.   83.]
 [  15.   13. 1097.   17.  266.]
 [  21.    4.    4.  794.  144.]
 [  11.    4.    4.    6.  356.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.189 | Acc: 57.396% | Wgt Acc: 64.502% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  3.  8. 23. 29.]
 [ 1. 48.  7.  2. 16.]
 [ 2. 14. 46.  3. 43.]
 [12.  8. 11. 56. 22.]
 [ 2.  5.  3.  2. 70.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.304 | Acc: 83.125% | Wgt Acc: 92.582%
	I - Batch: 100 | Loss: 0.302 | Acc: 81.562% | Wgt Acc: 92.263%
	I - Batch: 150 | Loss: 0.302 | Acc: 81.333% | Wgt Acc: 91.857%
	I - Batch: 200 | Loss: 0.305 | Acc: 81.531% | Wgt Acc: 91.499%
	I - Batch: 250 | Loss: 0.306 | Acc: 81.950% | Wgt Acc: 91.572%
I - num batch: 285
I - Train -- Loss: 0.304 | Acc: 82.110% | Wgt Acc: 91.764% | LR: 1.250000e-04 | Dur: 184.94s
I - Confusion Matrix: [row->prediction - col->label]
[[ 745.    1.   10.   28.  144.]
 [   1.  755.    8.    6.   92.]
 [  19.   13. 1082.   10.  273.]
 [  30.    5.   10.  793.  130.]
 [  11.    6.   14.    3.  361.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.183 | Acc: 56.213% | Wgt Acc: 63.603% | Dur: 15.01s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  2.  7. 16. 29.]
 [ 1. 38.  6.  1. 12.]
 [ 3. 22. 48.  2. 54.]
 [13. 14. 11. 64. 18.]
 [ 3.  2.  3.  3. 67.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.262 | Acc: 83.875% | Wgt Acc: 93.406%
	I - Batch: 100 | Loss: 0.269 | Acc: 83.312% | Wgt Acc: 93.392%
	I - Batch: 150 | Loss: 0.268 | Acc: 83.208% | Wgt Acc: 93.236%
	I - Batch: 200 | Loss: 0.272 | Acc: 83.156% | Wgt Acc: 93.139%
	I - Batch: 250 | Loss: 0.282 | Acc: 82.725% | Wgt Acc: 92.552%
I - num batch: 285
I - Train -- Loss: 0.282 | Acc: 82.923% | Wgt Acc: 92.596% | LR: 1.250000e-04 | Dur: 182.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 750.    2.    4.   20.  147.]
 [   3.  755.    7.    6.   81.]
 [  15.   16. 1099.    9.  265.]
 [  26.    2.    3.  801.  139.]
 [  12.    5.   11.    4.  368.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.115 | Acc: 61.736% | Wgt Acc: 66.936% | Dur: 16.87s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  4.  9. 16. 20.]
 [ 1. 47.  8.  2. 12.]
 [ 3. 15. 48.  2. 41.]
 [12. 10.  7. 62. 20.]
 [ 3.  2.  3.  4. 87.]]

I - Local maximum validation set accuracy:  61.74

I - Validation set results: 
[14-1-2-0.88][50-3-4-0.19][124-2-2-1.44][127-0-0-5.56][443-2-2-2.28][567-0-0-2.77][573-1-1-4.65][615-0-0-2.36][695-1-2-2.23][722-3-3-3.34]
[826-0-0-3.51][878-0-0-4.17][1103-0-0-2.83][1212-3-4--0.00][1368-0-0-3.36][2181-2-3-0.06][2476-2-2-0.89][2721-2-2-4.55][2818-1-3-1.93][2886-2-1-2.24]
[3231-2-2-1.54][3333-2-2-1.49][3482-2-2-3.82][3536-3-0-2.09][3625-1-1-3.71][3909-0-0-2.73][4035-0-0-1.76][4140-0-0-3.11][4214-1-1-2.42][4346-1-3-0.80]
[4581-2-2-2.38][4708-3-3-3.04][4838-3-3-1.47][4845-1-3-0.60][4868-0-0-2.34][4939-0-3-0.48][4984-2-2-1.01][5078-1-2-1.16][5396-0-0-5.81][5479-1-1-2.97]
[5717-0-0-3.97][5843-1-1-2.62][5949-3-3-1.83][5987-2-4-3.05][6014-3-3-2.00][6033-3-3--0.21][6313-0-3-1.99][6421-3-3-1.47][6500-1-1-0.26][6583-3-3-2.21]
[6683-3-3-1.76][6825-2-1-2.95][6998-3-3-0.02][7049-3-3-2.02][7517-1-2-1.80][7521-1-1-2.32][7528-1-3-2.20][7949-1-2-3.63][8135-1-0-0.50][8185-3-0-5.41]
[8269-3-2-0.87][8273-3-3-2.91][8543-3-0-6.14][8666-1-1-1.90][8672-0-0-5.02][8903-1-2-2.06][9001-2-2-2.14][9036-2-2-4.49][9281-3-3-1.57][9300-2-2-7.58]
[9571-0-3-1.24][9617-1-1-1.35][9644-2-2-1.43][9705-2-0--0.25][9801-0-0-2.31][9803-3-3-2.72][9865-3-3-5.97][9896-2-2-3.58][10314-1-4--0.17][10337-3-3-5.13]
[10403-0-0-1.49][10653-2-1-1.54][10704-2-2-1.89][10719-1-2-3.29][10727-1-2-1.65][10836-0-0-5.97][10969-2-3-3.39][11042-0-0-1.76][11088-1-1-2.25][11322-0-0-3.56]
[11398-2-2-1.44][11499-0-0-2.76][11502-3-0-0.58][11512-3-3-2.51][11608-1-1-4.31][11610-0-0-3.52][11692-0-3-1.84][11905-0-0-3.53][11993-1-1-6.08][12002-2-0-1.95]
[12052-0-0-4.03][12201-0-3-3.04][12235-2-2-2.86][12320-1-4-2.27][12377-2-2-2.51][12398-2-3-1.64][12503-1-1-1.01][12617-0-3-0.17][12685-3-3-0.14][12738-2-3-0.45]
[12742-2-2-4.77][12823-0-0-3.87][13110-1-1-0.72][13240-3-3-2.98][13253-1-1-4.26][13273-0-0-6.27][13634-1-1-4.90][13763-2-2-0.11][13905-3-0-0.06][14060-2-2-0.95]
[14065-3-0-0.76][14147-3-3-1.52][14595-2-2-1.81][14687-2-2-2.93][14788-2-2-2.39][14869-1-1-5.72][14872-3-3-0.59][14877-1-1-1.42][14927-0-3-0.54][15066-0-0-6.64]
[15175-1-1-2.76][15178-2-0-0.41][15375-3-2--0.35][15389-3-0-1.80][15568-2-1-1.22][15675-3-3-5.20][15869-1-1-0.42][16207-3-1-0.05][16236-0-0-0.78][16302-3-3-1.03]
[16331-2-2-4.33][16381-0-0-2.83][16488-1-1-4.63][16495-0-0-4.77][16650-0-0-4.80][16719-1-2-1.40][16801-0-0-4.95][16828-0-0-3.28][17137-3-3-0.20][17245-1-3-0.95]
[17278-3-3-0.14][17282-0-0-2.75][17311-2-2-1.09][17336-2-1-1.47][17608-3-3-3.77][17627-0-0-0.08][17877-3-0-2.36][17924-1-3-0.03][17984-3-3-3.18][18211-0-3-3.69]
[18276-3-3-0.72][18287-1-1-2.10][18394-0-0-4.95][18428-0-0-4.35][18442-0-0-3.43][18478-3-3-2.84][18607-0-0-4.16][18616-0-4-0.82][18663-0-0-2.74][18718-0-0-3.07]
[18766-2-2-5.33][18824-2-2-2.61][18890-3-3-2.71][18930-3-4-1.51][18938-3-3-3.45][19817-1-2-3.17][19839-0-2-0.81][19930-3-3-2.85][19944-0-2-0.71][20036-2-2-5.99]
[20101-3-3-2.62][20474-1-1-2.63][20547-3-3-2.16][20929-2-2-2.48][21245-1-2-1.48][21257-3-3-2.61][21293-1-2-5.06][21316-1-1-5.65][21384-1-1-1.37][21448-1-1-2.07]
[21483-0-0-3.32][21487-2-2-2.85][21714-0-0-0.23][21943-3-3-0.86][21947-0-0-2.58][21948-0-0-6.74][21965-2-2-2.36][21998-1-1-2.92][22025-0-2-0.83][22228-3-3-3.71]
[22446-1-1-3.64][22494-3-3-2.47][22757-0-0-4.91][22811-3-3-3.98][22976-3-4-0.59][22985-3-3-4.49][23014-0-0-2.75][23112-1-1-4.18][23144-3-3-3.25][23168-2-0-1.30]
[23219-0-0-2.21][23363-3-3-4.61][23470-0-0-1.02][23486-2-2-0.12][23497-0-3-3.48][23516-0-0-3.95][23690-1-1-0.06][23921-2-2-1.67][23936-1-2-1.01][24040-3-0-0.19]
[24111-1-2-1.08][24182-0-0-3.90][24238-3-3-1.60][24290-2-0-3.32][24345-0-0-2.15][24364-1-2-1.08][24427-3-0-2.91][24477-2-2-1.49][24495-2-2-2.15][24893-2-1-2.90]
[25012-1-3-0.18][25121-2-4-2.56][25165-3-3-0.96][25183-0-0-3.82][25297-3-3-3.56][25398-0-0-2.91][25574-2-2-1.43][25644-1-1-2.69][25718-1-3-1.29][25774-2-2-1.44]
[26032-3-3-2.92][26051-3-3-4.41][26120-0-0-1.80][26321-1-1-3.80][26732-1-1-2.23][26784-3-3-3.52][26827-3-0-2.65][26833-0-3-2.37][26838-2-3-0.54][26860-1-0-0.11]
[26948-0-0-3.11][27049-3-0-2.29][27098-1-0--0.18][27526-0-0-4.45][27639-3-3-1.83][27698-3-3-3.73][27772-0-0-2.66][27890-1-1-5.74][28040-0-0-1.67][28503-2-2-3.01]
[28577-1-1-3.98][28959-0-0-4.79][29198-3-3-1.79][29777-0-0-4.91][29877-2-2-0.76][30035-1-1-4.95][30098-0-3-2.48][30326-1-1-3.27][30572-2-2-1.41][30716-0-4-2.15]
[30806-2-3-2.18][30906-1-1-2.77][31007-0-0-1.70][31181-3-0-3.18][31238-0-0-3.47][31347-0-0-2.70][31422-2-2-3.93][31429-3-3-2.92][31431-0-0--0.20][31432-1-1-3.18]
[31477-0-0-3.93][31524-1-1-0.36][31597-1-1-0.35][31619-1-3-0.49][31701-0-0-4.40][31755-0-0-3.10][31854-3-3-3.05][32074-1-0-0.02][32078-3-3-4.91][32111-1-1-1.91]
[32127-1-1-1.82][32140-3-3-2.71][32263-2-0-1.05][32365-0-0-3.75][32411-2-0-3.80][32429-3-3-2.45][32473-3-0-1.28][32574-3-3-4.18][32584-0-4-1.57][32622-0-1--0.35]
[32858-3-0-1.88][32969-3-0-2.86][33016-2-2-4.13][33031-1-3-0.88][33035-2-2-4.04][33133-2-2-1.35][33173-2-2-1.37][33175-3-1-1.14][33306-3-3-3.96][33309-2-0-0.99]
[33474-0-0-1.17][33478-2-0--0.61][33618-1-1-2.45][33712-0-3-1.79][33782-2-4-0.96][33914-3-3-3.25][34076-3-3-3.12][34112-2-2-5.85][34138-2-3-3.58][34239-1-1-1.26]
[34364-2-2-4.85][34617-1-1-0.70][34751-3-3-1.61][34783-2-1-1.03][35015-3-3-1.26][35018-1-1-3.10][35288-2-1-0.29][0-4-2-3.99][1-4-4-1.67][2-4-0-1.71]
[3-4-4-2.54][4-4-4-1.70][5-4-1-3.15][6-4-4-3.91][7-4-4-1.37][8-4-2-0.51][9-4-4-0.82][10-4-4-4.44][11-4-2-3.30][12-4-1-1.56]
[14-4-4-0.37][15-4-0-2.42][16-4-4-2.69][17-4-2-0.92][18-4-4-2.17][19-4-0-1.91][20-4-0-1.82][21-4-2-2.27][22-4-4-0.57][23-4-4-2.23]
[24-4-4-1.39][25-4-3-1.38][26-4-2-0.15][27-4-0-1.19][28-4-4-3.52][29-4-2-2.08][30-4-3-1.04][31-4-4-1.36][32-4-4-1.38][33-4-2-3.86]
[34-4-0-0.68][35-4-0-1.03][37-4-4-1.28][39-4-0-4.17][40-4-2-0.45][41-4-4--0.04][42-4-4-0.14][43-4-1-0.74][45-4-3-1.32][46-4-4-3.64]
[47-4-4-4.18][48-4-4-2.59][51-4-4-3.57][52-4-4-2.44][53-4-2-0.99][54-4-3-1.65][55-4-3-1.20][56-4-4-1.22][57-4-3-1.45][58-4-2-3.49]
[59-4-0-1.92][60-4-0--0.17][61-4-4-3.03][62-4-3-1.33][63-4-3-0.74][64-4-2-1.33][65-4-4-3.44][66-4-4-2.15][67-4-3-0.49][68-4-1-1.07]
[69-4-2-2.73][70-4-4-2.46][72-4-2-2.44][73-4-1-1.26][74-4-2-1.43][75-4-0-1.23][77-4-4-2.42][78-4-2-1.00][79-4-2-2.84][80-4-4-2.98]
[81-4-1-2.48][82-4-3-1.08][83-4-4-0.26][84-4-4-4.05][85-4-4-2.78][86-4-2-2.06][87-4-4-2.34][88-4-4-3.08][89-4-4-1.39][90-4-4-1.11]
[91-4-4-0.48][92-4-3--0.60][93-4-0-1.72][94-4-4-2.77][95-4-4-1.18][96-4-4-0.64][97-4-4-3.34][98-4-2-1.49][99-4-2-1.59][100-4-2-3.88]
[101-4-4-4.35][102-4-4-1.57][103-4-0-1.44][104-4-4-0.84][105-4-4-1.45][106-4-4-1.45][107-4-0-1.65][108-4-2-0.53][109-4-1-1.40][110-4-1-2.79]
[111-4-0-5.20][112-4-2-0.72][113-4-4--0.07][114-4-0--0.02][115-4-4-0.34][116-4-3-0.36][117-4-4-2.31][119-4-2-3.32][121-4-4-0.66][122-4-4-2.66]
[124-4-3-0.74][125-4-4-2.77][126-4-4-0.79][127-4-2-3.38][128-4-3-1.24][129-4-4-1.22][130-4-2-0.81][131-4-2-0.78][132-4-2-0.13][133-4-4-3.90]
[135-4-2-1.71][136-4-1-0.83][137-4-4-1.12][138-4-4-0.14][139-4-4-1.87][140-4-1-0.70][141-4-3-0.85][142-4-4-3.56][143-4-4-2.91][144-4-4-4.24]
[145-4-4-2.03][148-4-0-3.81][149-4-4-0.78][150-4-4-2.51][151-4-4-1.96][152-4-4-1.66][153-4-4-2.58][154-4-4-3.12][155-4-4-4.08][156-4-3-1.84]
[157-4-0-1.51][158-4-4-1.46][160-4-1-0.32][161-4-2-4.08][162-4-2-0.73][164-4-4-1.29][165-4-2-1.55][167-4-0-2.54][168-4-4-0.99][170-4-4-0.80]
[171-4-4-1.36][172-4-4-2.77][173-4-4-3.14][174-4-0-1.84][175-4-4-2.12][177-4-4-3.30][178-4-4-0.49][179-4-4-1.56][180-4-4-2.72][181-4-3-2.19]
[182-4-2-3.31][183-4-4-1.83][184-4-2-2.73][186-4-3-0.05][187-4-2-2.62][188-4-2-0.48][189-4-2-2.22][190-4-2-1.52][191-4-4-3.15][192-4-4-0.49]
[193-4-2-1.10][194-4-3-1.15][195-4-2-0.11][196-4-2-4.07][197-4-1-1.32][198-4-4-3.65][199-4-3-1.18]
---------------------------
I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.286 | Acc: 82.875% | Wgt Acc: 92.191%
	I - Batch: 100 | Loss: 0.289 | Acc: 83.188% | Wgt Acc: 92.596%
	I - Batch: 150 | Loss: 0.285 | Acc: 83.125% | Wgt Acc: 92.801%
	I - Batch: 200 | Loss: 0.286 | Acc: 82.969% | Wgt Acc: 92.740%
	I - Batch: 250 | Loss: 0.285 | Acc: 83.175% | Wgt Acc: 92.770%
I - num batch: 285
I - Train -- Loss: 0.284 | Acc: 83.099% | Wgt Acc: 92.839% | LR: 1.250000e-04 | Dur: 176.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 760.    6.    5.   17.  125.]
 [   4.  754.    5.    5.   86.]
 [  14.   11. 1101.    8.  268.]
 [  18.    2.    6.  800.  155.]
 [  10.    7.    7.   10.  366.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 58.974% | Wgt Acc: 63.672% | Dur: 14.72s
I - Confusion Matrix: [row->prediction - col->label]
[[51.  0.  2.  9. 12.]
 [ 0. 38.  3.  1.  9.]
 [ 8. 23. 57.  5. 57.]
 [22.  4.  7. 68. 17.]
 [ 7. 13.  6.  3. 85.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.267 | Acc: 84.250% | Wgt Acc: 93.295%
	I - Batch: 100 | Loss: 0.275 | Acc: 83.562% | Wgt Acc: 93.110%
	I - Batch: 150 | Loss: 0.275 | Acc: 83.625% | Wgt Acc: 93.204%
	I - Batch: 200 | Loss: 0.272 | Acc: 83.656% | Wgt Acc: 93.117%
	I - Batch: 250 | Loss: 0.265 | Acc: 83.900% | Wgt Acc: 93.327%
I - num batch: 285
I - Train -- Loss: 0.269 | Acc: 83.670% | Wgt Acc: 93.244% | LR: 1.250000e-04 | Dur: 174.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 764.    3.    5.   12.  143.]
 [   8.  755.    7.    3.   79.]
 [  11.   11. 1101.   14.  260.]
 [  15.    5.    2.  807.  138.]
 [   8.    6.    9.    4.  380.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.145 | Acc: 59.369% | Wgt Acc: 63.718% | Dur: 15.82s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  3.  4.  7. 17.]
 [ 0. 40.  9.  2. 15.]
 [ 3. 17. 43.  0. 32.]
 [23. 13. 15. 73. 28.]
 [ 5.  5.  4.  4. 88.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.272 | Acc: 84.000% | Wgt Acc: 92.621%
	I - Batch: 100 | Loss: 0.263 | Acc: 84.125% | Wgt Acc: 93.146%
	I - Batch: 150 | Loss: 0.263 | Acc: 84.208% | Wgt Acc: 93.143%
	I - Batch: 200 | Loss: 0.268 | Acc: 83.906% | Wgt Acc: 93.034%
	I - Batch: 250 | Loss: 0.265 | Acc: 83.900% | Wgt Acc: 92.941%
I - num batch: 285
I - Train -- Loss: 0.265 | Acc: 84.044% | Wgt Acc: 93.036% | LR: 1.250000e-04 | Dur: 173.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 752.    0.    4.   18.  119.]
 [   3.  758.    5.    5.   93.]
 [  14.   11. 1097.    5.  243.]
 [  23.    4.    6.  805.  133.]
 [  14.    7.   12.    7.  412.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.195 | Acc: 56.213% | Wgt Acc: 60.835% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  2.  4. 17. 20.]
 [ 0. 36. 11.  2. 12.]
 [ 3. 21. 39.  0. 42.]
 [13. 10. 16. 62. 26.]
 [ 4.  9.  5.  5. 80.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.274 | Acc: 82.625% | Wgt Acc: 92.532%
	I - Batch: 100 | Loss: 0.260 | Acc: 84.250% | Wgt Acc: 93.218%
	I - Batch: 150 | Loss: 0.263 | Acc: 84.583% | Wgt Acc: 93.324%
	I - Batch: 200 | Loss: 0.261 | Acc: 84.312% | Wgt Acc: 93.260%
	I - Batch: 250 | Loss: 0.261 | Acc: 84.125% | Wgt Acc: 93.123%
I - num batch: 285
I - Train -- Loss: 0.256 | Acc: 84.571% | Wgt Acc: 93.288% | LR: 1.250000e-04 | Dur: 179.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 760.    4.    9.   15.  124.]
 [   3.  763.    6.    4.   73.]
 [   9.    6. 1094.   12.  229.]
 [  21.    3.    9.  801.  144.]
 [  13.    4.    6.    8.  430.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.140 | Acc: 61.933% | Wgt Acc: 66.659% | Dur: 18.39s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  1.  6. 14. 16.]
 [ 0. 44.  5.  1.  8.]
 [ 2. 19. 47.  1. 45.]
 [16.  8. 12. 67. 21.]
 [ 4.  6.  5.  3. 90.]]

I - Local maximum validation set accuracy:  61.93

I - Validation set results: 
[14-1-2-2.54][50-3-4-0.66][124-2-2-1.21][127-0-0-6.28][443-2-2-2.57][567-0-0-3.03][573-1-1-5.20][615-0-0-2.78][695-1-2-2.48][722-3-3-3.62]
[826-0-0-2.62][878-0-0-2.93][1103-0-0-3.01][1212-3-4-1.14][1368-0-0-4.54][2181-2-3-0.60][2476-2-2-1.76][2721-2-2-3.94][2818-1-3-1.04][2886-2-1-2.35]
[3231-2-2-3.06][3333-2-2-1.32][3482-2-2-4.65][3536-3-3-2.88][3625-1-1-3.43][3909-0-0-2.69][4035-0-0-1.33][4140-0-0-2.80][4214-1-4-1.66][4346-1-3-1.15]
[4581-2-2-3.80][4708-3-3-3.29][4838-3-3-3.16][4845-1-4-0.75][4868-0-0-2.98][4939-0-3-0.58][4984-2-3-1.12][5078-1-2-2.04][5396-0-0-5.49][5479-1-1-4.91]
[5717-0-0-2.02][5843-1-1-0.82][5949-3-3-3.12][5987-2-4-1.73][6014-3-3-2.14][6033-3-0-0.13][6313-0-3-3.08][6421-3-3-2.23][6500-1-4-1.01][6583-3-3-2.08]
[6683-3-3-2.42][6825-2-1-2.63][6998-3-3-0.42][7049-3-3-3.22][7517-1-1-1.37][7521-1-1-0.40][7528-1-3-2.81][7949-1-2-4.64][8135-1-0-0.21][8185-3-0-5.63]
[8269-3-2-0.63][8273-3-3-2.88][8543-3-0-7.08][8666-1-1-2.47][8672-0-0-5.56][8903-1-2-1.42][9001-2-2-2.32][9036-2-2-5.32][9281-3-3-1.05][9300-2-2-5.71]
[9571-0-3-0.56][9617-1-1-0.76][9644-2-2-2.42][9705-2-0-0.06][9801-0-3-2.34][9803-3-3-3.49][9865-3-3-6.72][9896-2-2-4.86][10314-1-1-0.41][10337-3-3-5.14]
[10403-0-0-1.59][10653-2-1-0.52][10704-2-2-1.50][10719-1-2-2.70][10727-1-2-0.63][10836-0-0-6.83][10969-2-3-3.20][11042-0-0-2.00][11088-1-1-4.79][11322-0-0-3.80]
[11398-2-2-1.38][11499-0-0-3.09][11502-3-0-1.37][11512-3-3-3.25][11608-1-1-5.23][11610-0-0-1.34][11692-0-3-2.97][11905-0-0-4.11][11993-1-1-5.02][12002-2-3-0.27]
[12052-0-0-2.42][12201-0-3-4.30][12235-2-2-2.23][12320-1-4-2.72][12377-2-4-1.93][12398-2-3-1.56][12503-1-2-1.36][12617-0-3-1.33][12685-3-3-1.27][12738-2-0-0.69]
[12742-2-2-5.50][12823-0-0-3.62][13110-1-1-1.16][13240-3-3-3.29][13253-1-1-3.05][13273-0-0-6.94][13634-1-1-4.45][13763-2-3-0.63][13905-3-3-0.14][14060-2-2-0.61]
[14065-3-3-1.73][14147-3-3-3.18][14595-2-1-1.49][14687-2-2-3.64][14788-2-2-2.59][14869-1-1-5.53][14872-3-3-0.27][14877-1-1-2.12][14927-0-3-1.50][15066-0-0-5.97]
[15175-1-1-3.68][15178-2-3-1.38][15375-3-0--0.04][15389-3-3-3.32][15568-2-4-0.67][15675-3-3-5.68][15869-1-3-1.35][16207-3-0-0.24][16236-0-0-0.30][16302-3-3-1.63]
[16331-2-2-5.09][16381-0-0-2.02][16488-1-1-4.27][16495-0-0-5.40][16650-0-0-4.71][16719-1-4-1.11][16801-0-0-6.03][16828-0-0-3.70][17137-3-0-1.46][17245-1-2-1.46]
[17278-3-0-0.57][17282-0-0-3.12][17311-2-2-1.98][17336-2-2-0.49][17608-3-3-5.25][17627-0-0-0.97][17877-3-0-1.18][17924-1-3-0.77][17984-3-3-2.84][18211-0-3-3.10]
[18276-3-3-1.54][18287-1-1-1.88][18394-0-0-5.39][18428-0-0-4.60][18442-0-3-2.70][18478-3-3-3.69][18607-0-0-3.26][18616-0-4-1.32][18663-0-0-3.10][18718-0-0-4.62]
[18766-2-2-6.05][18824-2-2-2.84][18890-3-3-3.51][18930-3-4-2.01][18938-3-3-3.73][19817-1-2-2.97][19839-0-0-0.65][19930-3-3-3.79][19944-0-3-0.86][20036-2-2-6.70]
[20101-3-3-2.85][20474-1-1-3.19][20547-3-3-2.49][20929-2-2-5.02][21245-1-2-2.27][21257-3-3-2.59][21293-1-2-5.29][21316-1-1-5.18][21384-1-1-1.39][21448-1-1-2.91]
[21483-0-0-3.06][21487-2-2-3.23][21714-0-2-0.04][21943-3-3-1.69][21947-0-0-4.67][21948-0-0-7.24][21965-2-2-5.09][21998-1-1-5.34][22025-0-2-0.55][22228-3-3-5.16]
[22446-1-1-1.99][22494-3-0-2.82][22757-0-0-4.53][22811-3-3-3.01][22976-3-3-1.98][22985-3-3-5.67][23014-0-0-2.69][23112-1-1-3.22][23144-3-3-6.41][23168-2-3-2.09]
[23219-0-0-2.40][23363-3-3-5.94][23470-0-0-1.76][23486-2-2-2.98][23497-0-3-4.96][23516-0-0-5.06][23690-1-2-0.27][23921-2-2-2.66][23936-1-2-0.83][24040-3-3-1.13]
[24111-1-1-2.10][24182-0-0-5.36][24238-3-3-3.21][24290-2-0-4.87][24345-0-0-3.08][24364-1-2-0.35][24427-3-0-3.13][24477-2-2-2.83][24495-2-2-0.52][24893-2-1-3.40]
[25012-1-3-1.14][25121-2-4-2.79][25165-3-3-2.60][25183-0-0-3.39][25297-3-3-3.76][25398-0-0-3.54][25574-2-2-1.97][25644-1-1-2.10][25718-1-3-1.24][25774-2-2-1.69]
[26032-3-3-3.52][26051-3-3-6.58][26120-0-4-2.11][26321-1-4-0.14][26732-1-1-2.38][26784-3-3-5.65][26827-3-3-3.31][26833-0-3-2.60][26838-2-3-1.48][26860-1-1-0.78]
[26948-0-0-3.40][27049-3-0-2.62][27098-1-1-0.65][27526-0-0-5.05][27639-3-3-2.82][27698-3-3-1.84][27772-0-0-1.50][27890-1-1-3.83][28040-0-0-0.96][28503-2-2-4.90]
[28577-1-1-1.61][28959-0-0-6.14][29198-3-3-3.08][29777-0-0-5.29][29877-2-2-0.14][30035-1-1-4.00][30098-0-3-3.07][30326-1-1-3.20][30572-2-2-1.02][30716-0-4-2.75]
[30806-2-3-1.52][30906-1-1-3.55][31007-0-0-2.64][31181-3-3-1.96][31238-0-0-3.55][31347-0-0-2.71][31422-2-2-2.13][31429-3-3-3.90][31431-0-0-2.86][31432-1-1-4.94]
[31477-0-0-2.87][31524-1-2-0.63][31597-1-2-0.93][31619-1-2-0.87][31701-0-0-3.34][31755-0-0-2.24][31854-3-3-3.78][32074-1-1-0.04][32078-3-3-4.84][32111-1-1-3.20]
[32127-1-1-1.63][32140-3-3-3.78][32263-2-0-1.05][32365-0-0-4.28][32411-2-3-4.91][32429-3-0-3.04][32473-3-0-1.41][32574-3-3-4.47][32584-0-4-1.82][32622-0-3--0.32]
[32858-3-3-1.64][32969-3-3-3.82][33016-2-2-5.72][33031-1-3-2.19][33035-2-2-5.17][33133-2-2-0.88][33173-2-2-1.91][33175-3-1-0.72][33306-3-3-4.17][33309-2-0-0.84]
[33474-0-0-0.36][33478-2-0-0.98][33618-1-1-1.51][33712-0-3-2.40][33782-2-4-0.92][33914-3-3-3.80][34076-3-3-2.99][34112-2-2-3.29][34138-2-3-2.79][34239-1-1-1.61]
[34364-2-2-4.82][34617-1-1--0.25][34751-3-3-2.08][34783-2-2-1.40][35015-3-3-2.28][35018-1-2-2.23][35288-2-2-0.41][0-4-4-0.85][1-4-4-2.49][2-4-4-1.76]
[3-4-4-2.54][4-4-4-0.76][5-4-3-0.47][6-4-4-4.67][7-4-4-2.14][8-4-2-0.25][9-4-4-1.33][10-4-4-5.03][11-4-2-3.03][12-4-4-0.91]
[14-4-4-1.19][15-4-3-2.17][16-4-4-2.58][17-4-4-0.38][18-4-4-2.16][19-4-0-2.34][20-4-0-1.33][21-4-2-2.72][22-4-4-0.52][23-4-3-2.15]
[24-4-4-2.92][25-4-3-2.28][26-4-4-0.87][27-4-4-1.38][28-4-4-3.64][29-4-1-1.65][30-4-3-2.03][31-4-4-1.73][32-4-4-1.65][33-4-2-4.14]
[34-4-4-0.42][35-4-0-1.16][37-4-4-0.41][39-4-0-1.97][40-4-4--0.06][41-4-4-0.30][42-4-2-0.08][43-4-2-0.02][45-4-1-1.80][46-4-4-4.19]
[47-4-4-3.94][48-4-4-2.73][51-4-4-2.77][52-4-0-0.83][53-4-2-1.10][54-4-3-1.65][55-4-3-1.49][56-4-1-1.05][57-4-3-2.75][58-4-2-3.02]
[59-4-0-2.25][60-4-4-0.93][61-4-4-2.29][62-4-3-1.40][63-4-2-2.17][64-4-2-1.07][65-4-4-3.46][66-4-4-2.23][67-4-2-0.91][68-4-3-0.41]
[69-4-0-1.80][70-4-2-2.61][72-4-1-0.95][73-4-1-2.05][74-4-2-3.72][75-4-0-1.49][77-4-4-2.39][78-4-2-0.59][79-4-2-2.72][80-4-4-2.11]
[81-4-2-2.35][82-4-3-1.15][83-4-4-0.36][84-4-4-2.64][85-4-4-3.58][86-4-2-1.05][87-4-4-2.26][88-4-4-1.29][89-4-4-1.35][90-4-3-0.13]
[91-4-4-0.31][92-4-3-1.18][93-4-0-1.73][94-4-4-1.47][95-4-4-0.62][96-4-3-0.59][97-4-4-4.19][98-4-2-2.44][99-4-2-1.05][100-4-2-3.01]
[101-4-4-5.75][102-4-4-1.10][103-4-2-0.83][104-4-4-0.74][105-4-4-1.20][106-4-4-2.93][107-4-0-1.81][108-4-2-0.27][109-4-4-1.68][110-4-1-1.32]
[111-4-0-4.46][112-4-4-0.41][113-4-3-0.42][114-4-2-0.60][115-4-4-0.93][116-4-3-0.47][117-4-4-1.90][119-4-2-3.59][121-4-4-1.44][122-4-4-2.76]
[124-4-4-0.67][125-4-4-2.65][126-4-4-3.73][127-4-2-1.73][128-4-2-0.83][129-4-4-1.54][130-4-3-0.74][131-4-2-1.97][132-4-0-0.10][133-4-4-3.96]
[135-4-2-1.84][136-4-2-1.41][137-4-4-0.98][138-4-2-0.46][139-4-4-1.65][140-4-1-1.20][141-4-2-2.01][142-4-4-3.34][143-4-4-2.03][144-4-4-1.81]
[145-4-2-1.56][148-4-0-3.59][149-4-4-1.38][150-4-4-3.06][151-4-4-2.77][152-4-4-1.52][153-4-2-2.45][154-4-4-4.23][155-4-4-3.66][156-4-3-2.12]
[157-4-2-1.86][158-4-4-1.55][160-4-4-0.12][161-4-2-3.59][162-4-4-0.58][164-4-2-1.40][165-4-4-1.74][167-4-0-3.56][168-4-4-0.95][170-4-4-1.08]
[171-4-4-2.19][172-4-4-3.13][173-4-4-4.08][174-4-0-1.15][175-4-2-1.65][177-4-4-2.41][178-4-2-0.68][179-4-4-2.02][180-4-4-2.97][181-4-4-0.81]
[182-4-2-2.28][183-4-4-2.62][184-4-2-2.37][186-4-0-0.14][187-4-2-2.48][188-4-4-0.55][189-4-2-2.14][190-4-2-0.14][191-4-4-2.60][192-4-4-0.73]
[193-4-2-4.13][194-4-3-0.53][195-4-3--0.27][196-4-2-4.30][197-4-1-2.35][198-4-4-4.12][199-4-3-0.39]
---------------------------
I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.223 | Acc: 84.625% | Wgt Acc: 93.217%
	I - Batch: 100 | Loss: 0.225 | Acc: 84.562% | Wgt Acc: 93.597%
	I - Batch: 150 | Loss: 0.228 | Acc: 84.958% | Wgt Acc: 93.680%
	I - Batch: 200 | Loss: 0.240 | Acc: 83.906% | Wgt Acc: 93.290%
	I - Batch: 250 | Loss: 0.248 | Acc: 83.775% | Wgt Acc: 93.028%
I - num batch: 285
I - Train -- Loss: 0.250 | Acc: 83.736% | Wgt Acc: 93.125% | LR: 1.250000e-04 | Dur: 171.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 763.    1.    2.   18.  140.]
 [   2.  760.    6.    5.   81.]
 [  13.    9. 1100.   11.  257.]
 [  18.    2.    9.  798.  133.]
 [  10.    8.    7.    8.  389.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.179 | Acc: 59.763% | Wgt Acc: 64.168% | Dur: 13.96s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  8.  7. 20. 22.]
 [ 1. 35.  5.  0.  7.]
 [ 1. 17. 46.  1. 41.]
 [13.  9. 10. 63. 23.]
 [ 1.  9.  7.  2. 87.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.220 | Acc: 87.250% | Wgt Acc: 95.442%
	I - Batch: 100 | Loss: 0.223 | Acc: 87.000% | Wgt Acc: 95.311%
	I - Batch: 150 | Loss: 0.228 | Acc: 86.083% | Wgt Acc: 94.518%
	I - Batch: 200 | Loss: 0.232 | Acc: 85.688% | Wgt Acc: 94.360%
	I - Batch: 250 | Loss: 0.231 | Acc: 85.525% | Wgt Acc: 94.302%
I - num batch: 285
I - Train -- Loss: 0.234 | Acc: 85.538% | Wgt Acc: 94.203% | LR: 1.250000e-04 | Dur: 170.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 764.    1.    3.   12.  136.]
 [   2.  760.    2.    4.   70.]
 [  14.   10. 1114.    8.  214.]
 [  11.    4.    3.  812.  138.]
 [  15.    5.    2.    4.  442.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.073 | Acc: 63.511% | Wgt Acc: 65.171% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   5.  14.  17.]
 [  0.  41.   6.   2.   9.]
 [  2.  13.  42.   1.  35.]
 [ 16.  12.  12.  65.  12.]
 [  3.   9.  10.   4. 107.]]

I - Local maximum validation set accuracy:  63.51

I - Validation set results: 
[14-1-2-1.60][50-3-4-1.11][124-2-2-2.64][127-0-0-6.67][443-2-4-2.50][567-0-0-3.16][573-1-1-3.58][615-0-0-2.54][695-1-2-2.18][722-3-3-3.81]
[826-0-0-6.57][878-0-0-4.11][1103-0-0-3.77][1212-3-4-1.28][1368-0-0-3.88][2181-2-3-1.18][2476-2-2-1.51][2721-2-2-4.64][2818-1-3-2.10][2886-2-1-2.17]
[3231-2-2-3.21][3333-2-2-1.42][3482-2-2-4.40][3536-3-3-1.84][3625-1-1-3.40][3909-0-0-3.38][4035-0-0-2.07][4140-0-0-3.58][4214-1-1-1.80][4346-1-4-1.76]
[4581-2-2-2.83][4708-3-3-2.30][4838-3-3-2.43][4845-1-4-0.68][4868-0-0-4.06][4939-0-3-0.22][4984-2-2-2.56][5078-1-3-0.09][5396-0-0-5.88][5479-1-1-4.33]
[5717-0-0-3.10][5843-1-1-0.94][5949-3-3-3.00][5987-2-4-2.91][6014-3-3-2.51][6033-3-3-1.11][6313-0-3-3.79][6421-3-3-1.92][6500-1-4-1.76][6583-3-3-0.47]
[6683-3-3-2.29][6825-2-1-3.34][6998-3-3--0.03][7049-3-3-3.11][7517-1-2-2.62][7521-1-1-1.50][7528-1-3-1.93][7949-1-2-4.79][8135-1-0-0.44][8185-3-0-5.44]
[8269-3-1-2.72][8273-3-3-3.33][8543-3-0-4.18][8666-1-1-1.94][8672-0-0-4.93][8903-1-2-1.05][9001-2-2-1.59][9036-2-2-4.75][9281-3-3-1.09][9300-2-2-7.81]
[9571-0-0-0.14][9617-1-1-1.01][9644-2-2-1.58][9705-2-0-0.28][9801-0-0-3.60][9803-3-3-2.94][9865-3-3-7.21][9896-2-2-3.46][10314-1-1-1.08][10337-3-3-3.75]
[10403-0-0-2.01][10653-2-4-0.30][10704-2-2-3.57][10719-1-2-2.38][10727-1-4-0.85][10836-0-0-8.20][10969-2-3-3.69][11042-0-0-2.42][11088-1-1-3.24][11322-0-0-4.70]
[11398-2-2-0.53][11499-0-0-3.22][11502-3-0-0.66][11512-3-3-3.17][11608-1-2-3.93][11610-0-0-2.22][11692-0-3-1.59][11905-0-0-3.44][11993-1-1-6.37][12002-2-0--0.36]
[12052-0-0-4.13][12201-0-3-3.76][12235-2-2-2.23][12320-1-4-1.82][12377-2-2-1.82][12398-2-3-1.67][12503-1-1-0.82][12617-0-3-0.27][12685-3-3-0.58][12738-2-3-1.46]
[12742-2-2-6.52][12823-0-3-3.34][13110-1-3-0.74][13240-3-3-3.29][13253-1-1-3.67][13273-0-0-7.30][13634-1-1-4.14][13763-2-3-0.45][13905-3-0-1.11][14060-2-4-1.93]
[14065-3-0-1.11][14147-3-3-2.11][14595-2-2-1.65][14687-2-2-3.80][14788-2-2-1.61][14869-1-1-2.14][14872-3-3-0.65][14877-1-1-1.92][14927-0-3-1.82][15066-0-0-5.93]
[15175-1-1-3.07][15178-2-3-1.49][15375-3-0-1.06][15389-3-3-2.46][15568-2-4-1.07][15675-3-3-7.37][15869-1-3--0.44][16207-3-0--0.07][16236-0-0-2.66][16302-3-3-1.10]
[16331-2-2-4.10][16381-0-3-1.51][16488-1-1-4.86][16495-0-0-5.32][16650-0-0-5.30][16719-1-4-1.32][16801-0-0-7.01][16828-0-0-4.38][17137-3-3-0.50][17245-1-4-2.52]
[17278-3-0-1.11][17282-0-0-4.11][17311-2-2-1.32][17336-2-1-1.07][17608-3-3-4.84][17627-0-0-1.79][17877-3-4-1.62][17924-1-3-1.03][17984-3-3-2.81][18211-0-3-3.67]
[18276-3-3-1.55][18287-1-1-1.88][18394-0-0-5.34][18428-0-0-2.89][18442-0-0-3.69][18478-3-3-3.78][18607-0-0-4.19][18616-0-4-1.28][18663-0-0-3.66][18718-0-0-5.44]
[18766-2-2-6.44][18824-2-4-2.30][18890-3-3-2.96][18930-3-4-1.83][18938-3-3-3.32][19817-1-1-1.80][19839-0-2-0.50][19930-3-3-3.47][19944-0-3-0.20][20036-2-2-7.42]
[20101-3-3-3.15][20474-1-1-2.48][20547-3-3-3.30][20929-2-2-6.16][21245-1-2-1.43][21257-3-3-2.20][21293-1-2-6.16][21316-1-1-4.25][21384-1-1-1.28][21448-1-1-1.86]
[21483-0-0-3.78][21487-2-2-3.94][21714-0-3-0.43][21943-3-3-1.52][21947-0-0-3.94][21948-0-0-8.77][21965-2-2-1.94][21998-1-1-3.20][22025-0-2-2.14][22228-3-3-4.27]
[22446-1-1-0.65][22494-3-0-2.80][22757-0-0-5.54][22811-3-3-4.13][22976-3-2-0.49][22985-3-3-4.70][23014-0-0-4.83][23112-1-1-4.00][23144-3-3-5.13][23168-2-3-1.88]
[23219-0-0-2.24][23363-3-3-6.55][23470-0-0-1.73][23486-2-4-0.35][23497-0-3-4.75][23516-0-0-3.03][23690-1-0-0.61][23921-2-1-1.10][23936-1-2-1.19][24040-3-3-0.32]
[24111-1-1-1.37][24182-0-0-6.91][24238-3-3-2.94][24290-2-0-4.31][24345-0-0-1.19][24364-1-3-1.26][24427-3-0-3.83][24477-2-2-1.83][24495-2-4-0.34][24893-2-1-1.93]
[25012-1-3-0.58][25121-2-4-1.59][25165-3-3-1.98][25183-0-0-4.36][25297-3-3-3.55][25398-0-0-3.44][25574-2-2-1.82][25644-1-1-1.31][25718-1-4-1.08][25774-2-2-2.44]
[26032-3-3-2.70][26051-3-3-6.57][26120-0-0-2.09][26321-1-1-6.54][26732-1-1-3.15][26784-3-3-4.99][26827-3-3-2.58][26833-0-3-1.27][26838-2-3-0.32][26860-1-1-0.51]
[26948-0-0-2.33][27049-3-0-2.73][27098-1-3-0.61][27526-0-0-3.99][27639-3-3-2.50][27698-3-3-2.43][27772-0-0-3.27][27890-1-1-5.12][28040-0-0-1.36][28503-2-2-4.51]
[28577-1-1-0.80][28959-0-0-7.87][29198-3-3-2.64][29777-0-0-6.11][29877-2-2-0.91][30035-1-1-5.79][30098-0-3-3.46][30326-1-1-3.85][30572-2-2-2.64][30716-0-4-2.55]
[30806-2-3-2.00][30906-1-1-4.15][31007-0-0-2.44][31181-3-0-3.28][31238-0-0-3.28][31347-0-0-4.32][31422-2-2-1.93][31429-3-3-2.95][31431-0-0-4.26][31432-1-1-4.75]
[31477-0-0-4.53][31524-1-3-0.34][31597-1-4-1.31][31619-1-0-1.16][31701-0-0-5.99][31755-0-0-3.73][31854-3-3-3.32][32074-1-3-0.64][32078-3-3-4.60][32111-1-1-3.91]
[32127-1-2-1.31][32140-3-3-3.81][32263-2-0-1.51][32365-0-0-5.13][32411-2-3-5.26][32429-3-3-2.83][32473-3-0-2.03][32574-3-3-4.94][32584-0-4-2.50][32622-0-3--0.58]
[32858-3-0-2.43][32969-3-3-3.47][33016-2-2-7.21][33031-1-3-1.08][33035-2-2-5.77][33133-2-2-1.10][33173-2-2-1.66][33175-3-1-1.01][33306-3-3-4.08][33309-2-3-1.16]
[33474-0-0-1.29][33478-2-0-0.01][33618-1-1-0.95][33712-0-3-2.48][33782-2-4-1.35][33914-3-3-2.29][34076-3-3-2.64][34112-2-2-5.09][34138-2-3-3.38][34239-1-1-1.78]
[34364-2-2-5.91][34617-1-2-2.42][34751-3-3-2.62][34783-2-2-0.56][35015-3-3-1.90][35018-1-2-2.70][35288-2-1-0.11][0-4-2-3.25][1-4-4-2.69][2-4-0-1.54]
[3-4-4-1.99][4-4-4-1.52][5-4-1-0.26][6-4-4-2.25][7-4-4-1.74][8-4-2-0.19][9-4-4-1.51][10-4-4-3.74][11-4-2-2.88][12-4-4-1.31]
[14-4-4-1.38][15-4-0-2.57][16-4-4-1.86][17-4-1-0.43][18-4-4-4.14][19-4-0-2.26][20-4-0-1.71][21-4-2-2.43][22-4-1-0.53][23-4-4-2.27]
[24-4-4-1.41][25-4-4-1.33][26-4-4-1.74][27-4-4-1.88][28-4-4-2.22][29-4-1-1.85][30-4-3-1.28][31-4-4-2.41][32-4-4-1.77][33-4-2-2.96]
[34-4-4-1.12][35-4-4-1.89][37-4-4-1.53][39-4-0-4.47][40-4-4-0.29][41-4-3-0.47][42-4-4-1.14][43-4-1-0.10][45-4-2-1.11][46-4-4-4.34]
[47-4-4-3.66][48-4-4-1.77][51-4-4-2.41][52-4-4-1.11][53-4-0-0.42][54-4-3-1.74][55-4-2--0.39][56-4-2-0.48][57-4-3-2.25][58-4-2-3.35]
[59-4-0-3.59][60-4-4-2.10][61-4-4-2.24][62-4-4-1.77][63-4-2-1.28][64-4-2-0.21][65-4-4-3.19][66-4-4-1.20][67-4-3-0.42][68-4-3-0.98]
[69-4-4-0.43][70-4-4-1.80][72-4-4-0.30][73-4-1-2.33][74-4-2-2.28][75-4-4-1.18][77-4-4-4.66][78-4-2-0.02][79-4-4-1.82][80-4-4-2.15]
[81-4-2-2.58][82-4-3-0.60][83-4-4-0.57][84-4-4-3.84][85-4-4-2.85][86-4-2-1.15][87-4-4-2.08][88-4-4-1.72][89-4-4-0.53][90-4-2-0.60]
[91-4-4-1.70][92-4-0-0.66][93-4-0-2.50][94-4-4-2.28][95-4-2-1.33][96-4-4-0.52][97-4-4-4.03][98-4-2-2.39][99-4-2-1.27][100-4-2-3.44]
[101-4-4-5.26][102-4-4-1.90][103-4-2-2.05][104-4-4-0.30][105-4-4-0.76][106-4-1-1.81][107-4-4-0.80][108-4-4-1.23][109-4-4-1.96][110-4-4-0.75]
[111-4-0-6.92][112-4-3-0.85][113-4-4-1.11][114-4-3--0.02][115-4-4-0.85][116-4-2-0.51][117-4-4-2.77][119-4-2-1.03][121-4-4-1.97][122-4-4-3.48]
[124-4-4-1.20][125-4-4-2.84][126-4-4-1.44][127-4-2-2.67][128-4-3-1.66][129-4-4-2.32][130-4-4-0.37][131-4-4-1.02][132-4-0-0.55][133-4-4-4.02]
[135-4-4-1.17][136-4-4-0.44][137-4-2-1.53][138-4-4-0.76][139-4-4-2.08][140-4-1--0.09][141-4-0-2.39][142-4-4-3.19][143-4-4-3.27][144-4-4-3.75]
[145-4-4-2.33][148-4-0-2.99][149-4-0-1.18][150-4-4-3.72][151-4-4-2.21][152-4-4-2.18][153-4-4-1.71][154-4-4-4.43][155-4-4-3.39][156-4-3-0.92]
[157-4-2-0.87][158-4-4-2.12][160-4-4-0.13][161-4-2-3.84][162-4-4-1.43][164-4-4-1.91][165-4-2-1.00][167-4-0-2.23][168-4-0-1.24][170-4-4-1.37]
[171-4-4-2.16][172-4-4-3.40][173-4-4-4.89][174-4-0-1.64][175-4-4-1.41][177-4-4-2.30][178-4-2-0.20][179-4-4-2.10][180-4-4-1.60][181-4-4-1.03]
[182-4-2-1.31][183-4-4-2.50][184-4-4-1.65][186-4-4-1.04][187-4-2-2.03][188-4-4-1.81][189-4-2-2.67][190-4-2-2.14][191-4-4-3.65][192-4-4-2.04]
[193-4-2-2.76][194-4-3-1.62][195-4-4-0.26][196-4-2-4.45][197-4-1-0.81][198-4-4-3.45][199-4-4-1.18]
---------------------------
I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.217 | Acc: 84.375% | Wgt Acc: 94.369%
	I - Batch: 100 | Loss: 0.222 | Acc: 85.125% | Wgt Acc: 94.251%
	I - Batch: 150 | Loss: 0.226 | Acc: 85.333% | Wgt Acc: 94.354%
	I - Batch: 200 | Loss: 0.231 | Acc: 85.125% | Wgt Acc: 94.120%
	I - Batch: 250 | Loss: 0.235 | Acc: 84.900% | Wgt Acc: 94.015%
I - num batch: 285
I - Train -- Loss: 0.232 | Acc: 85.121% | Wgt Acc: 94.225% | LR: 1.250000e-04 | Dur: 178.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 773.    1.    4.   12.  134.]
 [   4.  767.    2.    3.   71.]
 [  11.    9. 1104.    6.  251.]
 [   9.    2.    5.  812.  127.]
 [   9.    1.    9.    7.  417.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.050 | Acc: 64.103% | Wgt Acc: 63.672% | Dur: 18.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   2.   5.  18.  18.]
 [  0.  34.   5.   2.   4.]
 [  2.  20.  48.   0.  32.]
 [ 13.   8.  10.  59.   9.]
 [  6.  14.   7.   7. 117.]]

I - Local maximum validation set accuracy:  64.10

I - Validation set results: 
[14-1-2-1.39][50-3-4-1.48][124-2-2-2.15][127-0-0-7.02][443-2-4-2.93][567-0-0-2.75][573-1-1-4.95][615-0-0-2.74][695-1-2-2.03][722-3-0-2.17]
[826-0-0-2.19][878-0-0-4.41][1103-0-0-1.58][1212-3-4-0.98][1368-0-0-4.46][2181-2-0-0.22][2476-2-2-0.70][2721-2-2-4.88][2818-1-3-1.27][2886-2-1-1.20]
[3231-2-2-4.79][3333-2-2-1.76][3482-2-2-4.82][3536-3-4-0.35][3625-1-1-3.41][3909-0-0-3.45][4035-0-0-2.02][4140-0-0-3.26][4214-1-4-1.34][4346-1-4-1.70]
[4581-2-2-2.33][4708-3-3-2.16][4838-3-3-2.23][4845-1-3--0.04][4868-0-0-3.35][4939-0-4-0.30][4984-2-2-2.46][5078-1-2-0.71][5396-0-0-4.24][5479-1-1-4.52]
[5717-0-0-2.92][5843-1-2-0.60][5949-3-3-2.75][5987-2-4-2.91][6014-3-3-2.28][6033-3-0-0.12][6313-0-3-2.61][6421-3-3-2.13][6500-1-4-1.19][6583-3-3-1.14]
[6683-3-3-2.46][6825-2-1-2.01][6998-3-3--0.59][7049-3-3-1.06][7517-1-2-2.82][7521-1-1-1.01][7528-1-3-1.55][7949-1-2-3.22][8135-1-4-0.37][8185-3-0-6.33]
[8269-3-1-0.51][8273-3-3-2.67][8543-3-0-4.85][8666-1-1-2.74][8672-0-0-3.92][8903-1-4-0.25][9001-2-2-2.57][9036-2-2-3.84][9281-3-3-0.58][9300-2-2-8.59]
[9571-0-3-0.26][9617-1-4-0.74][9644-2-2-0.73][9705-2-0-0.01][9801-0-0-2.76][9803-3-3-2.93][9865-3-3-6.88][9896-2-2-2.00][10314-1-4-0.99][10337-3-3-4.26]
[10403-0-0-2.29][10653-2-4-0.33][10704-2-2-3.53][10719-1-2-2.27][10727-1-4-0.63][10836-0-0-7.80][10969-2-3-3.07][11042-0-0-1.71][11088-1-1-2.82][11322-0-0-4.66]
[11398-2-2-1.01][11499-0-0-3.43][11502-3-0-1.20][11512-3-3-2.02][11608-1-2-3.52][11610-0-0-3.43][11692-0-0-2.72][11905-0-0-3.54][11993-1-1-4.58][12002-2-0-2.37]
[12052-0-0-4.13][12201-0-3-2.94][12235-2-2-2.28][12320-1-4-2.47][12377-2-4-3.13][12398-2-3-1.07][12503-1-2-1.77][12617-0-3-0.14][12685-3-4-0.16][12738-2-3-1.04]
[12742-2-2-6.22][12823-0-0-4.82][13110-1-1-0.71][13240-3-3-2.87][13253-1-1-1.54][13273-0-0-6.46][13634-1-1-3.84][13763-2-2-0.99][13905-3-0-0.39][14060-2-4-1.57]
[14065-3-3-1.57][14147-3-3-1.84][14595-2-2-0.59][14687-2-2-4.08][14788-2-2-3.35][14869-1-1-3.63][14872-3-4-0.70][14877-1-1-1.74][14927-0-3-0.34][15066-0-0-6.11]
[15175-1-1-3.08][15178-2-3-0.28][15375-3-3-1.26][15389-3-0-1.19][15568-2-1-1.92][15675-3-3-6.31][15869-1-0-0.13][16207-3-0--0.39][16236-0-0-0.53][16302-3-0-0.72]
[16331-2-2-5.21][16381-0-3-1.20][16488-1-1-3.89][16495-0-0-5.73][16650-0-0-5.40][16719-1-4-1.49][16801-0-0-6.04][16828-0-0-4.03][17137-3-0-2.46][17245-1-4-0.77]
[17278-3-3-0.72][17282-0-0-5.33][17311-2-2-1.17][17336-2-1-0.11][17608-3-3-4.12][17627-0-0--0.35][17877-3-4-1.89][17924-1-3-0.59][17984-3-3-2.84][18211-0-3-2.87]
[18276-3-3-1.74][18287-1-1-1.16][18394-0-0-5.42][18428-0-3-0.60][18442-0-3-2.13][18478-3-3-3.32][18607-0-0-3.39][18616-0-4-1.45][18663-0-0-3.84][18718-0-0-4.92]
[18766-2-2-6.36][18824-2-2-3.12][18890-3-3-2.55][18930-3-4-1.81][18938-3-3-3.71][19817-1-1-1.34][19839-0-2-0.54][19930-3-3-3.14][19944-0-4-1.00][20036-2-2-7.11]
[20101-3-3-2.30][20474-1-1-3.35][20547-3-3-1.80][20929-2-2-5.74][21245-1-2-3.60][21257-3-3-1.76][21293-1-2-5.37][21316-1-1-6.12][21384-1-1-1.33][21448-1-2-2.40]
[21483-0-0-4.37][21487-2-2-2.20][21714-0-0-0.92][21943-3-3-1.51][21947-0-0-3.02][21948-0-0-7.57][21965-2-2-4.95][21998-1-1-4.89][22025-0-2-1.37][22228-3-3-3.75]
[22446-1-1-3.56][22494-3-0-2.67][22757-0-0-3.81][22811-3-3-4.50][22976-3-3-1.28][22985-3-3-3.97][23014-0-0-3.36][23112-1-1-3.44][23144-3-3-4.41][23168-2-3-1.10]
[23219-0-0-2.04][23363-3-3-4.32][23470-0-0-2.38][23486-2-2-0.26][23497-0-3-5.17][23516-0-0-4.35][23690-1-4-0.87][23921-2-2-2.69][23936-1-2-1.87][24040-3-0-0.37]
[24111-1-4-1.11][24182-0-0-6.06][24238-3-3-2.29][24290-2-0-3.31][24345-0-0-0.83][24364-1-2-0.32][24427-3-0-3.53][24477-2-2-3.47][24495-2-2-0.60][24893-2-1-2.50]
[25012-1-2-0.85][25121-2-4-2.01][25165-3-3-1.94][25183-0-0-3.42][25297-3-3-3.59][25398-0-0-4.96][25574-2-2-2.19][25644-1-2-1.86][25718-1-3-0.41][25774-2-4-1.32]
[26032-3-0-2.61][26051-3-3-5.59][26120-0-0-1.20][26321-1-1-7.74][26732-1-1-2.94][26784-3-3-4.60][26827-3-3-2.86][26833-0-3-1.42][26838-2-2-0.56][26860-1-3--0.22]
[26948-0-0-4.96][27049-3-0-3.46][27098-1-3-1.56][27526-0-0-3.54][27639-3-3-3.16][27698-3-3-4.54][27772-0-0-2.80][27890-1-1-5.87][28040-0-4-1.09][28503-2-2-5.21]
[28577-1-1-1.15][28959-0-0-6.61][29198-3-3-1.57][29777-0-0-5.02][29877-2-2-0.86][30035-1-1-3.08][30098-0-3-3.50][30326-1-1-2.80][30572-2-2-3.29][30716-0-4-3.11]
[30806-2-3-1.16][30906-1-1-1.65][31007-0-0-2.46][31181-3-0-2.63][31238-0-0-3.44][31347-0-0-3.42][31422-2-2-2.77][31429-3-3-3.29][31431-0-0-2.26][31432-1-1-3.93]
[31477-0-0-3.75][31524-1-2-0.50][31597-1-4-0.91][31619-1-0-0.53][31701-0-0-4.19][31755-0-0-3.63][31854-3-3-2.69][32074-1-1-1.86][32078-3-3-4.00][32111-1-1-1.63]
[32127-1-2-1.58][32140-3-3-3.56][32263-2-0-1.07][32365-0-0-5.96][32411-2-3-4.27][32429-3-3-1.96][32473-3-0-1.77][32574-3-3-4.07][32584-0-4-2.81][32622-0-0--0.55]
[32858-3-0-2.16][32969-3-3-3.23][33016-2-2-7.72][33031-1-3-2.01][33035-2-2-4.43][33133-2-2-1.54][33173-2-2-1.77][33175-3-1-1.07][33306-3-3-3.77][33309-2-3-0.84]
[33474-0-0-1.74][33478-2-3-0.67][33618-1-1-1.33][33712-0-3-1.34][33782-2-2-2.12][33914-3-3-3.98][34076-3-3-1.64][34112-2-2-2.75][34138-2-3-2.55][34239-1-1-1.80]
[34364-2-2-5.12][34617-1-2-3.32][34751-3-3-1.95][34783-2-2-1.65][35015-3-3-1.98][35018-1-2-1.91][35288-2-2-1.17][0-4-4-1.72][1-4-4-3.19][2-4-0-1.36]
[3-4-4-2.39][4-4-4-1.41][5-4-3-0.40][6-4-4-3.59][7-4-4-1.60][8-4-2-0.75][9-4-4-1.82][10-4-4-4.09][11-4-4-2.41][12-4-4-1.45]
[14-4-4-1.48][15-4-0-0.79][16-4-4-2.15][17-4-4-0.87][18-4-4-2.17][19-4-0-2.62][20-4-0-1.07][21-4-2-2.97][22-4-4-1.25][23-4-4-2.65]
[24-4-4-2.65][25-4-4-1.29][26-4-4-0.41][27-4-4-2.37][28-4-4-3.51][29-4-2-1.12][30-4-3-1.58][31-4-4-2.16][32-4-4-2.15][33-4-2-3.22]
[34-4-4-1.07][35-4-4-1.33][37-4-4-1.97][39-4-0-3.16][40-4-1--0.46][41-4-4-0.76][42-4-4-1.09][43-4-4-0.99][45-4-2-1.22][46-4-4-4.40]
[47-4-4-4.49][48-4-4-2.44][51-4-4-3.27][52-4-4-1.64][53-4-2-1.33][54-4-3-2.35][55-4-4-2.35][56-4-2-0.37][57-4-3-3.13][58-4-2-4.23]
[59-4-0-2.06][60-4-4-2.34][61-4-4-2.65][62-4-4-1.11][63-4-2-2.35][64-4-2-1.06][65-4-4-3.83][66-4-4-1.37][67-4-2-1.04][68-4-3-0.15]
[69-4-0-1.72][70-4-4-2.65][72-4-4-1.53][73-4-1-1.25][74-4-2-2.89][75-4-0-1.77][77-4-4-3.28][78-4-2--0.04][79-4-2-3.64][80-4-4-3.63]
[81-4-4-1.29][82-4-4-0.38][83-4-4-1.89][84-4-4-3.29][85-4-4-3.76][86-4-2-1.31][87-4-4-2.89][88-4-4-2.35][89-4-4-1.88][90-4-4-0.82]
[91-4-4-1.79][92-4-0-0.77][93-4-0-1.71][94-4-4-2.30][95-4-4-1.80][96-4-4-1.16][97-4-4-4.29][98-4-2-2.86][99-4-4-1.02][100-4-2-3.87]
[101-4-4-5.64][102-4-4-1.85][103-4-2-1.03][104-4-4-1.44][105-4-4-1.64][106-4-4-2.08][107-4-0-1.08][108-4-4-1.38][109-4-4-2.89][110-4-1-2.02]
[111-4-0-4.15][112-4-4-1.28][113-4-4-0.35][114-4-4-0.47][115-4-4-1.23][116-4-3-0.16][117-4-4-3.17][119-4-2-3.60][121-4-4-2.53][122-4-4-3.19]
[124-4-3-0.06][125-4-4-2.84][126-4-4-3.31][127-4-2-2.48][128-4-3-1.39][129-4-4-2.13][130-4-4-1.05][131-4-4-1.04][132-4-4--0.31][133-4-4-3.74]
[135-4-2-1.52][136-4-4-0.97][137-4-4-1.36][138-4-4-1.44][139-4-4-2.45][140-4-2-0.02][141-4-0-2.42][142-4-4-3.57][143-4-4-2.25][144-4-4-3.72]
[145-4-4-2.65][148-4-0-3.28][149-4-4-0.82][150-4-4-3.67][151-4-4-2.89][152-4-4-1.96][153-4-2-2.90][154-4-4-4.64][155-4-4-3.11][156-4-3-2.53]
[157-4-2-1.15][158-4-4-2.21][160-4-4-0.37][161-4-2-2.09][162-4-4-1.31][164-4-4-2.01][165-4-4-1.68][167-4-0-3.39][168-4-4-1.82][170-4-4-1.58]
[171-4-4-2.16][172-4-4-3.90][173-4-4-5.24][174-4-0-0.85][175-4-4-2.27][177-4-4-4.42][178-4-4-0.80][179-4-4-2.66][180-4-4-2.88][181-4-4-1.36]
[182-4-2-1.23][183-4-4-3.08][184-4-2-2.27][186-4-0-0.88][187-4-2-2.50][188-4-4-2.15][189-4-2-1.49][190-4-2-1.01][191-4-4-2.68][192-4-4-0.85]
[193-4-2-1.86][194-4-4-0.39][195-4-0-1.53][196-4-2-4.79][197-4-1-1.13][198-4-4-4.85][199-4-4-0.65]
---------------------------
I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.216 | Acc: 86.250% | Wgt Acc: 94.980%
	I - Batch: 100 | Loss: 0.220 | Acc: 86.125% | Wgt Acc: 95.052%
	I - Batch: 150 | Loss: 0.222 | Acc: 86.250% | Wgt Acc: 94.828%
	I - Batch: 200 | Loss: 0.220 | Acc: 86.188% | Wgt Acc: 94.828%
	I - Batch: 250 | Loss: 0.222 | Acc: 85.975% | Wgt Acc: 94.684%
I - num batch: 285
I - Train -- Loss: 0.225 | Acc: 85.868% | Wgt Acc: 94.474% | LR: 1.250000e-04 | Dur: 174.99s
I - Confusion Matrix: [row->prediction - col->label]
[[ 781.    4.    2.   14.  120.]
 [   3.  767.    3.    5.   59.]
 [   4.    7. 1100.    3.  257.]
 [   9.    0.    6.  811.  116.]
 [   9.    2.   13.    7.  448.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.219 | Acc: 59.566% | Wgt Acc: 64.572% | Dur: 15.58s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  2.  3.  8. 25.]
 [ 1. 37.  6.  2.  7.]
 [ 1. 19. 43.  1. 33.]
 [20. 16. 18. 73. 30.]
 [ 2.  4.  5.  2. 85.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.212 | Acc: 85.625% | Wgt Acc: 95.192%
	I - Batch: 100 | Loss: 0.219 | Acc: 86.000% | Wgt Acc: 94.405%
	I - Batch: 150 | Loss: 0.215 | Acc: 86.167% | Wgt Acc: 94.647%
	I - Batch: 200 | Loss: 0.219 | Acc: 85.625% | Wgt Acc: 94.308%
	I - Batch: 250 | Loss: 0.222 | Acc: 85.675% | Wgt Acc: 94.366%
I - num batch: 285
I - Train -- Loss: 0.224 | Acc: 85.582% | Wgt Acc: 94.314% | LR: 1.250000e-04 | Dur: 188.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 775.    2.    4.   16.  129.]
 [   2.  767.    2.    3.   76.]
 [  10.    4. 1105.    5.  214.]
 [   6.    2.    3.  808.  142.]
 [  13.    5.   10.    8.  439.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.190 | Acc: 60.750% | Wgt Acc: 63.303% | Dur: 15.63s
I - Confusion Matrix: [row->prediction - col->label]
[[75.  3. 11. 25. 33.]
 [ 1. 43.  7.  1. 11.]
 [ 2. 13. 39.  0. 27.]
 [ 8. 11.  9. 54. 12.]
 [ 2.  8.  9.  6. 97.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.236 | Acc: 85.625% | Wgt Acc: 94.200%
	I - Batch: 100 | Loss: 0.232 | Acc: 84.688% | Wgt Acc: 93.973%
	I - Batch: 150 | Loss: 0.220 | Acc: 85.333% | Wgt Acc: 94.360%
	I - Batch: 200 | Loss: 0.223 | Acc: 85.312% | Wgt Acc: 94.224%
	I - Batch: 250 | Loss: 0.225 | Acc: 85.075% | Wgt Acc: 94.231%
I - num batch: 285
I - Train -- Loss: 0.222 | Acc: 85.451% | Wgt Acc: 94.442% | LR: 1.250000e-04 | Dur: 174.94s
I - Confusion Matrix: [row->prediction - col->label]
[[ 774.    2.    4.    8.  141.]
 [   3.  767.    6.    2.   78.]
 [   7.    7. 1104.    4.  243.]
 [  12.    1.    3.  817.  112.]
 [  10.    3.    7.    9.  426.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.110 | Acc: 60.947% | Wgt Acc: 64.802% | Dur: 16.27s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  3.  3. 14. 20.]
 [ 1. 40.  8.  3. 15.]
 [ 6. 19. 47.  3. 39.]
 [13.  6. 10. 64. 14.]
 [ 2. 10.  7.  2. 92.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.191 | Acc: 86.750% | Wgt Acc: 94.887%
	I - Batch: 100 | Loss: 0.189 | Acc: 87.625% | Wgt Acc: 95.415%
	I - Batch: 150 | Loss: 0.201 | Acc: 86.792% | Wgt Acc: 95.026%
	I - Batch: 200 | Loss: 0.201 | Acc: 86.844% | Wgt Acc: 94.888%
	I - Batch: 250 | Loss: 0.203 | Acc: 86.350% | Wgt Acc: 94.837%
I - num batch: 285
I - Train -- Loss: 0.207 | Acc: 86.220% | Wgt Acc: 94.726% | LR: 1.250000e-04 | Dur: 177.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 772.    1.    5.    8.  134.]
 [   4.  771.    3.    2.   67.]
 [   7.    3. 1105.    6.  223.]
 [   8.    1.    5.  818.  119.]
 [  15.    4.    6.    6.  457.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.239 | Acc: 57.199% | Wgt Acc: 64.687% | Dur: 22.78s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  3.  5. 13. 24.]
 [ 2. 45.  9.  2. 16.]
 [ 2. 11. 44.  0. 49.]
 [17. 14. 13. 68. 22.]
 [ 3.  5.  4.  3. 69.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.219 | Acc: 85.375% | Wgt Acc: 94.134%
	I - Batch: 100 | Loss: 0.210 | Acc: 86.125% | Wgt Acc: 94.920%
	I - Batch: 150 | Loss: 0.208 | Acc: 86.542% | Wgt Acc: 94.944%
	I - Batch: 200 | Loss: 0.203 | Acc: 86.906% | Wgt Acc: 95.232%
	I - Batch: 250 | Loss: 0.206 | Acc: 86.475% | Wgt Acc: 94.879%
I - num batch: 285
I - Train -- Loss: 0.209 | Acc: 86.527% | Wgt Acc: 94.867% | LR: 1.250000e-04 | Dur: 182.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 774.    0.    1.   16.  119.]
 [   3.  771.    3.    4.   65.]
 [  11.    6. 1115.    6.  225.]
 [  10.    0.    2.  810.  124.]
 [   8.    3.    3.    4.  467.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 63.511% | Wgt Acc: 67.109% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  2.  6. 14. 19.]
 [ 1. 41.  3.  2. 11.]
 [ 1. 19. 51.  0. 34.]
 [17.  9. 11. 68. 18.]
 [ 5.  7.  4.  2. 98.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.182 | Acc: 87.375% | Wgt Acc: 95.179%
	I - Batch: 100 | Loss: 0.192 | Acc: 87.125% | Wgt Acc: 95.164%
	I - Batch: 150 | Loss: 0.195 | Acc: 87.458% | Wgt Acc: 95.211%
	I - Batch: 200 | Loss: 0.195 | Acc: 87.219% | Wgt Acc: 95.221%
	I - Batch: 250 | Loss: 0.191 | Acc: 87.475% | Wgt Acc: 95.329%
I - num batch: 285
I - Train -- Loss: 0.191 | Acc: 87.560% | Wgt Acc: 95.366% | LR: 1.250000e-04 | Dur: 171.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 782.    4.    3.   11.  101.]
 [   2.  767.    1.    0.   61.]
 [   7.    2. 1114.    4.  204.]
 [   6.    2.    1.  818.  131.]
 [   9.    5.    5.    7.  503.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.086 | Acc: 64.892% | Wgt Acc: 66.463% | Dur: 14.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   2.   4.  10.  14.]
 [  0.  40.   7.   1.   9.]
 [  4.  15.  46.   1.  36.]
 [ 14.   8.  12.  68.  11.]
 [  5.  13.   6.   6. 110.]]

I - Local maximum validation set accuracy:  64.89

I - Validation set results: 
[14-1-2-1.78][50-3-4-1.09][124-2-2-1.01][127-0-0-6.96][443-2-4-3.47][567-0-0-2.58][573-1-1-5.57][615-0-0-1.93][695-1-2-2.44][722-3-3-3.17]
[826-0-0-2.82][878-0-3-4.00][1103-0-0-3.02][1212-3-4-0.85][1368-0-0-3.66][2181-2-3-0.30][2476-2-2-0.71][2721-2-2-4.12][2818-1-1-1.23][2886-2-1-1.30]
[3231-2-2-3.06][3333-2-2-2.48][3482-2-2-5.08][3536-3-3-1.53][3625-1-1-4.42][3909-0-0-2.73][4035-0-0-2.12][4140-0-0-3.63][4214-1-4-1.82][4346-1-4-1.64]
[4581-2-2-2.98][4708-3-3-0.79][4838-3-3-2.63][4845-1-3-0.77][4868-0-0-3.81][4939-0-4-0.14][4984-2-2-3.44][5078-1-2-1.00][5396-0-0-5.27][5479-1-1-6.10]
[5717-0-0-4.88][5843-1-1-3.44][5949-3-3-2.71][5987-2-4-3.31][6014-3-3-2.75][6033-3-4-0.37][6313-0-3-4.64][6421-3-3-2.86][6500-1-4-1.41][6583-3-3-1.25]
[6683-3-3-2.50][6825-2-1-1.80][6998-3-3-0.13][7049-3-3-1.74][7517-1-2-2.23][7521-1-1-2.83][7528-1-3-1.35][7949-1-2-4.33][8135-1-4-0.26][8185-3-0-4.24]
[8269-3-2-1.33][8273-3-3-3.22][8543-3-0-4.45][8666-1-1-2.80][8672-0-0-5.21][8903-1-2-1.84][9001-2-2-1.18][9036-2-2-3.98][9281-3-3-2.02][9300-2-2-8.61]
[9571-0-0-1.08][9617-1-1-0.80][9644-2-2--0.01][9705-2-0-0.48][9801-0-0-1.78][9803-3-3-2.97][9865-3-3-7.86][9896-2-2-2.67][10314-1-1-1.05][10337-3-3-4.86]
[10403-0-0-1.23][10653-2-2-0.15][10704-2-2-3.43][10719-1-2-2.45][10727-1-4-0.38][10836-0-0-7.89][10969-2-3-4.37][11042-0-0-2.33][11088-1-1-4.47][11322-0-0-3.56]
[11398-2-2-1.22][11499-0-0-3.89][11502-3-3-1.20][11512-3-3-2.71][11608-1-2-5.55][11610-0-0-2.26][11692-0-3-2.53][11905-0-0-3.69][11993-1-1-7.49][12002-2-0-2.90]
[12052-0-0-4.09][12201-0-3-4.26][12235-2-2-2.55][12320-1-4-4.21][12377-2-4-2.72][12398-2-3-1.53][12503-1-2-1.52][12617-0-3-0.63][12685-3-3--0.14][12738-2-3-1.12]
[12742-2-2-6.93][12823-0-3-4.36][13110-1-3-1.25][13240-3-3-2.59][13253-1-1-3.02][13273-0-0-7.04][13634-1-1-4.00][13763-2-3-0.52][13905-3-3-0.33][14060-2-4-1.29]
[14065-3-0-1.66][14147-3-3-2.31][14595-2-1-1.09][14687-2-2-4.90][14788-2-2-3.34][14869-1-1-3.48][14872-3-3-1.51][14877-1-1-1.88][14927-0-3-3.03][15066-0-0-6.82]
[15175-1-1-3.16][15178-2-3-0.23][15375-3-3-0.44][15389-3-3-3.74][15568-2-1-2.46][15675-3-3-5.91][15869-1-0-0.90][16207-3-0-0.08][16236-0-0-2.58][16302-3-3-1.66]
[16331-2-2-5.69][16381-0-0-1.79][16488-1-1-4.51][16495-0-0-4.93][16650-0-0-6.52][16719-1-4-1.40][16801-0-0-5.47][16828-0-0-3.45][17137-3-3-1.64][17245-1-4-2.16]
[17278-3-3-0.53][17282-0-0-2.68][17311-2-2-2.61][17336-2-1-0.57][17608-3-3-6.18][17627-0-2-0.10][17877-3-0-1.52][17924-1-3-0.75][17984-3-3-4.00][18211-0-3-2.88]
[18276-3-3-2.22][18287-1-1-1.84][18394-0-0-5.36][18428-0-0-3.07][18442-0-3-4.13][18478-3-3-3.96][18607-0-0-5.12][18616-0-0-0.19][18663-0-0-3.63][18718-0-0-3.79]
[18766-2-2-5.92][18824-2-4-3.01][18890-3-3-3.08][18930-3-4-1.82][18938-3-3-3.67][19817-1-1-1.49][19839-0-2-0.87][19930-3-3-3.39][19944-0-2-0.73][20036-2-2-7.56]
[20101-3-3-4.00][20474-1-1-3.20][20547-3-3-2.59][20929-2-2-5.44][21245-1-2-2.33][21257-3-3-2.25][21293-1-2-6.03][21316-1-1-6.40][21384-1-4-1.69][21448-1-2-1.32]
[21483-0-0-3.59][21487-2-2-3.34][21714-0-0--0.29][21943-3-3-2.32][21947-0-0-4.95][21948-0-0-7.77][21965-2-2-1.92][21998-1-1-5.29][22025-0-4-1.09][22228-3-3-4.80]
[22446-1-1-3.42][22494-3-3-2.61][22757-0-0-3.90][22811-3-3-5.01][22976-3-4-0.54][22985-3-3-4.84][23014-0-3-3.98][23112-1-1-3.41][23144-3-3-4.81][23168-2-3-1.88]
[23219-0-0-2.89][23363-3-3-5.60][23470-0-0-1.36][23486-2-2-1.14][23497-0-3-6.57][23516-0-0-5.09][23690-1-4-0.87][23921-2-2-2.76][23936-1-2-1.83][24040-3-0-0.39]
[24111-1-1-1.61][24182-0-0-6.08][24238-3-3-2.82][24290-2-0-3.96][24345-0-0-2.78][24364-1-3-0.42][24427-3-0-4.67][24477-2-2-2.21][24495-2-2-1.10][24893-2-1-1.78]
[25012-1-4--0.02][25121-2-4-2.45][25165-3-3-2.77][25183-0-0-4.60][25297-3-3-4.12][25398-0-0-3.75][25574-2-2-1.92][25644-1-2-1.19][25718-1-4-1.49][25774-2-2-3.22]
[26032-3-3-3.67][26051-3-3-6.04][26120-0-0-1.60][26321-1-1-8.74][26732-1-1-2.86][26784-3-3-5.52][26827-3-3-2.37][26833-0-3-2.76][26838-2-3-0.06][26860-1-1-0.49]
[26948-0-0-2.27][27049-3-0-1.14][27098-1-1-0.65][27526-0-0-4.23][27639-3-3-4.06][27698-3-3-5.92][27772-0-0-3.70][27890-1-1-5.62][28040-0-4-0.91][28503-2-2-4.93]
[28577-1-1-2.23][28959-0-0-7.43][29198-3-3-3.02][29777-0-0-5.06][29877-2-2-2.57][30035-1-1-1.68][30098-0-3-3.61][30326-1-1-3.69][30572-2-2-2.74][30716-0-4-2.75]
[30806-2-2-1.25][30906-1-1-2.54][31007-0-2-0.41][31181-3-3-1.75][31238-0-0-2.91][31347-0-0-2.86][31422-2-2-2.58][31429-3-3-3.23][31431-0-0-4.74][31432-1-1-3.18]
[31477-0-0-2.98][31524-1-3-0.58][31597-1-4-0.83][31619-1-0-2.15][31701-0-0-3.31][31755-0-0-2.93][31854-3-3-2.91][32074-1-3-1.04][32078-3-3-4.81][32111-1-1-4.23]
[32127-1-1-1.03][32140-3-3-2.87][32263-2-0-0.91][32365-0-0-4.96][32411-2-3-4.63][32429-3-0-3.41][32473-3-0-1.89][32574-3-3-5.25][32584-0-4-2.74][32622-0-0-0.13]
[32858-3-3-2.30][32969-3-3-3.83][33016-2-2-6.96][33031-1-3-1.04][33035-2-2-5.92][33133-2-2-1.24][33173-2-2-1.34][33175-3-1-1.55][33306-3-3-3.73][33309-2-3-2.34]
[33474-0-0-0.75][33478-2-3-0.96][33618-1-1-2.81][33712-0-3-1.29][33782-2-2-2.16][33914-3-3-1.74][34076-3-4-1.52][34112-2-2-4.69][34138-2-3-2.71][34239-1-1-1.57]
[34364-2-2-5.85][34617-1-2-2.83][34751-3-3-1.97][34783-2-2-0.87][35015-3-3-1.74][35018-1-1-1.24][35288-2-1-0.95][0-4-2-3.04][1-4-4-1.98][2-4-0-2.16]
[3-4-4-2.59][4-4-4-1.10][5-4-1-1.02][6-4-4-4.41][7-4-4-1.58][8-4-4--0.15][9-4-4-1.40][10-4-4-4.48][11-4-4-2.11][12-4-4-0.89]
[14-4-4-1.53][15-4-0-1.53][16-4-4-1.81][17-4-4-0.66][18-4-4-3.83][19-4-0-2.03][20-4-0-1.01][21-4-2-2.27][22-4-4-0.14][23-4-4-2.15]
[24-4-4-5.45][25-4-3-0.59][26-4-4-0.48][27-4-4-2.08][28-4-4-4.22][29-4-1-1.54][30-4-3-2.25][31-4-4-2.23][32-4-4-1.78][33-4-2-4.26]
[34-4-4-0.74][35-4-4-2.17][37-4-4-2.26][39-4-0-3.71][40-4-4-0.91][41-4-4-1.22][42-4-2-0.46][43-4-4-0.67][45-4-1-2.26][46-4-4-4.02]
[47-4-4-4.73][48-4-4-4.10][51-4-4-2.47][52-4-4-1.22][53-4-2-0.96][54-4-3-1.52][55-4-3-0.05][56-4-4-0.57][57-4-3-4.51][58-4-2-2.53]
[59-4-4-1.93][60-4-4-0.78][61-4-4-3.01][62-4-4-1.19][63-4-2-2.25][64-4-2-1.39][65-4-4-4.85][66-4-4-2.35][67-4-3-0.77][68-4-3-0.44]
[69-4-0-1.63][70-4-4-2.56][72-4-4-1.59][73-4-1-2.88][74-4-2-2.05][75-4-4-1.36][77-4-4-5.18][78-4-4-0.11][79-4-2-3.40][80-4-4-2.01]
[81-4-4-1.15][82-4-1-0.46][83-4-4-0.49][84-4-4-3.31][85-4-4-3.79][86-4-2-0.74][87-4-4-3.03][88-4-4-1.75][89-4-4-0.82][90-4-2-0.17]
[91-4-4-0.79][92-4-4-1.16][93-4-0-3.17][94-4-4-2.38][95-4-2-1.26][96-4-3-0.75][97-4-4-2.73][98-4-2-4.29][99-4-2-1.02][100-4-2-1.90]
[101-4-4-5.39][102-4-4-0.78][103-4-2-1.32][104-4-4-1.06][105-4-2-1.07][106-4-4-2.91][107-4-0-0.97][108-4-4-1.16][109-4-4-2.06][110-4-4-1.35]
[111-4-0-5.14][112-4-3-1.36][113-4-2-1.07][114-4-4-0.03][115-4-4-1.14][116-4-4-0.36][117-4-4-2.98][119-4-2-4.28][121-4-4-1.38][122-4-4-3.03]
[124-4-1-0.42][125-4-2-2.03][126-4-4-5.57][127-4-2-2.95][128-4-3-1.29][129-4-4-2.49][130-4-4-1.27][131-4-4-1.19][132-4-4-0.43][133-4-4-4.22]
[135-4-2-1.27][136-4-4-0.84][137-4-2-1.46][138-4-4-1.88][139-4-4-2.29][140-4-1-1.37][141-4-0-2.98][142-4-4-3.99][143-4-4-2.04][144-4-4-3.73]
[145-4-4-2.15][148-4-0-3.54][149-4-4-1.60][150-4-4-4.09][151-4-4-3.69][152-4-4-2.53][153-4-2-2.62][154-4-4-5.26][155-4-4-4.23][156-4-3-1.28]
[157-4-2-1.12][158-4-4-1.59][160-4-1-1.16][161-4-2-5.74][162-4-4-1.22][164-4-4-1.45][165-4-2-1.69][167-4-0-3.51][168-4-4-0.95][170-4-4-1.69]
[171-4-4-1.97][172-4-4-4.80][173-4-4-5.23][174-4-0-1.38][175-4-4-2.08][177-4-4-4.50][178-4-0-0.92][179-4-4-1.26][180-4-4-2.65][181-4-4-1.30]
[182-4-2-2.66][183-4-4-3.29][184-4-4-2.93][186-4-4-0.52][187-4-2-2.35][188-4-2-2.29][189-4-2-2.35][190-4-2-0.81][191-4-4-1.05][192-4-4-1.15]
[193-4-2-2.03][194-4-2-1.27][195-4-4--0.43][196-4-2-3.89][197-4-1-0.59][198-4-4-5.97][199-4-4-0.90]
---------------------------
I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.182 | Acc: 88.000% | Wgt Acc: 95.773%
	I - Batch: 100 | Loss: 0.196 | Acc: 87.188% | Wgt Acc: 95.224%
	I - Batch: 150 | Loss: 0.191 | Acc: 87.458% | Wgt Acc: 95.200%
	I - Batch: 200 | Loss: 0.188 | Acc: 87.625% | Wgt Acc: 95.307%
	I - Batch: 250 | Loss: 0.192 | Acc: 87.250% | Wgt Acc: 95.187%
I - num batch: 285
I - Train -- Loss: 0.191 | Acc: 87.429% | Wgt Acc: 95.241% | LR: 1.250000e-04 | Dur: 182.72s
I - Confusion Matrix: [row->prediction - col->label]
[[ 779.    2.    4.   11.  105.]
 [   3.  769.    1.    1.   75.]
 [   6.    5. 1107.    3.  192.]
 [   8.    0.    1.  821.  126.]
 [  10.    4.   11.    4.  502.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.077 | Acc: 63.511% | Wgt Acc: 65.356% | Dur: 19.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   4.   8.  11.]
 [  2.  43.   9.   3.   9.]
 [  4.  17.  48.   3.  40.]
 [ 13.   8.   7.  63.  14.]
 [  7.   8.   7.   9. 106.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.168 | Acc: 87.750% | Wgt Acc: 96.163%
	I - Batch: 100 | Loss: 0.175 | Acc: 88.625% | Wgt Acc: 96.040%
	I - Batch: 150 | Loss: 0.189 | Acc: 87.625% | Wgt Acc: 95.488%
	I - Batch: 200 | Loss: 0.194 | Acc: 86.969% | Wgt Acc: 95.154%
	I - Batch: 250 | Loss: 0.196 | Acc: 87.000% | Wgt Acc: 95.068%
I - num batch: 285
I - Train -- Loss: 0.195 | Acc: 87.121% | Wgt Acc: 95.130% | LR: 1.250000e-04 | Dur: 179.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 783.    0.    7.   11.  128.]
 [   0.  772.    3.    3.   73.]
 [   2.    3. 1106.    5.  189.]
 [  12.    1.    1.  814.  121.]
 [   9.    4.    7.    7.  489.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.035 | Acc: 64.694% | Wgt Acc: 64.387% | Dur: 14.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   0.   6.  15.  13.]
 [  1.  41.   6.   1.   5.]
 [  2.  15.  43.   1.  37.]
 [ 14.   5.   9.  61.   7.]
 [  6.  17.  11.   8. 118.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.166 | Acc: 88.250% | Wgt Acc: 96.553%
	I - Batch: 100 | Loss: 0.167 | Acc: 88.250% | Wgt Acc: 96.106%
	I - Batch: 150 | Loss: 0.170 | Acc: 88.083% | Wgt Acc: 95.899%
	I - Batch: 200 | Loss: 0.175 | Acc: 87.531% | Wgt Acc: 95.573%
	I - Batch: 250 | Loss: 0.180 | Acc: 87.575% | Wgt Acc: 95.499%
I - num batch: 285
I - Train -- Loss: 0.181 | Acc: 87.187% | Wgt Acc: 95.327% | LR: 1.250000e-04 | Dur: 180.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 778.    3.    2.    6.  132.]
 [   1.  771.    1.    1.   56.]
 [   6.    1. 1111.    5.  218.]
 [  12.    1.    3.  823.  110.]
 [   9.    4.    7.    5.  484.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 62.327% | Wgt Acc: 64.076% | Dur: 16.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   2.   7.  22.  26.]
 [  0.  42.   8.   0.   9.]
 [  0.  11.  40.   0.  22.]
 [ 12.  10.  12.  59.  19.]
 [  5.  13.   8.   5. 104.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.188 | Acc: 87.500% | Wgt Acc: 95.353%
	I - Batch: 100 | Loss: 0.189 | Acc: 87.250% | Wgt Acc: 95.292%
	I - Batch: 150 | Loss: 0.187 | Acc: 87.250% | Wgt Acc: 95.374%
	I - Batch: 200 | Loss: 0.180 | Acc: 87.688% | Wgt Acc: 95.511%
	I - Batch: 250 | Loss: 0.183 | Acc: 87.700% | Wgt Acc: 95.463%
I - num batch: 285
I - Train -- Loss: 0.185 | Acc: 87.451% | Wgt Acc: 95.351% | LR: 1.250000e-04 | Dur: 173.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 781.    0.    0.    4.  120.]
 [   1.  770.    3.    0.   57.]
 [   3.    3. 1108.    3.  195.]
 [   9.    3.    2.  822.  130.]
 [  12.    4.   11.   11.  498.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.103 | Acc: 63.116% | Wgt Acc: 64.745% | Dur: 14.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   1.   4.   8.  15.]
 [  0.  34.   4.   1.   9.]
 [  3.  23.  47.   1.  27.]
 [ 20.  12.  14.  73.  22.]
 [  6.   8.   6.   3. 107.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.190 | Acc: 88.000% | Wgt Acc: 95.154%
	I - Batch: 100 | Loss: 0.185 | Acc: 87.688% | Wgt Acc: 95.140%
	I - Batch: 150 | Loss: 0.178 | Acc: 88.292% | Wgt Acc: 95.485%
	I - Batch: 200 | Loss: 0.181 | Acc: 87.969% | Wgt Acc: 95.370%
	I - Batch: 250 | Loss: 0.180 | Acc: 87.800% | Wgt Acc: 95.446%
I - num batch: 285
I - Train -- Loss: 0.181 | Acc: 87.670% | Wgt Acc: 95.372% | LR: 1.250000e-04 | Dur: 175.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 776.    1.    1.    8.  106.]
 [   1.  767.    0.    5.   67.]
 [   5.    4. 1120.    8.  186.]
 [  10.    4.    2.  817.  132.]
 [  14.    4.    1.    2.  509.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.082 | Acc: 64.694% | Wgt Acc: 62.219% | Dur: 15.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   2.   5.  19.  17.]
 [  0.  35.   5.   1.   6.]
 [  3.  15.  37.   0.  21.]
 [ 11.   7.  15.  60.   8.]
 [  6.  19.  13.   6. 128.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.166 | Acc: 90.375% | Wgt Acc: 96.720%
	I - Batch: 100 | Loss: 0.175 | Acc: 88.312% | Wgt Acc: 96.030%
	I - Batch: 150 | Loss: 0.175 | Acc: 88.292% | Wgt Acc: 95.861%
	I - Batch: 200 | Loss: 0.172 | Acc: 88.281% | Wgt Acc: 95.818%
	I - Batch: 250 | Loss: 0.174 | Acc: 88.150% | Wgt Acc: 95.765%
I - num batch: 285
I - Train -- Loss: 0.174 | Acc: 88.000% | Wgt Acc: 95.797% | LR: 1.250000e-04 | Dur: 171.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 786.    1.    2.    7.  124.]
 [   1.  771.    1.    1.   65.]
 [   2.    3. 1116.    5.  184.]
 [   8.    1.    1.  823.  119.]
 [   9.    4.    4.    4.  508.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.113 | Acc: 61.538% | Wgt Acc: 64.768% | Dur: 14.59s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  2.  3. 15. 19.]
 [ 2. 42. 10.  3. 13.]
 [ 2. 18. 46.  3. 41.]
 [14.  8. 11. 63. 11.]
 [ 5.  8.  5.  2. 96.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.182 | Acc: 87.125% | Wgt Acc: 94.947%
	I - Batch: 100 | Loss: 0.175 | Acc: 87.562% | Wgt Acc: 95.414%
	I - Batch: 150 | Loss: 0.177 | Acc: 87.792% | Wgt Acc: 95.419%
	I - Batch: 200 | Loss: 0.178 | Acc: 87.406% | Wgt Acc: 95.358%
	I - Batch: 250 | Loss: 0.174 | Acc: 87.725% | Wgt Acc: 95.483%
I - num batch: 285
I - Train -- Loss: 0.176 | Acc: 87.648% | Wgt Acc: 95.529% | LR: 1.250000e-04 | Dur: 176.56s
I - Confusion Matrix: [row->prediction - col->label]
[[ 781.    1.    1.    7.  140.]
 [   4.  772.    2.    1.   64.]
 [   6.    2. 1110.    2.  178.]
 [   3.    2.    1.  824.  117.]
 [  12.    3.   10.    6.  501.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.117 | Acc: 62.130% | Wgt Acc: 62.323% | Dur: 15.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   1.   7.  17.  17.]
 [  1.  37.   8.   1.   9.]
 [  1.  17.  38.   0.  34.]
 [ 10.   7.  12.  60.   9.]
 [  7.  16.  10.   8. 111.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.188 | Acc: 87.000% | Wgt Acc: 95.310%
	I - Batch: 100 | Loss: 0.183 | Acc: 87.062% | Wgt Acc: 95.201%
	I - Batch: 150 | Loss: 0.183 | Acc: 87.167% | Wgt Acc: 95.363%
	I - Batch: 200 | Loss: 0.176 | Acc: 87.656% | Wgt Acc: 95.651%
	I - Batch: 250 | Loss: 0.176 | Acc: 87.950% | Wgt Acc: 95.749%
I - num batch: 285
I - Train -- Loss: 0.173 | Acc: 88.220% | Wgt Acc: 95.881% | LR: 1.250000e-04 | Dur: 176.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 780.    0.    0.    4.  119.]
 [   1.  778.    0.    2.   68.]
 [   8.    2. 1116.    2.  189.]
 [  10.    0.    1.  823.  107.]
 [   7.    0.    7.    9.  517.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.177 | Acc: 62.722% | Wgt Acc: 64.306% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   1.   3.   9.  14.]
 [  0.  34.   3.   0.   8.]
 [  4.  19.  48.   1.  43.]
 [ 18.   9.  12.  68.   9.]
 [  4.  15.   9.   8. 106.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840.  781.]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.148 | Acc: 90.500% | Wgt Acc: 96.807%
	I - Batch: 100 | Loss: 0.146 | Acc: 90.438% | Wgt Acc: 96.656%
	I - Batch: 150 | Loss: 0.156 | Acc: 89.667% | Wgt Acc: 96.194%
	I - Batch: 200 | Loss: 0.157 | Acc: 89.812% | Wgt Acc: 96.206%
	I - Batch: 250 | Loss: 0.155 | Acc: 89.850% | Wgt Acc: 96.311%
I - num batch: 271
I - Train -- Loss: 0.155 | Acc: 89.818% | Wgt Acc: 96.293% | LR: 1.250000e-04 | Dur: 166.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 787.    1.    1.    8.   81.]
 [   2.  772.    2.    0.   53.]
 [   6.    3. 1116.    7.  148.]
 [   7.    2.    2.  819.  103.]
 [   4.    2.    3.    6.  396.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 62.327% | Wgt Acc: 64.964% | Dur: 14.74s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  2.  5. 22. 19.]
 [ 0. 39.  6.  2. 12.]
 [ 3. 22. 53.  1. 38.]
 [11.  5.  9. 54. 12.]
 [ 3. 10.  2.  7. 99.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.147 | Acc: 89.000% | Wgt Acc: 96.265%
	I - Batch: 100 | Loss: 0.160 | Acc: 88.375% | Wgt Acc: 95.760%
	I - Batch: 150 | Loss: 0.162 | Acc: 88.250% | Wgt Acc: 95.749%
	I - Batch: 200 | Loss: 0.167 | Acc: 88.125% | Wgt Acc: 95.702%
	I - Batch: 250 | Loss: 0.168 | Acc: 88.150% | Wgt Acc: 95.723%
I - num batch: 285
I - Train -- Loss: 0.168 | Acc: 88.000% | Wgt Acc: 95.704% | LR: 1.250000e-04 | Dur: 184.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 775.    0.    0.    3.  129.]
 [   2.  772.    0.    2.   70.]
 [   7.    3. 1118.    3.  171.]
 [   5.    0.    0.  826.  117.]
 [  17.    5.    6.    6.  513.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.102 | Acc: 64.694% | Wgt Acc: 64.745% | Dur: 16.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   0.   5.  14.  13.]
 [  0.  38.   7.   1.   3.]
 [  3.  19.  48.   2.  42.]
 [  8.   6.   6.  60.   6.]
 [ 11.  15.   9.   9. 116.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.148 | Acc: 90.500% | Wgt Acc: 96.973%
	I - Batch: 100 | Loss: 0.156 | Acc: 89.438% | Wgt Acc: 96.441%
	I - Batch: 150 | Loss: 0.154 | Acc: 89.333% | Wgt Acc: 96.551%
	I - Batch: 200 | Loss: 0.161 | Acc: 89.156% | Wgt Acc: 96.383%
	I - Batch: 250 | Loss: 0.161 | Acc: 89.075% | Wgt Acc: 96.275%
I - num batch: 285
I - Train -- Loss: 0.162 | Acc: 88.967% | Wgt Acc: 96.197% | LR: 1.250000e-04 | Dur: 180.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 783.    2.    2.    7.   98.]
 [   0.  773.    1.    1.   51.]
 [   3.    3. 1120.    1.  190.]
 [   7.    0.    0.  827.  116.]
 [  13.    2.    1.    4.  545.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 63.116% | Wgt Acc: 62.519% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   1.   4.  19.  10.]
 [  1.  37.   6.   2.   7.]
 [  3.  16.  44.   1.  34.]
 [ 13.   9.  12.  56.  13.]
 [  4.  15.   9.   8. 116.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.164 | Acc: 89.500% | Wgt Acc: 96.165%
	I - Batch: 100 | Loss: 0.155 | Acc: 89.562% | Wgt Acc: 96.618%
	I - Batch: 150 | Loss: 0.159 | Acc: 88.875% | Wgt Acc: 96.260%
	I - Batch: 200 | Loss: 0.159 | Acc: 89.031% | Wgt Acc: 96.252%
	I - Batch: 250 | Loss: 0.161 | Acc: 89.125% | Wgt Acc: 96.170%
I - num batch: 285
I - Train -- Loss: 0.160 | Acc: 89.275% | Wgt Acc: 96.234% | LR: 1.250000e-04 | Dur: 171.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 782.    0.    1.    5.  106.]
 [   4.  772.    1.    0.   58.]
 [   4.    2. 1120.    0.  181.]
 [   8.    3.    1.  827.   94.]
 [   8.    3.    1.    8.  561.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.199 | Acc: 62.722% | Wgt Acc: 62.138% | Dur: 14.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   3.   5.  13.  21.]
 [  0.  28.   3.   1.   6.]
 [  3.  21.  42.   0.  27.]
 [ 14.  10.  12.  67.  10.]
 [  6.  16.  13.   5. 116.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.158 | Acc: 89.125% | Wgt Acc: 96.159%
	I - Batch: 100 | Loss: 0.147 | Acc: 89.625% | Wgt Acc: 96.523%
	I - Batch: 150 | Loss: 0.155 | Acc: 89.250% | Wgt Acc: 96.252%
	I - Batch: 200 | Loss: 0.161 | Acc: 89.344% | Wgt Acc: 96.152%
	I - Batch: 250 | Loss: 0.160 | Acc: 89.125% | Wgt Acc: 96.153%
I - num batch: 285
I - Train -- Loss: 0.159 | Acc: 89.143% | Wgt Acc: 96.113% | LR: 1.250000e-04 | Dur: 178.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 783.    1.    0.    5.  123.]
 [   2.  770.    2.    2.   61.]
 [   7.    6. 1119.    0.  167.]
 [   8.    1.    0.  825.   90.]
 [   6.    2.    3.    8.  559.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.243 | Acc: 59.961% | Wgt Acc: 64.041% | Dur: 19.80s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  0.  1.  5. 16.]
 [ 0. 35.  6.  2. 10.]
 [ 6. 19. 50.  2. 49.]
 [22. 12. 14. 73. 15.]
 [ 4. 12.  4.  4. 90.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.135 | Acc: 90.375% | Wgt Acc: 96.693%
	I - Batch: 100 | Loss: 0.149 | Acc: 89.375% | Wgt Acc: 96.315%
	I - Batch: 150 | Loss: 0.150 | Acc: 89.542% | Wgt Acc: 96.306%
	I - Batch: 200 | Loss: 0.151 | Acc: 89.875% | Wgt Acc: 96.471%
	I - Batch: 250 | Loss: 0.151 | Acc: 89.950% | Wgt Acc: 96.450%
I - num batch: 285
I - Train -- Loss: 0.155 | Acc: 89.604% | Wgt Acc: 96.242% | LR: 1.250000e-04 | Dur: 172.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 784.    2.    1.    4.  111.]
 [   2.  774.    1.    1.   59.]
 [   6.    2. 1111.    1.  149.]
 [   5.    0.    4.  828.  101.]
 [   9.    2.    7.    6.  580.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.320 | Acc: 59.172% | Wgt Acc: 62.911% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  1.  3. 12. 17.]
 [ 0. 35.  4.  2.  4.]
 [ 1. 19. 44.  0. 34.]
 [25. 17. 20. 70. 35.]
 [ 1.  6.  4.  2. 90.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.151 | Acc: 89.125% | Wgt Acc: 96.052%
	I - Batch: 100 | Loss: 0.149 | Acc: 89.312% | Wgt Acc: 96.440%
	I - Batch: 150 | Loss: 0.152 | Acc: 88.958% | Wgt Acc: 96.157%
	I - Batch: 200 | Loss: 0.151 | Acc: 88.938% | Wgt Acc: 96.261%
	I - Batch: 250 | Loss: 0.147 | Acc: 89.625% | Wgt Acc: 96.473%
I - num batch: 285
I - Train -- Loss: 0.148 | Acc: 89.648% | Wgt Acc: 96.418% | LR: 1.250000e-04 | Dur: 172.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    6.  106.]
 [   1.  775.    0.    4.   54.]
 [   3.    1. 1120.    3.  174.]
 [   5.    1.    1.  820.   93.]
 [   6.    3.    3.    7.  573.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.225 | Acc: 61.341% | Wgt Acc: 65.425% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  1.  6. 13. 28.]
 [ 0. 44.  8.  2.  9.]
 [ 1. 16. 40.  1. 33.]
 [13.  9. 12. 66. 18.]
 [ 5.  8.  9.  4. 92.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.137 | Acc: 89.875% | Wgt Acc: 96.461%
	I - Batch: 100 | Loss: 0.143 | Acc: 89.812% | Wgt Acc: 96.544%
	I - Batch: 150 | Loss: 0.145 | Acc: 89.708% | Wgt Acc: 96.547%
	I - Batch: 200 | Loss: 0.149 | Acc: 89.625% | Wgt Acc: 96.397%
	I - Batch: 250 | Loss: 0.148 | Acc: 89.975% | Wgt Acc: 96.498%
I - num batch: 285
I - Train -- Loss: 0.150 | Acc: 89.670% | Wgt Acc: 96.388% | LR: 1.250000e-04 | Dur: 177.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    7.  105.]
 [   1.  775.    0.    3.   51.]
 [   3.    3. 1115.    2.  158.]
 [   6.    0.    3.  822.  110.]
 [   4.    2.    6.    6.  576.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.156 | Acc: 63.116% | Wgt Acc: 63.776% | Dur: 17.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   1.   4.  12.  14.]
 [  0.  38.   6.   3.   8.]
 [  0.  18.  40.   0.  36.]
 [ 15.  11.  15.  65.  11.]
 [  7.  10.  10.   6. 111.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.154 | Acc: 91.375% | Wgt Acc: 97.050%
	I - Batch: 100 | Loss: 0.150 | Acc: 90.938% | Wgt Acc: 97.081%
	I - Batch: 150 | Loss: 0.150 | Acc: 90.625% | Wgt Acc: 96.916%
	I - Batch: 200 | Loss: 0.150 | Acc: 90.406% | Wgt Acc: 96.952%
	I - Batch: 250 | Loss: 0.151 | Acc: 90.275% | Wgt Acc: 96.851%
I - num batch: 285
I - Train -- Loss: 0.150 | Acc: 90.308% | Wgt Acc: 96.851% | LR: 1.250000e-04 | Dur: 176.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    1.    0.  117.]
 [   0.  771.    0.    2.   53.]
 [   5.    1. 1121.    1.  150.]
 [   3.    2.    0.  832.   89.]
 [   4.    6.    2.    5.  591.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.222 | Acc: 62.327% | Wgt Acc: 65.656% | Dur: 14.92s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  2.  7. 11. 23.]
 [ 0. 37.  6.  3.  7.]
 [ 1. 19. 46.  1. 41.]
 [13. 11. 10. 67. 12.]
 [ 5.  9.  6.  4. 97.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.139 | Acc: 89.500% | Wgt Acc: 96.939%
	I - Batch: 100 | Loss: 0.133 | Acc: 90.812% | Wgt Acc: 96.982%
	I - Batch: 150 | Loss: 0.135 | Acc: 90.292% | Wgt Acc: 96.677%
	I - Batch: 200 | Loss: 0.136 | Acc: 90.125% | Wgt Acc: 96.636%
	I - Batch: 250 | Loss: 0.136 | Acc: 90.225% | Wgt Acc: 96.619%
I - num batch: 285
I - Train -- Loss: 0.138 | Acc: 90.176% | Wgt Acc: 96.568% | LR: 1.250000e-04 | Dur: 174.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    1.    1.    6.  116.]
 [   1.  771.    1.    0.   40.]
 [   3.    2. 1118.    3.  150.]
 [   4.    1.    0.  829.   97.]
 [  10.    5.    4.    2.  597.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.113 | Acc: 65.483% | Wgt Acc: 65.010% | Dur: 16.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   3.  10.  12.]
 [  0.  35.   4.   2.   5.]
 [  1.  16.  40.   0.  24.]
 [ 15.  10.  17.  72.  18.]
 [  8.  15.  11.   2. 121.]]

I - Local maximum validation set accuracy:  65.48

I - Validation set results: 
[14-1-2-1.84][50-3-3-1.68][124-2-4-1.38][127-0-0-8.23][443-2-2-3.81][567-0-0-2.42][573-1-1-4.13][615-0-0-3.77][695-1-2-2.34][722-3-3-3.78]
[826-0-0-6.85][878-0-3-3.63][1103-0-4-1.77][1212-3-3-2.07][1368-0-0-4.84][2181-2-3-1.60][2476-2-2-1.05][2721-2-2-5.18][2818-1-3-1.40][2886-2-4-1.09]
[3231-2-2-4.04][3333-2-2-1.98][3482-2-2-4.35][3536-3-3-2.26][3625-1-1-3.58][3909-0-3-2.89][4035-0-0-3.09][4140-0-0-3.89][4214-1-1-2.26][4346-1-4-2.04]
[4581-2-2-3.59][4708-3-3-2.38][4838-3-3-3.59][4845-1-3-2.17][4868-0-0-4.40][4939-0-4-0.82][4984-2-3-1.40][5078-1-2-1.51][5396-0-0-6.06][5479-1-1-6.01]
[5717-0-0-4.20][5843-1-1-2.55][5949-3-3-3.64][5987-2-4-3.11][6014-3-3-3.13][6033-3-3-1.33][6313-0-3-4.41][6421-3-3-4.16][6500-1-4-1.65][6583-3-3-0.83]
[6683-3-3-3.12][6825-2-1-2.35][6998-3-3-0.94][7049-3-3-3.27][7517-1-1-1.71][7521-1-1-0.08][7528-1-3-4.40][7949-1-2-4.12][8135-1-0-0.18][8185-3-0-6.10]
[8269-3-1-1.77][8273-3-3-4.39][8543-3-0-5.59][8666-1-1-2.64][8672-0-0-4.30][8903-1-3-1.13][9001-2-4-1.61][9036-2-2-6.93][9281-3-3-1.35][9300-2-2-10.94]
[9571-0-4-0.10][9617-1-1-0.58][9644-2-2-1.67][9705-2-0-1.01][9801-0-0-3.16][9803-3-3-5.08][9865-3-3-8.21][9896-2-2-2.68][10314-1-4-1.02][10337-3-3-5.56]
[10403-0-4-2.00][10653-2-1-3.87][10704-2-2-1.36][10719-1-2-2.41][10727-1-4-1.72][10836-0-0-8.63][10969-2-3-5.82][11042-0-0-2.18][11088-1-1-2.85][11322-0-0-3.59]
[11398-2-2-1.36][11499-0-3-3.09][11502-3-3-2.34][11512-3-3-3.46][11608-1-2-3.22][11610-0-0-4.62][11692-0-3-2.97][11905-0-0-4.18][11993-1-1-7.93][12002-2-3-0.47]
[12052-0-0-2.46][12201-0-0-3.84][12235-2-2-5.21][12320-1-4-2.91][12377-2-4-4.46][12398-2-3-3.29][12503-1-2-2.15][12617-0-3-1.91][12685-3-3-0.31][12738-2-4-1.40]
[12742-2-2-6.31][12823-0-3-4.10][13110-1-3-0.99][13240-3-3-2.80][13253-1-1-2.35][13273-0-0-6.95][13634-1-1-4.22][13763-2-3--0.37][13905-3-0-0.60][14060-2-4-1.94]
[14065-3-3-3.36][14147-3-3-2.62][14595-2-2-1.62][14687-2-2-4.18][14788-2-2-3.13][14869-1-1-4.37][14872-3-3-2.00][14877-1-1-1.30][14927-0-3-2.47][15066-0-0-7.13]
[15175-1-4-2.21][15178-2-3-0.92][15375-3-3-2.19][15389-3-3-2.80][15568-2-1-0.64][15675-3-3-9.15][15869-1-3--0.17][16207-3-0--0.71][16236-0-0-3.28][16302-3-3-3.41]
[16331-2-2-5.56][16381-0-3-3.25][16488-1-1-4.72][16495-0-0-5.98][16650-0-0-7.34][16719-1-4-0.88][16801-0-0-6.51][16828-0-0-4.34][17137-3-3-1.63][17245-1-3-1.70]
[17278-3-0-1.24][17282-0-0-1.86][17311-2-2-2.29][17336-2-4-0.51][17608-3-3-3.25][17627-0-0--0.14][17877-3-4-1.97][17924-1-4--0.04][17984-3-3-4.95][18211-0-3-4.11]
[18276-3-3-2.66][18287-1-4-1.19][18394-0-0-5.88][18428-0-0-7.04][18442-0-0-4.16][18478-3-3-3.81][18607-0-0-4.80][18616-0-0-2.33][18663-0-0-3.83][18718-0-0-5.14]
[18766-2-2-4.20][18824-2-2-2.38][18890-3-3-2.76][18930-3-4-3.26][18938-3-3-3.93][19817-1-2-3.40][19839-0-0-0.13][19930-3-3-4.49][19944-0-0-1.95][20036-2-2-7.51]
[20101-3-3-5.87][20474-1-1-2.79][20547-3-3-0.81][20929-2-2-4.97][21245-1-2-1.33][21257-3-3-3.12][21293-1-2-5.32][21316-1-1-7.85][21384-1-4-1.17][21448-1-1-1.10]
[21483-0-0-3.50][21487-2-2-4.08][21714-0-3-0.43][21943-3-3-2.45][21947-0-0-4.18][21948-0-0-7.81][21965-2-2-6.30][21998-1-1-4.59][22025-0-2-1.56][22228-3-3-7.21]
[22446-1-1-2.61][22494-3-3-3.94][22757-0-0-6.25][22811-3-3-4.14][22976-3-3-1.20][22985-3-3-6.10][23014-0-0-4.58][23112-1-1-5.57][23144-3-3-5.28][23168-2-3-3.41]
[23219-0-0-2.89][23363-3-3-5.59][23470-0-0-2.24][23486-2-3-0.96][23497-0-3-6.61][23516-0-0-3.24][23690-1-2-1.05][23921-2-2-0.46][23936-1-2-0.35][24040-3-0-0.30]
[24111-1-4-1.64][24182-0-0-6.97][24238-3-3-2.97][24290-2-0-3.23][24345-0-0-1.70][24364-1-2-0.43][24427-3-0-3.83][24477-2-2-3.21][24495-2-4-1.95][24893-2-1-1.19]
[25012-1-3-0.06][25121-2-4-4.02][25165-3-3-4.30][25183-0-0-5.48][25297-3-3-4.43][25398-0-0-3.67][25574-2-2-1.40][25644-1-2-1.60][25718-1-4-1.18][25774-2-3-0.97]
[26032-3-3-4.21][26051-3-3-7.23][26120-0-0-2.66][26321-1-1-9.17][26732-1-1-2.77][26784-3-3-6.51][26827-3-3-4.08][26833-0-0-3.47][26838-2-3-0.36][26860-1-0-0.29]
[26948-0-0-5.52][27049-3-0-1.56][27098-1-1-1.10][27526-0-0-4.34][27639-3-3-3.56][27698-3-3-1.97][27772-0-0-3.97][27890-1-1-6.85][28040-0-4-0.95][28503-2-2-5.23]
[28577-1-1-1.94][28959-0-0-7.06][29198-3-3-3.37][29777-0-0-6.61][29877-2-3-0.40][30035-1-1-6.02][30098-0-3-4.34][30326-1-1-3.80][30572-2-2-0.68][30716-0-4-2.60]
[30806-2-3-2.33][30906-1-1-3.77][31007-0-0-2.01][31181-3-3-2.71][31238-0-0-3.92][31347-0-0-2.70][31422-2-2-1.91][31429-3-3-4.73][31431-0-0-3.09][31432-1-1-4.07]
[31477-0-0-4.48][31524-1-3-0.22][31597-1-4-1.24][31619-1-2-0.72][31701-0-0-6.55][31755-0-0-4.70][31854-3-3-4.09][32074-1-1-3.39][32078-3-3-6.31][32111-1-1-5.07]
[32127-1-2-1.84][32140-3-3-4.21][32263-2-0-1.62][32365-0-0-7.19][32411-2-3-4.94][32429-3-0-4.01][32473-3-0-1.79][32574-3-3-5.31][32584-0-3-2.01][32622-0-4-0.80]
[32858-3-3-3.28][32969-3-3-6.26][33016-2-2-3.77][33031-1-3-2.82][33035-2-2-4.73][33133-2-2-1.92][33173-2-2-1.15][33175-3-1-1.28][33306-3-3-3.80][33309-2-3-2.14]
[33474-0-4-1.60][33478-2-3-1.83][33618-1-1-2.61][33712-0-3-2.59][33782-2-4-1.93][33914-3-3-3.84][34076-3-3-2.78][34112-2-2-1.59][34138-2-3-3.36][34239-1-1-0.89]
[34364-2-2-6.03][34617-1-4-1.02][34751-3-3-3.61][34783-2-2-1.34][35015-3-3-3.34][35018-1-4-1.99][35288-2-2-0.75][0-4-2-4.71][1-4-4-0.37][2-4-4-2.14]
[3-4-2-2.82][4-4-4-2.21][5-4-1-2.44][6-4-4-4.41][7-4-4-4.24][8-4-4-0.08][9-4-4-2.57][10-4-4-5.37][11-4-4-3.38][12-4-4-0.93]
[14-4-4-1.44][15-4-0-3.04][16-4-4-2.33][17-4-4-1.73][18-4-4-3.97][19-4-0-3.63][20-4-4-1.51][21-4-4-1.23][22-4-4-1.81][23-4-3-2.36]
[24-4-4-2.75][25-4-3-2.01][26-4-4-0.67][27-4-4-2.61][28-4-4-2.18][29-4-2-0.55][30-4-4-1.19][31-4-4-2.13][32-4-4-2.59][33-4-2-3.88]
[34-4-4-1.68][35-4-4-1.60][37-4-4-0.90][39-4-0-2.34][40-4-4-1.52][41-4-4-2.53][42-4-4-1.30][43-4-4-1.98][45-4-1-1.83][46-4-4-3.48]
[47-4-4-4.72][48-4-4-2.88][51-4-4-3.62][52-4-4-1.68][53-4-2-2.57][54-4-3-3.47][55-4-0-0.11][56-4-4-0.97][57-4-3-5.22][58-4-2-2.20]
[59-4-4-2.81][60-4-4-2.56][61-4-4-4.05][62-4-3-1.86][63-4-4-1.74][64-4-2-1.31][65-4-4-5.08][66-4-4-2.99][67-4-4-0.65][68-4-3-2.05]
[69-4-0-1.81][70-4-4-2.94][72-4-4-1.33][73-4-1-2.89][74-4-2-1.87][75-4-4-1.96][77-4-4-4.14][78-4-4-0.73][79-4-2-4.58][80-4-4-4.51]
[81-4-2-3.34][82-4-4-0.49][83-4-4-2.28][84-4-4-4.08][85-4-4-4.23][86-4-4-1.92][87-4-4-3.23][88-4-4-3.45][89-4-4-0.73][90-4-4-1.11]
[91-4-4-1.62][92-4-3-0.42][93-4-0-0.96][94-4-4-1.91][95-4-4-2.11][96-4-3-1.62][97-4-4-5.84][98-4-2-3.99][99-4-4-0.75][100-4-2-5.61]
[101-4-4-4.99][102-4-4-2.55][103-4-3-0.46][104-4-4-2.07][105-4-4-2.14][106-4-4-3.56][107-4-0-2.38][108-4-4-2.12][109-4-4-3.54][110-4-1-1.22]
[111-4-0-5.53][112-4-3-2.58][113-4-3--0.03][114-4-3-0.34][115-4-4-1.92][116-4-4-0.34][117-4-4-4.33][119-4-2-3.49][121-4-4-2.99][122-4-4-1.88]
[124-4-4-1.32][125-4-2-2.65][126-4-4-2.47][127-4-2-1.74][128-4-3-1.49][129-4-4-3.36][130-4-4-1.38][131-4-4-1.90][132-4-4--0.32][133-4-4-2.35]
[135-4-4-2.99][136-4-4-1.65][137-4-2-3.77][138-4-4-0.73][139-4-4-1.35][140-4-1-1.80][141-4-0-2.46][142-4-4-5.00][143-4-4-5.15][144-4-4-1.82]
[145-4-2-2.68][148-4-0-3.67][149-4-4-1.46][150-4-4-4.56][151-4-4-3.16][152-4-4-3.17][153-4-4-3.71][154-4-4-4.22][155-4-4-4.43][156-4-3-1.08]
[157-4-0-2.04][158-4-4-1.83][160-4-4-1.24][161-4-2-5.25][162-4-4-0.55][164-4-4-2.57][165-4-4-1.72][167-4-0-2.46][168-4-4-1.88][170-4-4-1.31]
[171-4-4-2.14][172-4-4-4.26][173-4-4-6.50][174-4-3-0.58][175-4-4-3.97][177-4-4-2.27][178-4-4-1.28][179-4-4-2.25][180-4-4-3.33][181-4-3-3.64]
[182-4-3-0.95][183-4-4-3.01][184-4-2-3.11][186-4-4-1.45][187-4-2-1.86][188-4-4-1.18][189-4-2-1.38][190-4-2-1.05][191-4-4-2.87][192-4-4-2.59]
[193-4-2-2.82][194-4-3-1.12][195-4-4-0.98][196-4-2-4.42][197-4-4-1.94][198-4-4-6.08][199-4-4-1.69]
---------------------------
I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.137 | Acc: 91.000% | Wgt Acc: 97.010%
	I - Batch: 100 | Loss: 0.142 | Acc: 89.688% | Wgt Acc: 96.569%
	I - Batch: 150 | Loss: 0.145 | Acc: 89.458% | Wgt Acc: 96.512%
	I - Batch: 200 | Loss: 0.143 | Acc: 89.188% | Wgt Acc: 96.491%
	I - Batch: 250 | Loss: 0.141 | Acc: 89.200% | Wgt Acc: 96.522%
I - num batch: 285
I - Train -- Loss: 0.141 | Acc: 89.297% | Wgt Acc: 96.612% | LR: 1.250000e-04 | Dur: 172.12s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    2.    0.    3.  132.]
 [   0.  774.    1.    0.   61.]
 [   1.    2. 1121.    1.  152.]
 [   2.    1.    0.  830.  111.]
 [   9.    1.    2.    6.  544.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.183 | Acc: 64.103% | Wgt Acc: 64.964% | Dur: 14.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 79.   6.   8.  25.  29.]
 [  0.  42.   8.   2.   9.]
 [  1.  15.  36.   0.  21.]
 [  7.   8.   9.  57.  10.]
 [  1.   7.  14.   2. 111.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.113 | Acc: 92.375% | Wgt Acc: 97.489%
	I - Batch: 100 | Loss: 0.117 | Acc: 91.625% | Wgt Acc: 97.134%
	I - Batch: 150 | Loss: 0.118 | Acc: 91.458% | Wgt Acc: 97.129%
	I - Batch: 200 | Loss: 0.127 | Acc: 90.812% | Wgt Acc: 96.860%
	I - Batch: 250 | Loss: 0.130 | Acc: 90.775% | Wgt Acc: 96.826%
I - num batch: 285
I - Train -- Loss: 0.132 | Acc: 90.747% | Wgt Acc: 96.839% | LR: 1.250000e-04 | Dur: 171.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    2.   95.]
 [   0.  774.    2.    0.   54.]
 [   3.    1. 1117.    1.  148.]
 [   4.    1.    0.  829.   86.]
 [   7.    4.    5.    8.  617.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.184 | Acc: 61.538% | Wgt Acc: 61.043% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   2.  13.  12.]
 [  0.  29.   5.   1.   6.]
 [  4.  21.  47.   4.  36.]
 [ 15.  10.  12.  61.  13.]
 [  7.  16.   9.   7. 113.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.145 | Acc: 89.375% | Wgt Acc: 96.172%
	I - Batch: 100 | Loss: 0.146 | Acc: 89.375% | Wgt Acc: 96.155%
	I - Batch: 150 | Loss: 0.146 | Acc: 89.417% | Wgt Acc: 96.351%
	I - Batch: 200 | Loss: 0.142 | Acc: 89.781% | Wgt Acc: 96.478%
	I - Batch: 250 | Loss: 0.141 | Acc: 89.975% | Wgt Acc: 96.477%
I - num batch: 285
I - Train -- Loss: 0.143 | Acc: 89.758% | Wgt Acc: 96.373% | LR: 1.250000e-04 | Dur: 181.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    1.    1.    3.  106.]
 [   0.  774.    3.    1.   58.]
 [   2.    3. 1112.    2.  160.]
 [   5.    0.    2.  824.   94.]
 [   7.    2.    6.   10.  582.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.154 | Acc: 63.905% | Wgt Acc: 62.726% | Dur: 15.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   2.  13.  10.]
 [  0.  35.   3.   1.   5.]
 [  3.  13.  42.   0.  28.]
 [ 18.   7.  11.  65.  16.]
 [  6.  21.  17.   7. 121.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.133 | Acc: 90.000% | Wgt Acc: 96.391%
	I - Batch: 100 | Loss: 0.138 | Acc: 90.688% | Wgt Acc: 96.602%
	I - Batch: 150 | Loss: 0.142 | Acc: 90.417% | Wgt Acc: 96.520%
	I - Batch: 200 | Loss: 0.140 | Acc: 90.531% | Wgt Acc: 96.556%
	I - Batch: 250 | Loss: 0.139 | Acc: 90.425% | Wgt Acc: 96.618%
I - num batch: 285
I - Train -- Loss: 0.140 | Acc: 90.264% | Wgt Acc: 96.533% | LR: 1.250000e-04 | Dur: 179.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 786.    0.    0.    2.   97.]
 [   2.  772.    2.    2.   57.]
 [   4.    2. 1115.    1.  141.]
 [   6.    0.    3.  830.  101.]
 [   8.    6.    4.    5.  604.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.109 | Acc: 64.497% | Wgt Acc: 64.721% | Dur: 15.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   2.   3.  17.  20.]
 [  0.  39.   5.   1.   8.]
 [  1.  15.  47.   0.  24.]
 [ 16.   5.  10.  61.  13.]
 [  6.  17.  10.   7. 115.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.132 | Acc: 90.250% | Wgt Acc: 96.821%
	I - Batch: 100 | Loss: 0.130 | Acc: 90.250% | Wgt Acc: 96.884%
	I - Batch: 150 | Loss: 0.126 | Acc: 90.625% | Wgt Acc: 97.006%
	I - Batch: 200 | Loss: 0.129 | Acc: 90.469% | Wgt Acc: 96.931%
	I - Batch: 250 | Loss: 0.131 | Acc: 90.200% | Wgt Acc: 96.799%
I - num batch: 285
I - Train -- Loss: 0.131 | Acc: 90.154% | Wgt Acc: 96.683% | LR: 1.250000e-04 | Dur: 172.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    1.    2.    5.  104.]
 [   0.  774.    0.    1.   64.]
 [   4.    1. 1119.    0.  138.]
 [   1.    1.    1.  827.  104.]
 [   9.    3.    2.    7.  590.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.112 | Acc: 62.525% | Wgt Acc: 62.772% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   0.   7.  15.  22.]
 [  1.  42.  10.   3.  10.]
 [  2.  11.  34.   0.  22.]
 [ 11.  12.  13.  63.  14.]
 [  8.  13.  11.   5. 112.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.119 | Acc: 92.625% | Wgt Acc: 97.679%
	I - Batch: 100 | Loss: 0.122 | Acc: 91.938% | Wgt Acc: 97.606%
	I - Batch: 150 | Loss: 0.124 | Acc: 91.458% | Wgt Acc: 97.507%
	I - Batch: 200 | Loss: 0.125 | Acc: 91.438% | Wgt Acc: 97.392%
	I - Batch: 250 | Loss: 0.125 | Acc: 91.425% | Wgt Acc: 97.334%
I - num batch: 285
I - Train -- Loss: 0.124 | Acc: 91.385% | Wgt Acc: 97.278% | LR: 1.250000e-04 | Dur: 178.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    1.    3.   97.]
 [   0.  774.    0.    0.   49.]
 [   2.    1. 1119.    0.  139.]
 [   1.    1.    0.  835.   82.]
 [   6.    4.    4.    2.  633.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.194 | Acc: 64.694% | Wgt Acc: 63.776% | Dur: 17.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   1.   3.  15.  12.]
 [  0.  36.   6.   1.   5.]
 [  3.  17.  41.   1.  26.]
 [ 13.   7.  12.  64.  16.]
 [  6.  17.  13.   5. 121.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.114 | Acc: 92.875% | Wgt Acc: 97.443%
	I - Batch: 100 | Loss: 0.118 | Acc: 92.125% | Wgt Acc: 97.452%
	I - Batch: 150 | Loss: 0.125 | Acc: 91.167% | Wgt Acc: 97.068%
	I - Batch: 200 | Loss: 0.129 | Acc: 90.781% | Wgt Acc: 96.718%
	I - Batch: 250 | Loss: 0.130 | Acc: 90.825% | Wgt Acc: 96.844%
I - num batch: 285
I - Train -- Loss: 0.131 | Acc: 90.879% | Wgt Acc: 96.866% | LR: 1.250000e-04 | Dur: 174.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    2.    1.    3.   84.]
 [   2.  768.    1.    1.   54.]
 [   1.    3. 1117.    2.  137.]
 [   0.    0.    0.  829.  102.]
 [   5.    7.    5.    5.  623.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.124 | Acc: 64.892% | Wgt Acc: 61.884% | Dur: 21.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   1.   3.  14.  12.]
 [  1.  39.   9.   2.   6.]
 [  3.  15.  38.   1.  23.]
 [ 13.   6.  10.  59.   8.]
 [  9.  17.  15.  10. 131.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.121 | Acc: 92.000% | Wgt Acc: 97.251%
	I - Batch: 100 | Loss: 0.120 | Acc: 92.000% | Wgt Acc: 97.277%
	I - Batch: 150 | Loss: 0.128 | Acc: 91.250% | Wgt Acc: 96.802%
	I - Batch: 200 | Loss: 0.126 | Acc: 91.281% | Wgt Acc: 96.980%
	I - Batch: 250 | Loss: 0.127 | Acc: 91.050% | Wgt Acc: 96.945%
I - num batch: 285
I - Train -- Loss: 0.128 | Acc: 90.967% | Wgt Acc: 96.946% | LR: 1.250000e-04 | Dur: 175.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    1.    2.    3.   90.]
 [   1.  774.    0.    1.   46.]
 [   4.    1. 1116.    0.  154.]
 [   4.    2.    0.  834.   85.]
 [   7.    2.    6.    2.  625.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.255 | Acc: 61.144% | Wgt Acc: 62.807% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   1.   4.  16.  17.]
 [  1.  40.   7.   0.  14.]
 [  3.  17.  41.   1.  29.]
 [ 18.   8.  13.  65.  17.]
 [  5.  12.  10.   4. 103.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.126 | Acc: 92.375% | Wgt Acc: 97.228%
	I - Batch: 100 | Loss: 0.115 | Acc: 92.500% | Wgt Acc: 97.400%
	I - Batch: 150 | Loss: 0.114 | Acc: 92.042% | Wgt Acc: 97.322%
	I - Batch: 200 | Loss: 0.117 | Acc: 91.750% | Wgt Acc: 97.329%
	I - Batch: 250 | Loss: 0.120 | Acc: 91.700% | Wgt Acc: 97.309%
I - num batch: 285
I - Train -- Loss: 0.121 | Acc: 91.495% | Wgt Acc: 97.281% | LR: 1.250000e-04 | Dur: 178.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    2.   93.]
 [   2.  776.    0.    0.   51.]
 [   1.    0. 1120.    1.  131.]
 [   1.    3.    0.  832.   86.]
 [   6.    1.    4.    5.  639.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.168 | Acc: 63.905% | Wgt Acc: 62.703% | Dur: 15.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   7.  10.  12.]
 [  0.  35.   6.   2.   9.]
 [  1.  15.  41.   2.  25.]
 [ 15.   9.  10.  64.  13.]
 [  9.  15.  11.   8. 121.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.105 | Acc: 91.875% | Wgt Acc: 97.218%
	I - Batch: 100 | Loss: 0.116 | Acc: 91.188% | Wgt Acc: 96.926%
	I - Batch: 150 | Loss: 0.120 | Acc: 90.875% | Wgt Acc: 96.872%
	I - Batch: 200 | Loss: 0.122 | Acc: 90.812% | Wgt Acc: 96.932%
	I - Batch: 250 | Loss: 0.122 | Acc: 90.775% | Wgt Acc: 96.890%
I - num batch: 285
I - Train -- Loss: 0.121 | Acc: 91.055% | Wgt Acc: 96.992% | LR: 1.250000e-04 | Dur: 173.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 786.    0.    0.    3.   86.]
 [   1.  777.    0.    0.   44.]
 [   2.    0. 1118.    0.  138.]
 [   3.    0.    0.  834.  104.]
 [  14.    3.    6.    3.  628.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.172 | Acc: 65.878% | Wgt Acc: 66.036% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   7.  13.  20.]
 [  0.  40.   7.   0.   9.]
 [  2.  16.  44.   0.  24.]
 [ 13.   6.   9.  64.   9.]
 [  5.  13.   8.   9. 118.]]

I - Local maximum validation set accuracy:  65.88

I - Validation set results: 
[14-1-2-2.57][50-3-4-1.61][124-2-2-2.31][127-0-0-8.59][443-2-4-4.62][567-0-0-3.32][573-1-1-5.33][615-0-0-3.50][695-1-2-2.11][722-3-3-2.72]
[826-0-0-5.25][878-0-0-5.95][1103-0-0-2.04][1212-3-4-1.97][1368-0-0-4.37][2181-2-4-1.02][2476-2-0-0.29][2721-2-2-5.41][2818-1-3-0.95][2886-2-1-2.94]
[3231-2-2-4.43][3333-2-2-1.32][3482-2-2-5.49][3536-3-0-1.63][3625-1-1-4.97][3909-0-0-3.07][4035-0-0-3.32][4140-0-0-4.72][4214-1-4-2.00][4346-1-4-1.98]
[4581-2-2-3.37][4708-3-4-2.46][4838-3-3-2.43][4845-1-3-0.88][4868-0-0-7.69][4939-0-3-0.89][4984-2-3-3.41][5078-1-2-1.58][5396-0-0-6.69][5479-1-1-5.54]
[5717-0-0-6.28][5843-1-1-3.50][5949-3-3-3.15][5987-2-2-2.48][6014-3-3-2.03][6033-3-3-0.91][6313-0-3-3.67][6421-3-3-4.67][6500-1-4-2.87][6583-3-3-1.13]
[6683-3-3-3.04][6825-2-1-2.74][6998-3-3--0.24][7049-3-3-3.07][7517-1-1-1.97][7521-1-1-1.15][7528-1-3-4.02][7949-1-2-5.83][8135-1-0-1.08][8185-3-0-7.05]
[8269-3-4-0.28][8273-3-3-4.05][8543-3-0-7.96][8666-1-1-3.30][8672-0-0-6.54][8903-1-2-1.01][9001-2-4-1.92][9036-2-2-6.94][9281-3-3-1.00][9300-2-2-8.45]
[9571-0-4-0.60][9617-1-4-0.59][9644-2-2-1.37][9705-2-0-1.76][9801-0-3-2.60][9803-3-3-3.27][9865-3-3-8.38][9896-2-2-4.38][10314-1-1-0.56][10337-3-3-4.55]
[10403-0-0-1.74][10653-2-1-2.88][10704-2-2-3.92][10719-1-2-2.13][10727-1-4-1.15][10836-0-0-9.00][10969-2-3-3.91][11042-0-0-2.48][11088-1-1-3.01][11322-0-0-5.08]
[11398-2-2-1.74][11499-0-0-4.05][11502-3-3-2.90][11512-3-3-3.43][11608-1-1-6.08][11610-0-0-5.07][11692-0-0-2.62][11905-0-0-5.29][11993-1-1-8.60][12002-2-0-4.57]
[12052-0-0-4.89][12201-0-3-3.90][12235-2-2-5.29][12320-1-4-2.01][12377-2-2-3.07][12398-2-3-2.04][12503-1-2-1.53][12617-0-2-4.03][12685-3-4-0.54][12738-2-2-0.67]
[12742-2-2-7.84][12823-0-3-4.16][13110-1-1-1.46][13240-3-3-2.46][13253-1-1-4.11][13273-0-0-9.54][13634-1-1-4.41][13763-2-2-0.54][13905-3-0-0.23][14060-2-4-2.51]
[14065-3-0-2.03][14147-3-3-3.29][14595-2-1-2.86][14687-2-2-4.96][14788-2-2-3.68][14869-1-1-5.90][14872-3-3-1.20][14877-1-1-2.62][14927-0-3-1.34][15066-0-0-7.72]
[15175-1-4-2.17][15178-2-3-1.84][15375-3-3-2.97][15389-3-3-3.54][15568-2-1-2.20][15675-3-3-10.71][15869-1-0-1.04][16207-3-0-0.46][16236-0-0-2.10][16302-3-3-3.28]
[16331-2-2-6.01][16381-0-0-3.81][16488-1-1-5.07][16495-0-0-3.24][16650-0-0-7.79][16719-1-4-1.85][16801-0-0-6.66][16828-0-0-3.81][17137-3-3-1.34][17245-1-4-1.52]
[17278-3-0-0.41][17282-0-0-1.94][17311-2-4-0.83][17336-2-1-1.99][17608-3-3-3.93][17627-0-4-2.15][17877-3-0-3.06][17924-1-3-0.77][17984-3-3-5.40][18211-0-3-2.64]
[18276-3-3-2.23][18287-1-1-1.60][18394-0-0-6.99][18428-0-0-6.38][18442-0-3-4.53][18478-3-3-2.90][18607-0-0-4.42][18616-0-0-2.03][18663-0-0-3.43][18718-0-0-5.36]
[18766-2-2-7.36][18824-2-2-2.99][18890-3-3-3.17][18930-3-4-2.94][18938-3-3-3.05][19817-1-2-2.42][19839-0-0-0.63][19930-3-3-3.80][19944-0-3-1.13][20036-2-2-8.17]
[20101-3-3-4.67][20474-1-1-2.80][20547-3-3-1.56][20929-2-2-7.71][21245-1-2-2.19][21257-3-3-1.51][21293-1-2-7.17][21316-1-1-6.37][21384-1-1-1.19][21448-1-1-3.93]
[21483-0-0-4.88][21487-2-2-3.03][21714-0-0-0.98][21943-3-3-2.60][21947-0-0-5.62][21948-0-0-9.26][21965-2-2-6.15][21998-1-1-1.19][22025-0-2-2.34][22228-3-3-5.16]
[22446-1-1-1.66][22494-3-0-3.27][22757-0-0-6.00][22811-3-3-6.06][22976-3-4-0.80][22985-3-3-4.81][23014-0-3-4.78][23112-1-1-6.24][23144-3-3-4.37][23168-2-0-1.90]
[23219-0-0-3.31][23363-3-3-6.38][23470-0-0-2.19][23486-2-4-2.03][23497-0-3-7.31][23516-0-0-4.32][23690-1-4-0.07][23921-2-2-1.94][23936-1-2-1.85][24040-3-4-0.51]
[24111-1-1-2.87][24182-0-0-7.84][24238-3-3-3.66][24290-2-0-4.12][24345-0-0-2.38][24364-1-1-0.56][24427-3-0-4.02][24477-2-2-1.37][24495-2-4-0.99][24893-2-1-2.39]
[25012-1-2-0.17][25121-2-4-2.96][25165-3-3-2.11][25183-0-0-4.07][25297-3-3-4.74][25398-0-0-4.76][25574-2-2-3.09][25644-1-1-2.33][25718-1-4-1.92][25774-2-2-1.26]
[26032-3-3-4.01][26051-3-3-6.56][26120-0-0-3.48][26321-1-1-10.47][26732-1-1-3.93][26784-3-3-6.99][26827-3-3-2.77][26833-0-0-2.35][26838-2-2-0.06][26860-1-4-0.64]
[26948-0-0-2.72][27049-3-0-3.58][27098-1-3-0.65][27526-0-0-4.13][27639-3-3-2.91][27698-3-3-2.97][27772-0-0-4.89][27890-1-1-7.50][28040-0-4-2.11][28503-2-2-3.43]
[28577-1-1-1.46][28959-0-0-7.96][29198-3-3-3.41][29777-0-0-7.94][29877-2-3-1.68][30035-1-1-4.88][30098-0-3-3.94][30326-1-1-4.21][30572-2-2-3.99][30716-0-4-2.85]
[30806-2-3-1.97][30906-1-1-4.37][31007-0-0-2.50][31181-3-3-3.24][31238-0-0-2.94][31347-0-0-3.66][31422-2-2-2.23][31429-3-3-4.15][31431-0-0-6.10][31432-1-1-4.32]
[31477-0-0-5.13][31524-1-2-0.29][31597-1-4-1.29][31619-1-0-1.44][31701-0-0-6.01][31755-0-0-6.17][31854-3-3-4.18][32074-1-1-3.02][32078-3-3-6.22][32111-1-1-6.37]
[32127-1-2-1.39][32140-3-3-3.39][32263-2-0-1.66][32365-0-0-8.38][32411-2-0-5.85][32429-3-0-3.99][32473-3-0-2.10][32574-3-3-5.22][32584-0-0-2.40][32622-0-4-0.51]
[32858-3-3-2.90][32969-3-3-5.68][33016-2-2-3.57][33031-1-3-1.40][33035-2-2-6.30][33133-2-2-2.67][33173-2-2-2.28][33175-3-4-1.86][33306-3-3-4.53][33309-2-3-0.98]
[33474-0-0-1.15][33478-2-3-0.36][33618-1-1-3.04][33712-0-3-2.29][33782-2-2-2.47][33914-3-3-5.56][34076-3-3-3.38][34112-2-2-6.47][34138-2-3-2.92][34239-1-1-0.97]
[34364-2-2-5.30][34617-1-2-2.67][34751-3-3-4.06][34783-2-2-1.76][35015-3-3-2.32][35018-1-2-1.72][35288-2-2--0.09][0-4-4-3.28][1-4-4-3.29][2-4-4-2.64]
[3-4-4-3.02][4-4-1-0.78][5-4-1-1.17][6-4-4-3.07][7-4-4-1.51][8-4-4-0.27][9-4-4-2.67][10-4-4-4.69][11-4-4-4.50][12-4-4-2.15]
[14-4-4-2.41][15-4-3-2.28][16-4-4-1.69][17-4-4-1.36][18-4-4-5.62][19-4-0-3.57][20-4-0-2.60][21-4-4-1.25][22-4-4-0.57][23-4-4-1.59]
[24-4-4-6.61][25-4-4-3.39][26-4-4-1.14][27-4-0-1.72][28-4-4-4.63][29-4-1-1.74][30-4-3-0.87][31-4-4-2.93][32-4-4-3.11][33-4-2-4.59]
[34-4-4-2.12][35-4-4-3.85][37-4-4-3.04][39-4-0-5.23][40-4-4-1.44][41-4-0-0.81][42-4-4-1.37][43-4-4-1.32][45-4-1-2.60][46-4-4-4.51]
[47-4-4-3.90][48-4-4-1.98][51-4-4-2.97][52-4-0-2.71][53-4-4-1.55][54-4-3-1.36][55-4-2-0.18][56-4-2-1.15][57-4-3-3.88][58-4-2-5.41]
[59-4-0-4.44][60-4-4-3.32][61-4-4-2.97][62-4-4-2.72][63-4-4-2.18][64-4-2-1.49][65-4-4-3.87][66-4-4-2.46][67-4-2-1.46][68-4-3-2.57]
[69-4-4-1.42][70-4-4-3.04][72-4-2-2.63][73-4-1-3.53][74-4-2-3.44][75-4-0-1.84][77-4-4-3.23][78-4-0-0.14][79-4-2-5.03][80-4-4-3.85]
[81-4-2-3.29][82-4-1-1.94][83-4-4-1.91][84-4-4-4.35][85-4-4-4.84][86-4-0-0.91][87-4-4-2.63][88-4-4-2.37][89-4-4-1.57][90-4-4-0.75]
[91-4-4-0.94][92-4-0-1.40][93-4-0-3.50][94-4-4-2.32][95-4-4-0.74][96-4-4-2.31][97-4-4-5.81][98-4-2-1.91][99-4-4--0.10][100-4-2-4.28]
[101-4-4-6.03][102-4-4-2.67][103-4-0-0.93][104-4-4-1.32][105-4-4-1.35][106-4-4-2.93][107-4-0-1.84][108-4-4-1.96][109-4-4-2.37][110-4-4-1.46]
[111-4-0-6.58][112-4-3-1.13][113-4-4-2.12][114-4-4-0.65][115-4-4-0.97][116-4-4-0.01][117-4-4-5.20][119-4-4-2.16][121-4-4-2.80][122-4-4-4.28]
[124-4-1-1.41][125-4-4-2.45][126-4-4-4.38][127-4-2-1.30][128-4-2-0.92][129-4-4-2.90][130-4-4-1.47][131-4-2-1.92][132-4-4-0.47][133-4-4-4.06]
[135-4-4-2.97][136-4-1-1.01][137-4-4-2.56][138-4-4-3.58][139-4-4-2.88][140-4-1-1.54][141-4-3-1.30][142-4-4-1.81][143-4-4-5.64][144-4-4-3.62]
[145-4-4-2.90][148-4-0-4.13][149-4-4-2.66][150-4-4-4.95][151-4-4-3.37][152-4-4-3.99][153-4-2-3.08][154-4-4-5.07][155-4-4-4.55][156-4-3-3.78]
[157-4-0-2.18][158-4-4-2.21][160-4-4-0.95][161-4-2-6.71][162-4-4-1.49][164-4-4-1.91][165-4-4-1.95][167-4-0-2.39][168-4-0-1.72][170-4-4-3.53]
[171-4-4-3.34][172-4-4-5.77][173-4-4-5.59][174-4-0-1.08][175-4-4-3.42][177-4-4-4.78][178-4-4-1.18][179-4-4-4.07][180-4-4-2.78][181-4-4-2.31]
[182-4-2-1.47][183-4-4-4.26][184-4-2-3.05][186-4-4-1.29][187-4-2-4.64][188-4-4-2.12][189-4-2-2.82][190-4-2-1.96][191-4-4-4.48][192-4-4-2.55]
[193-4-2-2.72][194-4-4-1.30][195-4-3-0.18][196-4-2-3.66][197-4-4-1.68][198-4-4-4.93][199-4-4-1.72]
---------------------------
I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.100 | Acc: 92.625% | Wgt Acc: 97.907%
	I - Batch: 100 | Loss: 0.106 | Acc: 92.250% | Wgt Acc: 97.581%
	I - Batch: 150 | Loss: 0.112 | Acc: 91.708% | Wgt Acc: 97.420%
	I - Batch: 200 | Loss: 0.113 | Acc: 91.750% | Wgt Acc: 97.403%
	I - Batch: 250 | Loss: 0.117 | Acc: 91.600% | Wgt Acc: 97.280%
I - num batch: 285
I - Train -- Loss: 0.118 | Acc: 91.429% | Wgt Acc: 97.245% | LR: 1.250000e-04 | Dur: 175.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 799.    1.    1.    1.   96.]
 [   0.  775.    0.    0.   50.]
 [   0.    2. 1116.    1.  130.]
 [   0.    0.    0.  833.   87.]
 [   7.    2.    7.    5.  637.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.310 | Acc: 62.130% | Wgt Acc: 65.679% | Dur: 14.66s
I - Confusion Matrix: [row->prediction - col->label]
[[72.  3.  4. 11. 24.]
 [ 2. 42. 13.  3. 16.]
 [ 1. 13. 37.  0. 25.]
 [11. 10. 15. 68. 19.]
 [ 2. 10.  6.  4. 96.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.113 | Acc: 91.000% | Wgt Acc: 97.236%
	I - Batch: 100 | Loss: 0.127 | Acc: 91.000% | Wgt Acc: 96.817%
	I - Batch: 150 | Loss: 0.130 | Acc: 90.583% | Wgt Acc: 96.755%
	I - Batch: 200 | Loss: 0.126 | Acc: 91.125% | Wgt Acc: 96.932%
	I - Batch: 250 | Loss: 0.125 | Acc: 91.025% | Wgt Acc: 96.961%
I - num batch: 285
I - Train -- Loss: 0.125 | Acc: 90.945% | Wgt Acc: 96.894% | LR: 1.250000e-04 | Dur: 179.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    1.    0.    2.  102.]
 [   2.  774.    0.    1.   64.]
 [   2.    2. 1119.    0.  128.]
 [   1.    1.    1.  831.   80.]
 [  13.    2.    4.    6.  626.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.285 | Acc: 64.103% | Wgt Acc: 60.016% | Dur: 19.94s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   2.   5.  21.  16.]
 [  0.  33.   5.   2.   6.]
 [  0.  15.  35.   0.  17.]
 [ 10.  11.  13.  54.   7.]
 [  9.  17.  17.   9. 134.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.127 | Acc: 91.250% | Wgt Acc: 97.359%
	I - Batch: 100 | Loss: 0.128 | Acc: 90.812% | Wgt Acc: 97.034%
	I - Batch: 150 | Loss: 0.124 | Acc: 91.375% | Wgt Acc: 97.274%
	I - Batch: 200 | Loss: 0.123 | Acc: 91.219% | Wgt Acc: 97.218%
	I - Batch: 250 | Loss: 0.123 | Acc: 90.950% | Wgt Acc: 97.094%
I - num batch: 285
I - Train -- Loss: 0.122 | Acc: 91.099% | Wgt Acc: 97.143% | LR: 1.250000e-04 | Dur: 172.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    1.    3.  102.]
 [   0.  775.    0.    0.   48.]
 [   1.    1. 1121.    1.  123.]
 [   3.    0.    0.  834.  104.]
 [  10.    4.    2.    2.  623.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.172 | Acc: 62.525% | Wgt Acc: 62.473% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   1.   5.  15.  18.]
 [  0.  36.   6.   1.   8.]
 [  1.  15.  39.   2.  28.]
 [ 15.   8.  12.  62.  13.]
 [  5.  18.  13.   6. 113.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.100 | Acc: 92.375% | Wgt Acc: 97.712%
	I - Batch: 100 | Loss: 0.104 | Acc: 92.500% | Wgt Acc: 97.549%
	I - Batch: 150 | Loss: 0.112 | Acc: 92.167% | Wgt Acc: 97.278%
	I - Batch: 200 | Loss: 0.111 | Acc: 91.906% | Wgt Acc: 97.363%
	I - Batch: 250 | Loss: 0.114 | Acc: 91.775% | Wgt Acc: 97.228%
I - num batch: 285
I - Train -- Loss: 0.114 | Acc: 91.824% | Wgt Acc: 97.256% | LR: 1.250000e-04 | Dur: 178.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    4.   86.]
 [   0.  779.    0.    0.   45.]
 [   3.    0. 1119.    0.  124.]
 [   2.    1.    0.  827.   86.]
 [   7.    0.    5.    9.  659.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.107 | Acc: 64.103% | Wgt Acc: 61.781% | Dur: 15.80s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   4.  15.  15.]
 [  0.  36.   8.   1.   9.]
 [  3.  14.  40.   1.  26.]
 [ 10.   5.   8.  57.   4.]
 [  9.  21.  15.  12. 126.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.103 | Acc: 92.125% | Wgt Acc: 97.603%
	I - Batch: 100 | Loss: 0.110 | Acc: 92.438% | Wgt Acc: 97.388%
	I - Batch: 150 | Loss: 0.116 | Acc: 91.958% | Wgt Acc: 97.228%
	I - Batch: 200 | Loss: 0.115 | Acc: 92.094% | Wgt Acc: 97.338%
	I - Batch: 250 | Loss: 0.117 | Acc: 91.700% | Wgt Acc: 97.196%
I - num batch: 285
I - Train -- Loss: 0.116 | Acc: 91.626% | Wgt Acc: 97.194% | LR: 1.250000e-04 | Dur: 170.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    1.    0.    4.   95.]
 [   1.  779.    0.    1.   48.]
 [   2.    0. 1116.    0.  118.]
 [   2.    0.    0.  831.   88.]
 [   9.    0.    8.    4.  651.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.157 | Acc: 63.708% | Wgt Acc: 63.568% | Dur: 14.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   1.   5.  11.  15.]
 [  0.  37.   7.   3.  10.]
 [  3.  18.  39.   1.  24.]
 [ 15.   9.  13.  67.  15.]
 [  6.  13.  11.   4. 116.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.117 | Acc: 91.125% | Wgt Acc: 97.195%
	I - Batch: 100 | Loss: 0.108 | Acc: 91.875% | Wgt Acc: 97.384%
	I - Batch: 150 | Loss: 0.113 | Acc: 91.750% | Wgt Acc: 97.298%
	I - Batch: 200 | Loss: 0.114 | Acc: 91.812% | Wgt Acc: 97.349%
	I - Batch: 250 | Loss: 0.117 | Acc: 91.550% | Wgt Acc: 97.221%
I - num batch: 285
I - Train -- Loss: 0.117 | Acc: 91.429% | Wgt Acc: 97.143% | LR: 1.250000e-04 | Dur: 175.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    4.   99.]
 [   2.  776.    0.    0.   49.]
 [   1.    0. 1118.    0.  122.]
 [   2.    0.    0.  832.   88.]
 [   9.    4.    6.    4.  642.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.199 | Acc: 61.341% | Wgt Acc: 62.334% | Dur: 18.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   7.  14.  28.]
 [  0.  36.   7.   2.   8.]
 [  2.  17.  40.   2.  28.]
 [ 14.  10.  11.  62.  10.]
 [  5.  11.  10.   6. 106.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.103 | Acc: 92.625% | Wgt Acc: 97.762%
	I - Batch: 100 | Loss: 0.107 | Acc: 92.312% | Wgt Acc: 97.704%
	I - Batch: 150 | Loss: 0.106 | Acc: 92.458% | Wgt Acc: 97.739%
	I - Batch: 200 | Loss: 0.104 | Acc: 92.500% | Wgt Acc: 97.745%
	I - Batch: 250 | Loss: 0.105 | Acc: 92.425% | Wgt Acc: 97.746%
I - num batch: 285
I - Train -- Loss: 0.107 | Acc: 92.176% | Wgt Acc: 97.655% | LR: 1.250000e-04 | Dur: 171.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    0.    0.    0.   88.]
 [   0.  780.    0.    0.   50.]
 [   1.    0. 1117.    0.  128.]
 [   1.    0.    0.  838.   73.]
 [   6.    0.    7.    2.  661.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.185 | Acc: 62.130% | Wgt Acc: 61.735% | Dur: 14.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   5.  18.  21.]
 [  0.  38.   6.   0.   7.]
 [  2.  12.  35.   1.  24.]
 [ 14.  10.  16.  61.  14.]
 [  5.  15.  13.   6. 114.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840.  781.]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.114 | Acc: 91.750% | Wgt Acc: 97.095%
	I - Batch: 100 | Loss: 0.102 | Acc: 93.312% | Wgt Acc: 97.722%
	I - Batch: 150 | Loss: 0.105 | Acc: 92.875% | Wgt Acc: 97.664%
	I - Batch: 200 | Loss: 0.109 | Acc: 92.875% | Wgt Acc: 97.619%
	I - Batch: 250 | Loss: 0.110 | Acc: 92.575% | Wgt Acc: 97.509%
I - num batch: 271
I - Train -- Loss: 0.109 | Acc: 92.611% | Wgt Acc: 97.530% | LR: 1.250000e-04 | Dur: 164.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    3.    3.   80.]
 [   1.  778.    5.    0.   34.]
 [   4.    2. 1113.    0.  103.]
 [   0.    0.    1.  833.   72.]
 [   6.    0.    2.    4.  492.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.189 | Acc: 62.525% | Wgt Acc: 64.110% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 75.   2.   8.  18.  24.]
 [  0.  36.   4.   3.  11.]
 [  0.  16.  40.   1.  26.]
 [  9.  10.  14.  61.  14.]
 [  4.  14.   9.   3. 105.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.122 | Acc: 90.250% | Wgt Acc: 96.620%
	I - Batch: 100 | Loss: 0.116 | Acc: 90.500% | Wgt Acc: 96.776%
	I - Batch: 150 | Loss: 0.115 | Acc: 90.708% | Wgt Acc: 96.791%
	I - Batch: 200 | Loss: 0.115 | Acc: 90.969% | Wgt Acc: 96.936%
	I - Batch: 250 | Loss: 0.113 | Acc: 91.175% | Wgt Acc: 96.982%
I - num batch: 285
I - Train -- Loss: 0.112 | Acc: 91.363% | Wgt Acc: 97.108% | LR: 1.250000e-04 | Dur: 176.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    0.    3.    1.   94.]
 [   0.  770.    1.    0.   49.]
 [   3.    0. 1113.    0.  128.]
 [   1.    0.    0.  836.   89.]
 [   4.   10.    7.    3.  640.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.115 | Acc: 64.694% | Wgt Acc: 65.736% | Dur: 14.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   5.  12.  16.]
 [  0.  40.   8.   2.   8.]
 [  3.  14.  43.   1.  30.]
 [ 15.   9.  12.  66.  14.]
 [  3.  12.   7.   5. 112.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.108 | Acc: 92.750% | Wgt Acc: 97.762%
	I - Batch: 100 | Loss: 0.111 | Acc: 92.438% | Wgt Acc: 97.600%
	I - Batch: 150 | Loss: 0.116 | Acc: 91.958% | Wgt Acc: 97.336%
	I - Batch: 200 | Loss: 0.113 | Acc: 92.594% | Wgt Acc: 97.587%
	I - Batch: 250 | Loss: 0.111 | Acc: 92.675% | Wgt Acc: 97.698%
I - num batch: 285
I - Train -- Loss: 0.112 | Acc: 92.615% | Wgt Acc: 97.593% | LR: 1.250000e-04 | Dur: 172.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    2.    3.   70.]
 [   1.  777.    1.    0.   46.]
 [   2.    0. 1117.    1.  111.]
 [   1.    0.    1.  834.   84.]
 [   5.    3.    3.    2.  689.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.258 | Acc: 61.736% | Wgt Acc: 65.067% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  7.  6. 13. 32.]
 [ 0. 35.  4.  1.  7.]
 [ 1. 14. 43.  1. 18.]
 [14. 11. 15. 68. 27.]
 [ 2. 11.  7.  3. 96.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.108 | Acc: 92.000% | Wgt Acc: 97.921%
	I - Batch: 100 | Loss: 0.110 | Acc: 91.812% | Wgt Acc: 97.341%
	I - Batch: 150 | Loss: 0.114 | Acc: 91.625% | Wgt Acc: 97.225%
	I - Batch: 200 | Loss: 0.114 | Acc: 91.625% | Wgt Acc: 97.244%
	I - Batch: 250 | Loss: 0.110 | Acc: 91.800% | Wgt Acc: 97.400%
I - num batch: 285
I - Train -- Loss: 0.112 | Acc: 91.736% | Wgt Acc: 97.345% | LR: 1.250000e-04 | Dur: 183.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    1.    0.  104.]
 [   0.  776.    0.    3.   44.]
 [   3.    1. 1118.    1.  117.]
 [   2.    0.    0.  833.   85.]
 [   4.    3.    5.    3.  650.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.200 | Acc: 64.892% | Wgt Acc: 65.067% | Dur: 15.63s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   3.   8.   9.]
 [  0.  43.   8.   3.  10.]
 [  3.  17.  44.   0.  28.]
 [ 22.   5.   9.  69.  16.]
 [  7.  11.  11.   6. 117.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.115 | Acc: 89.625% | Wgt Acc: 96.460%
	I - Batch: 100 | Loss: 0.107 | Acc: 91.250% | Wgt Acc: 97.195%
	I - Batch: 150 | Loss: 0.108 | Acc: 91.500% | Wgt Acc: 97.353%
	I - Batch: 200 | Loss: 0.109 | Acc: 91.500% | Wgt Acc: 97.402%
	I - Batch: 250 | Loss: 0.108 | Acc: 91.825% | Wgt Acc: 97.481%
I - num batch: 285
I - Train -- Loss: 0.107 | Acc: 91.956% | Wgt Acc: 97.487% | LR: 1.250000e-04 | Dur: 172.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 799.    0.    0.    0.   88.]
 [   1.  777.    0.    1.   47.]
 [   3.    0. 1117.    1.  132.]
 [   1.    0.    1.  835.   77.]
 [   2.    3.    6.    3.  656.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 66.272% | Wgt Acc: 63.199% | Dur: 14.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   4.  13.   8.]
 [  0.  40.   8.   1.   6.]
 [  2.  12.  37.   0.  25.]
 [ 16.   7.  14.  62.   7.]
 [  7.  17.  12.  10. 134.]]

I - Local maximum validation set accuracy:  66.27

I - Validation set results: 
[14-1-2-2.67][50-3-4-0.91][124-2-2-0.88][127-0-0-8.48][443-2-4-4.57][567-0-0-3.24][573-1-1-6.64][615-0-0-2.15][695-1-2-2.96][722-3-0-3.89]
[826-0-0-4.70][878-0-3-3.82][1103-0-4-1.36][1212-3-4-1.71][1368-0-0-5.08][2181-2-3-1.06][2476-2-2-0.62][2721-2-2-3.17][2818-1-4-1.06][2886-2-1-2.79]
[3231-2-2-5.05][3333-2-1-0.42][3482-2-2-4.24][3536-3-3--0.03][3625-1-1-3.83][3909-0-3-1.44][4035-0-0-2.95][4140-0-0-3.14][4214-1-1-1.77][4346-1-4-2.75]
[4581-2-2-2.96][4708-3-3-4.14][4838-3-3-1.72][4845-1-3-1.64][4868-0-0-4.19][4939-0-0-0.77][4984-2-3-2.98][5078-1-2-2.63][5396-0-0-4.54][5479-1-1-6.86]
[5717-0-0-1.40][5843-1-4-0.46][5949-3-3-3.27][5987-2-4-3.07][6014-3-3-1.81][6033-3-4-1.49][6313-0-3-3.54][6421-3-3-3.08][6500-1-4-1.77][6583-3-3-0.60]
[6683-3-3-3.14][6825-2-1-3.79][6998-3-3-0.22][7049-3-3-3.48][7517-1-1-0.47][7521-1-1--0.32][7528-1-3-5.13][7949-1-2-4.45][8135-1-4-0.36][8185-3-0-5.25]
[8269-3-1-5.18][8273-3-3-3.11][8543-3-0-6.04][8666-1-1-3.96][8672-0-0-3.96][8903-1-3-3.05][9001-2-2-0.19][9036-2-2-5.22][9281-3-0-1.57][9300-2-2-9.28]
[9571-0-4-0.72][9617-1-1-3.61][9644-2-3--0.56][9705-2-1--0.07][9801-0-0-3.67][9803-3-3-2.56][9865-3-3-9.30][9896-2-2-2.64][10314-1-4-0.47][10337-3-3-7.04]
[10403-0-4-1.10][10653-2-1-4.62][10704-2-2-2.54][10719-1-2-2.22][10727-1-4-1.34][10836-0-0-9.17][10969-2-3-4.95][11042-0-0-1.82][11088-1-1-7.15][11322-0-0-6.89]
[11398-2-4-2.31][11499-0-0-3.32][11502-3-0-0.68][11512-3-3-1.87][11608-1-1-5.15][11610-0-0-6.31][11692-0-3-3.33][11905-0-0-3.43][11993-1-1-7.81][12002-2-0-1.90]
[12052-0-0-4.88][12201-0-3-4.22][12235-2-2-2.99][12320-1-4-1.88][12377-2-4-3.51][12398-2-3-1.28][12503-1-2-1.02][12617-0-2-0.75][12685-3-4-0.28][12738-2-3-2.32]
[12742-2-2-7.33][12823-0-0-5.08][13110-1-1-2.53][13240-3-3-2.53][13253-1-1-2.06][13273-0-0-9.22][13634-1-1-3.84][13763-2-3-0.36][13905-3-4--0.10][14060-2-4-1.90]
[14065-3-3-1.03][14147-3-3-2.87][14595-2-2-2.30][14687-2-2-3.77][14788-2-2-3.61][14869-1-1-3.58][14872-3-4-1.29][14877-1-1-5.58][14927-0-3-0.81][15066-0-0-6.83]
[15175-1-1-5.91][15178-2-3-0.66][15375-3-3--0.62][15389-3-3-2.92][15568-2-4-2.03][15675-3-3-8.11][15869-1-0-2.26][16207-3-0-0.46][16236-0-0-2.81][16302-3-3-4.38]
[16331-2-2-6.47][16381-0-3-2.87][16488-1-1-7.63][16495-0-0-4.29][16650-0-0-7.44][16719-1-4-2.01][16801-0-0-6.36][16828-0-0-5.42][17137-3-3-2.01][17245-1-4-1.05]
[17278-3-0-2.37][17282-0-0-2.07][17311-2-4-0.99][17336-2-4-0.02][17608-3-3-5.61][17627-0-4-1.81][17877-3-4-3.42][17924-1-3-3.10][17984-3-3-4.03][18211-0-0-2.40]
[18276-3-3-2.62][18287-1-1-2.40][18394-0-0-5.99][18428-0-3-2.30][18442-0-3-3.10][18478-3-3-2.48][18607-0-0-5.11][18616-0-0-2.92][18663-0-0-4.06][18718-0-0-3.71]
[18766-2-2-4.40][18824-2-2-2.68][18890-3-3-3.27][18930-3-4-3.31][18938-3-3-2.87][19817-1-2-2.16][19839-0-2-0.40][19930-3-3-3.19][19944-0-3-3.80][20036-2-2-6.48]
[20101-3-3-4.45][20474-1-1-3.14][20547-3-0-2.16][20929-2-2-3.87][21245-1-1-1.98][21257-3-3-1.54][21293-1-2-5.71][21316-1-1-4.65][21384-1-3-1.30][21448-1-1-3.27]
[21483-0-0-4.31][21487-2-2-2.82][21714-0-3-1.28][21943-3-3-2.44][21947-0-0-2.43][21948-0-0-8.63][21965-2-2-5.34][21998-1-1-7.25][22025-0-4-3.24][22228-3-3-4.22]
[22446-1-1-4.16][22494-3-0-3.04][22757-0-0-6.77][22811-3-3-4.14][22976-3-3-0.86][22985-3-3-4.20][23014-0-3-3.49][23112-1-1-6.81][23144-3-3-4.14][23168-2-3-2.11]
[23219-0-0-3.43][23363-3-3-6.23][23470-0-0-3.08][23486-2-2-0.75][23497-0-3-7.41][23516-0-0-4.15][23690-1-4-1.81][23921-2-2-2.09][23936-1-0-1.06][24040-3-4--0.14]
[24111-1-1-2.96][24182-0-0-7.72][24238-3-3-3.74][24290-2-0-3.35][24345-0-0-2.87][24364-1-2-0.21][24427-3-0-2.77][24477-2-2-2.98][24495-2-2-1.22][24893-2-1-2.16]
[25012-1-4-0.15][25121-2-4-3.86][25165-3-3-3.04][25183-0-0-5.91][25297-3-3-4.88][25398-0-0-6.04][25574-2-2-3.21][25644-1-1-2.21][25718-1-3-0.45][25774-2-4-1.78]
[26032-3-3-4.62][26051-3-3-5.66][26120-0-0-2.62][26321-1-1-8.81][26732-1-1-4.29][26784-3-3-6.54][26827-3-0-3.88][26833-0-0-2.91][26838-2-3-0.55][26860-1-1-2.56]
[26948-0-0-4.79][27049-3-0-2.78][27098-1-1-0.91][27526-0-0-4.51][27639-3-3-3.34][27698-3-3-1.34][27772-0-0-2.68][27890-1-1-4.59][28040-0-0-0.68][28503-2-2-3.51]
[28577-1-1-0.88][28959-0-0-7.73][29198-3-3-3.03][29777-0-0-5.85][29877-2-2-0.64][30035-1-1-6.65][30098-0-3-2.07][30326-1-1-3.76][30572-2-2-1.75][30716-0-4-3.49]
[30806-2-3-2.95][30906-1-1-3.60][31007-0-0-0.34][31181-3-3-2.85][31238-0-0-4.31][31347-0-0-2.42][31422-2-2-2.74][31429-3-3-3.70][31431-0-0--0.18][31432-1-1-4.40]
[31477-0-0-5.47][31524-1-4-0.19][31597-1-4-1.37][31619-1-2-5.61][31701-0-0-5.01][31755-0-0-5.87][31854-3-3-4.57][32074-1-4-1.27][32078-3-3-5.41][32111-1-1-4.80]
[32127-1-4-2.30][32140-3-3-3.71][32263-2-0-0.70][32365-0-0-5.85][32411-2-3-4.29][32429-3-3-3.08][32473-3-0-2.09][32574-3-3-4.36][32584-0-3-2.03][32622-0-4-1.71]
[32858-3-3-2.89][32969-3-3-4.26][33016-2-2-5.70][33031-1-3-1.32][33035-2-2-3.90][33133-2-1-1.39][33173-2-1-0.58][33175-3-4-1.52][33306-3-3-5.96][33309-2-3-1.55]
[33474-0-0-1.12][33478-2-0-1.23][33618-1-4-1.43][33712-0-3-1.55][33782-2-4-2.89][33914-3-3-4.40][34076-3-3-2.25][34112-2-2-1.95][34138-2-3-4.32][34239-1-1-2.43]
[34364-2-2-6.03][34617-1-2-3.07][34751-3-3-2.88][34783-2-2-1.28][35015-3-3-1.78][35018-1-2-1.84][35288-2-4--0.17][0-4-2-2.47][1-4-4-2.16][2-4-4-2.66]
[3-4-4-3.49][4-4-4-3.43][5-4-1-1.28][6-4-4-5.25][7-4-4-2.57][8-4-2-0.20][9-4-4-2.96][10-4-4-5.43][11-4-4-3.48][12-4-4-2.75]
[14-4-4-1.50][15-4-3-0.91][16-4-4-3.77][17-4-4-1.35][18-4-4-5.65][19-4-0-4.34][20-4-4-1.66][21-4-4-1.87][22-4-4-1.05][23-4-4-3.44]
[24-4-4-6.24][25-4-4-2.94][26-4-4-1.47][27-4-4-3.27][28-4-4-4.35][29-4-1-2.01][30-4-4-2.08][31-4-4-2.78][32-4-4-4.41][33-4-2-3.67]
[34-4-4-1.98][35-4-4-3.82][37-4-4-2.47][39-4-0-3.33][40-4-4-1.08][41-4-4-0.36][42-4-2-1.56][43-4-4-2.19][45-4-4-2.43][46-4-4-4.27]
[47-4-4-5.10][48-4-4-3.11][51-4-4-4.24][52-4-4-1.08][53-4-2-0.91][54-4-4-1.33][55-4-4-1.49][56-4-1-1.56][57-4-3-1.78][58-4-2-3.27]
[59-4-4-3.89][60-4-4-4.09][61-4-4-3.67][62-4-2-2.83][63-4-4-2.34][64-4-4-1.71][65-4-4-4.72][66-4-4-3.40][67-4-3-1.68][68-4-3-1.24]
[69-4-4-0.67][70-4-4-3.06][72-4-1-1.17][73-4-1-3.92][74-4-2-1.05][75-4-0-2.24][77-4-4-5.41][78-4-4-0.65][79-4-2-3.95][80-4-4-5.28]
[81-4-2-1.56][82-4-4-1.29][83-4-4-2.10][84-4-4-5.04][85-4-4-5.63][86-4-2-2.04][87-4-4-5.73][88-4-4-3.36][89-4-4-1.92][90-4-4-1.07]
[91-4-4-1.24][92-4-4--0.02][93-4-0-2.20][94-4-4-2.45][95-4-4-1.51][96-4-4-2.24][97-4-4-3.49][98-4-2-2.15][99-4-4-0.66][100-4-2-5.14]
[101-4-4-6.49][102-4-4-1.89][103-4-2-1.73][104-4-4-1.94][105-4-4-2.06][106-4-4-4.23][107-4-4-2.16][108-4-4-0.04][109-4-4-2.11][110-4-1-2.54]
[111-4-0-6.51][112-4-4-1.68][113-4-4-2.63][114-4-4-1.02][115-4-4-2.93][116-4-4-1.39][117-4-4-5.10][119-4-4-1.92][121-4-4-3.31][122-4-4-4.61]
[124-4-4-2.08][125-4-4-4.14][126-4-4-7.33][127-4-2-1.67][128-4-2-0.73][129-4-4-3.93][130-4-4-1.47][131-4-4-1.53][132-4-4-0.32][133-4-4-5.47]
[135-4-4-3.08][136-4-2-1.50][137-4-4-2.96][138-4-4-2.10][139-4-4-2.94][140-4-2-0.88][141-4-0-2.00][142-4-4-3.81][143-4-4-5.99][144-4-4-4.23]
[145-4-4-3.19][148-4-0-3.20][149-4-4-0.93][150-4-4-5.83][151-4-4-4.76][152-4-4-5.06][153-4-4-3.04][154-4-4-7.00][155-4-4-6.00][156-4-3-1.10]
[157-4-0-1.26][158-4-4-1.93][160-4-4-0.68][161-4-2-4.43][162-4-4-1.39][164-4-4-2.24][165-4-4-2.64][167-4-4-2.42][168-4-4-1.65][170-4-4-1.51]
[171-4-4-2.46][172-4-4-4.76][173-4-4-6.09][174-4-4--0.08][175-4-4-3.75][177-4-4-4.55][178-4-4-1.36][179-4-4-4.12][180-4-4-4.15][181-4-3-1.96]
[182-4-3-1.52][183-4-4-5.05][184-4-2-2.64][186-4-4-0.52][187-4-2-3.04][188-4-4-2.02][189-4-2-3.05][190-4-2-1.18][191-4-4-4.48][192-4-4-2.30]
[193-4-4-0.65][194-4-4-1.12][195-4-2-0.20][196-4-2-3.28][197-4-4-1.85][198-4-4-5.98][199-4-4-1.55]
---------------------------
I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.107 | Acc: 92.000% | Wgt Acc: 97.728%
	I - Batch: 100 | Loss: 0.108 | Acc: 91.688% | Wgt Acc: 97.527%
	I - Batch: 150 | Loss: 0.110 | Acc: 92.000% | Wgt Acc: 97.539%
	I - Batch: 200 | Loss: 0.110 | Acc: 92.281% | Wgt Acc: 97.592%
	I - Batch: 250 | Loss: 0.109 | Acc: 92.225% | Wgt Acc: 97.570%
I - num batch: 285
I - Train -- Loss: 0.108 | Acc: 92.374% | Wgt Acc: 97.598% | LR: 1.250000e-04 | Dur: 185.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    1.    0.    3.  102.]
 [   1.  777.    1.    0.   43.]
 [   3.    2. 1119.    0.  117.]
 [   0.    0.    2.  837.   63.]
 [   7.    0.    2.    0.  675.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.244 | Acc: 62.919% | Wgt Acc: 62.346% | Dur: 15.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   6.  15.  16.]
 [  0.  37.   7.   2.   7.]
 [  1.  17.  40.   0.  24.]
 [ 15.   8.  13.  60.  17.]
 [  6.  12.   9.   9. 116.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.110 | Acc: 91.250% | Wgt Acc: 97.274%
	I - Batch: 100 | Loss: 0.105 | Acc: 92.188% | Wgt Acc: 97.381%
	I - Batch: 150 | Loss: 0.109 | Acc: 91.875% | Wgt Acc: 97.378%
	I - Batch: 200 | Loss: 0.109 | Acc: 91.906% | Wgt Acc: 97.382%
	I - Batch: 250 | Loss: 0.108 | Acc: 92.100% | Wgt Acc: 97.499%
I - num batch: 285
I - Train -- Loss: 0.107 | Acc: 92.088% | Wgt Acc: 97.472% | LR: 1.250000e-04 | Dur: 178.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    1.    0.    3.   88.]
 [   0.  777.    0.    1.   44.]
 [   1.    0. 1122.    2.  113.]
 [   1.    0.    0.  831.   91.]
 [   8.    2.    2.    3.  664.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 66.272% | Wgt Acc: 64.906% | Dur: 17.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   0.   6.  16.  13.]
 [  0.  38.   3.   1.   6.]
 [  0.  23.  49.   3.  29.]
 [ 14.   5.   9.  56.   7.]
 [  6.  12.   8.  10. 125.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.102 | Acc: 92.875% | Wgt Acc: 97.888%
	I - Batch: 100 | Loss: 0.103 | Acc: 92.438% | Wgt Acc: 97.646%
	I - Batch: 150 | Loss: 0.100 | Acc: 92.167% | Wgt Acc: 97.560%
	I - Batch: 200 | Loss: 0.104 | Acc: 92.281% | Wgt Acc: 97.560%
	I - Batch: 250 | Loss: 0.106 | Acc: 92.500% | Wgt Acc: 97.533%
I - num batch: 285
I - Train -- Loss: 0.106 | Acc: 92.484% | Wgt Acc: 97.535% | LR: 1.250000e-04 | Dur: 175.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    0.    0.    3.   87.]
 [   0.  777.    0.    0.   37.]
 [   1.    0. 1117.    2.  122.]
 [   3.    1.    1.  832.   70.]
 [   4.    2.    6.    3.  684.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.226 | Acc: 65.680% | Wgt Acc: 64.295% | Dur: 15.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   0.   3.  11.  14.]
 [  0.  36.   7.   1.   5.]
 [  2.  18.  38.   0.  24.]
 [ 12.  10.  14.  65.  12.]
 [  5.  14.  13.   9. 125.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.093 | Acc: 93.875% | Wgt Acc: 98.357%
	I - Batch: 100 | Loss: 0.095 | Acc: 93.375% | Wgt Acc: 98.036%
	I - Batch: 150 | Loss: 0.096 | Acc: 93.042% | Wgt Acc: 97.952%
	I - Batch: 200 | Loss: 0.099 | Acc: 92.656% | Wgt Acc: 97.739%
	I - Batch: 250 | Loss: 0.101 | Acc: 92.525% | Wgt Acc: 97.657%
I - num batch: 285
I - Train -- Loss: 0.101 | Acc: 92.681% | Wgt Acc: 97.711% | LR: 1.250000e-04 | Dur: 187.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    0.    0.    0.   76.]
 [   0.  779.    0.    1.   51.]
 [   1.    0. 1120.    1.  106.]
 [   0.    0.    0.  833.   80.]
 [   7.    1.    4.    5.  687.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.250 | Acc: 64.497% | Wgt Acc: 63.211% | Dur: 19.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   5.  12.   8.]
 [  0.  30.   4.   0.   5.]
 [  1.  21.  46.   3.  38.]
 [ 13.   5.  12.  63.   7.]
 [  8.  20.   8.   8. 122.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.097 | Acc: 93.250% | Wgt Acc: 98.043%
	I - Batch: 100 | Loss: 0.096 | Acc: 93.625% | Wgt Acc: 97.967%
	I - Batch: 150 | Loss: 0.100 | Acc: 93.125% | Wgt Acc: 97.685%
	I - Batch: 200 | Loss: 0.100 | Acc: 93.219% | Wgt Acc: 97.732%
	I - Batch: 250 | Loss: 0.105 | Acc: 92.750% | Wgt Acc: 97.530%
I - num batch: 285
I - Train -- Loss: 0.105 | Acc: 92.835% | Wgt Acc: 97.564% | LR: 1.250000e-04 | Dur: 179.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    3.    3.   70.]
 [   1.  777.    0.    0.   41.]
 [   3.    0. 1117.    1.  116.]
 [   0.    0.    2.  832.   70.]
 [   7.    3.    2.    4.  703.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.215 | Acc: 66.272% | Wgt Acc: 63.672% | Dur: 15.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   2.   5.  18.  17.]
 [  0.  31.   4.   1.   2.]
 [  3.  16.  43.   0.  24.]
 [  9.   6.  10.  60.   6.]
 [  5.  23.  13.   7. 131.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.104 | Acc: 91.750% | Wgt Acc: 97.302%
	I - Batch: 100 | Loss: 0.098 | Acc: 92.875% | Wgt Acc: 97.691%
	I - Batch: 150 | Loss: 0.096 | Acc: 92.833% | Wgt Acc: 97.714%
	I - Batch: 200 | Loss: 0.101 | Acc: 92.750% | Wgt Acc: 97.573%
	I - Batch: 250 | Loss: 0.101 | Acc: 92.575% | Wgt Acc: 97.522%
I - num batch: 285
I - Train -- Loss: 0.102 | Acc: 92.505% | Wgt Acc: 97.515% | LR: 1.250000e-04 | Dur: 175.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 801.    0.    2.    0.   82.]
 [   0.  775.    0.    1.   46.]
 [   2.    1. 1116.    0.  111.]
 [   0.    0.    0.  831.   75.]
 [   3.    4.    6.    8.  686.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.177 | Acc: 65.878% | Wgt Acc: 64.283% | Dur: 14.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   3.   6.  14.  22.]
 [  0.  34.   7.   0.   4.]
 [  0.  12.  39.   0.  18.]
 [ 10.   8.  11.  63.  10.]
 [  6.  21.  12.   9. 126.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.090 | Acc: 93.000% | Wgt Acc: 97.998%
	I - Batch: 100 | Loss: 0.094 | Acc: 93.062% | Wgt Acc: 97.884%
	I - Batch: 150 | Loss: 0.092 | Acc: 93.042% | Wgt Acc: 97.922%
	I - Batch: 200 | Loss: 0.093 | Acc: 93.188% | Wgt Acc: 97.907%
	I - Batch: 250 | Loss: 0.097 | Acc: 92.950% | Wgt Acc: 97.815%
I - num batch: 285
I - Train -- Loss: 0.096 | Acc: 93.099% | Wgt Acc: 97.813% | LR: 1.250000e-04 | Dur: 181.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    0.    0.    5.   78.]
 [   0.  779.    0.    0.   42.]
 [   1.    0. 1122.    0.  100.]
 [   2.    0.    0.  831.   74.]
 [   5.    1.    2.    4.  706.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.181 | Acc: 66.469% | Wgt Acc: 63.303% | Dur: 17.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   1.   1.   6.   7.]
 [  0.  40.   6.   2.   4.]
 [  2.   8.  32.   0.  15.]
 [ 21.   9.  18.  73.  18.]
 [  9.  20.  18.   5. 136.]]

I - Local maximum validation set accuracy:  66.47

I - Validation set results: 
[14-1-2-0.96][50-3-4-3.32][124-2-4-2.74][127-0-0-5.95][443-2-4-3.76][567-0-0-2.47][573-1-1-3.34][615-0-0-3.03][695-1-2-2.43][722-3-3-3.23]
[826-0-0-3.35][878-0-0-5.49][1103-0-4-2.16][1212-3-3-1.55][1368-0-0-5.14][2181-2-3-3.06][2476-2-2--0.56][2721-2-2-4.20][2818-1-3-1.89][2886-2-1-1.09]
[3231-2-2-2.84][3333-2-2-0.53][3482-2-2-3.60][3536-3-3-1.59][3625-1-1-2.33][3909-0-3-2.84][4035-0-0-1.27][4140-0-0-3.52][4214-1-1-2.64][4346-1-4-3.31]
[4581-2-2-1.21][4708-3-3-5.55][4838-3-4-1.58][4845-1-3-1.38][4868-0-0-5.39][4939-0-0-0.67][4984-2-3-2.39][5078-1-4-0.08][5396-0-0-5.05][5479-1-1-7.17]
[5717-0-0-3.26][5843-1-4-1.53][5949-3-3-3.19][5987-2-4-3.28][6014-3-3-4.45][6033-3-0-1.98][6313-0-3-5.11][6421-3-3-5.70][6500-1-4-0.97][6583-3-3-0.99]
[6683-3-3-3.67][6825-2-1-3.35][6998-3-3-0.94][7049-3-3-4.07][7517-1-1-2.78][7521-1-1-2.76][7528-1-3-4.27][7949-1-2-4.12][8135-1-1-0.19][8185-3-3-5.21]
[8269-3-1-5.83][8273-3-3-5.26][8543-3-0-5.95][8666-1-1-3.55][8672-0-0-4.72][8903-1-3-2.30][9001-2-4-1.83][9036-2-2-3.17][9281-3-3-0.97][9300-2-2-9.50]
[9571-0-3-1.47][9617-1-1-2.86][9644-2-4-1.60][9705-2-2--0.89][9801-0-3-3.25][9803-3-3-3.17][9865-3-3-9.30][9896-2-2-2.19][10314-1-4-1.85][10337-3-3-6.21]
[10403-0-4-2.24][10653-2-4-0.29][10704-2-2-2.13][10719-1-4-2.14][10727-1-4-2.00][10836-0-0-5.77][10969-2-3-6.32][11042-0-0-2.09][11088-1-1-3.94][11322-0-0-3.37]
[11398-2-4-1.50][11499-0-0-4.51][11502-3-3-0.92][11512-3-3-3.50][11608-1-1-3.40][11610-0-0-3.45][11692-0-3-2.15][11905-0-0-3.70][11993-1-1-6.17][12002-2-3-1.65]
[12052-0-0-2.49][12201-0-3-5.55][12235-2-2-5.47][12320-1-4-4.27][12377-2-4-3.25][12398-2-3-1.15][12503-1-1-1.65][12617-0-2-0.04][12685-3-3-2.24][12738-2-3-1.95]
[12742-2-2-6.79][12823-0-3-3.45][13110-1-1-1.34][13240-3-3-3.48][13253-1-1-2.11][13273-0-0-8.03][13634-1-1-3.41][13763-2-3-1.46][13905-3-3-0.01][14060-2-4-1.89]
[14065-3-3-2.04][14147-3-3-6.12][14595-2-2-2.05][14687-2-2-3.91][14788-2-4-2.36][14869-1-4-3.45][14872-3-3-0.50][14877-1-1-4.37][14927-0-3-2.74][15066-0-0-5.03]
[15175-1-1-3.63][15178-2-3-0.94][15375-3-3-1.41][15389-3-3-4.32][15568-2-4-2.69][15675-3-3-8.33][15869-1-0-0.48][16207-3-1--0.02][16236-0-0-0.47][16302-3-3-4.75]
[16331-2-2-5.78][16381-0-0-3.80][16488-1-1-4.08][16495-0-0-4.85][16650-0-0-6.04][16719-1-4-2.64][16801-0-0-2.23][16828-0-0-2.64][17137-3-3-2.94][17245-1-4-3.76]
[17278-3-0-0.26][17282-0-0-5.30][17311-2-2-1.28][17336-2-3-1.05][17608-3-3-5.95][17627-0-4-1.61][17877-3-4-2.54][17924-1-3-3.21][17984-3-3-6.10][18211-0-0-2.71]
[18276-3-3-5.32][18287-1-4-1.78][18394-0-0-4.91][18428-0-3-2.84][18442-0-3-5.48][18478-3-3-3.17][18607-0-0-4.02][18616-0-4-2.02][18663-0-0-2.83][18718-0-0-3.33]
[18766-2-2-3.32][18824-2-4-2.94][18890-3-3-3.24][18930-3-4-3.85][18938-3-3-3.60][19817-1-2-2.34][19839-0-2-0.63][19930-3-3-4.79][19944-0-3-5.00][20036-2-2-7.93]
[20101-3-3-5.26][20474-1-1-3.41][20547-3-0-1.84][20929-2-2-6.41][21245-1-3-1.42][21257-3-3-2.99][21293-1-2-6.15][21316-1-1-7.62][21384-1-4-3.68][21448-1-1-3.15]
[21483-0-0-2.06][21487-2-2-1.71][21714-0-0-0.74][21943-3-3-3.31][21947-0-0-3.15][21948-0-0-6.58][21965-2-1-1.45][21998-1-1-2.57][22025-0-4-3.23][22228-3-3-6.86]
[22446-1-1-3.23][22494-3-3-3.23][22757-0-0-4.07][22811-3-3-7.34][22976-3-3-1.27][22985-3-3-6.02][23014-0-3-4.53][23112-1-1-4.08][23144-3-3-9.40][23168-2-3-1.48]
[23219-0-0-3.94][23363-3-3-5.82][23470-0-3--0.01][23486-2-4-1.45][23497-0-3-5.96][23516-0-0-2.65][23690-1-4-2.88][23921-2-2-1.50][23936-1-2-0.12][24040-3-3-0.62]
[24111-1-4-2.70][24182-0-0-5.62][24238-3-3-4.59][24290-2-0-3.86][24345-0-0-0.79][24364-1-1-0.32][24427-3-0-2.80][24477-2-2-2.19][24495-2-4-1.63][24893-2-1-0.75]
[25012-1-4-3.09][25121-2-4-6.18][25165-3-3-3.39][25183-0-0-2.98][25297-3-3-6.06][25398-0-0-6.18][25574-2-2-0.97][25644-1-4-1.27][25718-1-3-1.28][25774-2-2-2.32]
[26032-3-3-6.15][26051-3-3-8.67][26120-0-4-4.60][26321-1-1-0.30][26732-1-1-4.14][26784-3-3-8.13][26827-3-3-3.99][26833-0-3-1.88][26838-2-3-0.27][26860-1-1-1.05]
[26948-0-0-4.84][27049-3-0-0.49][27098-1-4-0.38][27526-0-0-4.18][27639-3-3-4.50][27698-3-3-3.58][27772-0-0-1.26][27890-1-1-6.09][28040-0-4-2.93][28503-2-2-4.15]
[28577-1-1-1.08][28959-0-0-5.63][29198-3-3-4.46][29777-0-0-4.58][29877-2-3-1.90][30035-1-1-9.89][30098-0-3-4.32][30326-1-1-4.53][30572-2-2-2.34][30716-0-4-4.13]
[30806-2-3-4.25][30906-1-1-2.84][31007-0-0-2.61][31181-3-3-4.90][31238-0-3-2.18][31347-0-3-2.89][31422-2-4-1.46][31429-3-3-6.21][31431-0-0-3.99][31432-1-1-4.66]
[31477-0-3-2.27][31524-1-1-1.54][31597-1-1-0.62][31619-1-2-3.78][31701-0-0-6.33][31755-0-0-4.88][31854-3-3-4.65][32074-1-3-3.26][32078-3-3-8.37][32111-1-1-8.84]
[32127-1-1-3.07][32140-3-3-6.46][32263-2-4-1.99][32365-0-0-6.27][32411-2-3-4.96][32429-3-3-4.11][32473-3-3-1.65][32574-3-3-6.55][32584-0-3-2.64][32622-0-4-2.07]
[32858-3-3-4.19][32969-3-3-7.16][33016-2-2-3.56][33031-1-3-1.93][33035-2-2-4.37][33133-2-1-1.43][33173-2-3-1.86][33175-3-4-1.67][33306-3-3-6.51][33309-2-3-2.87]
[33474-0-0-0.99][33478-2-3-2.94][33618-1-4-1.44][33712-0-3-3.18][33782-2-4-3.49][33914-3-3-5.45][34076-3-3-4.88][34112-2-2-0.85][34138-2-3-3.44][34239-1-1-2.13]
[34364-2-2-6.06][34617-1-4-2.62][34751-3-3-4.02][34783-2-1-1.29][35015-3-3-3.54][35018-1-2-1.70][35288-2-2-1.36][0-4-4-4.56][1-4-4-3.75][2-4-4-3.65]
[3-4-4-4.25][4-4-4-1.57][5-4-3-1.38][6-4-4-8.15][7-4-4-4.13][8-4-4--0.17][9-4-4-1.16][10-4-4-5.57][11-4-4-3.93][12-4-4-1.17]
[14-4-4-0.74][15-4-3-3.08][16-4-4-4.65][17-4-4-2.77][18-4-4-3.79][19-4-0-3.10][20-4-4-1.70][21-4-4-2.49][22-4-0-1.51][23-4-4-2.74]
[24-4-4-7.97][25-4-3-3.73][26-4-4-2.76][27-4-4-3.05][28-4-4-6.60][29-4-1-1.02][30-4-4-2.36][31-4-4-2.00][32-4-4-4.28][33-4-4-1.96]
[34-4-4-1.46][35-4-4-3.27][37-4-4-2.38][39-4-0-4.25][40-4-4-1.65][41-4-4-3.09][42-4-4-1.52][43-4-4-4.65][45-4-2-0.12][46-4-2-3.49]
[47-4-4-6.93][48-4-4-5.28][51-4-4-5.64][52-4-4-4.06][53-4-4-2.62][54-4-4-2.15][55-4-2-1.11][56-4-4-2.55][57-4-3-1.17][58-4-4-1.56]
[59-4-4-3.73][60-4-4-2.36][61-4-4-4.04][62-4-3-2.44][63-4-4-2.58][64-4-4-1.64][65-4-4-5.94][66-4-4-2.40][67-4-3-1.51][68-4-3-1.11]
[69-4-4-1.64][70-4-4-3.29][72-4-4-2.11][73-4-1-5.13][74-4-2-1.80][75-4-4-2.25][77-4-4-7.85][78-4-4-1.15][79-4-4-4.09][80-4-4-4.16]
[81-4-2-2.99][82-4-1-1.88][83-4-4-1.27][84-4-4-4.46][85-4-4-4.63][86-4-2-1.88][87-4-4-5.48][88-4-4-5.41][89-4-3-0.07][90-4-4-1.75]
[91-4-3-2.57][92-4-4-1.83][93-4-0-2.11][94-4-4-3.48][95-4-4-3.80][96-4-4-2.56][97-4-4-5.88][98-4-2-1.98][99-4-4-0.44][100-4-2-4.59]
[101-4-4-6.79][102-4-4-2.97][103-4-3-1.77][104-4-4-2.47][105-4-4-3.22][106-4-4-4.72][107-4-4-2.79][108-4-4-0.92][109-4-4-5.48][110-4-1-3.68]
[111-4-0-5.62][112-4-4-2.87][113-4-3-1.29][114-4-4-1.18][115-4-4-2.76][116-4-4-1.54][117-4-4-5.45][119-4-4-2.37][121-4-4-3.30][122-4-4-3.26]
[124-4-3-1.79][125-4-4-5.16][126-4-4-8.47][127-4-2-1.19][128-4-3-0.72][129-4-4-4.64][130-4-3-1.09][131-4-4-0.58][132-4-4-0.15][133-4-4-6.98]
[135-4-4-4.25][136-4-4-1.62][137-4-4-2.60][138-4-4-2.33][139-4-4-1.86][140-4-2-0.82][141-4-3-1.19][142-4-4-6.35][143-4-4-7.81][144-4-4-6.73]
[145-4-4-3.85][148-4-0-3.71][149-4-4-2.30][150-4-4-4.22][151-4-4-5.18][152-4-4-3.29][153-4-4-4.03][154-4-4-8.10][155-4-4-7.67][156-4-3-3.08]
[157-4-4-0.45][158-4-4-2.03][160-4-4-1.50][161-4-2-6.17][162-4-4-0.53][164-4-4-3.67][165-4-4-3.63][167-4-0-4.15][168-4-4-1.87][170-4-4-2.49]
[171-4-4-1.91][172-4-4-5.32][173-4-4-7.09][174-4-4--0.05][175-4-4-4.46][177-4-4-3.79][178-4-4-2.32][179-4-4-3.57][180-4-4-6.01][181-4-4-2.58]
[182-4-3-3.20][183-4-4-5.46][184-4-2-3.49][186-4-4-3.31][187-4-2-1.28][188-4-4-1.50][189-4-2-3.89][190-4-4-1.36][191-4-4-4.53][192-4-4-4.33]
[193-4-4-0.69][194-4-3-1.25][195-4-4-1.52][196-4-2-1.99][197-4-4-2.43][198-4-4-8.64][199-4-4-1.99]
---------------------------
I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.100 | Acc: 91.000% | Wgt Acc: 96.919%
	I - Batch: 100 | Loss: 0.094 | Acc: 92.750% | Wgt Acc: 97.676%
	I - Batch: 150 | Loss: 0.097 | Acc: 92.958% | Wgt Acc: 97.683%
	I - Batch: 200 | Loss: 0.094 | Acc: 93.062% | Wgt Acc: 97.814%
	I - Batch: 250 | Loss: 0.094 | Acc: 93.150% | Wgt Acc: 97.820%
I - num batch: 285
I - Train -- Loss: 0.094 | Acc: 93.253% | Wgt Acc: 97.898% | LR: 1.250000e-04 | Dur: 181.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    0.    0.    1.   85.]
 [   1.  778.    0.    0.   39.]
 [   2.    0. 1122.    1.   89.]
 [   3.    0.    1.  834.   76.]
 [   2.    2.    1.    4.  711.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.192 | Acc: 65.878% | Wgt Acc: 64.433% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   2.   8.  13.  10.]
 [  0.  35.   3.   0.   4.]
 [  0.  18.  44.   2.  31.]
 [ 11.   5.   6.  60.  10.]
 [  7.  18.  14.  11. 125.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.086 | Acc: 93.500% | Wgt Acc: 98.248%
	I - Batch: 100 | Loss: 0.090 | Acc: 93.438% | Wgt Acc: 98.047%
	I - Batch: 150 | Loss: 0.092 | Acc: 92.917% | Wgt Acc: 97.802%
	I - Batch: 200 | Loss: 0.095 | Acc: 92.844% | Wgt Acc: 97.697%
	I - Batch: 250 | Loss: 0.098 | Acc: 92.625% | Wgt Acc: 97.543%
I - num batch: 285
I - Train -- Loss: 0.099 | Acc: 92.725% | Wgt Acc: 97.611% | LR: 1.250000e-04 | Dur: 181.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 799.    1.    0.    0.   86.]
 [   0.  774.    0.    0.   45.]
 [   3.    1. 1121.    1.  104.]
 [   1.    1.    0.  831.   71.]
 [   3.    3.    3.    8.  694.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.108 | Acc: 66.667% | Wgt Acc: 64.145% | Dur: 17.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   1.   4.  11.  12.]
 [  0.  40.   5.   1.   7.]
 [  0.  12.  41.   1.  20.]
 [ 14.   6.  10.  62.   9.]
 [ 11.  19.  15.  11. 132.]]

I - Local maximum validation set accuracy:  66.67

I - Validation set results: 
[14-1-2-2.56][50-3-3-1.59][124-2-4-2.19][127-0-0-8.32][443-2-4-4.12][567-0-0-3.03][573-1-1-4.67][615-0-0-2.51][695-1-2-4.05][722-3-3-3.63]
[826-0-0-4.59][878-0-0-5.58][1103-0-4-1.87][1212-3-4-1.40][1368-0-0-5.20][2181-2-3-1.68][2476-2-2-1.20][2721-2-2-3.99][2818-1-3-1.13][2886-2-1-1.73]
[3231-2-2-6.22][3333-2-2-0.76][3482-2-2-4.99][3536-3-0-0.83][3625-1-1-2.26][3909-0-0-3.24][4035-0-0-4.25][4140-0-0-5.25][4214-1-1-0.43][4346-1-4-1.03]
[4581-2-2-2.78][4708-3-3-2.56][4838-3-3-2.21][4845-1-4-1.03][4868-0-0-4.75][4939-0-4-0.42][4984-2-3-1.88][5078-1-2-1.32][5396-0-0-5.47][5479-1-1-7.88]
[5717-0-0-2.59][5843-1-4-1.12][5949-3-3-3.38][5987-2-4-4.71][6014-3-3-2.96][6033-3-4-2.72][6313-0-3-4.42][6421-3-3-2.78][6500-1-1-1.04][6583-3-1--0.33]
[6683-3-3-2.52][6825-2-1-5.23][6998-3-4--0.32][7049-3-3-2.81][7517-1-1-3.97][7521-1-4-0.47][7528-1-3-3.47][7949-1-2-5.26][8135-1-4-1.06][8185-3-0-4.84]
[8269-3-4-1.21][8273-3-3-4.21][8543-3-0-7.59][8666-1-1-2.31][8672-0-0-6.17][8903-1-3-1.21][9001-2-2-0.47][9036-2-2-5.60][9281-3-3-1.88][9300-2-2-9.13]
[9571-0-0-1.27][9617-1-1-1.52][9644-2-2--0.28][9705-2-0--0.62][9801-0-0-4.43][9803-3-3-3.58][9865-3-3-6.93][9896-2-2-4.36][10314-1-4-1.54][10337-3-3-4.00]
[10403-0-4-1.69][10653-2-4-0.82][10704-2-2-2.33][10719-1-4-2.85][10727-1-4-1.96][10836-0-0-11.06][10969-2-3-6.07][11042-0-0-2.51][11088-1-1-6.00][11322-0-0-6.57]
[11398-2-4-2.31][11499-0-0-4.65][11502-3-3-0.65][11512-3-3-1.59][11608-1-1-5.11][11610-0-0-4.19][11692-0-3-2.81][11905-0-0-5.36][11993-1-1-5.28][12002-2-2-0.57]
[12052-0-0-5.13][12201-0-3-3.50][12235-2-2-4.71][12320-1-4-4.06][12377-2-4-3.57][12398-2-3-1.96][12503-1-1-1.39][12617-0-3--0.08][12685-3-4-0.50][12738-2-2-1.84]
[12742-2-2-9.00][12823-0-0-4.65][13110-1-2-1.87][13240-3-3-2.86][13253-1-1-3.39][13273-0-0-9.17][13634-1-1-3.47][13763-2-3-0.44][13905-3-4--0.44][14060-2-4-1.94]
[14065-3-3-1.52][14147-3-3-2.25][14595-2-4-1.95][14687-2-2-3.90][14788-2-2-3.55][14869-1-1-3.45][14872-3-4-0.88][14877-1-1-4.21][14927-0-3-0.36][15066-0-0-6.25]
[15175-1-4-2.52][15178-2-3-1.41][15375-3-2--0.29][15389-3-3-3.03][15568-2-4-2.26][15675-3-3-7.76][15869-1-0-1.43][16207-3-0-1.41][16236-0-0-3.38][16302-3-3-2.46]
[16331-2-2-6.93][16381-0-0-2.24][16488-1-1-7.76][16495-0-0-5.63][16650-0-0-9.57][16719-1-1-2.08][16801-0-0-6.20][16828-0-0-3.84][17137-3-3-1.73][17245-1-3-1.12]
[17278-3-0-1.22][17282-0-0-1.95][17311-2-2-2.14][17336-2-4-0.20][17608-3-3-5.80][17627-0-4-2.08][17877-3-4-3.71][17924-1-3-2.44][17984-3-3-5.12][18211-0-3-2.54]
[18276-3-3-1.97][18287-1-1-1.75][18394-0-0-6.41][18428-0-0-7.09][18442-0-3-3.49][18478-3-3-3.08][18607-0-0-4.44][18616-0-4-0.77][18663-0-0-4.12][18718-0-0-5.09]
[18766-2-2-5.16][18824-2-4-3.71][18890-3-3-3.88][18930-3-4-2.35][18938-3-3-2.84][19817-1-2-2.83][19839-0-3-0.62][19930-3-3-3.89][19944-0-3-2.17][20036-2-2-8.77]
[20101-3-3-3.22][20474-1-1-3.63][20547-3-3-2.81][20929-2-2-6.13][21245-1-2-3.76][21257-3-3-2.84][21293-1-2-6.94][21316-1-1-4.07][21384-1-4-3.95][21448-1-1-3.73]
[21483-0-0-4.60][21487-2-2-3.82][21714-0-3-0.84][21943-3-3-0.80][21947-0-0-6.09][21948-0-0-8.27][21965-2-2-0.61][21998-1-1-1.75][22025-0-4-2.24][22228-3-3-4.82]
[22446-1-1-4.67][22494-3-0-3.83][22757-0-0-6.89][22811-3-3-5.26][22976-3-4-2.74][22985-3-3-4.60][23014-0-3-3.67][23112-1-1-3.92][23144-3-3-6.70][23168-2-4-1.00]
[23219-0-0-4.78][23363-3-3-7.63][23470-0-0-0.10][23486-2-2-1.54][23497-0-3-5.58][23516-0-0-3.34][23690-1-4-2.02][23921-2-2-2.16][23936-1-2-1.54][24040-3-0--0.01]
[24111-1-4-3.49][24182-0-0-7.73][24238-3-3-3.66][24290-2-0-3.81][24345-0-0-4.65][24364-1-2-1.43][24427-3-0-4.39][24477-2-2-3.27][24495-2-4-1.31][24893-2-1-2.59]
[25012-1-4-0.83][25121-2-4-4.30][25165-3-3-2.79][25183-0-0-4.56][25297-3-3-6.23][25398-0-0-6.72][25574-2-2-2.83][25644-1-1-1.98][25718-1-4-1.19][25774-2-2-2.34]
[26032-3-3-3.45][26051-3-3-6.62][26120-0-4-4.04][26321-1-1-8.52][26732-1-1-4.29][26784-3-3-6.50][26827-3-3-4.92][26833-0-0-1.96][26838-2-2-0.67][26860-1-4-0.03]
[26948-0-0-2.76][27049-3-0-3.12][27098-1-1-0.31][27526-0-0-5.29][27639-3-3-2.55][27698-3-3-1.09][27772-0-0-3.09][27890-1-1-6.25][28040-0-4-2.02][28503-2-2-4.03]
[28577-1-1-0.71][28959-0-0-8.06][29198-3-3-3.92][29777-0-0-6.90][29877-2-1-0.02][30035-1-1-6.35][30098-0-3-2.48][30326-1-1-3.96][30572-2-2-2.25][30716-0-4-3.02]
[30806-2-3-2.41][30906-1-1-2.98][31007-0-0-1.96][31181-3-3-2.77][31238-0-0-3.97][31347-0-0-2.20][31422-2-2-2.16][31429-3-3-4.65][31431-0-0-1.51][31432-1-1-4.61]
[31477-0-0-4.72][31524-1-1-1.65][31597-1-4-1.74][31619-1-4-1.36][31701-0-0-4.54][31755-0-0-5.39][31854-3-3-5.49][32074-1-1-2.50][32078-3-3-4.84][32111-1-1-3.90]
[32127-1-1-1.97][32140-3-3-3.59][32263-2-0-1.56][32365-0-0-6.64][32411-2-3-6.03][32429-3-0-4.00][32473-3-0-1.82][32574-3-3-4.12][32584-0-4-1.08][32622-0-4-0.73]
[32858-3-3-2.26][32969-3-3-5.24][33016-2-2-4.43][33031-1-3-2.45][33035-2-2-4.14][33133-2-2-1.94][33173-2-2-1.05][33175-3-4-1.96][33306-3-3-4.89][33309-2-3-1.46]
[33474-0-0-1.06][33478-2-0-1.50][33618-1-4-2.12][33712-0-3-3.02][33782-2-4-3.21][33914-3-3-5.57][34076-3-3-2.12][34112-2-2-6.82][34138-2-3-3.45][34239-1-1-2.26]
[34364-2-2-7.07][34617-1-2-1.54][34751-3-3-1.27][34783-2-2-1.05][35015-3-3-1.69][35018-1-2-3.28][35288-2-1-0.73][0-4-4-2.92][1-4-2-1.31][2-4-4-3.23]
[3-4-4-2.29][4-4-1-0.86][5-4-1-0.89][6-4-4-5.84][7-4-4-1.27][8-4-4-0.53][9-4-4-2.61][10-4-4-5.19][11-4-4-4.72][12-4-4-1.14]
[14-4-4-1.25][15-4-0-2.03][16-4-4-2.87][17-4-4-1.33][18-4-4-4.97][19-4-0-2.87][20-4-4-1.28][21-4-4-0.99][22-4-4-2.15][23-4-4-2.76]
[24-4-4-8.42][25-4-3-2.44][26-4-4-1.53][27-4-4-2.78][28-4-4-5.90][29-4-2-1.69][30-4-3-2.16][31-4-4-2.58][32-4-4-3.63][33-4-4-3.20]
[34-4-4-2.08][35-4-4-2.51][37-4-4-1.77][39-4-0-4.03][40-4-1-0.05][41-4-4-2.66][42-4-4-2.38][43-4-4-4.33][45-4-4-0.72][46-4-2-1.65]
[47-4-4-4.12][48-4-4-3.42][51-4-4-4.80][52-4-4-2.53][53-4-2-1.62][54-4-3-3.31][55-4-4-1.77][56-4-4-2.01][57-4-3-4.32][58-4-2-3.14]
[59-4-4-3.78][60-4-0-0.26][61-4-4-3.99][62-4-4-2.10][63-4-4-1.74][64-4-4-1.02][65-4-4-6.49][66-4-4-3.85][67-4-3-1.91][68-4-3-1.34]
[69-4-0-0.83][70-4-4-2.58][72-4-4-2.34][73-4-1-3.94][74-4-2-1.84][75-4-4-1.34][77-4-4-3.86][78-4-4-1.01][79-4-4-4.12][80-4-4-5.62]
[81-4-4-2.45][82-4-1-1.66][83-4-4-2.20][84-4-4-3.71][85-4-4-4.93][86-4-4-2.49][87-4-4-4.22][88-4-4-3.77][89-4-0-1.00][90-4-3-1.13]
[91-4-4-1.73][92-4-4-2.69][93-4-4-1.20][94-4-4-3.06][95-4-4-2.35][96-4-4-2.55][97-4-4-4.62][98-4-2-3.18][99-4-4-0.19][100-4-2-5.34]
[101-4-4-4.60][102-4-4-2.74][103-4-2-2.13][104-4-4-2.12][105-4-4-2.96][106-4-4-4.80][107-4-4-3.20][108-4-4-0.89][109-4-4-2.76][110-4-1-2.37]
[111-4-0-4.35][112-4-4-1.98][113-4-4-1.32][114-4-4-0.87][115-4-4-2.16][116-4-4-2.08][117-4-4-3.82][119-4-2-2.80][121-4-4-3.71][122-4-4-2.61]
[124-4-4-1.41][125-4-4-4.48][126-4-4-8.22][127-4-2-1.54][128-4-3-0.74][129-4-4-4.06][130-4-4-1.40][131-4-4-1.40][132-4-4-0.77][133-4-4-4.20]
[135-4-4-3.66][136-4-4-1.84][137-4-4-3.08][138-4-4-1.59][139-4-4-1.67][140-4-1-0.32][141-4-0-3.70][142-4-4-4.90][143-4-4-3.73][144-4-4-5.40]
[145-4-4-3.14][148-4-0-1.71][149-4-4-3.20][150-4-4-4.40][151-4-4-3.90][152-4-4-2.70][153-4-4-4.64][154-4-4-5.60][155-4-4-5.85][156-4-3-1.36]
[157-4-0-0.68][158-4-4-1.77][160-4-4-1.25][161-4-2-7.18][162-4-4-1.92][164-4-4-2.81][165-4-4-2.17][167-4-0-3.25][168-4-4-1.53][170-4-4-2.17]
[171-4-4-2.41][172-4-4-5.91][173-4-4-5.01][174-4-0-0.47][175-4-4-4.06][177-4-4-1.93][178-4-4-2.04][179-4-4-3.10][180-4-4-3.57][181-4-4-2.70]
[182-4-2-2.44][183-4-4-4.40][184-4-2-2.60][186-4-4-2.29][187-4-2-1.56][188-4-4-2.49][189-4-2-3.17][190-4-2-0.94][191-4-4-3.23][192-4-4-1.77]
[193-4-2-2.76][194-4-4-1.46][195-4-2-2.17][196-4-2-4.09][197-4-4-2.05][198-4-4-7.57][199-4-4-0.80]
---------------------------
I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 93.000% | Wgt Acc: 97.322%
	I - Batch: 100 | Loss: 0.094 | Acc: 92.812% | Wgt Acc: 97.374%
	I - Batch: 150 | Loss: 0.093 | Acc: 93.000% | Wgt Acc: 97.604%
	I - Batch: 200 | Loss: 0.092 | Acc: 93.031% | Wgt Acc: 97.702%
	I - Batch: 250 | Loss: 0.093 | Acc: 93.025% | Wgt Acc: 97.694%
I - num batch: 285
I - Train -- Loss: 0.094 | Acc: 93.055% | Wgt Acc: 97.706% | LR: 1.250000e-04 | Dur: 178.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    1.    0.   83.]
 [   0.  778.    0.    0.   41.]
 [   0.    0. 1118.    1.  102.]
 [   2.    0.    0.  834.   65.]
 [   9.    2.    5.    5.  709.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.220 | Acc: 64.694% | Wgt Acc: 61.239% | Dur: 14.63s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   5.  10.   7.]
 [  0.  32.   6.   1.   5.]
 [  2.  17.  42.   0.  23.]
 [ 18.   5.   8.  64.  12.]
 [ 11.  22.  14.  11. 133.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.078 | Acc: 94.750% | Wgt Acc: 98.461%
	I - Batch: 100 | Loss: 0.086 | Acc: 93.875% | Wgt Acc: 98.286%
	I - Batch: 150 | Loss: 0.085 | Acc: 94.042% | Wgt Acc: 98.303%
	I - Batch: 200 | Loss: 0.086 | Acc: 93.969% | Wgt Acc: 98.287%
	I - Batch: 250 | Loss: 0.088 | Acc: 93.600% | Wgt Acc: 98.173%
I - num batch: 285
I - Train -- Loss: 0.090 | Acc: 93.538% | Wgt Acc: 98.035% | LR: 1.250000e-04 | Dur: 181.78s
I - Confusion Matrix: [row->prediction - col->label]
[[ 801.    1.    0.    0.   85.]
 [   0.  777.    0.    1.   31.]
 [   2.    1. 1121.    0.  108.]
 [   0.    0.    0.  836.   55.]
 [   3.    1.    3.    3.  721.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.153 | Acc: 65.483% | Wgt Acc: 64.560% | Dur: 16.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   4.   8.   9.]
 [  0.  38.   7.   2.   9.]
 [  2.  14.  40.   1.  25.]
 [ 16.  11.  14.  69.  14.]
 [  8.  12.  10.   6. 123.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.099 | Acc: 92.750% | Wgt Acc: 97.663%
	I - Batch: 100 | Loss: 0.098 | Acc: 92.562% | Wgt Acc: 97.695%
	I - Batch: 150 | Loss: 0.094 | Acc: 92.833% | Wgt Acc: 97.766%
	I - Batch: 200 | Loss: 0.094 | Acc: 93.062% | Wgt Acc: 97.812%
	I - Batch: 250 | Loss: 0.095 | Acc: 93.100% | Wgt Acc: 97.804%
I - num batch: 285
I - Train -- Loss: 0.095 | Acc: 93.143% | Wgt Acc: 97.789% | LR: 1.250000e-04 | Dur: 176.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    0.    2.   82.]
 [   1.  780.    1.    0.   31.]
 [   0.    0. 1118.    0.  106.]
 [   0.    0.    0.  833.   71.]
 [   8.    0.    5.    5.  710.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.162 | Acc: 65.483% | Wgt Acc: 62.761% | Dur: 14.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   4.   9.   9.]
 [  1.  34.   6.   1.   6.]
 [  2.  18.  36.   2.  24.]
 [ 13.   9.  15.  65.  10.]
 [  6.  15.  14.   9. 131.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.092 | Acc: 94.000% | Wgt Acc: 98.264%
	I - Batch: 100 | Loss: 0.088 | Acc: 94.250% | Wgt Acc: 98.194%
	I - Batch: 150 | Loss: 0.085 | Acc: 93.958% | Wgt Acc: 98.149%
	I - Batch: 200 | Loss: 0.089 | Acc: 93.438% | Wgt Acc: 97.947%
	I - Batch: 250 | Loss: 0.092 | Acc: 93.550% | Wgt Acc: 97.956%
I - num batch: 285
I - Train -- Loss: 0.093 | Acc: 93.297% | Wgt Acc: 97.858% | LR: 1.250000e-04 | Dur: 182.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    0.    0.   70.]
 [   1.  778.    1.    1.   48.]
 [   2.    0. 1118.    0.  104.]
 [   2.    0.    1.  838.   62.]
 [   6.    2.    4.    1.  716.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.193 | Acc: 66.075% | Wgt Acc: 64.203% | Dur: 21.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   1.   7.  13.  13.]
 [  0.  38.   7.   1.   2.]
 [  2.  19.  35.   0.  23.]
 [ 12.   3.  13.  64.  14.]
 [  4.  17.  13.   8. 128.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.096 | Acc: 92.875% | Wgt Acc: 97.700%
	I - Batch: 100 | Loss: 0.095 | Acc: 93.250% | Wgt Acc: 97.852%
	I - Batch: 150 | Loss: 0.091 | Acc: 93.625% | Wgt Acc: 97.932%
	I - Batch: 200 | Loss: 0.095 | Acc: 93.188% | Wgt Acc: 97.718%
	I - Batch: 250 | Loss: 0.096 | Acc: 93.375% | Wgt Acc: 97.757%
I - num batch: 285
I - Train -- Loss: 0.097 | Acc: 93.275% | Wgt Acc: 97.751% | LR: 1.250000e-04 | Dur: 181.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 801.    0.    1.    2.   72.]
 [   0.  776.    1.    0.   37.]
 [   1.    2. 1118.    0.   97.]
 [   1.    0.    1.  830.   75.]
 [   3.    2.    3.    8.  719.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.295 | Acc: 64.497% | Wgt Acc: 55.945% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   3.  12.   7.]
 [  1.  30.   6.   1.   3.]
 [  0.  11.  27.   0.   8.]
 [ 16.   4.   7.  54.   6.]
 [ 11.  29.  32.  19. 156.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.098 | Acc: 93.000% | Wgt Acc: 97.761%
	I - Batch: 100 | Loss: 0.103 | Acc: 92.250% | Wgt Acc: 97.556%
	I - Batch: 150 | Loss: 0.101 | Acc: 92.750% | Wgt Acc: 97.790%
	I - Batch: 200 | Loss: 0.099 | Acc: 92.844% | Wgt Acc: 97.852%
	I - Batch: 250 | Loss: 0.098 | Acc: 92.925% | Wgt Acc: 97.809%
I - num batch: 285
I - Train -- Loss: 0.097 | Acc: 92.901% | Wgt Acc: 97.811% | LR: 1.250000e-04 | Dur: 187.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 800.    0.    1.    0.   75.]
 [   0.  778.    1.    0.   43.]
 [   0.    0. 1119.    0.  109.]
 [   2.    1.    0.  835.   78.]
 [   4.    1.    3.    5.  695.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.207 | Acc: 66.469% | Wgt Acc: 63.326% | Dur: 16.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   6.   5.  14.  11.]
 [  0.  30.   3.   1.   4.]
 [  1.  16.  40.   1.  18.]
 [ 10.   6.  12.  61.  13.]
 [  5.  20.  15.   9. 134.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.089 | Acc: 93.375% | Wgt Acc: 97.478%
	I - Batch: 100 | Loss: 0.087 | Acc: 93.125% | Wgt Acc: 97.572%
	I - Batch: 150 | Loss: 0.087 | Acc: 93.583% | Wgt Acc: 97.805%
	I - Batch: 200 | Loss: 0.090 | Acc: 93.344% | Wgt Acc: 97.735%
	I - Batch: 250 | Loss: 0.093 | Acc: 93.025% | Wgt Acc: 97.622%
I - num batch: 285
I - Train -- Loss: 0.095 | Acc: 93.011% | Wgt Acc: 97.595% | LR: 1.250000e-04 | Dur: 180.97s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    1.    2.   78.]
 [   1.  777.    0.    1.   37.]
 [   3.    1. 1119.    0.  100.]
 [   3.    0.    0.  835.   73.]
 [  10.    2.    4.    2.  712.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.303 | Acc: 61.341% | Wgt Acc: 63.718% | Dur: 16.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   6.   7.  11.  22.]
 [  1.  36.   8.   2.  17.]
 [  2.  15.  38.   0.  27.]
 [ 16.  12.  14.  68.  14.]
 [  0.   9.   8.   5. 100.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.093 | Acc: 92.250% | Wgt Acc: 97.566%
	I - Batch: 100 | Loss: 0.089 | Acc: 92.938% | Wgt Acc: 97.906%
	I - Batch: 150 | Loss: 0.087 | Acc: 93.417% | Wgt Acc: 97.959%
	I - Batch: 200 | Loss: 0.085 | Acc: 93.688% | Wgt Acc: 98.041%
	I - Batch: 250 | Loss: 0.086 | Acc: 93.775% | Wgt Acc: 98.063%
I - num batch: 285
I - Train -- Loss: 0.088 | Acc: 93.451% | Wgt Acc: 97.973% | LR: 1.250000e-04 | Dur: 171.88s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    0.    0.   80.]
 [   1.  778.    0.    1.   31.]
 [   1.    0. 1122.    0.   96.]
 [   4.    0.    0.  836.   74.]
 [   3.    2.    2.    3.  719.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.222 | Acc: 64.497% | Wgt Acc: 60.281% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   5.  15.  13.]
 [  0.  34.   5.   1.   5.]
 [  3.  12.  31.   1.  17.]
 [ 11.   5.  14.  60.   9.]
 [  8.  23.  20.   9. 136.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.093 | Acc: 93.625% | Wgt Acc: 98.133%
	I - Batch: 100 | Loss: 0.095 | Acc: 93.000% | Wgt Acc: 97.735%
	I - Batch: 150 | Loss: 0.095 | Acc: 93.083% | Wgt Acc: 97.771%
	I - Batch: 200 | Loss: 0.094 | Acc: 93.062% | Wgt Acc: 97.822%
	I - Batch: 250 | Loss: 0.094 | Acc: 93.150% | Wgt Acc: 97.875%
I - num batch: 285
I - Train -- Loss: 0.094 | Acc: 93.143% | Wgt Acc: 97.901% | LR: 1.250000e-04 | Dur: 188.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   93.]
 [   1.  780.    1.    1.   34.]
 [   1.    0. 1122.    0.   89.]
 [   2.    0.    0.  838.   79.]
 [   9.    0.    1.    1.  705.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.212 | Acc: 65.089% | Wgt Acc: 63.015% | Dur: 20.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   2.  10.  10.]
 [  0.  32.   2.   1.   5.]
 [  1.  16.  47.   0.  29.]
 [ 17.   7.  15.  64.   9.]
 [ 10.  21.   9.  11. 127.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.086 | Acc: 93.625% | Wgt Acc: 97.898%
	I - Batch: 100 | Loss: 0.091 | Acc: 93.000% | Wgt Acc: 97.784%
	I - Batch: 150 | Loss: 0.086 | Acc: 93.625% | Wgt Acc: 97.975%
	I - Batch: 200 | Loss: 0.086 | Acc: 93.875% | Wgt Acc: 98.022%
	I - Batch: 250 | Loss: 0.088 | Acc: 93.675% | Wgt Acc: 97.898%
I - num batch: 285
I - Train -- Loss: 0.088 | Acc: 93.868% | Wgt Acc: 97.969% | LR: 1.250000e-04 | Dur: 179.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 800.    1.    0.    2.   57.]
 [   0.  775.    0.    0.   34.]
 [   0.    1. 1120.    0.   90.]
 [   1.    1.    0.  833.   76.]
 [   5.    2.    4.    5.  743.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.349 | Acc: 60.947% | Wgt Acc: 63.245% | Dur: 14.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   3.  12.  19.]
 [  0.  39.   3.   1.   9.]
 [  1.  13.  40.   1.  29.]
 [ 21.  16.  20.  69.  23.]
 [  5.   8.   9.   3. 100.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.105 | Acc: 91.125% | Wgt Acc: 97.184%
	I - Batch: 100 | Loss: 0.101 | Acc: 92.500% | Wgt Acc: 97.525%
	I - Batch: 150 | Loss: 0.103 | Acc: 92.292% | Wgt Acc: 97.495%
	I - Batch: 200 | Loss: 0.101 | Acc: 92.531% | Wgt Acc: 97.574%
	I - Batch: 250 | Loss: 0.099 | Acc: 92.850% | Wgt Acc: 97.682%
I - num batch: 285
I - Train -- Loss: 0.098 | Acc: 92.813% | Wgt Acc: 97.731% | LR: 1.250000e-04 | Dur: 179.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 800.    0.    0.    1.   77.]
 [   1.  775.    2.    0.   48.]
 [   2.    1. 1116.    0.  106.]
 [   1.    1.    2.  838.   75.]
 [   2.    3.    4.    1.  694.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 65.089% | Wgt Acc: 60.108% | Dur: 18.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   1.   2.  10.   5.]
 [  0.  34.   7.   1.   8.]
 [  2.  18.  37.   3.  20.]
 [ 13.   5.  13.  63.   6.]
 [ 18.  20.  16.   9. 141.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.088 | Acc: 93.875% | Wgt Acc: 97.969%
	I - Batch: 100 | Loss: 0.090 | Acc: 93.500% | Wgt Acc: 97.986%
	I - Batch: 150 | Loss: 0.088 | Acc: 93.333% | Wgt Acc: 98.030%
	I - Batch: 200 | Loss: 0.089 | Acc: 93.219% | Wgt Acc: 97.999%
	I - Batch: 250 | Loss: 0.088 | Acc: 93.700% | Wgt Acc: 98.088%
I - num batch: 285
I - Train -- Loss: 0.089 | Acc: 93.582% | Wgt Acc: 98.072% | LR: 1.250000e-04 | Dur: 181.99s
I - Confusion Matrix: [row->prediction - col->label]
[[ 802.    0.    0.    0.   73.]
 [   0.  778.    1.    0.   39.]
 [   1.    0. 1118.    0.   91.]
 [   1.    0.    1.  838.   75.]
 [   2.    2.    4.    2.  722.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.185 | Acc: 65.680% | Wgt Acc: 62.530% | Dur: 15.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   0.   4.  12.  11.]
 [  0.  33.   6.   1.   5.]
 [  2.  21.  40.   0.  28.]
 [ 11.   5.  10.  62.   3.]
 [ 10.  19.  15.  11. 133.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.103 | Acc: 93.250% | Wgt Acc: 97.417%
	I - Batch: 100 | Loss: 0.097 | Acc: 93.062% | Wgt Acc: 97.506%
	I - Batch: 150 | Loss: 0.099 | Acc: 92.958% | Wgt Acc: 97.488%
	I - Batch: 200 | Loss: 0.099 | Acc: 93.031% | Wgt Acc: 97.614%
	I - Batch: 250 | Loss: 0.099 | Acc: 92.725% | Wgt Acc: 97.468%
I - num batch: 285
I - Train -- Loss: 0.096 | Acc: 93.055% | Wgt Acc: 97.594% | LR: 1.250000e-04 | Dur: 175.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    1.    3.   77.]
 [   0.  778.    0.    1.   34.]
 [   1.    0. 1116.    1.  105.]
 [   4.    0.    1.  829.   70.]
 [   4.    2.    6.    6.  714.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.223 | Acc: 66.075% | Wgt Acc: 61.884% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   6.  12.  11.]
 [  0.  33.  10.   2.   4.]
 [  1.  15.  34.   1.  16.]
 [ 15.   9.  10.  65.  10.]
 [  8.  19.  15.   6. 139.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840.  781.]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.077 | Acc: 94.000% | Wgt Acc: 98.425%
	I - Batch: 100 | Loss: 0.073 | Acc: 94.625% | Wgt Acc: 98.509%
	I - Batch: 150 | Loss: 0.072 | Acc: 94.667% | Wgt Acc: 98.542%
	I - Batch: 200 | Loss: 0.073 | Acc: 94.656% | Wgt Acc: 98.570%
	I - Batch: 250 | Loss: 0.073 | Acc: 94.450% | Wgt Acc: 98.453%
I - num batch: 271
I - Train -- Loss: 0.073 | Acc: 94.505% | Wgt Acc: 98.458% | LR: 1.250000e-04 | Dur: 172.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 802.    0.    1.    1.   54.]
 [   0.  777.    0.    0.   32.]
 [   0.    0. 1123.    0.   82.]
 [   0.    2.    0.  839.   61.]
 [   4.    1.    0.    0.  552.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.201 | Acc: 67.850% | Wgt Acc: 65.114% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   2.   4.  12.  13.]
 [  0.  38.   4.   0.   4.]
 [  3.  13.  36.   1.  12.]
 [  8.   8.  13.  63.  16.]
 [  5.  17.  18.  10. 135.]]

I - Local maximum validation set accuracy:  67.85

I - Validation set results: 
[14-1-2-1.51][50-3-4-2.32][124-2-4-3.36][127-0-0-7.24][443-2-4-4.17][567-0-0-4.63][573-1-1-4.11][615-0-0-2.96][695-1-2-2.06][722-3-3-4.27]
[826-0-0-6.21][878-0-3-4.52][1103-0-0-4.06][1212-3-3-0.26][1368-0-0-5.18][2181-2-3-2.36][2476-2-4-0.92][2721-2-2-4.64][2818-1-1-2.44][2886-2-4-2.06]
[3231-2-2-4.79][3333-2-2-0.49][3482-2-2-3.54][3536-3-3-3.61][3625-1-1-3.77][3909-0-0-4.57][4035-0-0-2.84][4140-0-0-5.90][4214-1-1-0.65][4346-1-4-2.76]
[4581-2-2-1.58][4708-3-4-1.37][4838-3-3-2.27][4845-1-3-1.01][4868-0-0-6.76][4939-0-4-1.19][4984-2-3-2.55][5078-1-2-2.64][5396-0-0-8.84][5479-1-1-7.03]
[5717-0-0-5.11][5843-1-1-2.04][5949-3-3-2.76][5987-2-4-3.64][6014-3-3-1.57][6033-3-3-3.95][6313-0-3-4.09][6421-3-3-5.57][6500-1-4-2.59][6583-3-3-0.69]
[6683-3-3-3.13][6825-2-1-2.91][6998-3-3-1.38][7049-3-3-2.37][7517-1-1-1.24][7521-1-4-0.62][7528-1-3-2.53][7949-1-2-3.98][8135-1-4-0.09][8185-3-0-6.61]
[8269-3-4-1.44][8273-3-3-5.31][8543-3-0-5.30][8666-1-1-2.73][8672-0-0-4.42][8903-1-3-3.30][9001-2-3-0.56][9036-2-2-4.16][9281-3-3-0.31][9300-2-2-8.21]
[9571-0-4-0.34][9617-1-4-2.34][9644-2-4-0.11][9705-2-2-0.56][9801-0-0-3.57][9803-3-3-3.68][9865-3-3-8.39][9896-2-2-3.93][10314-1-4-2.59][10337-3-3-5.13]
[10403-0-0-2.27][10653-2-1-3.40][10704-2-2-1.45][10719-1-2-3.48][10727-1-4-2.26][10836-0-0-10.04][10969-2-3-7.87][11042-0-0-2.78][11088-1-1-5.05][11322-0-0-5.37]
[11398-2-4-1.68][11499-0-0-5.04][11502-3-0-0.80][11512-3-3-0.46][11608-1-2-3.44][11610-0-0-4.76][11692-0-0-2.61][11905-0-0-5.12][11993-1-1-5.57][12002-2-0-1.13]
[12052-0-0-5.44][12201-0-0-4.07][12235-2-4-2.23][12320-1-4-2.04][12377-2-4-3.13][12398-2-3-3.48][12503-1-1-0.63][12617-0-2-1.53][12685-3-4-0.33][12738-2-4-1.65]
[12742-2-2-7.52][12823-0-0-5.64][13110-1-3-1.59][13240-3-3-3.42][13253-1-4-1.76][13273-0-0-9.32][13634-1-1-4.31][13763-2-3-0.46][13905-3-3-0.10][14060-2-4-1.49]
[14065-3-3-3.50][14147-3-0-1.45][14595-2-2-1.50][14687-2-2-5.15][14788-2-2-3.74][14869-1-1-4.73][14872-3-3-2.38][14877-1-1-2.97][14927-0-3-1.69][15066-0-0-7.65]
[15175-1-4-2.92][15178-2-3-1.51][15375-3-3-0.85][15389-3-3-3.96][15568-2-4-0.48][15675-3-3-11.14][15869-1-1-0.18][16207-3-0-0.71][16236-0-0-2.79][16302-3-3-4.61]
[16331-2-2-4.82][16381-0-0-3.09][16488-1-1-5.13][16495-0-0-4.19][16650-0-0-10.24][16719-1-1-1.64][16801-0-0-6.23][16828-0-0-4.90][17137-3-3-2.07][17245-1-4-4.06]
[17278-3-0-1.13][17282-0-0-4.91][17311-2-4-1.13][17336-2-4-0.12][17608-3-3-6.06][17627-0-0-1.90][17877-3-4-3.73][17924-1-3-3.35][17984-3-3-6.85][18211-0-0-1.92]
[18276-3-3-2.13][18287-1-4-2.00][18394-0-0-7.31][18428-0-0-7.36][18442-0-0-3.86][18478-3-3-2.63][18607-0-0-5.82][18616-0-0-1.81][18663-0-0-3.69][18718-0-0-4.88]
[18766-2-2-2.90][18824-2-4-4.91][18890-3-4-1.27][18930-3-4-2.48][18938-3-3-2.06][19817-1-2-2.15][19839-0-2--0.36][19930-3-3-3.38][19944-0-3-4.50][20036-2-2-9.01]
[20101-3-3-2.75][20474-1-1-1.95][20547-3-3-2.53][20929-2-2-7.48][21245-1-3-1.22][21257-3-3-3.39][21293-1-2-6.36][21316-1-1-6.02][21384-1-3-1.95][21448-1-1-0.33]
[21483-0-0-5.25][21487-2-2-1.64][21714-0-0-1.96][21943-3-3-1.85][21947-0-0-5.75][21948-0-0-8.25][21965-2-2-6.04][21998-1-1-3.42][22025-0-2-1.83][22228-3-3-6.53]
[22446-1-1-0.55][22494-3-0-4.05][22757-0-0-7.69][22811-3-3-4.65][22976-3-2-1.25][22985-3-3-4.37][23014-0-0-4.47][23112-1-1-3.41][23144-3-3-6.30][23168-2-3-1.67]
[23219-0-0-6.05][23363-3-3-5.71][23470-0-0-1.00][23486-2-4-0.70][23497-0-3-7.84][23516-0-0-3.84][23690-1-0-0.22][23921-2-2-1.54][23936-1-2-1.57][24040-3-4--0.14]
[24111-1-1-1.88][24182-0-0-7.99][24238-3-3-3.49][24290-2-0-4.67][24345-0-0-1.08][24364-1-1--0.10][24427-3-0-5.86][24477-2-2-2.46][24495-2-4-3.90][24893-2-1-0.95]
[25012-1-4-1.14][25121-2-4-3.09][25165-3-3-2.43][25183-0-0-4.29][25297-3-3-6.02][25398-0-0-5.82][25574-2-2-0.90][25644-1-2-2.07][25718-1-2-0.57][25774-2-2-0.64]
[26032-3-3-5.40][26051-3-3-8.12][26120-0-0-3.00][26321-1-1-8.67][26732-1-1-3.11][26784-3-3-7.18][26827-3-0-4.69][26833-0-3-3.51][26838-2-2-0.77][26860-1-1-0.54]
[26948-0-0-3.87][27049-3-0-3.95][27098-1-4-1.78][27526-0-0-2.87][27639-3-3-2.43][27698-3-3-1.16][27772-0-0-2.59][27890-1-1-3.41][28040-0-4-1.87][28503-2-2-3.28]
[28577-1-1--0.64][28959-0-0-8.05][29198-3-3-5.30][29777-0-0-9.41][29877-2-3-1.73][30035-1-1-7.29][30098-0-3-3.33][30326-1-1-4.36][30572-2-2-2.97][30716-0-4-2.27]
[30806-2-3-1.95][30906-1-1-1.97][31007-0-0-5.38][31181-3-0-2.16][31238-0-0-5.05][31347-0-0-4.49][31422-2-2-1.02][31429-3-3-4.22][31431-0-0-3.80][31432-1-1-6.20]
[31477-0-0-3.50][31524-1-1-0.25][31597-1-4-2.46][31619-1-0-2.01][31701-0-0-5.79][31755-0-0-7.52][31854-3-3-3.97][32074-1-1-1.30][32078-3-3-6.76][32111-1-1-7.67]
[32127-1-2-1.45][32140-3-3-4.50][32263-2-0-1.26][32365-0-0-7.47][32411-2-0-5.94][32429-3-3-3.55][32473-3-0-1.50][32574-3-3-5.23][32584-0-0-4.49][32622-0-4--0.10]
[32858-3-3-4.14][32969-3-3-6.19][33016-2-2-5.57][33031-1-3-4.21][33035-2-2-5.06][33133-2-1-1.98][33173-2-2-0.94][33175-3-4-2.94][33306-3-3-5.13][33309-2-3-2.93]
[33474-0-0-0.69][33478-2-3-2.23][33618-1-1-0.93][33712-0-3-1.78][33782-2-2-2.80][33914-3-3-4.70][34076-3-4-2.84][34112-2-2-0.73][34138-2-3-4.70][34239-1-4-0.95]
[34364-2-2-7.74][34617-1-4-1.03][34751-3-3-4.45][34783-2-2-1.07][35015-3-3-2.62][35018-1-2-2.44][35288-2-2-1.43][0-4-4-4.28][1-4-4-1.43][2-4-4-2.75]
[3-4-2-1.51][4-4-4-0.68][5-4-1-0.48][6-4-4-3.94][7-4-4-2.39][8-4-4-0.87][9-4-4-2.85][10-4-4-4.12][11-4-2-4.68][12-4-4-0.58]
[14-4-4-3.10][15-4-0-1.82][16-4-4-1.51][17-4-4-2.56][18-4-4-3.97][19-4-0-3.93][20-4-4-2.07][21-4-3-0.53][22-4-4-1.91][23-4-4-2.19]
[24-4-4-2.42][25-4-4-4.33][26-4-4-1.71][27-4-4-2.65][28-4-4-2.19][29-4-4-1.30][30-4-3-1.70][31-4-4-2.82][32-4-4-3.46][33-4-4-4.66]
[34-4-4-3.50][35-4-4-4.91][37-4-4-2.21][39-4-0-6.17][40-4-4-2.24][41-4-3-1.15][42-4-4-1.68][43-4-4-3.69][45-4-3-0.51][46-4-4-4.05]
[47-4-4-4.32][48-4-4-2.91][51-4-4-2.16][52-4-4-4.19][53-4-4-1.67][54-4-3-3.04][55-4-4-3.85][56-4-1-0.74][57-4-3-2.75][58-4-2-5.15]
[59-4-0-3.85][60-4-4-1.17][61-4-4-3.88][62-4-4-3.75][63-4-4-2.15][64-4-2-1.77][65-4-4-5.42][66-4-4-3.72][67-4-3-0.67][68-4-3-1.92]
[69-4-0-1.44][70-4-4-3.14][72-4-4-2.94][73-4-1-2.25][74-4-4-2.52][75-4-0-2.94][77-4-4-6.13][78-4-4-1.78][79-4-4-3.41][80-4-4-3.02]
[81-4-2-3.78][82-4-4-1.09][83-4-4-2.56][84-4-4-4.13][85-4-4-4.28][86-4-4-4.90][87-4-4-3.97][88-4-4-3.35][89-4-4-0.63][90-4-4-1.90]
[91-4-4-2.05][92-4-4-2.68][93-4-0-0.93][94-4-4-2.83][95-4-4-0.46][96-4-3-1.47][97-4-4-6.41][98-4-2-2.72][99-4-4-0.69][100-4-2-2.26]
[101-4-4-4.01][102-4-4-3.16][103-4-4-0.73][104-4-4-2.92][105-4-4-3.51][106-4-4-3.01][107-4-4-1.89][108-4-4-2.34][109-4-4-4.90][110-4-4-3.08]
[111-4-0-6.43][112-4-3-2.87][113-4-3-0.45][114-4-4-2.16][115-4-4-1.57][116-4-4-1.62][117-4-4-5.27][119-4-4-2.45][121-4-4-3.02][122-4-4-3.36]
[124-4-4-1.93][125-4-4-3.92][126-4-4-3.59][127-4-2-2.10][128-4-3-1.07][129-4-4-4.85][130-4-4-2.67][131-4-4-1.73][132-4-4--0.06][133-4-4-4.56]
[135-4-4-4.09][136-4-4-1.32][137-4-4-3.03][138-4-4-1.98][139-4-4-0.97][140-4-4-1.53][141-4-0-5.56][142-4-4-2.58][143-4-4-4.78][144-4-4-5.18]
[145-4-4-2.49][148-4-0-4.42][149-4-0-1.13][150-4-4-5.28][151-4-4-3.14][152-4-4-3.28][153-4-4-2.26][154-4-4-4.56][155-4-4-3.88][156-4-4-1.14]
[157-4-4-1.91][158-4-4-2.25][160-4-4-1.20][161-4-2-4.83][162-4-4-1.58][164-4-4-3.40][165-4-4-1.69][167-4-4-2.46][168-4-4-1.59][170-4-3-1.74]
[171-4-4-2.52][172-4-4-3.49][173-4-4-5.27][174-4-0-1.55][175-4-4-4.05][177-4-4-3.21][178-4-4-2.27][179-4-4-3.32][180-4-4-2.86][181-4-4-3.13]
[182-4-3-4.83][183-4-4-4.03][184-4-2-3.09][186-4-4-2.59][187-4-2-2.84][188-4-4-2.38][189-4-1-0.34][190-4-3-0.01][191-4-4-3.56][192-4-3-1.91]
[193-4-4-1.29][194-4-4-1.87][195-4-0-1.07][196-4-2-3.32][197-4-4-2.51][198-4-4-2.98][199-4-4-3.12]
---------------------------
I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.074 | Acc: 94.875% | Wgt Acc: 98.463%
	I - Batch: 100 | Loss: 0.074 | Acc: 94.500% | Wgt Acc: 98.243%
	I - Batch: 150 | Loss: 0.079 | Acc: 94.083% | Wgt Acc: 98.209%
	I - Batch: 200 | Loss: 0.081 | Acc: 93.969% | Wgt Acc: 98.135%
	I - Batch: 250 | Loss: 0.081 | Acc: 94.125% | Wgt Acc: 98.219%
I - num batch: 285
I - Train -- Loss: 0.084 | Acc: 94.000% | Wgt Acc: 98.154% | LR: 1.250000e-04 | Dur: 183.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 799.    0.    0.    0.   70.]
 [   0.  776.    1.    0.   43.]
 [   1.    1. 1123.    0.   82.]
 [   0.    0.    0.  837.   63.]
 [   6.    3.    0.    3.  742.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.177 | Acc: 64.694% | Wgt Acc: 60.708% | Dur: 15.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   5.   4.  15.  10.]
 [  0.  40.  10.   4.  12.]
 [  2.  15.  29.   1.  17.]
 [  8.   3.  11.  55.   6.]
 [  9.  15.  21.  11. 135.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.083 | Acc: 94.375% | Wgt Acc: 98.466%
	I - Batch: 100 | Loss: 0.083 | Acc: 94.625% | Wgt Acc: 98.522%
	I - Batch: 150 | Loss: 0.083 | Acc: 94.458% | Wgt Acc: 98.443%
	I - Batch: 200 | Loss: 0.084 | Acc: 94.500% | Wgt Acc: 98.428%
	I - Batch: 250 | Loss: 0.085 | Acc: 94.400% | Wgt Acc: 98.342%
I - num batch: 285
I - Train -- Loss: 0.085 | Acc: 94.242% | Wgt Acc: 98.284% | LR: 1.250000e-04 | Dur: 177.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    0.    0.    0.   70.]
 [   0.  779.    0.    1.   41.]
 [   1.    0. 1122.    0.   82.]
 [   2.    0.    0.  839.   57.]
 [   5.    1.    2.    0.  750.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.191 | Acc: 65.878% | Wgt Acc: 66.036% | Dur: 15.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   3.   6.  14.  17.]
 [  0.  40.   8.   1.  10.]
 [  2.  14.  41.   1.  26.]
 [  9.  10.  11.  64.   9.]
 [  6.  11.   9.   6. 118.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.092 | Acc: 92.875% | Wgt Acc: 97.671%
	I - Batch: 100 | Loss: 0.084 | Acc: 93.500% | Wgt Acc: 97.909%
	I - Batch: 150 | Loss: 0.084 | Acc: 93.375% | Wgt Acc: 97.930%
	I - Batch: 200 | Loss: 0.086 | Acc: 93.531% | Wgt Acc: 97.929%
	I - Batch: 250 | Loss: 0.089 | Acc: 93.425% | Wgt Acc: 97.885%
I - num batch: 285
I - Train -- Loss: 0.089 | Acc: 93.451% | Wgt Acc: 97.886% | LR: 1.250000e-04 | Dur: 170.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    0.    3.   78.]
 [   0.  778.    0.    1.   35.]
 [   0.    0. 1121.    1.   91.]
 [   3.    0.    1.  833.   73.]
 [   6.    2.    2.    2.  723.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.231 | Acc: 65.089% | Wgt Acc: 64.583% | Dur: 13.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   0.   4.  10.   9.]
 [  0.  36.   5.   1.   7.]
 [  1.  20.  42.   3.  28.]
 [ 14.   9.  10.  67.  16.]
 [  8.  13.  14.   5. 120.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.080 | Acc: 93.875% | Wgt Acc: 98.317%
	I - Batch: 100 | Loss: 0.081 | Acc: 94.312% | Wgt Acc: 98.122%
	I - Batch: 150 | Loss: 0.082 | Acc: 94.125% | Wgt Acc: 98.103%
	I - Batch: 200 | Loss: 0.085 | Acc: 93.750% | Wgt Acc: 97.964%
	I - Batch: 250 | Loss: 0.090 | Acc: 93.500% | Wgt Acc: 97.926%
I - num batch: 285
I - Train -- Loss: 0.088 | Acc: 93.692% | Wgt Acc: 98.009% | LR: 1.250000e-04 | Dur: 182.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 799.    0.    0.    2.   79.]
 [   0.  777.    0.    0.   33.]
 [   0.    0. 1122.    1.   98.]
 [   0.    1.    0.  834.   59.]
 [   7.    2.    2.    3.  731.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.284 | Acc: 63.116% | Wgt Acc: 62.519% | Dur: 20.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   5.  13.  18.]
 [  1.  37.   6.   1.   8.]
 [  1.  15.  33.   1.  19.]
 [ 15.  12.  16.  65.  18.]
 [  3.  11.  15.   6. 117.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.075 | Acc: 94.500% | Wgt Acc: 98.477%
	I - Batch: 100 | Loss: 0.080 | Acc: 93.750% | Wgt Acc: 98.289%
	I - Batch: 150 | Loss: 0.078 | Acc: 94.083% | Wgt Acc: 98.328%
	I - Batch: 200 | Loss: 0.083 | Acc: 93.656% | Wgt Acc: 98.100%
	I - Batch: 250 | Loss: 0.083 | Acc: 93.600% | Wgt Acc: 98.056%
I - num batch: 285
I - Train -- Loss: 0.085 | Acc: 93.429% | Wgt Acc: 97.985% | LR: 1.250000e-04 | Dur: 179.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 799.    0.    0.    1.   81.]
 [   0.  779.    0.    0.   40.]
 [   0.    0. 1122.    1.   85.]
 [   2.    0.    0.  834.   77.]
 [   5.    1.    2.    4.  717.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.207 | Acc: 64.103% | Wgt Acc: 59.613% | Dur: 15.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   6.   8.   6.]
 [  0.  34.   6.   5.  10.]
 [  1.  14.  30.   0.  14.]
 [ 19.   8.  15.  63.  13.]
 [  7.  19.  18.  10. 137.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 0.074 | Acc: 95.250% | Wgt Acc: 98.557%
	I - Batch: 100 | Loss: 0.078 | Acc: 94.312% | Wgt Acc: 98.238%
	I - Batch: 150 | Loss: 0.082 | Acc: 93.958% | Wgt Acc: 98.175%
	I - Batch: 200 | Loss: 0.079 | Acc: 94.031% | Wgt Acc: 98.272%
	I - Batch: 250 | Loss: 0.080 | Acc: 93.975% | Wgt Acc: 98.220%
I - num batch: 285
I - Train -- Loss: 0.081 | Acc: 93.890% | Wgt Acc: 98.207% | LR: 1.250000e-04 | Dur: 178.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 804.    1.    0.    0.   73.]
 [   0.  776.    0.    0.   45.]
 [   0.    0. 1122.    0.   86.]
 [   0.    0.    0.  837.   63.]
 [   2.    3.    2.    3.  733.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.189 | Acc: 65.286% | Wgt Acc: 59.094% | Dur: 16.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   8.  12.  13.]
 [  0.  29.   4.   0.   1.]
 [  3.  13.  25.   1.   8.]
 [ 12.   6.  13.  64.  11.]
 [  7.  28.  25.   9. 147.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 0.071 | Acc: 95.375% | Wgt Acc: 98.807%
	I - Batch: 100 | Loss: 0.078 | Acc: 94.750% | Wgt Acc: 98.459%
	I - Batch: 150 | Loss: 0.080 | Acc: 94.292% | Wgt Acc: 98.262%
	I - Batch: 200 | Loss: 0.080 | Acc: 94.156% | Wgt Acc: 98.144%
	I - Batch: 250 | Loss: 0.082 | Acc: 94.025% | Wgt Acc: 98.051%
I - num batch: 285
I - Train -- Loss: 0.084 | Acc: 93.714% | Wgt Acc: 97.996% | LR: 1.250000e-04 | Dur: 170.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    2.   80.]
 [   0.  778.    0.    1.   30.]
 [   0.    0. 1123.    0.   93.]
 [   3.    0.    0.  834.   64.]
 [   7.    2.    1.    3.  733.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.267 | Acc: 65.089% | Wgt Acc: 56.130% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   1.  17.   6.]
 [  0.  27.   3.   3.   6.]
 [  3.  12.  32.   1.  10.]
 [  7.   6.   4.  47.   0.]
 [ 12.  29.  35.  18. 158.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 0.066 | Acc: 96.000% | Wgt Acc: 98.402%
	I - Batch: 100 | Loss: 0.075 | Acc: 95.125% | Wgt Acc: 98.163%
	I - Batch: 150 | Loss: 0.080 | Acc: 94.375% | Wgt Acc: 98.001%
	I - Batch: 200 | Loss: 0.078 | Acc: 94.438% | Wgt Acc: 98.125%
	I - Batch: 250 | Loss: 0.083 | Acc: 94.025% | Wgt Acc: 97.965%
I - num batch: 285
I - Train -- Loss: 0.083 | Acc: 94.132% | Wgt Acc: 98.036% | LR: 1.250000e-04 | Dur: 184.62s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    0.    0.    2.   59.]
 [   0.  777.    0.    0.   37.]
 [   2.    0. 1121.    0.   87.]
 [   1.    0.    0.  832.   62.]
 [   5.    3.    3.    6.  755.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.173 | Acc: 67.258% | Wgt Acc: 66.209% | Dur: 15.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 74.   3.   7.  17.  18.]
 [  0.  42.   5.   2.  11.]
 [  1.  13.  37.   0.  17.]
 [  9.   8.  10.  62.   8.]
 [  4.  12.  16.   5. 126.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 0.076 | Acc: 94.250% | Wgt Acc: 98.187%
	I - Batch: 100 | Loss: 0.078 | Acc: 94.750% | Wgt Acc: 98.299%
	I - Batch: 150 | Loss: 0.080 | Acc: 94.458% | Wgt Acc: 98.261%
	I - Batch: 200 | Loss: 0.080 | Acc: 94.469% | Wgt Acc: 98.203%
	I - Batch: 250 | Loss: 0.083 | Acc: 94.175% | Wgt Acc: 98.079%
I - num batch: 285
I - Train -- Loss: 0.083 | Acc: 94.176% | Wgt Acc: 98.048% | LR: 1.250000e-04 | Dur: 180.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 798.    1.    0.    4.   67.]
 [   0.  774.    0.    1.   31.]
 [   1.    1. 1122.    0.   81.]
 [   3.    1.    0.  834.   64.]
 [   4.    3.    2.    1.  757.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.264 | Acc: 65.878% | Wgt Acc: 60.281% | Dur: 16.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   7.  15.  10.]
 [  0.  31.   3.   1.   3.]
 [  0.   9.  30.   1.  12.]
 [ 14.   5.   9.  62.  10.]
 [  8.  30.  26.   7. 145.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 0.082 | Acc: 93.500% | Wgt Acc: 97.981%
	I - Batch: 100 | Loss: 0.085 | Acc: 94.062% | Wgt Acc: 98.124%
	I - Batch: 150 | Loss: 0.087 | Acc: 93.792% | Wgt Acc: 97.989%
	I - Batch: 200 | Loss: 0.084 | Acc: 93.969% | Wgt Acc: 98.084%
	I - Batch: 250 | Loss: 0.085 | Acc: 93.700% | Wgt Acc: 97.949%
I - num batch: 285
I - Train -- Loss: 0.086 | Acc: 93.780% | Wgt Acc: 98.006% | LR: 1.250000e-04 | Dur: 177.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   78.]
 [   0.  778.    0.    0.   29.]
 [   3.    0. 1119.    0.   98.]
 [   2.    0.    0.  840.   58.]
 [   8.    2.    5.    0.  737.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.328 | Acc: 63.708% | Wgt Acc: 61.146% | Dur: 14.78s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.   4.   6.  20.  20.]
 [  0.  31.   4.   1.   2.]
 [  1.  19.  38.   1.  23.]
 [  8.   7.  12.  55.   9.]
 [  6.  17.  15.   9. 126.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 0.056 | Acc: 96.625% | Wgt Acc: 99.050%
	I - Batch: 100 | Loss: 0.070 | Acc: 95.188% | Wgt Acc: 98.553%
	I - Batch: 150 | Loss: 0.071 | Acc: 94.833% | Wgt Acc: 98.419%
	I - Batch: 200 | Loss: 0.077 | Acc: 94.250% | Wgt Acc: 98.063%
	I - Batch: 250 | Loss: 0.079 | Acc: 93.975% | Wgt Acc: 98.010%
I - num batch: 285
I - Train -- Loss: 0.079 | Acc: 94.022% | Wgt Acc: 98.014% | LR: 1.250000e-04 | Dur: 174.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    1.    1.    0.   67.]
 [   0.  776.    0.    0.   28.]
 [   1.    0. 1120.    1.   90.]
 [   1.    1.    1.  835.   65.]
 [   7.    2.    2.    4.  750.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 64.497% | Wgt Acc: 60.270% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   3.  14.   6.]
 [  0.  37.   8.   2.   6.]
 [  1.  12.  36.   1.  20.]
 [ 17.   6.  10.  58.  12.]
 [ 10.  20.  18.  11. 136.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 0.064 | Acc: 95.750% | Wgt Acc: 98.824%
	I - Batch: 100 | Loss: 0.071 | Acc: 95.250% | Wgt Acc: 98.614%
	I - Batch: 150 | Loss: 0.075 | Acc: 94.750% | Wgt Acc: 98.304%
	I - Batch: 200 | Loss: 0.077 | Acc: 94.688% | Wgt Acc: 98.240%
	I - Batch: 250 | Loss: 0.079 | Acc: 94.375% | Wgt Acc: 98.232%
I - num batch: 285
I - Train -- Loss: 0.077 | Acc: 94.505% | Wgt Acc: 98.284% | LR: 1.250000e-04 | Dur: 173.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 801.    0.    0.    0.   60.]
 [   1.  777.    0.    0.   37.]
 [   1.    1. 1120.    1.   81.]
 [   0.    0.    1.  837.   57.]
 [   3.    2.    3.    2.  765.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.174 | Acc: 67.258% | Wgt Acc: 63.176% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   3.   9.   8.]
 [  0.  33.   2.   1.   2.]
 [  1.  14.  41.   1.  26.]
 [ 12.   8.  12.  61.   4.]
 [  9.  20.  17.  14. 140.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 0.080 | Acc: 94.500% | Wgt Acc: 98.105%
	I - Batch: 100 | Loss: 0.079 | Acc: 94.312% | Wgt Acc: 98.157%
	I - Batch: 150 | Loss: 0.079 | Acc: 94.500% | Wgt Acc: 98.180%
	I - Batch: 200 | Loss: 0.077 | Acc: 94.594% | Wgt Acc: 98.289%
	I - Batch: 250 | Loss: 0.077 | Acc: 94.200% | Wgt Acc: 98.167%
I - num batch: 285
I - Train -- Loss: 0.078 | Acc: 94.198% | Wgt Acc: 98.163% | LR: 1.250000e-04 | Dur: 175.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    0.    0.   71.]
 [   0.  775.    0.    0.   43.]
 [   0.    1. 1124.    0.   76.]
 [   1.    0.    0.  837.   57.]
 [   8.    4.    0.    3.  753.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.222 | Acc: 64.497% | Wgt Acc: 58.955% | Dur: 18.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   3.  14.  11.]
 [  0.  33.   5.   1.   3.]
 [  3.  16.  32.   0.  14.]
 [ 12.   6.  13.  57.  10.]
 [ 10.  21.  22.  14. 142.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 0.058 | Acc: 95.625% | Wgt Acc: 98.900%
	I - Batch: 100 | Loss: 0.062 | Acc: 96.062% | Wgt Acc: 98.947%
	I - Batch: 150 | Loss: 0.067 | Acc: 95.542% | Wgt Acc: 98.757%
	I - Batch: 200 | Loss: 0.070 | Acc: 95.125% | Wgt Acc: 98.589%
	I - Batch: 250 | Loss: 0.070 | Acc: 95.100% | Wgt Acc: 98.613%
I - num batch: 285
I - Train -- Loss: 0.070 | Acc: 95.099% | Wgt Acc: 98.584% | LR: 1.250000e-04 | Dur: 173.83s
I - Confusion Matrix: [row->prediction - col->label]
[[ 803.    0.    0.    0.   56.]
 [   0.  779.    0.    0.   34.]
 [   1.    0. 1121.    1.   66.]
 [   0.    0.    0.  839.   59.]
 [   2.    1.    3.    0.  785.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.255 | Acc: 65.286% | Wgt Acc: 60.708% | Dur: 14.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   3.   4.  15.  14.]
 [  0.  37.   7.   1.   3.]
 [  0.  11.  27.   1.  13.]
 [ 10.   5.  12.  58.  11.]
 [  8.  22.  25.  11. 139.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 0.068 | Acc: 95.000% | Wgt Acc: 98.453%
	I - Batch: 100 | Loss: 0.068 | Acc: 95.000% | Wgt Acc: 98.525%
	I - Batch: 150 | Loss: 0.069 | Acc: 95.167% | Wgt Acc: 98.642%
	I - Batch: 200 | Loss: 0.070 | Acc: 94.875% | Wgt Acc: 98.543%
	I - Batch: 250 | Loss: 0.070 | Acc: 94.825% | Wgt Acc: 98.511%
I - num batch: 285
I - Train -- Loss: 0.072 | Acc: 94.549% | Wgt Acc: 98.441% | LR: 1.250000e-04 | Dur: 173.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 803.    0.    0.    0.   62.]
 [   0.  779.    0.    0.   38.]
 [   0.    0. 1122.    0.   75.]
 [   0.    0.    0.  838.   65.]
 [   3.    1.    2.    2.  760.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.198 | Acc: 65.483% | Wgt Acc: 62.726% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   4.   5.   7.]
 [  1.  35.   4.   2.   5.]
 [  2.  13.  34.   2.  20.]
 [ 21.  10.  17.  72.  16.]
 [  5.  18.  16.   5. 132.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 0.065 | Acc: 96.125% | Wgt Acc: 98.990%
	I - Batch: 100 | Loss: 0.074 | Acc: 95.188% | Wgt Acc: 98.558%
	I - Batch: 150 | Loss: 0.071 | Acc: 94.958% | Wgt Acc: 98.540%
	I - Batch: 200 | Loss: 0.069 | Acc: 95.062% | Wgt Acc: 98.614%
	I - Batch: 250 | Loss: 0.071 | Acc: 94.775% | Wgt Acc: 98.473%
I - num batch: 285
I - Train -- Loss: 0.072 | Acc: 94.747% | Wgt Acc: 98.449% | LR: 1.250000e-04 | Dur: 176.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 804.    0.    0.    1.   58.]
 [   0.  779.    0.    1.   27.]
 [   0.    0. 1120.    0.   80.]
 [   0.    0.    0.  837.   64.]
 [   2.    1.    4.    1.  771.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.167 | Acc: 63.116% | Wgt Acc: 59.970% | Dur: 16.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   1.   2.  14.  13.]
 [  0.  36.   5.   1.   7.]
 [  2.  17.  36.   6.  24.]
 [ 12.   6.  10.  54.   8.]
 [  8.  18.  22.  11. 128.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 0.096 | Acc: 92.125% | Wgt Acc: 97.096%
	I - Batch: 100 | Loss: 0.087 | Acc: 93.500% | Wgt Acc: 97.837%
	I - Batch: 150 | Loss: 0.081 | Acc: 94.000% | Wgt Acc: 98.037%
	I - Batch: 200 | Loss: 0.078 | Acc: 94.062% | Wgt Acc: 98.098%
	I - Batch: 250 | Loss: 0.076 | Acc: 94.100% | Wgt Acc: 98.134%
I - num batch: 285
I - Train -- Loss: 0.075 | Acc: 94.220% | Wgt Acc: 98.163% | LR: 1.250000e-04 | Dur: 173.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 801.    0.    0.    0.   72.]
 [   0.  776.    0.    2.   29.]
 [   1.    0. 1122.    0.   85.]
 [   1.    1.    1.  834.   60.]
 [   3.    3.    1.    4.  754.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.181 | Acc: 66.469% | Wgt Acc: 64.941% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   0.   3.   9.   9.]
 [  0.  36.   6.   1.   7.]
 [  1.  19.  45.   4.  27.]
 [ 13.   8.  10.  65.  10.]
 [ 10.  15.  11.   7. 127.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 806.  780. 1124.  840. 1000.]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 0.072 | Acc: 96.000% | Wgt Acc: 98.778%
	I - Batch: 100 | Loss: 0.076 | Acc: 95.062% | Wgt Acc: 98.561%
	I - Batch: 150 | Loss: 0.075 | Acc: 95.042% | Wgt Acc: 98.383%
