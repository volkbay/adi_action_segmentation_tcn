Sat Oct 29 16:23:17 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.23, 0.24, 0.23, 0.25, 0.1], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'singleBackgroundPath': 'new_background', 'singleBackgroundPickle': True, 'tossFirstLastFrames': True}, 'dataPath': '/data_ssd/processed/kinetics400/', 'dropoutRate': 0.6, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat', 'background'], 'lastLayerInitUniform': False, 'learningRate': 0.0001, 'logBatchAt': 50, 'maxValidationAcc': 71.20315581854044, 'maxValidationTrainNo': 64, 'modelVersion': 17, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 66, 'validationAccThr': 70, 'warmStartConfig': {'checkpointFile': './sav/model17_trainNo60_at_epoch_197_with_acc_71_60_checkpoint.pth.tar', 'checkpointModelNo': 17, 'freezeSpatialCNN': False, 'warmStartFlag': False}, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Train set length:  3547
I - Label distribution: [ 697.  578.  734.  538. 1000.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test
I - Loading file: dataset_test_background00_no_samples180.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test/new_background
I - New label distribution: [ 88.  78.  75.  86. 180.]

I - Test set length:  507
I - Label distribution: [ 88.  78.  75.  86. 180.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(4) data number in dataset:  tensor([ 28869, 176675,  32487,    498,    523, 141150,  24821, 182728, 158220,
         48278, 125074,  20813,    860,   1778,    383, 121265])
I - Initializing model TCNv17
I - Number of Model Parameters: 638752
I - Model output shape:  torch.Size([16, 5])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv17                                   [16, 5]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 128, 64, 64]         6
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Conv2d: 2-2                       --                        6,272
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─OutputShiftSqueeze: 2-4           --                        --
│    └─One: 2-5                          [1]                       --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─OutputScale: 2-7                  --                        --
│    └─Empty: 2-8                        [128, 48, 1, 1]           --
│    └─Empty: 2-9                        [128, 48, 1, 1]           --
│    └─Empty: 2-10                       [128]                     --
│    └─Empty: 2-11                       [128]                     --
│    └─BatchNorm2d: 2-12                 [16, 128, 64, 64]         --
│    └─Scaler: 2-13                      [16, 128, 64, 64]         --
│    └─ReLU: 2-14                        [16, 128, 64, 64]         --
│    └─Empty: 2-15                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Conv2d: 2-18                      --                        147,584
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-20                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-3          [16, 128, 32, 32]         147,590
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-22                   [16, 128, 32, 32]         --
│    └─Empty: 2-23                       [16, 128, 32, 32]         --
│    └─Empty: 2-24                       [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [128, 128, 3, 3]          --
│    └─Empty: 2-29                       [128, 128, 3, 3]          --
│    └─Empty: 2-30                       [128]                     --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-33                      --                        147,584
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-35                       [128]                     --
│    └─BatchNorm2d: 2-36                 [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-38                      [16, 128, 32, 32]         --
│    └─ReLU: 2-39                        [16, 128, 32, 32]         --
│    └─Empty: 2-40                       [16, 128, 32, 32]         --
│    └─Clamp: 2-41                       [16, 128, 32, 32]         --
├─Dropout2d: 1-5                         [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-6          [16, 128, 16, 16]         131,078
│    └─MaxPool2d: 2-42                   [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─Empty: 2-45                       [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Conv2d: 2-47                      --                        16,512
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-49                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-50          --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-52                         [1]                       --
│    └─OutputScale: 2-53                 --                        --
│    └─Empty: 2-54                       [128, 128, 3, 3]          --
│    └─Empty: 2-55                       [128, 128, 3, 3]          --
│    └─Empty: 2-56                       [128]                     --
│    └─Empty: 2-57                       [128]                     --
│    └─BatchNorm2d: 2-58                 [16, 128, 16, 16]         --
│    └─Scaler: 2-59                      [16, 128, 16, 16]         --
│    └─ReLU: 2-60                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-63                      --                        147,584
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-65                       [16, 128, 16, 16]         --
│    └─Clamp: 2-66                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-8                 [16, 128, 16, 16]         16,518
│    └─OutputShiftSqueeze: 2-68          --                        --
│    └─One: 2-69                         [1]                       --
│    └─OutputScale: 2-70                 --                        --
│    └─Empty: 2-71                       [128, 128, 1, 1]          --
│    └─Empty: 2-72                       [128, 128, 1, 1]          --
│    └─Empty: 2-73                       [128]                     --
│    └─Empty: 2-74                       [128]                     --
│    └─BatchNorm2d: 2-75                 [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-78                      --                        147,584
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-80                      [16, 128, 16, 16]         --
│    └─ReLU: 2-81                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-83                       [16, 128, 16, 16]         --
│    └─Clamp: 2-84                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-10         [16, 128, 16, 16]         145,526
│    └─MaxPool2d: 2-85                   [16, 128, 16, 16]         --
│    └─Empty: 2-86                       [16, 128, 16, 16]         --
│    └─Empty: 2-87                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-88          --                        --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-91                         [1]                       --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Conv2d: 2-93                      --                        2,064
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-95                 --                        --
│    └─Empty: 2-96                       [128, 128, 3, 3]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-98                       [128, 128, 3, 3]          --
│    └─Empty: 2-99                       [128]                     --
│    └─Empty: 2-100                      [128]                     --
│    └─BatchNorm2d: 2-101                [16, 128, 16, 16]         --
│    └─Scaler: 2-102                     [16, 128, 16, 16]         --
│    └─ReLU: 2-103                       [16, 128, 16, 16]         --
│    └─Empty: 2-104                      [16, 128, 16, 16]         --
│    └─Clamp: 2-105                      [16, 128, 16, 16]         --
├─Dropout2d: 1-11                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-108                     --                        18,448
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-13         [16, 128, 8, 8]           147,590
│    └─MaxPool2d: 2-110                  [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-112                      [16, 128, 8, 8]           --
│    └─Empty: 2-113                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-114         --                        --
│    └─One: 2-115                        [1]                       --
│    └─OutputScale: 2-116                --                        --
│    └─Empty: 2-117                      [128, 128, 3, 3]          --
│    └─Empty: 2-118                      [128, 128, 3, 3]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-121                      [128]                     --
│    └─Empty: 2-122                      [128]                     --
│    └─BatchNorm2d: 2-123                [16, 128, 8, 8]           --
│    └─Scaler: 2-124                     [16, 128, 8, 8]           --
│    └─ReLU: 2-125                       [16, 128, 8, 8]           --
│    └─Empty: 2-126                      [16, 128, 8, 8]           --
│    └─Clamp: 2-127                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-14                [16, 16, 8, 8]            2,070
├─Linear: 1                              --                        --
│    └─Scaler: 2-128                     --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-129         --                        --
│    └─One: 2-130                        [1]                       --
│    └─OutputScale: 2-131                --                        --
│    └─Empty: 2-132                      [16, 128, 1, 1]           --
│    └─Empty: 2-133                      [16, 128, 1, 1]           --
│    └─Empty: 2-134                      [16]                      --
│    └─Empty: 2-135                      [16]                      --
│    └─BatchNorm2d: 2-136                [16, 16, 8, 8]            --
│    └─Scaler: 2-137                     [16, 16, 8, 8]            --
│    └─ReLU: 2-138                       [16, 16, 8, 8]            --
│    └─Empty: 2-139                      [16, 16, 8, 8]            --
│    └─Clamp: 2-140                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 16, 8, 8]            18,454
│    └─MaxPool2d: 2-141                  [16, 128, 8, 8]           --
│    └─Empty: 2-142                      [16, 128, 8, 8]           --
│    └─Empty: 2-143                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-144         --                        --
│    └─One: 2-145                        [1]                       --
│    └─OutputScale: 2-146                --                        --
│    └─Empty: 2-147                      [16, 128, 3, 3]           --
│    └─Empty: 2-148                      [16, 128, 3, 3]           --
│    └─Empty: 2-149                      [16]                      --
│    └─Empty: 2-150                      [16]                      --
│    └─BatchNorm2d: 2-151                [16, 16, 8, 8]            --
│    └─Scaler: 2-152                     [16, 16, 8, 8]            --
│    └─ReLU: 2-153                       [16, 16, 8, 8]            --
│    └─Empty: 2-154                      [16, 16, 8, 8]            --
│    └─Clamp: 2-155                      [16, 16, 8, 8]            --
├─Dropout2d: 1-16                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-17                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-156         --                        --
│    └─One: 2-157                        [1]                       --
│    └─OutputScale: 2-158                --                        --
│    └─Empty: 2-159                      [128, 48, 1, 1]           --
│    └─Empty: 2-160                      [128, 48, 1, 1]           --
│    └─Empty: 2-161                      [128]                     --
│    └─Empty: 2-162                      [128]                     --
│    └─BatchNorm2d: 2-163                [16, 128, 64, 64]         --
│    └─Scaler: 2-164                     [16, 128, 64, 64]         --
│    └─ReLU: 2-165                       [16, 128, 64, 64]         --
│    └─Empty: 2-166                      [16, 128, 64, 64]         --
│    └─Clamp: 2-167                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-18         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-168                  [16, 128, 32, 32]         --
│    └─Empty: 2-169                      [16, 128, 32, 32]         --
│    └─Empty: 2-170                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-171         --                        --
│    └─One: 2-172                        [1]                       --
│    └─OutputScale: 2-173                --                        --
│    └─Empty: 2-174                      [128, 128, 3, 3]          --
│    └─Empty: 2-175                      [128, 128, 3, 3]          --
│    └─Empty: 2-176                      [128]                     --
│    └─Empty: 2-177                      [128]                     --
│    └─BatchNorm2d: 2-178                [16, 128, 32, 32]         --
│    └─Scaler: 2-179                     [16, 128, 32, 32]         --
│    └─ReLU: 2-180                       [16, 128, 32, 32]         --
│    └─Empty: 2-181                      [16, 128, 32, 32]         --
│    └─Clamp: 2-182                      [16, 128, 32, 32]         --
├─Dropout2d: 1-19                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-20         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-183                  [16, 128, 16, 16]         --
│    └─Empty: 2-184                      [16, 128, 16, 16]         --
│    └─Empty: 2-185                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-186         --                        --
│    └─One: 2-187                        [1]                       --
│    └─OutputScale: 2-188                --                        --
│    └─Empty: 2-189                      [128, 128, 3, 3]          --
│    └─Empty: 2-190                      [128, 128, 3, 3]          --
│    └─Empty: 2-191                      [128]                     --
│    └─Empty: 2-192                      [128]                     --
│    └─BatchNorm2d: 2-193                [16, 128, 16, 16]         --
│    └─Scaler: 2-194                     [16, 128, 16, 16]         --
│    └─ReLU: 2-195                       [16, 128, 16, 16]         --
│    └─Empty: 2-196                      [16, 128, 16, 16]         --
│    └─Clamp: 2-197                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-21                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-198         --                        --
│    └─One: 2-199                        [1]                       --
│    └─OutputScale: 2-200                --                        --
│    └─Empty: 2-201                      [128, 128, 1, 1]          --
│    └─Empty: 2-202                      [128, 128, 1, 1]          --
│    └─Empty: 2-203                      [128]                     --
│    └─Empty: 2-204                      [128]                     --
│    └─BatchNorm2d: 2-205                [16, 128, 16, 16]         --
│    └─Scaler: 2-206                     [16, 128, 16, 16]         --
│    └─ReLU: 2-207                       [16, 128, 16, 16]         --
│    └─Empty: 2-208                      [16, 128, 16, 16]         --
│    └─Clamp: 2-209                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-22         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-210                  [16, 128, 16, 16]         --
│    └─Empty: 2-211                      [16, 128, 16, 16]         --
│    └─Empty: 2-212                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-213         --                        --
│    └─One: 2-214                        [1]                       --
│    └─OutputScale: 2-215                --                        --
│    └─Empty: 2-216                      [128, 128, 3, 3]          --
│    └─Empty: 2-217                      [128, 128, 3, 3]          --
│    └─Empty: 2-218                      [128]                     --
│    └─Empty: 2-219                      [128]                     --
│    └─BatchNorm2d: 2-220                [16, 128, 16, 16]         --
│    └─Scaler: 2-221                     [16, 128, 16, 16]         --
│    └─ReLU: 2-222                       [16, 128, 16, 16]         --
│    └─Empty: 2-223                      [16, 128, 16, 16]         --
│    └─Clamp: 2-224                      [16, 128, 16, 16]         --
├─Dropout2d: 1-23                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-24         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-225                  [16, 128, 8, 8]           --
│    └─Empty: 2-226                      [16, 128, 8, 8]           --
│    └─Empty: 2-227                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-228         --                        --
│    └─One: 2-229                        [1]                       --
│    └─OutputScale: 2-230                --                        --
│    └─Empty: 2-231                      [128, 128, 3, 3]          --
│    └─Empty: 2-232                      [128, 128, 3, 3]          --
│    └─Empty: 2-233                      [128]                     --
│    └─Empty: 2-234                      [128]                     --
│    └─BatchNorm2d: 2-235                [16, 128, 8, 8]           --
│    └─Scaler: 2-236                     [16, 128, 8, 8]           --
│    └─ReLU: 2-237                       [16, 128, 8, 8]           --
│    └─Empty: 2-238                      [16, 128, 8, 8]           --
│    └─Clamp: 2-239                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-25                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-240         --                        --
│    └─One: 2-241                        [1]                       --
│    └─OutputScale: 2-242                --                        --
│    └─Empty: 2-243                      [16, 128, 1, 1]           --
│    └─Empty: 2-244                      [16, 128, 1, 1]           --
│    └─Empty: 2-245                      [16]                      --
│    └─Empty: 2-246                      [16]                      --
│    └─BatchNorm2d: 2-247                [16, 16, 8, 8]            --
│    └─Scaler: 2-248                     [16, 16, 8, 8]            --
│    └─ReLU: 2-249                       [16, 16, 8, 8]            --
│    └─Empty: 2-250                      [16, 16, 8, 8]            --
│    └─Clamp: 2-251                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-252                  [16, 128, 8, 8]           --
│    └─Empty: 2-253                      [16, 128, 8, 8]           --
│    └─Empty: 2-254                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-255         --                        --
│    └─One: 2-256                        [1]                       --
│    └─OutputScale: 2-257                --                        --
│    └─Empty: 2-258                      [16, 128, 3, 3]           --
│    └─Empty: 2-259                      [16, 128, 3, 3]           --
│    └─Empty: 2-260                      [16]                      --
│    └─Empty: 2-261                      [16]                      --
│    └─BatchNorm2d: 2-262                [16, 16, 8, 8]            --
│    └─Scaler: 2-263                     [16, 16, 8, 8]            --
│    └─ReLU: 2-264                       [16, 16, 8, 8]            --
│    └─Empty: 2-265                      [16, 16, 8, 8]            --
│    └─Clamp: 2-266                      [16, 16, 8, 8]            --
├─Dropout2d: 1-27                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-28                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-267         --                        --
│    └─One: 2-268                        [1]                       --
│    └─OutputScale: 2-269                --                        --
│    └─Empty: 2-270                      [128, 48, 1, 1]           --
│    └─Empty: 2-271                      [128, 48, 1, 1]           --
│    └─Empty: 2-272                      [128]                     --
│    └─Empty: 2-273                      [128]                     --
│    └─BatchNorm2d: 2-274                [16, 128, 64, 64]         --
│    └─Scaler: 2-275                     [16, 128, 64, 64]         --
│    └─ReLU: 2-276                       [16, 128, 64, 64]         --
│    └─Empty: 2-277                      [16, 128, 64, 64]         --
│    └─Clamp: 2-278                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-29         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-279                  [16, 128, 32, 32]         --
│    └─Empty: 2-280                      [16, 128, 32, 32]         --
│    └─Empty: 2-281                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-282         --                        --
│    └─One: 2-283                        [1]                       --
│    └─OutputScale: 2-284                --                        --
│    └─Empty: 2-285                      [128, 128, 3, 3]          --
│    └─Empty: 2-286                      [128, 128, 3, 3]          --
│    └─Empty: 2-287                      [128]                     --
│    └─Empty: 2-288                      [128]                     --
│    └─BatchNorm2d: 2-289                [16, 128, 32, 32]         --
│    └─Scaler: 2-290                     [16, 128, 32, 32]         --
│    └─ReLU: 2-291                       [16, 128, 32, 32]         --
│    └─Empty: 2-292                      [16, 128, 32, 32]         --
│    └─Clamp: 2-293                      [16, 128, 32, 32]         --
├─Dropout2d: 1-30                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-294                  [16, 128, 16, 16]         --
│    └─Empty: 2-295                      [16, 128, 16, 16]         --
│    └─Empty: 2-296                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-297         --                        --
│    └─One: 2-298                        [1]                       --
│    └─OutputScale: 2-299                --                        --
│    └─Empty: 2-300                      [128, 128, 3, 3]          --
│    └─Empty: 2-301                      [128, 128, 3, 3]          --
│    └─Empty: 2-302                      [128]                     --
│    └─Empty: 2-303                      [128]                     --
│    └─BatchNorm2d: 2-304                [16, 128, 16, 16]         --
│    └─Scaler: 2-305                     [16, 128, 16, 16]         --
│    └─ReLU: 2-306                       [16, 128, 16, 16]         --
│    └─Empty: 2-307                      [16, 128, 16, 16]         --
│    └─Clamp: 2-308                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-32                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-309         --                        --
│    └─One: 2-310                        [1]                       --
│    └─OutputScale: 2-311                --                        --
│    └─Empty: 2-312                      [128, 128, 1, 1]          --
│    └─Empty: 2-313                      [128, 128, 1, 1]          --
│    └─Empty: 2-314                      [128]                     --
│    └─Empty: 2-315                      [128]                     --
│    └─BatchNorm2d: 2-316                [16, 128, 16, 16]         --
│    └─Scaler: 2-317                     [16, 128, 16, 16]         --
│    └─ReLU: 2-318                       [16, 128, 16, 16]         --
│    └─Empty: 2-319                      [16, 128, 16, 16]         --
│    └─Clamp: 2-320                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-33         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-321                  [16, 128, 16, 16]         --
│    └─Empty: 2-322                      [16, 128, 16, 16]         --
│    └─Empty: 2-323                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-324         --                        --
│    └─One: 2-325                        [1]                       --
│    └─OutputScale: 2-326                --                        --
│    └─Empty: 2-327                      [128, 128, 3, 3]          --
│    └─Empty: 2-328                      [128, 128, 3, 3]          --
│    └─Empty: 2-329                      [128]                     --
│    └─Empty: 2-330                      [128]                     --
│    └─BatchNorm2d: 2-331                [16, 128, 16, 16]         --
│    └─Scaler: 2-332                     [16, 128, 16, 16]         --
│    └─ReLU: 2-333                       [16, 128, 16, 16]         --
│    └─Empty: 2-334                      [16, 128, 16, 16]         --
│    └─Clamp: 2-335                      [16, 128, 16, 16]         --
├─Dropout2d: 1-34                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-35         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-336                  [16, 128, 8, 8]           --
│    └─Empty: 2-337                      [16, 128, 8, 8]           --
│    └─Empty: 2-338                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-339         --                        --
│    └─One: 2-340                        [1]                       --
│    └─OutputScale: 2-341                --                        --
│    └─Empty: 2-342                      [128, 128, 3, 3]          --
│    └─Empty: 2-343                      [128, 128, 3, 3]          --
│    └─Empty: 2-344                      [128]                     --
│    └─Empty: 2-345                      [128]                     --
│    └─BatchNorm2d: 2-346                [16, 128, 8, 8]           --
│    └─Scaler: 2-347                     [16, 128, 8, 8]           --
│    └─ReLU: 2-348                       [16, 128, 8, 8]           --
│    └─Empty: 2-349                      [16, 128, 8, 8]           --
│    └─Clamp: 2-350                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-36                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-351         --                        --
│    └─One: 2-352                        [1]                       --
│    └─OutputScale: 2-353                --                        --
│    └─Empty: 2-354                      [16, 128, 1, 1]           --
│    └─Empty: 2-355                      [16, 128, 1, 1]           --
│    └─Empty: 2-356                      [16]                      --
│    └─Empty: 2-357                      [16]                      --
│    └─BatchNorm2d: 2-358                [16, 16, 8, 8]            --
│    └─Scaler: 2-359                     [16, 16, 8, 8]            --
│    └─ReLU: 2-360                       [16, 16, 8, 8]            --
│    └─Empty: 2-361                      [16, 16, 8, 8]            --
│    └─Clamp: 2-362                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-37         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-363                  [16, 128, 8, 8]           --
│    └─Empty: 2-364                      [16, 128, 8, 8]           --
│    └─Empty: 2-365                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-366         --                        --
│    └─One: 2-367                        [1]                       --
│    └─OutputScale: 2-368                --                        --
│    └─Empty: 2-369                      [16, 128, 3, 3]           --
│    └─Empty: 2-370                      [16, 128, 3, 3]           --
│    └─Empty: 2-371                      [16]                      --
│    └─Empty: 2-372                      [16]                      --
│    └─BatchNorm2d: 2-373                [16, 16, 8, 8]            --
│    └─Scaler: 2-374                     [16, 16, 8, 8]            --
│    └─ReLU: 2-375                       [16, 16, 8, 8]            --
│    └─Empty: 2-376                      [16, 16, 8, 8]            --
│    └─Clamp: 2-377                      [16, 16, 8, 8]            --
├─Dropout2d: 1-38                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-39                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-378         --                        --
│    └─One: 2-379                        [1]                       --
│    └─OutputScale: 2-380                --                        --
│    └─Empty: 2-381                      [128, 48, 1, 1]           --
│    └─Empty: 2-382                      [128, 48, 1, 1]           --
│    └─Empty: 2-383                      [128]                     --
│    └─Empty: 2-384                      [128]                     --
│    └─BatchNorm2d: 2-385                [16, 128, 64, 64]         --
│    └─Scaler: 2-386                     [16, 128, 64, 64]         --
│    └─ReLU: 2-387                       [16, 128, 64, 64]         --
│    └─Empty: 2-388                      [16, 128, 64, 64]         --
│    └─Clamp: 2-389                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-40         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-390                  [16, 128, 32, 32]         --
│    └─Empty: 2-391                      [16, 128, 32, 32]         --
│    └─Empty: 2-392                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-393         --                        --
│    └─One: 2-394                        [1]                       --
│    └─OutputScale: 2-395                --                        --
│    └─Empty: 2-396                      [128, 128, 3, 3]          --
│    └─Empty: 2-397                      [128, 128, 3, 3]          --
│    └─Empty: 2-398                      [128]                     --
│    └─Empty: 2-399                      [128]                     --
│    └─BatchNorm2d: 2-400                [16, 128, 32, 32]         --
│    └─Scaler: 2-401                     [16, 128, 32, 32]         --
│    └─ReLU: 2-402                       [16, 128, 32, 32]         --
│    └─Empty: 2-403                      [16, 128, 32, 32]         --
│    └─Clamp: 2-404                      [16, 128, 32, 32]         --
├─Dropout2d: 1-41                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-42         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-405                  [16, 128, 16, 16]         --
│    └─Empty: 2-406                      [16, 128, 16, 16]         --
│    └─Empty: 2-407                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-408         --                        --
│    └─One: 2-409                        [1]                       --
│    └─OutputScale: 2-410                --                        --
│    └─Empty: 2-411                      [128, 128, 3, 3]          --
│    └─Empty: 2-412                      [128, 128, 3, 3]          --
│    └─Empty: 2-413                      [128]                     --
│    └─Empty: 2-414                      [128]                     --
│    └─BatchNorm2d: 2-415                [16, 128, 16, 16]         --
│    └─Scaler: 2-416                     [16, 128, 16, 16]         --
│    └─ReLU: 2-417                       [16, 128, 16, 16]         --
│    └─Empty: 2-418                      [16, 128, 16, 16]         --
│    └─Clamp: 2-419                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-43                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-420         --                        --
│    └─One: 2-421                        [1]                       --
│    └─OutputScale: 2-422                --                        --
│    └─Empty: 2-423                      [128, 128, 1, 1]          --
│    └─Empty: 2-424                      [128, 128, 1, 1]          --
│    └─Empty: 2-425                      [128]                     --
│    └─Empty: 2-426                      [128]                     --
│    └─BatchNorm2d: 2-427                [16, 128, 16, 16]         --
│    └─Scaler: 2-428                     [16, 128, 16, 16]         --
│    └─ReLU: 2-429                       [16, 128, 16, 16]         --
│    └─Empty: 2-430                      [16, 128, 16, 16]         --
│    └─Clamp: 2-431                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-44         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-432                  [16, 128, 16, 16]         --
│    └─Empty: 2-433                      [16, 128, 16, 16]         --
│    └─Empty: 2-434                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-435         --                        --
│    └─One: 2-436                        [1]                       --
│    └─OutputScale: 2-437                --                        --
│    └─Empty: 2-438                      [128, 128, 3, 3]          --
│    └─Empty: 2-439                      [128, 128, 3, 3]          --
│    └─Empty: 2-440                      [128]                     --
│    └─Empty: 2-441                      [128]                     --
│    └─BatchNorm2d: 2-442                [16, 128, 16, 16]         --
│    └─Scaler: 2-443                     [16, 128, 16, 16]         --
│    └─ReLU: 2-444                       [16, 128, 16, 16]         --
│    └─Empty: 2-445                      [16, 128, 16, 16]         --
│    └─Clamp: 2-446                      [16, 128, 16, 16]         --
├─Dropout2d: 1-45                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-447                  [16, 128, 8, 8]           --
│    └─Empty: 2-448                      [16, 128, 8, 8]           --
│    └─Empty: 2-449                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-450         --                        --
│    └─One: 2-451                        [1]                       --
│    └─OutputScale: 2-452                --                        --
│    └─Empty: 2-453                      [128, 128, 3, 3]          --
│    └─Empty: 2-454                      [128, 128, 3, 3]          --
│    └─Empty: 2-455                      [128]                     --
│    └─Empty: 2-456                      [128]                     --
│    └─BatchNorm2d: 2-457                [16, 128, 8, 8]           --
│    └─Scaler: 2-458                     [16, 128, 8, 8]           --
│    └─ReLU: 2-459                       [16, 128, 8, 8]           --
│    └─Empty: 2-460                      [16, 128, 8, 8]           --
│    └─Clamp: 2-461                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-47                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-462         --                        --
│    └─One: 2-463                        [1]                       --
│    └─OutputScale: 2-464                --                        --
│    └─Empty: 2-465                      [16, 128, 1, 1]           --
│    └─Empty: 2-466                      [16, 128, 1, 1]           --
│    └─Empty: 2-467                      [16]                      --
│    └─Empty: 2-468                      [16]                      --
│    └─BatchNorm2d: 2-469                [16, 16, 8, 8]            --
│    └─Scaler: 2-470                     [16, 16, 8, 8]            --
│    └─ReLU: 2-471                       [16, 16, 8, 8]            --
│    └─Empty: 2-472                      [16, 16, 8, 8]            --
│    └─Clamp: 2-473                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-474                  [16, 128, 8, 8]           --
│    └─Empty: 2-475                      [16, 128, 8, 8]           --
│    └─Empty: 2-476                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-477         --                        --
│    └─One: 2-478                        [1]                       --
│    └─OutputScale: 2-479                --                        --
│    └─Empty: 2-480                      [16, 128, 3, 3]           --
│    └─Empty: 2-481                      [16, 128, 3, 3]           --
│    └─Empty: 2-482                      [16]                      --
│    └─Empty: 2-483                      [16]                      --
│    └─BatchNorm2d: 2-484                [16, 16, 8, 8]            --
│    └─Scaler: 2-485                     [16, 16, 8, 8]            --
│    └─ReLU: 2-486                       [16, 16, 8, 8]            --
│    └─Empty: 2-487                      [16, 16, 8, 8]            --
│    └─Clamp: 2-488                      [16, 16, 8, 8]            --
├─Dropout2d: 1-49                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-50                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-489         --                        --
│    └─One: 2-490                        [1]                       --
│    └─OutputScale: 2-491                --                        --
│    └─Empty: 2-492                      [128, 48, 1, 1]           --
│    └─Empty: 2-493                      [128, 48, 1, 1]           --
│    └─Empty: 2-494                      [128]                     --
│    └─Empty: 2-495                      [128]                     --
│    └─BatchNorm2d: 2-496                [16, 128, 64, 64]         --
│    └─Scaler: 2-497                     [16, 128, 64, 64]         --
│    └─ReLU: 2-498                       [16, 128, 64, 64]         --
│    └─Empty: 2-499                      [16, 128, 64, 64]         --
│    └─Clamp: 2-500                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-501                  [16, 128, 32, 32]         --
│    └─Empty: 2-502                      [16, 128, 32, 32]         --
│    └─Empty: 2-503                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-504         --                        --
│    └─One: 2-505                        [1]                       --
│    └─OutputScale: 2-506                --                        --
│    └─Empty: 2-507                      [128, 128, 3, 3]          --
│    └─Empty: 2-508                      [128, 128, 3, 3]          --
│    └─Empty: 2-509                      [128]                     --
│    └─Empty: 2-510                      [128]                     --
│    └─BatchNorm2d: 2-511                [16, 128, 32, 32]         --
│    └─Scaler: 2-512                     [16, 128, 32, 32]         --
│    └─ReLU: 2-513                       [16, 128, 32, 32]         --
│    └─Empty: 2-514                      [16, 128, 32, 32]         --
│    └─Clamp: 2-515                      [16, 128, 32, 32]         --
├─Dropout2d: 1-52                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-53         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-516                  [16, 128, 16, 16]         --
│    └─Empty: 2-517                      [16, 128, 16, 16]         --
│    └─Empty: 2-518                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-519         --                        --
│    └─One: 2-520                        [1]                       --
│    └─OutputScale: 2-521                --                        --
│    └─Empty: 2-522                      [128, 128, 3, 3]          --
│    └─Empty: 2-523                      [128, 128, 3, 3]          --
│    └─Empty: 2-524                      [128]                     --
│    └─Empty: 2-525                      [128]                     --
│    └─BatchNorm2d: 2-526                [16, 128, 16, 16]         --
│    └─Scaler: 2-527                     [16, 128, 16, 16]         --
│    └─ReLU: 2-528                       [16, 128, 16, 16]         --
│    └─Empty: 2-529                      [16, 128, 16, 16]         --
│    └─Clamp: 2-530                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-54                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-531         --                        --
│    └─One: 2-532                        [1]                       --
│    └─OutputScale: 2-533                --                        --
│    └─Empty: 2-534                      [128, 128, 1, 1]          --
│    └─Empty: 2-535                      [128, 128, 1, 1]          --
│    └─Empty: 2-536                      [128]                     --
│    └─Empty: 2-537                      [128]                     --
│    └─BatchNorm2d: 2-538                [16, 128, 16, 16]         --
│    └─Scaler: 2-539                     [16, 128, 16, 16]         --
│    └─ReLU: 2-540                       [16, 128, 16, 16]         --
│    └─Empty: 2-541                      [16, 128, 16, 16]         --
│    └─Clamp: 2-542                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-55         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-543                  [16, 128, 16, 16]         --
│    └─Empty: 2-544                      [16, 128, 16, 16]         --
│    └─Empty: 2-545                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-546         --                        --
│    └─One: 2-547                        [1]                       --
│    └─OutputScale: 2-548                --                        --
│    └─Empty: 2-549                      [128, 128, 3, 3]          --
│    └─Empty: 2-550                      [128, 128, 3, 3]          --
│    └─Empty: 2-551                      [128]                     --
│    └─Empty: 2-552                      [128]                     --
│    └─BatchNorm2d: 2-553                [16, 128, 16, 16]         --
│    └─Scaler: 2-554                     [16, 128, 16, 16]         --
│    └─ReLU: 2-555                       [16, 128, 16, 16]         --
│    └─Empty: 2-556                      [16, 128, 16, 16]         --
│    └─Clamp: 2-557                      [16, 128, 16, 16]         --
├─Dropout2d: 1-56                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-57         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-558                  [16, 128, 8, 8]           --
│    └─Empty: 2-559                      [16, 128, 8, 8]           --
│    └─Empty: 2-560                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-561         --                        --
│    └─One: 2-562                        [1]                       --
│    └─OutputScale: 2-563                --                        --
│    └─Empty: 2-564                      [128, 128, 3, 3]          --
│    └─Empty: 2-565                      [128, 128, 3, 3]          --
│    └─Empty: 2-566                      [128]                     --
│    └─Empty: 2-567                      [128]                     --
│    └─BatchNorm2d: 2-568                [16, 128, 8, 8]           --
│    └─Scaler: 2-569                     [16, 128, 8, 8]           --
│    └─ReLU: 2-570                       [16, 128, 8, 8]           --
│    └─Empty: 2-571                      [16, 128, 8, 8]           --
│    └─Clamp: 2-572                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-58                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-573         --                        --
│    └─One: 2-574                        [1]                       --
│    └─OutputScale: 2-575                --                        --
│    └─Empty: 2-576                      [16, 128, 1, 1]           --
│    └─Empty: 2-577                      [16, 128, 1, 1]           --
│    └─Empty: 2-578                      [16]                      --
│    └─Empty: 2-579                      [16]                      --
│    └─BatchNorm2d: 2-580                [16, 16, 8, 8]            --
│    └─Scaler: 2-581                     [16, 16, 8, 8]            --
│    └─ReLU: 2-582                       [16, 16, 8, 8]            --
│    └─Empty: 2-583                      [16, 16, 8, 8]            --
│    └─Clamp: 2-584                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-59         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-585                  [16, 128, 8, 8]           --
│    └─Empty: 2-586                      [16, 128, 8, 8]           --
│    └─Empty: 2-587                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-588         --                        --
│    └─One: 2-589                        [1]                       --
│    └─OutputScale: 2-590                --                        --
│    └─Empty: 2-591                      [16, 128, 3, 3]           --
│    └─Empty: 2-592                      [16, 128, 3, 3]           --
│    └─Empty: 2-593                      [16]                      --
│    └─Empty: 2-594                      [16]                      --
│    └─BatchNorm2d: 2-595                [16, 16, 8, 8]            --
│    └─Scaler: 2-596                     [16, 16, 8, 8]            --
│    └─ReLU: 2-597                       [16, 16, 8, 8]            --
│    └─Empty: 2-598                      [16, 16, 8, 8]            --
│    └─Clamp: 2-599                      [16, 16, 8, 8]            --
├─Dropout2d: 1-60                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-61                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-600         --                        --
│    └─One: 2-601                        [1]                       --
│    └─OutputScale: 2-602                --                        --
│    └─Empty: 2-603                      [128, 48, 1, 1]           --
│    └─Empty: 2-604                      [128, 48, 1, 1]           --
│    └─Empty: 2-605                      [128]                     --
│    └─Empty: 2-606                      [128]                     --
│    └─BatchNorm2d: 2-607                [16, 128, 64, 64]         --
│    └─Scaler: 2-608                     [16, 128, 64, 64]         --
│    └─ReLU: 2-609                       [16, 128, 64, 64]         --
│    └─Empty: 2-610                      [16, 128, 64, 64]         --
│    └─Clamp: 2-611                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-62         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-612                  [16, 128, 32, 32]         --
│    └─Empty: 2-613                      [16, 128, 32, 32]         --
│    └─Empty: 2-614                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-615         --                        --
│    └─One: 2-616                        [1]                       --
│    └─OutputScale: 2-617                --                        --
│    └─Empty: 2-618                      [128, 128, 3, 3]          --
│    └─Empty: 2-619                      [128, 128, 3, 3]          --
│    └─Empty: 2-620                      [128]                     --
│    └─Empty: 2-621                      [128]                     --
│    └─BatchNorm2d: 2-622                [16, 128, 32, 32]         --
│    └─Scaler: 2-623                     [16, 128, 32, 32]         --
│    └─ReLU: 2-624                       [16, 128, 32, 32]         --
│    └─Empty: 2-625                      [16, 128, 32, 32]         --
│    └─Clamp: 2-626                      [16, 128, 32, 32]         --
├─Dropout2d: 1-63                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-64         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-627                  [16, 128, 16, 16]         --
│    └─Empty: 2-628                      [16, 128, 16, 16]         --
│    └─Empty: 2-629                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-630         --                        --
│    └─One: 2-631                        [1]                       --
│    └─OutputScale: 2-632                --                        --
│    └─Empty: 2-633                      [128, 128, 3, 3]          --
│    └─Empty: 2-634                      [128, 128, 3, 3]          --
│    └─Empty: 2-635                      [128]                     --
│    └─Empty: 2-636                      [128]                     --
│    └─BatchNorm2d: 2-637                [16, 128, 16, 16]         --
│    └─Scaler: 2-638                     [16, 128, 16, 16]         --
│    └─ReLU: 2-639                       [16, 128, 16, 16]         --
│    └─Empty: 2-640                      [16, 128, 16, 16]         --
│    └─Clamp: 2-641                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-65                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-642         --                        --
│    └─One: 2-643                        [1]                       --
│    └─OutputScale: 2-644                --                        --
│    └─Empty: 2-645                      [128, 128, 1, 1]          --
│    └─Empty: 2-646                      [128, 128, 1, 1]          --
│    └─Empty: 2-647                      [128]                     --
│    └─Empty: 2-648                      [128]                     --
│    └─BatchNorm2d: 2-649                [16, 128, 16, 16]         --
│    └─Scaler: 2-650                     [16, 128, 16, 16]         --
│    └─ReLU: 2-651                       [16, 128, 16, 16]         --
│    └─Empty: 2-652                      [16, 128, 16, 16]         --
│    └─Clamp: 2-653                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-654                  [16, 128, 16, 16]         --
│    └─Empty: 2-655                      [16, 128, 16, 16]         --
│    └─Empty: 2-656                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-657         --                        --
│    └─One: 2-658                        [1]                       --
│    └─OutputScale: 2-659                --                        --
│    └─Empty: 2-660                      [128, 128, 3, 3]          --
│    └─Empty: 2-661                      [128, 128, 3, 3]          --
│    └─Empty: 2-662                      [128]                     --
│    └─Empty: 2-663                      [128]                     --
│    └─BatchNorm2d: 2-664                [16, 128, 16, 16]         --
│    └─Scaler: 2-665                     [16, 128, 16, 16]         --
│    └─ReLU: 2-666                       [16, 128, 16, 16]         --
│    └─Empty: 2-667                      [16, 128, 16, 16]         --
│    └─Clamp: 2-668                      [16, 128, 16, 16]         --
├─Dropout2d: 1-67                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-669                  [16, 128, 8, 8]           --
│    └─Empty: 2-670                      [16, 128, 8, 8]           --
│    └─Empty: 2-671                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-672         --                        --
│    └─One: 2-673                        [1]                       --
│    └─OutputScale: 2-674                --                        --
│    └─Empty: 2-675                      [128, 128, 3, 3]          --
│    └─Empty: 2-676                      [128, 128, 3, 3]          --
│    └─Empty: 2-677                      [128]                     --
│    └─Empty: 2-678                      [128]                     --
│    └─BatchNorm2d: 2-679                [16, 128, 8, 8]           --
│    └─Scaler: 2-680                     [16, 128, 8, 8]           --
│    └─ReLU: 2-681                       [16, 128, 8, 8]           --
│    └─Empty: 2-682                      [16, 128, 8, 8]           --
│    └─Clamp: 2-683                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-69                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-684         --                        --
│    └─One: 2-685                        [1]                       --
│    └─OutputScale: 2-686                --                        --
│    └─Empty: 2-687                      [16, 128, 1, 1]           --
│    └─Empty: 2-688                      [16, 128, 1, 1]           --
│    └─Empty: 2-689                      [16]                      --
│    └─Empty: 2-690                      [16]                      --
│    └─BatchNorm2d: 2-691                [16, 16, 8, 8]            --
│    └─Scaler: 2-692                     [16, 16, 8, 8]            --
│    └─ReLU: 2-693                       [16, 16, 8, 8]            --
│    └─Empty: 2-694                      [16, 16, 8, 8]            --
│    └─Clamp: 2-695                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-70         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-696                  [16, 128, 8, 8]           --
│    └─Empty: 2-697                      [16, 128, 8, 8]           --
│    └─Empty: 2-698                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-699         --                        --
│    └─One: 2-700                        [1]                       --
│    └─OutputScale: 2-701                --                        --
│    └─Empty: 2-702                      [16, 128, 3, 3]           --
│    └─Empty: 2-703                      [16, 128, 3, 3]           --
│    └─Empty: 2-704                      [16]                      --
│    └─Empty: 2-705                      [16]                      --
│    └─BatchNorm2d: 2-706                [16, 16, 8, 8]            --
│    └─Scaler: 2-707                     [16, 16, 8, 8]            --
│    └─ReLU: 2-708                       [16, 16, 8, 8]            --
│    └─Empty: 2-709                      [16, 16, 8, 8]            --
│    └─Clamp: 2-710                      [16, 16, 8, 8]            --
├─Dropout2d: 1-71                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-72                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-711         --                        --
│    └─One: 2-712                        [1]                       --
│    └─OutputScale: 2-713                --                        --
│    └─Empty: 2-714                      [128, 48, 1, 1]           --
│    └─Empty: 2-715                      [128, 48, 1, 1]           --
│    └─Empty: 2-716                      [128]                     --
│    └─Empty: 2-717                      [128]                     --
│    └─BatchNorm2d: 2-718                [16, 128, 64, 64]         --
│    └─Scaler: 2-719                     [16, 128, 64, 64]         --
│    └─ReLU: 2-720                       [16, 128, 64, 64]         --
│    └─Empty: 2-721                      [16, 128, 64, 64]         --
│    └─Clamp: 2-722                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-723                  [16, 128, 32, 32]         --
│    └─Empty: 2-724                      [16, 128, 32, 32]         --
│    └─Empty: 2-725                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-726         --                        --
│    └─One: 2-727                        [1]                       --
│    └─OutputScale: 2-728                --                        --
│    └─Empty: 2-729                      [128, 128, 3, 3]          --
│    └─Empty: 2-730                      [128, 128, 3, 3]          --
│    └─Empty: 2-731                      [128]                     --
│    └─Empty: 2-732                      [128]                     --
│    └─BatchNorm2d: 2-733                [16, 128, 32, 32]         --
│    └─Scaler: 2-734                     [16, 128, 32, 32]         --
│    └─ReLU: 2-735                       [16, 128, 32, 32]         --
│    └─Empty: 2-736                      [16, 128, 32, 32]         --
│    └─Clamp: 2-737                      [16, 128, 32, 32]         --
├─Dropout2d: 1-74                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-738                  [16, 128, 16, 16]         --
│    └─Empty: 2-739                      [16, 128, 16, 16]         --
│    └─Empty: 2-740                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-741         --                        --
│    └─One: 2-742                        [1]                       --
│    └─OutputScale: 2-743                --                        --
│    └─Empty: 2-744                      [128, 128, 3, 3]          --
│    └─Empty: 2-745                      [128, 128, 3, 3]          --
│    └─Empty: 2-746                      [128]                     --
│    └─Empty: 2-747                      [128]                     --
│    └─BatchNorm2d: 2-748                [16, 128, 16, 16]         --
│    └─Scaler: 2-749                     [16, 128, 16, 16]         --
│    └─ReLU: 2-750                       [16, 128, 16, 16]         --
│    └─Empty: 2-751                      [16, 128, 16, 16]         --
│    └─Clamp: 2-752                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-76                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-753         --                        --
│    └─One: 2-754                        [1]                       --
│    └─OutputScale: 2-755                --                        --
│    └─Empty: 2-756                      [128, 128, 1, 1]          --
│    └─Empty: 2-757                      [128, 128, 1, 1]          --
│    └─Empty: 2-758                      [128]                     --
│    └─Empty: 2-759                      [128]                     --
│    └─BatchNorm2d: 2-760                [16, 128, 16, 16]         --
│    └─Scaler: 2-761                     [16, 128, 16, 16]         --
│    └─ReLU: 2-762                       [16, 128, 16, 16]         --
│    └─Empty: 2-763                      [16, 128, 16, 16]         --
│    └─Clamp: 2-764                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-77         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-765                  [16, 128, 16, 16]         --
│    └─Empty: 2-766                      [16, 128, 16, 16]         --
│    └─Empty: 2-767                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-768         --                        --
│    └─One: 2-769                        [1]                       --
│    └─OutputScale: 2-770                --                        --
│    └─Empty: 2-771                      [128, 128, 3, 3]          --
│    └─Empty: 2-772                      [128, 128, 3, 3]          --
│    └─Empty: 2-773                      [128]                     --
│    └─Empty: 2-774                      [128]                     --
│    └─BatchNorm2d: 2-775                [16, 128, 16, 16]         --
│    └─Scaler: 2-776                     [16, 128, 16, 16]         --
│    └─ReLU: 2-777                       [16, 128, 16, 16]         --
│    └─Empty: 2-778                      [16, 128, 16, 16]         --
│    └─Clamp: 2-779                      [16, 128, 16, 16]         --
├─Dropout2d: 1-78                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-79         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-780                  [16, 128, 8, 8]           --
│    └─Empty: 2-781                      [16, 128, 8, 8]           --
│    └─Empty: 2-782                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-783         --                        --
│    └─One: 2-784                        [1]                       --
│    └─OutputScale: 2-785                --                        --
│    └─Empty: 2-786                      [128, 128, 3, 3]          --
│    └─Empty: 2-787                      [128, 128, 3, 3]          --
│    └─Empty: 2-788                      [128]                     --
│    └─Empty: 2-789                      [128]                     --
│    └─BatchNorm2d: 2-790                [16, 128, 8, 8]           --
│    └─Scaler: 2-791                     [16, 128, 8, 8]           --
│    └─ReLU: 2-792                       [16, 128, 8, 8]           --
│    └─Empty: 2-793                      [16, 128, 8, 8]           --
│    └─Clamp: 2-794                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-80                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-795         --                        --
│    └─One: 2-796                        [1]                       --
│    └─OutputScale: 2-797                --                        --
│    └─Empty: 2-798                      [16, 128, 1, 1]           --
│    └─Empty: 2-799                      [16, 128, 1, 1]           --
│    └─Empty: 2-800                      [16]                      --
│    └─Empty: 2-801                      [16]                      --
│    └─BatchNorm2d: 2-802                [16, 16, 8, 8]            --
│    └─Scaler: 2-803                     [16, 16, 8, 8]            --
│    └─ReLU: 2-804                       [16, 16, 8, 8]            --
│    └─Empty: 2-805                      [16, 16, 8, 8]            --
│    └─Clamp: 2-806                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-807                  [16, 128, 8, 8]           --
│    └─Empty: 2-808                      [16, 128, 8, 8]           --
│    └─Empty: 2-809                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-810         --                        --
│    └─One: 2-811                        [1]                       --
│    └─OutputScale: 2-812                --                        --
│    └─Empty: 2-813                      [16, 128, 3, 3]           --
│    └─Empty: 2-814                      [16, 128, 3, 3]           --
│    └─Empty: 2-815                      [16]                      --
│    └─Empty: 2-816                      [16]                      --
│    └─BatchNorm2d: 2-817                [16, 16, 8, 8]            --
│    └─Scaler: 2-818                     [16, 16, 8, 8]            --
│    └─ReLU: 2-819                       [16, 16, 8, 8]            --
│    └─Empty: 2-820                      [16, 16, 8, 8]            --
│    └─Clamp: 2-821                      [16, 16, 8, 8]            --
├─Dropout2d: 1-82                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-83                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-822         --                        --
│    └─One: 2-823                        [1]                       --
│    └─OutputScale: 2-824                --                        --
│    └─Empty: 2-825                      [128, 48, 1, 1]           --
│    └─Empty: 2-826                      [128, 48, 1, 1]           --
│    └─Empty: 2-827                      [128]                     --
│    └─Empty: 2-828                      [128]                     --
│    └─BatchNorm2d: 2-829                [16, 128, 64, 64]         --
│    └─Scaler: 2-830                     [16, 128, 64, 64]         --
│    └─ReLU: 2-831                       [16, 128, 64, 64]         --
│    └─Empty: 2-832                      [16, 128, 64, 64]         --
│    └─Clamp: 2-833                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-84         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-834                  [16, 128, 32, 32]         --
│    └─Empty: 2-835                      [16, 128, 32, 32]         --
│    └─Empty: 2-836                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-837         --                        --
│    └─One: 2-838                        [1]                       --
│    └─OutputScale: 2-839                --                        --
│    └─Empty: 2-840                      [128, 128, 3, 3]          --
│    └─Empty: 2-841                      [128, 128, 3, 3]          --
│    └─Empty: 2-842                      [128]                     --
│    └─Empty: 2-843                      [128]                     --
│    └─BatchNorm2d: 2-844                [16, 128, 32, 32]         --
│    └─Scaler: 2-845                     [16, 128, 32, 32]         --
│    └─ReLU: 2-846                       [16, 128, 32, 32]         --
│    └─Empty: 2-847                      [16, 128, 32, 32]         --
│    └─Clamp: 2-848                      [16, 128, 32, 32]         --
├─Dropout2d: 1-85                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-86         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-849                  [16, 128, 16, 16]         --
│    └─Empty: 2-850                      [16, 128, 16, 16]         --
│    └─Empty: 2-851                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-852         --                        --
│    └─One: 2-853                        [1]                       --
│    └─OutputScale: 2-854                --                        --
│    └─Empty: 2-855                      [128, 128, 3, 3]          --
│    └─Empty: 2-856                      [128, 128, 3, 3]          --
│    └─Empty: 2-857                      [128]                     --
│    └─Empty: 2-858                      [128]                     --
│    └─BatchNorm2d: 2-859                [16, 128, 16, 16]         --
│    └─Scaler: 2-860                     [16, 128, 16, 16]         --
│    └─ReLU: 2-861                       [16, 128, 16, 16]         --
│    └─Empty: 2-862                      [16, 128, 16, 16]         --
│    └─Clamp: 2-863                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-87                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-864         --                        --
│    └─One: 2-865                        [1]                       --
│    └─OutputScale: 2-866                --                        --
│    └─Empty: 2-867                      [128, 128, 1, 1]          --
│    └─Empty: 2-868                      [128, 128, 1, 1]          --
│    └─Empty: 2-869                      [128]                     --
│    └─Empty: 2-870                      [128]                     --
│    └─BatchNorm2d: 2-871                [16, 128, 16, 16]         --
│    └─Scaler: 2-872                     [16, 128, 16, 16]         --
│    └─ReLU: 2-873                       [16, 128, 16, 16]         --
│    └─Empty: 2-874                      [16, 128, 16, 16]         --
│    └─Clamp: 2-875                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-88         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-876                  [16, 128, 16, 16]         --
│    └─Empty: 2-877                      [16, 128, 16, 16]         --
│    └─Empty: 2-878                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-879         --                        --
│    └─One: 2-880                        [1]                       --
│    └─OutputScale: 2-881                --                        --
│    └─Empty: 2-882                      [128, 128, 3, 3]          --
│    └─Empty: 2-883                      [128, 128, 3, 3]          --
│    └─Empty: 2-884                      [128]                     --
│    └─Empty: 2-885                      [128]                     --
│    └─BatchNorm2d: 2-886                [16, 128, 16, 16]         --
│    └─Scaler: 2-887                     [16, 128, 16, 16]         --
│    └─ReLU: 2-888                       [16, 128, 16, 16]         --
│    └─Empty: 2-889                      [16, 128, 16, 16]         --
│    └─Clamp: 2-890                      [16, 128, 16, 16]         --
├─Dropout2d: 1-89                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-90         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-891                  [16, 128, 8, 8]           --
│    └─Empty: 2-892                      [16, 128, 8, 8]           --
│    └─Empty: 2-893                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-894         --                        --
│    └─One: 2-895                        [1]                       --
│    └─OutputScale: 2-896                --                        --
│    └─Empty: 2-897                      [128, 128, 3, 3]          --
│    └─Empty: 2-898                      [128, 128, 3, 3]          --
│    └─Empty: 2-899                      [128]                     --
│    └─Empty: 2-900                      [128]                     --
│    └─BatchNorm2d: 2-901                [16, 128, 8, 8]           --
│    └─Scaler: 2-902                     [16, 128, 8, 8]           --
│    └─ReLU: 2-903                       [16, 128, 8, 8]           --
│    └─Empty: 2-904                      [16, 128, 8, 8]           --
│    └─Clamp: 2-905                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-91                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-906         --                        --
│    └─One: 2-907                        [1]                       --
│    └─OutputScale: 2-908                --                        --
│    └─Empty: 2-909                      [16, 128, 1, 1]           --
│    └─Empty: 2-910                      [16, 128, 1, 1]           --
│    └─Empty: 2-911                      [16]                      --
│    └─Empty: 2-912                      [16]                      --
│    └─BatchNorm2d: 2-913                [16, 16, 8, 8]            --
│    └─Scaler: 2-914                     [16, 16, 8, 8]            --
│    └─ReLU: 2-915                       [16, 16, 8, 8]            --
│    └─Empty: 2-916                      [16, 16, 8, 8]            --
│    └─Clamp: 2-917                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-92         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-918                  [16, 128, 8, 8]           --
│    └─Empty: 2-919                      [16, 128, 8, 8]           --
│    └─Empty: 2-920                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-921         --                        --
│    └─One: 2-922                        [1]                       --
│    └─OutputScale: 2-923                --                        --
│    └─Empty: 2-924                      [16, 128, 3, 3]           --
│    └─Empty: 2-925                      [16, 128, 3, 3]           --
│    └─Empty: 2-926                      [16]                      --
│    └─Empty: 2-927                      [16]                      --
│    └─BatchNorm2d: 2-928                [16, 16, 8, 8]            --
│    └─Scaler: 2-929                     [16, 16, 8, 8]            --
│    └─ReLU: 2-930                       [16, 16, 8, 8]            --
│    └─Empty: 2-931                      [16, 16, 8, 8]            --
│    └─Clamp: 2-932                      [16, 16, 8, 8]            --
├─Dropout2d: 1-93                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-94                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-933         --                        --
│    └─One: 2-934                        [1]                       --
│    └─OutputScale: 2-935                --                        --
│    └─Empty: 2-936                      [128, 48, 1, 1]           --
│    └─Empty: 2-937                      [128, 48, 1, 1]           --
│    └─Empty: 2-938                      [128]                     --
│    └─Empty: 2-939                      [128]                     --
│    └─BatchNorm2d: 2-940                [16, 128, 64, 64]         --
│    └─Scaler: 2-941                     [16, 128, 64, 64]         --
│    └─ReLU: 2-942                       [16, 128, 64, 64]         --
│    └─Empty: 2-943                      [16, 128, 64, 64]         --
│    └─Clamp: 2-944                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-95         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-945                  [16, 128, 32, 32]         --
│    └─Empty: 2-946                      [16, 128, 32, 32]         --
│    └─Empty: 2-947                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-948         --                        --
│    └─One: 2-949                        [1]                       --
│    └─OutputScale: 2-950                --                        --
│    └─Empty: 2-951                      [128, 128, 3, 3]          --
│    └─Empty: 2-952                      [128, 128, 3, 3]          --
│    └─Empty: 2-953                      [128]                     --
│    └─Empty: 2-954                      [128]                     --
│    └─BatchNorm2d: 2-955                [16, 128, 32, 32]         --
│    └─Scaler: 2-956                     [16, 128, 32, 32]         --
│    └─ReLU: 2-957                       [16, 128, 32, 32]         --
│    └─Empty: 2-958                      [16, 128, 32, 32]         --
│    └─Clamp: 2-959                      [16, 128, 32, 32]         --
├─Dropout2d: 1-96                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-97         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-960                  [16, 128, 16, 16]         --
│    └─Empty: 2-961                      [16, 128, 16, 16]         --
│    └─Empty: 2-962                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-963         --                        --
│    └─One: 2-964                        [1]                       --
│    └─OutputScale: 2-965                --                        --
│    └─Empty: 2-966                      [128, 128, 3, 3]          --
│    └─Empty: 2-967                      [128, 128, 3, 3]          --
│    └─Empty: 2-968                      [128]                     --
│    └─Empty: 2-969                      [128]                     --
│    └─BatchNorm2d: 2-970                [16, 128, 16, 16]         --
│    └─Scaler: 2-971                     [16, 128, 16, 16]         --
│    └─ReLU: 2-972                       [16, 128, 16, 16]         --
│    └─Empty: 2-973                      [16, 128, 16, 16]         --
│    └─Clamp: 2-974                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-98                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-975         --                        --
│    └─One: 2-976                        [1]                       --
│    └─OutputScale: 2-977                --                        --
│    └─Empty: 2-978                      [128, 128, 1, 1]          --
│    └─Empty: 2-979                      [128, 128, 1, 1]          --
│    └─Empty: 2-980                      [128]                     --
│    └─Empty: 2-981                      [128]                     --
│    └─BatchNorm2d: 2-982                [16, 128, 16, 16]         --
│    └─Scaler: 2-983                     [16, 128, 16, 16]         --
│    └─ReLU: 2-984                       [16, 128, 16, 16]         --
│    └─Empty: 2-985                      [16, 128, 16, 16]         --
│    └─Clamp: 2-986                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-99         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-987                  [16, 128, 16, 16]         --
│    └─Empty: 2-988                      [16, 128, 16, 16]         --
│    └─Empty: 2-989                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-990         --                        --
│    └─One: 2-991                        [1]                       --
│    └─OutputScale: 2-992                --                        --
│    └─Empty: 2-993                      [128, 128, 3, 3]          --
│    └─Empty: 2-994                      [128, 128, 3, 3]          --
│    └─Empty: 2-995                      [128]                     --
│    └─Empty: 2-996                      [128]                     --
│    └─BatchNorm2d: 2-997                [16, 128, 16, 16]         --
│    └─Scaler: 2-998                     [16, 128, 16, 16]         --
│    └─ReLU: 2-999                       [16, 128, 16, 16]         --
│    └─Empty: 2-1000                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1001                     [16, 128, 16, 16]         --
├─Dropout2d: 1-100                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-101        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1002                 [16, 128, 8, 8]           --
│    └─Empty: 2-1003                     [16, 128, 8, 8]           --
│    └─Empty: 2-1004                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1005        --                        --
│    └─One: 2-1006                       [1]                       --
│    └─OutputScale: 2-1007               --                        --
│    └─Empty: 2-1008                     [128, 128, 3, 3]          --
│    └─Empty: 2-1009                     [128, 128, 3, 3]          --
│    └─Empty: 2-1010                     [128]                     --
│    └─Empty: 2-1011                     [128]                     --
│    └─BatchNorm2d: 2-1012               [16, 128, 8, 8]           --
│    └─Scaler: 2-1013                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1014                      [16, 128, 8, 8]           --
│    └─Empty: 2-1015                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1016                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-102               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1017        --                        --
│    └─One: 2-1018                       [1]                       --
│    └─OutputScale: 2-1019               --                        --
│    └─Empty: 2-1020                     [16, 128, 1, 1]           --
│    └─Empty: 2-1021                     [16, 128, 1, 1]           --
│    └─Empty: 2-1022                     [16]                      --
│    └─Empty: 2-1023                     [16]                      --
│    └─BatchNorm2d: 2-1024               [16, 16, 8, 8]            --
│    └─Scaler: 2-1025                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1026                      [16, 16, 8, 8]            --
│    └─Empty: 2-1027                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1028                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-103        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1029                 [16, 128, 8, 8]           --
│    └─Empty: 2-1030                     [16, 128, 8, 8]           --
│    └─Empty: 2-1031                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1032        --                        --
│    └─One: 2-1033                       [1]                       --
│    └─OutputScale: 2-1034               --                        --
│    └─Empty: 2-1035                     [16, 128, 3, 3]           --
│    └─Empty: 2-1036                     [16, 128, 3, 3]           --
│    └─Empty: 2-1037                     [16]                      --
│    └─Empty: 2-1038                     [16]                      --
│    └─BatchNorm2d: 2-1039               [16, 16, 8, 8]            --
│    └─Scaler: 2-1040                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1041                      [16, 16, 8, 8]            --
│    └─Empty: 2-1042                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1043                     [16, 16, 8, 8]            --
├─Dropout2d: 1-104                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-105               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1044        --                        --
│    └─One: 2-1045                       [1]                       --
│    └─OutputScale: 2-1046               --                        --
│    └─Empty: 2-1047                     [128, 48, 1, 1]           --
│    └─Empty: 2-1048                     [128, 48, 1, 1]           --
│    └─Empty: 2-1049                     [128]                     --
│    └─Empty: 2-1050                     [128]                     --
│    └─BatchNorm2d: 2-1051               [16, 128, 64, 64]         --
│    └─Scaler: 2-1052                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1053                      [16, 128, 64, 64]         --
│    └─Empty: 2-1054                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1055                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-106        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1056                 [16, 128, 32, 32]         --
│    └─Empty: 2-1057                     [16, 128, 32, 32]         --
│    └─Empty: 2-1058                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1059        --                        --
│    └─One: 2-1060                       [1]                       --
│    └─OutputScale: 2-1061               --                        --
│    └─Empty: 2-1062                     [128, 128, 3, 3]          --
│    └─Empty: 2-1063                     [128, 128, 3, 3]          --
│    └─Empty: 2-1064                     [128]                     --
│    └─Empty: 2-1065                     [128]                     --
│    └─BatchNorm2d: 2-1066               [16, 128, 32, 32]         --
│    └─Scaler: 2-1067                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1068                      [16, 128, 32, 32]         --
│    └─Empty: 2-1069                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1070                     [16, 128, 32, 32]         --
├─Dropout2d: 1-107                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-108        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1071                 [16, 128, 16, 16]         --
│    └─Empty: 2-1072                     [16, 128, 16, 16]         --
│    └─Empty: 2-1073                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1074        --                        --
│    └─One: 2-1075                       [1]                       --
│    └─OutputScale: 2-1076               --                        --
│    └─Empty: 2-1077                     [128, 128, 3, 3]          --
│    └─Empty: 2-1078                     [128, 128, 3, 3]          --
│    └─Empty: 2-1079                     [128]                     --
│    └─Empty: 2-1080                     [128]                     --
│    └─BatchNorm2d: 2-1081               [16, 128, 16, 16]         --
│    └─Scaler: 2-1082                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1083                      [16, 128, 16, 16]         --
│    └─Empty: 2-1084                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1085                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-109               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1086        --                        --
│    └─One: 2-1087                       [1]                       --
│    └─OutputScale: 2-1088               --                        --
│    └─Empty: 2-1089                     [128, 128, 1, 1]          --
│    └─Empty: 2-1090                     [128, 128, 1, 1]          --
│    └─Empty: 2-1091                     [128]                     --
│    └─Empty: 2-1092                     [128]                     --
│    └─BatchNorm2d: 2-1093               [16, 128, 16, 16]         --
│    └─Scaler: 2-1094                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1095                      [16, 128, 16, 16]         --
│    └─Empty: 2-1096                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1097                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-110        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1098                 [16, 128, 16, 16]         --
│    └─Empty: 2-1099                     [16, 128, 16, 16]         --
│    └─Empty: 2-1100                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1101        --                        --
│    └─One: 2-1102                       [1]                       --
│    └─OutputScale: 2-1103               --                        --
│    └─Empty: 2-1104                     [128, 128, 3, 3]          --
│    └─Empty: 2-1105                     [128, 128, 3, 3]          --
│    └─Empty: 2-1106                     [128]                     --
│    └─Empty: 2-1107                     [128]                     --
│    └─BatchNorm2d: 2-1108               [16, 128, 16, 16]         --
│    └─Scaler: 2-1109                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1110                      [16, 128, 16, 16]         --
│    └─Empty: 2-1111                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1112                     [16, 128, 16, 16]         --
├─Dropout2d: 1-111                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-112        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1113                 [16, 128, 8, 8]           --
│    └─Empty: 2-1114                     [16, 128, 8, 8]           --
│    └─Empty: 2-1115                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1116        --                        --
│    └─One: 2-1117                       [1]                       --
│    └─OutputScale: 2-1118               --                        --
│    └─Empty: 2-1119                     [128, 128, 3, 3]          --
│    └─Empty: 2-1120                     [128, 128, 3, 3]          --
│    └─Empty: 2-1121                     [128]                     --
│    └─Empty: 2-1122                     [128]                     --
│    └─BatchNorm2d: 2-1123               [16, 128, 8, 8]           --
│    └─Scaler: 2-1124                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1125                      [16, 128, 8, 8]           --
│    └─Empty: 2-1126                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1127                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-113               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1128        --                        --
│    └─One: 2-1129                       [1]                       --
│    └─OutputScale: 2-1130               --                        --
│    └─Empty: 2-1131                     [16, 128, 1, 1]           --
│    └─Empty: 2-1132                     [16, 128, 1, 1]           --
│    └─Empty: 2-1133                     [16]                      --
│    └─Empty: 2-1134                     [16]                      --
│    └─BatchNorm2d: 2-1135               [16, 16, 8, 8]            --
│    └─Scaler: 2-1136                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1137                      [16, 16, 8, 8]            --
│    └─Empty: 2-1138                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1139                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-114        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1140                 [16, 128, 8, 8]           --
│    └─Empty: 2-1141                     [16, 128, 8, 8]           --
│    └─Empty: 2-1142                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1143        --                        --
│    └─One: 2-1144                       [1]                       --
│    └─OutputScale: 2-1145               --                        --
│    └─Empty: 2-1146                     [16, 128, 3, 3]           --
│    └─Empty: 2-1147                     [16, 128, 3, 3]           --
│    └─Empty: 2-1148                     [16]                      --
│    └─Empty: 2-1149                     [16]                      --
│    └─BatchNorm2d: 2-1150               [16, 16, 8, 8]            --
│    └─Scaler: 2-1151                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1152                      [16, 16, 8, 8]            --
│    └─Empty: 2-1153                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1154                     [16, 16, 8, 8]            --
├─Dropout2d: 1-115                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-116               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1155        --                        --
│    └─One: 2-1156                       [1]                       --
│    └─OutputScale: 2-1157               --                        --
│    └─Empty: 2-1158                     [128, 48, 1, 1]           --
│    └─Empty: 2-1159                     [128, 48, 1, 1]           --
│    └─Empty: 2-1160                     [128]                     --
│    └─Empty: 2-1161                     [128]                     --
│    └─BatchNorm2d: 2-1162               [16, 128, 64, 64]         --
│    └─Scaler: 2-1163                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1164                      [16, 128, 64, 64]         --
│    └─Empty: 2-1165                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1166                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-117        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1167                 [16, 128, 32, 32]         --
│    └─Empty: 2-1168                     [16, 128, 32, 32]         --
│    └─Empty: 2-1169                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1170        --                        --
│    └─One: 2-1171                       [1]                       --
│    └─OutputScale: 2-1172               --                        --
│    └─Empty: 2-1173                     [128, 128, 3, 3]          --
│    └─Empty: 2-1174                     [128, 128, 3, 3]          --
│    └─Empty: 2-1175                     [128]                     --
│    └─Empty: 2-1176                     [128]                     --
│    └─BatchNorm2d: 2-1177               [16, 128, 32, 32]         --
│    └─Scaler: 2-1178                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1179                      [16, 128, 32, 32]         --
│    └─Empty: 2-1180                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1181                     [16, 128, 32, 32]         --
├─Dropout2d: 1-118                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-119        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1182                 [16, 128, 16, 16]         --
│    └─Empty: 2-1183                     [16, 128, 16, 16]         --
│    └─Empty: 2-1184                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1185        --                        --
│    └─One: 2-1186                       [1]                       --
│    └─OutputScale: 2-1187               --                        --
│    └─Empty: 2-1188                     [128, 128, 3, 3]          --
│    └─Empty: 2-1189                     [128, 128, 3, 3]          --
│    └─Empty: 2-1190                     [128]                     --
│    └─Empty: 2-1191                     [128]                     --
│    └─BatchNorm2d: 2-1192               [16, 128, 16, 16]         --
│    └─Scaler: 2-1193                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1194                      [16, 128, 16, 16]         --
│    └─Empty: 2-1195                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1196                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-120               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1197        --                        --
│    └─One: 2-1198                       [1]                       --
│    └─OutputScale: 2-1199               --                        --
│    └─Empty: 2-1200                     [128, 128, 1, 1]          --
│    └─Empty: 2-1201                     [128, 128, 1, 1]          --
│    └─Empty: 2-1202                     [128]                     --
│    └─Empty: 2-1203                     [128]                     --
│    └─BatchNorm2d: 2-1204               [16, 128, 16, 16]         --
│    └─Scaler: 2-1205                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1206                      [16, 128, 16, 16]         --
│    └─Empty: 2-1207                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1208                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-121        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1209                 [16, 128, 16, 16]         --
│    └─Empty: 2-1210                     [16, 128, 16, 16]         --
│    └─Empty: 2-1211                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1212        --                        --
│    └─One: 2-1213                       [1]                       --
│    └─OutputScale: 2-1214               --                        --
│    └─Empty: 2-1215                     [128, 128, 3, 3]          --
│    └─Empty: 2-1216                     [128, 128, 3, 3]          --
│    └─Empty: 2-1217                     [128]                     --
│    └─Empty: 2-1218                     [128]                     --
│    └─BatchNorm2d: 2-1219               [16, 128, 16, 16]         --
│    └─Scaler: 2-1220                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1221                      [16, 128, 16, 16]         --
│    └─Empty: 2-1222                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1223                     [16, 128, 16, 16]         --
├─Dropout2d: 1-122                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-123        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1224                 [16, 128, 8, 8]           --
│    └─Empty: 2-1225                     [16, 128, 8, 8]           --
│    └─Empty: 2-1226                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1227        --                        --
│    └─One: 2-1228                       [1]                       --
│    └─OutputScale: 2-1229               --                        --
│    └─Empty: 2-1230                     [128, 128, 3, 3]          --
│    └─Empty: 2-1231                     [128, 128, 3, 3]          --
│    └─Empty: 2-1232                     [128]                     --
│    └─Empty: 2-1233                     [128]                     --
│    └─BatchNorm2d: 2-1234               [16, 128, 8, 8]           --
│    └─Scaler: 2-1235                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1236                      [16, 128, 8, 8]           --
│    └─Empty: 2-1237                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1238                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-124               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1239        --                        --
│    └─One: 2-1240                       [1]                       --
│    └─OutputScale: 2-1241               --                        --
│    └─Empty: 2-1242                     [16, 128, 1, 1]           --
│    └─Empty: 2-1243                     [16, 128, 1, 1]           --
│    └─Empty: 2-1244                     [16]                      --
│    └─Empty: 2-1245                     [16]                      --
│    └─BatchNorm2d: 2-1246               [16, 16, 8, 8]            --
│    └─Scaler: 2-1247                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1248                      [16, 16, 8, 8]            --
│    └─Empty: 2-1249                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1250                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-125        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1251                 [16, 128, 8, 8]           --
│    └─Empty: 2-1252                     [16, 128, 8, 8]           --
│    └─Empty: 2-1253                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1254        --                        --
│    └─One: 2-1255                       [1]                       --
│    └─OutputScale: 2-1256               --                        --
│    └─Empty: 2-1257                     [16, 128, 3, 3]           --
│    └─Empty: 2-1258                     [16, 128, 3, 3]           --
│    └─Empty: 2-1259                     [16]                      --
│    └─Empty: 2-1260                     [16]                      --
│    └─BatchNorm2d: 2-1261               [16, 16, 8, 8]            --
│    └─Scaler: 2-1262                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1263                      [16, 16, 8, 8]            --
│    └─Empty: 2-1264                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1265                     [16, 16, 8, 8]            --
├─Dropout2d: 1-126                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-127               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1266        --                        --
│    └─One: 2-1267                       [1]                       --
│    └─OutputScale: 2-1268               --                        --
│    └─Empty: 2-1269                     [128, 48, 1, 1]           --
│    └─Empty: 2-1270                     [128, 48, 1, 1]           --
│    └─Empty: 2-1271                     [128]                     --
│    └─Empty: 2-1272                     [128]                     --
│    └─BatchNorm2d: 2-1273               [16, 128, 64, 64]         --
│    └─Scaler: 2-1274                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1275                      [16, 128, 64, 64]         --
│    └─Empty: 2-1276                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1277                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-128        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1278                 [16, 128, 32, 32]         --
│    └─Empty: 2-1279                     [16, 128, 32, 32]         --
│    └─Empty: 2-1280                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1281        --                        --
│    └─One: 2-1282                       [1]                       --
│    └─OutputScale: 2-1283               --                        --
│    └─Empty: 2-1284                     [128, 128, 3, 3]          --
│    └─Empty: 2-1285                     [128, 128, 3, 3]          --
│    └─Empty: 2-1286                     [128]                     --
│    └─Empty: 2-1287                     [128]                     --
│    └─BatchNorm2d: 2-1288               [16, 128, 32, 32]         --
│    └─Scaler: 2-1289                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1290                      [16, 128, 32, 32]         --
│    └─Empty: 2-1291                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1292                     [16, 128, 32, 32]         --
├─Dropout2d: 1-129                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-130        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1293                 [16, 128, 16, 16]         --
│    └─Empty: 2-1294                     [16, 128, 16, 16]         --
│    └─Empty: 2-1295                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1296        --                        --
│    └─One: 2-1297                       [1]                       --
│    └─OutputScale: 2-1298               --                        --
│    └─Empty: 2-1299                     [128, 128, 3, 3]          --
│    └─Empty: 2-1300                     [128, 128, 3, 3]          --
│    └─Empty: 2-1301                     [128]                     --
│    └─Empty: 2-1302                     [128]                     --
│    └─BatchNorm2d: 2-1303               [16, 128, 16, 16]         --
│    └─Scaler: 2-1304                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1305                      [16, 128, 16, 16]         --
│    └─Empty: 2-1306                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1307                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-131               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1308        --                        --
│    └─One: 2-1309                       [1]                       --
│    └─OutputScale: 2-1310               --                        --
│    └─Empty: 2-1311                     [128, 128, 1, 1]          --
│    └─Empty: 2-1312                     [128, 128, 1, 1]          --
│    └─Empty: 2-1313                     [128]                     --
│    └─Empty: 2-1314                     [128]                     --
│    └─BatchNorm2d: 2-1315               [16, 128, 16, 16]         --
│    └─Scaler: 2-1316                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1317                      [16, 128, 16, 16]         --
│    └─Empty: 2-1318                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1319                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-132        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1320                 [16, 128, 16, 16]         --
│    └─Empty: 2-1321                     [16, 128, 16, 16]         --
│    └─Empty: 2-1322                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1323        --                        --
│    └─One: 2-1324                       [1]                       --
│    └─OutputScale: 2-1325               --                        --
│    └─Empty: 2-1326                     [128, 128, 3, 3]          --
│    └─Empty: 2-1327                     [128, 128, 3, 3]          --
│    └─Empty: 2-1328                     [128]                     --
│    └─Empty: 2-1329                     [128]                     --
│    └─BatchNorm2d: 2-1330               [16, 128, 16, 16]         --
│    └─Scaler: 2-1331                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1332                      [16, 128, 16, 16]         --
│    └─Empty: 2-1333                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1334                     [16, 128, 16, 16]         --
├─Dropout2d: 1-133                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-134        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1335                 [16, 128, 8, 8]           --
│    └─Empty: 2-1336                     [16, 128, 8, 8]           --
│    └─Empty: 2-1337                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1338        --                        --
│    └─One: 2-1339                       [1]                       --
│    └─OutputScale: 2-1340               --                        --
│    └─Empty: 2-1341                     [128, 128, 3, 3]          --
│    └─Empty: 2-1342                     [128, 128, 3, 3]          --
│    └─Empty: 2-1343                     [128]                     --
│    └─Empty: 2-1344                     [128]                     --
│    └─BatchNorm2d: 2-1345               [16, 128, 8, 8]           --
│    └─Scaler: 2-1346                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1347                      [16, 128, 8, 8]           --
│    └─Empty: 2-1348                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1349                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-135               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1350        --                        --
│    └─One: 2-1351                       [1]                       --
│    └─OutputScale: 2-1352               --                        --
│    └─Empty: 2-1353                     [16, 128, 1, 1]           --
│    └─Empty: 2-1354                     [16, 128, 1, 1]           --
│    └─Empty: 2-1355                     [16]                      --
│    └─Empty: 2-1356                     [16]                      --
│    └─BatchNorm2d: 2-1357               [16, 16, 8, 8]            --
│    └─Scaler: 2-1358                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1359                      [16, 16, 8, 8]            --
│    └─Empty: 2-1360                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1361                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-136        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1362                 [16, 128, 8, 8]           --
│    └─Empty: 2-1363                     [16, 128, 8, 8]           --
│    └─Empty: 2-1364                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1365        --                        --
│    └─One: 2-1366                       [1]                       --
│    └─OutputScale: 2-1367               --                        --
│    └─Empty: 2-1368                     [16, 128, 3, 3]           --
│    └─Empty: 2-1369                     [16, 128, 3, 3]           --
│    └─Empty: 2-1370                     [16]                      --
│    └─Empty: 2-1371                     [16]                      --
│    └─BatchNorm2d: 2-1372               [16, 16, 8, 8]            --
│    └─Scaler: 2-1373                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1374                      [16, 16, 8, 8]            --
│    └─Empty: 2-1375                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1376                     [16, 16, 8, 8]            --
├─Dropout2d: 1-137                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-138               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1377        --                        --
│    └─One: 2-1378                       [1]                       --
│    └─OutputScale: 2-1379               --                        --
│    └─Empty: 2-1380                     [128, 48, 1, 1]           --
│    └─Empty: 2-1381                     [128, 48, 1, 1]           --
│    └─Empty: 2-1382                     [128]                     --
│    └─Empty: 2-1383                     [128]                     --
│    └─BatchNorm2d: 2-1384               [16, 128, 64, 64]         --
│    └─Scaler: 2-1385                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1386                      [16, 128, 64, 64]         --
│    └─Empty: 2-1387                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1388                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-139        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1389                 [16, 128, 32, 32]         --
│    └─Empty: 2-1390                     [16, 128, 32, 32]         --
│    └─Empty: 2-1391                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1392        --                        --
│    └─One: 2-1393                       [1]                       --
│    └─OutputScale: 2-1394               --                        --
│    └─Empty: 2-1395                     [128, 128, 3, 3]          --
│    └─Empty: 2-1396                     [128, 128, 3, 3]          --
│    └─Empty: 2-1397                     [128]                     --
│    └─Empty: 2-1398                     [128]                     --
│    └─BatchNorm2d: 2-1399               [16, 128, 32, 32]         --
│    └─Scaler: 2-1400                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1401                      [16, 128, 32, 32]         --
│    └─Empty: 2-1402                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1403                     [16, 128, 32, 32]         --
├─Dropout2d: 1-140                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-141        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1404                 [16, 128, 16, 16]         --
│    └─Empty: 2-1405                     [16, 128, 16, 16]         --
│    └─Empty: 2-1406                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1407        --                        --
│    └─One: 2-1408                       [1]                       --
│    └─OutputScale: 2-1409               --                        --
│    └─Empty: 2-1410                     [128, 128, 3, 3]          --
│    └─Empty: 2-1411                     [128, 128, 3, 3]          --
│    └─Empty: 2-1412                     [128]                     --
│    └─Empty: 2-1413                     [128]                     --
│    └─BatchNorm2d: 2-1414               [16, 128, 16, 16]         --
│    └─Scaler: 2-1415                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1416                      [16, 128, 16, 16]         --
│    └─Empty: 2-1417                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1418                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-142               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1419        --                        --
│    └─One: 2-1420                       [1]                       --
│    └─OutputScale: 2-1421               --                        --
│    └─Empty: 2-1422                     [128, 128, 1, 1]          --
│    └─Empty: 2-1423                     [128, 128, 1, 1]          --
│    └─Empty: 2-1424                     [128]                     --
│    └─Empty: 2-1425                     [128]                     --
│    └─BatchNorm2d: 2-1426               [16, 128, 16, 16]         --
│    └─Scaler: 2-1427                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1428                      [16, 128, 16, 16]         --
│    └─Empty: 2-1429                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1430                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-143        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1431                 [16, 128, 16, 16]         --
│    └─Empty: 2-1432                     [16, 128, 16, 16]         --
│    └─Empty: 2-1433                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1434        --                        --
│    └─One: 2-1435                       [1]                       --
│    └─OutputScale: 2-1436               --                        --
│    └─Empty: 2-1437                     [128, 128, 3, 3]          --
│    └─Empty: 2-1438                     [128, 128, 3, 3]          --
│    └─Empty: 2-1439                     [128]                     --
│    └─Empty: 2-1440                     [128]                     --
│    └─BatchNorm2d: 2-1441               [16, 128, 16, 16]         --
│    └─Scaler: 2-1442                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1443                      [16, 128, 16, 16]         --
│    └─Empty: 2-1444                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1445                     [16, 128, 16, 16]         --
├─Dropout2d: 1-144                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-145        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1446                 [16, 128, 8, 8]           --
│    └─Empty: 2-1447                     [16, 128, 8, 8]           --
│    └─Empty: 2-1448                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1449        --                        --
│    └─One: 2-1450                       [1]                       --
│    └─OutputScale: 2-1451               --                        --
│    └─Empty: 2-1452                     [128, 128, 3, 3]          --
│    └─Empty: 2-1453                     [128, 128, 3, 3]          --
│    └─Empty: 2-1454                     [128]                     --
│    └─Empty: 2-1455                     [128]                     --
│    └─BatchNorm2d: 2-1456               [16, 128, 8, 8]           --
│    └─Scaler: 2-1457                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1458                      [16, 128, 8, 8]           --
│    └─Empty: 2-1459                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1460                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-146               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1461        --                        --
│    └─One: 2-1462                       [1]                       --
│    └─OutputScale: 2-1463               --                        --
│    └─Empty: 2-1464                     [16, 128, 1, 1]           --
│    └─Empty: 2-1465                     [16, 128, 1, 1]           --
│    └─Empty: 2-1466                     [16]                      --
│    └─Empty: 2-1467                     [16]                      --
│    └─BatchNorm2d: 2-1468               [16, 16, 8, 8]            --
│    └─Scaler: 2-1469                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1470                      [16, 16, 8, 8]            --
│    └─Empty: 2-1471                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1472                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-147        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1473                 [16, 128, 8, 8]           --
│    └─Empty: 2-1474                     [16, 128, 8, 8]           --
│    └─Empty: 2-1475                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1476        --                        --
│    └─One: 2-1477                       [1]                       --
│    └─OutputScale: 2-1478               --                        --
│    └─Empty: 2-1479                     [16, 128, 3, 3]           --
│    └─Empty: 2-1480                     [16, 128, 3, 3]           --
│    └─Empty: 2-1481                     [16]                      --
│    └─Empty: 2-1482                     [16]                      --
│    └─BatchNorm2d: 2-1483               [16, 16, 8, 8]            --
│    └─Scaler: 2-1484                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1485                      [16, 16, 8, 8]            --
│    └─Empty: 2-1486                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1487                     [16, 16, 8, 8]            --
├─Dropout2d: 1-148                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-149               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1488        --                        --
│    └─One: 2-1489                       [1]                       --
│    └─OutputScale: 2-1490               --                        --
│    └─Empty: 2-1491                     [128, 48, 1, 1]           --
│    └─Empty: 2-1492                     [128, 48, 1, 1]           --
│    └─Empty: 2-1493                     [128]                     --
│    └─Empty: 2-1494                     [128]                     --
│    └─BatchNorm2d: 2-1495               [16, 128, 64, 64]         --
│    └─Scaler: 2-1496                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1497                      [16, 128, 64, 64]         --
│    └─Empty: 2-1498                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1499                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-150        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1500                 [16, 128, 32, 32]         --
│    └─Empty: 2-1501                     [16, 128, 32, 32]         --
│    └─Empty: 2-1502                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1503        --                        --
│    └─One: 2-1504                       [1]                       --
│    └─OutputScale: 2-1505               --                        --
│    └─Empty: 2-1506                     [128, 128, 3, 3]          --
│    └─Empty: 2-1507                     [128, 128, 3, 3]          --
│    └─Empty: 2-1508                     [128]                     --
│    └─Empty: 2-1509                     [128]                     --
│    └─BatchNorm2d: 2-1510               [16, 128, 32, 32]         --
│    └─Scaler: 2-1511                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1512                      [16, 128, 32, 32]         --
│    └─Empty: 2-1513                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1514                     [16, 128, 32, 32]         --
├─Dropout2d: 1-151                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-152        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1515                 [16, 128, 16, 16]         --
│    └─Empty: 2-1516                     [16, 128, 16, 16]         --
│    └─Empty: 2-1517                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1518        --                        --
│    └─One: 2-1519                       [1]                       --
│    └─OutputScale: 2-1520               --                        --
│    └─Empty: 2-1521                     [128, 128, 3, 3]          --
│    └─Empty: 2-1522                     [128, 128, 3, 3]          --
│    └─Empty: 2-1523                     [128]                     --
│    └─Empty: 2-1524                     [128]                     --
│    └─BatchNorm2d: 2-1525               [16, 128, 16, 16]         --
│    └─Scaler: 2-1526                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1527                      [16, 128, 16, 16]         --
│    └─Empty: 2-1528                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1529                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-153               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1530        --                        --
│    └─One: 2-1531                       [1]                       --
│    └─OutputScale: 2-1532               --                        --
│    └─Empty: 2-1533                     [128, 128, 1, 1]          --
│    └─Empty: 2-1534                     [128, 128, 1, 1]          --
│    └─Empty: 2-1535                     [128]                     --
│    └─Empty: 2-1536                     [128]                     --
│    └─BatchNorm2d: 2-1537               [16, 128, 16, 16]         --
│    └─Scaler: 2-1538                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1539                      [16, 128, 16, 16]         --
│    └─Empty: 2-1540                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1541                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-154        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1542                 [16, 128, 16, 16]         --
│    └─Empty: 2-1543                     [16, 128, 16, 16]         --
│    └─Empty: 2-1544                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1545        --                        --
│    └─One: 2-1546                       [1]                       --
│    └─OutputScale: 2-1547               --                        --
│    └─Empty: 2-1548                     [128, 128, 3, 3]          --
│    └─Empty: 2-1549                     [128, 128, 3, 3]          --
│    └─Empty: 2-1550                     [128]                     --
│    └─Empty: 2-1551                     [128]                     --
│    └─BatchNorm2d: 2-1552               [16, 128, 16, 16]         --
│    └─Scaler: 2-1553                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1554                      [16, 128, 16, 16]         --
│    └─Empty: 2-1555                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1556                     [16, 128, 16, 16]         --
├─Dropout2d: 1-155                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-156        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1557                 [16, 128, 8, 8]           --
│    └─Empty: 2-1558                     [16, 128, 8, 8]           --
│    └─Empty: 2-1559                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1560        --                        --
│    └─One: 2-1561                       [1]                       --
│    └─OutputScale: 2-1562               --                        --
│    └─Empty: 2-1563                     [128, 128, 3, 3]          --
│    └─Empty: 2-1564                     [128, 128, 3, 3]          --
│    └─Empty: 2-1565                     [128]                     --
│    └─Empty: 2-1566                     [128]                     --
│    └─BatchNorm2d: 2-1567               [16, 128, 8, 8]           --
│    └─Scaler: 2-1568                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1569                      [16, 128, 8, 8]           --
│    └─Empty: 2-1570                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1571                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-157               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1572        --                        --
│    └─One: 2-1573                       [1]                       --
│    └─OutputScale: 2-1574               --                        --
│    └─Empty: 2-1575                     [16, 128, 1, 1]           --
│    └─Empty: 2-1576                     [16, 128, 1, 1]           --
│    └─Empty: 2-1577                     [16]                      --
│    └─Empty: 2-1578                     [16]                      --
│    └─BatchNorm2d: 2-1579               [16, 16, 8, 8]            --
│    └─Scaler: 2-1580                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1581                      [16, 16, 8, 8]            --
│    └─Empty: 2-1582                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1583                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-158        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1584                 [16, 128, 8, 8]           --
│    └─Empty: 2-1585                     [16, 128, 8, 8]           --
│    └─Empty: 2-1586                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1587        --                        --
│    └─One: 2-1588                       [1]                       --
│    └─OutputScale: 2-1589               --                        --
│    └─Empty: 2-1590                     [16, 128, 3, 3]           --
│    └─Empty: 2-1591                     [16, 128, 3, 3]           --
│    └─Empty: 2-1592                     [16]                      --
│    └─Empty: 2-1593                     [16]                      --
│    └─BatchNorm2d: 2-1594               [16, 16, 8, 8]            --
│    └─Scaler: 2-1595                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1596                      [16, 16, 8, 8]            --
│    └─Empty: 2-1597                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1598                     [16, 16, 8, 8]            --
├─Dropout2d: 1-159                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-160               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1599        --                        --
│    └─One: 2-1600                       [1]                       --
│    └─OutputScale: 2-1601               --                        --
│    └─Empty: 2-1602                     [128, 48, 1, 1]           --
│    └─Empty: 2-1603                     [128, 48, 1, 1]           --
│    └─Empty: 2-1604                     [128]                     --
│    └─Empty: 2-1605                     [128]                     --
│    └─BatchNorm2d: 2-1606               [16, 128, 64, 64]         --
│    └─Scaler: 2-1607                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1608                      [16, 128, 64, 64]         --
│    └─Empty: 2-1609                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1610                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-161        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1611                 [16, 128, 32, 32]         --
│    └─Empty: 2-1612                     [16, 128, 32, 32]         --
│    └─Empty: 2-1613                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1614        --                        --
│    └─One: 2-1615                       [1]                       --
│    └─OutputScale: 2-1616               --                        --
│    └─Empty: 2-1617                     [128, 128, 3, 3]          --
│    └─Empty: 2-1618                     [128, 128, 3, 3]          --
│    └─Empty: 2-1619                     [128]                     --
│    └─Empty: 2-1620                     [128]                     --
│    └─BatchNorm2d: 2-1621               [16, 128, 32, 32]         --
│    └─Scaler: 2-1622                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1623                      [16, 128, 32, 32]         --
│    └─Empty: 2-1624                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1625                     [16, 128, 32, 32]         --
├─Dropout2d: 1-162                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-163        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1626                 [16, 128, 16, 16]         --
│    └─Empty: 2-1627                     [16, 128, 16, 16]         --
│    └─Empty: 2-1628                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1629        --                        --
│    └─One: 2-1630                       [1]                       --
│    └─OutputScale: 2-1631               --                        --
│    └─Empty: 2-1632                     [128, 128, 3, 3]          --
│    └─Empty: 2-1633                     [128, 128, 3, 3]          --
│    └─Empty: 2-1634                     [128]                     --
│    └─Empty: 2-1635                     [128]                     --
│    └─BatchNorm2d: 2-1636               [16, 128, 16, 16]         --
│    └─Scaler: 2-1637                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1638                      [16, 128, 16, 16]         --
│    └─Empty: 2-1639                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1640                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-164               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1641        --                        --
│    └─One: 2-1642                       [1]                       --
│    └─OutputScale: 2-1643               --                        --
│    └─Empty: 2-1644                     [128, 128, 1, 1]          --
│    └─Empty: 2-1645                     [128, 128, 1, 1]          --
│    └─Empty: 2-1646                     [128]                     --
│    └─Empty: 2-1647                     [128]                     --
│    └─BatchNorm2d: 2-1648               [16, 128, 16, 16]         --
│    └─Scaler: 2-1649                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1650                      [16, 128, 16, 16]         --
│    └─Empty: 2-1651                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1652                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-165        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1653                 [16, 128, 16, 16]         --
│    └─Empty: 2-1654                     [16, 128, 16, 16]         --
│    └─Empty: 2-1655                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1656        --                        --
│    └─One: 2-1657                       [1]                       --
│    └─OutputScale: 2-1658               --                        --
│    └─Empty: 2-1659                     [128, 128, 3, 3]          --
│    └─Empty: 2-1660                     [128, 128, 3, 3]          --
│    └─Empty: 2-1661                     [128]                     --
│    └─Empty: 2-1662                     [128]                     --
│    └─BatchNorm2d: 2-1663               [16, 128, 16, 16]         --
│    └─Scaler: 2-1664                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1665                      [16, 128, 16, 16]         --
│    └─Empty: 2-1666                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1667                     [16, 128, 16, 16]         --
├─Dropout2d: 1-166                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-167        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1668                 [16, 128, 8, 8]           --
│    └─Empty: 2-1669                     [16, 128, 8, 8]           --
│    └─Empty: 2-1670                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1671        --                        --
│    └─One: 2-1672                       [1]                       --
│    └─OutputScale: 2-1673               --                        --
│    └─Empty: 2-1674                     [128, 128, 3, 3]          --
│    └─Empty: 2-1675                     [128, 128, 3, 3]          --
│    └─Empty: 2-1676                     [128]                     --
│    └─Empty: 2-1677                     [128]                     --
│    └─BatchNorm2d: 2-1678               [16, 128, 8, 8]           --
│    └─Scaler: 2-1679                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1680                      [16, 128, 8, 8]           --
│    └─Empty: 2-1681                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1682                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-168               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1683        --                        --
│    └─One: 2-1684                       [1]                       --
│    └─OutputScale: 2-1685               --                        --
│    └─Empty: 2-1686                     [16, 128, 1, 1]           --
│    └─Empty: 2-1687                     [16, 128, 1, 1]           --
│    └─Empty: 2-1688                     [16]                      --
│    └─Empty: 2-1689                     [16]                      --
│    └─BatchNorm2d: 2-1690               [16, 16, 8, 8]            --
│    └─Scaler: 2-1691                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1692                      [16, 16, 8, 8]            --
│    └─Empty: 2-1693                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1694                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-169        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1695                 [16, 128, 8, 8]           --
│    └─Empty: 2-1696                     [16, 128, 8, 8]           --
│    └─Empty: 2-1697                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1698        --                        --
│    └─One: 2-1699                       [1]                       --
│    └─OutputScale: 2-1700               --                        --
│    └─Empty: 2-1701                     [16, 128, 3, 3]           --
│    └─Empty: 2-1702                     [16, 128, 3, 3]           --
│    └─Empty: 2-1703                     [16]                      --
│    └─Empty: 2-1704                     [16]                      --
│    └─BatchNorm2d: 2-1705               [16, 16, 8, 8]            --
│    └─Scaler: 2-1706                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1707                      [16, 16, 8, 8]            --
│    └─Empty: 2-1708                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1709                     [16, 16, 8, 8]            --
├─Dropout2d: 1-170                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-171               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1710        --                        --
│    └─One: 2-1711                       [1]                       --
│    └─OutputScale: 2-1712               --                        --
│    └─Empty: 2-1713                     [128, 48, 1, 1]           --
│    └─Empty: 2-1714                     [128, 48, 1, 1]           --
│    └─Empty: 2-1715                     [128]                     --
│    └─Empty: 2-1716                     [128]                     --
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Empty: 2-1720                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1721                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Empty: 2-1723                     [16, 128, 32, 32]         --
│    └─Empty: 2-1724                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1725        --                        --
│    └─One: 2-1726                       [1]                       --
│    └─OutputScale: 2-1727               --                        --
│    └─Empty: 2-1728                     [128, 128, 3, 3]          --
│    └─Empty: 2-1729                     [128, 128, 3, 3]          --
│    └─Empty: 2-1730                     [128]                     --
│    └─Empty: 2-1731                     [128]                     --
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         --
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─Empty: 2-1735                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1736                     [16, 128, 32, 32]         --
├─Dropout2d: 1-173                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1740        --                        --
│    └─One: 2-1741                       [1]                       --
│    └─OutputScale: 2-1742               --                        --
│    └─Empty: 2-1743                     [128, 128, 3, 3]          --
│    └─Empty: 2-1744                     [128, 128, 3, 3]          --
│    └─Empty: 2-1745                     [128]                     --
│    └─Empty: 2-1746                     [128]                     --
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─Empty: 2-1750                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1751                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-175               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1752        --                        --
│    └─One: 2-1753                       [1]                       --
│    └─OutputScale: 2-1754               --                        --
│    └─Empty: 2-1755                     [128, 128, 1, 1]          --
│    └─Empty: 2-1756                     [128, 128, 1, 1]          --
│    └─Empty: 2-1757                     [128]                     --
│    └─Empty: 2-1758                     [128]                     --
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Empty: 2-1762                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1763                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Empty: 2-1765                     [16, 128, 16, 16]         --
│    └─Empty: 2-1766                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1767        --                        --
│    └─One: 2-1768                       [1]                       --
│    └─OutputScale: 2-1769               --                        --
│    └─Empty: 2-1770                     [128, 128, 3, 3]          --
│    └─Empty: 2-1771                     [128, 128, 3, 3]          --
│    └─Empty: 2-1772                     [128]                     --
│    └─Empty: 2-1773                     [128]                     --
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─Empty: 2-1777                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1778                     [16, 128, 16, 16]         --
├─Dropout2d: 1-177                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1782        --                        --
│    └─One: 2-1783                       [1]                       --
│    └─OutputScale: 2-1784               --                        --
│    └─Empty: 2-1785                     [128, 128, 3, 3]          --
│    └─Empty: 2-1786                     [128, 128, 3, 3]          --
│    └─Empty: 2-1787                     [128]                     --
│    └─Empty: 2-1788                     [128]                     --
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─Empty: 2-1792                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1793                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-179               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1794        --                        --
│    └─One: 2-1795                       [1]                       --
│    └─OutputScale: 2-1796               --                        --
│    └─Empty: 2-1797                     [16, 128, 1, 1]           --
│    └─Empty: 2-1798                     [16, 128, 1, 1]           --
│    └─Empty: 2-1799                     [16]                      --
│    └─Empty: 2-1800                     [16]                      --
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Empty: 2-1804                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1805                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1809        --                        --
│    └─One: 2-1810                       [1]                       --
│    └─OutputScale: 2-1811               --                        --
│    └─Empty: 2-1812                     [16, 128, 3, 3]           --
│    └─Empty: 2-1813                     [16, 128, 3, 3]           --
│    └─Empty: 2-1814                     [16]                      --
│    └─Empty: 2-1815                     [16]                      --
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─Empty: 2-1819                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1820                     [16, 16, 8, 8]            --
├─Dropout2d: 1-181                       [16, 16, 8, 8]            --
├─Linear: 1-182                          [16, 5]                   5,126
│    └─OutputShiftSqueeze: 2-1821        --                        --
│    └─One: 2-1822                       [1]                       --
│    └─OutputScale: 2-1823               --                        --
│    └─Empty: 2-1824                     [5, 1024]                 --
│    └─Empty: 2-1825                     [5, 1024]                 --
│    └─Empty: 2-1826                     [16, 5]                   --
│    └─Empty: 2-1827                     [16, 5]                   --
│    └─Clamp: 2-1828                     [16, 5]                   --
├─Linear: 1-183                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1829        --                        --
│    └─One: 2-1830                       [1]                       --
│    └─OutputScale: 2-1831               --                        --
│    └─Empty: 2-1832                     [5, 1024]                 --
│    └─Empty: 2-1833                     [5, 1024]                 --
│    └─Empty: 2-1834                     [16, 5]                   --
│    └─Empty: 2-1835                     [16, 5]                   --
│    └─Clamp: 2-1836                     [16, 5]                   --
├─Linear: 1-184                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1837        --                        --
│    └─One: 2-1838                       [1]                       --
│    └─OutputScale: 2-1839               --                        --
│    └─Empty: 2-1840                     [5, 1024]                 --
│    └─Empty: 2-1841                     [5, 1024]                 --
│    └─Empty: 2-1842                     [16, 5]                   --
│    └─Empty: 2-1843                     [16, 5]                   --
│    └─Clamp: 2-1844                     [16, 5]                   --
├─Linear: 1-185                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1845        --                        --
│    └─One: 2-1846                       [1]                       --
│    └─OutputScale: 2-1847               --                        --
│    └─Empty: 2-1848                     [5, 1024]                 --
│    └─Empty: 2-1849                     [5, 1024]                 --
│    └─Empty: 2-1850                     [16, 5]                   --
│    └─Empty: 2-1851                     [16, 5]                   --
│    └─Clamp: 2-1852                     [16, 5]                   --
├─Linear: 1-186                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1853        --                        --
│    └─One: 2-1854                       [1]                       --
│    └─OutputScale: 2-1855               --                        --
│    └─Empty: 2-1856                     [5, 1024]                 --
│    └─Empty: 2-1857                     [5, 1024]                 --
│    └─Empty: 2-1858                     [16, 5]                   --
│    └─Empty: 2-1859                     [16, 5]                   --
│    └─Clamp: 2-1860                     [16, 5]                   --
├─Linear: 1-187                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1861        --                        --
│    └─One: 2-1862                       [1]                       --
│    └─OutputScale: 2-1863               --                        --
│    └─Empty: 2-1864                     [5, 1024]                 --
│    └─Empty: 2-1865                     [5, 1024]                 --
│    └─Empty: 2-1866                     [16, 5]                   --
│    └─Empty: 2-1867                     [16, 5]                   --
│    └─Clamp: 2-1868                     [16, 5]                   --
├─Linear: 1-188                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1869        --                        --
│    └─One: 2-1870                       [1]                       --
│    └─OutputScale: 2-1871               --                        --
│    └─Empty: 2-1872                     [5, 1024]                 --
│    └─Empty: 2-1873                     [5, 1024]                 --
│    └─Empty: 2-1874                     [16, 5]                   --
│    └─Empty: 2-1875                     [16, 5]                   --
│    └─Clamp: 2-1876                     [16, 5]                   --
├─Linear: 1-189                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1877        --                        --
│    └─One: 2-1878                       [1]                       --
│    └─OutputScale: 2-1879               --                        --
│    └─Empty: 2-1880                     [5, 1024]                 --
│    └─Empty: 2-1881                     [5, 1024]                 --
│    └─Empty: 2-1882                     [16, 5]                   --
│    └─Empty: 2-1883                     [16, 5]                   --
│    └─Clamp: 2-1884                     [16, 5]                   --
├─Linear: 1-190                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1885        --                        --
│    └─One: 2-1886                       [1]                       --
│    └─OutputScale: 2-1887               --                        --
│    └─Empty: 2-1888                     [5, 1024]                 --
│    └─Empty: 2-1889                     [5, 1024]                 --
│    └─Empty: 2-1890                     [16, 5]                   --
│    └─Empty: 2-1891                     [16, 5]                   --
│    └─Clamp: 2-1892                     [16, 5]                   --
├─Linear: 1-191                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1893        --                        --
│    └─One: 2-1894                       [1]                       --
│    └─OutputScale: 2-1895               --                        --
│    └─Empty: 2-1896                     [5, 1024]                 --
│    └─Empty: 2-1897                     [5, 1024]                 --
│    └─Empty: 2-1898                     [16, 5]                   --
│    └─Empty: 2-1899                     [16, 5]                   --
│    └─Clamp: 2-1900                     [16, 5]                   --
├─Linear: 1-192                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1901        --                        --
│    └─One: 2-1902                       [1]                       --
│    └─OutputScale: 2-1903               --                        --
│    └─Empty: 2-1904                     [5, 1024]                 --
│    └─Empty: 2-1905                     [5, 1024]                 --
│    └─Empty: 2-1906                     [16, 5]                   --
│    └─Empty: 2-1907                     [16, 5]                   --
│    └─Clamp: 2-1908                     [16, 5]                   --
├─Linear: 1-193                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1909        --                        --
│    └─One: 2-1910                       [1]                       --
│    └─OutputScale: 2-1911               --                        --
│    └─Empty: 2-1912                     [5, 1024]                 --
│    └─Empty: 2-1913                     [5, 1024]                 --
│    └─Empty: 2-1914                     [16, 5]                   --
│    └─Empty: 2-1915                     [16, 5]                   --
│    └─Clamp: 2-1916                     [16, 5]                   --
├─Linear: 1-194                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1917        --                        --
│    └─One: 2-1918                       [1]                       --
│    └─OutputScale: 2-1919               --                        --
│    └─Empty: 2-1920                     [5, 1024]                 --
│    └─Empty: 2-1921                     [5, 1024]                 --
│    └─Empty: 2-1922                     [16, 5]                   --
│    └─Empty: 2-1923                     [16, 5]                   --
│    └─Clamp: 2-1924                     [16, 5]                   --
├─Linear: 1-195                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1925        --                        --
│    └─One: 2-1926                       [1]                       --
│    └─OutputScale: 2-1927               --                        --
│    └─Empty: 2-1928                     [5, 1024]                 --
│    └─Empty: 2-1929                     [5, 1024]                 --
│    └─Empty: 2-1930                     [16, 5]                   --
│    └─Empty: 2-1931                     [16, 5]                   --
│    └─Clamp: 2-1932                     [16, 5]                   --
├─Linear: 1-196                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1933        --                        --
│    └─One: 2-1934                       [1]                       --
│    └─OutputScale: 2-1935               --                        --
│    └─Empty: 2-1936                     [5, 1024]                 --
│    └─Empty: 2-1937                     [5, 1024]                 --
│    └─Empty: 2-1938                     [16, 5]                   --
│    └─Empty: 2-1939                     [16, 5]                   --
│    └─Clamp: 2-1940                     [16, 5]                   --
├─Linear: 1-197                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1941        --                        --
│    └─One: 2-1942                       [1]                       --
│    └─OutputScale: 2-1943               --                        --
│    └─Empty: 2-1944                     [5, 1024]                 --
│    └─Empty: 2-1945                     [5, 1024]                 --
│    └─Empty: 2-1946                     [16, 5]                   --
│    └─Empty: 2-1947                     [16, 5]                   --
│    └─Clamp: 2-1948                     [16, 5]                   --
==========================================================================================
Total params: 638,806
Trainable params: 638,752
Non-trainable params: 54
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 0.00 
Params size (MB): 2.53
Estimated Total Size (MB): 203.86
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.578 | Acc: 21.625% | Wgt Acc: 25.449%
	I - Batch: 100 | Loss: 1.552 | Acc: 24.438% | Wgt Acc: 28.880%
	I - Batch: 150 | Loss: 1.523 | Acc: 26.750% | Wgt Acc: 31.456%
	I - Batch: 200 | Loss: 1.500 | Acc: 28.000% | Wgt Acc: 32.815%
I - num batch: 222
I - Train -- Loss: 1.490 | Acc: 28.898% | Wgt Acc: 33.844% | LR: 1.000000e-04 | Dur: 134.74s
I - Confusion Matrix: [row->prediction - col->label]
[[485. 142. 265. 378. 370.]
 [ 20.  97.  60.  26.  99.]
 [153. 323. 394.  85. 521.]
 [ 39.  16.  15.  49.  10.]
 [  0.   0.   0.   0.   0.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.551 | Acc: 19.724% | Wgt Acc: 24.167% | Dur: 13.99s
I - Confusion Matrix: [row->prediction - col->label]
[[ 25.   0.   2.  21.   5.]
 [  5.  11.  10.  12.  12.]
 [ 55.  67.  63.  52. 163.]
 [  3.   0.   0.   1.   0.]
 [  0.   0.   0.   0.   0.]]

I - Local maximum validation set accuracy:  19.72

I - Validation set results: 
[14-1-2-0.61][50-3-2-0.46][124-2-2-0.65][127-0-0-0.14][443-2-2-0.96][567-0-2-0.25][573-1-2-0.69][615-0-2-0.13][695-1-2-0.43][722-3-0-0.06]
[826-0-0-0.24][878-0-0-0.00][1103-0-2-0.64][1212-3-2-0.30][1368-0-2-0.37][2181-2-1-0.32][2476-2-2-0.57][2721-2-2-0.93][2818-1-2-0.77][2886-2-2-1.11]
[3231-2-2-0.79][3333-2-1-0.45][3482-2-2-0.93][3536-3-1-0.37][3625-1-2-0.89][3909-0-2-0.73][4035-0-0-0.25][4140-0-2-0.49][4214-1-2-0.25][4346-1-2-0.27]
[4581-2-2-0.80][4708-3-1-0.55][4838-3-1-0.41][4845-1-2-0.63][4868-0-0-0.27][4939-0-2-1.13][4984-2-2-0.17][5078-1-2-0.81][5396-0-0-0.47][5479-1-2-0.69]
[5717-0-2-0.47][5843-1-1-0.61][5949-3-2-0.22][5987-2-2-0.83][6014-3-1-0.40][6033-3-2-0.48][6313-0-2-0.23][6421-3-0-0.16][6500-1-2-0.53][6583-3-2-0.30]
[6683-3-2-0.40][6825-2-2-0.42][6998-3-2-0.53][7049-3-2-0.49][7517-1-2-0.69][7521-1-1-0.50][7528-1-1-0.50][7949-1-2-0.87][8135-1-2-0.67][8185-3-0-0.28]
[8269-3-2-0.68][8273-3-0-0.21][8543-3-0-0.12][8666-1-1-0.46][8672-0-0-0.45][8903-1-2-0.73][9001-2-1-0.68][9036-2-2-0.95][9281-3-2-0.54][9300-2-2-0.81]
[9571-0-2-0.18][9617-1-2-0.96][9644-2-2-0.54][9705-2-2-0.76][9801-0-2-0.33][9803-3-2-0.39][9865-3-0-0.30][9896-2-2-0.92][10314-1-2-1.05][10337-3-0-0.10]
[10403-0-2-0.75][10653-2-1-0.65][10704-2-1-0.60][10719-1-2-0.97][10727-1-2-0.90][10836-0-0-0.55][10969-2-2-0.39][11042-0-2-0.53][11088-1-2-0.76][11322-0-2-0.23]
[11398-2-2-0.64][11499-0-2-0.47][11502-3-2-0.17][11512-3-2-0.57][11608-1-2-0.98][11610-0-2-0.36][11692-0-1-0.04][11905-0-0-0.74][11993-1-2-0.87][12002-2-0-0.38]
[12052-0-2-0.23][12201-0-0-0.17][12235-2-2-0.89][12320-1-2-0.72][12377-2-2-0.86][12398-2-2-0.47][12503-1-2-0.48][12617-0-1-0.59][12685-3-1-0.48][12738-2-2-0.48]
[12742-2-2-0.88][12823-0-3-0.37][13110-1-2-0.61][13240-3-2-0.26][13253-1-2-0.97][13273-0-0-0.33][13634-1-2-0.62][13763-2-1-0.43][13905-3-2-0.20][14060-2-1-0.77]
[14065-3-0-0.20][14147-3-2-0.34][14595-2-2-0.78][14687-2-2-0.47][14788-2-2-0.63][14869-1-2-0.52][14872-3-2-0.72][14877-1-2-0.59][14927-0-2-0.11][15066-0-0-0.35]
[15175-1-2-0.89][15178-2-2-0.26][15375-3-2-0.15][15389-3-0-0.27][15568-2-1-0.52][15675-3-2-0.40][15869-1-2-0.69][16207-3-2-0.38][16236-0-2-0.27][16302-3-2-0.22]
[16331-2-2-0.99][16381-0-2-0.46][16488-1-2-0.73][16495-0-2-0.52][16650-0-0-0.26][16719-1-1-0.49][16801-0-0-0.32][16828-0-2-0.10][17137-3-2-0.30][17245-1-2-0.61]
[17278-3-2-0.59][17282-0-2-0.58][17311-2-2-0.76][17336-2-2-0.68][17608-3-0-0.35][17627-0-2-0.43][17877-3-2-0.61][17924-1-2-0.64][17984-3-0-0.42][18211-0-1-0.33]
[18276-3-2-0.08][18287-1-2-0.74][18394-0-0-0.14][18428-0-2-0.87][18442-0-2-0.15][18478-3-0-0.33][18607-0-2-0.69][18616-0-1-0.48][18663-0-2-0.06][18718-0-2-0.11]
[18766-2-2-0.71][18824-2-2-0.88][18890-3-2-0.97][18930-3-2-0.70][18938-3-2-0.25][19817-1-2-0.72][19839-0-2-0.79][19930-3-2-0.08][19944-0-2-0.79][20036-2-2-0.97]
[20101-3-1-0.36][20474-1-2-0.97][20547-3-2-0.59][20929-2-2-0.57][21245-1-2-0.88][21257-3-2-0.45][21293-1-2-0.82][21316-1-2-0.59][21384-1-2-0.66][21448-1-2-0.87]
[21483-0-2-0.16][21487-2-2-0.89][21714-0-2-0.36][21943-3-2-0.93][21947-0-2-0.31][21948-0-0-0.30][21965-2-2-0.85][21998-1-2-0.59][22025-0-1-0.39][22228-3-2-0.27]
[22446-1-1-0.68][22494-3-0-0.19][22757-0-0-0.78][22811-3-2-0.41][22976-3-2-0.71][22985-3-0-0.29][23014-0-2-0.13][23112-1-2-0.76][23144-3-3-0.31][23168-2-2-0.59]
[23219-0-2-0.37][23363-3-1-0.34][23470-0-2-0.78][23486-2-1-0.34][23497-0-3-0.59][23516-0-0-0.95][23690-1-2-0.46][23921-2-2-0.80][23936-1-2-0.51][24040-3-2-0.54]
[24111-1-2-1.14][24182-0-3-0.62][24238-3-2-0.09][24290-2-2-0.77][24345-0-2-0.60][24364-1-2-0.27][24427-3-1-0.30][24477-2-2-0.84][24495-2-2-0.87][24893-2-2-1.04]
[25012-1-2-0.52][25121-2-2-0.75][25165-3-2-0.26][25183-0-2-0.55][25297-3-2-0.17][25398-0-2-0.12][25574-2-2-0.57][25644-1-2-0.76][25718-1-1-0.48][25774-2-2-0.49]
[26032-3-2-0.21][26051-3-0-0.23][26120-0-2-0.84][26321-1-1-0.53][26732-1-2-0.69][26784-3-0-0.67][26827-3-1-0.34][26833-0-0-0.07][26838-2-2-0.68][26860-1-2-0.79]
[26948-0-2-0.46][27049-3-2-0.24][27098-1-2-0.73][27526-0-0-0.13][27639-3-2-0.34][27698-3-0-0.06][27772-0-0-0.14][27890-1-2-0.71][28040-0-2-0.46][28503-2-2-0.98]
[28577-1-1-0.55][28959-0-0-0.71][29198-3-2-0.83][29777-0-0-0.71][29877-2-2-0.62][30035-1-2-1.07][30098-0-2-0.26][30326-1-1-0.77][30572-2-2-0.47][30716-0-2-0.59]
[30806-2-2-0.46][30906-1-2-1.01][31007-0-2-0.68][31181-3-2-0.63][31238-0-2-0.23][31347-0-2-0.11][31422-2-2-0.69][31429-3-2-0.37][31431-0-0-0.29][31432-1-2-0.71]
[31477-0-0-0.39][31524-1-2-0.41][31597-1-2-0.82][31619-1-2-0.48][31701-0-2-0.27][31755-0-2-0.17][31854-3-1-0.47][32074-1-2-0.57][32078-3-2-0.38][32111-1-2-1.05]
[32127-1-2-0.71][32140-3-2-0.07][32263-2-2-0.48][32365-0-2-0.56][32411-2-0-0.56][32429-3-0-0.13][32473-3-1--0.04][32574-3-0-0.47][32584-0-2-0.83][32622-0-2-0.58]
[32858-3-2-0.30][32969-3-0-0.41][33016-2-2-0.76][33031-1-2-0.55][33035-2-2-0.68][33133-2-2-0.72][33173-2-2-0.62][33175-3-2-0.89][33306-3-2-0.58][33309-2-2-0.55]
[33474-0-2-0.46][33478-2-2-0.44][33618-1-1-0.36][33712-0-2-0.28][33782-2-2-0.78][33914-3-1-0.46][34076-3-2-0.28][34112-2-1-0.58][34138-2-2-0.54][34239-1-2-0.93]
[34364-2-2-0.74][34617-1-2-0.67][34751-3-0-0.28][34783-2-2-0.97][35015-3-2-0.21][35018-1-2-1.00][35288-2-2-0.46][0-4-2-0.84][1-4-2-0.26][2-4-2-0.67]
[3-4-2-0.53][4-4-2-0.46][5-4-2-0.52][6-4-2-0.79][7-4-2-0.62][8-4-2-0.65][9-4-2-0.75][10-4-2-0.53][11-4-2-0.61][12-4-1-0.41]
[14-4-2-0.48][15-4-0-0.19][16-4-1-0.52][17-4-2-0.39][18-4-2-0.88][19-4-0-0.40][20-4-2-0.56][21-4-2-0.69][22-4-2-1.21][23-4-1-0.79]
[24-4-2-0.82][25-4-2-0.27][26-4-2-0.45][27-4-2-0.36][28-4-2-0.77][29-4-2-0.54][30-4-2-0.59][31-4-2-0.40][32-4-2-0.84][33-4-2-0.52]
[34-4-2-0.39][35-4-0-0.31][37-4-2-0.34][39-4-1-0.08][40-4-2-0.70][41-4-2-0.74][42-4-2-0.23][43-4-2-0.53][45-4-2-0.77][46-4-2-0.75]
[47-4-2-0.91][48-4-2-0.71][51-4-2-0.80][52-4-2-0.77][53-4-2-0.50][54-4-2-0.66][55-4-2-0.31][56-4-2-0.87][57-4-2-0.21][58-4-2-0.74]
[59-4-2-0.46][60-4-2-0.62][61-4-2-1.09][62-4-2-0.44][63-4-2-0.58][64-4-2-0.96][65-4-2-0.77][66-4-2-0.89][67-4-2-0.57][68-4-2-0.70]
[69-4-2-0.40][70-4-2-0.91][72-4-2-1.07][73-4-2-0.80][74-4-2-0.86][75-4-2-0.41][77-4-2-1.05][78-4-2-0.64][79-4-2-0.72][80-4-2-0.99]
[81-4-2-1.14][82-4-2-1.03][83-4-2-0.59][84-4-2-0.63][85-4-2-0.64][86-4-2-0.86][87-4-2-0.62][88-4-2-0.59][89-4-2-0.57][90-4-2-0.53]
[91-4-2-0.66][92-4-2-0.60][93-4-2-0.76][94-4-2-0.77][95-4-2-0.66][96-4-2-0.50][97-4-2-0.88][98-4-2-0.83][99-4-2-0.58][100-4-2-0.88]
[101-4-2-0.56][102-4-2-0.77][103-4-2-0.35][104-4-2-1.02][105-4-2-1.05][106-4-1-0.81][107-4-2-0.69][108-4-2-0.92][109-4-2-0.65][110-4-2-0.71]
[111-4-0-0.20][112-4-2-0.73][113-4-1-0.38][114-4-2-0.34][115-4-2-0.63][116-4-2-0.82][117-4-2-0.66][119-4-2-0.69][121-4-2-0.72][122-4-1-0.50]
[124-4-1-0.62][125-4-2-0.99][126-4-2-0.85][127-4-2-1.09][128-4-2-0.41][129-4-2-0.65][130-4-2-0.60][131-4-2-0.47][132-4-2-0.84][133-4-2-0.49]
[135-4-2-0.65][136-4-2-0.52][137-4-2-0.43][138-4-2-0.39][139-4-2-0.31][140-4-2-0.62][141-4-2-0.30][142-4-2-0.64][143-4-2-1.14][144-4-2-0.95]
[145-4-2-0.67][148-4-2-0.35][149-4-2-0.68][150-4-2-0.67][151-4-2-0.87][152-4-1-0.67][153-4-2-0.65][154-4-2-0.65][155-4-1-0.69][156-4-2-0.39]
[157-4-2-0.77][158-4-2-0.48][160-4-2-0.71][161-4-1-0.45][162-4-2-0.77][164-4-2-0.82][165-4-2-0.81][167-4-2-0.45][168-4-2-0.77][170-4-2-0.10]
[171-4-2-0.58][172-4-2-0.91][173-4-2-0.37][174-4-2-0.35][175-4-2-0.72][177-4-2-0.45][178-4-2-1.01][179-4-2-0.56][180-4-2-0.84][181-4-0-0.06]
[182-4-2-0.69][183-4-2-0.76][184-4-2-0.82][186-4-2-0.62][187-4-2-0.66][188-4-2-0.81][189-4-2-0.80][190-4-1-0.45][191-4-2-0.92][192-4-2-0.77]
[193-4-2-0.91][194-4-2-0.61][195-4-2-0.42][196-4-2-0.55][197-4-2-0.89][198-4-2-0.63][199-4-2-0.70]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.365 | Acc: 33.250% | Wgt Acc: 39.539%	I - Batch: 100 | Loss: 1.344 | Acc: 35.312% | Wgt Acc: 41.550%
	I - Batch: 150 | Loss: 1.323 | Acc: 36.458% | Wgt Acc: 42.889%
	I - Batch: 200 | Loss: 1.314 | Acc: 36.375% | Wgt Acc: 42.884%
I - num batch: 222
I - Train -- Loss: 1.311 | Acc: 36.369% | Wgt Acc: 42.842% | LR: 1.000000e-04 | Dur: 136.34s
I - Confusion Matrix: [row->prediction - col->label]
[[488.  55.  82. 276. 184.]
 [ 26. 157. 116.  32. 114.]
 [ 96. 335. 491.  79. 666.]
 [ 86.  31.  45. 151.  33.]
 [  1.   0.   0.   0.   3.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.472 | Acc: 25.444% | Wgt Acc: 30.425% | Dur: 14.72s
I - Confusion Matrix: [row->prediction - col->label]
[[ 35.   0.   2.  24.   6.]
 [ 12.  25.  24.  24.  35.]
 [ 32.  52.  49.  26. 128.]
 [  8.   0.   0.  12.   3.]
 [  1.   1.   0.   0.   8.]]

I - Local maximum validation set accuracy:  25.44

I - Validation set results: 
[14-1-1-1.06][50-3-2-0.72][124-2-2-0.69][127-0-0-0.73][443-2-2-1.49][567-0-0-0.11][573-1-2-0.96][615-0-2-0.04][695-1-2-0.47][722-3-0-0.61]
[826-0-0-0.73][878-0-1--0.06][1103-0-2-0.71][1212-3-2-0.36][1368-0-0-0.46][2181-2-1-0.16][2476-2-1-0.60][2721-2-2-0.97][2818-1-2-0.82][2886-2-2-1.65]
[3231-2-1-1.03][3333-2-1-0.81][3482-2-2-0.99][3536-3-1-0.30][3625-1-2-1.29][3909-0-2-0.95][4035-0-3-0.62][4140-0-2-0.38][4214-1-1-0.71][4346-1-1-0.39]
[4581-2-2-1.02][4708-3-1-1.00][4838-3-2-0.42][4845-1-2-1.01][4868-0-0-0.68][4939-0-2-1.62][4984-2-0-0.25][5078-1-2-0.90][5396-0-0-1.64][5479-1-1-0.98]
[5717-0-2-0.57][5843-1-1-1.17][5949-3-0-0.56][5987-2-2-1.03][6014-3-1-0.91][6033-3-0-0.22][6313-0-0--0.01][6421-3-2--0.01][6500-1-1-1.09][6583-3-2-0.15]
[6683-3-2-0.35][6825-2-2-0.17][6998-3-2-0.55][7049-3-1-0.52][7517-1-1-1.32][7521-1-1-0.64][7528-1-1-0.78][7949-1-2-1.13][8135-1-1-0.56][8185-3-0-1.06]
[8269-3-2-0.79][8273-3-3-0.13][8543-3-0-1.13][8666-1-1-0.84][8672-0-0-0.68][8903-1-2-0.74][9001-2-1-1.23][9036-2-2-1.42][9281-3-2-0.75][9300-2-2-1.10]
[9571-0-2-0.22][9617-1-2-0.91][9644-2-2-0.67][9705-2-2-0.61][9801-0-1-0.40][9803-3-1-0.41][9865-3-0-0.90][9896-2-2-1.18][10314-1-2-1.13][10337-3-0-0.51]
[10403-0-2-0.91][10653-2-1-1.02][10704-2-1-1.19][10719-1-2-1.23][10727-1-2-1.43][10836-0-0-1.48][10969-2-1-0.43][11042-0-2-0.53][11088-1-2-0.99][11322-0-2-0.08]
[11398-2-2-0.62][11499-0-2-0.67][11502-3-1-0.07][11512-3-2-1.04][11608-1-2-1.38][11610-0-3-0.03][11692-0-0-0.37][11905-0-0-1.09][11993-1-2-1.09][12002-2-2-0.14]
[12052-0-0-0.57][12201-0-0-0.57][12235-2-2-1.01][12320-1-4-0.54][12377-2-2-1.33][12398-2-1-0.51][12503-1-2-0.67][12617-0-1-1.09][12685-3-1-0.80][12738-2-2-0.58]
[12742-2-2-1.24][12823-0-3-0.81][13110-1-2-0.94][13240-3-2-0.34][13253-1-2-1.52][13273-0-0-0.92][13634-1-1-0.78][13763-2-1-0.80][13905-3-0-0.37][14060-2-1-1.46]
[14065-3-0-0.78][14147-3-1-0.33][14595-2-2-1.01][14687-2-1-0.71][14788-2-2-0.74][14869-1-1-0.71][14872-3-2-0.45][14877-1-2-0.74][14927-0-3-0.41][15066-0-0-1.01]
[15175-1-2-1.08][15178-2-2-0.25][15375-3-0-1.26][15389-3-0-0.57][15568-2-1-0.73][15675-3-1-0.46][15869-1-2-1.01][16207-3-0-0.26][16236-0-2-0.26][16302-3-2-0.02]
[16331-2-2-1.38][16381-0-2-0.24][16488-1-2-0.99][16495-0-2-0.29][16650-0-0-0.51][16719-1-1-0.77][16801-0-0-1.20][16828-0-0-0.13][17137-3-0-0.27][17245-1-2-0.73]
[17278-3-2-0.34][17282-0-2-0.77][17311-2-2-1.10][17336-2-2-1.12][17608-3-0-0.88][17627-0-2-0.30][17877-3-1-1.06][17924-1-2-0.34][17984-3-0-1.23][18211-0-1-0.91]
[18276-3-0-0.12][18287-1-2-0.68][18394-0-0-0.62][18428-0-2-1.12][18442-0-1-0.02][18478-3-0-0.47][18607-0-2-0.87][18616-0-1-0.72][18663-0-2-0.05][18718-0-0-0.54]
[18766-2-1-1.15][18824-2-2-1.13][18890-3-2-1.07][18930-3-2-1.21][18938-3-1-0.15][19817-1-2-1.11][19839-0-2-0.59][19930-3-1-0.14][19944-0-2-0.40][20036-2-2-1.23]
[20101-3-3-0.11][20474-1-2-0.87][20547-3-2-0.70][20929-2-2-1.24][21245-1-1-1.11][21257-3-1-0.42][21293-1-1-1.54][21316-1-1-1.00][21384-1-2-0.86][21448-1-2-1.06]
[21483-0-0-0.66][21487-2-2-1.06][21714-0-2-0.35][21943-3-2-1.21][21947-0-2-0.63][21948-0-0-0.70][21965-2-2-1.13][21998-1-2-0.61][22025-0-1-0.93][22228-3-2-0.24]
[22446-1-1-1.15][22494-3-0-0.64][22757-0-0-1.16][22811-3-3-0.61][22976-3-2-0.76][22985-3-0-0.77][23014-0-0-0.38][23112-1-2-1.43][23144-3-0-1.00][23168-2-1-0.38]
[23219-0-1-0.47][23363-3-1-0.41][23470-0-2-1.09][23486-2-1-0.51][23497-0-3-1.28][23516-0-0-1.73][23690-1-2-0.56][23921-2-2-1.11][23936-1-2-0.81][24040-3-2-0.33]
[24111-1-2-1.60][24182-0-3-1.46][24238-3-3-0.08][24290-2-2-0.57][24345-0-0-0.43][24364-1-2-0.60][24427-3-1-0.71][24477-2-2-0.86][24495-2-2-1.15][24893-2-2-1.48]
[25012-1-2-0.70][25121-2-2-0.89][25165-3-1-0.29][25183-0-1-0.43][25297-3-1-0.50][25398-0-0-0.09][25574-2-1-0.54][25644-1-1-1.25][25718-1-1-1.03][25774-2-2-0.49]
[26032-3-3-0.41][26051-3-3-0.46][26120-0-2-0.59][26321-1-2-0.66][26732-1-2-1.22][26784-3-3-1.18][26827-3-1-0.94][26833-0-3-0.91][26838-2-1-0.66][26860-1-2-0.59]
[26948-0-2-0.35][27049-3-0-0.48][27098-1-2-0.47][27526-0-0-0.16][27639-3-1-0.50][27698-3-3-0.41][27772-0-0-1.60][27890-1-2-1.02][28040-0-4-0.40][28503-2-2-1.13]
[28577-1-1-0.91][28959-0-0-1.96][29198-3-1-0.98][29777-0-0-1.99][29877-2-2-0.99][30035-1-2-1.59][30098-0-1-0.18][30326-1-1-1.31][30572-2-2-0.63][30716-0-1-0.96]
[30806-2-2-0.50][30906-1-2-1.26][31007-0-2-0.46][31181-3-2-0.09][31238-0-0-0.16][31347-0-0-0.48][31422-2-1-0.88][31429-3-2-0.30][31431-0-2-0.09][31432-1-1-1.05]
[31477-0-3-1.25][31524-1-2-0.74][31597-1-2-1.39][31619-1-2-0.60][31701-0-0-0.46][31755-0-0-0.33][31854-3-1-0.87][32074-1-2-0.81][32078-3-1-0.29][32111-1-2-1.14]
[32127-1-2-1.09][32140-3-3-0.05][32263-2-2-0.57][32365-0-2-0.69][32411-2-0-1.09][32429-3-0-0.97][32473-3-3-0.41][32574-3-3-0.51][32584-0-2-1.09][32622-0-2-1.06]
[32858-3-0-0.14][32969-3-0-0.79][33016-2-2-0.64][33031-1-1-0.92][33035-2-2-0.97][33133-2-2-0.98][33173-2-1-0.86][33175-3-2-1.16][33306-3-2-0.90][33309-2-2-0.44]
[33474-0-1-0.48][33478-2-2-0.46][33618-1-1-0.67][33712-0-2-0.27][33782-2-2-1.16][33914-3-1-0.70][34076-3-1-0.57][34112-2-1-1.11][34138-2-1-0.94][34239-1-2-0.98]
[34364-2-1-1.38][34617-1-2-1.17][34751-3-3-0.66][34783-2-2-1.17][35015-3-2-0.15][35018-1-2-1.40][35288-2-1-0.57][0-4-2-1.03][1-4-2-0.22][2-4-2-0.64]
[3-4-2-0.61][4-4-2-0.49][5-4-1-0.88][6-4-1-0.28][7-4-1-0.92][8-4-2-0.64][9-4-2-0.82][10-4-2-1.22][11-4-2-0.95][12-4-1-0.69]
[14-4-1-0.36][15-4-0-0.56][16-4-4-0.71][17-4-2-0.42][18-4-2-1.37][19-4-3-0.87][20-4-2-0.43][21-4-2-0.98][22-4-2-1.58][23-4-1-1.13]
[24-4-2-0.59][25-4-2-0.47][26-4-2-0.05][27-4-2-0.19][28-4-2-1.00][29-4-2-0.99][30-4-2-0.17][31-4-2-0.54][32-4-2-1.04][33-4-2-0.60]
[34-4-2-0.41][35-4-0-0.72][37-4-2-0.44][39-4-0-0.98][40-4-2-0.58][41-4-2-1.29][42-4-2-0.58][43-4-1-0.66][45-4-1-0.80][46-4-2-1.10]
[47-4-2-1.14][48-4-2-0.67][51-4-2-0.87][52-4-2-0.74][53-4-1-0.59][54-4-1-0.79][55-4-2--0.01][56-4-1-0.97][57-4-3-0.13][58-4-1-1.13]
[59-4-2-0.44][60-4-1-0.94][61-4-2-1.35][62-4-2-0.45][63-4-2-0.71][64-4-2-1.04][65-4-2-0.85][66-4-2-1.01][67-4-2-0.62][68-4-1-1.12]
[69-4-2-0.24][70-4-2-1.08][72-4-2-1.69][73-4-2-1.15][74-4-2-1.14][75-4-2-0.50][77-4-2-1.19][78-4-2-0.75][79-4-2-1.10][80-4-2-1.34]
[81-4-2-1.26][82-4-2-1.21][83-4-2-0.63][84-4-2-0.85][85-4-4-0.94][86-4-2-0.96][87-4-4-0.61][88-4-2-0.66][89-4-2-0.55][90-4-2-0.31]
[91-4-2-0.95][92-4-1-0.63][93-4-2-0.92][94-4-2-1.04][95-4-2-0.94][96-4-2-0.61][97-4-2-1.16][98-4-2-0.87][99-4-2-0.78][100-4-2-1.50]
[101-4-4-0.94][102-4-2-1.09][103-4-2-0.01][104-4-2-1.40][105-4-2-1.45][106-4-1-1.03][107-4-4-0.79][108-4-2-1.19][109-4-1-1.31][110-4-2-0.99]
[111-4-3-0.60][112-4-2-0.87][113-4-1-0.87][114-4-2-0.20][115-4-2-0.80][116-4-2-1.15][117-4-1-0.80][119-4-2-0.83][121-4-2-0.90][122-4-1-0.89]
[124-4-1-1.13][125-4-2-1.02][126-4-2-0.50][127-4-2-1.61][128-4-2-0.76][129-4-2-1.06][130-4-2-0.90][131-4-2-0.51][132-4-2-0.96][133-4-0-0.48]
[135-4-2-0.97][136-4-1-0.84][137-4-2-0.58][138-4-2-0.32][139-4-2-0.60][140-4-2-0.69][141-4-2-0.05][142-4-2-0.74][143-4-2-1.39][144-4-2-0.98]
[145-4-2-0.65][148-4-0-0.64][149-4-2-0.67][150-4-2-1.00][151-4-2-1.22][152-4-1-1.34][153-4-4-0.67][154-4-1-0.80][155-4-1-0.85][156-4-4-0.46]
[157-4-2-0.69][158-4-1-0.66][160-4-2-0.94][161-4-1-0.89][162-4-2-0.97][164-4-2-1.20][165-4-2-0.95][167-4-2-0.42][168-4-2-0.94][170-4-1-0.21]
[171-4-1-1.13][172-4-2-1.21][173-4-2-0.59][174-4-0-0.42][175-4-2-1.11][177-4-2-0.31][178-4-2-0.96][179-4-1-1.04][180-4-2-0.98][181-4-1-0.45]
[182-4-1-0.71][183-4-1-0.81][184-4-2-0.82][186-4-2-0.76][187-4-2-1.05][188-4-2-0.84][189-4-2-0.73][190-4-1-0.83][191-4-2-1.05][192-4-2-1.24]
[193-4-2-1.27][194-4-2-0.32][195-4-2-0.25][196-4-1-0.71][197-4-2-1.29][198-4-4-0.47][199-4-2-0.65]
---------------------------
I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.284 | Acc: 38.000% | Wgt Acc: 45.375%
	I - Batch: 100 | Loss: 1.269 | Acc: 38.938% | Wgt Acc: 45.793%
	I - Batch: 150 | Loss: 1.258 | Acc: 39.083% | Wgt Acc: 45.784%
	I - Batch: 200 | Loss: 1.241 | Acc: 40.250% | Wgt Acc: 46.736%
I - num batch: 222
I - Train -- Loss: 1.236 | Acc: 40.851% | Wgt Acc: 47.333% | LR: 1.000000e-04 | Dur: 141.01s
I - Confusion Matrix: [row->prediction - col->label]
[[478.  27.  51. 232. 171.]
 [ 27. 225. 131.  35. 134.]
 [ 80. 279. 495.  73. 581.]
 [105.  39.  53. 197.  60.]
 [  7.   8.   4.   1.  54.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.362 | Acc: 36.095% | Wgt Acc: 38.773% | Dur: 14.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   3.  33.  10.]
 [  4.  22.  13.  13.  20.]
 [ 20.  52.  53.  25. 103.]
 [  8.   1.   3.  13.   5.]
 [  3.   2.   3.   2.  42.]]

I - Local maximum validation set accuracy:  36.09

I - Validation set results: 
[14-1-2-0.72][50-3-1-0.47][124-2-2-0.45][127-0-0-1.16][443-2-2-1.57][567-0-0-0.44][573-1-1-0.59][615-0-2-0.06][695-1-2-0.56][722-3-0-1.03]
[826-0-0-0.24][878-0-0-0.51][1103-0-4-0.16][1212-3-2-0.40][1368-0-0-0.52][2181-2-3-0.03][2476-2-2-0.38][2721-2-2-1.16][2818-1-2-0.92][2886-2-2-1.28]
[3231-2-2-1.90][3333-2-1-0.54][3482-2-2-0.99][3536-3-3-0.47][3625-1-2-1.34][3909-0-0-0.03][4035-0-0-1.34][4140-0-0-0.76][4214-1-2-0.16][4346-1-2-0.23]
[4581-2-2-1.19][4708-3-2-0.72][4838-3-1-0.48][4845-1-2-1.00][4868-0-0-1.21][4939-0-2-1.13][4984-2-2-0.06][5078-1-4-0.68][5396-0-0-1.29][5479-1-1-0.64]
[5717-0-0-0.45][5843-1-1-1.36][5949-3-0-0.35][5987-2-4-0.92][6014-3-3-0.42][6033-3-2-0.47][6313-0-0-0.24][6421-3-0-0.18][6500-1-1-0.82][6583-3-2--0.06]
[6683-3-3-0.28][6825-2-0-0.37][6998-3-2--0.08][7049-3-2-0.25][7517-1-2-1.26][7521-1-2--0.02][7528-1-2-0.53][7949-1-2-1.73][8135-1-0-0.43][8185-3-0-1.19]
[8269-3-2-1.01][8273-3-0-0.53][8543-3-0-1.46][8666-1-1-0.34][8672-0-0-1.07][8903-1-2-0.96][9001-2-1-0.98][9036-2-2-1.56][9281-3-1-0.88][9300-2-2-0.95]
[9571-0-2--0.00][9617-1-2-0.44][9644-2-2-1.00][9705-2-4-0.40][9801-0-2-0.11][9803-3-3-0.23][9865-3-0-0.70][9896-2-2-1.18][10314-1-2-1.02][10337-3-0-1.10]
[10403-0-2-0.78][10653-2-2-0.73][10704-2-1-1.15][10719-1-2-1.33][10727-1-2-1.02][10836-0-0-2.38][10969-2-3-0.12][11042-0-0-0.31][11088-1-2-1.17][11322-0-0-1.17]
[11398-2-2-1.14][11499-0-0-0.31][11502-3-0-0.17][11512-3-1-0.44][11608-1-1-1.65][11610-0-3-1.11][11692-0-0-0.24][11905-0-0-1.47][11993-1-2-0.92][12002-2-3--0.02]
[12052-0-0-0.80][12201-0-0-0.63][12235-2-2-1.48][12320-1-4-0.50][12377-2-2-1.24][12398-2-2-0.09][12503-1-2-1.46][12617-0-1-1.40][12685-3-1-0.92][12738-2-2-0.47]
[12742-2-2-1.21][12823-0-3-0.97][13110-1-1-0.72][13240-3-0-0.16][13253-1-2-1.17][13273-0-0-1.31][13634-1-2-1.19][13763-2-1-0.35][13905-3-0-0.10][14060-2-1-1.35]
[14065-3-0-0.57][14147-3-2-0.10][14595-2-2-1.01][14687-2-2-0.47][14788-2-2-0.69][14869-1-2-1.12][14872-3-2-0.58][14877-1-2-0.76][14927-0-3-0.46][15066-0-0-1.89]
[15175-1-2-1.33][15178-2-2-0.33][15375-3-0-1.43][15389-3-0-0.41][15568-2-1-1.33][15675-3-3-0.66][15869-1-2-0.92][16207-3-0-0.64][16236-0-2-0.52][16302-3-2-0.27]
[16331-2-2-0.86][16381-0-2-0.27][16488-1-2-0.72][16495-0-0-0.48][16650-0-0-1.34][16719-1-1-1.02][16801-0-0-2.24][16828-0-0-0.61][17137-3-0-0.96][17245-1-2-0.60]
[17278-3-2-0.30][17282-0-0-0.11][17311-2-2-1.10][17336-2-2-0.90][17608-3-0-1.08][17627-0-2-0.24][17877-3-1-0.60][17924-1-2-0.55][17984-3-0-1.69][18211-0-3-0.33]
[18276-3-0-0.59][18287-1-2-0.30][18394-0-0-0.81][18428-0-4-0.12][18442-0-0-0.76][18478-3-0-0.63][18607-0-2-0.23][18616-0-2-0.67][18663-0-0-0.65][18718-0-0-0.85]
[18766-2-2-1.16][18824-2-2-0.85][18890-3-2-0.90][18930-3-2-1.04][18938-3-2-0.04][19817-1-2-1.16][19839-0-4-0.63][19930-3-0-0.41][19944-0-2-1.12][20036-2-2-1.29]
[20101-3-4-0.13][20474-1-2-1.23][20547-3-2-0.47][20929-2-2-1.19][21245-1-1-1.13][21257-3-4-0.31][21293-1-1-1.40][21316-1-2-0.42][21384-1-2-1.11][21448-1-2-1.01]
[21483-0-0-0.88][21487-2-2-1.27][21714-0-2-0.04][21943-3-2-1.40][21947-0-0-0.43][21948-0-0-2.17][21965-2-2-1.57][21998-1-3-0.05][22025-0-1-0.59][22228-3-0-1.01]
[22446-1-1-1.68][22494-3-0-0.74][22757-0-0-1.66][22811-3-3-0.45][22976-3-1-0.98][22985-3-0-0.73][23014-0-0-0.27][23112-1-1-1.28][23144-3-0-1.25][23168-2-1-0.23]
[23219-0-0-0.01][23363-3-3-0.68][23470-0-2-0.52][23486-2-1-0.49][23497-0-0-1.84][23516-0-0-2.32][23690-1-2-1.20][23921-2-2-0.96][23936-1-2-0.50][24040-3-2-0.46]
[24111-1-2-1.62][24182-0-3-1.68][24238-3-3-0.10][24290-2-0-0.61][24345-0-2-0.87][24364-1-2-0.72][24427-3-3-0.20][24477-2-2-0.82][24495-2-1-0.92][24893-2-2-1.19]
[25012-1-1-0.55][25121-2-4-1.39][25165-3-1-0.04][25183-0-0-0.25][25297-3-1-0.26][25398-0-0-0.68][25574-2-1-1.06][25644-1-1-1.14][25718-1-1-1.21][25774-2-2-0.64]
[26032-3-3-0.96][26051-3-0-0.52][26120-0-2-0.70][26321-1-1-0.54][26732-1-2-1.01][26784-3-0-0.86][26827-3-2-0.60][26833-0-3-1.23][26838-2-2-0.73][26860-1-2-0.99]
[26948-0-0-0.98][27049-3-0-0.98][27098-1-2-0.35][27526-0-0-1.03][27639-3-1-0.42][27698-3-3-0.72][27772-0-0-2.13][27890-1-1-0.58][28040-0-2-0.58][28503-2-2-1.26]
[28577-1-1-1.30][28959-0-0-2.27][29198-3-1-0.59][29777-0-0-1.54][29877-2-2-0.58][30035-1-2-1.55][30098-0-3-0.17][30326-1-1-1.10][30572-2-2-0.45][30716-0-1-0.71]
[30806-2-2-0.34][30906-1-2-1.14][31007-0-0-0.07][31181-3-2-0.23][31238-0-0-0.66][31347-0-0-0.27][31422-2-2-0.67][31429-3-0-0.34][31431-0-3-0.55][31432-1-1-0.87]
[31477-0-0-0.99][31524-1-2-0.45][31597-1-2-1.21][31619-1-2-0.19][31701-0-2-0.23][31755-0-0-0.18][31854-3-1-0.32][32074-1-2-0.16][32078-3-2-0.17][32111-1-2-1.32]
[32127-1-2-1.67][32140-3-2-0.29][32263-2-2-0.44][32365-0-2-0.07][32411-2-0-1.64][32429-3-0-1.20][32473-3-3-0.27][32574-3-0-2.02][32584-0-2-0.74][32622-0-1-0.66]
[32858-3-0-0.56][32969-3-0-0.69][33016-2-2-1.60][33031-1-1--0.01][33035-2-2-1.21][33133-2-2-1.03][33173-2-1-0.41][33175-3-2-1.23][33306-3-2-1.01][33309-2-2-0.36]
[33474-0-2-0.05][33478-2-2-0.32][33618-1-1-0.59][33712-0-0-0.20][33782-2-2-1.11][33914-3-1-0.37][34076-3-2-0.53][34112-2-1-1.10][34138-2-2-0.93][34239-1-2-0.90]
[34364-2-2-1.43][34617-1-2-0.98][34751-3-3-0.71][34783-2-1-1.01][35015-3-2-0.45][35018-1-2-1.47][35288-2-2-0.57][0-4-4-1.16][1-4-4-0.91][2-4-4-0.68]
[3-4-4-1.13][4-4-2-0.39][5-4-1-0.58][6-4-0-0.94][7-4-2-1.01][8-4-2-0.63][9-4-2-0.85][10-4-4-1.46][11-4-2-1.08][12-4-1-1.51]
[14-4-0-0.29][15-4-0-0.82][16-4-4-1.16][17-4-2-0.25][18-4-2-1.26][19-4-0-0.64][20-4-2-0.13][21-4-2-1.30][22-4-2-1.19][23-4-1-0.93]
[24-4-2-1.36][25-4-0-0.25][26-4-2-0.39][27-4-0-0.52][28-4-4-1.39][29-4-2-0.36][30-4-2-0.92][31-4-2-0.86][32-4-4-1.04][33-4-2-0.41]
[34-4-2-0.27][35-4-3-0.12][37-4-2-0.37][39-4-3-0.45][40-4-2-0.80][41-4-2-0.63][42-4-3-0.71][43-4-2-0.56][45-4-2-1.07][46-4-2-1.47]
[47-4-4-1.42][48-4-2-1.10][51-4-2-1.16][52-4-2-0.93][53-4-1-1.09][54-4-1-0.68][55-4-2-0.68][56-4-1-1.11][57-4-3-0.18][58-4-2-1.29]
[59-4-1-0.17][60-4-2-0.72][61-4-2-1.06][62-4-4-0.20][63-4-2-0.97][64-4-2-1.11][65-4-2-1.04][66-4-2-1.36][67-4-2-0.69][68-4-1-1.37]
[69-4-4-0.13][70-4-2-0.94][72-4-2-1.02][73-4-1-1.39][74-4-2-0.93][75-4-4-0.37][77-4-4-1.88][78-4-2-0.57][79-4-2-1.01][80-4-1-1.13]
[81-4-2-0.98][82-4-2-1.27][83-4-2-0.65][84-4-2-1.45][85-4-4-1.10][86-4-2-0.70][87-4-4-0.92][88-4-4-0.66][89-4-2-0.80][90-4-4-1.33]
[91-4-2-0.84][92-4-4-0.39][93-4-4-0.21][94-4-2-1.11][95-4-2-0.97][96-4-2-0.95][97-4-2-1.13][98-4-2-1.10][99-4-4-0.62][100-4-2-1.03]
[101-4-4-1.50][102-4-2-1.24][103-4-2-0.20][104-4-2-1.07][105-4-2-1.68][106-4-2-1.19][107-4-4-0.89][108-4-2-0.43][109-4-1-1.20][110-4-1-0.79]
[111-4-3-0.95][112-4-2-0.88][113-4-1-1.40][114-4-2-0.08][115-4-2-0.59][116-4-4-0.02][117-4-1-1.35][119-4-2-1.12][121-4-4-0.95][122-4-4-0.44]
[124-4-2-0.77][125-4-4-1.18][126-4-2-1.21][127-4-2-1.58][128-4-2-0.56][129-4-2-0.41][130-4-2-1.43][131-4-2-0.70][132-4-4-0.30][133-4-4-0.55]
[135-4-2-0.58][136-4-1-0.46][137-4-2-0.57][138-4-2-0.36][139-4-2-0.81][140-4-2-0.56][141-4-2-0.26][142-4-4-1.22][143-4-4-1.39][144-4-4-1.38]
[145-4-2-1.22][148-4-0-0.80][149-4-2-0.62][150-4-1-1.14][151-4-2-1.41][152-4-2-1.43][153-4-2-1.28][154-4-2-1.28][155-4-4-0.99][156-4-4-0.46]
[157-4-2-0.69][158-4-2-0.38][160-4-4-0.58][161-4-2-0.37][162-4-2-0.61][164-4-2-0.70][165-4-2-1.09][167-4-0-0.22][168-4-2-0.73][170-4-2-0.36]
[171-4-1-0.87][172-4-2-1.44][173-4-2-0.41][174-4-0-0.38][175-4-2-0.79][177-4-4-0.52][178-4-4-1.55][179-4-2-0.48][180-4-4-1.45][181-4-1-0.67]
[182-4-2-1.05][183-4-4-0.90][184-4-2-0.96][186-4-4-0.08][187-4-1-1.25][188-4-2-0.87][189-4-4-0.91][190-4-1-0.78][191-4-4-0.72][192-4-2-0.78]
[193-4-2-1.29][194-4-2-0.41][195-4-0-0.20][196-4-2-0.56][197-4-2-1.51][198-4-4-1.52][199-4-2-0.68]
---------------------------
I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.214 | Acc: 43.125% | Wgt Acc: 47.961%
	I - Batch: 100 | Loss: 1.199 | Acc: 44.625% | Wgt Acc: 49.357%
	I - Batch: 150 | Loss: 1.194 | Acc: 44.292% | Wgt Acc: 49.167%
	I - Batch: 200 | Loss: 1.185 | Acc: 45.750% | Wgt Acc: 50.611%
I - num batch: 222
I - Train -- Loss: 1.188 | Acc: 45.842% | Wgt Acc: 50.590% | LR: 1.000000e-04 | Dur: 133.49s
I - Confusion Matrix: [row->prediction - col->label]
[[512.  30.  59. 216. 180.]
 [ 19. 238. 136.  36. 148.]
 [ 67. 254. 473.  73. 430.]
 [ 90.  35.  46. 209.  48.]
 [  9.  21.  20.   4. 194.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.269 | Acc: 47.337% | Wgt Acc: 45.042% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  1.  5. 38. 15.]
 [ 1. 29. 13. 11. 17.]
 [ 3. 36. 39. 11. 46.]
 [ 9.  1.  3. 18.  4.]
 [19. 11. 15.  8. 98.]]

I - Local maximum validation set accuracy:  47.34

I - Validation set results: 
[14-1-2-0.74][50-3-2-0.16][124-2-2-0.47][127-0-0-0.89][443-2-2-1.55][567-0-0-0.32][573-1-1-0.65][615-0-0-0.30][695-1-2-0.49][722-3-0-1.38]
[826-0-0-1.61][878-0-0-1.09][1103-0-4-0.39][1212-3-2-0.01][1368-0-0-0.42][2181-2-3-0.41][2476-2-4-0.37][2721-2-2-0.82][2818-1-2-0.67][2886-2-1-1.24]
[3231-2-2-1.41][3333-2-1-0.96][3482-2-2-0.73][3536-3-3-0.18][3625-1-1-1.15][3909-0-0-0.13][4035-0-3-1.69][4140-0-0-0.62][4214-1-1-0.28][4346-1-3-0.24]
[4581-2-2-0.73][4708-3-1-0.93][4838-3-2-0.21][4845-1-2-0.88][4868-0-0-1.48][4939-0-4-0.45][4984-2-0-0.06][5078-1-4-0.86][5396-0-0-2.64][5479-1-1-0.79]
[5717-0-0-0.98][5843-1-1-1.55][5949-3-0-1.03][5987-2-4-1.07][6014-3-1-0.60][6033-3-0-0.30][6313-0-0--0.00][6421-3-3-0.24][6500-1-1-0.79][6583-3-0-0.18]
[6683-3-3-0.41][6825-2-0-0.24][6998-3-2-0.23][7049-3-0--0.09][7517-1-1-1.58][7521-1-1-0.17][7528-1-2-0.11][7949-1-2-1.33][8135-1-0-0.94][8185-3-0-1.48]
[8269-3-2-1.16][8273-3-0-0.98][8543-3-0-1.98][8666-1-1-0.39][8672-0-0-1.75][8903-1-2-0.70][9001-2-1-0.86][9036-2-2-1.37][9281-3-4-0.42][9300-2-2-0.86]
[9571-0-4--0.03][9617-1-1-0.49][9644-2-2-0.94][9705-2-4-0.19][9801-0-0-0.63][9803-3-0-0.39][9865-3-0-1.01][9896-2-4-1.14][10314-1-4-1.22][10337-3-0-1.31]
[10403-0-4-0.87][10653-2-2-0.72][10704-2-1-1.31][10719-1-1-1.06][10727-1-4-1.01][10836-0-0-3.00][10969-2-3-0.39][11042-0-0-0.43][11088-1-2-1.35][11322-0-0-1.61]
[11398-2-2-1.88][11499-0-0-0.56][11502-3-0--0.04][11512-3-1-0.43][11608-1-1-1.54][11610-0-3-1.63][11692-0-0-0.81][11905-0-0-1.72][11993-1-2-0.81][12002-2-0-0.70]
[12052-0-0-0.88][12201-0-0-1.62][12235-2-2-1.15][12320-1-4-1.18][12377-2-4-1.10][12398-2-2-0.17][12503-1-2-1.26][12617-0-1-1.32][12685-3-1-0.90][12738-2-3-0.04]
[12742-2-2-1.22][12823-0-0-1.84][13110-1-2-0.90][13240-3-0-0.24][13253-1-2-1.04][13273-0-0-2.22][13634-1-2-0.73][13763-2-1-0.54][13905-3-0-0.68][14060-2-1-1.42]
[14065-3-3-1.19][14147-3-0--0.16][14595-2-4-0.95][14687-2-2-0.78][14788-2-2-0.68][14869-1-2-0.92][14872-3-4-0.49][14877-1-2-0.76][14927-0-3-0.59][15066-0-0-2.14]
[15175-1-2-0.87][15178-2-2-0.09][15375-3-0-0.28][15389-3-0-0.92][15568-2-4-0.74][15675-3-3-0.22][15869-1-2-0.68][16207-3-0-0.41][16236-0-2-0.45][16302-3-0-0.39]
[16331-2-2-0.98][16381-0-4-0.17][16488-1-4-0.78][16495-0-0-0.48][16650-0-0-2.02][16719-1-2-0.40][16801-0-0-2.50][16828-0-0-0.81][17137-3-0-0.97][17245-1-2-0.18]
[17278-3-2--0.04][17282-0-0--0.01][17311-2-2-0.99][17336-2-2-1.18][17608-3-0-1.54][17627-0-0-0.05][17877-3-1-0.76][17924-1-4-0.39][17984-3-0-1.87][18211-0-3-0.29]
[18276-3-0-1.00][18287-1-1-0.66][18394-0-0-1.37][18428-0-0-2.19][18442-0-0-1.08][18478-3-0-1.04][18607-0-4-0.31][18616-0-4-0.73][18663-0-3-0.75][18718-0-0-1.51]
[18766-2-1-0.93][18824-2-4-1.05][18890-3-2-0.87][18930-3-4-1.00][18938-3-3-0.11][19817-1-2-1.03][19839-0-4-0.64][19930-3-1--0.11][19944-0-4-1.01][20036-2-2-1.30]
[20101-3-0-0.55][20474-1-2-1.13][20547-3-4-0.60][20929-2-2-1.31][21245-1-1-1.04][21257-3-4-0.38][21293-1-1-1.76][21316-1-1-1.02][21384-1-4-1.28][21448-1-1-0.90]
[21483-0-0-1.39][21487-2-2-1.15][21714-0-4-0.01][21943-3-2-1.14][21947-0-4-0.22][21948-0-0-2.43][21965-2-2-1.34][21998-1-2-0.18][22025-0-3-0.18][22228-3-0-1.08]
[22446-1-1-1.58][22494-3-0-1.29][22757-0-0-2.22][22811-3-3-1.76][22976-3-4-0.86][22985-3-0-1.42][23014-0-0-1.80][23112-1-2-1.21][23144-3-0-1.69][23168-2-1-0.43]
[23219-0-0-0.05][23363-3-3-0.68][23470-0-4-0.58][23486-2-2-0.41][23497-0-0-2.46][23516-0-0-2.74][23690-1-4-0.57][23921-2-2-0.88][23936-1-2-0.37][24040-3-4-0.07]
[24111-1-4-1.49][24182-0-0-3.24][24238-3-3-0.80][24290-2-0-0.57][24345-0-0-1.42][24364-1-2-0.67][24427-3-3-0.23][24477-2-2-0.59][24495-2-1-0.86][24893-2-2-1.03]
[25012-1-2-0.45][25121-2-4-1.42][25165-3-1-0.02][25183-0-4-0.14][25297-3-3-0.06][25398-0-0-0.47][25574-2-2-0.81][25644-1-1-1.42][25718-1-1-0.45][25774-2-2-0.51]
[26032-3-0-1.69][26051-3-0-1.12][26120-0-4-0.89][26321-1-1-0.62][26732-1-1-1.02][26784-3-0-1.77][26827-3-1-0.37][26833-0-3-1.83][26838-2-4-0.96][26860-1-4-0.81]
[26948-0-4-0.12][27049-3-0-1.21][27098-1-2-0.13][27526-0-0-0.90][27639-3-1-0.20][27698-3-3-1.37][27772-0-0-2.80][27890-1-1-0.64][28040-0-4-0.94][28503-2-2-1.16]
[28577-1-1-0.98][28959-0-0-2.70][29198-3-4-0.77][29777-0-0-2.83][29877-2-2-0.62][30035-1-2-1.08][30098-0-0-0.01][30326-1-1-1.44][30572-2-2-1.00][30716-0-4-0.96]
[30806-2-4-0.23][30906-1-1-0.98][31007-0-0-0.66][31181-3-2-0.02][31238-0-3-0.76][31347-0-0-1.64][31422-2-4-0.67][31429-3-0-0.12][31431-0-3-0.88][31432-1-1-0.83]
[31477-0-0-1.70][31524-1-2-0.71][31597-1-2-1.47][31619-1-2-0.31][31701-0-0-0.96][31755-0-0-0.81][31854-3-3-0.19][32074-1-2-0.73][32078-3-3-0.42][32111-1-4-0.83]
[32127-1-2-1.35][32140-3-3-0.03][32263-2-4-0.59][32365-0-4-0.19][32411-2-0-1.89][32429-3-0-1.69][32473-3-3-1.06][32574-3-0-2.78][32584-0-2-0.32][32622-0-2-0.45]
[32858-3-0-1.52][32969-3-0-1.20][33016-2-2-1.57][33031-1-1-0.39][33035-2-2-1.10][33133-2-2-0.93][33173-2-1-0.31][33175-3-2-1.14][33306-3-1-1.14][33309-2-4-0.26]
[33474-0-4-0.09][33478-2-2-0.21][33618-1-1-0.61][33712-0-0-0.29][33782-2-2-1.07][33914-3-1-0.15][34076-3-2-0.20][34112-2-1-1.05][34138-2-1-0.89][34239-1-2-0.71]
[34364-2-2-1.34][34617-1-2-0.87][34751-3-3-1.31][34783-2-4-1.04][35015-3-3-0.24][35018-1-2-1.26][35288-2-1-0.21][0-4-4-1.31][1-4-4-0.93][2-4-4-0.68]
[3-4-4-1.26][4-4-2-0.07][5-4-1-0.61][6-4-0-1.67][7-4-4-0.85][8-4-2-0.46][9-4-4-1.01][10-4-4-1.29][11-4-2-1.01][12-4-1-0.72]
[14-4-0-0.06][15-4-0-1.74][16-4-4-1.31][17-4-2-0.06][18-4-4-1.66][19-4-3-1.33][20-4-4-0.34][21-4-2-1.28][22-4-4-1.22][23-4-1-0.80]
[24-4-4-1.62][25-4-2--0.01][26-4-4-0.08][27-4-0-0.97][28-4-4-1.36][29-4-4-0.40][30-4-2-0.75][31-4-4-0.79][32-4-4-1.39][33-4-2-0.29]
[34-4-2-0.09][35-4-3-0.48][37-4-2-0.35][39-4-0-0.88][40-4-4-0.36][41-4-2-0.70][42-4-3-1.05][43-4-2-0.22][45-4-2-0.89][46-4-4-1.19]
[47-4-4-1.47][48-4-4-1.18][51-4-4-1.04][52-4-4-0.66][53-4-1-0.74][54-4-4-0.48][55-4-2-0.63][56-4-4-1.09][57-4-3-0.43][58-4-2-1.32]
[59-4-4-0.36][60-4-2-0.52][61-4-4-1.44][62-4-0-0.09][63-4-2-1.12][64-4-4-0.82][65-4-4-1.28][66-4-4-1.25][67-4-4-0.61][68-4-1-1.33]
[69-4-4-0.35][70-4-4-1.03][72-4-1-0.97][73-4-4-1.33][74-4-2-0.53][75-4-4-0.45][77-4-4-1.96][78-4-2-0.62][79-4-4-1.37][80-4-4-1.16]
[81-4-4-1.12][82-4-2-0.72][83-4-1-0.77][84-4-4-0.54][85-4-4-1.22][86-4-4-0.98][87-4-4-1.05][88-4-4-0.75][89-4-2-0.73][90-4-4-0.58]
[91-4-4-0.72][92-4-2-0.18][93-4-4-0.93][94-4-4-1.31][95-4-2-0.65][96-4-4-0.94][97-4-4-1.05][98-4-2-1.17][99-4-4-0.36][100-4-1-1.03]
[101-4-4-1.62][102-4-2-1.33][103-4-0-0.12][104-4-4-1.06][105-4-1-1.05][106-4-4-1.27][107-4-4-1.05][108-4-2-0.31][109-4-4-0.30][110-4-2-0.53]
[111-4-0-1.94][112-4-4-0.78][113-4-1-0.99][114-4-0-0.00][115-4-4-0.65][116-4-4-0.96][117-4-4-0.79][119-4-2-1.11][121-4-4-0.74][122-4-4-0.47]
[124-4-1-0.74][125-4-4-0.99][126-4-4-1.15][127-4-2-1.01][128-4-4-0.44][129-4-2-0.40][130-4-4-1.24][131-4-2-0.50][132-4-4-1.19][133-4-0-0.59]
[135-4-2-0.52][136-4-4-0.59][137-4-4-0.06][138-4-4-0.04][139-4-2-0.30][140-4-4-0.69][141-4-2-0.77][142-4-4-1.28][143-4-4-1.64][144-4-4-1.95]
[145-4-1-0.96][148-4-0-1.15][149-4-2-0.35][150-4-4-0.88][151-4-4-1.24][152-4-2-0.98][153-4-4-1.11][154-4-4-0.89][155-4-4-1.24][156-4-4-0.54]
[157-4-2-0.35][158-4-2-0.02][160-4-4-0.65][161-4-1-0.09][162-4-4-0.66][164-4-2-0.57][165-4-4-0.68][167-4-0-0.39][168-4-4-0.59][170-4-2-0.10]
[171-4-1-1.18][172-4-4-1.68][173-4-4-0.47][174-4-0-1.30][175-4-4-0.79][177-4-0-0.54][178-4-4-1.63][179-4-4-0.53][180-4-4-1.47][181-4-2-0.73]
[182-4-2-0.64][183-4-4-1.09][184-4-4-1.27][186-4-4-0.68][187-4-1-1.00][188-4-2-0.91][189-4-4-1.05][190-4-1-0.71][191-4-4-1.16][192-4-4-0.60]
[193-4-1-1.22][194-4-2-0.07][195-4-0-0.13][196-4-2-0.51][197-4-2-1.13][198-4-4-1.45][199-4-4-0.74]
---------------------------
I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 1.165 | Acc: 49.000% | Wgt Acc: 54.007%
	I - Batch: 100 | Loss: 1.166 | Acc: 48.688% | Wgt Acc: 53.066%
	I - Batch: 150 | Loss: 1.157 | Acc: 49.042% | Wgt Acc: 53.363%
	I - Batch: 200 | Loss: 1.150 | Acc: 49.750% | Wgt Acc: 54.069%
I - num batch: 222
I - Train -- Loss: 1.145 | Acc: 50.183% | Wgt Acc: 54.355% | LR: 1.000000e-04 | Dur: 136.41s
I - Confusion Matrix: [row->prediction - col->label]
[[506.  26.  38. 185. 218.]
 [ 16. 291. 131.  34. 130.]
 [ 49. 206. 463.  64. 339.]
 [102.  26.  52. 247.  40.]
 [ 24.  29.  50.   8. 273.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.293 | Acc: 45.365% | Wgt Acc: 43.621% | Dur: 21.56s
I - Confusion Matrix: [row->prediction - col->label]
[[37.  0.  2. 21.  7.]
 [ 6. 35. 18. 10. 26.]
 [ 6. 31. 43. 19. 50.]
 [15.  1.  1. 23.  5.]
 [24. 11. 11. 13. 92.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 1.117 | Acc: 54.125% | Wgt Acc: 58.358%
	I - Batch: 100 | Loss: 1.108 | Acc: 54.375% | Wgt Acc: 57.972%
	I - Batch: 150 | Loss: 1.095 | Acc: 54.042% | Wgt Acc: 57.809%
	I - Batch: 200 | Loss: 1.103 | Acc: 53.312% | Wgt Acc: 56.931%
I - num batch: 222
I - Train -- Loss: 1.100 | Acc: 53.792% | Wgt Acc: 57.262% | LR: 1.000000e-04 | Dur: 134.79s
I - Confusion Matrix: [row->prediction - col->label]
[[513.  18.  30. 174. 151.]
 [ 17. 320. 122.  27. 128.]
 [ 33. 172. 473.  68. 325.]
 [105.  34.  56. 256.  50.]
 [ 29.  34.  53.  13. 346.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.248 | Acc: 51.677% | Wgt Acc: 50.465% | Dur: 14.73s
I - Confusion Matrix: [row->prediction - col->label]
[[50.  2.  4. 20. 12.]
 [ 4. 35. 10.  7. 20.]
 [ 3. 30. 50. 19. 49.]
 [13.  0.  0. 28.  0.]
 [18. 11. 11. 12. 99.]]

I - Local maximum validation set accuracy:  51.68

I - Validation set results: 
[14-1-2-1.12][50-3-4-0.22][124-2-2-1.11][127-0-0-2.82][443-2-2-1.72][567-0-0-0.73][573-1-1-1.62][615-0-0-0.35][695-1-2-0.71][722-3-3-1.45]
[826-0-0-0.54][878-0-0-1.06][1103-0-4-0.72][1212-3-4-0.53][1368-0-0-1.74][2181-2-2-0.08][2476-2-4-0.51][2721-2-2-1.25][2818-1-2-0.59][2886-2-1-1.80]
[3231-2-2-2.01][3333-2-2-1.05][3482-2-2-1.02][3536-3-3-0.51][3625-1-1-1.88][3909-0-1-0.46][4035-0-3-1.22][4140-0-0-0.56][4214-1-1-0.53][4346-1-0-0.17]
[4581-2-2-1.28][4708-3-2-1.37][4838-3-4-0.26][4845-1-2-1.28][4868-0-0-1.40][4939-0-1-0.65][4984-2-2-0.72][5078-1-4-0.88][5396-0-0-2.10][5479-1-1-1.28]
[5717-0-0-0.30][5843-1-1-2.06][5949-3-0-0.91][5987-2-4-1.37][6014-3-1-1.29][6033-3-0-0.91][6313-0-3-0.27][6421-3-3-0.29][6500-1-1-1.08][6583-3-2-0.23]
[6683-3-3-0.27][6825-2-2-0.26][6998-3-2-0.34][7049-3-2-0.64][7517-1-1-1.91][7521-1-1-0.79][7528-1-2-0.44][7949-1-4-1.27][8135-1-0-0.68][8185-3-0-1.18]
[8269-3-2-1.32][8273-3-3-0.40][8543-3-0-2.61][8666-1-1-1.55][8672-0-0-2.10][8903-1-2-0.38][9001-2-1-1.05][9036-2-2-1.97][9281-3-1-0.40][9300-2-2-1.23]
[9571-0-4-0.18][9617-1-1-1.32][9644-2-2-1.24][9705-2-4-0.68][9801-0-3-0.07][9803-3-3-0.25][9865-3-0-0.71][9896-2-2-1.24][10314-1-4-1.17][10337-3-3-0.74]
[10403-0-4-1.07][10653-2-1-0.60][10704-2-1-1.49][10719-1-1-1.00][10727-1-1-1.29][10836-0-0-2.81][10969-2-2-0.39][11042-0-0-0.77][11088-1-2-1.70][11322-0-0-1.28]
[11398-2-4-1.25][11499-0-4--0.04][11502-3-3-0.36][11512-3-2-0.47][11608-1-1-2.13][11610-0-3-0.57][11692-0-0-0.60][11905-0-0-2.64][11993-1-2-0.90][12002-2-0--0.04]
[12052-0-0-0.62][12201-0-0-0.70][12235-2-2-1.04][12320-1-4-0.76][12377-2-4-1.16][12398-2-2-0.67][12503-1-2-1.36][12617-0-1-1.86][12685-3-1-1.00][12738-2-2-0.66]
[12742-2-2-1.37][12823-0-3-0.87][13110-1-2-1.37][13240-3-3-0.19][13253-1-1-1.49][13273-0-0-2.04][13634-1-2-0.89][13763-2-2-0.88][13905-3-3--0.09][14060-2-1-2.12]
[14065-3-0-0.87][14147-3-4-0.08][14595-2-2-1.21][14687-2-2-1.34][14788-2-2-0.98][14869-1-4-0.87][14872-3-4-0.86][14877-1-1-1.15][14927-0-3-0.86][15066-0-0-2.67]
[15175-1-4-0.74][15178-2-2-0.24][15375-3-0-0.98][15389-3-3-0.89][15568-2-1-0.76][15675-3-3-0.56][15869-1-2-0.81][16207-3-0-0.32][16236-0-2-0.74][16302-3-2-0.21]
[16331-2-2-1.35][16381-0-4-0.17][16488-1-1-1.74][16495-0-0-1.03][16650-0-0-1.86][16719-1-1-0.73][16801-0-0-2.77][16828-0-0-0.21][17137-3-0-0.87][17245-1-2-0.91]
[17278-3-4-0.50][17282-0-2-0.15][17311-2-2-1.16][17336-2-2-1.59][17608-3-3-1.29][17627-0-4-0.12][17877-3-1-0.61][17924-1-2-0.70][17984-3-0-1.48][18211-0-3-0.08]
[18276-3-0-0.46][18287-1-1-0.74][18394-0-0-1.07][18428-0-1-0.65][18442-0-3-0.51][18478-3-0-0.83][18607-0-4-0.66][18616-0-4-0.42][18663-0-3-0.52][18718-0-0-1.49]
[18766-2-2-1.72][18824-2-4-1.17][18890-3-2-1.21][18930-3-4-1.08][18938-3-2-0.22][19817-1-2-1.11][19839-0-4-1.17][19930-3-3-0.10][19944-0-4-1.10][20036-2-2-1.68]
[20101-3-1-0.19][20474-1-2-1.54][20547-3-4-0.21][20929-2-2-1.35][21245-1-1-1.51][21257-3-4-0.65][21293-1-1-2.02][21316-1-1-1.68][21384-1-4-1.46][21448-1-2-1.27]
[21483-0-0-1.22][21487-2-2-1.45][21714-0-4-0.10][21943-3-2-1.40][21947-0-0-0.95][21948-0-0-2.97][21965-2-1-1.48][21998-1-1-0.86][22025-0-2-0.21][22228-3-3-0.39]
[22446-1-1-1.91][22494-3-0-0.90][22757-0-0-1.19][22811-3-2-0.31][22976-3-2-0.91][22985-3-3-0.67][23014-0-0-0.83][23112-1-2-1.71][23144-3-0-0.46][23168-2-0-0.30]
[23219-0-0-0.33][23363-3-3-0.92][23470-0-4-0.57][23486-2-2-0.73][23497-0-3-1.72][23516-0-0-2.96][23690-1-4-1.01][23921-2-2-1.30][23936-1-2-1.21][24040-3-4-0.76]
[24111-1-4-1.17][24182-0-0-1.85][24238-3-3-0.45][24290-2-0-0.46][24345-0-0-1.08][24364-1-2-0.79][24427-3-0-0.14][24477-2-4-1.13][24495-2-2-1.12][24893-2-2-1.29]
[25012-1-1-0.81][25121-2-4-1.69][25165-3-1-0.34][25183-0-0-0.99][25297-3-3-0.18][25398-0-0-0.54][25574-2-2-1.07][25644-1-1-1.97][25718-1-1-1.19][25774-2-2-1.02]
[26032-3-3-0.61][26051-3-3-0.65][26120-0-4-0.90][26321-1-2-0.97][26732-1-1-1.39][26784-3-3-1.45][26827-3-2-0.42][26833-0-3-1.55][26838-2-2-0.86][26860-1-4-1.33]
[26948-0-0-0.41][27049-3-0-1.22][27098-1-2-0.41][27526-0-0-1.29][27639-3-0-0.14][27698-3-3-0.67][27772-0-0-2.76][27890-1-1-1.22][28040-0-4-0.64][28503-2-2-1.57]
[28577-1-1-1.16][28959-0-0-2.92][29198-3-1-1.10][29777-0-0-2.40][29877-2-2-1.03][30035-1-2-1.83][30098-0-0-0.51][30326-1-1-2.58][30572-2-2-0.90][30716-0-4-1.04]
[30806-2-2-0.57][30906-1-1-1.03][31007-0-0-0.33][31181-3-3-0.05][31238-0-3-0.21][31347-0-0-0.80][31422-2-2-0.83][31429-3-2-0.38][31431-0-3-0.51][31432-1-1-1.28]
[31477-0-0-1.62][31524-1-2-1.31][31597-1-2-1.82][31619-1-2-0.83][31701-0-0-0.90][31755-0-0-0.30][31854-3-3-0.23][32074-1-2-0.95][32078-3-4-0.38][32111-1-1-1.34]
[32127-1-2-1.94][32140-3-3-0.31][32263-2-4-0.22][32365-0-0-0.00][32411-2-0-2.08][32429-3-0-1.50][32473-3-3-0.42][32574-3-0-1.37][32584-0-4-0.73][32622-0-4-0.45]
[32858-3-4-0.53][32969-3-0-1.09][33016-2-2-1.55][33031-1-1-0.42][33035-2-2-1.58][33133-2-2-1.16][33173-2-1-1.25][33175-3-2-1.60][33306-3-2-1.71][33309-2-4-0.27]
[33474-0-4--0.04][33478-2-2-0.25][33618-1-4-0.48][33712-0-0-0.72][33782-2-4-1.50][33914-3-2-0.70][34076-3-2-0.78][34112-2-1-1.16][34138-2-2-1.50][34239-1-2-1.21]
[34364-2-2-1.73][34617-1-2-1.04][34751-3-3-0.94][34783-2-1-1.03][35015-3-2-0.27][35018-1-1-1.24][35288-2-2-0.69][0-4-4-1.58][1-4-4-1.42][2-4-4-1.18]
[3-4-4-1.23][4-4-2-0.20][5-4-1-1.74][6-4-0-1.85][7-4-4-1.34][8-4-2-0.89][9-4-2-0.90][10-4-4-1.38][11-4-2-1.26][12-4-1-0.89]
[14-4-4-0.44][15-4-0-0.65][16-4-4-1.12][17-4-4-0.33][18-4-4-1.82][19-4-0-1.00][20-4-0-0.07][21-4-2-1.55][22-4-4-1.12][23-4-1-1.29]
[24-4-4-1.78][25-4-2-0.19][26-4-2-0.17][27-4-4-0.41][28-4-4-1.78][29-4-1-0.83][30-4-4-0.39][31-4-2-0.83][32-4-4-1.50][33-4-2-0.77]
[34-4-4-0.51][35-4-0-0.45][37-4-2-0.82][39-4-0-1.58][40-4-4-0.74][41-4-2-0.60][42-4-2-0.59][43-4-2-0.76][45-4-2-1.29][46-4-4-1.52]
[47-4-4-1.75][48-4-4-1.27][51-4-4-1.48][52-4-4-0.96][53-4-1-0.15][54-4-1-0.40][55-4-2-1.33][56-4-1-0.94][57-4-4-0.02][58-4-2-1.72]
[59-4-4-0.58][60-4-1-0.67][61-4-4-1.52][62-4-2-0.63][63-4-2-1.28][64-4-4-0.95][65-4-4-1.37][66-4-1-1.87][67-4-4-0.86][68-4-2-1.38]
[69-4-2-0.42][70-4-4-1.04][72-4-2-1.26][73-4-1-1.37][74-4-2-0.92][75-4-4-0.27][77-4-4-2.41][78-4-2-0.49][79-4-4-1.46][80-4-4-1.41]
[81-4-4-1.31][82-4-4-0.56][83-4-4-0.82][84-4-4-1.01][85-4-4-1.47][86-4-4-1.06][87-4-4-1.33][88-4-4-1.04][89-4-2-0.85][90-4-4-1.15]
[91-4-4-0.97][92-4-4-0.42][93-4-0-0.95][94-4-4-1.39][95-4-4-0.65][96-4-4-1.15][97-4-4-1.40][98-4-2-1.61][99-4-4-0.54][100-4-2-1.28]
[101-4-4-1.94][102-4-2-1.09][103-4-2-0.57][104-4-4-1.32][105-4-4-1.22][106-4-4-1.25][107-4-4-1.45][108-4-2-1.02][109-4-1-1.00][110-4-1-0.78]
[111-4-0-2.04][112-4-4-0.52][113-4-1-1.15][114-4-2-0.64][115-4-4-0.44][116-4-4-0.60][117-4-4-1.14][119-4-2-1.42][121-4-4-1.18][122-4-1-0.75]
[124-4-2-1.58][125-4-4-1.42][126-4-4-1.07][127-4-4-1.20][128-4-4-0.89][129-4-1-1.11][130-4-4-1.58][131-4-2-0.86][132-4-2-1.03][133-4-4-0.90]
[135-4-4-0.73][136-4-1-0.79][137-4-2-0.76][138-4-2-0.43][139-4-4-0.65][140-4-2-0.94][141-4-4-0.55][142-4-4-1.42][143-4-4-1.90][144-4-4-1.90]
[145-4-4-0.94][148-4-0-1.96][149-4-4-1.07][150-4-4-1.16][151-4-4-1.08][152-4-1-1.09][153-4-2-1.28][154-4-4-0.95][155-4-4-0.96][156-4-2-0.45]
[157-4-4-0.40][158-4-4-0.63][160-4-4-0.96][161-4-2-0.76][162-4-4-0.81][164-4-2-1.02][165-4-1-0.81][167-4-0-0.98][168-4-4-0.75][170-4-2-0.55]
[171-4-1-1.89][172-4-4-1.19][173-4-4-0.54][174-4-0-0.56][175-4-4-0.89][177-4-4-0.85][178-4-4-1.68][179-4-4-0.94][180-4-4-1.44][181-4-2-0.45]
[182-4-2-1.05][183-4-4-1.21][184-4-4-1.44][186-4-4-0.79][187-4-1-1.44][188-4-4-1.13][189-4-4-1.36][190-4-4-0.80][191-4-4-1.48][192-4-4-1.27]
[193-4-2-1.71][194-4-2-1.00][195-4-0-1.18][196-4-2-0.40][197-4-2-1.80][198-4-4-1.89][199-4-2-1.11]
---------------------------
I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 1.059 | Acc: 56.125% | Wgt Acc: 59.437%
	I - Batch: 100 | Loss: 1.060 | Acc: 54.812% | Wgt Acc: 58.040%
	I - Batch: 150 | Loss: 1.060 | Acc: 54.833% | Wgt Acc: 58.486%
	I - Batch: 200 | Loss: 1.068 | Acc: 54.688% | Wgt Acc: 58.135%
I - num batch: 222
I - Train -- Loss: 1.072 | Acc: 54.891% | Wgt Acc: 58.239% | LR: 1.000000e-04 | Dur: 141.61s
I - Confusion Matrix: [row->prediction - col->label]
[[517.  18.  27. 162. 163.]
 [ 16. 304. 108.  31. 128.]
 [ 36. 175. 494.  64. 288.]
 [ 91.  38.  56. 269.  58.]
 [ 37.  43.  49.  12. 363.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.265 | Acc: 48.718% | Wgt Acc: 47.717% | Dur: 16.91s
I - Confusion Matrix: [row->prediction - col->label]
[[48.  1.  1. 22. 10.]
 [ 3. 34. 13. 11. 21.]
 [ 7. 32. 49. 20. 54.]
 [ 9.  1.  1. 24.  3.]
 [21. 10. 11.  9. 92.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 1.048 | Acc: 55.875% | Wgt Acc: 59.384%
	I - Batch: 100 | Loss: 1.042 | Acc: 56.938% | Wgt Acc: 59.782%
	I - Batch: 150 | Loss: 1.034 | Acc: 56.875% | Wgt Acc: 59.928%
	I - Batch: 200 | Loss: 1.037 | Acc: 56.188% | Wgt Acc: 59.168%
I - num batch: 222
I - Train -- Loss: 1.034 | Acc: 55.935% | Wgt Acc: 58.878% | LR: 1.000000e-04 | Dur: 139.13s
I - Confusion Matrix: [row->prediction - col->label]
[[505.  10.  24. 162. 147.]
 [  8. 316. 107.  31. 121.]
 [ 30. 189. 498.  64. 281.]
 [105.  29.  44. 270.  56.]
 [ 49.  34.  61.  11. 395.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.211 | Acc: 52.465% | Wgt Acc: 49.943% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   1.   1.  18.   7.]
 [  2.  34.   9.  11.  15.]
 [  5.  27.  48.  17.  44.]
 [ 14.   0.   2.  29.   4.]
 [ 22.  16.  15.  11. 110.]]

I - Local maximum validation set accuracy:  52.47

I - Validation set results: 
[14-1-2-1.59][50-3-4-1.12][124-2-2-1.20][127-0-0-1.76][443-2-2-2.39][567-0-4-0.71][573-1-1-0.78][615-0-3-0.23][695-1-2-0.99][722-3-3-1.47]
[826-0-0-1.51][878-0-0-1.24][1103-0-4-0.86][1212-3-4-0.49][1368-0-0-1.11][2181-2-3-0.50][2476-2-2-0.54][2721-2-2-0.97][2818-1-2-0.83][2886-2-1-1.32]
[3231-2-2-1.67][3333-2-2-1.58][3482-2-2-1.04][3536-3-3-0.43][3625-1-1-1.75][3909-0-4-0.02][4035-0-3-1.40][4140-0-4-0.59][4214-1-1-1.01][4346-1-0-0.44]
[4581-2-2-1.80][4708-3-2-1.25][4838-3-2-0.45][4845-1-2-1.01][4868-0-0-1.80][4939-0-4-1.18][4984-2-2-0.90][5078-1-4-1.41][5396-0-0-2.26][5479-1-1-1.50]
[5717-0-0-0.46][5843-1-1-1.38][5949-3-0-0.86][5987-2-4-1.53][6014-3-1-1.18][6033-3-0-0.51][6313-0-3-0.23][6421-3-2-0.14][6500-1-1-0.90][6583-3-2-0.19]
[6683-3-2-0.14][6825-2-1-0.64][6998-3-2-0.43][7049-3-2-0.43][7517-1-1-2.08][7521-1-1-0.17][7528-1-2-0.21][7949-1-2-1.86][8135-1-4-0.42][8185-3-0-1.48]
[8269-3-4-0.84][8273-3-3-0.88][8543-3-0-2.80][8666-1-1-1.14][8672-0-0-1.10][8903-1-2-1.22][9001-2-4-0.59][9036-2-2-1.96][9281-3-4-0.62][9300-2-2-1.57]
[9571-0-3-0.15][9617-1-1-0.83][9644-2-2-1.58][9705-2-4-0.08][9801-0-3-0.64][9803-3-1--0.11][9865-3-0-1.15][9896-2-1-1.12][10314-1-4-1.13][10337-3-3-1.36]
[10403-0-4-1.13][10653-2-2-0.82][10704-2-1-1.49][10719-1-1-1.18][10727-1-4-1.55][10836-0-0-2.84][10969-2-3-0.35][11042-0-4-0.42][11088-1-1-1.73][11322-0-0-1.16]
[11398-2-2-1.81][11499-0-0-0.05][11502-3-3-0.16][11512-3-2-0.51][11608-1-1-1.99][11610-0-3-0.79][11692-0-0-0.95][11905-0-0-2.02][11993-1-1-1.44][12002-2-1-0.08]
[12052-0-0-0.54][12201-0-0-1.10][12235-2-2-1.25][12320-1-4-1.33][12377-2-4-1.64][12398-2-2-0.38][12503-1-4-1.18][12617-0-1-1.03][12685-3-1-0.65][12738-2-2--0.02]
[12742-2-2-2.07][12823-0-0-1.60][13110-1-1-1.20][13240-3-3--0.15][13253-1-4-1.54][13273-0-0-1.90][13634-1-4-0.68][13763-2-2-0.77][13905-3-3--0.19][14060-2-1-1.62]
[14065-3-3-0.88][14147-3-1-0.11][14595-2-4-1.03][14687-2-2-1.79][14788-2-2-1.47][14869-1-2-1.20][14872-3-4-0.92][14877-1-1-0.81][14927-0-3-1.19][15066-0-0-1.74]
[15175-1-4-1.02][15178-2-2-0.22][15375-3-0-1.27][15389-3-3-1.30][15568-2-4-0.79][15675-3-1-0.76][15869-1-2-0.92][16207-3-0-0.38][16236-0-2-0.48][16302-3-0-0.57]
[16331-2-2-2.38][16381-0-4-0.25][16488-1-1-1.76][16495-0-0-0.97][16650-0-0-1.67][16719-1-4-0.57][16801-0-0-2.49][16828-0-0-0.74][17137-3-3-0.90][17245-1-2-0.79]
[17278-3-2--0.16][17282-0-2-0.54][17311-2-2-1.69][17336-2-1-1.46][17608-3-3-1.44][17627-0-0--0.02][17877-3-1-0.56][17924-1-2-0.55][17984-3-0-1.49][18211-0-1--0.13]
[18276-3-0-0.26][18287-1-2-1.16][18394-0-0-1.35][18428-0-0-0.97][18442-0-3-0.55][18478-3-0-0.61][18607-0-4-0.89][18616-0-4-0.57][18663-0-3-0.37][18718-0-0-1.16]
[18766-2-2-1.17][18824-2-4-1.77][18890-3-2-1.24][18930-3-4-1.38][18938-3-2-0.05][19817-1-2-1.54][19839-0-4-0.66][19930-3-1-0.04][19944-0-4-0.95][20036-2-2-2.13]
[20101-3-1-0.23][20474-1-2-1.50][20547-3-4-0.73][20929-2-2-1.36][21245-1-1-1.55][21257-3-4-0.47][21293-1-1-2.24][21316-1-1-1.90][21384-1-4-1.82][21448-1-2-1.33]
[21483-0-0-1.42][21487-2-2-1.81][21714-0-2-0.40][21943-3-2-1.41][21947-0-4-0.65][21948-0-0-2.80][21965-2-2-1.41][21998-1-2-0.75][22025-0-2-0.19][22228-3-3-0.14]
[22446-1-1-1.59][22494-3-0-1.34][22757-0-0-1.60][22811-3-3-2.14][22976-3-4-1.14][22985-3-3-0.88][23014-0-0-1.83][23112-1-2-1.59][23144-3-3-1.07][23168-2-4-0.34]
[23219-0-4-0.44][23363-3-3-0.52][23470-0-4-0.12][23486-2-2-0.83][23497-0-3-2.43][23516-0-0-2.04][23690-1-1-0.97][23921-2-2-1.58][23936-1-2-1.37][24040-3-4-0.16]
[24111-1-4-1.92][24182-0-0-2.39][24238-3-3-1.07][24290-2-4-0.17][24345-0-0-0.51][24364-1-2-1.31][24427-3-0--0.15][24477-2-2-1.02][24495-2-1-1.00][24893-2-2-1.64]
[25012-1-1-0.89][25121-2-4-1.74][25165-3-1-0.52][25183-0-0-0.72][25297-3-1-0.43][25398-0-0-0.78][25574-2-2-1.40][25644-1-1-2.36][25718-1-1-0.82][25774-2-2-0.73]
[26032-3-3-1.21][26051-3-3-1.29][26120-0-4-1.22][26321-1-2-0.66][26732-1-1-1.21][26784-3-3-2.05][26827-3-2-0.31][26833-0-3-1.98][26838-2-1-1.05][26860-1-4-0.60]
[26948-0-0-0.60][27049-3-0-0.57][27098-1-4-0.27][27526-0-0-0.82][27639-3-3--0.06][27698-3-3-0.92][27772-0-0-1.65][27890-1-1-1.14][28040-0-4-0.90][28503-2-2-1.73]
[28577-1-1-1.38][28959-0-0-2.74][29198-3-4-0.93][29777-0-0-3.06][29877-2-2-1.63][30035-1-1-1.78][30098-0-3-0.20][30326-1-1-2.49][30572-2-2-1.13][30716-0-4-1.63]
[30806-2-2-0.50][30906-1-2-1.20][31007-0-0-0.22][31181-3-3-0.04][31238-0-3-0.65][31347-0-0-2.06][31422-2-4-0.92][31429-3-2-0.62][31431-0-3-0.34][31432-1-1-1.10]
[31477-0-0-1.69][31524-1-2-0.35][31597-1-2-1.97][31619-1-2-0.53][31701-0-0-1.48][31755-0-0-1.02][31854-3-1-0.15][32074-1-2-1.48][32078-3-3-0.49][32111-1-1-1.40]
[32127-1-2-2.04][32140-3-3-0.74][32263-2-4-0.82][32365-0-4-0.02][32411-2-0-1.70][32429-3-0-1.39][32473-3-3-0.65][32574-3-0-2.01][32584-0-4-0.89][32622-0-4-1.02]
[32858-3-0-1.16][32969-3-0-1.46][33016-2-2-2.05][33031-1-1-0.72][33035-2-2-1.91][33133-2-2-1.45][33173-2-2-0.72][33175-3-2-1.23][33306-3-2-1.90][33309-2-4-0.27]
[33474-0-2--0.07][33478-2-2-0.50][33618-1-1-0.56][33712-0-4-0.39][33782-2-4-1.56][33914-3-3-0.26][34076-3-3-0.29][34112-2-2-0.76][34138-2-2-1.33][34239-1-2-0.66]
[34364-2-2-2.08][34617-1-4-1.43][34751-3-3-1.48][34783-2-4-1.23][35015-3-2-0.34][35018-1-4-1.33][35288-2-2-1.25][0-4-4-0.66][1-4-4-1.39][2-4-4-1.13]
[3-4-4-1.64][4-4-2-0.48][5-4-1-0.90][6-4-0-1.27][7-4-4-1.42][8-4-2-0.30][9-4-2-0.90][10-4-4-1.11][11-4-2-2.07][12-4-2-0.40]
[14-4-0-0.16][15-4-3-1.09][16-4-4-1.41][17-4-4-1.05][18-4-4-1.86][19-4-3-1.31][20-4-4-0.77][21-4-4-1.28][22-4-4-1.99][23-4-1-0.65]
[24-4-4-2.17][25-4-2-0.27][26-4-1-0.60][27-4-2-0.76][28-4-4-1.21][29-4-2-1.12][30-4-4-0.48][31-4-2-0.85][32-4-4-1.61][33-4-2-0.76]
[34-4-2-0.42][35-4-3-0.48][37-4-4-0.39][39-4-0-1.56][40-4-4-0.62][41-4-4-0.66][42-4-2-0.70][43-4-2-0.59][45-4-2-1.53][46-4-4-1.81]
[47-4-4-2.29][48-4-4-1.76][51-4-4-1.82][52-4-4-0.86][53-4-1-0.61][54-4-4-0.32][55-4-2-0.80][56-4-4-1.21][57-4-3-0.56][58-4-2-1.56]
[59-4-4-0.90][60-4-4-0.86][61-4-4-1.83][62-4-2-0.34][63-4-2-1.46][64-4-4-0.78][65-4-4-1.72][66-4-4-1.58][67-4-4-0.70][68-4-1-1.45]
[69-4-4-0.56][70-4-4-1.84][72-4-4-1.41][73-4-1-1.23][74-4-2-1.16][75-4-4-0.87][77-4-4-2.31][78-4-2-0.97][79-4-4-1.79][80-4-4-1.31]
[81-4-1-1.16][82-4-2-1.19][83-4-1-0.87][84-4-4-1.69][85-4-4-1.75][86-4-4-1.18][87-4-4-1.54][88-4-4-1.28][89-4-2-1.09][90-4-4-1.28]
[91-4-4-1.09][92-4-4-0.68][93-4-4-0.72][94-4-4-1.83][95-4-2-0.38][96-4-4-0.85][97-4-4-1.40][98-4-2-1.68][99-4-4-1.14][100-4-4-1.29]
[101-4-4-2.37][102-4-2-1.11][103-4-4--0.15][104-4-4-1.70][105-4-4-1.25][106-4-4-1.59][107-4-4-1.22][108-4-4-0.98][109-4-4-0.94][110-4-4-1.31]
[111-4-0-2.02][112-4-4-1.10][113-4-2-0.62][114-4-2-0.65][115-4-4-1.08][116-4-4-1.12][117-4-4-1.40][119-4-4-0.78][121-4-4-1.60][122-4-4-0.69]
[124-4-2-0.87][125-4-4-1.52][126-4-4-1.17][127-4-2-1.63][128-4-4-0.58][129-4-4-0.92][130-4-4-1.59][131-4-2-0.49][132-4-2-1.16][133-4-4-1.37]
[135-4-4-0.62][136-4-1-0.60][137-4-4-1.19][138-4-4-0.34][139-4-4-0.81][140-4-1-0.71][141-4-0-0.90][142-4-4-1.85][143-4-4-2.11][144-4-4-2.18]
[145-4-4-0.87][148-4-0-1.63][149-4-2-0.27][150-4-4-1.33][151-4-4-1.45][152-4-4-1.45][153-4-2-1.18][154-4-2-1.07][155-4-4-1.27][156-4-4-0.84]
[157-4-4-0.57][158-4-4-0.46][160-4-4-1.00][161-4-1-1.06][162-4-4-0.90][164-4-2-1.16][165-4-1-1.27][167-4-4-0.89][168-4-4-1.14][170-4-4-0.23]
[171-4-1-0.90][172-4-4-2.17][173-4-4-1.59][174-4-0-0.89][175-4-4-1.44][177-4-4-0.95][178-4-4-1.88][179-4-4-1.09][180-4-4-1.92][181-4-2-0.74]
[182-4-2-0.62][183-4-4-1.35][184-4-4-1.39][186-4-4-0.57][187-4-1-1.28][188-4-2-0.89][189-4-4-0.93][190-4-1-0.74][191-4-4-1.14][192-4-4-1.41]
[193-4-2-1.84][194-4-2-0.78][195-4-2-0.54][196-4-2-0.84][197-4-2-1.57][198-4-4-2.25][199-4-2-0.52]
---------------------------
I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 1.013 | Acc: 59.250% | Wgt Acc: 63.102%
	I - Batch: 100 | Loss: 1.010 | Acc: 57.062% | Wgt Acc: 60.692%
	I - Batch: 150 | Loss: 1.010 | Acc: 57.125% | Wgt Acc: 60.472%
	I - Batch: 200 | Loss: 1.010 | Acc: 57.656% | Wgt Acc: 60.917%
I - num batch: 222
I - Train -- Loss: 1.007 | Acc: 57.485% | Wgt Acc: 60.812% | LR: 1.000000e-04 | Dur: 136.63s
I - Confusion Matrix: [row->prediction - col->label]
[[510.  11.  23. 135. 182.]
 [  8. 343. 108.  30. 123.]
 [ 26. 173. 488.  58. 244.]
 [102.  22.  52. 303.  56.]
 [ 51.  29.  63.  12. 395.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.208 | Acc: 53.057% | Wgt Acc: 48.783% | Dur: 14.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   1.   3.  22.   9.]
 [  3.  28.   3.   4.   5.]
 [  2.  28.  50.  21.  44.]
 [  5.   1.   2.  20.   0.]
 [ 29.  20.  17.  19. 122.]]

I - Local maximum validation set accuracy:  53.06

I - Validation set results: 
[14-1-2-1.56][50-3-4-0.36][124-2-2-1.19][127-0-0-2.61][443-2-2-2.33][567-0-4-0.76][573-1-1-1.83][615-0-4-0.50][695-1-2-1.03][722-3-0-1.98]
[826-0-0-0.87][878-0-0-1.32][1103-0-4-0.76][1212-3-4-1.11][1368-0-0-1.42][2181-2-3-0.03][2476-2-4-0.45][2721-2-2-1.69][2818-1-2-1.06][2886-2-1-1.85]
[3231-2-2-2.39][3333-2-2-1.58][3482-2-2-1.31][3536-3-3-0.60][3625-1-1-1.70][3909-0-4-0.08][4035-0-3-1.52][4140-0-4-0.69][4214-1-1-1.37][4346-1-4-1.04]
[4581-2-2-1.69][4708-3-2-1.59][4838-3-4-0.30][4845-1-2-1.78][4868-0-0-2.03][4939-0-4-0.76][4984-2-2-1.83][5078-1-4-1.37][5396-0-0-2.62][5479-1-1-0.89]
[5717-0-0-0.99][5843-1-1-1.43][5949-3-0-1.19][5987-2-4-1.56][6014-3-1-0.09][6033-3-0-1.43][6313-0-0-0.56][6421-3-2-0.61][6500-1-1-0.77][6583-3-2-0.78]
[6683-3-3-0.49][6825-2-2-0.18][6998-3-2-0.34][7049-3-2-0.90][7517-1-1-1.91][7521-1-1--0.01][7528-1-2-0.77][7949-1-4-1.64][8135-1-0-2.41][8185-3-0-1.59]
[8269-3-4-1.12][8273-3-3-0.66][8543-3-0-4.07][8666-1-1-1.35][8672-0-0-2.08][8903-1-2-1.14][9001-2-1-1.41][9036-2-2-2.18][9281-3-2-0.51][9300-2-2-1.75]
[9571-0-4--0.01][9617-1-1-0.15][9644-2-2-1.81][9705-2-4-1.01][9801-0-4--0.05][9803-3-3-0.05][9865-3-0-1.24][9896-2-4-1.85][10314-1-4-1.52][10337-3-3-1.38]
[10403-0-4-1.62][10653-2-4-1.06][10704-2-2-0.96][10719-1-2-1.49][10727-1-4-1.52][10836-0-0-3.87][10969-2-3-0.40][11042-0-4-0.57][11088-1-2-2.34][11322-0-0-2.05]
[11398-2-2-2.98][11499-0-4-0.23][11502-3-2--0.18][11512-3-2-1.03][11608-1-1-2.15][11610-0-3-0.70][11692-0-0-0.91][11905-0-0-2.57][11993-1-1-1.40][12002-2-0-0.89]
[12052-0-0-1.70][12201-0-0-1.16][12235-2-2-1.20][12320-1-4-2.21][12377-2-4-2.26][12398-2-2-0.08][12503-1-2-1.41][12617-0-1-0.49][12685-3-2-0.69][12738-2-4-0.57]
[12742-2-2-2.77][12823-0-0-1.47][13110-1-1-1.06][13240-3-4--0.14][13253-1-1-1.84][13273-0-0-2.63][13634-1-4-1.41][13763-2-2-0.31][13905-3-4--0.09][14060-2-1-1.11]
[14065-3-0-0.79][14147-3-4-0.22][14595-2-2-1.54][14687-2-2-2.30][14788-2-2-1.70][14869-1-2-1.77][14872-3-4-1.33][14877-1-1-1.34][14927-0-3-0.71][15066-0-0-3.02]
[15175-1-4-1.44][15178-2-2-0.34][15375-3-0-0.03][15389-3-0-0.96][15568-2-4-1.04][15675-3-3-0.28][15869-1-2-0.51][16207-3-4-0.39][16236-0-2-0.66][16302-3-4-0.53]
[16331-2-2-2.69][16381-0-4-0.63][16488-1-1-2.17][16495-0-0-1.62][16650-0-0-1.53][16719-1-4-1.01][16801-0-0-3.20][16828-0-0-0.79][17137-3-0-1.08][17245-1-4-1.06]
[17278-3-4-0.25][17282-0-1-0.15][17311-2-2-2.16][17336-2-2-2.36][17608-3-3-1.53][17627-0-4-0.71][17877-3-2-1.43][17924-1-2-0.86][17984-3-0-1.91][18211-0-1-0.04]
[18276-3-0-0.52][18287-1-2-0.05][18394-0-0-1.51][18428-0-0-2.69][18442-0-0-0.36][18478-3-0-0.61][18607-0-4-0.90][18616-0-4-1.49][18663-0-0-0.49][18718-0-0-1.28]
[18766-2-2-1.86][18824-2-4-1.69][18890-3-2-0.63][18930-3-4-1.70][18938-3-2-0.21][19817-1-2-1.90][19839-0-4-1.89][19930-3-3--0.08][19944-0-4-2.24][20036-2-2-2.07]
[20101-3-3-0.70][20474-1-2-1.40][20547-3-4-0.90][20929-2-2-1.65][21245-1-1-1.34][21257-3-4-0.73][21293-1-2-1.87][21316-1-1-1.10][21384-1-4-2.35][21448-1-2-1.71]
[21483-0-0-1.21][21487-2-2-2.39][21714-0-4-0.44][21943-3-2-2.02][21947-0-4-0.69][21948-0-0-3.58][21965-2-4-1.35][21998-1-1-0.64][22025-0-4-0.72][22228-3-3-0.49]
[22446-1-1-1.59][22494-3-0-1.13][22757-0-0-2.88][22811-3-3-0.01][22976-3-2-1.24][22985-3-3-0.74][23014-0-0-1.05][23112-1-2-1.66][23144-3-0-0.52][23168-2-0-1.07]
[23219-0-2-0.32][23363-3-3-1.08][23470-0-4-0.55][23486-2-2-0.88][23497-0-3-2.16][23516-0-0-3.46][23690-1-2-0.80][23921-2-2-1.78][23936-1-2-1.92][24040-3-4-1.15]
[24111-1-4-1.87][24182-0-0-2.85][24238-3-3-0.71][24290-2-4-0.55][24345-0-0-1.30][24364-1-2-1.75][24427-3-4--0.09][24477-2-2-1.53][24495-2-2-0.95][24893-2-2-1.79]
[25012-1-1-0.63][25121-2-4-2.27][25165-3-1-0.30][25183-0-0-2.32][25297-3-1--0.02][25398-0-0-0.94][25574-2-2-1.50][25644-1-1-2.44][25718-1-2-0.25][25774-2-2-0.85]
[26032-3-3-0.84][26051-3-3-0.85][26120-0-4-1.54][26321-1-2-1.06][26732-1-1-1.33][26784-3-3-1.59][26827-3-2-0.56][26833-0-3-1.74][26838-2-2-0.76][26860-1-4-1.81]
[26948-0-0-1.23][27049-3-0-1.24][27098-1-4-0.20][27526-0-0-1.48][27639-3-0-0.12][27698-3-0-1.12][27772-0-0-3.01][27890-1-1-1.12][28040-0-4-1.10][28503-2-2-1.96]
[28577-1-1-1.31][28959-0-0-2.85][29198-3-4-0.80][29777-0-0-3.12][29877-2-2-1.46][30035-1-2-1.89][30098-0-0-0.86][30326-1-1-2.96][30572-2-2-0.90][30716-0-4-1.69]
[30806-2-2-0.60][30906-1-4-1.00][31007-0-4-0.80][31181-3-0-0.30][31238-0-0-0.64][31347-0-0-0.94][31422-2-4-0.66][31429-3-2-0.62][31431-0-0-0.77][31432-1-1-0.86]
[31477-0-0-1.77][31524-1-4-0.18][31597-1-2-1.94][31619-1-2-1.22][31701-0-0-0.84][31755-0-0-0.76][31854-3-1-0.07][32074-1-4-0.38][32078-3-4-0.75][32111-1-1-1.57]
[32127-1-2-2.49][32140-3-3-0.23][32263-2-4-0.60][32365-0-4--0.04][32411-2-0-1.80][32429-3-0-2.01][32473-3-3-0.41][32574-3-0-2.10][32584-0-4-1.45][32622-0-4-1.45]
[32858-3-4-0.53][32969-3-0-1.52][33016-2-2-0.93][33031-1-3--0.01][33035-2-2-2.13][33133-2-2-1.51][33173-2-2-0.76][33175-3-2-1.60][33306-3-2-2.30][33309-2-4-0.30]
[33474-0-0-0.44][33478-2-2-0.36][33618-1-4-0.73][33712-0-4-0.62][33782-2-4-2.35][33914-3-2--0.01][34076-3-2-0.68][34112-2-2-1.30][34138-2-2-2.04][34239-1-2-0.93]
[34364-2-2-2.44][34617-1-4-1.61][34751-3-3-1.41][34783-2-4-1.05][35015-3-2-0.59][35018-1-4-1.46][35288-2-2-1.21][0-4-4-2.13][1-4-4-2.64][2-4-4-1.60]
[3-4-4-2.13][4-4-2-0.60][5-4-1-0.51][6-4-4-2.16][7-4-4-2.01][8-4-2-0.99][9-4-2-1.15][10-4-4-1.92][11-4-2-2.34][12-4-2-1.23]
[14-4-4-0.83][15-4-0-0.90][16-4-4-1.84][17-4-4-0.25][18-4-4-2.26][19-4-0-1.85][20-4-4-0.90][21-4-2-1.65][22-4-4-1.98][23-4-2-0.63]
[24-4-4-2.79][25-4-2-0.57][26-4-4-0.66][27-4-4-0.82][28-4-4-2.06][29-4-2-1.35][30-4-4-0.48][31-4-4-1.60][32-4-4-1.66][33-4-2-1.15]
[34-4-2-0.81][35-4-0-1.07][37-4-2-1.16][39-4-0-1.74][40-4-4-0.94][41-4-2-0.80][42-4-2-0.77][43-4-2-1.19][45-4-0--0.03][46-4-4-2.40]
[47-4-4-2.64][48-4-4-1.84][51-4-4-2.20][52-4-4-1.28][53-4-4-0.57][54-4-4-0.29][55-4-2-1.21][56-4-4-1.09][57-4-4-0.33][58-4-2-2.05]
[59-4-4-1.00][60-4-4-1.12][61-4-4-2.19][62-4-2-0.81][63-4-2-1.73][64-4-4-0.94][65-4-4-2.09][66-4-4-2.17][67-4-4-1.28][68-4-1-1.25]
[69-4-4-0.82][70-4-4-1.85][72-4-2-1.70][73-4-1-1.36][74-4-2-1.65][75-4-4-0.92][77-4-4-2.82][78-4-2-0.90][79-4-4-1.81][80-4-4-2.20]
[81-4-4-2.54][82-4-2-0.86][83-4-4-0.87][84-4-4-2.07][85-4-4-2.14][86-4-4-1.62][87-4-4-1.88][88-4-4-1.91][89-4-2-1.42][90-4-4-0.89]
[91-4-4-1.54][92-4-4-0.72][93-4-0-1.03][94-4-4-2.05][95-4-4-1.07][96-4-4-1.35][97-4-4-1.85][98-4-2-1.85][99-4-4-1.39][100-4-4-1.55]
[101-4-4-3.16][102-4-2-1.40][103-4-4-0.29][104-4-4-1.58][105-4-4-1.24][106-4-4-1.67][107-4-4-1.90][108-4-1-0.92][109-4-4-0.99][110-4-4-1.00]
[111-4-0-3.35][112-4-4-1.06][113-4-2-0.92][114-4-2-0.75][115-4-4-1.34][116-4-4-0.95][117-4-4-1.89][119-4-4-1.76][121-4-4-1.64][122-4-4-1.22]
[124-4-2-2.00][125-4-4-1.52][126-4-4-2.00][127-4-2-1.58][128-4-4-1.05][129-4-4-0.93][130-4-4-1.05][131-4-2-1.29][132-4-4-1.41][133-4-4-2.26]
[135-4-4-1.31][136-4-4-0.88][137-4-4-0.68][138-4-4-1.25][139-4-4-1.70][140-4-2-1.01][141-4-4-0.90][142-4-4-2.14][143-4-4-2.44][144-4-4-2.21]
[145-4-4-1.33][148-4-0-2.26][149-4-4-1.47][150-4-4-1.89][151-4-4-1.45][152-4-2-1.50][153-4-4-2.07][154-4-2-1.82][155-4-4-1.88][156-4-4-1.03]
[157-4-4-0.77][158-4-4-0.93][160-4-4-1.66][161-4-2-1.02][162-4-2-1.22][164-4-2-1.32][165-4-4-1.64][167-4-4-1.06][168-4-4-0.98][170-4-4-0.67]
[171-4-4-1.25][172-4-4-2.47][173-4-4-1.96][174-4-0-0.68][175-4-4-1.77][177-4-4-2.00][178-4-4-2.19][179-4-4-1.49][180-4-4-2.14][181-4-2-1.05]
[182-4-2-1.35][183-4-4-1.82][184-4-4-2.05][186-4-4-0.87][187-4-1-1.84][188-4-4-1.57][189-4-4-1.92][190-4-4-0.81][191-4-4-1.69][192-4-4-2.42]
[193-4-2-2.12][194-4-2-1.11][195-4-4-0.37][196-4-2-0.62][197-4-2-2.12][198-4-4-3.14][199-4-2-1.36]
---------------------------
I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 0.951 | Acc: 61.750% | Wgt Acc: 65.220%
	I - Batch: 100 | Loss: 0.988 | Acc: 58.750% | Wgt Acc: 62.133%
	I - Batch: 150 | Loss: 0.984 | Acc: 58.792% | Wgt Acc: 62.104%
	I - Batch: 200 | Loss: 0.991 | Acc: 58.688% | Wgt Acc: 62.026%
I - num batch: 222
I - Train -- Loss: 0.992 | Acc: 58.303% | Wgt Acc: 61.696% | LR: 1.000000e-04 | Dur: 133.42s
I - Confusion Matrix: [row->prediction - col->label]
[[509.  17.  22. 136. 165.]
 [  9. 337.  95.  29. 116.]
 [ 28. 164. 514.  52. 256.]
 [103.  25.  46. 309.  64.]
 [ 48.  35.  57.  12. 399.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.199 | Acc: 57.002% | Wgt Acc: 50.841% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   1.   1.  14.   5.]
 [  2.  28.   4.   5.   5.]
 [  2.  24.  48.  13.  25.]
 [  8.   1.   1.  25.   2.]
 [ 31.  24.  21.  29. 143.]]

I - Local maximum validation set accuracy:  57.00

I - Validation set results: 
[14-1-2-1.43][50-3-4-0.59][124-2-2-1.70][127-0-0-2.94][443-2-2-2.39][567-0-4-0.70][573-1-1-1.19][615-0-4-0.74][695-1-2-1.46][722-3-0-1.54]
[826-0-0-0.87][878-0-0-1.08][1103-0-4-0.82][1212-3-4-1.65][1368-0-0-1.50][2181-2-4--0.05][2476-2-4-0.71][2721-2-2-1.52][2818-1-4-1.24][2886-2-1-1.36]
[3231-2-2-2.20][3333-2-2-1.48][3482-2-2-1.41][3536-3-3-0.67][3625-1-1-1.72][3909-0-4-0.41][4035-0-3-1.60][4140-0-4-0.74][4214-1-1-0.90][4346-1-4-0.63]
[4581-2-2-1.36][4708-3-2-1.12][4838-3-4-0.52][4845-1-2-1.48][4868-0-0-1.59][4939-0-4-1.00][4984-2-2-2.12][5078-1-4-1.26][5396-0-0-2.58][5479-1-1-1.11]
[5717-0-4-0.83][5843-1-1-1.39][5949-3-0-1.11][5987-2-4-1.64][6014-3-1-0.95][6033-3-4-0.92][6313-0-0-0.12][6421-3-2-0.08][6500-1-1-1.43][6583-3-2-0.23]
[6683-3-3-0.08][6825-2-2-0.29][6998-3-2-0.60][7049-3-2-0.97][7517-1-1-1.62][7521-1-1--0.25][7528-1-2-1.10][7949-1-2-1.66][8135-1-0-1.67][8185-3-0-1.49]
[8269-3-4-1.07][8273-3-3-0.96][8543-3-0-3.19][8666-1-1-1.91][8672-0-0-1.36][8903-1-2-1.37][9001-2-1-1.93][9036-2-2-2.06][9281-3-4-0.74][9300-2-2-1.50]
[9571-0-4-0.65][9617-1-4-1.06][9644-2-2-1.73][9705-2-4-0.99][9801-0-4-0.25][9803-3-3-0.04][9865-3-0-1.03][9896-2-4-1.86][10314-1-4-1.20][10337-3-3-1.34]
[10403-0-4-1.82][10653-2-4-1.40][10704-2-1-0.76][10719-1-2-1.57][10727-1-4-1.94][10836-0-0-3.58][10969-2-3-0.17][11042-0-4-0.74][11088-1-1-2.08][11322-0-0-1.60]
[11398-2-2-2.98][11499-0-4-0.43][11502-3-3--0.15][11512-3-2-0.55][11608-1-1-2.08][11610-0-3-1.05][11692-0-0-0.67][11905-0-0-2.34][11993-1-2-1.33][12002-2-2-0.32]
[12052-0-0-1.36][12201-0-0-1.01][12235-2-4-1.29][12320-1-4-2.18][12377-2-4-2.46][12398-2-2-0.05][12503-1-4-1.64][12617-0-1-1.22][12685-3-1-0.76][12738-2-2-0.84]
[12742-2-2-2.50][12823-0-0-1.28][13110-1-1-1.70][13240-3-4--0.01][13253-1-4-1.61][13273-0-0-2.16][13634-1-4-1.69][13763-2-2-0.87][13905-3-4--0.08][14060-2-1-1.60]
[14065-3-0-0.89][14147-3-4-0.23][14595-2-4-1.69][14687-2-2-2.42][14788-2-2-2.03][14869-1-2-1.80][14872-3-4-1.00][14877-1-1-1.67][14927-0-3-0.84][15066-0-0-2.59]
[15175-1-4-1.31][15178-2-2-0.40][15375-3-3-0.37][15389-3-3-1.11][15568-2-4-1.20][15675-3-3-0.62][15869-1-2-0.78][16207-3-4-0.41][16236-0-2-0.63][16302-3-4-0.67]
[16331-2-2-2.74][16381-0-4-0.78][16488-1-1-1.93][16495-0-0-0.06][16650-0-0-1.14][16719-1-4-0.87][16801-0-0-2.64][16828-0-0-0.98][17137-3-3-0.73][17245-1-2-0.83]
[17278-3-4-0.25][17282-0-2-0.30][17311-2-2-2.23][17336-2-2-2.27][17608-3-3-1.59][17627-0-4-1.02][17877-3-4-0.95][17924-1-2-0.92][17984-3-0-2.14][18211-0-1-0.42]
[18276-3-0-0.32][18287-1-1-0.81][18394-0-0-1.14][18428-0-4-0.66][18442-0-3-0.42][18478-3-0-0.48][18607-0-4-0.90][18616-0-4-1.34][18663-0-3-0.47][18718-0-0-0.96]
[18766-2-2-1.94][18824-2-4-1.76][18890-3-2-1.33][18930-3-4-1.91][18938-3-4-0.42][19817-1-2-1.74][19839-0-4-1.61][19930-3-1--0.23][19944-0-4-2.32][20036-2-2-1.65]
[20101-3-3-0.05][20474-1-2-1.37][20547-3-4-1.18][20929-2-2-1.66][21245-1-1-1.21][21257-3-4-0.83][21293-1-1-2.00][21316-1-1-1.73][21384-1-4-2.83][21448-1-2-1.41]
[21483-0-0-1.34][21487-2-2-1.91][21714-0-4-0.70][21943-3-2-1.40][21947-0-0-1.19][21948-0-0-3.34][21965-2-2-1.81][21998-1-1-0.76][22025-0-4-0.98][22228-3-3-0.35]
[22446-1-1-1.61][22494-3-0-0.97][22757-0-0-2.11][22811-3-3-1.46][22976-3-4-1.39][22985-3-3-0.83][23014-0-0-0.73][23112-1-2-1.46][23144-3-3-0.73][23168-2-4-0.37]
[23219-0-4-0.87][23363-3-3-0.85][23470-0-4-0.72][23486-2-2-1.28][23497-0-3-2.12][23516-0-0-2.84][23690-1-4-1.64][23921-2-2-1.74][23936-1-2-2.25][24040-3-4-1.14]
[24111-1-4-1.82][24182-0-0-2.00][24238-3-3-0.58][24290-2-4-0.37][24345-0-0-1.00][24364-1-2-1.66][24427-3-4-0.58][24477-2-4-1.60][24495-2-4-1.03][24893-2-2-1.36]
[25012-1-1-0.74][25121-2-4-2.06][25165-3-1-0.44][25183-0-0-1.13][25297-3-4--0.17][25398-0-0-0.75][25574-2-2-1.88][25644-1-4-1.49][25718-1-4-0.46][25774-2-2-0.46]
[26032-3-3-0.92][26051-3-3-1.45][26120-0-4-1.83][26321-1-1-1.10][26732-1-1-1.47][26784-3-3-1.99][26827-3-2-0.80][26833-0-3-1.99][26838-2-2-0.66][26860-1-4-1.88]
[26948-0-0-0.39][27049-3-0-0.96][27098-1-4-0.50][27526-0-0-1.93][27639-3-4-0.35][27698-3-3-0.68][27772-0-0-1.80][27890-1-1-1.41][28040-0-4-2.18][28503-2-2-2.08]
[28577-1-1-1.57][28959-0-0-2.80][29198-3-4-1.04][29777-0-0-3.12][29877-2-2-1.55][30035-1-2-1.61][30098-0-0-0.58][30326-1-1-2.78][30572-2-2-1.30][30716-0-4-1.93]
[30806-2-2-0.91][30906-1-4-0.94][31007-0-0-0.08][31181-3-4-0.27][31238-0-3-0.51][31347-0-0-0.73][31422-2-4-0.95][31429-3-2-0.56][31431-0-4--0.14][31432-1-1-1.28]
[31477-0-0-2.08][31524-1-2-1.05][31597-1-2-1.85][31619-1-2-1.09][31701-0-0-1.15][31755-0-0-0.88][31854-3-1--0.04][32074-1-2-1.39][32078-3-4-0.66][32111-1-1-1.03]
[32127-1-2-2.05][32140-3-3-0.34][32263-2-4-0.89][32365-0-0-0.11][32411-2-0-1.65][32429-3-0-1.78][32473-3-3-0.35][32574-3-0-2.02][32584-0-4-1.61][32622-0-4-1.50]
[32858-3-4-0.39][32969-3-0-1.41][33016-2-2-2.23][33031-1-3-0.07][33035-2-2-2.05][33133-2-2-1.63][33173-2-2-0.67][33175-3-4-1.39][33306-3-2-2.23][33309-2-4-0.36]
[33474-0-4-0.28][33478-2-2-0.87][33618-1-4-1.15][33712-0-4-0.88][33782-2-4-2.73][33914-3-4-0.14][34076-3-2-0.49][34112-2-2-0.69][34138-2-2-1.92][34239-1-4-0.84]
[34364-2-2-2.29][34617-1-4-1.73][34751-3-3-1.63][34783-2-4-1.45][35015-3-2-0.44][35018-1-4-1.15][35288-2-2-0.84][0-4-4-2.36][1-4-4-2.12][2-4-4-1.59]
[3-4-4-2.24][4-4-2-0.78][5-4-1-1.13][6-4-4-1.28][7-4-4-1.79][8-4-2-1.13][9-4-4-1.37][10-4-4-2.18][11-4-4-2.39][12-4-2-1.12]
[14-4-4-0.60][15-4-3-0.54][16-4-4-1.73][17-4-4-1.05][18-4-4-2.83][19-4-3-0.54][20-4-4-0.96][21-4-4-1.34][22-4-4-1.90][23-4-4-0.80]
[24-4-4-3.34][25-4-2-0.30][26-4-4-0.93][27-4-4-0.85][28-4-4-2.15][29-4-2-1.43][30-4-4-1.30][31-4-4-1.52][32-4-4-1.62][33-4-2-1.38]
[34-4-4-1.04][35-4-0-0.29][37-4-4-0.98][39-4-0-1.50][40-4-4-0.91][41-4-4-0.75][42-4-2-0.86][43-4-4-1.01][45-4-2-1.11][46-4-4-2.47]
[47-4-4-2.54][48-4-4-2.08][51-4-4-2.14][52-4-4-1.60][53-4-4-0.99][54-4-4-0.28][55-4-4-1.51][56-4-4-1.13][57-4-0-0.25][58-4-2-2.00]
[59-4-4-1.06][60-4-4-0.95][61-4-4-1.62][62-4-2-1.21][63-4-2-1.72][64-4-4-0.98][65-4-4-2.26][66-4-4-1.93][67-4-4-1.49][68-4-1-1.34]
[69-4-4-1.28][70-4-4-2.09][72-4-4-1.53][73-4-1-1.36][74-4-4-1.22][75-4-4-1.40][77-4-4-1.69][78-4-2-1.16][79-4-4-2.04][80-4-4-1.84]
[81-4-4-2.64][82-4-2-0.72][83-4-4-1.08][84-4-4-2.68][85-4-4-2.20][86-4-4-1.58][87-4-4-1.92][88-4-4-1.49][89-4-4-1.30][90-4-4-0.93]
[91-4-4-1.64][92-4-4-0.87][93-4-4-0.60][94-4-4-2.03][95-4-4-1.09][96-4-4-1.32][97-4-4-2.37][98-4-2-1.79][99-4-4-1.54][100-4-4-1.75]
[101-4-4-3.40][102-4-2-1.42][103-4-4-0.42][104-4-4-1.44][105-4-4-1.31][106-4-4-2.14][107-4-4-1.75][108-4-4-1.28][109-4-4-1.37][110-4-4-1.22]
[111-4-0-2.01][112-4-4-0.67][113-4-2-0.80][114-4-2-0.78][115-4-4-1.30][116-4-4-0.53][117-4-4-1.65][119-4-4-1.97][121-4-4-1.94][122-4-4-1.44]
[124-4-2-1.84][125-4-4-1.85][126-4-4-2.61][127-4-4-1.20][128-4-4-1.10][129-4-1-1.03][130-4-4-1.07][131-4-2-1.23][132-4-4-1.75][133-4-4-2.29]
[135-4-4-1.74][136-4-4-0.56][137-4-4-1.28][138-4-4-1.14][139-4-4-2.12][140-4-4-1.03][141-4-2-0.84][142-4-4-2.33][143-4-4-2.15][144-4-4-2.41]
[145-4-4-1.34][148-4-0-2.41][149-4-4-1.40][150-4-4-2.02][151-4-4-2.08][152-4-4-2.15][153-4-2-1.97][154-4-4-2.07][155-4-4-1.45][156-4-4-0.91]
[157-4-4-0.73][158-4-4-1.14][160-4-4-0.75][161-4-2-0.99][162-4-4-1.38][164-4-4-1.68][165-4-4-1.13][167-4-4-0.91][168-4-4-1.30][170-4-4-1.00]
[171-4-4-1.45][172-4-4-2.78][173-4-4-2.53][174-4-4-0.68][175-4-4-1.59][177-4-4-2.27][178-4-4-1.87][179-4-4-1.89][180-4-4-1.95][181-4-4-0.34]
[182-4-2-1.63][183-4-4-1.69][184-4-4-2.25][186-4-4-0.88][187-4-1-1.85][188-4-4-1.66][189-4-4-2.10][190-4-4-0.73][191-4-4-1.79][192-4-4-2.84]
[193-4-2-2.61][194-4-2-0.31][195-4-4-0.81][196-4-4-0.99][197-4-4-1.46][198-4-4-2.85][199-4-4-1.43]
---------------------------
I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 0.959 | Acc: 61.625% | Wgt Acc: 63.457%
	I - Batch: 100 | Loss: 0.953 | Acc: 61.125% | Wgt Acc: 63.245%
	I - Batch: 150 | Loss: 0.954 | Acc: 60.167% | Wgt Acc: 62.832%
	I - Batch: 200 | Loss: 0.949 | Acc: 60.688% | Wgt Acc: 63.322%
I - num batch: 222
I - Train -- Loss: 0.949 | Acc: 60.671% | Wgt Acc: 63.286% | LR: 5.000000e-05 | Dur: 133.17s
I - Confusion Matrix: [row->prediction - col->label]
[[535.  12.  21. 135. 153.]
 [  7. 325. 100.  21. 110.]
 [ 22. 180. 513.  53. 209.]
 [ 87.  31.  46. 317.  66.]
 [ 46.  30.  54.  12. 462.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.174 | Acc: 56.607% | Wgt Acc: 51.207% | Dur: 17.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   1.   2.  16.   8.]
 [  2.  31.   4.   5.   7.]
 [  3.  24.  47.  19.  27.]
 [  8.   0.   2.  25.   1.]
 [ 28.  22.  20.  21. 137.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 0.946 | Acc: 59.250% | Wgt Acc: 62.281%
	I - Batch: 100 | Loss: 0.932 | Acc: 61.125% | Wgt Acc: 64.175%
	I - Batch: 150 | Loss: 0.933 | Acc: 60.708% | Wgt Acc: 63.659%
	I - Batch: 200 | Loss: 0.932 | Acc: 61.219% | Wgt Acc: 64.393%
I - num batch: 222
I - Train -- Loss: 0.930 | Acc: 61.319% | Wgt Acc: 64.555% | LR: 5.000000e-05 | Dur: 131.98s
I - Confusion Matrix: [row->prediction - col->label]
[[543.  15.  21. 128. 163.]
 [  9. 364.  98.  35. 110.]
 [ 17. 147. 515.  49. 236.]
 [ 82.  19.  47. 316.  54.]
 [ 46.  33.  53.  10. 437.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 57.594% | Wgt Acc: 53.067% | Dur: 14.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   1.   1.  18.   6.]
 [  2.  32.   7.   7.  10.]
 [  1.  22.  49.  14.  29.]
 [ 12.   0.   2.  30.   2.]
 [ 25.  23.  16.  17. 133.]]

I - Local maximum validation set accuracy:  57.59

I - Validation set results: 
[14-1-1-1.64][50-3-4-0.44][124-2-2-1.94][127-0-0-2.88][443-2-2-2.38][567-0-4-0.95][573-1-1-2.17][615-0-3-0.48][695-1-2-1.33][722-3-0-2.47]
[826-0-0-1.44][878-0-0-1.68][1103-0-4-1.09][1212-3-4-0.78][1368-0-0-1.39][2181-2-3-0.08][2476-2-2-0.66][2721-2-2-1.56][2818-1-4-0.94][2886-2-1-1.74]
[3231-2-2-1.97][3333-2-2-1.66][3482-2-2-1.47][3536-3-3-0.17][3625-1-1-2.11][3909-0-0-0.38][4035-0-3-1.44][4140-0-4-0.81][4214-1-4-0.65][4346-1-0-0.25]
[4581-2-2-1.21][4708-3-2-1.72][4838-3-4-0.50][4845-1-2-1.22][4868-0-0-2.28][4939-0-4-1.29][4984-2-2-2.15][5078-1-4-1.50][5396-0-0-2.73][5479-1-1-1.33]
[5717-0-0-1.24][5843-1-1-1.63][5949-3-0-1.24][5987-2-4-1.90][6014-3-1-1.56][6033-3-4-0.39][6313-0-3-0.08][6421-3-1-0.28][6500-1-1-1.43][6583-3-2-0.33]
[6683-3-3-0.26][6825-2-1-0.54][6998-3-2-0.51][7049-3-2-0.66][7517-1-2-2.06][7521-1-4--0.32][7528-1-2-1.22][7949-1-2-1.86][8135-1-4-0.29][8185-3-0-1.62]
[8269-3-4-0.93][8273-3-3-1.19][8543-3-0-4.16][8666-1-1-2.14][8672-0-0-2.71][8903-1-2-0.92][9001-2-1-1.84][9036-2-2-2.31][9281-3-4-0.63][9300-2-2-1.76]
[9571-0-4-0.40][9617-1-4-0.66][9644-2-2-2.12][9705-2-4-0.19][9801-0-3-0.44][9803-3-1-0.27][9865-3-3-0.99][9896-2-2-1.03][10314-1-4-1.49][10337-3-3-1.53]
[10403-0-4-1.64][10653-2-4-0.71][10704-2-1-1.09][10719-1-1-1.39][10727-1-4-1.54][10836-0-0-4.47][10969-2-3-0.41][11042-0-0-0.91][11088-1-1-2.62][11322-0-0-2.09]
[11398-2-2-1.44][11499-0-0-0.56][11502-3-3-0.38][11512-3-2-0.03][11608-1-1-2.37][11610-0-3-0.66][11692-0-0-0.83][11905-0-0-2.44][11993-1-1-1.55][12002-2-2-0.14]
[12052-0-0-1.57][12201-0-0-1.48][12235-2-4-1.20][12320-1-4-2.30][12377-2-4-2.43][12398-2-2-0.04][12503-1-2-1.32][12617-0-1-1.10][12685-3-2-0.93][12738-2-2-0.46]
[12742-2-2-2.73][12823-0-0-1.84][13110-1-1-1.60][13240-3-0-0.16][13253-1-4-1.45][13273-0-0-3.34][13634-1-4-1.13][13763-2-2-0.52][13905-3-3--0.22][14060-2-1-1.86]
[14065-3-0-0.78][14147-3-2-0.33][14595-2-2-1.23][14687-2-2-2.53][14788-2-2-2.05][14869-1-2-1.50][14872-3-4-1.14][14877-1-1-1.81][14927-0-3-1.19][15066-0-0-2.89]
[15175-1-4-1.31][15178-2-4--0.02][15375-3-3-0.69][15389-3-3-1.51][15568-2-4-1.06][15675-3-3-0.92][15869-1-2-0.86][16207-3-4-0.24][16236-0-2-0.54][16302-3-4-0.11]
[16331-2-2-3.16][16381-0-4-0.81][16488-1-1-3.19][16495-0-0-1.41][16650-0-0-2.12][16719-1-4-0.82][16801-0-0-3.09][16828-0-0-1.41][17137-3-3-0.85][17245-1-2-1.01]
[17278-3-1--0.10][17282-0-1-0.45][17311-2-2-2.14][17336-2-2-2.46][17608-3-3-1.92][17627-0-0-0.22][17877-3-4-0.84][17924-1-2-0.79][17984-3-0-1.50][18211-0-3-0.27]
[18276-3-0-0.33][18287-1-1-0.29][18394-0-0-1.30][18428-0-0-0.73][18442-0-3-1.19][18478-3-0-1.03][18607-0-4-0.89][18616-0-4-1.40][18663-0-3-0.55][18718-0-0-1.76]
[18766-2-2-1.78][18824-2-4-2.27][18890-3-4-0.53][18930-3-4-1.67][18938-3-2-0.25][19817-1-2-1.60][19839-0-4-0.90][19930-3-3-0.12][19944-0-4-1.53][20036-2-2-2.16]
[20101-3-0--0.08][20474-1-2-1.84][20547-3-4-0.75][20929-2-2-1.99][21245-1-1-1.54][21257-3-2-0.91][21293-1-1-2.70][21316-1-1-2.37][21384-1-4-2.59][21448-1-1-1.38]
[21483-0-0-1.56][21487-2-2-2.01][21714-0-0-0.22][21943-3-2-1.18][21947-0-4-0.92][21948-0-0-3.64][21965-2-2-1.93][21998-1-2-1.40][22025-0-4-0.49][22228-3-3-0.30]
[22446-1-1-1.95][22494-3-0-1.32][22757-0-0-2.25][22811-3-3-2.04][22976-3-2-1.47][22985-3-3-1.01][23014-0-0-2.41][23112-1-1-1.55][23144-3-3-1.05][23168-2-4-0.74]
[23219-0-4-0.49][23363-3-3-0.64][23470-0-4-0.75][23486-2-2-0.79][23497-0-3-2.14][23516-0-0-2.56][23690-1-4-1.35][23921-2-2-1.53][23936-1-2-2.29][24040-3-4-0.45]
[24111-1-4-2.37][24182-0-0-2.84][24238-3-3-1.13][24290-2-4-0.54][24345-0-4-1.46][24364-1-2-1.59][24427-3-0-0.95][24477-2-2-1.83][24495-2-1-1.21][24893-2-2-1.45]
[25012-1-4-0.53][25121-2-4-2.47][25165-3-1-0.56][25183-0-4-1.45][25297-3-3-0.11][25398-0-0-0.58][25574-2-2-2.04][25644-1-1-2.76][25718-1-4-0.70][25774-2-2-1.09]
[26032-3-3-1.31][26051-3-3-1.74][26120-0-4-1.61][26321-1-1-1.51][26732-1-1-2.14][26784-3-3-2.62][26827-3-2--0.09][26833-0-3-2.19][26838-2-2-0.73][26860-1-4-1.11]
[26948-0-0-1.00][27049-3-0-1.01][27098-1-4--0.01][27526-0-0-0.90][27639-3-4--0.08][27698-3-3-0.83][27772-0-0-2.97][27890-1-1-1.35][28040-0-4-1.50][28503-2-2-2.64]
[28577-1-1-1.72][28959-0-0-2.96][29198-3-4-1.43][29777-0-0-3.52][29877-2-2-1.24][30035-1-1-1.54][30098-0-0-0.58][30326-1-1-3.33][30572-2-2-2.11][30716-0-4-2.18]
[30806-2-2-0.46][30906-1-1-1.19][31007-0-4-0.11][31181-3-3-0.24][31238-0-3-0.80][31347-0-0-1.79][31422-2-4-0.65][31429-3-1-0.78][31431-0-0-0.36][31432-1-1-1.53]
[31477-0-0-2.24][31524-1-2-1.83][31597-1-2-2.09][31619-1-2-0.99][31701-0-0-1.23][31755-0-0-1.00][31854-3-3-0.07][32074-1-2-0.59][32078-3-3-0.53][32111-1-1-1.24]
[32127-1-2-2.44][32140-3-3-0.62][32263-2-4-1.27][32365-0-0-0.17][32411-2-0-1.59][32429-3-0-1.33][32473-3-0-0.61][32574-3-0-1.91][32584-0-4-1.78][32622-0-4-0.82]
[32858-3-0-0.94][32969-3-0-1.42][33016-2-2-2.14][33031-1-1-0.27][33035-2-2-2.51][33133-2-2-1.69][33173-2-1-0.55][33175-3-4-1.36][33306-3-2-1.95][33309-2-4-0.27]
[33474-0-4-0.50][33478-2-2-0.12][33618-1-4-0.46][33712-0-4-0.67][33782-2-4-2.47][33914-3-3-0.76][34076-3-1--0.07][34112-2-2-1.11][34138-2-2-1.90][34239-1-2-0.79]
[34364-2-2-2.49][34617-1-4-1.84][34751-3-3-1.77][34783-2-4-1.16][35015-3-2-0.24][35018-1-4-1.68][35288-2-2-0.65][0-4-4-2.55][1-4-4-2.76][2-4-4-1.41]
[3-4-4-2.45][4-4-4-0.78][5-4-1-1.99][6-4-4-2.09][7-4-4-2.35][8-4-2-0.86][9-4-4-1.30][10-4-4-2.41][11-4-2-1.79][12-4-2-0.75]
[14-4-4-0.43][15-4-3-1.00][16-4-4-2.21][17-4-4-0.66][18-4-4-2.92][19-4-0-1.84][20-4-4-0.97][21-4-2-1.58][22-4-4-2.57][23-4-4-1.54]
[24-4-4-2.72][25-4-2-0.33][26-4-4-0.44][27-4-2-0.93][28-4-4-1.69][29-4-1-1.15][30-4-4-0.93][31-4-4-1.72][32-4-4-1.80][33-4-2-1.43]
[34-4-4-0.43][35-4-3-0.25][37-4-2-1.28][39-4-0-1.93][40-4-4-0.80][41-4-2-1.22][42-4-2-0.86][43-4-2-0.53][45-4-2-1.32][46-4-4-2.86]
[47-4-4-3.20][48-4-4-2.33][51-4-4-2.15][52-4-4-1.08][53-4-4-0.91][54-4-0-0.18][55-4-4-1.18][56-4-4-1.10][57-4-4-0.19][58-4-2-2.18]
[59-4-4-1.26][60-4-4-0.63][61-4-4-2.23][62-4-2-0.54][63-4-2-1.80][64-4-4-0.98][65-4-4-2.21][66-4-4-2.07][67-4-2-0.91][68-4-1-1.59]
[69-4-4-0.50][70-4-4-2.18][72-4-4-1.24][73-4-4-1.52][74-4-2-1.29][75-4-4-0.64][77-4-4-2.20][78-4-2-0.82][79-4-4-2.31][80-4-4-2.63]
[81-4-4-2.69][82-4-4-0.89][83-4-4-1.25][84-4-4-1.86][85-4-4-2.47][86-4-4-1.08][87-4-4-2.02][88-4-4-2.01][89-4-2-1.34][90-4-4-0.58]
[91-4-4-1.31][92-4-4-0.49][93-4-4-0.64][94-4-4-2.37][95-4-4-0.83][96-4-4-1.11][97-4-4-2.00][98-4-2-1.86][99-4-4-1.22][100-4-1-1.64]
[101-4-4-3.34][102-4-2-1.36][103-4-4-0.31][104-4-4-2.49][105-4-4-1.36][106-4-4-2.07][107-4-4-1.95][108-4-4-1.01][109-4-4-1.31][110-4-4-1.83]
[111-4-0-2.61][112-4-4-1.29][113-4-4-0.75][114-4-2-0.75][115-4-4-1.45][116-4-4-0.83][117-4-4-1.85][119-4-4-1.25][121-4-4-1.34][122-4-4-1.60]
[124-4-2-1.69][125-4-4-2.14][126-4-4-1.74][127-4-4-1.59][128-4-4-1.00][129-4-1-1.02][130-4-4-1.38][131-4-2-1.28][132-4-4-1.28][133-4-4-3.45]
[135-4-2-1.07][136-4-1-0.60][137-4-4-1.52][138-4-4-0.93][139-4-4-1.61][140-4-1-1.09][141-4-4-0.21][142-4-4-2.46][143-4-4-2.81][144-4-4-2.66]
[145-4-4-1.38][148-4-0-2.37][149-4-4-0.73][150-4-4-2.28][151-4-4-2.00][152-4-4-1.66][153-4-4-1.95][154-4-4-1.33][155-4-4-1.93][156-4-4-0.74]
[157-4-4-0.62][158-4-4-1.22][160-4-4-1.07][161-4-1-1.08][162-4-4-0.83][164-4-4-1.54][165-4-4-1.00][167-4-4-1.45][168-4-4-1.60][170-4-4-0.35]
[171-4-4-1.59][172-4-4-2.91][173-4-4-2.41][174-4-0-1.07][175-4-4-1.51][177-4-4-2.28][178-4-4-1.36][179-4-4-1.77][180-4-4-2.37][181-4-4-0.04]
[182-4-2-0.99][183-4-4-2.04][184-4-4-1.52][186-4-4-1.50][187-4-1-1.49][188-4-4-1.18][189-4-4-1.06][190-4-1-0.64][191-4-4-1.84][192-4-4-1.07]
[193-4-2-2.40][194-4-2-1.58][195-4-4-0.46][196-4-4-0.94][197-4-4-1.68][198-4-4-3.01][199-4-2-0.92]
---------------------------
I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 0.920 | Acc: 63.000% | Wgt Acc: 65.739%
	I - Batch: 100 | Loss: 0.904 | Acc: 63.250% | Wgt Acc: 66.015%
	I - Batch: 150 | Loss: 0.900 | Acc: 63.375% | Wgt Acc: 66.309%
	I - Batch: 200 | Loss: 0.907 | Acc: 63.344% | Wgt Acc: 66.181%
I - num batch: 222
I - Train -- Loss: 0.908 | Acc: 63.349% | Wgt Acc: 66.203% | LR: 5.000000e-05 | Dur: 136.84s
I - Confusion Matrix: [row->prediction - col->label]
[[539.   9.  18. 118. 145.]
 [  6. 359.  85.  23. 103.]
 [ 21. 160. 535.  46. 213.]
 [ 88.  21.  39. 336.  61.]
 [ 43.  29.  57.  15. 478.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.098 | Acc: 58.185% | Wgt Acc: 53.568% | Dur: 14.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   3.   1.  22.  10.]
 [  3.  29.   5.   4.   6.]
 [  1.  21.  48.   7.  29.]
 [ 12.   1.   3.  34.   0.]
 [ 23.  24.  18.  19. 135.]]

I - Local maximum validation set accuracy:  58.19

I - Validation set results: 
[14-1-2-1.49][50-3-4-0.73][124-2-2-1.82][127-0-0-3.45][443-2-2-2.46][567-0-4-0.88][573-1-1-2.27][615-0-3-0.35][695-1-2-0.79][722-3-0-2.14]
[826-0-0-1.48][878-0-0-1.62][1103-0-4-0.96][1212-3-4-0.83][1368-0-0-2.30][2181-2-3-0.21][2476-2-2-0.52][2721-2-2-1.28][2818-1-4-1.16][2886-2-1-1.79]
[3231-2-2-2.41][3333-2-2-1.38][3482-2-2-1.49][3536-3-3-1.02][3625-1-1-2.25][3909-0-0-0.26][4035-0-3-1.68][4140-0-0-1.12][4214-1-4-0.86][4346-1-0-0.35]
[4581-2-2-1.14][4708-3-2-1.54][4838-3-4-0.39][4845-1-2-1.31][4868-0-0-2.86][4939-0-4-0.73][4984-2-2-1.79][5078-1-4-1.65][5396-0-0-2.92][5479-1-1-1.15]
[5717-0-0-1.52][5843-1-1-1.74][5949-3-0-1.31][5987-2-4-1.84][6014-3-1-1.36][6033-3-4-0.56][6313-0-3-0.35][6421-3-3-0.23][6500-1-1-1.26][6583-3-3-0.04]
[6683-3-3-0.83][6825-2-3--0.14][6998-3-2--0.27][7049-3-3-0.11][7517-1-1-2.03][7521-1-4--0.31][7528-1-2-1.17][7949-1-4-2.15][8135-1-0-0.51][8185-3-0-1.87]
[8269-3-4-0.97][8273-3-0-1.78][8543-3-0-4.49][8666-1-1-1.89][8672-0-0-2.36][8903-1-2-1.32][9001-2-1-2.15][9036-2-2-2.46][9281-3-4-0.47][9300-2-2-1.91]
[9571-0-4-0.45][9617-1-1-1.33][9644-2-2-2.18][9705-2-4-0.45][9801-0-3-0.62][9803-3-3-0.14][9865-3-0-1.95][9896-2-4-1.50][10314-1-4-1.66][10337-3-3-1.89]
[10403-0-4-1.71][10653-2-1-0.49][10704-2-2-0.83][10719-1-2-1.73][10727-1-4-1.35][10836-0-0-4.82][10969-2-3-0.63][11042-0-0-0.73][11088-1-2-2.18][11322-0-0-2.23]
[11398-2-2-3.17][11499-0-0-0.36][11502-3-3-0.31][11512-3-2-0.46][11608-1-4-1.58][11610-0-3-0.81][11692-0-0-1.21][11905-0-0-2.89][11993-1-1-1.59][12002-2-4--0.17]
[12052-0-0-2.18][12201-0-0-1.63][12235-2-4-1.35][12320-1-4-2.23][12377-2-4-2.59][12398-2-2--0.13][12503-1-4-0.89][12617-0-1-0.72][12685-3-2-0.44][12738-2-2-0.23]
[12742-2-2-3.16][12823-0-3-1.20][13110-1-1-1.45][13240-3-0-0.48][13253-1-1-1.64][13273-0-0-2.99][13634-1-4-1.21][13763-2-2-0.87][13905-3-4--0.20][14060-2-1-1.72]
[14065-3-0-0.64][14147-3-0-0.08][14595-2-4-1.43][14687-2-2-2.73][14788-2-2-2.24][14869-1-2-1.56][14872-3-4-1.37][14877-1-1-2.07][14927-0-3-1.29][15066-0-0-3.05]
[15175-1-4-1.65][15178-2-4-0.46][15375-3-3--0.23][15389-3-3-1.44][15568-2-4-1.16][15675-3-3-0.62][15869-1-2-0.95][16207-3-0-0.91][16236-0-2-0.31][16302-3-4-0.06]
[16331-2-2-3.03][16381-0-4-0.47][16488-1-1-2.92][16495-0-0-1.75][16650-0-0-1.99][16719-1-1-0.72][16801-0-0-3.44][16828-0-0-1.85][17137-3-3-0.87][17245-1-4-1.23]
[17278-3-1--0.34][17282-0-1-0.19][17311-2-2-2.19][17336-2-2-2.23][17608-3-3-2.25][17627-0-4-0.47][17877-3-4-1.17][17924-1-2-0.41][17984-3-0-2.31][18211-0-1-0.06]
[18276-3-0-0.71][18287-1-4-0.04][18394-0-0-2.19][18428-0-0-3.63][18442-0-3-1.20][18478-3-0-1.17][18607-0-4-0.88][18616-0-4-0.88][18663-0-3-0.47][18718-0-0-2.08]
[18766-2-2-1.61][18824-2-4-2.16][18890-3-4-0.57][18930-3-4-1.76][18938-3-4--0.12][19817-1-2-1.78][19839-0-4-1.04][19930-3-3-0.10][19944-0-4-1.59][20036-2-2-2.24]
[20101-3-3-0.62][20474-1-2-1.53][20547-3-4-0.85][20929-2-2-1.73][21245-1-1-1.79][21257-3-2-1.21][21293-1-1-2.36][21316-1-1-2.17][21384-1-4-2.92][21448-1-2-1.65]
[21483-0-0-2.19][21487-2-2-2.56][21714-0-0-0.17][21943-3-2-1.47][21947-0-0-1.04][21948-0-0-3.79][21965-2-2-2.20][21998-1-1-0.68][22025-0-4-0.57][22228-3-3-0.88]
[22446-1-1-1.96][22494-3-0-1.96][22757-0-0-2.16][22811-3-3-2.18][22976-3-4-1.41][22985-3-3-1.09][23014-0-0-2.42][23112-1-2-1.75][23144-3-3-1.15][23168-2-4-0.48]
[23219-0-4-0.30][23363-3-3-1.41][23470-0-4-0.75][23486-2-2-1.02][23497-0-3-2.52][23516-0-0-2.71][23690-1-4-1.20][23921-2-2-1.84][23936-1-2-2.01][24040-3-0-0.41]
[24111-1-4-2.63][24182-0-0-2.75][24238-3-3-1.22][24290-2-4-0.55][24345-0-0-1.21][24364-1-2-1.60][24427-3-0-0.80][24477-2-2-1.69][24495-2-2-0.68][24893-2-2-1.70]
[25012-1-1-0.80][25121-2-4-2.56][25165-3-1-0.20][25183-0-0-1.50][25297-3-3-0.14][25398-0-0-1.41][25574-2-2-1.69][25644-1-1-1.88][25718-1-4-0.31][25774-2-4-0.87]
[26032-3-3-1.47][26051-3-3-1.75][26120-0-4-1.93][26321-1-1-1.15][26732-1-1-1.74][26784-3-3-2.51][26827-3-3--0.09][26833-0-3-2.49][26838-2-2-0.71][26860-1-4-1.18]
[26948-0-0-1.34][27049-3-0-1.49][27098-1-0-0.26][27526-0-0-1.85][27639-3-3-0.00][27698-3-3-0.90][27772-0-0-3.21][27890-1-1-1.14][28040-0-4-1.10][28503-2-2-2.33]
[28577-1-1-1.90][28959-0-0-3.26][29198-3-4-0.98][29777-0-0-3.83][29877-2-2-1.75][30035-1-2-1.74][30098-0-0-0.98][30326-1-1-3.47][30572-2-2-1.46][30716-0-4-2.18]
[30806-2-2-0.57][30906-1-1-1.99][31007-0-4-1.47][31181-3-3--0.08][31238-0-3-1.08][31347-0-0-2.29][31422-2-4-0.78][31429-3-1-0.22][31431-0-4-0.39][31432-1-1-1.63]
[31477-0-0-2.34][31524-1-4-0.38][31597-1-2-2.10][31619-1-2-0.24][31701-0-0-1.86][31755-0-0-1.88][31854-3-3-0.51][32074-1-4-0.26][32078-3-3-0.94][32111-1-1-1.46]
[32127-1-2-2.15][32140-3-3-0.87][32263-2-4-1.11][32365-0-0--0.27][32411-2-0-2.05][32429-3-0-1.97][32473-3-0-0.65][32574-3-0-2.40][32584-0-4-1.53][32622-0-4-1.19]
[32858-3-0-1.74][32969-3-0-1.59][33016-2-2-1.21][33031-1-3-0.35][33035-2-2-2.55][33133-2-2-1.73][33173-2-2-0.61][33175-3-4-1.26][33306-3-2-2.06][33309-2-4-0.17]
[33474-0-4-0.16][33478-2-2--0.13][33618-1-4-0.55][33712-0-4-0.49][33782-2-4-2.40][33914-3-3-0.69][34076-3-4-0.13][34112-2-2-1.75][34138-2-2-1.89][34239-1-4-0.67]
[34364-2-1-2.01][34617-1-2-1.85][34751-3-3-2.04][34783-2-2-0.97][35015-3-4-0.15][35018-1-4-1.70][35288-2-2-0.71][0-4-4-2.50][1-4-4-2.40][2-4-4-1.90]
[3-4-4-2.59][4-4-4-1.08][5-4-1-1.12][6-4-4-2.05][7-4-4-1.90][8-4-2-0.40][9-4-4-1.19][10-4-4-1.96][11-4-2-2.26][12-4-2-0.78]
[14-4-4-0.52][15-4-0-1.41][16-4-4-2.16][17-4-4-0.20][18-4-4-3.01][19-4-0-1.99][20-4-4-1.08][21-4-2-1.53][22-4-4-2.61][23-4-4-1.19]
[24-4-4-3.48][25-4-4-0.26][26-4-4-0.16][27-4-4-0.55][28-4-4-2.03][29-4-1-0.88][30-4-4-1.12][31-4-4-1.59][32-4-4-1.72][33-4-2-0.76]
[34-4-4-0.48][35-4-0-0.87][37-4-4-1.36][39-4-0-1.00][40-4-4-1.10][41-4-4-0.59][42-4-4-1.17][43-4-4-0.60][45-4-2-1.68][46-4-4-3.03]
[47-4-4-3.38][48-4-4-2.51][51-4-4-2.42][52-4-4-1.53][53-4-4-0.86][54-4-4-0.53][55-4-4-1.06][56-4-4-1.54][57-4-0-1.18][58-4-2-1.92]
[59-4-4-1.08][60-4-4-1.22][61-4-4-2.13][62-4-2-0.36][63-4-4-1.81][64-4-4-1.20][65-4-4-2.89][66-4-4-2.10][67-4-2-0.98][68-4-1-1.01]
[69-4-4-0.87][70-4-4-2.24][72-4-2-1.40][73-4-4-1.56][74-4-2-1.07][75-4-4-0.90][77-4-4-2.40][78-4-4-0.85][79-4-4-2.25][80-4-4-2.12]
[81-4-2-1.20][82-4-2-0.53][83-4-4-1.00][84-4-4-2.10][85-4-4-2.54][86-4-4-1.32][87-4-4-2.35][88-4-4-1.79][89-4-2-0.97][90-4-4-0.97]
[91-4-4-1.32][92-4-4-0.25][93-4-4-1.77][94-4-4-2.42][95-4-4-0.78][96-4-4-1.08][97-4-4-2.32][98-4-2-1.77][99-4-4-1.16][100-4-4-1.43]
[101-4-4-3.66][102-4-4-1.42][103-4-4-0.21][104-4-4-2.14][105-4-4-2.45][106-4-4-2.46][107-4-4-1.65][108-4-4-1.16][109-4-4-1.61][110-4-4-1.24]
[111-4-0-1.92][112-4-4-1.16][113-4-2-0.89][114-4-2-0.39][115-4-4-1.59][116-4-4-1.29][117-4-4-2.00][119-4-2-1.98][121-4-4-2.13][122-4-4-1.62]
[124-4-2-1.58][125-4-4-2.39][126-4-4-2.00][127-4-2-1.79][128-4-4-0.82][129-4-4-0.78][130-4-4-2.18][131-4-2-0.76][132-4-2-0.90][133-4-4-3.43]
[135-4-2-1.06][136-4-4-0.59][137-4-4-1.31][138-4-4-1.10][139-4-4-1.62][140-4-1-1.12][141-4-0-0.36][142-4-4-2.57][143-4-4-2.82][144-4-4-3.04]
[145-4-4-1.25][148-4-0-3.19][149-4-2-0.82][150-4-4-1.86][151-4-4-2.13][152-4-4-1.49][153-4-4-1.99][154-4-4-2.76][155-4-4-2.37][156-4-4-1.01]
[157-4-4-0.77][158-4-4-0.79][160-4-4-1.35][161-4-2-1.13][162-4-4-0.84][164-4-4-1.92][165-4-4-0.94][167-4-4-1.50][168-4-4-1.89][170-4-4-0.39]
[171-4-4-1.60][172-4-4-3.00][173-4-4-2.41][174-4-0-1.92][175-4-4-1.60][177-4-4-1.69][178-4-4-1.32][179-4-4-1.71][180-4-4-2.61][181-4-2-0.90]
[182-4-2-0.96][183-4-4-1.90][184-4-4-1.67][186-4-4-1.24][187-4-1-1.70][188-4-4-1.17][189-4-4-1.35][190-4-1-0.78][191-4-4-1.92][192-4-4-1.91]
[193-4-2-1.86][194-4-2-1.33][195-4-0-0.71][196-4-4-0.97][197-4-4-1.70][198-4-4-3.71][199-4-4-1.23]
---------------------------
I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 0.920 | Acc: 62.125% | Wgt Acc: 65.312%
	I - Batch: 100 | Loss: 0.896 | Acc: 63.688% | Wgt Acc: 66.467%
	I - Batch: 150 | Loss: 0.888 | Acc: 64.292% | Wgt Acc: 67.107%
	I - Batch: 200 | Loss: 0.894 | Acc: 63.500% | Wgt Acc: 66.518%
I - num batch: 222
I - Train -- Loss: 0.901 | Acc: 62.785% | Wgt Acc: 65.816% | LR: 5.000000e-05 | Dur: 133.62s
I - Confusion Matrix: [row->prediction - col->label]
[[542.   9.  17. 118. 151.]
 [  6. 357.  87.  22. 104.]
 [ 17. 153. 524.  48. 200.]
 [ 85.  31.  44. 340.  81.]
 [ 47.  28.  62.  10. 464.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.138 | Acc: 56.607% | Wgt Acc: 51.290% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   2.   2.  18.   7.]
 [  2.  32.   8.   6.   9.]
 [  2.  27.  46.  13.  28.]
 [  9.   1.   1.  22.   0.]
 [ 24.  16.  18.  27. 136.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 0.875 | Acc: 66.250% | Wgt Acc: 69.764%
	I - Batch: 100 | Loss: 0.868 | Acc: 65.812% | Wgt Acc: 68.684%
	I - Batch: 150 | Loss: 0.885 | Acc: 64.167% | Wgt Acc: 67.059%
	I - Batch: 200 | Loss: 0.881 | Acc: 64.406% | Wgt Acc: 67.036%
I - num batch: 222
I - Train -- Loss: 0.878 | Acc: 64.562% | Wgt Acc: 67.163% | LR: 5.000000e-05 | Dur: 136.30s
I - Confusion Matrix: [row->prediction - col->label]
[[534.  14.  19. 110. 143.]
 [  7. 372.  81.  25.  92.]
 [ 13. 143. 539.  47. 201.]
 [ 91.  19.  42. 341.  60.]
 [ 52.  30.  53.  15. 504.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.113 | Acc: 58.185% | Wgt Acc: 53.035% | Dur: 14.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   2.   3.  20.  10.]
 [  1.  25.   4.   2.   8.]
 [  3.  31.  50.  17.  24.]
 [  8.   1.   2.  30.   0.]
 [ 24.  19.  16.  17. 138.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 0.850 | Acc: 65.750% | Wgt Acc: 68.895%
	I - Batch: 100 | Loss: 0.857 | Acc: 65.312% | Wgt Acc: 68.431%
	I - Batch: 150 | Loss: 0.854 | Acc: 65.417% | Wgt Acc: 68.596%
	I - Batch: 200 | Loss: 0.861 | Acc: 65.312% | Wgt Acc: 68.310%
I - num batch: 222
I - Train -- Loss: 0.859 | Acc: 65.182% | Wgt Acc: 68.249% | LR: 5.000000e-05 | Dur: 136.38s
I - Confusion Matrix: [row->prediction - col->label]
[[552.  10.  14. 109. 161.]
 [  5. 374.  75.  19. 101.]
 [ 17. 133. 556.  51. 195.]
 [ 76.  25.  34. 345.  58.]
 [ 47.  36.  55.  14. 485.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.076 | Acc: 59.369% | Wgt Acc: 57.747% | Dur: 14.63s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   1.   4.  13.   9.]
 [  3.  33.   5.   5.  13.]
 [  2.  27.  53.  10.  35.]
 [ 18.   4.   4.  46.   6.]
 [ 13.  13.   9.  12. 117.]]

I - Local maximum validation set accuracy:  59.37

I - Validation set results: 
[14-1-2-1.63][50-3-1--0.21][124-2-2-1.87][127-0-0-3.15][443-2-2-2.65][567-0-0-1.08][573-1-1-2.51][615-0-3-0.74][695-1-2-1.36][722-3-0-2.59]
[826-0-0-1.94][878-0-0-2.42][1103-0-4-0.50][1212-3-3-0.27][1368-0-0-1.99][2181-2-3-0.47][2476-2-2-0.30][2721-2-2-1.87][2818-1-2-0.93][2886-2-1-1.79]
[3231-2-2-2.27][3333-2-2-1.39][3482-2-2-1.34][3536-3-3-0.78][3625-1-1-2.52][3909-0-0-1.52][4035-0-3-1.89][4140-0-0-1.79][4214-1-1-0.63][4346-1-3-0.23]
[4581-2-2-1.15][4708-3-2-0.89][4838-3-4--0.02][4845-1-2-0.84][4868-0-0-2.41][4939-0-1-0.93][4984-2-2-2.25][5078-1-4-0.66][5396-0-0-2.76][5479-1-2-0.83]
[5717-0-0-1.20][5843-1-1-2.04][5949-3-0-1.37][5987-2-4-1.51][6014-3-1-0.70][6033-3-0-1.03][6313-0-3-0.57][6421-3-3-1.03][6500-1-1-1.24][6583-3-2-0.93]
[6683-3-3-0.81][6825-2-3-0.12][6998-3-3-0.70][7049-3-3-0.70][7517-1-2-2.04][7521-1-3--0.32][7528-1-2-1.46][7949-1-2-1.84][8135-1-0-2.35][8185-3-0-1.73]
[8269-3-4-0.56][8273-3-3-1.39][8543-3-0-4.26][8666-1-1-1.82][8672-0-0-2.91][8903-1-2-1.62][9001-2-1-0.95][9036-2-2-2.49][9281-3-1-0.05][9300-2-2-1.83]
[9571-0-3-0.10][9617-1-1-0.75][9644-2-2-2.11][9705-2-4-0.58][9801-0-3-1.30][9803-3-3-0.61][9865-3-3-1.82][9896-2-2-1.13][10314-1-4-1.21][10337-3-3-2.45]
[10403-0-4-1.24][10653-2-1-0.54][10704-2-2-0.83][10719-1-1-1.46][10727-1-4-1.43][10836-0-0-4.30][10969-2-3-0.89][11042-0-0-1.68][11088-1-1-2.84][11322-0-0-2.39]
[11398-2-2-3.42][11499-0-0-1.02][11502-3-3-0.80][11512-3-3-0.52][11608-1-1-1.39][11610-0-3-0.95][11692-0-0-1.07][11905-0-0-3.13][11993-1-2-1.30][12002-2-0-2.21]
[12052-0-0-1.65][12201-0-0-1.57][12235-2-2-0.95][12320-1-4-1.64][12377-2-4-1.96][12398-2-3-0.23][12503-1-1-1.35][12617-0-1-1.19][12685-3-2-0.74][12738-2-2-0.09]
[12742-2-2-3.47][12823-0-3-1.65][13110-1-1-2.22][13240-3-3-0.64][13253-1-1-1.55][13273-0-0-3.35][13634-1-4-1.22][13763-2-2-0.55][13905-3-3-0.07][14060-2-1-1.39]
[14065-3-3-0.65][14147-3-3-0.12][14595-2-2-1.53][14687-2-2-2.76][14788-2-2-1.92][14869-1-1-1.42][14872-3-4-1.05][14877-1-1-2.46][14927-0-3-1.54][15066-0-0-2.86]
[15175-1-4-0.87][15178-2-4-0.03][15375-3-3--0.16][15389-3-3-1.85][15568-2-1-1.31][15675-3-3-1.61][15869-1-2-0.35][16207-3-0-1.10][16236-0-2-0.47][16302-3-4-0.20]
[16331-2-2-3.21][16381-0-0-0.18][16488-1-1-1.29][16495-0-0-2.79][16650-0-0-2.52][16719-1-1-0.59][16801-0-0-3.63][16828-0-0-1.49][17137-3-3-0.77][17245-1-2-1.16]
[17278-3-1--0.08][17282-0-1-0.57][17311-2-2-2.26][17336-2-2-1.52][17608-3-3-2.74][17627-0-4-0.46][17877-3-4-1.32][17924-1-2-0.76][17984-3-3-2.04][18211-0-3-1.50]
[18276-3-3-1.24][18287-1-1-1.16][18394-0-0-1.79][18428-0-0-3.25][18442-0-3-1.35][18478-3-0-1.23][18607-0-0-1.12][18616-0-4-0.38][18663-0-3-0.95][18718-0-0-1.75]
[18766-2-2-2.31][18824-2-4-1.46][18890-3-2-1.04][18930-3-4-1.39][18938-3-2-0.29][19817-1-2-1.83][19839-0-4-1.10][19930-3-3-0.56][19944-0-4-1.51][20036-2-2-2.40]
[20101-3-3-0.83][20474-1-2-1.33][20547-3-4-0.18][20929-2-2-2.29][21245-1-1-1.18][21257-3-4-0.36][21293-1-1-2.28][21316-1-1-1.88][21384-1-4-2.54][21448-1-2-1.30]
[21483-0-0-1.68][21487-2-2-2.51][21714-0-0-0.27][21943-3-2-1.91][21947-0-0-2.22][21948-0-0-4.17][21965-2-2-2.42][21998-1-2-0.58][22025-0-2-0.17][22228-3-3-1.35]
[22446-1-1-2.07][22494-3-0-1.14][22757-0-0-2.30][22811-3-3-1.33][22976-3-2-0.99][22985-3-3-1.80][23014-0-3-2.05][23112-1-1-2.04][23144-3-3-1.67][23168-2-0-0.92]
[23219-0-3--0.08][23363-3-3-1.92][23470-0-4--0.22][23486-2-2-1.12][23497-0-3-2.68][23516-0-0-2.84][23690-1-2-0.99][23921-2-2-1.74][23936-1-2-1.93][24040-3-4-1.08]
[24111-1-4-2.10][24182-0-3-2.26][24238-3-3-1.53][24290-2-0-0.59][24345-0-0-1.04][24364-1-2-1.56][24427-3-0-1.21][24477-2-2-1.63][24495-2-2-0.84][24893-2-2-1.57]
[25012-1-1-0.72][25121-2-4-1.69][25165-3-1--0.06][25183-0-0-3.11][25297-3-3-1.00][25398-0-0-1.05][25574-2-2-1.55][25644-1-1-3.05][25718-1-4--0.04][25774-2-2-0.74]
[26032-3-3-1.23][26051-3-3-2.17][26120-0-4-1.29][26321-1-2-0.94][26732-1-1-2.01][26784-3-3-2.93][26827-3-3-0.34][26833-0-3-2.63][26838-2-2-0.75][26860-1-4-1.21]
[26948-0-0-1.08][27049-3-0-1.66][27098-1-4-0.50][27526-0-0-1.46][27639-3-3-0.79][27698-3-3-1.17][27772-0-0-2.39][27890-1-1-1.49][28040-0-4-1.57][28503-2-2-2.25]
[28577-1-1-1.63][28959-0-0-2.86][29198-3-4-0.51][29777-0-0-3.23][29877-2-2-1.58][30035-1-2-1.95][30098-0-0-1.22][30326-1-1-3.91][30572-2-2-1.55][30716-0-4-1.37]
[30806-2-2-0.60][30906-1-1-1.48][31007-0-4-1.15][31181-3-3-0.54][31238-0-3-1.34][31347-0-0-1.54][31422-2-4-0.59][31429-3-2-0.20][31431-0-0-0.28][31432-1-1-1.63]
[31477-0-3-2.27][31524-1-2-0.62][31597-1-2-2.13][31619-1-2-0.90][31701-0-0-2.55][31755-0-0-1.64][31854-3-3-0.71][32074-1-3-0.01][32078-3-3-0.69][32111-1-1-1.60]
[32127-1-2-2.26][32140-3-3-1.28][32263-2-4-0.58][32365-0-0-0.13][32411-2-0-1.93][32429-3-0-1.24][32473-3-3-0.53][32574-3-3-2.07][32584-0-4-1.16][32622-0-4-0.87]
[32858-3-0-0.92][32969-3-0-1.85][33016-2-2-1.93][33031-1-3-0.90][33035-2-2-2.22][33133-2-2-1.79][33173-2-2-0.86][33175-3-4-1.37][33306-3-2-2.01][33309-2-2-0.09]
[33474-0-0-0.22][33478-2-2-0.27][33618-1-1-0.39][33712-0-0-0.57][33782-2-4-2.00][33914-3-3-1.35][34076-3-4-0.33][34112-2-2-0.62][34138-2-2-2.10][34239-1-2-0.53]
[34364-2-2-2.63][34617-1-4-1.15][34751-3-3-2.14][34783-2-2-1.08][35015-3-2-0.21][35018-1-4-1.10][35288-2-2-1.19][0-4-4-1.21][1-4-4-2.54][2-4-4-1.46]
[3-4-4-1.51][4-4-4-0.44][5-4-1-2.18][6-4-4-0.86][7-4-4-1.51][8-4-2-0.91][9-4-2-0.95][10-4-4-2.44][11-4-4-1.82][12-4-2-0.99]
[14-4-4-0.48][15-4-3-1.50][16-4-4-1.17][17-4-4-0.45][18-4-4-2.09][19-4-3-0.87][20-4-4-0.69][21-4-2-1.92][22-4-4-1.65][23-4-4-0.65]
[24-4-4-3.01][25-4-3-0.15][26-4-4--0.04][27-4-2-0.42][28-4-4-1.71][29-4-2-1.32][30-4-1-0.02][31-4-4-1.58][32-4-1-1.35][33-4-2-0.66]
[34-4-4-0.46][35-4-0-0.72][37-4-4-0.84][39-4-0-2.38][40-4-4-0.78][41-4-2-0.34][42-4-4-1.25][43-4-4-0.45][45-4-2-2.10][46-4-4-2.43]
[47-4-4-2.84][48-4-4-2.00][51-4-4-1.60][52-4-4-1.12][53-4-4-0.37][54-4-4--0.03][55-4-2-0.74][56-4-1-0.87][57-4-0-1.36][58-4-2-2.27]
[59-4-0-1.23][60-4-4-1.05][61-4-4-2.11][62-4-2-0.72][63-4-2-1.61][64-4-4-1.01][65-4-4-1.74][66-4-4-1.79][67-4-2-1.29][68-4-2-0.84]
[69-4-2-0.44][70-4-4-2.07][72-4-1-1.12][73-4-1-1.98][74-4-2-1.74][75-4-4-0.20][77-4-4-2.09][78-4-4-0.52][79-4-4-1.89][80-4-4-1.97]
[81-4-4-2.18][82-4-4-0.40][83-4-4-0.92][84-4-4-2.21][85-4-4-2.19][86-4-4-1.21][87-4-4-1.79][88-4-4-0.95][89-4-4-1.19][90-4-4-0.27]
[91-4-4-1.24][92-4-0-0.26][93-4-4-1.16][94-4-4-1.84][95-4-4-0.98][96-4-4-0.93][97-4-4-1.85][98-4-2-2.09][99-4-4-0.83][100-4-1-1.37]
[101-4-4-3.47][102-4-4-0.97][103-4-4-0.00][104-4-1-1.22][105-4-4-1.41][106-4-4-1.93][107-4-4-1.33][108-4-4-0.65][109-4-4-0.77][110-4-2-0.87]
[111-4-0-2.93][112-4-2-0.40][113-4-2-0.22][114-4-2-0.56][115-4-4-0.87][116-4-4-1.27][117-4-4-1.45][119-4-2-1.53][121-4-1-0.99][122-4-4-1.35]
[124-4-2-1.18][125-4-4-1.82][126-4-4-2.65][127-4-2-1.11][128-4-4-1.05][129-4-1-0.61][130-4-4-1.25][131-4-2-1.27][132-4-4-0.91][133-4-4-2.70]
[135-4-4-0.63][136-4-4-0.62][137-4-4-0.66][138-4-4-0.86][139-4-4-1.42][140-4-2-0.42][141-4-3-0.55][142-4-4-1.94][143-4-4-2.11][144-4-4-2.04]
[145-4-1-0.98][148-4-0-2.61][149-4-4-0.63][150-4-4-2.30][151-4-4-1.99][152-4-4-1.32][153-4-2-1.36][154-4-4-1.98][155-4-4-1.93][156-4-4-0.38]
[157-4-4-0.41][158-4-4-0.86][160-4-1-0.64][161-4-2-1.09][162-4-4-0.90][164-4-4-1.39][165-4-4-0.54][167-4-4-1.06][168-4-4-1.05][170-4-4-0.45]
[171-4-4-1.27][172-4-4-2.22][173-4-4-1.67][174-4-0-1.01][175-4-4-1.51][177-4-4-1.97][178-4-4-1.31][179-4-4-1.43][180-4-4-1.82][181-4-3-0.10]
[182-4-2-0.78][183-4-4-1.30][184-4-4-2.10][186-4-4-0.29][187-4-1-2.01][188-4-4-1.47][189-4-4-1.23][190-4-2-0.40][191-4-4-2.10][192-4-4-1.51]
[193-4-2-2.58][194-4-3-0.13][195-4-0-1.54][196-4-2-0.64][197-4-2-1.78][198-4-4-2.99][199-4-2-1.18]
---------------------------
I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 0.845 | Acc: 64.625% | Wgt Acc: 68.123%
	I - Batch: 100 | Loss: 0.837 | Acc: 66.750% | Wgt Acc: 70.043%
	I - Batch: 150 | Loss: 0.857 | Acc: 65.167% | Wgt Acc: 68.427%
	I - Batch: 200 | Loss: 0.854 | Acc: 65.375% | Wgt Acc: 68.719%
I - num batch: 222
I - Train -- Loss: 0.854 | Acc: 65.351% | Wgt Acc: 68.650% | LR: 5.000000e-05 | Dur: 134.09s
I - Confusion Matrix: [row->prediction - col->label]
[[564.  14.  17. 101. 150.]
 [  7. 383.  72.  26.  89.]
 [  9. 134. 549.  55. 222.]
 [ 82.  23.  42. 347.  64.]
 [ 35.  24.  54.   9. 475.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.157 | Acc: 56.607% | Wgt Acc: 51.353% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   1.   2.  19.   8.]
 [  4.  36.  10.  10.  10.]
 [  4.  19.  46.  14.  25.]
 [ 10.   1.   1.  23.   1.]
 [ 24.  21.  16.  20. 136.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 0.826 | Acc: 68.250% | Wgt Acc: 71.399%
	I - Batch: 100 | Loss: 0.823 | Acc: 67.688% | Wgt Acc: 70.856%
	I - Batch: 150 | Loss: 0.840 | Acc: 66.417% | Wgt Acc: 69.476%
	I - Batch: 200 | Loss: 0.843 | Acc: 66.031% | Wgt Acc: 69.091%
I - num batch: 222
I - Train -- Loss: 0.843 | Acc: 66.056% | Wgt Acc: 69.134% | LR: 5.000000e-05 | Dur: 135.56s
I - Confusion Matrix: [row->prediction - col->label]
[[550.   7.  12. 102. 151.]
 [ 11. 397.  74.  31. 110.]
 [ 13. 126. 556.  45. 191.]
 [ 79.  20.  34. 346.  54.]
 [ 44.  28.  58.  14. 494.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 59.172% | Wgt Acc: 53.140% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   3.  23.  12.]
 [  0.  24.   3.   3.   3.]
 [  1.  28.  43.   9.  19.]
 [  9.   1.   2.  30.   0.]
 [ 21.  23.  24.  21. 146.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 0.823 | Acc: 69.750% | Wgt Acc: 72.235%
	I - Batch: 100 | Loss: 0.824 | Acc: 68.250% | Wgt Acc: 71.015%
	I - Batch: 150 | Loss: 0.816 | Acc: 68.000% | Wgt Acc: 71.014%
	I - Batch: 200 | Loss: 0.821 | Acc: 67.031% | Wgt Acc: 69.876%
I - num batch: 222
I - Train -- Loss: 0.825 | Acc: 66.732% | Wgt Acc: 69.448% | LR: 5.000000e-05 | Dur: 136.47s
I - Confusion Matrix: [row->prediction - col->label]
[[562.   8.  10. 110. 130.]
 [  7. 386.  71.  33.  96.]
 [ 10. 135. 559.  42. 184.]
 [ 73.  16.  40. 342.  72.]
 [ 45.  33.  54.  11. 518.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.130 | Acc: 56.410% | Wgt Acc: 50.319% | Dur: 15.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 42.   1.   0.  15.   6.]
 [  0.  22.   3.   2.   4.]
 [  2.  25.  49.  14.  26.]
 [ 12.   1.   3.  31.   2.]
 [ 32.  29.  20.  24. 142.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 0.788 | Acc: 66.125% | Wgt Acc: 69.242%
	I - Batch: 100 | Loss: 0.808 | Acc: 66.750% | Wgt Acc: 69.615%
	I - Batch: 150 | Loss: 0.811 | Acc: 67.250% | Wgt Acc: 70.218%
	I - Batch: 200 | Loss: 0.815 | Acc: 67.688% | Wgt Acc: 70.614%
I - num batch: 222
I - Train -- Loss: 0.817 | Acc: 67.550% | Wgt Acc: 70.577% | LR: 5.000000e-05 | Dur: 133.28s
I - Confusion Matrix: [row->prediction - col->label]
[[545.   6.  16.  99. 141.]
 [ 12. 414.  67.  17. 106.]
 [ 19. 107. 558.  45. 180.]
 [ 79.  23.  40. 365.  59.]
 [ 42.  28.  53.  12. 514.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.115 | Acc: 57.791% | Wgt Acc: 53.328% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   0.   1.  15.   7.]
 [  0.  24.   4.   2.   7.]
 [  3.  29.  55.  18.  31.]
 [ 14.   1.   2.  35.   2.]
 [ 25.  24.  13.  16. 133.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.799 | Acc: 69.750% | Wgt Acc: 73.621%
	I - Batch: 100 | Loss: 0.803 | Acc: 67.938% | Wgt Acc: 71.426%
	I - Batch: 150 | Loss: 0.798 | Acc: 68.375% | Wgt Acc: 71.808%
	I - Batch: 200 | Loss: 0.803 | Acc: 68.406% | Wgt Acc: 71.579%
I - num batch: 222
I - Train -- Loss: 0.796 | Acc: 68.565% | Wgt Acc: 71.752% | LR: 2.500000e-05 | Dur: 136.02s
I - Confusion Matrix: [row->prediction - col->label]
[[562.   3.  11.  96. 159.]
 [  4. 412.  65.  24.  76.]
 [ 17. 120. 572.  38. 183.]
 [ 72.  18.  41. 371.  67.]
 [ 42.  25.  45.   9. 515.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.066 | Acc: 61.341% | Wgt Acc: 56.964% | Dur: 22.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   3.  23.  11.]
 [  1.  30.   5.   1.   8.]
 [  0.  23.  44.   5.  19.]
 [ 11.   2.   8.  38.   3.]
 [ 16.  20.  15.  19. 139.]]

I - Local maximum validation set accuracy:  61.34

I - Validation set results: 
[14-1-2-1.15][50-3-4-0.56][124-2-2-1.45][127-0-0-3.29][443-2-2-2.70][567-0-0-1.30][573-1-1-1.26][615-0-3-1.16][695-1-2-0.98][722-3-0-3.17]
[826-0-0-2.09][878-0-0-1.78][1103-0-0-0.94][1212-3-4-0.45][1368-0-0-2.65][2181-2-3-0.57][2476-2-2-0.07][2721-2-2-1.49][2818-1-4-0.36][2886-2-1-1.59]
[3231-2-2-2.95][3333-2-2-1.32][3482-2-2-1.41][3536-3-3-1.24][3625-1-1-2.64][3909-0-0-1.13][4035-0-3-1.79][4140-0-0-1.79][4214-1-4-0.69][4346-1-3-0.48]
[4581-2-2-1.83][4708-3-2-1.47][4838-3-4-0.14][4845-1-2-0.96][4868-0-0-2.87][4939-0-4-0.78][4984-2-2-2.18][5078-1-4-1.16][5396-0-0-3.59][5479-1-2-0.58]
[5717-0-0-2.71][5843-1-1-1.25][5949-3-0-1.55][5987-2-4-1.95][6014-3-1-1.28][6033-3-0-0.30][6313-0-3-0.63][6421-3-3-1.19][6500-1-1-0.79][6583-3-3-0.33]
[6683-3-3-0.91][6825-2-3-0.20][6998-3-3-0.27][7049-3-2--0.08][7517-1-2-2.29][7521-1-0--0.16][7528-1-2-1.79][7949-1-2-2.02][8135-1-0-2.57][8185-3-0-3.21]
[8269-3-4-1.00][8273-3-3-1.74][8543-3-0-4.06][8666-1-1-0.85][8672-0-0-1.76][8903-1-2-0.85][9001-2-1-2.07][9036-2-2-3.09][9281-3-4-0.42][9300-2-2-2.43]
[9571-0-0-0.36][9617-1-1-0.39][9644-2-2-2.22][9705-2-4-0.34][9801-0-0-1.41][9803-3-3-0.54][9865-3-3-2.12][9896-2-2-0.89][10314-1-4-1.55][10337-3-3-2.57]
[10403-0-4-1.21][10653-2-1-0.31][10704-2-2-0.82][10719-1-1-1.46][10727-1-4-1.27][10836-0-0-5.80][10969-2-3-1.16][11042-0-0-1.46][11088-1-1-3.19][11322-0-0-2.83]
[11398-2-2-4.21][11499-0-0-1.42][11502-3-3-1.22][11512-3-3-0.73][11608-1-1-1.92][11610-0-0-1.17][11692-0-0-1.74][11905-0-0-2.94][11993-1-1-1.70][12002-2-3-0.34]
[12052-0-0-2.75][12201-0-0-1.87][12235-2-4-1.42][12320-1-4-2.65][12377-2-4-2.04][12398-2-3-0.10][12503-1-1-2.18][12617-0-1-0.50][12685-3-2-0.44][12738-2-0-0.44]
[12742-2-2-3.47][12823-0-3-1.51][13110-1-1-1.36][13240-3-0-1.36][13253-1-1-1.41][13273-0-0-4.15][13634-1-4-1.12][13763-2-3-0.71][13905-3-4--0.40][14060-2-4-0.91]
[14065-3-0-1.20][14147-3-0-1.29][14595-2-2-0.99][14687-2-2-3.00][14788-2-2-1.87][14869-1-1-1.43][14872-3-4-1.54][14877-1-1-2.75][14927-0-3-1.39][15066-0-0-3.19]
[15175-1-4-1.41][15178-2-3-0.20][15375-3-0-1.82][15389-3-3-1.87][15568-2-1-1.02][15675-3-3-2.23][15869-1-2-0.25][16207-3-0-0.95][16236-0-4-0.00][16302-3-4-0.19]
[16331-2-2-3.46][16381-0-0-0.35][16488-1-1-3.43][16495-0-0-3.14][16650-0-0-3.42][16719-1-2-0.39][16801-0-0-3.95][16828-0-0-1.67][17137-3-3-1.09][17245-1-4-1.31]
[17278-3-3-0.01][17282-0-0--0.28][17311-2-2-2.11][17336-2-1-1.07][17608-3-3-2.40][17627-0-4-0.10][17877-3-4-1.56][17924-1-4-0.13][17984-3-0-2.49][18211-0-3-1.44]
[18276-3-3-1.23][18287-1-4--0.09][18394-0-0-2.77][18428-0-0-3.23][18442-0-3-1.58][18478-3-0-1.47][18607-0-0-1.36][18616-0-4-1.12][18663-0-0-1.14][18718-0-0-2.77]
[18766-2-2-1.74][18824-2-4-2.28][18890-3-2-0.98][18930-3-4-1.79][18938-3-3-0.06][19817-1-2-1.72][19839-0-0-0.69][19930-3-3-0.92][19944-0-4-1.73][20036-2-2-2.76]
[20101-3-3-1.07][20474-1-2-1.28][20547-3-0-0.48][20929-2-2-2.38][21245-1-1-0.80][21257-3-4-0.66][21293-1-1-2.04][21316-1-1-1.27][21384-1-4-2.57][21448-1-2-1.67]
[21483-0-0-2.55][21487-2-2-2.50][21714-0-0-0.36][21943-3-4-1.11][21947-0-0-2.77][21948-0-0-4.47][21965-2-2-2.35][21998-1-1-1.40][22025-0-4-0.93][22228-3-3-1.74]
[22446-1-1-1.39][22494-3-0-1.87][22757-0-0-2.48][22811-3-3-1.99][22976-3-4-1.02][22985-3-3-1.88][23014-0-0-2.95][23112-1-1-1.92][23144-3-3-1.70][23168-2-0-0.11]
[23219-0-4-0.22][23363-3-3-2.00][23470-0-4-0.64][23486-2-4-0.57][23497-0-3-2.29][23516-0-0-3.01][23690-1-4-1.34][23921-2-2-1.31][23936-1-2-1.91][24040-3-0-1.32]
[24111-1-4-2.26][24182-0-0-3.86][24238-3-3-1.51][24290-2-4-0.79][24345-0-0-1.57][24364-1-2-1.20][24427-3-0-1.88][24477-2-2-1.68][24495-2-4-0.55][24893-2-2-1.40]
[25012-1-2-0.05][25121-2-4-2.11][25165-3-3--0.27][25183-0-0-2.05][25297-3-3-1.52][25398-0-0-2.13][25574-2-2-1.96][25644-1-1-1.31][25718-1-4-0.33][25774-2-4-0.51]
[26032-3-0-1.55][26051-3-3-2.18][26120-0-4-2.29][26321-1-2-0.59][26732-1-1-1.90][26784-3-3-3.19][26827-3-4-0.25][26833-0-3-2.40][26838-2-2-0.70][26860-1-4-0.89]
[26948-0-0-2.05][27049-3-0-2.21][27098-1-0-1.37][27526-0-0-1.95][27639-3-3-0.75][27698-3-3-1.25][27772-0-0-2.57][27890-1-1-1.07][28040-0-4-1.78][28503-2-2-2.72]
[28577-1-1-1.57][28959-0-0-2.91][29198-3-4-1.30][29777-0-0-3.99][29877-2-2-0.97][30035-1-2-1.49][30098-0-0-1.55][30326-1-1-3.80][30572-2-2-1.59][30716-0-4-1.87]
[30806-2-2-0.40][30906-1-1-1.20][31007-0-0-1.27][31181-3-3-0.54][31238-0-3-1.55][31347-0-0-3.03][31422-2-4-0.56][31429-3-3-0.19][31431-0-0-1.30][31432-1-1-1.48]
[31477-0-3-2.29][31524-1-2-1.78][31597-1-2-2.25][31619-1-2-0.85][31701-0-0-2.54][31755-0-0-2.23][31854-3-3-1.34][32074-1-2-1.05][32078-3-3-1.23][32111-1-1-1.13]
[32127-1-2-2.49][32140-3-3-1.35][32263-2-4-0.69][32365-0-0-0.57][32411-2-0-2.53][32429-3-0-2.20][32473-3-0-0.98][32574-3-0-3.13][32584-0-4-1.99][32622-0-4-1.07]
[32858-3-0-1.78][32969-3-0-2.32][33016-2-2-0.85][33031-1-3-0.84][33035-2-2-2.34][33133-2-2-1.64][33173-2-2-0.41][33175-3-4-1.07][33306-3-2-1.50][33309-2-4-0.05]
[33474-0-4-0.22][33478-2-3--0.22][33618-1-4-0.46][33712-0-4-0.50][33782-2-4-2.00][33914-3-4-0.75][34076-3-4-0.67][34112-2-2-1.10][34138-2-2-1.81][34239-1-4-0.74]
[34364-2-2-2.48][34617-1-4-2.08][34751-3-3-2.63][34783-2-2-1.08][35015-3-4-0.34][35018-1-4-1.45][35288-2-2-0.79][0-4-4-1.70][1-4-4-3.26][2-4-4-1.26]
[3-4-4-2.25][4-4-4-0.45][5-4-1-0.08][6-4-4-3.09][7-4-4-3.10][8-4-4-0.23][9-4-2-1.15][10-4-4-3.02][11-4-4-1.80][12-4-2-1.23]
[14-4-4-0.84][15-4-0-1.60][16-4-4-1.84][17-4-4-0.43][18-4-4-2.75][19-4-0-2.79][20-4-4-1.02][21-4-4-0.92][22-4-4-2.45][23-4-4-1.36]
[24-4-4-3.65][25-4-3-0.30][26-4-3-0.29][27-4-2-0.61][28-4-4-1.94][29-4-1-0.93][30-4-4--0.18][31-4-4-1.72][32-4-1-1.33][33-4-2-0.45]
[34-4-4-0.75][35-4-0-1.51][37-4-4-1.51][39-4-0-2.54][40-4-4-0.98][41-4-4-0.71][42-4-4-1.08][43-4-4-1.03][45-4-1-0.10][46-4-4-3.36]
[47-4-4-3.67][48-4-4-2.03][51-4-4-2.50][52-4-4-1.53][53-4-4-0.85][54-4-4-0.19][55-4-4-1.27][56-4-4-1.22][57-4-0-1.49][58-4-2-2.01]
[59-4-0-1.37][60-4-4-0.42][61-4-4-2.37][62-4-4-0.53][63-4-4-0.95][64-4-4-0.63][65-4-4-2.28][66-4-4-2.39][67-4-4-0.74][68-4-2-0.65]
[69-4-4-0.82][70-4-4-2.44][72-4-1-1.18][73-4-1-1.31][74-4-2-1.16][75-4-4-0.24][77-4-4-2.91][78-4-2-0.61][79-4-4-2.07][80-4-4-2.67]
[81-4-4-1.76][82-4-4-0.60][83-4-4-1.19][84-4-4-2.21][85-4-4-2.21][86-4-4-1.21][87-4-4-1.87][88-4-4-1.34][89-4-4-1.00][90-4-4-0.62]
[91-4-4-1.73][92-4-4-0.77][93-4-0-1.76][94-4-4-2.69][95-4-4-1.14][96-4-4-0.98][97-4-4-2.64][98-4-2-1.93][99-4-4-0.43][100-4-4-1.07]
[101-4-4-3.92][102-4-4-1.37][103-4-4-0.11][104-4-4-1.90][105-4-2-1.28][106-4-4-2.03][107-4-4-1.84][108-4-4-1.30][109-4-4-1.51][110-4-4-1.06]
[111-4-0-2.30][112-4-4-0.66][113-4-4-0.58][114-4-2-0.54][115-4-4-1.42][116-4-4-1.12][117-4-4-1.87][119-4-2-2.18][121-4-4-1.51][122-4-4-1.69]
[124-4-2-1.10][125-4-4-2.46][126-4-4-2.57][127-4-2-1.73][128-4-4-0.78][129-4-4-1.31][130-4-4-2.01][131-4-2-1.21][132-4-4-0.73][133-4-4-3.65]
[135-4-4-0.88][136-4-4-0.59][137-4-4-0.65][138-4-4-1.13][139-4-4-1.55][140-4-1-1.01][141-4-3-0.53][142-4-4-2.70][143-4-4-2.80][144-4-4-2.16]
[145-4-4-1.17][148-4-0-3.55][149-4-4-0.72][150-4-4-2.67][151-4-4-2.41][152-4-4-1.49][153-4-4-2.13][154-4-4-2.49][155-4-4-1.70][156-4-4-0.59]
[157-4-0-1.60][158-4-4-1.34][160-4-4-1.16][161-4-2-0.92][162-4-4-1.16][164-4-4-1.51][165-4-4-0.60][167-4-4-1.41][168-4-4-1.25][170-4-4-0.96]
[171-4-4-1.63][172-4-4-2.70][173-4-4-2.61][174-4-0-2.85][175-4-4-1.27][177-4-4-2.56][178-4-4-2.52][179-4-4-1.70][180-4-4-2.47][181-4-2-0.63]
[182-4-4-0.72][183-4-4-1.85][184-4-4-2.01][186-4-4-0.41][187-4-1-1.32][188-4-4-1.20][189-4-4-0.82][190-4-4-0.57][191-4-4-2.18][192-4-4-1.60]
[193-4-2-3.02][194-4-4--0.01][195-4-4-0.78][196-4-4-0.71][197-4-2-1.81][198-4-4-3.88][199-4-4-0.82]
---------------------------
I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 0.807 | Acc: 67.500% | Wgt Acc: 70.719%
	I - Batch: 100 | Loss: 0.815 | Acc: 67.438% | Wgt Acc: 70.634%
	I - Batch: 150 | Loss: 0.795 | Acc: 68.375% | Wgt Acc: 71.566%
	I - Batch: 200 | Loss: 0.789 | Acc: 69.250% | Wgt Acc: 72.448%
I - num batch: 222
I - Train -- Loss: 0.792 | Acc: 68.960% | Wgt Acc: 72.249% | LR: 2.500000e-05 | Dur: 134.31s
I - Confusion Matrix: [row->prediction - col->label]
[[555.   4.  12.  86. 155.]
 [  9. 423.  61.  21.  81.]
 [ 15. 104. 574.  34. 178.]
 [ 70.  18.  42. 379.  71.]
 [ 48.  29.  45.  18. 515.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.033 | Acc: 60.947% | Wgt Acc: 56.483% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   3.  22.  12.]
 [  0.  32.   3.   2.   8.]
 [  3.  22.  46.   7.  21.]
 [ 10.   1.   4.  37.   0.]
 [ 20.  21.  19.  18. 139.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.784 | Acc: 69.500% | Wgt Acc: 72.396%
	I - Batch: 100 | Loss: 0.786 | Acc: 70.250% | Wgt Acc: 73.209%
	I - Batch: 150 | Loss: 0.794 | Acc: 69.458% | Wgt Acc: 72.286%
	I - Batch: 200 | Loss: 0.778 | Acc: 70.500% | Wgt Acc: 73.170%
I - num batch: 222
I - Train -- Loss: 0.775 | Acc: 70.651% | Wgt Acc: 73.332% | LR: 2.500000e-05 | Dur: 134.26s
I - Confusion Matrix: [row->prediction - col->label]
[[564.   9.  14.  89. 142.]
 [  6. 421.  48.  22.  73.]
 [ 15. 108. 583.  36. 165.]
 [ 73.  15.  40. 376.  58.]
 [ 39.  25.  49.  15. 562.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 57.199% | Wgt Acc: 49.901% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   2.  23.   8.]
 [  0.  25.   3.   3.   4.]
 [  1.  23.  39.  10.  17.]
 [  4.   1.   2.  22.   0.]
 [ 30.  28.  29.  28. 151.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 0.794 | Acc: 67.500% | Wgt Acc: 70.346%
	I - Batch: 100 | Loss: 0.774 | Acc: 69.688% | Wgt Acc: 72.541%
	I - Batch: 150 | Loss: 0.786 | Acc: 68.542% | Wgt Acc: 71.875%
	I - Batch: 200 | Loss: 0.778 | Acc: 69.406% | Wgt Acc: 72.729%
I - num batch: 222
I - Train -- Loss: 0.774 | Acc: 69.552% | Wgt Acc: 72.928% | LR: 2.500000e-05 | Dur: 134.76s
I - Confusion Matrix: [row->prediction - col->label]
[[571.   9.  11.  96. 140.]
 [  7. 431.  54.  27.  96.]
 [ 12.  93. 578.  32. 172.]
 [ 65.  17.  41. 372.  77.]
 [ 42.  28.  50.  11. 515.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.081 | Acc: 59.763% | Wgt Acc: 55.605% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   2.   4.  14.  11.]
 [  1.  28.   4.   2.   6.]
 [  2.  23.  50.  13.  26.]
 [ 15.   2.   3.  40.   2.]
 [ 20.  23.  14.  17. 135.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.795 | Acc: 69.000% | Wgt Acc: 72.127%
	I - Batch: 100 | Loss: 0.798 | Acc: 68.375% | Wgt Acc: 71.679%
	I - Batch: 150 | Loss: 0.794 | Acc: 68.333% | Wgt Acc: 71.444%
	I - Batch: 200 | Loss: 0.781 | Acc: 68.938% | Wgt Acc: 72.272%
I - num batch: 222
I - Train -- Loss: 0.778 | Acc: 69.072% | Wgt Acc: 72.400% | LR: 2.500000e-05 | Dur: 134.93s
I - Confusion Matrix: [row->prediction - col->label]
[[566.   8.  14.  97. 157.]
 [  9. 431.  63.  22.  77.]
 [ 12.  99. 571.  35. 190.]
 [ 61.  16.  33. 369.  63.]
 [ 49.  24.  53.  15. 513.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.091 | Acc: 58.580% | Wgt Acc: 52.753% | Dur: 14.83s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   1.   1.  14.   7.]
 [  1.  26.   5.   3.   7.]
 [  2.  23.  44.   9.  21.]
 [  9.   1.   2.  32.   1.]
 [ 25.  27.  23.  28. 144.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.752 | Acc: 71.375% | Wgt Acc: 74.563%
	I - Batch: 100 | Loss: 0.748 | Acc: 71.812% | Wgt Acc: 75.236%
	I - Batch: 150 | Loss: 0.754 | Acc: 71.292% | Wgt Acc: 74.557%
	I - Batch: 200 | Loss: 0.762 | Acc: 71.062% | Wgt Acc: 74.403%
I - num batch: 222
I - Train -- Loss: 0.763 | Acc: 70.905% | Wgt Acc: 74.219% | LR: 1.250000e-05 | Dur: 135.91s
I - Confusion Matrix: [row->prediction - col->label]
[[577.   7.   8.  76. 125.]
 [  6. 430.  52.  19.  95.]
 [ 10.  89. 584.  39. 189.]
 [ 59.  20.  37. 391.  58.]
 [ 45.  32.  53.  13. 533.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.023 | Acc: 61.538% | Wgt Acc: 56.974% | Dur: 14.80s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   2.  22.  13.]
 [  0.  30.   3.   2.   4.]
 [  2.  21.  44.   7.  19.]
 [ 13.   1.   6.  40.   3.]
 [ 16.  23.  20.  15. 141.]]

I - Local maximum validation set accuracy:  61.54

I - Validation set results: 
[14-1-2-1.19][50-3-4-0.67][124-2-2-1.31][127-0-0-3.56][443-2-2-2.80][567-0-0-1.43][573-1-1-2.75][615-0-3-1.11][695-1-2-1.45][722-3-0-3.10]
[826-0-0-2.01][878-0-0-2.71][1103-0-4-0.97][1212-3-3-0.45][1368-0-0-2.20][2181-2-3-0.55][2476-2-2-0.21][2721-2-2-1.55][2818-1-2-0.65][2886-2-1-1.49]
[3231-2-2-2.61][3333-2-2-1.18][3482-2-2-1.36][3536-3-3-1.16][3625-1-1-1.98][3909-0-0-1.08][4035-0-3-1.77][4140-0-0-1.62][4214-1-4-1.05][4346-1-0-0.45]
[4581-2-4-0.77][4708-3-4-1.11][4838-3-4-0.24][4845-1-2-0.67][4868-0-0-2.23][4939-0-4-1.15][4984-2-2-2.65][5078-1-4-1.27][5396-0-0-3.24][5479-1-2-0.55]
[5717-0-0-2.08][5843-1-1-0.94][5949-3-0-1.41][5987-2-4-2.01][6014-3-1-0.69][6033-3-0-1.45][6313-0-3-0.38][6421-3-3-1.19][6500-1-1-1.23][6583-3-2-0.29]
[6683-3-3-0.78][6825-2-1-0.24][6998-3-3-0.48][7049-3-3-0.20][7517-1-2-2.08][7521-1-0-0.38][7528-1-2-0.69][7949-1-2-1.71][8135-1-0-2.64][8185-3-0-3.24]
[8269-3-4-0.86][8273-3-3-1.74][8543-3-0-4.50][8666-1-1-1.95][8672-0-0-3.25][8903-1-2-1.50][9001-2-1-1.70][9036-2-2-2.42][9281-3-4-0.31][9300-2-2-2.74]
[9571-0-4-0.17][9617-1-1-1.14][9644-2-2-2.06][9705-2-4-0.46][9801-0-3-1.34][9803-3-3-0.51][9865-3-3-2.09][9896-2-4-1.29][10314-1-4-1.51][10337-3-3-2.57]
[10403-0-4-1.45][10653-2-4-0.46][10704-2-2-0.56][10719-1-1-1.77][10727-1-4-1.94][10836-0-0-5.56][10969-2-3-1.34][11042-0-0-2.10][11088-1-1-3.14][11322-0-0-3.12]
[11398-2-4-1.28][11499-0-0-1.49][11502-3-3-1.42][11512-3-3-1.19][11608-1-1-1.37][11610-0-0-1.86][11692-0-0-1.53][11905-0-0-2.94][11993-1-1-1.81][12002-2-2--0.00]
[12052-0-0-1.74][12201-0-3-1.89][12235-2-4-1.44][12320-1-4-2.78][12377-2-4-2.72][12398-2-3-0.35][12503-1-1-2.17][12617-0-2--0.10][12685-3-2-0.16][12738-2-3-0.13]
[12742-2-2-3.72][12823-0-0-2.21][13110-1-1-1.49][13240-3-0-1.57][13253-1-4-1.47][13273-0-0-4.07][13634-1-4-1.37][13763-2-3-0.32][13905-3-4--0.27][14060-2-4-1.02]
[14065-3-0-0.89][14147-3-4-0.13][14595-2-2-1.15][14687-2-2-3.14][14788-2-2-1.59][14869-1-4-1.25][14872-3-4-1.19][14877-1-1-3.19][14927-0-3-1.57][15066-0-0-2.40]
[15175-1-4-1.57][15178-2-3-0.38][15375-3-0-1.43][15389-3-3-1.83][15568-2-4-0.63][15675-3-3-2.20][15869-1-4-0.56][16207-3-0-0.67][16236-0-0-0.03][16302-3-0-0.24]
[16331-2-2-3.64][16381-0-3-0.29][16488-1-1-3.89][16495-0-0-1.95][16650-0-0-2.81][16719-1-2-0.48][16801-0-0-3.83][16828-0-0-1.59][17137-3-3-0.81][17245-1-2-0.81]
[17278-3-3--0.24][17282-0-2--0.20][17311-2-2-2.28][17336-2-2-2.19][17608-3-3-2.55][17627-0-4-0.26][17877-3-0-0.75][17924-1-2-0.20][17984-3-3-2.04][18211-0-3-1.50]
[18276-3-3-1.63][18287-1-4-0.09][18394-0-0-2.70][18428-0-0-2.41][18442-0-3-1.89][18478-3-0-1.33][18607-0-0-1.40][18616-0-4-1.22][18663-0-0-0.71][18718-0-0-2.46]
[18766-2-2-1.73][18824-2-4-2.13][18890-3-2-1.06][18930-3-4-1.86][18938-3-2-0.15][19817-1-2-1.58][19839-0-4-0.74][19930-3-3-1.05][19944-0-4-2.06][20036-2-2-2.81]
[20101-3-0-0.21][20474-1-2-1.01][20547-3-4-0.49][20929-2-2-2.25][21245-1-2-1.08][21257-3-2-0.95][21293-1-2-1.99][21316-1-1-2.04][21384-1-4-2.63][21448-1-2-1.46]
[21483-0-0-2.30][21487-2-2-2.77][21714-0-0-0.68][21943-3-4-1.18][21947-0-0-2.53][21948-0-0-4.47][21965-2-2-1.84][21998-1-1-1.35][22025-0-4-1.05][22228-3-3-1.62]
[22446-1-1-1.01][22494-3-0-1.86][22757-0-0-2.47][22811-3-3-2.61][22976-3-4-1.26][22985-3-3-2.12][23014-0-0-2.93][23112-1-1-1.48][23144-3-3-1.68][23168-2-0-0.11]
[23219-0-0-0.59][23363-3-3-2.32][23470-0-4-0.57][23486-2-2-1.00][23497-0-3-2.75][23516-0-0-2.72][23690-1-1-0.17][23921-2-2-1.45][23936-1-2-2.13][24040-3-0-0.81]
[24111-1-4-2.56][24182-0-0-2.90][24238-3-3-1.57][24290-2-4-0.51][24345-0-0-1.30][24364-1-1-1.13][24427-3-0-1.36][24477-2-2-1.40][24495-2-4-0.79][24893-2-2-1.15]
[25012-1-4-0.13][25121-2-4-2.10][25165-3-1--0.46][25183-0-0-1.88][25297-3-3-1.63][25398-0-0-1.79][25574-2-2-2.02][25644-1-1-3.44][25718-1-4-0.13][25774-2-4-0.91]
[26032-3-3-1.45][26051-3-3-2.46][26120-0-4-3.06][26321-1-1-1.53][26732-1-1-1.88][26784-3-3-3.48][26827-3-3-0.72][26833-0-3-2.52][26838-2-2-0.34][26860-1-4-0.89]
[26948-0-0-0.89][27049-3-0-1.76][27098-1-4-0.36][27526-0-0-1.61][27639-3-3-0.76][27698-3-3-1.08][27772-0-0-3.30][27890-1-1-0.91][28040-0-4-1.75][28503-2-2-2.66]
[28577-1-1-1.46][28959-0-0-2.87][29198-3-4-0.84][29777-0-0-4.03][29877-2-2-2.09][30035-1-1-2.36][30098-0-0-2.03][30326-1-1-4.03][30572-2-2-1.48][30716-0-4-1.82]
[30806-2-2-0.38][30906-1-1-1.85][31007-0-0-1.33][31181-3-3-0.70][31238-0-3-1.53][31347-0-0-3.09][31422-2-4-0.86][31429-3-3-0.20][31431-0-0-1.67][31432-1-1-0.84]
[31477-0-3-2.34][31524-1-4-0.17][31597-1-2-2.20][31619-1-2-0.60][31701-0-0-3.31][31755-0-0-2.33][31854-3-3-1.43][32074-1-4-0.10][32078-3-3-1.50][32111-1-1-1.08]
[32127-1-2-2.15][32140-3-3-1.78][32263-2-4-1.01][32365-0-0-1.27][32411-2-0-1.84][32429-3-0-1.76][32473-3-0-0.87][32574-3-0-2.39][32584-0-4-1.83][32622-0-4-1.30]
[32858-3-0-1.40][32969-3-0-2.06][33016-2-2-2.15][33031-1-3-0.93][33035-2-2-2.20][33133-2-2-1.85][33173-2-2-0.83][33175-3-4-1.25][33306-3-2-1.25][33309-2-4-0.15]
[33474-0-0-0.58][33478-2-2--0.02][33618-1-4-0.77][33712-0-4-0.70][33782-2-4-2.42][33914-3-3-2.21][34076-3-4-0.75][34112-2-2-1.12][34138-2-2-1.76][34239-1-4-0.69]
[34364-2-2-2.73][34617-1-4-1.77][34751-3-3-2.37][34783-2-4-1.37][35015-3-2-1.07][35018-1-4-1.48][35288-2-2-1.12][0-4-4-2.37][1-4-4-2.85][2-4-4-1.29]
[3-4-4-2.10][4-4-4-0.70][5-4-1-0.63][6-4-0-1.65][7-4-4-2.94][8-4-2-0.30][9-4-4-1.04][10-4-4-2.80][11-4-2-2.34][12-4-2-0.61]
[14-4-4-0.84][15-4-3-1.56][16-4-4-1.93][17-4-4-0.70][18-4-4-3.17][19-4-0-1.94][20-4-0-0.77][21-4-2-1.25][22-4-4-2.51][23-4-4-1.31]
[24-4-4-4.72][25-4-4-0.27][26-4-4-0.34][27-4-2-0.78][28-4-4-2.11][29-4-2-1.10][30-4-1--0.16][31-4-4-1.70][32-4-4-1.34][33-4-2-0.49]
[34-4-4-0.81][35-4-0-1.10][37-4-4-1.44][39-4-0-2.47][40-4-4-1.20][41-4-4-0.55][42-4-4-0.81][43-4-4-1.23][45-4-4-1.94][46-4-4-3.34]
[47-4-4-3.97][48-4-4-2.44][51-4-4-2.09][52-4-4-1.46][53-4-4-0.90][54-4-4-0.29][55-4-4-1.63][56-4-4-1.12][57-4-0-1.58][58-4-2-1.53]
[59-4-4-1.61][60-4-4-1.46][61-4-4-2.12][62-4-4-0.43][63-4-4-0.95][64-4-4-0.69][65-4-4-2.94][66-4-4-2.19][67-4-4-0.79][68-4-4-0.50]
[69-4-0-0.82][70-4-4-2.27][72-4-4-1.09][73-4-1-1.04][74-4-4-1.52][75-4-4-0.88][77-4-4-1.65][78-4-4-0.76][79-4-4-2.35][80-4-4-1.82]
[81-4-4-2.61][82-4-4-0.87][83-4-4-1.34][84-4-4-2.64][85-4-4-2.60][86-4-4-0.96][87-4-4-2.16][88-4-4-2.12][89-4-4-0.98][90-4-4-0.47]
[91-4-4-1.70][92-4-4-1.13][93-4-0-2.00][94-4-4-2.57][95-4-4-1.26][96-4-4-1.04][97-4-4-1.96][98-4-2-2.02][99-4-4-1.01][100-4-4-1.46]
[101-4-4-4.14][102-4-4-1.37][103-4-4-0.24][104-4-4-1.45][105-4-4-2.43][106-4-4-2.44][107-4-4-1.92][108-4-4-0.98][109-4-4-1.80][110-4-4-1.05]
[111-4-0-2.50][112-4-4-0.44][113-4-4-1.15][114-4-2-0.31][115-4-4-1.49][116-4-4-1.33][117-4-4-1.91][119-4-2-1.75][121-4-4-1.41][122-4-4-1.80]
[124-4-2-0.91][125-4-4-2.65][126-4-4-2.73][127-4-4-1.14][128-4-4-1.00][129-4-4-0.76][130-4-4-2.48][131-4-2-1.11][132-4-4-1.13][133-4-4-3.80]
[135-4-2-1.25][136-4-4-1.13][137-4-4-0.77][138-4-4-1.13][139-4-4-1.46][140-4-1-1.73][141-4-2-1.06][142-4-4-2.47][143-4-4-2.67][144-4-4-2.69]
[145-4-4-1.00][148-4-0-3.20][149-4-4-0.91][150-4-4-2.40][151-4-4-2.58][152-4-4-1.72][153-4-4-2.23][154-4-4-1.80][155-4-4-1.75][156-4-4-0.41]
[157-4-4-0.66][158-4-4-1.33][160-4-4-0.54][161-4-2-1.01][162-4-4-1.04][164-4-4-1.75][165-4-4-1.64][167-4-0-2.08][168-4-4-1.47][170-4-4-0.78]
[171-4-4-1.70][172-4-4-3.01][173-4-4-2.45][174-4-0-1.91][175-4-4-2.44][177-4-4-2.62][178-4-4-1.25][179-4-4-2.20][180-4-4-2.32][181-4-3-0.47]
[182-4-3-0.35][183-4-4-1.96][184-4-4-2.09][186-4-4-0.49][187-4-4-1.14][188-4-4-1.72][189-4-4-0.77][190-4-4-0.58][191-4-4-1.87][192-4-4-1.67]
[193-4-2-2.25][194-4-2-1.48][195-4-0-1.25][196-4-4-1.03][197-4-4-1.38][198-4-4-3.99][199-4-2-0.89]
---------------------------
I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.746 | Acc: 73.625% | Wgt Acc: 75.899%
	I - Batch: 100 | Loss: 0.733 | Acc: 73.000% | Wgt Acc: 75.337%
	I - Batch: 150 | Loss: 0.728 | Acc: 72.875% | Wgt Acc: 75.436%
	I - Batch: 200 | Loss: 0.730 | Acc: 71.875% | Wgt Acc: 74.767%
I - num batch: 208
I - Train -- Loss: 0.729 | Acc: 71.755% | Wgt Acc: 74.701% | LR: 1.250000e-05 | Dur: 126.16s
I - Confusion Matrix: [row->prediction - col->label]
[[569.   4.   5.  76. 106.]
 [ 15. 434.  56.  22.  65.]
 [ 14.  98. 589.  39. 142.]
 [ 63.  19.  44. 388.  60.]
 [ 36.  23.  40.  13. 408.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.027 | Acc: 60.750% | Wgt Acc: 56.149% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   1.  19.  12.]
 [  0.  34.   3.   2.   7.]
 [  2.  24.  41.  10.  19.]
 [ 11.   2.   5.  38.   2.]
 [ 20.  16.  25.  17. 140.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.717 | Acc: 71.625% | Wgt Acc: 76.122%
	I - Batch: 100 | Loss: 0.722 | Acc: 72.375% | Wgt Acc: 76.751%
	I - Batch: 150 | Loss: 0.742 | Acc: 71.458% | Wgt Acc: 75.514%
	I - Batch: 200 | Loss: 0.740 | Acc: 71.688% | Wgt Acc: 75.735%
I - num batch: 222
I - Train -- Loss: 0.741 | Acc: 71.384% | Wgt Acc: 75.339% | LR: 1.250000e-05 | Dur: 135.94s
I - Confusion Matrix: [row->prediction - col->label]
[[580.   4.   8.  83. 164.]
 [  8. 466.  48.  20.  73.]
 [ 10.  71. 600.  41. 181.]
 [ 58.  15.  33. 382.  78.]
 [ 41.  22.  45.  12. 504.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.119 | Acc: 56.213% | Wgt Acc: 50.402% | Dur: 16.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   1.   1.  17.   5.]
 [  1.  29.   7.   3.   8.]
 [  2.  25.  43.  13.  26.]
 [ 11.   1.   3.  30.   1.]
 [ 31.  22.  21.  23. 140.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.753 | Acc: 71.000% | Wgt Acc: 74.909%
	I - Batch: 100 | Loss: 0.737 | Acc: 71.562% | Wgt Acc: 75.332%
	I - Batch: 150 | Loss: 0.737 | Acc: 71.917% | Wgt Acc: 75.438%
	I - Batch: 200 | Loss: 0.737 | Acc: 71.531% | Wgt Acc: 74.846%
I - num batch: 222
I - Train -- Loss: 0.737 | Acc: 71.553% | Wgt Acc: 74.956% | LR: 1.250000e-05 | Dur: 136.51s
I - Confusion Matrix: [row->prediction - col->label]
[[569.  10.   8.  79. 139.]
 [ 10. 449.  51.  16.  80.]
 [ 15.  77. 589.  34. 184.]
 [ 64.  13.  39. 395.  61.]
 [ 39.  29.  47.  14. 536.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.066 | Acc: 58.974% | Wgt Acc: 54.331% | Dur: 15.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   1.   1.  17.   9.]
 [  1.  32.   5.   3.   9.]
 [  2.  22.  48.  13.  22.]
 [ 12.   1.   2.  36.   3.]
 [ 27.  22.  19.  17. 137.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.752 | Acc: 70.875% | Wgt Acc: 74.908%
	I - Batch: 100 | Loss: 0.736 | Acc: 71.375% | Wgt Acc: 75.223%
	I - Batch: 150 | Loss: 0.721 | Acc: 71.583% | Wgt Acc: 75.169%
	I - Batch: 200 | Loss: 0.732 | Acc: 71.250% | Wgt Acc: 74.803%
I - num batch: 222
I - Train -- Loss: 0.733 | Acc: 71.159% | Wgt Acc: 74.738% | LR: 1.250000e-05 | Dur: 134.27s
I - Confusion Matrix: [row->prediction - col->label]
[[580.   5.   8.  76. 149.]
 [  7. 439.  45.  25.  82.]
 [ 11.  94. 597.  37. 175.]
 [ 69.  18.  35. 387.  73.]
 [ 30.  22.  49.  13. 521.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.099 | Acc: 58.383% | Wgt Acc: 54.676% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   0.   1.  13.   7.]
 [  2.  34.   5.   3.   7.]
 [  2.  26.  53.  12.  36.]
 [ 13.   1.   2.  34.   1.]
 [ 25.  17.  14.  24. 129.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.735 | Acc: 72.250% | Wgt Acc: 74.807%
	I - Batch: 100 | Loss: 0.743 | Acc: 71.375% | Wgt Acc: 74.259%
	I - Batch: 150 | Loss: 0.746 | Acc: 71.167% | Wgt Acc: 74.128%
	I - Batch: 200 | Loss: 0.737 | Acc: 71.812% | Wgt Acc: 74.949%
I - num batch: 222
I - Train -- Loss: 0.740 | Acc: 71.779% | Wgt Acc: 74.887% | LR: 1.250000e-05 | Dur: 133.18s
I - Confusion Matrix: [row->prediction - col->label]
[[569.   6.  10.  91. 144.]
 [  4. 457.  53.  15.  75.]
 [ 11.  75. 589.  37. 171.]
 [ 72.  15.  35. 379.  58.]
 [ 41.  25.  47.  16. 552.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.100 | Acc: 59.369% | Wgt Acc: 52.513% | Dur: 13.88s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   0.   2.  13.   5.]
 [  0.  26.   3.   3.   3.]
 [  2.  25.  44.  11.  17.]
 [ 13.   1.   2.  33.   2.]
 [ 28.  26.  24.  26. 153.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.732 | Acc: 71.250% | Wgt Acc: 74.488%
	I - Batch: 100 | Loss: 0.733 | Acc: 71.188% | Wgt Acc: 74.501%
	I - Batch: 150 | Loss: 0.726 | Acc: 71.833% | Wgt Acc: 75.211%
	I - Batch: 200 | Loss: 0.731 | Acc: 71.188% | Wgt Acc: 74.761%
I - num batch: 222
I - Train -- Loss: 0.735 | Acc: 71.243% | Wgt Acc: 74.785% | LR: 1.250000e-05 | Dur: 133.23s
I - Confusion Matrix: [row->prediction - col->label]
[[580.   7.   8.  79. 144.]
 [  6. 443.  46.  20.  75.]
 [ 11.  89. 586.  35. 186.]
 [ 62.  17.  38. 393.  70.]
 [ 38.  22.  56.  11. 525.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.068 | Acc: 61.144% | Wgt Acc: 54.968% | Dur: 14.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   2.  24.  15.]
 [  0.  26.   1.   0.   4.]
 [  1.  21.  38.   6.  10.]
 [  6.   2.   4.  34.   0.]
 [ 20.  26.  30.  22. 151.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.752 | Acc: 70.750% | Wgt Acc: 74.657%
	I - Batch: 100 | Loss: 0.732 | Acc: 72.062% | Wgt Acc: 75.544%
	I - Batch: 150 | Loss: 0.721 | Acc: 72.875% | Wgt Acc: 76.401%
	I - Batch: 200 | Loss: 0.722 | Acc: 72.781% | Wgt Acc: 76.201%
I - num batch: 222
I - Train -- Loss: 0.728 | Acc: 72.315% | Wgt Acc: 75.595% | LR: 1.250000e-05 | Dur: 138.21s
I - Confusion Matrix: [row->prediction - col->label]
[[593.   4.   8.  77. 121.]
 [  6. 449.  52.  25.  77.]
 [  7.  83. 588.  32. 193.]
 [ 60.  17.  40. 387.  61.]
 [ 31.  25.  46.  17. 548.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.092 | Acc: 57.002% | Wgt Acc: 50.549% | Dur: 15.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   0.   1.  17.  11.]
 [  0.  25.   2.   2.   2.]
 [  2.  26.  43.  11.  20.]
 [ 13.   1.   2.  32.   1.]
 [ 30.  26.  27.  24. 146.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.714 | Acc: 71.500% | Wgt Acc: 75.417%
	I - Batch: 100 | Loss: 0.734 | Acc: 71.625% | Wgt Acc: 75.160%
	I - Batch: 150 | Loss: 0.714 | Acc: 73.250% | Wgt Acc: 76.568%
	I - Batch: 200 | Loss: 0.722 | Acc: 72.656% | Wgt Acc: 75.921%
I - num batch: 222
I - Train -- Loss: 0.723 | Acc: 72.540% | Wgt Acc: 75.672% | LR: 1.250000e-05 | Dur: 136.69s
I - Confusion Matrix: [row->prediction - col->label]
[[575.   3.   9.  87. 131.]
 [  7. 466.  54.  23.  66.]
 [ 15.  73. 588.  34. 176.]
 [ 60.  16.  32. 385.  68.]
 [ 40.  20.  51.   9. 559.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 61.341% | Wgt Acc: 58.228% | Dur: 14.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   2.   3.  17.  12.]
 [  0.  31.   3.   1.  12.]
 [  1.  27.  45.   8.  23.]
 [ 13.   2.   7.  46.   2.]
 [ 16.  16.  17.  14. 131.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.707 | Acc: 73.250% | Wgt Acc: 76.691%
	I - Batch: 100 | Loss: 0.705 | Acc: 72.938% | Wgt Acc: 76.543%
	I - Batch: 150 | Loss: 0.718 | Acc: 72.625% | Wgt Acc: 76.246%
	I - Batch: 200 | Loss: 0.717 | Acc: 72.844% | Wgt Acc: 76.393%
I - num batch: 222
I - Train -- Loss: 0.719 | Acc: 72.738% | Wgt Acc: 76.251% | LR: 1.250000e-05 | Dur: 138.26s
I - Confusion Matrix: [row->prediction - col->label]
[[575.   7.   8.  68. 145.]
 [ 10. 469.  51.  20.  80.]
 [  8.  73. 598.  38. 166.]
 [ 60.  12.  34. 396.  67.]
 [ 44.  17.  43.  16. 542.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.129 | Acc: 57.002% | Wgt Acc: 50.893% | Dur: 15.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 38.   1.   1.   8.   7.]
 [  0.  22.   3.   2.   5.]
 [  2.  24.  48.  12.  23.]
 [ 16.   1.   2.  37.   1.]
 [ 32.  30.  21.  27. 144.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.724 | Acc: 70.875% | Wgt Acc: 74.214%
	I - Batch: 100 | Loss: 0.722 | Acc: 71.688% | Wgt Acc: 75.046%
	I - Batch: 150 | Loss: 0.725 | Acc: 72.000% | Wgt Acc: 75.244%
	I - Batch: 200 | Loss: 0.716 | Acc: 72.250% | Wgt Acc: 75.639%
I - num batch: 222
I - Train -- Loss: 0.712 | Acc: 72.822% | Wgt Acc: 76.085% | LR: 1.250000e-05 | Dur: 135.79s
I - Confusion Matrix: [row->prediction - col->label]
[[570.   6.   5.  68. 143.]
 [  3. 456.  49.  16.  76.]
 [ 10.  73. 593.  33. 163.]
 [ 71.  18.  35. 407.  61.]
 [ 43.  25.  52.  14. 557.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 61.341% | Wgt Acc: 56.650% | Dur: 13.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   1.   2.  17.   9.]
 [  1.  36.   4.   2.   9.]
 [  2.  22.  49.  11.  20.]
 [ 12.   1.   2.  33.   1.]
 [ 21.  18.  18.  23. 141.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.702 | Acc: 72.500% | Wgt Acc: 75.741%
	I - Batch: 100 | Loss: 0.712 | Acc: 72.125% | Wgt Acc: 75.259%
	I - Batch: 150 | Loss: 0.714 | Acc: 72.250% | Wgt Acc: 75.806%
	I - Batch: 200 | Loss: 0.710 | Acc: 72.344% | Wgt Acc: 75.948%
I - num batch: 222
I - Train -- Loss: 0.710 | Acc: 72.174% | Wgt Acc: 75.730% | LR: 1.250000e-05 | Dur: 133.29s
I - Confusion Matrix: [row->prediction - col->label]
[[578.   7.   7.  80. 151.]
 [  6. 458.  46.  24.  79.]
 [ 10.  79. 601.  33. 160.]
 [ 63.  17.  34. 390.  77.]
 [ 40.  17.  46.  11. 533.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.088 | Acc: 57.988% | Wgt Acc: 52.993% | Dur: 19.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 44.   1.   0.  10.  10.]
 [  0.  24.   3.   2.   5.]
 [  1.  29.  48.  12.  25.]
 [ 17.   2.   4.  40.   2.]
 [ 26.  22.  20.  22. 138.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.713 | Acc: 70.500% | Wgt Acc: 74.651%
	I - Batch: 100 | Loss: 0.697 | Acc: 73.062% | Wgt Acc: 76.736%
	I - Batch: 150 | Loss: 0.708 | Acc: 72.625% | Wgt Acc: 76.272%
	I - Batch: 200 | Loss: 0.706 | Acc: 72.906% | Wgt Acc: 76.316%
I - num batch: 222
I - Train -- Loss: 0.704 | Acc: 72.991% | Wgt Acc: 76.351% | LR: 1.250000e-05 | Dur: 134.49s
I - Confusion Matrix: [row->prediction - col->label]
[[575.   1.   7.  79. 133.]
 [  4. 470.  46.  20.  85.]
 [ 16.  68. 600.  33. 165.]
 [ 69.  19.  34. 392.  65.]
 [ 33.  20.  47.  14. 552.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.057 | Acc: 60.158% | Wgt Acc: 54.843% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   0.   2.  21.   7.]
 [  0.  29.   3.   4.   6.]
 [  2.  27.  47.   6.  23.]
 [  8.   1.   2.  32.   1.]
 [ 24.  21.  21.  23. 143.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.751 | Acc: 71.000% | Wgt Acc: 74.484%
	I - Batch: 100 | Loss: 0.708 | Acc: 73.562% | Wgt Acc: 77.095%
	I - Batch: 150 | Loss: 0.711 | Acc: 72.375% | Wgt Acc: 76.055%
	I - Batch: 200 | Loss: 0.715 | Acc: 72.281% | Wgt Acc: 76.068%
I - num batch: 222
I - Train -- Loss: 0.722 | Acc: 72.117% | Wgt Acc: 75.843% | LR: 1.250000e-05 | Dur: 139.68s
I - Confusion Matrix: [row->prediction - col->label]
[[572.   8.  11.  66. 151.]
 [  6. 463.  40.  17.  80.]
 [ 11.  70. 600.  37. 178.]
 [ 65.  19.  38. 398.  66.]
 [ 43.  18.  45.  20. 525.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.031 | Acc: 62.130% | Wgt Acc: 57.737% | Dur: 14.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   2.  18.  13.]
 [  0.  36.   3.   1.   9.]
 [  2.  20.  42.   6.  14.]
 [ 12.   1.   5.  39.   3.]
 [ 17.  18.  23.  22. 141.]]

I - Local maximum validation set accuracy:  62.13

I - Validation set results: 
[14-1-2-1.41][50-3-4-0.83][124-2-2-1.80][127-0-0-3.78][443-2-2-3.16][567-0-0-1.12][573-1-1-1.27][615-0-3-1.07][695-1-2-1.63][722-3-0-2.87]
[826-0-0-1.99][878-0-0-1.76][1103-0-4-1.16][1212-3-4-0.96][1368-0-0-2.52][2181-2-3-0.55][2476-2-4-0.30][2721-2-2-1.84][2818-1-4-0.20][2886-2-1-2.33]
[3231-2-2-3.28][3333-2-2-1.53][3482-2-2-1.60][3536-3-3-0.90][3625-1-1-2.67][3909-0-0-1.28][4035-0-3-1.68][4140-0-0-1.96][4214-1-1-0.63][4346-1-0-0.32]
[4581-2-2-1.70][4708-3-2-1.25][4838-3-4-0.32][4845-1-2-0.78][4868-0-0-2.79][4939-0-2-0.84][4984-2-2-2.55][5078-1-4-1.45][5396-0-0-3.22][5479-1-2-0.71]
[5717-0-0-2.46][5843-1-1-1.20][5949-3-0-1.29][5987-2-4-2.10][6014-3-1-1.21][6033-3-0-0.77][6313-0-0-0.32][6421-3-3-1.07][6500-1-1-1.45][6583-3-2-0.36]
[6683-3-3-1.39][6825-2-3--0.09][6998-3-3-0.66][7049-3-3-0.15][7517-1-2-2.08][7521-1-0--0.08][7528-1-2-1.45][7949-1-2-1.83][8135-1-0-1.76][8185-3-0-2.42]
[8269-3-4-1.07][8273-3-3-1.90][8543-3-0-4.44][8666-1-1-2.24][8672-0-0-1.21][8903-1-2-1.77][9001-2-4-0.89][9036-2-2-2.58][9281-3-4-0.86][9300-2-2-3.04]
[9571-0-3-0.50][9617-1-1-1.38][9644-2-2-1.70][9705-2-4-0.46][9801-0-0-1.32][9803-3-3-0.46][9865-3-3-2.26][9896-2-4-1.55][10314-1-4-1.61][10337-3-3-2.58]
[10403-0-4-1.47][10653-2-4-0.61][10704-2-2-1.12][10719-1-1-1.68][10727-1-4-1.55][10836-0-0-5.85][10969-2-3-1.28][11042-0-0-0.71][11088-1-1-2.48][11322-0-0-3.20]
[11398-2-2-4.28][11499-0-0-1.35][11502-3-3-1.05][11512-3-3-1.03][11608-1-1-2.30][11610-0-0-0.61][11692-0-0-1.30][11905-0-0-2.82][11993-1-1-2.09][12002-2-0-1.85]
[12052-0-0-2.87][12201-0-3-1.81][12235-2-4-1.43][12320-1-4-3.32][12377-2-4-2.67][12398-2-4-0.09][12503-1-1-1.24][12617-0-2-0.42][12685-3-4-0.75][12738-2-3-0.26]
[12742-2-2-3.79][12823-0-0-2.10][13110-1-1-2.24][13240-3-0-1.42][13253-1-1-1.71][13273-0-0-4.44][13634-1-4-1.55][13763-2-2-0.93][13905-3-4--0.21][14060-2-4-1.04]
[14065-3-0-0.88][14147-3-4-0.42][14595-2-2-1.11][14687-2-2-3.36][14788-2-2-1.66][14869-1-1-1.40][14872-3-4-1.50][14877-1-1-3.43][14927-0-3-1.36][15066-0-0-2.56]
[15175-1-4-1.77][15178-2-3-0.13][15375-3-3-0.73][15389-3-0-1.31][15568-2-1-1.16][15675-3-3-2.13][15869-1-2-0.09][16207-3-0-1.04][16236-0-4-0.60][16302-3-4-0.45]
[16331-2-2-4.00][16381-0-4-0.66][16488-1-1-4.42][16495-0-0-3.51][16650-0-0-3.11][16719-1-1-0.67][16801-0-0-4.06][16828-0-0-1.73][17137-3-3-0.84][17245-1-2-1.14]
[17278-3-4-0.07][17282-0-0--0.11][17311-2-2-2.45][17336-2-2-2.09][17608-3-3-2.52][17627-0-4-0.95][17877-3-4-2.02][17924-1-2-0.36][17984-3-3-2.32][18211-0-3-1.34]
[18276-3-3-1.50][18287-1-1-0.48][18394-0-0-2.09][18428-0-0-3.05][18442-0-3-1.62][18478-3-0-1.31][18607-0-0-1.96][18616-0-4-1.46][18663-0-3-0.92][18718-0-0-2.38]
[18766-2-2-1.77][18824-2-4-1.71][18890-3-2-0.49][18930-3-4-1.79][18938-3-2-0.28][19817-1-2-1.67][19839-0-0-0.82][19930-3-3-1.11][19944-0-4-2.34][20036-2-2-3.40]
[20101-3-3-1.13][20474-1-2-1.11][20547-3-4-0.73][20929-2-2-2.68][21245-1-2-1.02][21257-3-3-0.37][21293-1-2-1.96][21316-1-1-2.17][21384-1-4-2.65][21448-1-1-1.38]
[21483-0-0-2.41][21487-2-2-2.75][21714-0-0-0.49][21943-3-2-1.86][21947-0-0-1.60][21948-0-0-4.62][21965-2-2-2.49][21998-1-1-1.66][22025-0-4-1.60][22228-3-3-1.91]
[22446-1-1-1.36][22494-3-0-1.65][22757-0-0-2.19][22811-3-3-1.92][22976-3-4-1.35][22985-3-3-1.82][23014-0-0-2.61][23112-1-1-2.37][23144-3-3-1.64][23168-2-4-0.85]
[23219-0-4-0.68][23363-3-3-1.91][23470-0-4-0.50][23486-2-4-0.63][23497-0-3-2.76][23516-0-0-2.72][23690-1-4-1.75][23921-2-2-1.66][23936-1-2-2.38][24040-3-4-1.24]
[24111-1-4-2.39][24182-0-0-3.25][24238-3-3-1.21][24290-2-4-0.77][24345-0-0-1.20][24364-1-2-0.93][24427-3-0-1.61][24477-2-4-1.50][24495-2-1-0.94][24893-2-2-1.23]
[25012-1-1-0.11][25121-2-4-2.37][25165-3-4--0.45][25183-0-0-2.09][25297-3-3-1.85][25398-0-0-1.76][25574-2-2-2.32][25644-1-1-3.81][25718-1-4-0.51][25774-2-4-1.40]
[26032-3-0-1.48][26051-3-3-2.29][26120-0-4-1.85][26321-1-1-1.62][26732-1-1-2.47][26784-3-3-3.13][26827-3-3-0.37][26833-0-3-2.54][26838-2-4-1.13][26860-1-4-1.12]
[26948-0-0-0.95][27049-3-0-1.68][27098-1-4-0.55][27526-0-0-1.81][27639-3-3-0.60][27698-3-3-1.29][27772-0-0-2.20][27890-1-1-1.43][28040-0-4-2.10][28503-2-2-2.81]
[28577-1-1-1.66][28959-0-0-2.53][29198-3-4-1.21][29777-0-0-3.90][29877-2-2-2.08][30035-1-1-2.41][30098-0-0-1.80][30326-1-1-4.81][30572-2-2-1.70][30716-0-4-1.95]
[30806-2-2-0.77][30906-1-1-1.29][31007-0-0-0.64][31181-3-4-0.22][31238-0-3-1.40][31347-0-0-3.06][31422-2-4-0.84][31429-3-3-0.20][31431-0-0-1.72][31432-1-1-1.91]
[31477-0-3-2.32][31524-1-4-0.23][31597-1-2-2.07][31619-1-2-0.33][31701-0-0-3.32][31755-0-0-2.62][31854-3-3-1.22][32074-1-1-0.09][32078-3-3-1.32][32111-1-1-1.32]
[32127-1-2-2.33][32140-3-3-1.35][32263-2-4-1.03][32365-0-0-0.55][32411-2-0-2.07][32429-3-3-1.12][32473-3-0-0.84][32574-3-0-2.59][32584-0-4-2.13][32622-0-4-1.41]
[32858-3-0-1.52][32969-3-0-1.88][33016-2-2-2.71][33031-1-3-0.84][33035-2-2-2.12][33133-2-2-2.39][33173-2-2-0.81][33175-3-4-1.27][33306-3-2-1.36][33309-2-4-0.19]
[33474-0-4-0.39][33478-2-2--0.01][33618-1-4-1.02][33712-0-4-0.76][33782-2-4-2.52][33914-3-3-1.79][34076-3-4-1.20][34112-2-2-2.09][34138-2-2-1.89][34239-1-4-0.77]
[34364-2-2-1.63][34617-1-4-2.08][34751-3-3-2.35][34783-2-4-1.60][35015-3-4-0.86][35018-1-4-1.66][35288-2-2-1.03][0-4-4-1.87][1-4-4-3.09][2-4-4-1.68]
[3-4-4-2.51][4-4-4-1.17][5-4-1-1.07][6-4-0-1.91][7-4-4-2.63][8-4-4-0.36][9-4-4-1.19][10-4-4-3.50][11-4-4-2.51][12-4-2-0.87]
[14-4-4-1.07][15-4-3-1.14][16-4-4-2.42][17-4-4-0.93][18-4-4-2.76][19-4-0-2.61][20-4-4-0.94][21-4-4-1.37][22-4-4-2.57][23-4-4-1.53]
[24-4-4-4.29][25-4-4-0.57][26-4-4-0.77][27-4-2-1.16][28-4-4-2.96][29-4-2-1.05][30-4-4--0.25][31-4-4-2.13][32-4-1-1.49][33-4-4-0.71]
[34-4-4-1.09][35-4-0-0.90][37-4-4-1.07][39-4-0-3.19][40-4-4-1.38][41-4-4-0.86][42-4-4-1.10][43-4-4-0.99][45-4-4-0.31][46-4-4-3.61]
[47-4-4-4.26][48-4-4-2.99][51-4-4-2.52][52-4-0-1.43][53-4-4-1.23][54-4-4-0.37][55-4-4-1.79][56-4-4-1.58][57-4-4-0.50][58-4-2-1.85]
[59-4-0-1.64][60-4-4-1.32][61-4-4-1.98][62-4-4-0.83][63-4-2-1.52][64-4-4-0.75][65-4-4-3.23][66-4-4-2.35][67-4-4-1.03][68-4-4-1.24]
[69-4-0-0.70][70-4-4-2.45][72-4-1-1.56][73-4-1-1.87][74-4-2-1.49][75-4-4-0.70][77-4-4-3.16][78-4-4-0.77][79-4-4-2.44][80-4-4-2.20]
[81-4-1-1.24][82-4-4-0.73][83-4-4-1.46][84-4-4-2.31][85-4-4-3.57][86-4-4-1.12][87-4-4-2.37][88-4-4-1.70][89-4-2-1.27][90-4-4-0.77]
[91-4-4-1.71][92-4-4-0.77][93-4-0-1.13][94-4-4-2.89][95-4-4-0.61][96-4-4-1.13][97-4-4-2.63][98-4-2-2.31][99-4-4-1.05][100-4-1-1.17]
[101-4-4-4.39][102-4-4-1.68][103-4-4-0.46][104-4-4-1.96][105-4-4-2.22][106-4-4-2.56][107-4-4-2.12][108-4-4-1.44][109-4-4-1.62][110-4-4-1.64]
[111-4-0-2.38][112-4-4-0.59][113-4-4-0.97][114-4-2-0.70][115-4-4-1.39][116-4-4-1.38][117-4-4-2.37][119-4-4-1.35][121-4-4-2.53][122-4-4-2.12]
[124-4-2-0.98][125-4-4-2.90][126-4-4-3.83][127-4-4-1.22][128-4-4-1.10][129-4-1-1.01][130-4-4-2.06][131-4-2-1.37][132-4-4-1.31][133-4-4-4.90]
[135-4-4-1.12][136-4-4-0.86][137-4-4-0.70][138-4-4-1.56][139-4-4-2.20][140-4-1-1.05][141-4-3-0.35][142-4-4-3.32][143-4-4-3.19][144-4-4-2.91]
[145-4-4-1.26][148-4-0-3.62][149-4-4-1.32][150-4-4-2.98][151-4-4-2.65][152-4-4-1.90][153-4-4-2.00][154-4-4-2.64][155-4-4-2.52][156-4-4-0.70]
[157-4-0-0.96][158-4-4-1.43][160-4-1-0.60][161-4-2-0.91][162-4-4-1.04][164-4-4-1.72][165-4-4-1.08][167-4-0-1.51][168-4-4-1.44][170-4-4-1.44]
[171-4-4-1.61][172-4-4-2.90][173-4-4-2.85][174-4-0-2.78][175-4-4-1.61][177-4-4-1.87][178-4-4-2.70][179-4-4-2.57][180-4-4-2.48][181-4-4-0.67]
[182-4-4-0.57][183-4-4-2.32][184-4-4-2.26][186-4-4-0.95][187-4-4-0.99][188-4-4-1.99][189-4-4-0.73][190-4-4-0.54][191-4-4-2.61][192-4-4-1.45]
[193-4-2-2.08][194-4-3-0.29][195-4-4-0.73][196-4-4-1.05][197-4-4-2.26][198-4-4-4.53][199-4-2-1.12]
---------------------------
I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.726 | Acc: 72.625% | Wgt Acc: 75.872%
	I - Batch: 100 | Loss: 0.731 | Acc: 72.438% | Wgt Acc: 75.634%
	I - Batch: 150 | Loss: 0.723 | Acc: 72.500% | Wgt Acc: 75.689%
	I - Batch: 200 | Loss: 0.708 | Acc: 73.188% | Wgt Acc: 76.599%
I - num batch: 222
I - Train -- Loss: 0.701 | Acc: 73.499% | Wgt Acc: 76.970% | LR: 1.250000e-05 | Dur: 135.10s
I - Confusion Matrix: [row->prediction - col->label]
[[593.   3.   7.  70. 135.]
 [  3. 462.  46.  20.  86.]
 [  9.  79. 601.  32. 163.]
 [ 60.  21.  35. 400.  65.]
 [ 32.  13.  45.  16. 551.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.093 | Acc: 59.763% | Wgt Acc: 54.299% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   6.  27.  18.]
 [  1.  24.   2.   1.   3.]
 [  0.  22.  32.   3.   9.]
 [  9.   4.   5.  38.   6.]
 [ 13.  24.  30.  17. 144.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.697 | Acc: 72.125% | Wgt Acc: 75.808%
	I - Batch: 100 | Loss: 0.705 | Acc: 72.125% | Wgt Acc: 75.454%
	I - Batch: 150 | Loss: 0.706 | Acc: 72.375% | Wgt Acc: 75.427%
	I - Batch: 200 | Loss: 0.702 | Acc: 72.438% | Wgt Acc: 75.741%
I - num batch: 222
I - Train -- Loss: 0.704 | Acc: 72.399% | Wgt Acc: 75.807% | LR: 1.250000e-05 | Dur: 134.35s
I - Confusion Matrix: [row->prediction - col->label]
[[581.   6.   8.  78. 138.]
 [  7. 450.  46.  22.  92.]
 [ 11.  76. 600.  28. 160.]
 [ 63.  18.  37. 394.  67.]
 [ 35.  28.  43.  16. 543.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.053 | Acc: 57.791% | Wgt Acc: 54.090% | Dur: 21.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   1.   2.  19.  10.]
 [  0.  34.   6.   2.   9.]
 [  1.  21.  46.  10.  31.]
 [ 14.   1.   3.  34.   2.]
 [ 22.  21.  18.  21. 128.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.708 | Acc: 71.625% | Wgt Acc: 75.761%
	I - Batch: 100 | Loss: 0.698 | Acc: 73.312% | Wgt Acc: 76.963%
	I - Batch: 150 | Loss: 0.693 | Acc: 73.542% | Wgt Acc: 77.032%
	I - Batch: 200 | Loss: 0.695 | Acc: 73.375% | Wgt Acc: 76.931%
I - num batch: 222
I - Train -- Loss: 0.692 | Acc: 73.555% | Wgt Acc: 76.936% | LR: 1.250000e-05 | Dur: 135.61s
I - Confusion Matrix: [row->prediction - col->label]
[[583.  10.   9.  75. 137.]
 [ 10. 467.  45.  22.  70.]
 [  9.  61. 608.  33. 169.]
 [ 54.  14.  28. 395.  68.]
 [ 41.  26.  44.  13. 556.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.038 | Acc: 62.525% | Wgt Acc: 57.580% | Dur: 14.69s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   1.  16.  12.]
 [  0.  32.   2.   1.   7.]
 [  1.  19.  42.   5.  13.]
 [ 12.   2.   7.  43.   2.]
 [ 21.  23.  23.  21. 146.]]

I - Local maximum validation set accuracy:  62.52

I - Validation set results: 
[14-1-2-1.64][50-3-4-0.46][124-2-2-2.21][127-0-0-3.76][443-2-2-3.09][567-0-0-1.31][573-1-1-3.25][615-0-3-1.03][695-1-2-1.74][722-3-0-3.38]
[826-0-0-1.44][878-0-0-2.17][1103-0-4-1.18][1212-3-4-0.97][1368-0-0-2.77][2181-2-3-0.54][2476-2-2-0.44][2721-2-2-2.14][2818-1-4-0.40][2886-2-1-1.67]
[3231-2-2-3.34][3333-2-2-1.41][3482-2-2-1.94][3536-3-3-0.64][3625-1-1-2.50][3909-0-0-1.00][4035-0-3-1.86][4140-0-0-1.93][4214-1-3-0.34][4346-1-4-0.95]
[4581-2-4-1.09][4708-3-2-1.32][4838-3-4-0.20][4845-1-2-1.03][4868-0-0-3.23][4939-0-4-1.08][4984-2-2-2.57][5078-1-4-1.39][5396-0-0-3.44][5479-1-2-0.89]
[5717-0-0-2.16][5843-1-1-1.00][5949-3-0-1.34][5987-2-4-2.56][6014-3-1-0.89][6033-3-0-0.55][6313-0-0-0.64][6421-3-3-1.06][6500-1-1-1.23][6583-3-3-0.38]
[6683-3-3-1.21][6825-2-3-0.00][6998-3-3-0.16][7049-3-2-0.17][7517-1-2-2.82][7521-1-4--0.21][7528-1-4-0.37][7949-1-2-1.68][8135-1-0-2.15][8185-3-0-2.87]
[8269-3-4-1.22][8273-3-3-1.94][8543-3-0-3.14][8666-1-1-2.39][8672-0-0-3.15][8903-1-2-0.58][9001-2-1-2.30][9036-2-2-3.06][9281-3-4-0.78][9300-2-2-2.92]
[9571-0-4-0.62][9617-1-1-0.68][9644-2-2-2.17][9705-2-4-0.72][9801-0-0-0.99][9803-3-3-0.75][9865-3-3-2.45][9896-2-4-1.74][10314-1-4-1.61][10337-3-3-2.62]
[10403-0-4-1.83][10653-2-4-0.99][10704-2-2-0.94][10719-1-1-1.32][10727-1-4-1.83][10836-0-0-5.67][10969-2-3-1.42][11042-0-0-1.98][11088-1-1-4.10][11322-0-0-2.84]
[11398-2-2-4.48][11499-0-0-1.57][11502-3-3-0.81][11512-3-3-1.16][11608-1-1-1.51][11610-0-3-0.98][11692-0-0-1.13][11905-0-0-3.19][11993-1-1-1.61][12002-2-3-0.16]
[12052-0-0-2.46][12201-0-3-1.93][12235-2-4-1.84][12320-1-4-3.38][12377-2-4-2.53][12398-2-3-0.23][12503-1-1-1.69][12617-0-2-0.68][12685-3-4-0.22][12738-2-0-0.58]
[12742-2-2-3.87][12823-0-0-1.90][13110-1-1-1.15][13240-3-3-1.48][13253-1-1-1.39][13273-0-0-4.33][13634-1-4-1.69][13763-2-3-0.70][13905-3-4--0.04][14060-2-4-0.91]
[14065-3-3-0.48][14147-3-0-0.85][14595-2-2-1.49][14687-2-2-3.47][14788-2-2-1.62][14869-1-1-1.32][14872-3-4-1.50][14877-1-1-3.62][14927-0-3-1.52][15066-0-0-2.65]
[15175-1-4-1.74][15178-2-4-0.83][15375-3-3-0.96][15389-3-3-1.39][15568-2-4-1.04][15675-3-3-2.48][15869-1-4-0.30][16207-3-0-1.07][16236-0-4-0.90][16302-3-0-0.74]
[16331-2-2-3.80][16381-0-3-0.47][16488-1-1-2.95][16495-0-0-3.15][16650-0-0-3.02][16719-1-4-0.46][16801-0-0-3.89][16828-0-0-1.66][17137-3-3-0.84][17245-1-4-1.05]
[17278-3-4-0.90][17282-0-0--0.12][17311-2-2-2.21][17336-2-2-1.36][17608-3-3-2.97][17627-0-4-0.62][17877-3-4-2.04][17924-1-2-0.55][17984-3-3-2.05][18211-0-3-1.64]
[18276-3-3-1.61][18287-1-1-0.49][18394-0-0-2.15][18428-0-0-0.46][18442-0-3-2.17][18478-3-0-1.36][18607-0-0-1.64][18616-0-4-1.57][18663-0-0-0.66][18718-0-0-2.45]
[18766-2-2-1.99][18824-2-4-2.13][18890-3-2-1.13][18930-3-4-2.03][18938-3-4-0.25][19817-1-2-1.63][19839-0-4-0.79][19930-3-3-0.69][19944-0-4-2.11][20036-2-2-3.29]
[20101-3-3-0.06][20474-1-2-1.11][20547-3-4-0.85][20929-2-2-1.98][21245-1-2-0.98][21257-3-4-0.86][21293-1-2-2.15][21316-1-1-2.35][21384-1-4-2.84][21448-1-2-1.62]
[21483-0-0-2.43][21487-2-2-2.41][21714-0-4-0.36][21943-3-4-1.62][21947-0-0-1.08][21948-0-0-4.31][21965-2-2-2.28][21998-1-1-1.52][22025-0-4-1.60][22228-3-3-2.20]
[22446-1-1-1.09][22494-3-3-1.43][22757-0-0-2.16][22811-3-3-1.48][22976-3-2-1.21][22985-3-3-2.04][23014-0-0-2.71][23112-1-1-1.99][23144-3-3-2.04][23168-2-4-0.57]
[23219-0-4-0.45][23363-3-3-2.06][23470-0-4-0.62][23486-2-2-1.02][23497-0-3-2.28][23516-0-0-2.48][23690-1-1-0.67][23921-2-4-0.78][23936-1-2-2.34][24040-3-0-0.86]
[24111-1-4-2.53][24182-0-0-4.07][24238-3-3-1.24][24290-2-4-0.88][24345-0-0-1.41][24364-1-1-1.24][24427-3-0-1.44][24477-2-4-1.69][24495-2-4-0.86][24893-2-2-1.20]
[25012-1-4-0.19][25121-2-4-2.09][25165-3-3--0.43][25183-0-0-2.47][25297-3-3-2.00][25398-0-0-1.81][25574-2-2-2.16][25644-1-1-3.14][25718-1-4-0.36][25774-2-2-0.68]
[26032-3-3-1.78][26051-3-3-2.13][26120-0-4-3.22][26321-1-1-1.10][26732-1-1-2.43][26784-3-3-3.17][26827-3-4-0.67][26833-0-3-2.60][26838-2-2-0.98][26860-1-4-1.46]
[26948-0-0-1.05][27049-3-0-2.22][27098-1-0-1.38][27526-0-0-2.18][27639-3-3-0.68][27698-3-3-1.34][27772-0-0-2.95][27890-1-1-1.67][28040-0-4-1.49][28503-2-2-3.03]
[28577-1-1-1.19][28959-0-0-2.45][29198-3-4-1.49][29777-0-0-4.17][29877-2-2-1.89][30035-1-1-2.20][30098-0-0-1.27][30326-1-1-4.68][30572-2-2-1.47][30716-0-4-2.01]
[30806-2-2-0.92][30906-1-4-0.99][31007-0-4-1.82][31181-3-4-0.36][31238-0-3-1.36][31347-0-0-2.66][31422-2-4-0.97][31429-3-3-0.48][31431-0-0-1.61][31432-1-1-1.86]
[31477-0-3-2.32][31524-1-2-1.81][31597-1-2-1.98][31619-1-2-0.60][31701-0-0-2.94][31755-0-0-2.86][31854-3-3-1.23][32074-1-4-0.68][32078-3-3-1.44][32111-1-1-0.54]
[32127-1-2-2.25][32140-3-3-1.46][32263-2-4-1.26][32365-0-0-1.16][32411-2-3-2.03][32429-3-0-1.67][32473-3-0-1.05][32574-3-3-2.77][32584-0-4-2.25][32622-0-4-1.50]
[32858-3-0-1.34][32969-3-0-1.60][33016-2-2-2.38][33031-1-3-1.11][33035-2-2-2.22][33133-2-2-1.89][33173-2-2-0.96][33175-3-4-1.36][33306-3-2-1.45][33309-2-4-0.12]
[33474-0-4-0.47][33478-2-4-0.13][33618-1-4-1.09][33712-0-4-0.83][33782-2-4-2.42][33914-3-3-1.47][34076-3-4-1.28][34112-2-2-1.51][34138-2-2-2.08][34239-1-4-0.70]
[34364-2-2-2.12][34617-1-4-2.61][34751-3-3-2.52][34783-2-4-2.03][35015-3-4-1.00][35018-1-2-1.24][35288-2-2-1.22][0-4-4-1.82][1-4-4-3.44][2-4-4-1.82]
[3-4-4-2.81][4-4-4-1.25][5-4-1-0.22][6-4-0-1.57][7-4-4-2.98][8-4-2-0.51][9-4-4-1.18][10-4-4-2.96][11-4-4-2.79][12-4-2-0.64]
[14-4-4-0.99][15-4-3-1.57][16-4-4-1.57][17-4-4-0.73][18-4-4-3.51][19-4-0-2.50][20-4-4-1.29][21-4-4-1.73][22-4-4-2.36][23-4-4-1.48]
[24-4-4-4.83][25-4-4-0.88][26-4-3-0.35][27-4-4-1.04][28-4-4-2.36][29-4-1-0.75][30-4-4--0.20][31-4-4-2.08][32-4-1-1.58][33-4-4-0.92]
[34-4-4-1.25][35-4-0-1.05][37-4-4-1.30][39-4-0-3.43][40-4-4-1.23][41-4-4-1.02][42-4-4-2.05][43-4-4-1.34][45-4-4-0.23][46-4-4-3.73]
[47-4-4-4.33][48-4-4-3.42][51-4-4-2.72][52-4-4-2.09][53-4-4-1.28][54-4-4-0.44][55-4-4-1.72][56-4-4-0.66][57-4-0-1.58][58-4-2-1.90]
[59-4-4-1.55][60-4-4-1.00][61-4-4-2.87][62-4-4-0.70][63-4-2-1.50][64-4-4-1.05][65-4-4-3.33][66-4-4-2.39][67-4-4-1.31][68-4-2-0.58]
[69-4-4-1.01][70-4-4-2.50][72-4-1-1.13][73-4-4-1.76][74-4-4-1.83][75-4-4-0.88][77-4-4-3.08][78-4-4-0.96][79-4-4-2.63][80-4-4-2.69]
[81-4-4-2.48][82-4-4-0.78][83-4-4-1.18][84-4-4-2.58][85-4-4-3.27][86-4-4-1.57][87-4-4-2.40][88-4-4-1.84][89-4-2-1.22][90-4-4-0.91]
[91-4-4-1.81][92-4-4-1.38][93-4-0-1.30][94-4-4-2.84][95-4-4-1.49][96-4-4-1.36][97-4-4-2.41][98-4-2-2.48][99-4-4-1.09][100-4-4-1.55]
[101-4-4-4.52][102-4-4-1.79][103-4-4-0.53][104-4-4-2.01][105-4-4-2.56][106-4-4-2.82][107-4-4-2.05][108-4-4-1.76][109-4-4-1.78][110-4-4-1.64]
[111-4-0-2.10][112-4-4-0.81][113-4-4-1.57][114-4-2-0.62][115-4-4-1.56][116-4-4-1.55][117-4-4-2.61][119-4-2-2.16][121-4-4-1.36][122-4-4-1.85]
[124-4-2-1.42][125-4-4-2.90][126-4-4-4.08][127-4-4-1.59][128-4-4-1.35][129-4-4-1.67][130-4-4-2.07][131-4-4-1.27][132-4-4-2.07][133-4-4-4.92]
[135-4-4-1.72][136-4-4-0.79][137-4-4-1.16][138-4-4-1.41][139-4-4-2.10][140-4-1-1.39][141-4-0-1.18][142-4-4-3.45][143-4-4-3.05][144-4-4-2.54]
[145-4-4-1.73][148-4-0-3.75][149-4-4-1.45][150-4-4-2.90][151-4-4-3.00][152-4-4-2.38][153-4-4-2.85][154-4-4-3.68][155-4-4-2.89][156-4-4-0.74]
[157-4-0-1.32][158-4-4-1.66][160-4-1-0.35][161-4-2-1.47][162-4-4-1.34][164-4-4-2.25][165-4-4-0.85][167-4-4-1.69][168-4-4-1.76][170-4-4-1.31]
[171-4-4-1.68][172-4-4-3.08][173-4-4-3.25][174-4-0-1.71][175-4-4-2.00][177-4-4-3.04][178-4-4-2.50][179-4-4-2.39][180-4-4-2.50][181-4-4-1.47]
[182-4-4-0.96][183-4-4-2.28][184-4-4-2.42][186-4-4-1.40][187-4-1-1.00][188-4-4-1.90][189-4-4-1.25][190-4-4-1.21][191-4-4-2.21][192-4-4-2.06]
[193-4-2-2.39][194-4-4-0.39][195-4-0-1.05][196-4-4-1.16][197-4-4-2.16][198-4-4-4.54][199-4-2-1.38]
---------------------------
I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.690 | Acc: 75.625% | Wgt Acc: 78.955%
	I - Batch: 100 | Loss: 0.691 | Acc: 74.812% | Wgt Acc: 77.920%
	I - Batch: 150 | Loss: 0.694 | Acc: 73.833% | Wgt Acc: 77.088%
	I - Batch: 200 | Loss: 0.696 | Acc: 73.531% | Wgt Acc: 76.873%
I - num batch: 222
I - Train -- Loss: 0.696 | Acc: 73.245% | Wgt Acc: 76.540% | LR: 1.250000e-05 | Dur: 136.44s
I - Confusion Matrix: [row->prediction - col->label]
[[582.   2.  12.  68. 139.]
 [ 10. 458.  38.  17.  73.]
 [  9.  77. 600.  39. 174.]
 [ 60.  22.  37. 400.  56.]
 [ 36.  19.  47.  14. 558.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.075 | Acc: 58.580% | Wgt Acc: 53.401% | Dur: 14.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   1.   0.  15.   7.]
 [  0.  28.   4.   2.   5.]
 [  2.  27.  45.  12.  26.]
 [ 14.   1.   4.  36.   2.]
 [ 24.  21.  22.  21. 140.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.686 | Acc: 74.625% | Wgt Acc: 78.344%
	I - Batch: 100 | Loss: 0.685 | Acc: 74.750% | Wgt Acc: 78.187%
	I - Batch: 150 | Loss: 0.695 | Acc: 74.750% | Wgt Acc: 77.986%
	I - Batch: 200 | Loss: 0.686 | Acc: 74.688% | Wgt Acc: 78.040%
I - num batch: 222
I - Train -- Loss: 0.687 | Acc: 74.485% | Wgt Acc: 77.748% | LR: 1.250000e-05 | Dur: 134.71s
I - Confusion Matrix: [row->prediction - col->label]
[[586.   2.  12.  74. 125.]
 [  7. 481.  36.  20.  67.]
 [ 13.  68. 598.  27. 173.]
 [ 52.   9.  40. 404.  62.]
 [ 39.  18.  48.  13. 573.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.050 | Acc: 61.538% | Wgt Acc: 56.138% | Dur: 16.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   3.  18.  11.]
 [  0.  28.   3.   2.   6.]
 [  2.  23.  43.   5.  15.]
 [ 12.   2.   2.  40.   1.]
 [ 20.  23.  24.  21. 147.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.629 | Acc: 76.375% | Wgt Acc: 79.542%
	I - Batch: 100 | Loss: 0.662 | Acc: 75.125% | Wgt Acc: 78.470%
	I - Batch: 150 | Loss: 0.682 | Acc: 73.875% | Wgt Acc: 77.447%
	I - Batch: 200 | Loss: 0.687 | Acc: 73.844% | Wgt Acc: 77.345%
I - num batch: 222
I - Train -- Loss: 0.686 | Acc: 73.978% | Wgt Acc: 77.315% | LR: 1.250000e-05 | Dur: 140.74s
I - Confusion Matrix: [row->prediction - col->label]
[[583.   9.  11.  74. 134.]
 [  5. 475.  37.  21.  85.]
 [  9.  61. 606.  36. 153.]
 [ 62.  10.  28. 397.  65.]
 [ 38.  23.  52.  10. 563.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.113 | Acc: 58.580% | Wgt Acc: 51.886% | Dur: 15.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   2.   2.  20.   9.]
 [  0.  23.   1.   1.   3.]
 [  2.  20.  44.   9.  18.]
 [ 11.   1.   2.  31.   0.]
 [ 26.  32.  26.  25. 150.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.707 | Acc: 73.000% | Wgt Acc: 76.911%
	I - Batch: 100 | Loss: 0.681 | Acc: 73.812% | Wgt Acc: 77.415%
	I - Batch: 150 | Loss: 0.688 | Acc: 73.583% | Wgt Acc: 77.182%
	I - Batch: 200 | Loss: 0.682 | Acc: 74.406% | Wgt Acc: 77.808%
I - num batch: 222
I - Train -- Loss: 0.683 | Acc: 74.345% | Wgt Acc: 77.678% | LR: 1.250000e-05 | Dur: 134.22s
I - Confusion Matrix: [row->prediction - col->label]
[[579.   5.   5.  71. 119.]
 [  8. 471.  34.  20.  70.]
 [  5.  69. 617.  32. 173.]
 [ 67.  15.  30. 403.  71.]
 [ 38.  18.  48.  12. 567.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.092 | Acc: 60.552% | Wgt Acc: 55.020% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   3.   2.  18.  11.]
 [  0.  27.   3.   2.   3.]
 [  2.  23.  44.   8.  19.]
 [ 12.   0.   3.  38.   1.]
 [ 22.  25.  23.  20. 146.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.670 | Acc: 74.625% | Wgt Acc: 77.045%
	I - Batch: 100 | Loss: 0.661 | Acc: 73.938% | Wgt Acc: 77.221%
	I - Batch: 150 | Loss: 0.667 | Acc: 73.833% | Wgt Acc: 77.375%
	I - Batch: 200 | Loss: 0.676 | Acc: 73.531% | Wgt Acc: 77.291%
I - num batch: 222
I - Train -- Loss: 0.675 | Acc: 73.724% | Wgt Acc: 77.480% | LR: 1.250000e-05 | Dur: 134.86s
I - Confusion Matrix: [row->prediction - col->label]
[[591.   5.  10.  69. 148.]
 [  7. 474.  39.  19.  80.]
 [ 10.  65. 608.  35. 169.]
 [ 55.  15.  33. 403.  64.]
 [ 34.  19.  44.  12. 539.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.084 | Acc: 60.750% | Wgt Acc: 54.258% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   2.   1.  14.   7.]
 [  0.  26.   4.   2.   3.]
 [  2.  20.  42.  10.  15.]
 [ 11.   1.   2.  36.   2.]
 [ 24.  29.  26.  24. 153.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.638 | Acc: 77.375% | Wgt Acc: 80.743%
	I - Batch: 100 | Loss: 0.650 | Acc: 76.688% | Wgt Acc: 79.714%
	I - Batch: 150 | Loss: 0.668 | Acc: 74.667% | Wgt Acc: 78.171%
	I - Batch: 200 | Loss: 0.674 | Acc: 74.312% | Wgt Acc: 77.795%
I - num batch: 222
I - Train -- Loss: 0.674 | Acc: 74.373% | Wgt Acc: 77.850% | LR: 1.250000e-05 | Dur: 137.22s
I - Confusion Matrix: [row->prediction - col->label]
[[587.   4.   7.  60. 128.]
 [  6. 469.  38.  20.  78.]
 [ 12.  67. 607.  29. 170.]
 [ 55.  15.  33. 414.  63.]
 [ 37.  23.  49.  15. 561.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.050 | Acc: 59.961% | Wgt Acc: 54.686% | Dur: 20.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   2.   2.  15.   7.]
 [  0.  33.   4.   2.   5.]
 [  2.  22.  44.   7.  24.]
 [ 13.   1.   2.  34.   1.]
 [ 23.  20.  23.  28. 143.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.652 | Acc: 75.500% | Wgt Acc: 79.391%
	I - Batch: 100 | Loss: 0.678 | Acc: 73.125% | Wgt Acc: 77.028%
	I - Batch: 150 | Loss: 0.676 | Acc: 73.958% | Wgt Acc: 77.338%
	I - Batch: 200 | Loss: 0.683 | Acc: 73.938% | Wgt Acc: 77.265%
I - num batch: 222
I - Train -- Loss: 0.678 | Acc: 73.865% | Wgt Acc: 77.406% | LR: 1.250000e-05 | Dur: 135.19s
I - Confusion Matrix: [row->prediction - col->label]
[[592.   5.   6.  68. 143.]
 [  8. 466.  49.  15.  68.]
 [ 12.  69. 602.  32. 164.]
 [ 52.  17.  28. 408.  73.]
 [ 33.  21.  49.  15. 552.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.037 | Acc: 60.355% | Wgt Acc: 55.041% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   1.  18.   9.]
 [  0.  32.   8.   4.   8.]
 [  1.  23.  41.   8.  18.]
 [ 10.   2.   3.  35.   1.]
 [ 23.  19.  22.  21. 144.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.633 | Acc: 76.875% | Wgt Acc: 80.533%
	I - Batch: 100 | Loss: 0.641 | Acc: 76.875% | Wgt Acc: 80.004%
	I - Batch: 150 | Loss: 0.649 | Acc: 76.250% | Wgt Acc: 79.319%
	I - Batch: 200 | Loss: 0.657 | Acc: 75.750% | Wgt Acc: 78.793%
I - num batch: 222
I - Train -- Loss: 0.662 | Acc: 75.444% | Wgt Acc: 78.532% | LR: 1.250000e-05 | Dur: 135.55s
I - Confusion Matrix: [row->prediction - col->label]
[[580.   6.   7.  72. 128.]
 [ 10. 486.  34.  21.  62.]
 [  9.  54. 616.  28. 152.]
 [ 62.  13.  24. 403.  67.]
 [ 36.  19.  53.  14. 591.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.006 | Acc: 62.722% | Wgt Acc: 58.009% | Dur: 14.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   2.  19.  11.]
 [  0.  34.   4.   4.   6.]
 [  1.  19.  38.   4.  18.]
 [  5.   2.   3.  38.   1.]
 [ 18.  21.  28.  21. 144.]]

I - Local maximum validation set accuracy:  62.72

I - Validation set results: 
[14-1-2-1.21][50-3-4-0.73][124-2-2-1.30][127-0-0-3.86][443-2-2-2.85][567-0-0-1.32][573-1-1-1.43][615-0-0-0.89][695-1-2-1.64][722-3-0-3.51]
[826-0-0-1.77][878-0-0-1.71][1103-0-4-1.23][1212-3-4-0.76][1368-0-0-2.62][2181-2-3-0.56][2476-2-2-0.15][2721-2-2-1.99][2818-1-4-0.36][2886-2-1-2.14]
[3231-2-2-3.17][3333-2-2-1.26][3482-2-4-1.58][3536-3-3-0.50][3625-1-1-2.46][3909-0-0-1.23][4035-0-0-1.77][4140-0-0-1.57][4214-1-4-1.59][4346-1-3-0.27]
[4581-2-2-1.42][4708-3-4-1.36][4838-3-4-0.42][4845-1-2-1.01][4868-0-0-2.85][4939-0-4-0.81][4984-2-2-2.62][5078-1-4-1.25][5396-0-0-3.56][5479-1-2-0.58]
[5717-0-0-2.46][5843-1-2-0.88][5949-3-0-1.30][5987-2-4-2.13][6014-3-1-0.84][6033-3-0-1.17][6313-0-0-0.25][6421-3-3-0.66][6500-1-1-1.36][6583-3-3-0.29]
[6683-3-3-1.55][6825-2-1-0.43][6998-3-4--0.02][7049-3-3-0.03][7517-1-2-2.46][7521-1-0--0.07][7528-1-2-1.34][7949-1-2-1.70][8135-1-0-0.88][8185-3-0-3.04]
[8269-3-2-0.70][8273-3-3-1.77][8543-3-0-4.18][8666-1-1-1.39][8672-0-0-3.33][8903-1-2-0.59][9001-2-1-2.10][9036-2-2-3.20][9281-3-4-0.65][9300-2-2-3.02]
[9571-0-4-0.42][9617-1-1-1.70][9644-2-2-1.75][9705-2-4-0.40][9801-0-0-1.45][9803-3-3-0.37][9865-3-3-2.27][9896-2-4-1.11][10314-1-4-1.76][10337-3-3-2.43]
[10403-0-4-1.49][10653-2-4-0.39][10704-2-2-0.73][10719-1-1-1.78][10727-1-4-1.42][10836-0-0-6.09][10969-2-3-1.17][11042-0-0-2.02][11088-1-1-4.77][11322-0-0-3.41]
[11398-2-4-1.33][11499-0-0-1.65][11502-3-3-0.73][11512-3-3-0.35][11608-1-1-1.52][11610-0-0-2.02][11692-0-0-1.50][11905-0-0-2.64][11993-1-1-2.54][12002-2-0-1.87]
[12052-0-0-1.81][12201-0-0-1.98][12235-2-4-1.63][12320-1-4-2.58][12377-2-4-2.79][12398-2-4-0.54][12503-1-1-2.95][12617-0-2-0.24][12685-3-4-0.21][12738-2-3--0.05]
[12742-2-2-3.53][12823-0-0-1.14][13110-1-1-2.71][13240-3-0-1.23][13253-1-1-1.56][13273-0-0-4.14][13634-1-4-1.61][13763-2-2-0.49][13905-3-4--0.21][14060-2-4-1.13]
[14065-3-3-0.20][14147-3-0-1.43][14595-2-2-1.00][14687-2-2-3.47][14788-2-2-1.78][14869-1-4-1.26][14872-3-4-1.30][14877-1-1-4.13][14927-0-3-1.44][15066-0-0-3.04]
[15175-1-4-1.65][15178-2-4-1.09][15375-3-3-0.62][15389-3-0-1.93][15568-2-4-0.89][15675-3-3-1.71][15869-1-4-0.69][16207-3-0-0.61][16236-0-4-0.30][16302-3-4-0.31]
[16331-2-2-3.82][16381-0-4-0.50][16488-1-1-4.20][16495-0-0-2.59][16650-0-0-3.55][16719-1-2-0.44][16801-0-0-4.06][16828-0-0-1.73][17137-3-3-1.08][17245-1-1-0.42]
[17278-3-3--0.20][17282-0-0--0.02][17311-2-2-2.27][17336-2-1-1.77][17608-3-3-2.17][17627-0-0-1.05][17877-3-4-0.80][17924-1-4-0.14][17984-3-3-1.75][18211-0-3-0.95]
[18276-3-3-1.43][18287-1-1-1.09][18394-0-0-2.43][18428-0-0-1.75][18442-0-0-1.70][18478-3-0-1.43][18607-0-0-1.66][18616-0-4-1.36][18663-0-0-0.60][18718-0-0-2.96]
[18766-2-2-1.35][18824-2-4-1.68][18890-3-1-0.61][18930-3-4-1.91][18938-3-2-0.36][19817-1-2-1.39][19839-0-0-0.50][19930-3-3-0.87][19944-0-4-2.34][20036-2-2-3.09]
[20101-3-3-0.93][20474-1-2-0.69][20547-3-4-1.18][20929-2-2-2.42][21245-1-1-0.71][21257-3-2-0.47][21293-1-1-1.87][21316-1-1-2.31][21384-1-4-2.67][21448-1-1-1.44]
[21483-0-0-2.53][21487-2-2-2.53][21714-0-0-0.24][21943-3-2-1.42][21947-0-0-2.80][21948-0-0-4.26][21965-2-2-2.29][21998-1-1-1.83][22025-0-4-1.81][22228-3-3-1.66]
[22446-1-1-1.22][22494-3-0-1.61][22757-0-0-2.51][22811-3-3-0.55][22976-3-4-1.55][22985-3-3-1.84][23014-0-0-2.50][23112-1-1-2.34][23144-3-3-1.89][23168-2-4-0.64]
[23219-0-4-0.91][23363-3-3-1.56][23470-0-4-0.43][23486-2-4-0.65][23497-0-3-2.74][23516-0-0-2.46][23690-1-4-1.82][23921-2-4-0.86][23936-1-2-2.26][24040-3-4-1.12]
[24111-1-4-2.26][24182-0-0-3.64][24238-3-3-1.12][24290-2-4-0.68][24345-0-0-1.35][24364-1-1-1.74][24427-3-0-1.51][24477-2-4-1.34][24495-2-4-0.86][24893-2-2-0.76]
[25012-1-1-0.40][25121-2-4-2.00][25165-3-1--0.43][25183-0-0-3.32][25297-3-3-1.96][25398-0-0-1.94][25574-2-2-2.46][25644-1-1-3.81][25718-1-4-0.47][25774-2-4-0.54]
[26032-3-0-1.87][26051-3-3-2.20][26120-0-4-1.88][26321-1-2-0.64][26732-1-1-2.28][26784-3-3-3.27][26827-3-3-0.30][26833-0-3-2.23][26838-2-4-0.05][26860-1-2-1.15]
[26948-0-0-0.85][27049-3-0-2.23][27098-1-4-0.44][27526-0-0-2.19][27639-3-4-0.61][27698-3-3-1.20][27772-0-0-3.28][27890-1-1-2.56][28040-0-4-0.69][28503-2-2-2.89]
[28577-1-1-1.90][28959-0-0-2.77][29198-3-4-0.93][29777-0-0-4.25][29877-2-2-1.96][30035-1-1-2.32][30098-0-0-1.82][30326-1-1-4.50][30572-2-2-1.56][30716-0-4-1.79]
[30806-2-2-0.68][30906-1-1-2.23][31007-0-0-0.36][31181-3-4-0.06][31238-0-3-1.38][31347-0-0-3.91][31422-2-4-0.93][31429-3-3-0.10][31431-0-0--0.05][31432-1-1-1.86]
[31477-0-0-2.27][31524-1-4-0.39][31597-1-2-1.46][31619-1-4-0.26][31701-0-0-3.62][31755-0-0-2.88][31854-3-3-1.01][32074-1-2-1.30][32078-3-3-1.63][32111-1-1-1.61]
[32127-1-2-1.78][32140-3-3-1.41][32263-2-4-1.23][32365-0-0-0.51][32411-2-0-2.29][32429-3-0-1.50][32473-3-0-1.07][32574-3-0-2.71][32584-0-4-2.22][32622-0-4-1.43]
[32858-3-0-1.51][32969-3-0-1.99][33016-2-2-2.95][33031-1-3-0.52][33035-2-2-1.62][33133-2-2-2.00][33173-2-2-0.60][33175-3-4-1.15][33306-3-1-1.39][33309-2-4-0.27]
[33474-0-4-0.52][33478-2-4--0.02][33618-1-4-1.20][33712-0-4-0.88][33782-2-4-2.21][33914-3-3-1.77][34076-3-4-1.25][34112-2-2-1.81][34138-2-2-1.64][34239-1-4-0.59]
[34364-2-2-2.29][34617-1-2-2.38][34751-3-3-2.17][34783-2-4-1.28][35015-3-4-0.94][35018-1-4-1.04][35288-2-2-1.01][0-4-4-0.75][1-4-4-3.02][2-4-4-1.35]
[3-4-4-2.86][4-4-4-0.90][5-4-1-1.85][6-4-4-3.69][7-4-4-2.79][8-4-4-0.11][9-4-4-1.10][10-4-4-3.65][11-4-4-2.60][12-4-2-0.85]
[14-4-4-0.92][15-4-0-1.63][16-4-4-2.10][17-4-4-0.98][18-4-4-3.60][19-4-0-2.61][20-4-4-0.90][21-4-4-1.31][22-4-4-2.11][23-4-4-1.74]
[24-4-4-5.00][25-4-4-0.81][26-4-4-0.90][27-4-2-0.98][28-4-4-2.66][29-4-2-1.11][30-4-4-0.84][31-4-4-2.26][32-4-4-1.27][33-4-4-1.32]
[34-4-4-1.19][35-4-0-1.31][37-4-4-2.24][39-4-0-2.58][40-4-4-0.99][41-4-4-0.58][42-4-4-0.51][43-4-4-0.54][45-4-4-0.23][46-4-4-3.20]
[47-4-4-4.02][48-4-4-3.10][51-4-4-2.95][52-4-4-1.84][53-4-4-1.50][54-4-4-0.32][55-4-4-1.83][56-4-1-0.91][57-4-0-0.59][58-4-2-1.65]
[59-4-4-1.26][60-4-4-0.74][61-4-4-1.79][62-4-4-1.20][63-4-4-1.22][64-4-4-1.31][65-4-4-2.94][66-4-4-2.07][67-4-4-0.84][68-4-4-0.48]
[69-4-4-0.64][70-4-4-2.61][72-4-1-1.41][73-4-1-1.45][74-4-2-1.20][75-4-4-0.70][77-4-4-2.79][78-4-4-0.63][79-4-4-2.45][80-4-4-2.24]
[81-4-1-1.56][82-4-4-0.99][83-4-4-1.04][84-4-4-3.53][85-4-4-3.15][86-4-4-0.83][87-4-4-2.34][88-4-4-2.58][89-4-2-0.96][90-4-4-0.71]
[91-4-4-1.96][92-4-4-1.40][93-4-0-1.47][94-4-4-2.60][95-4-4-0.61][96-4-4-1.34][97-4-4-2.39][98-4-2-1.94][99-4-4-1.26][100-4-4-1.75]
[101-4-4-4.16][102-4-4-2.01][103-4-4-0.45][104-4-4-1.46][105-4-2-1.41][106-4-4-2.71][107-4-4-2.12][108-4-4-1.23][109-4-4-1.42][110-4-4-1.91]
[111-4-0-2.21][112-4-4-0.56][113-4-4-1.78][114-4-2-0.58][115-4-4-1.79][116-4-0-0.67][117-4-4-2.60][119-4-2-1.56][121-4-4-2.16][122-4-4-2.45]
[124-4-2-0.82][125-4-4-2.73][126-4-4-4.23][127-4-2-1.65][128-4-4-1.17][129-4-4-1.49][130-4-4-2.76][131-4-2-1.21][132-4-4-0.94][133-4-4-4.60]
[135-4-2-1.46][136-4-4-0.74][137-4-4-1.03][138-4-4-1.56][139-4-4-2.15][140-4-1-2.43][141-4-0-0.91][142-4-4-3.43][143-4-4-2.72][144-4-4-2.45]
[145-4-4-1.82][148-4-0-3.59][149-4-4-1.38][150-4-4-2.91][151-4-4-2.70][152-4-4-2.41][153-4-4-2.73][154-4-4-2.17][155-4-4-2.64][156-4-4-0.75]
[157-4-4-0.58][158-4-4-1.77][160-4-4-0.59][161-4-2-0.75][162-4-4-1.49][164-4-4-1.97][165-4-4-0.99][167-4-4-1.72][168-4-4-1.45][170-4-4-1.29]
[171-4-4-2.05][172-4-4-2.97][173-4-4-3.09][174-4-0-1.88][175-4-4-1.77][177-4-4-3.28][178-4-4-1.40][179-4-4-1.80][180-4-4-2.14][181-4-4-0.81]
[182-4-4-0.83][183-4-4-2.37][184-4-4-2.70][186-4-4-0.56][187-4-2-1.04][188-4-4-1.99][189-4-4-0.79][190-4-4-1.17][191-4-4-2.43][192-4-4-1.41]
[193-4-2-2.67][194-4-3-0.37][195-4-4-0.60][196-4-4-1.39][197-4-4-2.33][198-4-4-4.60][199-4-2-0.98]
---------------------------
I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.689 | Acc: 73.625% | Wgt Acc: 76.890%
	I - Batch: 100 | Loss: 0.660 | Acc: 74.250% | Wgt Acc: 78.092%
	I - Batch: 150 | Loss: 0.657 | Acc: 74.917% | Wgt Acc: 78.200%
	I - Batch: 200 | Loss: 0.656 | Acc: 74.562% | Wgt Acc: 78.126%
I - num batch: 222
I - Train -- Loss: 0.663 | Acc: 74.204% | Wgt Acc: 77.850% | LR: 1.250000e-05 | Dur: 134.73s
I - Confusion Matrix: [row->prediction - col->label]
[[582.   5.   4.  67. 143.]
 [  7. 479.  40.  17.  89.]
 [  8.  62. 607.  25. 156.]
 [ 57.  17.  34. 413.  61.]
 [ 43.  15.  49.  16. 551.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.085 | Acc: 59.566% | Wgt Acc: 53.798% | Dur: 18.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   1.   2.  10.   5.]
 [  0.  27.   4.   2.   5.]
 [  1.  22.  47.   9.  22.]
 [ 13.   1.   3.  37.   2.]
 [ 29.  27.  19.  28. 146.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.643 | Acc: 74.625% | Wgt Acc: 78.518%
	I - Batch: 100 | Loss: 0.646 | Acc: 74.688% | Wgt Acc: 78.307%
	I - Batch: 150 | Loss: 0.657 | Acc: 74.750% | Wgt Acc: 78.257%
	I - Batch: 200 | Loss: 0.666 | Acc: 74.375% | Wgt Acc: 77.930%
I - num batch: 222
I - Train -- Loss: 0.665 | Acc: 74.147% | Wgt Acc: 77.782% | LR: 1.250000e-05 | Dur: 133.28s
I - Confusion Matrix: [row->prediction - col->label]
[[588.   1.   9.  71. 135.]
 [  6. 478.  33.  20.  76.]
 [ 12.  66. 608.  29. 171.]
 [ 52.  14.  32. 406.  68.]
 [ 39.  19.  52.  12. 550.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.086 | Acc: 60.552% | Wgt Acc: 53.976% | Dur: 15.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   1.  18.   9.]
 [  0.  26.   4.   1.   4.]
 [  0.  21.  39.   6.  14.]
 [ 10.   1.   3.  34.   0.]
 [ 23.  28.  28.  27. 153.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.681 | Acc: 74.750% | Wgt Acc: 77.714%
	I - Batch: 100 | Loss: 0.648 | Acc: 76.312% | Wgt Acc: 79.533%
	I - Batch: 150 | Loss: 0.653 | Acc: 75.125% | Wgt Acc: 78.475%
	I - Batch: 200 | Loss: 0.656 | Acc: 74.812% | Wgt Acc: 78.370%
I - num batch: 222
I - Train -- Loss: 0.661 | Acc: 74.485% | Wgt Acc: 78.212% | LR: 1.250000e-05 | Dur: 132.87s
I - Confusion Matrix: [row->prediction - col->label]
[[588.   6.   6.  66. 122.]
 [  5. 486.  44.  16.  80.]
 [ 12.  57. 603.  27. 172.]
 [ 50.  11.  32. 415.  76.]
 [ 42.  18.  49.  14. 550.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.082 | Acc: 58.383% | Wgt Acc: 52.575% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   2.   0.  11.   8.]
 [  0.  28.   4.   3.   7.]
 [  1.  29.  44.  11.  20.]
 [ 12.   1.   3.  34.   1.]
 [ 29.  18.  24.  27. 144.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.604 | Acc: 78.750% | Wgt Acc: 81.824%
	I - Batch: 100 | Loss: 0.621 | Acc: 77.812% | Wgt Acc: 81.052%
	I - Batch: 150 | Loss: 0.636 | Acc: 76.583% | Wgt Acc: 79.668%
	I - Batch: 200 | Loss: 0.638 | Acc: 76.531% | Wgt Acc: 79.781%
I - num batch: 208
I - Train -- Loss: 0.642 | Acc: 76.352% | Wgt Acc: 79.462% | LR: 1.250000e-05 | Dur: 126.86s
I - Confusion Matrix: [row->prediction - col->label]
[[580.   1.   7.  64.  95.]
 [  5. 484.  29.  19.  61.]
 [ 10.  61. 628.  35. 134.]
 [ 64.  17.  26. 412.  54.]
 [ 38.  15.  44.   8. 437.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 57.594% | Wgt Acc: 51.280% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 44.   2.   1.  15.   8.]
 [  0.  26.   3.   2.   6.]
 [  1.  24.  44.   9.  19.]
 [ 12.   1.   2.  32.   1.]
 [ 31.  25.  25.  28. 146.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.678 | Acc: 73.750% | Wgt Acc: 77.683%
	I - Batch: 100 | Loss: 0.666 | Acc: 74.000% | Wgt Acc: 78.188%
	I - Batch: 150 | Loss: 0.668 | Acc: 74.083% | Wgt Acc: 77.976%
	I - Batch: 200 | Loss: 0.664 | Acc: 74.062% | Wgt Acc: 77.822%
I - num batch: 222
I - Train -- Loss: 0.667 | Acc: 74.034% | Wgt Acc: 77.701% | LR: 1.250000e-05 | Dur: 136.73s
I - Confusion Matrix: [row->prediction - col->label]
[[575.   9.   9.  68. 154.]
 [  7. 474.  29.  24.  73.]
 [ 10.  61. 624.  27. 161.]
 [ 60.  13.  32. 406.  65.]
 [ 45.  21.  40.  13. 547.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.084 | Acc: 59.369% | Wgt Acc: 53.704% | Dur: 21.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   1.   1.  14.   9.]
 [  0.  34.   4.   2.   7.]
 [  2.  22.  41.  10.  19.]
 [  9.   1.   3.  34.   0.]
 [ 30.  20.  26.  26. 145.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.656 | Acc: 74.250% | Wgt Acc: 78.187%
	I - Batch: 100 | Loss: 0.649 | Acc: 74.562% | Wgt Acc: 78.489%
	I - Batch: 150 | Loss: 0.640 | Acc: 74.458% | Wgt Acc: 78.434%
	I - Batch: 200 | Loss: 0.643 | Acc: 74.188% | Wgt Acc: 78.105%
I - num batch: 222
I - Train -- Loss: 0.643 | Acc: 74.232% | Wgt Acc: 78.131% | LR: 1.250000e-05 | Dur: 135.60s
I - Confusion Matrix: [row->prediction - col->label]
[[588.   2.   4.  71. 137.]
 [  2. 485.  38.  15.  90.]
 [  6.  61. 617.  32. 176.]
 [ 57.  15.  30. 406.  60.]
 [ 44.  15.  45.  14. 537.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.063 | Acc: 59.369% | Wgt Acc: 53.767% | Dur: 14.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   1.   0.  13.   8.]
 [  0.  28.   3.   0.   5.]
 [  2.  27.  42.   9.  21.]
 [ 17.   2.   5.  40.   1.]
 [ 23.  20.  25.  24. 145.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.629 | Acc: 76.625% | Wgt Acc: 80.158%
	I - Batch: 100 | Loss: 0.636 | Acc: 75.250% | Wgt Acc: 79.004%
	I - Batch: 150 | Loss: 0.654 | Acc: 74.250% | Wgt Acc: 77.866%
	I - Batch: 200 | Loss: 0.653 | Acc: 74.500% | Wgt Acc: 78.007%
I - num batch: 222
I - Train -- Loss: 0.650 | Acc: 74.880% | Wgt Acc: 78.304% | LR: 1.250000e-05 | Dur: 134.70s
I - Confusion Matrix: [row->prediction - col->label]
[[593.   6.   7.  66. 136.]
 [  5. 487.  42.  15.  70.]
 [  8.  51. 590.  29. 154.]
 [ 48.  15.  37. 416.  70.]
 [ 43.  19.  58.  12. 570.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.104 | Acc: 59.566% | Wgt Acc: 52.575% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   1.   2.  13.   6.]
 [  0.  24.   4.   1.   2.]
 [  1.  23.  41.   9.  17.]
 [ 11.   1.   2.  32.   1.]
 [ 25.  29.  26.  31. 154.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.653 | Acc: 74.500% | Wgt Acc: 78.247%
	I - Batch: 100 | Loss: 0.638 | Acc: 75.625% | Wgt Acc: 79.402%
	I - Batch: 150 | Loss: 0.642 | Acc: 75.625% | Wgt Acc: 79.072%
	I - Batch: 200 | Loss: 0.649 | Acc: 75.219% | Wgt Acc: 78.692%
I - num batch: 222
I - Train -- Loss: 0.650 | Acc: 75.218% | Wgt Acc: 78.811% | LR: 1.250000e-05 | Dur: 137.06s
I - Confusion Matrix: [row->prediction - col->label]
[[592.   3.   8.  59. 147.]
 [  8. 482.  33.  15.  75.]
 [ 12.  57. 618.  35. 165.]
 [ 45.  17.  30. 413.  50.]
 [ 40.  19.  45.  16. 563.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.073 | Acc: 58.383% | Wgt Acc: 53.108% | Dur: 19.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   1.   0.  15.  10.]
 [  0.  25.   5.   3.   6.]
 [  1.  24.  45.   7.  23.]
 [ 12.   1.   4.  35.   1.]
 [ 24.  27.  21.  26. 140.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.633 | Acc: 76.000% | Wgt Acc: 79.599%
	I - Batch: 100 | Loss: 0.642 | Acc: 75.438% | Wgt Acc: 79.010%
	I - Batch: 150 | Loss: 0.647 | Acc: 75.167% | Wgt Acc: 78.582%
	I - Batch: 200 | Loss: 0.637 | Acc: 75.438% | Wgt Acc: 78.946%
I - num batch: 222
I - Train -- Loss: 0.644 | Acc: 74.937% | Wgt Acc: 78.408% | LR: 1.250000e-05 | Dur: 133.76s
I - Confusion Matrix: [row->prediction - col->label]
[[594.   2.   5.  66. 141.]
 [  7. 479.  33.  23.  64.]
 [ 12.  58. 619.  35. 164.]
 [ 48.  15.  27. 401.  66.]
 [ 36.  24.  50.  13. 565.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.097 | Acc: 58.974% | Wgt Acc: 52.910% | Dur: 16.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 44.   0.   1.   9.   4.]
 [  0.  30.   3.   4.   8.]
 [  2.  21.  44.   8.  20.]
 [ 13.   1.   2.  34.   1.]
 [ 29.  26.  25.  31. 147.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.627 | Acc: 76.875% | Wgt Acc: 80.226%
	I - Batch: 100 | Loss: 0.640 | Acc: 76.188% | Wgt Acc: 79.551%
	I - Batch: 150 | Loss: 0.639 | Acc: 75.833% | Wgt Acc: 79.018%
	I - Batch: 200 | Loss: 0.644 | Acc: 75.531% | Wgt Acc: 78.953%
I - num batch: 222
I - Train -- Loss: 0.642 | Acc: 75.670% | Wgt Acc: 79.117% | LR: 1.250000e-05 | Dur: 134.62s
I - Confusion Matrix: [row->prediction - col->label]
[[600.   3.   9.  66. 121.]
 [  4. 487.  43.  16.  70.]
 [  5.  47. 610.  33. 169.]
 [ 53.  19.  28. 412.  65.]
 [ 35.  22.  44.  11. 575.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.071 | Acc: 60.355% | Wgt Acc: 53.056% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   1.   2.  28.  10.]
 [  0.  27.   4.   2.   4.]
 [  1.  18.  34.   3.   9.]
 [  6.   1.   3.  27.   0.]
 [ 20.  31.  32.  26. 157.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.661 | Acc: 74.375% | Wgt Acc: 77.472%
	I - Batch: 100 | Loss: 0.647 | Acc: 74.938% | Wgt Acc: 78.410%
	I - Batch: 150 | Loss: 0.641 | Acc: 75.667% | Wgt Acc: 78.822%
	I - Batch: 200 | Loss: 0.637 | Acc: 75.719% | Wgt Acc: 78.974%
I - num batch: 222
I - Train -- Loss: 0.638 | Acc: 75.641% | Wgt Acc: 79.033% | LR: 1.250000e-05 | Dur: 135.60s
I - Confusion Matrix: [row->prediction - col->label]
[[585.   3.   8.  57. 129.]
 [  9. 490.  37.  18.  75.]
 [ 11.  48. 618.  36. 146.]
 [ 53.  15.  29. 412.  72.]
 [ 39.  22.  42.  15. 578.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.049 | Acc: 60.158% | Wgt Acc: 55.323% | Dur: 14.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   1.   1.  19.   8.]
 [  0.  26.   4.   3.   8.]
 [  1.  28.  46.   7.  21.]
 [ 11.   2.   3.  37.   3.]
 [ 20.  21.  21.  20. 140.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.639 | Acc: 75.000% | Wgt Acc: 78.459%
	I - Batch: 100 | Loss: 0.629 | Acc: 75.938% | Wgt Acc: 79.341%
	I - Batch: 150 | Loss: 0.635 | Acc: 75.500% | Wgt Acc: 79.001%
	I - Batch: 200 | Loss: 0.639 | Acc: 75.156% | Wgt Acc: 78.700%
I - num batch: 222
I - Train -- Loss: 0.639 | Acc: 75.134% | Wgt Acc: 78.683% | LR: 1.250000e-05 | Dur: 135.56s
I - Confusion Matrix: [row->prediction - col->label]
[[587.   5.   7.  67. 134.]
 [  6. 488.  44.  16.  76.]
 [  7.  56. 620.  31. 150.]
 [ 55.  12.  26. 406.  76.]
 [ 42.  17.  37.  18. 564.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.032 | Acc: 61.144% | Wgt Acc: 55.574% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   3.   1.  21.   9.]
 [  0.  32.   5.   2.   5.]
 [  1.  21.  43.   5.  18.]
 [ 11.   1.   4.  34.   1.]
 [ 22.  21.  22.  24. 147.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.628 | Acc: 78.250% | Wgt Acc: 81.242%
	I - Batch: 100 | Loss: 0.626 | Acc: 77.750% | Wgt Acc: 80.671%
	I - Batch: 150 | Loss: 0.627 | Acc: 77.125% | Wgt Acc: 80.409%
	I - Batch: 200 | Loss: 0.637 | Acc: 76.125% | Wgt Acc: 79.560%
I - num batch: 222
I - Train -- Loss: 0.636 | Acc: 76.092% | Wgt Acc: 79.673% | LR: 1.250000e-05 | Dur: 135.06s
I - Confusion Matrix: [row->prediction - col->label]
[[602.   3.   7.  74. 145.]
 [  5. 497.  31.  16.  78.]
 [  8.  53. 618.  27. 149.]
 [ 46.  11.  33. 410.  56.]
 [ 36.  14.  45.  11. 572.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.071 | Acc: 59.961% | Wgt Acc: 52.962% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 42.   1.   0.   7.   7.]
 [  0.  23.   3.   2.   2.]
 [  1.  25.  42.   5.  13.]
 [ 15.   1.   3.  41.   2.]
 [ 30.  28.  27.  31. 156.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.607 | Acc: 76.750% | Wgt Acc: 80.015%
	I - Batch: 100 | Loss: 0.610 | Acc: 76.812% | Wgt Acc: 80.341%
	I - Batch: 150 | Loss: 0.630 | Acc: 75.917% | Wgt Acc: 79.621%
	I - Batch: 200 | Loss: 0.637 | Acc: 75.594% | Wgt Acc: 79.339%
I - num batch: 222
I - Train -- Loss: 0.633 | Acc: 75.811% | Wgt Acc: 79.532% | LR: 1.250000e-05 | Dur: 135.21s
I - Confusion Matrix: [row->prediction - col->label]
[[595.   5.   6.  66. 148.]
 [  7. 493.  40.  15.  76.]
 [  6.  47. 619.  27. 147.]
 [ 45.  15.  24. 419.  66.]
 [ 44.  18.  45.  11. 563.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.052 | Acc: 62.130% | Wgt Acc: 55.104% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  18.   9.]
 [  0.  27.   2.   2.   2.]
 [  0.  16.  33.   1.   8.]
 [ 11.   1.   4.  41.   1.]
 [ 23.  32.  34.  24. 160.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.615 | Acc: 76.625% | Wgt Acc: 79.824%
	I - Batch: 100 | Loss: 0.621 | Acc: 75.562% | Wgt Acc: 79.050%
	I - Batch: 150 | Loss: 0.627 | Acc: 75.583% | Wgt Acc: 79.028%
	I - Batch: 200 | Loss: 0.628 | Acc: 75.312% | Wgt Acc: 78.949%
I - num batch: 222
I - Train -- Loss: 0.630 | Acc: 75.134% | Wgt Acc: 78.720% | LR: 1.250000e-05 | Dur: 136.10s
I - Confusion Matrix: [row->prediction - col->label]
[[580.   3.   7.  59. 143.]
 [  5. 483.  36.  17.  74.]
 [  7.  55. 624.  29. 148.]
 [ 62.  17.  26. 415.  72.]
 [ 43.  20.  41.  18. 563.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.121 | Acc: 60.158% | Wgt Acc: 54.164% | Dur: 14.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   2.   1.   7.   6.]
 [  0.  28.   4.   1.   5.]
 [  2.  27.  44.   9.  19.]
 [ 12.   1.   3.  39.   1.]
 [ 29.  20.  23.  30. 149.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.617 | Acc: 76.625% | Wgt Acc: 79.662%
	I - Batch: 100 | Loss: 0.618 | Acc: 76.812% | Wgt Acc: 80.314%
	I - Batch: 150 | Loss: 0.615 | Acc: 76.958% | Wgt Acc: 80.222%
	I - Batch: 200 | Loss: 0.617 | Acc: 76.656% | Wgt Acc: 80.220%
I - num batch: 222
I - Train -- Loss: 0.620 | Acc: 76.515% | Wgt Acc: 80.033% | LR: 1.250000e-05 | Dur: 139.39s
I - Confusion Matrix: [row->prediction - col->label]
[[597.   4.   6.  67. 133.]
 [  6. 497.  37.  13.  66.]
 [  8.  50. 624.  29. 159.]
 [ 46.  10.  28. 416.  62.]
 [ 40.  17.  39.  13. 580.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.058 | Acc: 61.144% | Wgt Acc: 54.999% | Dur: 14.89s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   0.  21.   9.]
 [  0.  27.   3.   1.   5.]
 [  0.  19.  41.   5.  14.]
 [ 10.   2.   4.  35.   1.]
 [ 22.  28.  27.  24. 151.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.638 | Acc: 77.375% | Wgt Acc: 80.537%
	I - Batch: 100 | Loss: 0.644 | Acc: 77.125% | Wgt Acc: 80.185%
	I - Batch: 150 | Loss: 0.633 | Acc: 77.083% | Wgt Acc: 80.416%
	I - Batch: 200 | Loss: 0.626 | Acc: 77.188% | Wgt Acc: 80.722%
I - num batch: 222
I - Train -- Loss: 0.629 | Acc: 76.938% | Wgt Acc: 80.493% | LR: 1.250000e-05 | Dur: 136.65s
I - Confusion Matrix: [row->prediction - col->label]
[[605.   1.   4.  62. 139.]
 [  7. 493.  34.  13.  69.]
 [  7.  49. 629.  30. 147.]
 [ 45.  12.  24. 420.  63.]
 [ 33.  23.  43.  13. 582.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.111 | Acc: 59.763% | Wgt Acc: 53.119% | Dur: 15.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   0.   1.  15.   7.]
 [  0.  23.   3.   1.   4.]
 [  2.  27.  44.   6.  17.]
 [ 12.   1.   2.  34.   0.]
 [ 24.  27.  25.  30. 152.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.631 | Acc: 76.250% | Wgt Acc: 80.670%
	I - Batch: 100 | Loss: 0.621 | Acc: 76.750% | Wgt Acc: 80.640%
	I - Batch: 150 | Loss: 0.608 | Acc: 77.083% | Wgt Acc: 80.691%
	I - Batch: 200 | Loss: 0.616 | Acc: 76.750% | Wgt Acc: 80.392%
I - num batch: 222
I - Train -- Loss: 0.622 | Acc: 76.403% | Wgt Acc: 80.014% | LR: 1.250000e-05 | Dur: 138.05s
I - Confusion Matrix: [row->prediction - col->label]
[[592.   4.   9.  60. 126.]
 [  7. 488.  26.  15.  70.]
 [  6.  47. 635.  23. 152.]
 [ 48.  16.  25. 421.  78.]
 [ 44.  23.  39.  19. 574.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.094 | Acc: 58.777% | Wgt Acc: 52.461% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   0.   1.  11.   6.]
 [  0.  23.   2.   1.   3.]
 [  2.  29.  43.   9.  22.]
 [ 12.   1.   3.  34.   1.]
 [ 24.  25.  26.  31. 148.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.617 | Acc: 75.000% | Wgt Acc: 79.381%
	I - Batch: 100 | Loss: 0.617 | Acc: 76.438% | Wgt Acc: 80.375%
	I - Batch: 150 | Loss: 0.620 | Acc: 75.917% | Wgt Acc: 79.894%
	I - Batch: 200 | Loss: 0.612 | Acc: 76.844% | Wgt Acc: 80.633%
I - num batch: 222
I - Train -- Loss: 0.615 | Acc: 76.713% | Wgt Acc: 80.524% | LR: 1.250000e-05 | Dur: 135.94s
I - Confusion Matrix: [row->prediction - col->label]
[[598.   2.  10.  54. 136.]
 [  5. 497.  29.  15.  57.]
 [  5.  45. 628.  22. 186.]
 [ 47.  16.  30. 430.  53.]
 [ 42.  18.  37.  17. 568.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.133 | Acc: 58.580% | Wgt Acc: 51.050% | Dur: 14.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   1.   1.  20.   6.]
 [  1.  25.   3.   3.   4.]
 [  2.  21.  42.   5.  14.]
 [  8.   0.   2.  29.   0.]
 [ 32.  31.  27.  29. 156.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.599 | Acc: 77.250% | Wgt Acc: 81.272%
	I - Batch: 100 | Loss: 0.598 | Acc: 77.750% | Wgt Acc: 81.364%
	I - Batch: 150 | Loss: 0.600 | Acc: 77.167% | Wgt Acc: 80.782%
	I - Batch: 200 | Loss: 0.605 | Acc: 76.969% | Wgt Acc: 80.613%
I - num batch: 222
I - Train -- Loss: 0.609 | Acc: 76.572% | Wgt Acc: 80.229% | LR: 1.250000e-05 | Dur: 134.26s
I - Confusion Matrix: [row->prediction - col->label]
[[602.   3.   3.  58. 139.]
 [  8. 494.  29.  12.  78.]
 [ 11.  48. 629.  31. 144.]
 [ 39.  12.  29. 418.  66.]
 [ 37.  21.  44.  19. 573.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.041 | Acc: 61.933% | Wgt Acc: 55.532% | Dur: 19.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   1.  16.   7.]
 [  0.  26.   4.   1.   4.]
 [  1.  22.  39.   4.  12.]
 [ 13.   1.   5.  41.   2.]
 [ 21.  28.  26.  24. 155.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.618 | Acc: 77.875% | Wgt Acc: 81.650%
	I - Batch: 100 | Loss: 0.620 | Acc: 76.625% | Wgt Acc: 80.632%
	I - Batch: 150 | Loss: 0.615 | Acc: 77.000% | Wgt Acc: 80.773%
	I - Batch: 200 | Loss: 0.610 | Acc: 77.000% | Wgt Acc: 80.623%
I - num batch: 222
I - Train -- Loss: 0.612 | Acc: 76.966% | Wgt Acc: 80.645% | LR: 1.250000e-05 | Dur: 134.79s
I - Confusion Matrix: [row->prediction - col->label]
[[597.   2.   3.  45. 128.]
 [  6. 494.  31.  16.  58.]
 [  9.  51. 633.  27. 174.]
 [ 48.  10.  21. 429.  63.]
 [ 37.  21.  46.  21. 577.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.142 | Acc: 59.172% | Wgt Acc: 52.544% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   1.   1.   9.   5.]
 [  0.  34.   4.   6.   7.]
 [  2.  19.  43.   8.  16.]
 [ 10.   1.   1.  29.   1.]
 [ 33.  23.  26.  34. 151.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.619 | Acc: 75.625% | Wgt Acc: 78.417%
	I - Batch: 100 | Loss: 0.612 | Acc: 75.312% | Wgt Acc: 78.672%
	I - Batch: 150 | Loss: 0.610 | Acc: 75.833% | Wgt Acc: 79.215%
	I - Batch: 200 | Loss: 0.609 | Acc: 76.281% | Wgt Acc: 79.520%
I - num batch: 222
I - Train -- Loss: 0.610 | Acc: 76.515% | Wgt Acc: 79.829% | LR: 1.250000e-05 | Dur: 134.66s
I - Confusion Matrix: [row->prediction - col->label]
[[591.   4.   6.  62. 131.]
 [  5. 496.  31.  17.  76.]
 [  9.  48. 627.  32. 130.]
 [ 50.  14.  29. 410.  73.]
 [ 42.  16.  41.  17. 590.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.150 | Acc: 58.383% | Wgt Acc: 51.165% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 38.   1.   0.   9.   5.]
 [  0.  25.   4.   1.   5.]
 [  1.  24.  46.  10.  15.]
 [ 14.   1.   3.  33.   1.]
 [ 35.  27.  22.  33. 154.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.583 | Acc: 77.125% | Wgt Acc: 80.989%
	I - Batch: 100 | Loss: 0.594 | Acc: 76.812% | Wgt Acc: 80.183%
	I - Batch: 150 | Loss: 0.604 | Acc: 76.000% | Wgt Acc: 79.611%
	I - Batch: 200 | Loss: 0.605 | Acc: 76.312% | Wgt Acc: 79.911%
I - num batch: 222
I - Train -- Loss: 0.599 | Acc: 76.459% | Wgt Acc: 80.185% | LR: 1.250000e-05 | Dur: 133.94s
I - Confusion Matrix: [row->prediction - col->label]
[[594.   1.  10.  53. 117.]
 [  3. 510.  35.  14.  87.]
 [ 10.  37. 617.  30. 156.]
 [ 51.   9.  25. 421.  70.]
 [ 39.  21.  47.  20. 570.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.140 | Acc: 57.396% | Wgt Acc: 50.287% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   0.   1.  12.   6.]
 [  0.  27.   3.   4.   6.]
 [  2.  24.  40.   7.  16.]
 [  9.   1.   2.  28.   1.]
 [ 32.  26.  29.  35. 151.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.605 | Acc: 78.250% | Wgt Acc: 81.664%
	I - Batch: 100 | Loss: 0.624 | Acc: 77.375% | Wgt Acc: 80.668%
	I - Batch: 150 | Loss: 0.610 | Acc: 77.500% | Wgt Acc: 80.921%
	I - Batch: 200 | Loss: 0.603 | Acc: 77.906% | Wgt Acc: 81.395%
I - num batch: 222
I - Train -- Loss: 0.600 | Acc: 77.812% | Wgt Acc: 81.290% | LR: 1.250000e-05 | Dur: 135.17s
I - Confusion Matrix: [row->prediction - col->label]
[[598.   1.   6.  47. 135.]
 [  5. 504.  30.  18.  80.]
 [  8.  42. 633.  28. 139.]
 [ 48.  15.  28. 429.  50.]
 [ 38.  16.  37.  16. 596.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 59.961% | Wgt Acc: 53.662% | Dur: 19.83s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   0.   2.   9.   7.]
 [  0.  28.   4.   2.   5.]
 [  3.  29.  48.   7.  17.]
 [ 10.   1.   2.  33.   1.]
 [ 30.  20.  19.  35. 150.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.604 | Acc: 77.125% | Wgt Acc: 80.245%
	I - Batch: 100 | Loss: 0.599 | Acc: 77.750% | Wgt Acc: 81.033%
	I - Batch: 150 | Loss: 0.602 | Acc: 77.583% | Wgt Acc: 81.010%
	I - Batch: 200 | Loss: 0.602 | Acc: 77.750% | Wgt Acc: 81.147%
I - num batch: 222
I - Train -- Loss: 0.603 | Acc: 77.784% | Wgt Acc: 81.202% | LR: 1.250000e-05 | Dur: 134.54s
I - Confusion Matrix: [row->prediction - col->label]
[[600.   3.   8.  51. 121.]
 [  6. 506.  29.  16.  73.]
 [  9.  36. 626.  26. 141.]
 [ 41.  14.  33. 428.  66.]
 [ 41.  19.  38.  17. 599.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.095 | Acc: 59.172% | Wgt Acc: 52.962% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   1.   3.  14.   9.]
 [  0.  21.   3.   0.   6.]
 [  2.  26.  43.   6.  17.]
 [ 11.   1.   3.  36.   0.]
 [ 23.  29.  23.  30. 148.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.608 | Acc: 77.625% | Wgt Acc: 80.878%
	I - Batch: 100 | Loss: 0.595 | Acc: 78.250% | Wgt Acc: 82.125%
	I - Batch: 150 | Loss: 0.601 | Acc: 78.125% | Wgt Acc: 81.706%
	I - Batch: 200 | Loss: 0.603 | Acc: 77.812% | Wgt Acc: 81.310%
I - num batch: 222
I - Train -- Loss: 0.602 | Acc: 77.643% | Wgt Acc: 81.271% | LR: 1.250000e-05 | Dur: 134.96s
I - Confusion Matrix: [row->prediction - col->label]
[[607.   5.   6.  55. 140.]
 [  4. 505.  27.  10.  66.]
 [ 10.  35. 630.  29. 139.]
 [ 41.  13.  26. 426.  69.]
 [ 35.  20.  45.  18. 586.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.064 | Acc: 60.750% | Wgt Acc: 53.882% | Dur: 14.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  17.   9.]
 [  0.  23.   3.   1.   2.]
 [  1.  23.  36.   2.  12.]
 [ 11.   1.   4.  39.   1.]
 [ 22.  29.  30.  27. 156.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.565 | Acc: 80.500% | Wgt Acc: 83.266%
	I - Batch: 100 | Loss: 0.589 | Acc: 77.938% | Wgt Acc: 81.181%
	I - Batch: 150 | Loss: 0.583 | Acc: 78.250% | Wgt Acc: 81.532%
	I - Batch: 200 | Loss: 0.591 | Acc: 77.938% | Wgt Acc: 81.349%
I - num batch: 222
I - Train -- Loss: 0.595 | Acc: 77.728% | Wgt Acc: 81.093% | LR: 1.250000e-05 | Dur: 133.20s
I - Confusion Matrix: [row->prediction - col->label]
[[599.   1.   5.  51. 135.]
 [  4. 504.  35.  13.  74.]
 [  8.  42. 626.  29. 129.]
 [ 44.  12.  25. 427.  61.]
 [ 42.  19.  43.  18. 601.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.157 | Acc: 59.763% | Wgt Acc: 52.575% | Dur: 14.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   1.   1.  10.   8.]
 [  0.  25.   2.   1.   3.]
 [  1.  20.  41.   5.  13.]
 [  8.   1.   1.  33.   0.]
 [ 31.  31.  30.  37. 156.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.610 | Acc: 78.250% | Wgt Acc: 81.722%
	I - Batch: 100 | Loss: 0.599 | Acc: 78.375% | Wgt Acc: 81.702%
	I - Batch: 150 | Loss: 0.586 | Acc: 78.250% | Wgt Acc: 81.704%
	I - Batch: 200 | Loss: 0.592 | Acc: 77.688% | Wgt Acc: 81.004%
I - num batch: 222
I - Train -- Loss: 0.589 | Acc: 77.699% | Wgt Acc: 81.116% | LR: 1.250000e-05 | Dur: 138.48s
I - Confusion Matrix: [row->prediction - col->label]
[[597.   1.   5.  55. 120.]
 [  3. 504.  30.  16.  73.]
 [ 12.  43. 630.  22. 141.]
 [ 41.  14.  25. 427.  68.]
 [ 44.  16.  44.  18. 598.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 62.327% | Wgt Acc: 55.794% | Dur: 16.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  18.   8.]
 [  0.  30.   3.   3.   5.]
 [  2.  20.  41.   3.  11.]
 [  9.   2.   4.  35.   0.]
 [ 23.  24.  25.  27. 156.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.636 | Acc: 75.250% | Wgt Acc: 79.380%
	I - Batch: 100 | Loss: 0.597 | Acc: 77.750% | Wgt Acc: 82.057%
	I - Batch: 150 | Loss: 0.599 | Acc: 77.083% | Wgt Acc: 81.551%
	I - Batch: 200 | Loss: 0.590 | Acc: 77.531% | Wgt Acc: 81.690%
I - num batch: 222
I - Train -- Loss: 0.590 | Acc: 77.559% | Wgt Acc: 81.543% | LR: 1.250000e-05 | Dur: 135.60s
I - Confusion Matrix: [row->prediction - col->label]
[[608.   4.   3.  51. 135.]
 [  7. 498.  27.  13.  66.]
 [  7.  50. 642.  22. 161.]
 [ 40.  13.  19. 436.  71.]
 [ 35.  13.  43.  16. 567.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.128 | Acc: 57.988% | Wgt Acc: 51.259% | Dur: 14.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   1.   1.  15.  10.]
 [  0.  25.   4.   1.   5.]
 [  0.  23.  43.   6.  16.]
 [  8.   1.   3.  28.   0.]
 [ 31.  28.  24.  36. 149.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.566 | Acc: 80.000% | Wgt Acc: 83.270%
	I - Batch: 100 | Loss: 0.580 | Acc: 78.625% | Wgt Acc: 82.171%
	I - Batch: 150 | Loss: 0.586 | Acc: 77.875% | Wgt Acc: 81.126%
	I - Batch: 200 | Loss: 0.588 | Acc: 77.812% | Wgt Acc: 80.984%
I - num batch: 222
I - Train -- Loss: 0.591 | Acc: 77.446% | Wgt Acc: 80.689% | LR: 1.250000e-05 | Dur: 135.66s
I - Confusion Matrix: [row->prediction - col->label]
[[593.   4.   5.  59. 114.]
 [  4. 513.  33.  14.  81.]
 [  8.  35. 622.  33. 141.]
 [ 52.  11.  25. 415.  60.]
 [ 40.  15.  49.  17. 604.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.083 | Acc: 59.172% | Wgt Acc: 53.161% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   0.  12.   8.]
 [  0.  21.   2.   1.   5.]
 [  2.  32.  43.   6.  19.]
 [ 14.   2.   6.  39.   1.]
 [ 22.  22.  24.  28. 147.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.546 | Acc: 81.125% | Wgt Acc: 84.568%
	I - Batch: 100 | Loss: 0.554 | Acc: 80.625% | Wgt Acc: 83.926%
	I - Batch: 150 | Loss: 0.564 | Acc: 79.708% | Wgt Acc: 82.912%
	I - Batch: 200 | Loss: 0.562 | Acc: 79.562% | Wgt Acc: 82.866%
I - num batch: 208
I - Train -- Loss: 0.565 | Acc: 79.357% | Wgt Acc: 82.714% | LR: 1.250000e-05 | Dur: 126.22s
I - Confusion Matrix: [row->prediction - col->label]
[[608.   4.   3.  46.  91.]
 [  3. 507.  25.   8.  56.]
 [ 12.  31. 642.  32. 135.]
 [ 40.  12.  31. 435.  50.]
 [ 34.  24.  33.  17. 449.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.081 | Acc: 59.566% | Wgt Acc: 53.944% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   2.   2.   9.   6.]
 [  0.  28.   3.   3.   4.]
 [  1.  27.  46.   6.  24.]
 [ 13.   1.   3.  37.   1.]
 [ 28.  20.  21.  31. 145.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.587 | Acc: 78.625% | Wgt Acc: 82.907%
	I - Batch: 100 | Loss: 0.594 | Acc: 77.438% | Wgt Acc: 81.942%
	I - Batch: 150 | Loss: 0.590 | Acc: 77.333% | Wgt Acc: 81.352%
	I - Batch: 200 | Loss: 0.584 | Acc: 77.812% | Wgt Acc: 81.786%
I - num batch: 222
I - Train -- Loss: 0.588 | Acc: 77.728% | Wgt Acc: 81.673% | LR: 1.250000e-05 | Dur: 135.87s
I - Confusion Matrix: [row->prediction - col->label]
[[603.   4.   9.  45. 139.]
 [  5. 520.  36.  15.  74.]
 [  5.  28. 628.  30. 139.]
 [ 51.  12.  34. 434.  76.]
 [ 33.  14.  27.  14. 572.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.060 | Acc: 60.947% | Wgt Acc: 55.208% | Dur: 13.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   1.   1.  10.   6.]
 [  0.  29.   3.   3.   3.]
 [  1.  21.  47.   7.  21.]
 [ 13.   2.   2.  36.   2.]
 [ 25.  25.  22.  30. 148.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.593 | Acc: 78.875% | Wgt Acc: 82.148%
	I - Batch: 100 | Loss: 0.588 | Acc: 78.812% | Wgt Acc: 82.233%
	I - Batch: 150 | Loss: 0.584 | Acc: 78.958% | Wgt Acc: 82.222%
	I - Batch: 200 | Loss: 0.588 | Acc: 78.469% | Wgt Acc: 81.800%
I - num batch: 222
I - Train -- Loss: 0.587 | Acc: 78.404% | Wgt Acc: 81.861% | LR: 1.250000e-05 | Dur: 133.75s
I - Confusion Matrix: [row->prediction - col->label]
[[606.   2.   8.  46. 111.]
 [  9. 511.  26.  15.  83.]
 [  6.  36. 631.  33. 131.]
 [ 36.  12.  21. 430.  72.]
 [ 40.  17.  48.  14. 603.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.085 | Acc: 59.369% | Wgt Acc: 53.422% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 44.   0.   0.  10.   6.]
 [  0.  25.   2.   2.   6.]
 [  2.  27.  47.   9.  19.]
 [ 19.   1.   5.  38.   2.]
 [ 23.  25.  21.  27. 147.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.570 | Acc: 79.250% | Wgt Acc: 82.985%
	I - Batch: 100 | Loss: 0.575 | Acc: 79.062% | Wgt Acc: 82.447%
	I - Batch: 150 | Loss: 0.575 | Acc: 78.333% | Wgt Acc: 81.985%
	I - Batch: 200 | Loss: 0.572 | Acc: 78.750% | Wgt Acc: 82.353%
I - num batch: 222
I - Train -- Loss: 0.570 | Acc: 78.602% | Wgt Acc: 82.261% | LR: 1.250000e-05 | Dur: 133.40s
I - Confusion Matrix: [row->prediction - col->label]
[[614.   1.   6.  53. 140.]
 [  3. 512.  23.  12.  53.]
 [  7.  38. 637.  28. 152.]
 [ 39.  10.  25. 431.  61.]
 [ 34.  17.  43.  14. 594.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.053 | Acc: 60.750% | Wgt Acc: 55.156% | Dur: 15.97s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   2.   1.  13.  10.]
 [  0.  28.   5.   3.   8.]
 [  1.  24.  42.   2.  13.]
 [ 16.   1.   5.  39.   2.]
 [ 19.  23.  22.  29. 147.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.571 | Acc: 77.750% | Wgt Acc: 81.054%
	I - Batch: 100 | Loss: 0.584 | Acc: 77.750% | Wgt Acc: 81.163%
	I - Batch: 150 | Loss: 0.581 | Acc: 78.125% | Wgt Acc: 81.720%
	I - Batch: 200 | Loss: 0.576 | Acc: 78.344% | Wgt Acc: 81.918%
I - num batch: 222
I - Train -- Loss: 0.574 | Acc: 78.404% | Wgt Acc: 82.032% | LR: 1.250000e-05 | Dur: 134.81s
I - Confusion Matrix: [row->prediction - col->label]
[[603.   1.   2.  46. 133.]
 [  6. 520.  32.  15.  65.]
 [  6.  32. 637.  31. 146.]
 [ 44.  10.  24. 427.  62.]
 [ 38.  15.  39.  19. 594.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.047 | Acc: 61.933% | Wgt Acc: 56.713% | Dur: 17.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  16.  10.]
 [  0.  31.   4.   1.   5.]
 [  2.  17.  39.   2.  15.]
 [ 11.   4.   4.  43.   3.]
 [ 21.  24.  26.  24. 147.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.578 | Acc: 76.750% | Wgt Acc: 80.227%
	I - Batch: 100 | Loss: 0.572 | Acc: 77.875% | Wgt Acc: 81.880%
	I - Batch: 150 | Loss: 0.573 | Acc: 78.458% | Wgt Acc: 82.434%
	I - Batch: 200 | Loss: 0.571 | Acc: 78.531% | Wgt Acc: 82.406%
I - num batch: 222
I - Train -- Loss: 0.566 | Acc: 78.743% | Wgt Acc: 82.479% | LR: 1.250000e-05 | Dur: 137.16s
I - Confusion Matrix: [row->prediction - col->label]
[[605.   1.   4.  41. 128.]
 [  3. 516.  23.  16.  64.]
 [  4.  30. 645.  26. 150.]
 [ 44.  15.  27. 435.  66.]
 [ 41.  16.  35.  20. 592.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.115 | Acc: 60.158% | Wgt Acc: 54.414% | Dur: 15.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   1.   2.   8.   7.]
 [  1.  30.   3.   2.   3.]
 [  2.  24.  45.  11.  23.]
 [ 10.   1.   2.  37.   0.]
 [ 29.  22.  23.  28. 147.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.552 | Acc: 80.375% | Wgt Acc: 84.124%
	I - Batch: 100 | Loss: 0.558 | Acc: 80.312% | Wgt Acc: 83.996%
	I - Batch: 150 | Loss: 0.563 | Acc: 79.667% | Wgt Acc: 83.143%
	I - Batch: 200 | Loss: 0.571 | Acc: 79.188% | Wgt Acc: 82.675%
I - num batch: 222
I - Train -- Loss: 0.568 | Acc: 79.306% | Wgt Acc: 82.677% | LR: 1.250000e-05 | Dur: 136.82s
I - Confusion Matrix: [row->prediction - col->label]
[[604.   4.   2.  52. 102.]
 [  6. 506.  29.   9.  67.]
 [  8.  37. 644.  22. 156.]
 [ 41.  13.  21. 442.  58.]
 [ 38.  18.  38.  13. 617.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.084 | Acc: 59.369% | Wgt Acc: 53.579% | Dur: 16.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   1.   0.   9.   6.]
 [  0.  23.   5.   1.   6.]
 [  1.  26.  45.   7.  21.]
 [ 14.   2.   4.  40.   1.]
 [ 26.  26.  21.  29. 146.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.520 | Acc: 81.750% | Wgt Acc: 85.571%
	I - Batch: 100 | Loss: 0.539 | Acc: 80.250% | Wgt Acc: 84.058%
	I - Batch: 150 | Loss: 0.555 | Acc: 78.792% | Wgt Acc: 82.598%
	I - Batch: 200 | Loss: 0.565 | Acc: 78.469% | Wgt Acc: 82.177%
I - num batch: 222
I - Train -- Loss: 0.563 | Acc: 78.714% | Wgt Acc: 82.339% | LR: 1.250000e-05 | Dur: 133.41s
I - Confusion Matrix: [row->prediction - col->label]
[[604.   5.   8.  50. 123.]
 [  3. 507.  22.  10.  61.]
 [  6.  40. 642.  24. 143.]
 [ 47.   8.  20. 441.  75.]
 [ 37.  18.  42.  13. 598.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 60.158% | Wgt Acc: 54.080% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   0.   3.   8.   7.]
 [  0.  27.   5.   1.   4.]
 [  1.  23.  41.   4.  18.]
 [ 11.   1.   2.  42.   1.]
 [ 31.  27.  24.  31. 150.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.569 | Acc: 78.000% | Wgt Acc: 81.440%
	I - Batch: 100 | Loss: 0.559 | Acc: 79.250% | Wgt Acc: 82.100%
	I - Batch: 150 | Loss: 0.557 | Acc: 79.042% | Wgt Acc: 81.985%
	I - Batch: 200 | Loss: 0.555 | Acc: 79.031% | Wgt Acc: 82.188%
I - num batch: 222
I - Train -- Loss: 0.556 | Acc: 78.884% | Wgt Acc: 82.052% | LR: 1.250000e-05 | Dur: 132.84s
I - Confusion Matrix: [row->prediction - col->label]
[[612.   2.   5.  52. 115.]
 [  5. 499.  27.  12.  66.]
 [  6.  46. 634.  28. 134.]
 [ 37.  12.  26. 431.  63.]
 [ 37.  19.  42.  15. 622.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.107 | Acc: 58.974% | Wgt Acc: 51.928% | Dur: 15.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   0.   2.  13.   6.]
 [  0.  25.   4.   2.   3.]
 [  1.  24.  38.   5.  16.]
 [ 10.   1.   3.  35.   1.]
 [ 30.  28.  28.  31. 154.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.588 | Acc: 77.875% | Wgt Acc: 81.139%
	I - Batch: 100 | Loss: 0.573 | Acc: 78.562% | Wgt Acc: 81.584%
	I - Batch: 150 | Loss: 0.570 | Acc: 77.958% | Wgt Acc: 81.529%
	I - Batch: 200 | Loss: 0.568 | Acc: 78.344% | Wgt Acc: 81.757%
I - num batch: 222
I - Train -- Loss: 0.566 | Acc: 78.545% | Wgt Acc: 81.850% | LR: 1.250000e-05 | Dur: 135.42s
I - Confusion Matrix: [row->prediction - col->label]
[[595.   4.   4.  48. 120.]
 [  3. 502.  27.  12.  67.]
 [  7.  43. 638.  26. 145.]
 [ 42.  12.  23. 438.  55.]
 [ 50.  17.  42.  14. 613.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.094 | Acc: 61.736% | Wgt Acc: 54.989% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   1.   1.  11.   6.]
 [  0.  31.   4.   3.   4.]
 [  2.  19.  42.   2.  12.]
 [ 12.   1.   2.  37.   1.]
 [ 28.  26.  26.  33. 157.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.547 | Acc: 79.250% | Wgt Acc: 83.626%
	I - Batch: 100 | Loss: 0.543 | Acc: 79.938% | Wgt Acc: 83.668%
	I - Batch: 150 | Loss: 0.541 | Acc: 80.083% | Wgt Acc: 84.079%
	I - Batch: 200 | Loss: 0.552 | Acc: 79.656% | Wgt Acc: 83.495%
I - num batch: 222
I - Train -- Loss: 0.556 | Acc: 79.419% | Wgt Acc: 83.242% | LR: 1.250000e-05 | Dur: 137.92s
I - Confusion Matrix: [row->prediction - col->label]
[[606.   2.   2.  41. 133.]
 [  4. 510.  25.  10.  66.]
 [  9.  34. 649.  20. 127.]
 [ 46.  11.  21. 456.  78.]
 [ 32.  21.  37.  11. 596.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.073 | Acc: 61.736% | Wgt Acc: 55.438% | Dur: 18.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   2.  19.  10.]
 [  0.  28.   3.   4.   4.]
 [  0.  23.  42.   1.  12.]
 [  8.   2.   4.  34.   1.]
 [ 24.  23.  24.  28. 153.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.542 | Acc: 79.875% | Wgt Acc: 83.705%
	I - Batch: 100 | Loss: 0.562 | Acc: 78.875% | Wgt Acc: 82.739%
	I - Batch: 150 | Loss: 0.562 | Acc: 78.625% | Wgt Acc: 82.408%
	I - Batch: 200 | Loss: 0.553 | Acc: 79.094% | Wgt Acc: 82.668%
I - num batch: 222
I - Train -- Loss: 0.557 | Acc: 78.771% | Wgt Acc: 82.510% | LR: 1.250000e-05 | Dur: 137.02s
I - Confusion Matrix: [row->prediction - col->label]
[[598.   2.   4.  47. 134.]
 [  3. 510.  28.  11.  66.]
 [  6.  38. 649.  26. 145.]
 [ 44.  10.  21. 444.  62.]
 [ 46.  18.  32.  10. 593.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 60.947% | Wgt Acc: 53.787% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   1.   1.  15.   8.]
 [  0.  27.   3.   2.   4.]
 [  2.  22.  38.   2.  10.]
 [  7.   2.   3.  34.   0.]
 [ 27.  26.  30.  33. 158.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.522 | Acc: 80.750% | Wgt Acc: 84.417%
	I - Batch: 100 | Loss: 0.536 | Acc: 79.312% | Wgt Acc: 83.021%
	I - Batch: 150 | Loss: 0.539 | Acc: 79.458% | Wgt Acc: 83.090%
	I - Batch: 200 | Loss: 0.550 | Acc: 79.406% | Wgt Acc: 82.939%
I - num batch: 222
I - Train -- Loss: 0.546 | Acc: 79.617% | Wgt Acc: 83.078% | LR: 1.250000e-05 | Dur: 135.46s
I - Confusion Matrix: [row->prediction - col->label]
[[608.   2.   6.  56. 121.]
 [  6. 527.  25.  10.  62.]
 [  1.  22. 641.  22. 142.]
 [ 43.  11.  25. 433.  60.]
 [ 39.  16.  37.  17. 615.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.068 | Acc: 61.736% | Wgt Acc: 56.723% | Dur: 15.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   0.   2.  13.   8.]
 [  0.  29.   5.   1.   8.]
 [  2.  25.  47.   3.  18.]
 [ 11.   1.   2.  43.   1.]
 [ 26.  23.  19.  26. 145.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.565 | Acc: 78.500% | Wgt Acc: 81.911%
	I - Batch: 100 | Loss: 0.566 | Acc: 78.250% | Wgt Acc: 81.787%
	I - Batch: 150 | Loss: 0.560 | Acc: 78.375% | Wgt Acc: 82.224%
	I - Batch: 200 | Loss: 0.550 | Acc: 79.469% | Wgt Acc: 83.185%
I - num batch: 222
I - Train -- Loss: 0.554 | Acc: 79.109% | Wgt Acc: 82.821% | LR: 1.250000e-05 | Dur: 136.78s
I - Confusion Matrix: [row->prediction - col->label]
[[614.   4.   5.  42. 123.]
 [  2. 513.  26.  10.  66.]
 [  9.  33. 635.  26. 151.]
 [ 34.  13.  26. 446.  62.]
 [ 38.  15.  42.  14. 598.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.073 | Acc: 60.355% | Wgt Acc: 55.146% | Dur: 14.62s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   0.   0.   7.   6.]
 [  0.  32.   4.   3.   8.]
 [  1.  26.  47.  10.  21.]
 [ 17.   1.   4.  40.   1.]
 [ 27.  19.  20.  26. 144.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.537 | Acc: 80.750% | Wgt Acc: 84.346%
	I - Batch: 100 | Loss: 0.545 | Acc: 80.188% | Wgt Acc: 83.856%
	I - Batch: 150 | Loss: 0.551 | Acc: 79.875% | Wgt Acc: 83.517%
	I - Batch: 200 | Loss: 0.546 | Acc: 80.344% | Wgt Acc: 83.863%
I - num batch: 222
I - Train -- Loss: 0.546 | Acc: 80.096% | Wgt Acc: 83.619% | LR: 1.250000e-05 | Dur: 137.36s
I - Confusion Matrix: [row->prediction - col->label]
[[605.   3.   3.  46. 124.]
 [  5. 522.  20.  13.  71.]
 [  4.  31. 654.  21. 125.]
 [ 40.   7.  22. 443.  63.]
 [ 43.  15.  35.  15. 617.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.039 | Acc: 63.511% | Wgt Acc: 57.946% | Dur: 15.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   2.   0.  12.   7.]
 [  0.  32.   4.   2.   5.]
 [  1.  18.  44.   4.  14.]
 [ 13.   1.   5.  42.   2.]
 [ 22.  25.  22.  26. 152.]]

I - Local maximum validation set accuracy:  63.51

I - Validation set results: 
[14-1-2-1.89][50-3-4-0.37][124-2-2-1.41][127-0-0-4.76][443-2-2-3.16][567-0-0-1.52][573-1-1-3.51][615-0-3-1.10][695-1-2-1.79][722-3-0-3.27]
[826-0-0-1.56][878-0-3-1.51][1103-0-4-1.82][1212-3-4-1.97][1368-0-0-2.57][2181-2-3-0.44][2476-2-2-0.60][2721-2-2-2.47][2818-1-4-0.49][2886-2-1-1.46]
[3231-2-2-4.45][3333-2-2-1.45][3482-2-2-2.40][3536-3-4-0.61][3625-1-1-1.56][3909-0-0-0.95][4035-0-3-1.40][4140-0-0-1.46][4214-1-1-0.78][4346-1-4-0.20]
[4581-2-2-1.44][4708-3-2-1.51][4838-3-4-0.16][4845-1-2-1.38][4868-0-0-3.48][4939-0-4-1.54][4984-2-2-3.22][5078-1-4-1.39][5396-0-0-3.94][5479-1-2-0.87]
[5717-0-0-2.41][5843-1-1-1.23][5949-3-0-1.02][5987-2-4-2.45][6014-3-1-0.68][6033-3-0-2.69][6313-0-0-0.57][6421-3-3-1.11][6500-1-1-0.90][6583-3-3-0.36]
[6683-3-3-1.63][6825-2-1-1.21][6998-3-3-0.45][7049-3-3-0.29][7517-1-1-1.96][7521-1-4-0.30][7528-1-2-1.04][7949-1-2-1.79][8135-1-0--0.25][8185-3-0-3.31]
[8269-3-4-1.34][8273-3-3-2.13][8543-3-0-4.61][8666-1-1-1.66][8672-0-0-3.27][8903-1-1-0.72][9001-2-1-2.04][9036-2-2-2.55][9281-3-4-1.05][9300-2-2-4.76]
[9571-0-4-0.64][9617-1-4-1.42][9644-2-2-1.58][9705-2-4-0.80][9801-0-0-1.02][9803-3-3-0.02][9865-3-3-2.57][9896-2-4-1.77][10314-1-4-2.31][10337-3-3-2.60]
[10403-0-4-1.50][10653-2-4-1.16][10704-2-2-1.61][10719-1-1-1.71][10727-1-4-2.17][10836-0-0-6.47][10969-2-3-1.12][11042-0-0-2.44][11088-1-1-5.00][11322-0-0-3.77]
[11398-2-2-2.36][11499-0-0-1.67][11502-3-3-1.10][11512-3-3-1.35][11608-1-2-2.18][11610-0-0-0.06][11692-0-0-0.24][11905-0-0-2.90][11993-1-1-2.65][12002-2-2-0.21]
[12052-0-0-2.06][12201-0-3-2.13][12235-2-2-2.28][12320-1-4-2.36][12377-2-4-2.91][12398-2-4-0.34][12503-1-1-1.51][12617-0-2-0.88][12685-3-4-0.87][12738-2-4-0.43]
[12742-2-2-4.10][12823-0-3-1.22][13110-1-1-1.14][13240-3-3-1.04][13253-1-4-1.90][13273-0-0-4.62][13634-1-4-2.31][13763-2-3-0.65][13905-3-4--0.46][14060-2-4-0.86]
[14065-3-0-0.53][14147-3-4-0.56][14595-2-2-1.43][14687-2-2-3.89][14788-2-2-1.82][14869-1-4-1.37][14872-3-4-1.62][14877-1-1-4.82][14927-0-3-1.38][15066-0-0-2.72]
[15175-1-4-1.96][15178-2-3-0.57][15375-3-0-1.94][15389-3-3-1.51][15568-2-4-1.34][15675-3-3-2.98][15869-1-4-0.50][16207-3-0-0.85][16236-0-4-0.97][16302-3-4-0.42]
[16331-2-2-4.47][16381-0-4-0.60][16488-1-1-2.33][16495-0-0-0.74][16650-0-0-3.07][16719-1-2-1.33][16801-0-0-3.48][16828-0-0-2.08][17137-3-3-1.10][17245-1-1-0.46]
[17278-3-4-1.27][17282-0-0-0.39][17311-2-2-2.67][17336-2-2-1.86][17608-3-3-3.32][17627-0-4-1.39][17877-3-4-1.99][17924-1-4-0.46][17984-3-3-2.40][18211-0-3-1.55]
[18276-3-3-1.80][18287-1-1-0.43][18394-0-0-2.32][18428-0-0-3.42][18442-0-3-1.97][18478-3-0-1.19][18607-0-0-2.50][18616-0-4-1.84][18663-0-0-0.45][18718-0-0-2.85]
[18766-2-2-2.18][18824-2-4-2.51][18890-3-2-1.07][18930-3-4-2.49][18938-3-2-0.63][19817-1-2-1.78][19839-0-4-0.37][19930-3-3-0.68][19944-0-4-1.77][20036-2-2-4.13]
[20101-3-4-0.52][20474-1-2-0.71][20547-3-4-1.11][20929-2-2-3.43][21245-1-1-1.43][21257-3-4-0.82][21293-1-2-2.73][21316-1-1-3.16][21384-1-4-2.96][21448-1-2-1.49]
[21483-0-0-2.64][21487-2-2-2.79][21714-0-4-0.21][21943-3-4-1.78][21947-0-0-3.69][21948-0-0-4.69][21965-2-2-1.58][21998-1-1-2.05][22025-0-4-2.41][22228-3-3-2.63]
[22446-1-1-1.97][22494-3-3-1.66][22757-0-0-2.49][22811-3-3-3.17][22976-3-2-1.64][22985-3-3-2.18][23014-0-3-2.05][23112-1-1-2.50][23144-3-3-2.40][23168-2-4-0.89]
[23219-0-4-0.90][23363-3-3-0.99][23470-0-4-0.63][23486-2-4-1.61][23497-0-3-2.39][23516-0-0-1.65][23690-1-4-1.52][23921-2-4-1.08][23936-1-2-2.83][24040-3-4-1.56]
[24111-1-4-2.69][24182-0-0-3.22][24238-3-3-0.95][24290-2-4-0.71][24345-0-4-1.15][24364-1-1-1.55][24427-3-4-1.21][24477-2-2-2.11][24495-2-4-1.70][24893-2-2-1.48]
[25012-1-2-1.09][25121-2-4-2.59][25165-3-4--0.42][25183-0-0-2.37][25297-3-3-2.67][25398-0-0-2.17][25574-2-2-2.80][25644-1-1-4.15][25718-1-4-0.69][25774-2-4-1.04]
[26032-3-3-0.92][26051-3-3-2.35][26120-0-4-2.47][26321-1-4-0.82][26732-1-1-2.64][26784-3-3-3.54][26827-3-3-0.66][26833-0-3-2.28][26838-2-2-0.57][26860-1-4-1.10]
[26948-0-0-1.11][27049-3-0-2.11][27098-1-0-0.85][27526-0-0-1.62][27639-3-3-0.80][27698-3-0-1.83][27772-0-0-3.35][27890-1-1-1.73][28040-0-4-0.95][28503-2-2-3.85]
[28577-1-1-1.59][28959-0-0-3.32][29198-3-4-1.27][29777-0-0-4.53][29877-2-2-2.75][30035-1-1-2.56][30098-0-0-1.15][30326-1-1-5.23][30572-2-2-3.45][30716-0-4-2.41]
[30806-2-2-0.61][30906-1-1-1.18][31007-0-0-0.55][31181-3-3-0.55][31238-0-3-1.84][31347-0-0-3.50][31422-2-4-1.26][31429-3-3-0.57][31431-0-0--0.05][31432-1-1-2.48]
[31477-0-3-2.22][31524-1-4-0.59][31597-1-2-1.68][31619-1-2-1.42][31701-0-0-4.03][31755-0-0-3.01][31854-3-3-0.96][32074-1-4-1.06][32078-3-3-2.43][32111-1-1-1.77]
[32127-1-2-1.73][32140-3-3-2.14][32263-2-4-1.83][32365-0-0-1.10][32411-2-3-1.86][32429-3-3-1.95][32473-3-4-1.20][32574-3-3-2.53][32584-0-4-2.61][32622-0-4-1.25]
[32858-3-3-1.00][32969-3-0-1.74][33016-2-2-1.83][33031-1-3-0.73][33035-2-2-2.18][33133-2-2-2.69][33173-2-2-0.78][33175-3-4-1.26][33306-3-1-1.15][33309-2-4-0.25]
[33474-0-4-0.56][33478-2-4-0.71][33618-1-4-1.90][33712-0-4-1.29][33782-2-1-2.90][33914-3-3-2.38][34076-3-4-1.99][34112-2-2-3.42][34138-2-2-2.05][34239-1-4-0.78]
[34364-2-2-4.19][34617-1-2-2.66][34751-3-3-2.07][34783-2-4-2.19][35015-3-4-1.21][35018-1-4-1.69][35288-2-2-1.58][0-4-4-2.74][1-4-4-3.85][2-4-4-1.68]
[3-4-4-2.54][4-4-4-1.47][5-4-1--0.28][6-4-4-1.97][7-4-4-3.86][8-4-4-0.85][9-4-2-1.55][10-4-4-4.22][11-4-4-3.38][12-4-2-1.55]
[14-4-4-1.58][15-4-3-1.66][16-4-4-2.45][17-4-4-1.11][18-4-4-2.98][19-4-0-2.26][20-4-4-1.29][21-4-4-1.53][22-4-4-2.81][23-4-4-2.21]
[24-4-4-5.91][25-4-4-1.19][26-4-4-1.81][27-4-2-1.48][28-4-4-3.02][29-4-2-1.06][30-4-4-0.94][31-4-4-2.56][32-4-4-1.79][33-4-4-1.30]
[34-4-4-1.80][35-4-4-1.09][37-4-4-2.38][39-4-0-3.22][40-4-4-1.31][41-4-4-0.64][42-4-4-0.80][43-4-4-1.32][45-4-4-0.68][46-4-4-4.14]
[47-4-4-4.91][48-4-4-3.70][51-4-4-2.93][52-4-4-2.50][53-4-4-1.83][54-4-4-0.71][55-4-4-2.28][56-4-1-0.54][57-4-0-0.77][58-4-4-2.12]
[59-4-4-1.81][60-4-4-2.15][61-4-4-2.75][62-4-4-0.89][63-4-4-2.64][64-4-4-1.10][65-4-4-3.70][66-4-4-3.07][67-4-4-1.45][68-4-4-0.89]
[69-4-4-0.82][70-4-4-2.79][72-4-1-1.25][73-4-4-2.00][74-4-2-1.77][75-4-4-1.06][77-4-4-3.83][78-4-4-0.86][79-4-4-3.07][80-4-4-2.67]
[81-4-1-1.41][82-4-4-1.40][83-4-4-1.28][84-4-4-4.10][85-4-4-3.73][86-4-4-1.20][87-4-4-2.78][88-4-4-2.63][89-4-4-1.28][90-4-4-1.04]
[91-4-4-2.45][92-4-4-1.92][93-4-4-1.96][94-4-4-3.44][95-4-4-1.71][96-4-4-1.15][97-4-4-2.86][98-4-2-2.85][99-4-4-1.50][100-4-4-1.85]
[101-4-4-5.55][102-4-4-2.28][103-4-4-0.65][104-4-4-2.34][105-4-4-2.28][106-4-4-3.39][107-4-4-2.59][108-4-4-1.73][109-4-4-2.44][110-4-4-2.12]
[111-4-0-1.21][112-4-4-0.44][113-4-4-1.35][114-4-2-0.43][115-4-4-2.45][116-4-4-1.97][117-4-4-2.79][119-4-2-3.02][121-4-4-2.31][122-4-4-2.81]
[124-4-4-0.24][125-4-4-3.42][126-4-4-5.31][127-4-2-1.79][128-4-4-1.23][129-4-4-1.19][130-4-4-0.71][131-4-2-1.51][132-4-4-2.58][133-4-4-5.24]
[135-4-4-1.82][136-4-4-1.48][137-4-4-2.03][138-4-4-2.33][139-4-4-2.65][140-4-1-2.26][141-4-4-0.83][142-4-4-3.88][143-4-4-3.80][144-4-4-3.48]
[145-4-4-2.23][148-4-0-3.67][149-4-4-1.83][150-4-4-3.09][151-4-4-3.70][152-4-4-2.77][153-4-4-2.95][154-4-4-4.05][155-4-4-3.37][156-4-4-0.72]
[157-4-4-1.11][158-4-4-2.57][160-4-4-0.86][161-4-2-2.05][162-4-4-1.70][164-4-4-2.55][165-4-4-0.98][167-4-0-2.29][168-4-4-1.92][170-4-4-1.63]
[171-4-4-2.40][172-4-4-3.99][173-4-4-3.96][174-4-0-2.82][175-4-4-3.07][177-4-4-3.59][178-4-4-2.83][179-4-4-2.53][180-4-4-3.21][181-4-4-1.55]
[182-4-4-1.61][183-4-4-2.87][184-4-4-2.88][186-4-4-0.46][187-4-2-1.30][188-4-4-2.46][189-4-4-1.26][190-4-4-0.89][191-4-4-3.37][192-4-4-2.49]
[193-4-2-2.71][194-4-3-0.45][195-4-4-0.94][196-4-4-1.69][197-4-4-2.60][198-4-4-5.77][199-4-2-1.85]
---------------------------
I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.557 | Acc: 79.000% | Wgt Acc: 82.384%
	I - Batch: 100 | Loss: 0.553 | Acc: 79.438% | Wgt Acc: 82.994%
	I - Batch: 150 | Loss: 0.557 | Acc: 79.042% | Wgt Acc: 82.503%
	I - Batch: 200 | Loss: 0.558 | Acc: 78.844% | Wgt Acc: 82.335%
I - num batch: 222
I - Train -- Loss: 0.555 | Acc: 78.912% | Wgt Acc: 82.396% | LR: 1.250000e-05 | Dur: 137.83s
I - Confusion Matrix: [row->prediction - col->label]
[[596.   0.   7.  47. 125.]
 [  5. 518.  26.  14.  63.]
 [  5.  27. 637.  23. 135.]
 [ 40.  12.  25. 440.  69.]
 [ 51.  21.  39.  14. 608.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.162 | Acc: 60.750% | Wgt Acc: 53.756% | Dur: 18.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 41.   1.   1.   7.   4.]
 [  1.  26.   2.   2.   3.]
 [  2.  21.  46.   9.  15.]
 [ 11.   0.   1.  38.   1.]
 [ 33.  30.  25.  30. 157.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.537 | Acc: 79.750% | Wgt Acc: 83.454%
	I - Batch: 100 | Loss: 0.550 | Acc: 78.938% | Wgt Acc: 82.650%
	I - Batch: 150 | Loss: 0.535 | Acc: 80.125% | Wgt Acc: 83.711%
	I - Batch: 200 | Loss: 0.537 | Acc: 79.406% | Wgt Acc: 83.236%
I - num batch: 222
I - Train -- Loss: 0.534 | Acc: 79.363% | Wgt Acc: 83.181% | LR: 1.250000e-05 | Dur: 137.37s
I - Confusion Matrix: [row->prediction - col->label]
[[621.   1.   4.  52. 140.]
 [  3. 522.  18.  12.  75.]
 [  5.  39. 647.  28. 136.]
 [ 34.   5.  23. 432.  56.]
 [ 34.  11.  42.  14. 593.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.084 | Acc: 61.341% | Wgt Acc: 55.543% | Dur: 14.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   2.   1.  11.   7.]
 [  1.  29.   4.   2.   5.]
 [  1.  22.  41.   4.  16.]
 [ 14.   2.   3.  42.   2.]
 [ 23.  23.  26.  27. 150.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.525 | Acc: 80.375% | Wgt Acc: 83.035%
	I - Batch: 100 | Loss: 0.541 | Acc: 79.188% | Wgt Acc: 82.606%
	I - Batch: 150 | Loss: 0.550 | Acc: 78.292% | Wgt Acc: 81.684%
	I - Batch: 200 | Loss: 0.550 | Acc: 78.906% | Wgt Acc: 82.255%
I - num batch: 222
I - Train -- Loss: 0.545 | Acc: 79.222% | Wgt Acc: 82.467% | LR: 1.250000e-05 | Dur: 138.71s
I - Confusion Matrix: [row->prediction - col->label]
[[606.   1.   3.  48. 126.]
 [  7. 505.  27.  13.  56.]
 [  9.  38. 641.  29. 135.]
 [ 39.  14.  23. 436.  61.]
 [ 36.  20.  40.  12. 622.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.033 | Acc: 62.327% | Wgt Acc: 57.037% | Dur: 21.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   1.   2.  12.  10.]
 [  0.  33.   4.   3.   6.]
 [  2.  21.  43.   4.  15.]
 [ 13.   2.   5.  41.   1.]
 [ 22.  21.  21.  26. 148.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.560 | Acc: 79.500% | Wgt Acc: 82.564%
	I - Batch: 100 | Loss: 0.559 | Acc: 79.938% | Wgt Acc: 83.222%
	I - Batch: 150 | Loss: 0.550 | Acc: 80.583% | Wgt Acc: 83.618%
	I - Batch: 200 | Loss: 0.545 | Acc: 80.375% | Wgt Acc: 83.639%
I - num batch: 222
I - Train -- Loss: 0.547 | Acc: 80.209% | Wgt Acc: 83.520% | LR: 1.250000e-05 | Dur: 136.49s
I - Confusion Matrix: [row->prediction - col->label]
[[610.   1.   3.  45. 134.]
 [  7. 520.  17.  15.  72.]
 [  9.  29. 645.  19. 109.]
 [ 37.   9.  26. 441.  56.]
 [ 34.  19.  43.  18. 629.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.068 | Acc: 61.538% | Wgt Acc: 55.825% | Dur: 13.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   1.   1.  16.   7.]
 [  0.  26.   5.   1.   3.]
 [  1.  25.  42.   4.  20.]
 [ 11.   1.   4.  39.   1.]
 [ 20.  25.  23.  26. 149.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.537 | Acc: 81.875% | Wgt Acc: 85.487%
	I - Batch: 100 | Loss: 0.525 | Acc: 81.375% | Wgt Acc: 85.172%
	I - Batch: 150 | Loss: 0.540 | Acc: 80.375% | Wgt Acc: 83.846%
	I - Batch: 200 | Loss: 0.528 | Acc: 81.000% | Wgt Acc: 84.364%
I - num batch: 222
I - Train -- Loss: 0.530 | Acc: 80.660% | Wgt Acc: 84.102% | LR: 1.250000e-05 | Dur: 135.30s
I - Confusion Matrix: [row->prediction - col->label]
[[620.   2.   7.  41. 102.]
 [  3. 519.  24.  10.  73.]
 [  4.  30. 646.  19. 130.]
 [ 30.  11.  17. 449.  68.]
 [ 40.  16.  40.  19. 627.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.100 | Acc: 61.538% | Wgt Acc: 55.700% | Dur: 15.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   2.  11.   7.]
 [  0.  29.   3.   3.   6.]
 [  2.  26.  45.   5.  16.]
 [ 10.   1.   2.  38.   1.]
 [ 26.  21.  23.  29. 150.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.507 | Acc: 81.625% | Wgt Acc: 85.121%
	I - Batch: 100 | Loss: 0.520 | Acc: 81.375% | Wgt Acc: 84.808%
	I - Batch: 150 | Loss: 0.529 | Acc: 81.125% | Wgt Acc: 84.757%
	I - Batch: 200 | Loss: 0.534 | Acc: 80.594% | Wgt Acc: 84.283%
I - num batch: 222
I - Train -- Loss: 0.532 | Acc: 80.829% | Wgt Acc: 84.523% | LR: 1.250000e-05 | Dur: 135.26s
I - Confusion Matrix: [row->prediction - col->label]
[[607.   1.   5.  43. 135.]
 [  8. 541.  21.  11.  68.]
 [  6.  15. 650.  19. 126.]
 [ 41.   5.  24. 452.  54.]
 [ 35.  16.  34.  13. 617.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.130 | Acc: 59.369% | Wgt Acc: 52.983% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   1.   1.  11.   8.]
 [  0.  26.   5.   2.   8.]
 [  1.  22.  40.   3.  14.]
 [ 11.   1.   4.  36.   0.]
 [ 27.  28.  25.  34. 150.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.510 | Acc: 82.375% | Wgt Acc: 85.257%
	I - Batch: 100 | Loss: 0.526 | Acc: 80.688% | Wgt Acc: 83.954%
	I - Batch: 150 | Loss: 0.531 | Acc: 80.667% | Wgt Acc: 84.151%
	I - Batch: 200 | Loss: 0.530 | Acc: 80.531% | Wgt Acc: 84.059%
I - num batch: 222
I - Train -- Loss: 0.530 | Acc: 80.519% | Wgt Acc: 84.046% | LR: 1.250000e-05 | Dur: 136.60s
I - Confusion Matrix: [row->prediction - col->label]
[[605.   0.   3.  42. 116.]
 [  5. 528.  26.  14.  62.]
 [ 10.  23. 651.  20. 141.]
 [ 42.  10.  21. 450.  59.]
 [ 35.  17.  33.  12. 622.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.115 | Acc: 61.144% | Wgt Acc: 53.850% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   0.   2.  14.   9.]
 [  0.  29.   4.   3.   3.]
 [  2.  15.  34.   3.   9.]
 [  4.   1.   2.  31.   0.]
 [ 25.  33.  33.  35. 159.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.523 | Acc: 79.750% | Wgt Acc: 83.755%
	I - Batch: 100 | Loss: 0.524 | Acc: 80.188% | Wgt Acc: 83.960%
	I - Batch: 150 | Loss: 0.525 | Acc: 80.333% | Wgt Acc: 84.026%
	I - Batch: 200 | Loss: 0.527 | Acc: 80.219% | Wgt Acc: 83.892%
I - num batch: 222
I - Train -- Loss: 0.531 | Acc: 79.927% | Wgt Acc: 83.702% | LR: 1.250000e-05 | Dur: 136.87s
I - Confusion Matrix: [row->prediction - col->label]
[[614.   2.   2.  43. 152.]
 [  5. 523.  21.  11.  63.]
 [  4.  23. 653.  14. 123.]
 [ 35.  13.  18. 443.  60.]
 [ 39.  17.  40.  27. 602.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.166 | Acc: 58.974% | Wgt Acc: 51.416% | Dur: 14.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   1.   2.  11.   7.]
 [  0.  25.   4.   3.   5.]
 [  1.  21.  40.   9.  11.]
 [ 11.   0.   1.  30.   0.]
 [ 29.  31.  28.  33. 157.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.520 | Acc: 79.500% | Wgt Acc: 83.446%
	I - Batch: 100 | Loss: 0.524 | Acc: 79.812% | Wgt Acc: 83.471%
	I - Batch: 150 | Loss: 0.523 | Acc: 80.167% | Wgt Acc: 83.634%
	I - Batch: 200 | Loss: 0.522 | Acc: 80.688% | Wgt Acc: 83.934%
I - num batch: 222
I - Train -- Loss: 0.522 | Acc: 80.603% | Wgt Acc: 83.773% | LR: 1.250000e-05 | Dur: 134.28s
I - Confusion Matrix: [row->prediction - col->label]
[[607.   2.   4.  44. 124.]
 [  3. 523.  22.  14.  50.]
 [  2.  29. 650.  23. 119.]
 [ 43.   9.  22. 439.  67.]
 [ 42.  15.  36.  18. 640.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.159 | Acc: 58.974% | Wgt Acc: 53.276% | Dur: 16.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   0.   1.  10.   8.]
 [  0.  28.   4.   3.   5.]
 [  2.  26.  48.  10.  23.]
 [ 11.   1.   2.  33.   0.]
 [ 29.  23.  20.  30. 144.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.548 | Acc: 79.125% | Wgt Acc: 82.860%
	I - Batch: 100 | Loss: 0.550 | Acc: 78.688% | Wgt Acc: 82.236%
	I - Batch: 150 | Loss: 0.535 | Acc: 79.625% | Wgt Acc: 83.322%
	I - Batch: 200 | Loss: 0.538 | Acc: 79.750% | Wgt Acc: 83.438%
I - num batch: 222
I - Train -- Loss: 0.535 | Acc: 79.729% | Wgt Acc: 83.463% | LR: 1.250000e-05 | Dur: 133.93s
I - Confusion Matrix: [row->prediction - col->label]
[[608.   4.   5.  45. 137.]
 [  4. 526.  29.  12.  81.]
 [  6.  16. 654.  27. 118.]
 [ 40.  13.  17. 438.  62.]
 [ 39.  19.  29.  16. 602.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.064 | Acc: 61.538% | Wgt Acc: 54.749% | Dur: 21.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   3.   2.  15.   7.]
 [  0.  27.   4.   3.   3.]
 [  2.  17.  36.   4.  11.]
 [ 12.   0.   3.  39.   2.]
 [ 21.  31.  30.  25. 157.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.505 | Acc: 80.625% | Wgt Acc: 84.480%
	I - Batch: 100 | Loss: 0.518 | Acc: 79.938% | Wgt Acc: 83.664%
	I - Batch: 150 | Loss: 0.531 | Acc: 79.417% | Wgt Acc: 83.147%
	I - Batch: 200 | Loss: 0.531 | Acc: 79.719% | Wgt Acc: 83.507%
I - num batch: 222
I - Train -- Loss: 0.528 | Acc: 80.011% | Wgt Acc: 83.693% | LR: 1.250000e-05 | Dur: 135.30s
I - Confusion Matrix: [row->prediction - col->label]
[[610.   2.   5.  39. 125.]
 [  2. 520.  23.  11.  57.]
 [ 10.  27. 654.  25. 131.]
 [ 37.  13.  22. 446.  79.]
 [ 38.  16.  30.  17. 608.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.082 | Acc: 63.116% | Wgt Acc: 56.086% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   0.   4.  15.   8.]
 [  0.  29.   4.   2.   3.]
 [  0.  16.  40.   1.   8.]
 [  8.   2.   2.  36.   0.]
 [ 26.  31.  25.  32. 161.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.505 | Acc: 82.500% | Wgt Acc: 86.306%
	I - Batch: 100 | Loss: 0.519 | Acc: 81.312% | Wgt Acc: 85.181%
	I - Batch: 150 | Loss: 0.526 | Acc: 81.042% | Wgt Acc: 84.866%
	I - Batch: 200 | Loss: 0.531 | Acc: 80.812% | Wgt Acc: 84.710%
I - num batch: 222
I - Train -- Loss: 0.531 | Acc: 80.942% | Wgt Acc: 84.710% | LR: 1.250000e-05 | Dur: 138.97s
I - Confusion Matrix: [row->prediction - col->label]
[[615.   3.   6.  42. 108.]
 [  7. 530.  17.   9.  60.]
 [  7.  21. 662.  22. 157.]
 [ 28.   8.  17. 451.  62.]
 [ 40.  16.  32.  14. 613.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.045 | Acc: 62.327% | Wgt Acc: 56.828% | Dur: 20.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   1.   1.  14.   8.]
 [  0.  30.   6.   2.   7.]
 [  0.  18.  40.   3.  15.]
 [  7.   1.   3.  39.   1.]
 [ 23.  28.  25.  28. 149.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.459 | Acc: 84.500% | Wgt Acc: 87.530%
	I - Batch: 100 | Loss: 0.489 | Acc: 82.812% | Wgt Acc: 85.997%
	I - Batch: 150 | Loss: 0.494 | Acc: 83.125% | Wgt Acc: 86.217%
	I - Batch: 200 | Loss: 0.500 | Acc: 82.656% | Wgt Acc: 85.739%
I - num batch: 208
I - Train -- Loss: 0.502 | Acc: 82.542% | Wgt Acc: 85.599% | LR: 1.250000e-05 | Dur: 122.93s
I - Confusion Matrix: [row->prediction - col->label]
[[618.   4.   2.  34.  96.]
 [  5. 526.  19.   9.  46.]
 [  4.  24. 651.  22.  93.]
 [ 34.   8.  29. 461.  55.]
 [ 36.  16.  33.  12. 491.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.137 | Acc: 60.552% | Wgt Acc: 54.164% | Dur: 13.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   0.   1.   7.   7.]
 [  0.  27.   3.   1.   5.]
 [  2.  22.  46.   7.  15.]
 [ 12.   1.   2.  36.   1.]
 [ 28.  28.  23.  35. 152.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.537 | Acc: 80.750% | Wgt Acc: 84.828%
	I - Batch: 100 | Loss: 0.527 | Acc: 80.375% | Wgt Acc: 84.367%
	I - Batch: 150 | Loss: 0.528 | Acc: 80.625% | Wgt Acc: 84.835%
	I - Batch: 200 | Loss: 0.528 | Acc: 80.531% | Wgt Acc: 84.695%
I - num batch: 222
I - Train -- Loss: 0.528 | Acc: 80.772% | Wgt Acc: 84.754% | LR: 1.250000e-05 | Dur: 135.61s
I - Confusion Matrix: [row->prediction - col->label]
[[626.   1.   3.  37. 141.]
 [  5. 530.  24.  12.  51.]
 [  5.  26. 658.  25. 146.]
 [ 29.   8.  15. 451.  62.]
 [ 32.  13.  34.  13. 600.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.067 | Acc: 62.327% | Wgt Acc: 56.598% | Dur: 24.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   2.  14.   9.]
 [  0.  26.   4.   2.   6.]
 [  1.  22.  40.   1.  14.]
 [ 11.   1.   3.  43.   0.]
 [ 20.  27.  26.  26. 151.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.480 | Acc: 81.875% | Wgt Acc: 85.894%
	I - Batch: 100 | Loss: 0.509 | Acc: 81.188% | Wgt Acc: 85.221%
	I - Batch: 150 | Loss: 0.519 | Acc: 80.542% | Wgt Acc: 84.414%
	I - Batch: 200 | Loss: 0.521 | Acc: 80.344% | Wgt Acc: 84.035%
I - num batch: 222
I - Train -- Loss: 0.514 | Acc: 80.547% | Wgt Acc: 84.159% | LR: 1.250000e-05 | Dur: 136.27s
I - Confusion Matrix: [row->prediction - col->label]
[[608.   2.   3.  47. 116.]
 [  1. 528.  19.  12.  71.]
 [  4.  22. 666.  17. 133.]
 [ 38.  12.  21. 439.  64.]
 [ 46.  14.  25.  23. 616.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.091 | Acc: 60.750% | Wgt Acc: 53.976% | Dur: 14.63s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   2.  14.   7.]
 [  0.  27.   3.   1.   5.]
 [  2.  22.  41.   6.  12.]
 [  9.   1.   2.  35.   1.]
 [ 27.  27.  27.  30. 155.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.525 | Acc: 80.125% | Wgt Acc: 84.312%
	I - Batch: 100 | Loss: 0.523 | Acc: 80.562% | Wgt Acc: 84.323%
	I - Batch: 150 | Loss: 0.521 | Acc: 80.042% | Wgt Acc: 84.128%
	I - Batch: 200 | Loss: 0.523 | Acc: 80.406% | Wgt Acc: 84.353%
I - num batch: 222
I - Train -- Loss: 0.521 | Acc: 80.434% | Wgt Acc: 84.367% | LR: 1.250000e-05 | Dur: 135.89s
I - Confusion Matrix: [row->prediction - col->label]
[[611.   2.   3.  33. 155.]
 [  6. 527.  16.   9.  55.]
 [  5.  23. 653.  15. 134.]
 [ 33.   8.  25. 461.  55.]
 [ 42.  18.  37.  20. 601.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.058 | Acc: 62.327% | Wgt Acc: 56.650% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   2.  15.  10.]
 [  0.  27.   4.   2.   7.]
 [  1.  17.  35.   1.  12.]
 [ 13.   3.   8.  45.   0.]
 [ 16.  28.  26.  23. 151.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.488 | Acc: 83.000% | Wgt Acc: 86.458%
	I - Batch: 100 | Loss: 0.485 | Acc: 82.438% | Wgt Acc: 85.923%
	I - Batch: 150 | Loss: 0.499 | Acc: 81.833% | Wgt Acc: 85.371%
	I - Batch: 200 | Loss: 0.493 | Acc: 82.125% | Wgt Acc: 85.541%
I - num batch: 222
I - Train -- Loss: 0.497 | Acc: 81.957% | Wgt Acc: 85.440% | LR: 1.250000e-05 | Dur: 135.37s
I - Confusion Matrix: [row->prediction - col->label]
[[621.   2.   3.  25. 126.]
 [  2. 533.  22.  14.  55.]
 [  6.  22. 653.  17. 135.]
 [ 32.  10.  24. 461.  45.]
 [ 36.  11.  32.  21. 639.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 60.947% | Wgt Acc: 53.704% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   0.   2.  10.   8.]
 [  0.  30.   3.   2.   3.]
 [  2.  21.  38.   4.  10.]
 [ 12.   1.   3.  35.   0.]
 [ 27.  26.  29.  35. 159.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.515 | Acc: 81.500% | Wgt Acc: 85.755%
	I - Batch: 100 | Loss: 0.524 | Acc: 80.750% | Wgt Acc: 85.125%
	I - Batch: 150 | Loss: 0.516 | Acc: 81.083% | Wgt Acc: 84.860%
	I - Batch: 200 | Loss: 0.512 | Acc: 81.062% | Wgt Acc: 84.894%
I - num batch: 222
I - Train -- Loss: 0.515 | Acc: 80.913% | Wgt Acc: 84.791% | LR: 1.250000e-05 | Dur: 140.52s
I - Confusion Matrix: [row->prediction - col->label]
[[616.   1.   5.  39. 124.]
 [  7. 536.  17.   9.  62.]
 [  6.  23. 662.  23. 132.]
 [ 34.   5.  17. 449.  75.]
 [ 34.  13.  33.  18. 607.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 60.552% | Wgt Acc: 53.996% | Dur: 16.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   1.   1.  10.   5.]
 [  0.  30.   5.   4.   5.]
 [  2.  22.  44.   5.  16.]
 [ 10.   1.   3.  33.   1.]
 [ 29.  24.  22.  34. 153.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 0.487 | Acc: 84.000% | Wgt Acc: 87.491%
	I - Batch: 100 | Loss: 0.493 | Acc: 82.188% | Wgt Acc: 85.890%
	I - Batch: 150 | Loss: 0.498 | Acc: 81.542% | Wgt Acc: 85.076%
	I - Batch: 200 | Loss: 0.502 | Acc: 81.562% | Wgt Acc: 85.325%
I - num batch: 222
I - Train -- Loss: 0.500 | Acc: 81.759% | Wgt Acc: 85.474% | LR: 1.250000e-05 | Dur: 134.06s
I - Confusion Matrix: [row->prediction - col->label]
[[626.   1.   3.  48. 117.]
 [  1. 543.  19.  10.  51.]
 [  6.  14. 656.  16. 148.]
 [ 31.   6.  17. 451.  60.]
 [ 33.  14.  39.  13. 624.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.123 | Acc: 62.130% | Wgt Acc: 55.344% | Dur: 14.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   0.   1.  10.   6.]
 [  0.  32.   4.   2.   6.]
 [  2.  21.  43.   3.   9.]
 [ 12.   1.   2.  37.   1.]
 [ 29.  24.  25.  34. 158.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 0.480 | Acc: 82.250% | Wgt Acc: 86.222%
	I - Batch: 100 | Loss: 0.503 | Acc: 81.688% | Wgt Acc: 85.423%
	I - Batch: 150 | Loss: 0.510 | Acc: 81.417% | Wgt Acc: 85.122%
	I - Batch: 200 | Loss: 0.506 | Acc: 81.250% | Wgt Acc: 84.839%
I - num batch: 222
I - Train -- Loss: 0.508 | Acc: 81.026% | Wgt Acc: 84.731% | LR: 1.250000e-05 | Dur: 138.61s
I - Confusion Matrix: [row->prediction - col->label]
[[627.   3.   5.  32. 123.]
 [  7. 520.  21.  12.  59.]
 [  5.  23. 655.  26. 127.]
 [ 28.   9.  21. 455.  74.]
 [ 30.  23.  32.  13. 617.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.130 | Acc: 59.763% | Wgt Acc: 54.185% | Dur: 17.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   0.   1.  10.   9.]
 [  1.  30.   6.   3.   8.]
 [  2.  20.  41.   8.  18.]
 [ 12.   1.   4.  36.   0.]
 [ 22.  27.  23.  29. 145.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 0.482 | Acc: 81.625% | Wgt Acc: 85.552%
	I - Batch: 100 | Loss: 0.494 | Acc: 81.625% | Wgt Acc: 85.451%
	I - Batch: 150 | Loss: 0.492 | Acc: 82.042% | Wgt Acc: 85.775%
	I - Batch: 200 | Loss: 0.497 | Acc: 82.125% | Wgt Acc: 85.647%
I - num batch: 222
I - Train -- Loss: 0.498 | Acc: 82.069% | Wgt Acc: 85.678% | LR: 1.250000e-05 | Dur: 134.72s
I - Confusion Matrix: [row->prediction - col->label]
[[618.   0.   3.  35. 105.]
 [  2. 528.  15.  10.  55.]
 [  4.  24. 670.  19. 138.]
 [ 26.   9.   8. 462.  69.]
 [ 47.  17.  38.  12. 633.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.096 | Acc: 61.341% | Wgt Acc: 55.741% | Dur: 14.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   2.   3.  16.   9.]
 [  0.  29.   5.   2.   4.]
 [  1.  19.  38.   1.  16.]
 [ 14.   4.   4.  45.   2.]
 [ 23.  24.  25.  22. 149.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 0.490 | Acc: 82.750% | Wgt Acc: 86.290%
	I - Batch: 100 | Loss: 0.497 | Acc: 82.250% | Wgt Acc: 85.663%
	I - Batch: 150 | Loss: 0.493 | Acc: 82.333% | Wgt Acc: 85.576%
	I - Batch: 200 | Loss: 0.488 | Acc: 82.781% | Wgt Acc: 86.029%
I - num batch: 222
I - Train -- Loss: 0.488 | Acc: 82.464% | Wgt Acc: 85.725% | LR: 1.250000e-05 | Dur: 137.53s
I - Confusion Matrix: [row->prediction - col->label]
[[628.   2.   4.  41. 106.]
 [  2. 540.  18.   7.  55.]
 [  5.  13. 655.  28. 125.]
 [ 29.   8.  21. 448.  60.]
 [ 33.  15.  36.  14. 654.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.153 | Acc: 58.580% | Wgt Acc: 50.674% | Dur: 14.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 44.   0.   1.   7.   6.]
 [  0.  28.   5.   6.   4.]
 [  1.  19.  37.   4.  11.]
 [  9.   1.   1.  29.   0.]
 [ 34.  30.  31.  40. 159.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 0.510 | Acc: 81.750% | Wgt Acc: 85.185%
	I - Batch: 100 | Loss: 0.500 | Acc: 81.062% | Wgt Acc: 84.914%
	I - Batch: 150 | Loss: 0.499 | Acc: 81.042% | Wgt Acc: 84.800%
	I - Batch: 200 | Loss: 0.494 | Acc: 81.219% | Wgt Acc: 85.052%
I - num batch: 222
I - Train -- Loss: 0.498 | Acc: 81.308% | Wgt Acc: 85.089% | LR: 1.250000e-05 | Dur: 134.90s
I - Confusion Matrix: [row->prediction - col->label]
[[611.   1.   2.  37. 125.]
 [  6. 541.  16.   9.  59.]
 [  5.  20. 660.  20. 121.]
 [ 36.   7.  19. 455.  78.]
 [ 39.   9.  37.  17. 617.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.118 | Acc: 61.736% | Wgt Acc: 54.916% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   1.   1.   7.   9.]
 [  0.  22.   2.   2.   4.]
 [  1.  19.  44.   4.  10.]
 [ 10.   2.   4.  38.   0.]
 [ 25.  34.  24.  35. 157.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 0.519 | Acc: 81.250% | Wgt Acc: 84.775%
	I - Batch: 100 | Loss: 0.492 | Acc: 82.250% | Wgt Acc: 85.863%
	I - Batch: 150 | Loss: 0.492 | Acc: 81.875% | Wgt Acc: 85.727%
	I - Batch: 200 | Loss: 0.499 | Acc: 81.156% | Wgt Acc: 85.027%
I - num batch: 222
I - Train -- Loss: 0.497 | Acc: 81.336% | Wgt Acc: 85.094% | LR: 1.250000e-05 | Dur: 131.95s
I - Confusion Matrix: [row->prediction - col->label]
[[613.   2.   4.  32. 136.]
 [  3. 536.  19.  10.  58.]
 [  4.  19. 656.  23. 127.]
 [ 36.   9.  20. 461.  60.]
 [ 41.  12.  35.  12. 619.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.163 | Acc: 60.947% | Wgt Acc: 53.641% | Dur: 13.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   2.   1.  11.   7.]
 [  0.  29.   4.   7.   4.]
 [  1.  19.  44.   3.  11.]
 [  6.   1.   2.  26.   0.]
 [ 29.  27.  24.  39. 158.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 0.487 | Acc: 80.500% | Wgt Acc: 83.957%
	I - Batch: 100 | Loss: 0.482 | Acc: 81.188% | Wgt Acc: 84.675%
	I - Batch: 150 | Loss: 0.491 | Acc: 81.333% | Wgt Acc: 84.930%
	I - Batch: 200 | Loss: 0.495 | Acc: 81.125% | Wgt Acc: 84.817%
I - num batch: 222
I - Train -- Loss: 0.499 | Acc: 80.998% | Wgt Acc: 84.699% | LR: 1.250000e-05 | Dur: 133.19s
I - Confusion Matrix: [row->prediction - col->label]
[[618.   1.   5.  43. 131.]
 [  7. 532.  19.  11.  55.]
 [  8.  18. 657.  21. 132.]
 [ 26.   8.  18. 449.  65.]
 [ 38.  19.  35.  14. 617.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 61.144% | Wgt Acc: 53.662% | Dur: 16.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   1.   2.  10.   7.]
 [  0.  25.   4.   3.   4.]
 [  2.  21.  41.   4.   8.]
 [ 12.   1.   4.  37.   0.]
 [ 28.  30.  24.  32. 161.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 0.467 | Acc: 84.375% | Wgt Acc: 87.910%
	I - Batch: 100 | Loss: 0.473 | Acc: 84.062% | Wgt Acc: 87.748%
	I - Batch: 150 | Loss: 0.478 | Acc: 83.208% | Wgt Acc: 86.849%
	I - Batch: 200 | Loss: 0.484 | Acc: 82.750% | Wgt Acc: 86.534%
I - num batch: 222
I - Train -- Loss: 0.489 | Acc: 82.408% | Wgt Acc: 86.162% | LR: 1.250000e-05 | Dur: 136.37s
I - Confusion Matrix: [row->prediction - col->label]
[[630.   2.   5.  29. 110.]
 [  5. 536.  15.  11.  73.]
 [  6.  16. 664.  17. 132.]
 [ 26.   8.  16. 464.  56.]
 [ 30.  16.  34.  17. 629.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.084 | Acc: 63.708% | Wgt Acc: 57.173% | Dur: 13.94s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   2.  18.   9.]
 [  0.  26.   4.   1.   2.]
 [  0.  15.  38.   3.   9.]
 [  9.   1.   3.  42.   1.]
 [ 21.  33.  28.  22. 159.]]

I - Local maximum validation set accuracy:  63.71

I - Validation set results: 
[14-1-2-1.60][50-3-4-1.50][124-2-2-2.28][127-0-0-5.26][443-2-2-3.26][567-0-0-1.57][573-1-1-3.48][615-0-0-0.81][695-1-2-1.33][722-3-0-3.92]
[826-0-0-2.82][878-0-0-2.22][1103-0-4-1.78][1212-3-4-1.58][1368-0-0-2.99][2181-2-3-0.73][2476-2-4-0.11][2721-2-2-2.18][2818-1-4-0.68][2886-2-1-1.30]
[3231-2-2-4.20][3333-2-2-1.15][3482-2-2-1.93][3536-3-3-0.98][3625-1-1-1.18][3909-0-0-1.32][4035-0-0-1.85][4140-0-0-1.36][4214-1-3-0.90][4346-1-4-0.41]
[4581-2-2-1.35][4708-3-4-2.14][4838-3-4-1.28][4845-1-2-1.30][4868-0-0-3.08][4939-0-4-1.49][4984-2-2-2.58][5078-1-4-1.59][5396-0-0-4.28][5479-1-2-0.42]
[5717-0-0-2.10][5843-1-4-0.66][5949-3-0-1.40][5987-2-4-3.03][6014-3-3-0.60][6033-3-0-1.25][6313-0-0-1.38][6421-3-3-0.99][6500-1-1-0.48][6583-3-3-0.62]
[6683-3-3-1.29][6825-2-1-1.16][6998-3-3-0.82][7049-3-3-0.95][7517-1-1-1.52][7521-1-4-0.11][7528-1-2-0.45][7949-1-4-1.86][8135-1-0-0.12][8185-3-0-3.95]
[8269-3-4-1.56][8273-3-3-2.00][8543-3-0-4.52][8666-1-1-1.78][8672-0-0-2.14][8903-1-2-1.61][9001-2-1-1.40][9036-2-2-3.91][9281-3-4-1.40][9300-2-2-4.98]
[9571-0-4-1.15][9617-1-4-0.91][9644-2-2-1.64][9705-2-4-0.84][9801-0-0-1.06][9803-3-3-0.37][9865-3-3-3.01][9896-2-4-1.48][10314-1-4-2.26][10337-3-3-2.66]
[10403-0-4-1.53][10653-2-4-1.41][10704-2-2-1.06][10719-1-1-2.18][10727-1-4-2.01][10836-0-0-7.22][10969-2-3-1.45][11042-0-0-1.09][11088-1-1-4.12][11322-0-0-4.14]
[11398-2-2-4.62][11499-0-0-2.32][11502-3-3-0.60][11512-3-3-1.90][11608-1-4-1.85][11610-0-0-0.25][11692-0-0-1.92][11905-0-0-3.35][11993-1-1-3.21][12002-2-2-0.61]
[12052-0-0-1.78][12201-0-0-2.04][12235-2-4-2.18][12320-1-4-3.06][12377-2-4-3.31][12398-2-4-1.21][12503-1-1-2.60][12617-0-4-0.34][12685-3-4-0.77][12738-2-4-0.18]
[12742-2-2-4.55][12823-0-3-1.55][13110-1-4-0.69][13240-3-0-2.07][13253-1-4-1.28][13273-0-0-5.09][13634-1-4-2.36][13763-2-2-0.66][13905-3-0--0.01][14060-2-4-1.99]
[14065-3-0-1.18][14147-3-3-0.67][14595-2-4-1.29][14687-2-2-3.65][14788-2-2-1.58][14869-1-4-1.92][14872-3-4-2.06][14877-1-1-5.16][14927-0-3-1.93][15066-0-0-2.58]
[15175-1-4-2.22][15178-2-3-0.63][15375-3-0-1.21][15389-3-3-2.13][15568-2-4-1.28][15675-3-3-3.43][15869-1-4-0.27][16207-3-0-1.09][16236-0-4-1.75][16302-3-0-1.84]
[16331-2-2-4.66][16381-0-4-1.10][16488-1-1-3.48][16495-0-0-1.99][16650-0-0-3.38][16719-1-2-1.24][16801-0-0-4.85][16828-0-0-2.40][17137-3-3-0.80][17245-1-4-2.62]
[17278-3-4-0.16][17282-0-0-0.32][17311-2-2-2.20][17336-2-1-0.86][17608-3-3-3.21][17627-0-0-0.71][17877-3-4-1.34][17924-1-4-0.71][17984-3-3-2.42][18211-0-3-1.60]
[18276-3-3-1.52][18287-1-4-0.21][18394-0-0-3.01][18428-0-0-2.17][18442-0-3-2.23][18478-3-0-1.72][18607-0-0-2.40][18616-0-4-2.23][18663-0-3-0.72][18718-0-0-3.07]
[18766-2-4-0.90][18824-2-4-3.30][18890-3-2-0.76][18930-3-4-2.25][18938-3-2-0.67][19817-1-2-1.27][19839-0-4-0.39][19930-3-3-1.06][19944-0-4-1.76][20036-2-2-3.81]
[20101-3-3-1.30][20474-1-2-0.20][20547-3-4-1.38][20929-2-2-3.72][21245-1-4-0.18][21257-3-4-1.35][21293-1-2-2.50][21316-1-1-0.89][21384-1-4-2.85][21448-1-2-1.27]
[21483-0-0-3.13][21487-2-2-3.40][21714-0-0-1.12][21943-3-4-2.27][21947-0-0-4.16][21948-0-0-5.01][21965-2-2-1.59][21998-1-1-2.11][22025-0-4-3.04][22228-3-3-2.62]
[22446-1-1-1.37][22494-3-3-1.20][22757-0-0-2.98][22811-3-3-3.15][22976-3-2-1.60][22985-3-3-2.29][23014-0-3-2.33][23112-1-1-2.45][23144-3-3-2.53][23168-2-4-1.01]
[23219-0-4-1.81][23363-3-3-1.47][23470-0-4-0.20][23486-2-4-2.24][23497-0-3-2.76][23516-0-0-2.59][23690-1-4-2.09][23921-2-4-1.03][23936-1-2-2.11][24040-3-4-1.25]
[24111-1-4-2.50][24182-0-0-3.49][24238-3-3-0.77][24290-2-0-1.11][24345-0-0-2.28][24364-1-1-1.58][24427-3-0-1.71][24477-2-2-2.48][24495-2-4-1.45][24893-2-2-0.85]
[25012-1-4-0.55][25121-2-4-1.82][25165-3-3--0.35][25183-0-0-2.45][25297-3-3-2.62][25398-0-0-2.48][25574-2-2-2.96][25644-1-1-2.93][25718-1-4-0.45][25774-2-4-1.43]
[26032-3-0-1.69][26051-3-3-2.27][26120-0-4-4.50][26321-1-1-1.11][26732-1-1-2.08][26784-3-3-3.60][26827-3-4-0.94][26833-0-3-1.97][26838-2-4-1.15][26860-1-4-1.42]
[26948-0-0-1.47][27049-3-0-2.99][27098-1-0-1.88][27526-0-0-1.82][27639-3-4-1.14][27698-3-0-0.93][27772-0-0-3.58][27890-1-1-1.49][28040-0-4-0.60][28503-2-2-3.24]
[28577-1-1-1.61][28959-0-0-3.81][29198-3-4-1.37][29777-0-0-5.21][29877-2-2-0.97][30035-1-1-1.95][30098-0-0-1.87][30326-1-1-4.75][30572-2-2-2.99][30716-0-4-2.24]
[30806-2-2-0.73][30906-1-1-2.16][31007-0-4-1.37][31181-3-3-0.94][31238-0-3-1.11][31347-0-0-4.24][31422-2-4-1.41][31429-3-3-0.78][31431-0-0-2.00][31432-1-1-2.07]
[31477-0-0-2.51][31524-1-2-1.50][31597-1-4-1.64][31619-1-2-0.77][31701-0-0-3.97][31755-0-0-3.86][31854-3-3-1.23][32074-1-4-1.00][32078-3-3-2.41][32111-1-1-0.74]
[32127-1-2-1.82][32140-3-3-1.98][32263-2-4-1.90][32365-0-0-2.78][32411-2-0-2.39][32429-3-3-2.11][32473-3-4-1.36][32574-3-0-2.84][32584-0-4-2.86][32622-0-4-1.74]
[32858-3-3-1.01][32969-3-0-2.59][33016-2-2-2.93][33031-1-0-1.02][33035-2-4-1.89][33133-2-2-1.84][33173-2-2-0.11][33175-3-4-1.47][33306-3-1-0.60][33309-2-4-0.56]
[33474-0-4-0.72][33478-2-4-0.85][33618-1-4-2.18][33712-0-4-1.62][33782-2-4-2.81][33914-3-3-2.43][34076-3-4-2.02][34112-2-2-3.48][34138-2-2-1.21][34239-1-4-0.86]
[34364-2-2-1.66][34617-1-4-3.28][34751-3-3-1.99][34783-2-4-2.41][35015-3-4-1.58][35018-1-4-1.81][35288-2-2-0.99][0-4-4-3.24][1-4-4-5.12][2-4-4-2.36]
[3-4-4-3.15][4-4-4-1.34][5-4-1-1.40][6-4-4-2.17][7-4-4-3.81][8-4-4-0.65][9-4-2-1.68][10-4-4-4.60][11-4-4-3.46][12-4-4-1.48]
[14-4-4-1.64][15-4-0-1.77][16-4-4-2.21][17-4-4-1.45][18-4-4-4.23][19-4-0-2.51][20-4-4-1.60][21-4-4-2.71][22-4-4-2.94][23-4-4-2.27]
[24-4-4-6.38][25-4-4-1.61][26-4-4-1.31][27-4-4-1.40][28-4-4-3.26][29-4-2-0.78][30-4-3-0.01][31-4-4-2.82][32-4-4-1.92][33-4-4-1.78]
[34-4-4-1.99][35-4-0-1.86][37-4-4-1.86][39-4-0-3.95][40-4-4-1.26][41-4-4-1.06][42-4-4-1.75][43-4-4-1.55][45-4-4-1.18][46-4-4-3.83]
[47-4-4-5.11][48-4-4-3.57][51-4-4-3.58][52-4-4-2.81][53-4-4-2.01][54-4-4-1.10][55-4-4-1.10][56-4-4-0.92][57-4-0-1.93][58-4-4-1.87]
[59-4-4-2.60][60-4-4-1.10][61-4-4-3.41][62-4-4-1.23][63-4-2-1.44][64-4-4-1.24][65-4-4-4.10][66-4-4-3.14][67-4-4-1.27][68-4-4-2.03]
[69-4-4-0.88][70-4-4-3.01][72-4-4-1.32][73-4-4-1.03][74-4-4-1.72][75-4-4-1.42][77-4-4-2.65][78-4-4-0.86][79-4-4-3.32][80-4-4-2.77]
[81-4-4-1.82][82-4-4-1.79][83-4-4-1.51][84-4-4-4.83][85-4-4-3.98][86-4-4-1.22][87-4-4-3.08][88-4-4-2.63][89-4-4-1.63][90-4-4-1.11]
[91-4-4-2.57][92-4-4-1.47][93-4-4-0.20][94-4-4-3.64][95-4-4-1.96][96-4-4-1.63][97-4-4-3.16][98-4-2-1.86][99-4-4-2.17][100-4-4-1.77]
[101-4-4-5.59][102-4-4-2.65][103-4-4-0.83][104-4-4-2.62][105-4-4-1.37][106-4-4-4.11][107-4-4-2.24][108-4-4-1.98][109-4-4-2.78][110-4-4-2.28]
[111-4-0-2.31][112-4-4-1.04][113-4-4-2.41][114-4-4-0.17][115-4-4-2.40][116-4-4-2.01][117-4-4-3.58][119-4-2-2.52][121-4-4-2.39][122-4-4-3.01]
[124-4-4-0.83][125-4-4-3.54][126-4-4-4.21][127-4-4-1.05][128-4-4-1.30][129-4-4-1.99][130-4-4-3.17][131-4-2-1.46][132-4-4-1.95][133-4-4-5.59]
[135-4-4-1.97][136-4-4-1.20][137-4-4-1.59][138-4-4-2.64][139-4-4-2.90][140-4-1-1.85][141-4-4-0.54][142-4-4-4.05][143-4-4-3.68][144-4-4-3.08]
[145-4-4-2.51][148-4-0-4.25][149-4-4-2.44][150-4-4-3.96][151-4-4-3.66][152-4-4-3.06][153-4-4-3.07][154-4-4-3.58][155-4-4-3.31][156-4-4-1.03]
[157-4-4-1.48][158-4-4-2.74][160-4-4-0.57][161-4-2-1.45][162-4-4-1.27][164-4-4-2.68][165-4-4-1.85][167-4-0-2.77][168-4-4-2.24][170-4-4-2.87]
[171-4-4-2.38][172-4-4-4.00][173-4-4-4.26][174-4-0-2.15][175-4-4-2.96][177-4-4-4.30][178-4-4-1.55][179-4-4-2.66][180-4-4-2.94][181-4-4-2.08]
[182-4-4-1.76][183-4-4-3.45][184-4-4-3.38][186-4-4-0.94][187-4-4-1.27][188-4-4-2.78][189-4-4-1.24][190-4-4-1.29][191-4-4-4.09][192-4-4-1.91]
[193-4-2-3.48][194-4-4-0.88][195-4-4-1.16][196-4-4-1.92][197-4-4-2.63][198-4-4-5.88][199-4-2-1.43]
---------------------------
I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 0.514 | Acc: 80.875% | Wgt Acc: 84.151%
	I - Batch: 100 | Loss: 0.495 | Acc: 82.250% | Wgt Acc: 85.625%
	I - Batch: 150 | Loss: 0.493 | Acc: 82.083% | Wgt Acc: 85.473%
	I - Batch: 200 | Loss: 0.492 | Acc: 82.312% | Wgt Acc: 85.838%
I - num batch: 222
I - Train -- Loss: 0.495 | Acc: 82.239% | Wgt Acc: 85.762% | LR: 1.250000e-05 | Dur: 134.57s
I - Confusion Matrix: [row->prediction - col->label]
[[608.   2.   3.  30. 107.]
 [  4. 534.  13.   8.  66.]
 [  7.  19. 670.  22. 120.]
 [ 36.   9.  16. 465.  67.]
 [ 42.  14.  32.  13. 640.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.341 | Acc: 56.410% | Wgt Acc: 46.819% | Dur: 14.65s
I - Confusion Matrix: [row->prediction - col->label]
[[ 36.   1.   1.   7.   2.]
 [  0.  23.   1.   3.   2.]
 [  2.  26.  42.  13.  10.]
 [  8.   0.   0.  19.   0.]
 [ 42.  28.  31.  44. 166.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 0.480 | Acc: 84.000% | Wgt Acc: 87.181%
	I - Batch: 100 | Loss: 0.476 | Acc: 83.375% | Wgt Acc: 86.581%
	I - Batch: 150 | Loss: 0.481 | Acc: 82.958% | Wgt Acc: 86.324%
	I - Batch: 200 | Loss: 0.481 | Acc: 82.656% | Wgt Acc: 86.264%
I - num batch: 222
I - Train -- Loss: 0.483 | Acc: 82.549% | Wgt Acc: 86.111% | LR: 1.250000e-05 | Dur: 133.80s
I - Confusion Matrix: [row->prediction - col->label]
[[621.   1.   6.  33. 107.]
 [  4. 542.  17.   8.  65.]
 [  6.  19. 668.  17. 121.]
 [ 28.   4.  15. 457.  67.]
 [ 38.  12.  28.  23. 640.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.165 | Acc: 59.566% | Wgt Acc: 51.896% | Dur: 14.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   2.   1.  10.   6.]
 [  0.  26.   3.   3.   5.]
 [  2.  19.  39.   4.  10.]
 [  9.   1.   3.  31.   0.]
 [ 30.  30.  29.  38. 159.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 0.460 | Acc: 81.375% | Wgt Acc: 85.313%
	I - Batch: 100 | Loss: 0.478 | Acc: 81.625% | Wgt Acc: 85.349%
	I - Batch: 150 | Loss: 0.488 | Acc: 81.333% | Wgt Acc: 85.161%
	I - Batch: 200 | Loss: 0.488 | Acc: 81.344% | Wgt Acc: 85.035%
I - num batch: 222
I - Train -- Loss: 0.487 | Acc: 81.477% | Wgt Acc: 85.205% | LR: 1.250000e-05 | Dur: 143.31s
I - Confusion Matrix: [row->prediction - col->label]
[[607.   2.   5.  37. 128.]
 [  1. 533.  15.   9.  57.]
 [  7.  20. 672.  21. 139.]
 [ 37.   8.  15. 457.  55.]
 [ 45.  15.  27.  14. 621.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.162 | Acc: 59.369% | Wgt Acc: 52.314% | Dur: 16.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   0.   1.   9.   5.]
 [  0.  24.   4.   3.   6.]
 [  1.  24.  47.   8.  14.]
 [ 12.   1.   1.  31.   1.]
 [ 30.  29.  22.  35. 154.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 0.470 | Acc: 82.375% | Wgt Acc: 85.960%
	I - Batch: 100 | Loss: 0.467 | Acc: 82.688% | Wgt Acc: 86.240%
	I - Batch: 150 | Loss: 0.477 | Acc: 82.542% | Wgt Acc: 86.096%
	I - Batch: 200 | Loss: 0.476 | Acc: 82.469% | Wgt Acc: 85.935%
I - num batch: 222
I - Train -- Loss: 0.480 | Acc: 82.182% | Wgt Acc: 85.564% | LR: 1.250000e-05 | Dur: 136.26s
I - Confusion Matrix: [row->prediction - col->label]
[[612.   3.   1.  28. 108.]
 [  6. 543.  21.  18.  56.]
 [  7.  12. 661.  24. 127.]
 [ 34.   3.  21. 453.  63.]
 [ 38.  17.  30.  15. 646.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.075 | Acc: 61.538% | Wgt Acc: 56.744% | Dur: 15.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   0.   2.  11.   6.]
 [  1.  34.   6.   4.  12.]
 [  2.  18.  46.   7.  18.]
 [ 14.   3.   3.  40.   1.]
 [ 22.  23.  18.  24. 143.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 0.480 | Acc: 82.875% | Wgt Acc: 86.994%
	I - Batch: 100 | Loss: 0.476 | Acc: 83.438% | Wgt Acc: 87.014%
	I - Batch: 150 | Loss: 0.475 | Acc: 83.333% | Wgt Acc: 86.900%
	I - Batch: 200 | Loss: 0.480 | Acc: 82.938% | Wgt Acc: 86.536%
I - num batch: 222
I - Train -- Loss: 0.484 | Acc: 82.802% | Wgt Acc: 86.363% | LR: 1.250000e-05 | Dur: 138.35s
I - Confusion Matrix: [row->prediction - col->label]
[[632.   3.   2.  32. 114.]
 [  2. 535.  13.   3.  63.]
 [  6.  15. 662.  21. 115.]
 [ 29.   9.  18. 465.  65.]
 [ 28.  16.  39.  17. 643.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.124 | Acc: 60.552% | Wgt Acc: 53.526% | Dur: 14.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  14.  10.]
 [  0.  22.   3.   2.   3.]
 [  1.  22.  41.   5.  10.]
 [  8.   1.   3.  34.   1.]
 [ 25.  31.  26.  31. 156.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 126
I - Training: 
	I - Batch: 50 | Loss: 0.464 | Acc: 83.375% | Wgt Acc: 87.293%
	I - Batch: 100 | Loss: 0.462 | Acc: 83.062% | Wgt Acc: 86.420%
	I - Batch: 150 | Loss: 0.469 | Acc: 82.958% | Wgt Acc: 86.279%
	I - Batch: 200 | Loss: 0.481 | Acc: 82.156% | Wgt Acc: 85.657%
I - num batch: 222
I - Train -- Loss: 0.478 | Acc: 82.182% | Wgt Acc: 85.796% | LR: 1.250000e-05 | Dur: 137.75s
I - Confusion Matrix: [row->prediction - col->label]
[[616.   2.   3.  32.  99.]
 [  6. 540.  13.   8.  66.]
 [  6.  20. 667.  24. 138.]
 [ 23.   4.  24. 458.  63.]
 [ 46.  12.  27.  16. 634.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.124 | Acc: 62.327% | Wgt Acc: 56.232% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   0.   1.  15.   9.]
 [  0.  25.   3.   1.   3.]
 [  2.  24.  46.   5.  13.]
 [ 12.   4.   4.  39.   2.]
 [ 21.  25.  21.  26. 153.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 127
I - Training: 
	I - Batch: 50 | Loss: 0.468 | Acc: 83.625% | Wgt Acc: 86.718%
	I - Batch: 100 | Loss: 0.461 | Acc: 83.625% | Wgt Acc: 86.951%
	I - Batch: 150 | Loss: 0.467 | Acc: 82.708% | Wgt Acc: 86.321%
	I - Batch: 200 | Loss: 0.473 | Acc: 82.344% | Wgt Acc: 86.046%
I - num batch: 222
I - Train -- Loss: 0.475 | Acc: 82.154% | Wgt Acc: 85.795% | LR: 1.250000e-05 | Dur: 137.55s
I - Confusion Matrix: [row->prediction - col->label]
[[625.   2.   4.  29. 122.]
 [  6. 528.  20.  11.  56.]
 [  2.  25. 667.  18. 129.]
 [ 31.  10.  16. 462.  61.]
 [ 33.  13.  27.  18. 632.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.041 | Acc: 62.919% | Wgt Acc: 58.343% | Dur: 18.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   0.   2.  14.  10.]
 [  1.  33.   5.   2.   9.]
 [  2.  21.  44.   3.  17.]
 [ 11.   6.   4.  43.   0.]
 [ 19.  18.  20.  24. 144.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 128
I - Training: 
	I - Batch: 50 | Loss: 0.471 | Acc: 83.750% | Wgt Acc: 86.533%
	I - Batch: 100 | Loss: 0.476 | Acc: 83.375% | Wgt Acc: 86.414%
	I - Batch: 150 | Loss: 0.478 | Acc: 83.333% | Wgt Acc: 86.402%
	I - Batch: 200 | Loss: 0.475 | Acc: 83.344% | Wgt Acc: 86.511%
I - num batch: 222
I - Train -- Loss: 0.476 | Acc: 83.084% | Wgt Acc: 86.259% | LR: 1.250000e-05 | Dur: 135.57s
I - Confusion Matrix: [row->prediction - col->label]
[[609.   1.   5.  34. 102.]
 [  5. 539.  14.   8.  58.]
 [  3.  17. 672.  21. 121.]
 [ 33.  12.  18. 461.  53.]
 [ 47.   9.  25.  14. 666.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.262 | Acc: 59.763% | Wgt Acc: 51.656% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   0.   1.   9.   5.]
 [  0.  27.   5.   4.   4.]
 [  2.  21.  42.   4.   9.]
 [  6.   1.   0.  27.   0.]
 [ 35.  29.  27.  42. 162.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 129
I - Training: 
	I - Batch: 50 | Loss: 0.496 | Acc: 81.000% | Wgt Acc: 85.110%
	I - Batch: 100 | Loss: 0.482 | Acc: 82.000% | Wgt Acc: 85.722%
	I - Batch: 150 | Loss: 0.477 | Acc: 82.208% | Wgt Acc: 85.950%
	I - Batch: 200 | Loss: 0.471 | Acc: 82.625% | Wgt Acc: 86.283%
I - num batch: 222
I - Train -- Loss: 0.477 | Acc: 82.379% | Wgt Acc: 86.201% | LR: 1.250000e-05 | Dur: 135.70s
I - Confusion Matrix: [row->prediction - col->label]
[[615.   0.   3.  30. 136.]
 [  2. 545.  15.   9.  63.]
 [  5.  11. 671.  17. 111.]
 [ 32.   9.  14. 465.  64.]
 [ 43.  13.  31.  17. 626.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.153 | Acc: 61.341% | Wgt Acc: 54.895% | Dur: 14.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   2.   2.   9.   8.]
 [  0.  25.   4.   2.   4.]
 [  3.  23.  45.   6.  14.]
 [ 10.   1.   2.  39.   0.]
 [ 27.  27.  22.  30. 154.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 130
I - Training: 
	I - Batch: 50 | Loss: 0.446 | Acc: 83.250% | Wgt Acc: 87.044%
	I - Batch: 100 | Loss: 0.453 | Acc: 83.125% | Wgt Acc: 86.488%
	I - Batch: 150 | Loss: 0.452 | Acc: 83.458% | Wgt Acc: 86.765%
	I - Batch: 200 | Loss: 0.452 | Acc: 83.656% | Wgt Acc: 87.063%
I - num batch: 222
I - Train -- Loss: 0.455 | Acc: 83.564% | Wgt Acc: 87.079% | LR: 1.250000e-05 | Dur: 133.79s
I - Confusion Matrix: [row->prediction - col->label]
[[624.   0.   1.  29. 110.]
 [  2. 542.  12.   7.  46.]
 [  4.  15. 670.  16. 124.]
 [ 27.   5.  16. 474.  66.]
 [ 40.  16.  35.  12. 654.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.110 | Acc: 61.736% | Wgt Acc: 54.310% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   1.   2.  10.   7.]
 [  0.  27.   4.   2.   3.]
 [  2.  23.  42.   4.   8.]
 [ 12.   1.   3.  39.   0.]
 [ 31.  26.  24.  31. 162.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 131
I - Training: 
	I - Batch: 50 | Loss: 0.478 | Acc: 81.500% | Wgt Acc: 85.915%
	I - Batch: 100 | Loss: 0.453 | Acc: 82.562% | Wgt Acc: 86.665%
	I - Batch: 150 | Loss: 0.468 | Acc: 82.125% | Wgt Acc: 86.207%
	I - Batch: 200 | Loss: 0.465 | Acc: 82.594% | Wgt Acc: 86.495%
I - num batch: 222
I - Train -- Loss: 0.471 | Acc: 82.154% | Wgt Acc: 85.961% | LR: 1.250000e-05 | Dur: 132.91s
I - Confusion Matrix: [row->prediction - col->label]
[[616.   1.   2.  35. 135.]
 [  4. 539.  16.   8.  62.]
 [  1.  17. 672.  16. 120.]
 [ 36.   8.  16. 463.  59.]
 [ 40.  13.  28.  16. 624.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.087 | Acc: 63.314% | Wgt Acc: 57.120% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   2.  17.   8.]
 [  0.  26.   3.   4.   5.]
 [  1.  20.  37.   1.  10.]
 [ 11.   4.   6.  43.   1.]
 [ 17.  26.  27.  21. 156.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 132
I - Training: 
	I - Batch: 50 | Loss: 0.478 | Acc: 82.250% | Wgt Acc: 86.331%
	I - Batch: 100 | Loss: 0.475 | Acc: 82.812% | Wgt Acc: 86.462%
	I - Batch: 150 | Loss: 0.477 | Acc: 82.958% | Wgt Acc: 86.428%
	I - Batch: 200 | Loss: 0.466 | Acc: 83.406% | Wgt Acc: 86.944%
I - num batch: 222
I - Train -- Loss: 0.468 | Acc: 83.282% | Wgt Acc: 86.922% | LR: 1.250000e-05 | Dur: 132.69s
I - Confusion Matrix: [row->prediction - col->label]
[[622.   2.   3.  30. 117.]
 [  2. 544.  14.   9.  60.]
 [  6.  18. 676.  14. 105.]
 [ 26.   3.  14. 468.  74.]
 [ 41.  11.  27.  17. 644.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.108 | Acc: 62.919% | Wgt Acc: 56.347% | Dur: 22.81s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   2.   1.   9.   6.]
 [  0.  30.   4.   3.   5.]
 [  1.  18.  40.   4.  11.]
 [  9.   1.   2.  40.   0.]
 [ 27.  27.  28.  30. 158.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 133
I - Training: 
	I - Batch: 50 | Loss: 0.453 | Acc: 83.125% | Wgt Acc: 87.136%
	I - Batch: 100 | Loss: 0.457 | Acc: 83.750% | Wgt Acc: 87.677%
	I - Batch: 150 | Loss: 0.465 | Acc: 83.625% | Wgt Acc: 87.438%
	I - Batch: 200 | Loss: 0.465 | Acc: 83.688% | Wgt Acc: 87.600%
I - num batch: 222
I - Train -- Loss: 0.465 | Acc: 83.620% | Wgt Acc: 87.556% | LR: 1.250000e-05 | Dur: 134.87s
I - Confusion Matrix: [row->prediction - col->label]
[[639.   2.   5.  26. 120.]
 [  5. 545.  14.   6.  67.]
 [  6.  12. 676.  12. 119.]
 [ 22.   4.  12. 474.  62.]
 [ 25.  15.  27.  20. 632.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.114 | Acc: 63.116% | Wgt Acc: 56.504% | Dur: 18.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   2.  19.   9.]
 [  0.  30.   3.   3.   4.]
 [  1.  17.  39.   1.   9.]
 [  8.   1.   4.  36.   0.]
 [ 22.  28.  27.  27. 158.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 134
I - Training: 
	I - Batch: 50 | Loss: 0.445 | Acc: 82.750% | Wgt Acc: 86.280%
	I - Batch: 100 | Loss: 0.465 | Acc: 82.438% | Wgt Acc: 85.770%
	I - Batch: 150 | Loss: 0.445 | Acc: 83.708% | Wgt Acc: 87.114%
	I - Batch: 200 | Loss: 0.443 | Acc: 83.688% | Wgt Acc: 87.103%
I - num batch: 208
I - Train -- Loss: 0.440 | Acc: 83.924% | Wgt Acc: 87.326% | LR: 1.250000e-05 | Dur: 127.89s
I - Confusion Matrix: [row->prediction - col->label]
[[618.   0.   2.  28.  93.]
 [  2. 539.  14.   8.  55.]
 [  5.  20. 677.  17.  97.]
 [ 29.   6.  18. 474.  51.]
 [ 43.  13.  23.  11. 485.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.146 | Acc: 60.552% | Wgt Acc: 54.132% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   0.   2.   8.   7.]
 [  0.  28.   5.   5.   6.]
 [  2.  25.  50.   5.  14.]
 [ 11.   1.   0.  34.   1.]
 [ 32.  24.  18.  34. 152.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 135
I - Training: 
	I - Batch: 50 | Loss: 0.479 | Acc: 82.125% | Wgt Acc: 86.647%
	I - Batch: 100 | Loss: 0.464 | Acc: 82.688% | Wgt Acc: 86.739%
	I - Batch: 150 | Loss: 0.470 | Acc: 82.500% | Wgt Acc: 86.489%
	I - Batch: 200 | Loss: 0.467 | Acc: 82.594% | Wgt Acc: 86.567%
I - num batch: 222
I - Train -- Loss: 0.465 | Acc: 82.831% | Wgt Acc: 86.710% | LR: 1.250000e-05 | Dur: 136.99s
I - Confusion Matrix: [row->prediction - col->label]
[[621.   2.   2.  33. 117.]
 [  2. 540.  12.   6.  59.]
 [  6.  15. 681.  19. 130.]
 [ 33.  10.  13. 469.  67.]
 [ 35.  11.  26.  11. 627.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.155 | Acc: 60.750% | Wgt Acc: 53.735% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   0.   3.   7.   5.]
 [  0.  28.   5.   2.   4.]
 [  2.  18.  39.   5.  13.]
 [ 11.   1.   2.  36.   1.]
 [ 27.  31.  26.  36. 157.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 136
I - Training: 
	I - Batch: 50 | Loss: 0.436 | Acc: 83.375% | Wgt Acc: 87.061%
	I - Batch: 100 | Loss: 0.437 | Acc: 83.562% | Wgt Acc: 87.066%
	I - Batch: 150 | Loss: 0.452 | Acc: 83.333% | Wgt Acc: 86.769%
	I - Batch: 200 | Loss: 0.449 | Acc: 83.844% | Wgt Acc: 87.123%
I - num batch: 222
I - Train -- Loss: 0.451 | Acc: 83.592% | Wgt Acc: 87.028% | LR: 1.250000e-05 | Dur: 135.40s
I - Confusion Matrix: [row->prediction - col->label]
[[617.   3.   3.  29. 116.]
 [  1. 551.  12.  10.  58.]
 [  4.   9. 673.  17. 104.]
 [ 34.   3.  17. 466.  64.]
 [ 41.  12.  29.  16. 658.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.194 | Acc: 60.947% | Wgt Acc: 52.837% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   2.   1.  10.   5.]
 [  1.  24.   5.   2.   4.]
 [  2.  21.  44.   6.   7.]
 [  8.   0.   0.  29.   0.]
 [ 29.  31.  25.  39. 164.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 137
I - Training: 
	I - Batch: 50 | Loss: 0.433 | Acc: 86.125% | Wgt Acc: 88.750%
	I - Batch: 100 | Loss: 0.437 | Acc: 84.625% | Wgt Acc: 88.531%
	I - Batch: 150 | Loss: 0.456 | Acc: 83.208% | Wgt Acc: 87.017%
	I - Batch: 200 | Loss: 0.457 | Acc: 83.188% | Wgt Acc: 87.027%
I - num batch: 222
I - Train -- Loss: 0.461 | Acc: 82.831% | Wgt Acc: 86.770% | LR: 1.250000e-05 | Dur: 133.49s
I - Confusion Matrix: [row->prediction - col->label]
[[624.   1.   3.  28. 129.]
 [  6. 546.  15.  10.  50.]
 [  3.  12. 684.  22. 133.]
 [ 29.   6.  10. 461.  65.]
 [ 35.  13.  22.  17. 623.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.186 | Acc: 59.172% | Wgt Acc: 51.123% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   2.   2.  11.   7.]
 [  0.  22.   5.   2.   5.]
 [  2.  20.  36.   4.   7.]
 [  8.   0.   1.  32.   0.]
 [ 29.  34.  31.  37. 161.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 138
I - Training: 
	I - Batch: 50 | Loss: 0.454 | Acc: 82.375% | Wgt Acc: 86.363%
	I - Batch: 100 | Loss: 0.453 | Acc: 82.875% | Wgt Acc: 86.252%
	I - Batch: 150 | Loss: 0.462 | Acc: 82.542% | Wgt Acc: 85.869%
	I - Batch: 200 | Loss: 0.462 | Acc: 82.656% | Wgt Acc: 86.218%
I - num batch: 222
I - Train -- Loss: 0.455 | Acc: 83.141% | Wgt Acc: 86.651% | LR: 1.250000e-05 | Dur: 136.10s
I - Confusion Matrix: [row->prediction - col->label]
[[620.   2.   3.  24. 118.]
 [  2. 542.  19.   7.  49.]
 [  4.  16. 667.  18. 128.]
 [ 24.   5.  17. 470.  55.]
 [ 47.  13.  28.  19. 650.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.108 | Acc: 61.538% | Wgt Acc: 56.013% | Dur: 15.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   1.  10.  10.]
 [  0.  31.   3.   3.   6.]
 [  2.  23.  44.   4.  15.]
 [ 15.   2.   4.  39.   1.]
 [ 21.  21.  23.  30. 148.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 139
I - Training: 
	I - Batch: 50 | Loss: 0.481 | Acc: 83.250% | Wgt Acc: 86.565%
	I - Batch: 100 | Loss: 0.451 | Acc: 84.250% | Wgt Acc: 87.333%
	I - Batch: 150 | Loss: 0.459 | Acc: 83.750% | Wgt Acc: 87.004%
	I - Batch: 200 | Loss: 0.459 | Acc: 83.188% | Wgt Acc: 86.824%
I - num batch: 222
I - Train -- Loss: 0.455 | Acc: 83.310% | Wgt Acc: 86.968% | LR: 1.250000e-05 | Dur: 135.41s
I - Confusion Matrix: [row->prediction - col->label]
[[628.   1.   2.  31. 128.]
 [  3. 540.  14.   7.  65.]
 [  4.  13. 676.  18. 108.]
 [ 25.  12.  15. 468.  56.]
 [ 37.  12.  27.  14. 643.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.187 | Acc: 59.369% | Wgt Acc: 51.353% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 41.   0.   1.   6.   2.]
 [  1.  25.   3.   2.   5.]
 [  1.  17.  44.   4.  12.]
 [ 11.   0.   0.  30.   0.]
 [ 34.  36.  27.  44. 161.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 140
I - Training: 
	I - Batch: 50 | Loss: 0.464 | Acc: 82.500% | Wgt Acc: 86.248%
	I - Batch: 100 | Loss: 0.453 | Acc: 83.312% | Wgt Acc: 86.761%
	I - Batch: 150 | Loss: 0.449 | Acc: 83.417% | Wgt Acc: 86.841%
	I - Batch: 200 | Loss: 0.447 | Acc: 83.562% | Wgt Acc: 86.987%
I - num batch: 222
I - Train -- Loss: 0.449 | Acc: 83.648% | Wgt Acc: 87.056% | LR: 1.250000e-05 | Dur: 133.79s
I - Confusion Matrix: [row->prediction - col->label]
[[632.   1.   3.  35.  99.]
 [  1. 550.  19.   6.  57.]
 [  1.  14. 666.  16. 129.]
 [ 28.   2.  14. 460.  56.]
 [ 35.  11.  32.  21. 659.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.159 | Acc: 60.750% | Wgt Acc: 53.652% | Dur: 13.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   0.   1.   8.   6.]
 [  0.  25.   4.   1.   3.]
 [  2.  19.  38.   6.  12.]
 [ 12.   1.   2.  40.   1.]
 [ 27.  33.  30.  31. 158.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 141
I - Training: 
	I - Batch: 50 | Loss: 0.442 | Acc: 83.375% | Wgt Acc: 86.913%
	I - Batch: 100 | Loss: 0.435 | Acc: 83.875% | Wgt Acc: 87.809%
	I - Batch: 150 | Loss: 0.446 | Acc: 83.417% | Wgt Acc: 87.433%
	I - Batch: 200 | Loss: 0.445 | Acc: 83.625% | Wgt Acc: 87.467%
I - num batch: 222
I - Train -- Loss: 0.446 | Acc: 83.874% | Wgt Acc: 87.586% | LR: 1.250000e-05 | Dur: 134.34s
I - Confusion Matrix: [row->prediction - col->label]
[[627.   1.   4.  30. 105.]
 [  2. 555.  16.   6.  62.]
 [  1.  12. 680.  17. 120.]
 [ 25.   3.  12. 467.  67.]
 [ 42.   7.  22.  18. 646.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.205 | Acc: 58.974% | Wgt Acc: 51.416% | Dur: 14.80s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   1.   2.   8.   4.]
 [  0.  23.   3.   3.   3.]
 [  1.  23.  43.   4.  15.]
 [ 13.   1.   2.  31.   1.]
 [ 29.  30.  25.  40. 157.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 142
I - Training: 
	I - Batch: 50 | Loss: 0.449 | Acc: 83.750% | Wgt Acc: 87.060%
	I - Batch: 100 | Loss: 0.462 | Acc: 82.875% | Wgt Acc: 86.537%
	I - Batch: 150 | Loss: 0.454 | Acc: 83.250% | Wgt Acc: 86.769%
	I - Batch: 200 | Loss: 0.450 | Acc: 83.406% | Wgt Acc: 86.870%
I - num batch: 222
I - Train -- Loss: 0.447 | Acc: 83.282% | Wgt Acc: 86.742% | LR: 1.250000e-05 | Dur: 134.19s
I - Confusion Matrix: [row->prediction - col->label]
[[619.   1.   2.  31. 114.]
 [  4. 540.  10.   9.  60.]
 [  7.  11. 677.  19. 111.]
 [ 28.   8.  13. 465.  62.]
 [ 39.  18.  32.  14. 653.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.158 | Acc: 60.355% | Wgt Acc: 53.599% | Dur: 14.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 42.   2.   1.   9.   5.]
 [  0.  33.   5.   3.   4.]
 [  2.  15.  39.   6.  16.]
 [ 12.   1.   2.  37.   0.]
 [ 32.  27.  28.  31. 155.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 143
I - Training: 
	I - Batch: 50 | Loss: 0.468 | Acc: 82.625% | Wgt Acc: 86.247%
	I - Batch: 100 | Loss: 0.472 | Acc: 82.375% | Wgt Acc: 86.359%
	I - Batch: 150 | Loss: 0.461 | Acc: 83.417% | Wgt Acc: 86.996%
	I - Batch: 200 | Loss: 0.452 | Acc: 83.562% | Wgt Acc: 87.240%
I - num batch: 222
I - Train -- Loss: 0.449 | Acc: 83.874% | Wgt Acc: 87.348% | LR: 1.250000e-05 | Dur: 131.18s
I - Confusion Matrix: [row->prediction - col->label]
[[630.   2.   3.  32. 109.]
 [  3. 545.  14.   8.  54.]
 [  7.  13. 683.  20. 110.]
 [ 26.   6.  13. 460.  70.]
 [ 31.  12.  21.  18. 657.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.188 | Acc: 59.369% | Wgt Acc: 52.011% | Dur: 13.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 40.   0.   1.   7.   4.]
 [  1.  30.   5.   3.   6.]
 [  1.  15.  41.   3.  12.]
 [ 10.   1.   0.  33.   1.]
 [ 36.  32.  28.  40. 157.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 144
I - Training: 
	I - Batch: 50 | Loss: 0.438 | Acc: 83.625% | Wgt Acc: 88.282%
	I - Batch: 100 | Loss: 0.440 | Acc: 83.188% | Wgt Acc: 87.511%
	I - Batch: 150 | Loss: 0.439 | Acc: 83.125% | Wgt Acc: 87.152%
	I - Batch: 200 | Loss: 0.447 | Acc: 83.281% | Wgt Acc: 87.244%
I - num batch: 222
I - Train -- Loss: 0.445 | Acc: 83.564% | Wgt Acc: 87.367% | LR: 1.250000e-05 | Dur: 136.25s
I - Confusion Matrix: [row->prediction - col->label]
[[628.   0.   0.  27. 121.]
 [  4. 549.  13.  10.  58.]
 [  7.  12. 674.  17. 123.]
 [ 23.   9.  14. 474.  59.]
 [ 35.   8.  33.  10. 639.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.145 | Acc: 62.919% | Wgt Acc: 56.191% | Dur: 17.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   1.   3.   9.   6.]
 [  0.  28.   3.   2.   3.]
 [  1.  19.  43.   5.  12.]
 [ 10.   1.   2.  40.   0.]
 [ 28.  29.  24.  30. 159.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 145
I - Training: 
	I - Batch: 50 | Loss: 0.455 | Acc: 82.500% | Wgt Acc: 85.695%
	I - Batch: 100 | Loss: 0.451 | Acc: 83.688% | Wgt Acc: 86.979%
	I - Batch: 150 | Loss: 0.450 | Acc: 83.542% | Wgt Acc: 86.877%
	I - Batch: 200 | Loss: 0.440 | Acc: 84.250% | Wgt Acc: 87.576%
I - num batch: 222
I - Train -- Loss: 0.443 | Acc: 84.268% | Wgt Acc: 87.622% | LR: 1.250000e-05 | Dur: 134.37s
I - Confusion Matrix: [row->prediction - col->label]
[[613.   2.   6.  25. 106.]
 [  5. 555.  16.   4.  58.]
 [  5.   8. 669.  15.  93.]
 [ 34.   2.  14. 481.  72.]
 [ 40.  11.  29.  13. 671.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.183 | Acc: 59.961% | Wgt Acc: 52.231% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   2.  13.   7.]
 [  0.  23.   4.   3.   3.]
 [  2.  20.  39.   3.  10.]
 [  9.   1.   2.  32.   0.]
 [ 27.  33.  28.  35. 160.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 146
I - Training: 
	I - Batch: 50 | Loss: 0.426 | Acc: 85.375% | Wgt Acc: 88.927%
	I - Batch: 100 | Loss: 0.435 | Acc: 84.062% | Wgt Acc: 87.860%
	I - Batch: 150 | Loss: 0.433 | Acc: 84.458% | Wgt Acc: 88.275%
	I - Batch: 200 | Loss: 0.433 | Acc: 84.125% | Wgt Acc: 87.966%
I - num batch: 222
I - Train -- Loss: 0.440 | Acc: 83.648% | Wgt Acc: 87.542% | LR: 1.250000e-05 | Dur: 137.88s
I - Confusion Matrix: [row->prediction - col->label]
[[620.   1.   0.  21. 117.]
 [  2. 552.   7.   7.  53.]
 [  4.  12. 679.  17. 130.]
 [ 29.   4.  20. 480.  64.]
 [ 42.   9.  28.  13. 636.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.137 | Acc: 60.158% | Wgt Acc: 52.690% | Dur: 18.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   1.   2.   8.   8.]
 [  0.  23.   4.   2.   5.]
 [  2.  18.  35.   4.   7.]
 [ 10.   1.   2.  36.   1.]
 [ 24.  35.  32.  36. 159.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 147
I - Training: 
	I - Batch: 50 | Loss: 0.412 | Acc: 85.375% | Wgt Acc: 89.490%
	I - Batch: 100 | Loss: 0.437 | Acc: 83.812% | Wgt Acc: 87.557%
	I - Batch: 150 | Loss: 0.439 | Acc: 83.417% | Wgt Acc: 87.490%
	I - Batch: 200 | Loss: 0.435 | Acc: 83.719% | Wgt Acc: 87.582%
I - num batch: 222
I - Train -- Loss: 0.434 | Acc: 83.733% | Wgt Acc: 87.630% | LR: 1.250000e-05 | Dur: 138.25s
I - Confusion Matrix: [row->prediction - col->label]
[[629.   1.   1.  25. 127.]
 [  3. 548.  10.   6.  63.]
 [  2.  10. 686.  19. 112.]
 [ 24.   5.  13. 472.  63.]
 [ 39.  14.  24.  16. 635.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.094 | Acc: 61.736% | Wgt Acc: 55.449% | Dur: 15.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   2.  16.  11.]
 [  0.  32.   4.   4.   5.]
 [  1.  16.  33.   1.   9.]
 [  9.   1.   4.  39.   1.]
 [ 23.  27.  32.  26. 154.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 148
I - Training: 
	I - Batch: 50 | Loss: 0.425 | Acc: 84.375% | Wgt Acc: 87.926%
	I - Batch: 100 | Loss: 0.431 | Acc: 84.125% | Wgt Acc: 87.766%
	I - Batch: 150 | Loss: 0.431 | Acc: 84.667% | Wgt Acc: 88.099%
	I - Batch: 200 | Loss: 0.432 | Acc: 84.562% | Wgt Acc: 87.948%
I - num batch: 222
I - Train -- Loss: 0.432 | Acc: 84.494% | Wgt Acc: 87.993% | LR: 1.250000e-05 | Dur: 136.69s
I - Confusion Matrix: [row->prediction - col->label]
[[634.   1.   0.  32. 109.]
 [  1. 548.   9.   9.  60.]
 [  5.   7. 681.  17.  97.]
 [ 28.   7.  12. 471.  71.]
 [ 29.  15.  32.   9. 663.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 62.525% | Wgt Acc: 55.794% | Dur: 14.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   3.  18.   6.]
 [  0.  27.   4.   3.   8.]
 [  1.  15.  35.   3.   8.]
 [  7.   1.   2.  38.   0.]
 [ 21.  33.  31.  24. 158.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 149
I - Training: 
	I - Batch: 50 | Loss: 0.442 | Acc: 83.875% | Wgt Acc: 87.468%
	I - Batch: 100 | Loss: 0.441 | Acc: 83.375% | Wgt Acc: 86.734%
	I - Batch: 150 | Loss: 0.447 | Acc: 83.250% | Wgt Acc: 86.728%
	I - Batch: 200 | Loss: 0.445 | Acc: 83.531% | Wgt Acc: 87.099%
I - num batch: 222
I - Train -- Loss: 0.442 | Acc: 83.648% | Wgt Acc: 87.200% | LR: 1.250000e-05 | Dur: 135.49s
I - Confusion Matrix: [row->prediction - col->label]
[[617.   0.   1.  33. 111.]
 [  4. 549.  10.   5.  67.]
 [  7.  12. 676.  15. 110.]
 [ 33.   4.  14. 472.  59.]
 [ 36.  13.  33.  13. 653.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.119 | Acc: 61.538% | Wgt Acc: 54.540% | Dur: 14.56s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   2.  20.   7.]
 [  0.  28.   6.   2.   5.]
 [  2.  19.  34.   0.  10.]
 [  8.   0.   4.  35.   0.]
 [ 21.  29.  29.  29. 158.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 150
I - Training: 
	I - Batch: 50 | Loss: 0.405 | Acc: 86.250% | Wgt Acc: 89.670%
	I - Batch: 100 | Loss: 0.419 | Acc: 85.062% | Wgt Acc: 88.518%
	I - Batch: 150 | Loss: 0.427 | Acc: 84.292% | Wgt Acc: 87.852%
	I - Batch: 200 | Loss: 0.426 | Acc: 84.312% | Wgt Acc: 88.046%
I - num batch: 222
I - Train -- Loss: 0.428 | Acc: 84.381% | Wgt Acc: 88.005% | LR: 1.250000e-05 | Dur: 135.28s
I - Confusion Matrix: [row->prediction - col->label]
[[627.   0.   1.  30. 114.]
 [  3. 551.  13.   5.  53.]
 [  3.  10. 685.  17. 104.]
 [ 25.   4.  10. 474.  73.]
 [ 39.  13.  25.  12. 656.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.144 | Acc: 62.130% | Wgt Acc: 56.661% | Dur: 15.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   1.   3.   8.   9.]
 [  1.  33.   2.   3.   6.]
 [  2.  19.  40.   3.  15.]
 [ 10.   1.   2.  41.   1.]
 [ 23.  24.  28.  31. 149.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 151
I - Training: 
	I - Batch: 50 | Loss: 0.411 | Acc: 85.750% | Wgt Acc: 89.075%
	I - Batch: 100 | Loss: 0.405 | Acc: 85.875% | Wgt Acc: 88.849%
	I - Batch: 150 | Loss: 0.420 | Acc: 85.167% | Wgt Acc: 88.265%
	I - Batch: 200 | Loss: 0.424 | Acc: 84.938% | Wgt Acc: 87.988%
I - num batch: 222
I - Train -- Loss: 0.422 | Acc: 84.917% | Wgt Acc: 88.040% | LR: 1.250000e-05 | Dur: 134.49s
I - Confusion Matrix: [row->prediction - col->label]
[[621.   0.   2.  24.  99.]
 [  6. 555.  10.   9.  47.]
 [  7.   7. 674.  13. 108.]
 [ 24.   4.  23. 474.  58.]
 [ 39.  12.  25.  18. 688.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.342 | Acc: 57.199% | Wgt Acc: 47.487% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 42.   1.   2.  10.   3.]
 [  1.  22.   3.   4.   3.]
 [  2.  17.  32.   4.   5.]
 [  6.   0.   0.  25.   0.]
 [ 37.  38.  38.  43. 169.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 152
I - Training: 
	I - Batch: 50 | Loss: 0.440 | Acc: 85.125% | Wgt Acc: 88.522%
	I - Batch: 100 | Loss: 0.435 | Acc: 84.812% | Wgt Acc: 88.342%
	I - Batch: 150 | Loss: 0.428 | Acc: 84.667% | Wgt Acc: 88.123%
	I - Batch: 200 | Loss: 0.429 | Acc: 84.562% | Wgt Acc: 88.070%
I - num batch: 222
I - Train -- Loss: 0.430 | Acc: 84.522% | Wgt Acc: 88.046% | LR: 1.250000e-05 | Dur: 134.49s
I - Confusion Matrix: [row->prediction - col->label]
[[632.   2.   4.  27. 109.]
 [  2. 542.  12.   4.  52.]
 [  4.  20. 680.  13. 104.]
 [ 27.   4.  17. 481.  72.]
 [ 32.  10.  21.  13. 663.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.176 | Acc: 61.538% | Wgt Acc: 54.404% | Dur: 17.83s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   0.   1.   7.   6.]
 [  0.  28.   3.   5.   4.]
 [  2.  21.  45.   6.  11.]
 [ 11.   1.   3.  35.   0.]
 [ 30.  28.  23.  33. 159.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 153
I - Training: 
	I - Batch: 50 | Loss: 0.389 | Acc: 86.000% | Wgt Acc: 89.652%
	I - Batch: 100 | Loss: 0.401 | Acc: 85.625% | Wgt Acc: 89.186%
	I - Batch: 150 | Loss: 0.416 | Acc: 84.583% | Wgt Acc: 88.156%
	I - Batch: 200 | Loss: 0.416 | Acc: 84.844% | Wgt Acc: 88.474%
I - num batch: 222
I - Train -- Loss: 0.417 | Acc: 84.889% | Wgt Acc: 88.501% | LR: 1.250000e-05 | Dur: 138.57s
I - Confusion Matrix: [row->prediction - col->label]
[[628.   2.   3.  29. 110.]
 [  3. 554.  10.   8.  58.]
 [  4.   6. 688.  13. 113.]
 [ 26.   5.  10. 479.  57.]
 [ 36.  11.  23.   9. 662.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.164 | Acc: 63.116% | Wgt Acc: 56.546% | Dur: 16.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   2.   2.  16.   9.]
 [  0.  30.   5.   2.   4.]
 [  0.  18.  36.   0.   9.]
 [  7.   0.   3.  38.   0.]
 [ 23.  28.  29.  30. 158.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 154
I - Training: 
	I - Batch: 50 | Loss: 0.432 | Acc: 85.250% | Wgt Acc: 88.467%
	I - Batch: 100 | Loss: 0.417 | Acc: 85.562% | Wgt Acc: 88.685%
	I - Batch: 150 | Loss: 0.425 | Acc: 84.792% | Wgt Acc: 88.296%
	I - Batch: 200 | Loss: 0.424 | Acc: 84.750% | Wgt Acc: 88.187%
I - num batch: 222
I - Train -- Loss: 0.423 | Acc: 84.607% | Wgt Acc: 88.133% | LR: 1.250000e-05 | Dur: 134.72s
I - Confusion Matrix: [row->prediction - col->label]
[[625.   1.   0.  24. 116.]
 [  2. 548.   7.   8.  56.]
 [  4.   8. 691.  14. 103.]
 [ 25.   3.  16. 474.  62.]
 [ 41.  18.  20.  18. 663.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.208 | Acc: 60.158% | Wgt Acc: 52.419% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 40.   1.   1.   7.   6.]
 [  0.  29.   4.   2.   5.]
 [  2.  18.  42.   5.   8.]
 [ 12.   1.   2.  33.   0.]
 [ 34.  29.  26.  39. 161.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 155
I - Training: 
	I - Batch: 50 | Loss: 0.424 | Acc: 85.125% | Wgt Acc: 88.245%
	I - Batch: 100 | Loss: 0.430 | Acc: 84.438% | Wgt Acc: 87.977%
	I - Batch: 150 | Loss: 0.424 | Acc: 84.458% | Wgt Acc: 88.026%
	I - Batch: 200 | Loss: 0.426 | Acc: 84.812% | Wgt Acc: 88.316%
I - num batch: 222
I - Train -- Loss: 0.425 | Acc: 84.889% | Wgt Acc: 88.353% | LR: 1.250000e-05 | Dur: 133.42s
I - Confusion Matrix: [row->prediction - col->label]
[[633.   1.   4.  22. 118.]
 [  4. 545.  12.   6.  55.]
 [  4.  11. 687.  15. 108.]
 [ 24.   4.   9. 477.  50.]
 [ 32.  17.  22.  18. 669.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.130 | Acc: 60.947% | Wgt Acc: 52.638% | Dur: 20.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   0.   1.  13.   6.]
 [  0.  27.   3.   5.   2.]
 [  1.  15.  37.   2.   6.]
 [  7.   1.   3.  31.   0.]
 [ 32.  35.  31.  35. 166.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 156
I - Training: 
	I - Batch: 50 | Loss: 0.438 | Acc: 84.750% | Wgt Acc: 88.633%
	I - Batch: 100 | Loss: 0.429 | Acc: 84.562% | Wgt Acc: 88.433%
	I - Batch: 150 | Loss: 0.433 | Acc: 84.250% | Wgt Acc: 88.083%
	I - Batch: 200 | Loss: 0.426 | Acc: 84.438% | Wgt Acc: 88.283%
I - num batch: 222
I - Train -- Loss: 0.424 | Acc: 84.409% | Wgt Acc: 88.325% | LR: 1.250000e-05 | Dur: 133.99s
I - Confusion Matrix: [row->prediction - col->label]
[[632.   5.   3.  26. 133.]
 [  4. 549.   9.   4.  59.]
 [  7.  11. 686.  12.  95.]
 [ 22.   3.   9. 485.  71.]
 [ 32.  10.  27.  11. 642.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.231 | Acc: 59.763% | Wgt Acc: 51.552% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 40.   1.   1.   8.   6.]
 [  0.  25.   4.   3.   2.]
 [  1.  17.  38.   2.   8.]
 [ 13.   1.   2.  36.   0.]
 [ 34.  34.  30.  37. 164.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 157
I - Training: 
	I - Batch: 50 | Loss: 0.371 | Acc: 88.750% | Wgt Acc: 91.827%
	I - Batch: 100 | Loss: 0.402 | Acc: 86.312% | Wgt Acc: 89.801%
	I - Batch: 150 | Loss: 0.396 | Acc: 86.792% | Wgt Acc: 90.222%
	I - Batch: 200 | Loss: 0.404 | Acc: 86.188% | Wgt Acc: 89.579%
I - num batch: 222
I - Train -- Loss: 0.403 | Acc: 86.186% | Wgt Acc: 89.525% | LR: 1.250000e-05 | Dur: 135.18s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   0.   2.  24. 116.]
 [  3. 556.  12.   4.  43.]
 [  4.   5. 684.  15.  98.]
 [ 14.   3.  11. 484.  54.]
 [ 32.  14.  25.  11. 689.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 63.511% | Wgt Acc: 57.455% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   2.   3.  10.  10.]
 [  1.  31.   3.   1.   3.]
 [  1.  17.  38.   2.  10.]
 [ 10.   2.   4.  45.   1.]
 [ 24.  26.  27.  28. 156.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 158
I - Training: 
	I - Batch: 50 | Loss: 0.419 | Acc: 84.375% | Wgt Acc: 88.508%
	I - Batch: 100 | Loss: 0.443 | Acc: 83.125% | Wgt Acc: 87.205%
	I - Batch: 150 | Loss: 0.441 | Acc: 83.292% | Wgt Acc: 87.276%
	I - Batch: 200 | Loss: 0.435 | Acc: 83.562% | Wgt Acc: 87.481%
I - num batch: 222
I - Train -- Loss: 0.432 | Acc: 83.817% | Wgt Acc: 87.620% | LR: 1.250000e-05 | Dur: 136.30s
I - Confusion Matrix: [row->prediction - col->label]
[[623.   2.   2.  22. 126.]
 [  5. 554.  10.   6.  55.]
 [  3.  11. 685.  19. 112.]
 [ 25.   4.  13. 470.  66.]
 [ 41.   7.  24.  21. 641.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.097 | Acc: 61.736% | Wgt Acc: 54.780% | Dur: 20.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   2.   1.  14.   7.]
 [  0.  29.   3.   2.   4.]
 [  1.  20.  35.   2.   9.]
 [ 12.   3.   4.  41.   1.]
 [ 26.  24.  32.  27. 159.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 159
I - Training: 
	I - Batch: 50 | Loss: 0.410 | Acc: 84.875% | Wgt Acc: 88.825%
	I - Batch: 100 | Loss: 0.408 | Acc: 84.500% | Wgt Acc: 88.665%
	I - Batch: 150 | Loss: 0.411 | Acc: 84.958% | Wgt Acc: 88.364%
	I - Batch: 200 | Loss: 0.418 | Acc: 84.406% | Wgt Acc: 87.940%
I - num batch: 222
I - Train -- Loss: 0.419 | Acc: 84.353% | Wgt Acc: 87.856% | LR: 1.250000e-05 | Dur: 133.94s
I - Confusion Matrix: [row->prediction - col->label]
[[623.   1.   1.  31. 124.]
 [  2. 558.  14.   5.  53.]
 [  5.   8. 680.  16. 104.]
 [ 17.   5.  17. 469.  57.]
 [ 50.   6.  22.  17. 662.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.215 | Acc: 60.552% | Wgt Acc: 52.878% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 41.   0.   1.   8.   5.]
 [  0.  29.   4.   3.   3.]
 [  2.  22.  44.   6.  10.]
 [ 10.   1.   1.  32.   1.]
 [ 35.  26.  25.  37. 161.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 160
I - Training: 
	I - Batch: 50 | Loss: 0.428 | Acc: 83.500% | Wgt Acc: 87.585%
	I - Batch: 100 | Loss: 0.406 | Acc: 85.250% | Wgt Acc: 89.254%
	I - Batch: 150 | Loss: 0.412 | Acc: 85.292% | Wgt Acc: 89.230%
	I - Batch: 200 | Loss: 0.415 | Acc: 85.031% | Wgt Acc: 88.955%
I - num batch: 222
I - Train -- Loss: 0.417 | Acc: 84.917% | Wgt Acc: 88.819% | LR: 1.250000e-05 | Dur: 133.56s
I - Confusion Matrix: [row->prediction - col->label]
[[637.   3.   2.  27. 112.]
 [  2. 557.   9.   5.  64.]
 [  2.   8. 691.  14. 115.]
 [ 22.   3.   8. 480.  62.]
 [ 34.   7.  24.  12. 647.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.268 | Acc: 58.974% | Wgt Acc: 50.120% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   0.   1.   6.   4.]
 [  0.  24.   4.   4.   2.]
 [  0.  23.  39.   4.   8.]
 [ 11.   1.   1.  27.   0.]
 [ 34.  30.  30.  45. 166.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 161
I - Training: 
	I - Batch: 50 | Loss: 0.391 | Acc: 87.125% | Wgt Acc: 90.002%
	I - Batch: 100 | Loss: 0.385 | Acc: 86.938% | Wgt Acc: 90.016%
	I - Batch: 150 | Loss: 0.399 | Acc: 86.042% | Wgt Acc: 89.222%
	I - Batch: 200 | Loss: 0.398 | Acc: 86.156% | Wgt Acc: 89.257%
I - num batch: 208
I - Train -- Loss: 0.400 | Acc: 85.968% | Wgt Acc: 89.156% | LR: 1.250000e-05 | Dur: 124.75s
I - Confusion Matrix: [row->prediction - col->label]
[[636.   0.   2.  21.  91.]
 [  2. 550.   8.   6.  44.]
 [  4.  10. 686.  18.  82.]
 [ 19.   6.  18. 478.  53.]
 [ 36.  12.  20.  15. 511.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.169 | Acc: 62.130% | Wgt Acc: 54.738% | Dur: 14.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   3.  18.   7.]
 [  0.  29.   4.   3.   4.]
 [  2.  19.  39.   3.   8.]
 [  9.   0.   2.  29.   0.]
 [ 20.  28.  27.  33. 161.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 162
I - Training: 
	I - Batch: 50 | Loss: 0.414 | Acc: 84.875% | Wgt Acc: 89.120%
	I - Batch: 100 | Loss: 0.402 | Acc: 85.938% | Wgt Acc: 89.725%
	I - Batch: 150 | Loss: 0.401 | Acc: 85.750% | Wgt Acc: 89.423%
	I - Batch: 200 | Loss: 0.409 | Acc: 85.188% | Wgt Acc: 88.959%
I - num batch: 222
I - Train -- Loss: 0.409 | Acc: 85.255% | Wgt Acc: 88.947% | LR: 1.250000e-05 | Dur: 137.90s
I - Confusion Matrix: [row->prediction - col->label]
[[637.   1.   2.  27. 130.]
 [  2. 559.  12.  10.  48.]
 [  2.   7. 690.  13. 104.]
 [ 23.   2.  12. 477.  57.]
 [ 33.   9.  18.  11. 661.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.071 | Acc: 63.314% | Wgt Acc: 58.144% | Dur: 19.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   3.  19.  12.]
 [  0.  33.   5.   4.   6.]
 [  1.  16.  39.   2.   8.]
 [ 12.   3.   4.  43.   5.]
 [ 18.  24.  24.  18. 149.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 163
I - Training: 
	I - Batch: 50 | Loss: 0.387 | Acc: 86.375% | Wgt Acc: 90.163%
	I - Batch: 100 | Loss: 0.394 | Acc: 86.000% | Wgt Acc: 89.930%
	I - Batch: 150 | Loss: 0.402 | Acc: 85.208% | Wgt Acc: 88.974%
	I - Batch: 200 | Loss: 0.406 | Acc: 85.125% | Wgt Acc: 88.790%
I - num batch: 222
I - Train -- Loss: 0.408 | Acc: 85.199% | Wgt Acc: 88.789% | LR: 1.250000e-05 | Dur: 136.73s
I - Confusion Matrix: [row->prediction - col->label]
[[631.   3.   2.  25. 103.]
 [  4. 551.   6.   6.  48.]
 [  5.   9. 693.  14. 126.]
 [ 24.   2.  11. 481.  57.]
 [ 33.  13.  22.  12. 666.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.154 | Acc: 61.933% | Wgt Acc: 55.020% | Dur: 14.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   1.   3.  11.   9.]
 [  0.  25.   4.   2.   6.]
 [  2.  23.  33.   0.   5.]
 [ 10.   1.   4.  43.   1.]
 [ 22.  28.  31.  30. 159.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 164
I - Training: 
	I - Batch: 50 | Loss: 0.365 | Acc: 86.250% | Wgt Acc: 90.295%
	I - Batch: 100 | Loss: 0.387 | Acc: 84.875% | Wgt Acc: 88.893%
	I - Batch: 150 | Loss: 0.395 | Acc: 84.750% | Wgt Acc: 88.634%
	I - Batch: 200 | Loss: 0.400 | Acc: 84.500% | Wgt Acc: 88.439%
I - num batch: 222
I - Train -- Loss: 0.401 | Acc: 84.550% | Wgt Acc: 88.450% | LR: 1.250000e-05 | Dur: 138.10s
I - Confusion Matrix: [row->prediction - col->label]
[[636.   0.   3.  17. 123.]
 [  3. 556.  11.   4.  53.]
 [  2.   7. 682.  20. 112.]
 [ 14.   3.  15. 481.  68.]
 [ 42.  12.  23.  16. 644.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.113 | Acc: 62.919% | Wgt Acc: 55.491% | Dur: 22.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   1.   3.  16.   6.]
 [  0.  30.   5.   2.   5.]
 [  1.  15.  33.   1.   5.]
 [  9.   2.   3.  38.   0.]
 [ 24.  30.  31.  29. 164.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 165
I - Training: 
	I - Batch: 50 | Loss: 0.399 | Acc: 85.625% | Wgt Acc: 89.381%
	I - Batch: 100 | Loss: 0.383 | Acc: 86.625% | Wgt Acc: 90.152%
	I - Batch: 150 | Loss: 0.386 | Acc: 86.167% | Wgt Acc: 89.744%
	I - Batch: 200 | Loss: 0.397 | Acc: 85.688% | Wgt Acc: 89.391%
I - num batch: 222
I - Train -- Loss: 0.407 | Acc: 85.312% | Wgt Acc: 88.950% | LR: 1.250000e-05 | Dur: 134.02s
I - Confusion Matrix: [row->prediction - col->label]
[[631.   3.   1.  21. 118.]
 [  2. 554.  11.   5.  58.]
 [  3.   7. 685.  12. 107.]
 [ 22.   4.  10. 490.  51.]
 [ 39.  10.  27.  10. 666.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 64.300% | Wgt Acc: 58.750% | Dur: 15.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   2.  17.  16.]
 [  0.  37.   7.   2.   4.]
 [  0.   8.  29.   1.   6.]
 [  8.   1.   6.  45.   0.]
 [ 19.  30.  31.  21. 154.]]

I - Local maximum validation set accuracy:  64.30

I - Validation set results: 
[14-1-2-0.97][50-3-4-1.08][124-2-4-1.82][127-0-0-5.76][443-2-2-3.14][567-0-0-2.34][573-1-1-3.55][615-0-0-2.20][695-1-2-0.97][722-3-0-4.46]
[826-0-0-2.26][878-0-0-3.83][1103-0-4-1.80][1212-3-4-2.01][1368-0-0-3.98][2181-2-3-1.11][2476-2-4--0.37][2721-2-2-1.93][2818-1-1-0.68][2886-2-1-2.11]
[3231-2-2-4.59][3333-2-2-0.70][3482-2-4-2.07][3536-3-3-1.62][3625-1-1-2.21][3909-0-0-2.58][4035-0-0-2.04][4140-0-0-2.28][4214-1-4-1.68][4346-1-3-0.54]
[4581-2-2-0.82][4708-3-4-2.03][4838-3-4-1.24][4845-1-4-0.83][4868-0-0-4.19][4939-0-4-1.16][4984-2-2-2.12][5078-1-4-1.17][5396-0-0-5.15][5479-1-4-0.47]
[5717-0-0-4.11][5843-1-1-0.56][5949-3-0-2.23][5987-2-4-2.78][6014-3-3-1.24][6033-3-0-2.73][6313-0-0-1.18][6421-3-3-1.31][6500-1-4-0.16][6583-3-3-0.87]
[6683-3-3-1.64][6825-2-1-1.26][6998-3-3-0.17][7049-3-3-0.80][7517-1-2-1.23][7521-1-4--0.03][7528-1-2-2.73][7949-1-4-1.77][8135-1-0-0.60][8185-3-0-5.63]
[8269-3-4-1.37][8273-3-3-3.18][8543-3-0-3.71][8666-1-1-2.91][8672-0-0-4.30][8903-1-1-1.27][9001-2-1-1.65][9036-2-2-3.01][9281-3-4-0.80][9300-2-2-4.62]
[9571-0-4-0.69][9617-1-1-1.12][9644-2-2-1.12][9705-2-4-0.78][9801-0-0-1.84][9803-3-3-0.15][9865-3-3-3.59][9896-2-4-1.55][10314-1-4-1.95][10337-3-3-2.94]
[10403-0-4-1.01][10653-2-4-1.53][10704-2-4-1.03][10719-1-1-2.63][10727-1-4-1.34][10836-0-0-7.89][10969-2-3-1.86][11042-0-0-2.27][11088-1-1-7.01][11322-0-0-5.07]
[11398-2-2-1.54][11499-0-0-3.27][11502-3-3-0.97][11512-3-3-3.11][11608-1-2-2.51][11610-0-0-1.75][11692-0-0-2.43][11905-0-0-3.43][11993-1-1-3.58][12002-2-3-0.32]
[12052-0-0-2.98][12201-0-0-2.70][12235-2-4-1.92][12320-1-4-3.81][12377-2-4-2.42][12398-2-4-1.21][12503-1-1-4.07][12617-0-4--0.09][12685-3-4-0.98][12738-2-0-1.43]
[12742-2-2-4.43][12823-0-3-1.63][13110-1-1-3.67][13240-3-0-2.36][13253-1-1-2.05][13273-0-0-6.20][13634-1-4-1.86][13763-2-3-1.31][13905-3-0-0.23][14060-2-4-1.47]
[14065-3-0-1.71][14147-3-3-1.30][14595-2-4-1.22][14687-2-2-3.95][14788-2-2-0.55][14869-1-4-1.43][14872-3-4-1.54][14877-1-1-7.05][14927-0-3-2.24][15066-0-0-2.58]
[15175-1-4-2.29][15178-2-3-1.16][15375-3-3-0.69][15389-3-3-2.11][15568-2-1-0.57][15675-3-3-3.82][15869-1-4-0.39][16207-3-0-2.35][16236-0-4-0.47][16302-3-3-0.39]
[16331-2-2-4.67][16381-0-4-0.71][16488-1-1-5.60][16495-0-0-2.38][16650-0-0-5.09][16719-1-4-0.47][16801-0-0-5.44][16828-0-0-2.19][17137-3-3-1.68][17245-1-1--0.03]
[17278-3-4-1.01][17282-0-0-1.54][17311-2-2-1.60][17336-2-1-0.62][17608-3-3-3.19][17627-0-0-1.97][17877-3-4-0.98][17924-1-4-0.78][17984-3-3-2.65][18211-0-3-1.84]
[18276-3-3-1.64][18287-1-1-0.28][18394-0-0-4.47][18428-0-0-3.89][18442-0-3-2.56][18478-3-0-2.46][18607-0-0-3.91][18616-0-4-2.19][18663-0-3-0.83][18718-0-0-4.12]
[18766-2-4-0.79][18824-2-4-2.33][18890-3-1-0.96][18930-3-4-2.35][18938-3-3-1.25][19817-1-2-1.82][19839-0-0-0.76][19930-3-3-1.44][19944-0-4-1.61][20036-2-2-3.71]
[20101-3-3-1.78][20474-1-4-0.28][20547-3-4-0.96][20929-2-2-4.15][21245-1-1-1.02][21257-3-3-1.16][21293-1-2-2.28][21316-1-1-2.47][21384-1-4-2.71][21448-1-1-1.94]
[21483-0-0-3.66][21487-2-2-3.12][21714-0-0-1.84][21943-3-4-2.03][21947-0-0-5.34][21948-0-0-5.99][21965-2-2-1.07][21998-1-1-3.89][22025-0-4-3.14][22228-3-3-2.92]
[22446-1-1-2.25][22494-3-3-2.00][22757-0-0-3.57][22811-3-3-3.31][22976-3-2-0.76][22985-3-3-2.20][23014-0-0-4.81][23112-1-1-3.81][23144-3-3-2.99][23168-2-4-1.28]
[23219-0-4-1.37][23363-3-3-0.97][23470-0-0-1.14][23486-2-4-1.49][23497-0-3-3.04][23516-0-0-3.63][23690-1-4-1.74][23921-2-4-0.65][23936-1-2-2.38][24040-3-0-1.58]
[24111-1-4-2.01][24182-0-0-4.03][24238-3-0-2.09][24290-2-4-0.99][24345-0-0-2.26][24364-1-1-0.64][24427-3-0-2.37][24477-2-4-2.19][24495-2-4-1.52][24893-2-4-1.02]
[25012-1-1--0.01][25121-2-1-1.68][25165-3-3-0.53][25183-0-0-2.79][25297-3-3-3.56][25398-0-0-3.05][25574-2-4-2.18][25644-1-1-3.74][25718-1-4-0.26][25774-2-4-1.26]
[26032-3-0-2.37][26051-3-3-3.00][26120-0-4-3.69][26321-1-1-1.65][26732-1-1-3.21][26784-3-3-4.02][26827-3-4-1.10][26833-0-3-2.19][26838-2-4-0.54][26860-1-4-0.90]
[26948-0-0-2.59][27049-3-0-3.01][27098-1-4-0.66][27526-0-0-1.84][27639-3-4-0.84][27698-3-0-3.08][27772-0-0-4.79][27890-1-1-1.40][28040-0-4-0.53][28503-2-2-2.94]
[28577-1-1-2.58][28959-0-0-4.45][29198-3-4-1.15][29777-0-0-5.95][29877-2-2-1.93][30035-1-1-2.71][30098-0-0-3.33][30326-1-1-5.93][30572-2-2-2.83][30716-0-4-2.24]
[30806-2-3-0.83][30906-1-1-2.84][31007-0-4-1.75][31181-3-4-0.73][31238-0-3-1.33][31347-0-0-5.34][31422-2-4-1.25][31429-3-3-0.57][31431-0-0-0.14][31432-1-1-2.69]
[31477-0-0-2.83][31524-1-4-0.45][31597-1-4-1.15][31619-1-4-0.29][31701-0-0-4.73][31755-0-0-5.34][31854-3-3-1.72][32074-1-1-1.08][32078-3-3-3.30][32111-1-1-1.35]
[32127-1-1-1.37][32140-3-3-2.70][32263-2-4-1.18][32365-0-0-3.77][32411-2-0-2.72][32429-3-3-2.44][32473-3-4-1.24][32574-3-3-2.84][32584-0-4-2.95][32622-0-4-1.79]
[32858-3-3-1.31][32969-3-0-2.76][33016-2-2-1.55][33031-1-0-1.80][33035-2-4-2.46][33133-2-2-1.25][33173-2-2-0.64][33175-3-4-1.10][33306-3-1-1.06][33309-2-4-0.29]
[33474-0-4-0.75][33478-2-4-0.59][33618-1-4-2.05][33712-0-4-1.17][33782-2-1-3.74][33914-3-3-0.82][34076-3-4-2.61][34112-2-2-2.15][34138-2-2-0.65][34239-1-4-0.77]
[34364-2-2-3.48][34617-1-4-3.40][34751-3-3-1.83][34783-2-4-1.93][35015-3-4-1.68][35018-1-4-1.55][35288-2-2-0.57][0-4-4-0.96][1-4-4-5.08][2-4-4-1.86]
[3-4-4-3.70][4-4-4-1.44][5-4-1-1.25][6-4-4-4.32][7-4-4-3.71][8-4-4-0.59][9-4-2-1.55][10-4-4-4.86][11-4-4-4.58][12-4-4-1.44]
[14-4-4-1.77][15-4-0-2.46][16-4-4-1.87][17-4-4-0.85][18-4-4-3.88][19-4-0-2.42][20-4-0-2.31][21-4-4-2.18][22-4-4-2.45][23-4-4-2.29]
[24-4-4-5.29][25-4-4-1.14][26-4-4-0.98][27-4-4-1.07][28-4-4-3.52][29-4-4-0.65][30-4-0-0.33][31-4-4-3.09][32-4-4-1.59][33-4-4-2.09]
[34-4-4-2.26][35-4-0-2.42][37-4-4-2.57][39-4-0-5.12][40-4-0-1.36][41-4-4-0.85][42-4-4-1.84][43-4-4-1.87][45-4-4-1.08][46-4-4-3.99]
[47-4-4-5.41][48-4-4-3.03][51-4-4-3.50][52-4-4-1.93][53-4-4-1.50][54-4-4-1.53][55-4-4-2.77][56-4-4-1.48][57-4-0-2.75][58-4-4-2.21]
[59-4-4-2.71][60-4-4-1.53][61-4-4-2.77][62-4-4-1.27][63-4-4-1.64][64-4-4-2.08][65-4-4-3.70][66-4-4-3.18][67-4-4-1.21][68-4-4-1.82]
[69-4-4-1.72][70-4-4-3.02][72-4-1-1.42][73-4-4-1.39][74-4-4-1.38][75-4-4-0.86][77-4-4-2.86][78-4-4-1.07][79-4-4-2.80][80-4-4-2.70]
[81-4-4-1.98][82-4-4-0.95][83-4-4-1.99][84-4-4-4.63][85-4-4-4.70][86-4-4-1.87][87-4-4-2.77][88-4-4-3.67][89-4-4-1.33][90-4-4-0.83]
[91-4-4-2.43][92-4-4-1.22][93-4-0-0.79][94-4-4-3.17][95-4-4-1.79][96-4-4-1.23][97-4-4-3.35][98-4-2-2.17][99-4-4-1.47][100-4-4-1.39]
[101-4-4-6.53][102-4-4-1.57][103-4-4-0.93][104-4-4-2.76][105-4-4-2.06][106-4-4-3.45][107-4-4-2.35][108-4-4-1.44][109-4-4-3.01][110-4-4-2.27]
[111-4-0-2.48][112-4-4-0.60][113-4-4-2.12][114-4-4-0.07][115-4-4-2.30][116-4-4-2.20][117-4-4-3.28][119-4-2-2.19][121-4-4-2.33][122-4-4-3.30]
[124-4-4-0.84][125-4-4-3.55][126-4-4-4.36][127-4-4-1.00][128-4-4-1.42][129-4-4-1.51][130-4-4-1.91][131-4-4-1.53][132-4-4-1.70][133-4-4-5.34]
[135-4-4-2.34][136-4-4-0.83][137-4-4-1.18][138-4-4-2.77][139-4-4-3.04][140-4-1-1.58][141-4-0-1.85][142-4-4-4.66][143-4-4-4.07][144-4-4-3.12]
[145-4-4-2.48][148-4-0-4.78][149-4-4-2.84][150-4-4-4.18][151-4-4-3.25][152-4-4-3.18][153-4-4-3.33][154-4-4-3.57][155-4-4-3.70][156-4-0-0.68]
[157-4-4-1.36][158-4-4-2.57][160-4-4-0.69][161-4-2-1.73][162-4-4-1.73][164-4-4-2.86][165-4-4-0.92][167-4-0-2.81][168-4-4-2.06][170-4-4-1.89]
[171-4-4-2.52][172-4-4-3.92][173-4-4-4.63][174-4-0-3.82][175-4-4-1.82][177-4-4-4.22][178-4-4-1.70][179-4-4-1.96][180-4-4-2.19][181-4-4-2.03]
[182-4-4-1.93][183-4-4-3.27][184-4-4-3.69][186-4-4-0.58][187-4-1-0.76][188-4-4-2.54][189-4-4-1.03][190-4-4-1.63][191-4-4-3.93][192-4-4-2.21]
[193-4-2-2.25][194-4-4-1.01][195-4-0-1.46][196-4-4-1.85][197-4-4-2.95][198-4-4-5.92][199-4-2-2.17]
---------------------------
I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 166
I - Training: 
	I - Batch: 50 | Loss: 0.427 | Acc: 83.875% | Wgt Acc: 88.241%
	I - Batch: 100 | Loss: 0.432 | Acc: 84.062% | Wgt Acc: 87.954%
	I - Batch: 150 | Loss: 0.417 | Acc: 84.500% | Wgt Acc: 88.563%
	I - Batch: 200 | Loss: 0.410 | Acc: 84.969% | Wgt Acc: 88.864%
I - num batch: 222
I - Train -- Loss: 0.409 | Acc: 85.001% | Wgt Acc: 88.823% | LR: 1.250000e-05 | Dur: 134.71s
I - Confusion Matrix: [row->prediction - col->label]
[[640.   1.   1.  25. 125.]
 [  6. 552.  10.   4.  57.]
 [  4.   9. 689.  17. 102.]
 [ 15.   3.  10. 482.  64.]
 [ 32.  13.  24.  10. 652.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.158 | Acc: 62.525% | Wgt Acc: 55.689% | Dur: 16.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   0.   1.  10.   6.]
 [  0.  30.   5.   3.   4.]
 [  2.  16.  40.   2.  10.]
 [  9.   1.   3.  38.   1.]
 [ 27.  31.  26.  33. 159.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 167
I - Training: 
	I - Batch: 50 | Loss: 0.418 | Acc: 83.625% | Wgt Acc: 87.468%
	I - Batch: 100 | Loss: 0.407 | Acc: 84.750% | Wgt Acc: 88.421%
	I - Batch: 150 | Loss: 0.397 | Acc: 85.292% | Wgt Acc: 88.980%
	I - Batch: 200 | Loss: 0.401 | Acc: 85.219% | Wgt Acc: 88.832%
I - num batch: 222
I - Train -- Loss: 0.401 | Acc: 85.452% | Wgt Acc: 89.031% | LR: 1.250000e-05 | Dur: 134.31s
I - Confusion Matrix: [row->prediction - col->label]
[[640.   1.   3.  18. 109.]
 [  2. 554.   8.   4.  52.]
 [  3.   8. 680.  18. 119.]
 [ 12.   5.  12. 487.  50.]
 [ 40.  10.  31.  11. 670.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.088 | Acc: 62.919% | Wgt Acc: 56.912% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   3.   2.  18.  11.]
 [  0.  32.   5.   5.   6.]
 [  2.  16.  37.   0.   8.]
 [ 10.   2.   4.  40.   1.]
 [ 20.  25.  27.  23. 154.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 168
I - Training: 
	I - Batch: 50 | Loss: 0.383 | Acc: 87.500% | Wgt Acc: 90.859%
	I - Batch: 100 | Loss: 0.390 | Acc: 86.000% | Wgt Acc: 89.582%
	I - Batch: 150 | Loss: 0.386 | Acc: 86.250% | Wgt Acc: 89.708%
	I - Batch: 200 | Loss: 0.396 | Acc: 85.562% | Wgt Acc: 89.146%
I - num batch: 222
I - Train -- Loss: 0.399 | Acc: 85.481% | Wgt Acc: 89.060% | LR: 1.250000e-05 | Dur: 141.26s
I - Confusion Matrix: [row->prediction - col->label]
[[635.   1.   4.  25. 107.]
 [  6. 555.  11.   5.  46.]
 [  1.  12. 687.  14. 110.]
 [ 22.   2.   8. 485.  67.]
 [ 33.   8.  24.   9. 670.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.241 | Acc: 59.961% | Wgt Acc: 52.105% | Dur: 18.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 44.   1.   2.  10.   7.]
 [  1.  30.   4.   4.   5.]
 [  2.  19.  40.   4.   7.]
 [ 11.   1.   2.  29.   0.]
 [ 30.  27.  27.  39. 161.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 169
I - Training: 
	I - Batch: 50 | Loss: 0.407 | Acc: 85.500% | Wgt Acc: 88.820%
	I - Batch: 100 | Loss: 0.390 | Acc: 86.250% | Wgt Acc: 89.486%
	I - Batch: 150 | Loss: 0.403 | Acc: 85.250% | Wgt Acc: 88.821%
	I - Batch: 200 | Loss: 0.402 | Acc: 85.219% | Wgt Acc: 88.821%
I - num batch: 222
I - Train -- Loss: 0.403 | Acc: 85.086% | Wgt Acc: 88.721% | LR: 1.250000e-05 | Dur: 135.47s
I - Confusion Matrix: [row->prediction - col->label]
[[633.   1.   2.  23. 118.]
 [  2. 550.   5.   4.  55.]
 [  4.  10. 688.  15. 104.]
 [ 19.   3.  14. 484.  60.]
 [ 39.  14.  25.  12. 663.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.094 | Acc: 61.538% | Wgt Acc: 55.114% | Dur: 19.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   2.   1.  10.   8.]
 [  2.  30.   4.   2.   5.]
 [  2.  22.  35.   3.  12.]
 [ 13.   0.   3.  42.   0.]
 [ 21.  24.  32.  29. 155.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 170
I - Training: 
	I - Batch: 50 | Loss: 0.406 | Acc: 85.875% | Wgt Acc: 89.408%
	I - Batch: 100 | Loss: 0.403 | Acc: 85.625% | Wgt Acc: 89.203%
	I - Batch: 150 | Loss: 0.395 | Acc: 86.167% | Wgt Acc: 89.589%
	I - Batch: 200 | Loss: 0.401 | Acc: 85.438% | Wgt Acc: 89.081%
I - num batch: 222
I - Train -- Loss: 0.397 | Acc: 85.847% | Wgt Acc: 89.418% | LR: 1.250000e-05 | Dur: 133.45s
I - Confusion Matrix: [row->prediction - col->label]
[[640.   1.   1.  27. 118.]
 [  4. 553.   9.   2.  56.]
 [  4.   6. 697.  15.  97.]
 [ 16.   5.  10. 482.  56.]
 [ 33.  13.  17.  12. 673.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.209 | Acc: 60.750% | Wgt Acc: 52.461% | Dur: 14.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   1.   1.  18.   8.]
 [  0.  27.   4.   2.   1.]
 [  0.  11.  30.   1.   5.]
 [  6.   1.   2.  34.   0.]
 [ 31.  38.  38.  31. 166.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 171
I - Training: 
	I - Batch: 50 | Loss: 0.382 | Acc: 86.125% | Wgt Acc: 89.709%
	I - Batch: 100 | Loss: 0.393 | Acc: 85.750% | Wgt Acc: 89.233%
	I - Batch: 150 | Loss: 0.390 | Acc: 86.000% | Wgt Acc: 89.500%
	I - Batch: 200 | Loss: 0.400 | Acc: 85.531% | Wgt Acc: 89.060%
I - num batch: 222
I - Train -- Loss: 0.395 | Acc: 85.819% | Wgt Acc: 89.379% | LR: 1.250000e-05 | Dur: 133.63s
I - Confusion Matrix: [row->prediction - col->label]
[[638.   0.   0.  18. 127.]
 [  3. 553.   7.   5.  50.]
 [  6.   6. 693.   8.  93.]
 [ 17.   7.  13. 486.  56.]
 [ 33.  12.  21.  21. 674.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.215 | Acc: 61.736% | Wgt Acc: 53.965% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   0.   1.   8.   4.]
 [  0.  28.   5.   2.   5.]
 [  2.  21.  43.   5.   7.]
 [ 11.   1.   3.  35.   0.]
 [ 32.  28.  23.  36. 164.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 172
I - Training: 
	I - Batch: 50 | Loss: 0.409 | Acc: 85.375% | Wgt Acc: 88.720%
	I - Batch: 100 | Loss: 0.413 | Acc: 85.062% | Wgt Acc: 88.671%
	I - Batch: 150 | Loss: 0.403 | Acc: 85.792% | Wgt Acc: 89.406%
	I - Batch: 200 | Loss: 0.401 | Acc: 85.875% | Wgt Acc: 89.339%
I - num batch: 222
I - Train -- Loss: 0.399 | Acc: 85.904% | Wgt Acc: 89.323% | LR: 1.250000e-05 | Dur: 135.15s
I - Confusion Matrix: [row->prediction - col->label]
[[635.   0.   4.  22. 113.]
 [  1. 553.   5.   7.  51.]
 [  1.   9. 693.  10.  93.]
 [ 15.   4.  11. 484.  61.]
 [ 45.  12.  21.  15. 682.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.158 | Acc: 61.933% | Wgt Acc: 54.414% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   1.   2.  11.   7.]
 [  0.  25.   3.   1.   3.]
 [  1.  14.  35.   2.   7.]
 [ 10.   0.   3.  40.   0.]
 [ 26.  38.  32.  32. 163.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 173
I - Training: 
	I - Batch: 50 | Loss: 0.383 | Acc: 85.625% | Wgt Acc: 89.354%
	I - Batch: 100 | Loss: 0.387 | Acc: 85.375% | Wgt Acc: 89.493%
	I - Batch: 150 | Loss: 0.376 | Acc: 86.458% | Wgt Acc: 90.168%
	I - Batch: 200 | Loss: 0.383 | Acc: 85.812% | Wgt Acc: 89.588%
I - num batch: 222
I - Train -- Loss: 0.385 | Acc: 85.622% | Wgt Acc: 89.482% | LR: 1.250000e-05 | Dur: 139.72s
I - Confusion Matrix: [row->prediction - col->label]
[[639.   1.   0.  21. 127.]
 [  3. 558.   6.   5.  47.]
 [  2.   7. 693.  11. 114.]
 [ 18.   2.   7. 490.  55.]
 [ 35.  10.  28.  11. 657.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.170 | Acc: 61.736% | Wgt Acc: 54.529% | Dur: 19.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   2.   2.  13.   7.]
 [  0.  28.   5.   4.   5.]
 [  2.  17.  39.   4.   8.]
 [ 10.   0.   3.  36.   0.]
 [ 26.  31.  26.  29. 160.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 174
I - Training: 
	I - Batch: 50 | Loss: 0.385 | Acc: 86.500% | Wgt Acc: 89.858%
	I - Batch: 100 | Loss: 0.397 | Acc: 85.312% | Wgt Acc: 88.735%
	I - Batch: 150 | Loss: 0.401 | Acc: 85.208% | Wgt Acc: 88.649%
	I - Batch: 200 | Loss: 0.397 | Acc: 84.906% | Wgt Acc: 88.468%
I - num batch: 222
I - Train -- Loss: 0.397 | Acc: 85.058% | Wgt Acc: 88.605% | LR: 1.250000e-05 | Dur: 136.97s
I - Confusion Matrix: [row->prediction - col->label]
[[628.   1.   2.  25. 110.]
 [  1. 550.   9.   4.  61.]
 [  5.  13. 691.  18. 104.]
 [ 25.   2.   9. 481.  58.]
 [ 38.  12.  23.  10. 667.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.269 | Acc: 58.185% | Wgt Acc: 51.196% | Dur: 17.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   0.   2.  11.   6.]
 [  0.  26.   2.   4.   4.]
 [  2.  25.  47.   8.  19.]
 [  8.   1.   1.  26.   0.]
 [ 33.  26.  23.  37. 151.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 175
I - Training: 
	I - Batch: 50 | Loss: 0.394 | Acc: 84.000% | Wgt Acc: 88.293%
	I - Batch: 100 | Loss: 0.401 | Acc: 84.188% | Wgt Acc: 88.089%
	I - Batch: 150 | Loss: 0.403 | Acc: 84.583% | Wgt Acc: 88.279%
	I - Batch: 200 | Loss: 0.408 | Acc: 84.688% | Wgt Acc: 88.178%
I - num batch: 222
I - Train -- Loss: 0.406 | Acc: 84.860% | Wgt Acc: 88.302% | LR: 1.250000e-05 | Dur: 135.95s
I - Confusion Matrix: [row->prediction - col->label]
[[629.   1.   4.  24.  99.]
 [  3. 551.   7.   8.  55.]
 [  3.  10. 686.  18. 106.]
 [ 17.   4.  13. 474.  70.]
 [ 45.  12.  24.  14. 670.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.157 | Acc: 61.144% | Wgt Acc: 53.996% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   3.   2.  18.   8.]
 [  0.  27.   4.   2.   6.]
 [  1.  16.  33.   1.   6.]
 [ 11.   0.   4.  39.   1.]
 [ 24.  32.  32.  26. 159.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 176
I - Training: 
	I - Batch: 50 | Loss: 0.379 | Acc: 85.250% | Wgt Acc: 89.582%
	I - Batch: 100 | Loss: 0.388 | Acc: 85.562% | Wgt Acc: 89.491%
	I - Batch: 150 | Loss: 0.392 | Acc: 85.542% | Wgt Acc: 89.246%
	I - Batch: 200 | Loss: 0.396 | Acc: 85.625% | Wgt Acc: 89.151%
I - num batch: 222
I - Train -- Loss: 0.393 | Acc: 85.706% | Wgt Acc: 89.314% | LR: 1.250000e-05 | Dur: 136.02s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   2.   1.  27. 104.]
 [  4. 557.   6.   7.  57.]
 [  2.   8. 695.  19. 111.]
 [ 16.   1.   9. 475.  59.]
 [ 31.  10.  23.  10. 669.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.275 | Acc: 58.777% | Wgt Acc: 50.266% | Dur: 15.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 42.   3.   2.   8.   7.]
 [  0.  25.   2.   4.   3.]
 [  1.  13.  35.   2.   6.]
 [  9.   0.   2.  32.   0.]
 [ 36.  37.  34.  40. 164.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 177
I - Training: 
	I - Batch: 50 | Loss: 0.366 | Acc: 87.875% | Wgt Acc: 91.423%
	I - Batch: 100 | Loss: 0.379 | Acc: 86.250% | Wgt Acc: 89.765%
	I - Batch: 150 | Loss: 0.383 | Acc: 85.667% | Wgt Acc: 89.485%
	I - Batch: 200 | Loss: 0.386 | Acc: 85.594% | Wgt Acc: 89.407%
I - num batch: 222
I - Train -- Loss: 0.384 | Acc: 85.678% | Wgt Acc: 89.465% | LR: 1.250000e-05 | Dur: 137.58s
I - Confusion Matrix: [row->prediction - col->label]
[[638.   1.   1.  24. 109.]
 [  2. 562.  11.   4.  46.]
 [  1.   6. 693.  11. 119.]
 [ 22.   2.   7. 485.  65.]
 [ 34.   7.  22.  14. 661.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.178 | Acc: 60.947% | Wgt Acc: 53.547% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   0.   2.  12.   6.]
 [  0.  26.   4.   3.   2.]
 [  2.  20.  41.   2.  11.]
 [ 12.   1.   3.  36.   1.]
 [ 28.  31.  25.  33. 160.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 178
I - Training: 
	I - Batch: 50 | Loss: 0.373 | Acc: 85.875% | Wgt Acc: 90.092%
	I - Batch: 100 | Loss: 0.373 | Acc: 86.250% | Wgt Acc: 89.953%
	I - Batch: 150 | Loss: 0.370 | Acc: 86.667% | Wgt Acc: 90.274%
	I - Batch: 200 | Loss: 0.370 | Acc: 86.750% | Wgt Acc: 90.318%
I - num batch: 222
I - Train -- Loss: 0.373 | Acc: 86.693% | Wgt Acc: 90.298% | LR: 1.250000e-05 | Dur: 134.08s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   2.   2.  17. 110.]
 [  3. 561.   8.   4.  48.]
 [  2.   6. 695.  13. 105.]
 [ 12.   3.   8. 494.  56.]
 [ 36.   6.  21.  10. 681.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.210 | Acc: 59.961% | Wgt Acc: 52.147% | Dur: 15.62s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   2.   9.   8.]
 [  0.  22.   4.   2.   4.]
 [  2.  17.  36.   5.   7.]
 [ 11.   0.   1.  35.   0.]
 [ 25.  38.  32.  35. 161.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 179
I - Training: 
	I - Batch: 50 | Loss: 0.361 | Acc: 87.750% | Wgt Acc: 90.952%
	I - Batch: 100 | Loss: 0.373 | Acc: 87.188% | Wgt Acc: 90.236%
	I - Batch: 150 | Loss: 0.379 | Acc: 86.458% | Wgt Acc: 89.954%
	I - Batch: 200 | Loss: 0.376 | Acc: 86.781% | Wgt Acc: 90.304%
I - num batch: 222
I - Train -- Loss: 0.379 | Acc: 86.524% | Wgt Acc: 90.023% | LR: 1.250000e-05 | Dur: 133.88s
I - Confusion Matrix: [row->prediction - col->label]
[[637.   0.   1.  14. 113.]
 [  2. 556.   5.   4.  47.]
 [  4.   6. 696.  11.  96.]
 [ 16.   0.  10. 495.  59.]
 [ 38.  16.  22.  14. 685.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 62.525% | Wgt Acc: 55.940% | Dur: 18.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   0.   2.  12.  10.]
 [  0.  28.   4.   3.   4.]
 [  1.  16.  39.   2.   9.]
 [  8.   3.   4.  38.   0.]
 [ 24.  31.  26.  31. 157.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 180
I - Training: 
	I - Batch: 50 | Loss: 0.385 | Acc: 85.875% | Wgt Acc: 89.922%
	I - Batch: 100 | Loss: 0.390 | Acc: 85.750% | Wgt Acc: 89.634%
	I - Batch: 150 | Loss: 0.387 | Acc: 86.000% | Wgt Acc: 89.560%
	I - Batch: 200 | Loss: 0.380 | Acc: 85.906% | Wgt Acc: 89.525%
I - num batch: 222
I - Train -- Loss: 0.380 | Acc: 86.073% | Wgt Acc: 89.754% | LR: 1.250000e-05 | Dur: 133.75s
I - Confusion Matrix: [row->prediction - col->label]
[[630.   1.   3.  20. 103.]
 [  2. 565.   8.   2.  56.]
 [  3.   2. 698.  16. 111.]
 [ 23.   2.   8. 489.  59.]
 [ 39.   8.  17.  11. 671.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 61.933% | Wgt Acc: 54.268% | Dur: 14.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   0.   2.   7.   5.]
 [  0.  28.   4.   3.   4.]
 [  1.  17.  38.   3.   7.]
 [ 11.   1.   1.  38.   0.]
 [ 30.  32.  30.  35. 164.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 181
I - Training: 
	I - Batch: 50 | Loss: 0.365 | Acc: 87.500% | Wgt Acc: 90.824%
	I - Batch: 100 | Loss: 0.364 | Acc: 87.625% | Wgt Acc: 90.948%
	I - Batch: 150 | Loss: 0.374 | Acc: 86.917% | Wgt Acc: 90.231%
	I - Batch: 200 | Loss: 0.376 | Acc: 86.781% | Wgt Acc: 90.207%
I - num batch: 222
I - Train -- Loss: 0.375 | Acc: 86.721% | Wgt Acc: 90.204% | LR: 1.250000e-05 | Dur: 134.92s
I - Confusion Matrix: [row->prediction - col->label]
[[646.   4.   1.  23. 103.]
 [  3. 560.   4.   4.  54.]
 [  4.   2. 694.  13.  93.]
 [ 12.   2.  11. 489.  63.]
 [ 32.  10.  24.   9. 687.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.144 | Acc: 62.525% | Wgt Acc: 55.605% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  17.   8.]
 [  0.  30.   4.   2.   6.]
 [  1.  16.  40.   1.   7.]
 [ 10.   1.   4.  34.   0.]
 [ 23.  29.  25.  32. 159.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 182
I - Training: 
	I - Batch: 50 | Loss: 0.406 | Acc: 84.750% | Wgt Acc: 88.866%
	I - Batch: 100 | Loss: 0.403 | Acc: 85.125% | Wgt Acc: 89.298%
	I - Batch: 150 | Loss: 0.397 | Acc: 85.583% | Wgt Acc: 89.447%
	I - Batch: 200 | Loss: 0.385 | Acc: 86.188% | Wgt Acc: 89.720%
I - num batch: 222
I - Train -- Loss: 0.380 | Acc: 86.383% | Wgt Acc: 89.756% | LR: 1.250000e-05 | Dur: 137.97s
I - Confusion Matrix: [row->prediction - col->label]
[[634.   1.   1.  25.  89.]
 [  2. 555.   7.   3.  56.]
 [  2.   9. 701.  13. 102.]
 [ 20.   3.   5. 485.  64.]
 [ 39.  10.  20.  12. 689.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.285 | Acc: 59.172% | Wgt Acc: 50.977% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 41.   0.   2.   6.   4.]
 [  0.  21.   3.   2.   3.]
 [  2.  19.  44.   5.  11.]
 [ 10.   1.   1.  32.   0.]
 [ 35.  37.  25.  41. 162.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 183
I - Training: 
	I - Batch: 50 | Loss: 0.403 | Acc: 85.750% | Wgt Acc: 88.977%
	I - Batch: 100 | Loss: 0.398 | Acc: 86.312% | Wgt Acc: 89.671%
	I - Batch: 150 | Loss: 0.396 | Acc: 86.458% | Wgt Acc: 90.043%
	I - Batch: 200 | Loss: 0.386 | Acc: 86.781% | Wgt Acc: 90.260%
I - num batch: 222
I - Train -- Loss: 0.385 | Acc: 86.721% | Wgt Acc: 90.157% | LR: 1.250000e-05 | Dur: 134.68s
I - Confusion Matrix: [row->prediction - col->label]
[[640.   0.   3.  19. 117.]
 [  4. 554.   6.   4.  50.]
 [  2.   4. 697.   9.  80.]
 [ 17.   5.   9. 495.  63.]
 [ 34.  15.  19.  11. 690.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 61.341% | Wgt Acc: 54.174% | Dur: 21.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   0.   1.  15.   7.]
 [  0.  25.   5.   2.   4.]
 [  1.  20.  37.   1.   8.]
 [  9.   2.   6.  37.   2.]
 [ 25.  31.  26.  31. 159.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 184
I - Training: 
	I - Batch: 50 | Loss: 0.360 | Acc: 87.500% | Wgt Acc: 90.923%
	I - Batch: 100 | Loss: 0.361 | Acc: 88.000% | Wgt Acc: 91.089%
	I - Batch: 150 | Loss: 0.361 | Acc: 87.833% | Wgt Acc: 90.844%
	I - Batch: 200 | Loss: 0.365 | Acc: 87.312% | Wgt Acc: 90.577%
I - num batch: 222
I - Train -- Loss: 0.364 | Acc: 87.257% | Wgt Acc: 90.583% | LR: 1.250000e-05 | Dur: 134.99s
I - Confusion Matrix: [row->prediction - col->label]
[[639.   2.   0.  17. 111.]
 [  2. 553.   3.   2.  45.]
 [  2.  11. 704.  10.  80.]
 [ 19.   2.  14. 498.  63.]
 [ 35.  10.  13.  11. 701.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.155 | Acc: 62.919% | Wgt Acc: 56.514% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   1.   2.  12.  10.]
 [  0.  29.   5.   2.   3.]
 [  0.  16.  37.   3.   9.]
 [ 11.   0.   4.  42.   1.]
 [ 23.  32.  27.  27. 157.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 185
I - Training: 
	I - Batch: 50 | Loss: 0.368 | Acc: 86.750% | Wgt Acc: 89.926%
	I - Batch: 100 | Loss: 0.379 | Acc: 85.500% | Wgt Acc: 88.712%
	I - Batch: 150 | Loss: 0.375 | Acc: 85.917% | Wgt Acc: 89.319%
	I - Batch: 200 | Loss: 0.374 | Acc: 85.812% | Wgt Acc: 89.192%
I - num batch: 222
I - Train -- Loss: 0.373 | Acc: 85.988% | Wgt Acc: 89.327% | LR: 1.250000e-05 | Dur: 135.62s
I - Confusion Matrix: [row->prediction - col->label]
[[634.   0.   2.  24. 115.]
 [  3. 558.   8.  10.  54.]
 [  5.   7. 690.   8.  85.]
 [ 15.   2.  10. 481.  59.]
 [ 40.  11.  24.  15. 687.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.225 | Acc: 60.947% | Wgt Acc: 54.070% | Dur: 20.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   2.   1.  13.   7.]
 [  0.  28.   4.   2.   5.]
 [  1.  17.  40.   2.  12.]
 [  9.   1.   3.  34.   0.]
 [ 27.  30.  27.  35. 156.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 186
I - Training: 
	I - Batch: 50 | Loss: 0.359 | Acc: 86.625% | Wgt Acc: 90.167%
	I - Batch: 100 | Loss: 0.367 | Acc: 86.375% | Wgt Acc: 90.335%
	I - Batch: 150 | Loss: 0.359 | Acc: 86.833% | Wgt Acc: 90.714%
	I - Batch: 200 | Loss: 0.365 | Acc: 86.688% | Wgt Acc: 90.377%
I - num batch: 222
I - Train -- Loss: 0.372 | Acc: 86.326% | Wgt Acc: 90.109% | LR: 1.250000e-05 | Dur: 138.04s
I - Confusion Matrix: [row->prediction - col->label]
[[649.   1.   1.  20. 119.]
 [  2. 557.   8.   4.  51.]
 [  2.   8. 701.  13. 101.]
 [ 14.   1.   7. 488.  62.]
 [ 30.  11.  17.  13. 667.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.184 | Acc: 61.736% | Wgt Acc: 54.937% | Dur: 14.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   3.   9.   6.]
 [  0.  26.   4.   3.   6.]
 [  1.  17.  37.   5.   9.]
 [  7.   0.   3.  37.   2.]
 [ 24.  33.  28.  32. 157.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 187
I - Training: 
	I - Batch: 50 | Loss: 0.386 | Acc: 87.125% | Wgt Acc: 90.805%
	I - Batch: 100 | Loss: 0.385 | Acc: 86.312% | Wgt Acc: 89.966%
	I - Batch: 150 | Loss: 0.383 | Acc: 86.000% | Wgt Acc: 89.712%
	I - Batch: 200 | Loss: 0.381 | Acc: 86.062% | Wgt Acc: 89.804%
I - num batch: 222
I - Train -- Loss: 0.382 | Acc: 86.073% | Wgt Acc: 89.757% | LR: 1.250000e-05 | Dur: 134.28s
I - Confusion Matrix: [row->prediction - col->label]
[[635.   0.   2.  17. 113.]
 [  2. 559.   6.   6.  56.]
 [  4.   6. 695.  13.  99.]
 [ 15.   3.   6. 493.  61.]
 [ 41.  10.  25.   9. 671.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.247 | Acc: 59.172% | Wgt Acc: 51.008% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 43.   1.   1.  10.   6.]
 [  0.  26.   4.   3.   4.]
 [  2.  23.  38.   4.   8.]
 [ 10.   1.   2.  31.   0.]
 [ 33.  27.  30.  38. 162.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 188
I - Training: 
	I - Batch: 50 | Loss: 0.349 | Acc: 89.500% | Wgt Acc: 91.942%
	I - Batch: 100 | Loss: 0.356 | Acc: 88.062% | Wgt Acc: 91.075%
	I - Batch: 150 | Loss: 0.344 | Acc: 88.500% | Wgt Acc: 91.278%
	I - Batch: 200 | Loss: 0.347 | Acc: 88.031% | Wgt Acc: 91.051%
I - num batch: 208
I - Train -- Loss: 0.347 | Acc: 87.921% | Wgt Acc: 91.010% | LR: 1.250000e-05 | Dur: 125.08s
I - Confusion Matrix: [row->prediction - col->label]
[[647.   1.   2.  13.  87.]
 [  3. 552.   6.   3.  44.]
 [  3.  12. 698.  15.  78.]
 [ 14.   4.  10. 497.  40.]
 [ 30.   9.  18.  10. 532.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.117 | Acc: 62.130% | Wgt Acc: 55.961% | Dur: 14.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   1.   3.  14.  10.]
 [  1.  30.   7.   4.   7.]
 [  1.  18.  36.   2.  10.]
 [  7.   1.   4.  35.   0.]
 [ 18.  28.  25.  31. 153.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 189
I - Training: 
	I - Batch: 50 | Loss: 0.362 | Acc: 86.750% | Wgt Acc: 90.764%
	I - Batch: 100 | Loss: 0.366 | Acc: 86.062% | Wgt Acc: 89.881%
	I - Batch: 150 | Loss: 0.378 | Acc: 86.000% | Wgt Acc: 89.705%
	I - Batch: 200 | Loss: 0.378 | Acc: 85.750% | Wgt Acc: 89.440%
I - num batch: 222
I - Train -- Loss: 0.378 | Acc: 86.016% | Wgt Acc: 89.653% | LR: 1.250000e-05 | Dur: 133.28s
I - Confusion Matrix: [row->prediction - col->label]
[[643.   2.   1.  22. 125.]
 [  4. 561.   9.   6.  45.]
 [  1.   4. 690.  11.  95.]
 [ 16.   0.  10. 485.  63.]
 [ 33.  11.  24.  14. 672.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.157 | Acc: 60.158% | Wgt Acc: 53.714% | Dur: 17.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   3.   2.  16.   9.]
 [  0.  28.   6.   3.   6.]
 [  2.  18.  34.   3.  12.]
 [ 10.   2.   5.  37.   1.]
 [ 22.  27.  28.  27. 152.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 190
I - Training: 
	I - Batch: 50 | Loss: 0.371 | Acc: 86.625% | Wgt Acc: 90.092%
	I - Batch: 100 | Loss: 0.364 | Acc: 86.875% | Wgt Acc: 90.324%
	I - Batch: 150 | Loss: 0.374 | Acc: 86.500% | Wgt Acc: 89.868%
	I - Batch: 200 | Loss: 0.373 | Acc: 86.688% | Wgt Acc: 89.873%
I - num batch: 222
I - Train -- Loss: 0.369 | Acc: 86.890% | Wgt Acc: 90.066% | LR: 1.250000e-05 | Dur: 134.04s
I - Confusion Matrix: [row->prediction - col->label]
[[645.   0.   2.  22.  90.]
 [  2. 560.   9.   4.  48.]
 [  2.   4. 691.  14.  98.]
 [ 16.   4.   9. 482.  60.]
 [ 32.  10.  23.  16. 704.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.206 | Acc: 61.341% | Wgt Acc: 54.289% | Dur: 17.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   1.   2.  18.   9.]
 [  0.  31.   3.   4.   6.]
 [  0.  18.  30.   2.   7.]
 [  4.   1.   3.  33.   0.]
 [ 25.  27.  37.  29. 158.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 191
I - Training: 
	I - Batch: 50 | Loss: 0.340 | Acc: 88.250% | Wgt Acc: 91.376%
	I - Batch: 100 | Loss: 0.363 | Acc: 87.500% | Wgt Acc: 90.825%
	I - Batch: 150 | Loss: 0.369 | Acc: 86.875% | Wgt Acc: 90.432%
	I - Batch: 200 | Loss: 0.372 | Acc: 86.500% | Wgt Acc: 90.141%
I - num batch: 222
I - Train -- Loss: 0.366 | Acc: 86.637% | Wgt Acc: 90.330% | LR: 1.250000e-05 | Dur: 133.69s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   1.   3.  19. 120.]
 [  1. 563.   2.   5.  39.]
 [  3.   4. 703.  11.  99.]
 [ 19.   2.   7. 488.  67.]
 [ 30.   8.  19.  15. 675.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.245 | Acc: 60.355% | Wgt Acc: 52.220% | Dur: 14.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   1.   2.  14.   7.]
 [  0.  24.   1.   1.   2.]
 [  1.  13.  36.   3.   7.]
 [  9.   0.   0.  34.   0.]
 [ 30.  40.  36.  34. 164.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 192
I - Training: 
	I - Batch: 50 | Loss: 0.348 | Acc: 87.750% | Wgt Acc: 90.658%
	I - Batch: 100 | Loss: 0.360 | Acc: 86.875% | Wgt Acc: 90.294%
	I - Batch: 150 | Loss: 0.369 | Acc: 86.042% | Wgt Acc: 89.436%
	I - Batch: 200 | Loss: 0.368 | Acc: 86.375% | Wgt Acc: 89.678%
I - num batch: 222
I - Train -- Loss: 0.363 | Acc: 86.749% | Wgt Acc: 89.971% | LR: 1.250000e-05 | Dur: 134.34s
I - Confusion Matrix: [row->prediction - col->label]
[[631.   0.   3.  19. 121.]
 [  4. 560.   7.   2.  47.]
 [  2.  10. 691.  11.  89.]
 [ 20.   3.   8. 493.  41.]
 [ 40.   5.  25.  13. 702.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.111 | Acc: 64.103% | Wgt Acc: 58.019% | Dur: 16.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  13.   8.]
 [  1.  31.   5.   3.   6.]
 [  2.  18.  39.   1.   9.]
 [ 10.   1.   4.  44.   0.]
 [ 21.  26.  25.  25. 157.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 193
I - Training: 
	I - Batch: 50 | Loss: 0.352 | Acc: 88.500% | Wgt Acc: 91.267%
	I - Batch: 100 | Loss: 0.366 | Acc: 87.000% | Wgt Acc: 90.339%
	I - Batch: 150 | Loss: 0.361 | Acc: 87.208% | Wgt Acc: 90.578%
	I - Batch: 200 | Loss: 0.357 | Acc: 86.969% | Wgt Acc: 90.514%
I - num batch: 222
I - Train -- Loss: 0.361 | Acc: 86.693% | Wgt Acc: 90.300% | LR: 1.250000e-05 | Dur: 134.72s
I - Confusion Matrix: [row->prediction - col->label]
[[639.   1.   4.  22. 115.]
 [  1. 561.   6.   4.  51.]
 [  3.   4. 707.  14.  95.]
 [ 18.   3.   2. 488.  59.]
 [ 36.   9.  15.  10. 680.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.159 | Acc: 61.736% | Wgt Acc: 54.331% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  14.   7.]
 [  0.  29.   4.   2.   3.]
 [  0.  12.  29.   0.   6.]
 [ 10.   2.   3.  39.   2.]
 [ 24.  33.  37.  31. 162.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 194
I - Training: 
	I - Batch: 50 | Loss: 0.379 | Acc: 86.250% | Wgt Acc: 89.868%
	I - Batch: 100 | Loss: 0.369 | Acc: 87.188% | Wgt Acc: 90.298%
	I - Batch: 150 | Loss: 0.368 | Acc: 87.167% | Wgt Acc: 90.310%
	I - Batch: 200 | Loss: 0.367 | Acc: 87.000% | Wgt Acc: 90.282%
I - num batch: 222
I - Train -- Loss: 0.367 | Acc: 86.947% | Wgt Acc: 90.241% | LR: 1.250000e-05 | Dur: 133.47s
I - Confusion Matrix: [row->prediction - col->label]
[[640.   2.   1.  18.  97.]
 [  3. 567.  13.   6.  48.]
 [  4.   3. 686.  15. 103.]
 [ 12.   2.   9. 491.  52.]
 [ 38.   4.  25.   8. 700.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.260 | Acc: 61.341% | Wgt Acc: 53.558% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   2.   8.   7.]
 [  0.  23.   2.   1.   2.]
 [  2.  14.  33.   1.   7.]
 [  9.   2.   3.  41.   0.]
 [ 27.  38.  35.  35. 164.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 195
I - Training: 
	I - Batch: 50 | Loss: 0.377 | Acc: 87.000% | Wgt Acc: 89.818%
	I - Batch: 100 | Loss: 0.372 | Acc: 86.688% | Wgt Acc: 89.844%
	I - Batch: 150 | Loss: 0.375 | Acc: 86.458% | Wgt Acc: 89.692%
	I - Batch: 200 | Loss: 0.376 | Acc: 86.500% | Wgt Acc: 89.879%
I - num batch: 222
I - Train -- Loss: 0.370 | Acc: 86.665% | Wgt Acc: 90.140% | LR: 1.250000e-05 | Dur: 133.50s
I - Confusion Matrix: [row->prediction - col->label]
[[645.   3.   2.  25. 104.]
 [  4. 558.   6.   4.  50.]
 [  3.   6. 701.   8.  96.]
 [ 15.   2.   6. 484.  64.]
 [ 30.   9.  19.  17. 686.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.204 | Acc: 61.736% | Wgt Acc: 55.929% | Dur: 17.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   3.  11.  13.]
 [  0.  27.   4.   2.   5.]
 [  1.  13.  36.   1.  10.]
 [ 10.   2.   4.  45.   1.]
 [ 23.  34.  28.  27. 151.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 196
I - Training: 
	I - Batch: 50 | Loss: 0.363 | Acc: 87.000% | Wgt Acc: 90.003%
	I - Batch: 100 | Loss: 0.365 | Acc: 87.625% | Wgt Acc: 90.590%
	I - Batch: 150 | Loss: 0.371 | Acc: 87.000% | Wgt Acc: 90.101%
	I - Batch: 200 | Loss: 0.366 | Acc: 86.781% | Wgt Acc: 90.077%
I - num batch: 222
I - Train -- Loss: 0.363 | Acc: 86.975% | Wgt Acc: 90.207% | LR: 1.250000e-05 | Dur: 135.20s
I - Confusion Matrix: [row->prediction - col->label]
[[630.   1.   2.  21. 105.]
 [  4. 564.   5.   2.  47.]
 [  1.   1. 692.  11.  88.]
 [ 16.   2.   8. 495.  56.]
 [ 46.  10.  27.   9. 704.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.199 | Acc: 60.552% | Wgt Acc: 53.181% | Dur: 14.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   1.   2.  13.   9.]
 [  0.  30.   5.   3.   4.]
 [  2.  15.  37.   2.   8.]
 [  8.   1.   2.  33.   0.]
 [ 30.  31.  29.  35. 159.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 197
I - Training: 
	I - Batch: 50 | Loss: 0.338 | Acc: 89.125% | Wgt Acc: 92.400%
	I - Batch: 100 | Loss: 0.346 | Acc: 87.812% | Wgt Acc: 91.418%
	I - Batch: 150 | Loss: 0.353 | Acc: 87.292% | Wgt Acc: 90.887%
	I - Batch: 200 | Loss: 0.355 | Acc: 87.062% | Wgt Acc: 90.571%
I - num batch: 222
I - Train -- Loss: 0.359 | Acc: 86.975% | Wgt Acc: 90.392% | LR: 1.250000e-05 | Dur: 138.36s
I - Confusion Matrix: [row->prediction - col->label]
[[639.   1.   0.  18. 114.]
 [  2. 551.   3.   5.  46.]
 [  5.  13. 707.   6.  92.]
 [ 12.   0.   9. 495.  55.]
 [ 39.  13.  15.  14. 693.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.390 | Acc: 56.805% | Wgt Acc: 47.686% | Dur: 15.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 37.   0.   1.   6.   2.]
 [  0.  20.   2.   2.   2.]
 [  1.  27.  41.   9.  12.]
 [  8.   0.   0.  26.   0.]
 [ 42.  31.  31.  43. 164.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 198
I - Training: 
	I - Batch: 50 | Loss: 0.359 | Acc: 87.250% | Wgt Acc: 90.998%
	I - Batch: 100 | Loss: 0.362 | Acc: 86.875% | Wgt Acc: 90.181%
	I - Batch: 150 | Loss: 0.365 | Acc: 86.167% | Wgt Acc: 89.853%
	I - Batch: 200 | Loss: 0.362 | Acc: 86.594% | Wgt Acc: 90.144%
I - num batch: 222
I - Train -- Loss: 0.364 | Acc: 86.693% | Wgt Acc: 90.136% | LR: 1.250000e-05 | Dur: 134.93s
I - Confusion Matrix: [row->prediction - col->label]
[[641.   1.   4.  17. 105.]
 [  3. 565.   5.   2.  45.]
 [  3.   4. 693.  13.  92.]
 [ 14.   1.  12. 487.  69.]
 [ 36.   7.  20.  19. 689.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.214 | Acc: 61.538% | Wgt Acc: 54.498% | Dur: 16.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   1.   1.  11.   5.]
 [  0.  27.   3.   2.   5.]
 [  1.  19.  37.   1.  10.]
 [ 14.   3.   5.  40.   1.]
 [ 24.  28.  29.  32. 159.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 199
I - Training: 
	I - Batch: 50 | Loss: 0.358 | Acc: 87.125% | Wgt Acc: 90.234%
	I - Batch: 100 | Loss: 0.353 | Acc: 87.062% | Wgt Acc: 90.657%
	I - Batch: 150 | Loss: 0.347 | Acc: 87.833% | Wgt Acc: 91.175%
	I - Batch: 200 | Loss: 0.355 | Acc: 87.594% | Wgt Acc: 90.892%
I - num batch: 222
I - Train -- Loss: 0.355 | Acc: 87.482% | Wgt Acc: 90.878% | LR: 1.250000e-05 | Dur: 134.93s
I - Confusion Matrix: [row->prediction - col->label]
[[647.   1.   0.  12.  96.]
 [  2. 558.   5.   3.  59.]
 [  3.   5. 705.  12.  90.]
 [ 10.   3.   4. 494.  56.]
 [ 35.  11.  20.  17. 699.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.230 | Acc: 60.750% | Wgt Acc: 53.871% | Dur: 14.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 45.   1.   1.  10.   6.]
 [  1.  24.   3.   3.   6.]
 [  2.  23.  45.   4.  12.]
 [ 11.   0.   2.  38.   0.]
 [ 29.  30.  24.  31. 156.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 200
I - Training: 
	I - Batch: 50 | Loss: 0.352 | Acc: 88.000% | Wgt Acc: 91.813%
	I - Batch: 100 | Loss: 0.352 | Acc: 87.812% | Wgt Acc: 91.533%
	I - Batch: 150 | Loss: 0.357 | Acc: 87.417% | Wgt Acc: 91.029%
	I - Batch: 200 | Loss: 0.353 | Acc: 87.531% | Wgt Acc: 91.022%
I - num batch: 222
I - Train -- Loss: 0.359 | Acc: 87.200% | Wgt Acc: 90.744% | LR: 1.250000e-05 | Dur: 136.58s
I - Confusion Matrix: [row->prediction - col->label]
[[649.   1.   1.  20. 112.]
 [  3. 560.   6.   3.  40.]
 [  3.   3. 699.   7.  98.]
 [ 11.   3.   8. 496.  61.]
 [ 31.  11.  20.  12. 689.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.169 | Acc: 62.327% | Wgt Acc: 55.532% | Dur: 17.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   2.   4.  14.   8.]
 [  1.  30.   3.   2.   5.]
 [  2.  13.  38.   1.   7.]
 [ 13.   1.   4.  42.   1.]
 [ 25.  32.  26.  27. 159.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 201
I - Training: 
	I - Batch: 50 | Loss: 0.328 | Acc: 88.625% | Wgt Acc: 91.690%
	I - Batch: 100 | Loss: 0.341 | Acc: 87.938% | Wgt Acc: 91.526%
	I - Batch: 150 | Loss: 0.339 | Acc: 87.792% | Wgt Acc: 91.614%
	I - Batch: 200 | Loss: 0.341 | Acc: 87.688% | Wgt Acc: 91.547%
I - num batch: 222
I - Train -- Loss: 0.345 | Acc: 87.370% | Wgt Acc: 91.301% | LR: 1.250000e-05 | Dur: 134.37s
I - Confusion Matrix: [row->prediction - col->label]
[[650.   0.   1.  14. 109.]
 [  1. 567.   2.   3.  61.]
 [  2.   2. 709.   6.  96.]
 [  6.   1.   4. 502.  63.]
 [ 38.   8.  18.  13. 671.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.236 | Acc: 62.130% | Wgt Acc: 54.759% | Dur: 14.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   2.   1.   9.   8.]
 [  0.  28.   4.   2.   3.]
 [  0.  15.  37.   2.   7.]
 [  8.   0.   3.  37.   0.]
 [ 29.  33.  30.  36. 162.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 202
I - Training: 
	I - Batch: 50 | Loss: 0.357 | Acc: 85.875% | Wgt Acc: 89.614%
	I - Batch: 100 | Loss: 0.344 | Acc: 87.312% | Wgt Acc: 90.730%
	I - Batch: 150 | Loss: 0.344 | Acc: 87.250% | Wgt Acc: 90.542%
	I - Batch: 200 | Loss: 0.351 | Acc: 86.938% | Wgt Acc: 90.348%
I - num batch: 222
I - Train -- Loss: 0.357 | Acc: 86.524% | Wgt Acc: 90.014% | LR: 1.250000e-05 | Dur: 135.38s
I - Confusion Matrix: [row->prediction - col->label]
[[635.   2.   1.  18. 107.]
 [  3. 559.   4.   4.  60.]
 [  2.   7. 700.  10.  85.]
 [ 17.   2.   6. 490.  63.]
 [ 40.   8.  23.  16. 685.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.204 | Acc: 60.750% | Wgt Acc: 53.579% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   3.   1.  14.   8.]
 [  0.  28.   5.   3.   5.]
 [  1.  19.  39.   2.   9.]
 [ 11.   1.   4.  35.   0.]
 [ 28.  27.  26.  32. 158.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 203
I - Training: 
	I - Batch: 50 | Loss: 0.321 | Acc: 88.625% | Wgt Acc: 91.984%
	I - Batch: 100 | Loss: 0.337 | Acc: 87.812% | Wgt Acc: 91.030%
	I - Batch: 150 | Loss: 0.346 | Acc: 87.917% | Wgt Acc: 91.082%
	I - Batch: 200 | Loss: 0.347 | Acc: 87.656% | Wgt Acc: 91.010%
I - num batch: 222
I - Train -- Loss: 0.350 | Acc: 87.398% | Wgt Acc: 90.879% | LR: 1.250000e-05 | Dur: 133.83s
I - Confusion Matrix: [row->prediction - col->label]
[[648.   3.   1.  11. 111.]
 [  3. 561.   4.   3.  53.]
 [  2.   1. 702.  13.  92.]
 [  9.   2.   8. 495.  50.]
 [ 35.  11.  19.  16. 694.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.174 | Acc: 61.538% | Wgt Acc: 54.174% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   1.   1.  12.   6.]
 [  0.  28.   4.   3.   4.]
 [  1.  18.  38.   3.   8.]
 [ 12.   2.   3.  37.   1.]
 [ 27.  29.  29.  31. 161.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 204
I - Training: 
	I - Batch: 50 | Loss: 0.359 | Acc: 86.875% | Wgt Acc: 90.353%
	I - Batch: 100 | Loss: 0.364 | Acc: 87.062% | Wgt Acc: 90.244%
	I - Batch: 150 | Loss: 0.357 | Acc: 86.917% | Wgt Acc: 90.289%
	I - Batch: 200 | Loss: 0.350 | Acc: 87.250% | Wgt Acc: 90.660%
I - num batch: 222
I - Train -- Loss: 0.351 | Acc: 87.059% | Wgt Acc: 90.419% | LR: 1.250000e-05 | Dur: 136.47s
I - Confusion Matrix: [row->prediction - col->label]
[[643.   0.   2.  15.  98.]
 [  1. 559.   3.   2.  40.]
 [  1.   4. 697.   9. 111.]
 [ 17.   3.   2. 492.  54.]
 [ 35.  12.  30.  20. 697.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.282 | Acc: 61.144% | Wgt Acc: 53.056% | Dur: 16.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   2.  11.   5.]
 [  0.  25.   3.   3.   3.]
 [  1.  13.  33.   1.   7.]
 [  7.   0.   2.  34.   0.]
 [ 27.  39.  35.  37. 165.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 205
I - Training: 
	I - Batch: 50 | Loss: 0.352 | Acc: 87.625% | Wgt Acc: 91.578%
	I - Batch: 100 | Loss: 0.343 | Acc: 87.875% | Wgt Acc: 91.531%
	I - Batch: 150 | Loss: 0.333 | Acc: 88.292% | Wgt Acc: 91.664%
	I - Batch: 200 | Loss: 0.339 | Acc: 87.812% | Wgt Acc: 91.183%
I - num batch: 222
I - Train -- Loss: 0.337 | Acc: 87.990% | Wgt Acc: 91.372% | LR: 1.250000e-05 | Dur: 134.84s
I - Confusion Matrix: [row->prediction - col->label]
[[642.   0.   1.  17. 103.]
 [  1. 566.   4.   6.  40.]
 [  1.   4. 705.   8.  99.]
 [ 14.   3.   3. 502.  52.]
 [ 39.   5.  21.   5. 706.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.241 | Acc: 60.552% | Wgt Acc: 53.286% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   2.   3.  12.   9.]
 [  1.  26.   3.   2.   3.]
 [  2.  10.  33.   1.   9.]
 [  9.   0.   3.  40.   0.]
 [ 27.  40.  33.  31. 159.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 206
I - Training: 
	I - Batch: 50 | Loss: 0.352 | Acc: 88.000% | Wgt Acc: 91.141%
	I - Batch: 100 | Loss: 0.351 | Acc: 87.500% | Wgt Acc: 90.958%
	I - Batch: 150 | Loss: 0.353 | Acc: 87.458% | Wgt Acc: 90.894%
	I - Batch: 200 | Loss: 0.345 | Acc: 87.844% | Wgt Acc: 91.092%
I - num batch: 222
I - Train -- Loss: 0.348 | Acc: 87.482% | Wgt Acc: 90.859% | LR: 1.250000e-05 | Dur: 134.31s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   1.   1.  16. 105.]
 [  1. 567.   5.   1.  38.]
 [  3.   4. 695.   9.  97.]
 [ 17.   1.   6. 496.  59.]
 [ 32.   5.  27.  16. 701.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.216 | Acc: 62.525% | Wgt Acc: 55.073% | Dur: 18.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   3.   2.  13.   8.]
 [  1.  31.   3.   2.   3.]
 [  0.  10.  33.   2.   6.]
 [  5.   0.   3.  34.   0.]
 [ 26.  34.  34.  35. 163.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 207
I - Training: 
	I - Batch: 50 | Loss: 0.325 | Acc: 89.000% | Wgt Acc: 92.534%
	I - Batch: 100 | Loss: 0.345 | Acc: 88.562% | Wgt Acc: 91.965%
	I - Batch: 150 | Loss: 0.344 | Acc: 88.292% | Wgt Acc: 91.668%
	I - Batch: 200 | Loss: 0.345 | Acc: 87.906% | Wgt Acc: 91.393%
I - num batch: 222
I - Train -- Loss: 0.343 | Acc: 87.962% | Wgt Acc: 91.447% | LR: 1.250000e-05 | Dur: 138.98s
I - Confusion Matrix: [row->prediction - col->label]
[[651.   1.   0.  18. 101.]
 [  2. 565.   5.   3.  50.]
 [  2.   4. 710.  10.  87.]
 [ 11.   3.   7. 495.  63.]
 [ 31.   5.  12.  12. 699.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.166 | Acc: 63.314% | Wgt Acc: 56.065% | Dur: 15.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   1.  16.   6.]
 [  0.  26.   6.   2.   4.]
 [  0.  12.  32.   0.   7.]
 [  3.   1.   3.  38.   0.]
 [ 23.  36.  33.  30. 163.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 208
I - Training: 
	I - Batch: 50 | Loss: 0.332 | Acc: 86.500% | Wgt Acc: 90.274%
	I - Batch: 100 | Loss: 0.332 | Acc: 87.562% | Wgt Acc: 91.067%
	I - Batch: 150 | Loss: 0.335 | Acc: 87.708% | Wgt Acc: 90.929%
	I - Batch: 200 | Loss: 0.344 | Acc: 87.312% | Wgt Acc: 90.540%
I - num batch: 222
I - Train -- Loss: 0.343 | Acc: 87.229% | Wgt Acc: 90.535% | LR: 1.250000e-05 | Dur: 136.59s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   2.   0.  22. 100.]
 [  2. 562.   7.   4.  45.]
 [  2.   3. 699.   9. 100.]
 [ 15.   2.   7. 488.  54.]
 [ 34.   9.  21.  15. 701.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.194 | Acc: 61.933% | Wgt Acc: 55.020% | Dur: 15.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   0.   2.   8.   7.]
 [  0.  29.   4.   2.   4.]
 [  1.  14.  37.   3.  10.]
 [  9.   2.   2.  41.   0.]
 [ 30.  33.  30.  32. 159.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 209
I - Training: 
	I - Batch: 50 | Loss: 0.331 | Acc: 88.750% | Wgt Acc: 91.804%
	I - Batch: 100 | Loss: 0.341 | Acc: 88.000% | Wgt Acc: 91.178%
	I - Batch: 150 | Loss: 0.344 | Acc: 87.958% | Wgt Acc: 91.208%
	I - Batch: 200 | Loss: 0.338 | Acc: 88.281% | Wgt Acc: 91.499%
I - num batch: 222
I - Train -- Loss: 0.338 | Acc: 88.272% | Wgt Acc: 91.386% | LR: 1.250000e-05 | Dur: 135.30s
I - Confusion Matrix: [row->prediction - col->label]
[[645.   1.   1.  13.  92.]
 [  1. 560.   3.   5.  45.]
 [  2.   8. 705.   7.  90.]
 [ 17.   3.   7. 499.  51.]
 [ 32.   6.  18.  14. 722.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.191 | Acc: 63.314% | Wgt Acc: 55.888% | Dur: 18.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   2.  23.   7.]
 [  0.  28.   5.   1.   3.]
 [  0.  13.  32.   1.   5.]
 [  5.   0.   5.  35.   1.]
 [ 21.  34.  31.  26. 164.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 210
I - Training: 
	I - Batch: 50 | Loss: 0.355 | Acc: 86.250% | Wgt Acc: 90.072%
	I - Batch: 100 | Loss: 0.337 | Acc: 87.812% | Wgt Acc: 91.520%
	I - Batch: 150 | Loss: 0.348 | Acc: 87.583% | Wgt Acc: 91.125%
	I - Batch: 200 | Loss: 0.350 | Acc: 87.344% | Wgt Acc: 91.002%
I - num batch: 222
I - Train -- Loss: 0.349 | Acc: 87.313% | Wgt Acc: 91.009% | LR: 1.250000e-05 | Dur: 133.05s
I - Confusion Matrix: [row->prediction - col->label]
[[652.   0.   1.  15. 132.]
 [  1. 562.   4.   3.  46.]
 [  0.   4. 712.  13.  82.]
 [ 13.   1.   0. 490.  59.]
 [ 31.  11.  17.  17. 681.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.304 | Acc: 60.552% | Wgt Acc: 52.189% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   2.   1.   9.   5.]
 [  0.  26.   2.   1.   2.]
 [  2.  12.  34.   0.   7.]
 [  5.   0.   4.  33.   0.]
 [ 33.  38.  34.  43. 166.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 211
I - Training: 
	I - Batch: 50 | Loss: 0.333 | Acc: 86.750% | Wgt Acc: 89.901%
	I - Batch: 100 | Loss: 0.334 | Acc: 86.562% | Wgt Acc: 90.306%
	I - Batch: 150 | Loss: 0.340 | Acc: 87.208% | Wgt Acc: 90.769%
	I - Batch: 200 | Loss: 0.337 | Acc: 87.625% | Wgt Acc: 90.995%
I - num batch: 222
I - Train -- Loss: 0.337 | Acc: 87.567% | Wgt Acc: 90.926% | LR: 1.250000e-05 | Dur: 134.06s
I - Confusion Matrix: [row->prediction - col->label]
[[638.   0.   1.  12. 107.]
 [  2. 561.   8.   6.  45.]
 [  3.   8. 703.  10.  93.]
 [ 16.   3.   6. 501.  52.]
 [ 38.   6.  16.   9. 703.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.203 | Acc: 59.961% | Wgt Acc: 52.983% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   2.   1.  15.   9.]
 [  0.  26.   5.   4.   6.]
 [  2.  21.  37.   3.   9.]
 [ 10.   0.   2.  34.   1.]
 [ 24.  29.  30.  30. 155.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 212
I - Training: 
	I - Batch: 50 | Loss: 0.339 | Acc: 87.375% | Wgt Acc: 90.974%
	I - Batch: 100 | Loss: 0.347 | Acc: 87.125% | Wgt Acc: 90.837%
	I - Batch: 150 | Loss: 0.342 | Acc: 87.542% | Wgt Acc: 91.074%
	I - Batch: 200 | Loss: 0.334 | Acc: 87.875% | Wgt Acc: 91.320%
I - num batch: 222
I - Train -- Loss: 0.340 | Acc: 87.511% | Wgt Acc: 91.017% | LR: 1.250000e-05 | Dur: 135.96s
I - Confusion Matrix: [row->prediction - col->label]
[[646.   0.   3.  16. 114.]
 [  4. 566.   4.   3.  49.]
 [  2.   3. 703.  11.  85.]
 [ 16.   1.   8. 495.  58.]
 [ 29.   8.  16.  13. 694.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.192 | Acc: 62.130% | Wgt Acc: 55.511% | Dur: 14.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   2.   1.  11.   5.]
 [  0.  35.   6.   4.   7.]
 [  2.  14.  36.   2.  10.]
 [  8.   2.   3.  37.   1.]
 [ 28.  25.  29.  32. 157.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 213
I - Training: 
	I - Batch: 50 | Loss: 0.341 | Acc: 87.500% | Wgt Acc: 90.163%
	I - Batch: 100 | Loss: 0.338 | Acc: 87.312% | Wgt Acc: 90.565%
	I - Batch: 150 | Loss: 0.337 | Acc: 87.083% | Wgt Acc: 90.454%
	I - Batch: 200 | Loss: 0.343 | Acc: 86.844% | Wgt Acc: 90.492%
I - num batch: 222
I - Train -- Loss: 0.343 | Acc: 86.862% | Wgt Acc: 90.493% | LR: 1.250000e-05 | Dur: 138.04s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   0.   0.  16. 115.]
 [  3. 558.   4.   5.  46.]
 [  3.   4. 703.   8.  89.]
 [ 16.   4.   9. 495.  69.]
 [ 31.  12.  18.  14. 681.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.164 | Acc: 64.103% | Wgt Acc: 57.173% | Dur: 18.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   3.  12.   9.]
 [  0.  26.   2.   3.   2.]
 [  1.  17.  35.   0.   6.]
 [ 10.   2.   5.  45.   0.]
 [ 21.  31.  30.  26. 163.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 214
I - Training: 
	I - Batch: 50 | Loss: 0.315 | Acc: 89.875% | Wgt Acc: 92.581%
	I - Batch: 100 | Loss: 0.323 | Acc: 88.438% | Wgt Acc: 91.787%
	I - Batch: 150 | Loss: 0.332 | Acc: 88.042% | Wgt Acc: 91.446%
	I - Batch: 200 | Loss: 0.340 | Acc: 87.375% | Wgt Acc: 90.882%
I - num batch: 222
I - Train -- Loss: 0.341 | Acc: 87.285% | Wgt Acc: 90.808% | LR: 1.250000e-05 | Dur: 137.95s
I - Confusion Matrix: [row->prediction - col->label]
[[649.   1.   0.  15. 106.]
 [  2. 559.   3.   4.  48.]
 [  0.   6. 707.   9.  97.]
 [ 11.   2.   5. 491.  59.]
 [ 35.  10.  19.  19. 690.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.139 | Acc: 63.511% | Wgt Acc: 57.288% | Dur: 15.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   1.   0.  15.  10.]
 [  0.  28.   5.   1.   6.]
 [  1.  13.  33.   0.   7.]
 [  8.   3.   5.  45.   0.]
 [ 20.  33.  32.  25. 157.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 215
I - Training: 
	I - Batch: 50 | Loss: 0.299 | Acc: 90.000% | Wgt Acc: 92.725%
	I - Batch: 100 | Loss: 0.316 | Acc: 88.938% | Wgt Acc: 91.899%
	I - Batch: 150 | Loss: 0.308 | Acc: 89.083% | Wgt Acc: 92.154%
	I - Batch: 200 | Loss: 0.314 | Acc: 88.531% | Wgt Acc: 91.844%
I - num batch: 208
I - Train -- Loss: 0.317 | Acc: 88.371% | Wgt Acc: 91.751% | LR: 1.250000e-05 | Dur: 125.90s
I - Confusion Matrix: [row->prediction - col->label]
[[647.   0.   0.  12.  88.]
 [  2. 560.   3.   3.  43.]
 [  2.   6. 712.  11.  78.]
 [ 14.   2.   6. 501.  51.]
 [ 32.  10.  13.  11. 521.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.190 | Acc: 62.130% | Wgt Acc: 54.759% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   3.  13.   7.]
 [  0.  26.   4.   2.   5.]
 [  1.  13.  36.   1.   6.]
 [  8.   2.   2.  38.   0.]
 [ 26.  36.  30.  32. 162.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 216
I - Training: 
	I - Batch: 50 | Loss: 0.323 | Acc: 87.500% | Wgt Acc: 90.951%
	I - Batch: 100 | Loss: 0.336 | Acc: 87.250% | Wgt Acc: 90.705%
	I - Batch: 150 | Loss: 0.330 | Acc: 87.875% | Wgt Acc: 91.206%
	I - Batch: 200 | Loss: 0.335 | Acc: 87.594% | Wgt Acc: 91.044%
I - num batch: 222
I - Train -- Loss: 0.331 | Acc: 87.821% | Wgt Acc: 91.251% | LR: 1.250000e-05 | Dur: 137.24s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   0.   2.  19. 111.]
 [  1. 562.   0.   6.  42.]
 [  4.   3. 710.  10.  97.]
 [ 14.   2.   5. 498.  49.]
 [ 34.  11.  17.   5. 701.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.273 | Acc: 60.552% | Wgt Acc: 52.565% | Dur: 20.97s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   2.   1.  11.   6.]
 [  0.  25.   3.   2.   3.]
 [  1.  15.  34.   1.   8.]
 [  6.   0.   1.  32.   0.]
 [ 28.  36.  36.  40. 163.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 217
I - Training: 
	I - Batch: 50 | Loss: 0.285 | Acc: 90.875% | Wgt Acc: 93.180%
	I - Batch: 100 | Loss: 0.313 | Acc: 89.000% | Wgt Acc: 91.900%
	I - Batch: 150 | Loss: 0.323 | Acc: 88.833% | Wgt Acc: 91.802%
	I - Batch: 200 | Loss: 0.336 | Acc: 88.156% | Wgt Acc: 91.242%
I - num batch: 222
I - Train -- Loss: 0.338 | Acc: 88.187% | Wgt Acc: 91.275% | LR: 1.250000e-05 | Dur: 140.26s
I - Confusion Matrix: [row->prediction - col->label]
[[641.   0.   2.  15.  86.]
 [  3. 556.   5.   2.  51.]
 [  1.   8. 705.   8.  84.]
 [ 15.   2.   6. 503.  56.]
 [ 37.  12.  16.  10. 723.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.189 | Acc: 61.144% | Wgt Acc: 54.185% | Dur: 16.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   1.   0.   8.   7.]
 [  0.  28.   3.   1.   3.]
 [  2.  15.  37.   3.  10.]
 [ 15.   1.   5.  41.   2.]
 [ 25.  33.  30.  33. 158.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 218
I - Training: 
	I - Batch: 50 | Loss: 0.338 | Acc: 88.750% | Wgt Acc: 92.051%
	I - Batch: 100 | Loss: 0.339 | Acc: 87.875% | Wgt Acc: 91.396%
	I - Batch: 150 | Loss: 0.327 | Acc: 88.542% | Wgt Acc: 91.904%
	I - Batch: 200 | Loss: 0.330 | Acc: 88.375% | Wgt Acc: 91.772%
I - num batch: 222
I - Train -- Loss: 0.334 | Acc: 88.131% | Wgt Acc: 91.561% | LR: 1.250000e-05 | Dur: 135.13s
I - Confusion Matrix: [row->prediction - col->label]
[[647.   4.   1.  12. 107.]
 [  1. 559.   5.   3.  45.]
 [  3.   6. 707.   5.  89.]
 [ 13.   2.   5. 508.  54.]
 [ 33.   7.  16.  10. 705.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.338 | Acc: 60.750% | Wgt Acc: 52.596% | Dur: 16.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   2.   2.   7.   4.]
 [  0.  25.   5.   1.   3.]
 [  2.  16.  35.   4.   8.]
 [  8.   1.   1.  35.   0.]
 [ 30.  34.  32.  39. 165.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 219
I - Training: 
	I - Batch: 50 | Loss: 0.335 | Acc: 89.000% | Wgt Acc: 91.898%
	I - Batch: 100 | Loss: 0.333 | Acc: 88.375% | Wgt Acc: 91.475%
	I - Batch: 150 | Loss: 0.332 | Acc: 88.250% | Wgt Acc: 91.540%
	I - Batch: 200 | Loss: 0.333 | Acc: 88.250% | Wgt Acc: 91.569%
I - num batch: 222
I - Train -- Loss: 0.334 | Acc: 88.244% | Wgt Acc: 91.593% | LR: 1.250000e-05 | Dur: 134.96s
I - Confusion Matrix: [row->prediction - col->label]
[[652.   0.   1.  19. 102.]
 [  3. 563.   6.   3.  44.]
 [  3.   2. 709.   9.  97.]
 [  9.   4.   8. 497.  48.]
 [ 30.   9.  10.  10. 709.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.240 | Acc: 61.736% | Wgt Acc: 54.216% | Dur: 14.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   2.  12.   7.]
 [  2.  27.   4.   3.   3.]
 [  1.  15.  32.   2.   7.]
 [ 10.   0.   4.  41.   0.]
 [ 25.  35.  33.  28. 163.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 220
I - Training: 
	I - Batch: 50 | Loss: 0.317 | Acc: 88.125% | Wgt Acc: 91.495%
	I - Batch: 100 | Loss: 0.331 | Acc: 87.562% | Wgt Acc: 91.030%
	I - Batch: 150 | Loss: 0.332 | Acc: 87.208% | Wgt Acc: 90.550%
	I - Batch: 200 | Loss: 0.329 | Acc: 87.125% | Wgt Acc: 90.549%
I - num batch: 222
I - Train -- Loss: 0.329 | Acc: 87.313% | Wgt Acc: 90.684% | LR: 1.250000e-05 | Dur: 136.82s
I - Confusion Matrix: [row->prediction - col->label]
[[646.   1.   2.  18. 107.]
 [  2. 554.   7.   1.  37.]
 [  2.  12. 701.   9.  94.]
 [ 12.   2.   6. 497.  63.]
 [ 35.   9.  18.  13. 699.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.279 | Acc: 59.961% | Wgt Acc: 51.823% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   2.   2.  12.   7.]
 [  2.  27.   4.   2.   3.]
 [  2.  18.  35.   2.   7.]
 [  6.   0.   1.  30.   0.]
 [ 29.  31.  33.  40. 163.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 221
I - Training: 
	I - Batch: 50 | Loss: 0.343 | Acc: 87.375% | Wgt Acc: 91.048%
	I - Batch: 100 | Loss: 0.338 | Acc: 87.312% | Wgt Acc: 90.965%
	I - Batch: 150 | Loss: 0.333 | Acc: 87.875% | Wgt Acc: 91.294%
	I - Batch: 200 | Loss: 0.336 | Acc: 87.812% | Wgt Acc: 91.186%
I - num batch: 222
I - Train -- Loss: 0.336 | Acc: 87.764% | Wgt Acc: 91.185% | LR: 1.250000e-05 | Dur: 136.25s
I - Confusion Matrix: [row->prediction - col->label]
[[642.   1.   2.  13. 113.]
 [  1. 562.   2.   4.  38.]
 [  3.   2. 710.  12. 101.]
 [ 13.   2.   4. 498.  47.]
 [ 38.  11.  16.  11. 701.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.278 | Acc: 60.552% | Wgt Acc: 52.450% | Dur: 14.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 48.   2.   0.  11.   5.]
 [  0.  27.   3.   3.   3.]
 [  2.  15.  36.   2.   8.]
 [  9.   0.   3.  32.   0.]
 [ 29.  34.  33.  38. 164.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 222
I - Training: 
	I - Batch: 50 | Loss: 0.332 | Acc: 87.875% | Wgt Acc: 90.878%
