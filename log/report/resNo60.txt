Thu Oct 27 23:00:34 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.23, 0.24, 0.23, 0.25, 0.05], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'singleBackgroundPath': 'new_background', 'singleBackgroundPickle': True, 'tossFirstLastFrames': True}, 'dataPath': '/data_ssd/processed/kinetics400/', 'dropoutRate': 0.5, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat', 'background'], 'lastLayerInitUniform': False, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 73.70030581039755, 'maxValidationTrainNo': 55, 'modelVersion': 17, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 60, 'validationAccThr': 70, 'warmStartConfig': {'checkpointFile': './sav/model15_trainNo55_at_epoch_119_with_acc_73_70_checkpoint.pth.tar', 'checkpointModelNo': 15, 'freezeSpatialCNN': False, 'warmStartFlag': False}, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Train set length:  3547
I - Label distribution: [ 697.  578.  734.  538. 1000.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test
I - Loading file: dataset_test_background00_no_samples180.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test/new_background
I - New label distribution: [ 88.  78.  75.  86. 180.]

I - Test set length:  507
I - Label distribution: [ 88.  78.  75.  86. 180.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(4) data number in dataset:  tensor([125260, 149200,    994, 206794,  46381, 119711,    996, 157089,    434,
        218094,  50282,    964, 176675,  37253,  32312,  33867])
I - Initializing model TCNv17
I - Number of Model Parameters: 638752
I - Model output shape:  torch.Size([16, 5])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv17                                   [16, 5]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 128, 64, 64]         6
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Conv2d: 2-2                       --                        6,272
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─OutputShiftSqueeze: 2-4           --                        --
│    └─One: 2-5                          [1]                       --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─OutputScale: 2-7                  --                        --
│    └─Empty: 2-8                        [128, 48, 1, 1]           --
│    └─Empty: 2-9                        [128, 48, 1, 1]           --
│    └─Empty: 2-10                       [128]                     --
│    └─Empty: 2-11                       [128]                     --
│    └─BatchNorm2d: 2-12                 [16, 128, 64, 64]         --
│    └─Scaler: 2-13                      [16, 128, 64, 64]         --
│    └─ReLU: 2-14                        [16, 128, 64, 64]         --
│    └─Empty: 2-15                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Conv2d: 2-18                      --                        147,584
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-20                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-3          [16, 128, 32, 32]         147,590
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-22                   [16, 128, 32, 32]         --
│    └─Empty: 2-23                       [16, 128, 32, 32]         --
│    └─Empty: 2-24                       [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [128, 128, 3, 3]          --
│    └─Empty: 2-29                       [128, 128, 3, 3]          --
│    └─Empty: 2-30                       [128]                     --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-33                      --                        147,584
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-35                       [128]                     --
│    └─BatchNorm2d: 2-36                 [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-38                      [16, 128, 32, 32]         --
│    └─ReLU: 2-39                        [16, 128, 32, 32]         --
│    └─Empty: 2-40                       [16, 128, 32, 32]         --
│    └─Clamp: 2-41                       [16, 128, 32, 32]         --
├─Dropout2d: 1-5                         [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-6          [16, 128, 16, 16]         131,078
│    └─MaxPool2d: 2-42                   [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─Empty: 2-45                       [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Conv2d: 2-47                      --                        16,512
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-49                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-50          --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-52                         [1]                       --
│    └─OutputScale: 2-53                 --                        --
│    └─Empty: 2-54                       [128, 128, 3, 3]          --
│    └─Empty: 2-55                       [128, 128, 3, 3]          --
│    └─Empty: 2-56                       [128]                     --
│    └─Empty: 2-57                       [128]                     --
│    └─BatchNorm2d: 2-58                 [16, 128, 16, 16]         --
│    └─Scaler: 2-59                      [16, 128, 16, 16]         --
│    └─ReLU: 2-60                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-63                      --                        147,584
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-65                       [16, 128, 16, 16]         --
│    └─Clamp: 2-66                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-8                 [16, 128, 16, 16]         16,518
│    └─OutputShiftSqueeze: 2-68          --                        --
│    └─One: 2-69                         [1]                       --
│    └─OutputScale: 2-70                 --                        --
│    └─Empty: 2-71                       [128, 128, 1, 1]          --
│    └─Empty: 2-72                       [128, 128, 1, 1]          --
│    └─Empty: 2-73                       [128]                     --
│    └─Empty: 2-74                       [128]                     --
│    └─BatchNorm2d: 2-75                 [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-78                      --                        147,584
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-80                      [16, 128, 16, 16]         --
│    └─ReLU: 2-81                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-83                       [16, 128, 16, 16]         --
│    └─Clamp: 2-84                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-10         [16, 128, 16, 16]         145,526
│    └─MaxPool2d: 2-85                   [16, 128, 16, 16]         --
│    └─Empty: 2-86                       [16, 128, 16, 16]         --
│    └─Empty: 2-87                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-88          --                        --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-91                         [1]                       --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Conv2d: 2-93                      --                        2,064
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-95                 --                        --
│    └─Empty: 2-96                       [128, 128, 3, 3]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-98                       [128, 128, 3, 3]          --
│    └─Empty: 2-99                       [128]                     --
│    └─Empty: 2-100                      [128]                     --
│    └─BatchNorm2d: 2-101                [16, 128, 16, 16]         --
│    └─Scaler: 2-102                     [16, 128, 16, 16]         --
│    └─ReLU: 2-103                       [16, 128, 16, 16]         --
│    └─Empty: 2-104                      [16, 128, 16, 16]         --
│    └─Clamp: 2-105                      [16, 128, 16, 16]         --
├─Dropout2d: 1-11                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-108                     --                        18,448
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-13         [16, 128, 8, 8]           147,590
│    └─MaxPool2d: 2-110                  [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-112                      [16, 128, 8, 8]           --
│    └─Empty: 2-113                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-114         --                        --
│    └─One: 2-115                        [1]                       --
│    └─OutputScale: 2-116                --                        --
│    └─Empty: 2-117                      [128, 128, 3, 3]          --
│    └─Empty: 2-118                      [128, 128, 3, 3]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-121                      [128]                     --
│    └─Empty: 2-122                      [128]                     --
│    └─BatchNorm2d: 2-123                [16, 128, 8, 8]           --
│    └─Scaler: 2-124                     [16, 128, 8, 8]           --
│    └─ReLU: 2-125                       [16, 128, 8, 8]           --
│    └─Empty: 2-126                      [16, 128, 8, 8]           --
│    └─Clamp: 2-127                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-14                [16, 16, 8, 8]            2,070
├─Linear: 1                              --                        --
│    └─Scaler: 2-128                     --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-129         --                        --
│    └─One: 2-130                        [1]                       --
│    └─OutputScale: 2-131                --                        --
│    └─Empty: 2-132                      [16, 128, 1, 1]           --
│    └─Empty: 2-133                      [16, 128, 1, 1]           --
│    └─Empty: 2-134                      [16]                      --
│    └─Empty: 2-135                      [16]                      --
│    └─BatchNorm2d: 2-136                [16, 16, 8, 8]            --
│    └─Scaler: 2-137                     [16, 16, 8, 8]            --
│    └─ReLU: 2-138                       [16, 16, 8, 8]            --
│    └─Empty: 2-139                      [16, 16, 8, 8]            --
│    └─Clamp: 2-140                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 16, 8, 8]            18,454
│    └─MaxPool2d: 2-141                  [16, 128, 8, 8]           --
│    └─Empty: 2-142                      [16, 128, 8, 8]           --
│    └─Empty: 2-143                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-144         --                        --
│    └─One: 2-145                        [1]                       --
│    └─OutputScale: 2-146                --                        --
│    └─Empty: 2-147                      [16, 128, 3, 3]           --
│    └─Empty: 2-148                      [16, 128, 3, 3]           --
│    └─Empty: 2-149                      [16]                      --
│    └─Empty: 2-150                      [16]                      --
│    └─BatchNorm2d: 2-151                [16, 16, 8, 8]            --
│    └─Scaler: 2-152                     [16, 16, 8, 8]            --
│    └─ReLU: 2-153                       [16, 16, 8, 8]            --
│    └─Empty: 2-154                      [16, 16, 8, 8]            --
│    └─Clamp: 2-155                      [16, 16, 8, 8]            --
├─Dropout2d: 1-16                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-17                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-156         --                        --
│    └─One: 2-157                        [1]                       --
│    └─OutputScale: 2-158                --                        --
│    └─Empty: 2-159                      [128, 48, 1, 1]           --
│    └─Empty: 2-160                      [128, 48, 1, 1]           --
│    └─Empty: 2-161                      [128]                     --
│    └─Empty: 2-162                      [128]                     --
│    └─BatchNorm2d: 2-163                [16, 128, 64, 64]         --
│    └─Scaler: 2-164                     [16, 128, 64, 64]         --
│    └─ReLU: 2-165                       [16, 128, 64, 64]         --
│    └─Empty: 2-166                      [16, 128, 64, 64]         --
│    └─Clamp: 2-167                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-18         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-168                  [16, 128, 32, 32]         --
│    └─Empty: 2-169                      [16, 128, 32, 32]         --
│    └─Empty: 2-170                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-171         --                        --
│    └─One: 2-172                        [1]                       --
│    └─OutputScale: 2-173                --                        --
│    └─Empty: 2-174                      [128, 128, 3, 3]          --
│    └─Empty: 2-175                      [128, 128, 3, 3]          --
│    └─Empty: 2-176                      [128]                     --
│    └─Empty: 2-177                      [128]                     --
│    └─BatchNorm2d: 2-178                [16, 128, 32, 32]         --
│    └─Scaler: 2-179                     [16, 128, 32, 32]         --
│    └─ReLU: 2-180                       [16, 128, 32, 32]         --
│    └─Empty: 2-181                      [16, 128, 32, 32]         --
│    └─Clamp: 2-182                      [16, 128, 32, 32]         --
├─Dropout2d: 1-19                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-20         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-183                  [16, 128, 16, 16]         --
│    └─Empty: 2-184                      [16, 128, 16, 16]         --
│    └─Empty: 2-185                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-186         --                        --
│    └─One: 2-187                        [1]                       --
│    └─OutputScale: 2-188                --                        --
│    └─Empty: 2-189                      [128, 128, 3, 3]          --
│    └─Empty: 2-190                      [128, 128, 3, 3]          --
│    └─Empty: 2-191                      [128]                     --
│    └─Empty: 2-192                      [128]                     --
│    └─BatchNorm2d: 2-193                [16, 128, 16, 16]         --
│    └─Scaler: 2-194                     [16, 128, 16, 16]         --
│    └─ReLU: 2-195                       [16, 128, 16, 16]         --
│    └─Empty: 2-196                      [16, 128, 16, 16]         --
│    └─Clamp: 2-197                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-21                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-198         --                        --
│    └─One: 2-199                        [1]                       --
│    └─OutputScale: 2-200                --                        --
│    └─Empty: 2-201                      [128, 128, 1, 1]          --
│    └─Empty: 2-202                      [128, 128, 1, 1]          --
│    └─Empty: 2-203                      [128]                     --
│    └─Empty: 2-204                      [128]                     --
│    └─BatchNorm2d: 2-205                [16, 128, 16, 16]         --
│    └─Scaler: 2-206                     [16, 128, 16, 16]         --
│    └─ReLU: 2-207                       [16, 128, 16, 16]         --
│    └─Empty: 2-208                      [16, 128, 16, 16]         --
│    └─Clamp: 2-209                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-22         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-210                  [16, 128, 16, 16]         --
│    └─Empty: 2-211                      [16, 128, 16, 16]         --
│    └─Empty: 2-212                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-213         --                        --
│    └─One: 2-214                        [1]                       --
│    └─OutputScale: 2-215                --                        --
│    └─Empty: 2-216                      [128, 128, 3, 3]          --
│    └─Empty: 2-217                      [128, 128, 3, 3]          --
│    └─Empty: 2-218                      [128]                     --
│    └─Empty: 2-219                      [128]                     --
│    └─BatchNorm2d: 2-220                [16, 128, 16, 16]         --
│    └─Scaler: 2-221                     [16, 128, 16, 16]         --
│    └─ReLU: 2-222                       [16, 128, 16, 16]         --
│    └─Empty: 2-223                      [16, 128, 16, 16]         --
│    └─Clamp: 2-224                      [16, 128, 16, 16]         --
├─Dropout2d: 1-23                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-24         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-225                  [16, 128, 8, 8]           --
│    └─Empty: 2-226                      [16, 128, 8, 8]           --
│    └─Empty: 2-227                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-228         --                        --
│    └─One: 2-229                        [1]                       --
│    └─OutputScale: 2-230                --                        --
│    └─Empty: 2-231                      [128, 128, 3, 3]          --
│    └─Empty: 2-232                      [128, 128, 3, 3]          --
│    └─Empty: 2-233                      [128]                     --
│    └─Empty: 2-234                      [128]                     --
│    └─BatchNorm2d: 2-235                [16, 128, 8, 8]           --
│    └─Scaler: 2-236                     [16, 128, 8, 8]           --
│    └─ReLU: 2-237                       [16, 128, 8, 8]           --
│    └─Empty: 2-238                      [16, 128, 8, 8]           --
│    └─Clamp: 2-239                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-25                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-240         --                        --
│    └─One: 2-241                        [1]                       --
│    └─OutputScale: 2-242                --                        --
│    └─Empty: 2-243                      [16, 128, 1, 1]           --
│    └─Empty: 2-244                      [16, 128, 1, 1]           --
│    └─Empty: 2-245                      [16]                      --
│    └─Empty: 2-246                      [16]                      --
│    └─BatchNorm2d: 2-247                [16, 16, 8, 8]            --
│    └─Scaler: 2-248                     [16, 16, 8, 8]            --
│    └─ReLU: 2-249                       [16, 16, 8, 8]            --
│    └─Empty: 2-250                      [16, 16, 8, 8]            --
│    └─Clamp: 2-251                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-252                  [16, 128, 8, 8]           --
│    └─Empty: 2-253                      [16, 128, 8, 8]           --
│    └─Empty: 2-254                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-255         --                        --
│    └─One: 2-256                        [1]                       --
│    └─OutputScale: 2-257                --                        --
│    └─Empty: 2-258                      [16, 128, 3, 3]           --
│    └─Empty: 2-259                      [16, 128, 3, 3]           --
│    └─Empty: 2-260                      [16]                      --
│    └─Empty: 2-261                      [16]                      --
│    └─BatchNorm2d: 2-262                [16, 16, 8, 8]            --
│    └─Scaler: 2-263                     [16, 16, 8, 8]            --
│    └─ReLU: 2-264                       [16, 16, 8, 8]            --
│    └─Empty: 2-265                      [16, 16, 8, 8]            --
│    └─Clamp: 2-266                      [16, 16, 8, 8]            --
├─Dropout2d: 1-27                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-28                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-267         --                        --
│    └─One: 2-268                        [1]                       --
│    └─OutputScale: 2-269                --                        --
│    └─Empty: 2-270                      [128, 48, 1, 1]           --
│    └─Empty: 2-271                      [128, 48, 1, 1]           --
│    └─Empty: 2-272                      [128]                     --
│    └─Empty: 2-273                      [128]                     --
│    └─BatchNorm2d: 2-274                [16, 128, 64, 64]         --
│    └─Scaler: 2-275                     [16, 128, 64, 64]         --
│    └─ReLU: 2-276                       [16, 128, 64, 64]         --
│    └─Empty: 2-277                      [16, 128, 64, 64]         --
│    └─Clamp: 2-278                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-29         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-279                  [16, 128, 32, 32]         --
│    └─Empty: 2-280                      [16, 128, 32, 32]         --
│    └─Empty: 2-281                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-282         --                        --
│    └─One: 2-283                        [1]                       --
│    └─OutputScale: 2-284                --                        --
│    └─Empty: 2-285                      [128, 128, 3, 3]          --
│    └─Empty: 2-286                      [128, 128, 3, 3]          --
│    └─Empty: 2-287                      [128]                     --
│    └─Empty: 2-288                      [128]                     --
│    └─BatchNorm2d: 2-289                [16, 128, 32, 32]         --
│    └─Scaler: 2-290                     [16, 128, 32, 32]         --
│    └─ReLU: 2-291                       [16, 128, 32, 32]         --
│    └─Empty: 2-292                      [16, 128, 32, 32]         --
│    └─Clamp: 2-293                      [16, 128, 32, 32]         --
├─Dropout2d: 1-30                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-294                  [16, 128, 16, 16]         --
│    └─Empty: 2-295                      [16, 128, 16, 16]         --
│    └─Empty: 2-296                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-297         --                        --
│    └─One: 2-298                        [1]                       --
│    └─OutputScale: 2-299                --                        --
│    └─Empty: 2-300                      [128, 128, 3, 3]          --
│    └─Empty: 2-301                      [128, 128, 3, 3]          --
│    └─Empty: 2-302                      [128]                     --
│    └─Empty: 2-303                      [128]                     --
│    └─BatchNorm2d: 2-304                [16, 128, 16, 16]         --
│    └─Scaler: 2-305                     [16, 128, 16, 16]         --
│    └─ReLU: 2-306                       [16, 128, 16, 16]         --
│    └─Empty: 2-307                      [16, 128, 16, 16]         --
│    └─Clamp: 2-308                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-32                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-309         --                        --
│    └─One: 2-310                        [1]                       --
│    └─OutputScale: 2-311                --                        --
│    └─Empty: 2-312                      [128, 128, 1, 1]          --
│    └─Empty: 2-313                      [128, 128, 1, 1]          --
│    └─Empty: 2-314                      [128]                     --
│    └─Empty: 2-315                      [128]                     --
│    └─BatchNorm2d: 2-316                [16, 128, 16, 16]         --
│    └─Scaler: 2-317                     [16, 128, 16, 16]         --
│    └─ReLU: 2-318                       [16, 128, 16, 16]         --
│    └─Empty: 2-319                      [16, 128, 16, 16]         --
│    └─Clamp: 2-320                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-33         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-321                  [16, 128, 16, 16]         --
│    └─Empty: 2-322                      [16, 128, 16, 16]         --
│    └─Empty: 2-323                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-324         --                        --
│    └─One: 2-325                        [1]                       --
│    └─OutputScale: 2-326                --                        --
│    └─Empty: 2-327                      [128, 128, 3, 3]          --
│    └─Empty: 2-328                      [128, 128, 3, 3]          --
│    └─Empty: 2-329                      [128]                     --
│    └─Empty: 2-330                      [128]                     --
│    └─BatchNorm2d: 2-331                [16, 128, 16, 16]         --
│    └─Scaler: 2-332                     [16, 128, 16, 16]         --
│    └─ReLU: 2-333                       [16, 128, 16, 16]         --
│    └─Empty: 2-334                      [16, 128, 16, 16]         --
│    └─Clamp: 2-335                      [16, 128, 16, 16]         --
├─Dropout2d: 1-34                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-35         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-336                  [16, 128, 8, 8]           --
│    └─Empty: 2-337                      [16, 128, 8, 8]           --
│    └─Empty: 2-338                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-339         --                        --
│    └─One: 2-340                        [1]                       --
│    └─OutputScale: 2-341                --                        --
│    └─Empty: 2-342                      [128, 128, 3, 3]          --
│    └─Empty: 2-343                      [128, 128, 3, 3]          --
│    └─Empty: 2-344                      [128]                     --
│    └─Empty: 2-345                      [128]                     --
│    └─BatchNorm2d: 2-346                [16, 128, 8, 8]           --
│    └─Scaler: 2-347                     [16, 128, 8, 8]           --
│    └─ReLU: 2-348                       [16, 128, 8, 8]           --
│    └─Empty: 2-349                      [16, 128, 8, 8]           --
│    └─Clamp: 2-350                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-36                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-351         --                        --
│    └─One: 2-352                        [1]                       --
│    └─OutputScale: 2-353                --                        --
│    └─Empty: 2-354                      [16, 128, 1, 1]           --
│    └─Empty: 2-355                      [16, 128, 1, 1]           --
│    └─Empty: 2-356                      [16]                      --
│    └─Empty: 2-357                      [16]                      --
│    └─BatchNorm2d: 2-358                [16, 16, 8, 8]            --
│    └─Scaler: 2-359                     [16, 16, 8, 8]            --
│    └─ReLU: 2-360                       [16, 16, 8, 8]            --
│    └─Empty: 2-361                      [16, 16, 8, 8]            --
│    └─Clamp: 2-362                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-37         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-363                  [16, 128, 8, 8]           --
│    └─Empty: 2-364                      [16, 128, 8, 8]           --
│    └─Empty: 2-365                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-366         --                        --
│    └─One: 2-367                        [1]                       --
│    └─OutputScale: 2-368                --                        --
│    └─Empty: 2-369                      [16, 128, 3, 3]           --
│    └─Empty: 2-370                      [16, 128, 3, 3]           --
│    └─Empty: 2-371                      [16]                      --
│    └─Empty: 2-372                      [16]                      --
│    └─BatchNorm2d: 2-373                [16, 16, 8, 8]            --
│    └─Scaler: 2-374                     [16, 16, 8, 8]            --
│    └─ReLU: 2-375                       [16, 16, 8, 8]            --
│    └─Empty: 2-376                      [16, 16, 8, 8]            --
│    └─Clamp: 2-377                      [16, 16, 8, 8]            --
├─Dropout2d: 1-38                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-39                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-378         --                        --
│    └─One: 2-379                        [1]                       --
│    └─OutputScale: 2-380                --                        --
│    └─Empty: 2-381                      [128, 48, 1, 1]           --
│    └─Empty: 2-382                      [128, 48, 1, 1]           --
│    └─Empty: 2-383                      [128]                     --
│    └─Empty: 2-384                      [128]                     --
│    └─BatchNorm2d: 2-385                [16, 128, 64, 64]         --
│    └─Scaler: 2-386                     [16, 128, 64, 64]         --
│    └─ReLU: 2-387                       [16, 128, 64, 64]         --
│    └─Empty: 2-388                      [16, 128, 64, 64]         --
│    └─Clamp: 2-389                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-40         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-390                  [16, 128, 32, 32]         --
│    └─Empty: 2-391                      [16, 128, 32, 32]         --
│    └─Empty: 2-392                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-393         --                        --
│    └─One: 2-394                        [1]                       --
│    └─OutputScale: 2-395                --                        --
│    └─Empty: 2-396                      [128, 128, 3, 3]          --
│    └─Empty: 2-397                      [128, 128, 3, 3]          --
│    └─Empty: 2-398                      [128]                     --
│    └─Empty: 2-399                      [128]                     --
│    └─BatchNorm2d: 2-400                [16, 128, 32, 32]         --
│    └─Scaler: 2-401                     [16, 128, 32, 32]         --
│    └─ReLU: 2-402                       [16, 128, 32, 32]         --
│    └─Empty: 2-403                      [16, 128, 32, 32]         --
│    └─Clamp: 2-404                      [16, 128, 32, 32]         --
├─Dropout2d: 1-41                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-42         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-405                  [16, 128, 16, 16]         --
│    └─Empty: 2-406                      [16, 128, 16, 16]         --
│    └─Empty: 2-407                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-408         --                        --
│    └─One: 2-409                        [1]                       --
│    └─OutputScale: 2-410                --                        --
│    └─Empty: 2-411                      [128, 128, 3, 3]          --
│    └─Empty: 2-412                      [128, 128, 3, 3]          --
│    └─Empty: 2-413                      [128]                     --
│    └─Empty: 2-414                      [128]                     --
│    └─BatchNorm2d: 2-415                [16, 128, 16, 16]         --
│    └─Scaler: 2-416                     [16, 128, 16, 16]         --
│    └─ReLU: 2-417                       [16, 128, 16, 16]         --
│    └─Empty: 2-418                      [16, 128, 16, 16]         --
│    └─Clamp: 2-419                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-43                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-420         --                        --
│    └─One: 2-421                        [1]                       --
│    └─OutputScale: 2-422                --                        --
│    └─Empty: 2-423                      [128, 128, 1, 1]          --
│    └─Empty: 2-424                      [128, 128, 1, 1]          --
│    └─Empty: 2-425                      [128]                     --
│    └─Empty: 2-426                      [128]                     --
│    └─BatchNorm2d: 2-427                [16, 128, 16, 16]         --
│    └─Scaler: 2-428                     [16, 128, 16, 16]         --
│    └─ReLU: 2-429                       [16, 128, 16, 16]         --
│    └─Empty: 2-430                      [16, 128, 16, 16]         --
│    └─Clamp: 2-431                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-44         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-432                  [16, 128, 16, 16]         --
│    └─Empty: 2-433                      [16, 128, 16, 16]         --
│    └─Empty: 2-434                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-435         --                        --
│    └─One: 2-436                        [1]                       --
│    └─OutputScale: 2-437                --                        --
│    └─Empty: 2-438                      [128, 128, 3, 3]          --
│    └─Empty: 2-439                      [128, 128, 3, 3]          --
│    └─Empty: 2-440                      [128]                     --
│    └─Empty: 2-441                      [128]                     --
│    └─BatchNorm2d: 2-442                [16, 128, 16, 16]         --
│    └─Scaler: 2-443                     [16, 128, 16, 16]         --
│    └─ReLU: 2-444                       [16, 128, 16, 16]         --
│    └─Empty: 2-445                      [16, 128, 16, 16]         --
│    └─Clamp: 2-446                      [16, 128, 16, 16]         --
├─Dropout2d: 1-45                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-447                  [16, 128, 8, 8]           --
│    └─Empty: 2-448                      [16, 128, 8, 8]           --
│    └─Empty: 2-449                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-450         --                        --
│    └─One: 2-451                        [1]                       --
│    └─OutputScale: 2-452                --                        --
│    └─Empty: 2-453                      [128, 128, 3, 3]          --
│    └─Empty: 2-454                      [128, 128, 3, 3]          --
│    └─Empty: 2-455                      [128]                     --
│    └─Empty: 2-456                      [128]                     --
│    └─BatchNorm2d: 2-457                [16, 128, 8, 8]           --
│    └─Scaler: 2-458                     [16, 128, 8, 8]           --
│    └─ReLU: 2-459                       [16, 128, 8, 8]           --
│    └─Empty: 2-460                      [16, 128, 8, 8]           --
│    └─Clamp: 2-461                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-47                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-462         --                        --
│    └─One: 2-463                        [1]                       --
│    └─OutputScale: 2-464                --                        --
│    └─Empty: 2-465                      [16, 128, 1, 1]           --
│    └─Empty: 2-466                      [16, 128, 1, 1]           --
│    └─Empty: 2-467                      [16]                      --
│    └─Empty: 2-468                      [16]                      --
│    └─BatchNorm2d: 2-469                [16, 16, 8, 8]            --
│    └─Scaler: 2-470                     [16, 16, 8, 8]            --
│    └─ReLU: 2-471                       [16, 16, 8, 8]            --
│    └─Empty: 2-472                      [16, 16, 8, 8]            --
│    └─Clamp: 2-473                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-474                  [16, 128, 8, 8]           --
│    └─Empty: 2-475                      [16, 128, 8, 8]           --
│    └─Empty: 2-476                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-477         --                        --
│    └─One: 2-478                        [1]                       --
│    └─OutputScale: 2-479                --                        --
│    └─Empty: 2-480                      [16, 128, 3, 3]           --
│    └─Empty: 2-481                      [16, 128, 3, 3]           --
│    └─Empty: 2-482                      [16]                      --
│    └─Empty: 2-483                      [16]                      --
│    └─BatchNorm2d: 2-484                [16, 16, 8, 8]            --
│    └─Scaler: 2-485                     [16, 16, 8, 8]            --
│    └─ReLU: 2-486                       [16, 16, 8, 8]            --
│    └─Empty: 2-487                      [16, 16, 8, 8]            --
│    └─Clamp: 2-488                      [16, 16, 8, 8]            --
├─Dropout2d: 1-49                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-50                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-489         --                        --
│    └─One: 2-490                        [1]                       --
│    └─OutputScale: 2-491                --                        --
│    └─Empty: 2-492                      [128, 48, 1, 1]           --
│    └─Empty: 2-493                      [128, 48, 1, 1]           --
│    └─Empty: 2-494                      [128]                     --
│    └─Empty: 2-495                      [128]                     --
│    └─BatchNorm2d: 2-496                [16, 128, 64, 64]         --
│    └─Scaler: 2-497                     [16, 128, 64, 64]         --
│    └─ReLU: 2-498                       [16, 128, 64, 64]         --
│    └─Empty: 2-499                      [16, 128, 64, 64]         --
│    └─Clamp: 2-500                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-501                  [16, 128, 32, 32]         --
│    └─Empty: 2-502                      [16, 128, 32, 32]         --
│    └─Empty: 2-503                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-504         --                        --
│    └─One: 2-505                        [1]                       --
│    └─OutputScale: 2-506                --                        --
│    └─Empty: 2-507                      [128, 128, 3, 3]          --
│    └─Empty: 2-508                      [128, 128, 3, 3]          --
│    └─Empty: 2-509                      [128]                     --
│    └─Empty: 2-510                      [128]                     --
│    └─BatchNorm2d: 2-511                [16, 128, 32, 32]         --
│    └─Scaler: 2-512                     [16, 128, 32, 32]         --
│    └─ReLU: 2-513                       [16, 128, 32, 32]         --
│    └─Empty: 2-514                      [16, 128, 32, 32]         --
│    └─Clamp: 2-515                      [16, 128, 32, 32]         --
├─Dropout2d: 1-52                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-53         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-516                  [16, 128, 16, 16]         --
│    └─Empty: 2-517                      [16, 128, 16, 16]         --
│    └─Empty: 2-518                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-519         --                        --
│    └─One: 2-520                        [1]                       --
│    └─OutputScale: 2-521                --                        --
│    └─Empty: 2-522                      [128, 128, 3, 3]          --
│    └─Empty: 2-523                      [128, 128, 3, 3]          --
│    └─Empty: 2-524                      [128]                     --
│    └─Empty: 2-525                      [128]                     --
│    └─BatchNorm2d: 2-526                [16, 128, 16, 16]         --
│    └─Scaler: 2-527                     [16, 128, 16, 16]         --
│    └─ReLU: 2-528                       [16, 128, 16, 16]         --
│    └─Empty: 2-529                      [16, 128, 16, 16]         --
│    └─Clamp: 2-530                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-54                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-531         --                        --
│    └─One: 2-532                        [1]                       --
│    └─OutputScale: 2-533                --                        --
│    └─Empty: 2-534                      [128, 128, 1, 1]          --
│    └─Empty: 2-535                      [128, 128, 1, 1]          --
│    └─Empty: 2-536                      [128]                     --
│    └─Empty: 2-537                      [128]                     --
│    └─BatchNorm2d: 2-538                [16, 128, 16, 16]         --
│    └─Scaler: 2-539                     [16, 128, 16, 16]         --
│    └─ReLU: 2-540                       [16, 128, 16, 16]         --
│    └─Empty: 2-541                      [16, 128, 16, 16]         --
│    └─Clamp: 2-542                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-55         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-543                  [16, 128, 16, 16]         --
│    └─Empty: 2-544                      [16, 128, 16, 16]         --
│    └─Empty: 2-545                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-546         --                        --
│    └─One: 2-547                        [1]                       --
│    └─OutputScale: 2-548                --                        --
│    └─Empty: 2-549                      [128, 128, 3, 3]          --
│    └─Empty: 2-550                      [128, 128, 3, 3]          --
│    └─Empty: 2-551                      [128]                     --
│    └─Empty: 2-552                      [128]                     --
│    └─BatchNorm2d: 2-553                [16, 128, 16, 16]         --
│    └─Scaler: 2-554                     [16, 128, 16, 16]         --
│    └─ReLU: 2-555                       [16, 128, 16, 16]         --
│    └─Empty: 2-556                      [16, 128, 16, 16]         --
│    └─Clamp: 2-557                      [16, 128, 16, 16]         --
├─Dropout2d: 1-56                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-57         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-558                  [16, 128, 8, 8]           --
│    └─Empty: 2-559                      [16, 128, 8, 8]           --
│    └─Empty: 2-560                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-561         --                        --
│    └─One: 2-562                        [1]                       --
│    └─OutputScale: 2-563                --                        --
│    └─Empty: 2-564                      [128, 128, 3, 3]          --
│    └─Empty: 2-565                      [128, 128, 3, 3]          --
│    └─Empty: 2-566                      [128]                     --
│    └─Empty: 2-567                      [128]                     --
│    └─BatchNorm2d: 2-568                [16, 128, 8, 8]           --
│    └─Scaler: 2-569                     [16, 128, 8, 8]           --
│    └─ReLU: 2-570                       [16, 128, 8, 8]           --
│    └─Empty: 2-571                      [16, 128, 8, 8]           --
│    └─Clamp: 2-572                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-58                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-573         --                        --
│    └─One: 2-574                        [1]                       --
│    └─OutputScale: 2-575                --                        --
│    └─Empty: 2-576                      [16, 128, 1, 1]           --
│    └─Empty: 2-577                      [16, 128, 1, 1]           --
│    └─Empty: 2-578                      [16]                      --
│    └─Empty: 2-579                      [16]                      --
│    └─BatchNorm2d: 2-580                [16, 16, 8, 8]            --
│    └─Scaler: 2-581                     [16, 16, 8, 8]            --
│    └─ReLU: 2-582                       [16, 16, 8, 8]            --
│    └─Empty: 2-583                      [16, 16, 8, 8]            --
│    └─Clamp: 2-584                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-59         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-585                  [16, 128, 8, 8]           --
│    └─Empty: 2-586                      [16, 128, 8, 8]           --
│    └─Empty: 2-587                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-588         --                        --
│    └─One: 2-589                        [1]                       --
│    └─OutputScale: 2-590                --                        --
│    └─Empty: 2-591                      [16, 128, 3, 3]           --
│    └─Empty: 2-592                      [16, 128, 3, 3]           --
│    └─Empty: 2-593                      [16]                      --
│    └─Empty: 2-594                      [16]                      --
│    └─BatchNorm2d: 2-595                [16, 16, 8, 8]            --
│    └─Scaler: 2-596                     [16, 16, 8, 8]            --
│    └─ReLU: 2-597                       [16, 16, 8, 8]            --
│    └─Empty: 2-598                      [16, 16, 8, 8]            --
│    └─Clamp: 2-599                      [16, 16, 8, 8]            --
├─Dropout2d: 1-60                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-61                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-600         --                        --
│    └─One: 2-601                        [1]                       --
│    └─OutputScale: 2-602                --                        --
│    └─Empty: 2-603                      [128, 48, 1, 1]           --
│    └─Empty: 2-604                      [128, 48, 1, 1]           --
│    └─Empty: 2-605                      [128]                     --
│    └─Empty: 2-606                      [128]                     --
│    └─BatchNorm2d: 2-607                [16, 128, 64, 64]         --
│    └─Scaler: 2-608                     [16, 128, 64, 64]         --
│    └─ReLU: 2-609                       [16, 128, 64, 64]         --
│    └─Empty: 2-610                      [16, 128, 64, 64]         --
│    └─Clamp: 2-611                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-62         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-612                  [16, 128, 32, 32]         --
│    └─Empty: 2-613                      [16, 128, 32, 32]         --
│    └─Empty: 2-614                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-615         --                        --
│    └─One: 2-616                        [1]                       --
│    └─OutputScale: 2-617                --                        --
│    └─Empty: 2-618                      [128, 128, 3, 3]          --
│    └─Empty: 2-619                      [128, 128, 3, 3]          --
│    └─Empty: 2-620                      [128]                     --
│    └─Empty: 2-621                      [128]                     --
│    └─BatchNorm2d: 2-622                [16, 128, 32, 32]         --
│    └─Scaler: 2-623                     [16, 128, 32, 32]         --
│    └─ReLU: 2-624                       [16, 128, 32, 32]         --
│    └─Empty: 2-625                      [16, 128, 32, 32]         --
│    └─Clamp: 2-626                      [16, 128, 32, 32]         --
├─Dropout2d: 1-63                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-64         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-627                  [16, 128, 16, 16]         --
│    └─Empty: 2-628                      [16, 128, 16, 16]         --
│    └─Empty: 2-629                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-630         --                        --
│    └─One: 2-631                        [1]                       --
│    └─OutputScale: 2-632                --                        --
│    └─Empty: 2-633                      [128, 128, 3, 3]          --
│    └─Empty: 2-634                      [128, 128, 3, 3]          --
│    └─Empty: 2-635                      [128]                     --
│    └─Empty: 2-636                      [128]                     --
│    └─BatchNorm2d: 2-637                [16, 128, 16, 16]         --
│    └─Scaler: 2-638                     [16, 128, 16, 16]         --
│    └─ReLU: 2-639                       [16, 128, 16, 16]         --
│    └─Empty: 2-640                      [16, 128, 16, 16]         --
│    └─Clamp: 2-641                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-65                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-642         --                        --
│    └─One: 2-643                        [1]                       --
│    └─OutputScale: 2-644                --                        --
│    └─Empty: 2-645                      [128, 128, 1, 1]          --
│    └─Empty: 2-646                      [128, 128, 1, 1]          --
│    └─Empty: 2-647                      [128]                     --
│    └─Empty: 2-648                      [128]                     --
│    └─BatchNorm2d: 2-649                [16, 128, 16, 16]         --
│    └─Scaler: 2-650                     [16, 128, 16, 16]         --
│    └─ReLU: 2-651                       [16, 128, 16, 16]         --
│    └─Empty: 2-652                      [16, 128, 16, 16]         --
│    └─Clamp: 2-653                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-654                  [16, 128, 16, 16]         --
│    └─Empty: 2-655                      [16, 128, 16, 16]         --
│    └─Empty: 2-656                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-657         --                        --
│    └─One: 2-658                        [1]                       --
│    └─OutputScale: 2-659                --                        --
│    └─Empty: 2-660                      [128, 128, 3, 3]          --
│    └─Empty: 2-661                      [128, 128, 3, 3]          --
│    └─Empty: 2-662                      [128]                     --
│    └─Empty: 2-663                      [128]                     --
│    └─BatchNorm2d: 2-664                [16, 128, 16, 16]         --
│    └─Scaler: 2-665                     [16, 128, 16, 16]         --
│    └─ReLU: 2-666                       [16, 128, 16, 16]         --
│    └─Empty: 2-667                      [16, 128, 16, 16]         --
│    └─Clamp: 2-668                      [16, 128, 16, 16]         --
├─Dropout2d: 1-67                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-669                  [16, 128, 8, 8]           --
│    └─Empty: 2-670                      [16, 128, 8, 8]           --
│    └─Empty: 2-671                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-672         --                        --
│    └─One: 2-673                        [1]                       --
│    └─OutputScale: 2-674                --                        --
│    └─Empty: 2-675                      [128, 128, 3, 3]          --
│    └─Empty: 2-676                      [128, 128, 3, 3]          --
│    └─Empty: 2-677                      [128]                     --
│    └─Empty: 2-678                      [128]                     --
│    └─BatchNorm2d: 2-679                [16, 128, 8, 8]           --
│    └─Scaler: 2-680                     [16, 128, 8, 8]           --
│    └─ReLU: 2-681                       [16, 128, 8, 8]           --
│    └─Empty: 2-682                      [16, 128, 8, 8]           --
│    └─Clamp: 2-683                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-69                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-684         --                        --
│    └─One: 2-685                        [1]                       --
│    └─OutputScale: 2-686                --                        --
│    └─Empty: 2-687                      [16, 128, 1, 1]           --
│    └─Empty: 2-688                      [16, 128, 1, 1]           --
│    └─Empty: 2-689                      [16]                      --
│    └─Empty: 2-690                      [16]                      --
│    └─BatchNorm2d: 2-691                [16, 16, 8, 8]            --
│    └─Scaler: 2-692                     [16, 16, 8, 8]            --
│    └─ReLU: 2-693                       [16, 16, 8, 8]            --
│    └─Empty: 2-694                      [16, 16, 8, 8]            --
│    └─Clamp: 2-695                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-70         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-696                  [16, 128, 8, 8]           --
│    └─Empty: 2-697                      [16, 128, 8, 8]           --
│    └─Empty: 2-698                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-699         --                        --
│    └─One: 2-700                        [1]                       --
│    └─OutputScale: 2-701                --                        --
│    └─Empty: 2-702                      [16, 128, 3, 3]           --
│    └─Empty: 2-703                      [16, 128, 3, 3]           --
│    └─Empty: 2-704                      [16]                      --
│    └─Empty: 2-705                      [16]                      --
│    └─BatchNorm2d: 2-706                [16, 16, 8, 8]            --
│    └─Scaler: 2-707                     [16, 16, 8, 8]            --
│    └─ReLU: 2-708                       [16, 16, 8, 8]            --
│    └─Empty: 2-709                      [16, 16, 8, 8]            --
│    └─Clamp: 2-710                      [16, 16, 8, 8]            --
├─Dropout2d: 1-71                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-72                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-711         --                        --
│    └─One: 2-712                        [1]                       --
│    └─OutputScale: 2-713                --                        --
│    └─Empty: 2-714                      [128, 48, 1, 1]           --
│    └─Empty: 2-715                      [128, 48, 1, 1]           --
│    └─Empty: 2-716                      [128]                     --
│    └─Empty: 2-717                      [128]                     --
│    └─BatchNorm2d: 2-718                [16, 128, 64, 64]         --
│    └─Scaler: 2-719                     [16, 128, 64, 64]         --
│    └─ReLU: 2-720                       [16, 128, 64, 64]         --
│    └─Empty: 2-721                      [16, 128, 64, 64]         --
│    └─Clamp: 2-722                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-723                  [16, 128, 32, 32]         --
│    └─Empty: 2-724                      [16, 128, 32, 32]         --
│    └─Empty: 2-725                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-726         --                        --
│    └─One: 2-727                        [1]                       --
│    └─OutputScale: 2-728                --                        --
│    └─Empty: 2-729                      [128, 128, 3, 3]          --
│    └─Empty: 2-730                      [128, 128, 3, 3]          --
│    └─Empty: 2-731                      [128]                     --
│    └─Empty: 2-732                      [128]                     --
│    └─BatchNorm2d: 2-733                [16, 128, 32, 32]         --
│    └─Scaler: 2-734                     [16, 128, 32, 32]         --
│    └─ReLU: 2-735                       [16, 128, 32, 32]         --
│    └─Empty: 2-736                      [16, 128, 32, 32]         --
│    └─Clamp: 2-737                      [16, 128, 32, 32]         --
├─Dropout2d: 1-74                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-738                  [16, 128, 16, 16]         --
│    └─Empty: 2-739                      [16, 128, 16, 16]         --
│    └─Empty: 2-740                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-741         --                        --
│    └─One: 2-742                        [1]                       --
│    └─OutputScale: 2-743                --                        --
│    └─Empty: 2-744                      [128, 128, 3, 3]          --
│    └─Empty: 2-745                      [128, 128, 3, 3]          --
│    └─Empty: 2-746                      [128]                     --
│    └─Empty: 2-747                      [128]                     --
│    └─BatchNorm2d: 2-748                [16, 128, 16, 16]         --
│    └─Scaler: 2-749                     [16, 128, 16, 16]         --
│    └─ReLU: 2-750                       [16, 128, 16, 16]         --
│    └─Empty: 2-751                      [16, 128, 16, 16]         --
│    └─Clamp: 2-752                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-76                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-753         --                        --
│    └─One: 2-754                        [1]                       --
│    └─OutputScale: 2-755                --                        --
│    └─Empty: 2-756                      [128, 128, 1, 1]          --
│    └─Empty: 2-757                      [128, 128, 1, 1]          --
│    └─Empty: 2-758                      [128]                     --
│    └─Empty: 2-759                      [128]                     --
│    └─BatchNorm2d: 2-760                [16, 128, 16, 16]         --
│    └─Scaler: 2-761                     [16, 128, 16, 16]         --
│    └─ReLU: 2-762                       [16, 128, 16, 16]         --
│    └─Empty: 2-763                      [16, 128, 16, 16]         --
│    └─Clamp: 2-764                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-77         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-765                  [16, 128, 16, 16]         --
│    └─Empty: 2-766                      [16, 128, 16, 16]         --
│    └─Empty: 2-767                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-768         --                        --
│    └─One: 2-769                        [1]                       --
│    └─OutputScale: 2-770                --                        --
│    └─Empty: 2-771                      [128, 128, 3, 3]          --
│    └─Empty: 2-772                      [128, 128, 3, 3]          --
│    └─Empty: 2-773                      [128]                     --
│    └─Empty: 2-774                      [128]                     --
│    └─BatchNorm2d: 2-775                [16, 128, 16, 16]         --
│    └─Scaler: 2-776                     [16, 128, 16, 16]         --
│    └─ReLU: 2-777                       [16, 128, 16, 16]         --
│    └─Empty: 2-778                      [16, 128, 16, 16]         --
│    └─Clamp: 2-779                      [16, 128, 16, 16]         --
├─Dropout2d: 1-78                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-79         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-780                  [16, 128, 8, 8]           --
│    └─Empty: 2-781                      [16, 128, 8, 8]           --
│    └─Empty: 2-782                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-783         --                        --
│    └─One: 2-784                        [1]                       --
│    └─OutputScale: 2-785                --                        --
│    └─Empty: 2-786                      [128, 128, 3, 3]          --
│    └─Empty: 2-787                      [128, 128, 3, 3]          --
│    └─Empty: 2-788                      [128]                     --
│    └─Empty: 2-789                      [128]                     --
│    └─BatchNorm2d: 2-790                [16, 128, 8, 8]           --
│    └─Scaler: 2-791                     [16, 128, 8, 8]           --
│    └─ReLU: 2-792                       [16, 128, 8, 8]           --
│    └─Empty: 2-793                      [16, 128, 8, 8]           --
│    └─Clamp: 2-794                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-80                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-795         --                        --
│    └─One: 2-796                        [1]                       --
│    └─OutputScale: 2-797                --                        --
│    └─Empty: 2-798                      [16, 128, 1, 1]           --
│    └─Empty: 2-799                      [16, 128, 1, 1]           --
│    └─Empty: 2-800                      [16]                      --
│    └─Empty: 2-801                      [16]                      --
│    └─BatchNorm2d: 2-802                [16, 16, 8, 8]            --
│    └─Scaler: 2-803                     [16, 16, 8, 8]            --
│    └─ReLU: 2-804                       [16, 16, 8, 8]            --
│    └─Empty: 2-805                      [16, 16, 8, 8]            --
│    └─Clamp: 2-806                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-807                  [16, 128, 8, 8]           --
│    └─Empty: 2-808                      [16, 128, 8, 8]           --
│    └─Empty: 2-809                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-810         --                        --
│    └─One: 2-811                        [1]                       --
│    └─OutputScale: 2-812                --                        --
│    └─Empty: 2-813                      [16, 128, 3, 3]           --
│    └─Empty: 2-814                      [16, 128, 3, 3]           --
│    └─Empty: 2-815                      [16]                      --
│    └─Empty: 2-816                      [16]                      --
│    └─BatchNorm2d: 2-817                [16, 16, 8, 8]            --
│    └─Scaler: 2-818                     [16, 16, 8, 8]            --
│    └─ReLU: 2-819                       [16, 16, 8, 8]            --
│    └─Empty: 2-820                      [16, 16, 8, 8]            --
│    └─Clamp: 2-821                      [16, 16, 8, 8]            --
├─Dropout2d: 1-82                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-83                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-822         --                        --
│    └─One: 2-823                        [1]                       --
│    └─OutputScale: 2-824                --                        --
│    └─Empty: 2-825                      [128, 48, 1, 1]           --
│    └─Empty: 2-826                      [128, 48, 1, 1]           --
│    └─Empty: 2-827                      [128]                     --
│    └─Empty: 2-828                      [128]                     --
│    └─BatchNorm2d: 2-829                [16, 128, 64, 64]         --
│    └─Scaler: 2-830                     [16, 128, 64, 64]         --
│    └─ReLU: 2-831                       [16, 128, 64, 64]         --
│    └─Empty: 2-832                      [16, 128, 64, 64]         --
│    └─Clamp: 2-833                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-84         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-834                  [16, 128, 32, 32]         --
│    └─Empty: 2-835                      [16, 128, 32, 32]         --
│    └─Empty: 2-836                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-837         --                        --
│    └─One: 2-838                        [1]                       --
│    └─OutputScale: 2-839                --                        --
│    └─Empty: 2-840                      [128, 128, 3, 3]          --
│    └─Empty: 2-841                      [128, 128, 3, 3]          --
│    └─Empty: 2-842                      [128]                     --
│    └─Empty: 2-843                      [128]                     --
│    └─BatchNorm2d: 2-844                [16, 128, 32, 32]         --
│    └─Scaler: 2-845                     [16, 128, 32, 32]         --
│    └─ReLU: 2-846                       [16, 128, 32, 32]         --
│    └─Empty: 2-847                      [16, 128, 32, 32]         --
│    └─Clamp: 2-848                      [16, 128, 32, 32]         --
├─Dropout2d: 1-85                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-86         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-849                  [16, 128, 16, 16]         --
│    └─Empty: 2-850                      [16, 128, 16, 16]         --
│    └─Empty: 2-851                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-852         --                        --
│    └─One: 2-853                        [1]                       --
│    └─OutputScale: 2-854                --                        --
│    └─Empty: 2-855                      [128, 128, 3, 3]          --
│    └─Empty: 2-856                      [128, 128, 3, 3]          --
│    └─Empty: 2-857                      [128]                     --
│    └─Empty: 2-858                      [128]                     --
│    └─BatchNorm2d: 2-859                [16, 128, 16, 16]         --
│    └─Scaler: 2-860                     [16, 128, 16, 16]         --
│    └─ReLU: 2-861                       [16, 128, 16, 16]         --
│    └─Empty: 2-862                      [16, 128, 16, 16]         --
│    └─Clamp: 2-863                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-87                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-864         --                        --
│    └─One: 2-865                        [1]                       --
│    └─OutputScale: 2-866                --                        --
│    └─Empty: 2-867                      [128, 128, 1, 1]          --
│    └─Empty: 2-868                      [128, 128, 1, 1]          --
│    └─Empty: 2-869                      [128]                     --
│    └─Empty: 2-870                      [128]                     --
│    └─BatchNorm2d: 2-871                [16, 128, 16, 16]         --
│    └─Scaler: 2-872                     [16, 128, 16, 16]         --
│    └─ReLU: 2-873                       [16, 128, 16, 16]         --
│    └─Empty: 2-874                      [16, 128, 16, 16]         --
│    └─Clamp: 2-875                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-88         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-876                  [16, 128, 16, 16]         --
│    └─Empty: 2-877                      [16, 128, 16, 16]         --
│    └─Empty: 2-878                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-879         --                        --
│    └─One: 2-880                        [1]                       --
│    └─OutputScale: 2-881                --                        --
│    └─Empty: 2-882                      [128, 128, 3, 3]          --
│    └─Empty: 2-883                      [128, 128, 3, 3]          --
│    └─Empty: 2-884                      [128]                     --
│    └─Empty: 2-885                      [128]                     --
│    └─BatchNorm2d: 2-886                [16, 128, 16, 16]         --
│    └─Scaler: 2-887                     [16, 128, 16, 16]         --
│    └─ReLU: 2-888                       [16, 128, 16, 16]         --
│    └─Empty: 2-889                      [16, 128, 16, 16]         --
│    └─Clamp: 2-890                      [16, 128, 16, 16]         --
├─Dropout2d: 1-89                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-90         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-891                  [16, 128, 8, 8]           --
│    └─Empty: 2-892                      [16, 128, 8, 8]           --
│    └─Empty: 2-893                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-894         --                        --
│    └─One: 2-895                        [1]                       --
│    └─OutputScale: 2-896                --                        --
│    └─Empty: 2-897                      [128, 128, 3, 3]          --
│    └─Empty: 2-898                      [128, 128, 3, 3]          --
│    └─Empty: 2-899                      [128]                     --
│    └─Empty: 2-900                      [128]                     --
│    └─BatchNorm2d: 2-901                [16, 128, 8, 8]           --
│    └─Scaler: 2-902                     [16, 128, 8, 8]           --
│    └─ReLU: 2-903                       [16, 128, 8, 8]           --
│    └─Empty: 2-904                      [16, 128, 8, 8]           --
│    └─Clamp: 2-905                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-91                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-906         --                        --
│    └─One: 2-907                        [1]                       --
│    └─OutputScale: 2-908                --                        --
│    └─Empty: 2-909                      [16, 128, 1, 1]           --
│    └─Empty: 2-910                      [16, 128, 1, 1]           --
│    └─Empty: 2-911                      [16]                      --
│    └─Empty: 2-912                      [16]                      --
│    └─BatchNorm2d: 2-913                [16, 16, 8, 8]            --
│    └─Scaler: 2-914                     [16, 16, 8, 8]            --
│    └─ReLU: 2-915                       [16, 16, 8, 8]            --
│    └─Empty: 2-916                      [16, 16, 8, 8]            --
│    └─Clamp: 2-917                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-92         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-918                  [16, 128, 8, 8]           --
│    └─Empty: 2-919                      [16, 128, 8, 8]           --
│    └─Empty: 2-920                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-921         --                        --
│    └─One: 2-922                        [1]                       --
│    └─OutputScale: 2-923                --                        --
│    └─Empty: 2-924                      [16, 128, 3, 3]           --
│    └─Empty: 2-925                      [16, 128, 3, 3]           --
│    └─Empty: 2-926                      [16]                      --
│    └─Empty: 2-927                      [16]                      --
│    └─BatchNorm2d: 2-928                [16, 16, 8, 8]            --
│    └─Scaler: 2-929                     [16, 16, 8, 8]            --
│    └─ReLU: 2-930                       [16, 16, 8, 8]            --
│    └─Empty: 2-931                      [16, 16, 8, 8]            --
│    └─Clamp: 2-932                      [16, 16, 8, 8]            --
├─Dropout2d: 1-93                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-94                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-933         --                        --
│    └─One: 2-934                        [1]                       --
│    └─OutputScale: 2-935                --                        --
│    └─Empty: 2-936                      [128, 48, 1, 1]           --
│    └─Empty: 2-937                      [128, 48, 1, 1]           --
│    └─Empty: 2-938                      [128]                     --
│    └─Empty: 2-939                      [128]                     --
│    └─BatchNorm2d: 2-940                [16, 128, 64, 64]         --
│    └─Scaler: 2-941                     [16, 128, 64, 64]         --
│    └─ReLU: 2-942                       [16, 128, 64, 64]         --
│    └─Empty: 2-943                      [16, 128, 64, 64]         --
│    └─Clamp: 2-944                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-95         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-945                  [16, 128, 32, 32]         --
│    └─Empty: 2-946                      [16, 128, 32, 32]         --
│    └─Empty: 2-947                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-948         --                        --
│    └─One: 2-949                        [1]                       --
│    └─OutputScale: 2-950                --                        --
│    └─Empty: 2-951                      [128, 128, 3, 3]          --
│    └─Empty: 2-952                      [128, 128, 3, 3]          --
│    └─Empty: 2-953                      [128]                     --
│    └─Empty: 2-954                      [128]                     --
│    └─BatchNorm2d: 2-955                [16, 128, 32, 32]         --
│    └─Scaler: 2-956                     [16, 128, 32, 32]         --
│    └─ReLU: 2-957                       [16, 128, 32, 32]         --
│    └─Empty: 2-958                      [16, 128, 32, 32]         --
│    └─Clamp: 2-959                      [16, 128, 32, 32]         --
├─Dropout2d: 1-96                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-97         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-960                  [16, 128, 16, 16]         --
│    └─Empty: 2-961                      [16, 128, 16, 16]         --
│    └─Empty: 2-962                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-963         --                        --
│    └─One: 2-964                        [1]                       --
│    └─OutputScale: 2-965                --                        --
│    └─Empty: 2-966                      [128, 128, 3, 3]          --
│    └─Empty: 2-967                      [128, 128, 3, 3]          --
│    └─Empty: 2-968                      [128]                     --
│    └─Empty: 2-969                      [128]                     --
│    └─BatchNorm2d: 2-970                [16, 128, 16, 16]         --
│    └─Scaler: 2-971                     [16, 128, 16, 16]         --
│    └─ReLU: 2-972                       [16, 128, 16, 16]         --
│    └─Empty: 2-973                      [16, 128, 16, 16]         --
│    └─Clamp: 2-974                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-98                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-975         --                        --
│    └─One: 2-976                        [1]                       --
│    └─OutputScale: 2-977                --                        --
│    └─Empty: 2-978                      [128, 128, 1, 1]          --
│    └─Empty: 2-979                      [128, 128, 1, 1]          --
│    └─Empty: 2-980                      [128]                     --
│    └─Empty: 2-981                      [128]                     --
│    └─BatchNorm2d: 2-982                [16, 128, 16, 16]         --
│    └─Scaler: 2-983                     [16, 128, 16, 16]         --
│    └─ReLU: 2-984                       [16, 128, 16, 16]         --
│    └─Empty: 2-985                      [16, 128, 16, 16]         --
│    └─Clamp: 2-986                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-99         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-987                  [16, 128, 16, 16]         --
│    └─Empty: 2-988                      [16, 128, 16, 16]         --
│    └─Empty: 2-989                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-990         --                        --
│    └─One: 2-991                        [1]                       --
│    └─OutputScale: 2-992                --                        --
│    └─Empty: 2-993                      [128, 128, 3, 3]          --
│    └─Empty: 2-994                      [128, 128, 3, 3]          --
│    └─Empty: 2-995                      [128]                     --
│    └─Empty: 2-996                      [128]                     --
│    └─BatchNorm2d: 2-997                [16, 128, 16, 16]         --
│    └─Scaler: 2-998                     [16, 128, 16, 16]         --
│    └─ReLU: 2-999                       [16, 128, 16, 16]         --
│    └─Empty: 2-1000                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1001                     [16, 128, 16, 16]         --
├─Dropout2d: 1-100                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-101        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1002                 [16, 128, 8, 8]           --
│    └─Empty: 2-1003                     [16, 128, 8, 8]           --
│    └─Empty: 2-1004                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1005        --                        --
│    └─One: 2-1006                       [1]                       --
│    └─OutputScale: 2-1007               --                        --
│    └─Empty: 2-1008                     [128, 128, 3, 3]          --
│    └─Empty: 2-1009                     [128, 128, 3, 3]          --
│    └─Empty: 2-1010                     [128]                     --
│    └─Empty: 2-1011                     [128]                     --
│    └─BatchNorm2d: 2-1012               [16, 128, 8, 8]           --
│    └─Scaler: 2-1013                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1014                      [16, 128, 8, 8]           --
│    └─Empty: 2-1015                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1016                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-102               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1017        --                        --
│    └─One: 2-1018                       [1]                       --
│    └─OutputScale: 2-1019               --                        --
│    └─Empty: 2-1020                     [16, 128, 1, 1]           --
│    └─Empty: 2-1021                     [16, 128, 1, 1]           --
│    └─Empty: 2-1022                     [16]                      --
│    └─Empty: 2-1023                     [16]                      --
│    └─BatchNorm2d: 2-1024               [16, 16, 8, 8]            --
│    └─Scaler: 2-1025                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1026                      [16, 16, 8, 8]            --
│    └─Empty: 2-1027                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1028                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-103        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1029                 [16, 128, 8, 8]           --
│    └─Empty: 2-1030                     [16, 128, 8, 8]           --
│    └─Empty: 2-1031                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1032        --                        --
│    └─One: 2-1033                       [1]                       --
│    └─OutputScale: 2-1034               --                        --
│    └─Empty: 2-1035                     [16, 128, 3, 3]           --
│    └─Empty: 2-1036                     [16, 128, 3, 3]           --
│    └─Empty: 2-1037                     [16]                      --
│    └─Empty: 2-1038                     [16]                      --
│    └─BatchNorm2d: 2-1039               [16, 16, 8, 8]            --
│    └─Scaler: 2-1040                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1041                      [16, 16, 8, 8]            --
│    └─Empty: 2-1042                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1043                     [16, 16, 8, 8]            --
├─Dropout2d: 1-104                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-105               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1044        --                        --
│    └─One: 2-1045                       [1]                       --
│    └─OutputScale: 2-1046               --                        --
│    └─Empty: 2-1047                     [128, 48, 1, 1]           --
│    └─Empty: 2-1048                     [128, 48, 1, 1]           --
│    └─Empty: 2-1049                     [128]                     --
│    └─Empty: 2-1050                     [128]                     --
│    └─BatchNorm2d: 2-1051               [16, 128, 64, 64]         --
│    └─Scaler: 2-1052                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1053                      [16, 128, 64, 64]         --
│    └─Empty: 2-1054                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1055                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-106        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1056                 [16, 128, 32, 32]         --
│    └─Empty: 2-1057                     [16, 128, 32, 32]         --
│    └─Empty: 2-1058                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1059        --                        --
│    └─One: 2-1060                       [1]                       --
│    └─OutputScale: 2-1061               --                        --
│    └─Empty: 2-1062                     [128, 128, 3, 3]          --
│    └─Empty: 2-1063                     [128, 128, 3, 3]          --
│    └─Empty: 2-1064                     [128]                     --
│    └─Empty: 2-1065                     [128]                     --
│    └─BatchNorm2d: 2-1066               [16, 128, 32, 32]         --
│    └─Scaler: 2-1067                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1068                      [16, 128, 32, 32]         --
│    └─Empty: 2-1069                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1070                     [16, 128, 32, 32]         --
├─Dropout2d: 1-107                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-108        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1071                 [16, 128, 16, 16]         --
│    └─Empty: 2-1072                     [16, 128, 16, 16]         --
│    └─Empty: 2-1073                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1074        --                        --
│    └─One: 2-1075                       [1]                       --
│    └─OutputScale: 2-1076               --                        --
│    └─Empty: 2-1077                     [128, 128, 3, 3]          --
│    └─Empty: 2-1078                     [128, 128, 3, 3]          --
│    └─Empty: 2-1079                     [128]                     --
│    └─Empty: 2-1080                     [128]                     --
│    └─BatchNorm2d: 2-1081               [16, 128, 16, 16]         --
│    └─Scaler: 2-1082                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1083                      [16, 128, 16, 16]         --
│    └─Empty: 2-1084                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1085                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-109               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1086        --                        --
│    └─One: 2-1087                       [1]                       --
│    └─OutputScale: 2-1088               --                        --
│    └─Empty: 2-1089                     [128, 128, 1, 1]          --
│    └─Empty: 2-1090                     [128, 128, 1, 1]          --
│    └─Empty: 2-1091                     [128]                     --
│    └─Empty: 2-1092                     [128]                     --
│    └─BatchNorm2d: 2-1093               [16, 128, 16, 16]         --
│    └─Scaler: 2-1094                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1095                      [16, 128, 16, 16]         --
│    └─Empty: 2-1096                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1097                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-110        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1098                 [16, 128, 16, 16]         --
│    └─Empty: 2-1099                     [16, 128, 16, 16]         --
│    └─Empty: 2-1100                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1101        --                        --
│    └─One: 2-1102                       [1]                       --
│    └─OutputScale: 2-1103               --                        --
│    └─Empty: 2-1104                     [128, 128, 3, 3]          --
│    └─Empty: 2-1105                     [128, 128, 3, 3]          --
│    └─Empty: 2-1106                     [128]                     --
│    └─Empty: 2-1107                     [128]                     --
│    └─BatchNorm2d: 2-1108               [16, 128, 16, 16]         --
│    └─Scaler: 2-1109                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1110                      [16, 128, 16, 16]         --
│    └─Empty: 2-1111                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1112                     [16, 128, 16, 16]         --
├─Dropout2d: 1-111                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-112        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1113                 [16, 128, 8, 8]           --
│    └─Empty: 2-1114                     [16, 128, 8, 8]           --
│    └─Empty: 2-1115                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1116        --                        --
│    └─One: 2-1117                       [1]                       --
│    └─OutputScale: 2-1118               --                        --
│    └─Empty: 2-1119                     [128, 128, 3, 3]          --
│    └─Empty: 2-1120                     [128, 128, 3, 3]          --
│    └─Empty: 2-1121                     [128]                     --
│    └─Empty: 2-1122                     [128]                     --
│    └─BatchNorm2d: 2-1123               [16, 128, 8, 8]           --
│    └─Scaler: 2-1124                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1125                      [16, 128, 8, 8]           --
│    └─Empty: 2-1126                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1127                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-113               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1128        --                        --
│    └─One: 2-1129                       [1]                       --
│    └─OutputScale: 2-1130               --                        --
│    └─Empty: 2-1131                     [16, 128, 1, 1]           --
│    └─Empty: 2-1132                     [16, 128, 1, 1]           --
│    └─Empty: 2-1133                     [16]                      --
│    └─Empty: 2-1134                     [16]                      --
│    └─BatchNorm2d: 2-1135               [16, 16, 8, 8]            --
│    └─Scaler: 2-1136                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1137                      [16, 16, 8, 8]            --
│    └─Empty: 2-1138                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1139                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-114        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1140                 [16, 128, 8, 8]           --
│    └─Empty: 2-1141                     [16, 128, 8, 8]           --
│    └─Empty: 2-1142                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1143        --                        --
│    └─One: 2-1144                       [1]                       --
│    └─OutputScale: 2-1145               --                        --
│    └─Empty: 2-1146                     [16, 128, 3, 3]           --
│    └─Empty: 2-1147                     [16, 128, 3, 3]           --
│    └─Empty: 2-1148                     [16]                      --
│    └─Empty: 2-1149                     [16]                      --
│    └─BatchNorm2d: 2-1150               [16, 16, 8, 8]            --
│    └─Scaler: 2-1151                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1152                      [16, 16, 8, 8]            --
│    └─Empty: 2-1153                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1154                     [16, 16, 8, 8]            --
├─Dropout2d: 1-115                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-116               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1155        --                        --
│    └─One: 2-1156                       [1]                       --
│    └─OutputScale: 2-1157               --                        --
│    └─Empty: 2-1158                     [128, 48, 1, 1]           --
│    └─Empty: 2-1159                     [128, 48, 1, 1]           --
│    └─Empty: 2-1160                     [128]                     --
│    └─Empty: 2-1161                     [128]                     --
│    └─BatchNorm2d: 2-1162               [16, 128, 64, 64]         --
│    └─Scaler: 2-1163                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1164                      [16, 128, 64, 64]         --
│    └─Empty: 2-1165                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1166                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-117        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1167                 [16, 128, 32, 32]         --
│    └─Empty: 2-1168                     [16, 128, 32, 32]         --
│    └─Empty: 2-1169                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1170        --                        --
│    └─One: 2-1171                       [1]                       --
│    └─OutputScale: 2-1172               --                        --
│    └─Empty: 2-1173                     [128, 128, 3, 3]          --
│    └─Empty: 2-1174                     [128, 128, 3, 3]          --
│    └─Empty: 2-1175                     [128]                     --
│    └─Empty: 2-1176                     [128]                     --
│    └─BatchNorm2d: 2-1177               [16, 128, 32, 32]         --
│    └─Scaler: 2-1178                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1179                      [16, 128, 32, 32]         --
│    └─Empty: 2-1180                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1181                     [16, 128, 32, 32]         --
├─Dropout2d: 1-118                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-119        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1182                 [16, 128, 16, 16]         --
│    └─Empty: 2-1183                     [16, 128, 16, 16]         --
│    └─Empty: 2-1184                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1185        --                        --
│    └─One: 2-1186                       [1]                       --
│    └─OutputScale: 2-1187               --                        --
│    └─Empty: 2-1188                     [128, 128, 3, 3]          --
│    └─Empty: 2-1189                     [128, 128, 3, 3]          --
│    └─Empty: 2-1190                     [128]                     --
│    └─Empty: 2-1191                     [128]                     --
│    └─BatchNorm2d: 2-1192               [16, 128, 16, 16]         --
│    └─Scaler: 2-1193                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1194                      [16, 128, 16, 16]         --
│    └─Empty: 2-1195                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1196                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-120               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1197        --                        --
│    └─One: 2-1198                       [1]                       --
│    └─OutputScale: 2-1199               --                        --
│    └─Empty: 2-1200                     [128, 128, 1, 1]          --
│    └─Empty: 2-1201                     [128, 128, 1, 1]          --
│    └─Empty: 2-1202                     [128]                     --
│    └─Empty: 2-1203                     [128]                     --
│    └─BatchNorm2d: 2-1204               [16, 128, 16, 16]         --
│    └─Scaler: 2-1205                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1206                      [16, 128, 16, 16]         --
│    └─Empty: 2-1207                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1208                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-121        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1209                 [16, 128, 16, 16]         --
│    └─Empty: 2-1210                     [16, 128, 16, 16]         --
│    └─Empty: 2-1211                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1212        --                        --
│    └─One: 2-1213                       [1]                       --
│    └─OutputScale: 2-1214               --                        --
│    └─Empty: 2-1215                     [128, 128, 3, 3]          --
│    └─Empty: 2-1216                     [128, 128, 3, 3]          --
│    └─Empty: 2-1217                     [128]                     --
│    └─Empty: 2-1218                     [128]                     --
│    └─BatchNorm2d: 2-1219               [16, 128, 16, 16]         --
│    └─Scaler: 2-1220                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1221                      [16, 128, 16, 16]         --
│    └─Empty: 2-1222                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1223                     [16, 128, 16, 16]         --
├─Dropout2d: 1-122                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-123        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1224                 [16, 128, 8, 8]           --
│    └─Empty: 2-1225                     [16, 128, 8, 8]           --
│    └─Empty: 2-1226                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1227        --                        --
│    └─One: 2-1228                       [1]                       --
│    └─OutputScale: 2-1229               --                        --
│    └─Empty: 2-1230                     [128, 128, 3, 3]          --
│    └─Empty: 2-1231                     [128, 128, 3, 3]          --
│    └─Empty: 2-1232                     [128]                     --
│    └─Empty: 2-1233                     [128]                     --
│    └─BatchNorm2d: 2-1234               [16, 128, 8, 8]           --
│    └─Scaler: 2-1235                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1236                      [16, 128, 8, 8]           --
│    └─Empty: 2-1237                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1238                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-124               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1239        --                        --
│    └─One: 2-1240                       [1]                       --
│    └─OutputScale: 2-1241               --                        --
│    └─Empty: 2-1242                     [16, 128, 1, 1]           --
│    └─Empty: 2-1243                     [16, 128, 1, 1]           --
│    └─Empty: 2-1244                     [16]                      --
│    └─Empty: 2-1245                     [16]                      --
│    └─BatchNorm2d: 2-1246               [16, 16, 8, 8]            --
│    └─Scaler: 2-1247                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1248                      [16, 16, 8, 8]            --
│    └─Empty: 2-1249                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1250                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-125        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1251                 [16, 128, 8, 8]           --
│    └─Empty: 2-1252                     [16, 128, 8, 8]           --
│    └─Empty: 2-1253                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1254        --                        --
│    └─One: 2-1255                       [1]                       --
│    └─OutputScale: 2-1256               --                        --
│    └─Empty: 2-1257                     [16, 128, 3, 3]           --
│    └─Empty: 2-1258                     [16, 128, 3, 3]           --
│    └─Empty: 2-1259                     [16]                      --
│    └─Empty: 2-1260                     [16]                      --
│    └─BatchNorm2d: 2-1261               [16, 16, 8, 8]            --
│    └─Scaler: 2-1262                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1263                      [16, 16, 8, 8]            --
│    └─Empty: 2-1264                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1265                     [16, 16, 8, 8]            --
├─Dropout2d: 1-126                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-127               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1266        --                        --
│    └─One: 2-1267                       [1]                       --
│    └─OutputScale: 2-1268               --                        --
│    └─Empty: 2-1269                     [128, 48, 1, 1]           --
│    └─Empty: 2-1270                     [128, 48, 1, 1]           --
│    └─Empty: 2-1271                     [128]                     --
│    └─Empty: 2-1272                     [128]                     --
│    └─BatchNorm2d: 2-1273               [16, 128, 64, 64]         --
│    └─Scaler: 2-1274                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1275                      [16, 128, 64, 64]         --
│    └─Empty: 2-1276                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1277                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-128        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1278                 [16, 128, 32, 32]         --
│    └─Empty: 2-1279                     [16, 128, 32, 32]         --
│    └─Empty: 2-1280                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1281        --                        --
│    └─One: 2-1282                       [1]                       --
│    └─OutputScale: 2-1283               --                        --
│    └─Empty: 2-1284                     [128, 128, 3, 3]          --
│    └─Empty: 2-1285                     [128, 128, 3, 3]          --
│    └─Empty: 2-1286                     [128]                     --
│    └─Empty: 2-1287                     [128]                     --
│    └─BatchNorm2d: 2-1288               [16, 128, 32, 32]         --
│    └─Scaler: 2-1289                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1290                      [16, 128, 32, 32]         --
│    └─Empty: 2-1291                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1292                     [16, 128, 32, 32]         --
├─Dropout2d: 1-129                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-130        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1293                 [16, 128, 16, 16]         --
│    └─Empty: 2-1294                     [16, 128, 16, 16]         --
│    └─Empty: 2-1295                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1296        --                        --
│    └─One: 2-1297                       [1]                       --
│    └─OutputScale: 2-1298               --                        --
│    └─Empty: 2-1299                     [128, 128, 3, 3]          --
│    └─Empty: 2-1300                     [128, 128, 3, 3]          --
│    └─Empty: 2-1301                     [128]                     --
│    └─Empty: 2-1302                     [128]                     --
│    └─BatchNorm2d: 2-1303               [16, 128, 16, 16]         --
│    └─Scaler: 2-1304                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1305                      [16, 128, 16, 16]         --
│    └─Empty: 2-1306                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1307                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-131               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1308        --                        --
│    └─One: 2-1309                       [1]                       --
│    └─OutputScale: 2-1310               --                        --
│    └─Empty: 2-1311                     [128, 128, 1, 1]          --
│    └─Empty: 2-1312                     [128, 128, 1, 1]          --
│    └─Empty: 2-1313                     [128]                     --
│    └─Empty: 2-1314                     [128]                     --
│    └─BatchNorm2d: 2-1315               [16, 128, 16, 16]         --
│    └─Scaler: 2-1316                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1317                      [16, 128, 16, 16]         --
│    └─Empty: 2-1318                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1319                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-132        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1320                 [16, 128, 16, 16]         --
│    └─Empty: 2-1321                     [16, 128, 16, 16]         --
│    └─Empty: 2-1322                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1323        --                        --
│    └─One: 2-1324                       [1]                       --
│    └─OutputScale: 2-1325               --                        --
│    └─Empty: 2-1326                     [128, 128, 3, 3]          --
│    └─Empty: 2-1327                     [128, 128, 3, 3]          --
│    └─Empty: 2-1328                     [128]                     --
│    └─Empty: 2-1329                     [128]                     --
│    └─BatchNorm2d: 2-1330               [16, 128, 16, 16]         --
│    └─Scaler: 2-1331                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1332                      [16, 128, 16, 16]         --
│    └─Empty: 2-1333                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1334                     [16, 128, 16, 16]         --
├─Dropout2d: 1-133                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-134        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1335                 [16, 128, 8, 8]           --
│    └─Empty: 2-1336                     [16, 128, 8, 8]           --
│    └─Empty: 2-1337                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1338        --                        --
│    └─One: 2-1339                       [1]                       --
│    └─OutputScale: 2-1340               --                        --
│    └─Empty: 2-1341                     [128, 128, 3, 3]          --
│    └─Empty: 2-1342                     [128, 128, 3, 3]          --
│    └─Empty: 2-1343                     [128]                     --
│    └─Empty: 2-1344                     [128]                     --
│    └─BatchNorm2d: 2-1345               [16, 128, 8, 8]           --
│    └─Scaler: 2-1346                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1347                      [16, 128, 8, 8]           --
│    └─Empty: 2-1348                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1349                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-135               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1350        --                        --
│    └─One: 2-1351                       [1]                       --
│    └─OutputScale: 2-1352               --                        --
│    └─Empty: 2-1353                     [16, 128, 1, 1]           --
│    └─Empty: 2-1354                     [16, 128, 1, 1]           --
│    └─Empty: 2-1355                     [16]                      --
│    └─Empty: 2-1356                     [16]                      --
│    └─BatchNorm2d: 2-1357               [16, 16, 8, 8]            --
│    └─Scaler: 2-1358                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1359                      [16, 16, 8, 8]            --
│    └─Empty: 2-1360                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1361                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-136        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1362                 [16, 128, 8, 8]           --
│    └─Empty: 2-1363                     [16, 128, 8, 8]           --
│    └─Empty: 2-1364                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1365        --                        --
│    └─One: 2-1366                       [1]                       --
│    └─OutputScale: 2-1367               --                        --
│    └─Empty: 2-1368                     [16, 128, 3, 3]           --
│    └─Empty: 2-1369                     [16, 128, 3, 3]           --
│    └─Empty: 2-1370                     [16]                      --
│    └─Empty: 2-1371                     [16]                      --
│    └─BatchNorm2d: 2-1372               [16, 16, 8, 8]            --
│    └─Scaler: 2-1373                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1374                      [16, 16, 8, 8]            --
│    └─Empty: 2-1375                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1376                     [16, 16, 8, 8]            --
├─Dropout2d: 1-137                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-138               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1377        --                        --
│    └─One: 2-1378                       [1]                       --
│    └─OutputScale: 2-1379               --                        --
│    └─Empty: 2-1380                     [128, 48, 1, 1]           --
│    └─Empty: 2-1381                     [128, 48, 1, 1]           --
│    └─Empty: 2-1382                     [128]                     --
│    └─Empty: 2-1383                     [128]                     --
│    └─BatchNorm2d: 2-1384               [16, 128, 64, 64]         --
│    └─Scaler: 2-1385                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1386                      [16, 128, 64, 64]         --
│    └─Empty: 2-1387                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1388                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-139        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1389                 [16, 128, 32, 32]         --
│    └─Empty: 2-1390                     [16, 128, 32, 32]         --
│    └─Empty: 2-1391                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1392        --                        --
│    └─One: 2-1393                       [1]                       --
│    └─OutputScale: 2-1394               --                        --
│    └─Empty: 2-1395                     [128, 128, 3, 3]          --
│    └─Empty: 2-1396                     [128, 128, 3, 3]          --
│    └─Empty: 2-1397                     [128]                     --
│    └─Empty: 2-1398                     [128]                     --
│    └─BatchNorm2d: 2-1399               [16, 128, 32, 32]         --
│    └─Scaler: 2-1400                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1401                      [16, 128, 32, 32]         --
│    └─Empty: 2-1402                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1403                     [16, 128, 32, 32]         --
├─Dropout2d: 1-140                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-141        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1404                 [16, 128, 16, 16]         --
│    └─Empty: 2-1405                     [16, 128, 16, 16]         --
│    └─Empty: 2-1406                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1407        --                        --
│    └─One: 2-1408                       [1]                       --
│    └─OutputScale: 2-1409               --                        --
│    └─Empty: 2-1410                     [128, 128, 3, 3]          --
│    └─Empty: 2-1411                     [128, 128, 3, 3]          --
│    └─Empty: 2-1412                     [128]                     --
│    └─Empty: 2-1413                     [128]                     --
│    └─BatchNorm2d: 2-1414               [16, 128, 16, 16]         --
│    └─Scaler: 2-1415                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1416                      [16, 128, 16, 16]         --
│    └─Empty: 2-1417                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1418                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-142               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1419        --                        --
│    └─One: 2-1420                       [1]                       --
│    └─OutputScale: 2-1421               --                        --
│    └─Empty: 2-1422                     [128, 128, 1, 1]          --
│    └─Empty: 2-1423                     [128, 128, 1, 1]          --
│    └─Empty: 2-1424                     [128]                     --
│    └─Empty: 2-1425                     [128]                     --
│    └─BatchNorm2d: 2-1426               [16, 128, 16, 16]         --
│    └─Scaler: 2-1427                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1428                      [16, 128, 16, 16]         --
│    └─Empty: 2-1429                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1430                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-143        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1431                 [16, 128, 16, 16]         --
│    └─Empty: 2-1432                     [16, 128, 16, 16]         --
│    └─Empty: 2-1433                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1434        --                        --
│    └─One: 2-1435                       [1]                       --
│    └─OutputScale: 2-1436               --                        --
│    └─Empty: 2-1437                     [128, 128, 3, 3]          --
│    └─Empty: 2-1438                     [128, 128, 3, 3]          --
│    └─Empty: 2-1439                     [128]                     --
│    └─Empty: 2-1440                     [128]                     --
│    └─BatchNorm2d: 2-1441               [16, 128, 16, 16]         --
│    └─Scaler: 2-1442                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1443                      [16, 128, 16, 16]         --
│    └─Empty: 2-1444                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1445                     [16, 128, 16, 16]         --
├─Dropout2d: 1-144                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-145        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1446                 [16, 128, 8, 8]           --
│    └─Empty: 2-1447                     [16, 128, 8, 8]           --
│    └─Empty: 2-1448                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1449        --                        --
│    └─One: 2-1450                       [1]                       --
│    └─OutputScale: 2-1451               --                        --
│    └─Empty: 2-1452                     [128, 128, 3, 3]          --
│    └─Empty: 2-1453                     [128, 128, 3, 3]          --
│    └─Empty: 2-1454                     [128]                     --
│    └─Empty: 2-1455                     [128]                     --
│    └─BatchNorm2d: 2-1456               [16, 128, 8, 8]           --
│    └─Scaler: 2-1457                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1458                      [16, 128, 8, 8]           --
│    └─Empty: 2-1459                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1460                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-146               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1461        --                        --
│    └─One: 2-1462                       [1]                       --
│    └─OutputScale: 2-1463               --                        --
│    └─Empty: 2-1464                     [16, 128, 1, 1]           --
│    └─Empty: 2-1465                     [16, 128, 1, 1]           --
│    └─Empty: 2-1466                     [16]                      --
│    └─Empty: 2-1467                     [16]                      --
│    └─BatchNorm2d: 2-1468               [16, 16, 8, 8]            --
│    └─Scaler: 2-1469                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1470                      [16, 16, 8, 8]            --
│    └─Empty: 2-1471                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1472                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-147        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1473                 [16, 128, 8, 8]           --
│    └─Empty: 2-1474                     [16, 128, 8, 8]           --
│    └─Empty: 2-1475                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1476        --                        --
│    └─One: 2-1477                       [1]                       --
│    └─OutputScale: 2-1478               --                        --
│    └─Empty: 2-1479                     [16, 128, 3, 3]           --
│    └─Empty: 2-1480                     [16, 128, 3, 3]           --
│    └─Empty: 2-1481                     [16]                      --
│    └─Empty: 2-1482                     [16]                      --
│    └─BatchNorm2d: 2-1483               [16, 16, 8, 8]            --
│    └─Scaler: 2-1484                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1485                      [16, 16, 8, 8]            --
│    └─Empty: 2-1486                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1487                     [16, 16, 8, 8]            --
├─Dropout2d: 1-148                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-149               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1488        --                        --
│    └─One: 2-1489                       [1]                       --
│    └─OutputScale: 2-1490               --                        --
│    └─Empty: 2-1491                     [128, 48, 1, 1]           --
│    └─Empty: 2-1492                     [128, 48, 1, 1]           --
│    └─Empty: 2-1493                     [128]                     --
│    └─Empty: 2-1494                     [128]                     --
│    └─BatchNorm2d: 2-1495               [16, 128, 64, 64]         --
│    └─Scaler: 2-1496                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1497                      [16, 128, 64, 64]         --
│    └─Empty: 2-1498                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1499                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-150        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1500                 [16, 128, 32, 32]         --
│    └─Empty: 2-1501                     [16, 128, 32, 32]         --
│    └─Empty: 2-1502                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1503        --                        --
│    └─One: 2-1504                       [1]                       --
│    └─OutputScale: 2-1505               --                        --
│    └─Empty: 2-1506                     [128, 128, 3, 3]          --
│    └─Empty: 2-1507                     [128, 128, 3, 3]          --
│    └─Empty: 2-1508                     [128]                     --
│    └─Empty: 2-1509                     [128]                     --
│    └─BatchNorm2d: 2-1510               [16, 128, 32, 32]         --
│    └─Scaler: 2-1511                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1512                      [16, 128, 32, 32]         --
│    └─Empty: 2-1513                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1514                     [16, 128, 32, 32]         --
├─Dropout2d: 1-151                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-152        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1515                 [16, 128, 16, 16]         --
│    └─Empty: 2-1516                     [16, 128, 16, 16]         --
│    └─Empty: 2-1517                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1518        --                        --
│    └─One: 2-1519                       [1]                       --
│    └─OutputScale: 2-1520               --                        --
│    └─Empty: 2-1521                     [128, 128, 3, 3]          --
│    └─Empty: 2-1522                     [128, 128, 3, 3]          --
│    └─Empty: 2-1523                     [128]                     --
│    └─Empty: 2-1524                     [128]                     --
│    └─BatchNorm2d: 2-1525               [16, 128, 16, 16]         --
│    └─Scaler: 2-1526                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1527                      [16, 128, 16, 16]         --
│    └─Empty: 2-1528                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1529                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-153               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1530        --                        --
│    └─One: 2-1531                       [1]                       --
│    └─OutputScale: 2-1532               --                        --
│    └─Empty: 2-1533                     [128, 128, 1, 1]          --
│    └─Empty: 2-1534                     [128, 128, 1, 1]          --
│    └─Empty: 2-1535                     [128]                     --
│    └─Empty: 2-1536                     [128]                     --
│    └─BatchNorm2d: 2-1537               [16, 128, 16, 16]         --
│    └─Scaler: 2-1538                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1539                      [16, 128, 16, 16]         --
│    └─Empty: 2-1540                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1541                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-154        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1542                 [16, 128, 16, 16]         --
│    └─Empty: 2-1543                     [16, 128, 16, 16]         --
│    └─Empty: 2-1544                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1545        --                        --
│    └─One: 2-1546                       [1]                       --
│    └─OutputScale: 2-1547               --                        --
│    └─Empty: 2-1548                     [128, 128, 3, 3]          --
│    └─Empty: 2-1549                     [128, 128, 3, 3]          --
│    └─Empty: 2-1550                     [128]                     --
│    └─Empty: 2-1551                     [128]                     --
│    └─BatchNorm2d: 2-1552               [16, 128, 16, 16]         --
│    └─Scaler: 2-1553                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1554                      [16, 128, 16, 16]         --
│    └─Empty: 2-1555                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1556                     [16, 128, 16, 16]         --
├─Dropout2d: 1-155                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-156        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1557                 [16, 128, 8, 8]           --
│    └─Empty: 2-1558                     [16, 128, 8, 8]           --
│    └─Empty: 2-1559                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1560        --                        --
│    └─One: 2-1561                       [1]                       --
│    └─OutputScale: 2-1562               --                        --
│    └─Empty: 2-1563                     [128, 128, 3, 3]          --
│    └─Empty: 2-1564                     [128, 128, 3, 3]          --
│    └─Empty: 2-1565                     [128]                     --
│    └─Empty: 2-1566                     [128]                     --
│    └─BatchNorm2d: 2-1567               [16, 128, 8, 8]           --
│    └─Scaler: 2-1568                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1569                      [16, 128, 8, 8]           --
│    └─Empty: 2-1570                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1571                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-157               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1572        --                        --
│    └─One: 2-1573                       [1]                       --
│    └─OutputScale: 2-1574               --                        --
│    └─Empty: 2-1575                     [16, 128, 1, 1]           --
│    └─Empty: 2-1576                     [16, 128, 1, 1]           --
│    └─Empty: 2-1577                     [16]                      --
│    └─Empty: 2-1578                     [16]                      --
│    └─BatchNorm2d: 2-1579               [16, 16, 8, 8]            --
│    └─Scaler: 2-1580                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1581                      [16, 16, 8, 8]            --
│    └─Empty: 2-1582                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1583                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-158        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1584                 [16, 128, 8, 8]           --
│    └─Empty: 2-1585                     [16, 128, 8, 8]           --
│    └─Empty: 2-1586                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1587        --                        --
│    └─One: 2-1588                       [1]                       --
│    └─OutputScale: 2-1589               --                        --
│    └─Empty: 2-1590                     [16, 128, 3, 3]           --
│    └─Empty: 2-1591                     [16, 128, 3, 3]           --
│    └─Empty: 2-1592                     [16]                      --
│    └─Empty: 2-1593                     [16]                      --
│    └─BatchNorm2d: 2-1594               [16, 16, 8, 8]            --
│    └─Scaler: 2-1595                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1596                      [16, 16, 8, 8]            --
│    └─Empty: 2-1597                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1598                     [16, 16, 8, 8]            --
├─Dropout2d: 1-159                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-160               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1599        --                        --
│    └─One: 2-1600                       [1]                       --
│    └─OutputScale: 2-1601               --                        --
│    └─Empty: 2-1602                     [128, 48, 1, 1]           --
│    └─Empty: 2-1603                     [128, 48, 1, 1]           --
│    └─Empty: 2-1604                     [128]                     --
│    └─Empty: 2-1605                     [128]                     --
│    └─BatchNorm2d: 2-1606               [16, 128, 64, 64]         --
│    └─Scaler: 2-1607                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1608                      [16, 128, 64, 64]         --
│    └─Empty: 2-1609                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1610                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-161        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1611                 [16, 128, 32, 32]         --
│    └─Empty: 2-1612                     [16, 128, 32, 32]         --
│    └─Empty: 2-1613                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1614        --                        --
│    └─One: 2-1615                       [1]                       --
│    └─OutputScale: 2-1616               --                        --
│    └─Empty: 2-1617                     [128, 128, 3, 3]          --
│    └─Empty: 2-1618                     [128, 128, 3, 3]          --
│    └─Empty: 2-1619                     [128]                     --
│    └─Empty: 2-1620                     [128]                     --
│    └─BatchNorm2d: 2-1621               [16, 128, 32, 32]         --
│    └─Scaler: 2-1622                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1623                      [16, 128, 32, 32]         --
│    └─Empty: 2-1624                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1625                     [16, 128, 32, 32]         --
├─Dropout2d: 1-162                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-163        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1626                 [16, 128, 16, 16]         --
│    └─Empty: 2-1627                     [16, 128, 16, 16]         --
│    └─Empty: 2-1628                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1629        --                        --
│    └─One: 2-1630                       [1]                       --
│    └─OutputScale: 2-1631               --                        --
│    └─Empty: 2-1632                     [128, 128, 3, 3]          --
│    └─Empty: 2-1633                     [128, 128, 3, 3]          --
│    └─Empty: 2-1634                     [128]                     --
│    └─Empty: 2-1635                     [128]                     --
│    └─BatchNorm2d: 2-1636               [16, 128, 16, 16]         --
│    └─Scaler: 2-1637                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1638                      [16, 128, 16, 16]         --
│    └─Empty: 2-1639                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1640                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-164               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1641        --                        --
│    └─One: 2-1642                       [1]                       --
│    └─OutputScale: 2-1643               --                        --
│    └─Empty: 2-1644                     [128, 128, 1, 1]          --
│    └─Empty: 2-1645                     [128, 128, 1, 1]          --
│    └─Empty: 2-1646                     [128]                     --
│    └─Empty: 2-1647                     [128]                     --
│    └─BatchNorm2d: 2-1648               [16, 128, 16, 16]         --
│    └─Scaler: 2-1649                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1650                      [16, 128, 16, 16]         --
│    └─Empty: 2-1651                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1652                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-165        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1653                 [16, 128, 16, 16]         --
│    └─Empty: 2-1654                     [16, 128, 16, 16]         --
│    └─Empty: 2-1655                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1656        --                        --
│    └─One: 2-1657                       [1]                       --
│    └─OutputScale: 2-1658               --                        --
│    └─Empty: 2-1659                     [128, 128, 3, 3]          --
│    └─Empty: 2-1660                     [128, 128, 3, 3]          --
│    └─Empty: 2-1661                     [128]                     --
│    └─Empty: 2-1662                     [128]                     --
│    └─BatchNorm2d: 2-1663               [16, 128, 16, 16]         --
│    └─Scaler: 2-1664                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1665                      [16, 128, 16, 16]         --
│    └─Empty: 2-1666                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1667                     [16, 128, 16, 16]         --
├─Dropout2d: 1-166                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-167        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1668                 [16, 128, 8, 8]           --
│    └─Empty: 2-1669                     [16, 128, 8, 8]           --
│    └─Empty: 2-1670                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1671        --                        --
│    └─One: 2-1672                       [1]                       --
│    └─OutputScale: 2-1673               --                        --
│    └─Empty: 2-1674                     [128, 128, 3, 3]          --
│    └─Empty: 2-1675                     [128, 128, 3, 3]          --
│    └─Empty: 2-1676                     [128]                     --
│    └─Empty: 2-1677                     [128]                     --
│    └─BatchNorm2d: 2-1678               [16, 128, 8, 8]           --
│    └─Scaler: 2-1679                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1680                      [16, 128, 8, 8]           --
│    └─Empty: 2-1681                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1682                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-168               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1683        --                        --
│    └─One: 2-1684                       [1]                       --
│    └─OutputScale: 2-1685               --                        --
│    └─Empty: 2-1686                     [16, 128, 1, 1]           --
│    └─Empty: 2-1687                     [16, 128, 1, 1]           --
│    └─Empty: 2-1688                     [16]                      --
│    └─Empty: 2-1689                     [16]                      --
│    └─BatchNorm2d: 2-1690               [16, 16, 8, 8]            --
│    └─Scaler: 2-1691                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1692                      [16, 16, 8, 8]            --
│    └─Empty: 2-1693                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1694                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-169        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1695                 [16, 128, 8, 8]           --
│    └─Empty: 2-1696                     [16, 128, 8, 8]           --
│    └─Empty: 2-1697                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1698        --                        --
│    └─One: 2-1699                       [1]                       --
│    └─OutputScale: 2-1700               --                        --
│    └─Empty: 2-1701                     [16, 128, 3, 3]           --
│    └─Empty: 2-1702                     [16, 128, 3, 3]           --
│    └─Empty: 2-1703                     [16]                      --
│    └─Empty: 2-1704                     [16]                      --
│    └─BatchNorm2d: 2-1705               [16, 16, 8, 8]            --
│    └─Scaler: 2-1706                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1707                      [16, 16, 8, 8]            --
│    └─Empty: 2-1708                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1709                     [16, 16, 8, 8]            --
├─Dropout2d: 1-170                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-171               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1710        --                        --
│    └─One: 2-1711                       [1]                       --
│    └─OutputScale: 2-1712               --                        --
│    └─Empty: 2-1713                     [128, 48, 1, 1]           --
│    └─Empty: 2-1714                     [128, 48, 1, 1]           --
│    └─Empty: 2-1715                     [128]                     --
│    └─Empty: 2-1716                     [128]                     --
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Empty: 2-1720                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1721                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Empty: 2-1723                     [16, 128, 32, 32]         --
│    └─Empty: 2-1724                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1725        --                        --
│    └─One: 2-1726                       [1]                       --
│    └─OutputScale: 2-1727               --                        --
│    └─Empty: 2-1728                     [128, 128, 3, 3]          --
│    └─Empty: 2-1729                     [128, 128, 3, 3]          --
│    └─Empty: 2-1730                     [128]                     --
│    └─Empty: 2-1731                     [128]                     --
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         --
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─Empty: 2-1735                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1736                     [16, 128, 32, 32]         --
├─Dropout2d: 1-173                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1740        --                        --
│    └─One: 2-1741                       [1]                       --
│    └─OutputScale: 2-1742               --                        --
│    └─Empty: 2-1743                     [128, 128, 3, 3]          --
│    └─Empty: 2-1744                     [128, 128, 3, 3]          --
│    └─Empty: 2-1745                     [128]                     --
│    └─Empty: 2-1746                     [128]                     --
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─Empty: 2-1750                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1751                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-175               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1752        --                        --
│    └─One: 2-1753                       [1]                       --
│    └─OutputScale: 2-1754               --                        --
│    └─Empty: 2-1755                     [128, 128, 1, 1]          --
│    └─Empty: 2-1756                     [128, 128, 1, 1]          --
│    └─Empty: 2-1757                     [128]                     --
│    └─Empty: 2-1758                     [128]                     --
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Empty: 2-1762                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1763                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Empty: 2-1765                     [16, 128, 16, 16]         --
│    └─Empty: 2-1766                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1767        --                        --
│    └─One: 2-1768                       [1]                       --
│    └─OutputScale: 2-1769               --                        --
│    └─Empty: 2-1770                     [128, 128, 3, 3]          --
│    └─Empty: 2-1771                     [128, 128, 3, 3]          --
│    └─Empty: 2-1772                     [128]                     --
│    └─Empty: 2-1773                     [128]                     --
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─Empty: 2-1777                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1778                     [16, 128, 16, 16]         --
├─Dropout2d: 1-177                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1782        --                        --
│    └─One: 2-1783                       [1]                       --
│    └─OutputScale: 2-1784               --                        --
│    └─Empty: 2-1785                     [128, 128, 3, 3]          --
│    └─Empty: 2-1786                     [128, 128, 3, 3]          --
│    └─Empty: 2-1787                     [128]                     --
│    └─Empty: 2-1788                     [128]                     --
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─Empty: 2-1792                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1793                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-179               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1794        --                        --
│    └─One: 2-1795                       [1]                       --
│    └─OutputScale: 2-1796               --                        --
│    └─Empty: 2-1797                     [16, 128, 1, 1]           --
│    └─Empty: 2-1798                     [16, 128, 1, 1]           --
│    └─Empty: 2-1799                     [16]                      --
│    └─Empty: 2-1800                     [16]                      --
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Empty: 2-1804                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1805                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1809        --                        --
│    └─One: 2-1810                       [1]                       --
│    └─OutputScale: 2-1811               --                        --
│    └─Empty: 2-1812                     [16, 128, 3, 3]           --
│    └─Empty: 2-1813                     [16, 128, 3, 3]           --
│    └─Empty: 2-1814                     [16]                      --
│    └─Empty: 2-1815                     [16]                      --
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─Empty: 2-1819                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1820                     [16, 16, 8, 8]            --
├─Dropout2d: 1-181                       [16, 16, 8, 8]            --
├─Linear: 1-182                          [16, 5]                   5,126
│    └─OutputShiftSqueeze: 2-1821        --                        --
│    └─One: 2-1822                       [1]                       --
│    └─OutputScale: 2-1823               --                        --
│    └─Empty: 2-1824                     [5, 1024]                 --
│    └─Empty: 2-1825                     [5, 1024]                 --
│    └─Empty: 2-1826                     [16, 5]                   --
│    └─Empty: 2-1827                     [16, 5]                   --
│    └─Clamp: 2-1828                     [16, 5]                   --
├─Linear: 1-183                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1829        --                        --
│    └─One: 2-1830                       [1]                       --
│    └─OutputScale: 2-1831               --                        --
│    └─Empty: 2-1832                     [5, 1024]                 --
│    └─Empty: 2-1833                     [5, 1024]                 --
│    └─Empty: 2-1834                     [16, 5]                   --
│    └─Empty: 2-1835                     [16, 5]                   --
│    └─Clamp: 2-1836                     [16, 5]                   --
├─Linear: 1-184                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1837        --                        --
│    └─One: 2-1838                       [1]                       --
│    └─OutputScale: 2-1839               --                        --
│    └─Empty: 2-1840                     [5, 1024]                 --
│    └─Empty: 2-1841                     [5, 1024]                 --
│    └─Empty: 2-1842                     [16, 5]                   --
│    └─Empty: 2-1843                     [16, 5]                   --
│    └─Clamp: 2-1844                     [16, 5]                   --
├─Linear: 1-185                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1845        --                        --
│    └─One: 2-1846                       [1]                       --
│    └─OutputScale: 2-1847               --                        --
│    └─Empty: 2-1848                     [5, 1024]                 --
│    └─Empty: 2-1849                     [5, 1024]                 --
│    └─Empty: 2-1850                     [16, 5]                   --
│    └─Empty: 2-1851                     [16, 5]                   --
│    └─Clamp: 2-1852                     [16, 5]                   --
├─Linear: 1-186                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1853        --                        --
│    └─One: 2-1854                       [1]                       --
│    └─OutputScale: 2-1855               --                        --
│    └─Empty: 2-1856                     [5, 1024]                 --
│    └─Empty: 2-1857                     [5, 1024]                 --
│    └─Empty: 2-1858                     [16, 5]                   --
│    └─Empty: 2-1859                     [16, 5]                   --
│    └─Clamp: 2-1860                     [16, 5]                   --
├─Linear: 1-187                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1861        --                        --
│    └─One: 2-1862                       [1]                       --
│    └─OutputScale: 2-1863               --                        --
│    └─Empty: 2-1864                     [5, 1024]                 --
│    └─Empty: 2-1865                     [5, 1024]                 --
│    └─Empty: 2-1866                     [16, 5]                   --
│    └─Empty: 2-1867                     [16, 5]                   --
│    └─Clamp: 2-1868                     [16, 5]                   --
├─Linear: 1-188                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1869        --                        --
│    └─One: 2-1870                       [1]                       --
│    └─OutputScale: 2-1871               --                        --
│    └─Empty: 2-1872                     [5, 1024]                 --
│    └─Empty: 2-1873                     [5, 1024]                 --
│    └─Empty: 2-1874                     [16, 5]                   --
│    └─Empty: 2-1875                     [16, 5]                   --
│    └─Clamp: 2-1876                     [16, 5]                   --
├─Linear: 1-189                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1877        --                        --
│    └─One: 2-1878                       [1]                       --
│    └─OutputScale: 2-1879               --                        --
│    └─Empty: 2-1880                     [5, 1024]                 --
│    └─Empty: 2-1881                     [5, 1024]                 --
│    └─Empty: 2-1882                     [16, 5]                   --
│    └─Empty: 2-1883                     [16, 5]                   --
│    └─Clamp: 2-1884                     [16, 5]                   --
├─Linear: 1-190                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1885        --                        --
│    └─One: 2-1886                       [1]                       --
│    └─OutputScale: 2-1887               --                        --
│    └─Empty: 2-1888                     [5, 1024]                 --
│    └─Empty: 2-1889                     [5, 1024]                 --
│    └─Empty: 2-1890                     [16, 5]                   --
│    └─Empty: 2-1891                     [16, 5]                   --
│    └─Clamp: 2-1892                     [16, 5]                   --
├─Linear: 1-191                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1893        --                        --
│    └─One: 2-1894                       [1]                       --
│    └─OutputScale: 2-1895               --                        --
│    └─Empty: 2-1896                     [5, 1024]                 --
│    └─Empty: 2-1897                     [5, 1024]                 --
│    └─Empty: 2-1898                     [16, 5]                   --
│    └─Empty: 2-1899                     [16, 5]                   --
│    └─Clamp: 2-1900                     [16, 5]                   --
├─Linear: 1-192                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1901        --                        --
│    └─One: 2-1902                       [1]                       --
│    └─OutputScale: 2-1903               --                        --
│    └─Empty: 2-1904                     [5, 1024]                 --
│    └─Empty: 2-1905                     [5, 1024]                 --
│    └─Empty: 2-1906                     [16, 5]                   --
│    └─Empty: 2-1907                     [16, 5]                   --
│    └─Clamp: 2-1908                     [16, 5]                   --
├─Linear: 1-193                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1909        --                        --
│    └─One: 2-1910                       [1]                       --
│    └─OutputScale: 2-1911               --                        --
│    └─Empty: 2-1912                     [5, 1024]                 --
│    └─Empty: 2-1913                     [5, 1024]                 --
│    └─Empty: 2-1914                     [16, 5]                   --
│    └─Empty: 2-1915                     [16, 5]                   --
│    └─Clamp: 2-1916                     [16, 5]                   --
├─Linear: 1-194                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1917        --                        --
│    └─One: 2-1918                       [1]                       --
│    └─OutputScale: 2-1919               --                        --
│    └─Empty: 2-1920                     [5, 1024]                 --
│    └─Empty: 2-1921                     [5, 1024]                 --
│    └─Empty: 2-1922                     [16, 5]                   --
│    └─Empty: 2-1923                     [16, 5]                   --
│    └─Clamp: 2-1924                     [16, 5]                   --
├─Linear: 1-195                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1925        --                        --
│    └─One: 2-1926                       [1]                       --
│    └─OutputScale: 2-1927               --                        --
│    └─Empty: 2-1928                     [5, 1024]                 --
│    └─Empty: 2-1929                     [5, 1024]                 --
│    └─Empty: 2-1930                     [16, 5]                   --
│    └─Empty: 2-1931                     [16, 5]                   --
│    └─Clamp: 2-1932                     [16, 5]                   --
├─Linear: 1-196                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1933        --                        --
│    └─One: 2-1934                       [1]                       --
│    └─OutputScale: 2-1935               --                        --
│    └─Empty: 2-1936                     [5, 1024]                 --
│    └─Empty: 2-1937                     [5, 1024]                 --
│    └─Empty: 2-1938                     [16, 5]                   --
│    └─Empty: 2-1939                     [16, 5]                   --
│    └─Clamp: 2-1940                     [16, 5]                   --
├─Linear: 1-197                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1941        --                        --
│    └─One: 2-1942                       [1]                       --
│    └─OutputScale: 2-1943               --                        --
│    └─Empty: 2-1944                     [5, 1024]                 --
│    └─Empty: 2-1945                     [5, 1024]                 --
│    └─Empty: 2-1946                     [16, 5]                   --
│    └─Empty: 2-1947                     [16, 5]                   --
│    └─Clamp: 2-1948                     [16, 5]                   --
==========================================================================================
Total params: 638,806
Trainable params: 638,752
Non-trainable params: 54
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 0.00
Params size (MB): 2.53
Estimated Total Size (MB): 203.86
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.490 | Acc: 25.500% | Wgt Acc: 31.577%
	I - Batch: 100 | Loss: 1.409 | Acc: 29.812% | Wgt Acc: 37.910%
	I - Batch: 150 | Loss: 1.363 | Acc: 32.292% | Wgt Acc: 41.120%
	I - Batch: 200 | Loss: 1.326 | Acc: 33.438% | Wgt Acc: 42.629%
I - num batch: 222
I - Train -- Loss: 1.311 | Acc: 34.198% | Wgt Acc: 43.656% | LR: 1.000000e-03 | Dur: 135.34s
I - Confusion Matrix: [row->prediction - col->label]
[[386.  49.  97. 211. 194.]
 [ 43. 234. 148.  41. 231.]
 [147. 257. 420. 113. 491.]
 [121.  38.  69. 173.  84.]
 [  0.   0.   0.   0.   0.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.580 | Acc: 28.402% | Wgt Acc: 39.015% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[37.  1.  2. 17.  9.]
 [11. 55. 27. 22. 86.]
 [34. 22. 44. 39. 84.]
 [ 6.  0.  2.  8.  1.]
 [ 0.  0.  0.  0.  0.]]

I - Local maximum validation set accuracy:  28.40

I - Validation set results: 
[14-1-1-1.16][50-3-1-0.06][124-2-2-1.21][127-0-1-0.15][443-2-2-1.65][567-0-2-0.18][573-1-1-1.63][615-0-2-0.24][695-1-2-1.28][722-3-0-0.82]
[826-0-2-0.20][878-0-2--0.02][1103-0-2-0.42][1212-3-2-0.93][1368-0-2-0.61][2181-2-1-0.58][2476-2-2-0.56][2721-2-2-1.33][2818-1-1-0.54][2886-2-1-1.81]
[3231-2-1-1.67][3333-2-1-0.82][3482-2-2-1.15][3536-3-1-1.05][3625-1-1-1.41][3909-0-2-0.49][4035-0-3-1.37][4140-0-0-0.90][4214-1-2-0.74][4346-1-0-0.23]
[4581-2-2-0.89][4708-3-2-0.74][4838-3-1-0.59][4845-1-1-1.35][4868-0-0-0.62][4939-0-1-1.05][4984-2-2-0.91][5078-1-1-1.22][5396-0-0-1.23][5479-1-1-1.95]
[5717-0-0-0.63][5843-1-1-2.13][5949-3-0-0.18][5987-2-1-1.33][6014-3-1-1.06][6033-3-0-0.31][6313-0-0-0.65][6421-3-2-0.79][6500-1-1-2.10][6583-3-2-0.37]
[6683-3-2-0.23][6825-2-2-0.07][6998-3-2-0.68][7049-3-2-0.80][7517-1-1-1.73][7521-1-1-1.22][7528-1-1-1.27][7949-1-2-1.24][8135-1-2-0.15][8185-3-0-0.56]
[8269-3-2-0.49][8273-3-2-0.31][8543-3-0-1.27][8666-1-1-1.15][8672-0-0-0.49][8903-1-1-0.95][9001-2-1-1.96][9036-2-2-1.36][9281-3-1-0.58][9300-2-2-0.74]
[9571-0-2-0.43][9617-1-1-1.42][9644-2-2-1.28][9705-2-1-1.03][9801-0-2-0.60][9803-3-3-0.00][9865-3-0-0.12][9896-2-2-1.17][10314-1-2-0.81][10337-3-3-0.58]
[10403-0-2-0.87][10653-2-1-1.36][10704-2-1-1.65][10719-1-1-2.03][10727-1-1-1.64][10836-0-0-2.48][10969-2-2-0.68][11042-0-0-0.06][11088-1-1-2.21][11322-0-0-0.91]
[11398-2-2-1.51][11499-0-2-0.35][11502-3-2-0.47][11512-3-1-1.32][11608-1-1-2.10][11610-0-2-0.03][11692-0-2-0.53][11905-0-0-0.96][11993-1-2-1.03][12002-2-3-0.07]
[12052-0-0-0.51][12201-0-0-0.30][12235-2-1-1.37][12320-1-1-0.49][12377-2-1-1.03][12398-2-2-0.82][12503-1-2-0.60][12617-0-1-2.19][12685-3-1-1.46][12738-2-2-0.26]
[12742-2-2-1.61][12823-0-3-0.65][13110-1-1-0.98][13240-3-2-0.51][13253-1-1-1.60][13273-0-0-0.51][13634-1-1-1.32][13763-2-1-0.65][13905-3-2-0.30][14060-2-1-2.53]
[14065-3-2-0.12][14147-3-2-0.40][14595-2-2-1.66][14687-2-1-1.48][14788-2-2-0.88][14869-1-1-1.15][14872-3-0-0.14][14877-1-1-1.40][14927-0-2-0.96][15066-0-0-1.89]
[15175-1-1-1.10][15178-2-2-0.47][15375-3-0-0.30][15389-3-2-0.36][15568-2-1-1.81][15675-3-3-0.22][15869-1-2-0.87][16207-3-2-0.06][16236-0-2-0.75][16302-3-2-0.82]
[16331-2-2-1.39][16381-0-2-0.32][16488-1-1-1.91][16495-0-0-1.43][16650-0-0-0.75][16719-1-1-1.27][16801-0-0-2.19][16828-0-0-0.41][17137-3-2--0.06][17245-1-1-1.03]
[17278-3-1-0.32][17282-0-1-0.70][17311-2-2-1.28][17336-2-2-1.03][17608-3-0-0.93][17627-0-2-0.56][17877-3-1-1.10][17924-1-2-0.98][17984-3-0-1.33][18211-0-1-0.84]
[18276-3-0-0.50][18287-1-1-0.38][18394-0-2-0.24][18428-0-0-1.12][18442-0-1-0.34][18478-3-0-0.12][18607-0-0-0.32][18616-0-1-0.77][18663-0-3-0.52][18718-0-0-0.38]
[18766-2-2-1.84][18824-2-1-1.08][18890-3-2-1.09][18930-3-2-0.47][18938-3-2-0.74][19817-1-2-1.12][19839-0-2-1.12][19930-3-1-0.46][19944-0-2-0.52][20036-2-2-1.25]
[20101-3-2-0.15][20474-1-2-1.30][20547-3-1-0.68][20929-2-2-1.57][21245-1-1-1.82][21257-3-1-1.22][21293-1-1-2.32][21316-1-1-2.19][21384-1-1-1.57][21448-1-1-1.20]
[21483-0-0-0.40][21487-2-1-1.18][21714-0-2-0.38][21943-3-2-1.15][21947-0-2-0.01][21948-0-0-2.58][21965-2-2-0.95][21998-1-2-1.23][22025-0-2-0.66][22228-3-3-0.67]
[22446-1-1-2.76][22494-3-2-0.09][22757-0-0-1.99][22811-3-2-0.28][22976-3-1-1.31][22985-3-3-0.23][23014-0-2-0.30][23112-1-1-1.73][23144-3-0-0.64][23168-2-0-0.18]
[23219-0-2-0.38][23363-3-3-0.60][23470-0-1-0.39][23486-2-2-0.81][23497-0-0-0.99][23516-0-0-1.86][23690-1-1-0.85][23921-2-2-1.19][23936-1-2-1.43][24040-3-2-0.58]
[24111-1-1-1.28][24182-0-0-0.88][24238-3-1-0.69][24290-2-0-0.04][24345-0-0-1.28][24364-1-2-1.09][24427-3-1-0.40][24477-2-2-1.33][24495-2-1-0.94][24893-2-1-1.41]
[25012-1-1-1.04][25121-2-2-1.14][25165-3-2-0.18][25183-0-0-0.68][25297-3-1-0.90][25398-0-0-0.23][25574-2-2-0.93][25644-1-1-1.46][25718-1-1-2.32][25774-2-2-0.81]
[26032-3-2-0.34][26051-3-0-0.32][26120-0-2-0.36][26321-1-1-0.85][26732-1-1-1.18][26784-3-3-0.23][26827-3-1-1.03][26833-0-3-0.68][26838-2-1-0.78][26860-1-2-1.35]
[26948-0-2-0.38][27049-3-0-0.67][27098-1-2-0.47][27526-0-0-0.45][27639-3-1-0.74][27698-3-0-0.10][27772-0-0-1.04][27890-1-1-1.48][28040-0-2-0.48][28503-2-2-2.12]
[28577-1-1-1.58][28959-0-0-2.05][29198-3-1-0.95][29777-0-0-1.30][29877-2-1-0.84][30035-1-2-1.24][30098-0-3-0.21][30326-1-1-1.67][30572-2-2-0.90][30716-0-1-0.41]
[30806-2-1-1.14][30906-1-1-1.09][31007-0-0-0.60][31181-3-2-1.03][31238-0-3-0.38][31347-0-2-0.43][31422-2-1-1.30][31429-3-2-0.13][31431-0-2-0.21][31432-1-1-1.64]
[31477-0-0-0.40][31524-1-2-0.03][31597-1-2-1.87][31619-1-2-1.10][31701-0-2-0.63][31755-0-2-0.77][31854-3-1-0.68][32074-1-1-0.65][32078-3-2-0.77][32111-1-1-0.67]
[32127-1-2-2.39][32140-3-2-0.73][32263-2-2-0.62][32365-0-0-0.11][32411-2-3-0.43][32429-3-0-1.50][32473-3-2-0.42][32574-3-3-1.68][32584-0-2-1.12][32622-0-1-1.15]
[32858-3-2-0.79][32969-3-2-0.30][33016-2-2-1.04][33031-1-1-0.13][33035-2-2-2.01][33133-2-2-1.10][33173-2-1-1.48][33175-3-2-1.63][33306-3-1-1.24][33309-2-2-0.64]
[33474-0-1-0.33][33478-2-2-1.18][33618-1-1-1.49][33712-0-2-0.20][33782-2-1-1.61][33914-3-1-1.12][34076-3-2-1.36][34112-2-1-2.13][34138-2-2-1.19][34239-1-2-1.26]
[34364-2-2-1.63][34617-1-1-1.16][34751-3-2-0.94][34783-2-1-1.08][35015-3-2-0.97][35018-1-1-1.61][35288-2-2-1.25][0-4-2-0.97][1-4-2-0.87][2-4-2-0.77]
[3-4-2-0.92][4-4-2-0.90][5-4-1-2.30][6-4-1-0.10][7-4-1-0.91][8-4-2-1.33][9-4-1-2.04][10-4-2-0.70][11-4-2-1.32][12-4-1-1.94]
[14-4-1-0.77][15-4-0-0.29][16-4-2-0.44][17-4-2-0.43][18-4-1-1.19][19-4-0-0.13][20-4-2-0.44][21-4-1-1.25][22-4-1-0.57][23-4-1-1.70]
[24-4-2-0.60][25-4-2-0.65][26-4-2-0.25][27-4-0-1.02][28-4-1-0.87][29-4-1-1.12][30-4-2-0.77][31-4-2-1.08][32-4-1-1.93][33-4-2-1.27]
[34-4-2-0.87][35-4-1-0.31][37-4-2-0.70][39-4-3-0.22][40-4-2-0.47][41-4-2-0.75][42-4-1-1.85][43-4-1-0.76][45-4-1-0.97][46-4-2-0.82]
[47-4-1-0.55][48-4-2-0.46][51-4-1-0.49][52-4-2-1.06][53-4-1-1.21][54-4-1-0.94][55-4-2-0.53][56-4-1-1.27][57-4-2-0.50][58-4-1-1.66]
[59-4-1-0.03][60-4-1-0.85][61-4-1-1.32][62-4-2-0.81][63-4-2-1.28][64-4-1-0.87][65-4-1-0.97][66-4-2-0.53][67-4-2-0.91][68-4-1-2.05]
[69-4-2-0.27][70-4-1-0.85][72-4-1-1.22][73-4-1-1.68][74-4-1-1.18][75-4-2-0.72][77-4-1-1.11][78-4-1-0.99][79-4-1-1.21][80-4-1-2.13]
[81-4-1-1.80][82-4-2-0.91][83-4-1-1.73][84-4-2-0.76][85-4-1-0.68][86-4-1-1.14][87-4-2-0.38][88-4-1-1.13][89-4-2-1.38][90-4-1-0.70]
[91-4-2-0.79][92-4-2-0.85][93-4-0-0.11][94-4-1-0.51][95-4-1-0.90][96-4-2-1.05][97-4-2-0.89][98-4-2-1.61][99-4-2-0.61][100-4-1-1.70]
[101-4-2-0.75][102-4-2-1.47][103-4-2-0.52][104-4-1-1.04][105-4-1-1.17][106-4-1-1.52][107-4-1-1.41][108-4-1-0.48][109-4-1-0.45][110-4-1-0.54]
[111-4-0-1.03][112-4-2-0.62][113-4-1-1.21][114-4-2-0.58][115-4-2-0.56][116-4-1-0.81][117-4-1-1.27][119-4-2-1.55][121-4-1-1.18][122-4-1-0.79]
[124-4-2-1.70][125-4-1-1.15][126-4-1-0.83][127-4-2-1.07][128-4-2-0.87][129-4-1-1.68][130-4-2-1.11][131-4-2-1.17][132-4-2-0.81][133-4-0-0.14]
[135-4-2-1.23][136-4-2-0.98][137-4-2-0.98][138-4-2-1.28][139-4-2-1.20][140-4-2-1.21][141-4-2-0.77][142-4-2-0.34][143-4-1-1.24][144-4-2-1.11]
[145-4-1-1.04][148-4-0-0.84][149-4-2-1.09][150-4-1-1.88][151-4-1-1.07][152-4-1-1.30][153-4-1-1.93][154-4-2-1.00][155-4-1-0.97][156-4-2-0.44]
[157-4-2-0.88][158-4-1-1.23][160-4-1-1.00][161-4-1-0.81][162-4-2-0.74][164-4-1-0.71][165-4-1-0.72][167-4-0-0.39][168-4-1-0.24][170-4-1-0.87]
[171-4-1-2.26][172-4-2-0.93][173-4-2-0.22][174-4-2-0.28][175-4-1-1.19][177-4-2-0.31][178-4-1-1.10][179-4-1-1.34][180-4-1-0.28][181-4-1-1.20]
[182-4-2-1.51][183-4-1-1.13][184-4-2-1.10][186-4-2-1.05][187-4-1-2.43][188-4-2-1.03][189-4-2-1.06][190-4-1-0.90][191-4-2-0.80][192-4-1-0.87]
[193-4-2-1.82][194-4-2-1.09][195-4-0-0.30][196-4-2-0.68][197-4-2-1.56][198-4-1-0.51][199-4-2-0.91]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.169 | Acc: 40.250% | Wgt Acc: 50.911%
	I - Batch: 100 | Loss: 1.136 | Acc: 42.000% | Wgt Acc: 53.317%
	I - Batch: 150 | Loss: 1.110 | Acc: 43.417% | Wgt Acc: 54.140%
	I - Batch: 200 | Loss: 1.111 | Acc: 43.812% | Wgt Acc: 54.442%
I - num batch: 222
I - Train -- Loss: 1.111 | Acc: 43.812% | Wgt Acc: 54.492% | LR: 1.000000e-03 | Dur: 135.44s
I - Confusion Matrix: [row->prediction - col->label]
[[486.  30.  60. 184. 165.]
 [ 23. 308. 151.  40. 232.]
 [ 64. 200. 471.  79. 474.]
 [121.  35.  46. 235.  75.]
 [  3.   5.   6.   0.  54.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.490 | Acc: 36.686% | Wgt Acc: 46.834% | Dur: 16.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 40.   4.   3.  11.  20.]
 [  5.  29.   8.   5.  24.]
 [ 18.  41.  60.  31. 112.]
 [ 24.   2.   4.  39.   6.]
 [  1.   2.   0.   0.  18.]]

I - Local maximum validation set accuracy:  36.69

I - Validation set results: 
[14-1-2-1.07][50-3-3--0.26][124-2-2-1.44][127-0-0-0.88][443-2-2-1.92][567-0-0-0.97][573-1-1-0.01][615-0-3-0.30][695-1-2-1.15][722-3-3-2.18]
[826-0-3-0.16][878-0-0-1.00][1103-0-0-0.70][1212-3-2-0.92][1368-0-0-0.95][2181-2-1-0.32][2476-2-2-0.52][2721-2-2-1.85][2818-1-0--0.24][2886-2-2-1.07]
[3231-2-2-2.40][3333-2-2-0.83][3482-2-2-1.58][3536-3-3-0.46][3625-1-1-1.24][3909-0-2--0.07][4035-0-3-2.46][4140-0-0-1.43][4214-1-2-0.45][4346-1-0-0.87]
[4581-2-2-2.02][4708-3-2-1.14][4838-3-2-0.04][4845-1-2-1.22][4868-0-0-1.13][4939-0-1--0.00][4984-2-2-1.24][5078-1-1-0.86][5396-0-0-1.73][5479-1-1-1.49]
[5717-0-0-1.76][5843-1-1-1.85][5949-3-0-0.24][5987-2-2-1.09][6014-3-1-0.40][6033-3-3-0.09][6313-0-0-0.68][6421-3-3-0.33][6500-1-1-1.38][6583-3-2-0.66]
[6683-3-3-0.47][6825-2-3-0.60][6998-3-3-0.38][7049-3-2-0.75][7517-1-2-1.47][7521-1-1-0.88][7528-1-1-0.39][7949-1-2-1.29][8135-1-0-0.03][8185-3-3-1.21]
[8269-3-2-1.98][8273-3-1-0.24][8543-3-0-1.36][8666-1-1-1.52][8672-0-0-1.26][8903-1-2-0.35][9001-2-2-0.91][9036-2-2-1.83][9281-3-2-0.48][9300-2-2-1.16]
[9571-0-3--0.02][9617-1-1-0.84][9644-2-2-1.92][9705-2-2-1.00][9801-0-2-0.67][9803-3-3-0.76][9865-3-3-1.27][9896-2-2-1.51][10314-1-2-1.17][10337-3-3-1.64]
[10403-0-2-0.55][10653-2-1-0.56][10704-2-1-1.15][10719-1-1-1.09][10727-1-2-0.91][10836-0-3-2.87][10969-2-2-0.65][11042-0-0-0.71][11088-1-2-2.65][11322-0-0-1.44]
[11398-2-2-2.52][11499-0-2-0.22][11502-3-2-0.39][11512-3-2-0.94][11608-1-1-1.74][11610-0-3--0.00][11692-0-2-0.39][11905-0-0-3.05][11993-1-2-1.69][12002-2-3-0.71]
[12052-0-0-1.22][12201-0-3-1.34][12235-2-2-0.93][12320-1-2-0.18][12377-2-2-0.90][12398-2-3-0.06][12503-1-2-1.61][12617-0-1-1.39][12685-3-1-0.90][12738-2-2-0.33]
[12742-2-2-1.86][12823-0-3-1.40][13110-1-2-0.32][13240-3-3-0.92][13253-1-2-1.13][13273-0-3-1.75][13634-1-2-1.65][13763-2-2-0.67][13905-3-3-0.48][14060-2-1-1.63]
[14065-3-0-0.38][14147-3-2-0.18][14595-2-2-1.65][14687-2-2-1.84][14788-2-2-1.55][14869-1-1-0.78][14872-3-0-0.84][14877-1-1-0.55][14927-0-2-1.16][15066-0-0-3.36]
[15175-1-2-0.85][15178-2-2-0.47][15375-3-3-0.91][15389-3-2-0.59][15568-2-1-1.25][15675-3-3-1.16][15869-1-2-1.08][16207-3-0-0.40][16236-0-2-0.63][16302-3-2-0.72]
[16331-2-2-2.11][16381-0-0-0.20][16488-1-2-1.31][16495-0-0-1.44][16650-0-0-1.75][16719-1-2-0.67][16801-0-0-2.98][16828-0-0-0.59][17137-3-3-0.51][17245-1-2-0.92]
[17278-3-0-0.11][17282-0-0-0.06][17311-2-2-2.02][17336-2-2-1.20][17608-3-3-1.90][17627-0-2-0.74][17877-3-2-1.15][17924-1-2-0.74][17984-3-0-2.20][18211-0-3-0.92]
[18276-3-0-1.27][18287-1-1-0.27][18394-0-3-0.73][18428-0-1-0.06][18442-0-3-0.92][18478-3-3-0.82][18607-0-0-0.32][18616-0-0-0.20][18663-0-0-0.86][18718-0-0-1.14]
[18766-2-2-2.17][18824-2-2-0.89][18890-3-2-0.64][18930-3-2-0.39][18938-3-2-0.60][19817-1-2-0.99][19839-0-2-1.45][19930-3-3-0.60][19944-0-2-1.54][20036-2-2-1.75]
[20101-3-3-1.03][20474-1-2-2.16][20547-3-0-0.48][20929-2-2-1.70][21245-1-2-1.31][21257-3-1-0.53][21293-1-1-1.91][21316-1-1-0.72][21384-1-2-1.17][21448-1-2-1.01]
[21483-0-0-0.85][21487-2-2-1.53][21714-0-2-0.05][21943-3-2-1.36][21947-0-0-1.12][21948-0-0-3.97][21965-2-2-1.45][21998-1-1-0.29][22025-0-2-0.52][22228-3-3-1.87]
[22446-1-1-1.53][22494-3-3-1.08][22757-0-0-2.01][22811-3-3-0.85][22976-3-2-0.89][22985-3-3-1.19][23014-0-3-1.48][23112-1-2-1.11][23144-3-3-1.54][23168-2-0-0.56]
[23219-0-3-0.24][23363-3-3-1.30][23470-0-1-0.37][23486-2-2-0.83][23497-0-3-2.26][23516-0-0-2.62][23690-1-4-1.32][23921-2-2-0.99][23936-1-2-1.42][24040-3-2-0.99]
[24111-1-4-0.87][24182-0-3-2.23][24238-3-3-0.66][24290-2-0-0.82][24345-0-0-1.27][24364-1-2-1.03][24427-3-3-0.79][24477-2-2-1.97][24495-2-1-0.59][24893-2-2-1.30]
[25012-1-1-0.22][25121-2-2-0.99][25165-3-3-0.30][25183-0-0-1.39][25297-3-2-0.75][25398-0-0-0.73][25574-2-2-0.94][25644-1-1-1.20][25718-1-1-1.20][25774-2-2-1.29]
[26032-3-3-1.30][26051-3-3-1.65][26120-0-2-0.47][26321-1-2-0.50][26732-1-1-0.78][26784-3-3-2.43][26827-3-2-0.73][26833-0-3-2.00][26838-2-2-0.32][26860-1-2-1.24]
[26948-0-0-1.47][27049-3-0-1.19][27098-1-2-0.72][27526-0-0-1.96][27639-3-2-0.29][27698-3-3-1.72][27772-0-3-0.84][27890-1-1-0.86][28040-0-2-0.04][28503-2-2-2.73]
[28577-1-1-1.26][28959-0-0-2.51][29198-3-0-0.19][29777-0-0-1.87][29877-2-2-0.85][30035-1-2-1.20][30098-0-3-0.91][30326-1-1-1.69][30572-2-2-1.22][30716-0-4-0.20]
[30806-2-2-0.72][30906-1-1-0.50][31007-0-0-1.64][31181-3-2-0.80][31238-0-3-0.63][31347-0-3-0.99][31422-2-2-1.12][31429-3-3-0.14][31431-0-3--0.10][31432-1-1-1.18]
[31477-0-3-1.24][31524-1-0--0.02][31597-1-2-1.62][31619-1-2-0.47][31701-0-2-1.04][31755-0-2-0.72][31854-3-1-0.14][32074-1-3--0.08][32078-3-2-0.85][32111-1-2-0.74]
[32127-1-2-2.79][32140-3-2-0.71][32263-2-0-0.12][32365-0-0-0.88][32411-2-3-1.78][32429-3-3-1.25][32473-3-2--0.01][32574-3-3-1.96][32584-0-2-1.37][32622-0-2-0.37]
[32858-3-2-0.72][32969-3-3-1.05][33016-2-2-2.15][33031-1-3-1.35][33035-2-2-3.04][33133-2-2-1.61][33173-2-1-0.95][33175-3-2-1.79][33306-3-2-1.36][33309-2-2-0.62]
[33474-0-1-0.01][33478-2-2-0.88][33618-1-1-0.74][33712-0-3-0.28][33782-2-2-1.68][33914-3-3-0.75][34076-3-2-0.89][34112-2-1-0.78][34138-2-2-1.57][34239-1-2-1.49]
[34364-2-2-1.94][34617-1-2-1.17][34751-3-3-1.61][34783-2-2-1.01][35015-3-2-1.62][35018-1-2-1.31][35288-2-2-1.14][0-4-2-1.33][1-4-2-1.00][2-4-2-0.36]
[3-4-2-1.05][4-4-2-0.21][5-4-1-0.13][6-4-0-0.55][7-4-2-0.53][8-4-2-1.39][9-4-2-0.76][10-4-4-0.59][11-4-2-1.83][12-4-1-1.45]
[14-4-1-0.15][15-4-3-1.25][16-4-2-0.15][17-4-2-0.03][18-4-4-0.55][19-4-3-0.11][20-4-2-0.40][21-4-2-1.58][22-4-4-0.00][23-4-2-0.75]
[24-4-2-0.92][25-4-2-0.57][26-4-3-0.84][27-4-0-1.23][28-4-4-0.68][29-4-2-0.88][30-4-2-0.66][31-4-2-1.91][32-4-2-1.11][33-4-2-1.07]
[34-4-2-0.82][35-4-3-1.27][37-4-2-1.38][39-4-0-1.16][40-4-2-1.14][41-4-2-0.37][42-4-1-1.08][43-4-2-1.88][45-4-2-1.36][46-4-2-1.50]
[47-4-4-0.98][48-4-2-0.86][51-4-4-0.37][52-4-2-1.72][53-4-1-0.61][54-4-0-0.02][55-4-2-1.41][56-4-1-1.48][57-4-3-0.32][58-4-2-1.81]
[59-4-0-0.79][60-4-1-0.11][61-4-2-0.72][62-4-2-0.75][63-4-2-1.79][64-4-2-0.40][65-4-2-0.87][66-4-4-1.31][67-4-2-1.43][68-4-1-1.48]
[69-4-0-1.00][70-4-4-0.37][72-4-2-1.24][73-4-1-1.00][74-4-2-1.76][75-4-0-0.46][77-4-4-1.29][78-4-2-0.68][79-4-2-0.91][80-4-1-0.92]
[81-4-2-1.41][82-4-2-0.51][83-4-1-0.96][84-4-2-1.28][85-4-4-0.67][86-4-2-0.94][87-4-4-0.72][88-4-1-1.05][89-4-2-1.37][90-4-2-0.23]
[91-4-2-1.15][92-4-2-0.55][93-4-0-0.60][94-4-4-0.74][95-4-2-1.02][96-4-2-0.86][97-4-2-0.74][98-4-2-1.88][99-4-2--0.00][100-4-2-1.07]
[101-4-4-1.15][102-4-2-1.32][103-4-2-0.81][104-4-2-0.87][105-4-2-0.76][106-4-2-0.82][107-4-1-1.06][108-4-2-0.74][109-4-1-0.90][110-4-2-0.25]
[111-4-0-2.03][112-4-2-0.49][113-4-2-0.99][114-4-2-0.76][115-4-0-0.14][116-4-0-0.42][117-4-1-1.09][119-4-2-2.37][121-4-1-1.02][122-4-2-0.75]
[124-4-2-1.76][125-4-2-0.84][126-4-2-0.36][127-4-2-1.18][128-4-2-1.06][129-4-1-0.94][130-4-2-0.92][131-4-2-1.32][132-4-2-1.51][133-4-0-0.40]
[135-4-2-1.35][136-4-1-0.74][137-4-2-0.27][138-4-2-0.96][139-4-2-1.70][140-4-2-1.46][141-4-2-0.84][142-4-4-1.13][143-4-4-0.86][144-4-4-1.07]
[145-4-2-1.04][148-4-0-1.54][149-4-2-0.99][150-4-2-1.01][151-4-2-1.26][152-4-1-1.00][153-4-2-1.91][154-4-2-1.47][155-4-2-0.30][156-4-2-0.40]
[157-4-2-0.98][158-4-2-0.87][160-4-2-0.82][161-4-2-1.22][162-4-2-0.15][164-4-2-0.94][165-4-2-0.87][167-4-0-1.23][168-4-0--0.09][170-4-3-0.93]
[171-4-1-1.49][172-4-4-0.90][173-4-0-0.55][174-4-0-0.25][175-4-2-0.45][177-4-2-0.09][178-4-2-1.04][179-4-1-0.99][180-4-0-0.46][181-4-2-1.33]
[182-4-2-1.60][183-4-1-1.17][184-4-2-1.60][186-4-0-0.15][187-4-1-1.67][188-4-2-1.30][189-4-2-1.47][190-4-1-0.64][191-4-2-0.61][192-4-2-1.41]
[193-4-2-1.74][194-4-2-0.69][195-4-0-0.59][196-4-2-0.79][197-4-2-1.56][198-4-4-0.81][199-4-2-1.35]
---------------------------
I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.070 | Acc: 46.750% | Wgt Acc: 56.856%
	I - Batch: 100 | Loss: 1.057 | Acc: 47.938% | Wgt Acc: 57.874%
	I - Batch: 150 | Loss: 1.045 | Acc: 48.042% | Wgt Acc: 57.906%
	I - Batch: 200 | Loss: 1.053 | Acc: 47.844% | Wgt Acc: 57.794%
I - num batch: 222
I - Train -- Loss: 1.055 | Acc: 47.871% | Wgt Acc: 57.990% | LR: 1.000000e-03 | Dur: 136.29s
I - Confusion Matrix: [row->prediction - col->label]
[[495.  25.  45. 169. 172.]
 [ 32. 342. 118.  37. 194.]
 [ 53. 167. 494.  80. 438.]
 [110.  37.  61. 252.  81.]
 [  7.   7.  16.   0. 115.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.355 | Acc: 42.406% | Wgt Acc: 49.960% | Dur: 14.73s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  2.  6. 27. 24.]
 [ 8. 39. 14.  9. 28.]
 [ 6. 32. 47. 15. 80.]
 [15.  1.  2. 34.  8.]
 [ 4.  4.  6.  1. 40.]]

I - Local maximum validation set accuracy:  42.41

I - Validation set results: 
[14-1-1-1.65][50-3-3-0.06][124-2-2-1.30][127-0-0-3.06][443-2-2-1.97][567-0-0-2.24][573-1-1-0.16][615-0-0-1.63][695-1-2-0.55][722-3-0-2.24]
[826-0-0-1.87][878-0-0-1.67][1103-0-4-0.42][1212-3-0-0.32][1368-0-0-2.69][2181-2-2-0.34][2476-2-2-0.57][2721-2-2-1.02][2818-1-2-0.02][2886-2-4-1.50]
[3231-2-2-2.19][3333-2-1-1.21][3482-2-2-0.82][3536-3-3-0.42][3625-1-1-2.04][3909-0-1-0.35][4035-0-3-1.72][4140-0-0-1.18][4214-1-1-0.34][4346-1-0-1.41]
[4581-2-2-1.82][4708-3-2-0.25][4838-3-0-0.69][4845-1-2-0.88][4868-0-0-2.37][4939-0-4-0.94][4984-2-3-0.34][5078-1-1-0.39][5396-0-0-3.55][5479-1-1-2.66]
[5717-0-0-2.02][5843-1-2-1.16][5949-3-0-1.59][5987-2-4-1.63][6014-3-1-0.08][6033-3-0-1.17][6313-0-3-1.45][6421-3-3-1.53][6500-1-1-0.63][6583-3-1-0.83]
[6683-3-2-0.19][6825-2-0-0.30][6998-3-3-0.10][7049-3-2-0.40][7517-1-1-1.94][7521-1-1-1.26][7528-1-2-0.87][7949-1-2-0.92][8135-1-0-0.49][8185-3-0-1.60]
[8269-3-4-0.51][8273-3-3-0.75][8543-3-0-2.74][8666-1-1-0.95][8672-0-0-2.51][8903-1-2-0.95][9001-2-1-1.49][9036-2-2-2.09][9281-3-2-0.29][9300-2-2-1.05]
[9571-0-0-1.10][9617-1-2-0.58][9644-2-2-1.09][9705-2-1-1.26][9801-0-3-0.37][9803-3-3-0.73][9865-3-3-1.73][9896-2-2-1.80][10314-1-2-1.16][10337-3-3-1.26]
[10403-0-2-0.42][10653-2-1-0.88][10704-2-1-0.93][10719-1-1-2.00][10727-1-4-2.05][10836-0-0-3.76][10969-2-2-0.65][11042-0-0-1.24][11088-1-1-1.15][11322-0-0-1.52]
[11398-2-2-1.16][11499-0-1-0.10][11502-3-3-0.89][11512-3-1-1.53][11608-1-1-2.27][11610-0-1-0.48][11692-0-3-0.99][11905-0-0-3.87][11993-1-2-1.09][12002-2-3-1.03]
[12052-0-0-1.54][12201-0-3-1.22][12235-2-2-1.68][12320-1-2-0.60][12377-2-4-1.28][12398-2-2-0.03][12503-1-2-1.13][12617-0-2-1.10][12685-3-2-0.43][12738-2-1-0.73]
[12742-2-2-1.69][12823-0-3-1.78][13110-1-2-1.50][13240-3-3-0.27][13253-1-4-1.66][13273-0-0-3.29][13634-1-2-2.03][13763-2-2-1.14][13905-3-3-0.75][14060-2-1-1.68]
[14065-3-0-1.61][14147-3-1-0.62][14595-2-2-1.47][14687-2-2-1.26][14788-2-2-1.10][14869-1-2-1.06][14872-3-0-0.74][14877-1-1-0.27][14927-0-3-1.08][15066-0-0-3.78]
[15175-1-2-1.07][15178-2-0-0.64][15375-3-3-1.33][15389-3-3-1.13][15568-2-1-1.05][15675-3-1-0.30][15869-1-2-0.45][16207-3-0-0.40][16236-0-0-0.64][16302-3-0-0.26]
[16331-2-2-1.89][16381-0-0-1.61][16488-1-1-2.14][16495-0-0-1.45][16650-0-0-3.10][16719-1-2-0.59][16801-0-0-3.48][16828-0-0-1.34][17137-3-0-1.10][17245-1-1-0.23]
[17278-3-1-0.17][17282-0-0-0.59][17311-2-2-1.68][17336-2-1-1.41][17608-3-0-2.26][17627-0-1-0.99][17877-3-1-0.91][17924-1-2-0.29][17984-3-0-2.58][18211-0-3-0.44]
[18276-3-0-2.38][18287-1-1-0.59][18394-0-0-1.09][18428-0-1-1.27][18442-0-3-0.88][18478-3-0-1.21][18607-0-0-1.01][18616-0-0-0.76][18663-0-0-1.51][18718-0-0-2.67]
[18766-2-2-1.68][18824-2-4-0.92][18890-3-2-1.17][18930-3-2-0.72][18938-3-3-0.38][19817-1-2-1.61][19839-0-4-0.77][19930-3-3-0.97][19944-0-2-0.98][20036-2-2-1.95]
[20101-3-3-0.72][20474-1-1-1.40][20547-3-0-0.53][20929-2-2-2.02][21245-1-1-1.75][21257-3-2-0.17][21293-1-1-2.99][21316-1-1-2.59][21384-1-1-2.29][21448-1-1-1.53]
[21483-0-0-1.64][21487-2-2-1.39][21714-0-2-0.20][21943-3-2-0.91][21947-0-0-0.62][21948-0-0-4.40][21965-2-2-2.06][21998-1-1-1.28][22025-0-2-0.69][22228-3-3-0.76]
[22446-1-2-1.45][22494-3-0-1.48][22757-0-3-1.46][22811-3-3-1.05][22976-3-2-1.38][22985-3-0-1.66][23014-0-0-1.71][23112-1-1-1.13][23144-3-3-1.78][23168-2-0-1.58]
[23219-0-0-1.34][23363-3-3-1.28][23470-0-1-0.76][23486-2-2-1.48][23497-0-3-2.16][23516-0-0-3.88][23690-1-2-0.38][23921-2-2-1.53][23936-1-2-0.99][24040-3-1-0.82]
[24111-1-4-2.49][24182-0-0-2.20][24238-3-3-1.19][24290-2-0-1.04][24345-0-0-1.05][24364-1-2-0.95][24427-3-3-0.46][24477-2-2-1.83][24495-2-1-1.04][24893-2-2-1.76]
[25012-1-2--0.02][25121-2-4-0.85][25165-3-3-0.33][25183-0-0-1.42][25297-3-3-0.66][25398-0-0-1.69][25574-2-2-0.74][25644-1-1-2.26][25718-1-1-0.50][25774-2-2-1.30]
[26032-3-3-1.03][26051-3-3-1.89][26120-0-0-1.46][26321-1-1-1.43][26732-1-1-0.68][26784-3-3-2.55][26827-3-3-0.59][26833-0-3-1.94][26838-2-1-1.40][26860-1-1-0.77]
[26948-0-0-0.99][27049-3-0-1.50][27098-1-1-0.87][27526-0-0-1.59][27639-3-3-0.86][27698-3-3-0.94][27772-0-0-2.17][27890-1-1-1.13][28040-0-0-0.49][28503-2-2-2.07]
[28577-1-1-1.12][28959-0-0-4.75][29198-3-0-0.13][29777-0-0-3.77][29877-2-2-1.16][30035-1-1-1.26][30098-0-3-0.74][30326-1-1-2.20][30572-2-2-1.63][30716-0-4-0.50]
[30806-2-1-0.84][30906-1-1-1.78][31007-0-1-0.50][31181-3-3-0.86][31238-0-3-0.09][31347-0-3-0.99][31422-2-2-0.77][31429-3-0-0.13][31431-0-0-1.07][31432-1-1-2.03]
[31477-0-0-2.37][31524-1-2-1.45][31597-1-2-1.56][31619-1-2-0.81][31701-0-0-1.06][31755-0-0-0.52][31854-3-0-0.17][32074-1-2-0.85][32078-3-1-0.56][32111-1-4-1.26]
[32127-1-2-2.65][32140-3-3-1.26][32263-2-0-0.04][32365-0-0-0.71][32411-2-0-2.23][32429-3-0-1.45][32473-3-0-0.79][32574-3-0-2.00][32584-0-1-0.13][32622-0-2-0.56]
[32858-3-2-0.26][32969-3-0-1.70][33016-2-2-1.35][33031-1-3-1.26][33035-2-2-2.36][33133-2-1-1.86][33173-2-2-1.03][33175-3-2-1.75][33306-3-2-1.80][33309-2-2-0.48]
[33474-0-0-0.11][33478-2-2-0.64][33618-1-1-0.85][33712-0-0-0.27][33782-2-2-1.75][33914-3-3-0.86][34076-3-2-0.65][34112-2-1-0.94][34138-2-2-1.70][34239-1-2-1.26]
[34364-2-2-1.92][34617-1-2-0.59][34751-3-3-1.90][34783-2-4-1.32][35015-3-2-1.00][35018-1-1-1.46][35288-2-2-0.90][0-4-2-0.24][1-4-0-0.58][2-4-4-0.65]
[3-4-2-0.68][4-4-3-0.18][5-4-1-1.47][6-4-0-1.46][7-4-0-0.40][8-4-2-1.24][9-4-2-0.37][10-4-4-0.48][11-4-2-2.07][12-4-2-0.63]
[14-4-4--0.33][15-4-3-1.59][16-4-0-0.66][17-4-2-0.10][18-4-4-1.12][19-4-0-1.50][20-4-2-0.30][21-4-2-1.90][22-4-4-1.61][23-4-1-0.51]
[24-4-4-0.18][25-4-2-0.63][26-4-2--0.01][27-4-0-0.37][28-4-4-0.45][29-4-2-1.11][30-4-0-1.01][31-4-2-1.12][32-4-1-1.16][33-4-3-0.68]
[34-4-2-0.32][35-4-3-0.97][37-4-2-1.46][39-4-0-1.29][40-4-2-0.62][41-4-2-0.87][42-4-2-2.10][43-4-2-1.18][45-4-2-1.56][46-4-2-0.98]
[47-4-4-0.97][48-4-2-1.00][51-4-2-1.00][52-4-2-1.07][53-4-2-0.07][54-4-4-0.14][55-4-0-0.55][56-4-2-0.73][57-4-3-0.91][58-4-2-2.21]
[59-4-0-0.80][60-4-1-0.68][61-4-4-1.37][62-4-2-0.59][63-4-2-2.49][64-4-2-0.40][65-4-4-1.55][66-4-1-1.56][67-4-2-0.56][68-4-2-1.01]
[69-4-0-1.34][70-4-2-1.08][72-4-4-1.80][73-4-2-0.87][74-4-2-0.72][75-4-3-0.30][77-4-4-1.24][78-4-1-1.37][79-4-2-1.54][80-4-4-1.53]
[81-4-4-2.30][82-4-2-0.78][83-4-1-1.28][84-4-0-0.48][85-4-4-0.75][86-4-1-1.02][87-4-4-0.83][88-4-1-1.01][89-4-2-0.98][90-4-4-1.73]
[91-4-2-0.75][92-4-2-0.54][93-4-2-0.11][94-4-4-1.53][95-4-2-0.69][96-4-1-1.06][97-4-4-0.86][98-4-2-1.22][99-4-4-1.02][100-4-4-1.33]
[101-4-4-0.54][102-4-2-1.21][103-4-3-0.49][104-4-4-1.74][105-4-2-1.33][106-4-1-1.89][107-4-0-0.51][108-4-2-1.06][109-4-1-1.09][110-4-2-1.36]
[111-4-0-2.92][112-4-2-0.77][113-4-2-0.75][114-4-3-0.62][115-4-2-0.01][116-4-1-0.81][117-4-1-0.55][119-4-2-2.41][121-4-2-0.71][122-4-0--0.03]
[124-4-2-1.21][125-4-4-1.55][126-4-4-0.13][127-4-2-1.59][128-4-2-0.36][129-4-2-1.01][130-4-2-1.10][131-4-2-0.82][132-4-1-1.94][133-4-0-1.44]
[135-4-2-1.16][136-4-1-0.64][137-4-1-0.27][138-4-1-0.39][139-4-0-0.28][140-4-1-0.76][141-4-2-0.25][142-4-4-1.62][143-4-4-1.68][144-4-4-1.79]
[145-4-2-0.83][148-4-0-2.83][149-4-2-0.88][150-4-2-1.79][151-4-2-1.58][152-4-1-0.77][153-4-2-1.12][154-4-2-0.71][155-4-4-0.13][156-4-0-0.52]
[157-4-2-0.78][158-4-2-0.63][160-4-4-1.49][161-4-1-1.82][162-4-2-0.34][164-4-2-1.15][165-4-1-0.70][167-4-0-1.49][168-4-4-0.59][170-4-0-0.96]
[171-4-1-1.61][172-4-4-1.55][173-4-2-0.20][174-4-0-1.58][175-4-1-1.13][177-4-0-1.23][178-4-4-1.71][179-4-1-0.61][180-4-4-1.06][181-4-2-0.94]
[182-4-2-0.89][183-4-1-0.41][184-4-4-1.19][186-4-0-0.35][187-4-1-1.40][188-4-2-1.08][189-4-4-0.98][190-4-2-0.69][191-4-4-1.42][192-4-4-0.91]
[193-4-2-2.59][194-4-2-0.72][195-4-1-1.37][196-4-2-0.54][197-4-2-2.29][198-4-4-1.69][199-4-2-0.83]
---------------------------
I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.020 | Acc: 49.500% | Wgt Acc: 59.260%
	I - Batch: 100 | Loss: 1.016 | Acc: 50.562% | Wgt Acc: 60.500%
	I - Batch: 150 | Loss: 1.001 | Acc: 50.875% | Wgt Acc: 60.496%
	I - Batch: 200 | Loss: 0.992 | Acc: 51.344% | Wgt Acc: 61.321%
I - num batch: 222
I - Train -- Loss: 0.995 | Acc: 51.226% | Wgt Acc: 61.016% | LR: 1.000000e-03 | Dur: 134.89s
I - Confusion Matrix: [row->prediction - col->label]
[[520.  19.  29. 145. 174.]
 [ 35. 343. 123.  38. 198.]
 [ 45. 158. 509.  68. 384.]
 [ 87.  43.  55. 284.  83.]
 [ 10.  15.  18.   3. 161.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.399 | Acc: 41.026% | Wgt Acc: 51.563% | Dur: 14.97s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  8.  7. 34. 23.]
 [ 3. 43. 17.  4. 40.]
 [ 4. 17. 39.  9. 70.]
 [15.  9. 11. 38. 23.]
 [ 2.  1.  1.  1. 24.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 0.933 | Acc: 52.375% | Wgt Acc: 62.737%
	I - Batch: 100 | Loss: 0.925 | Acc: 53.562% | Wgt Acc: 63.729%
	I - Batch: 150 | Loss: 0.934 | Acc: 53.042% | Wgt Acc: 63.563%
	I - Batch: 200 | Loss: 0.950 | Acc: 53.344% | Wgt Acc: 63.151%
I - num batch: 222
I - Train -- Loss: 0.957 | Acc: 53.059% | Wgt Acc: 62.888% | LR: 1.000000e-03 | Dur: 133.97s
I - Confusion Matrix: [row->prediction - col->label]
[[516.  30.  26. 143. 176.]
 [ 20. 351. 103.  29. 167.]
 [ 39. 165. 531.  53. 382.]
 [113.  25.  53. 305.  96.]
 [  9.   7.  21.   8. 179.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.350 | Acc: 44.773% | Wgt Acc: 53.362% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  5.  7. 37. 37.]
 [ 3. 52. 23.  4. 43.]
 [ 3. 12. 30.  3. 44.]
 [11.  5. 14. 37. 16.]
 [ 3.  4.  1.  5. 40.]]

I - Local maximum validation set accuracy:  44.77

I - Validation set results: 
[14-1-2-1.17][50-3-3-0.22][124-2-3-0.36][127-0-0-2.98][443-2-2-1.81][567-0-0-1.65][573-1-1-1.66][615-0-0-2.02][695-1-1--0.09][722-3-0-2.65]
[826-0-0-3.88][878-0-0-2.84][1103-0-0-0.64][1212-3-0-1.68][1368-0-0-1.60][2181-2-3-0.86][2476-2-2-0.06][2721-2-1-0.90][2818-1-1-0.45][2886-2-1-1.69]
[3231-2-2-2.21][3333-2-3-0.45][3482-2-3-0.12][3536-3-3-1.19][3625-1-1-2.59][3909-0-0-0.19][4035-0-3-1.96][4140-0-0-1.11][4214-1-2--0.03][4346-1-0-1.80]
[4581-2-2-1.58][4708-3-3-0.50][4838-3-3--0.06][4845-1-1-1.02][4868-0-0-4.14][4939-0-2-1.21][4984-2-2-0.25][5078-1-1-0.83][5396-0-0-4.92][5479-1-1-1.01]
[5717-0-0-1.25][5843-1-1-1.04][5949-3-0-1.89][5987-2-1-1.49][6014-3-3-1.04][6033-3-0-1.11][6313-0-3-1.46][6421-3-3-2.24][6500-1-1-0.76][6583-3-3-0.55]
[6683-3-3-1.13][6825-2-0-0.72][6998-3-1--0.13][7049-3-3-1.15][7517-1-2-1.09][7521-1-1-0.40][7528-1-3-0.73][7949-1-4-1.02][8135-1-0-0.17][8185-3-0-3.56]
[8269-3-4-0.44][8273-3-3-1.61][8543-3-0-4.03][8666-1-1-1.49][8672-0-0-3.37][8903-1-1-1.05][9001-2-1-1.88][9036-2-2-1.60][9281-3-2-0.04][9300-2-2-1.05]
[9571-0-0-1.29][9617-1-1-0.89][9644-2-1-0.80][9705-2-1-0.14][9801-0-3-1.47][9803-3-0-1.28][9865-3-0-3.45][9896-2-2-1.23][10314-1-1-1.34][10337-3-0-2.57]
[10403-0-4-0.08][10653-2-1-1.02][10704-2-1-0.28][10719-1-1-2.51][10727-1-2-1.18][10836-0-0-4.51][10969-2-3-1.15][11042-0-0-1.91][11088-1-1-2.87][11322-0-0-2.13]
[11398-2-2-0.45][11499-0-0-0.56][11502-3-0-1.38][11512-3-3-0.28][11608-1-1-2.30][11610-0-3-1.47][11692-0-0-2.05][11905-0-0-3.44][11993-1-2-1.67][12002-2-0-3.43]
[12052-0-0-1.83][12201-0-0-3.30][12235-2-1-1.34][12320-1-4-0.61][12377-2-1-0.43][12398-2-3-0.61][12503-1-1-1.40][12617-0-2-0.62][12685-3-3-0.91][12738-2-0--0.13]
[12742-2-2-1.56][12823-0-0-2.91][13110-1-1-1.26][13240-3-0-1.69][13253-1-1-2.00][13273-0-0-4.13][13634-1-1-1.03][13763-2-3-0.75][13905-3-0-1.17][14060-2-1-0.81]
[14065-3-0-2.34][14147-3-3-0.78][14595-2-1-1.07][14687-2-2-1.51][14788-2-2-0.25][14869-1-1-2.41][14872-3-4-0.87][14877-1-1-1.67][14927-0-3-0.48][15066-0-0-3.79]
[15175-1-1-0.65][15178-2-0-1.11][15375-3-3-0.39][15389-3-0-3.20][15568-2-1-0.66][15675-3-3-1.60][15869-1-3-0.32][16207-3-0-0.88][16236-0-0-1.51][16302-3-0-3.48]
[16331-2-2-1.95][16381-0-0-1.77][16488-1-1-2.36][16495-0-0-2.12][16650-0-0-3.67][16719-1-1-0.28][16801-0-0-3.73][16828-0-3-1.23][17137-3-0-1.39][17245-1-1--0.08]
[17278-3-0-0.67][17282-0-0-0.44][17311-2-2-0.60][17336-2-1-1.58][17608-3-0-3.67][17627-0-3-1.25][17877-3-1-1.87][17924-1-1-0.10][17984-3-0-3.43][18211-0-3-1.33]
[18276-3-0-2.61][18287-1-1-0.85][18394-0-0-3.10][18428-0-0-0.44][18442-0-0-2.07][18478-3-0-2.71][18607-0-0-0.74][18616-0-3-0.26][18663-0-0-1.52][18718-0-0-3.57]
[18766-2-1-2.08][18824-2-2-0.78][18890-3-3-0.11][18930-3-4-0.24][18938-3-0-1.15][19817-1-2-1.04][19839-0-4-0.48][19930-3-3-1.18][19944-0-1-0.79][20036-2-2-1.19]
[20101-3-3-1.20][20474-1-1-1.97][20547-3-4-0.04][20929-2-2-1.99][21245-1-1-1.24][21257-3-3-0.02][21293-1-1-1.74][21316-1-1-2.90][21384-1-1-2.03][21448-1-1-0.95]
[21483-0-0-3.63][21487-2-2-0.96][21714-0-0-0.74][21943-3-2-0.41][21947-0-0-1.25][21948-0-0-4.63][21965-2-1-0.86][21998-1-1--0.04][22025-0-3-0.70][22228-3-3-2.11]
[22446-1-1-2.45][22494-3-0-2.68][22757-0-0-3.80][22811-3-3-2.10][22976-3-1-0.39][22985-3-0-2.92][23014-0-0-3.76][23112-1-1-2.45][23144-3-0-4.14][23168-2-0-0.27]
[23219-0-0-2.22][23363-3-3-1.10][23470-0-1-0.30][23486-2-3-0.71][23497-0-0-4.00][23516-0-0-3.18][23690-1-4-1.56][23921-2-2-0.93][23936-1-3-0.20][24040-3-4-0.02]
[24111-1-4-1.68][24182-0-0-3.74][24238-3-3-1.87][24290-2-0-1.67][24345-0-0-1.96][24364-1-2-0.32][24427-3-0-1.60][24477-2-2-0.79][24495-2-1-0.28][24893-2-1-1.40]
[25012-1-2--0.24][25121-2-4-1.16][25165-3-3-0.93][25183-0-0-0.22][25297-3-3-1.21][25398-0-0-2.31][25574-2-3-0.63][25644-1-1-2.84][25718-1-0--0.16][25774-2-3-0.93]
[26032-3-0-2.09][26051-3-0-2.69][26120-0-0-1.03][26321-1-1-0.67][26732-1-1-0.95][26784-3-0-3.26][26827-3-3-1.06][26833-0-3-2.55][26838-2-3--0.20][26860-1-1-0.70]
[26948-0-0-2.46][27049-3-0-1.55][27098-1-0-0.51][27526-0-0-1.88][27639-3-3-0.28][27698-3-3-2.53][27772-0-0-1.79][27890-1-1-2.08][28040-0-0-0.03][28503-2-2-1.95]
[28577-1-1-2.52][28959-0-0-3.68][29198-3-3-0.33][29777-0-0-5.85][29877-2-2-0.28][30035-1-2-1.85][30098-0-0-1.34][30326-1-1-2.11][30572-2-2-0.58][30716-0-4-0.60]
[30806-2-3-0.60][30906-1-1-1.89][31007-0-0-1.40][31181-3-3-1.12][31238-0-0-1.83][31347-0-0-3.38][31422-2-2-0.66][31429-3-0-2.15][31431-0-0-2.55][31432-1-1-1.98]
[31477-0-0-4.21][31524-1-2-0.12][31597-1-1-1.30][31619-1-0-0.75][31701-0-0-3.88][31755-0-0-2.04][31854-3-3-1.16][32074-1-3-0.43][32078-3-3-0.66][32111-1-1-1.85]
[32127-1-2-2.34][32140-3-3-1.83][32263-2-2--0.08][32365-0-2--0.31][32411-2-0-2.28][32429-3-0-3.03][32473-3-0-1.59][32574-3-0-2.79][32584-0-0-1.40][32622-0-1--0.04]
[32858-3-0-2.52][32969-3-0-2.77][33016-2-2-1.15][33031-1-3-0.84][33035-2-2-1.03][33133-2-1-1.43][33173-2-1-0.47][33175-3-1-1.25][33306-3-2-1.07][33309-2-2--0.14]
[33474-0-0-0.31][33478-2-3--0.03][33618-1-1-0.87][33712-0-0-0.92][33782-2-1-1.88][33914-3-3-1.24][34076-3-3-1.79][34112-2-2-0.41][34138-2-2-0.57][34239-1-1-1.39]
[34364-2-1-2.22][34617-1-2-1.22][34751-3-3-3.65][34783-2-1-1.46][35015-3-3-0.98][35018-1-1-1.42][35288-2-3-0.60][0-4-2--0.18][1-4-0-0.89][2-4-0-0.52]
[3-4-4-0.68][4-4-0-0.49][5-4-1-1.98][6-4-4-1.10][7-4-4-0.71][8-4-1-0.05][9-4-1-1.91][10-4-4-1.04][11-4-2-0.73][12-4-3-0.20]
[14-4-3-1.28][15-4-0-3.24][16-4-2-0.21][17-4-1-1.08][18-4-1-0.90][19-4-0-2.78][20-4-2-0.09][21-4-2-0.37][22-4-4-0.77][23-4-1-0.14]
[24-4-4-0.93][25-4-3-1.41][26-4-3--0.13][27-4-0-1.41][28-4-4-1.03][29-4-2-1.01][30-4-4-0.17][31-4-1-0.79][32-4-1-1.40][33-4-3-0.77]
[34-4-0-0.91][35-4-0-2.58][37-4-0-0.52][39-4-0-2.07][40-4-2-0.16][41-4-2-0.85][42-4-2-0.21][43-4-2-0.73][45-4-2-0.19][46-4-4-1.26]
[47-4-4-1.58][48-4-2-0.80][51-4-4-1.55][52-4-2-0.82][53-4-1-0.08][54-4-3-0.55][55-4-0-2.14][56-4-1-1.36][57-4-3-1.47][58-4-2-1.54]
[59-4-0-0.72][60-4-4-0.35][61-4-4-0.76][62-4-0-1.30][63-4-2-0.58][64-4-2-0.76][65-4-4-1.69][66-4-4-2.29][67-4-2--0.01][68-4-1-1.48]
[69-4-0-2.09][70-4-2-0.94][72-4-4-0.93][73-4-1-1.28][74-4-1-0.01][75-4-0-0.62][77-4-4-2.42][78-4-1-0.50][79-4-2-1.29][80-4-1-1.17]
[81-4-1-1.33][82-4-1-0.81][83-4-1-0.94][84-4-0-1.36][85-4-4-0.51][86-4-2-0.77][87-4-4-1.85][88-4-1-1.45][89-4-2-0.04][90-4-0-0.69]
[91-4-0--0.04][92-4-3-0.19][93-4-4-0.36][94-4-4-1.49][95-4-3--0.22][96-4-1-0.64][97-4-4-0.56][98-4-2-1.21][99-4-4-1.15][100-4-1-1.31]
[101-4-4-1.45][102-4-1-0.45][103-4-3-0.33][104-4-1-0.77][105-4-4-1.48][106-4-1-0.95][107-4-1-1.33][108-4-2-0.75][109-4-0-0.45][110-4-2-0.58]
[111-4-0-2.52][112-4-4-0.72][113-4-3-0.61][114-4-0-1.59][115-4-4-0.26][116-4-0--0.03][117-4-1-1.43][119-4-2-1.07][121-4-2-1.54][122-4-0-1.04]
[124-4-2-0.57][125-4-1-1.56][126-4-2-0.06][127-4-2-0.51][128-4-0-0.75][129-4-2-0.08][130-4-2-1.08][131-4-3-0.53][132-4-0-2.06][133-4-0-1.21]
[135-4-3--0.28][136-4-1-0.70][137-4-2--0.23][138-4-1-0.27][139-4-0-0.22][140-4-2-0.36][141-4-3-0.85][142-4-4-1.59][143-4-4-1.51][144-4-4-1.69]
[145-4-1-2.18][148-4-0-3.44][149-4-0--0.04][150-4-1-1.38][151-4-2-1.21][152-4-1-1.17][153-4-1-2.01][154-4-1-1.55][155-4-4-0.50][156-4-0-0.36]
[157-4-0-1.11][158-4-3-0.72][160-4-1-0.39][161-4-1-0.49][162-4-2-0.17][164-4-2-1.00][165-4-2-0.50][167-4-0-0.48][168-4-4-0.95][170-4-0-2.77]
[171-4-2--0.12][172-4-4-0.67][173-4-0-0.64][174-4-0-3.49][175-4-4-0.25][177-4-3--0.10][178-4-4-0.27][179-4-2--0.20][180-4-4-1.86][181-4-1-0.27]
[182-4-1-0.51][183-4-1-0.75][184-4-4-0.70][186-4-0-0.56][187-4-1-2.14][188-4-2-0.39][189-4-4-1.01][190-4-1-0.18][191-4-2-0.81][192-4-4-0.05]
[193-4-1-2.22][194-4-1-0.64][195-4-2-0.25][196-4-2-0.13][197-4-2-1.80][198-4-4-2.11][199-4-0-0.40]
---------------------------
I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 0.932 | Acc: 54.500% | Wgt Acc: 63.975%
	I - Batch: 100 | Loss: 0.932 | Acc: 55.875% | Wgt Acc: 65.471%
	I - Batch: 150 | Loss: 0.922 | Acc: 56.375% | Wgt Acc: 66.237%
	I - Batch: 200 | Loss: 0.915 | Acc: 56.344% | Wgt Acc: 66.320%
I - num batch: 222
I - Train -- Loss: 0.912 | Acc: 55.991% | Wgt Acc: 65.960% | LR: 1.000000e-03 | Dur: 137.59s
I - Confusion Matrix: [row->prediction - col->label]
[[520.  19.  28. 117. 156.]
 [ 24. 375.  77.  32. 204.]
 [ 39. 140. 553.  51. 338.]
 [106.  32.  50. 333.  97.]
 [  8.  12.  26.   5. 205.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.328 | Acc: 46.154% | Wgt Acc: 55.172% | Dur: 15.40s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  3.  6. 22. 22.]
 [ 6. 50. 16. 11. 49.]
 [ 6. 17. 47. 14. 59.]
 [14.  2.  2. 36. 10.]
 [ 1.  6.  4.  3. 40.]]

I - Local maximum validation set accuracy:  46.15

I - Validation set results: 
[14-1-2-1.94][50-3-1-0.22][124-2-2-1.11][127-0-0-2.81][443-2-2-1.92][567-0-0-1.82][573-1-1-1.44][615-0-0-1.98][695-1-2-1.42][722-3-0-2.56]
[826-0-0-2.25][878-0-0-2.69][1103-0-0-0.39][1212-3-3-0.32][1368-0-0-2.60][2181-2-0-0.36][2476-2-2-0.81][2721-2-2-2.32][2818-1-4-0.24][2886-2-1-1.99]
[3231-2-2-3.22][3333-2-2-1.58][3482-2-2-1.59][3536-3-3-0.72][3625-1-1-3.09][3909-0-1-0.93][4035-0-3-1.61][4140-0-0-1.43][4214-1-1-1.27][4346-1-0-1.14]
[4581-2-2-2.06][4708-3-2-0.98][4838-3-0-0.56][4845-1-1-1.45][4868-0-0-3.38][4939-0-1-1.42][4984-2-2-0.95][5078-1-4-0.25][5396-0-0-3.86][5479-1-1-2.96]
[5717-0-0-0.74][5843-1-1-2.05][5949-3-0-1.35][5987-2-4-2.08][6014-3-3-1.02][6033-3-0-1.17][6313-0-0-1.65][6421-3-3-2.46][6500-1-1--0.02][6583-3-2-1.25]
[6683-3-3-0.56][6825-2-3-0.47][6998-3-3-0.26][7049-3-2-0.77][7517-1-1-3.12][7521-1-1-1.17][7528-1-3-1.15][7949-1-2-1.56][8135-1-0-0.49][8185-3-0-2.02]
[8269-3-4-0.98][8273-3-3-0.68][8543-3-0-4.19][8666-1-1-2.51][8672-0-0-3.79][8903-1-2-1.31][9001-2-1-1.84][9036-2-2-3.03][9281-3-2-0.17][9300-2-2-1.96]
[9571-0-3-0.70][9617-1-4-0.94][9644-2-2-1.32][9705-2-2-0.39][9801-0-3-0.41][9803-3-3-0.83][9865-3-3-2.27][9896-2-2-2.01][10314-1-1-1.59][10337-3-3-2.27]
[10403-0-4-0.14][10653-2-1-1.48][10704-2-1-1.86][10719-1-1-2.56][10727-1-4-1.87][10836-0-0-4.87][10969-2-2-0.83][11042-0-0-1.12][11088-1-1-2.31][11322-0-0-1.83]
[11398-2-2-1.04][11499-0-0-0.01][11502-3-3-1.37][11512-3-1-1.59][11608-1-1-3.06][11610-0-0-0.38][11692-0-3-1.37][11905-0-0-3.51][11993-1-2-1.84][12002-2-0-3.45]
[12052-0-0-2.01][12201-0-3-1.67][12235-2-1-1.59][12320-1-4-0.64][12377-2-4-1.70][12398-2-4--0.20][12503-1-2-1.65][12617-0-1-1.39][12685-3-2-0.33][12738-2-2-0.43]
[12742-2-2-2.61][12823-0-3-1.52][13110-1-1-1.67][13240-3-3-0.69][13253-1-1-1.89][13273-0-0-2.88][13634-1-1-1.72][13763-2-2-1.69][13905-3-3--0.07][14060-2-1-2.67]
[14065-3-0-0.78][14147-3-2-0.41][14595-2-2-2.38][14687-2-2-1.69][14788-2-2-1.24][14869-1-1-1.90][14872-3-4-0.84][14877-1-1-0.33][14927-0-3-0.18][15066-0-0-3.79]
[15175-1-1-0.95][15178-2-3-0.27][15375-3-3-0.42][15389-3-3-2.14][15568-2-1-2.00][15675-3-1-1.04][15869-1-2-0.39][16207-3-0-0.58][16236-0-2-0.13][16302-3-0-1.09]
[16331-2-2-2.75][16381-0-0-1.02][16488-1-1-3.17][16495-0-0-2.11][16650-0-0-2.88][16719-1-2-0.86][16801-0-0-4.03][16828-0-0-1.26][17137-3-0-0.78][17245-1-2-0.34]
[17278-3-1-0.32][17282-0-0-0.28][17311-2-2-2.32][17336-2-1-1.68][17608-3-3-2.39][17627-0-0-0.40][17877-3-1-1.76][17924-1-1-0.42][17984-3-0-3.80][18211-0-3-0.77]
[18276-3-0-1.29][18287-1-1-1.96][18394-0-0-1.78][18428-0-1-1.28][18442-0-3-1.21][18478-3-3-0.98][18607-0-0-0.96][18616-0-0-0.40][18663-0-0-2.13][18718-0-0-2.67]
[18766-2-2-2.90][18824-2-2-1.20][18890-3-2-1.15][18930-3-2-1.25][18938-3-3-0.73][19817-1-2-1.51][19839-0-2-0.44][19930-3-3-1.66][19944-0-2-1.10][20036-2-2-2.69]
[20101-3-1-0.44][20474-1-1-1.70][20547-3-0--0.02][20929-2-2-2.90][21245-1-2-2.07][21257-3-4--0.03][21293-1-1-4.00][21316-1-1-3.11][21384-1-1-1.65][21448-1-1-1.60]
[21483-0-0-2.86][21487-2-2-2.14][21714-0-2--0.18][21943-3-2-0.93][21947-0-0-2.07][21948-0-0-5.40][21965-2-2-2.68][21998-1-1-0.89][22025-0-2-0.56][22228-3-3-1.64]
[22446-1-1-2.00][22494-3-0-1.84][22757-0-0-2.91][22811-3-3-0.40][22976-3-2-1.47][22985-3-0-1.94][23014-0-0-2.02][23112-1-1-2.13][23144-3-3-2.42][23168-2-0-0.16]
[23219-0-0-0.22][23363-3-1-0.34][23470-0-1-1.22][23486-2-2-0.78][23497-0-0-2.60][23516-0-0-3.95][23690-1-1-0.62][23921-2-1-1.40][23936-1-2-1.73][24040-3-2-0.53]
[24111-1-4-3.23][24182-0-3-1.99][24238-3-3-1.30][24290-2-0-1.75][24345-0-0-2.03][24364-1-2-1.34][24427-3-0-0.74][24477-2-2-2.43][24495-2-1-1.80][24893-2-1-1.77]
[25012-1-1-1.26][25121-2-4-1.41][25165-3-3-0.18][25183-0-0-1.23][25297-3-1-0.80][25398-0-0-2.31][25574-2-2-0.96][25644-1-1-1.45][25718-1-1-2.63][25774-2-2-1.51]
[26032-3-3-0.68][26051-3-3-2.25][26120-0-0-1.03][26321-1-1-1.44][26732-1-1-1.99][26784-3-3-2.13][26827-3-3-0.78][26833-0-3-2.17][26838-2-2-0.81][26860-1-2-0.81]
[26948-0-0-1.01][27049-3-0-1.16][27098-1-1-0.97][27526-0-0-2.42][27639-3-3-0.07][27698-3-3-1.17][27772-0-0-2.82][27890-1-1-2.16][28040-0-0-0.21][28503-2-2-3.26]
[28577-1-1-2.75][28959-0-0-4.67][29198-3-1-0.42][29777-0-0-4.76][29877-2-1-1.76][30035-1-1-2.08][30098-0-3-0.29][30326-1-1-3.50][30572-2-2-2.00][30716-0-1-1.08]
[30806-2-1-0.12][30906-1-1-2.87][31007-0-0-0.60][31181-3-3-1.00][31238-0-0-0.50][31347-0-0-1.77][31422-2-1-1.17][31429-3-0--0.19][31431-0-0-0.69][31432-1-1-2.54]
[31477-0-0-2.78][31524-1-2-1.63][31597-1-2-3.06][31619-1-0-1.32][31701-0-0-1.57][31755-0-0-0.86][31854-3-3-0.24][32074-1-1-1.26][32078-3-1-0.04][32111-1-1-1.41]
[32127-1-2-2.05][32140-3-3-1.72][32263-2-0-0.28][32365-0-0-1.07][32411-2-0-2.19][32429-3-0-2.23][32473-3-0-1.31][32574-3-3-1.19][32584-0-0--0.01][32622-0-2-0.24]
[32858-3-0-0.98][32969-3-0-2.90][33016-2-2-1.71][33031-1-3-1.39][33035-2-2-2.43][33133-2-2-2.33][33173-2-2-1.50][33175-3-1-1.52][33306-3-2-1.86][33309-2-2-0.76]
[33474-0-3-0.24][33478-2-2-0.31][33618-1-1-0.93][33712-0-3-0.33][33782-2-1-1.41][33914-3-2-0.48][34076-3-3-0.71][34112-2-1-1.57][34138-2-2-1.93][34239-1-1-1.60]
[34364-2-2-2.75][34617-1-1-1.08][34751-3-3-2.58][34783-2-2-1.83][35015-3-2-1.18][35018-1-1-2.36][35288-2-2-1.09][0-4-4-1.66][1-4-0-1.10][2-4-4-1.08]
[3-4-2-0.30][4-4-2--0.17][5-4-1-0.49][6-4-1--0.21][7-4-2-1.04][8-4-2-1.29][9-4-2-0.93][10-4-4-1.94][11-4-2-3.13][12-4-1-0.80]
[14-4-3-0.42][15-4-3-1.89][16-4-2-0.39][17-4-1-0.48][18-4-4-1.38][19-4-0-1.56][20-4-1--0.03][21-4-1-2.32][22-4-4-1.82][23-4-1-1.04]
[24-4-4-2.86][25-4-2-0.30][26-4-1-0.64][27-4-0-0.45][28-4-4-1.74][29-4-1-0.56][30-4-0-1.26][31-4-2-2.22][32-4-1-2.73][33-4-3-0.91]
[34-4-2-0.38][35-4-0-1.71][37-4-2-1.78][39-4-3-0.50][40-4-4-0.69][41-4-2-0.70][42-4-1-1.54][43-4-2-1.20][45-4-2-1.01][46-4-2-1.48]
[47-4-4-1.26][48-4-1-1.32][51-4-4-2.18][52-4-2-1.06][53-4-1-0.91][54-4-4-0.11][55-4-2-0.19][56-4-1-2.15][57-4-3-0.90][58-4-2-3.21]
[59-4-0-0.99][60-4-1-1.01][61-4-4-1.20][62-4-2-1.39][63-4-2-2.63][64-4-1-1.00][65-4-4-2.00][66-4-1-1.86][67-4-1-0.58][68-4-1-1.49]
[69-4-0-1.18][70-4-1-0.61][72-4-2-1.62][73-4-1-1.85][74-4-2-2.19][75-4-0-0.53][77-4-4-2.14][78-4-2-0.52][79-4-2-2.26][80-4-1-1.61]
[81-4-4-1.57][82-4-1-1.23][83-4-2-0.34][84-4-2-0.35][85-4-1-0.89][86-4-1-0.60][87-4-4-1.92][88-4-1-1.53][89-4-4-2.15][90-4-4-0.15]
[91-4-2-0.90][92-4-1-0.65][93-4-4-0.58][94-4-4-2.08][95-4-2-0.57][96-4-4-0.68][97-4-4-0.77][98-4-2-1.38][99-4-4-0.29][100-4-4-1.56]
[101-4-4-0.89][102-4-2-0.75][103-4-3-0.89][104-4-4-1.77][105-4-1-1.89][106-4-1-2.32][107-4-4-1.04][108-4-2-0.65][109-4-1-0.05][110-4-1-0.83]
[111-4-0-2.28][112-4-0-0.52][113-4-2-0.78][114-4-3-0.24][115-4-0-0.38][116-4-4-0.52][117-4-1-1.21][119-4-2-0.79][121-4-1-1.12][122-4-3-0.04]
[124-4-2-1.71][125-4-1-2.09][126-4-2-0.70][127-4-2-1.29][128-4-0-0.02][129-4-1-1.42][130-4-2-0.72][131-4-2-1.58][132-4-0-0.76][133-4-0-1.93]
[135-4-2-1.71][136-4-1-0.82][137-4-1-0.64][138-4-4--0.04][139-4-2-0.62][140-4-2-1.21][141-4-3-1.04][142-4-4-0.69][143-4-4-1.50][144-4-4-3.05]
[145-4-1-2.37][148-4-0-3.57][149-4-2-0.42][150-4-2-1.01][151-4-2-1.61][152-4-1-2.02][153-4-2-2.74][154-4-2-0.17][155-4-1-0.62][156-4-0-0.88]
[157-4-2-0.82][158-4-2-0.43][160-4-1-1.57][161-4-1-2.04][162-4-2-0.37][164-4-2-1.26][165-4-1-1.44][167-4-0-1.46][168-4-4-0.94][170-4-0-0.30]
[171-4-2-0.69][172-4-4-1.85][173-4-1-0.36][174-4-0-2.36][175-4-1-0.63][177-4-0-1.58][178-4-4-1.07][179-4-1-1.81][180-4-4-2.46][181-4-3-0.75]
[182-4-2-0.81][183-4-4-0.36][184-4-2-1.59][186-4-2-0.14][187-4-1-2.12][188-4-2-1.10][189-4-2-1.03][190-4-1-1.30][191-4-4-1.34][192-4-4-0.94]
[193-4-2-3.39][194-4-0-0.08][195-4-0-1.09][196-4-1-0.86][197-4-2-1.35][198-4-4-2.98][199-4-2-0.95]
---------------------------
I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 0.810 | Acc: 59.000% | Wgt Acc: 68.388%
	I - Batch: 100 | Loss: 0.850 | Acc: 57.625% | Wgt Acc: 67.122%
	I - Batch: 150 | Loss: 0.873 | Acc: 56.667% | Wgt Acc: 66.175%
	I - Batch: 200 | Loss: 0.892 | Acc: 55.500% | Wgt Acc: 65.143%
I - num batch: 222
I - Train -- Loss: 0.893 | Acc: 55.540% | Wgt Acc: 65.223% | LR: 1.000000e-03 | Dur: 135.47s
I - Confusion Matrix: [row->prediction - col->label]
[[519.  15.  28. 136. 167.]
 [ 22. 396.  98.  36. 165.]
 [ 38. 126. 534.  49. 347.]
 [104.  30.  51. 311. 111.]
 [ 14.  11.  23.   6. 210.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.319 | Acc: 47.337% | Wgt Acc: 56.764% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  5. 19. 34.]
 [ 2. 42. 16.  1. 28.]
 [ 4. 23. 37.  5. 53.]
 [20.  6. 12. 58. 23.]
 [ 1.  2.  5.  3. 42.]]

I - Local maximum validation set accuracy:  47.34

I - Validation set results: 
[14-1-2-1.12][50-3-3-0.05][124-2-1-0.06][127-0-0-2.65][443-2-2-0.93][567-0-0-2.06][573-1-1-1.84][615-0-3-2.04][695-1-2--0.05][722-3-0-2.89]
[826-0-0-2.00][878-0-3-2.18][1103-0-0-1.89][1212-3-3-1.37][1368-0-0-2.60][2181-2-3-0.67][2476-2-3-0.02][2721-2-2-0.75][2818-1-3-0.75][2886-2-1-1.47]
[3231-2-2-2.46][3333-2-3-1.02][3482-2-2-0.97][3536-3-3-2.01][3625-1-1-2.31][3909-0-0-0.47][4035-0-3-2.82][4140-0-0-2.22][4214-1-3-1.76][4346-1-0-1.08]
[4581-2-1-0.97][4708-3-3-0.92][4838-3-0-1.09][4845-1-1-0.26][4868-0-0-3.20][4939-0-1-1.15][4984-2-3-0.68][5078-1-1-0.36][5396-0-0-3.49][5479-1-1-1.86]
[5717-0-0-0.38][5843-1-1-1.74][5949-3-3-1.71][5987-2-4-1.19][6014-3-3-1.30][6033-3-0-1.41][6313-0-0-1.82][6421-3-3-2.55][6500-1-1-0.35][6583-3-3-0.92]
[6683-3-3-0.78][6825-2-0-0.54][6998-3-3-0.45][7049-3-3-1.56][7517-1-2-1.84][7521-1-2-0.26][7528-1-3-0.98][7949-1-2-0.85][8135-1-0-2.22][8185-3-0-2.27]
[8269-3-4-0.50][8273-3-3-1.43][8543-3-0-4.45][8666-1-1-0.42][8672-0-0-3.94][8903-1-2-0.50][9001-2-2-1.89][9036-2-2-1.96][9281-3-3-0.05][9300-2-2-0.96]
[9571-0-0-0.91][9617-1-1--0.17][9644-2-2-0.25][9705-2-1-0.85][9801-0-3-1.55][9803-3-3-1.74][9865-3-3-2.61][9896-2-2-0.87][10314-1-1-1.34][10337-3-3-3.42]
[10403-0-0-0.85][10653-2-1-1.03][10704-2-2-0.56][10719-1-1-1.62][10727-1-1-0.77][10836-0-0-4.78][10969-2-2-1.00][11042-0-0-1.69][11088-1-1-1.50][11322-0-0-2.35]
[11398-2-2-0.78][11499-0-0-0.86][11502-3-3-1.79][11512-3-3-0.63][11608-1-2-2.37][11610-0-0-0.59][11692-0-3-1.30][11905-0-0-4.29][11993-1-1-1.39][12002-2-0-0.62]
[12052-0-0-2.50][12201-0-3-2.93][12235-2-2-0.93][12320-1-0-1.53][12377-2-4-0.97][12398-2-3-0.15][12503-1-4-1.33][12617-0-2-1.62][12685-3-3-0.85][12738-2-1-0.03]
[12742-2-2-1.40][12823-0-3-2.20][13110-1-2-0.29][13240-3-3-1.54][13253-1-1-2.03][13273-0-0-3.26][13634-1-2-0.95][13763-2-3-1.13][13905-3-3-0.74][14060-2-1-0.87]
[14065-3-0-1.89][14147-3-3-0.68][14595-2-2-1.24][14687-2-2-0.78][14788-2-2-0.60][14869-1-1-1.60][14872-3-0-0.51][14877-1-1-1.46][14927-0-3-1.49][15066-0-0-4.24]
[15175-1-1-0.37][15178-2-3-0.48][15375-3-3-1.55][15389-3-3-3.07][15568-2-4-0.05][15675-3-3-1.02][15869-1-2-0.75][16207-3-0-0.58][16236-0-3-0.61][16302-3-0-0.68]
[16331-2-2-1.16][16381-0-0-1.61][16488-1-1-1.78][16495-0-0-1.03][16650-0-0-3.98][16719-1-0-0.23][16801-0-0-4.71][16828-0-3-1.39][17137-3-0-1.79][17245-1-2-0.71]
[17278-3-3-0.21][17282-0-0-0.53][17311-2-2-1.15][17336-2-1-0.97][17608-3-3-3.08][17627-0-0-0.28][17877-3-1-1.03][17924-1-2-0.40][17984-3-0-3.01][18211-0-3-1.36]
[18276-3-0-1.95][18287-1-1-1.63][18394-0-0-2.10][18428-0-0-3.36][18442-0-3-2.04][18478-3-3-1.67][18607-0-0-1.18][18616-0-0-1.07][18663-0-0-1.71][18718-0-0-2.35]
[18766-2-2-1.72][18824-2-1-0.63][18890-3-2-0.69][18930-3-2-0.30][18938-3-3-1.26][19817-1-2-1.18][19839-0-2-0.18][19930-3-3-2.26][19944-0-2-0.47][20036-2-2-2.14]
[20101-3-3-1.80][20474-1-1-1.83][20547-3-0-0.65][20929-2-1-1.92][21245-1-1-1.44][21257-3-3-0.74][21293-1-1-2.07][21316-1-3-0.77][21384-1-1-0.50][21448-1-1-1.23]
[21483-0-0-2.89][21487-2-2-1.56][21714-0-0-0.78][21943-3-2-0.58][21947-0-0-1.79][21948-0-0-5.95][21965-2-1-1.79][21998-1-1-1.10][22025-0-2-0.59][22228-3-3-2.84]
[22446-1-2-1.75][22494-3-3-1.96][22757-0-0-3.15][22811-3-3-3.09][22976-3-4-0.70][22985-3-3-2.70][23014-0-3-2.83][23112-1-1-1.79][23144-3-3-3.12][23168-2-0-2.74]
[23219-0-0-1.35][23363-3-3-2.30][23470-0-1-1.15][23486-2-2-0.84][23497-0-3-3.55][23516-0-0-3.53][23690-1-2-0.80][23921-2-1-1.18][23936-1-2-1.15][24040-3-2--0.04]
[24111-1-4-2.34][24182-0-3-3.34][24238-3-3-2.66][24290-2-0-2.19][24345-0-0-2.26][24364-1-2-0.58][24427-3-3-1.12][24477-2-2-1.29][24495-2-1-0.65][24893-2-1-1.19]
[25012-1-2-0.21][25121-2-4-0.97][25165-3-3-1.13][25183-0-0-1.22][25297-3-3-1.94][25398-0-0-2.32][25574-2-2-1.02][25644-1-1-1.87][25718-1-2-0.06][25774-2-2-1.68]
[26032-3-3-2.17][26051-3-3-2.42][26120-0-0-0.55][26321-1-1-1.22][26732-1-1-1.28][26784-3-3-3.87][26827-3-3-1.48][26833-0-3-2.45][26838-2-3-0.35][26860-1-2-0.39]
[26948-0-0-1.29][27049-3-0-1.35][27098-1-1-0.83][27526-0-0-2.43][27639-3-3-0.94][27698-3-3-2.15][27772-0-0-2.55][27890-1-1-1.94][28040-0-0-0.89][28503-2-2-1.69]
[28577-1-1-1.94][28959-0-0-4.03][29198-3-3-0.14][29777-0-0-4.26][29877-2-1-0.83][30035-1-1-1.63][30098-0-0-1.40][30326-1-1-2.68][30572-2-2-1.47][30716-0-4-0.67]
[30806-2-3-0.49][30906-1-1-2.35][31007-0-0-0.80][31181-3-3-1.66][31238-0-3-1.35][31347-0-3-2.31][31422-2-2-0.69][31429-3-0-0.43][31431-0-0-1.09][31432-1-1-1.52]
[31477-0-3-3.16][31524-1-2-0.80][31597-1-2-2.00][31619-1-0-0.65][31701-0-0-1.67][31755-0-0-1.38][31854-3-3-1.03][32074-1-1-0.06][32078-3-3-0.98][32111-1-1-0.79]
[32127-1-2-1.36][32140-3-3-2.87][32263-2-0-0.53][32365-0-0-0.44][32411-2-3-2.85][32429-3-0-1.54][32473-3-0-1.23][32574-3-3-1.98][32584-0-0-0.69][32622-0-0--0.04]
[32858-3-0-0.83][32969-3-0-2.88][33016-2-2-0.85][33031-1-3-2.25][33035-2-2-1.61][33133-2-2-1.16][33173-2-1-0.98][33175-3-4-0.89][33306-3-2-0.93][33309-2-3-0.62]
[33474-0-0-0.78][33478-2-3-0.08][33618-1-1-0.48][33712-0-3-1.16][33782-2-2-0.97][33914-3-3-1.45][34076-3-3-0.81][34112-2-2-1.66][34138-2-2-1.43][34239-1-1-0.81]
[34364-2-2-2.05][34617-1-3--0.02][34751-3-3-3.42][34783-2-4-1.02][35015-3-3-0.87][35018-1-1-1.46][35288-2-2-0.32][0-4-4-0.95][1-4-4-1.31][2-4-0-1.23]
[3-4-4-0.62][4-4-3-0.13][5-4-3-0.08][6-4-4-0.90][7-4-0-1.08][8-4-2-0.52][9-4-1-0.69][10-4-4-1.65][11-4-2-2.32][12-4-2-1.13]
[14-4-3-0.28][15-4-3-2.71][16-4-4-0.19][17-4-0-0.70][18-4-2-1.16][19-4-3-2.34][20-4-3-0.19][21-4-2-1.70][22-4-4-1.17][23-4-2-0.58]
[24-4-4-0.10][25-4-3-0.59][26-4-3-0.42][27-4-3--0.05][28-4-4-0.98][29-4-1-0.85][30-4-0-1.12][31-4-2-1.08][32-4-1-1.59][33-4-3-1.03]
[34-4-2-0.17][35-4-0-1.50][37-4-2-0.74][39-4-0-2.59][40-4-0-0.22][41-4-2-0.19][42-4-1-0.78][43-4-2--0.01][45-4-2-1.52][46-4-4-0.60]
[47-4-4-1.34][48-4-1-0.22][51-4-4-0.86][52-4-2-0.27][53-4-0--0.20][54-4-3-0.60][55-4-0-1.03][56-4-1-0.82][57-4-3-1.48][58-4-2-2.67]
[59-4-0-1.21][60-4-1--0.19][61-4-4-1.67][62-4-2-0.76][63-4-2-1.78][64-4-0--0.19][65-4-4-1.71][66-4-4-1.72][67-4-1-0.23][68-4-2-0.56]
[69-4-3-0.62][70-4-4-1.15][72-4-1-1.23][73-4-1-0.94][74-4-2-1.22][75-4-0-1.47][77-4-4-2.68][78-4-2-0.21][79-4-2-1.68][80-4-4-0.91]
[81-4-1-1.12][82-4-1-0.90][83-4-1-0.08][84-4-0-0.06][85-4-4-0.57][86-4-2-0.37][87-4-4-1.67][88-4-4-0.54][89-4-3-0.37][90-4-0-0.13]
[91-4-2-0.48][92-4-3-0.79][93-4-4-0.52][94-4-4-1.30][95-4-2-0.62][96-4-4-0.73][97-4-0-0.88][98-4-1-0.05][99-4-4-0.64][100-4-1-1.08]
[101-4-4-1.30][102-4-2-0.37][103-4-3-1.53][104-4-4-1.06][105-4-2-1.26][106-4-4-1.15][107-4-0-0.98][108-4-1-0.70][109-4-1-0.57][110-4-2-0.02]
[111-4-0-2.67][112-4-0-1.20][113-4-3-0.77][114-4-3-0.56][115-4-0-0.98][116-4-1-0.57][117-4-2-0.59][119-4-2-1.67][121-4-2-0.54][122-4-3-0.90]
[124-4-2-0.63][125-4-4-1.61][126-4-2-0.23][127-4-2-1.50][128-4-0-0.08][129-4-1-0.92][130-4-4-0.54][131-4-2-1.12][132-4-3-0.43][133-4-0-1.90]
[135-4-2-1.04][136-4-2--0.25][137-4-1-0.67][138-4-0-0.30][139-4-0-0.21][140-4-2-0.03][141-4-3-0.67][142-4-4-1.26][143-4-4-1.51][144-4-4-1.91]
[145-4-2-1.01][148-4-0-3.93][149-4-2-0.34][150-4-2-1.43][151-4-2-1.15][152-4-2-0.21][153-4-2-2.39][154-4-4-0.49][155-4-4-0.48][156-4-3-0.95]
[157-4-0-1.11][158-4-2--0.12][160-4-1-1.25][161-4-1-1.45][162-4-2-0.04][164-4-2-0.50][165-4-2-0.12][167-4-0-1.40][168-4-4-0.09][170-4-0-1.63]
[171-4-2-0.77][172-4-4-0.69][173-4-0-0.70][174-4-0-1.76][175-4-4-0.47][177-4-0-1.69][178-4-1-0.48][179-4-1-0.50][180-4-4-1.85][181-4-2-0.24]
[182-4-2-0.94][183-4-4-0.74][184-4-2-0.64][186-4-0-0.83][187-4-1-1.23][188-4-2-0.35][189-4-4-0.27][190-4-1-0.25][191-4-0-0.11][192-4-1-0.64]
[193-4-1-2.23][194-4-0-0.26][195-4-0-0.66][196-4-3-0.41][197-4-2-1.19][198-4-4-2.38][199-4-2-0.36]
---------------------------
I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 0.844 | Acc: 57.500% | Wgt Acc: 67.869%
	I - Batch: 100 | Loss: 0.856 | Acc: 56.188% | Wgt Acc: 66.925%
	I - Batch: 150 | Loss: 0.859 | Acc: 56.250% | Wgt Acc: 66.728%
	I - Batch: 200 | Loss: 0.863 | Acc: 56.531% | Wgt Acc: 67.072%
I - num batch: 222
I - Train -- Loss: 0.854 | Acc: 57.260% | Wgt Acc: 67.807% | LR: 1.000000e-03 | Dur: 138.27s
I - Confusion Matrix: [row->prediction - col->label]
[[524.  22.  23. 126. 175.]
 [ 28. 419.  91.  24. 156.]
 [ 33. 108. 557.  54. 361.]
 [102.  23.  42. 333. 110.]
 [ 10.   6.  21.   1. 198.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.309 | Acc: 47.140% | Wgt Acc: 56.925% | Dur: 15.54s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  2.  4. 14. 18.]
 [ 2. 39.  9.  3. 21.]
 [13. 28. 55. 16. 93.]
 [17.  6.  3. 51.  9.]
 [ 1.  3.  4.  2. 39.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 0.833 | Acc: 57.125% | Wgt Acc: 68.163%
	I - Batch: 100 | Loss: 0.823 | Acc: 57.875% | Wgt Acc: 68.264%
	I - Batch: 150 | Loss: 0.821 | Acc: 57.958% | Wgt Acc: 68.810%
	I - Batch: 200 | Loss: 0.826 | Acc: 58.000% | Wgt Acc: 68.561%
I - num batch: 222
I - Train -- Loss: 0.837 | Acc: 57.626% | Wgt Acc: 68.298% | LR: 1.000000e-03 | Dur: 135.94s
I - Confusion Matrix: [row->prediction - col->label]
[[546.  22.  27. 111. 194.]
 [ 17. 412.  92.  26. 143.]
 [ 38. 116. 551.  57. 358.]
 [ 85.  19.  47. 338. 108.]
 [ 11.   9.  17.   6. 197.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.402 | Acc: 46.351% | Wgt Acc: 57.733% | Dur: 14.65s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  6.  7. 22. 24.]
 [ 0. 35.  8.  1. 17.]
 [ 4. 30. 50.  8. 85.]
 [14.  6. 10. 53. 24.]
 [ 3.  1.  0.  2. 30.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 0.777 | Acc: 59.625% | Wgt Acc: 70.336%
	I - Batch: 100 | Loss: 0.807 | Acc: 59.188% | Wgt Acc: 69.968%
	I - Batch: 150 | Loss: 0.803 | Acc: 59.542% | Wgt Acc: 70.516%
	I - Batch: 200 | Loss: 0.806 | Acc: 59.719% | Wgt Acc: 70.502%
I - num batch: 222
I - Train -- Loss: 0.801 | Acc: 59.994% | Wgt Acc: 70.821% | LR: 1.000000e-03 | Dur: 135.09s
I - Confusion Matrix: [row->prediction - col->label]
[[545.  18.  21. 105. 206.]
 [ 25. 426.  73.  27. 140.]
 [ 29.  99. 582.  38. 337.]
 [ 84.  24.  39. 359. 101.]
 [ 14.  11.  19.   9. 216.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.333 | Acc: 47.535% | Wgt Acc: 57.848% | Dur: 17.02s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  4.  4. 14. 18.]
 [ 5. 51. 25.  7. 51.]
 [ 3. 13. 35.  9. 57.]
 [14.  6.  9. 53. 16.]
 [ 2.  4.  2.  3. 38.]]

I - Local maximum validation set accuracy:  47.53

I - Validation set results: 
[14-1-1-1.73][50-3-1-0.14][124-2-2-0.94][127-0-0-2.66][443-2-2-2.21][567-0-0-1.58][573-1-1-2.54][615-0-0-1.39][695-1-0--0.22][722-3-0-2.38]
[826-0-0-2.16][878-0-3-1.54][1103-0-0-0.29][1212-3-3-1.33][1368-0-0-3.54][2181-2-3-0.70][2476-2-2-0.30][2721-2-1-2.12][2818-1-3-0.72][2886-2-1-2.41]
[3231-2-2-3.22][3333-2-3-0.27][3482-2-2-1.70][3536-3-3-1.18][3625-1-1-3.81][3909-0-0-1.60][4035-0-3-2.56][4140-0-0-1.97][4214-1-3-0.82][4346-1-0-1.74]
[4581-2-2-1.28][4708-3-3-0.92][4838-3-3-0.32][4845-1-1-1.06][4868-0-0-2.95][4939-0-1-0.62][4984-2-2-1.84][5078-1-2-0.25][5396-0-0-3.41][5479-1-1-2.96]
[5717-0-0-1.09][5843-1-1-2.17][5949-3-3-1.46][5987-2-4-1.31][6014-3-1-1.49][6033-3-2-1.09][6313-0-0-2.84][6421-3-3-3.02][6500-1-1-0.95][6583-3-2-0.36]
[6683-3-3-1.06][6825-2-1-0.76][6998-3-3-0.59][7049-3-3-1.10][7517-1-1-3.28][7521-1-1-0.40][7528-1-3-1.45][7949-1-2-1.86][8135-1-0-0.96][8185-3-0-2.75]
[8269-3-2-0.41][8273-3-3-1.46][8543-3-0-3.60][8666-1-1-0.97][8672-0-0-3.10][8903-1-1-0.49][9001-2-2-0.79][9036-2-2-2.01][9281-3-2-0.38][9300-2-2-2.65]
[9571-0-3-0.71][9617-1-1-0.83][9644-2-2-1.78][9705-2-1-1.69][9801-0-3-0.44][9803-3-3-1.09][9865-3-3-2.73][9896-2-1-1.46][10314-1-1-1.46][10337-3-3-3.56]
[10403-0-0--0.04][10653-2-1-1.99][10704-2-1-1.85][10719-1-1-3.24][10727-1-1-2.56][10836-0-0-6.55][10969-2-3-0.98][11042-0-0-1.32][11088-1-1-3.18][11322-0-0-2.03]
[11398-2-2-1.73][11499-0-0-1.40][11502-3-3-1.54][11512-3-3-0.62][11608-1-1-2.12][11610-0-0-1.65][11692-0-3-0.57][11905-0-0-3.31][11993-1-1-1.64][12002-2-0-2.71]
[12052-0-0-3.17][12201-0-3-2.54][12235-2-1-1.11][12320-1-4-1.38][12377-2-4-0.63][12398-2-3-0.63][12503-1-4-2.08][12617-0-2-1.15][12685-3-3-0.16][12738-2-1-0.25]
[12742-2-2-1.99][12823-0-0-2.12][13110-1-1-0.72][13240-3-3-1.49][13253-1-1-4.51][13273-0-0-3.36][13634-1-2-0.86][13763-2-2-0.87][13905-3-3--0.64][14060-2-1-1.57]
[14065-3-3-0.98][14147-3-0-0.11][14595-2-1-1.92][14687-2-2-1.57][14788-2-2-1.87][14869-1-1-2.26][14872-3-4-0.99][14877-1-1-1.87][14927-0-3-0.08][15066-0-0-4.15]
[15175-1-2-0.27][15178-2-3-0.27][15375-3-3-0.62][15389-3-3-2.33][15568-2-1-1.84][15675-3-3-1.10][15869-1-2-0.91][16207-3-0-1.09][16236-0-2-0.28][16302-3-2-0.39]
[16331-2-2-2.46][16381-0-3-0.93][16488-1-1-4.65][16495-0-0-3.74][16650-0-0-4.43][16719-1-2-1.08][16801-0-0-3.82][16828-0-0-1.74][17137-3-0-2.35][17245-1-1-1.26]
[17278-3-4-0.07][17282-0-0-1.23][17311-2-2-1.96][17336-2-1-2.20][17608-3-3-3.50][17627-0-1-0.25][17877-3-1-1.98][17924-1-1-0.82][17984-3-0-4.50][18211-0-3-1.51]
[18276-3-3-1.12][18287-1-1-1.87][18394-0-0-2.20][18428-0-1-0.14][18442-0-3-1.57][18478-3-3-1.09][18607-0-0-1.53][18616-0-0-0.45][18663-0-0-1.84][18718-0-0-2.43]
[18766-2-1-2.06][18824-2-2-0.64][18890-3-3-1.18][18930-3-2-0.54][18938-3-3-1.44][19817-1-2-1.37][19839-0-1-1.00][19930-3-3-2.09][19944-0-1-0.73][20036-2-2-3.43]
[20101-3-1-0.97][20474-1-1-2.26][20547-3-4--0.10][20929-2-2-2.87][21245-1-1-1.51][21257-3-3-0.56][21293-1-1-3.85][21316-1-1-1.89][21384-1-1-1.25][21448-1-1-2.21]
[21483-0-0-4.32][21487-2-2-0.92][21714-0-0-1.22][21943-3-2-0.83][21947-0-0-0.87][21948-0-0-6.46][21965-2-1-1.48][21998-1-1-2.37][22025-0-2-0.94][22228-3-3-3.05]
[22446-1-1-1.09][22494-3-3-1.67][22757-0-0-2.52][22811-3-3-2.60][22976-3-2-0.88][22985-3-3-2.12][23014-0-0-2.34][23112-1-1-2.44][23144-3-3-2.61][23168-2-0-1.47]
[23219-0-0-1.09][23363-3-3-1.65][23470-0-0-0.84][23486-2-2-0.77][23497-0-3-3.80][23516-0-0-3.13][23690-1-4-0.95][23921-2-1-2.83][23936-1-2-1.29][24040-3-1-0.26]
[24111-1-4-1.59][24182-0-0-3.68][24238-3-3-2.55][24290-2-0-1.89][24345-0-0-0.90][24364-1-2-1.48][24427-3-0-1.57][24477-2-2-2.10][24495-2-1-1.16][24893-2-2-1.15]
[25012-1-2--0.00][25121-2-2-1.73][25165-3-3-0.38][25183-0-0-1.44][25297-3-3-1.23][25398-0-0-3.17][25574-2-2-1.45][25644-1-1-3.78][25718-1-1-1.13][25774-2-2-1.05]
[26032-3-0-1.29][26051-3-3-1.97][26120-0-4-0.79][26321-1-1-2.17][26732-1-1-2.36][26784-3-3-2.78][26827-3-3-0.84][26833-0-3-1.89][26838-2-1-0.29][26860-1-1-1.33]
[26948-0-0-2.21][27049-3-0-2.13][27098-1-1-1.48][27526-0-0-1.19][27639-3-3-0.26][27698-3-3-1.61][27772-0-0-1.32][27890-1-1-2.68][28040-0-0-1.13][28503-2-2-2.92]
[28577-1-1-2.70][28959-0-0-3.90][29198-3-3-0.82][29777-0-0-4.39][29877-2-1-2.11][30035-1-2-1.80][30098-0-0-1.47][30326-1-1-3.82][30572-2-2-2.46][30716-0-4-1.03]
[30806-2-3-0.45][30906-1-1-3.25][31007-0-0-2.08][31181-3-3-1.76][31238-0-3-1.01][31347-0-0-2.25][31422-2-1-0.14][31429-3-3-0.23][31431-0-0-0.70][31432-1-1-2.51]
[31477-0-0-2.71][31524-1-0-0.04][31597-1-2-1.68][31619-1-3-0.16][31701-0-0-0.71][31755-0-0-1.04][31854-3-3-0.73][32074-1-3--0.16][32078-3-0-0.46][32111-1-1-2.96]
[32127-1-2-2.73][32140-3-3-2.87][32263-2-0-0.37][32365-0-0-2.49][32411-2-3-2.15][32429-3-0-1.81][32473-3-3-0.62][32574-3-0-2.36][32584-0-3-0.80][32622-0-0--0.31]
[32858-3-0-0.71][32969-3-3-2.85][33016-2-2-2.24][33031-1-3-2.49][33035-2-2-3.10][33133-2-1-1.33][33173-2-1-1.96][33175-3-1-0.72][33306-3-1-1.14][33309-2-3-0.68]
[33474-0-0-1.66][33478-2-3-0.07][33618-1-1-2.42][33712-0-0-1.07][33782-2-1-2.36][33914-3-3-1.31][34076-3-3-0.49][34112-2-2-0.97][34138-2-1-0.20][34239-1-1-1.83]
[34364-2-2-1.26][34617-1-1-0.16][34751-3-3-2.48][34783-2-2-1.20][35015-3-2-1.54][35018-1-1-1.68][35288-2-1-0.89][0-4-2-1.06][1-4-4-1.16][2-4-4-0.62]
[3-4-2-1.31][4-4-2-0.67][5-4-1-3.02][6-4-4-1.99][7-4-2-1.04][8-4-1-0.70][9-4-1-1.48][10-4-4-1.61][11-4-2-2.69][12-4-2-1.54]
[14-4-3-1.01][15-4-3-1.38][16-4-4-1.40][17-4-1-1.11][18-4-4-0.74][19-4-3-1.90][20-4-2--0.06][21-4-2-2.15][22-4-4-0.83][23-4-2-0.40]
[24-4-4-3.56][25-4-3-0.91][26-4-1-0.32][27-4-2-0.37][28-4-4-0.34][29-4-1-0.78][30-4-0-0.72][31-4-2-1.53][32-4-1-1.47][33-4-2-1.48]
[34-4-2--0.04][35-4-3-1.10][37-4-2-1.08][39-4-0-3.09][40-4-2-0.07][41-4-2-0.36][42-4-2-1.02][43-4-2-0.29][45-4-2-0.30][46-4-2-1.41]
[47-4-4-1.99][48-4-4-1.13][51-4-4-2.12][52-4-1-0.42][53-4-0--0.23][54-4-3-0.76][55-4-2-0.51][56-4-1-1.41][57-4-3-1.32][58-4-2-2.14]
[59-4-0-1.79][60-4-1-0.82][61-4-2-0.41][62-4-3-0.12][63-4-2-1.97][64-4-2-0.10][65-4-4-1.45][66-4-1-2.73][67-4-1-0.55][68-4-3-1.08]
[69-4-2-0.30][70-4-4-0.66][72-4-1-1.93][73-4-1-0.89][74-4-2-0.56][75-4-3-0.21][77-4-4-3.06][78-4-2-0.84][79-4-2-0.91][80-4-1-1.39]
[81-4-1-1.75][82-4-1-1.46][83-4-1-1.39][84-4-2-2.06][85-4-4-1.36][86-4-1-1.47][87-4-4-1.81][88-4-4-1.07][89-4-2-1.33][90-4-4-1.65]
[91-4-2-1.05][92-4-0-0.32][93-4-0-0.31][94-4-4-2.13][95-4-2-0.57][96-4-1-0.86][97-4-0-0.20][98-4-1-0.20][99-4-4-1.23][100-4-1-2.95]
[101-4-4-2.30][102-4-2-1.24][103-4-3-1.20][104-4-4-0.69][105-4-4-2.70][106-4-1-2.15][107-4-1-1.30][108-4-1--0.21][109-4-1-0.97][110-4-2--0.17]
[111-4-0-2.56][112-4-2-0.58][113-4-2-0.62][114-4-3-1.09][115-4-0--0.22][116-4-0-0.35][117-4-4-0.92][119-4-2-0.46][121-4-1-1.89][122-4-3-0.83]
[124-4-1-0.43][125-4-1-1.66][126-4-4-1.66][127-4-2-0.84][128-4-1-0.41][129-4-1-1.28][130-4-2-0.65][131-4-2-1.84][132-4-2--0.19][133-4-4-1.90]
[135-4-2-1.08][136-4-1-0.70][137-4-2-0.45][138-4-2--0.01][139-4-2-1.11][140-4-1-0.34][141-4-0-0.87][142-4-4-2.23][143-4-4-1.64][144-4-4-2.38]
[145-4-2-1.54][148-4-0-4.55][149-4-1-0.41][150-4-2-0.96][151-4-2-1.30][152-4-2-0.67][153-4-1-1.03][154-4-2-1.13][155-4-4-1.41][156-4-3-0.89]
[157-4-0-1.39][158-4-2-0.17][160-4-1-2.79][161-4-1-1.25][162-4-1--0.26][164-4-3-0.12][165-4-1-0.86][167-4-0-0.80][168-4-4-0.60][170-4-3-1.03]
[171-4-1-1.73][172-4-4-1.19][173-4-4-0.25][174-4-0-2.35][175-4-1-0.97][177-4-0-1.02][178-4-4-1.05][179-4-1-0.85][180-4-4-2.08][181-4-2-0.80]
[182-4-2-1.39][183-4-4-1.07][184-4-1-1.67][186-4-0--0.19][187-4-1-2.07][188-4-2-0.62][189-4-1-1.58][190-4-1-1.28][191-4-2-0.56][192-4-1-0.86]
[193-4-1-1.89][194-4-1-0.92][195-4-0-0.60][196-4-2-0.39][197-4-1-1.25][198-4-4-3.76][199-4-1-1.29]
---------------------------
I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 0.734 | Acc: 60.250% | Wgt Acc: 71.235%
	I - Batch: 100 | Loss: 0.722 | Acc: 62.625% | Wgt Acc: 72.981%
	I - Batch: 150 | Loss: 0.733 | Acc: 61.958% | Wgt Acc: 72.649%
	I - Batch: 200 | Loss: 0.718 | Acc: 62.406% | Wgt Acc: 73.201%
I - num batch: 222
I - Train -- Loss: 0.719 | Acc: 62.532% | Wgt Acc: 73.404% | LR: 5.000000e-04 | Dur: 133.84s
I - Confusion Matrix: [row->prediction - col->label]
[[558.  13.  21.  90. 168.]
 [ 18. 445.  66.  25. 154.]
 [ 28.  88. 592.  34. 323.]
 [ 79.  23.  39. 382. 114.]
 [ 14.   9.  16.   7. 241.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.139 | Acc: 56.016% | Wgt Acc: 61.792% | Dur: 14.82s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  3.  7. 23. 19.]
 [ 1. 44.  8.  2. 28.]
 [ 4. 20. 49.  6. 49.]
 [ 5.  4.  5. 48. 11.]
 [ 8.  7.  6.  7. 73.]]

I - Local maximum validation set accuracy:  56.02

I - Validation set results: 
[14-1-2-1.88][50-3-3-0.80][124-2-2-0.86][127-0-0-3.78][443-2-2-3.06][567-0-0-1.56][573-1-1-1.87][615-0-0-2.05][695-1-2-1.05][722-3-0-3.08]
[826-0-0-2.82][878-0-0-1.90][1103-0-4-0.45][1212-3-3-1.14][1368-0-0-3.08][2181-2-0-1.09][2476-2-2-1.02][2721-2-2-2.08][2818-1-3-1.10][2886-2-1-1.32]
[3231-2-2-4.11][3333-2-2-1.26][3482-2-2-1.28][3536-3-3-0.71][3625-1-1-3.02][3909-0-0-1.79][4035-0-0-1.20][4140-0-0-2.07][4214-1-1-0.38][4346-1-0-0.44]
[4581-2-2-1.73][4708-3-2-0.01][4838-3-3-0.68][4845-1-2-1.26][4868-0-0-3.61][4939-0-2-0.68][4984-2-2-2.56][5078-1-2-0.33][5396-0-0-4.00][5479-1-1-1.62]
[5717-0-0-1.75][5843-1-1-1.65][5949-3-0-1.14][5987-2-4-1.51][6014-3-3-1.61][6033-3-0-1.10][6313-0-0-1.84][6421-3-3-2.84][6500-1-1-0.24][6583-3-2-0.94]
[6683-3-3-0.50][6825-2-1-0.85][6998-3-3-0.07][7049-3-3-0.76][7517-1-1-2.41][7521-1-2-0.16][7528-1-3-1.64][7949-1-2-1.84][8135-1-0-0.98][8185-3-0-2.49]
[8269-3-4-0.72][8273-3-3-1.96][8543-3-0-3.67][8666-1-1-2.72][8672-0-0-2.26][8903-1-2-2.38][9001-2-2-1.22][9036-2-2-2.61][9281-3-3-0.94][9300-2-2-2.27]
[9571-0-0-0.85][9617-1-1-1.86][9644-2-2-1.63][9705-2-4-0.67][9801-0-0-0.82][9803-3-3-1.14][9865-3-3-2.64][9896-2-2-1.68][10314-1-2-1.37][10337-3-3-3.29]
[10403-0-4-0.25][10653-2-1-1.61][10704-2-1-1.94][10719-1-1-3.50][10727-1-1-1.43][10836-0-0-7.16][10969-2-3-0.69][11042-0-0-1.79][11088-1-1-3.23][11322-0-0-2.42]
[11398-2-4-2.20][11499-0-0-0.83][11502-3-3-1.63][11512-3-3-0.81][11608-1-2-1.94][11610-0-0-1.12][11692-0-0-0.57][11905-0-0-2.72][11993-1-2-1.11][12002-2-0-2.57]
[12052-0-0-2.59][12201-0-3-1.66][12235-2-2-1.19][12320-1-4-1.57][12377-2-2-0.97][12398-2-4-0.01][12503-1-2-0.76][12617-0-2-1.00][12685-3-3-0.08][12738-2-0-0.35]
[12742-2-2-3.81][12823-0-0-2.21][13110-1-1-0.60][13240-3-0-1.73][13253-1-1-2.27][13273-0-0-4.36][13634-1-2-1.32][13763-2-3-1.07][13905-3-4--0.65][14060-2-1-0.27]
[14065-3-0-1.52][14147-3-2-0.09][14595-2-2-2.44][14687-2-2-2.06][14788-2-2-2.30][14869-1-1-3.20][14872-3-4-0.92][14877-1-1-1.61][14927-0-0--0.47][15066-0-0-4.47]
[15175-1-1-0.95][15178-2-3-0.65][15375-3-3-0.80][15389-3-3-2.11][15568-2-1-1.64][15675-3-3-1.19][15869-1-3--0.21][16207-3-0-0.63][16236-0-2-0.15][16302-3-0-0.77]
[16331-2-2-3.29][16381-0-0-0.61][16488-1-1-2.59][16495-0-0-3.19][16650-0-0-4.37][16719-1-2-0.46][16801-0-0-4.03][16828-0-0-1.49][17137-3-0-2.38][17245-1-1-0.54]
[17278-3-0-0.21][17282-0-0-0.28][17311-2-2-2.72][17336-2-1-2.47][17608-3-3-3.10][17627-0-0-0.88][17877-3-1-1.96][17924-1-1-0.60][17984-3-0-3.97][18211-0-3-2.14]
[18276-3-3-1.41][18287-1-1-0.40][18394-0-0-1.77][18428-0-0-2.63][18442-0-3-1.58][18478-3-3-0.83][18607-0-0-1.76][18616-0-0-0.74][18663-0-0-2.35][18718-0-0-3.10]
[18766-2-2-2.29][18824-2-2-0.96][18890-3-3-1.02][18930-3-2-0.75][18938-3-3-0.67][19817-1-2-1.46][19839-0-4-0.65][19930-3-3-2.07][19944-0-4-2.61][20036-2-2-3.53]
[20101-3-3-0.88][20474-1-1-1.57][20547-3-4-0.71][20929-2-2-2.84][21245-1-2-1.17][21257-3-4-0.05][21293-1-1-2.65][21316-1-1-3.74][21384-1-4-1.96][21448-1-1-1.54]
[21483-0-0-3.38][21487-2-2-2.59][21714-0-0-0.78][21943-3-3-0.89][21947-0-0-2.27][21948-0-0-5.74][21965-2-2-2.05][21998-1-1-1.44][22025-0-2-0.52][22228-3-3-2.83]
[22446-1-1-0.92][22494-3-0-1.40][22757-0-0-3.52][22811-3-0--0.13][22976-3-2-1.01][22985-3-3-2.28][23014-0-0-3.00][23112-1-1-2.53][23144-3-3-2.58][23168-2-3-0.61]
[23219-0-0-0.41][23363-3-3-0.41][23470-0-4-0.06][23486-2-2-1.17][23497-0-0-2.56][23516-0-0-3.56][23690-1-1-1.63][23921-2-2-2.68][23936-1-2-2.62][24040-3-4-1.90]
[24111-1-4-2.68][24182-0-0-4.68][24238-3-3-0.67][24290-2-0-1.88][24345-0-0-1.25][24364-1-2-1.20][24427-3-0-1.82][24477-2-2-2.90][24495-2-0-0.11][24893-2-2-1.46]
[25012-1-4-0.26][25121-2-4-1.65][25165-3-3-0.55][25183-0-0-1.56][25297-3-3-0.21][25398-0-0-2.84][25574-2-2-1.26][25644-1-1-4.31][25718-1-4-0.27][25774-2-2-0.73]
[26032-3-3-0.70][26051-3-3-2.60][26120-0-4-2.01][26321-1-1-1.66][26732-1-1-1.63][26784-3-3-1.65][26827-3-3-0.54][26833-0-3-1.90][26838-2-2-0.87][26860-1-4-2.30]
[26948-0-0-0.96][27049-3-0-2.40][27098-1-1-0.41][27526-0-0-2.27][27639-3-0-0.27][27698-3-3-1.79][27772-0-0-3.20][27890-1-1-1.85][28040-0-4-1.03][28503-2-2-4.66]
[28577-1-1-2.86][28959-0-0-3.83][29198-3-3-1.14][29777-0-0-4.76][29877-2-1-1.15][30035-1-1-2.39][30098-0-0-0.65][30326-1-1-2.73][30572-2-2-2.58][30716-0-4-0.86]
[30806-2-4-0.67][30906-1-1-3.45][31007-0-0-1.09][31181-3-0-0.67][31238-0-0-1.47][31347-0-0-3.57][31422-2-2-1.10][31429-3-3-0.34][31431-0-0-0.95][31432-1-1-2.04]
[31477-0-0-3.83][31524-1-4--0.08][31597-1-2-3.15][31619-1-0-0.52][31701-0-0-3.04][31755-0-0-0.98][31854-3-3-1.15][32074-1-1-0.97][32078-3-3-0.66][32111-1-1-1.26]
[32127-1-2-2.87][32140-3-3-2.15][32263-2-0-0.20][32365-0-0-1.97][32411-2-0-2.38][32429-3-0-2.10][32473-3-0-0.78][32574-3-0-3.21][32584-0-0-1.39][32622-0-1-0.13]
[32858-3-0-1.61][32969-3-0-2.90][33016-2-2-2.84][33031-1-3-2.78][33035-2-2-1.90][33133-2-2-2.10][33173-2-2-0.39][33175-3-4-1.64][33306-3-1-0.74][33309-2-3-0.53]
[33474-0-3-0.83][33478-2-2--0.02][33618-1-1-2.77][33712-0-0-1.08][33782-2-2-1.64][33914-3-3-1.50][34076-3-3--0.06][34112-2-2-2.73][34138-2-2-1.42][34239-1-1-1.80]
[34364-2-2-3.87][34617-1-2-0.74][34751-3-3-1.98][34783-2-2-1.33][35015-3-2-0.90][35018-1-1-2.63][35288-2-2-0.81][0-4-2-1.58][1-4-4-2.07][2-4-4-0.81]
[3-4-2-0.99][4-4-2-0.99][5-4-1-1.74][6-4-4-1.06][7-4-2-0.91][8-4-2-1.06][9-4-1-1.08][10-4-4-0.61][11-4-2-4.30][12-4-2-1.00]
[14-4-0-1.47][15-4-3-1.48][16-4-4-1.22][17-4-0--0.32][18-4-4-1.03][19-4-0-1.92][20-4-2-0.40][21-4-2-1.09][22-4-4-1.31][23-4-2-0.96]
[24-4-4-4.81][25-4-3-0.67][26-4-3--0.23][27-4-2-0.59][28-4-4-2.08][29-4-2--0.07][30-4-0-0.86][31-4-2-1.06][32-4-1-1.41][33-4-3-0.85]
[34-4-4-0.27][35-4-0-1.60][37-4-4-0.68][39-4-0-2.56][40-4-4-0.64][41-4-2-0.10][42-4-2-1.12][43-4-2-1.16][45-4-2-1.12][46-4-2-1.13]
[47-4-4-1.66][48-4-1-1.28][51-4-4-1.49][52-4-4-0.73][53-4-1-0.86][54-4-3-1.03][55-4-4-0.96][56-4-1-1.12][57-4-3-0.77][58-4-2-2.02]
[59-4-0-2.09][60-4-1-0.65][61-4-4-2.22][62-4-2-0.72][63-4-2-2.30][64-4-1-0.35][65-4-4-2.62][66-4-4-2.58][67-4-4-1.14][68-4-3-0.64]
[69-4-0-0.78][70-4-2-1.04][72-4-1-1.12][73-4-2-0.74][74-4-2-1.61][75-4-3-0.39][77-4-4-3.93][78-4-4--0.39][79-4-2-2.18][80-4-4-1.98]
[81-4-4-2.52][82-4-1-1.13][83-4-1-1.02][84-4-4-1.02][85-4-4-1.14][86-4-4-1.44][87-4-4-2.22][88-4-1-0.79][89-4-2-1.03][90-4-4-0.23]
[91-4-1-1.05][92-4-0-0.16][93-4-0-1.53][94-4-4-1.35][95-4-3--0.00][96-4-4-0.96][97-4-4-0.61][98-4-2-1.19][99-4-4-1.08][100-4-1-1.53]
[101-4-4-2.39][102-4-2-0.73][103-4-4-0.42][104-4-1-0.74][105-4-1-1.69][106-4-1-1.67][107-4-4-1.65][108-4-4-0.10][109-4-4-0.30][110-4-2-0.14]
[111-4-0-3.22][112-4-2-0.76][113-4-4-0.28][114-4-2-0.12][115-4-4-0.27][116-4-2-1.02][117-4-1-1.00][119-4-2-2.63][121-4-1-1.03][122-4-4-1.41]
[124-4-2-0.58][125-4-4-2.69][126-4-4-3.88][127-4-2-1.16][128-4-4-0.48][129-4-1-0.79][130-4-2-1.75][131-4-2-0.60][132-4-4-0.61][133-4-4-2.37]
[135-4-2-1.25][136-4-1-0.20][137-4-1--0.16][138-4-4-1.06][139-4-4-0.42][140-4-2-1.07][141-4-2-1.32][142-4-4-2.96][143-4-4-1.75][144-4-4-3.69]
[145-4-1-1.80][148-4-0-4.32][149-4-4-0.63][150-4-2-1.02][151-4-4-2.17][152-4-4-0.44][153-4-2-2.51][154-4-4-3.84][155-4-4-1.55][156-4-3-0.61]
[157-4-2-1.51][158-4-4-0.55][160-4-4-1.40][161-4-2-1.80][162-4-4-0.19][164-4-2-0.88][165-4-4-1.90][167-4-0-1.41][168-4-4-0.56][170-4-0-0.61]
[171-4-4-1.74][172-4-4-1.46][173-4-0-0.90][174-4-0-2.53][175-4-4-1.30][177-4-0-0.55][178-4-4-1.02][179-4-1--0.12][180-4-4-2.45][181-4-3-0.43]
[182-4-1-0.98][183-4-4-1.50][184-4-4-1.87][186-4-0-0.11][187-4-1-1.56][188-4-2-1.64][189-4-4-2.42][190-4-1-0.07][191-4-4-0.66][192-4-1-0.50]
[193-4-2-2.36][194-4-2-1.67][195-4-0-1.22][196-4-2-0.28][197-4-4-1.51][198-4-4-4.93][199-4-4-1.44]
---------------------------
I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 0.678 | Acc: 65.875% | Wgt Acc: 76.195%
	I - Batch: 100 | Loss: 0.693 | Acc: 64.500% | Wgt Acc: 75.138%
	I - Batch: 150 | Loss: 0.692 | Acc: 64.542% | Wgt Acc: 75.341%
	I - Batch: 200 | Loss: 0.682 | Acc: 65.344% | Wgt Acc: 75.953%
I - num batch: 222
I - Train -- Loss: 0.680 | Acc: 65.436% | Wgt Acc: 76.247% | LR: 5.000000e-04 | Dur: 134.11s
I - Confusion Matrix: [row->prediction - col->label]
[[576.  11.  19.  77. 161.]
 [ 21. 469.  50.  23. 143.]
 [ 28.  64. 602.  31. 317.]
 [ 63.  18.  38. 401. 106.]
 [  9.  16.  25.   6. 273.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.213 | Acc: 51.677% | Wgt Acc: 59.451% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  4.  6. 19. 37.]
 [ 1. 41.  6.  1. 17.]
 [ 5. 20. 40.  4. 50.]
 [12.  8. 17. 57. 19.]
 [ 3.  5.  6.  5. 57.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 0.667 | Acc: 65.625% | Wgt Acc: 74.083%
	I - Batch: 100 | Loss: 0.653 | Acc: 65.938% | Wgt Acc: 76.238%
	I - Batch: 150 | Loss: 0.667 | Acc: 65.417% | Wgt Acc: 75.757%
	I - Batch: 200 | Loss: 0.664 | Acc: 65.406% | Wgt Acc: 75.966%
I - num batch: 222
I - Train -- Loss: 0.666 | Acc: 65.492% | Wgt Acc: 75.824% | LR: 5.000000e-04 | Dur: 133.52s
I - Confusion Matrix: [row->prediction - col->label]
[[569.  14.  26.  90. 165.]
 [ 18. 465.  51.  22. 142.]
 [ 19.  65. 614.  34. 287.]
 [ 75.  19.  23. 386. 117.]
 [ 16.  15.  20.   6. 289.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 51.677% | Wgt Acc: 59.440% | Dur: 14.00s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  1.  7. 11. 16.]
 [ 2. 40.  7.  3. 16.]
 [ 5. 23. 52. 10. 78.]
 [17.  5.  3. 57. 13.]
 [ 8.  9.  6.  5. 57.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 0.671 | Acc: 63.750% | Wgt Acc: 75.954%
	I - Batch: 100 | Loss: 0.663 | Acc: 64.812% | Wgt Acc: 76.476%
	I - Batch: 150 | Loss: 0.658 | Acc: 65.083% | Wgt Acc: 76.707%
	I - Batch: 200 | Loss: 0.650 | Acc: 65.969% | Wgt Acc: 77.340%
I - num batch: 222
I - Train -- Loss: 0.648 | Acc: 66.140% | Wgt Acc: 77.270% | LR: 5.000000e-04 | Dur: 137.51s
I - Confusion Matrix: [row->prediction - col->label]
[[574.  10.  19.  92. 173.]
 [ 22. 475.  43.  20. 140.]
 [ 21.  59. 640.  32. 295.]
 [ 68.  19.  20. 390. 125.]
 [ 12.  15.  12.   4. 267.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.209 | Acc: 52.860% | Wgt Acc: 59.636% | Dur: 16.63s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  3.  5. 17. 31.]
 [ 3. 41. 10.  4. 23.]
 [ 2. 17. 42.  2. 37.]
 [18. 10. 14. 59. 25.]
 [ 3.  7.  4.  4. 64.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 0.641 | Acc: 64.750% | Wgt Acc: 75.895%
	I - Batch: 100 | Loss: 0.605 | Acc: 67.438% | Wgt Acc: 78.633%
	I - Batch: 150 | Loss: 0.618 | Acc: 67.708% | Wgt Acc: 78.125%
	I - Batch: 200 | Loss: 0.622 | Acc: 67.719% | Wgt Acc: 78.060%
I - num batch: 222
I - Train -- Loss: 0.628 | Acc: 67.353% | Wgt Acc: 77.839% | LR: 5.000000e-04 | Dur: 133.18s
I - Confusion Matrix: [row->prediction - col->label]
[[572.  14.  21.  82. 167.]
 [ 19. 487.  46.  17. 131.]
 [ 20.  53. 628.  37. 295.]
 [ 67.  13.  18. 399. 104.]
 [ 19.  11.  21.   3. 303.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.104 | Acc: 56.016% | Wgt Acc: 60.950% | Dur: 14.09s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  2.  5. 17. 16.]
 [ 1. 41. 11.  2. 19.]
 [ 2. 17. 43.  4. 45.]
 [16.  9. 10. 58. 22.]
 [ 5.  9.  6.  5. 78.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 0.628 | Acc: 66.875% | Wgt Acc: 77.769%
	I - Batch: 100 | Loss: 0.627 | Acc: 66.938% | Wgt Acc: 77.249%
	I - Batch: 150 | Loss: 0.611 | Acc: 67.625% | Wgt Acc: 77.974%
	I - Batch: 200 | Loss: 0.593 | Acc: 68.375% | Wgt Acc: 78.879%
I - num batch: 222
I - Train -- Loss: 0.586 | Acc: 68.650% | Wgt Acc: 79.427% | LR: 5.000000e-04 | Dur: 134.84s
I - Confusion Matrix: [row->prediction - col->label]
[[582.   9.  16.  72. 160.]
 [ 13. 503.  48.  16. 114.]
 [ 24.  45. 627.  24. 308.]
 [ 64.  15.  26. 416. 111.]
 [ 14.   6.  17.  10. 307.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.206 | Acc: 53.057% | Wgt Acc: 59.636% | Dur: 16.15s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  4.  4. 11. 17.]
 [ 2. 42. 10.  5. 19.]
 [ 5. 22. 52. 10. 67.]
 [20.  5.  6. 56. 12.]
 [ 7.  5.  3.  4. 65.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 0.576 | Acc: 70.250% | Wgt Acc: 78.902%
	I - Batch: 100 | Loss: 0.578 | Acc: 69.812% | Wgt Acc: 79.203%
	I - Batch: 150 | Loss: 0.563 | Acc: 70.792% | Wgt Acc: 80.342%
	I - Batch: 200 | Loss: 0.568 | Acc: 70.656% | Wgt Acc: 80.621%
I - num batch: 222
I - Train -- Loss: 0.566 | Acc: 70.567% | Wgt Acc: 80.569% | LR: 5.000000e-04 | Dur: 138.16s
I - Confusion Matrix: [row->prediction - col->label]
[[581.  10.  11.  80. 154.]
 [ 17. 498.  35.  12. 132.]
 [ 14.  50. 658.  22. 269.]
 [ 72.   8.  18. 414.  93.]
 [ 13.  12.  12.  10. 352.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.154 | Acc: 55.424% | Wgt Acc: 62.115% | Dur: 14.74s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  3.  5. 15. 21.]
 [ 1. 39.  9.  2. 16.]
 [ 3. 25. 54. 10. 60.]
 [13.  5.  3. 54. 15.]
 [ 5.  6.  4.  5. 68.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 0.500 | Acc: 71.875% | Wgt Acc: 82.318%
	I - Batch: 100 | Loss: 0.547 | Acc: 70.750% | Wgt Acc: 81.124%
	I - Batch: 150 | Loss: 0.551 | Acc: 70.750% | Wgt Acc: 80.885%
	I - Batch: 200 | Loss: 0.564 | Acc: 69.781% | Wgt Acc: 80.235%
I - num batch: 222
I - Train -- Loss: 0.560 | Acc: 69.862% | Wgt Acc: 80.374% | LR: 5.000000e-04 | Dur: 136.18s
I - Confusion Matrix: [row->prediction - col->label]
[[589.   9.  13.  78. 160.]
 [ 13. 502.  34.  15. 141.]
 [ 15.  42. 649.  27. 266.]
 [ 65.  14.  17. 411. 106.]
 [ 15.  11.  21.   7. 327.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.177 | Acc: 55.030% | Wgt Acc: 62.127% | Dur: 15.25s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  3.  7. 16. 35.]
 [ 1. 42.  7.  2. 21.]
 [ 2. 22. 46.  5. 43.]
 [13.  6.  9. 58. 15.]
 [ 5.  5.  6.  5. 66.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 0.554 | Acc: 69.875% | Wgt Acc: 78.754%
	I - Batch: 100 | Loss: 0.569 | Acc: 69.500% | Wgt Acc: 79.525%
	I - Batch: 150 | Loss: 0.556 | Acc: 70.458% | Wgt Acc: 80.162%
	I - Batch: 200 | Loss: 0.567 | Acc: 70.375% | Wgt Acc: 80.381%
I - num batch: 222
I - Train -- Loss: 0.561 | Acc: 70.792% | Wgt Acc: 80.783% | LR: 5.000000e-04 | Dur: 134.52s
I - Confusion Matrix: [row->prediction - col->label]
[[591.  12.  11.  71. 166.]
 [ 14. 498.  39.  16. 115.]
 [ 27.  43. 648.  23. 253.]
 [ 50.  12.  19. 419. 111.]
 [ 15.  13.  17.   9. 355.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.298 | Acc: 49.310% | Wgt Acc: 58.298% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  3.  1.  8. 19.]
 [ 1. 37. 11.  2. 21.]
 [ 1. 24. 46.  6. 60.]
 [27.  8. 13. 66. 32.]
 [ 6.  6.  4.  4. 48.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 0.471 | Acc: 74.000% | Wgt Acc: 84.168%
	I - Batch: 100 | Loss: 0.510 | Acc: 72.375% | Wgt Acc: 82.453%
	I - Batch: 150 | Loss: 0.502 | Acc: 73.042% | Wgt Acc: 82.984%
	I - Batch: 200 | Loss: 0.517 | Acc: 72.188% | Wgt Acc: 82.116%
I - num batch: 222
I - Train -- Loss: 0.521 | Acc: 72.145% | Wgt Acc: 81.953% | LR: 5.000000e-04 | Dur: 137.43s
I - Confusion Matrix: [row->prediction - col->label]
[[596.  14.  18.  61. 152.]
 [ 12. 511.  29.  12. 128.]
 [ 21.  35. 645.  27. 248.]
 [ 50.   8.  23. 431.  96.]
 [ 18.  10.  19.   7. 376.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.134 | Acc: 56.805% | Wgt Acc: 61.688% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  4.  3. 16. 19.]
 [ 2. 45. 15.  4. 26.]
 [ 1. 18. 41.  2. 32.]
 [16.  6. 11. 60. 23.]
 [ 7.  5.  5.  4. 80.]]

I - Local maximum validation set accuracy:  56.80

I - Validation set results: 
[14-1-2-1.22][50-3-3-0.13][124-2-2-1.78][127-0-0-3.34][443-2-2-2.71][567-0-0-1.80][573-1-1-2.42][615-0-3-2.90][695-1-0--0.10][722-3-0-2.44]
[826-0-0-3.84][878-0-0-2.47][1103-0-4-0.78][1212-3-3-1.42][1368-0-0-3.13][2181-2-3-1.53][2476-2-2-1.55][2721-2-2-1.55][2818-1-3-1.34][2886-2-1-1.22]
[3231-2-2-3.88][3333-2-2-0.84][3482-2-2-1.69][3536-3-3-0.48][3625-1-1-3.73][3909-0-0-1.87][4035-0-0-1.35][4140-0-0-1.50][4214-1-1-2.03][4346-1-3-1.06]
[4581-2-2-1.76][4708-3-3-0.70][4838-3-3-0.43][4845-1-1-0.39][4868-0-0-3.73][4939-0-4-1.18][4984-2-2-1.71][5078-1-2-0.73][5396-0-0-4.57][5479-1-1-1.98]
[5717-0-0-0.25][5843-1-1-1.66][5949-3-3-1.33][5987-2-4-1.70][6014-3-3-1.63][6033-3-3-0.54][6313-0-3-2.28][6421-3-3-3.67][6500-1-3-0.42][6583-3-2--0.08]
[6683-3-3-0.50][6825-2-1-1.39][6998-3-3-0.33][7049-3-3-0.54][7517-1-1-3.69][7521-1-1-1.57][7528-1-3-1.68][7949-1-2-2.29][8135-1-0-0.88][8185-3-0-4.04]
[8269-3-1-1.84][8273-3-3-2.71][8543-3-0-3.83][8666-1-1-2.55][8672-0-0-4.38][8903-1-1-0.87][9001-2-2-2.00][9036-2-2-3.75][9281-3-3-0.88][9300-2-2-3.74]
[9571-0-3-1.29][9617-1-4-1.51][9644-2-2-1.77][9705-2-1-0.96][9801-0-3-1.44][9803-3-3-0.35][9865-3-3-4.02][9896-2-2-1.21][10314-1-2-0.37][10337-3-3-3.82]
[10403-0-4-0.54][10653-2-1-1.07][10704-2-1-2.38][10719-1-1-3.24][10727-1-4-0.82][10836-0-0-7.09][10969-2-3-0.44][11042-0-0-1.69][11088-1-1-3.40][11322-0-0-2.91]
[11398-2-2-2.72][11499-0-0-2.22][11502-3-3-2.23][11512-3-3-0.24][11608-1-2-2.09][11610-0-0-1.75][11692-0-3-0.74][11905-0-0-2.54][11993-1-1-1.60][12002-2-3-0.74]
[12052-0-0-2.17][12201-0-3-3.33][12235-2-2-1.10][12320-1-4-1.21][12377-2-4-2.04][12398-2-3-1.04][12503-1-2-0.67][12617-0-3-0.80][12685-3-3-0.59][12738-2-0-0.83]
[12742-2-2-3.39][12823-0-0-1.19][13110-1-1-0.04][13240-3-0-1.49][13253-1-1-2.24][13273-0-0-4.09][13634-1-1--0.01][13763-2-3-1.11][13905-3-0--0.15][14060-2-1-1.65]
[14065-3-0-1.48][14147-3-3-1.10][14595-2-2-2.30][14687-2-2-2.21][14788-2-2-1.45][14869-1-1-3.28][14872-3-3-1.64][14877-1-1-1.94][14927-0-3-0.31][15066-0-0-4.11]
[15175-1-1-0.75][15178-2-3-1.29][15375-3-3-0.53][15389-3-3-2.52][15568-2-1-0.79][15675-3-3-1.68][15869-1-0--0.09][16207-3-0-0.91][16236-0-0-0.58][16302-3-0-1.44]
[16331-2-2-3.76][16381-0-3-2.31][16488-1-1-5.19][16495-0-0-2.26][16650-0-0-4.32][16719-1-2-0.90][16801-0-0-4.71][16828-0-0-2.19][17137-3-3-0.68][17245-1-1-0.79]
[17278-3-4-0.66][17282-0-0-0.70][17311-2-2-1.53][17336-2-1-1.41][17608-3-3-4.67][17627-0-0-0.25][17877-3-1-1.02][17924-1-2-0.25][17984-3-0-3.84][18211-0-3-1.52]
[18276-3-3-2.22][18287-1-1-1.68][18394-0-0-3.10][18428-0-0-1.56][18442-0-3-1.58][18478-3-3-2.04][18607-0-0-1.67][18616-0-0-0.85][18663-0-0-2.37][18718-0-0-2.48]
[18766-2-2-1.68][18824-2-2-1.02][18890-3-3-2.55][18930-3-4-0.88][18938-3-3-2.21][19817-1-2-1.40][19839-0-1-0.61][19930-3-3-1.97][19944-0-4-1.76][20036-2-2-4.98]
[20101-3-1-0.97][20474-1-1-1.25][20547-3-3-0.34][20929-2-2-1.77][21245-1-1-2.63][21257-3-3-0.35][21293-1-1-2.96][21316-1-1-3.90][21384-1-4-1.63][21448-1-1-2.37]
[21483-0-0-2.99][21487-2-2-1.84][21714-0-0-1.25][21943-3-3-0.60][21947-0-0-1.95][21948-0-0-6.02][21965-2-2-2.57][21998-1-1-3.41][22025-0-2-0.04][22228-3-3-2.30]
[22446-1-1-4.24][22494-3-3-1.96][22757-0-0-3.32][22811-3-3-2.79][22976-3-4-0.81][22985-3-3-3.70][23014-0-0-3.09][23112-1-1-1.97][23144-3-3-4.69][23168-2-3-0.48]
[23219-0-0-1.23][23363-3-3-3.11][23470-0-0-1.06][23486-2-3-0.64][23497-0-3-3.69][23516-0-0-2.66][23690-1-3-0.52][23921-2-2-1.04][23936-1-2-1.29][24040-3-0-0.65]
[24111-1-4-3.14][24182-0-0-4.10][24238-3-3-2.94][24290-2-0-1.54][24345-0-0-2.17][24364-1-2-1.26][24427-3-0-3.20][24477-2-2-2.67][24495-2-1-1.26][24893-2-2-1.96]
[25012-1-1--0.18][25121-2-2-1.86][25165-3-3-1.10][25183-0-0-1.12][25297-3-3-2.04][25398-0-0-3.20][25574-2-2-1.61][25644-1-1-4.30][25718-1-1-0.43][25774-2-2-1.01]
[26032-3-0-1.60][26051-3-3-4.19][26120-0-4-0.70][26321-1-1-2.89][26732-1-1-3.83][26784-3-3-3.80][26827-3-3-1.48][26833-0-3-1.28][26838-2-1--0.13][26860-1-2-0.97]
[26948-0-0-1.55][27049-3-0-0.84][27098-1-1-1.63][27526-0-3-1.85][27639-3-3-1.79][27698-3-3-1.68][27772-0-0-3.61][27890-1-1-2.82][28040-0-4-0.55][28503-2-2-4.42]
[28577-1-1-3.15][28959-0-0-3.90][29198-3-3-1.32][29777-0-0-5.38][29877-2-1-1.94][30035-1-2-2.24][30098-0-0-1.14][30326-1-1-4.54][30572-2-2-2.23][30716-0-4-1.00]
[30806-2-3-0.80][30906-1-1-3.68][31007-0-0-0.89][31181-3-3-1.04][31238-0-0-2.14][31347-0-0-4.74][31422-2-1-0.53][31429-3-3-0.38][31431-0-0-0.58][31432-1-1-2.98]
[31477-0-3-3.04][31524-1-0-0.30][31597-1-2-1.15][31619-1-2-0.42][31701-0-0-2.41][31755-0-0-1.93][31854-3-3-1.81][32074-1-1-1.14][32078-3-3-1.28][32111-1-1-2.00]
[32127-1-2-2.22][32140-3-3-2.94][32263-2-4--0.16][32365-0-0-1.70][32411-2-3-3.30][32429-3-0-1.08][32473-3-3-1.26][32574-3-0-2.23][32584-0-0-1.45][32622-0-1-0.17]
[32858-3-0-1.52][32969-3-3-3.24][33016-2-2-3.44][33031-1-3-2.08][33035-2-2-2.00][33133-2-1-1.04][33173-2-1-1.06][33175-3-4-1.75][33306-3-1-0.35][33309-2-3-0.51]
[33474-0-0-0.16][33478-2-0--0.32][33618-1-1-2.21][33712-0-3-1.37][33782-2-4-1.85][33914-3-3-1.99][34076-3-2-0.17][34112-2-2-3.97][34138-2-2--0.04][34239-1-1-1.68]
[34364-2-2-2.97][34617-1-2-0.87][34751-3-3-2.70][34783-2-4-1.86][35015-3-3-1.73][35018-1-2-2.13][35288-2-1-0.33][0-4-4-1.97][1-4-4-0.94][2-4-4-1.17]
[3-4-2-0.67][4-4-2-0.18][5-4-1-0.42][6-4-4-0.22][7-4-4-0.64][8-4-2-0.65][9-4-1-1.66][10-4-4-2.26][11-4-2-2.18][12-4-1-0.13]
[14-4-3-2.22][15-4-3-3.01][16-4-2-0.44][17-4-4-0.69][18-4-4-2.55][19-4-3-1.12][20-4-0-0.58][21-4-2-0.74][22-4-4-1.53][23-4-2-0.11]
[24-4-4-3.74][25-4-3-0.65][26-4-3-0.21][27-4-4-0.42][28-4-4-1.20][29-4-1-0.74][30-4-4-1.18][31-4-2-0.45][32-4-4-0.82][33-4-3-1.08]
[34-4-4-0.06][35-4-3-0.93][37-4-2--0.45][39-4-0-1.75][40-4-4-0.24][41-4-2-0.31][42-4-2-0.71][43-4-1-1.89][45-4-2-1.37][46-4-4-2.07]
[47-4-4-2.46][48-4-4-1.60][51-4-4-1.72][52-4-0-0.70][53-4-4-0.28][54-4-3-1.45][55-4-2-1.22][56-4-1-1.74][57-4-3-2.32][58-4-2-2.68]
[59-4-0-1.14][60-4-1-0.24][61-4-4-1.01][62-4-3-0.43][63-4-2-1.38][64-4-1-0.43][65-4-4-2.99][66-4-4-2.84][67-4-0--0.25][68-4-3-0.44]
[69-4-3-0.78][70-4-4-1.58][72-4-1-1.26][73-4-1-1.35][74-4-2-1.72][75-4-3-0.85][77-4-4-2.79][78-4-1--0.01][79-4-4-1.60][80-4-4-0.29]
[81-4-1-1.34][82-4-1-1.48][83-4-1-0.20][84-4-4-0.98][85-4-2-0.95][86-4-4-0.70][87-4-4-1.56][88-4-4-1.71][89-4-3-0.01][90-4-4-0.39]
[91-4-4-0.39][92-4-2-0.66][93-4-4-0.50][94-4-4-1.72][95-4-4-0.51][96-4-4-1.04][97-4-4-1.67][98-4-2-1.14][99-4-4-2.02][100-4-1-1.61]
[101-4-4-2.65][102-4-4-1.03][103-4-0-0.34][104-4-4-1.85][105-4-4-3.40][106-4-1-3.11][107-4-4-1.58][108-4-2-0.51][109-4-4-0.07][110-4-4-0.06]
[111-4-0-3.52][112-4-2-1.69][113-4-3-0.22][114-4-3-0.85][115-4-4-0.24][116-4-4-0.91][117-4-4-1.05][119-4-4-1.31][121-4-4-1.22][122-4-4-0.62]
[124-4-3-0.27][125-4-2-2.13][126-4-4-1.48][127-4-2-1.13][128-4-0-0.47][129-4-1-0.42][130-4-4--0.14][131-4-3-1.76][132-4-0-0.14][133-4-0-2.09]
[135-4-2-1.21][136-4-0-0.06][137-4-4-0.58][138-4-4-0.57][139-4-4-0.05][140-4-1-0.95][141-4-3--0.19][142-4-4-1.42][143-4-4-1.81][144-4-4-3.13]
[145-4-4-1.34][148-4-0-3.78][149-4-4-0.76][150-4-4-0.86][151-4-4-2.52][152-4-1-0.37][153-4-1-0.85][154-4-4-3.55][155-4-4-1.08][156-4-3-2.18]
[157-4-0-0.97][158-4-3-0.77][160-4-1-0.40][161-4-2-1.66][162-4-4-0.49][164-4-3-0.30][165-4-2-1.01][167-4-0-0.58][168-4-4-0.88][170-4-0-0.18]
[171-4-4-1.36][172-4-4-2.07][173-4-0-0.63][174-4-0-3.29][175-4-4-0.96][177-4-0-1.68][178-4-4-1.58][179-4-1-0.16][180-4-4-2.54][181-4-4-0.38]
[182-4-3-2.75][183-4-4-1.04][184-4-2-1.23][186-4-0-0.63][187-4-1-0.99][188-4-2-1.10][189-4-4-0.76][190-4-1-1.10][191-4-4-1.72][192-4-1-1.43]
[193-4-1-2.39][194-4-2-1.58][195-4-2-1.35][196-4-2-0.53][197-4-4-1.60][198-4-4-4.51][199-4-2-1.42]
---------------------------
I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.460 | Acc: 76.500% | Wgt Acc: 86.403%
	I - Batch: 100 | Loss: 0.467 | Acc: 75.938% | Wgt Acc: 86.098%
	I - Batch: 150 | Loss: 0.469 | Acc: 75.500% | Wgt Acc: 85.716%
	I - Batch: 200 | Loss: 0.468 | Acc: 75.375% | Wgt Acc: 85.585%
I - num batch: 222
I - Train -- Loss: 0.465 | Acc: 75.388% | Wgt Acc: 85.563% | LR: 2.500000e-04 | Dur: 138.09s
I - Confusion Matrix: [row->prediction - col->label]
[[622.   9.   6.  44. 164.]
 [ 11. 517.  25.   6. 118.]
 [ 14.  31. 676.  15. 215.]
 [ 34.  10.  11. 463. 107.]
 [ 16.  11.  16.  10. 396.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.092 | Acc: 58.383% | Wgt Acc: 62.357% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  4. 16. 30.]
 [ 0. 39.  8.  0.  8.]
 [ 1. 18. 42.  2. 29.]
 [15.  8. 14. 63. 26.]
 [ 7.  9.  7.  5. 87.]]

I - Local maximum validation set accuracy:  58.38

I - Validation set results: 
[14-1-2-1.47][50-3-4-0.85][124-2-3-0.30][127-0-0-4.51][443-2-2-3.02][567-0-0-2.85][573-1-1-2.60][615-0-0-1.98][695-1-2-0.38][722-3-0-3.13]
[826-0-0-4.41][878-0-0-3.02][1103-0-0-2.16][1212-3-3-1.19][1368-0-0-4.51][2181-2-3-1.02][2476-2-2-1.59][2721-2-2-2.01][2818-1-3-1.77][2886-2-4-2.01]
[3231-2-2-4.91][3333-2-2-0.86][3482-2-2-1.49][3536-3-3-2.01][3625-1-1-2.57][3909-0-0-1.19][4035-0-3-1.23][4140-0-0-1.41][4214-1-3-0.95][4346-1-3-1.02]
[4581-2-2-1.33][4708-3-3-0.47][4838-3-3--0.08][4845-1-2-0.82][4868-0-0-3.82][4939-0-4-0.70][4984-2-2-1.61][5078-1-4-0.58][5396-0-0-5.20][5479-1-1-3.07]
[5717-0-0-2.75][5843-1-1-2.17][5949-3-3-1.71][5987-2-4-1.72][6014-3-3-1.59][6033-3-3-0.36][6313-0-3-2.10][6421-3-3-3.65][6500-1-3--0.26][6583-3-3-0.82]
[6683-3-3-1.71][6825-2-1-1.79][6998-3-3-1.00][7049-3-3-1.68][7517-1-1-1.79][7521-1-1-0.15][7528-1-3-0.97][7949-1-2-1.38][8135-1-0-1.06][8185-3-0-4.12]
[8269-3-2-0.92][8273-3-3-2.11][8543-3-0-4.51][8666-1-1-2.98][8672-0-0-5.67][8903-1-1-2.45][9001-2-2-2.66][9036-2-2-2.55][9281-3-0-0.23][9300-2-2-4.11]
[9571-0-3-1.32][9617-1-1-0.85][9644-2-2-1.14][9705-2-4-0.30][9801-0-3-2.76][9803-3-3-1.20][9865-3-3-4.69][9896-2-2-1.36][10314-1-2-0.46][10337-3-3-4.64]
[10403-0-4-0.45][10653-2-1-0.30][10704-2-1-1.17][10719-1-1-2.88][10727-1-4-1.28][10836-0-0-8.11][10969-2-3-1.49][11042-0-0-2.50][11088-1-1-3.01][11322-0-0-4.00]
[11398-2-2-2.84][11499-0-0-1.94][11502-3-3-2.35][11512-3-3-2.16][11608-1-2-2.48][11610-0-0-1.78][11692-0-3-1.69][11905-0-0-3.25][11993-1-1-2.50][12002-2-3-0.76]
[12052-0-0-3.56][12201-0-3-4.61][12235-2-2-3.06][12320-1-0-2.54][12377-2-4-1.81][12398-2-3-0.82][12503-1-2-0.99][12617-0-3-1.23][12685-3-3-0.85][12738-2-0-1.10]
[12742-2-2-3.85][12823-0-0-3.01][13110-1-1-1.94][13240-3-3-2.50][13253-1-4-1.97][13273-0-0-5.99][13634-1-4-0.05][13763-2-2-2.64][13905-3-3-0.56][14060-2-2-0.94]
[14065-3-0-2.10][14147-3-3-2.51][14595-2-2-1.92][14687-2-2-3.02][14788-2-2-1.73][14869-1-1-3.47][14872-3-4-0.91][14877-1-1-2.38][14927-0-3-1.38][15066-0-0-4.75]
[15175-1-1-0.40][15178-2-3-1.33][15375-3-0-0.90][15389-3-3-4.84][15568-2-1-0.79][15675-3-3-3.23][15869-1-4-0.29][16207-3-0-0.86][16236-0-0-0.56][16302-3-3-0.84]
[16331-2-2-4.14][16381-0-3-1.42][16488-1-1-2.36][16495-0-0-3.37][16650-0-0-4.49][16719-1-2-0.81][16801-0-0-4.20][16828-0-0-3.28][17137-3-0-0.73][17245-1-1-0.43]
[17278-3-0-0.88][17282-0-0-0.35][17311-2-2-2.03][17336-2-1-0.57][17608-3-3-5.34][17627-0-0-1.50][17877-3-0-0.84][17924-1-2-0.34][17984-3-0-4.05][18211-0-3-0.92]
[18276-3-3-1.56][18287-1-1-0.52][18394-0-0-3.37][18428-0-0-6.70][18442-0-3-2.89][18478-3-3-2.19][18607-0-0-0.76][18616-0-0-1.71][18663-0-0-3.50][18718-0-0-4.03]
[18766-2-2-2.47][18824-2-2-1.79][18890-3-3-2.18][18930-3-4-0.95][18938-3-3-2.43][19817-1-2-1.74][19839-0-0-0.58][19930-3-3-2.35][19944-0-4-1.52][20036-2-2-4.66]
[20101-3-3-2.63][20474-1-1-1.19][20547-3-0-1.03][20929-2-2-3.48][21245-1-1-1.16][21257-3-3-0.47][21293-1-2-1.87][21316-1-1-0.10][21384-1-4-2.48][21448-1-1-2.29]
[21483-0-0-3.39][21487-2-2-1.81][21714-0-0-1.88][21943-3-2-0.81][21947-0-0-1.31][21948-0-0-6.60][21965-2-2-2.43][21998-1-1-3.60][22025-0-2-0.73][22228-3-3-3.63]
[22446-1-1-4.12][22494-3-3-2.32][22757-0-0-4.59][22811-3-3-4.22][22976-3-4-0.87][22985-3-3-2.75][23014-0-0-3.32][23112-1-1-2.96][23144-3-3-4.74][23168-2-3-0.58]
[23219-0-3-0.24][23363-3-3-1.48][23470-0-0-0.11][23486-2-2-0.81][23497-0-3-4.39][23516-0-0-4.25][23690-1-4-0.11][23921-2-1-0.48][23936-1-2-1.64][24040-3-0-0.83]
[24111-1-4-3.01][24182-0-0-4.85][24238-3-3-3.51][24290-2-0-1.68][24345-0-4-0.58][24364-1-3-0.41][24427-3-0-2.59][24477-2-2-2.76][24495-2-4-1.00][24893-2-2-1.15]
[25012-1-1-0.09][25121-2-1-0.90][25165-3-3-1.18][25183-0-0-1.51][25297-3-3-2.69][25398-0-0-3.45][25574-2-2-2.45][25644-1-1-3.38][25718-1-4-0.12][25774-2-3-0.73]
[26032-3-3-2.14][26051-3-3-4.16][26120-0-4-1.16][26321-1-1-2.59][26732-1-1-1.98][26784-3-3-4.98][26827-3-3-2.48][26833-0-3-2.24][26838-2-3-0.84][26860-1-2-0.43]
[26948-0-0-1.66][27049-3-0-2.22][27098-1-0-1.80][27526-0-0-1.52][27639-3-3-1.31][27698-3-3-2.89][27772-0-0-3.37][27890-1-1-2.70][28040-0-0-1.87][28503-2-2-4.23]
[28577-1-1-1.71][28959-0-0-4.58][29198-3-3-1.41][29777-0-0-6.14][29877-2-3-0.56][30035-1-2-1.39][30098-0-0-1.90][30326-1-1-4.15][30572-2-2-2.55][30716-0-4-0.88]
[30806-2-3-2.25][30906-1-1-3.71][31007-0-0-0.78][31181-3-3-2.02][31238-0-0-1.79][31347-0-0-4.91][31422-2-2-0.23][31429-3-3-0.63][31431-0-0-0.34][31432-1-1-1.93]
[31477-0-0-4.50][31524-1-1-2.10][31597-1-2-1.01][31619-1-0-0.45][31701-0-0-2.57][31755-0-0-2.91][31854-3-3-3.13][32074-1-3-1.32][32078-3-3-3.69][32111-1-1-1.47]
[32127-1-2-2.04][32140-3-3-4.32][32263-2-0-0.30][32365-0-0-3.29][32411-2-3-3.14][32429-3-0-1.56][32473-3-3-1.76][32574-3-3-2.87][32584-0-0-0.96][32622-0-4--0.27]
[32858-3-3-2.49][32969-3-3-3.25][33016-2-2-4.29][33031-1-3-1.92][33035-2-2-1.81][33133-2-1-0.72][33173-2-2-0.44][33175-3-4-2.00][33306-3-3-0.46][33309-2-3-0.57]
[33474-0-0-0.28][33478-2-0--0.26][33618-1-1-1.46][33712-0-3-1.44][33782-2-4-1.74][33914-3-3-3.24][34076-3-3-0.75][34112-2-2-2.40][34138-2-3-0.50][34239-1-2-0.31]
[34364-2-2-3.82][34617-1-2-1.93][34751-3-3-2.95][34783-2-2-1.31][35015-3-3-1.82][35018-1-1-0.79][35288-2-4-0.19][0-4-2-1.02][1-4-4-1.32][2-4-4-1.82]
[3-4-1-0.57][4-4-0-1.25][5-4-1-1.60][6-4-4-2.49][7-4-4-0.66][8-4-2-0.81][9-4-1-0.81][10-4-4-2.26][11-4-2-2.71][12-4-0-0.26]
[14-4-3-1.91][15-4-3-3.53][16-4-0-0.97][17-4-0-0.26][18-4-4-2.43][19-4-0-2.66][20-4-2-0.46][21-4-2-1.10][22-4-4-2.54][23-4-3--0.31]
[24-4-4-4.15][25-4-3-0.85][26-4-3-0.11][27-4-4-1.17][28-4-4-1.77][29-4-2-0.84][30-4-0-1.70][31-4-4-0.67][32-4-4-1.38][33-4-3-1.16]
[34-4-4-0.29][35-4-0-1.74][37-4-3-0.34][39-4-0-3.90][40-4-0-1.28][41-4-2-0.45][42-4-4--0.15][43-4-2-0.62][45-4-2-0.94][46-4-4-2.37]
[47-4-4-2.16][48-4-4-1.48][51-4-4-1.93][52-4-4-0.65][53-4-0-0.54][54-4-3-1.29][55-4-3-0.58][56-4-1-1.56][57-4-0-2.57][58-4-2-2.68]
[59-4-0-1.81][60-4-2-0.16][61-4-4-1.31][62-4-3-0.77][63-4-2-2.15][64-4-4-0.25][65-4-4-3.53][66-4-4-1.72][67-4-3--0.22][68-4-3-0.13]
[69-4-0-0.32][70-4-4-1.79][72-4-4-1.71][73-4-1-1.09][74-4-2-1.82][75-4-3-0.58][77-4-4-3.34][78-4-3-0.56][79-4-4-1.86][80-4-4-1.45]
[81-4-4-3.99][82-4-1-0.95][83-4-4--0.19][84-4-0-1.66][85-4-4-2.11][86-4-2-1.01][87-4-4-1.76][88-4-4-1.54][89-4-3--0.15][90-4-0-0.54]
[91-4-4-0.12][92-4-3-0.42][93-4-0-1.60][94-4-4-1.82][95-4-3-0.47][96-4-4-1.16][97-4-4-2.16][98-4-2-0.80][99-4-4-1.80][100-4-4-1.66]
[101-4-4-3.65][102-4-2-1.10][103-4-3-0.99][104-4-4-2.20][105-4-4-2.20][106-4-1-2.20][107-4-0-1.57][108-4-4-0.80][109-4-4-0.57][110-4-4-1.41]
[111-4-0-3.94][112-4-2-0.37][113-4-3-1.14][114-4-3-0.35][115-4-4-0.07][116-4-4-1.40][117-4-4-1.29][119-4-4-1.39][121-4-4-1.03][122-4-0-1.61]
[124-4-2--0.02][125-4-4-2.08][126-4-4-2.87][127-4-2-1.99][128-4-0-1.13][129-4-2-0.94][130-4-4-1.39][131-4-3-0.83][132-4-0-1.34][133-4-0-2.17]
[135-4-2-1.04][136-4-0-0.01][137-4-4-0.64][138-4-4-0.74][139-4-4-0.62][140-4-4-0.54][141-4-0-1.61][142-4-4-1.20][143-4-4-1.29][144-4-4-2.48]
[145-4-4-1.52][148-4-0-4.87][149-4-4-1.35][150-4-4-2.00][151-4-4-1.71][152-4-4-0.68][153-4-2-1.59][154-4-4-4.63][155-4-4-1.55][156-4-3-1.88]
[157-4-0-1.95][158-4-3-1.75][160-4-1-0.30][161-4-2-0.86][162-4-4-0.59][164-4-3-0.75][165-4-4-0.80][167-4-0-1.52][168-4-4-1.33][170-4-4-0.61]
[171-4-4-1.88][172-4-4-2.67][173-4-0-0.47][174-4-0-3.73][175-4-4-1.35][177-4-0-3.06][178-4-4-1.14][179-4-4-1.14][180-4-4-1.94][181-4-3-2.11]
[182-4-3-1.93][183-4-4-0.97][184-4-4-0.62][186-4-4-0.32][187-4-4-0.59][188-4-2-1.59][189-4-4-1.02][190-4-2-0.28][191-4-4-2.33][192-4-4-0.83]
[193-4-2-2.85][194-4-2-1.44][195-4-2-0.25][196-4-4-0.20][197-4-4-2.91][198-4-4-4.43][199-4-2-0.50]
---------------------------
I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 0.408 | Acc: 77.000% | Wgt Acc: 87.834%
	I - Batch: 100 | Loss: 0.402 | Acc: 78.312% | Wgt Acc: 88.882%
	I - Batch: 150 | Loss: 0.415 | Acc: 77.083% | Wgt Acc: 87.969%
	I - Batch: 200 | Loss: 0.429 | Acc: 76.156% | Wgt Acc: 87.239%
I - num batch: 222
I - Train -- Loss: 0.433 | Acc: 76.346% | Wgt Acc: 87.177% | LR: 2.500000e-04 | Dur: 136.82s
I - Confusion Matrix: [row->prediction - col->label]
[[623.   5.   7.  45. 174.]
 [  9. 548.   9.   7. 109.]
 [ 16.  13. 688.  12. 231.]
 [ 35.   2.  15. 466. 103.]
 [ 14.  10.  15.   8. 383.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.134 | Acc: 57.002% | Wgt Acc: 60.846% | Dur: 16.04s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  4.  1. 11. 18.]
 [ 0. 35.  6.  1.  8.]
 [ 3. 23. 49.  4. 49.]
 [20.  9. 11. 62. 20.]
 [ 7.  7.  8.  8. 85.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.392 | Acc: 78.000% | Wgt Acc: 88.556%
	I - Batch: 100 | Loss: 0.392 | Acc: 78.750% | Wgt Acc: 88.161%
	I - Batch: 150 | Loss: 0.402 | Acc: 78.292% | Wgt Acc: 87.789%
	I - Batch: 200 | Loss: 0.413 | Acc: 78.000% | Wgt Acc: 87.275%
I - num batch: 222
I - Train -- Loss: 0.417 | Acc: 77.869% | Wgt Acc: 87.061% | LR: 2.500000e-04 | Dur: 134.10s
I - Confusion Matrix: [row->prediction - col->label]
[[622.   9.   9.  44. 141.]
 [  7. 534.  16.  12. 102.]
 [ 19.  16. 689.  14. 208.]
 [ 35.   5.   9. 462.  94.]
 [ 14.  14.  11.   6. 455.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.126 | Acc: 56.213% | Wgt Acc: 62.265% | Dur: 17.03s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  5. 17. 25.]
 [ 0. 42. 11.  0. 15.]
 [ 5. 20. 47.  7. 49.]
 [13.  5.  6. 58. 18.]
 [ 5.  7.  6.  4. 73.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 0.377 | Acc: 79.625% | Wgt Acc: 88.868%
	I - Batch: 100 | Loss: 0.387 | Acc: 79.250% | Wgt Acc: 88.595%
	I - Batch: 150 | Loss: 0.377 | Acc: 79.833% | Wgt Acc: 89.300%
	I - Batch: 200 | Loss: 0.380 | Acc: 79.750% | Wgt Acc: 89.276%
I - num batch: 222
I - Train -- Loss: 0.389 | Acc: 79.053% | Wgt Acc: 88.871% | LR: 2.500000e-04 | Dur: 131.41s
I - Confusion Matrix: [row->prediction - col->label]
[[635.   2.   3.  35. 155.]
 [  8. 545.   7.   9. 110.]
 [  8.  13. 705.  12. 190.]
 [ 32.  12.   7. 474. 100.]
 [ 14.   6.  12.   8. 445.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.067 | Acc: 59.763% | Wgt Acc: 62.426% | Dur: 13.94s
I - Confusion Matrix: [row->prediction - col->label]
[[49.  2.  1.  6. 13.]
 [ 1. 34.  7.  1.  9.]
 [ 4. 26. 55.  5. 43.]
 [25.  4.  9. 69. 19.]
 [ 9. 12.  3.  5. 96.]]

I - Local maximum validation set accuracy:  59.76

I - Validation set results: 
[14-1-2-2.46][50-3-4-0.70][124-2-2-1.37][127-0-0-2.56][443-2-2-3.32][567-0-0-0.64][573-1-1-1.67][615-0-3-1.29][695-1-2-0.90][722-3-3-4.17]
[826-0-0-2.08][878-0-3-2.74][1103-0-0-1.27][1212-3-4-0.67][1368-0-0-3.20][2181-2-2-0.25][2476-2-2-1.33][2721-2-2-3.03][2818-1-3-1.71][2886-2-4-1.50]
[3231-2-2-4.87][3333-2-2-2.53][3482-2-2-1.98][3536-3-3-2.21][3625-1-1-3.02][3909-0-0-0.45][4035-0-3-1.12][4140-0-0-1.32][4214-1-1-1.13][4346-1-4-0.51]
[4581-2-1-1.52][4708-3-2-0.47][4838-3-3-1.20][4845-1-2-1.38][4868-0-0-3.37][4939-0-2-0.29][4984-2-2-2.34][5078-1-2-0.63][5396-0-0-3.79][5479-1-1-3.43]
[5717-0-0-0.32][5843-1-1-1.05][5949-3-3-2.00][5987-2-2-2.06][6014-3-3-2.20][6033-3-3-0.89][6313-0-3-1.58][6421-3-3-3.94][6500-1-2-0.27][6583-3-3-1.21]
[6683-3-3-1.02][6825-2-1-2.17][6998-3-3-0.49][7049-3-3-2.67][7517-1-2-2.84][7521-1-1-1.35][7528-1-3-0.34][7949-1-2-2.53][8135-1-0-1.37][8185-3-3-1.64]
[8269-3-1-2.60][8273-3-3-3.41][8543-3-0-3.77][8666-1-1-3.99][8672-0-3-2.48][8903-1-2-2.48][9001-2-2-1.66][9036-2-2-3.24][9281-3-3-0.92][9300-2-2-4.98]
[9571-0-3-0.81][9617-1-1-0.82][9644-2-2-2.36][9705-2-4-0.18][9801-0-3-2.33][9803-3-3-1.99][9865-3-3-5.05][9896-2-2-1.63][10314-1-2-0.45][10337-3-3-5.05]
[10403-0-4-0.58][10653-2-2-0.54][10704-2-1-1.88][10719-1-1-2.32][10727-1-4-0.84][10836-0-0-7.06][10969-2-2-0.76][11042-0-0-0.49][11088-1-1-4.59][11322-0-0-3.21]
[11398-2-2-1.39][11499-0-0-1.72][11502-3-3-2.34][11512-3-3-2.08][11608-1-2-1.99][11610-0-0-1.38][11692-0-3-1.49][11905-0-0-1.19][11993-1-1-1.96][12002-2-3-1.47]
[12052-0-0-2.28][12201-0-3-3.62][12235-2-2-2.65][12320-1-4-2.29][12377-2-4-1.05][12398-2-2-0.30][12503-1-2-1.30][12617-0-3-1.02][12685-3-3-0.40][12738-2-3-1.23]
[12742-2-2-4.41][12823-0-3-1.34][13110-1-2-0.61][13240-3-3-1.88][13253-1-4-1.80][13273-0-0-3.68][13634-1-4-1.65][13763-2-2-4.03][13905-3-3-0.03][14060-2-1-0.77]
[14065-3-3-1.09][14147-3-3-1.82][14595-2-2-2.62][14687-2-2-5.16][14788-2-2-3.39][14869-1-1-3.34][14872-3-3-0.79][14877-1-1-2.34][14927-0-3-2.60][15066-0-0-3.05]
[15175-1-1-0.94][15178-2-3-0.96][15375-3-3-1.64][15389-3-3-4.18][15568-2-1-1.18][15675-3-3-2.90][15869-1-4-0.50][16207-3-0-0.69][16236-0-3-1.20][16302-3-3-0.68]
[16331-2-2-5.73][16381-0-3-1.42][16488-1-1-1.81][16495-0-0-2.79][16650-0-0-2.88][16719-1-2-1.15][16801-0-0-3.77][16828-0-0-1.98][17137-3-3-0.76][17245-1-1-1.06]
[17278-3-3--0.08][17282-0-2-0.10][17311-2-2-2.44][17336-2-3-0.15][17608-3-3-5.17][17627-0-4--0.46][17877-3-0-0.70][17924-1-2-1.38][17984-3-0-2.35][18211-0-3-1.32]
[18276-3-3-1.07][18287-1-2-1.07][18394-0-0-1.93][18428-0-0-1.17][18442-0-3-2.23][18478-3-3-2.56][18607-0-0-0.58][18616-0-0-2.28][18663-0-0-2.51][18718-0-0-2.68]
[18766-2-2-3.06][18824-2-2-1.68][18890-3-3-1.98][18930-3-4-0.88][18938-3-3-1.22][19817-1-2-1.61][19839-0-2-0.18][19930-3-3-1.10][19944-0-4-1.50][20036-2-2-5.63]
[20101-3-3-2.18][20474-1-1-1.16][20547-3-3-0.91][20929-2-2-3.09][21245-1-2-1.00][21257-3-2-1.14][21293-1-2-2.62][21316-1-1-2.98][21384-1-4-2.77][21448-1-1-1.64]
[21483-0-0-2.07][21487-2-2-4.16][21714-0-0-0.63][21943-3-2-1.00][21947-0-0-1.42][21948-0-0-5.55][21965-2-2-4.08][21998-1-1-4.09][22025-0-2-1.31][22228-3-3-3.99]
[22446-1-1-3.58][22494-3-3-2.68][22757-0-0-3.61][22811-3-3-4.13][22976-3-4-0.43][22985-3-3-3.94][23014-0-3-3.49][23112-1-1-1.41][23144-3-3-4.57][23168-2-3-0.13]
[23219-0-4--0.13][23363-3-3-3.01][23470-0-4--0.04][23486-2-2-1.58][23497-0-3-5.15][23516-0-0-3.38][23690-1-2-0.02][23921-2-2-1.60][23936-1-2-3.19][24040-3-3-0.36]
[24111-1-4-2.41][24182-0-0-3.38][24238-3-3-2.06][24290-2-0-0.70][24345-0-0-1.50][24364-1-2-2.39][24427-3-0-1.19][24477-2-2-2.34][24495-2-1-0.26][24893-2-2-4.12]
[25012-1-4-0.47][25121-2-2-1.65][25165-3-3-0.95][25183-0-0-1.78][25297-3-3-2.56][25398-0-0-2.52][25574-2-2-2.85][25644-1-2-3.63][25718-1-4-0.90][25774-2-3-0.68]
[26032-3-3-3.33][26051-3-3-3.97][26120-0-4-1.38][26321-1-1-3.60][26732-1-1-1.95][26784-3-3-4.83][26827-3-3-0.88][26833-0-3-3.05][26838-2-2-0.86][26860-1-4-0.61]
[26948-0-0-1.33][27049-3-0-0.78][27098-1-0-0.42][27526-0-4-0.39][27639-3-3-1.18][27698-3-3-2.41][27772-0-0-1.96][27890-1-1-3.01][28040-0-0-0.30][28503-2-2-4.22]
[28577-1-1-2.55][28959-0-3-3.03][29198-3-3-1.70][29777-0-0-3.72][29877-2-1-0.65][30035-1-1-2.63][30098-0-3-1.43][30326-1-1-2.84][30572-2-2-2.52][30716-0-4-1.24]
[30806-2-3-1.49][30906-1-1-2.12][31007-0-4-1.46][31181-3-3-1.37][31238-0-3-1.54][31347-0-0-3.01][31422-2-2-0.85][31429-3-3-1.07][31431-0-3--0.21][31432-1-1-1.88]
[31477-0-3-4.17][31524-1-2-1.77][31597-1-2-1.72][31619-1-4-0.53][31701-0-0-1.88][31755-0-0-1.21][31854-3-3-1.47][32074-1-3-0.77][32078-3-3-3.41][32111-1-1-0.66]
[32127-1-2-2.38][32140-3-3-4.43][32263-2-2-0.24][32365-0-0-3.03][32411-2-3-3.29][32429-3-3-2.32][32473-3-3-1.27][32574-3-3-1.57][32584-0-0-0.67][32622-0-0--0.49]
[32858-3-3-3.02][32969-3-3-3.39][33016-2-2-1.90][33031-1-3-0.90][33035-2-2-2.78][33133-2-2-2.80][33173-2-2-0.86][33175-3-4-1.84][33306-3-3-0.33][33309-2-3-1.53]
[33474-0-1-0.25][33478-2-2-0.47][33618-1-1-1.37][33712-0-3-1.08][33782-2-2-1.38][33914-3-3-2.88][34076-3-2-0.90][34112-2-2-4.87][34138-2-2-1.21][34239-1-1-1.11]
[34364-2-2-5.01][34617-1-2-0.86][34751-3-3-2.55][34783-2-2-1.29][35015-3-2-2.34][35018-1-1-1.52][35288-2-2-0.55][0-4-2-2.44][1-4-4-2.04][2-4-4-1.38]
[3-4-4-1.39][4-4-2-0.33][5-4-3--0.20][6-4-4-3.71][7-4-4-0.78][8-4-4-0.23][9-4-4-0.66][10-4-4-2.43][11-4-2-3.30][12-4-4-0.53]
[14-4-3-1.60][15-4-3-2.98][16-4-4-0.65][17-4-4-0.12][18-4-4-1.82][19-4-3-1.73][20-4-0-0.01][21-4-4-0.92][22-4-4-1.53][23-4-4--0.03]
[24-4-4-5.48][25-4-3-0.86][26-4-3-0.18][27-4-2-1.02][28-4-4-3.85][29-4-1-1.27][30-4-0-0.35][31-4-4-1.31][32-4-2-1.20][33-4-2-1.74]
[34-4-4-0.75][35-4-3-1.71][37-4-4-1.22][39-4-0-2.81][40-4-4-0.06][41-4-2-0.07][42-4-4--0.50][43-4-2-0.47][45-4-2-1.01][46-4-4-2.36]
[47-4-4-2.56][48-4-4-1.20][51-4-4-1.10][52-4-4-0.67][53-4-4-0.66][54-4-3-2.02][55-4-2-0.92][56-4-1-1.06][57-4-3-1.77][58-4-2-2.88]
[59-4-0-1.32][60-4-1-0.19][61-4-4-1.66][62-4-2-2.32][63-4-2-2.07][64-4-2-0.64][65-4-4-3.25][66-4-4-1.83][67-4-2-0.51][68-4-4-0.33]
[69-4-3-0.49][70-4-2-0.99][72-4-4-1.65][73-4-1-0.78][74-4-2-2.58][75-4-3-0.45][77-4-4-3.47][78-4-4-0.00][79-4-4-2.06][80-4-4-2.28]
[81-4-4-1.39][82-4-1-1.78][83-4-4-0.65][84-4-4-2.33][85-4-4-2.03][86-4-4-1.17][87-4-4-1.19][88-4-4-1.01][89-4-2-1.44][90-4-4-0.37]
[91-4-4-0.21][92-4-3-0.42][93-4-0-0.96][94-4-4-1.58][95-4-4-0.07][96-4-4-0.44][97-4-4-1.82][98-4-1-0.95][99-4-4-0.64][100-4-4-1.70]
[101-4-4-4.35][102-4-2-0.89][103-4-3-0.62][104-4-4-1.03][105-4-4-2.61][106-4-1-2.27][107-4-4-0.85][108-4-2-0.37][109-4-4-0.73][110-4-4-0.44]
[111-4-0-3.03][112-4-2-0.45][113-4-2-0.98][114-4-3-0.58][115-4-2-0.06][116-4-2-1.01][117-4-4-1.47][119-4-2-2.28][121-4-2-1.30][122-4-4-2.08]
[124-4-2-0.24][125-4-4-2.65][126-4-4-4.50][127-4-2-0.95][128-4-0-0.61][129-4-1-0.64][130-4-4-0.20][131-4-2-2.67][132-4-3-0.39][133-4-4-2.86]
[135-4-2-2.60][136-4-4-0.51][137-4-2-0.10][138-4-2-1.37][139-4-4-1.03][140-4-1-0.98][141-4-3-0.83][142-4-4-1.17][143-4-2-1.34][144-4-4-3.41]
[145-4-2-3.02][148-4-0-3.75][149-4-4-1.31][150-4-4-2.83][151-4-4-2.52][152-4-4-1.12][153-4-2-2.89][154-4-4-4.32][155-4-4-1.55][156-4-3-1.74]
[157-4-0-0.87][158-4-4-0.92][160-4-4-0.63][161-4-2-1.71][162-4-0-0.02][164-4-2-1.30][165-4-4-1.83][167-4-4-0.64][168-4-4-0.42][170-4-4-0.55]
[171-4-4-2.50][172-4-4-2.29][173-4-4-2.27][174-4-0-2.22][175-4-4-0.92][177-4-0-2.34][178-4-4-1.43][179-4-4-0.94][180-4-4-2.40][181-4-3-1.65]
[182-4-4-0.91][183-4-4-1.85][184-4-4-1.53][186-4-3--0.48][187-4-2-1.31][188-4-2-1.40][189-4-4-1.11][190-4-4-0.35][191-4-4-0.75][192-4-4-0.83]
[193-4-2-2.68][194-4-2-1.91][195-4-0-0.49][196-4-2-1.24][197-4-4-0.88][198-4-4-4.81][199-4-2-1.67]
---------------------------
I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.389 | Acc: 80.375% | Wgt Acc: 90.164%
	I - Batch: 100 | Loss: 0.385 | Acc: 79.562% | Wgt Acc: 89.352%
	I - Batch: 150 | Loss: 0.374 | Acc: 79.208% | Wgt Acc: 89.279%
	I - Batch: 200 | Loss: 0.386 | Acc: 78.656% | Wgt Acc: 88.774%
I - num batch: 222
I - Train -- Loss: 0.389 | Acc: 78.235% | Wgt Acc: 88.503% | LR: 2.500000e-04 | Dur: 136.38s
I - Confusion Matrix: [row->prediction - col->label]
[[633.   5.   7.  48. 160.]
 [  8. 554.   7.   5. 108.]
 [  9.  12. 700.  10. 190.]
 [ 32.   2.   7. 467. 121.]
 [ 15.   5.  13.   8. 421.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.033 | Acc: 61.933% | Wgt Acc: 64.156% | Dur: 14.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   3.   1.   8.  11.]
 [  0.  37.   5.   0.   9.]
 [  2.  19.  50.   4.  37.]
 [ 23.   5.  13.  70.  21.]
 [  8.  14.   6.   4. 102.]]

I - Local maximum validation set accuracy:  61.93

I - Validation set results: 
[14-1-2-1.50][50-3-4-0.23][124-2-2-0.90][127-0-0-3.46][443-2-2-2.96][567-0-0-1.28][573-1-1-2.42][615-0-0-1.78][695-1-2-0.93][722-3-3-3.49]
[826-0-0-4.21][878-0-3-2.88][1103-0-4-0.70][1212-3-3-1.75][1368-0-0-2.66][2181-2-3-0.32][2476-2-2-1.99][2721-2-2-1.64][2818-1-3-2.04][2886-2-1-1.27]
[3231-2-2-5.33][3333-2-2-1.50][3482-2-2-1.64][3536-3-3-0.53][3625-1-1-3.08][3909-0-0-1.30][4035-0-3-1.97][4140-0-0-1.20][4214-1-3-1.95][4346-1-4-0.64]
[4581-2-2-1.41][4708-3-3-0.74][4838-3-3-0.41][4845-1-2-0.43][4868-0-0-3.82][4939-0-3-0.59][4984-2-2-2.34][5078-1-4-0.78][5396-0-0-4.37][5479-1-1-2.05]
[5717-0-0-2.25][5843-1-1-1.60][5949-3-3-2.05][5987-2-4-2.58][6014-3-3-1.81][6033-3-3-0.52][6313-0-3-1.73][6421-3-3-3.30][6500-1-4--0.04][6583-3-2-0.87]
[6683-3-3-1.25][6825-2-1-1.29][6998-3-3-1.25][7049-3-3-1.70][7517-1-1-2.78][7521-1-1-2.49][7528-1-3-2.67][7949-1-2-2.27][8135-1-0-0.88][8185-3-0-2.50]
[8269-3-2-1.44][8273-3-3-3.40][8543-3-0-4.04][8666-1-1-4.71][8672-0-0-3.11][8903-1-2-0.47][9001-2-2-1.94][9036-2-2-5.73][9281-3-3-0.89][9300-2-2-4.33]
[9571-0-3-1.23][9617-1-4-0.66][9644-2-2-2.11][9705-2-4-0.31][9801-0-3-2.07][9803-3-3-1.61][9865-3-3-4.83][9896-2-2-2.08][10314-1-2-0.44][10337-3-3-4.65]
[10403-0-4-1.12][10653-2-2-0.34][10704-2-1-2.33][10719-1-1-1.68][10727-1-1-0.86][10836-0-0-7.41][10969-2-3-0.51][11042-0-0-1.66][11088-1-1-4.25][11322-0-0-3.26]
[11398-2-2-1.43][11499-0-0-1.89][11502-3-3-2.51][11512-3-3-1.37][11608-1-2-3.01][11610-0-0-2.31][11692-0-3-1.72][11905-0-0-2.15][11993-1-1-2.57][12002-2-3-1.14]
[12052-0-0-3.14][12201-0-3-4.76][12235-2-2-3.61][12320-1-4-2.15][12377-2-4-1.90][12398-2-3-0.44][12503-1-4-1.58][12617-0-2-0.46][12685-3-2-0.43][12738-2-3-0.90]
[12742-2-2-3.72][12823-0-3-2.55][13110-1-1-1.17][13240-3-3-2.15][13253-1-1-2.92][13273-0-0-4.36][13634-1-4-0.55][13763-2-2-2.03][13905-3-3-0.91][14060-2-1-0.68]
[14065-3-3-1.87][14147-3-3-1.33][14595-2-2-3.39][14687-2-2-4.21][14788-2-2-2.38][14869-1-1-2.68][14872-3-3-1.51][14877-1-1-2.54][14927-0-3-2.57][15066-0-0-3.49]
[15175-1-1-0.38][15178-2-3-1.52][15375-3-3-1.55][15389-3-3-3.78][15568-2-4-1.81][15675-3-3-1.96][15869-1-4-0.00][16207-3-0-1.16][16236-0-3-1.17][16302-3-3-0.84]
[16331-2-2-5.34][16381-0-3-1.58][16488-1-1-1.07][16495-0-0-3.73][16650-0-0-4.19][16719-1-2-1.14][16801-0-0-4.52][16828-0-0-2.26][17137-3-3-1.72][17245-1-4-0.26]
[17278-3-0-0.49][17282-0-2-0.75][17311-2-2-2.94][17336-2-2-1.39][17608-3-3-5.02][17627-0-0-2.39][17877-3-0-0.53][17924-1-2-0.79][17984-3-0-2.92][18211-0-3-1.01]
[18276-3-3-2.47][18287-1-1-0.24][18394-0-0-2.88][18428-0-0-5.26][18442-0-3-2.42][18478-3-3-2.95][18607-0-0-1.30][18616-0-0-1.39][18663-0-0-2.64][18718-0-0-3.33]
[18766-2-2-3.52][18824-2-2-2.86][18890-3-3-3.60][18930-3-4-1.13][18938-3-3-2.47][19817-1-2-1.26][19839-0-0-1.14][19930-3-3-2.86][19944-0-4-1.20][20036-2-2-5.36]
[20101-3-3-2.17][20474-1-1-2.53][20547-3-3-1.13][20929-2-2-2.36][21245-1-1-1.04][21257-3-3-1.15][21293-1-2-2.16][21316-1-1-3.18][21384-1-4-1.78][21448-1-1-3.16]
[21483-0-0-2.53][21487-2-2-1.92][21714-0-0-1.51][21943-3-2-0.89][21947-0-0-1.15][21948-0-0-6.20][21965-2-2-4.34][21998-1-1-3.64][22025-0-4-0.84][22228-3-3-3.88]
[22446-1-1-4.66][22494-3-3-2.58][22757-0-0-4.20][22811-3-3-4.35][22976-3-4-0.82][22985-3-3-3.58][23014-0-3-3.17][23112-1-1-3.32][23144-3-3-4.76][23168-2-3--0.07]
[23219-0-3-0.73][23363-3-3-3.38][23470-0-0-1.07][23486-2-2-1.41][23497-0-3-4.81][23516-0-0-3.15][23690-1-4-1.36][23921-2-2-2.81][23936-1-2-2.63][24040-3-0-0.71]
[24111-1-4-3.18][24182-0-0-5.42][24238-3-3-3.52][24290-2-0-0.99][24345-0-0-1.49][24364-1-2-1.66][24427-3-0-1.79][24477-2-2-3.41][24495-2-1-1.29][24893-2-2-3.08]
[25012-1-1-0.76][25121-2-2-1.79][25165-3-3-1.38][25183-0-0-1.72][25297-3-3-3.05][25398-0-0-2.58][25574-2-2-2.72][25644-1-1-5.00][25718-1-4-0.55][25774-2-2-0.37]
[26032-3-3-2.99][26051-3-3-4.55][26120-0-4-0.99][26321-1-2-0.32][26732-1-1-2.04][26784-3-3-4.95][26827-3-3-2.81][26833-0-3-3.00][26838-2-3-0.39][26860-1-4-0.56]
[26948-0-0-1.20][27049-3-3-1.09][27098-1-0-0.16][27526-0-3-1.34][27639-3-3-1.88][27698-3-3-3.34][27772-0-0-2.50][27890-1-1-4.30][28040-0-0-0.90][28503-2-2-4.40]
[28577-1-1-3.63][28959-0-0-3.85][29198-3-3-1.10][29777-0-0-5.27][29877-2-2-2.33][30035-1-2-2.89][30098-0-3-1.40][30326-1-1-6.43][30572-2-2-3.36][30716-0-4-1.86]
[30806-2-3-1.93][30906-1-1-4.68][31007-0-0-1.08][31181-3-3-0.79][31238-0-0-0.98][31347-0-0-4.24][31422-2-2-0.36][31429-3-3-0.28][31431-0-3-1.37][31432-1-1-3.37]
[31477-0-3-3.74][31524-1-2-1.24][31597-1-2-2.17][31619-1-0-0.80][31701-0-0-2.26][31755-0-0-3.51][31854-3-3-2.95][32074-1-2-1.11][32078-3-3-3.57][32111-1-1-1.62]
[32127-1-1-2.30][32140-3-3-4.87][32263-2-2-0.26][32365-0-0-2.22][32411-2-3-3.58][32429-3-3-1.99][32473-3-3-1.78][32574-3-3-3.29][32584-0-0-1.29][32622-0-4-0.04]
[32858-3-3-2.92][32969-3-3-3.23][33016-2-2-3.30][33031-1-3-1.53][33035-2-2-2.02][33133-2-2-2.17][33173-2-2-1.02][33175-3-4-1.89][33306-3-3-0.89][33309-2-2-1.44]
[33474-0-4-0.19][33478-2-3--0.23][33618-1-1-0.84][33712-0-3-1.77][33782-2-4-2.08][33914-3-3-3.74][34076-3-3-0.90][34112-2-2-1.92][34138-2-3-0.91][34239-1-3-0.00]
[34364-2-2-4.25][34617-1-2-2.99][34751-3-3-2.81][34783-2-4-1.76][35015-3-3-1.79][35018-1-1-1.35][35288-2-3-0.29][0-4-4-1.46][1-4-4-1.48][2-4-4-1.46]
[3-4-4-1.61][4-4-2--0.00][5-4-3-0.31][6-4-4-2.84][7-4-4-1.00][8-4-3-0.82][9-4-2-0.58][10-4-4-2.81][11-4-2-3.80][12-4-2-0.41]
[14-4-3-1.61][15-4-3-3.27][16-4-4-2.34][17-4-4-0.86][18-4-4-0.90][19-4-3-2.20][20-4-0-0.06][21-4-2-1.16][22-4-4-2.57][23-4-4-0.23]
[24-4-4-5.45][25-4-3-1.09][26-4-4--0.18][27-4-4-1.02][28-4-4-1.91][29-4-1-0.89][30-4-3-0.82][31-4-4-0.76][32-4-4-2.27][33-4-2-1.55]
[34-4-4-0.08][35-4-0-1.42][37-4-4-0.45][39-4-0-1.82][40-4-4-0.64][41-4-4-0.48][42-4-2-1.75][43-4-2-1.28][45-4-2-1.51][46-4-4-3.35]
[47-4-4-3.37][48-4-4-1.53][51-4-4-3.25][52-4-4-0.41][53-4-4-0.63][54-4-3-2.24][55-4-2-1.70][56-4-1-1.57][57-4-0-2.07][58-4-2-3.27]
[59-4-0-1.40][60-4-1-0.65][61-4-4-2.68][62-4-2-1.70][63-4-2-1.34][64-4-4-0.91][65-4-4-3.37][66-4-2-1.51][67-4-3-0.20][68-4-4--0.12]
[69-4-3-0.45][70-4-4-1.71][72-4-1-0.97][73-4-1-1.89][74-4-2-2.03][75-4-3-0.34][77-4-4-4.12][78-4-3-1.07][79-4-4-2.28][80-4-4-2.23]
[81-4-4-2.03][82-4-1-1.47][83-4-4-0.87][84-4-4-2.06][85-4-4-1.31][86-4-4-0.36][87-4-4-2.18][88-4-4-1.62][89-4-4-1.01][90-4-4-0.33]
[91-4-1-0.75][92-4-2-0.78][93-4-0-0.80][94-4-4-2.57][95-4-3-0.46][96-4-2-1.56][97-4-4-1.78][98-4-2-0.58][99-4-4-2.37][100-4-1-0.72]
[101-4-4-4.03][102-4-4-1.30][103-4-3-0.79][104-4-4-2.17][105-4-4-1.55][106-4-4-2.17][107-4-4-1.14][108-4-2-0.37][109-4-4-0.10][110-4-4-1.53]
[111-4-0-3.45][112-4-2-1.03][113-4-4-0.62][114-4-2-0.32][115-4-4-0.59][116-4-4-0.90][117-4-4-1.95][119-4-4-0.79][121-4-4-1.62][122-4-4-1.87]
[124-4-3-0.40][125-4-4-2.79][126-4-4-2.54][127-4-2-0.87][128-4-0-0.73][129-4-2-1.72][130-4-4-0.99][131-4-2-1.11][132-4-2-0.49][133-4-4-4.56]
[135-4-2-1.22][136-4-4-0.46][137-4-4-0.48][138-4-4-0.96][139-4-4-1.09][140-4-1-0.56][141-4-3-1.25][142-4-4-4.04][143-4-4-2.36][144-4-4-3.94]
[145-4-4-2.27][148-4-0-3.06][149-4-4-0.68][150-4-4-2.20][151-4-4-2.23][152-4-4-0.73][153-4-4-2.07][154-4-4-4.28][155-4-4-1.88][156-4-3-1.52]
[157-4-2-0.75][158-4-3-1.28][160-4-4-0.19][161-4-2-0.98][162-4-2--0.03][164-4-2-1.77][165-4-2-1.12][167-4-4-1.18][168-4-4-1.75][170-4-3-0.93]
[171-4-4-1.75][172-4-4-2.46][173-4-4-2.26][174-4-0-3.23][175-4-4-1.16][177-4-4-2.74][178-4-4-2.34][179-4-4-0.83][180-4-4-3.29][181-4-3-2.94]
[182-4-3-1.80][183-4-4-1.67][184-4-2-1.39][186-4-4-0.39][187-4-2-0.66][188-4-2-2.17][189-4-4-0.59][190-4-4-0.15][191-4-4-1.70][192-4-4-0.50]
[193-4-2-2.33][194-4-2-1.06][195-4-0-1.35][196-4-4-0.44][197-4-4-1.36][198-4-4-4.42][199-4-2-1.52]
---------------------------
I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.349 | Acc: 79.875% | Wgt Acc: 89.789%
	I - Batch: 100 | Loss: 0.344 | Acc: 80.438% | Wgt Acc: 90.121%
	I - Batch: 150 | Loss: 0.362 | Acc: 79.542% | Wgt Acc: 89.526%
	I - Batch: 200 | Loss: 0.361 | Acc: 79.531% | Wgt Acc: 89.748%
I - num batch: 222
I - Train -- Loss: 0.357 | Acc: 79.899% | Wgt Acc: 89.959% | LR: 1.250000e-04 | Dur: 135.37s
I - Confusion Matrix: [row->prediction - col->label]
[[648.   5.   7.  24. 162.]
 [  4. 553.   5.   8.  90.]
 [  7.  11. 698.   7. 201.]
 [ 29.   4.   8. 489. 101.]
 [  9.   5.  16.  10. 446.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.052 | Acc: 61.144% | Wgt Acc: 64.652% | Dur: 13.91s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  5.  1. 18. 14.]
 [ 2. 42. 10.  2. 16.]
 [ 1. 20. 54.  7. 47.]
 [ 8.  5.  4. 54. 10.]
 [10.  6.  6.  5. 93.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.317 | Acc: 84.375% | Wgt Acc: 92.361%
	I - Batch: 100 | Loss: 0.310 | Acc: 84.750% | Wgt Acc: 92.603%
	I - Batch: 150 | Loss: 0.312 | Acc: 84.042% | Wgt Acc: 92.545%
	I - Batch: 200 | Loss: 0.314 | Acc: 83.594% | Wgt Acc: 92.143%
I - num batch: 208
I - Train -- Loss: 0.314 | Acc: 83.564% | Wgt Acc: 92.152% | LR: 1.250000e-04 | Dur: 127.73s
I - Confusion Matrix: [row->prediction - col->label]
[[653.   5.   5.  20. 105.]
 [  8. 557.   3.   4.  71.]
 [ 10.   8. 718.  11. 157.]
 [ 17.   2.   3. 497.  92.]
 [  9.   6.   5.   6. 356.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.111 | Acc: 57.988% | Wgt Acc: 63.499% | Dur: 14.66s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  3.  2. 13. 14.]
 [ 0. 38.  9.  1. 14.]
 [ 4. 20. 49.  2. 48.]
 [15.  7. 11. 64. 25.]
 [ 5. 10.  4.  6. 79.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.326 | Acc: 80.625% | Wgt Acc: 91.208%
	I - Batch: 100 | Loss: 0.334 | Acc: 80.562% | Wgt Acc: 91.065%
	I - Batch: 150 | Loss: 0.330 | Acc: 80.917% | Wgt Acc: 91.489%
	I - Batch: 200 | Loss: 0.328 | Acc: 81.031% | Wgt Acc: 91.344%
I - num batch: 222
I - Train -- Loss: 0.326 | Acc: 81.139% | Wgt Acc: 91.394% | LR: 1.250000e-04 | Dur: 134.79s
I - Confusion Matrix: [row->prediction - col->label]
[[649.   2.   9.  29. 157.]
 [  4. 565.   5.   2.  99.]
 [  8.   2. 713.   5. 183.]
 [ 22.   3.   3. 499. 109.]
 [ 14.   6.   4.   3. 452.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.974 | Acc: 64.103% | Wgt Acc: 64.675% | Dur: 14.94s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   2.  11.  17.]
 [  0.  39.   5.   1.   9.]
 [  4.  14.  48.   1.  30.]
 [ 16.   4.  11.  64.  11.]
 [  7.  19.   9.   9. 113.]]

I - Local maximum validation set accuracy:  64.10

I - Validation set results: 
[14-1-2-1.25][50-3-4-1.40][124-2-2-1.52][127-0-0-3.92][443-2-2-3.05][567-0-0-1.77][573-1-1-3.31][615-0-0-1.50][695-1-2-1.57][722-3-3-3.18]
[826-0-0-3.63][878-0-0-2.71][1103-0-4-1.38][1212-3-4-1.13][1368-0-0-3.30][2181-2-2-0.75][2476-2-2-1.13][2721-2-2-2.89][2818-1-3-0.62][2886-2-2-1.29]
[3231-2-2-5.05][3333-2-3-0.73][3482-2-2-1.83][3536-3-3-0.41][3625-1-1-2.94][3909-0-0-1.66][4035-0-3-0.93][4140-0-0-2.44][4214-1-3-2.12][4346-1-4-0.36]
[4581-2-2-1.73][4708-3-0-0.90][4838-3-3-0.53][4845-1-2-0.22][4868-0-0-3.03][4939-0-2-1.17][4984-2-2-2.55][5078-1-4-1.09][5396-0-0-4.46][5479-1-1-3.67]
[5717-0-0-1.17][5843-1-1-1.17][5949-3-3-1.64][5987-2-4-2.36][6014-3-3-1.11][6033-3-3-0.29][6313-0-0-2.42][6421-3-3-3.44][6500-1-4-0.37][6583-3-3-0.90]
[6683-3-3-1.05][6825-2-1-2.25][6998-3-3-0.30][7049-3-3-2.09][7517-1-1-2.33][7521-1-1-1.14][7528-1-3-0.54][7949-1-2-1.61][8135-1-0-1.27][8185-3-0-3.37]
[8269-3-1-1.88][8273-3-3-2.97][8543-3-0-3.95][8666-1-1-4.01][8672-0-0-5.10][8903-1-1-2.83][9001-2-2-0.63][9036-2-2-4.64][9281-3-3-0.65][9300-2-2-4.84]
[9571-0-4-0.63][9617-1-4-0.85][9644-2-2-2.09][9705-2-4-0.73][9801-0-3-1.84][9803-3-3-2.04][9865-3-3-4.57][9896-2-2-1.77][10314-1-4-0.29][10337-3-3-4.91]
[10403-0-4-1.07][10653-2-4-0.81][10704-2-1-1.42][10719-1-1-3.38][10727-1-4-1.02][10836-0-0-7.98][10969-2-3-0.61][11042-0-0-2.35][11088-1-1-5.50][11322-0-0-3.56]
[11398-2-2-1.55][11499-0-0-1.84][11502-3-3-2.50][11512-3-3-1.64][11608-1-2-2.40][11610-0-0-2.28][11692-0-3-1.62][11905-0-0-2.77][11993-1-1-1.71][12002-2-3-1.18]
[12052-0-0-2.67][12201-0-3-3.73][12235-2-2-2.20][12320-1-4-2.66][12377-2-4-1.80][12398-2-3-1.14][12503-1-4-0.50][12617-0-2-0.23][12685-3-4-0.59][12738-2-3-0.13]
[12742-2-2-3.59][12823-0-3-1.83][13110-1-1-2.23][13240-3-3-2.17][13253-1-4-1.46][13273-0-0-4.10][13634-1-4-0.99][13763-2-2-3.62][13905-3-3-0.96][14060-2-4-0.84]
[14065-3-0-1.43][14147-3-3-1.57][14595-2-2-1.91][14687-2-2-3.83][14788-2-2-1.22][14869-1-1-3.12][14872-3-4-0.99][14877-1-1-3.44][14927-0-3-2.26][15066-0-0-4.00]
[15175-1-1-1.22][15178-2-3-1.15][15375-3-0--0.01][15389-3-3-4.78][15568-2-4-1.06][15675-3-3-2.05][15869-1-4-1.05][16207-3-0-1.17][16236-0-0-1.45][16302-3-3-0.18]
[16331-2-2-5.64][16381-0-3-1.50][16488-1-1-5.70][16495-0-0-3.06][16650-0-0-3.95][16719-1-2-0.56][16801-0-0-4.77][16828-0-0-1.82][17137-3-3-1.01][17245-1-4-1.21]
[17278-3-4-1.41][17282-0-2-1.11][17311-2-2-2.10][17336-2-1-0.41][17608-3-3-5.81][17627-0-0-0.41][17877-3-4-1.38][17924-1-2-1.48][17984-3-3-2.31][18211-0-3-1.45]
[18276-3-0-1.57][18287-1-1-0.72][18394-0-0-2.82][18428-0-0-4.95][18442-0-3-2.88][18478-3-3-2.40][18607-0-0-1.47][18616-0-0-2.22][18663-0-0-2.91][18718-0-0-3.13]
[18766-2-2-2.43][18824-2-2-2.35][18890-3-3-1.98][18930-3-4-1.03][18938-3-3-1.43][19817-1-2-1.05][19839-0-0-0.61][19930-3-3-2.07][19944-0-4-2.42][20036-2-2-5.16]
[20101-3-3-2.13][20474-1-1-2.07][20547-3-3-0.68][20929-2-2-3.96][21245-1-1-1.22][21257-3-3-0.52][21293-1-2-2.74][21316-1-1-2.34][21384-1-4-2.81][21448-1-1-3.13]
[21483-0-0-3.08][21487-2-2-1.91][21714-0-0-0.80][21943-3-3-0.67][21947-0-0-2.67][21948-0-0-6.64][21965-2-2-1.77][21998-1-1-3.57][22025-0-2-1.01][22228-3-3-4.59]
[22446-1-1-5.50][22494-3-3-2.12][22757-0-0-3.83][22811-3-3-3.65][22976-3-2-0.65][22985-3-3-3.08][23014-0-3-3.12][23112-1-1-2.78][23144-3-3-5.23][23168-2-0-0.38]
[23219-0-0-0.99][23363-3-3-2.46][23470-0-0-0.67][23486-2-2-1.46][23497-0-3-4.25][23516-0-0-3.49][23690-1-4-1.51][23921-2-2-0.89][23936-1-2-2.15][24040-3-0-0.45]
[24111-1-4-2.45][24182-0-0-4.47][24238-3-3-2.99][24290-2-0-1.84][24345-0-0-2.76][24364-1-2-0.22][24427-3-0-1.44][24477-2-2-2.24][24495-2-1-0.95][24893-2-2-2.92]
[25012-1-1-0.58][25121-2-2-1.99][25165-3-3-1.22][25183-0-0-1.52][25297-3-3-3.22][25398-0-0-3.55][25574-2-2-1.88][25644-1-1-2.74][25718-1-4-0.68][25774-2-3-0.21]
[26032-3-3-2.86][26051-3-3-4.12][26120-0-4-3.05][26321-1-1-2.69][26732-1-1-2.21][26784-3-3-4.99][26827-3-3-1.93][26833-0-3-2.61][26838-2-3-0.51][26860-1-4-0.76]
[26948-0-0-1.96][27049-3-0-2.24][27098-1-0-0.45][27526-0-0-2.71][27639-3-3-1.70][27698-3-3-2.06][27772-0-0-4.20][27890-1-1-3.53][28040-0-0-0.47][28503-2-2-3.09]
[28577-1-1-3.55][28959-0-0-3.73][29198-3-3-1.73][29777-0-0-5.14][29877-2-1-0.97][30035-1-2-2.88][30098-0-3-1.58][30326-1-1-4.49][30572-2-2-2.15][30716-0-4-1.74]
[30806-2-3-1.86][30906-1-1-3.74][31007-0-0-1.75][31181-3-3-2.06][31238-0-3-1.48][31347-0-0-4.01][31422-2-2-0.73][31429-3-3-0.96][31431-0-0-0.88][31432-1-1-3.36]
[31477-0-3-2.84][31524-1-2-0.90][31597-1-1-0.77][31619-1-4-0.84][31701-0-0-2.60][31755-0-0-1.90][31854-3-3-1.57][32074-1-1-0.25][32078-3-3-3.53][32111-1-1-3.28]
[32127-1-1-1.91][32140-3-3-4.19][32263-2-2-0.90][32365-0-0-2.84][32411-2-3-3.99][32429-3-3-1.40][32473-3-0-1.08][32574-3-3-2.46][32584-0-0-1.84][32622-0-4-0.01]
[32858-3-3-2.28][32969-3-3-3.11][33016-2-2-4.02][33031-1-3-2.60][33035-2-2-1.50][33133-2-2-2.58][33173-2-2-0.50][33175-3-4-2.51][33306-3-3-0.68][33309-2-2-0.37]
[33474-0-0-0.27][33478-2-4--0.06][33618-1-1-2.04][33712-0-3-1.00][33782-2-4-2.38][33914-3-3-3.67][34076-3-4-0.87][34112-2-2-3.08][34138-2-3-0.69][34239-1-1-0.48]
[34364-2-2-3.30][34617-1-2-2.46][34751-3-3-2.56][34783-2-2-1.23][35015-3-3-1.54][35018-1-4-0.89][35288-2-4-0.10][0-4-4-2.64][1-4-4-3.14][2-4-4-2.30]
[3-4-4-1.76][4-4-2-0.08][5-4-1-1.64][6-4-0-2.61][7-4-4-2.92][8-4-4-0.60][9-4-1-0.91][10-4-4-3.59][11-4-4-3.03][12-4-4-0.37]
[14-4-3-0.97][15-4-3-3.13][16-4-4-1.28][17-4-4-1.02][18-4-4-3.86][19-4-3-1.77][20-4-2-0.62][21-4-2-0.95][22-4-4-1.65][23-4-4-0.55]
[24-4-4-6.06][25-4-3-0.79][26-4-4-0.32][27-4-4-1.36][28-4-4-2.62][29-4-1-0.18][30-4-3-0.91][31-4-4-1.90][32-4-4-1.60][33-4-2-0.71]
[34-4-4-1.23][35-4-0-1.97][37-4-4-1.86][39-4-0-2.46][40-4-4-0.14][41-4-2-0.02][42-4-2-1.74][43-4-2-0.36][45-4-4-1.69][46-4-4-2.87]
[47-4-4-3.40][48-4-4-1.67][51-4-4-3.08][52-4-4-1.10][53-4-4-1.01][54-4-3-1.59][55-4-2-1.83][56-4-1-1.93][57-4-0-2.22][58-4-2-2.78]
[59-4-0-1.89][60-4-4-1.10][61-4-4-1.94][62-4-2-1.62][63-4-2-2.38][64-4-2-0.70][65-4-4-3.66][66-4-4-1.43][67-4-2-1.63][68-4-4-0.87]
[69-4-3-0.40][70-4-4-2.04][72-4-1-1.10][73-4-1-1.58][74-4-2-2.20][75-4-3-0.59][77-4-4-2.94][78-4-3-0.54][79-4-4-2.71][80-4-4-1.94]
[81-4-4-3.11][82-4-1-1.51][83-4-4-1.45][84-4-4-1.60][85-4-4-2.76][86-4-2-1.23][87-4-4-2.26][88-4-4-2.03][89-4-3-0.17][90-4-4-1.65]
[91-4-4-1.26][92-4-2-0.57][93-4-0-0.21][94-4-4-2.50][95-4-4-1.59][96-4-4-1.01][97-4-4-2.69][98-4-2-0.74][99-4-4-2.21][100-4-1-1.48]
[101-4-4-5.00][102-4-4-1.31][103-4-0-0.31][104-4-4-1.82][105-4-4-3.46][106-4-4-2.60][107-4-4-1.85][108-4-4-0.90][109-4-4-0.90][110-4-4-1.40]
[111-4-0-4.38][112-4-2-1.12][113-4-4-1.11][114-4-2-0.75][115-4-4-0.67][116-4-4-1.06][117-4-4-2.41][119-4-4-1.89][121-4-4-1.94][122-4-4-2.47]
[124-4-4-0.60][125-4-4-2.53][126-4-4-3.73][127-4-2-2.30][128-4-0-0.81][129-4-1-0.89][130-4-2-1.39][131-4-2-1.50][132-4-4-0.67][133-4-4-3.68]
[135-4-2-0.66][136-4-0-0.61][137-4-4-1.11][138-4-4-0.88][139-4-4-1.30][140-4-4-0.77][141-4-0-1.06][142-4-4-3.56][143-4-2-1.65][144-4-4-2.29]
[145-4-4-2.18][148-4-0-3.42][149-4-4-1.41][150-4-4-2.97][151-4-4-2.39][152-4-4-1.53][153-4-4-1.99][154-4-4-5.59][155-4-4-2.64][156-4-3-1.69]
[157-4-2-1.68][158-4-4-1.48][160-4-4-0.54][161-4-2-1.35][162-4-4-1.14][164-4-4-1.01][165-4-4-1.50][167-4-0-1.58][168-4-4-1.60][170-4-4-1.32]
[171-4-4-2.71][172-4-4-2.62][173-4-4-2.69][174-4-0-2.18][175-4-4-2.78][177-4-4-0.91][178-4-4-1.93][179-4-4-1.70][180-4-4-2.16][181-4-4-1.45]
[182-4-4-1.24][183-4-4-2.28][184-4-4-2.18][186-4-0-0.87][187-4-2-1.00][188-4-4-1.75][189-4-4-1.32][190-4-4-0.32][191-4-4-3.00][192-4-4-0.72]
[193-4-2-2.70][194-4-0-0.66][195-4-0-1.66][196-4-2-0.98][197-4-4-2.30][198-4-4-6.91][199-4-2-1.65]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.280 | Acc: 85.000% | Wgt Acc: 93.953%
	I - Batch: 100 | Loss: 0.298 | Acc: 84.188% | Wgt Acc: 92.889%
	I - Batch: 150 | Loss: 0.291 | Acc: 83.917% | Wgt Acc: 92.825%
	I - Batch: 200 | Loss: 0.298 | Acc: 82.906% | Wgt Acc: 92.346%
I - num batch: 222
I - Train -- Loss: 0.306 | Acc: 82.549% | Wgt Acc: 91.899% | LR: 1.250000e-04 | Dur: 137.14s
I - Confusion Matrix: [row->prediction - col->label]
[[654.   1.   3.  23. 141.]
 [  5. 564.   9.   0.  66.]
 [ 10.   5. 709.   3. 200.]
 [ 17.   3.   4. 503.  95.]
 [ 11.   5.   9.   9. 498.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.031 | Acc: 61.538% | Wgt Acc: 62.680% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   3.  19.  19.]
 [  0.  39.   4.   0.   9.]
 [  1.  19.  47.   6.  30.]
 [ 14.   6.  11.  55.  17.]
 [  7.  12.  10.   6. 105.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.297 | Acc: 84.250% | Wgt Acc: 92.912%
	I - Batch: 100 | Loss: 0.294 | Acc: 83.312% | Wgt Acc: 92.705%
	I - Batch: 150 | Loss: 0.292 | Acc: 83.250% | Wgt Acc: 92.778%
	I - Batch: 200 | Loss: 0.299 | Acc: 83.156% | Wgt Acc: 92.196%
I - num batch: 222
I - Train -- Loss: 0.304 | Acc: 82.690% | Wgt Acc: 92.001% | LR: 1.250000e-04 | Dur: 137.65s
I - Confusion Matrix: [row->prediction - col->label]
[[653.   1.   2.  26. 160.]
 [  7. 568.   4.   5.  75.]
 [  9.   4. 717.   5. 180.]
 [ 14.   1.   2. 495.  85.]
 [ 14.   4.   9.   7. 500.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.034 | Acc: 63.708% | Wgt Acc: 62.207% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   6.   2.  13.  11.]
 [  0.  35.   4.   1.   3.]
 [  2.  18.  44.   3.  31.]
 [ 17.   3.  10.  63.  13.]
 [ 10.  16.  15.   6. 122.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.280 | Acc: 84.625% | Wgt Acc: 93.207%
	I - Batch: 100 | Loss: 0.279 | Acc: 85.125% | Wgt Acc: 93.465%
	I - Batch: 150 | Loss: 0.292 | Acc: 83.542% | Wgt Acc: 92.602%
	I - Batch: 200 | Loss: 0.301 | Acc: 83.469% | Wgt Acc: 92.370%
I - num batch: 222
I - Train -- Loss: 0.305 | Acc: 83.225% | Wgt Acc: 92.095% | LR: 1.250000e-04 | Dur: 139.07s
I - Confusion Matrix: [row->prediction - col->label]
[[656.   2.   3.  21. 136.]
 [  7. 566.   4.   5.  84.]
 [  4.   4. 712.   7. 168.]
 [ 17.   2.   5. 497.  91.]
 [ 13.   4.  10.   8. 521.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.072 | Acc: 60.947% | Wgt Acc: 62.069% | Dur: 16.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   5.   2.  17.  20.]
 [  0.  35.   7.   0.   7.]
 [  2.  21.  45.   6.  33.]
 [ 13.   5.  14.  56.  16.]
 [  4.  12.   7.   7. 104.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.291 | Acc: 82.875% | Wgt Acc: 92.708%
	I - Batch: 100 | Loss: 0.292 | Acc: 83.000% | Wgt Acc: 92.355%
	I - Batch: 150 | Loss: 0.297 | Acc: 82.750% | Wgt Acc: 92.126%
	I - Batch: 200 | Loss: 0.306 | Acc: 82.219% | Wgt Acc: 91.798%
I - num batch: 222
I - Train -- Loss: 0.308 | Acc: 82.210% | Wgt Acc: 91.773% | LR: 1.250000e-04 | Dur: 144.22s
I - Confusion Matrix: [row->prediction - col->label]
[[652.   5.   2.  20. 145.]
 [  7. 558.   2.   2.  94.]
 [  7.   7. 715.   6. 177.]
 [ 17.   2.   6. 504.  97.]
 [ 14.   6.   9.   6. 487.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.073 | Acc: 58.777% | Wgt Acc: 62.922% | Dur: 15.21s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  6.  4. 15. 30.]
 [ 1. 42.  8.  0. 11.]
 [ 2. 18. 45.  3. 33.]
 [16.  7. 11. 63. 19.]
 [ 8.  5.  7.  5. 87.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.280 | Acc: 83.250% | Wgt Acc: 92.632%
	I - Batch: 100 | Loss: 0.285 | Acc: 82.875% | Wgt Acc: 92.244%
	I - Batch: 150 | Loss: 0.291 | Acc: 82.667% | Wgt Acc: 92.008%
	I - Batch: 200 | Loss: 0.292 | Acc: 82.469% | Wgt Acc: 91.965%
I - num batch: 222
I - Train -- Loss: 0.294 | Acc: 82.520% | Wgt Acc: 91.984% | LR: 1.250000e-04 | Dur: 136.37s
I - Confusion Matrix: [row->prediction - col->label]
[[652.   5.   4.  16. 153.]
 [  7. 563.   3.   2.  92.]
 [  3.   5. 711.   6. 168.]
 [ 15.   0.   4. 507.  93.]
 [ 20.   5.  12.   7. 494.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.000 | Acc: 64.497% | Wgt Acc: 64.733% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   3.  10.  15.]
 [  0.  38.   9.   0.   8.]
 [  1.  19.  45.   3.  33.]
 [ 11.   3.   7.  62.   9.]
 [  9.  14.  11.  11. 115.]]

I - Local maximum validation set accuracy:  64.50

I - Validation set results: 
[14-1-2-0.49][50-3-4-0.53][124-2-2-1.33][127-0-0-4.92][443-2-2-2.71][567-0-0-1.54][573-1-1-2.82][615-0-0-1.41][695-1-2-2.05][722-3-3-2.55]
[826-0-0-2.93][878-0-0-4.03][1103-0-4-1.51][1212-3-4-1.10][1368-0-0-3.47][2181-2-3-0.94][2476-2-2-1.61][2721-2-2-3.77][2818-1-4-0.46][2886-2-4-1.78]
[3231-2-2-5.26][3333-2-2-0.52][3482-2-2-2.11][3536-3-3-1.55][3625-1-1-2.36][3909-0-0-1.51][4035-0-0-1.49][4140-0-0-1.90][4214-1-3-0.15][4346-1-4-1.13]
[4581-2-1-2.79][4708-3-3-0.41][4838-3-3-0.28][4845-1-2-0.27][4868-0-0-4.11][4939-0-3-0.63][4984-2-2-3.82][5078-1-4-0.63][5396-0-0-4.82][5479-1-1-3.48]
[5717-0-0-3.06][5843-1-1-2.11][5949-3-3-2.01][5987-2-4-2.73][6014-3-3-1.80][6033-3-4-0.57][6313-0-0-2.82][6421-3-3-3.35][6500-1-1--0.32][6583-3-3-1.25]
[6683-3-3-1.27][6825-2-1-1.65][6998-3-3-0.88][7049-3-3-3.08][7517-1-1-2.37][7521-1-1-1.19][7528-1-3-0.51][7949-1-2-2.85][8135-1-0-1.25][8185-3-0-3.46]
[8269-3-4-1.33][8273-3-3-3.28][8543-3-0-3.75][8666-1-1-4.79][8672-0-0-5.95][8903-1-2-1.78][9001-2-1-1.14][9036-2-2-3.20][9281-3-3-0.73][9300-2-2-4.59]
[9571-0-3-0.51][9617-1-1-2.96][9644-2-2-3.23][9705-2-4-0.99][9801-0-3-2.50][9803-3-3-2.04][9865-3-3-5.61][9896-2-2-3.93][10314-1-2-0.26][10337-3-3-5.22]
[10403-0-4-1.22][10653-2-2-0.55][10704-2-1-1.77][10719-1-1-2.87][10727-1-4-1.91][10836-0-0-8.94][10969-2-3-1.01][11042-0-0-2.74][11088-1-1-4.16][11322-0-0-4.54]
[11398-2-2-1.75][11499-0-0-2.16][11502-3-3-1.63][11512-3-3-1.89][11608-1-2-2.15][11610-0-0-2.56][11692-0-0-2.39][11905-0-0-3.41][11993-1-1-1.68][12002-2-3-1.50]
[12052-0-0-3.05][12201-0-3-3.23][12235-2-2-3.17][12320-1-4-2.62][12377-2-4-2.14][12398-2-3-1.28][12503-1-4-1.22][12617-0-3-0.30][12685-3-3-1.10][12738-2-4-0.75]
[12742-2-2-5.28][12823-0-0-3.13][13110-1-2-1.07][13240-3-3-2.65][13253-1-4-2.32][13273-0-0-4.94][13634-1-4-1.73][13763-2-2-2.83][13905-3-3-0.24][14060-2-1-1.56]
[14065-3-0-2.04][14147-3-3-1.65][14595-2-2-2.19][14687-2-2-4.30][14788-2-2-1.66][14869-1-1-2.68][14872-3-4-0.69][14877-1-1-3.03][14927-0-3-2.78][15066-0-0-5.42]
[15175-1-1-0.86][15178-2-2-0.49][15375-3-0-1.47][15389-3-3-4.54][15568-2-1-0.67][15675-3-3-3.99][15869-1-4-0.39][16207-3-0-1.13][16236-0-2-0.27][16302-3-3-0.66]
[16331-2-2-7.30][16381-0-3-1.37][16488-1-1-3.68][16495-0-0-4.71][16650-0-0-3.95][16719-1-2-1.07][16801-0-0-4.81][16828-0-0-3.75][17137-3-3-1.24][17245-1-4-1.33]
[17278-3-4-1.61][17282-0-0-0.92][17311-2-2-2.98][17336-2-1-2.69][17608-3-3-5.92][17627-0-0-0.56][17877-3-4-2.13][17924-1-2-2.09][17984-3-0-2.92][18211-0-3-1.32]
[18276-3-3-1.61][18287-1-2-0.18][18394-0-0-2.80][18428-0-0-0.87][18442-0-3-2.34][18478-3-3-2.00][18607-0-0-1.11][18616-0-0-2.66][18663-0-0-3.77][18718-0-0-3.36]
[18766-2-2-2.49][18824-2-2-2.15][18890-3-3-3.22][18930-3-4-1.88][18938-3-3-1.32][19817-1-2-1.22][19839-0-0-0.69][19930-3-3-1.69][19944-0-4-2.18][20036-2-2-5.88]
[20101-3-3-2.83][20474-1-1-1.46][20547-3-0-1.42][20929-2-2-3.49][21245-1-1-0.69][21257-3-2-0.67][21293-1-1-1.31][21316-1-1-3.86][21384-1-4-3.10][21448-1-1-3.44]
[21483-0-0-2.95][21487-2-2-1.94][21714-0-0-1.23][21943-3-3-0.73][21947-0-0-3.20][21948-0-0-7.41][21965-2-2-1.88][21998-1-1-3.47][22025-0-4-0.91][22228-3-3-4.83]
[22446-1-1-6.45][22494-3-3-3.04][22757-0-0-4.82][22811-3-3-4.18][22976-3-2-0.96][22985-3-3-2.97][23014-0-0-3.47][23112-1-1-2.86][23144-3-3-4.22][23168-2-0-0.63]
[23219-0-0-1.41][23363-3-3-1.79][23470-0-0-1.68][23486-2-2-1.73][23497-0-3-4.70][23516-0-0-4.39][23690-1-1-0.97][23921-2-2-1.23][23936-1-2-3.59][24040-3-4-0.37]
[24111-1-4-2.69][24182-0-0-4.71][24238-3-3-2.57][24290-2-0-1.83][24345-0-0-3.10][24364-1-2-0.43][24427-3-0-2.83][24477-2-2-3.72][24495-2-4-0.93][24893-2-2-3.37]
[25012-1-4-0.69][25121-2-1-2.18][25165-3-3-1.22][25183-0-0-3.23][25297-3-3-3.55][25398-0-0-3.00][25574-2-2-2.92][25644-1-1-3.35][25718-1-4--0.01][25774-2-3-0.09]
[26032-3-3-3.28][26051-3-3-4.05][26120-0-4-1.16][26321-1-2-0.60][26732-1-1-3.09][26784-3-3-5.49][26827-3-3-1.52][26833-0-3-3.47][26838-2-4-1.04][26860-1-2-1.72]
[26948-0-0-1.20][27049-3-0-2.59][27098-1-0-2.70][27526-0-0-3.50][27639-3-3-1.95][27698-3-3-3.30][27772-0-0-4.92][27890-1-1-3.76][28040-0-4-0.79][28503-2-2-4.53]
[28577-1-1-3.40][28959-0-0-4.04][29198-3-3-1.27][29777-0-0-5.91][29877-2-1-0.38][30035-1-2-2.04][30098-0-0-1.60][30326-1-1-5.30][30572-2-2-3.45][30716-0-4-1.38]
[30806-2-3-1.93][30906-1-1-4.74][31007-0-0-2.40][31181-3-3-2.10][31238-0-0-1.17][31347-0-0-4.25][31422-2-4-0.82][31429-3-3-0.91][31431-0-0-1.68][31432-1-1-2.13]
[31477-0-0-3.35][31524-1-0-0.39][31597-1-2-1.43][31619-1-0-1.07][31701-0-0-2.40][31755-0-0-2.56][31854-3-3-1.57][32074-1-1-1.23][32078-3-3-3.61][32111-1-1-1.40]
[32127-1-1-3.05][32140-3-3-4.24][32263-2-0-0.16][32365-0-0-2.55][32411-2-3-3.75][32429-3-0-1.73][32473-3-3-1.19][32574-3-3-2.57][32584-0-0-2.10][32622-0-4-0.90]
[32858-3-3-3.02][32969-3-3-2.55][33016-2-2-4.57][33031-1-3-2.11][33035-2-2-2.33][33133-2-2-2.14][33173-2-2-1.30][33175-3-4-2.59][33306-3-3-1.20][33309-2-2-1.39]
[33474-0-0-0.62][33478-2-4-0.07][33618-1-1-1.56][33712-0-4-1.08][33782-2-4-2.23][33914-3-3-3.10][34076-3-4-0.58][34112-2-2-3.66][34138-2-2-0.58][34239-1-1-0.29]
[34364-2-2-3.66][34617-1-2-2.30][34751-3-3-2.01][34783-2-4-2.40][35015-3-2-1.57][35018-1-1-1.41][35288-2-2-0.72][0-4-2-2.27][1-4-4-1.78][2-4-4-2.09]
[3-4-4-1.83][4-4-2-0.65][5-4-3-0.36][6-4-4-3.90][7-4-4-0.81][8-4-2-2.33][9-4-4-1.32][10-4-4-3.35][11-4-4-3.57][12-4-4-0.51]
[14-4-3-1.46][15-4-3-3.17][16-4-4-0.91][17-4-4-0.69][18-4-4-3.20][19-4-0-1.93][20-4-2-1.28][21-4-4-1.50][22-4-4-1.44][23-4-4-0.40]
[24-4-4-6.53][25-4-3-0.51][26-4-3-0.08][27-4-2-0.74][28-4-4-3.30][29-4-1-1.80][30-4-0-0.28][31-4-4-1.71][32-4-4-1.35][33-4-2-1.62]
[34-4-4-1.23][35-4-0-1.90][37-4-4-1.20][39-4-0-2.13][40-4-4-1.44][41-4-1--0.10][42-4-4-0.67][43-4-2-1.35][45-4-2--0.44][46-4-4-3.29]
[47-4-4-2.94][48-4-4-1.98][51-4-4-2.95][52-4-4-1.41][53-4-4-0.91][54-4-3-1.79][55-4-2-0.63][56-4-1-2.08][57-4-0-2.17][58-4-2-2.68]
[59-4-0-2.30][60-4-0-0.59][61-4-4-1.96][62-4-2-2.11][63-4-2-3.00][64-4-2-1.02][65-4-4-3.69][66-4-4-3.80][67-4-2-2.15][68-4-4-1.26]
[69-4-0--0.57][70-4-4-2.25][72-4-4-1.77][73-4-1-2.28][74-4-4-2.43][75-4-0-0.09][77-4-4-4.09][78-4-3-0.61][79-4-4-2.78][80-4-4-0.62]
[81-4-2-2.36][82-4-1-2.01][83-4-4-1.56][84-4-4-3.34][85-4-4-3.72][86-4-2-2.32][87-4-4-2.68][88-4-4-1.99][89-4-4-1.77][90-4-4-0.65]
[91-4-4-1.21][92-4-2--0.16][93-4-4-0.64][94-4-4-2.16][95-4-4-0.84][96-4-2-1.09][97-4-4-2.19][98-4-2-1.29][99-4-4-2.52][100-4-4-2.68]
[101-4-4-4.68][102-4-4-1.95][103-4-4-0.15][104-4-4-1.58][105-4-4-2.80][106-4-4-3.24][107-4-4-2.45][108-4-4-1.76][109-4-1-0.46][110-4-4-1.21]
[111-4-0-4.91][112-4-2-1.44][113-4-2-0.62][114-4-3-0.64][115-4-1-0.75][116-4-4-1.02][117-4-4-1.92][119-4-4-3.59][121-4-4-1.67][122-4-4-1.61]
[124-4-4-0.40][125-4-4-3.32][126-4-4-4.80][127-4-4-1.86][128-4-4-0.88][129-4-1-0.88][130-4-4-0.68][131-4-2-2.28][132-4-4-0.83][133-4-4-1.53]
[135-4-2-2.20][136-4-4--0.00][137-4-2-0.69][138-4-4-1.24][139-4-4-1.12][140-4-4-0.94][141-4-2-0.77][142-4-4-2.57][143-4-2-1.42][144-4-4-3.02]
[145-4-2-2.16][148-4-0-4.23][149-4-4-2.07][150-4-4-2.93][151-4-4-2.97][152-4-4-1.90][153-4-4-2.10][154-4-4-5.74][155-4-4-2.25][156-4-3-0.38]
[157-4-2-2.15][158-4-4-1.27][160-4-4-0.92][161-4-2-1.28][162-4-4-0.57][164-4-4-1.41][165-4-4-1.85][167-4-0-2.66][168-4-4-1.66][170-4-4-1.04]
[171-4-4-2.30][172-4-4-3.45][173-4-4-1.85][174-4-0-3.07][175-4-4-3.32][177-4-4-0.36][178-4-4-2.15][179-4-4-0.93][180-4-4-2.92][181-4-4-1.18]
[182-4-4-1.65][183-4-4-2.15][184-4-4-1.98][186-4-4-0.50][187-4-2-1.10][188-4-2-1.75][189-4-4-1.82][190-4-4-0.92][191-4-4-2.47][192-4-4-2.28]
[193-4-2-2.51][194-4-0-1.31][195-4-0-0.43][196-4-4-0.89][197-4-4-2.72][198-4-4-6.57][199-4-2-2.50]
---------------------------
I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.303 | Acc: 81.375% | Wgt Acc: 91.047%
	I - Batch: 100 | Loss: 0.301 | Acc: 82.312% | Wgt Acc: 92.146%
	I - Batch: 150 | Loss: 0.296 | Acc: 83.000% | Wgt Acc: 92.455%
	I - Batch: 200 | Loss: 0.293 | Acc: 83.156% | Wgt Acc: 92.545%
I - num batch: 222
I - Train -- Loss: 0.291 | Acc: 83.676% | Wgt Acc: 92.804% | LR: 1.250000e-04 | Dur: 137.04s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   3.   6.  12. 138.]
 [  4. 565.   1.   1.  74.]
 [  3.   4. 714.   6. 173.]
 [ 19.   2.   4. 509.  98.]
 [  8.   4.   9.  10. 517.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.167 | Acc: 60.158% | Wgt Acc: 63.199% | Dur: 14.47s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  5.  1. 11. 19.]
 [ 0. 37.  5.  1.  7.]
 [ 0. 14. 40.  1. 32.]
 [16.  8. 18. 69. 27.]
 [ 8. 14. 11.  4. 95.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.275 | Acc: 85.375% | Wgt Acc: 93.840%
	I - Batch: 100 | Loss: 0.275 | Acc: 84.562% | Wgt Acc: 93.531%
	I - Batch: 150 | Loss: 0.275 | Acc: 84.708% | Wgt Acc: 93.479%
	I - Batch: 200 | Loss: 0.273 | Acc: 84.844% | Wgt Acc: 93.536%
I - num batch: 222
I - Train -- Loss: 0.276 | Acc: 84.663% | Wgt Acc: 93.405% | LR: 1.250000e-04 | Dur: 135.18s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   0.   4.  17. 132.]
 [  2. 570.   2.   2.  91.]
 [  4.   3. 717.   5. 151.]
 [ 11.   1.   3. 507.  86.]
 [ 11.   4.   8.   7. 540.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.987 | Acc: 63.511% | Wgt Acc: 64.548% | Dur: 14.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   3.  13.  19.]
 [  2.  43.   7.   1.  10.]
 [  0.  16.  43.   1.  26.]
 [ 17.   4.  12.  64.  15.]
 [  7.  11.  10.   7. 110.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.237 | Acc: 87.125% | Wgt Acc: 94.834%
	I - Batch: 100 | Loss: 0.240 | Acc: 86.438% | Wgt Acc: 94.366%
	I - Batch: 150 | Loss: 0.252 | Acc: 85.458% | Wgt Acc: 93.723%
	I - Batch: 200 | Loss: 0.260 | Acc: 85.250% | Wgt Acc: 93.432%
I - num batch: 222
I - Train -- Loss: 0.262 | Acc: 85.199% | Wgt Acc: 93.389% | LR: 1.250000e-04 | Dur: 136.70s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   2.   3.  14. 129.]
 [  3. 564.   3.   1.  63.]
 [  3.   6. 722.   5. 151.]
 [ 20.   1.   1. 511.  92.]
 [ 11.   5.   5.   7. 565.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.066 | Acc: 61.538% | Wgt Acc: 64.595% | Dur: 16.40s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  5.  2. 13. 24.]
 [ 0. 41.  5.  0.  7.]
 [ 0. 16. 41.  2. 23.]
 [14.  5. 20. 65. 29.]
 [ 6. 11.  7.  6. 97.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.249 | Acc: 83.875% | Wgt Acc: 92.836%
	I - Batch: 100 | Loss: 0.252 | Acc: 84.000% | Wgt Acc: 93.419%
	I - Batch: 150 | Loss: 0.257 | Acc: 84.083% | Wgt Acc: 93.535%
	I - Batch: 200 | Loss: 0.262 | Acc: 83.875% | Wgt Acc: 93.478%
I - num batch: 222
I - Train -- Loss: 0.264 | Acc: 83.817% | Wgt Acc: 93.290% | LR: 1.250000e-04 | Dur: 134.71s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   0.   3.  17. 159.]
 [  1. 573.   2.   1.  86.]
 [  9.   2. 721.   2. 146.]
 [  8.   0.   3. 507. 103.]
 [ 13.   3.   5.  11. 506.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.039 | Acc: 62.919% | Wgt Acc: 63.983% | Dur: 14.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   6.   2.  13.  20.]
 [  0.  43.   6.   0.  12.]
 [  1.  17.  40.   1.  17.]
 [ 17.   3.  16.  65.  22.]
 [  8.   9.  11.   7. 109.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.250 | Acc: 86.125% | Wgt Acc: 93.900%
	I - Batch: 100 | Loss: 0.244 | Acc: 86.188% | Wgt Acc: 93.868%
	I - Batch: 150 | Loss: 0.244 | Acc: 85.875% | Wgt Acc: 93.940%
	I - Batch: 200 | Loss: 0.252 | Acc: 85.406% | Wgt Acc: 93.812%
I - num batch: 222
I - Train -- Loss: 0.255 | Acc: 85.424% | Wgt Acc: 93.715% | LR: 1.250000e-04 | Dur: 137.02s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   4.   6.  16. 121.]
 [  2. 567.   2.   0.  73.]
 [  3.   2. 717.   3. 160.]
 [ 10.   1.   2. 515.  82.]
 [ 15.   4.   7.   4. 564.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.983 | Acc: 65.089% | Wgt Acc: 65.264% | Dur: 16.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   3.  12.  18.]
 [  1.  47.   9.   0.  13.]
 [  0.  14.  38.   1.  20.]
 [ 15.   3.  13.  64.  12.]
 [  8.   9.  12.   9. 117.]]

I - Local maximum validation set accuracy:  65.09

I - Validation set results: 
[14-1-2-0.58][50-3-4-0.97][124-2-2-0.78][127-0-0-5.39][443-2-2-2.80][567-0-0-2.56][573-1-1-4.07][615-0-0-2.23][695-1-2-2.24][722-3-0-3.18]
[826-0-0-3.85][878-0-0-4.90][1103-0-0-1.32][1212-3-4-1.42][1368-0-0-4.49][2181-2-3-1.12][2476-2-2-1.96][2721-2-2-3.11][2818-1-3-0.95][2886-2-1-1.37]
[3231-2-2-6.04][3333-2-3-1.44][3482-2-2-2.09][3536-3-3-1.63][3625-1-1-3.55][3909-0-0-1.59][4035-0-3-1.14][4140-0-0-2.19][4214-1-3-2.37][4346-1-0-0.66]
[4581-2-2-1.61][4708-3-4-1.11][4838-3-3-0.64][4845-1-2-0.19][4868-0-0-6.29][4939-0-1-0.24][4984-2-2-3.36][5078-1-4-0.40][5396-0-0-5.68][5479-1-1-4.48]
[5717-0-0-3.72][5843-1-1-1.28][5949-3-3-1.39][5987-2-4-2.50][6014-3-3-1.29][6033-3-3-0.80][6313-0-0-2.88][6421-3-3-3.65][6500-1-4-0.05][6583-3-3-1.66]
[6683-3-3-1.03][6825-2-1-1.66][6998-3-3-0.17][7049-3-3-2.42][7517-1-1-2.39][7521-1-1-2.32][7528-1-1-0.56][7949-1-2-1.61][8135-1-0-2.09][8185-3-0-3.06]
[8269-3-4-0.58][8273-3-3-3.70][8543-3-0-5.09][8666-1-1-3.12][8672-0-0-4.07][8903-1-2-1.39][9001-2-1-1.44][9036-2-2-5.13][9281-3-3-0.99][9300-2-2-6.24]
[9571-0-3-0.82][9617-1-1-2.75][9644-2-2-1.92][9705-2-4-1.09][9801-0-3-2.69][9803-3-3-2.16][9865-3-3-4.88][9896-2-2-1.58][10314-1-1-1.24][10337-3-3-4.84]
[10403-0-4-1.04][10653-2-4-0.47][10704-2-1-0.58][10719-1-1-3.36][10727-1-1-1.34][10836-0-0-9.27][10969-2-3-0.84][11042-0-0-2.46][11088-1-1-4.75][11322-0-0-5.01]
[11398-2-2-4.35][11499-0-0-2.53][11502-3-3-2.07][11512-3-3-2.31][11608-1-2-1.92][11610-0-0-1.40][11692-0-0-1.73][11905-0-0-4.15][11993-1-1-4.28][12002-2-0-2.24]
[12052-0-0-3.22][12201-0-3-3.27][12235-2-4-0.88][12320-1-0-2.30][12377-2-4-1.63][12398-2-3-1.47][12503-1-4-0.29][12617-0-3-0.22][12685-3-3-0.76][12738-2-0-0.97]
[12742-2-2-4.44][12823-0-0-2.95][13110-1-1-1.77][13240-3-3-2.70][13253-1-1-2.69][13273-0-0-5.90][13634-1-4-0.80][13763-2-2-3.01][13905-3-3-0.54][14060-2-1-0.93]
[14065-3-0-2.24][14147-3-3-2.32][14595-2-2-1.67][14687-2-2-3.61][14788-2-2-2.48][14869-1-1-2.63][14872-3-3-0.82][14877-1-1-3.78][14927-0-3-2.35][15066-0-0-4.59]
[15175-1-1-1.81][15178-2-3-1.45][15375-3-0-0.43][15389-3-3-4.28][15568-2-1-0.57][15675-3-3-2.32][15869-1-0-0.14][16207-3-0-1.05][16236-0-3-0.29][16302-3-3-0.43]
[16331-2-2-7.21][16381-0-3-2.06][16488-1-1-2.99][16495-0-0-2.13][16650-0-0-5.44][16719-1-1-0.50][16801-0-0-5.44][16828-0-0-3.73][17137-3-3-1.01][17245-1-4-0.64]
[17278-3-0-0.92][17282-0-0-0.55][17311-2-2-2.18][17336-2-1-1.33][17608-3-3-6.15][17627-0-4--0.09][17877-3-4-1.07][17924-1-2-1.95][17984-3-0-3.48][18211-0-3-0.56]
[18276-3-3-1.26][18287-1-1-0.77][18394-0-0-3.99][18428-0-0-5.26][18442-0-3-2.74][18478-3-3-2.48][18607-0-0-2.50][18616-0-0-2.80][18663-0-0-3.51][18718-0-0-4.88]
[18766-2-2-3.22][18824-2-4-2.03][18890-3-3-2.49][18930-3-4-1.41][18938-3-3-1.85][19817-1-1-1.46][19839-0-4-0.53][19930-3-3-1.69][19944-0-4-2.27][20036-2-2-5.42]
[20101-3-3-2.85][20474-1-1-1.32][20547-3-3-0.38][20929-2-2-4.40][21245-1-1-1.35][21257-3-3-1.25][21293-1-2-1.73][21316-1-1-4.11][21384-1-4-1.88][21448-1-1-3.10]
[21483-0-0-3.76][21487-2-2-1.56][21714-0-0-2.65][21943-3-3-0.77][21947-0-0-2.03][21948-0-0-6.95][21965-2-2-1.53][21998-1-1-5.42][22025-0-4-1.46][22228-3-3-4.75]
[22446-1-1-2.66][22494-3-3-2.15][22757-0-0-4.60][22811-3-3-2.14][22976-3-2-0.95][22985-3-3-2.19][23014-0-3-3.04][23112-1-1-4.13][23144-3-3-4.20][23168-2-3-0.82]
[23219-0-0-1.41][23363-3-3-3.84][23470-0-0-2.49][23486-2-4-1.30][23497-0-3-4.27][23516-0-0-5.03][23690-1-1-0.61][23921-2-2-2.02][23936-1-2-2.33][24040-3-4-0.96]
[24111-1-1-1.18][24182-0-0-5.54][24238-3-3-2.83][24290-2-0-2.16][24345-0-0-3.09][24364-1-2-0.94][24427-3-0-2.69][24477-2-2-3.77][24495-2-4-0.99][24893-2-2-1.48]
[25012-1-1-0.75][25121-2-2-2.17][25165-3-3-1.32][25183-0-0-1.70][25297-3-3-4.16][25398-0-0-3.37][25574-2-2-2.89][25644-1-1-1.68][25718-1-4-0.16][25774-2-3-0.60]
[26032-3-3-3.11][26051-3-3-4.25][26120-0-4-1.13][26321-1-4-0.25][26732-1-1-2.65][26784-3-3-4.71][26827-3-3-1.99][26833-0-3-2.16][26838-2-3-0.52][26860-1-2-1.38]
[26948-0-0-2.34][27049-3-0-3.81][27098-1-0-1.86][27526-0-0-2.07][27639-3-3-1.38][27698-3-3-2.66][27772-0-0-5.34][27890-1-1-4.74][28040-0-0-2.36][28503-2-2-4.49]
[28577-1-1-3.39][28959-0-0-4.37][29198-3-3-1.89][29777-0-0-6.49][29877-2-3-1.05][30035-1-1-2.21][30098-0-3-2.41][30326-1-1-4.54][30572-2-2-3.11][30716-0-4-1.93]
[30806-2-3-2.42][30906-1-1-3.73][31007-0-0-1.84][31181-3-3-2.73][31238-0-0-1.17][31347-0-0-4.51][31422-2-4-0.57][31429-3-3-0.71][31431-0-0-2.13][31432-1-1-2.26]
[31477-0-0-3.61][31524-1-2-1.59][31597-1-2-0.68][31619-1-4-0.47][31701-0-0-2.52][31755-0-0-2.15][31854-3-3-2.98][32074-1-1-0.68][32078-3-3-2.71][32111-1-1-2.73]
[32127-1-1-2.37][32140-3-3-4.65][32263-2-3--0.02][32365-0-0-5.44][32411-2-3-4.41][32429-3-0-1.55][32473-3-3-1.57][32574-3-0-1.79][32584-0-0-1.52][32622-0-4-0.42]
[32858-3-3-1.64][32969-3-3-2.66][33016-2-2-4.88][33031-1-3-2.93][33035-2-2-1.86][33133-2-2-2.53][33173-2-1-1.15][33175-3-4-1.81][33306-3-3-1.16][33309-2-3-1.13]
[33474-0-0-0.42][33478-2-4-0.16][33618-1-1-1.59][33712-0-3-2.14][33782-2-4-2.27][33914-3-3-3.80][34076-3-4-1.59][34112-2-2-4.28][34138-2-2-0.56][34239-1-1-1.38]
[34364-2-2-4.03][34617-1-2-2.35][34751-3-3-1.75][34783-2-4-1.75][35015-3-3-1.35][35018-1-1-2.38][35288-2-1-0.74][0-4-4-1.43][1-4-4-2.68][2-4-4-1.78]
[3-4-4-2.04][4-4-0-0.37][5-4-1--0.20][6-4-4-1.70][7-4-4-1.82][8-4-4-0.69][9-4-4-0.67][10-4-4-4.63][11-4-4-3.86][12-4-2-0.55]
[14-4-4-0.99][15-4-3-2.95][16-4-4-1.60][17-4-4-0.74][18-4-4-2.95][19-4-3-1.71][20-4-0-0.55][21-4-4-0.64][22-4-4-1.74][23-4-4-0.39]
[24-4-4-5.60][25-4-4-0.75][26-4-3-0.39][27-4-4-1.21][28-4-4-1.92][29-4-1-3.02][30-4-3-0.05][31-4-4-1.77][32-4-4-0.85][33-4-2-1.75]
[34-4-4-1.93][35-4-0-1.67][37-4-4-1.18][39-4-0-2.44][40-4-0-0.49][41-4-1--0.25][42-4-4-0.50][43-4-4-1.04][45-4-4-3.78][46-4-4-4.20]
[47-4-4-3.30][48-4-4-0.98][51-4-4-2.08][52-4-4-1.09][53-4-4-0.52][54-4-3-2.29][55-4-4-2.22][56-4-1-1.70][57-4-0-2.08][58-4-2-2.26]
[59-4-0-3.51][60-4-0-0.24][61-4-4-1.91][62-4-4-1.87][63-4-2-3.46][64-4-2-0.44][65-4-4-3.72][66-4-4-1.30][67-4-2-1.69][68-4-4-1.06]
[69-4-2-0.38][70-4-4-2.21][72-4-1-1.77][73-4-1-2.01][74-4-2-1.63][75-4-3-0.56][77-4-4-2.02][78-4-3-0.48][79-4-4-2.40][80-4-1-0.83]
[81-4-4-3.17][82-4-1-2.08][83-4-4-0.74][84-4-4-2.59][85-4-4-3.01][86-4-4-1.18][87-4-4-1.68][88-4-4-0.66][89-4-3-0.48][90-4-4-1.18]
[91-4-4-0.50][92-4-4-0.80][93-4-0-2.56][94-4-4-2.42][95-4-4-1.25][96-4-4-1.31][97-4-4-3.51][98-4-1-1.05][99-4-4-1.49][100-4-1-2.25]
[101-4-4-4.98][102-4-4-1.75][103-4-4-0.15][104-4-1-2.25][105-4-1-1.42][106-4-4-2.01][107-4-4-2.01][108-4-4-1.23][109-4-4-0.89][110-4-4-1.51]
[111-4-0-3.63][112-4-4-0.40][113-4-3-1.38][114-4-2-0.74][115-4-0-1.30][116-4-0-0.89][117-4-4-2.13][119-4-4-3.74][121-4-4-2.61][122-4-4-1.99]
[124-4-4-1.02][125-4-4-3.04][126-4-4-4.05][127-4-2-0.63][128-4-4-0.52][129-4-4-1.36][130-4-2-2.46][131-4-2-2.00][132-4-4-1.74][133-4-4-3.61]
[135-4-4-2.67][136-4-2-0.66][137-4-4-0.64][138-4-4-1.63][139-4-4-1.02][140-4-4-1.35][141-4-0-1.07][142-4-4-2.37][143-4-4-1.67][144-4-4-3.02]
[145-4-4-2.30][148-4-0-4.76][149-4-4-2.59][150-4-4-4.10][151-4-4-2.05][152-4-4-1.77][153-4-2-1.51][154-4-4-3.08][155-4-4-2.65][156-4-4-0.44]
[157-4-2-0.69][158-4-4-1.49][160-4-1-0.32][161-4-2-1.94][162-4-4-1.36][164-4-4-2.19][165-4-4-1.34][167-4-0-3.06][168-4-4-1.49][170-4-4-1.73]
[171-4-4-3.40][172-4-4-2.42][173-4-4-2.87][174-4-0-3.27][175-4-4-2.33][177-4-0-2.67][178-4-2-1.24][179-4-4-1.68][180-4-4-1.49][181-4-3-2.58]
[182-4-3-1.03][183-4-4-1.83][184-4-4-2.36][186-4-4-0.83][187-4-2-0.90][188-4-4-2.26][189-4-4-1.86][190-4-4-0.46][191-4-4-1.60][192-4-4-1.34]
[193-4-2-2.55][194-4-4-0.21][195-4-0-1.89][196-4-3-0.92][197-4-4-2.71][198-4-4-6.21][199-4-2-1.74]
---------------------------
I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.265 | Acc: 85.250% | Wgt Acc: 93.480%
	I - Batch: 100 | Loss: 0.253 | Acc: 85.875% | Wgt Acc: 93.810%
	I - Batch: 150 | Loss: 0.256 | Acc: 85.750% | Wgt Acc: 94.036%
	I - Batch: 200 | Loss: 0.259 | Acc: 85.188% | Wgt Acc: 93.808%
I - num batch: 222
I - Train -- Loss: 0.258 | Acc: 85.283% | Wgt Acc: 93.772% | LR: 1.250000e-04 | Dur: 138.08s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   0.   1.  10. 139.]
 [  1. 567.   3.   0.  70.]
 [  9.   1. 721.   6. 148.]
 [  7.   0.   3. 519.  87.]
 [ 18.  10.   6.   3. 556.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.977 | Acc: 63.511% | Wgt Acc: 62.911% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   1.  12.  15.]
 [  0.  39.   5.   0.  13.]
 [  1.  17.  48.   5.  24.]
 [ 11.   3.  10.  58.  11.]
 [ 16.  17.  11.  11. 117.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.258 | Acc: 84.250% | Wgt Acc: 92.983%
	I - Batch: 100 | Loss: 0.251 | Acc: 84.688% | Wgt Acc: 93.829%
	I - Batch: 150 | Loss: 0.255 | Acc: 85.000% | Wgt Acc: 93.934%
	I - Batch: 200 | Loss: 0.260 | Acc: 84.688% | Wgt Acc: 93.604%
I - num batch: 222
I - Train -- Loss: 0.256 | Acc: 84.945% | Wgt Acc: 93.778% | LR: 1.250000e-04 | Dur: 136.96s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   1.   3.  11. 151.]
 [  3. 571.   2.   1.  78.]
 [  4.   2. 722.   6. 151.]
 [ 11.   1.   1. 513.  80.]
 [ 12.   3.   6.   7. 540.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.031 | Acc: 63.708% | Wgt Acc: 64.387% | Dur: 15.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   2.  14.  17.]
 [  1.  48.  15.   1.  12.]
 [  2.  15.  45.   4.  21.]
 [ 20.   5.   7.  61.  18.]
 [  8.   7.   6.   6. 112.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.257 | Acc: 86.000% | Wgt Acc: 94.014%
	I - Batch: 100 | Loss: 0.246 | Acc: 86.125% | Wgt Acc: 94.129%
	I - Batch: 150 | Loss: 0.246 | Acc: 85.917% | Wgt Acc: 94.201%
	I - Batch: 200 | Loss: 0.250 | Acc: 85.969% | Wgt Acc: 94.131%
I - num batch: 222
I - Train -- Loss: 0.248 | Acc: 85.875% | Wgt Acc: 94.155% | LR: 1.250000e-04 | Dur: 139.69s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   3.   3.   8. 113.]
 [  3. 566.   1.   2.  74.]
 [  3.   6. 723.   1. 139.]
 [ 10.   0.   2. 520. 105.]
 [ 13.   3.   5.   7. 569.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.986 | Acc: 65.286% | Wgt Acc: 62.115% | Dur: 15.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   1.  16.  12.]
 [  0.  39.   4.   0.   6.]
 [  1.  16.  47.   6.  25.]
 [ 12.   3.   7.  55.   5.]
 [ 17.  17.  16.   9. 132.]]

I - Local maximum validation set accuracy:  65.29

I - Validation set results: 
[14-1-2-1.45][50-3-4-1.27][124-2-4-1.03][127-0-0-4.02][443-2-2-2.54][567-0-0-1.41][573-1-1-3.75][615-0-0-1.26][695-1-2-2.77][722-3-0-1.84]
[826-0-0-4.31][878-0-0-2.45][1103-0-4-1.24][1212-3-4-1.66][1368-0-0-3.49][2181-2-2-0.69][2476-2-2-2.13][2721-2-2-2.34][2818-1-1-0.88][2886-2-4-2.03]
[3231-2-2-6.09][3333-2-3-0.79][3482-2-2-2.06][3536-3-3-0.90][3625-1-1-3.44][3909-0-0-1.31][4035-0-0-1.23][4140-0-0-1.59][4214-1-1-0.54][4346-1-4-1.34]
[4581-2-1-1.56][4708-3-4-1.12][4838-3-3-0.42][4845-1-2-0.89][4868-0-0-3.76][4939-0-4-0.24][4984-2-2-3.75][5078-1-4-1.36][5396-0-0-3.95][5479-1-1-3.63]
[5717-0-0-2.89][5843-1-1-3.04][5949-3-3-1.21][5987-2-4-3.05][6014-3-3-1.96][6033-3-2-0.19][6313-0-0-2.72][6421-3-3-2.74][6500-1-4-0.89][6583-3-3-1.80]
[6683-3-3-0.80][6825-2-1-3.44][6998-3-3-0.32][7049-3-3-2.16][7517-1-1-1.59][7521-1-1-2.62][7528-1-3-1.02][7949-1-2-2.25][8135-1-0-1.89][8185-3-0-4.43]
[8269-3-2-0.70][8273-3-3-3.25][8543-3-0-2.68][8666-1-1-4.77][8672-0-0-3.57][8903-1-1-3.47][9001-2-2-1.28][9036-2-2-4.01][9281-3-3-1.35][9300-2-2-6.87]
[9571-0-4-1.11][9617-1-4-2.28][9644-2-2-3.49][9705-2-4-0.99][9801-0-3-1.63][9803-3-3-1.27][9865-3-3-4.06][9896-2-2-1.43][10314-1-4-0.29][10337-3-3-3.77]
[10403-0-4-1.55][10653-2-4-1.24][10704-2-1-1.11][10719-1-1-3.32][10727-1-4-1.62][10836-0-0-8.00][10969-2-3-0.69][11042-0-0-2.15][11088-1-1-5.76][11322-0-0-3.79]
[11398-2-2-6.07][11499-0-0-2.21][11502-3-3-1.56][11512-3-3-1.86][11608-1-2-2.37][11610-0-0-1.75][11692-0-0-1.24][11905-0-0-3.88][11993-1-1-1.90][12002-2-3-2.41]
[12052-0-0-2.30][12201-0-3-2.77][12235-2-2-3.67][12320-1-4-3.03][12377-2-4-2.31][12398-2-3-1.12][12503-1-4-0.45][12617-0-3-1.02][12685-3-3-0.52][12738-2-4-0.63]
[12742-2-2-5.84][12823-0-3-1.26][13110-1-2-0.76][13240-3-3-2.03][13253-1-4-2.04][13273-0-0-4.95][13634-1-4-0.77][13763-2-2-4.69][13905-3-0--0.34][14060-2-4-1.19]
[14065-3-0-0.68][14147-3-3-1.39][14595-2-2-2.10][14687-2-2-4.49][14788-2-2-3.01][14869-1-1-2.70][14872-3-4-0.88][14877-1-1-3.95][14927-0-3-1.95][15066-0-0-4.51]
[15175-1-4-1.07][15178-2-4-0.29][15375-3-0-1.49][15389-3-3-2.13][15568-2-4-1.95][15675-3-3-3.97][15869-1-0-0.44][16207-3-0-1.62][16236-0-2-1.55][16302-3-2-0.57]
[16331-2-2-7.83][16381-0-3-1.20][16488-1-1-2.27][16495-0-0-4.95][16650-0-0-3.59][16719-1-2-0.57][16801-0-0-4.37][16828-0-0-2.72][17137-3-0-0.26][17245-1-2-1.36]
[17278-3-0-0.09][17282-0-0-0.46][17311-2-2-2.84][17336-2-1-2.46][17608-3-3-5.85][17627-0-0-1.52][17877-3-0-1.52][17924-1-2-1.89][17984-3-3-2.03][18211-0-3-2.08]
[18276-3-0-0.44][18287-1-4-0.43][18394-0-0-3.18][18428-0-0-5.73][18442-0-3-1.90][18478-3-3-1.65][18607-0-0-1.22][18616-0-0-1.48][18663-0-0-3.38][18718-0-0-3.30]
[18766-2-2-2.79][18824-2-4-2.02][18890-3-3-1.99][18930-3-4-1.67][18938-3-3-1.03][19817-1-2-1.21][19839-0-4-0.34][19930-3-3-1.78][19944-0-4-1.58][20036-2-2-6.57]
[20101-3-3-1.75][20474-1-1-2.08][20547-3-4-0.80][20929-2-2-3.72][21245-1-2-0.73][21257-3-2-0.25][21293-1-1-2.91][21316-1-1-6.18][21384-1-4-2.38][21448-1-1-2.35]
[21483-0-0-2.41][21487-2-2-3.97][21714-0-0-0.76][21943-3-3-0.69][21947-0-0-1.97][21948-0-0-6.49][21965-2-2-4.45][21998-1-1-5.08][22025-0-4-2.09][22228-3-3-3.30]
[22446-1-1-5.71][22494-3-3-1.37][22757-0-0-4.33][22811-3-3-2.91][22976-3-2-0.73][22985-3-3-1.98][23014-0-3-3.29][23112-1-1-3.55][23144-3-3-4.72][23168-2-3-0.28]
[23219-0-4-0.96][23363-3-3-3.50][23470-0-0-1.75][23486-2-2-2.21][23497-0-3-3.99][23516-0-0-3.64][23690-1-2-0.73][23921-2-2-2.55][23936-1-2-3.62][24040-3-4-0.65]
[24111-1-4-2.29][24182-0-0-4.14][24238-3-3-2.69][24290-2-0-1.59][24345-0-4-0.57][24364-1-3-0.50][24427-3-0-2.62][24477-2-2-3.20][24495-2-4-1.00][24893-2-2-2.74]
[25012-1-4-0.65][25121-2-2-2.65][25165-3-3-0.79][25183-0-0-2.53][25297-3-3-3.80][25398-0-0-2.57][25574-2-2-3.46][25644-1-1-1.58][25718-1-1-1.21][25774-2-4-0.39]
[26032-3-3-2.90][26051-3-3-4.22][26120-0-4-3.62][26321-1-1-4.32][26732-1-1-3.19][26784-3-3-4.85][26827-3-3-0.94][26833-0-3-3.33][26838-2-2-0.37][26860-1-2-0.82]
[26948-0-0-1.46][27049-3-0-2.40][27098-1-0-2.13][27526-0-0-2.85][27639-3-3-1.76][27698-3-3-1.83][27772-0-0-4.60][27890-1-1-1.86][28040-0-4-2.71][28503-2-2-3.86]
[28577-1-1-2.34][28959-0-0-3.20][29198-3-3-1.70][29777-0-0-5.06][29877-2-2-1.75][30035-1-1-2.31][30098-0-0-0.72][30326-1-1-3.77][30572-2-2-2.50][30716-0-4-1.59]
[30806-2-3-1.82][30906-1-1-2.15][31007-0-4-1.49][31181-3-3-0.75][31238-0-0-1.11][31347-0-0-3.67][31422-2-2-0.91][31429-3-3-0.58][31431-0-0-0.14][31432-1-1-3.84]
[31477-0-3-2.85][31524-1-1-1.54][31597-1-2-0.68][31619-1-4-1.00][31701-0-0-1.79][31755-0-0-2.14][31854-3-3-2.04][32074-1-1-1.48][32078-3-3-3.11][32111-1-1-2.17]
[32127-1-1-2.26][32140-3-3-3.32][32263-2-2-0.43][32365-0-0-3.17][32411-2-3-3.27][32429-3-0-0.36][32473-3-3-1.09][32574-3-0-1.35][32584-0-4-1.74][32622-0-4-0.81]
[32858-3-3-1.77][32969-3-0-2.16][33016-2-2-3.60][33031-1-3-2.64][33035-2-2-2.07][33133-2-2-1.70][33173-2-2-0.72][33175-3-4-2.59][33306-3-3-0.47][33309-2-2-0.96]
[33474-0-4-0.26][33478-2-4-0.46][33618-1-1-1.56][33712-0-4-0.69][33782-2-4-2.78][33914-3-3-3.53][34076-3-4-1.77][34112-2-2-3.72][34138-2-2-0.48][34239-1-1-1.78]
[34364-2-2-4.56][34617-1-2-2.05][34751-3-3-1.36][34783-2-4-2.65][35015-3-2-2.14][35018-1-4-1.26][35288-2-2-0.48][0-4-2-1.98][1-4-4-3.14][2-4-4-2.24]
[3-4-4-2.18][4-4-4-0.64][5-4-4--0.21][6-4-4-1.57][7-4-4-2.59][8-4-2-1.14][9-4-1-1.51][10-4-4-4.44][11-4-4-3.29][12-4-4-0.57]
[14-4-4-1.72][15-4-3-2.34][16-4-4-1.84][17-4-4-1.39][18-4-4-2.57][19-4-4-1.70][20-4-0-1.21][21-4-4-1.37][22-4-4-1.90][23-4-4-0.53]
[24-4-4-5.42][25-4-4-1.74][26-4-4-0.67][27-4-4-2.22][28-4-4-2.61][29-4-2-0.75][30-4-0-0.65][31-4-4-2.01][32-4-4-1.46][33-4-2-1.18]
[34-4-4-2.26][35-4-0-1.30][37-4-4-2.06][39-4-0-3.66][40-4-4-1.38][41-4-4-0.12][42-4-4-1.27][43-4-4-1.24][45-4-2-0.71][46-4-4-3.57]
[47-4-4-3.61][48-4-4-3.51][51-4-4-2.83][52-4-4-1.66][53-4-4-0.58][54-4-3-1.94][55-4-4-1.84][56-4-4-0.97][57-4-0-1.77][58-4-2-2.96]
[59-4-0-2.17][60-4-0-0.92][61-4-4-2.21][62-4-2-2.35][63-4-2-2.52][64-4-2-1.25][65-4-4-4.45][66-4-4-2.38][67-4-2-1.62][68-4-4-1.98]
[69-4-4--0.11][70-4-4-1.42][72-4-4-2.08][73-4-1-2.63][74-4-2-2.47][75-4-3--0.01][77-4-4-3.92][78-4-3-0.76][79-4-4-3.30][80-4-4-1.57]
[81-4-4-3.79][82-4-4-0.96][83-4-1-1.39][84-4-4-1.53][85-4-4-3.51][86-4-4-1.34][87-4-4-1.83][88-4-4-1.49][89-4-4-0.13][90-4-4-0.71]
[91-4-4-1.04][92-4-4-1.18][93-4-0-0.28][94-4-4-2.12][95-4-4-1.65][96-4-4-1.66][97-4-4-3.59][98-4-1-1.31][99-4-4-2.40][100-4-4-2.50]
[101-4-4-5.72][102-4-4-1.46][103-4-0-0.53][104-4-4-2.02][105-4-4-3.25][106-4-4-2.50][107-4-4-2.69][108-4-4-1.57][109-4-4-1.18][110-4-4-1.68]
[111-4-0-3.85][112-4-2-0.69][113-4-4-1.37][114-4-2-1.05][115-4-1-1.46][116-4-4-1.39][117-4-4-2.54][119-4-4-3.83][121-4-4-2.49][122-4-4-2.79]
[124-4-4-0.89][125-4-4-2.74][126-4-4-4.29][127-4-2-0.88][128-4-4-0.46][129-4-4-1.38][130-4-4-0.82][131-4-2-2.21][132-4-4-1.07][133-4-4-4.47]
[135-4-2-2.78][136-4-2-0.23][137-4-4-1.33][138-4-4-1.57][139-4-4-1.90][140-4-4-1.35][141-4-2-1.49][142-4-4-2.39][143-4-4-1.83][144-4-4-2.81]
[145-4-2-3.33][148-4-0-3.43][149-4-4-2.84][150-4-4-3.41][151-4-4-2.59][152-4-4-1.35][153-4-2-3.48][154-4-4-3.88][155-4-4-3.39][156-4-3-1.17]
[157-4-2-1.84][158-4-4-2.20][160-4-1-0.43][161-4-2-1.62][162-4-4-0.50][164-4-4-2.17][165-4-4-1.73][167-4-4-1.78][168-4-4-1.44][170-4-4-2.21]
[171-4-4-3.29][172-4-4-3.48][173-4-4-3.23][174-4-0-2.47][175-4-4-3.08][177-4-4-3.07][178-4-4-1.12][179-4-4-1.59][180-4-4-2.65][181-4-4-1.60]
[182-4-4-1.04][183-4-4-2.23][184-4-4-2.22][186-4-4-0.68][187-4-2-1.07][188-4-4-2.21][189-4-4-1.83][190-4-4-0.45][191-4-4-3.46][192-4-4-1.41]
[193-4-2-2.03][194-4-4-0.66][195-4-4-0.95][196-4-4-0.68][197-4-4-2.22][198-4-4-5.68][199-4-2-1.46]
---------------------------
I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.231 | Acc: 87.375% | Wgt Acc: 95.057%
	I - Batch: 100 | Loss: 0.231 | Acc: 86.938% | Wgt Acc: 94.962%
	I - Batch: 150 | Loss: 0.230 | Acc: 86.792% | Wgt Acc: 94.930%
	I - Batch: 200 | Loss: 0.233 | Acc: 86.531% | Wgt Acc: 94.710%
I - num batch: 222
I - Train -- Loss: 0.236 | Acc: 86.129% | Wgt Acc: 94.506% | LR: 1.250000e-04 | Dur: 136.19s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   3.   0.   8. 130.]
 [  0. 570.   2.   0.  63.]
 [  5.   1. 725.   3. 152.]
 [  9.   0.   4. 520.  87.]
 [ 11.   4.   3.   7. 568.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.051 | Acc: 64.497% | Wgt Acc: 62.588% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   1.   1.  10.  12.]
 [  0.  34.   3.   1.   7.]
 [  2.  24.  51.   5.  27.]
 [ 16.   4.   6.  61.   9.]
 [ 14.  15.  14.   9. 125.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.222 | Acc: 88.000% | Wgt Acc: 95.420%
	I - Batch: 100 | Loss: 0.229 | Acc: 86.438% | Wgt Acc: 94.576%
	I - Batch: 150 | Loss: 0.242 | Acc: 85.833% | Wgt Acc: 94.136%
	I - Batch: 200 | Loss: 0.241 | Acc: 85.688% | Wgt Acc: 94.101%
I - num batch: 222
I - Train -- Loss: 0.242 | Acc: 85.593% | Wgt Acc: 93.931% | LR: 1.250000e-04 | Dur: 132.52s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   1.   1.  14. 148.]
 [  5. 572.   1.   0.  58.]
 [  2.   2. 724.   2. 148.]
 [ 11.   0.   2. 514.  82.]
 [ 17.   3.   6.   8. 564.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.987 | Acc: 65.089% | Wgt Acc: 63.511% | Dur: 13.89s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   3.   1.   8.  13.]
 [  1.  37.   5.   0.  11.]
 [  1.  19.  47.   2.  21.]
 [ 19.   5.   9.  65.  10.]
 [ 11.  14.  13.  11. 125.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.221 | Acc: 87.500% | Wgt Acc: 94.684%
	I - Batch: 100 | Loss: 0.228 | Acc: 87.000% | Wgt Acc: 94.386%
	I - Batch: 150 | Loss: 0.230 | Acc: 86.625% | Wgt Acc: 94.111%
	I - Batch: 200 | Loss: 0.223 | Acc: 86.656% | Wgt Acc: 94.339%
I - num batch: 222
I - Train -- Loss: 0.227 | Acc: 86.383% | Wgt Acc: 94.069% | LR: 1.250000e-04 | Dur: 135.43s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   0.   3.   9. 117.]
 [  1. 572.   2.   2.  55.]
 [  7.   2. 717.   4. 151.]
 [ 11.   1.   2. 516.  82.]
 [ 14.   3.  10.   7. 595.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.059 | Acc: 61.736% | Wgt Acc: 63.833% | Dur: 14.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.   3.   4.  23.  33.]
 [  0.  42.   4.   0.   9.]
 [  1.  14.  41.   2.  20.]
 [  8.   8.  14.  56.  17.]
 [  6.  11.  12.   5. 101.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.215 | Acc: 87.250% | Wgt Acc: 95.644%
	I - Batch: 100 | Loss: 0.221 | Acc: 87.250% | Wgt Acc: 95.280%
	I - Batch: 150 | Loss: 0.222 | Acc: 86.667% | Wgt Acc: 95.079%
	I - Batch: 200 | Loss: 0.227 | Acc: 86.312% | Wgt Acc: 94.594%
I - num batch: 222
I - Train -- Loss: 0.230 | Acc: 86.214% | Wgt Acc: 94.508% | LR: 1.250000e-04 | Dur: 139.78s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   3.   4.   9. 144.]
 [  3. 570.   0.   3.  80.]
 [  6.   2. 723.   0. 131.]
 [  4.   1.   1. 522.  73.]
 [ 13.   2.   6.   4. 572.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.027 | Acc: 65.878% | Wgt Acc: 66.059% | Dur: 15.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   5.   3.  11.  15.]
 [  0.  42.   5.   1.   9.]
 [  1.  19.  45.   2.  22.]
 [ 14.   5.  11.  64.  16.]
 [  8.   7.  11.   8. 118.]]

I - Local maximum validation set accuracy:  65.88

I - Validation set results: 
[14-1-2-1.56][50-3-4-0.65][124-2-3-0.91][127-0-0-5.03][443-2-2-2.77][567-0-0-3.13][573-1-1-3.45][615-0-0-1.67][695-1-2-2.74][722-3-0-2.98]
[826-0-0-4.02][878-0-0-2.86][1103-0-0-2.41][1212-3-3-2.06][1368-0-0-3.70][2181-2-3-0.65][2476-2-2-1.62][2721-2-2-3.61][2818-1-3-0.21][2886-2-4-1.15]
[3231-2-2-7.62][3333-2-2-1.17][3482-2-2-2.83][3536-3-3-0.93][3625-1-1-1.22][3909-0-0-1.96][4035-0-0-1.24][4140-0-0-2.46][4214-1-3-1.34][4346-1-3-0.44]
[4581-2-2-3.00][4708-3-4-0.33][4838-3-3-1.01][4845-1-2-0.29][4868-0-0-4.84][4939-0-4-0.41][4984-2-2-4.04][5078-1-3-1.46][5396-0-0-4.58][5479-1-1-4.48]
[5717-0-0-3.95][5843-1-1-1.19][5949-3-3-2.29][5987-2-4-2.28][6014-3-3-1.41][6033-3-3-0.45][6313-0-0-2.33][6421-3-3-3.48][6500-1-1-0.27][6583-3-3-2.22]
[6683-3-3-1.55][6825-2-1-0.74][6998-3-3-0.53][7049-3-3-1.93][7517-1-1-2.32][7521-1-1--0.03][7528-1-1-1.20][7949-1-2-2.11][8135-1-0-1.32][8185-3-0-2.34]
[8269-3-1-1.11][8273-3-3-3.61][8543-3-0-4.17][8666-1-1-4.70][8672-0-0-2.57][8903-1-2-2.58][9001-2-2-1.93][9036-2-2-3.76][9281-3-3-1.28][9300-2-2-7.69]
[9571-0-3-0.86][9617-1-4-0.92][9644-2-2-3.76][9705-2-4--0.05][9801-0-3-2.34][9803-3-3-1.92][9865-3-3-4.66][9896-2-1-0.50][10314-1-2-0.91][10337-3-3-6.21]
[10403-0-4-0.75][10653-2-4-0.78][10704-2-2-0.42][10719-1-1-4.30][10727-1-1-0.95][10836-0-0-9.49][10969-2-3-1.44][11042-0-0-2.35][11088-1-1-6.19][11322-0-0-4.96]
[11398-2-2-2.89][11499-0-0-2.09][11502-3-3-1.81][11512-3-3-2.56][11608-1-2-2.44][11610-0-0-1.22][11692-0-3-2.01][11905-0-0-3.75][11993-1-1-2.53][12002-2-3-1.69]
[12052-0-0-3.13][12201-0-3-3.98][12235-2-2-1.13][12320-1-4-1.75][12377-2-4-1.37][12398-2-3-1.05][12503-1-0-0.56][12617-0-3-1.23][12685-3-4-0.60][12738-2-0-0.88]
[12742-2-2-5.71][12823-0-0-3.51][13110-1-1-2.40][13240-3-3-3.02][13253-1-1-2.24][13273-0-0-5.62][13634-1-1-0.93][13763-2-2-2.60][13905-3-3-1.21][14060-2-4-0.97]
[14065-3-0-1.77][14147-3-3-2.27][14595-2-2-2.27][14687-2-2-4.22][14788-2-2-2.43][14869-1-1-3.46][14872-3-3-1.70][14877-1-1-3.97][14927-0-3-3.41][15066-0-0-4.96]
[15175-1-1-2.50][15178-2-3-2.10][15375-3-0-0.57][15389-3-3-3.91][15568-2-1-1.74][15675-3-3-4.19][15869-1-0-0.66][16207-3-0-1.07][16236-0-2-0.41][16302-3-2-1.35]
[16331-2-2-9.46][16381-0-3-1.43][16488-1-1-6.97][16495-0-0-3.09][16650-0-0-4.49][16719-1-2-1.14][16801-0-0-5.37][16828-0-0-3.16][17137-3-3-1.38][17245-1-4-1.08]
[17278-3-4-0.40][17282-0-0-1.43][17311-2-2-3.86][17336-2-1-1.41][17608-3-3-5.60][17627-0-4--0.19][17877-3-4-2.07][17924-1-2-1.54][17984-3-0-3.37][18211-0-3-2.32]
[18276-3-0-1.55][18287-1-1-0.53][18394-0-0-3.94][18428-0-0-3.83][18442-0-3-3.08][18478-3-3-3.16][18607-0-0-2.88][18616-0-0-2.16][18663-0-0-4.03][18718-0-0-4.27]
[18766-2-2-3.81][18824-2-2-2.16][18890-3-3-3.11][18930-3-4-1.03][18938-3-3--0.39][19817-1-1-1.84][19839-0-0-0.46][19930-3-3-2.67][19944-0-4-2.11][20036-2-2-5.36]
[20101-3-3-2.96][20474-1-2-2.10][20547-3-0-1.18][20929-2-2-4.13][21245-1-2-2.21][21257-3-3-0.54][21293-1-2-2.25][21316-1-1-5.73][21384-1-1-3.62][21448-1-1-4.48]
[21483-0-0-3.11][21487-2-2-2.34][21714-0-0-0.78][21943-3-3-1.34][21947-0-0-1.75][21948-0-0-7.26][21965-2-2-4.62][21998-1-1-3.37][22025-0-4-2.26][22228-3-3-5.54]
[22446-1-1-4.46][22494-3-3-3.11][22757-0-0-4.98][22811-3-3-4.39][22976-3-2-0.73][22985-3-3-3.23][23014-0-3-4.08][23112-1-1-2.83][23144-3-3-5.60][23168-2-0-0.46]
[23219-0-0-1.74][23363-3-3-2.47][23470-0-0-2.08][23486-2-4-0.96][23497-0-3-5.49][23516-0-0-4.08][23690-1-4-0.68][23921-2-2-1.99][23936-1-2-3.83][24040-3-3-0.60]
[24111-1-4-2.01][24182-0-0-4.63][24238-3-3-3.52][24290-2-0-2.46][24345-0-0-2.34][24364-1-2-1.93][24427-3-3-0.91][24477-2-2-4.49][24495-2-4-0.89][24893-2-2-4.19]
[25012-1-4--0.23][25121-2-2-2.76][25165-3-3-2.08][25183-0-0-2.79][25297-3-3-3.71][25398-0-0-3.29][25574-2-2-3.37][25644-1-2-3.81][25718-1-0--0.28][25774-2-4-0.38]
[26032-3-3-3.15][26051-3-3-4.12][26120-0-4-1.00][26321-1-1-0.90][26732-1-1-2.42][26784-3-3-6.24][26827-3-3-1.89][26833-0-3-3.09][26838-2-2-1.12][26860-1-2-1.10]
[26948-0-0-2.17][27049-3-0-2.69][27098-1-2--0.16][27526-0-0-2.79][27639-3-3-1.01][27698-3-3-1.60][27772-0-0-3.94][27890-1-1-1.52][28040-0-0-1.38][28503-2-2-5.19]
[28577-1-1-3.10][28959-0-0-4.14][29198-3-3-1.40][29777-0-0-5.60][29877-2-2-1.16][30035-1-1-2.48][30098-0-0-1.31][30326-1-1-4.54][30572-2-2-3.32][30716-0-4-1.83]
[30806-2-3-2.85][30906-1-1-5.10][31007-0-0-3.00][31181-3-3-2.66][31238-0-0-2.18][31347-0-0-2.71][31422-2-2-2.07][31429-3-3-1.85][31431-0-0-0.60][31432-1-1-3.72]
[31477-0-3-2.96][31524-1-0-0.76][31597-1-1-1.02][31619-1-4-0.99][31701-0-0-2.11][31755-0-0-2.80][31854-3-3-1.69][32074-1-1-1.64][32078-3-3-5.38][32111-1-1-1.79]
[32127-1-2-1.90][32140-3-3-3.55][32263-2-2-1.00][32365-0-0-3.92][32411-2-3-3.69][32429-3-0-2.02][32473-3-3-1.27][32574-3-3-3.27][32584-0-0-2.12][32622-0-4-0.83]
[32858-3-3-3.76][32969-3-3-3.13][33016-2-2-5.67][33031-1-3-2.10][33035-2-2-1.92][33133-2-2-2.89][33173-2-3-0.53][33175-3-4-2.29][33306-3-3-2.46][33309-2-3-1.23]
[33474-0-0-1.77][33478-2-2-0.21][33618-1-1-3.05][33712-0-3-1.71][33782-2-4-1.55][33914-3-3-3.67][34076-3-4-1.26][34112-2-2-4.45][34138-2-3-1.10][34239-1-1-2.07]
[34364-2-2-4.73][34617-1-2-2.30][34751-3-3-1.93][34783-2-4-1.57][35015-3-3-1.64][35018-1-1-2.46][35288-2-1-0.77][0-4-2-1.80][1-4-4-2.68][2-4-4-1.95]
[3-4-4-1.25][4-4-4-1.12][5-4-1-2.23][6-4-4-3.06][7-4-4-2.04][8-4-2-0.45][9-4-1-1.25][10-4-4-3.76][11-4-4-2.99][12-4-2-0.45]
[14-4-4-1.17][15-4-3-3.15][16-4-4-0.91][17-4-0-0.93][18-4-4-4.10][19-4-0-2.14][20-4-3-0.92][21-4-4-1.34][22-4-4-1.60][23-4-4-0.97]
[24-4-4-5.77][25-4-3-1.63][26-4-3-1.26][27-4-4-2.31][28-4-4-3.42][29-4-1-1.52][30-4-3-0.70][31-4-4-1.63][32-4-4-1.43][33-4-3-1.07]
[34-4-4-1.37][35-4-4-0.89][37-4-2-0.79][39-4-0-3.71][40-4-4-0.62][41-4-4--0.19][42-4-3-1.17][43-4-4-0.95][45-4-2-0.14][46-4-4-3.41]
[47-4-4-3.42][48-4-4-2.35][51-4-4-2.33][52-4-4-0.85][53-4-2-0.75][54-4-3-1.64][55-4-4-1.62][56-4-4-0.51][57-4-3-1.55][58-4-2-2.92]
[59-4-0-2.02][60-4-4-1.93][61-4-4-2.14][62-4-3-1.44][63-4-2-3.33][64-4-2-1.59][65-4-4-4.28][66-4-4-1.88][67-4-2-0.81][68-4-4-1.40]
[69-4-3-0.38][70-4-4-1.92][72-4-4-1.18][73-4-1-0.99][74-4-2-3.99][75-4-3-1.59][77-4-4-4.35][78-4-3-0.37][79-4-4-3.00][80-4-4-1.88]
[81-4-4-3.37][82-4-1-1.39][83-4-4-1.42][84-4-4-4.05][85-4-4-3.55][86-4-2-0.52][87-4-4-1.54][88-4-4-0.93][89-4-4-0.36][90-4-4-1.07]
[91-4-4-0.42][92-4-2-0.84][93-4-0-1.43][94-4-4-2.12][95-4-4-0.11][96-4-4-1.45][97-4-4-2.96][98-4-2-1.85][99-4-4-1.79][100-4-1-1.05]
[101-4-4-5.09][102-4-4-1.65][103-4-4-0.27][104-4-4-1.43][105-4-4-3.24][106-4-4-2.90][107-4-0-1.24][108-4-4-1.29][109-4-4-1.59][110-4-4-0.70]
[111-4-0-4.72][112-4-4-0.18][113-4-4-1.16][114-4-3-0.89][115-4-4-0.11][116-4-0-1.24][117-4-4-1.80][119-4-4-2.70][121-4-2-1.71][122-4-4-2.62]
[124-4-4-0.60][125-4-4-2.95][126-4-4-4.19][127-4-2-1.05][128-4-0-0.78][129-4-4-0.85][130-4-4-1.16][131-4-2-2.12][132-4-4-2.02][133-4-4-3.34]
[135-4-4-1.70][136-4-4-0.21][137-4-4-0.50][138-4-4-0.60][139-4-4-1.11][140-4-1-1.13][141-4-3-1.13][142-4-4-2.09][143-4-4-1.38][144-4-4-3.60]
[145-4-4-1.32][148-4-0-3.55][149-4-4-2.37][150-4-4-3.44][151-4-4-3.03][152-4-4-1.40][153-4-4-1.71][154-4-4-3.68][155-4-4-2.84][156-4-3-1.37]
[157-4-2-0.64][158-4-4-2.05][160-4-4-0.36][161-4-2-2.87][162-4-4--0.06][164-4-4-1.56][165-4-4-1.23][167-4-0-3.19][168-4-4-0.92][170-4-4-1.63]
[171-4-4-3.11][172-4-4-2.89][173-4-4-2.95][174-4-0-2.43][175-4-4-2.58][177-4-4-2.39][178-4-4-1.59][179-4-4-1.86][180-4-4-2.40][181-4-4-1.51]
[182-4-4-1.68][183-4-4-2.46][184-4-4-1.71][186-4-0-1.39][187-4-1-0.64][188-4-4-1.83][189-4-4-0.71][190-4-1-0.93][191-4-4-2.52][192-4-4-0.88]
[193-4-2-3.44][194-4-0-1.54][195-4-0-0.55][196-4-2-0.67][197-4-4-1.96][198-4-4-6.17][199-4-2-2.40]
---------------------------
I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.203 | Acc: 87.750% | Wgt Acc: 95.654%
	I - Batch: 100 | Loss: 0.211 | Acc: 87.125% | Wgt Acc: 95.139%
	I - Batch: 150 | Loss: 0.216 | Acc: 87.333% | Wgt Acc: 95.187%
	I - Batch: 200 | Loss: 0.216 | Acc: 87.406% | Wgt Acc: 95.222%
I - num batch: 222
I - Train -- Loss: 0.216 | Acc: 87.341% | Wgt Acc: 95.138% | LR: 1.250000e-04 | Dur: 143.59s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   2.   1.  10. 108.]
 [  1. 571.   1.   3.  61.]
 [  0.   1. 728.   3. 142.]
 [  7.   0.   1. 519.  89.]
 [  9.   4.   3.   3. 600.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.994 | Acc: 65.680% | Wgt Acc: 62.127% | Dur: 15.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   2.  14.  10.]
 [  1.  42.   8.   1.  12.]
 [  0.  17.  39.   4.  13.]
 [ 13.   4.  10.  58.  10.]
 [ 15.  12.  16.   9. 135.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.213 | Acc: 87.750% | Wgt Acc: 94.561%
	I - Batch: 100 | Loss: 0.211 | Acc: 87.688% | Wgt Acc: 94.961%
	I - Batch: 150 | Loss: 0.212 | Acc: 87.375% | Wgt Acc: 94.846%
	I - Batch: 200 | Loss: 0.211 | Acc: 87.625% | Wgt Acc: 94.890%
I - num batch: 222
I - Train -- Loss: 0.209 | Acc: 87.764% | Wgt Acc: 94.963% | LR: 1.250000e-04 | Dur: 134.01s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   1.   4.   6. 123.]
 [  1. 570.   2.   1.  61.]
 [  5.   3. 722.   1. 117.]
 [  6.   2.   0. 524.  73.]
 [ 14.   2.   6.   6. 626.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.072 | Acc: 66.075% | Wgt Acc: 64.906% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   6.   6.  17.  29.]
 [  0.  39.   3.   1.   3.]
 [  0.  16.  43.   1.  12.]
 [ 11.   2.  11.  58.  12.]
 [  6.  15.  12.   9. 124.]]

I - Local maximum validation set accuracy:  66.07

I - Validation set results: 
[14-1-2-0.16][50-3-4-1.23][124-2-2-0.45][127-0-0-6.69][443-2-2-1.93][567-0-0-4.43][573-1-1-3.27][615-0-0-3.73][695-1-2-1.70][722-3-0-5.28]
[826-0-0-5.38][878-0-0-3.50][1103-0-0-2.64][1212-3-3-1.19][1368-0-0-4.64][2181-2-3-0.19][2476-2-2-1.08][2721-2-2-3.28][2818-1-0-1.40][2886-2-4-1.87]
[3231-2-2-6.15][3333-2-2-1.12][3482-2-2-1.42][3536-3-3-2.71][3625-1-1-1.37][3909-0-0-2.67][4035-0-0-2.37][4140-0-0-2.89][4214-1-3-1.90][4346-1-0-0.65]
[4581-2-4-1.43][4708-3-4-0.46][4838-3-3-0.76][4845-1-4--0.03][4868-0-0-5.04][4939-0-4-1.05][4984-2-2-3.13][5078-1-4-1.44][5396-0-0-5.71][5479-1-1-5.18]
[5717-0-0-1.85][5843-1-1-1.90][5949-3-3-1.62][5987-2-4-2.55][6014-3-3-1.63][6033-3-4-1.09][6313-0-0-3.47][6421-3-3-3.12][6500-1-1-0.61][6583-3-3-1.68]
[6683-3-3-2.03][6825-2-1-1.93][6998-3-0-0.64][7049-3-3-2.86][7517-1-1-3.01][7521-1-4--0.41][7528-1-1-0.80][7949-1-4-1.42][8135-1-0-2.55][8185-3-0-4.53]
[8269-3-1-2.89][8273-3-3-3.17][8543-3-0-5.25][8666-1-1-2.27][8672-0-0-7.53][8903-1-2-0.69][9001-2-2-2.00][9036-2-2-5.20][9281-3-3-1.40][9300-2-2-7.27]
[9571-0-3-1.44][9617-1-1-0.85][9644-2-2-2.54][9705-2-0-0.54][9801-0-3-3.04][9803-3-3-2.12][9865-3-3-5.84][9896-2-2-2.88][10314-1-2-0.49][10337-3-3-6.10]
[10403-0-0-1.04][10653-2-1-1.21][10704-2-2--0.29][10719-1-1-2.73][10727-1-4-1.62][10836-0-0-10.87][10969-2-3-1.76][11042-0-0-3.09][11088-1-1-6.09][11322-0-0-4.84]
[11398-2-2-3.34][11499-0-0-3.35][11502-3-3-1.27][11512-3-3-2.45][11608-1-2-1.84][11610-0-0-3.14][11692-0-0-3.31][11905-0-0-3.71][11993-1-1-0.98][12002-2-2-1.36]
[12052-0-0-3.60][12201-0-3-3.42][12235-2-2-1.66][12320-1-4-2.33][12377-2-4-2.41][12398-2-3-1.08][12503-1-0-1.10][12617-0-3-0.91][12685-3-3-0.95][12738-2-0-1.14]
[12742-2-2-6.05][12823-0-0-4.69][13110-1-2-1.64][13240-3-3-2.71][13253-1-4-1.87][13273-0-0-7.23][13634-1-4-0.44][13763-2-2-3.26][13905-3-0-0.34][14060-2-4-1.50]
[14065-3-0-2.49][14147-3-3-2.98][14595-2-2-2.83][14687-2-2-3.98][14788-2-2-1.42][14869-1-1-2.78][14872-3-3-1.74][14877-1-1-4.31][14927-0-3-2.62][15066-0-0-5.24]
[15175-1-1-1.52][15178-2-3-1.80][15375-3-0-0.92][15389-3-3-4.81][15568-2-4-1.53][15675-3-3-2.66][15869-1-0-1.27][16207-3-0-1.90][16236-0-0-2.78][16302-3-3--0.01]
[16331-2-2-8.50][16381-0-3-1.08][16488-1-1-3.42][16495-0-0-6.86][16650-0-0-5.93][16719-1-2-1.09][16801-0-0-5.59][16828-0-0-1.93][17137-3-3-1.43][17245-1-4-1.45]
[17278-3-0-1.66][17282-0-0-1.64][17311-2-2-2.61][17336-2-2-0.86][17608-3-3-5.95][17627-0-0-0.69][17877-3-4-1.34][17924-1-2-1.22][17984-3-0-3.55][18211-0-3-1.66]
[18276-3-3-2.09][18287-1-4-0.31][18394-0-0-4.88][18428-0-0-7.84][18442-0-3-2.95][18478-3-3-2.51][18607-0-0-2.11][18616-0-0-2.58][18663-0-0-4.97][18718-0-0-6.20]
[18766-2-2-2.86][18824-2-4-3.02][18890-3-3-3.62][18930-3-4-1.74][18938-3-3-0.43][19817-1-1-1.14][19839-0-0-0.34][19930-3-3-2.82][19944-0-4-2.11][20036-2-2-6.03]
[20101-3-3-2.22][20474-1-1-0.55][20547-3-3-1.45][20929-2-2-1.59][21245-1-2-1.78][21257-3-3-0.53][21293-1-2-2.57][21316-1-1-5.97][21384-1-4-1.84][21448-1-1-1.98]
[21483-0-0-5.16][21487-2-2-3.99][21714-0-0-3.15][21943-3-3-0.79][21947-0-0-3.03][21948-0-0-6.97][21965-2-2-2.04][21998-1-1-3.68][22025-0-4-1.71][22228-3-3-6.17]
[22446-1-1-4.80][22494-3-3-1.56][22757-0-0-5.75][22811-3-3-4.17][22976-3-2-0.15][22985-3-3-2.60][23014-0-0-4.28][23112-1-1-2.65][23144-3-3-5.88][23168-2-0-0.46]
[23219-0-0-2.36][23363-3-3-2.00][23470-0-0-1.40][23486-2-2-1.22][23497-0-3-4.66][23516-0-0-4.73][23690-1-4--0.02][23921-2-4-0.67][23936-1-2-3.63][24040-3-4-0.68]
[24111-1-4-2.97][24182-0-0-5.94][24238-3-3-3.20][24290-2-0-2.41][24345-0-0-2.07][24364-1-2-1.29][24427-3-0-3.67][24477-2-2-3.62][24495-2-4-0.99][24893-2-2-3.48]
[25012-1-4-0.83][25121-2-2-1.67][25165-3-3-2.57][25183-0-0-3.60][25297-3-3-3.44][25398-0-0-5.38][25574-2-2-2.58][25644-1-2-3.38][25718-1-1-0.07][25774-2-3-0.68]
[26032-3-3-3.49][26051-3-3-3.28][26120-0-4-2.82][26321-1-1-4.87][26732-1-1-3.72][26784-3-3-5.13][26827-3-3-1.97][26833-0-3-2.18][26838-2-3-0.47][26860-1-2-0.82]
[26948-0-0-3.00][27049-3-0-4.76][27098-1-0-3.39][27526-0-0-3.67][27639-3-0-1.41][27698-3-3-3.08][27772-0-0-6.00][27890-1-1-2.97][28040-0-0-1.95][28503-2-2-4.15]
[28577-1-1-1.73][28959-0-0-4.67][29198-3-4-1.82][29777-0-0-8.06][29877-2-3-1.27][30035-1-1-2.38][30098-0-0-3.34][30326-1-1-4.71][30572-2-2-3.38][30716-0-4-2.11]
[30806-2-3-3.33][30906-1-1-2.38][31007-0-0-2.46][31181-3-3-1.41][31238-0-0-2.01][31347-0-0-6.37][31422-2-4-0.48][31429-3-3-0.72][31431-0-0-2.45][31432-1-1-3.49]
[31477-0-0-3.94][31524-1-2-0.97][31597-1-1-0.52][31619-1-2-0.67][31701-0-0-4.57][31755-0-0-4.41][31854-3-3-2.14][32074-1-1-1.47][32078-3-3-5.14][32111-1-1-0.96]
[32127-1-1-1.68][32140-3-3-4.05][32263-2-0-0.48][32365-0-0-6.50][32411-2-3-3.97][32429-3-0-1.68][32473-3-0-1.48][32574-3-0-3.58][32584-0-0-3.23][32622-0-4-1.02]
[32858-3-3-2.68][32969-3-0-3.71][33016-2-2-5.19][33031-1-3-2.05][33035-2-2-2.20][33133-2-2-0.83][33173-2-2-0.66][33175-3-4-2.56][33306-3-3-2.40][33309-2-3-2.58]
[33474-0-0-1.56][33478-2-0--0.29][33618-1-1-2.71][33712-0-3-1.53][33782-2-4-3.09][33914-3-3-3.28][34076-3-4-0.84][34112-2-2-4.27][34138-2-3-1.59][34239-1-1-0.86]
[34364-2-2-4.36][34617-1-4-2.91][34751-3-3-2.29][34783-2-4-2.62][35015-3-3-1.48][35018-1-1-3.03][35288-2-1-0.99][0-4-4-0.47][1-4-4-1.83][2-4-4-2.44]
[3-4-4-0.97][4-4-4-1.91][5-4-3-0.34][6-4-0-3.17][7-4-4-1.02][8-4-4-0.32][9-4-1-0.86][10-4-4-4.49][11-4-4-4.07][12-4-2--0.08]
[14-4-3-1.04][15-4-3-3.77][16-4-4-0.92][17-4-4-0.98][18-4-4-1.49][19-4-0-2.97][20-4-0-2.15][21-4-4-0.75][22-4-4-2.83][23-4-4-0.49]
[24-4-4-5.40][25-4-3-1.62][26-4-3-1.78][27-4-4-2.17][28-4-4-4.39][29-4-4-0.28][30-4-0-0.78][31-4-4-0.98][32-4-4-1.52][33-4-4-0.26]
[34-4-4-1.48][35-4-0-1.46][37-4-4-1.56][39-4-0-5.83][40-4-4-1.07][41-4-4-0.14][42-4-4-1.34][43-4-4-1.40][45-4-4-1.17][46-4-4-3.98]
[47-4-4-4.44][48-4-4-1.66][51-4-4-3.30][52-4-0-2.35][53-4-4-0.82][54-4-3-1.62][55-4-0-0.79][56-4-1-1.54][57-4-3-0.53][58-4-2-2.97]
[59-4-0-2.25][60-4-0-1.47][61-4-4-3.05][62-4-4-1.40][63-4-2-3.39][64-4-4-1.52][65-4-4-3.46][66-4-4-4.55][67-4-3-0.97][68-4-4-1.14]
[69-4-4-0.32][70-4-4-2.93][72-4-4-1.98][73-4-1-2.17][74-4-4-1.86][75-4-0-0.80][77-4-4-2.81][78-4-3-0.56][79-4-4-3.51][80-4-4-2.91]
[81-4-4-1.83][82-4-4-0.84][83-4-4-0.41][84-4-0-2.31][85-4-4-3.52][86-4-4-1.02][87-4-4-2.00][88-4-4-1.42][89-4-0-1.05][90-4-0-0.94]
[91-4-4-1.26][92-4-4-0.64][93-4-0-2.03][94-4-4-3.03][95-4-4-1.02][96-4-4-1.78][97-4-4-4.45][98-4-2-0.80][99-4-4-0.91][100-4-4-2.14]
[101-4-4-4.29][102-4-4-2.04][103-4-0-0.80][104-4-4-2.30][105-4-4-2.14][106-4-4-2.53][107-4-4-2.39][108-4-4-1.45][109-4-4-1.38][110-4-4-2.25]
[111-4-0-5.54][112-4-4-1.02][113-4-2-1.11][114-4-4-0.19][115-4-4-0.89][116-4-0-0.89][117-4-4-1.66][119-4-4-2.15][121-4-4-2.94][122-4-4-2.27]
[124-4-4-1.04][125-4-4-3.27][126-4-4-3.37][127-4-2-0.94][128-4-0-0.90][129-4-4-1.40][130-4-4-1.89][131-4-2-0.93][132-4-0-1.49][133-4-4-2.80]
[135-4-4-2.55][136-4-0-0.18][137-4-4-1.40][138-4-4-0.89][139-4-4-1.31][140-4-4-1.10][141-4-3--0.14][142-4-4-3.18][143-4-4-2.34][144-4-4-3.70]
[145-4-2-3.46][148-4-0-4.53][149-4-4-2.07][150-4-4-4.10][151-4-4-2.39][152-4-4-1.63][153-4-4-1.85][154-4-4-5.49][155-4-4-2.66][156-4-3-1.61]
[157-4-0-1.32][158-4-4-1.77][160-4-4-0.54][161-4-2-1.54][162-4-0-0.44][164-4-4-2.16][165-4-4-1.81][167-4-0-2.35][168-4-4-1.94][170-4-4-1.50]
[171-4-4-3.01][172-4-4-3.42][173-4-4-2.79][174-4-0-4.90][175-4-4-2.72][177-4-4-1.16][178-4-4-1.46][179-4-4-2.80][180-4-4-2.42][181-4-4-1.76]
[182-4-3-1.39][183-4-4-2.34][184-4-4-1.62][186-4-0-1.95][187-4-2-0.40][188-4-4-2.09][189-4-4-1.45][190-4-4-0.68][191-4-4-3.03][192-4-4-1.26]
[193-4-2-1.98][194-4-0-0.24][195-4-0-0.95][196-4-4-1.11][197-4-4-3.28][198-4-4-6.08][199-4-2-2.00]
---------------------------
I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.208 | Acc: 88.125% | Wgt Acc: 95.387%
	I - Batch: 100 | Loss: 0.215 | Acc: 87.062% | Wgt Acc: 94.733%
	I - Batch: 150 | Loss: 0.211 | Acc: 87.250% | Wgt Acc: 94.708%
	I - Batch: 200 | Loss: 0.218 | Acc: 87.000% | Wgt Acc: 94.750%
I - num batch: 222
I - Train -- Loss: 0.220 | Acc: 86.919% | Wgt Acc: 94.612% | LR: 1.250000e-04 | Dur: 139.66s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   1.  10. 133.]
 [  0. 571.   3.   3.  67.]
 [  6.   3. 720.   1. 113.]
 [  5.   1.   1. 520.  87.]
 [ 14.   3.   9.   4. 600.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.029 | Acc: 65.089% | Wgt Acc: 62.115% | Dur: 16.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   0.   1.   6.   8.]
 [  1.  36.   3.   1.   8.]
 [  1.  18.  42.   2.  21.]
 [ 18.   4.  11.  68.  11.]
 [ 16.  20.  18.   9. 132.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.211 | Acc: 86.750% | Wgt Acc: 94.820%
	I - Batch: 100 | Loss: 0.218 | Acc: 85.875% | Wgt Acc: 94.666%
	I - Batch: 150 | Loss: 0.216 | Acc: 85.958% | Wgt Acc: 94.506%
	I - Batch: 200 | Loss: 0.218 | Acc: 86.188% | Wgt Acc: 94.619%
I - num batch: 222
I - Train -- Loss: 0.219 | Acc: 86.270% | Wgt Acc: 94.661% | LR: 1.250000e-04 | Dur: 133.52s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   1.   1.   8. 141.]
 [  3. 570.   1.   0.  73.]
 [  4.   0. 725.   4. 126.]
 [  1.   4.   2. 522.  91.]
 [ 15.   3.   5.   4. 569.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.077 | Acc: 64.892% | Wgt Acc: 67.132% | Dur: 14.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   4.   3.  18.  27.]
 [  0.  46.   4.   1.   8.]
 [  1.  17.  48.   3.  20.]
 [ 12.   1.  14.  58.  19.]
 [  4.  10.   6.   6. 106.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.198 | Acc: 88.000% | Wgt Acc: 94.906%
	I - Batch: 100 | Loss: 0.199 | Acc: 87.875% | Wgt Acc: 95.193%
	I - Batch: 150 | Loss: 0.198 | Acc: 88.167% | Wgt Acc: 95.505%
	I - Batch: 200 | Loss: 0.203 | Acc: 87.719% | Wgt Acc: 95.266%
I - num batch: 222
I - Train -- Loss: 0.206 | Acc: 87.652% | Wgt Acc: 95.207% | LR: 1.250000e-04 | Dur: 133.66s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   2.   2.   7. 123.]
 [  1. 571.   1.   0.  65.]
 [  1.   2. 726.   1. 116.]
 [  7.   0.   1. 523.  84.]
 [ 11.   3.   4.   7. 612.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.042 | Acc: 62.919% | Wgt Acc: 63.626% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   5.   4.  18.  19.]
 [  1.  40.   5.   0.  12.]
 [  2.  20.  44.   0.  24.]
 [ 15.   4.  12.  60.  15.]
 [  5.   9.  10.   8. 110.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.197 | Acc: 88.125% | Wgt Acc: 95.679%
	I - Batch: 100 | Loss: 0.199 | Acc: 87.438% | Wgt Acc: 95.149%
	I - Batch: 150 | Loss: 0.201 | Acc: 87.292% | Wgt Acc: 95.172%
	I - Batch: 200 | Loss: 0.207 | Acc: 87.219% | Wgt Acc: 94.925%
I - num batch: 222
I - Train -- Loss: 0.213 | Acc: 87.031% | Wgt Acc: 94.650% | LR: 1.250000e-04 | Dur: 132.26s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   2.   2.  11. 140.]
 [  0. 572.   1.   4.  56.]
 [  4.   1. 720.   4. 113.]
 [  5.   2.   2. 513.  88.]
 [  9.   1.   9.   6. 603.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.075 | Acc: 62.919% | Wgt Acc: 62.888% | Dur: 13.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 77.   6.   7.  32.  27.]
 [  1.  44.   7.   0.  10.]
 [  1.  14.  42.   4.  20.]
 [  3.   2.   7.  44.  11.]
 [  6.  12.  12.   6. 112.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.220 | Acc: 87.125% | Wgt Acc: 94.739%
	I - Batch: 100 | Loss: 0.218 | Acc: 87.438% | Wgt Acc: 95.105%
	I - Batch: 150 | Loss: 0.209 | Acc: 87.333% | Wgt Acc: 95.045%
	I - Batch: 200 | Loss: 0.209 | Acc: 87.531% | Wgt Acc: 95.223%
I - num batch: 222
I - Train -- Loss: 0.211 | Acc: 87.652% | Wgt Acc: 95.269% | LR: 1.250000e-04 | Dur: 133.46s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   1.   7. 124.]
 [  2. 572.   0.   1.  62.]
 [  0.   3. 726.   2. 114.]
 [  4.   0.   2. 525.  90.]
 [ 15.   3.   5.   3. 610.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.012 | Acc: 63.314% | Wgt Acc: 63.684% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   5.   1.  13.  12.]
 [  1.  35.   4.   1.   8.]
 [  2.  23.  49.   2.  38.]
 [ 12.   4.  11.  60.  10.]
 [  8.  11.  10.  10. 112.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.185 | Acc: 87.625% | Wgt Acc: 95.303%
	I - Batch: 100 | Loss: 0.192 | Acc: 87.688% | Wgt Acc: 95.365%
	I - Batch: 150 | Loss: 0.207 | Acc: 87.042% | Wgt Acc: 94.993%
	I - Batch: 200 | Loss: 0.203 | Acc: 87.344% | Wgt Acc: 95.061%
I - num batch: 222
I - Train -- Loss: 0.205 | Acc: 87.031% | Wgt Acc: 94.941% | LR: 1.250000e-04 | Dur: 138.11s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   2.   1.   3. 127.]
 [  0. 572.   1.   0.  73.]
 [  4.   3. 722.   3. 133.]
 [  6.   0.   1. 527.  73.]
 [ 15.   1.   9.   5. 594.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.053 | Acc: 63.708% | Wgt Acc: 63.211% | Dur: 14.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   5.   2.  16.  22.]
 [  0.  42.   7.   0.   9.]
 [  1.  15.  37.   2.  19.]
 [ 10.   3.  13.  58.  13.]
 [  8.  13.  16.  10. 117.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.168 | Acc: 90.000% | Wgt Acc: 96.475%
	I - Batch: 100 | Loss: 0.173 | Acc: 89.938% | Wgt Acc: 96.322%
	I - Batch: 150 | Loss: 0.174 | Acc: 89.583% | Wgt Acc: 96.292%
	I - Batch: 200 | Loss: 0.179 | Acc: 89.469% | Wgt Acc: 96.056%
I - num batch: 208
I - Train -- Loss: 0.179 | Acc: 89.513% | Wgt Acc: 96.088% | LR: 1.250000e-04 | Dur: 126.49s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   1.   0.   4. 103.]
 [  1. 574.   0.   0.  58.]
 [  3.   1. 728.   2.  89.]
 [  7.   1.   2. 527.  58.]
 [  9.   1.   4.   5. 473.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.067 | Acc: 64.892% | Wgt Acc: 62.807% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   3.  10.  12.]
 [  0.  37.   6.   0.   6.]
 [  0.  16.  41.   2.  20.]
 [ 21.   7.  11.  64.  15.]
 [  7.  14.  14.  10. 127.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.179 | Acc: 88.125% | Wgt Acc: 96.076%
	I - Batch: 100 | Loss: 0.195 | Acc: 87.562% | Wgt Acc: 95.162%
	I - Batch: 150 | Loss: 0.200 | Acc: 87.542% | Wgt Acc: 95.098%
	I - Batch: 200 | Loss: 0.196 | Acc: 88.156% | Wgt Acc: 95.348%
I - num batch: 222
I - Train -- Loss: 0.199 | Acc: 87.933% | Wgt Acc: 95.226% | LR: 1.250000e-04 | Dur: 135.60s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   1.   4.   7. 115.]
 [  0. 574.   0.   2.  63.]
 [  3.   0. 726.   1. 114.]
 [  6.   0.   2. 521.  84.]
 [ 14.   3.   2.   7. 624.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 64.103% | Wgt Acc: 62.277% | Dur: 17.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   7.   4.  13.  21.]
 [  0.  35.   3.   1.   5.]
 [  0.  14.  41.   1.  17.]
 [ 14.   3.  14.  61.  13.]
 [ 10.  19.  13.  10. 124.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.171 | Acc: 89.500% | Wgt Acc: 95.802%
	I - Batch: 100 | Loss: 0.169 | Acc: 89.375% | Wgt Acc: 96.084%
	I - Batch: 150 | Loss: 0.181 | Acc: 89.125% | Wgt Acc: 95.622%
	I - Batch: 200 | Loss: 0.190 | Acc: 88.344% | Wgt Acc: 95.271%
I - num batch: 222
I - Train -- Loss: 0.191 | Acc: 88.272% | Wgt Acc: 95.237% | LR: 1.250000e-04 | Dur: 131.88s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   1.  11. 111.]
 [  0. 571.   2.   0.  55.]
 [  2.   4. 724.   1. 126.]
 [  8.   2.   2. 523.  69.]
 [ 13.   1.   5.   3. 639.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.991 | Acc: 64.300% | Wgt Acc: 65.056% | Dur: 13.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   5.   2.  16.  18.]
 [  1.  49.   8.   1.  24.]
 [  0.  10.  43.   3.  17.]
 [ 13.   3.  12.  55.   9.]
 [  7.  11.  10.  11. 112.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.181 | Acc: 90.625% | Wgt Acc: 96.375%
	I - Batch: 100 | Loss: 0.188 | Acc: 89.438% | Wgt Acc: 95.884%
	I - Batch: 150 | Loss: 0.192 | Acc: 88.500% | Wgt Acc: 95.548%
	I - Batch: 200 | Loss: 0.191 | Acc: 88.594% | Wgt Acc: 95.528%
I - num batch: 222
I - Train -- Loss: 0.193 | Acc: 88.356% | Wgt Acc: 95.464% | LR: 1.250000e-04 | Dur: 135.96s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   0.   2.   4. 130.]
 [  1. 574.   0.   0.  51.]
 [  6.   1. 721.   2. 114.]
 [  3.   0.   1. 525.  70.]
 [  8.   3.  10.   7. 635.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.003 | Acc: 65.089% | Wgt Acc: 64.491% | Dur: 15.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   3.   3.  14.  18.]
 [  0.  42.   3.   0.   4.]
 [  0.  18.  43.   2.  31.]
 [ 12.   3.  10.  60.   7.]
 [ 11.  12.  16.  10. 120.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.178 | Acc: 87.125% | Wgt Acc: 95.781%
	I - Batch: 100 | Loss: 0.174 | Acc: 88.500% | Wgt Acc: 95.994%
	I - Batch: 150 | Loss: 0.171 | Acc: 89.208% | Wgt Acc: 96.175%
	I - Batch: 200 | Loss: 0.177 | Acc: 89.000% | Wgt Acc: 95.999%
I - num batch: 222
I - Train -- Loss: 0.181 | Acc: 88.779% | Wgt Acc: 95.852% | LR: 1.250000e-04 | Dur: 134.08s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   1.   1.   6. 114.]
 [  1. 574.   0.   1.  55.]
 [  0.   1. 730.   2. 123.]
 [  3.   0.   1. 524.  68.]
 [ 12.   2.   2.   5. 640.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.026 | Acc: 65.680% | Wgt Acc: 61.515% | Dur: 14.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   6.   7.  27.  25.]
 [  0.  49.  10.   1.   4.]
 [  0.   9.  31.   2.   9.]
 [  9.   1.  10.  46.   5.]
 [  9.  13.  17.  10. 137.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.196 | Acc: 89.375% | Wgt Acc: 96.056%
	I - Batch: 100 | Loss: 0.184 | Acc: 89.875% | Wgt Acc: 96.407%
	I - Batch: 150 | Loss: 0.182 | Acc: 89.708% | Wgt Acc: 96.269%
	I - Batch: 200 | Loss: 0.187 | Acc: 89.188% | Wgt Acc: 95.908%
I - num batch: 222
I - Train -- Loss: 0.187 | Acc: 89.033% | Wgt Acc: 95.867% | LR: 1.250000e-04 | Dur: 139.89s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   1.   0.   6. 125.]
 [  1. 575.   1.   0.  52.]
 [  0.   1. 725.   2. 106.]
 [  4.   0.   1. 524.  66.]
 [  9.   1.   7.   6. 651.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.027 | Acc: 64.300% | Wgt Acc: 65.736% | Dur: 16.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   5.   4.  17.  23.]
 [  0.  44.   5.   1.  13.]
 [  1.  13.  42.   1.  17.]
 [ 11.   6.   9.  60.  18.]
 [  5.  10.  15.   7. 109.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.181 | Acc: 89.250% | Wgt Acc: 95.548%
	I - Batch: 100 | Loss: 0.174 | Acc: 89.750% | Wgt Acc: 96.206%
	I - Batch: 150 | Loss: 0.169 | Acc: 90.042% | Wgt Acc: 96.312%
	I - Batch: 200 | Loss: 0.176 | Acc: 89.562% | Wgt Acc: 96.179%
I - num batch: 222
I - Train -- Loss: 0.175 | Acc: 89.597% | Wgt Acc: 96.229% | LR: 1.250000e-04 | Dur: 132.91s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   0.   4. 110.]
 [  1. 577.   0.   2.  46.]
 [  1.   0. 728.   0. 102.]
 [  2.   0.   1. 528.  78.]
 [ 12.   1.   5.   4. 664.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 67.061% | Wgt Acc: 66.555% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   4.   6.  15.  21.]
 [  0.  41.   4.   1.   7.]
 [  2.  11.  44.   0.  19.]
 [  8.   5.   9.  62.  10.]
 [  8.  17.  12.   8. 123.]]

I - Local maximum validation set accuracy:  67.06

I - Validation set results: 
[14-1-2-0.68][50-3-4-1.73][124-2-2-1.30][127-0-0-6.85][443-2-2-2.64][567-0-0-3.81][573-1-1-2.03][615-0-0-2.29][695-1-2-4.77][722-3-0-4.08]
[826-0-0-6.50][878-0-0-3.10][1103-0-4-1.39][1212-3-4-1.30][1368-0-0-4.33][2181-2-3-0.68][2476-2-2-2.69][2721-2-2-3.69][2818-1-0-0.68][2886-2-4-1.52]
[3231-2-2-6.27][3333-2-3-0.73][3482-2-2-0.85][3536-3-3-2.24][3625-1-1-2.27][3909-0-0-1.94][4035-0-0-1.58][4140-0-0-2.92][4214-1-3-2.35][4346-1-3-0.18]
[4581-2-1-1.93][4708-3-3-1.50][4838-3-3-1.85][4845-1-3-0.39][4868-0-0-5.56][4939-0-4-1.01][4984-2-2-4.21][5078-1-4-1.61][5396-0-0-6.97][5479-1-1-3.64]
[5717-0-0-0.84][5843-1-1-3.35][5949-3-3-1.55][5987-2-4-2.23][6014-3-3-3.02][6033-3-0-0.69][6313-0-0-2.54][6421-3-3-4.36][6500-1-4-0.77][6583-3-3-2.55]
[6683-3-3-1.77][6825-2-0-0.47][6998-3-3-0.22][7049-3-3-2.75][7517-1-1-3.30][7521-1-1-0.12][7528-1-1-0.77][7949-1-2-1.18][8135-1-0-0.49][8185-3-0-5.28]
[8269-3-1-5.77][8273-3-3-2.84][8543-3-0-3.90][8666-1-1-5.09][8672-0-0-5.08][8903-1-1-0.79][9001-2-4-0.93][9036-2-2-3.62][9281-3-3-0.87][9300-2-2-5.56]
[9571-0-3-0.56][9617-1-1-1.88][9644-2-2-3.61][9705-2-0-0.43][9801-0-3-2.47][9803-3-3-2.74][9865-3-3-3.63][9896-2-2-2.51][10314-1-4-1.04][10337-3-3-4.41]
[10403-0-4-1.10][10653-2-4-0.74][10704-2-1-0.60][10719-1-1-4.48][10727-1-4-1.60][10836-0-0-8.96][10969-2-3-1.96][11042-0-0-3.03][11088-1-1-5.36][11322-0-0-4.96]
[11398-2-2-5.61][11499-0-0-2.90][11502-3-3-1.17][11512-3-3-2.63][11608-1-1-2.25][11610-0-0-2.52][11692-0-0-3.24][11905-0-0-5.13][11993-1-1-2.86][12002-2-0-3.29]
[12052-0-0-4.67][12201-0-3-2.81][12235-2-2-3.29][12320-1-4-2.36][12377-2-4-2.89][12398-2-3-1.71][12503-1-4-1.06][12617-0-2-0.16][12685-3-3-1.83][12738-2-0-1.74]
[12742-2-2-7.88][12823-0-0-2.61][13110-1-2-1.70][13240-3-3-3.48][13253-1-4-1.68][13273-0-0-6.05][13634-1-4-0.39][13763-2-2-2.83][13905-3-3-1.97][14060-2-4-2.14]
[14065-3-0-2.56][14147-3-3-2.89][14595-2-2-2.61][14687-2-2-3.91][14788-2-2-3.93][14869-1-1-3.09][14872-3-4-1.54][14877-1-1-2.33][14927-0-3-2.54][15066-0-0-4.94]
[15175-1-1-1.29][15178-2-3-0.38][15375-3-0-2.68][15389-3-3-3.39][15568-2-1-1.52][15675-3-3-4.30][15869-1-3-1.35][16207-3-0-1.58][16236-0-0-0.98][16302-3-3-1.91]
[16331-2-2-9.19][16381-0-0-1.52][16488-1-1-0.99][16495-0-0-5.97][16650-0-0-6.22][16719-1-2-1.05][16801-0-0-6.21][16828-0-0-3.41][17137-3-3-2.30][17245-1-2-0.85]
[17278-3-3--0.18][17282-0-2-1.91][17311-2-2-2.62][17336-2-1-2.99][17608-3-3-5.85][17627-0-0-1.22][17877-3-4-1.37][17924-1-2-1.87][17984-3-0-3.24][18211-0-3-2.04]
[18276-3-0-1.77][18287-1-1-0.19][18394-0-0-5.33][18428-0-0-2.33][18442-0-3-1.75][18478-3-3-2.37][18607-0-0-2.20][18616-0-0-2.56][18663-0-0-4.14][18718-0-0-4.76]
[18766-2-2-3.25][18824-2-2-2.64][18890-3-3-3.95][18930-3-4-1.75][18938-3-3-2.66][19817-1-1-1.20][19839-0-0-1.52][19930-3-3-2.68][19944-0-0-2.06][20036-2-2-4.89]
[20101-3-3-1.65][20474-1-1-1.86][20547-3-3-0.97][20929-2-2-5.31][21245-1-1-0.94][21257-3-3-1.45][21293-1-1-2.86][21316-1-1-4.03][21384-1-4-1.47][21448-1-1-3.24]
[21483-0-0-4.35][21487-2-2-2.91][21714-0-0-1.66][21943-3-3-1.57][21947-0-0-3.63][21948-0-0-6.95][21965-2-2-4.68][21998-1-1-2.75][22025-0-4-1.97][22228-3-3-5.58]
[22446-1-1-6.89][22494-3-3-2.66][22757-0-0-6.40][22811-3-3-4.36][22976-3-4-0.61][22985-3-3-2.28][23014-0-0-3.09][23112-1-1-3.61][23144-3-3-5.18][23168-2-0-0.76]
[23219-0-0-1.44][23363-3-3-3.92][23470-0-0-1.43][23486-2-2-1.16][23497-0-0-3.43][23516-0-0-4.61][23690-1-4-1.13][23921-2-2-1.70][23936-1-2-2.03][24040-3-0-0.66]
[24111-1-4-2.41][24182-0-0-5.59][24238-3-3-3.49][24290-2-0-2.89][24345-0-0-2.43][24364-1-2-2.10][24427-3-0-3.20][24477-2-2-3.83][24495-2-4-1.53][24893-2-2-2.93]
[25012-1-4-1.50][25121-2-2-3.79][25165-3-3-1.36][25183-0-0-2.28][25297-3-3-4.34][25398-0-0-4.19][25574-2-2-2.44][25644-1-2-4.09][25718-1-4-0.41][25774-2-4-1.24]
[26032-3-3-3.37][26051-3-3-3.16][26120-0-4-1.50][26321-1-1-3.81][26732-1-1-3.09][26784-3-3-5.01][26827-3-3-2.00][26833-0-3-2.69][26838-2-3-1.35][26860-1-0-1.16]
[26948-0-0-1.90][27049-3-0-2.46][27098-1-0-2.87][27526-0-0-4.89][27639-3-3-2.35][27698-3-0-2.18][27772-0-0-5.01][27890-1-1-3.10][28040-0-4-2.72][28503-2-2-3.67]
[28577-1-1-3.80][28959-0-0-5.44][29198-3-3-1.86][29777-0-0-8.23][29877-2-2-0.98][30035-1-1-1.43][30098-0-0-1.69][30326-1-1-6.49][30572-2-2-3.73][30716-0-4-2.05]
[30806-2-3-3.07][30906-1-1-3.06][31007-0-0-2.29][31181-3-3-1.95][31238-0-0-3.10][31347-0-0-5.65][31422-2-2-2.36][31429-3-3-1.80][31431-0-0-3.02][31432-1-1-2.78]
[31477-0-0-3.49][31524-1-2-1.77][31597-1-4-0.61][31619-1-4-1.02][31701-0-0-3.95][31755-0-0-4.73][31854-3-3-3.03][32074-1-1-1.51][32078-3-3-4.72][32111-1-1-4.70]
[32127-1-1-3.26][32140-3-3-3.62][32263-2-2-0.66][32365-0-0-2.38][32411-2-3-3.46][32429-3-0-2.15][32473-3-0-2.28][32574-3-3-3.07][32584-0-0-2.61][32622-0-4-1.95]
[32858-3-3-3.71][32969-3-3-3.48][33016-2-2-4.77][33031-1-3-2.78][33035-2-4-1.91][33133-2-2-1.93][33173-2-2-1.41][33175-3-4-3.34][33306-3-3-1.47][33309-2-2-2.02]
[33474-0-0-1.42][33478-2-4-0.79][33618-1-1-1.32][33712-0-3-1.46][33782-2-4-2.64][33914-3-3-1.38][34076-3-4-0.89][34112-2-2-3.68][34138-2-3-1.51][34239-1-1-1.03]
[34364-2-2-5.00][34617-1-4-2.21][34751-3-3-2.00][34783-2-4-2.66][35015-3-3-1.10][35018-1-4-1.56][35288-2-2-0.56][0-4-4-3.66][1-4-0-1.37][2-4-4-2.21]
[3-4-1-1.41][4-4-0-0.48][5-4-2-0.11][6-4-4-4.25][7-4-0-3.25][8-4-2-0.49][9-4-4-1.59][10-4-4-3.29][11-4-4-4.23][12-4-4-0.04]
[14-4-4-0.94][15-4-3-2.18][16-4-4-0.82][17-4-4-0.68][18-4-4-4.55][19-4-0-2.29][20-4-0-1.67][21-4-4-1.46][22-4-4-2.41][23-4-4-0.46]
[24-4-4-5.52][25-4-3-1.81][26-4-3-1.05][27-4-4-2.79][28-4-4-3.63][29-4-1-0.87][30-4-4-0.75][31-4-4-1.97][32-4-4-2.08][33-4-4-0.95]
[34-4-4-1.87][35-4-0-2.37][37-4-4-1.13][39-4-0-4.45][40-4-4-0.68][41-4-4-0.19][42-4-2-1.04][43-4-2-1.48][45-4-4-3.96][46-4-4-3.69]
[47-4-4-4.50][48-4-4-1.74][51-4-4-3.70][52-4-0-1.48][53-4-4-1.81][54-4-3-2.59][55-4-4-1.42][56-4-1-1.42][57-4-4--0.40][58-4-2-1.31]
[59-4-0-2.86][60-4-4-0.46][61-4-4-3.39][62-4-3-1.49][63-4-2-4.46][64-4-4-1.89][65-4-4-3.96][66-4-4-3.13][67-4-2-3.48][68-4-4-1.40]
[69-4-4-0.72][70-4-4-3.16][72-4-4-1.32][73-4-1-4.07][74-4-2-2.42][75-4-3-1.41][77-4-4-5.12][78-4-4-1.18][79-4-4-3.14][80-4-4-1.29]
[81-4-4-3.63][82-4-1-1.56][83-4-4-1.09][84-4-4-2.30][85-4-4-3.06][86-4-4-1.00][87-4-4-3.39][88-4-4-2.21][89-4-0-0.57][90-4-4-1.23]
[91-4-4-1.81][92-4-2-0.56][93-4-0-0.32][94-4-4-2.70][95-4-4-0.66][96-4-4-1.96][97-4-4-3.34][98-4-2-0.97][99-4-4-0.77][100-4-4-2.38]
[101-4-4-4.45][102-4-4-1.67][103-4-0-2.08][104-4-4-1.88][105-4-2-2.12][106-4-4-3.02][107-4-4-1.47][108-4-4-2.10][109-4-4-2.77][110-4-4-1.83]
[111-4-0-4.61][112-4-2-1.20][113-4-3-0.22][114-4-3-0.59][115-4-1-1.51][116-4-4-1.78][117-4-4-2.69][119-4-4-1.84][121-4-4-2.21][122-4-4-2.57]
[124-4-4-0.83][125-4-4-3.71][126-4-4-4.23][127-4-2-1.47][128-4-0-0.26][129-4-4-1.57][130-4-4-2.15][131-4-4-1.03][132-4-4-0.59][133-4-4-4.26]
[135-4-4-1.98][136-4-4-0.64][137-4-4-1.79][138-4-4-1.37][139-4-4-1.56][140-4-4-1.13][141-4-0-2.74][142-4-4-3.12][143-4-4-3.09][144-4-4-4.07]
[145-4-2-2.10][148-4-0-5.04][149-4-4-1.73][150-4-4-4.77][151-4-4-2.81][152-4-4-2.20][153-4-2-1.87][154-4-4-3.40][155-4-4-3.20][156-4-3-1.67]
[157-4-0-2.50][158-4-4-2.03][160-4-1-0.25][161-4-2-1.55][162-4-4-0.79][164-4-4-1.48][165-4-4-1.60][167-4-4-2.51][168-4-4-2.21][170-4-4-1.57]
[171-4-4-2.24][172-4-4-3.08][173-4-4-2.82][174-4-0-5.22][175-4-4-3.60][177-4-0-2.77][178-4-4-2.17][179-4-4-2.56][180-4-4-2.32][181-4-4-1.86]
[182-4-3-1.51][183-4-4-2.98][184-4-4-1.50][186-4-4-0.81][187-4-2-1.88][188-4-4-2.20][189-4-0-0.74][190-4-4-0.06][191-4-4-2.01][192-4-4-1.48]
[193-4-2-2.41][194-4-4-0.31][195-4-0-1.89][196-4-4-1.47][197-4-4-2.63][198-4-4-6.68][199-4-2-2.57]
---------------------------
I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.191 | Acc: 88.125% | Wgt Acc: 95.768%
	I - Batch: 100 | Loss: 0.185 | Acc: 89.375% | Wgt Acc: 96.107%
	I - Batch: 150 | Loss: 0.190 | Acc: 88.750% | Wgt Acc: 95.680%
	I - Batch: 200 | Loss: 0.187 | Acc: 88.406% | Wgt Acc: 95.696%
I - num batch: 222
I - Train -- Loss: 0.185 | Acc: 88.582% | Wgt Acc: 95.780% | LR: 1.250000e-04 | Dur: 134.93s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   1.   0.   6. 126.]
 [  0. 574.   0.   1.  44.]
 [  1.   1. 725.   0. 111.]
 [  7.   0.   2. 527.  85.]
 [  7.   2.   7.   4. 634.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.955 | Acc: 67.258% | Wgt Acc: 64.675% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   1.   0.  12.  11.]
 [  0.  45.   9.   2.   9.]
 [  2.  16.  47.   4.  21.]
 [ 11.   2.   7.  57.   6.]
 [ 16.  14.  12.  11. 133.]]

I - Local maximum validation set accuracy:  67.26

I - Validation set results: 
[14-1-2-1.40][50-3-1-1.34][124-2-4-1.41][127-0-0-5.38][443-2-2-2.58][567-0-0-1.54][573-1-1-3.17][615-0-3-1.72][695-1-2-2.82][722-3-0-2.81]
[826-0-0-4.72][878-0-0-4.37][1103-0-4-1.51][1212-3-4-1.34][1368-0-0-3.83][2181-2-2--0.21][2476-2-2-2.32][2721-2-2-3.87][2818-1-1-0.47][2886-2-1-2.10]
[3231-2-2-7.34][3333-2-2-2.01][3482-2-2-2.67][3536-3-3-1.67][3625-1-1-2.50][3909-0-0-0.77][4035-0-0-1.35][4140-0-0-1.18][4214-1-3-1.93][4346-1-4-0.38]
[4581-2-2-1.64][4708-3-2-2.01][4838-3-4-0.53][4845-1-1-1.16][4868-0-0-4.98][4939-0-4-0.95][4984-2-2-3.57][5078-1-4-1.08][5396-0-0-5.49][5479-1-1-5.99]
[5717-0-0-3.15][5843-1-1-2.04][5949-3-3-1.39][5987-2-4-3.22][6014-3-3-2.52][6033-3-4-0.84][6313-0-0-1.68][6421-3-3-2.91][6500-1-1-0.55][6583-3-3-2.78]
[6683-3-3-1.52][6825-2-1-3.52][6998-3-3--0.41][7049-3-3-2.61][7517-1-1-4.56][7521-1-1-0.75][7528-1-2-0.24][7949-1-2-2.22][8135-1-4--0.11][8185-3-0-2.64]
[8269-3-1-5.47][8273-3-3-3.32][8543-3-0-5.90][8666-1-1-4.42][8672-0-3-2.50][8903-1-1-3.38][9001-2-4-1.09][9036-2-2-4.83][9281-3-3-2.82][9300-2-2-10.16]
[9571-0-4-1.30][9617-1-1-4.14][9644-2-2-3.29][9705-2-1-0.95][9801-0-3-2.77][9803-3-3-1.82][9865-3-3-4.83][9896-2-2-2.92][10314-1-4-0.95][10337-3-3-5.41]
[10403-0-4-1.35][10653-2-4-1.03][10704-2-2-2.35][10719-1-1-4.33][10727-1-4-1.83][10836-0-0-9.41][10969-2-3-1.09][11042-0-0-2.49][11088-1-1-7.05][11322-0-0-4.64]
[11398-2-2-7.61][11499-0-0-2.62][11502-3-3-1.21][11512-3-3-1.51][11608-1-2-2.88][11610-0-0-1.06][11692-0-0-2.13][11905-0-0-3.57][11993-1-1-3.63][12002-2-2-2.91]
[12052-0-0-3.57][12201-0-3-2.76][12235-2-2-2.59][12320-1-4-2.47][12377-2-4-1.71][12398-2-3-1.00][12503-1-2-2.44][12617-0-2-0.08][12685-3-2-0.70][12738-2-4--0.06]
[12742-2-2-8.29][12823-0-0-4.75][13110-1-2-1.78][13240-3-3-3.40][13253-1-4-2.40][13273-0-0-5.73][13634-1-1-1.09][13763-2-2-3.37][13905-3-3-0.93][14060-2-2-1.74]
[14065-3-0-1.30][14147-3-3-2.13][14595-2-2-1.98][14687-2-2-4.89][14788-2-2-3.70][14869-1-1-2.48][14872-3-4-1.26][14877-1-1-4.58][14927-0-3-3.40][15066-0-0-4.36]
[15175-1-1-3.33][15178-2-3-1.01][15375-3-0-1.28][15389-3-3-3.72][15568-2-1-0.89][15675-3-3-2.97][15869-1-4--0.08][16207-3-0-0.69][16236-0-0-1.83][16302-3-2-1.12]
[16331-2-2-9.83][16381-0-4-1.03][16488-1-1-5.10][16495-0-0-5.64][16650-0-0-5.16][16719-1-2-2.41][16801-0-0-6.18][16828-0-0-3.95][17137-3-3-2.35][17245-1-4-1.86]
[17278-3-0-0.44][17282-0-2-1.85][17311-2-2-3.64][17336-2-1-1.73][17608-3-3-4.30][17627-0-0-1.09][17877-3-0-0.47][17924-1-2-2.15][17984-3-3-3.33][18211-0-4-0.53]
[18276-3-3-2.08][18287-1-1-1.33][18394-0-0-3.76][18428-0-0-5.33][18442-0-3-2.39][18478-3-3-2.03][18607-0-0-1.30][18616-0-0-1.88][18663-0-0-3.90][18718-0-0-5.19]
[18766-2-2-4.28][18824-2-4-3.35][18890-3-3-4.28][18930-3-4-1.18][18938-3-3-1.36][19817-1-1-1.98][19839-0-0-0.60][19930-3-3-3.47][19944-0-4-2.07][20036-2-2-7.98]
[20101-3-3-2.41][20474-1-1-1.44][20547-3-4-0.73][20929-2-2-5.43][21245-1-2-2.64][21257-3-2-1.69][21293-1-1-4.22][21316-1-1-5.11][21384-1-1-3.47][21448-1-1-4.84]
[21483-0-0-3.62][21487-2-2-3.58][21714-0-0-0.98][21943-3-3-1.03][21947-0-0-2.98][21948-0-0-6.42][21965-2-2-3.18][21998-1-1-5.69][22025-0-4-2.75][22228-3-3-4.83]
[22446-1-1-8.06][22494-3-3-2.65][22757-0-0-5.89][22811-3-3-3.88][22976-3-4-2.04][22985-3-3-2.16][23014-0-3-3.56][23112-1-1-4.48][23144-3-3-4.52][23168-2-4--0.15]
[23219-0-4-1.36][23363-3-3-3.12][23470-0-4--0.13][23486-2-2-1.51][23497-0-3-4.08][23516-0-0-4.38][23690-1-4-0.65][23921-2-2-3.32][23936-1-2-2.14][24040-3-0-0.63]
[24111-1-4-3.02][24182-0-0-4.89][24238-3-3-3.10][24290-2-4-1.27][24345-0-0-1.54][24364-1-2-2.01][24427-3-0-2.60][24477-2-2-3.66][24495-2-1-2.10][24893-2-2-3.59]
[25012-1-4-1.02][25121-2-2-3.15][25165-3-3-2.41][25183-0-4-1.33][25297-3-3-3.12][25398-0-0-2.97][25574-2-2-3.93][25644-1-2-4.42][25718-1-4-0.46][25774-2-2-1.14]
[26032-3-3-3.71][26051-3-3-3.59][26120-0-4-1.95][26321-1-1-4.83][26732-1-1-2.85][26784-3-3-5.39][26827-3-3-1.52][26833-0-3-3.78][26838-2-2-0.63][26860-1-2-1.74]
[26948-0-0-1.79][27049-3-0-1.67][27098-1-0-0.71][27526-0-0-2.71][27639-3-3-2.27][27698-3-3-2.37][27772-0-0-4.61][27890-1-1-3.13][28040-0-4-1.66][28503-2-2-5.17]
[28577-1-1-4.47][28959-0-0-4.20][29198-3-4-2.46][29777-0-0-6.55][29877-2-1-0.87][30035-1-1-3.20][30098-0-3-0.98][30326-1-1-6.32][30572-2-2-4.46][30716-0-4-2.29]
[30806-2-3-2.09][30906-1-1-4.06][31007-0-0-1.15][31181-3-3-0.83][31238-0-0-2.01][31347-0-0-5.33][31422-2-1-0.59][31429-3-3-1.13][31431-0-0-0.37][31432-1-1-5.48]
[31477-0-0-3.76][31524-1-1-1.66][31597-1-1-1.28][31619-1-4-0.55][31701-0-0-2.51][31755-0-0-3.24][31854-3-3-1.39][32074-1-1-1.12][32078-3-3-4.72][32111-1-1-3.42]
[32127-1-2-4.18][32140-3-3-3.48][32263-2-2-1.75][32365-0-0-3.40][32411-2-3-3.86][32429-3-0-1.68][32473-3-3-1.98][32574-3-3-3.56][32584-0-4-2.59][32622-0-4-0.60]
[32858-3-3-4.06][32969-3-3-2.80][33016-2-2-8.16][33031-1-3-1.13][33035-2-2-2.04][33133-2-2-3.59][33173-2-2-1.66][33175-3-4-2.98][33306-3-3-1.85][33309-2-3-2.11]
[33474-0-0-0.45][33478-2-4-0.40][33618-1-1-1.27][33712-0-3-1.42][33782-2-4-2.70][33914-3-3-3.71][34076-3-4-1.44][34112-2-2-5.60][34138-2-3-0.66][34239-1-1-2.36]
[34364-2-2-5.69][34617-1-2-2.96][34751-3-3-2.30][34783-2-4-2.61][35015-3-4-1.40][35018-1-1-1.97][35288-2-1-0.54][0-4-4-3.58][1-4-4-2.00][2-4-4-1.94]
[3-4-4-1.59][4-4-0-0.33][5-4-1-2.15][6-4-4-2.80][7-4-4-1.41][8-4-4-0.14][9-4-4-1.48][10-4-4-4.27][11-4-4-3.61][12-4-4-0.38]
[14-4-4-1.70][15-4-3-3.73][16-4-4-1.46][17-4-4-1.22][18-4-4-3.06][19-4-0-1.44][20-4-4-0.91][21-4-2-1.45][22-4-4-3.51][23-4-4-0.87]
[24-4-4-3.85][25-4-4-1.86][26-4-4-0.59][27-4-4-3.14][28-4-4-2.53][29-4-1-1.14][30-4-4-0.57][31-4-4-1.51][32-4-4-2.01][33-4-2-2.34]
[34-4-4-2.25][35-4-4-0.62][37-4-4-1.74][39-4-0-2.88][40-4-4-0.83][41-4-1-0.90][42-4-4-1.98][43-4-4-1.28][45-4-4-1.04][46-4-4-3.62]
[47-4-4-4.53][48-4-4-1.74][51-4-4-2.61][52-4-4-0.46][53-4-4-0.50][54-4-3-1.64][55-4-4-2.44][56-4-1-2.27][57-4-0-0.46][58-4-2-2.91]
[59-4-0-1.68][60-4-4-2.19][61-4-4-3.63][62-4-2-2.13][63-4-2-2.37][64-4-4-1.90][65-4-4-3.19][66-4-4-4.32][67-4-4-0.49][68-4-4-0.93]
[69-4-2-1.01][70-4-4-2.72][72-4-4-2.44][73-4-1-2.16][74-4-2-2.78][75-4-3--0.27][77-4-4-4.74][78-4-4-0.11][79-4-4-3.62][80-4-4-1.95]
[81-4-4-4.29][82-4-1-2.29][83-4-4-2.03][84-4-4-1.76][85-4-4-2.91][86-4-4-0.69][87-4-4-2.35][88-4-4-1.73][89-4-4-0.73][90-4-4-1.21]
[91-4-4-1.36][92-4-4-2.13][93-4-4-0.69][94-4-4-2.46][95-4-4-1.24][96-4-2-1.97][97-4-4-3.21][98-4-2-1.06][99-4-4-1.03][100-4-1-2.10]
[101-4-4-4.51][102-4-2-1.82][103-4-4-1.09][104-4-4-2.55][105-4-4-2.70][106-4-4-3.07][107-4-4-1.30][108-4-4-2.05][109-4-4-1.11][110-4-4-2.33]
[111-4-0-4.01][112-4-2-1.51][113-4-4-0.91][114-4-4-0.45][115-4-1-2.12][116-4-4-1.56][117-4-4-2.42][119-4-4-2.80][121-4-4-2.83][122-4-4-2.45]
[124-4-4-1.60][125-4-4-3.36][126-4-4-3.69][127-4-2-0.70][128-4-4-1.04][129-4-4-2.36][130-4-4-1.98][131-4-2-1.99][132-4-0-1.01][133-4-4-3.74]
[135-4-4-2.30][136-4-4-1.15][137-4-4-1.76][138-4-4-1.43][139-4-4-1.28][140-4-1-2.31][141-4-0--0.28][142-4-4-3.32][143-4-4-2.55][144-4-4-4.08]
[145-4-2-5.39][148-4-0-3.38][149-4-4-2.49][150-4-4-4.30][151-4-4-3.05][152-4-4-2.28][153-4-4-1.80][154-4-4-3.08][155-4-4-2.82][156-4-3-1.01]
[157-4-2-1.12][158-4-4-1.76][160-4-4-1.06][161-4-2-1.28][162-4-4-0.59][164-4-4-2.22][165-4-4-2.24][167-4-4-2.07][168-4-4-1.97][170-4-4-1.70]
[171-4-4-3.10][172-4-4-3.83][173-4-4-3.42][174-4-0-3.30][175-4-4-2.47][177-4-4-2.95][178-4-4-3.61][179-4-4-1.72][180-4-4-2.56][181-4-3-2.25]
[182-4-3-1.31][183-4-4-2.57][184-4-4-1.90][186-4-4-1.36][187-4-2-1.39][188-4-4-2.91][189-4-4-0.73][190-4-4-0.42][191-4-4-3.41][192-4-4-1.64]
[193-4-2-2.91][194-4-2-1.20][195-4-0-1.23][196-4-2-1.54][197-4-4-2.60][198-4-4-5.23][199-4-2-2.29]
---------------------------
I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.148 | Acc: 91.125% | Wgt Acc: 97.017%
	I - Batch: 100 | Loss: 0.165 | Acc: 90.250% | Wgt Acc: 96.502%
	I - Batch: 150 | Loss: 0.168 | Acc: 89.792% | Wgt Acc: 96.282%
	I - Batch: 200 | Loss: 0.172 | Acc: 89.469% | Wgt Acc: 96.032%
I - num batch: 222
I - Train -- Loss: 0.172 | Acc: 89.371% | Wgt Acc: 95.991% | LR: 1.250000e-04 | Dur: 135.68s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   0.   1.   7. 104.]
 [  1. 576.   1.   1.  52.]
 [  2.   2. 727.   1. 110.]
 [  3.   0.   1. 525.  72.]
 [ 11.   0.   4.   4. 662.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.051 | Acc: 67.258% | Wgt Acc: 64.076% | Dur: 14.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   1.  12.  13.]
 [  0.  39.   4.   0.   2.]
 [  0.  15.  41.   2.  18.]
 [ 17.   3.  12.  61.  11.]
 [  7.  16.  17.  11. 136.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.163 | Acc: 91.500% | Wgt Acc: 96.516%
	I - Batch: 100 | Loss: 0.177 | Acc: 89.938% | Wgt Acc: 95.990%
	I - Batch: 150 | Loss: 0.181 | Acc: 89.375% | Wgt Acc: 95.818%
	I - Batch: 200 | Loss: 0.175 | Acc: 89.625% | Wgt Acc: 95.950%
I - num batch: 222
I - Train -- Loss: 0.173 | Acc: 89.625% | Wgt Acc: 96.017% | LR: 1.250000e-04 | Dur: 136.84s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   0.   0.   3. 109.]
 [  1. 574.   3.   1.  39.]
 [  2.   0. 725.   0. 101.]
 [  3.   2.   1. 530.  78.]
 [ 14.   2.   5.   4. 673.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.992 | Acc: 64.103% | Wgt Acc: 62.934% | Dur: 14.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   5.   2.  24.  23.]
 [  0.  40.   6.   0.  10.]
 [  0.  19.  42.   3.  20.]
 [  8.   1.   8.  51.   7.]
 [  8.  13.  17.   8. 120.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.166 | Acc: 90.250% | Wgt Acc: 96.571%
	I - Batch: 100 | Loss: 0.174 | Acc: 89.375% | Wgt Acc: 95.892%
	I - Batch: 150 | Loss: 0.181 | Acc: 88.750% | Wgt Acc: 95.571%
	I - Batch: 200 | Loss: 0.181 | Acc: 88.719% | Wgt Acc: 95.610%
I - num batch: 222
I - Train -- Loss: 0.179 | Acc: 88.695% | Wgt Acc: 95.640% | LR: 1.250000e-04 | Dur: 135.68s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   1.   7. 121.]
 [  0. 575.   0.   2.  67.]
 [  2.   2. 724.   1.  98.]
 [  5.   1.   2. 525.  70.]
 [ 12.   0.   7.   3. 644.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.005 | Acc: 66.864% | Wgt Acc: 64.468% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   2.   2.  15.  13.]
 [  0.  47.   6.   0.   8.]
 [  1.  13.  41.   2.  19.]
 [ 16.   3.  11.  61.   8.]
 [ 13.  13.  15.   8. 132.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.162 | Acc: 90.250% | Wgt Acc: 96.602%
	I - Batch: 100 | Loss: 0.162 | Acc: 90.062% | Wgt Acc: 96.405%
	I - Batch: 150 | Loss: 0.164 | Acc: 89.750% | Wgt Acc: 96.087%
	I - Batch: 200 | Loss: 0.166 | Acc: 89.500% | Wgt Acc: 96.050%
I - num batch: 222
I - Train -- Loss: 0.167 | Acc: 89.540% | Wgt Acc: 96.057% | LR: 1.250000e-04 | Dur: 134.76s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   3.   7. 116.]
 [  0. 573.   0.   0.  62.]
 [  3.   2. 729.   0.  81.]
 [  4.   0.   1. 524.  74.]
 [  7.   3.   1.   7. 667.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.022 | Acc: 65.483% | Wgt Acc: 64.964% | Dur: 14.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   3.  18.  15.]
 [  2.  45.   9.   0.  18.]
 [  0.  12.  44.   2.  21.]
 [ 11.   3.   8.  56.   6.]
 [  8.  14.  11.  10. 120.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.169 | Acc: 88.125% | Wgt Acc: 95.556%
	I - Batch: 100 | Loss: 0.177 | Acc: 88.125% | Wgt Acc: 95.359%
	I - Batch: 150 | Loss: 0.179 | Acc: 88.292% | Wgt Acc: 95.482%
	I - Batch: 200 | Loss: 0.182 | Acc: 88.250% | Wgt Acc: 95.294%
I - num batch: 222
I - Train -- Loss: 0.183 | Acc: 88.272% | Wgt Acc: 95.351% | LR: 1.250000e-04 | Dur: 137.05s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   0.   3. 117.]
 [  3. 569.   2.   1.  55.]
 [  1.   3. 724.   1. 114.]
 [  3.   0.   1. 525.  79.]
 [ 12.   6.   7.   8. 635.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.019 | Acc: 66.272% | Wgt Acc: 62.703% | Dur: 15.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   4.  20.  20.]
 [  0.  39.   3.   0.   5.]
 [  0.  17.  44.   2.  14.]
 [ 12.   2.   8.  50.   6.]
 [  8.  16.  16.  14. 135.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.167 | Acc: 89.125% | Wgt Acc: 95.946%
	I - Batch: 100 | Loss: 0.161 | Acc: 89.750% | Wgt Acc: 96.375%
	I - Batch: 150 | Loss: 0.158 | Acc: 89.917% | Wgt Acc: 96.385%
	I - Batch: 200 | Loss: 0.161 | Acc: 89.875% | Wgt Acc: 96.280%
I - num batch: 222
I - Train -- Loss: 0.164 | Acc: 89.766% | Wgt Acc: 96.217% | LR: 1.250000e-04 | Dur: 133.87s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   2.   4. 124.]
 [  0. 575.   0.   0.  49.]
 [  5.   2. 728.   0.  86.]
 [  2.   0.   1. 528.  69.]
 [  9.   1.   3.   6. 672.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.085 | Acc: 66.864% | Wgt Acc: 62.069% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   4.   4.  21.  18.]
 [  0.  37.   3.   1.   2.]
 [  0.  13.  38.   2.  11.]
 [ 11.   2.   8.  52.   7.]
 [  7.  22.  22.  10. 142.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.149 | Acc: 91.125% | Wgt Acc: 97.288%
	I - Batch: 100 | Loss: 0.159 | Acc: 89.938% | Wgt Acc: 96.392%
	I - Batch: 150 | Loss: 0.162 | Acc: 89.542% | Wgt Acc: 96.154%
	I - Batch: 200 | Loss: 0.167 | Acc: 89.156% | Wgt Acc: 96.029%
I - num batch: 222
I - Train -- Loss: 0.168 | Acc: 89.315% | Wgt Acc: 96.116% | LR: 1.250000e-04 | Dur: 134.58s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   0.   6. 112.]
 [  2. 575.   0.   1.  58.]
 [  1.   1. 731.   1.  88.]
 [  2.   0.   0. 526.  87.]
 [ 11.   2.   3.   4. 655.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.045 | Acc: 63.116% | Wgt Acc: 60.904% | Dur: 21.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   1.  25.  24.]
 [  0.  37.   4.   0.   3.]
 [  0.  16.  43.   1.  18.]
 [ 10.   3.  10.  49.  12.]
 [ 10.  19.  17.  11. 123.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.163 | Acc: 89.250% | Wgt Acc: 95.909%
	I - Batch: 100 | Loss: 0.153 | Acc: 90.938% | Wgt Acc: 96.707%
	I - Batch: 150 | Loss: 0.156 | Acc: 90.458% | Wgt Acc: 96.555%
	I - Batch: 200 | Loss: 0.160 | Acc: 89.969% | Wgt Acc: 96.374%
I - num batch: 222
I - Train -- Loss: 0.160 | Acc: 89.963% | Wgt Acc: 96.355% | LR: 1.250000e-04 | Dur: 133.43s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   1.   5.  98.]
 [  1. 574.   0.   0.  53.]
 [  1.   0. 729.   1. 102.]
 [  3.   0.   0. 529.  71.]
 [  9.   4.   4.   3. 676.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.033 | Acc: 67.456% | Wgt Acc: 62.911% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.   5.   4.  21.  20.]
 [  0.  41.   3.   0.   5.]
 [  0.  12.  34.   0.   8.]
 [  8.   2.  12.  52.   5.]
 [  7.  18.  22.  13. 142.]]

I - Local maximum validation set accuracy:  67.46

I - Validation set results: 
[14-1-1-0.40][50-3-4-1.85][124-2-4-1.43][127-0-0-7.93][443-2-2-1.23][567-0-0-4.53][573-1-1-1.88][615-0-0-3.44][695-1-2-1.57][722-3-0-4.18]
[826-0-0-5.70][878-0-0-3.91][1103-0-4-2.32][1212-3-4-1.96][1368-0-0-5.67][2181-2-3-0.62][2476-2-2-1.57][2721-2-2-1.83][2818-1-1-0.87][2886-2-4-2.28]
[3231-2-2-6.19][3333-2-3-0.88][3482-2-2-3.09][3536-3-3-3.08][3625-1-2-1.33][3909-0-0-2.12][4035-0-0-3.30][4140-0-0-3.67][4214-1-3-0.04][4346-1-0-0.78]
[4581-2-1-1.20][4708-3-3-0.61][4838-3-3-1.97][4845-1-4-0.30][4868-0-0-6.97][4939-0-4-1.77][4984-2-2-2.65][5078-1-4-0.99][5396-0-0-5.93][5479-1-1-5.54]
[5717-0-0-4.08][5843-1-1-0.45][5949-3-3-1.61][5987-2-4-3.87][6014-3-3-2.97][6033-3-4-1.01][6313-0-0-4.47][6421-3-3-5.10][6500-1-2-0.66][6583-3-3-4.24]
[6683-3-3-2.02][6825-2-1-2.72][6998-3-0-1.41][7049-3-3-3.59][7517-1-1-3.44][7521-1-1-0.64][7528-1-2-0.02][7949-1-2-1.80][8135-1-0-0.87][8185-3-0-5.27]
[8269-3-4-1.63][8273-3-3-3.77][8543-3-0-4.56][8666-1-1-5.54][8672-0-0-7.13][8903-1-1-0.66][9001-2-2-1.82][9036-2-2-4.65][9281-3-3-1.99][9300-2-2-8.98]
[9571-0-0-1.56][9617-1-1-2.30][9644-2-2-1.35][9705-2-0-1.35][9801-0-3-2.84][9803-3-3-1.86][9865-3-3-5.21][9896-2-4-1.02][10314-1-4-1.24][10337-3-3-5.07]
[10403-0-0-1.39][10653-2-4-1.25][10704-2-2-0.78][10719-1-1-3.02][10727-1-4-2.30][10836-0-0-11.17][10969-2-3-1.21][11042-0-0-3.79][11088-1-1-5.55][11322-0-0-5.89]
[11398-2-2-3.61][11499-0-0-4.07][11502-3-3-1.38][11512-3-3-3.11][11608-1-1-2.73][11610-0-0-2.00][11692-0-0-3.99][11905-0-0-5.59][11993-1-1-3.11][12002-2-2-0.74]
[12052-0-0-5.10][12201-0-3-3.38][12235-2-2-4.59][12320-1-4-3.05][12377-2-4-4.07][12398-2-3-1.45][12503-1-4-1.46][12617-0-3-1.10][12685-3-4-0.85][12738-2-0-1.94]
[12742-2-2-5.49][12823-0-0-4.60][13110-1-2-0.86][13240-3-3-3.74][13253-1-4-2.76][13273-0-0-7.29][13634-1-4-1.44][13763-2-2-2.06][13905-3-0-1.64][14060-2-4-1.23]
[14065-3-0-2.83][14147-3-3-2.98][14595-2-2-2.39][14687-2-2-4.89][14788-2-4-1.46][14869-1-1-2.44][14872-3-4-2.14][14877-1-1-2.85][14927-0-3-4.50][15066-0-0-6.70]
[15175-1-1-1.86][15178-2-3-1.91][15375-3-0-4.00][15389-3-3-4.17][15568-2-4-1.14][15675-3-3-5.08][15869-1-3-1.03][16207-3-0-2.73][16236-0-0-3.37][16302-3-3-0.70]
[16331-2-2-8.06][16381-0-3-1.41][16488-1-1-1.88][16495-0-0-7.79][16650-0-0-5.73][16719-1-2-2.52][16801-0-0-7.47][16828-0-0-5.69][17137-3-3-1.50][17245-1-4-2.44]
[17278-3-0-2.32][17282-0-0-2.45][17311-2-2-0.92][17336-2-1-4.14][17608-3-3-6.26][17627-0-0-3.20][17877-3-4-3.50][17924-1-2-0.76][17984-3-0-3.60][18211-0-0-1.90]
[18276-3-3-2.48][18287-1-4-0.98][18394-0-0-5.52][18428-0-0-8.40][18442-0-3-2.36][18478-3-3-3.26][18607-0-0-3.16][18616-0-0-2.40][18663-0-0-5.47][18718-0-0-6.19]
[18766-2-2-1.73][18824-2-4-2.66][18890-3-3-3.60][18930-3-4-1.71][18938-3-3-1.03][19817-1-2-1.87][19839-0-0-1.29][19930-3-3-2.22][19944-0-4-1.99][20036-2-2-5.70]
[20101-3-3-2.65][20474-1-1-1.99][20547-3-0-1.63][20929-2-2-3.11][21245-1-1-1.67][21257-3-3-1.29][21293-1-1-4.92][21316-1-1-5.11][21384-1-1-3.64][21448-1-1-4.05]
[21483-0-0-4.95][21487-2-4-1.62][21714-0-0-2.85][21943-3-3-1.25][21947-0-0-1.85][21948-0-0-8.76][21965-2-2-2.74][21998-1-1-2.89][22025-0-4-2.05][22228-3-3-4.99]
[22446-1-1-6.48][22494-3-3-2.99][22757-0-0-7.27][22811-3-3-5.61][22976-3-4-0.23][22985-3-0-2.81][23014-0-0-4.49][23112-1-1-4.14][23144-3-3-6.14][23168-2-0-1.22]
[23219-0-0-2.97][23363-3-3-3.35][23470-0-0-3.12][23486-2-4-1.60][23497-0-3-4.58][23516-0-0-5.54][23690-1-4-1.37][23921-2-4-0.50][23936-1-2-0.96][24040-3-0-1.65]
[24111-1-4-2.78][24182-0-0-6.93][24238-3-3-3.47][24290-2-0-3.73][24345-0-0-3.26][24364-1-4-0.34][24427-3-0-4.99][24477-2-2-1.65][24495-2-4-1.62][24893-2-2-3.65]
[25012-1-4-0.82][25121-2-2-3.11][25165-3-3-2.39][25183-0-0-1.66][25297-3-3-5.05][25398-0-0-6.03][25574-2-2-1.98][25644-1-2-3.48][25718-1-4-0.69][25774-2-4-0.50]
[26032-3-3-4.05][26051-3-3-4.98][26120-0-4-2.19][26321-1-1-4.12][26732-1-1-2.45][26784-3-3-7.10][26827-3-0-2.52][26833-0-3-3.42][26838-2-3-0.45][26860-1-4-0.97]
[26948-0-0-3.67][27049-3-0-5.04][27098-1-0-2.63][27526-0-0-2.72][27639-3-3-1.86][27698-3-0-3.33][27772-0-0-6.39][27890-1-1-2.67][28040-0-0-2.45][28503-2-2-3.86]
[28577-1-1-3.71][28959-0-0-5.83][29198-3-4-1.61][29777-0-0-8.04][29877-2-3-1.21][30035-1-1-4.15][30098-0-0-2.09][30326-1-1-5.81][30572-2-2-3.32][30716-0-4-2.06]
[30806-2-3-2.52][30906-1-1-6.52][31007-0-0-3.88][31181-3-3-1.82][31238-0-0-3.20][31347-0-0-5.93][31422-2-4-1.06][31429-3-3-1.24][31431-0-0-4.67][31432-1-1-4.65]
[31477-0-0-4.15][31524-1-4-1.15][31597-1-1-1.45][31619-1-0-1.46][31701-0-0-4.09][31755-0-0-5.12][31854-3-3-2.19][32074-1-1-2.34][32078-3-3-5.14][32111-1-1-3.95]
[32127-1-1-2.06][32140-3-3-4.43][32263-2-4-0.37][32365-0-0-5.66][32411-2-3-4.48][32429-3-0-3.40][32473-3-0-2.62][32574-3-0-3.85][32584-0-0-3.30][32622-0-4-1.03]
[32858-3-3-3.77][32969-3-0-3.42][33016-2-2-4.47][33031-1-0-1.73][33035-2-4-1.50][33133-2-2-2.07][33173-2-3-0.17][33175-3-4-3.13][33306-3-3-1.91][33309-2-3-1.02]
[33474-0-0-1.77][33478-2-4-0.72][33618-1-4-1.75][33712-0-0-2.42][33782-2-4-2.25][33914-3-3-4.11][34076-3-4-2.55][34112-2-2-3.93][34138-2-3-2.17][34239-1-1-1.97]
[34364-2-2-3.39][34617-1-2-2.37][34751-3-3-2.78][34783-2-4-1.86][35015-3-4-2.00][35018-1-1-2.11][35288-2-4-0.12][0-4-4-3.12][1-4-4-3.17][2-4-4-2.80]
[3-4-4-1.89][4-4-4-2.07][5-4-1-0.56][6-4-4-3.48][7-4-0-2.14][8-4-4-0.63][9-4-0-1.42][10-4-4-3.45][11-4-4-4.60][12-4-4-1.58]
[14-4-4-2.11][15-4-3-4.46][16-4-4-2.13][17-4-4-1.57][18-4-4-3.13][19-4-0-3.47][20-4-0-1.36][21-4-4-1.35][22-4-4-2.83][23-4-4-1.37]
[24-4-4-5.75][25-4-4-2.54][26-4-4-0.72][27-4-4-2.45][28-4-4-4.36][29-4-4-0.90][30-4-4-1.13][31-4-4-2.42][32-4-4-2.80][33-4-4-1.43]
[34-4-4-2.43][35-4-4-2.09][37-4-4-2.53][39-4-0-3.80][40-4-4-1.61][41-4-4-0.43][42-4-4-1.39][43-4-4-2.05][45-4-4-2.86][46-4-4-3.81]
[47-4-4-4.75][48-4-4-2.28][51-4-4-4.07][52-4-4-2.09][53-4-0-1.40][54-4-4-1.94][55-4-4-1.36][56-4-1-1.01][57-4-0--0.18][58-4-2-2.61]
[59-4-0-3.48][60-4-4-1.22][61-4-4-3.01][62-4-4-1.51][63-4-2-2.52][64-4-4-2.09][65-4-4-5.25][66-4-4-3.28][67-4-2-0.47][68-4-4-2.19]
[69-4-0-1.22][70-4-4-3.47][72-4-4-2.87][73-4-1-1.39][74-4-4-1.54][75-4-3-0.90][77-4-4-3.64][78-4-4-1.06][79-4-4-4.55][80-4-4-1.42]
[81-4-4-4.63][82-4-4-1.42][83-4-4-2.68][84-4-4-3.55][85-4-4-3.43][86-4-4-1.77][87-4-4-3.35][88-4-4-2.52][89-4-4-2.44][90-4-4-1.65]
[91-4-4-2.30][92-4-4-1.58][93-4-0-2.74][94-4-4-3.22][95-4-4-1.18][96-4-4-1.94][97-4-4-3.48][98-4-4-1.44][99-4-4-1.78][100-4-4-2.67]
[101-4-4-5.78][102-4-4-2.45][103-4-0-1.81][104-4-4-2.20][105-4-2-1.64][106-4-4-3.64][107-4-4-3.54][108-4-4-1.65][109-4-4-1.41][110-4-4-1.65]
[111-4-0-5.79][112-4-4-0.30][113-4-4-0.93][114-4-3-1.50][115-4-1-2.70][116-4-4-1.87][117-4-4-3.10][119-4-4-3.36][121-4-4-2.61][122-4-4-2.71]
[124-4-4-2.30][125-4-4-4.21][126-4-4-5.22][127-4-2-2.76][128-4-0-1.95][129-4-4-1.33][130-4-4-1.75][131-4-4-1.33][132-4-4-1.28][133-4-4-3.35]
[135-4-4-3.07][136-4-4-0.79][137-4-4-2.13][138-4-4-2.52][139-4-4-2.45][140-4-4-1.63][141-4-0-3.51][142-4-4-3.56][143-4-4-3.10][144-4-4-3.91]
[145-4-2-3.55][148-4-0-5.21][149-4-4-2.06][150-4-4-4.21][151-4-4-4.00][152-4-4-2.29][153-4-4-1.99][154-4-4-6.89][155-4-4-3.74][156-4-3-1.68]
[157-4-0-0.37][158-4-4-2.05][160-4-4-1.05][161-4-2-1.09][162-4-4-0.72][164-4-4-1.68][165-4-4-1.70][167-4-0-2.63][168-4-4-2.11][170-4-4-2.00]
[171-4-4-3.06][172-4-4-4.44][173-4-4-3.54][174-4-0-4.93][175-4-4-2.92][177-4-4-2.99][178-4-4-3.14][179-4-4-1.49][180-4-4-3.34][181-4-3-2.24]
[182-4-4-2.44][183-4-4-2.99][184-4-4-2.18][186-4-0-2.06][187-4-4-0.45][188-4-4-1.72][189-4-4-1.23][190-4-4--0.00][191-4-4-3.12][192-4-4-2.42]
[193-4-1-0.92][194-4-4-1.29][195-4-0-2.88][196-4-4-1.77][197-4-4-2.55][198-4-4-5.64][199-4-2-2.32]
---------------------------
I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.162 | Acc: 90.125% | Wgt Acc: 96.446%
	I - Batch: 100 | Loss: 0.148 | Acc: 91.000% | Wgt Acc: 96.867%
	I - Batch: 150 | Loss: 0.150 | Acc: 90.792% | Wgt Acc: 96.725%
	I - Batch: 200 | Loss: 0.153 | Acc: 90.438% | Wgt Acc: 96.534%
I - num batch: 222
I - Train -- Loss: 0.153 | Acc: 90.443% | Wgt Acc: 96.516% | LR: 1.250000e-04 | Dur: 136.20s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   1.   0.   1. 106.]
 [  0. 576.   0.   1.  43.]
 [  1.   0. 729.   0. 100.]
 [  2.   0.   1. 529.  59.]
 [ 12.   1.   4.   7. 692.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.027 | Acc: 66.469% | Wgt Acc: 65.321% | Dur: 14.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   6.   3.  13.  18.]
 [  0.  37.   6.   1.   7.]
 [  0.  17.  43.   0.  21.]
 [ 13.   4.  11.  63.   9.]
 [  6.  14.  12.   9. 125.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.147 | Acc: 92.125% | Wgt Acc: 96.851%
	I - Batch: 100 | Loss: 0.151 | Acc: 90.625% | Wgt Acc: 96.420%
	I - Batch: 150 | Loss: 0.154 | Acc: 90.167% | Wgt Acc: 96.309%
	I - Batch: 200 | Loss: 0.160 | Acc: 89.719% | Wgt Acc: 96.219%
I - num batch: 222
I - Train -- Loss: 0.157 | Acc: 89.907% | Wgt Acc: 96.318% | LR: 1.250000e-04 | Dur: 137.43s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   2.   1. 106.]
 [  3. 574.   1.   2.  49.]
 [  0.   0. 727.   1. 101.]
 [  3.   0.   0. 531.  69.]
 [  9.   4.   4.   3. 675.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.022 | Acc: 66.272% | Wgt Acc: 65.967% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   3.  10.  11.]
 [  0.  42.   7.   3.   7.]
 [  0.  16.  47.   2.  30.]
 [ 18.   4.  10.  64.  11.]
 [  8.  12.   8.   7. 121.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.140 | Acc: 91.125% | Wgt Acc: 97.263%
	I - Batch: 100 | Loss: 0.144 | Acc: 91.500% | Wgt Acc: 97.283%
	I - Batch: 150 | Loss: 0.153 | Acc: 90.917% | Wgt Acc: 96.851%
	I - Batch: 200 | Loss: 0.155 | Acc: 90.625% | Wgt Acc: 96.654%
I - num batch: 222
I - Train -- Loss: 0.162 | Acc: 90.302% | Wgt Acc: 96.439% | LR: 1.250000e-04 | Dur: 135.40s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   1.   5. 101.]
 [  0. 573.   1.   0.  45.]
 [  0.   1. 728.   1.  92.]
 [  1.   1.   1. 527.  74.]
 [  9.   3.   3.   5. 688.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.058 | Acc: 65.878% | Wgt Acc: 63.937% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   6.   3.  19.  18.]
 [  0.  44.   5.   1.   8.]
 [  0.  13.  41.   4.  23.]
 [  9.   2.  10.  52.   4.]
 [  9.  13.  16.  10. 127.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.169 | Acc: 88.125% | Wgt Acc: 96.261%
	I - Batch: 100 | Loss: 0.169 | Acc: 89.062% | Wgt Acc: 95.967%
	I - Batch: 150 | Loss: 0.162 | Acc: 90.042% | Wgt Acc: 96.293%
	I - Batch: 200 | Loss: 0.161 | Acc: 90.125% | Wgt Acc: 96.374%
I - num batch: 222
I - Train -- Loss: 0.159 | Acc: 90.189% | Wgt Acc: 96.293% | LR: 1.250000e-04 | Dur: 134.28s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   0.   8.  96.]
 [  0. 576.   1.   1.  44.]
 [  3.   0. 728.   1.  89.]
 [  4.   0.   0. 524.  83.]
 [  7.   2.   5.   4. 688.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.994 | Acc: 67.850% | Wgt Acc: 63.222% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   1.   8.   6.]
 [  0.  38.   5.   0.   6.]
 [  1.  18.  41.   1.  16.]
 [ 13.   3.  10.  62.   8.]
 [ 15.  16.  18.  15. 144.]]

I - Local maximum validation set accuracy:  67.85

I - Validation set results: 
[14-1-2-1.50][50-3-4-1.82][124-2-2-0.67][127-0-0-5.38][443-2-4-1.83][567-0-0-1.68][573-1-1-1.93][615-0-0-1.59][695-1-2-3.05][722-3-3-2.57]
[826-0-0-5.18][878-0-0-5.21][1103-0-4-1.61][1212-3-4-1.58][1368-0-0-4.68][2181-2-3-1.75][2476-2-2-1.46][2721-2-2-2.78][2818-1-0-1.20][2886-2-4-2.45]
[3231-2-2-8.33][3333-2-2-2.46][3482-2-2-2.57][3536-3-4-1.33][3625-1-1-2.40][3909-0-0-2.45][4035-0-0-2.73][4140-0-0-2.01][4214-1-1-1.40][4346-1-4-0.68]
[4581-2-2-1.78][4708-3-4-1.18][4838-3-3-1.35][4845-1-2-0.45][4868-0-0-5.81][4939-0-4-0.89][4984-2-2-3.23][5078-1-4-1.02][5396-0-0-4.65][5479-1-1-5.90]
[5717-0-0-3.29][5843-1-1-1.60][5949-3-4-1.46][5987-2-4-3.09][6014-3-3-3.12][6033-3-3-0.46][6313-0-0-1.79][6421-3-3-3.84][6500-1-4-1.63][6583-3-3-3.93]
[6683-3-3-1.81][6825-2-1-2.33][6998-3-0-0.91][7049-3-3-4.18][7517-1-1-3.53][7521-1-1-2.06][7528-1-3-0.01][7949-1-2-2.61][8135-1-0-1.89][8185-3-3-2.50]
[8269-3-4-1.22][8273-3-3-4.04][8543-3-0-3.56][8666-1-1-3.63][8672-0-0-6.16][8903-1-2-2.69][9001-2-4-1.98][9036-2-2-4.80][9281-3-3-1.84][9300-2-2-7.76]
[9571-0-4-1.57][9617-1-4-1.23][9644-2-2-4.13][9705-2-4-0.54][9801-0-3-2.25][9803-3-3-2.03][9865-3-3-6.12][9896-2-4-1.19][10314-1-4-0.99][10337-3-3-3.48]
[10403-0-4-1.00][10653-2-4-1.46][10704-2-1-0.41][10719-1-1-4.33][10727-1-4-2.50][10836-0-0-9.63][10969-2-3-1.32][11042-0-0-2.65][11088-1-1-5.21][11322-0-0-4.52]
[11398-2-2-1.55][11499-0-0-2.45][11502-3-3-2.06][11512-3-3-1.96][11608-1-2-1.89][11610-0-0-2.83][11692-0-3-2.96][11905-0-0-3.02][11993-1-1-3.49][12002-2-3-1.83]
[12052-0-0-3.30][12201-0-3-3.15][12235-2-2-4.07][12320-1-4-3.45][12377-2-4-3.75][12398-2-3-0.73][12503-1-4-0.58][12617-0-3-0.63][12685-3-3-1.40][12738-2-3-0.78]
[12742-2-2-6.17][12823-0-0-2.69][13110-1-1-1.76][13240-3-3-3.32][13253-1-4-2.43][13273-0-0-5.73][13634-1-4-2.39][13763-2-2-4.84][13905-3-3-0.61][14060-2-4-1.86]
[14065-3-3-1.63][14147-3-3-3.14][14595-2-2-1.71][14687-2-2-6.71][14788-2-2-5.61][14869-1-1-1.97][14872-3-4-1.31][14877-1-1-2.81][14927-0-3-3.38][15066-0-0-4.67]
[15175-1-1-2.21][15178-2-3-1.56][15375-3-0-1.19][15389-3-3-4.64][15568-2-4-2.86][15675-3-3-5.33][15869-1-4-0.83][16207-3-0-1.31][16236-0-2-0.74][16302-3-3-1.79]
[16331-2-2-10.29][16381-0-3-1.52][16488-1-1-5.32][16495-0-0-6.64][16650-0-0-5.71][16719-1-2-1.29][16801-0-0-6.57][16828-0-0-2.67][17137-3-3-2.21][17245-1-2-1.10]
[17278-3-4--0.16][17282-0-0-1.94][17311-2-2-2.84][17336-2-1-2.97][17608-3-3-6.20][17627-0-0-0.44][17877-3-4-3.07][17924-1-2-1.24][17984-3-3-3.61][18211-0-3-1.78]
[18276-3-3-1.85][18287-1-1-0.95][18394-0-0-3.03][18428-0-0--0.29][18442-0-3-4.55][18478-3-3-2.70][18607-0-0-2.52][18616-0-0-2.17][18663-0-0-4.99][18718-0-0-4.27]
[18766-2-2-4.78][18824-2-4-2.13][18890-3-3-4.06][18930-3-4-2.74][18938-3-3-1.48][19817-1-2-2.83][19839-0-0-0.66][19930-3-3-4.57][19944-0-4-2.21][20036-2-2-7.08]
[20101-3-3-1.99][20474-1-2-1.24][20547-3-4-1.02][20929-2-2-4.56][21245-1-2-1.91][21257-3-3-0.98][21293-1-1-4.30][21316-1-1-6.55][21384-1-1-3.88][21448-1-1-4.50]
[21483-0-0-3.26][21487-2-2-4.37][21714-0-0-0.02][21943-3-4-1.02][21947-0-0-2.88][21948-0-0-6.92][21965-2-2-2.23][21998-1-1-4.77][22025-0-4-3.26][22228-3-3-6.51]
[22446-1-1-6.49][22494-3-3-3.20][22757-0-0-4.30][22811-3-3-4.75][22976-3-4-2.49][22985-3-3-3.49][23014-0-3-3.51][23112-1-1-3.80][23144-3-3-6.67][23168-2-3-1.33]
[23219-0-0-1.22][23363-3-3-4.12][23470-0-0-2.04][23486-2-4-1.96][23497-0-3-5.41][23516-0-0-4.18][23690-1-4-1.80][23921-2-2-2.75][23936-1-2-2.62][24040-3-3-1.02]
[24111-1-4-2.93][24182-0-0-5.01][24238-3-3-3.79][24290-2-0-3.03][24345-0-0-2.12][24364-1-2-0.44][24427-3-0-2.23][24477-2-2-2.45][24495-2-1-0.79][24893-2-2-2.86]
[25012-1-4-0.31][25121-2-2-3.73][25165-3-3-3.13][25183-0-4-1.69][25297-3-3-4.17][25398-0-0-4.38][25574-2-2-2.56][25644-1-1-2.03][25718-1-4-1.29][25774-2-4-3.22]
[26032-3-3-4.80][26051-3-3-4.63][26120-0-4-2.62][26321-1-2-0.93][26732-1-1-3.03][26784-3-3-7.33][26827-3-3-2.83][26833-0-3-5.57][26838-2-4-0.86][26860-1-2-0.39]
[26948-0-0-1.61][27049-3-0-2.25][27098-1-0-0.50][27526-0-0-2.41][27639-3-3-1.88][27698-3-0-1.42][27772-0-0-4.51][27890-1-1-3.45][28040-0-4-1.38][28503-2-2-5.11]
[28577-1-1-3.88][28959-0-0-4.84][29198-3-3-1.75][29777-0-0-6.55][29877-2-2-2.39][30035-1-1-3.24][30098-0-3-1.99][30326-1-1-4.42][30572-2-2-4.41][30716-0-4-3.35]
[30806-2-3-1.79][30906-1-1-6.34][31007-0-4-1.72][31181-3-3-2.54][31238-0-0-1.42][31347-0-0-3.85][31422-2-4-0.48][31429-3-3-1.75][31431-0-0-0.18][31432-1-1-3.25]
[31477-0-3-2.90][31524-1-1-2.68][31597-1-1-2.27][31619-1-4-0.91][31701-0-4-1.48][31755-0-0-3.21][31854-3-3-2.60][32074-1-3-2.62][32078-3-3-4.09][32111-1-1-2.42]
[32127-1-2-2.26][32140-3-3-3.53][32263-2-2-0.94][32365-0-0-4.02][32411-2-3-3.70][32429-3-0-1.54][32473-3-3-1.60][32574-3-3-4.29][32584-0-4-2.30][32622-0-4-1.62]
[32858-3-3-4.96][32969-3-3-3.12][33016-2-2-4.52][33031-1-3-3.41][33035-2-2-1.63][33133-2-2-1.84][33173-2-2-1.47][33175-3-4-2.01][33306-3-3-2.03][33309-2-3-2.18]
[33474-0-0-1.57][33478-2-4-0.73][33618-1-1-1.29][33712-0-4-1.91][33782-2-4-3.00][33914-3-3-4.19][34076-3-4-1.50][34112-2-2-6.65][34138-2-2-1.46][34239-1-1-1.31]
[34364-2-2-4.02][34617-1-2-1.53][34751-3-3-3.26][34783-2-4-2.92][35015-3-2-2.79][35018-1-1-3.03][35288-2-1-1.26][0-4-4-4.02][1-4-4-4.31][2-4-4-2.86]
[3-4-4-2.39][4-4-4-3.49][5-4-1-1.23][6-4-4-4.31][7-4-4-2.97][8-4-3-0.40][9-4-4-1.71][10-4-4-4.48][11-4-4-4.97][12-4-4-2.45]
[14-4-4-1.67][15-4-3-3.53][16-4-4-3.17][17-4-4-1.58][18-4-4-2.78][19-4-3-1.69][20-4-4-1.37][21-4-2-1.44][22-4-4-2.49][23-4-4-1.71]
[24-4-4-7.08][25-4-4-2.07][26-4-4-0.85][27-4-4-3.56][28-4-4-6.38][29-4-1-1.59][30-4-4-0.88][31-4-4-3.30][32-4-4-2.11][33-4-4-1.19]
[34-4-4-2.14][35-4-4-1.68][37-4-4-2.29][39-4-0-1.25][40-4-4-0.77][41-4-4-0.48][42-4-4-0.13][43-4-2-2.16][45-4-4-4.71][46-4-4-4.08]
[47-4-4-5.69][48-4-4-3.38][51-4-4-3.43][52-4-4-0.67][53-4-4-2.00][54-4-4-1.95][55-4-4-1.71][56-4-1-1.46][57-4-4-0.02][58-4-2-2.83]
[59-4-4-2.60][60-4-4-3.13][61-4-4-2.85][62-4-2-2.73][63-4-4-3.48][64-4-2-1.14][65-4-4-6.04][66-4-4-4.62][67-4-4-0.64][68-4-4-2.17]
[69-4-4-0.76][70-4-4-3.02][72-4-4-2.27][73-4-1-3.54][74-4-2-2.58][75-4-3-0.79][77-4-4-6.11][78-4-3-0.41][79-4-4-4.18][80-4-4-4.87]
[81-4-4-3.75][82-4-4-1.28][83-4-4-1.84][84-4-4-7.16][85-4-4-4.83][86-4-4-0.94][87-4-4-3.12][88-4-4-2.58][89-4-2-4.20][90-4-4-2.49]
[91-4-4-2.26][92-4-4-1.14][93-4-0-1.26][94-4-4-3.18][95-4-4-1.43][96-4-4-2.17][97-4-4-4.36][98-4-3-0.76][99-4-4-1.52][100-4-4-2.17]
[101-4-4-7.17][102-4-4-2.93][103-4-4-0.45][104-4-4-2.58][105-4-4-1.89][106-4-4-4.24][107-4-4-2.74][108-4-4-1.63][109-4-4-2.34][110-4-4-2.30]
[111-4-0-3.32][112-4-2-1.19][113-4-4-2.70][114-4-4-0.94][115-4-4-1.52][116-4-4-2.16][117-4-4-2.45][119-4-4-4.17][121-4-4-2.88][122-4-4-4.21]
[124-4-4-1.77][125-4-4-3.65][126-4-4-6.15][127-4-2-4.06][128-4-0-0.83][129-4-4-1.70][130-4-4-2.70][131-4-2-2.02][132-4-4-2.62][133-4-4-4.85]
[135-4-4-3.22][136-4-4-1.27][137-4-4-1.45][138-4-4-1.83][139-4-4-3.36][140-4-1-1.36][141-4-3-0.13][142-4-4-4.04][143-4-4-3.85][144-4-4-3.89]
[145-4-4-3.82][148-4-0-4.21][149-4-4-1.90][150-4-4-5.84][151-4-4-4.70][152-4-4-2.78][153-4-2-2.69][154-4-4-5.68][155-4-4-3.56][156-4-4-0.95]
[157-4-2-0.46][158-4-4-2.57][160-4-4-1.13][161-4-2-0.73][162-4-4--0.05][164-4-4-1.36][165-4-4-2.29][167-4-4-2.03][168-4-4-1.53][170-4-4-3.25]
[171-4-4-3.74][172-4-4-5.12][173-4-4-5.00][174-4-0-3.72][175-4-4-2.88][177-4-4-5.79][178-4-2-1.13][179-4-4-3.65][180-4-4-3.76][181-4-4-2.49]
[182-4-4-1.82][183-4-4-3.78][184-4-4-2.01][186-4-4-1.37][187-4-2-1.75][188-4-4-1.81][189-4-4-0.80][190-4-4-0.76][191-4-4-2.02][192-4-4-2.43]
[193-4-1-2.15][194-4-3-0.77][195-4-4-1.59][196-4-4-1.61][197-4-4-2.80][198-4-4-7.21][199-4-2-1.80]
---------------------------
I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.153 | Acc: 89.500% | Wgt Acc: 96.347%
	I - Batch: 100 | Loss: 0.150 | Acc: 90.562% | Wgt Acc: 96.588%
	I - Batch: 150 | Loss: 0.155 | Acc: 90.542% | Wgt Acc: 96.581%
	I - Batch: 200 | Loss: 0.154 | Acc: 90.531% | Wgt Acc: 96.592%
I - num batch: 222
I - Train -- Loss: 0.154 | Acc: 90.555% | Wgt Acc: 96.605% | LR: 1.250000e-04 | Dur: 136.11s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   2.   6. 109.]
 [  1. 574.   1.   0.  46.]
 [  0.   0. 729.   0.  92.]
 [  6.   2.   0. 531.  59.]
 [  6.   2.   2.   1. 694.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.080 | Acc: 65.089% | Wgt Acc: 61.688% | Dur: 16.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   4.   3.  18.  17.]
 [  0.  35.   3.   1.   5.]
 [  0.  12.  33.   0.   7.]
 [ 10.   6.  15.  59.  18.]
 [  8.  21.  21.   8. 133.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.147 | Acc: 91.375% | Wgt Acc: 96.942%
	I - Batch: 100 | Loss: 0.147 | Acc: 91.312% | Wgt Acc: 96.862%
	I - Batch: 150 | Loss: 0.154 | Acc: 90.417% | Wgt Acc: 96.414%
	I - Batch: 200 | Loss: 0.154 | Acc: 90.625% | Wgt Acc: 96.424%
I - num batch: 222
I - Train -- Loss: 0.153 | Acc: 90.753% | Wgt Acc: 96.465% | LR: 1.250000e-04 | Dur: 141.45s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   3.   4.  98.]
 [  2. 574.   0.   0.  54.]
 [  3.   1. 724.   0.  87.]
 [  4.   0.   1. 531.  53.]
 [  6.   3.   6.   3. 708.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.968 | Acc: 68.047% | Wgt Acc: 63.857% | Dur: 15.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   2.   3.  21.  14.]
 [  0.  42.   4.   1.   6.]
 [  1.  13.  46.   2.  13.]
 [  9.   2.   6.  49.   6.]
 [ 11.  19.  16.  13. 141.]]

I - Local maximum validation set accuracy:  68.05

I - Validation set results: 
[14-1-2-1.91][50-3-4-1.59][124-2-2-1.75][127-0-0-6.24][443-2-2-3.19][567-0-0-3.92][573-1-1-3.67][615-0-0-2.49][695-1-2-2.99][722-3-0-2.95]
[826-0-0-6.20][878-0-0-4.73][1103-0-4-1.72][1212-3-4-1.35][1368-0-0-4.58][2181-2-2-0.41][2476-2-2-2.11][2721-2-2-2.49][2818-1-1-0.48][2886-2-4-1.71]
[3231-2-2-8.22][3333-2-2-1.97][3482-2-2-0.68][3536-3-3-1.56][3625-1-1-1.80][3909-0-0-1.64][4035-0-0-2.13][4140-0-0-2.10][4214-1-1-0.85][4346-1-4-0.77]
[4581-2-2-1.47][4708-3-4-0.92][4838-3-3-1.23][4845-1-4-0.87][4868-0-0-5.65][4939-0-3-0.19][4984-2-2-3.08][5078-1-4-1.14][5396-0-0-6.04][5479-1-1-6.65]
[5717-0-0-2.96][5843-1-1-1.42][5949-3-0-1.04][5987-2-4-3.17][6014-3-3-2.34][6033-3-3-0.43][6313-0-0-2.28][6421-3-3-1.46][6500-1-4-0.12][6583-3-3-2.74]
[6683-3-3-1.65][6825-2-1-3.29][6998-3-3--0.07][7049-3-3-2.64][7517-1-1-2.36][7521-1-1-2.16][7528-1-1-1.46][7949-1-2-1.77][8135-1-0-1.66][8185-3-0-3.71]
[8269-3-4-1.10][8273-3-3-3.32][8543-3-0-5.44][8666-1-1-5.61][8672-0-0-3.56][8903-1-1-2.82][9001-2-4-1.80][9036-2-2-5.45][9281-3-0-0.78][9300-2-2-7.87]
[9571-0-4-1.47][9617-1-1-4.80][9644-2-2-3.00][9705-2-4-0.02][9801-0-3-1.99][9803-3-3-1.12][9865-3-3-5.50][9896-2-2-2.52][10314-1-4-0.57][10337-3-3-4.13]
[10403-0-4-1.58][10653-2-4-1.06][10704-2-2-1.73][10719-1-1-3.88][10727-1-4-1.57][10836-0-0-9.01][10969-2-3-1.49][11042-0-0-1.59][11088-1-1-6.39][11322-0-0-4.89]
[11398-2-2-5.24][11499-0-0-3.42][11502-3-3-0.66][11512-3-2-0.90][11608-1-2-2.27][11610-0-0-2.50][11692-0-0-1.68][11905-0-0-3.78][11993-1-1-3.20][12002-2-2-0.50]
[12052-0-0-3.93][12201-0-3-2.83][12235-2-4-1.78][12320-1-4-2.76][12377-2-4-2.44][12398-2-3-1.63][12503-1-4-0.49][12617-0-2--0.24][12685-3-4-1.69][12738-2-0-1.52]
[12742-2-2-5.74][12823-0-0-4.14][13110-1-2-1.95][13240-3-3-1.94][13253-1-4-1.69][13273-0-0-5.86][13634-1-4-1.10][13763-2-2-2.27][13905-3-3-1.99][14060-2-2-2.14]
[14065-3-0-1.51][14147-3-3-2.18][14595-2-2-1.01][14687-2-2-5.17][14788-2-2-2.55][14869-1-1-3.03][14872-3-4-1.68][14877-1-1-4.73][14927-0-3-3.10][15066-0-0-5.86]
[15175-1-1-1.50][15178-2-2-0.96][15375-3-0-1.44][15389-3-3-4.24][15568-2-4-1.39][15675-3-3-4.08][15869-1-4-1.20][16207-3-0-1.21][16236-0-0-3.95][16302-3-3-0.88]
[16331-2-2-10.30][16381-0-3-1.17][16488-1-1-2.04][16495-0-0-5.67][16650-0-0-5.00][16719-1-4-0.92][16801-0-0-4.62][16828-0-0-2.21][17137-3-3-1.49][17245-1-2-1.38]
[17278-3-1-0.25][17282-0-0-0.87][17311-2-2-3.49][17336-2-1-1.21][17608-3-3-4.79][17627-0-0-0.30][17877-3-0-1.02][17924-1-2-1.65][17984-3-3-2.45][18211-0-3-2.40]
[18276-3-3-0.84][18287-1-4-0.87][18394-0-0-4.34][18428-0-0--0.80][18442-0-3-1.56][18478-3-3-2.20][18607-0-0-2.24][18616-0-0-3.00][18663-0-0-3.68][18718-0-0-4.70]
[18766-2-2-5.66][18824-2-4-2.59][18890-3-3-2.67][18930-3-4-1.51][18938-3-3-1.09][19817-1-2-3.05][19839-0-0-0.72][19930-3-3-3.13][19944-0-4-1.34][20036-2-2-6.27]
[20101-3-3-2.39][20474-1-1-2.21][20547-3-3-1.29][20929-2-2-6.11][21245-1-1-1.63][21257-3-4-1.54][21293-1-1-4.06][21316-1-1-5.27][21384-1-1-4.16][21448-1-1-3.57]
[21483-0-0-3.92][21487-2-2-1.78][21714-0-0-1.85][21943-3-4-1.03][21947-0-0-3.18][21948-0-0-7.28][21965-2-2-5.01][21998-1-1-4.55][22025-0-4-3.47][22228-3-3-4.26]
[22446-1-1-6.69][22494-3-3-1.23][22757-0-0-5.67][22811-3-3-3.63][22976-3-2-1.22][22985-3-0-2.26][23014-0-0-4.08][23112-1-1-3.32][23144-3-3-5.00][23168-2-0-0.05]
[23219-0-4-2.15][23363-3-3-1.17][23470-0-0-1.17][23486-2-4-1.18][23497-0-3-3.10][23516-0-0-4.03][23690-1-4-1.21][23921-2-2-1.39][23936-1-2-1.32][24040-3-0-0.49]
[24111-1-4-2.52][24182-0-0-4.69][24238-3-3-2.78][24290-2-0-2.79][24345-0-0-1.41][24364-1-2-1.01][24427-3-0-3.01][24477-2-2-3.52][24495-2-4-2.04][24893-2-2-4.67]
[25012-1-1-1.42][25121-2-2-2.68][25165-3-3-2.38][25183-0-0-1.84][25297-3-3-2.81][25398-0-0-4.23][25574-2-2-2.57][25644-1-2-6.43][25718-1-4-1.38][25774-2-4-1.83]
[26032-3-3-2.49][26051-3-3-3.62][26120-0-4-1.46][26321-1-1-1.39][26732-1-1-4.98][26784-3-3-5.00][26827-3-0-1.47][26833-0-3-2.56][26838-2-2--0.15][26860-1-2-0.37]
[26948-0-0-2.97][27049-3-0-2.76][27098-1-0-1.45][27526-0-4-1.51][27639-3-0-0.71][27698-3-0-3.24][27772-0-0-4.37][27890-1-1-3.75][28040-0-0-1.51][28503-2-2-4.89]
[28577-1-1-3.01][28959-0-0-4.84][29198-3-3-1.68][29777-0-0-7.21][29877-2-2-1.05][30035-1-1-4.10][30098-0-0-2.17][30326-1-1-7.24][30572-2-2-3.48][30716-0-4-2.29]
[30806-2-3-2.40][30906-1-1-4.03][31007-0-0-1.66][31181-3-0-0.60][31238-0-0-3.10][31347-0-0-5.44][31422-2-2-1.16][31429-3-3-1.47][31431-0-0-2.79][31432-1-1-3.79]
[31477-0-0-4.36][31524-1-4-0.72][31597-1-1-1.46][31619-1-4-0.71][31701-0-0-3.77][31755-0-0-4.68][31854-3-3-2.45][32074-1-3-0.82][32078-3-3-4.54][32111-1-1-1.92]
[32127-1-1-3.61][32140-3-3-1.67][32263-2-4-0.04][32365-0-0-3.73][32411-2-3-3.47][32429-3-0-2.30][32473-3-0-2.04][32574-3-3-2.66][32584-0-4-2.18][32622-0-4-1.10]
[32858-3-3-4.25][32969-3-0-2.07][33016-2-2-6.75][33031-1-3-1.79][33035-2-1-1.29][33133-2-2-1.52][33173-2-2-1.45][33175-3-4-2.57][33306-3-3-2.16][33309-2-3-2.51]
[33474-0-0-1.16][33478-2-4-0.58][33618-1-1-1.94][33712-0-0-2.02][33782-2-4-2.71][33914-3-4-1.01][34076-3-4-1.73][34112-2-2-4.72][34138-2-3-1.62][34239-1-1-2.23]
[34364-2-2-4.46][34617-1-2-2.46][34751-3-0-2.16][34783-2-4-2.15][35015-3-4-1.90][35018-1-4-2.21][35288-2-1-0.91][0-4-4-4.13][1-4-4-3.41][2-4-4-2.82]
[3-4-4-2.74][4-4-4-2.43][5-4-1-0.23][6-4-4-2.95][7-4-4-2.01][8-4-2--0.24][9-4-4-1.60][10-4-4-4.77][11-4-4-5.65][12-4-4-1.56]
[14-4-4-1.67][15-4-3-3.10][16-4-4-0.59][17-4-4-1.52][18-4-4-4.38][19-4-0-1.81][20-4-0-1.79][21-4-4-2.44][22-4-4-3.14][23-4-4-0.82]
[24-4-4-4.29][25-4-4-1.69][26-4-4-0.81][27-4-4-3.04][28-4-4-3.23][29-4-1-2.39][30-4-3-0.54][31-4-4-2.52][32-4-4-1.99][33-4-4-1.89]
[34-4-4-2.78][35-4-4-2.70][37-4-4-2.36][39-4-0-1.87][40-4-4-0.29][41-4-4-0.40][42-4-4-1.54][43-4-4-1.77][45-4-4-3.60][46-4-4-4.18]
[47-4-4-4.74][48-4-4-2.29][51-4-4-3.64][52-4-4-1.43][53-4-0-0.83][54-4-4-1.53][55-4-4-3.00][56-4-1-1.98][57-4-0-1.18][58-4-2-3.22]
[59-4-4-2.00][60-4-4-2.21][61-4-4-4.32][62-4-3-1.48][63-4-2-1.81][64-4-4-1.63][65-4-4-4.89][66-4-4-3.32][67-4-2-1.31][68-4-4-2.14]
[69-4-4-0.54][70-4-4-3.22][72-4-4-1.64][73-4-1-2.37][74-4-4-1.84][75-4-3-1.36][77-4-4-5.61][78-4-4-0.41][79-4-4-4.45][80-4-4-3.06]
[81-4-4-3.55][82-4-4-1.30][83-4-4-2.48][84-4-4-5.84][85-4-4-4.42][86-4-4-0.50][87-4-4-2.54][88-4-4-1.86][89-4-4-0.33][90-4-4-1.97]
[91-4-4-2.14][92-4-4-0.41][93-4-0-1.97][94-4-4-2.57][95-4-4-1.55][96-4-4-2.47][97-4-4-5.25][98-4-1-1.10][99-4-4-2.35][100-4-4-1.97]
[101-4-4-5.83][102-4-4-3.18][103-4-0-1.30][104-4-4-2.00][105-4-4-3.41][106-4-4-2.80][107-4-4-3.77][108-4-4-0.96][109-4-4-1.91][110-4-4-1.76]
[111-4-0-5.34][112-4-4-0.99][113-4-4-1.37][114-4-4-0.31][115-4-1-1.94][116-4-4-1.55][117-4-4-2.78][119-4-4-4.48][121-4-4-3.40][122-4-4-3.60]
[124-4-4-1.40][125-4-4-3.69][126-4-4-3.92][127-4-2-2.42][128-4-4-1.08][129-4-4-3.01][130-4-4-2.31][131-4-2-1.78][132-4-4-1.34][133-4-4-3.74]
[135-4-4-2.82][136-4-4-1.31][137-4-4-1.38][138-4-4-2.60][139-4-4-2.90][140-4-4-1.49][141-4-0-1.15][142-4-4-3.76][143-4-4-2.57][144-4-4-3.21]
[145-4-4-2.90][148-4-0-4.93][149-4-4-1.42][150-4-4-6.36][151-4-4-3.79][152-4-4-2.24][153-4-2-2.55][154-4-4-5.38][155-4-4-2.56][156-4-3-1.24]
[157-4-0-0.53][158-4-4-2.15][160-4-4-0.46][161-4-2-2.65][162-4-4-0.29][164-4-4-2.34][165-4-4-1.99][167-4-4-2.71][168-4-4-1.93][170-4-4-3.16]
[171-4-4-3.56][172-4-4-3.38][173-4-4-4.21][174-4-0-4.04][175-4-4-1.69][177-4-4-3.32][178-4-2-0.93][179-4-4-3.67][180-4-4-3.01][181-4-4-2.93]
[182-4-3-1.85][183-4-4-3.69][184-4-2-1.78][186-4-4-1.02][187-4-4-1.28][188-4-4-2.35][189-4-0-0.77][190-4-4-1.09][191-4-4-2.11][192-4-4-2.33]
[193-4-2-3.80][194-4-0-1.99][195-4-2-1.58][196-4-4-1.74][197-4-4-3.55][198-4-4-5.76][199-4-2-1.72]
---------------------------
I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.130 | Acc: 91.875% | Wgt Acc: 97.466%
	I - Batch: 100 | Loss: 0.149 | Acc: 90.625% | Wgt Acc: 96.867%
	I - Batch: 150 | Loss: 0.154 | Acc: 89.958% | Wgt Acc: 96.338%
	I - Batch: 200 | Loss: 0.154 | Acc: 89.969% | Wgt Acc: 96.354%
I - num batch: 222
I - Train -- Loss: 0.157 | Acc: 89.625% | Wgt Acc: 96.241% | LR: 1.250000e-04 | Dur: 134.67s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   2.   3. 122.]
 [  0. 574.   0.   0.  46.]
 [  2.   1. 727.   0.  96.]
 [  1.   1.   0. 531.  71.]
 [ 12.   2.   5.   4. 665.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.017 | Acc: 65.483% | Wgt Acc: 60.858% | Dur: 14.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   2.  18.  15.]
 [  0.  41.   6.   0.  10.]
 [  0.  11.  37.   3.  12.]
 [ 12.   3.   8.  51.   4.]
 [ 12.  19.  22.  14. 139.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.163 | Acc: 90.500% | Wgt Acc: 96.147%
	I - Batch: 100 | Loss: 0.156 | Acc: 90.625% | Wgt Acc: 96.336%
	I - Batch: 150 | Loss: 0.151 | Acc: 91.000% | Wgt Acc: 96.601%
	I - Batch: 200 | Loss: 0.146 | Acc: 91.219% | Wgt Acc: 96.825%
I - num batch: 222
I - Train -- Loss: 0.145 | Acc: 91.458% | Wgt Acc: 96.905% | LR: 1.250000e-04 | Dur: 135.86s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   2.   0.   3.  99.]
 [  0. 574.   0.   0.  46.]
 [  2.   2. 730.   0.  70.]
 [  2.   0.   0. 531.  61.]
 [  8.   0.   4.   4. 724.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.011 | Acc: 67.061% | Wgt Acc: 63.522% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   2.  15.  13.]
 [  0.  38.   6.   1.   5.]
 [  0.  17.  43.   3.  17.]
 [ 13.   3.  10.  58.   8.]
 [ 11.  15.  14.   9. 137.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.138 | Acc: 90.625% | Wgt Acc: 96.469%
	I - Batch: 100 | Loss: 0.146 | Acc: 90.062% | Wgt Acc: 96.221%
	I - Batch: 150 | Loss: 0.153 | Acc: 90.000% | Wgt Acc: 96.117%
	I - Batch: 200 | Loss: 0.149 | Acc: 90.469% | Wgt Acc: 96.475%
I - num batch: 222
I - Train -- Loss: 0.149 | Acc: 90.414% | Wgt Acc: 96.470% | LR: 1.250000e-04 | Dur: 134.98s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   1.   3. 101.]
 [  0. 573.   0.   1.  46.]
 [  0.   0. 729.   1.  96.]
 [  1.   1.   0. 527.  65.]
 [ 10.   4.   4.   6. 692.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.957 | Acc: 68.047% | Wgt Acc: 65.275% | Dur: 15.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   2.  14.  17.]
 [  0.  39.   5.   0.   3.]
 [  0.  15.  46.   2.  18.]
 [  9.   5.   7.  58.   7.]
 [ 12.  16.  15.  12. 135.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.152 | Acc: 90.875% | Wgt Acc: 97.053%
	I - Batch: 100 | Loss: 0.143 | Acc: 91.438% | Wgt Acc: 97.098%
	I - Batch: 150 | Loss: 0.146 | Acc: 91.000% | Wgt Acc: 96.873%
	I - Batch: 200 | Loss: 0.150 | Acc: 90.969% | Wgt Acc: 96.760%
I - num batch: 222
I - Train -- Loss: 0.148 | Acc: 91.119% | Wgt Acc: 96.848% | LR: 1.250000e-04 | Dur: 133.74s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   0.   3. 113.]
 [  0. 577.   0.   0.  40.]
 [  1.   0. 729.   0.  75.]
 [  0.   0.   0. 532.  61.]
 [ 13.   1.   5.   3. 711.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.108 | Acc: 63.116% | Wgt Acc: 61.954% | Dur: 14.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   8.   2.  19.  23.]
 [  1.  42.   5.   1.   7.]
 [  1.  11.  40.   2.  21.]
 [ 16.   2.  13.  56.  10.]
 [  7.  15.  15.   8. 119.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.140 | Acc: 91.250% | Wgt Acc: 97.128%
	I - Batch: 100 | Loss: 0.141 | Acc: 90.750% | Wgt Acc: 97.180%
	I - Batch: 150 | Loss: 0.148 | Acc: 90.375% | Wgt Acc: 96.710%
	I - Batch: 200 | Loss: 0.143 | Acc: 90.812% | Wgt Acc: 96.909%
I - num batch: 222
I - Train -- Loss: 0.144 | Acc: 90.668% | Wgt Acc: 96.865% | LR: 1.250000e-04 | Dur: 134.49s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   0.   3.  95.]
 [  1. 574.   0.   0.  55.]
 [  1.   0. 730.   0.  94.]
 [  2.   1.   1. 534.  66.]
 [  5.   3.   3.   1. 690.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.008 | Acc: 66.667% | Wgt Acc: 61.781% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   5.   2.  13.  12.]
 [  0.  41.   7.   0.   7.]
 [  0.   9.  35.   0.   7.]
 [ 12.   1.  10.  58.  11.]
 [ 15.  22.  21.  15. 143.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.130 | Acc: 92.000% | Wgt Acc: 97.045%
	I - Batch: 100 | Loss: 0.128 | Acc: 92.250% | Wgt Acc: 97.065%
	I - Batch: 150 | Loss: 0.131 | Acc: 91.875% | Wgt Acc: 96.954%
	I - Batch: 200 | Loss: 0.129 | Acc: 92.031% | Wgt Acc: 97.098%
I - num batch: 208
I - Train -- Loss: 0.128 | Acc: 92.097% | Wgt Acc: 97.099% | LR: 1.250000e-04 | Dur: 125.31s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   4.  70.]
 [  0. 574.   0.   0.  35.]
 [  3.   1. 725.   2.  70.]
 [  1.   1.   2. 528.  59.]
 [  2.   2.   6.   4. 547.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.025 | Acc: 66.272% | Wgt Acc: 62.749% | Dur: 14.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   5.   2.  15.  15.]
 [  0.  41.   4.   1.   6.]
 [  0.  12.  39.   1.  13.]
 [ 18.   5.  12.  60.  10.]
 [ 10.  15.  18.   9. 136.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.135 | Acc: 91.125% | Wgt Acc: 96.794%
	I - Batch: 100 | Loss: 0.148 | Acc: 91.125% | Wgt Acc: 96.672%
	I - Batch: 150 | Loss: 0.148 | Acc: 90.833% | Wgt Acc: 96.570%
	I - Batch: 200 | Loss: 0.149 | Acc: 91.125% | Wgt Acc: 96.706%
I - num batch: 222
I - Train -- Loss: 0.150 | Acc: 91.091% | Wgt Acc: 96.697% | LR: 1.250000e-04 | Dur: 147.47s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   1.   2.  84.]
 [  0. 577.   0.   0.  51.]
 [  1.   0. 727.   0.  88.]
 [  4.   0.   1. 530.  62.]
 [ 10.   1.   5.   6. 715.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.993 | Acc: 64.300% | Wgt Acc: 64.352% | Dur: 16.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   5.   2.  12.  19.]
 [  1.  42.   3.   0.   7.]
 [  1.  14.  42.   2.  14.]
 [ 19.   5.  16.  64.  24.]
 [  5.  12.  12.   8. 116.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.143 | Acc: 90.375% | Wgt Acc: 96.814%
	I - Batch: 100 | Loss: 0.139 | Acc: 91.125% | Wgt Acc: 96.766%
	I - Batch: 150 | Loss: 0.139 | Acc: 91.417% | Wgt Acc: 96.915%
	I - Batch: 200 | Loss: 0.135 | Acc: 91.688% | Wgt Acc: 97.087%
I - num batch: 222
I - Train -- Loss: 0.135 | Acc: 91.599% | Wgt Acc: 97.092% | LR: 1.250000e-04 | Dur: 133.65s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   0.   3.  97.]
 [  0. 577.   1.   0.  36.]
 [  0.   1. 729.   0.  90.]
 [  4.   0.   0. 533.  53.]
 [  7.   0.   4.   2. 724.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.985 | Acc: 68.047% | Wgt Acc: 63.026% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   1.  12.   8.]
 [  0.  38.   2.   2.   6.]
 [  0.  12.  42.   0.  14.]
 [ 18.   4.  11.  60.   6.]
 [ 11.  22.  19.  12. 146.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.122 | Acc: 93.750% | Wgt Acc: 98.068%
	I - Batch: 100 | Loss: 0.125 | Acc: 92.750% | Wgt Acc: 97.627%
	I - Batch: 150 | Loss: 0.135 | Acc: 92.000% | Wgt Acc: 97.285%
	I - Batch: 200 | Loss: 0.139 | Acc: 91.656% | Wgt Acc: 97.162%
I - num batch: 222
I - Train -- Loss: 0.139 | Acc: 91.683% | Wgt Acc: 97.087% | LR: 1.250000e-04 | Dur: 135.07s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   0.   0. 105.]
 [  1. 575.   0.   0.  39.]
 [  2.   0. 731.   0.  70.]
 [  0.   0.   0. 534.  58.]
 [ 10.   3.   3.   4. 728.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.072 | Acc: 66.075% | Wgt Acc: 62.161% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   7.   3.  14.  10.]
 [  0.  41.   9.   2.   7.]
 [  0.  13.  38.   1.  16.]
 [ 10.   1.  11.  55.  10.]
 [ 14.  16.  14.  14. 137.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.131 | Acc: 92.000% | Wgt Acc: 96.855%
	I - Batch: 100 | Loss: 0.133 | Acc: 91.375% | Wgt Acc: 96.942%
	I - Batch: 150 | Loss: 0.137 | Acc: 91.667% | Wgt Acc: 97.039%
	I - Batch: 200 | Loss: 0.139 | Acc: 91.312% | Wgt Acc: 96.965%
I - num batch: 222
I - Train -- Loss: 0.141 | Acc: 91.260% | Wgt Acc: 96.854% | LR: 1.250000e-04 | Dur: 132.55s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   0.   2.  90.]
 [  1. 572.   0.   1.  42.]
 [  2.   1. 729.   1.  91.]
 [  1.   0.   0. 533.  60.]
 [  7.   5.   5.   1. 717.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.068 | Acc: 65.089% | Wgt Acc: 64.110% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   8.   5.  25.  24.]
 [  0.  39.   4.   1.   7.]
 [  0.  15.  45.   1.  16.]
 [ 11.   3.   7.  54.  12.]
 [  6.  13.  14.   5. 121.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.148 | Acc: 91.375% | Wgt Acc: 97.044%
	I - Batch: 100 | Loss: 0.147 | Acc: 90.938% | Wgt Acc: 96.974%
	I - Batch: 150 | Loss: 0.143 | Acc: 90.917% | Wgt Acc: 97.055%
	I - Batch: 200 | Loss: 0.143 | Acc: 91.094% | Wgt Acc: 97.131%
I - num batch: 222
I - Train -- Loss: 0.144 | Acc: 91.091% | Wgt Acc: 97.087% | LR: 1.250000e-04 | Dur: 136.08s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   0.   2. 108.]
 [  1. 576.   0.   0.  43.]
 [  0.   0. 734.   0.  82.]
 [  1.   0.   0. 532.  66.]
 [  7.   2.   0.   4. 701.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.073 | Acc: 65.878% | Wgt Acc: 61.723% | Dur: 17.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   5.  21.  18.]
 [  0.  34.   2.   1.   4.]
 [  0.  17.  45.   4.  10.]
 [ 12.   4.   8.  51.  11.]
 [  9.  19.  15.   9. 137.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.141 | Acc: 89.875% | Wgt Acc: 96.716%
	I - Batch: 100 | Loss: 0.136 | Acc: 91.125% | Wgt Acc: 97.023%
	I - Batch: 150 | Loss: 0.130 | Acc: 91.458% | Wgt Acc: 97.184%
	I - Batch: 200 | Loss: 0.130 | Acc: 91.719% | Wgt Acc: 97.298%
I - num batch: 222
I - Train -- Loss: 0.132 | Acc: 91.570% | Wgt Acc: 97.133% | LR: 1.250000e-04 | Dur: 133.64s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   1.   2.  97.]
 [  0. 577.   0.   0.  41.]
 [  0.   0. 730.   0.  83.]
 [  1.   0.   1. 531.  58.]
 [  7.   1.   2.   5. 721.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.082 | Acc: 65.286% | Wgt Acc: 58.598% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   3.  19.  14.]
 [  0.  34.   4.   0.   2.]
 [  0.  13.  38.   0.  11.]
 [ 11.   3.   9.  49.   5.]
 [ 15.  25.  21.  18. 148.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.129 | Acc: 91.375% | Wgt Acc: 97.411%
	I - Batch: 100 | Loss: 0.130 | Acc: 91.438% | Wgt Acc: 97.320%
	I - Batch: 150 | Loss: 0.137 | Acc: 90.708% | Wgt Acc: 96.669%
	I - Batch: 200 | Loss: 0.136 | Acc: 91.125% | Wgt Acc: 96.927%
I - num batch: 222
I - Train -- Loss: 0.136 | Acc: 91.232% | Wgt Acc: 96.902% | LR: 1.250000e-04 | Dur: 134.07s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   1.   0.   4. 103.]
 [  0. 576.   1.   0.  36.]
 [  0.   0. 729.   0.  84.]
 [  2.   0.   1. 531.  63.]
 [  9.   1.   3.   3. 714.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.056 | Acc: 65.089% | Wgt Acc: 63.453% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   3.   5.  19.  19.]
 [  0.  40.   3.   0.   6.]
 [  0.  12.  34.   0.  11.]
 [  9.   6.  15.  61.  19.]
 [  9.  17.  18.   6. 125.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.135 | Acc: 90.750% | Wgt Acc: 96.322%
	I - Batch: 100 | Loss: 0.138 | Acc: 91.062% | Wgt Acc: 96.599%
	I - Batch: 150 | Loss: 0.136 | Acc: 91.250% | Wgt Acc: 96.713%
	I - Batch: 200 | Loss: 0.133 | Acc: 91.438% | Wgt Acc: 96.801%
I - num batch: 222
I - Train -- Loss: 0.133 | Acc: 91.683% | Wgt Acc: 96.790% | LR: 1.250000e-04 | Dur: 134.44s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   0.   5.  87.]
 [  0. 575.   0.   0.  45.]
 [  1.   1. 729.   0.  74.]
 [  2.   0.   0. 527.  56.]
 [ 11.   2.   5.   6. 738.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.025 | Acc: 67.258% | Wgt Acc: 63.926% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   1.   1.   7.  11.]
 [  0.  44.   5.   0.   8.]
 [  0.  13.  42.   0.  17.]
 [ 18.   6.  11.  61.   7.]
 [ 13.  14.  16.  18. 137.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.136 | Acc: 90.875% | Wgt Acc: 96.830%
	I - Batch: 100 | Loss: 0.134 | Acc: 91.438% | Wgt Acc: 97.022%
	I - Batch: 150 | Loss: 0.130 | Acc: 91.833% | Wgt Acc: 97.103%
	I - Batch: 200 | Loss: 0.133 | Acc: 91.844% | Wgt Acc: 97.097%
I - num batch: 222
I - Train -- Loss: 0.131 | Acc: 92.050% | Wgt Acc: 97.167% | LR: 1.250000e-04 | Dur: 133.00s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   5.  99.]
 [  0. 576.   0.   1.  39.]
 [  0.   2. 730.   1.  60.]
 [  2.   0.   0. 527.  61.]
 [  4.   0.   4.   4. 741.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.023 | Acc: 68.836% | Wgt Acc: 65.067% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   6.   1.  16.  11.]
 [  0.  39.   2.   1.   8.]
 [  0.  14.  45.   1.  13.]
 [ 10.   3.  12.  57.   7.]
 [ 11.  16.  15.  11. 141.]]

I - Local maximum validation set accuracy:  68.84

I - Validation set results: 
[14-1-2-1.39][50-3-4-1.71][124-2-2-1.03][127-0-0-8.04][443-2-2-2.34][567-0-0-2.64][573-1-4--0.20][615-0-0-3.65][695-1-2-2.70][722-3-0-3.96]
[826-0-0-4.57][878-0-0-3.15][1103-0-0-3.55][1212-3-3-1.59][1368-0-0-5.42][2181-2-3-2.00][2476-2-2-2.53][2721-2-2-2.84][2818-1-0-0.62][2886-2-4-1.14]
[3231-2-2-7.71][3333-2-2-1.52][3482-2-2-2.79][3536-3-3-2.16][3625-1-1-1.76][3909-0-0-2.84][4035-0-0-3.32][4140-0-0-3.93][4214-1-3-1.06][4346-1-3-0.51]
[4581-2-2-1.61][4708-3-4-1.42][4838-3-3-2.06][4845-1-4-0.78][4868-0-0-5.93][4939-0-3-0.82][4984-2-2-4.23][5078-1-4-2.07][5396-0-0-5.72][5479-1-1-5.94]
[5717-0-0-5.30][5843-1-1-0.09][5949-3-3-1.74][5987-2-4-2.96][6014-3-3-1.73][6033-3-4-0.64][6313-0-0-4.42][6421-3-3-4.39][6500-1-4-0.97][6583-3-3-3.09]
[6683-3-3-2.47][6825-2-1-1.87][6998-3-3-1.20][7049-3-3-3.97][7517-1-1-3.28][7521-1-1-1.65][7528-1-0--0.64][7949-1-2-2.37][8135-1-0-2.05][8185-3-0-4.30]
[8269-3-1-3.08][8273-3-3-4.49][8543-3-0-5.34][8666-1-1-6.16][8672-0-0-3.51][8903-1-2-3.02][9001-2-2-0.63][9036-2-2-5.63][9281-3-3-1.00][9300-2-2-9.34]
[9571-0-4-1.01][9617-1-4-2.00][9644-2-2-4.10][9705-2-4--0.13][9801-0-3-3.07][9803-3-3-1.56][9865-3-3-6.94][9896-2-2-1.25][10314-1-0-0.58][10337-3-3-6.09]
[10403-0-4-1.39][10653-2-4-0.95][10704-2-2-0.68][10719-1-1-5.61][10727-1-4-0.70][10836-0-0-10.60][10969-2-3-2.23][11042-0-0-2.05][11088-1-1-5.22][11322-0-0-5.48]
[11398-2-2-8.62][11499-0-0-3.83][11502-3-3-0.64][11512-3-3-2.92][11608-1-2-2.22][11610-0-0-2.61][11692-0-0-2.82][11905-0-0-5.48][11993-1-1-5.21][12002-2-3-2.45]
[12052-0-0-3.61][12201-0-3-3.01][12235-2-4-2.21][12320-1-4-2.26][12377-2-4-2.49][12398-2-3-1.65][12503-1-4-1.94][12617-0-3-1.71][12685-3-3-1.29][12738-2-3-0.09]
[12742-2-2-7.38][12823-0-3-2.61][13110-1-2-2.05][13240-3-3-2.79][13253-1-1-1.76][13273-0-0-8.49][13634-1-4-1.35][13763-2-2-3.34][13905-3-0-0.71][14060-2-4-2.17]
[14065-3-0-2.18][14147-3-3-3.35][14595-2-2-1.65][14687-2-2-8.03][14788-2-2-3.34][14869-1-1-4.83][14872-3-4-1.97][14877-1-1-3.67][14927-0-3-3.12][15066-0-0-5.56]
[15175-1-1-2.34][15178-2-3-1.44][15375-3-0-1.73][15389-3-3-3.67][15568-2-4-1.12][15675-3-3-5.67][15869-1-0-2.21][16207-3-0-2.35][16236-0-0-3.89][16302-3-3-1.48]
[16331-2-2-11.83][16381-0-4-2.39][16488-1-1-6.71][16495-0-0-3.38][16650-0-0-7.13][16719-1-2-2.39][16801-0-0-7.08][16828-0-0-4.20][17137-3-3-2.11][17245-1-4--0.24]
[17278-3-0-0.83][17282-0-0-1.93][17311-2-2-1.90][17336-2-1-4.94][17608-3-3-7.42][17627-0-0-0.05][17877-3-0-1.42][17924-1-2-1.52][17984-3-0-3.62][18211-0-3-2.88]
[18276-3-3-2.57][18287-1-1-0.70][18394-0-0-4.94][18428-0-0-8.06][18442-0-3-3.72][18478-3-3-2.34][18607-0-0-2.77][18616-0-0-2.92][18663-0-0-4.42][18718-0-0-6.36]
[18766-2-2-3.49][18824-2-4-3.22][18890-3-3-3.34][18930-3-4-1.27][18938-3-3-2.26][19817-1-1-3.79][19839-0-0-0.89][19930-3-3-3.16][19944-0-4-2.37][20036-2-2-6.96]
[20101-3-3-2.50][20474-1-1-1.57][20547-3-0-1.42][20929-2-2-5.49][21245-1-2-1.04][21257-3-3-1.63][21293-1-1-3.57][21316-1-1-4.17][21384-1-1-2.76][21448-1-1-4.96]
[21483-0-0-4.57][21487-2-2-2.52][21714-0-0-1.52][21943-3-4-1.24][21947-0-0-4.05][21948-0-0-7.98][21965-2-2-1.03][21998-1-1-5.26][22025-0-4-4.29][22228-3-3-7.20]
[22446-1-1-4.33][22494-3-3-3.72][22757-0-0-6.29][22811-3-3-2.45][22976-3-4-3.18][22985-3-3-3.24][23014-0-0-3.11][23112-1-1-4.18][23144-3-3-6.79][23168-2-3-0.96]
[23219-0-0-2.29][23363-3-3-5.09][23470-0-0-1.62][23486-2-4-2.08][23497-0-3-5.75][23516-0-0-5.38][23690-1-4-2.74][23921-2-2-2.08][23936-1-2-5.58][24040-3-2-0.40]
[24111-1-4-1.69][24182-0-0-6.56][24238-3-3-4.09][24290-2-0-3.59][24345-0-0-1.68][24364-1-2-1.32][24427-3-0-3.16][24477-2-2-4.15][24495-2-4-1.29][24893-2-2-4.13]
[25012-1-4-0.46][25121-2-2-5.07][25165-3-3-2.03][25183-0-4-1.64][25297-3-3-4.67][25398-0-0-5.50][25574-2-2-3.53][25644-1-2-5.02][25718-1-4-1.01][25774-2-4-2.66]
[26032-3-3-5.38][26051-3-3-5.39][26120-0-4-1.77][26321-1-1-4.93][26732-1-1-2.91][26784-3-3-7.36][26827-3-3-3.00][26833-0-3-3.65][26838-2-3-0.26][26860-1-2-1.10]
[26948-0-0-3.28][27049-3-0-3.10][27098-1-0-2.04][27526-0-0-4.93][27639-3-3-3.57][27698-3-0-3.27][27772-0-0-4.94][27890-1-1-3.33][28040-0-0-1.52][28503-2-2-4.88]
[28577-1-1-5.83][28959-0-0-5.75][29198-3-4-3.16][29777-0-0-8.01][29877-2-2-3.37][30035-1-1-4.34][30098-0-0-1.69][30326-1-1-4.53][30572-2-2-4.62][30716-0-4-2.77]
[30806-2-3-1.97][30906-1-1-7.78][31007-0-4-1.87][31181-3-3-2.46][31238-0-0-2.31][31347-0-0-5.04][31422-2-2-1.27][31429-3-3-1.96][31431-0-0-2.61][31432-1-1-4.94]
[31477-0-0-3.98][31524-1-2-0.04][31597-1-1-1.83][31619-1-4-0.81][31701-0-0-1.79][31755-0-0-5.10][31854-3-3-2.92][32074-1-1-2.33][32078-3-3-4.99][32111-1-1-3.15]
[32127-1-1-3.95][32140-3-3-3.29][32263-2-2-0.88][32365-0-0-5.87][32411-2-3-4.61][32429-3-3-2.74][32473-3-0-1.77][32574-3-0-2.50][32584-0-4-3.84][32622-0-4-1.40]
[32858-3-3-3.73][32969-3-3-4.77][33016-2-2-6.68][33031-1-3-4.30][33035-2-2-2.71][33133-2-2-3.04][33173-2-2-3.03][33175-3-4-2.58][33306-3-3-2.71][33309-2-3-2.39]
[33474-0-0-3.06][33478-2-4-0.16][33618-1-1-1.76][33712-0-0-2.71][33782-2-4-1.68][33914-3-3-5.01][34076-3-4-2.13][34112-2-2-4.36][34138-2-3-1.47][34239-1-1-0.84]
[34364-2-2-4.51][34617-1-4-3.90][34751-3-3-2.08][34783-2-4-1.13][35015-3-4-1.68][35018-1-1-4.79][35288-2-2-1.05][0-4-4-4.14][1-4-4-5.25][2-4-4-2.11]
[3-4-4-3.08][4-4-4-2.01][5-4-1-0.89][6-4-0-2.92][7-4-4-1.90][8-4-2-1.48][9-4-4-1.67][10-4-4-4.54][11-4-4-4.85][12-4-4-1.80]
[14-4-4-2.44][15-4-3-3.33][16-4-4-0.70][17-4-4-2.37][18-4-4-2.04][19-4-3-2.55][20-4-4-1.56][21-4-4-1.18][22-4-4-2.69][23-4-4-1.02]
[24-4-4-6.82][25-4-4-2.20][26-4-3-0.78][27-4-4-3.64][28-4-4-4.37][29-4-1-0.63][30-4-4-0.64][31-4-4-3.30][32-4-4-1.97][33-4-4-2.52]
[34-4-4-2.90][35-4-4-2.73][37-4-4-3.01][39-4-0-1.61][40-4-4-0.90][41-4-4-1.94][42-4-4-0.80][43-4-4-2.26][45-4-4-0.97][46-4-4-4.90]
[47-4-4-5.30][48-4-4-1.30][51-4-4-3.56][52-4-4-1.57][53-4-4-0.76][54-4-3-1.75][55-4-4-3.38][56-4-1-0.19][57-4-3-1.89][58-4-2-3.62]
[59-4-0-2.88][60-4-4-3.91][61-4-4-4.54][62-4-4-1.43][63-4-2-6.49][64-4-4-1.88][65-4-4-3.86][66-4-4-4.01][67-4-2-2.97][68-4-4-2.45]
[69-4-4--0.01][70-4-4-3.11][72-4-1-1.25][73-4-1-2.40][74-4-2-2.06][75-4-3-1.63][77-4-4-3.91][78-4-4-0.76][79-4-4-3.96][80-4-4-2.55]
[81-4-1-1.86][82-4-4-1.94][83-4-4-2.54][84-4-4-3.21][85-4-4-5.05][86-4-2-0.71][87-4-4-2.46][88-4-4-2.97][89-4-2-1.34][90-4-4-2.23]
[91-4-4-2.14][92-4-4-2.36][93-4-0-2.13][94-4-4-3.12][95-4-4-1.71][96-4-4-2.52][97-4-4-5.76][98-4-2-2.03][99-4-4-2.36][100-4-1-1.59]
[101-4-4-5.22][102-4-4-3.45][103-4-4-0.61][104-4-4-2.55][105-4-4-3.13][106-4-4-4.06][107-4-4-1.92][108-4-4-2.80][109-4-4-3.76][110-4-4-2.58]
[111-4-0-4.44][112-4-4-1.42][113-4-4-1.94][114-4-4-1.19][115-4-4-1.73][116-4-0-1.55][117-4-4-4.04][119-4-4-2.56][121-4-4-3.84][122-4-4-3.94]
[124-4-4-2.31][125-4-4-4.12][126-4-4-5.76][127-4-2-2.71][128-4-4-0.84][129-4-4-2.24][130-4-4-3.08][131-4-2-2.28][132-4-4-0.82][133-4-4-4.83]
[135-4-4-3.80][136-4-4-1.08][137-4-4-1.40][138-4-4-1.78][139-4-4-3.14][140-4-4-2.05][141-4-0-1.17][142-4-4-4.19][143-4-4-4.65][144-4-4-3.30]
[145-4-4-4.37][148-4-0-5.56][149-4-4-3.22][150-4-4-6.87][151-4-4-3.24][152-4-4-2.23][153-4-4-2.88][154-4-4-5.55][155-4-4-4.17][156-4-3-1.72]
[157-4-0-1.28][158-4-4-2.67][160-4-4-0.34][161-4-2-2.53][162-4-4-0.86][164-4-4-2.11][165-4-4-2.33][167-4-4-3.03][168-4-4-1.56][170-4-4-2.69]
[171-4-4-4.23][172-4-4-4.03][173-4-4-4.80][174-4-0-4.62][175-4-4-1.43][177-4-4-4.43][178-4-4-2.90][179-4-4-3.50][180-4-4-1.79][181-4-4-2.43]
[182-4-4-2.28][183-4-4-3.46][184-4-4-2.06][186-4-4-2.23][187-4-2-1.83][188-4-4-3.28][189-4-4-0.68][190-4-4-0.58][191-4-4-4.77][192-4-4-2.25]
[193-4-1-1.79][194-4-4-1.21][195-4-0-1.08][196-4-4-1.68][197-4-4-2.89][198-4-4-6.03][199-4-2-1.07]
---------------------------
I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.121 | Acc: 91.250% | Wgt Acc: 97.252%
	I - Batch: 100 | Loss: 0.125 | Acc: 91.938% | Wgt Acc: 97.210%
	I - Batch: 150 | Loss: 0.128 | Acc: 91.583% | Wgt Acc: 97.106%
	I - Batch: 200 | Loss: 0.130 | Acc: 91.688% | Wgt Acc: 97.081%
I - num batch: 222
I - Train -- Loss: 0.130 | Acc: 91.542% | Wgt Acc: 97.018% | LR: 1.250000e-04 | Dur: 137.86s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   3.  98.]
 [  0. 575.   0.   1.  49.]
 [  1.   2. 728.   0.  77.]
 [  1.   0.   1. 533.  52.]
 [  8.   1.   5.   1. 724.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.051 | Acc: 67.061% | Wgt Acc: 60.650% | Dur: 14.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   1.  13.   7.]
 [  1.  37.   5.   1.   6.]
 [  0.  17.  35.   1.  12.]
 [ 13.   1.  13.  60.   4.]
 [ 17.  21.  21.  11. 151.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.119 | Acc: 91.750% | Wgt Acc: 97.140%
	I - Batch: 100 | Loss: 0.123 | Acc: 92.438% | Wgt Acc: 97.279%
	I - Batch: 150 | Loss: 0.131 | Acc: 92.042% | Wgt Acc: 97.076%
	I - Batch: 200 | Loss: 0.133 | Acc: 91.844% | Wgt Acc: 96.953%
I - num batch: 222
I - Train -- Loss: 0.134 | Acc: 91.655% | Wgt Acc: 96.930% | LR: 1.250000e-04 | Dur: 134.03s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   2.   0.   5.  98.]
 [  1. 575.   2.   0.  44.]
 [  1.   0. 726.   0.  66.]
 [  2.   0.   0. 530.  60.]
 [  5.   1.   6.   3. 732.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.051 | Acc: 67.456% | Wgt Acc: 63.580% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   5.  15.  15.]
 [  0.  37.   5.   0.   4.]
 [  0.  14.  41.   4.  13.]
 [ 10.   2.  10.  56.   9.]
 [  9.  21.  14.  11. 139.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.115 | Acc: 92.875% | Wgt Acc: 97.460%
	I - Batch: 100 | Loss: 0.115 | Acc: 93.062% | Wgt Acc: 97.522%
	I - Batch: 150 | Loss: 0.117 | Acc: 92.667% | Wgt Acc: 97.307%
	I - Batch: 200 | Loss: 0.122 | Acc: 92.312% | Wgt Acc: 97.060%
I - num batch: 222
I - Train -- Loss: 0.126 | Acc: 92.078% | Wgt Acc: 96.960% | LR: 1.250000e-04 | Dur: 135.78s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   1.   4.  74.]
 [  0. 574.   0.   0.  42.]
 [  0.   0. 727.   1.  69.]
 [  2.   0.   1. 530.  65.]
 [ 10.   4.   5.   3. 750.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.997 | Acc: 68.047% | Wgt Acc: 61.377% | Dur: 20.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   2.  20.  12.]
 [  0.  43.   6.   1.   7.]
 [  0.   7.  34.   0.   6.]
 [  8.   3.   8.  49.   2.]
 [ 14.  22.  25.  16. 153.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.125 | Acc: 91.375% | Wgt Acc: 97.254%
	I - Batch: 100 | Loss: 0.124 | Acc: 91.875% | Wgt Acc: 97.033%
	I - Batch: 150 | Loss: 0.128 | Acc: 91.708% | Wgt Acc: 97.031%
	I - Batch: 200 | Loss: 0.130 | Acc: 91.656% | Wgt Acc: 97.033%
I - num batch: 222
I - Train -- Loss: 0.129 | Acc: 91.739% | Wgt Acc: 97.101% | LR: 1.250000e-04 | Dur: 135.26s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   1.   2.  94.]
 [  0. 576.   0.   0.  38.]
 [  0.   0. 726.   0.  87.]
 [  2.   0.   0. 533.  51.]
 [  6.   2.   7.   3. 730.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 67.850% | Wgt Acc: 64.249% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 76.   6.   4.  23.  19.]
 [  0.  49.   6.   1.   9.]
 [  1.  10.  34.   1.   9.]
 [  4.   2.  12.  47.   5.]
 [  7.  11.  19.  14. 138.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.118 | Acc: 92.125% | Wgt Acc: 97.437%
	I - Batch: 100 | Loss: 0.121 | Acc: 92.188% | Wgt Acc: 97.427%
	I - Batch: 150 | Loss: 0.127 | Acc: 91.750% | Wgt Acc: 97.214%
	I - Batch: 200 | Loss: 0.129 | Acc: 91.812% | Wgt Acc: 97.112%
I - num batch: 222
I - Train -- Loss: 0.129 | Acc: 91.852% | Wgt Acc: 97.140% | LR: 1.250000e-04 | Dur: 137.29s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   1.   0.   0.  90.]
 [  3. 575.   0.   1.  43.]
 [  1.   0. 728.   0.  80.]
 [  2.   0.   0. 536.  53.]
 [  6.   2.   6.   1. 734.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.998 | Acc: 66.667% | Wgt Acc: 61.089% | Dur: 14.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   4.   5.  26.  14.]
 [  0.  39.   3.   0.   5.]
 [  0.  15.  36.   1.   8.]
 [  8.   2.  10.  47.   8.]
 [  9.  18.  21.  12. 145.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.116 | Acc: 92.000% | Wgt Acc: 97.207%
	I - Batch: 100 | Loss: 0.116 | Acc: 92.688% | Wgt Acc: 97.402%
	I - Batch: 150 | Loss: 0.117 | Acc: 92.625% | Wgt Acc: 97.434%
	I - Batch: 200 | Loss: 0.120 | Acc: 92.250% | Wgt Acc: 97.231%
I - num batch: 222
I - Train -- Loss: 0.121 | Acc: 92.360% | Wgt Acc: 97.241% | LR: 1.250000e-04 | Dur: 132.87s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   1.   1.   1.  95.]
 [  0. 573.   0.   0.  29.]
 [  1.   0. 729.   0.  69.]
 [  3.   1.   0. 534.  54.]
 [  6.   3.   4.   3. 753.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.061 | Acc: 68.245% | Wgt Acc: 62.104% | Dur: 14.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   1.  10.   7.]
 [  0.  33.   1.   1.   3.]
 [  2.  14.  44.   2.  15.]
 [  8.   3.   8.  56.   4.]
 [ 16.  26.  21.  17. 151.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.112 | Acc: 93.625% | Wgt Acc: 97.753%
	I - Batch: 100 | Loss: 0.117 | Acc: 93.188% | Wgt Acc: 97.781%
	I - Batch: 150 | Loss: 0.120 | Acc: 93.042% | Wgt Acc: 97.639%
	I - Batch: 200 | Loss: 0.121 | Acc: 92.719% | Wgt Acc: 97.415%
I - num batch: 222
I - Train -- Loss: 0.122 | Acc: 92.613% | Wgt Acc: 97.448% | LR: 1.250000e-04 | Dur: 136.84s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   3.  86.]
 [  0. 577.   2.   1.  36.]
 [  1.   0. 729.   0.  73.]
 [  0.   1.   0. 532.  48.]
 [  6.   0.   3.   2. 757.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 67.456% | Wgt Acc: 62.784% | Dur: 16.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   3.   4.  18.  18.]
 [  0.  40.   5.   0.   3.]
 [  0.  10.  38.   0.  11.]
 [ 12.   1.   8.  56.   5.]
 [ 11.  24.  20.  12. 143.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.113 | Acc: 93.000% | Wgt Acc: 97.915%
	I - Batch: 100 | Loss: 0.121 | Acc: 92.938% | Wgt Acc: 97.548%
	I - Batch: 150 | Loss: 0.119 | Acc: 92.917% | Wgt Acc: 97.560%
	I - Batch: 200 | Loss: 0.119 | Acc: 92.781% | Wgt Acc: 97.460%
I - num batch: 222
I - Train -- Loss: 0.121 | Acc: 92.698% | Wgt Acc: 97.353% | LR: 1.250000e-04 | Dur: 134.99s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   1.   0.   3.  80.]
 [  1. 576.   0.   0.  36.]
 [  1.   0. 730.   1.  74.]
 [  0.   1.   1. 530.  46.]
 [  7.   0.   3.   4. 764.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.025 | Acc: 66.469% | Wgt Acc: 61.481% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   7.   2.  20.  12.]
 [  0.  42.   7.   0.   5.]
 [  2.  11.  39.   1.  17.]
 [  8.   1.   9.  47.   4.]
 [ 11.  17.  18.  18. 142.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.135 | Acc: 92.125% | Wgt Acc: 97.074%
	I - Batch: 100 | Loss: 0.126 | Acc: 92.875% | Wgt Acc: 97.488%
	I - Batch: 150 | Loss: 0.129 | Acc: 92.583% | Wgt Acc: 97.348%
	I - Batch: 200 | Loss: 0.128 | Acc: 92.625% | Wgt Acc: 97.300%
I - num batch: 222
I - Train -- Loss: 0.128 | Acc: 92.388% | Wgt Acc: 97.189% | LR: 1.250000e-04 | Dur: 135.38s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   0.   2. 101.]
 [  0. 574.   0.   1.  40.]
 [  1.   2. 732.   0.  52.]
 [  3.   0.   0. 532.  51.]
 [ 10.   2.   2.   3. 756.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.998 | Acc: 68.442% | Wgt Acc: 63.441% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   2.  11.  13.]
 [  0.  38.   3.   0.   3.]
 [  1.  14.  39.   1.  12.]
 [ 12.   3.  14.  64.   5.]
 [ 16.  21.  17.  10. 147.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.136 | Acc: 90.500% | Wgt Acc: 96.599%
	I - Batch: 100 | Loss: 0.127 | Acc: 91.375% | Wgt Acc: 97.079%
	I - Batch: 150 | Loss: 0.125 | Acc: 91.958% | Wgt Acc: 97.381%
	I - Batch: 200 | Loss: 0.121 | Acc: 92.156% | Wgt Acc: 97.450%
I - num batch: 222
I - Train -- Loss: 0.123 | Acc: 92.162% | Wgt Acc: 97.437% | LR: 1.250000e-04 | Dur: 135.74s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   1.  85.]
 [  1. 578.   0.   0.  38.]
 [  0.   0. 733.   1.  71.]
 [  1.   0.   0. 532.  69.]
 [  6.   0.   1.   4. 737.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.183 | Acc: 61.341% | Wgt Acc: 58.448% | Dur: 14.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   3.   1.  10.   7.]
 [  0.  35.   3.   0.   8.]
 [  3.  23.  53.  12.  36.]
 [ 15.   1.   2.  47.   6.]
 [ 17.  16.  16.  17. 123.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.100 | Acc: 93.875% | Wgt Acc: 98.114%
	I - Batch: 100 | Loss: 0.122 | Acc: 92.312% | Wgt Acc: 97.170%
	I - Batch: 150 | Loss: 0.121 | Acc: 92.208% | Wgt Acc: 97.169%
	I - Batch: 200 | Loss: 0.123 | Acc: 91.938% | Wgt Acc: 97.041%
I - num batch: 222
I - Train -- Loss: 0.123 | Acc: 91.655% | Wgt Acc: 96.951% | LR: 1.250000e-04 | Dur: 136.02s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   0.   3. 104.]
 [  0. 577.   0.   0.  39.]
 [  1.   0. 731.   1.  68.]
 [  3.   0.   0. 527.  58.]
 [  8.   1.   3.   7. 731.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.018 | Acc: 69.428% | Wgt Acc: 62.207% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   2.   1.  11.   4.]
 [  1.  42.   5.   0.   6.]
 [  1.   9.  34.   0.   5.]
 [ 14.   2.   9.  59.   6.]
 [ 14.  23.  26.  16. 159.]]

I - Local maximum validation set accuracy:  69.43

I - Validation set results: 
[14-1-2-1.36][50-3-4-1.63][124-2-4-1.47][127-0-0-6.97][443-2-2-2.19][567-0-0-2.63][573-1-1-3.04][615-0-0-2.01][695-1-2-3.00][722-3-0-4.66]
[826-0-0-5.64][878-0-0-3.22][1103-0-4-2.58][1212-3-3-2.29][1368-0-0-4.46][2181-2-3-1.07][2476-2-2-3.24][2721-2-2-2.33][2818-1-1-2.50][2886-2-4-1.48]
[3231-2-2-6.91][3333-2-2-2.13][3482-2-2-1.95][3536-3-4-2.01][3625-1-1-0.68][3909-0-0-1.69][4035-0-0-3.22][4140-0-0-2.47][4214-1-3-2.75][4346-1-4-1.93]
[4581-2-1-2.23][4708-3-4-1.26][4838-3-4-1.34][4845-1-4-1.31][4868-0-0-6.37][4939-0-4-0.81][4984-2-2-1.44][5078-1-4-1.30][5396-0-0-5.88][5479-1-1-5.46]
[5717-0-0-2.52][5843-1-4-0.94][5949-3-3-1.94][5987-2-4-3.72][6014-3-3-1.84][6033-3-4-0.95][6313-0-0-2.87][6421-3-3-3.36][6500-1-4-1.85][6583-3-3-2.02]
[6683-3-3-2.71][6825-2-1-3.63][6998-3-3-0.16][7049-3-3-4.38][7517-1-1-3.01][7521-1-1-3.10][7528-1-4--0.54][7949-1-4-2.10][8135-1-4-1.24][8185-3-0-2.95]
[8269-3-4-0.95][8273-3-3-4.56][8543-3-0-5.61][8666-1-1-3.44][8672-0-0-7.28][8903-1-1-1.51][9001-2-4-1.45][9036-2-2-3.79][9281-3-0-1.08][9300-2-2-7.67]
[9571-0-4-1.89][9617-1-4-2.61][9644-2-2-3.39][9705-2-4-0.93][9801-0-3-3.28][9803-3-3-1.35][9865-3-3-6.41][9896-2-2-2.26][10314-1-4-1.97][10337-3-3-4.98]
[10403-0-4-1.71][10653-2-4-2.56][10704-2-4-0.93][10719-1-1-3.69][10727-1-4-2.04][10836-0-0-10.11][10969-2-3-1.94][11042-0-0-2.62][11088-1-1-5.52][11322-0-0-5.34]
[11398-2-2-3.25][11499-0-0-4.22][11502-3-3-1.46][11512-3-3-2.60][11608-1-1-2.06][11610-0-0-0.77][11692-0-3-2.54][11905-0-0-5.84][11993-1-1-5.32][12002-2-3-0.36]
[12052-0-0-2.81][12201-0-3-3.32][12235-2-4-1.98][12320-1-4-3.61][12377-2-4-3.13][12398-2-3-2.44][12503-1-4-2.03][12617-0-3-0.57][12685-3-4-3.37][12738-2-4-1.05]
[12742-2-2-6.09][12823-0-3-1.68][13110-1-4-1.00][13240-3-3-3.72][13253-1-1-2.57][13273-0-0-6.78][13634-1-4-1.60][13763-2-2-1.65][13905-3-3-1.87][14060-2-4-2.36]
[14065-3-0-2.03][14147-3-3-2.73][14595-2-2-1.08][14687-2-2-5.68][14788-2-2-5.04][14869-1-1-3.12][14872-3-4-2.15][14877-1-1-4.01][14927-0-3-2.92][15066-0-0-6.11]
[15175-1-1-3.06][15178-2-3-2.52][15375-3-0-1.71][15389-3-3-4.01][15568-2-4-1.74][15675-3-3-4.87][15869-1-0-2.66][16207-3-0-1.51][16236-0-2-0.42][16302-3-3-1.97]
[16331-2-2-11.28][16381-0-3-2.01][16488-1-1-7.01][16495-0-0-7.03][16650-0-0-5.27][16719-1-2-2.59][16801-0-0-6.59][16828-0-0-4.89][17137-3-3-1.99][17245-1-4-1.94]
[17278-3-0-0.69][17282-0-0-1.31][17311-2-2-3.04][17336-2-1-0.78][17608-3-3-7.23][17627-0-0-3.20][17877-3-3-1.47][17924-1-2-1.52][17984-3-3-2.76][18211-0-3-1.09]
[18276-3-3-1.29][18287-1-1-1.06][18394-0-0-4.17][18428-0-0-6.57][18442-0-3-3.20][18478-3-3-2.78][18607-0-0-2.15][18616-0-0-4.40][18663-0-0-4.56][18718-0-0-5.55]
[18766-2-2-3.56][18824-2-4-3.23][18890-3-3-3.56][18930-3-4-2.68][18938-3-3-2.09][19817-1-1-2.14][19839-0-1-0.16][19930-3-3-2.54][19944-0-4-2.44][20036-2-2-5.65]
[20101-3-4-1.09][20474-1-1-1.61][20547-3-3-2.54][20929-2-2-3.29][21245-1-1-1.11][21257-3-3-0.93][21293-1-1-4.52][21316-1-1-4.62][21384-1-1-1.30][21448-1-1-4.08]
[21483-0-0-3.06][21487-2-2-2.69][21714-0-0-1.70][21943-3-3-1.52][21947-0-0-5.04][21948-0-0-7.38][21965-2-1-2.02][21998-1-1-5.43][22025-0-4-4.22][22228-3-3-6.00]
[22446-1-1-4.71][22494-3-3-3.24][22757-0-0-5.29][22811-3-3-5.27][22976-3-4-1.11][22985-3-3-3.41][23014-0-0-2.85][23112-1-1-3.96][23144-3-3-5.01][23168-2-4-1.25]
[23219-0-4-1.83][23363-3-3-3.26][23470-0-0-2.58][23486-2-4-1.71][23497-0-3-4.05][23516-0-0-5.31][23690-1-2-0.72][23921-2-2-1.50][23936-1-2-1.46][24040-3-4-0.67]
[24111-1-4-2.43][24182-0-0-6.47][24238-3-3-4.14][24290-2-0-1.96][24345-0-4-0.77][24364-1-2-0.81][24427-3-0-3.02][24477-2-2-3.83][24495-2-4-1.27][24893-2-2-3.14]
[25012-1-4-2.01][25121-2-2-4.38][25165-3-3-3.43][25183-0-0-2.00][25297-3-3-4.09][25398-0-0-4.31][25574-2-4-2.98][25644-1-2-4.49][25718-1-4-1.64][25774-2-4-1.97]
[26032-3-3-4.53][26051-3-3-5.69][26120-0-4-2.74][26321-1-1-4.92][26732-1-1-3.35][26784-3-3-7.31][26827-3-3-2.13][26833-0-3-5.26][26838-2-4-0.46][26860-1-2-1.63]
[26948-0-0-2.50][27049-3-0-3.05][27098-1-0-1.84][27526-0-0-4.24][27639-3-3-2.89][27698-3-3-1.80][27772-0-0-3.12][27890-1-1-5.21][28040-0-3-0.30][28503-2-2-6.28]
[28577-1-1-4.41][28959-0-0-4.46][29198-3-4-2.55][29777-0-0-7.73][29877-2-2-0.66][30035-1-1-4.40][30098-0-3-1.57][30326-1-1-5.59][30572-2-2-3.18][30716-0-4-2.81]
[30806-2-3-2.67][30906-1-1-3.86][31007-0-4-2.34][31181-3-3-1.33][31238-0-0-2.85][31347-0-0-4.48][31422-2-4-1.82][31429-3-3-1.71][31431-0-0-1.79][31432-1-1-3.97]
[31477-0-3-3.26][31524-1-1-1.89][31597-1-1-1.17][31619-1-4-1.82][31701-0-0-2.81][31755-0-0-3.57][31854-3-3-3.39][32074-1-1-3.70][32078-3-3-4.18][32111-1-1-4.42]
[32127-1-1-4.02][32140-3-3-3.16][32263-2-4-1.70][32365-0-0-4.11][32411-2-3-5.60][32429-3-0-2.64][32473-3-3-1.92][32574-3-3-3.17][32584-0-4-2.80][32622-0-4-2.40]
[32858-3-3-2.94][32969-3-3-3.26][33016-2-2-2.71][33031-1-3-3.96][33035-2-4-2.43][33133-2-2-1.77][33173-2-2-1.65][33175-3-4-2.24][33306-3-3-2.40][33309-2-3-1.71]
[33474-0-4-1.84][33478-2-4-1.44][33618-1-4-2.78][33712-0-0-2.30][33782-2-4-3.27][33914-3-3-4.46][34076-3-4-2.64][34112-2-3-2.21][34138-2-4-0.31][34239-1-1-1.60]
[34364-2-2-5.58][34617-1-4-4.06][34751-3-3-2.46][34783-2-4-3.07][35015-3-4-2.57][35018-1-4-1.41][35288-2-1-1.74][0-4-4-3.64][1-4-4-3.46][2-4-4-2.57]
[3-4-4-3.17][4-4-4-2.77][5-4-4-1.09][6-4-4-2.58][7-4-4-3.08][8-4-4-0.86][9-4-0-1.96][10-4-4-4.93][11-4-4-6.59][12-4-4-2.00]
[14-4-4-3.24][15-4-3-3.64][16-4-4-0.77][17-4-4-3.15][18-4-4-2.33][19-4-4-3.00][20-4-4-2.29][21-4-4-2.52][22-4-4-2.24][23-4-4-2.08]
[24-4-4-6.01][25-4-4-2.30][26-4-4-2.27][27-4-4-3.08][28-4-4-4.50][29-4-1-2.30][30-4-4-1.88][31-4-4-2.22][32-4-4-1.92][33-4-4-1.60]
[34-4-4-3.57][35-4-4-3.96][37-4-4-2.44][39-4-4-1.18][40-4-4-1.30][41-4-4-2.30][42-4-4-2.31][43-4-4-2.76][45-4-4-1.00][46-4-4-3.84]
[47-4-4-5.38][48-4-4-3.32][51-4-4-3.45][52-4-4-2.72][53-4-4-2.31][54-4-4-3.12][55-4-4-3.57][56-4-1-1.58][57-4-3-1.09][58-4-2-2.83]
[59-4-4-3.21][60-4-4-3.44][61-4-4-3.09][62-4-4-2.33][63-4-4-3.21][64-4-4-1.67][65-4-4-4.77][66-4-4-4.62][67-4-2-0.39][68-4-4-2.60]
[69-4-4-1.52][70-4-4-2.78][72-4-4-1.99][73-4-4-1.21][74-4-4-2.52][75-4-3-1.22][77-4-4-4.56][78-4-1-0.44][79-4-4-5.74][80-4-4-2.41]
[81-4-4-2.31][82-4-4-2.24][83-4-4-3.02][84-4-4-6.00][85-4-4-4.70][86-4-4-2.56][87-4-4-3.61][88-4-4-3.63][89-4-4-1.18][90-4-4-2.16]
[91-4-4-3.26][92-4-4-1.70][93-4-4-1.28][94-4-4-3.77][95-4-4-2.10][96-4-4-1.38][97-4-4-5.32][98-4-4-0.79][99-4-4-1.83][100-4-4-2.04]
[101-4-4-5.46][102-4-4-2.13][103-4-4-1.51][104-4-4-1.96][105-4-4-2.47][106-4-4-3.16][107-4-4-2.50][108-4-4-2.68][109-4-4-2.16][110-4-4-2.61]
[111-4-3-2.87][112-4-4-0.88][113-4-4-1.40][114-4-4-0.67][115-4-4-2.96][116-4-4-1.96][117-4-4-3.28][119-4-4-5.12][121-4-4-3.35][122-4-4-3.85]
[124-4-4-2.91][125-4-4-4.65][126-4-4-4.99][127-4-4-1.30][128-4-4-1.87][129-4-1-2.63][130-4-4-3.13][131-4-4-1.75][132-4-3-1.99][133-4-4-5.07]
[135-4-4-3.05][136-4-4-2.05][137-4-4-1.80][138-4-4-2.13][139-4-4-2.60][140-4-1-2.21][141-4-4-0.93][142-4-4-4.02][143-4-4-4.21][144-4-4-2.62]
[145-4-4-2.52][148-4-0-5.26][149-4-4-2.92][150-4-4-6.20][151-4-4-4.16][152-4-4-2.72][153-4-4-2.97][154-4-4-6.84][155-4-4-4.91][156-4-3-1.32]
[157-4-0-1.18][158-4-4-3.80][160-4-4-1.50][161-4-2-3.26][162-4-4-1.04][164-4-4-2.35][165-4-4-3.11][167-4-4-3.35][168-4-4-2.12][170-4-4-2.77]
[171-4-4-3.70][172-4-4-4.57][173-4-4-4.38][174-4-0-3.90][175-4-4-3.96][177-4-4-3.77][178-4-2-0.64][179-4-4-2.60][180-4-4-3.21][181-4-4-3.22]
[182-4-4-2.85][183-4-4-4.33][184-4-4-2.24][186-4-4-2.58][187-4-4-1.09][188-4-4-1.86][189-4-4-1.08][190-4-4-1.56][191-4-4-3.12][192-4-4-2.94]
[193-4-1-2.19][194-4-4-1.05][195-4-4-1.77][196-4-4-2.10][197-4-4-3.47][198-4-4-6.28][199-4-2-2.14]
---------------------------
I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.110 | Acc: 93.375% | Wgt Acc: 97.788%
	I - Batch: 100 | Loss: 0.106 | Acc: 93.438% | Wgt Acc: 97.972%
	I - Batch: 150 | Loss: 0.111 | Acc: 92.417% | Wgt Acc: 97.611%
	I - Batch: 200 | Loss: 0.109 | Acc: 92.594% | Wgt Acc: 97.529%
I - num batch: 222
I - Train -- Loss: 0.114 | Acc: 92.501% | Wgt Acc: 97.454% | LR: 1.250000e-04 | Dur: 138.24s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   1.   0.   2.  91.]
 [  0. 577.   1.   0.  36.]
 [  1.   0. 731.   0.  72.]
 [  2.   0.   1. 535.  49.]
 [  8.   0.   1.   1. 752.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.071 | Acc: 65.286% | Wgt Acc: 61.792% | Dur: 15.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   1.   2.  19.   9.]
 [  0.  41.   7.   1.   7.]
 [  2.  17.  45.   1.  28.]
 [  8.   1.   8.  49.   3.]
 [ 15.  18.  13.  16. 133.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.109 | Acc: 92.750% | Wgt Acc: 97.548%
	I - Batch: 100 | Loss: 0.115 | Acc: 92.688% | Wgt Acc: 97.331%
	I - Batch: 150 | Loss: 0.122 | Acc: 92.292% | Wgt Acc: 97.354%
	I - Batch: 200 | Loss: 0.119 | Acc: 92.469% | Wgt Acc: 97.349%
I - num batch: 222
I - Train -- Loss: 0.119 | Acc: 92.444% | Wgt Acc: 97.255% | LR: 1.250000e-04 | Dur: 135.63s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   0.   2.  87.]
 [  0. 573.   0.   0.  40.]
 [  0.   1. 733.   0.  60.]
 [  2.   1.   0. 531.  57.]
 [  9.   3.   1.   5. 756.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.065 | Acc: 67.061% | Wgt Acc: 60.501% | Dur: 14.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   3.  14.  15.]
 [  1.  32.   4.   0.   2.]
 [  0.  14.  38.   2.   8.]
 [ 12.   4.   9.  56.   4.]
 [ 12.  24.  21.  14. 151.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.108 | Acc: 92.125% | Wgt Acc: 97.200%
	I - Batch: 100 | Loss: 0.117 | Acc: 92.000% | Wgt Acc: 97.174%
	I - Batch: 150 | Loss: 0.119 | Acc: 92.125% | Wgt Acc: 97.195%
	I - Batch: 200 | Loss: 0.122 | Acc: 92.156% | Wgt Acc: 97.140%
I - num batch: 222
I - Train -- Loss: 0.120 | Acc: 92.219% | Wgt Acc: 97.201% | LR: 1.250000e-04 | Dur: 135.50s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   1.   2.  85.]
 [  1. 576.   0.   0.  34.]
 [  2.   1. 729.   0.  77.]
 [  2.   0.   1. 532.  56.]
 [  6.   1.   3.   4. 748.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.065 | Acc: 68.047% | Wgt Acc: 61.204% | Dur: 15.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   2.   1.   5.   4.]
 [  0.  36.   2.   0.   2.]
 [  1.  11.  44.   4.  13.]
 [ 22.   5.   6.  63.   6.]
 [ 18.  24.  22.  14. 155.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.113 | Acc: 92.875% | Wgt Acc: 97.828%
	I - Batch: 100 | Loss: 0.113 | Acc: 92.438% | Wgt Acc: 97.369%
	I - Batch: 150 | Loss: 0.111 | Acc: 92.458% | Wgt Acc: 97.482%
	I - Batch: 200 | Loss: 0.113 | Acc: 92.125% | Wgt Acc: 97.281%
I - num batch: 222
I - Train -- Loss: 0.116 | Acc: 92.134% | Wgt Acc: 97.255% | LR: 1.250000e-04 | Dur: 135.81s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   0. 105.]
 [  0. 574.   0.   1.  37.]
 [  1.   1. 732.   1.  59.]
 [  0.   0.   0. 531.  57.]
 [  7.   3.   2.   5. 742.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.997 | Acc: 67.258% | Wgt Acc: 62.680% | Dur: 14.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   2.  20.  15.]
 [  0.  38.   6.   1.   4.]
 [  0.  16.  38.   1.   9.]
 [ 11.   4.  10.  55.  10.]
 [  9.  16.  19.   9. 142.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.116 | Acc: 93.125% | Wgt Acc: 97.861%
	I - Batch: 100 | Loss: 0.115 | Acc: 92.438% | Wgt Acc: 97.700%
	I - Batch: 150 | Loss: 0.117 | Acc: 92.458% | Wgt Acc: 97.571%
	I - Batch: 200 | Loss: 0.117 | Acc: 92.500% | Wgt Acc: 97.490%
I - num batch: 222
I - Train -- Loss: 0.114 | Acc: 92.670% | Wgt Acc: 97.580% | LR: 1.250000e-04 | Dur: 136.18s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   3.  99.]
 [  0. 577.   0.   0.  33.]
 [  1.   0. 732.   0.  69.]
 [  0.   0.   0. 534.  44.]
 [  7.   1.   2.   1. 755.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.037 | Acc: 66.864% | Wgt Acc: 60.039% | Dur: 14.12s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   1.  13.   8.]
 [  0.  37.   2.   1.   4.]
 [  0.  12.  37.   2.  11.]
 [  9.   1.   9.  54.   5.]
 [ 20.  26.  26.  16. 152.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.115 | Acc: 93.625% | Wgt Acc: 97.528%
	I - Batch: 100 | Loss: 0.119 | Acc: 93.000% | Wgt Acc: 97.444%
	I - Batch: 150 | Loss: 0.124 | Acc: 92.583% | Wgt Acc: 97.346%
	I - Batch: 200 | Loss: 0.123 | Acc: 92.719% | Wgt Acc: 97.409%
I - num batch: 222
I - Train -- Loss: 0.121 | Acc: 92.783% | Wgt Acc: 97.374% | LR: 1.250000e-04 | Dur: 136.83s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   1.  66.]
 [  0. 577.   0.   0.  43.]
 [  2.   0. 731.   0.  73.]
 [  2.   0.   0. 529.  51.]
 [  6.   1.   3.   8. 767.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.048 | Acc: 67.850% | Wgt Acc: 65.586% | Dur: 15.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   2.   8.   8.]
 [  2.  45.  10.   2.  10.]
 [  1.  16.  45.   1.  23.]
 [ 13.   4.  10.  62.   6.]
 [ 13.  11.   8.  13. 133.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.106 | Acc: 91.500% | Wgt Acc: 97.221%
	I - Batch: 100 | Loss: 0.108 | Acc: 92.438% | Wgt Acc: 97.597%
	I - Batch: 150 | Loss: 0.106 | Acc: 92.958% | Wgt Acc: 97.792%
	I - Batch: 200 | Loss: 0.104 | Acc: 93.344% | Wgt Acc: 97.876%
I - num batch: 208
I - Train -- Loss: 0.104 | Acc: 93.299% | Wgt Acc: 97.852% | LR: 1.250000e-04 | Dur: 126.37s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   1.   0.   2.  68.]
 [  0. 575.   0.   0.  33.]
 [  0.   1. 732.   0.  57.]
 [  2.   0.   0. 534.  51.]
 [  3.   1.   2.   2. 572.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.069 | Acc: 64.694% | Wgt Acc: 61.665% | Dur: 14.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   6.   2.  13.  14.]
 [  1.  41.   5.   1.  10.]
 [  0.  13.  37.   1.  12.]
 [ 17.   4.  13.  60.  13.]
 [ 11.  14.  18.  11. 131.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.115 | Acc: 91.875% | Wgt Acc: 97.444%
	I - Batch: 100 | Loss: 0.117 | Acc: 92.188% | Wgt Acc: 97.435%
	I - Batch: 150 | Loss: 0.112 | Acc: 93.000% | Wgt Acc: 97.628%
	I - Batch: 200 | Loss: 0.111 | Acc: 93.094% | Wgt Acc: 97.638%
I - num batch: 222
I - Train -- Loss: 0.113 | Acc: 92.980% | Wgt Acc: 97.576% | LR: 1.250000e-04 | Dur: 135.33s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   1.   2.  85.]
 [  0. 576.   0.   0.  33.]
 [  1.   0. 730.   0.  63.]
 [  2.   0.   0. 533.  50.]
 [  4.   2.   3.   3. 769.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.096 | Acc: 65.286% | Wgt Acc: 62.288% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   5.   1.  11.  17.]
 [  0.  38.   3.   1.   5.]
 [  0.  10.  37.   0.   9.]
 [ 16.   3.  14.  63.  17.]
 [ 11.  22.  20.  11. 132.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.114 | Acc: 93.500% | Wgt Acc: 97.345%
	I - Batch: 100 | Loss: 0.107 | Acc: 94.250% | Wgt Acc: 97.809%
	I - Batch: 150 | Loss: 0.107 | Acc: 94.083% | Wgt Acc: 97.716%
	I - Batch: 200 | Loss: 0.109 | Acc: 93.719% | Wgt Acc: 97.719%
I - num batch: 222
I - Train -- Loss: 0.110 | Acc: 93.685% | Wgt Acc: 97.714% | LR: 1.250000e-04 | Dur: 132.49s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   1.  59.]
 [  0. 575.   1.   0.  40.]
 [  0.   1. 728.   0.  61.]
 [  3.   1.   3. 534.  44.]
 [  4.   1.   2.   3. 796.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.967 | Acc: 69.822% | Wgt Acc: 67.386% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   3.   3.  11.  10.]
 [  0.  45.   3.   0.   4.]
 [  0.  14.  42.   1.  19.]
 [ 10.   4.  10.  61.  10.]
 [  9.  12.  17.  13. 137.]]

I - Local maximum validation set accuracy:  69.82

I - Validation set results: 
[14-1-2-2.05][50-3-4-0.27][124-2-2-1.30][127-0-0-6.22][443-2-2-3.82][567-0-0-3.66][573-1-1-3.07][615-0-0-3.21][695-1-2-2.71][722-3-3-4.70]
[826-0-0-5.01][878-0-0-7.04][1103-0-0-2.59][1212-3-4-0.78][1368-0-0-5.01][2181-2-3-0.48][2476-2-2-3.77][2721-2-4-1.44][2818-1-3-0.59][2886-2-2-1.93]
[3231-2-2-8.26][3333-2-2-2.47][3482-2-2-1.91][3536-3-3-2.32][3625-1-1-3.29][3909-0-0-2.25][4035-0-0-2.94][4140-0-0-2.69][4214-1-3-2.88][4346-1-2-0.23]
[4581-2-2-2.93][4708-3-4-0.67][4838-3-3-2.46][4845-1-1-1.53][4868-0-0-4.98][4939-0-4-1.17][4984-2-2-3.90][5078-1-4-2.23][5396-0-0-5.36][5479-1-1-6.51]
[5717-0-0-2.79][5843-1-1-2.60][5949-3-0-1.75][5987-2-4-3.15][6014-3-3-2.23][6033-3-3-1.66][6313-0-0-1.64][6421-3-3-3.73][6500-1-1-0.48][6583-3-3-1.23]
[6683-3-3-1.84][6825-2-1-3.11][6998-3-3-0.62][7049-3-3-3.05][7517-1-1-4.76][7521-1-1-1.56][7528-1-4--0.33][7949-1-2-1.37][8135-1-3--0.02][8185-3-0-3.95]
[8269-3-2-1.84][8273-3-3-3.44][8543-3-0-6.47][8666-1-1-5.07][8672-0-0-6.49][8903-1-2-3.97][9001-2-2-1.83][9036-2-2-7.25][9281-3-0-0.33][9300-2-2-8.61]
[9571-0-0-1.05][9617-1-1-4.49][9644-2-2-4.71][9705-2-0-0.75][9801-0-3-3.31][9803-3-3-2.95][9865-3-3-7.08][9896-2-2-2.22][10314-1-4-2.18][10337-3-3-5.33]
[10403-0-4-1.48][10653-2-4-1.36][10704-2-2-1.86][10719-1-1-4.09][10727-1-1-1.54][10836-0-0-11.02][10969-2-3-2.35][11042-0-0-3.11][11088-1-1-7.17][11322-0-0-6.23]
[11398-2-4-0.82][11499-0-0-4.80][11502-3-3-1.04][11512-3-3-2.71][11608-1-1-2.04][11610-0-0-3.81][11692-0-0-3.13][11905-0-0-4.37][11993-1-1-2.26][12002-2-2--0.07]
[12052-0-0-4.41][12201-0-3-4.26][12235-2-4-2.30][12320-1-0-2.36][12377-2-4-2.14][12398-2-3-1.21][12503-1-2-2.50][12617-0-3--0.05][12685-3-4-0.79][12738-2-0-2.64]
[12742-2-2-8.67][12823-0-0-3.77][13110-1-2-5.50][13240-3-3-3.47][13253-1-1-4.12][13273-0-0-7.61][13634-1-4-0.39][13763-2-2-4.94][13905-3-3-0.78][14060-2-4-2.10]
[14065-3-3-1.76][14147-3-3-1.80][14595-2-2-3.23][14687-2-2-6.91][14788-2-2-3.61][14869-1-1-5.78][14872-3-3-1.96][14877-1-1-5.20][14927-0-3-2.78][15066-0-0-3.59]
[15175-1-1-1.69][15178-2-3-2.77][15375-3-3-2.64][15389-3-3-4.67][15568-2-4-1.80][15675-3-3-4.80][15869-1-4-0.91][16207-3-0-1.36][16236-0-0-2.21][16302-3-3-1.97]
[16331-2-2-12.21][16381-0-3-1.98][16488-1-1-2.51][16495-0-0-4.54][16650-0-0-5.67][16719-1-2-3.88][16801-0-0-8.23][16828-0-0-3.91][17137-3-3-2.52][17245-1-4-2.39]
[17278-3-4-1.87][17282-0-0-1.76][17311-2-2-4.30][17336-2-1-2.68][17608-3-3-7.69][17627-0-0-0.52][17877-3-0-1.49][17924-1-4-0.27][17984-3-3-5.05][18211-0-0-1.77]
[18276-3-3-2.39][18287-1-4-0.59][18394-0-0-5.38][18428-0-0-1.29][18442-0-3-3.01][18478-3-3-3.63][18607-0-0-2.74][18616-0-0-4.06][18663-0-0-5.04][18718-0-0-6.50]
[18766-2-2-2.75][18824-2-4-3.03][18890-3-3-4.35][18930-3-4-0.86][18938-3-3-1.05][19817-1-1-3.46][19839-0-0-2.08][19930-3-3-3.90][19944-0-4-1.80][20036-2-2-6.88]
[20101-3-4-0.45][20474-1-1-2.14][20547-3-3-1.82][20929-2-2-4.68][21245-1-2-1.68][21257-3-3-2.21][21293-1-1-4.78][21316-1-1-6.30][21384-1-1-5.32][21448-1-1-5.28]
[21483-0-0-4.23][21487-2-2-5.91][21714-0-0-0.58][21943-3-3-1.54][21947-0-0-3.75][21948-0-0-7.55][21965-2-2-4.68][21998-1-1-4.57][22025-0-4-3.49][22228-3-3-7.33]
[22446-1-1-6.07][22494-3-3-3.52][22757-0-0-6.62][22811-3-3-5.12][22976-3-4-3.37][22985-3-3-2.98][23014-0-0-4.73][23112-1-1-6.39][23144-3-3-5.97][23168-2-3-0.97]
[23219-0-0-1.59][23363-3-3-2.60][23470-0-0-3.50][23486-2-2-1.89][23497-0-3-3.46][23516-0-0-4.48][23690-1-2-1.03][23921-2-1-1.07][23936-1-2-2.67][24040-3-0-3.05]
[24111-1-4-2.57][24182-0-0-6.48][24238-3-3-3.10][24290-2-0-2.05][24345-0-0-2.27][24364-1-2-1.42][24427-3-0-3.96][24477-2-2-4.53][24495-2-4-1.15][24893-2-2-2.71]
[25012-1-4-1.78][25121-2-2-3.87][25165-3-3-3.36][25183-0-0-3.46][25297-3-3-4.54][25398-0-0-5.71][25574-2-4-2.67][25644-1-1-0.97][25718-1-4-0.96][25774-2-4-2.20]
[26032-3-3-4.80][26051-3-3-5.16][26120-0-0-2.56][26321-1-1-4.45][26732-1-1-2.63][26784-3-3-7.67][26827-3-3-2.75][26833-0-3-5.10][26838-2-2-1.21][26860-1-0-0.65]
[26948-0-0-1.46][27049-3-0-2.98][27098-1-0-2.72][27526-0-3-2.76][27639-3-3-0.69][27698-3-0-3.78][27772-0-0-6.46][27890-1-1-4.94][28040-0-4-0.94][28503-2-2-5.83]
[28577-1-1-3.98][28959-0-0-5.44][29198-3-4-2.36][29777-0-0-7.44][29877-2-3-1.71][30035-1-1-4.95][30098-0-0-2.02][30326-1-1-8.11][30572-2-2-5.65][30716-0-4-2.35]
[30806-2-3-2.53][30906-1-1-6.88][31007-0-0-3.19][31181-3-4-1.03][31238-0-0-2.24][31347-0-0-6.10][31422-2-4-0.86][31429-3-3-2.86][31431-0-0-0.56][31432-1-1-4.66]
[31477-0-3-3.57][31524-1-2-0.61][31597-1-1-2.57][31619-1-2-0.35][31701-0-0-3.58][31755-0-0-4.61][31854-3-3-3.14][32074-1-1-1.64][32078-3-3-4.20][32111-1-1-3.33]
[32127-1-1-3.62][32140-3-3-4.41][32263-2-4-0.77][32365-0-0-4.81][32411-2-3-4.88][32429-3-0-1.55][32473-3-3-1.81][32574-3-3-2.09][32584-0-4-2.86][32622-0-4-0.49]
[32858-3-3-3.62][32969-3-3-3.37][33016-2-2-7.30][33031-1-3-1.85][33035-2-2-3.00][33133-2-2-2.69][33173-2-2-4.04][33175-3-4-2.65][33306-3-3-3.48][33309-2-3-2.34]
[33474-0-4-0.83][33478-2-4-0.10][33618-1-1-1.73][33712-0-0-1.97][33782-2-4-2.92][33914-3-3-4.70][34076-3-4-1.87][34112-2-2-6.10][34138-2-3-0.85][34239-1-1-1.43]
[34364-2-2-5.20][34617-1-4-3.74][34751-3-3-3.92][34783-2-4-2.58][35015-3-4-1.71][35018-1-1-2.06][35288-2-2-2.10][0-4-4-2.81][1-4-4-2.29][2-4-4-2.14]
[3-4-4-1.65][4-4-0-1.60][5-4-3-1.55][6-4-4-2.69][7-4-4-1.26][8-4-3-0.09][9-4-4-2.37][10-4-4-3.23][11-4-4-4.91][12-4-4-1.15]
[14-4-4-1.75][15-4-3-3.32][16-4-4-1.08][17-4-4-1.42][18-4-4-4.21][19-4-0-2.07][20-4-4-1.13][21-4-4-2.66][22-4-4-3.20][23-4-4-1.17]
[24-4-4-4.92][25-4-3-2.16][26-4-3-0.91][27-4-4-3.21][28-4-4-4.61][29-4-1-1.49][30-4-4-1.19][31-4-4-2.92][32-4-4-2.89][33-4-4-2.43]
[34-4-4-2.28][35-4-4-2.62][37-4-4-2.89][39-4-0-1.50][40-4-4-1.22][41-4-4-1.89][42-4-4-1.78][43-4-4-3.17][45-4-4-2.41][46-4-4-3.27]
[47-4-4-6.10][48-4-4-3.10][51-4-4-3.84][52-4-4-1.87][53-4-4-1.72][54-4-4-2.39][55-4-4-1.78][56-4-1-2.45][57-4-3-2.06][58-4-2-1.32]
[59-4-4-2.07][60-4-4-4.50][61-4-4-4.36][62-4-4-1.79][63-4-2-2.56][64-4-4-2.43][65-4-4-5.24][66-4-4-4.38][67-4-4-1.42][68-4-4-1.23]
[69-4-3-0.44][70-4-4-2.88][72-4-4-1.17][73-4-4-1.24][74-4-2-3.13][75-4-3-0.83][77-4-4-5.02][78-4-4-0.84][79-4-4-4.20][80-4-4-5.07]
[81-4-2-2.29][82-4-4-2.25][83-4-4-1.71][84-4-4-3.68][85-4-4-2.99][86-4-4-0.74][87-4-4-2.88][88-4-4-2.02][89-4-4-0.75][90-4-4-1.95]
[91-4-4-2.33][92-4-4-1.50][93-4-0-1.70][94-4-4-2.54][95-4-4-1.79][96-4-4-2.50][97-4-4-4.55][98-4-2-3.44][99-4-4-2.06][100-4-1-2.25]
[101-4-4-3.80][102-4-2-3.27][103-4-4-1.12][104-4-4-2.18][105-4-4-4.24][106-4-4-4.09][107-4-4-4.13][108-4-4-2.35][109-4-4-2.09][110-4-4-3.54]
[111-4-0-4.07][112-4-2-2.54][113-4-2-1.41][114-4-4-1.51][115-4-4-1.37][116-4-4-1.47][117-4-4-3.52][119-4-4-2.11][121-4-4-3.50][122-4-4-1.96]
[124-4-4-1.73][125-4-4-4.06][126-4-4-4.24][127-4-2-2.89][128-4-4-0.67][129-4-4-2.10][130-4-4-3.19][131-4-2-1.77][132-4-4-2.74][133-4-4-1.43]
[135-4-4-1.76][136-4-4-1.06][137-4-4-1.33][138-4-2-1.98][139-4-4-1.43][140-4-4-1.72][141-4-0-0.13][142-4-4-4.36][143-4-4-3.04][144-4-4-1.99]
[145-4-2-3.36][148-4-4-2.98][149-4-4-1.93][150-4-4-5.67][151-4-4-4.05][152-4-4-2.74][153-4-2-2.78][154-4-4-4.84][155-4-4-3.63][156-4-3-1.39]
[157-4-2-0.32][158-4-4-1.96][160-4-1-0.52][161-4-2-2.35][162-4-4-0.32][164-4-4-2.23][165-4-4-2.44][167-4-4-2.34][168-4-4-2.24][170-4-4-2.38]
[171-4-4-2.08][172-4-4-3.83][173-4-4-3.97][174-4-0-4.17][175-4-4-3.58][177-4-4-2.68][178-4-2-0.80][179-4-4-1.56][180-4-4-3.07][181-4-4-2.41]
[182-4-3-2.02][183-4-4-3.17][184-4-4-2.57][186-4-4-1.13][187-4-2-2.43][188-4-4-2.78][189-4-0-1.26][190-4-4-1.04][191-4-4-3.51][192-4-4-1.45]
[193-4-2-2.30][194-4-0-2.06][195-4-0-1.39][196-4-4-1.72][197-4-4-3.30][198-4-4-5.23][199-4-2-2.59]
---------------------------
I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.122 | Acc: 93.375% | Wgt Acc: 97.679%
	I - Batch: 100 | Loss: 0.121 | Acc: 93.188% | Wgt Acc: 97.688%
	I - Batch: 150 | Loss: 0.119 | Acc: 92.792% | Wgt Acc: 97.553%
	I - Batch: 200 | Loss: 0.120 | Acc: 92.781% | Wgt Acc: 97.514%
I - num batch: 222
I - Train -- Loss: 0.119 | Acc: 92.811% | Wgt Acc: 97.498% | LR: 1.250000e-04 | Dur: 138.64s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   2.  89.]
 [  0. 577.   0.   1.  29.]
 [  2.   0. 730.   0.  73.]
 [  1.   0.   0. 531.  45.]
 [  4.   1.   4.   4. 764.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.968 | Acc: 68.836% | Wgt Acc: 65.413% | Dur: 18.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   5.   3.  12.  12.]
 [  1.  39.   3.   1.   5.]
 [  1.  16.  44.   3.  16.]
 [ 14.   2.   9.  63.   7.]
 [  9.  16.  16.   7. 140.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 93.875% | Wgt Acc: 98.086%
	I - Batch: 100 | Loss: 0.103 | Acc: 93.562% | Wgt Acc: 97.819%
	I - Batch: 150 | Loss: 0.107 | Acc: 93.375% | Wgt Acc: 97.827%
	I - Batch: 200 | Loss: 0.108 | Acc: 93.156% | Wgt Acc: 97.684%
I - num batch: 222
I - Train -- Loss: 0.109 | Acc: 93.036% | Wgt Acc: 97.613% | LR: 1.250000e-04 | Dur: 133.86s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   2.  78.]
 [  0. 574.   0.   0.  36.]
 [  0.   1. 733.   0.  66.]
 [  0.   0.   0. 532.  50.]
 [  6.   3.   1.   4. 770.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.102 | Acc: 66.667% | Wgt Acc: 60.812% | Dur: 13.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   5.  21.  18.]
 [  0.  41.   4.   0.   2.]
 [  0.  10.  30.   2.   5.]
 [  8.   3.  12.  52.   8.]
 [ 12.  21.  24.  11. 147.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.126 | Acc: 91.875% | Wgt Acc: 97.089%
	I - Batch: 100 | Loss: 0.117 | Acc: 92.188% | Wgt Acc: 97.251%
	I - Batch: 150 | Loss: 0.118 | Acc: 92.250% | Wgt Acc: 97.161%
	I - Batch: 200 | Loss: 0.116 | Acc: 92.250% | Wgt Acc: 97.190%
I - num batch: 222
I - Train -- Loss: 0.117 | Acc: 92.275% | Wgt Acc: 97.184% | LR: 1.250000e-04 | Dur: 133.26s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   0.  89.]
 [  0. 573.   0.   0.  39.]
 [  0.   1. 730.   0.  74.]
 [  1.   0.   0. 532.  47.]
 [  9.   4.   4.   6. 751.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.062 | Acc: 67.061% | Wgt Acc: 63.222% | Dur: 15.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   3.  11.  17.]
 [  0.  42.   2.   0.   3.]
 [  0.  15.  38.   2.   9.]
 [ 16.   2.  11.  61.  12.]
 [ 12.  15.  21.  12. 139.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 0.105 | Acc: 93.875% | Wgt Acc: 97.886%
	I - Batch: 100 | Loss: 0.121 | Acc: 92.625% | Wgt Acc: 97.219%
	I - Batch: 150 | Loss: 0.123 | Acc: 92.375% | Wgt Acc: 97.143%
	I - Batch: 200 | Loss: 0.123 | Acc: 92.375% | Wgt Acc: 97.180%
I - num batch: 222
I - Train -- Loss: 0.123 | Acc: 92.360% | Wgt Acc: 97.037% | LR: 1.250000e-04 | Dur: 136.28s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   1.   0.   2.  95.]
 [  0. 576.   1.   1.  27.]
 [  1.   0. 728.   2.  60.]
 [  3.   0.   0. 529.  58.]
 [ 10.   1.   5.   4. 760.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.003 | Acc: 68.639% | Wgt Acc: 60.858% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   1.  16.   5.]
 [  1.  41.   7.   1.   6.]
 [  1.  10.  39.   1.   6.]
 [  5.   2.   6.  47.   4.]
 [ 19.  23.  22.  21. 159.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 0.100 | Acc: 93.000% | Wgt Acc: 97.866%
	I - Batch: 100 | Loss: 0.106 | Acc: 93.062% | Wgt Acc: 97.759%
	I - Batch: 150 | Loss: 0.113 | Acc: 92.875% | Wgt Acc: 97.685%
	I - Batch: 200 | Loss: 0.114 | Acc: 92.562% | Wgt Acc: 97.535%
I - num batch: 222
I - Train -- Loss: 0.114 | Acc: 92.557% | Wgt Acc: 97.488% | LR: 1.250000e-04 | Dur: 136.99s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   0.   3.  87.]
 [  0. 577.   0.   1.  39.]
 [  1.   0. 733.   0.  66.]
 [  1.   0.   0. 532.  55.]
 [  7.   1.   1.   2. 753.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.062 | Acc: 67.850% | Wgt Acc: 60.927% | Dur: 17.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   4.   1.  13.   5.]
 [  0.  37.   2.   0.   5.]
 [  0.  14.  39.   3.  11.]
 [ 11.   2.   9.  53.   5.]
 [ 16.  21.  24.  17. 154.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 0.105 | Acc: 93.750% | Wgt Acc: 97.947%
	I - Batch: 100 | Loss: 0.106 | Acc: 93.625% | Wgt Acc: 97.731%
	I - Batch: 150 | Loss: 0.112 | Acc: 93.083% | Wgt Acc: 97.432%
	I - Batch: 200 | Loss: 0.114 | Acc: 92.969% | Wgt Acc: 97.377%
I - num batch: 222
I - Train -- Loss: 0.116 | Acc: 92.783% | Wgt Acc: 97.328% | LR: 1.250000e-04 | Dur: 141.85s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   0.   0.   3.  69.]
 [  0. 575.   0.   0.  39.]
 [  2.   0. 731.   1.  72.]
 [  1.   1.   0. 533.  51.]
 [ 11.   2.   3.   1. 769.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.049 | Acc: 67.258% | Wgt Acc: 57.998% | Dur: 15.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   2.  12.   6.]
 [  0.  34.   3.   1.   3.]
 [  0.   9.  32.   0.   5.]
 [ 10.   1.   5.  52.   2.]
 [ 19.  32.  33.  21. 164.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 0.111 | Acc: 92.875% | Wgt Acc: 97.563%
	I - Batch: 100 | Loss: 0.103 | Acc: 93.312% | Wgt Acc: 97.753%
	I - Batch: 150 | Loss: 0.102 | Acc: 93.583% | Wgt Acc: 97.896%
	I - Batch: 200 | Loss: 0.105 | Acc: 93.375% | Wgt Acc: 97.755%
I - num batch: 222
I - Train -- Loss: 0.105 | Acc: 93.459% | Wgt Acc: 97.785% | LR: 1.250000e-04 | Dur: 139.07s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   3.  79.]
 [  0. 575.   0.   0.  37.]
 [  0.   1. 734.   1.  58.]
 [  0.   0.   0. 532.  43.]
 [  6.   2.   0.   2. 783.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.024 | Acc: 68.245% | Wgt Acc: 60.524% | Dur: 15.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   3.   3.  17.  10.]
 [  0.  32.   3.   0.   3.]
 [  0.  14.  34.   1.   7.]
 [  7.   2.   4.  51.   2.]
 [ 10.  27.  31.  17. 158.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 0.106 | Acc: 92.625% | Wgt Acc: 97.610%
	I - Batch: 100 | Loss: 0.108 | Acc: 93.062% | Wgt Acc: 97.836%
	I - Batch: 150 | Loss: 0.110 | Acc: 93.333% | Wgt Acc: 97.788%
	I - Batch: 200 | Loss: 0.108 | Acc: 93.406% | Wgt Acc: 97.826%
I - num batch: 222
I - Train -- Loss: 0.109 | Acc: 93.149% | Wgt Acc: 97.770% | LR: 1.250000e-04 | Dur: 141.57s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   0.  87.]
 [  0. 576.   0.   0.  39.]
 [  0.   0. 732.   0.  64.]
 [  0.   1.   0. 536.  40.]
 [  7.   1.   2.   2. 770.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.079 | Acc: 66.667% | Wgt Acc: 61.931% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   3.  13.  10.]
 [  0.  34.   0.   1.   2.]
 [  0.  17.  40.   1.  13.]
 [ 13.  10.  11.  59.  13.]
 [ 12.  15.  21.  12. 142.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 0.105 | Acc: 93.625% | Wgt Acc: 97.751%
	I - Batch: 100 | Loss: 0.100 | Acc: 93.438% | Wgt Acc: 97.775%
	I - Batch: 150 | Loss: 0.100 | Acc: 93.542% | Wgt Acc: 97.729%
	I - Batch: 200 | Loss: 0.101 | Acc: 93.688% | Wgt Acc: 97.761%
I - num batch: 222
I - Train -- Loss: 0.101 | Acc: 93.685% | Wgt Acc: 97.765% | LR: 1.250000e-04 | Dur: 136.13s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   1.   0.   1.  76.]
 [  1. 574.   0.   0.  30.]
 [  0.   0. 733.   0.  63.]
 [  2.   1.   0. 533.  37.]
 [  5.   2.   1.   4. 794.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.064 | Acc: 67.653% | Wgt Acc: 61.319% | Dur: 14.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   1.   8.   9.]
 [  0.  44.   8.   2.   3.]
 [  0.   9.  28.   0.   8.]
 [ 15.   2.  10.  60.   8.]
 [ 14.  20.  28.  16. 152.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 0.111 | Acc: 92.500% | Wgt Acc: 97.434%
	I - Batch: 100 | Loss: 0.108 | Acc: 92.250% | Wgt Acc: 97.440%
	I - Batch: 150 | Loss: 0.110 | Acc: 92.667% | Wgt Acc: 97.494%
	I - Batch: 200 | Loss: 0.111 | Acc: 92.562% | Wgt Acc: 97.473%
I - num batch: 222
I - Train -- Loss: 0.108 | Acc: 92.867% | Wgt Acc: 97.575% | LR: 1.250000e-04 | Dur: 140.46s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   1.  85.]
 [  0. 577.   0.   1.  27.]
 [  0.   0. 731.   0.  69.]
 [  1.   0.   0. 533.  55.]
 [  7.   1.   3.   3. 764.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.077 | Acc: 67.258% | Wgt Acc: 62.542% | Dur: 16.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 76.   5.   3.  22.  23.]
 [  0.  34.   3.   0.   2.]
 [  0.  12.  38.   0.   8.]
 [  5.   6.  10.  51.   5.]
 [  7.  21.  21.  13. 142.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 0.102 | Acc: 94.375% | Wgt Acc: 98.076%
	I - Batch: 100 | Loss: 0.108 | Acc: 93.562% | Wgt Acc: 97.664%
	I - Batch: 150 | Loss: 0.105 | Acc: 93.333% | Wgt Acc: 97.632%
	I - Batch: 200 | Loss: 0.105 | Acc: 93.438% | Wgt Acc: 97.677%
I - num batch: 222
I - Train -- Loss: 0.104 | Acc: 93.375% | Wgt Acc: 97.691% | LR: 1.250000e-04 | Dur: 136.78s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   0.  83.]
 [  0. 575.   0.   0.  33.]
 [  0.   0. 731.   0.  60.]
 [  2.   0.   0. 536.  41.]
 [  8.   3.   3.   2. 783.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.999 | Acc: 66.864% | Wgt Acc: 60.155% | Dur: 14.65s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   3.   1.  13.   7.]
 [  1.  41.   5.   0.   7.]
 [  0.  11.  33.   0.   7.]
 [ 16.   2.   8.  57.   7.]
 [ 15.  21.  28.  16. 152.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 0.105 | Acc: 93.750% | Wgt Acc: 97.492%
	I - Batch: 100 | Loss: 0.094 | Acc: 94.188% | Wgt Acc: 97.852%
	I - Batch: 150 | Loss: 0.098 | Acc: 94.292% | Wgt Acc: 97.892%
	I - Batch: 200 | Loss: 0.103 | Acc: 93.750% | Wgt Acc: 97.737%
I - num batch: 222
I - Train -- Loss: 0.106 | Acc: 93.544% | Wgt Acc: 97.673% | LR: 1.250000e-04 | Dur: 135.86s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   0.   2.  76.]
 [  0. 575.   0.   1.  34.]
 [  0.   1. 731.   0.  51.]
 [  3.   0.   0. 533.  48.]
 [  6.   2.   3.   2. 791.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.079 | Acc: 67.061% | Wgt Acc: 61.965% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   4.  20.  13.]
 [  0.  41.   1.   0.   3.]
 [  1.  13.  35.   2.  13.]
 [  6.   4.   9.  52.   7.]
 [ 13.  17.  26.  12. 144.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 0.103 | Acc: 93.125% | Wgt Acc: 97.994%
	I - Batch: 100 | Loss: 0.104 | Acc: 93.812% | Wgt Acc: 98.069%
	I - Batch: 150 | Loss: 0.100 | Acc: 94.292% | Wgt Acc: 98.230%
	I - Batch: 200 | Loss: 0.107 | Acc: 93.844% | Wgt Acc: 98.006%
I - num batch: 222
I - Train -- Loss: 0.106 | Acc: 93.713% | Wgt Acc: 97.977% | LR: 1.250000e-04 | Dur: 136.09s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  77.]
 [  0. 577.   1.   0.  36.]
 [  1.   0. 730.   0.  59.]
 [  1.   0.   0. 535.  40.]
 [  1.   1.   3.   3. 788.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.068 | Acc: 66.864% | Wgt Acc: 63.637% | Dur: 14.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   5.   3.  11.  15.]
 [  1.  43.   2.   1.   4.]
 [  0.   6.  34.   0.   6.]
 [ 18.   6.  16.  63.  19.]
 [  6.  18.  20.  11. 136.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 0.097 | Acc: 93.000% | Wgt Acc: 97.826%
	I - Batch: 100 | Loss: 0.093 | Acc: 93.812% | Wgt Acc: 97.991%
	I - Batch: 150 | Loss: 0.092 | Acc: 94.208% | Wgt Acc: 98.169%
	I - Batch: 200 | Loss: 0.094 | Acc: 94.125% | Wgt Acc: 98.114%
I - num batch: 222
I - Train -- Loss: 0.097 | Acc: 94.023% | Wgt Acc: 98.032% | LR: 1.250000e-04 | Dur: 141.47s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   1.  71.]
 [  1. 576.   0.   0.  20.]
 [  0.   0. 734.   0.  63.]
 [  2.   0.   0. 535.  46.]
 [  4.   2.   0.   2. 800.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.108 | Acc: 67.653% | Wgt Acc: 64.018% | Dur: 14.56s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   5.   2.   9.  12.]
 [  0.  40.   4.   0.   9.]
 [  1.   7.  36.   1.   9.]
 [ 15.   9.  13.  62.  11.]
 [  6.  17.  20.  14. 139.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 0.116 | Acc: 92.500% | Wgt Acc: 97.390%
	I - Batch: 100 | Loss: 0.111 | Acc: 93.125% | Wgt Acc: 97.745%
	I - Batch: 150 | Loss: 0.105 | Acc: 93.625% | Wgt Acc: 97.917%
	I - Batch: 200 | Loss: 0.102 | Acc: 93.781% | Wgt Acc: 97.953%
I - num batch: 222
I - Train -- Loss: 0.100 | Acc: 93.826% | Wgt Acc: 97.949% | LR: 1.250000e-04 | Dur: 137.84s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   1.   0.   2.  67.]
 [  0. 577.   0.   0.  31.]
 [  0.   0. 731.   0.  64.]
 [  2.   0.   0. 534.  44.]
 [  3.   0.   3.   2. 794.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.114 | Acc: 68.047% | Wgt Acc: 61.262% | Dur: 16.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   3.   2.  15.   9.]
 [  0.  31.   4.   0.   1.]
 [  0.  11.  36.   0.   8.]
 [ 10.   4.  13.  59.   8.]
 [ 13.  29.  20.  12. 154.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 0.103 | Acc: 93.625% | Wgt Acc: 97.748%
	I - Batch: 100 | Loss: 0.097 | Acc: 93.812% | Wgt Acc: 97.994%
	I - Batch: 150 | Loss: 0.098 | Acc: 93.958% | Wgt Acc: 98.071%
	I - Batch: 200 | Loss: 0.102 | Acc: 93.688% | Wgt Acc: 97.933%
I - num batch: 222
I - Train -- Loss: 0.102 | Acc: 93.657% | Wgt Acc: 97.875% | LR: 1.250000e-04 | Dur: 133.61s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   1.  87.]
 [  0. 575.   0.   0.  29.]
 [  0.   1. 732.   0.  51.]
 [  1.   1.   0. 535.  44.]
 [  5.   1.   1.   2. 789.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.097 | Acc: 66.469% | Wgt Acc: 61.181% | Dur: 13.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   6.   3.  20.  20.]
 [  0.  44.   4.   0.   3.]
 [  1.   7.  27.   0.   5.]
 [  8.   3.   9.  51.   8.]
 [  8.  18.  32.  15. 144.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 126
I - Training: 
	I - Batch: 50 | Loss: 0.100 | Acc: 93.875% | Wgt Acc: 98.020%
	I - Batch: 100 | Loss: 0.097 | Acc: 94.188% | Wgt Acc: 98.140%
	I - Batch: 150 | Loss: 0.101 | Acc: 93.833% | Wgt Acc: 97.960%
	I - Batch: 200 | Loss: 0.097 | Acc: 94.125% | Wgt Acc: 98.083%
I - num batch: 222
I - Train -- Loss: 0.099 | Acc: 93.826% | Wgt Acc: 97.944% | LR: 1.250000e-04 | Dur: 137.07s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  73.]
 [  0. 574.   0.   0.  27.]
 [  0.   0. 732.   1.  59.]
 [  0.   0.   1. 534.  47.]
 [  3.   4.   1.   3. 794.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 65.878% | Wgt Acc: 59.105% | Dur: 21.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   4.   4.  23.  13.]
 [  0.  39.   4.   1.   8.]
 [  0.  11.  32.   0.   6.]
 [  6.   3.   9.  43.   4.]
 [ 11.  21.  26.  19. 149.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 127
I - Training: 
	I - Batch: 50 | Loss: 0.097 | Acc: 94.000% | Wgt Acc: 98.214%
	I - Batch: 100 | Loss: 0.099 | Acc: 93.938% | Wgt Acc: 97.951%
	I - Batch: 150 | Loss: 0.101 | Acc: 93.875% | Wgt Acc: 97.892%
	I - Batch: 200 | Loss: 0.099 | Acc: 93.938% | Wgt Acc: 97.929%
I - num batch: 222
I - Train -- Loss: 0.097 | Acc: 94.164% | Wgt Acc: 97.929% | LR: 1.250000e-04 | Dur: 138.17s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   2.  67.]
 [  0. 578.   0.   0.  24.]
 [  1.   0. 732.   0.  54.]
 [  0.   0.   0. 533.  45.]
 [  9.   0.   2.   3. 810.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.138 | Acc: 66.075% | Wgt Acc: 61.285% | Dur: 15.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   4.   3.  14.  12.]
 [  0.  33.   3.   0.   3.]
 [  1.  15.  43.   3.  16.]
 [ 10.   6.  10.  57.   8.]
 [ 16.  20.  16.  12. 141.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 128
I - Training: 
	I - Batch: 50 | Loss: 0.102 | Acc: 92.250% | Wgt Acc: 97.240%
	I - Batch: 100 | Loss: 0.101 | Acc: 93.125% | Wgt Acc: 97.534%
	I - Batch: 150 | Loss: 0.102 | Acc: 93.333% | Wgt Acc: 97.636%
	I - Batch: 200 | Loss: 0.101 | Acc: 93.438% | Wgt Acc: 97.710%
I - num batch: 222
I - Train -- Loss: 0.107 | Acc: 93.262% | Wgt Acc: 97.564% | LR: 1.250000e-04 | Dur: 137.21s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   1.   2.  72.]
 [  0. 574.   1.   0.  38.]
 [  0.   0. 730.   0.  57.]
 [  1.   1.   0. 532.  51.]
 [  6.   3.   2.   4. 782.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.169 | Acc: 67.061% | Wgt Acc: 57.906% | Dur: 15.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   3.   2.  22.  12.]
 [  0.  37.   3.   0.   3.]
 [  0.   9.  32.   0.   2.]
 [  6.   2.   9.  40.   1.]
 [ 13.  27.  29.  24. 162.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 129
I - Training: 
	I - Batch: 50 | Loss: 0.104 | Acc: 92.750% | Wgt Acc: 97.690%
	I - Batch: 100 | Loss: 0.103 | Acc: 93.562% | Wgt Acc: 97.813%
	I - Batch: 150 | Loss: 0.106 | Acc: 93.375% | Wgt Acc: 97.731%
	I - Batch: 200 | Loss: 0.106 | Acc: 93.531% | Wgt Acc: 97.861%
I - num batch: 222
I - Train -- Loss: 0.108 | Acc: 93.234% | Wgt Acc: 97.708% | LR: 1.250000e-04 | Dur: 135.67s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   0.  88.]
 [  0. 575.   1.   0.  37.]
 [  0.   0. 729.   0.  54.]
 [  0.   0.   0. 536.  45.]
 [  6.   3.   4.   2. 776.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.090 | Acc: 66.667% | Wgt Acc: 62.715% | Dur: 15.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   3.   3.  10.  10.]
 [  1.  42.   2.   1.  11.]
 [  1.  14.  39.   2.   9.]
 [ 20.   3.  11.  62.  11.]
 [ 10.  16.  20.  11. 139.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 130
I - Training: 
	I - Batch: 50 | Loss: 0.094 | Acc: 94.375% | Wgt Acc: 98.015%
	I - Batch: 100 | Loss: 0.093 | Acc: 93.750% | Wgt Acc: 97.832%
	I - Batch: 150 | Loss: 0.096 | Acc: 93.625% | Wgt Acc: 97.653%
	I - Batch: 200 | Loss: 0.097 | Acc: 93.812% | Wgt Acc: 97.752%
I - num batch: 222
I - Train -- Loss: 0.097 | Acc: 93.826% | Wgt Acc: 97.750% | LR: 1.250000e-04 | Dur: 139.97s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   1.   1.  72.]
 [  0. 577.   0.   0.  24.]
 [  0.   0. 731.   1.  52.]
 [  1.   0.   0. 532.  51.]
 [  9.   1.   2.   4. 801.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.049 | Acc: 69.822% | Wgt Acc: 64.491% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   2.   9.   9.]
 [  1.  40.   3.   0.   4.]
 [  0.   8.  35.   1.   5.]
 [  9.   4.  12.  64.  11.]
 [ 14.  21.  23.  12. 151.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 131
I - Training: 
	I - Batch: 50 | Loss: 0.085 | Acc: 93.125% | Wgt Acc: 97.907%
	I - Batch: 100 | Loss: 0.086 | Acc: 93.562% | Wgt Acc: 98.082%
	I - Batch: 150 | Loss: 0.088 | Acc: 94.083% | Wgt Acc: 98.159%
	I - Batch: 200 | Loss: 0.095 | Acc: 93.625% | Wgt Acc: 97.840%
I - num batch: 222
I - Train -- Loss: 0.095 | Acc: 93.769% | Wgt Acc: 97.886% | LR: 1.250000e-04 | Dur: 137.70s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   0.   1.  79.]
 [  0. 578.   0.   0.  31.]
 [  0.   0. 732.   0.  58.]
 [  1.   0.   1. 536.  38.]
 [ 10.   0.   1.   1. 794.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.013 | Acc: 68.047% | Wgt Acc: 62.565% | Dur: 14.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   5.   4.  22.  12.]
 [  0.  38.   3.   0.   5.]
 [  0.  10.  39.   1.  11.]
 [  8.   2.   7.  49.   5.]
 [  8.  23.  22.  14. 147.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 132
I - Training: 
	I - Batch: 50 | Loss: 0.101 | Acc: 93.625% | Wgt Acc: 98.007%
	I - Batch: 100 | Loss: 0.097 | Acc: 94.125% | Wgt Acc: 98.091%
	I - Batch: 150 | Loss: 0.097 | Acc: 93.875% | Wgt Acc: 98.014%
	I - Batch: 200 | Loss: 0.097 | Acc: 93.781% | Wgt Acc: 98.001%
I - num batch: 222
I - Train -- Loss: 0.096 | Acc: 93.769% | Wgt Acc: 98.026% | LR: 1.250000e-04 | Dur: 136.61s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  92.]
 [  0. 577.   0.   0.  19.]
 [  0.   0. 731.   0.  55.]
 [  0.   0.   0. 537.  45.]
 [  5.   1.   3.   1. 789.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.011 | Acc: 69.625% | Wgt Acc: 64.133% | Dur: 14.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   2.  10.  12.]
 [  0.  44.   5.   0.   3.]
 [  0.   8.  41.   1.   4.]
 [ 17.   1.  10.  58.  10.]
 [ 12.  22.  17.  17. 151.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 133
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 94.250% | Wgt Acc: 98.334%
	I - Batch: 100 | Loss: 0.097 | Acc: 93.750% | Wgt Acc: 97.994%
	I - Batch: 150 | Loss: 0.102 | Acc: 93.583% | Wgt Acc: 97.808%
	I - Batch: 200 | Loss: 0.103 | Acc: 93.438% | Wgt Acc: 97.779%
I - num batch: 222
I - Train -- Loss: 0.101 | Acc: 93.657% | Wgt Acc: 97.855% | LR: 1.250000e-04 | Dur: 135.33s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   1.   1.   0.  74.]
 [  0. 576.   0.   0.  34.]
 [  1.   0. 730.   0.  64.]
 [  0.   1.   0. 537.  38.]
 [  7.   0.   3.   1. 790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.011 | Acc: 69.231% | Wgt Acc: 63.211% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   2.   2.  13.   9.]
 [  0.  42.   5.   0.   8.]
 [  0.  12.  31.   2.   3.]
 [ 12.   3.  12.  60.   7.]
 [ 11.  19.  25.  11. 153.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 134
I - Training: 
	I - Batch: 50 | Loss: 0.094 | Acc: 92.875% | Wgt Acc: 97.857%
	I - Batch: 100 | Loss: 0.089 | Acc: 93.938% | Wgt Acc: 98.088%
	I - Batch: 150 | Loss: 0.088 | Acc: 94.042% | Wgt Acc: 98.156%
	I - Batch: 200 | Loss: 0.084 | Acc: 94.312% | Wgt Acc: 98.281%
I - num batch: 208
I - Train -- Loss: 0.084 | Acc: 94.231% | Wgt Acc: 98.266% | LR: 1.250000e-04 | Dur: 125.66s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   1.  60.]
 [  0. 576.   0.   0.  26.]
 [  0.   1. 733.   0.  48.]
 [  0.   0.   0. 535.  50.]
 [  2.   1.   1.   2. 597.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 68.836% | Wgt Acc: 61.412% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   3.   1.  13.   6.]
 [  0.  38.   2.   1.   2.]
 [  0.  13.  37.   3.  11.]
 [  7.   1.   5.  52.   3.]
 [ 17.  23.  30.  17. 158.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 135
I - Training: 
	I - Batch: 50 | Loss: 0.084 | Acc: 94.500% | Wgt Acc: 98.305%
	I - Batch: 100 | Loss: 0.091 | Acc: 94.250% | Wgt Acc: 98.067%
	I - Batch: 150 | Loss: 0.095 | Acc: 94.083% | Wgt Acc: 98.010%
	I - Batch: 200 | Loss: 0.096 | Acc: 93.906% | Wgt Acc: 97.994%
I - num batch: 222
I - Train -- Loss: 0.095 | Acc: 94.108% | Wgt Acc: 98.081% | LR: 1.250000e-04 | Dur: 137.34s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  77.]
 [  0. 575.   0.   0.  29.]
 [  0.   0. 732.   0.  47.]
 [  0.   1.   0. 535.  45.]
 [  3.   2.   2.   3. 802.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.020 | Acc: 68.245% | Wgt Acc: 61.423% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   1.   8.  10.]
 [  0.  36.   5.   1.   3.]
 [  1.  10.  37.   1.   7.]
 [ 18.   6.  11.  61.   5.]
 [ 12.  23.  21.  15. 155.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 136
I - Training: 
	I - Batch: 50 | Loss: 0.104 | Acc: 93.625% | Wgt Acc: 97.752%
	I - Batch: 100 | Loss: 0.094 | Acc: 94.062% | Wgt Acc: 97.878%
	I - Batch: 150 | Loss: 0.095 | Acc: 94.250% | Wgt Acc: 97.882%
	I - Batch: 200 | Loss: 0.097 | Acc: 94.094% | Wgt Acc: 97.854%
I - num batch: 222
I - Train -- Loss: 0.097 | Acc: 93.967% | Wgt Acc: 97.809% | LR: 1.250000e-04 | Dur: 133.74s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   0.   2.  71.]
 [  1. 575.   0.   1.  27.]
 [  1.   0. 734.   0.  57.]
 [  0.   1.   0. 531.  40.]
 [  7.   2.   0.   4. 805.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.085 | Acc: 67.850% | Wgt Acc: 59.797% | Dur: 14.89s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   1.   4.  12.  10.]
 [  1.  45.   6.   3.   7.]
 [  0.   6.  26.   0.   1.]
 [ 12.   1.   6.  54.   2.]
 [ 16.  25.  33.  17. 160.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 137
I - Training: 
	I - Batch: 50 | Loss: 0.104 | Acc: 93.750% | Wgt Acc: 97.546%
	I - Batch: 100 | Loss: 0.104 | Acc: 93.938% | Wgt Acc: 97.716%
	I - Batch: 150 | Loss: 0.099 | Acc: 94.167% | Wgt Acc: 97.871%
	I - Batch: 200 | Loss: 0.100 | Acc: 93.844% | Wgt Acc: 97.761%
I - num batch: 222
I - Train -- Loss: 0.099 | Acc: 93.769% | Wgt Acc: 97.770% | LR: 1.250000e-04 | Dur: 136.75s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   1.   1.  77.]
 [  0. 576.   0.   0.  21.]
 [  1.   1. 729.   0.  58.]
 [  1.   0.   0. 535.  46.]
 [  7.   1.   4.   2. 798.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.984 | Acc: 69.428% | Wgt Acc: 62.830% | Dur: 15.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   1.  10.  12.]
 [  0.  42.   5.   0.   3.]
 [  0.   7.  31.   1.   6.]
 [ 10.   5.  12.  59.   3.]
 [ 14.  22.  26.  16. 156.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 138
I - Training: 
	I - Batch: 50 | Loss: 0.106 | Acc: 93.250% | Wgt Acc: 97.629%
	I - Batch: 100 | Loss: 0.098 | Acc: 93.938% | Wgt Acc: 97.862%
	I - Batch: 150 | Loss: 0.098 | Acc: 93.875% | Wgt Acc: 97.792%
	I - Batch: 200 | Loss: 0.100 | Acc: 93.969% | Wgt Acc: 97.791%
I - num batch: 222
I - Train -- Loss: 0.100 | Acc: 93.939% | Wgt Acc: 97.808% | LR: 1.250000e-04 | Dur: 134.28s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   1.  64.]
 [  0. 577.   1.   0.  26.]
 [  0.   0. 730.   1.  64.]
 [  1.   0.   1. 532.  42.]
 [  7.   1.   2.   4. 804.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.162 | Acc: 66.864% | Wgt Acc: 61.331% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 79.  10.   7.  25.  26.]
 [  0.  37.   3.   0.   2.]
 [  0.  11.  31.   0.   4.]
 [  1.   2.  10.  47.   3.]
 [  8.  18.  24.  14. 145.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 139
I - Training: 
	I - Batch: 50 | Loss: 0.104 | Acc: 92.375% | Wgt Acc: 97.525%
	I - Batch: 100 | Loss: 0.101 | Acc: 93.438% | Wgt Acc: 97.840%
	I - Batch: 150 | Loss: 0.101 | Acc: 93.083% | Wgt Acc: 97.750%
	I - Batch: 200 | Loss: 0.099 | Acc: 93.438% | Wgt Acc: 97.868%
I - num batch: 222
I - Train -- Loss: 0.098 | Acc: 93.600% | Wgt Acc: 97.950% | LR: 1.250000e-04 | Dur: 136.24s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  85.]
 [  0. 576.   0.   0.  32.]
 [  0.   0. 730.   0.  57.]
 [  0.   0.   0. 537.  42.]
 [  4.   2.   4.   1. 784.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.054 | Acc: 68.245% | Wgt Acc: 62.565% | Dur: 14.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   1.  19.  16.]
 [  1.  43.   5.   1.   5.]
 [  0.   8.  36.   1.   7.]
 [ 10.   1.   9.  53.   3.]
 [ 12.  22.  24.  12. 149.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 140
I - Training: 
	I - Batch: 50 | Loss: 0.114 | Acc: 92.625% | Wgt Acc: 97.299%
	I - Batch: 100 | Loss: 0.106 | Acc: 93.000% | Wgt Acc: 97.535%
	I - Batch: 150 | Loss: 0.114 | Acc: 92.375% | Wgt Acc: 97.323%
	I - Batch: 200 | Loss: 0.108 | Acc: 92.938% | Wgt Acc: 97.529%
I - num batch: 222
I - Train -- Loss: 0.107 | Acc: 93.036% | Wgt Acc: 97.572% | LR: 1.250000e-04 | Dur: 136.94s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   0.  79.]
 [  0. 575.   1.   0.  44.]
 [  1.   0. 728.   0.  62.]
 [  1.   0.   1. 536.  43.]
 [  6.   3.   4.   2. 772.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.068 | Acc: 68.442% | Wgt Acc: 60.674% | Dur: 19.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   4.  14.   8.]
 [  0.  40.   4.   0.   3.]
 [  0.   9.  35.   0.   5.]
 [ 10.   1.   8.  51.   5.]
 [ 16.  24.  24.  21. 159.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 141
I - Training: 
	I - Batch: 50 | Loss: 0.097 | Acc: 93.500% | Wgt Acc: 97.839%
	I - Batch: 100 | Loss: 0.087 | Acc: 94.250% | Wgt Acc: 98.122%
	I - Batch: 150 | Loss: 0.096 | Acc: 93.792% | Wgt Acc: 97.933%
	I - Batch: 200 | Loss: 0.095 | Acc: 94.156% | Wgt Acc: 98.099%
I - num batch: 222
I - Train -- Loss: 0.095 | Acc: 94.192% | Wgt Acc: 98.055% | LR: 1.250000e-04 | Dur: 136.06s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   0.  67.]
 [  0. 575.   1.   0.  32.]
 [  0.   2. 732.   0.  56.]
 [  2.   0.   0. 537.  38.]
 [  5.   1.   1.   1. 807.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.139 | Acc: 65.680% | Wgt Acc: 62.207% | Dur: 16.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   8.   4.  21.  23.]
 [  0.  41.   5.   1.   5.]
 [  1.  13.  38.   0.  12.]
 [ 13.   3.  10.  53.   6.]
 [  7.  13.  18.  11. 134.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 142
I - Training: 
	I - Batch: 50 | Loss: 0.108 | Acc: 92.750% | Wgt Acc: 97.237%
	I - Batch: 100 | Loss: 0.103 | Acc: 93.125% | Wgt Acc: 97.505%
	I - Batch: 150 | Loss: 0.104 | Acc: 93.333% | Wgt Acc: 97.668%
	I - Batch: 200 | Loss: 0.102 | Acc: 93.469% | Wgt Acc: 97.713%
I - num batch: 222
I - Train -- Loss: 0.102 | Acc: 93.572% | Wgt Acc: 97.740% | LR: 1.250000e-04 | Dur: 136.37s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   0.   1.  68.]
 [  0. 576.   0.   0.  44.]
 [  1.   0. 731.   0.  56.]
 [  1.   0.   0. 534.  42.]
 [  7.   2.   3.   3. 790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.074 | Acc: 68.047% | Wgt Acc: 60.581% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   1.   2.  15.  10.]
 [  0.  34.   3.   0.   2.]
 [  0.  13.  39.   0.   8.]
 [ 11.   4.   8.  55.   3.]
 [ 17.  26.  23.  16. 157.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 143
I - Training: 
	I - Batch: 50 | Loss: 0.090 | Acc: 94.250% | Wgt Acc: 97.852%
	I - Batch: 100 | Loss: 0.092 | Acc: 94.312% | Wgt Acc: 97.931%
	I - Batch: 150 | Loss: 0.091 | Acc: 94.625% | Wgt Acc: 98.102%
	I - Batch: 200 | Loss: 0.089 | Acc: 94.781% | Wgt Acc: 98.230%
I - num batch: 222
I - Train -- Loss: 0.089 | Acc: 94.700% | Wgt Acc: 98.217% | LR: 1.250000e-04 | Dur: 136.52s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   2.  68.]
 [  0. 577.   1.   0.  24.]
 [  0.   0. 733.   0.  43.]
 [  0.   0.   0. 535.  41.]
 [  7.   1.   0.   1. 824.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.171 | Acc: 65.878% | Wgt Acc: 56.083% | Dur: 14.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   0.   0.  11.   6.]
 [  0.  35.   3.   0.   3.]
 [  0.   8.  28.   0.   5.]
 [  8.   1.   6.  49.   2.]
 [ 22.  34.  38.  26. 164.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 144
I - Training: 
	I - Batch: 50 | Loss: 0.098 | Acc: 93.750% | Wgt Acc: 97.858%
	I - Batch: 100 | Loss: 0.104 | Acc: 93.688% | Wgt Acc: 97.781%
	I - Batch: 150 | Loss: 0.098 | Acc: 93.958% | Wgt Acc: 97.989%
	I - Batch: 200 | Loss: 0.092 | Acc: 94.531% | Wgt Acc: 98.229%
I - num batch: 222
I - Train -- Loss: 0.094 | Acc: 94.446% | Wgt Acc: 98.200% | LR: 1.250000e-04 | Dur: 136.85s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   1.   0.  62.]
 [  0. 575.   0.   0.  31.]
 [  0.   0. 733.   0.  59.]
 [  1.   0.   0. 535.  35.]
 [  2.   3.   0.   3. 813.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.193 | Acc: 66.469% | Wgt Acc: 57.121% | Dur: 16.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   2.   9.   4.]
 [  1.  36.   4.   2.   5.]
 [  1.   8.  31.   1.   5.]
 [ 10.   5.  11.  50.   3.]
 [ 19.  27.  27.  24. 163.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 145
I - Training: 
	I - Batch: 50 | Loss: 0.088 | Acc: 95.375% | Wgt Acc: 98.703%
	I - Batch: 100 | Loss: 0.087 | Acc: 95.000% | Wgt Acc: 98.353%
	I - Batch: 150 | Loss: 0.088 | Acc: 94.500% | Wgt Acc: 98.149%
	I - Batch: 200 | Loss: 0.093 | Acc: 94.156% | Wgt Acc: 98.026%
I - num batch: 222
I - Train -- Loss: 0.091 | Acc: 94.136% | Wgt Acc: 98.033% | LR: 1.250000e-04 | Dur: 133.43s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  62.]
 [  1. 577.   0.   0.  43.]
 [  0.   0. 731.   0.  49.]
 [  0.   0.   0. 534.  41.]
 [  4.   1.   3.   4. 805.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.028 | Acc: 67.258% | Wgt Acc: 63.326% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   2.  11.  10.]
 [  1.  46.   5.   1.   7.]
 [  1.  10.  37.   3.  13.]
 [ 18.   5.  12.  61.  10.]
 [ 11.  15.  19.  10. 140.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 146
I - Training: 
	I - Batch: 50 | Loss: 0.083 | Acc: 94.625% | Wgt Acc: 98.143%
	I - Batch: 100 | Loss: 0.091 | Acc: 94.062% | Wgt Acc: 97.931%
	I - Batch: 150 | Loss: 0.092 | Acc: 94.083% | Wgt Acc: 97.913%
	I - Batch: 200 | Loss: 0.094 | Acc: 94.062% | Wgt Acc: 97.880%
I - num batch: 222
I - Train -- Loss: 0.095 | Acc: 93.854% | Wgt Acc: 97.814% | LR: 1.250000e-04 | Dur: 135.30s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   1.   3.  85.]
 [  1. 578.   0.   0.  36.]
 [  0.   0. 730.   1.  46.]
 [  3.   0.   0. 532.  33.]
 [  4.   0.   3.   2. 800.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.126 | Acc: 68.442% | Wgt Acc: 59.013% | Dur: 18.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   3.   2.  21.   8.]
 [  0.  32.   2.   1.   1.]
 [  1.  11.  34.   2.   4.]
 [  5.   3.  10.  46.   1.]
 [ 13.  29.  27.  16. 166.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 147
I - Training: 
	I - Batch: 50 | Loss: 0.102 | Acc: 93.625% | Wgt Acc: 98.173%
	I - Batch: 100 | Loss: 0.090 | Acc: 94.312% | Wgt Acc: 98.292%
	I - Batch: 150 | Loss: 0.093 | Acc: 94.583% | Wgt Acc: 98.320%
	I - Batch: 200 | Loss: 0.095 | Acc: 94.188% | Wgt Acc: 98.198%
I - num batch: 222
I - Train -- Loss: 0.096 | Acc: 94.305% | Wgt Acc: 98.223% | LR: 1.250000e-04 | Dur: 134.26s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  74.]
 [  0. 577.   0.   0.  22.]
 [  1.   0. 734.   0.  59.]
 [  0.   0.   0. 536.  39.]
 [  4.   1.   0.   2. 806.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.077 | Acc: 66.864% | Wgt Acc: 60.846% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   5.   1.   8.   9.]
 [  0.  37.   5.   1.   8.]
 [  0.  13.  34.   0.   9.]
 [ 18.   4.  11.  62.   5.]
 [ 13.  19.  24.  15. 149.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 148
I - Training: 
	I - Batch: 50 | Loss: 0.096 | Acc: 92.750% | Wgt Acc: 97.908%
	I - Batch: 100 | Loss: 0.091 | Acc: 93.875% | Wgt Acc: 98.068%
	I - Batch: 150 | Loss: 0.090 | Acc: 94.167% | Wgt Acc: 98.038%
	I - Batch: 200 | Loss: 0.093 | Acc: 94.000% | Wgt Acc: 97.965%
I - num batch: 222
I - Train -- Loss: 0.092 | Acc: 94.108% | Wgt Acc: 98.032% | LR: 1.250000e-04 | Dur: 135.36s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   1.  72.]
 [  0. 577.   0.   0.  26.]
 [  0.   1. 734.   0.  50.]
 [  3.   0.   0. 536.  48.]
 [  7.   0.   0.   1. 804.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.099 | Acc: 68.047% | Wgt Acc: 61.435% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   1.  11.  10.]
 [  0.  40.   2.   1.   3.]
 [  0.  11.  40.   0.  10.]
 [ 17.   3.   9.  53.   4.]
 [ 12.  21.  23.  21. 153.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 149
I - Training: 
	I - Batch: 50 | Loss: 0.084 | Acc: 94.250% | Wgt Acc: 97.899%
	I - Batch: 100 | Loss: 0.087 | Acc: 94.438% | Wgt Acc: 97.979%
	I - Batch: 150 | Loss: 0.094 | Acc: 94.167% | Wgt Acc: 97.771%
	I - Batch: 200 | Loss: 0.092 | Acc: 94.312% | Wgt Acc: 97.970%
I - num batch: 222
I - Train -- Loss: 0.092 | Acc: 94.220% | Wgt Acc: 97.940% | LR: 1.250000e-04 | Dur: 132.61s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   1.   0.   1.  66.]
 [  1. 575.   0.   0.  36.]
 [  0.   0. 732.   1.  46.]
 [  1.   0.   0. 533.  40.]
 [  5.   2.   2.   3. 812.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.099 | Acc: 66.864% | Wgt Acc: 60.247% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   2.  10.   8.]
 [  0.  35.   2.   0.   3.]
 [  0.  15.  38.   0.   9.]
 [ 16.   3.  11.  55.   9.]
 [ 12.  23.  22.  21. 151.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 150
I - Training: 
	I - Batch: 50 | Loss: 0.081 | Acc: 95.375% | Wgt Acc: 98.125%
	I - Batch: 100 | Loss: 0.085 | Acc: 94.938% | Wgt Acc: 98.137%
	I - Batch: 150 | Loss: 0.092 | Acc: 94.167% | Wgt Acc: 97.840%
	I - Batch: 200 | Loss: 0.093 | Acc: 94.375% | Wgt Acc: 97.904%
I - num batch: 222
I - Train -- Loss: 0.094 | Acc: 94.220% | Wgt Acc: 97.892% | LR: 1.250000e-04 | Dur: 135.07s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   1.   0.   0.  67.]
 [  0. 574.   1.   1.  28.]
 [  0.   2. 727.   0.  56.]
 [  0.   0.   0. 536.  35.]
 [  6.   1.   6.   1. 814.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.175 | Acc: 65.483% | Wgt Acc: 60.316% | Dur: 16.62s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.   4.   6.  17.  21.]
 [  0.  36.   3.   1.   3.]
 [  0.   7.  24.   0.   5.]
 [  7.   5.  12.  57.   9.]
 [  8.  26.  30.  11. 142.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 151
I - Training: 
	I - Batch: 50 | Loss: 0.090 | Acc: 94.625% | Wgt Acc: 98.032%
	I - Batch: 100 | Loss: 0.085 | Acc: 94.812% | Wgt Acc: 98.115%
	I - Batch: 150 | Loss: 0.092 | Acc: 94.167% | Wgt Acc: 97.925%
	I - Batch: 200 | Loss: 0.093 | Acc: 94.281% | Wgt Acc: 97.882%
I - num batch: 222
I - Train -- Loss: 0.095 | Acc: 94.108% | Wgt Acc: 97.779% | LR: 1.250000e-04 | Dur: 137.08s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   1.   0.  60.]
 [  0. 576.   0.   0.  30.]
 [  0.   1. 726.   0.  56.]
 [  1.   0.   1. 535.  41.]
 [  8.   1.   6.   3. 813.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.097 | Acc: 66.864% | Wgt Acc: 60.754% | Dur: 14.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   3.  18.  14.]
 [  1.  43.   7.   0.   6.]
 [  1.  12.  34.   1.   9.]
 [  6.   1.   8.  46.   3.]
 [ 12.  18.  23.  21. 148.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 152
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 93.875% | Wgt Acc: 97.827%
	I - Batch: 100 | Loss: 0.105 | Acc: 92.938% | Wgt Acc: 97.269%
	I - Batch: 150 | Loss: 0.106 | Acc: 93.167% | Wgt Acc: 97.429%
	I - Batch: 200 | Loss: 0.100 | Acc: 93.688% | Wgt Acc: 97.726%
I - num batch: 222
I - Train -- Loss: 0.101 | Acc: 93.628% | Wgt Acc: 97.748% | LR: 1.250000e-04 | Dur: 133.04s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   4.  66.]
 [  0. 575.   0.   0.  37.]
 [  1.   0. 731.   0.  54.]
 [  2.   0.   0. 532.  51.]
 [  3.   3.   3.   2. 792.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.044 | Acc: 69.034% | Wgt Acc: 64.122% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   2.  16.  14.]
 [  0.  44.   8.   1.   7.]
 [  0.  12.  35.   3.   7.]
 [  9.   3.   8.  56.   5.]
 [ 11.  16.  22.  10. 147.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 153
I - Training: 
	I - Batch: 50 | Loss: 0.092 | Acc: 94.875% | Wgt Acc: 98.118%
	I - Batch: 100 | Loss: 0.095 | Acc: 94.125% | Wgt Acc: 97.838%
	I - Batch: 150 | Loss: 0.093 | Acc: 94.417% | Wgt Acc: 97.979%
	I - Batch: 200 | Loss: 0.092 | Acc: 94.344% | Wgt Acc: 97.989%
I - num batch: 222
I - Train -- Loss: 0.092 | Acc: 94.361% | Wgt Acc: 98.012% | LR: 1.250000e-04 | Dur: 137.38s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   1.  71.]
 [  0. 575.   0.   1.  28.]
 [  1.   0. 731.   0.  44.]
 [  1.   0.   0. 535.  41.]
 [  5.   3.   3.   1. 816.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.145 | Acc: 68.245% | Wgt Acc: 61.342% | Dur: 14.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   5.  21.  14.]
 [  2.  45.   6.   0.   8.]
 [  2.  13.  37.   1.   4.]
 [  4.   1.   7.  44.   0.]
 [ 14.  16.  20.  20. 154.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 154
I - Training: 
	I - Batch: 50 | Loss: 0.088 | Acc: 94.000% | Wgt Acc: 97.737%
	I - Batch: 100 | Loss: 0.091 | Acc: 94.250% | Wgt Acc: 97.811%
	I - Batch: 150 | Loss: 0.089 | Acc: 93.875% | Wgt Acc: 97.735%
	I - Batch: 200 | Loss: 0.091 | Acc: 93.938% | Wgt Acc: 97.709%
I - num batch: 222
I - Train -- Loss: 0.091 | Acc: 94.023% | Wgt Acc: 97.793% | LR: 1.250000e-04 | Dur: 139.33s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   2.  75.]
 [  0. 574.   0.   0.  27.]
 [  0.   0. 734.   1.  45.]
 [  2.   0.   0. 530.  45.]
 [  6.   4.   0.   5. 808.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.103 | Acc: 68.836% | Wgt Acc: 60.639% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   1.   1.  17.   6.]
 [  0.  35.   2.   1.   3.]
 [  0.  12.  39.   1.   7.]
 [  6.   2.   5.  47.   3.]
 [ 15.  28.  28.  20. 161.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 155
I - Training: 
	I - Batch: 50 | Loss: 0.084 | Acc: 94.500% | Wgt Acc: 98.278%
	I - Batch: 100 | Loss: 0.086 | Acc: 94.375% | Wgt Acc: 98.160%
	I - Batch: 150 | Loss: 0.093 | Acc: 94.083% | Wgt Acc: 97.789%
	I - Batch: 200 | Loss: 0.091 | Acc: 94.344% | Wgt Acc: 98.000%
I - num batch: 222
I - Train -- Loss: 0.093 | Acc: 94.192% | Wgt Acc: 97.926% | LR: 1.250000e-04 | Dur: 133.98s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   1.   0.   3.  82.]
 [  1. 575.   0.   0.  25.]
 [  1.   0. 732.   0.  45.]
 [  0.   1.   0. 531.  37.]
 [  3.   1.   2.   4. 811.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 65.680% | Wgt Acc: 56.799% | Dur: 13.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   1.   2.  10.   4.]
 [  0.  32.   4.   0.   1.]
 [  0.  15.  36.   2.   9.]
 [  8.   3.   8.  48.   7.]
 [ 22.  27.  25.  26. 159.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 156
I - Training: 
	I - Batch: 50 | Loss: 0.095 | Acc: 93.500% | Wgt Acc: 97.879%
	I - Batch: 100 | Loss: 0.091 | Acc: 93.438% | Wgt Acc: 97.682%
	I - Batch: 150 | Loss: 0.091 | Acc: 93.500% | Wgt Acc: 97.749%
	I - Batch: 200 | Loss: 0.092 | Acc: 93.750% | Wgt Acc: 97.770%
I - num batch: 222
I - Train -- Loss: 0.094 | Acc: 93.685% | Wgt Acc: 97.682% | LR: 1.250000e-04 | Dur: 135.23s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   1.   0.   0.  72.]
 [  0. 574.   0.   1.  32.]
 [  0.   0. 729.   0.  51.]
 [  0.   0.   0. 533.  48.]
 [  7.   3.   5.   4. 797.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.057 | Acc: 69.034% | Wgt Acc: 62.553% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   2.   2.  14.   7.]
 [  1.  36.   2.   1.   4.]
 [  0.  13.  38.   0.  10.]
 [  8.   6.  12.  55.   5.]
 [ 12.  21.  21.  16. 154.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 157
I - Training: 
	I - Batch: 50 | Loss: 0.076 | Acc: 95.000% | Wgt Acc: 98.377%
	I - Batch: 100 | Loss: 0.080 | Acc: 95.000% | Wgt Acc: 98.388%
	I - Batch: 150 | Loss: 0.081 | Acc: 95.042% | Wgt Acc: 98.399%
	I - Batch: 200 | Loss: 0.082 | Acc: 95.062% | Wgt Acc: 98.404%
I - num batch: 222
I - Train -- Loss: 0.085 | Acc: 94.953% | Wgt Acc: 98.314% | LR: 1.250000e-04 | Dur: 131.97s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.  62.]
 [  0. 577.   0.   1.  22.]
 [  0.   0. 730.   0.  38.]
 [  0.   0.   0. 535.  46.]
 [  3.   1.   4.   1. 832.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.050 | Acc: 66.864% | Wgt Acc: 59.693% | Dur: 13.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   1.   9.   8.]
 [  0.  37.   5.   1.   2.]
 [  1.  10.  33.   1.  10.]
 [ 17.   3.  11.  57.   6.]
 [ 12.  25.  25.  18. 154.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 158
I - Training: 
	I - Batch: 50 | Loss: 0.087 | Acc: 94.750% | Wgt Acc: 98.231%
	I - Batch: 100 | Loss: 0.086 | Acc: 94.312% | Wgt Acc: 98.148%
	I - Batch: 150 | Loss: 0.088 | Acc: 94.167% | Wgt Acc: 98.162%
	I - Batch: 200 | Loss: 0.087 | Acc: 94.219% | Wgt Acc: 98.148%
I - num batch: 222
I - Train -- Loss: 0.089 | Acc: 93.910% | Wgt Acc: 98.035% | LR: 1.250000e-04 | Dur: 134.56s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   1.  78.]
 [  0. 578.   0.   0.  33.]
 [  0.   0. 731.   0.  51.]
 [  2.   0.   0. 536.  43.]
 [  4.   0.   3.   1. 795.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.042 | Acc: 67.653% | Wgt Acc: 61.804% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   1.  13.   5.]
 [  0.  40.   5.   1.  10.]
 [  0.  12.  34.   1.  12.]
 [  9.   2.  10.  56.   4.]
 [ 15.  22.  25.  15. 149.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 159
I - Training: 
	I - Batch: 50 | Loss: 0.084 | Acc: 94.625% | Wgt Acc: 98.393%
	I - Batch: 100 | Loss: 0.087 | Acc: 94.312% | Wgt Acc: 98.193%
	I - Batch: 150 | Loss: 0.086 | Acc: 94.500% | Wgt Acc: 98.281%
	I - Batch: 200 | Loss: 0.087 | Acc: 94.469% | Wgt Acc: 98.109%
I - num batch: 222
I - Train -- Loss: 0.087 | Acc: 94.305% | Wgt Acc: 98.111% | LR: 1.250000e-04 | Dur: 134.67s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   1.   0.   1.  60.]
 [  1. 574.   0.   0.  35.]
 [  0.   0. 732.   0.  50.]
 [  1.   0.   0. 537.  45.]
 [  3.   3.   2.   0. 810.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 0.985 | Acc: 68.836% | Wgt Acc: 65.990% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   2.  16.  11.]
 [  2.  49.   8.   1.  13.]
 [  0.   6.  40.   0.  12.]
 [  8.   2.   9.  56.   7.]
 [ 11.  18.  16.  13. 137.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 160
I - Training: 
	I - Batch: 50 | Loss: 0.073 | Acc: 95.000% | Wgt Acc: 98.394%
	I - Batch: 100 | Loss: 0.074 | Acc: 94.875% | Wgt Acc: 98.355%
	I - Batch: 150 | Loss: 0.084 | Acc: 94.667% | Wgt Acc: 98.203%
	I - Batch: 200 | Loss: 0.087 | Acc: 94.688% | Wgt Acc: 98.205%
I - num batch: 222
I - Train -- Loss: 0.089 | Acc: 94.559% | Wgt Acc: 98.154% | LR: 1.250000e-04 | Dur: 143.01s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   0.   1.  60.]
 [  1. 577.   0.   0.  29.]
 [  0.   0. 733.   0.  55.]
 [  1.   0.   0. 536.  36.]
 [  7.   1.   1.   1. 820.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.093 | Acc: 68.047% | Wgt Acc: 62.646% | Dur: 16.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   3.  12.   8.]
 [  0.  37.   3.   0.   2.]
 [  0.  14.  41.   2.  18.]
 [  8.   8.   9.  53.   5.]
 [ 13.  16.  19.  19. 147.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 161
I - Training: 
	I - Batch: 50 | Loss: 0.073 | Acc: 94.625% | Wgt Acc: 98.271%
	I - Batch: 100 | Loss: 0.075 | Acc: 94.312% | Wgt Acc: 98.176%
	I - Batch: 150 | Loss: 0.076 | Acc: 94.750% | Wgt Acc: 98.328%
	I - Batch: 200 | Loss: 0.076 | Acc: 95.000% | Wgt Acc: 98.376%
I - num batch: 208
I - Train -- Loss: 0.077 | Acc: 94.922% | Wgt Acc: 98.369% | LR: 1.250000e-04 | Dur: 124.50s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   0.  51.]
 [  0. 577.   0.   0.  23.]
 [  1.   0. 733.   0.  35.]
 [  0.   0.   0. 537.  49.]
 [  7.   1.   1.   1. 623.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.106 | Acc: 65.483% | Wgt Acc: 62.277% | Dur: 14.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   6.   5.  17.  22.]
 [  1.  40.   6.   0.   8.]
 [  0.   9.  31.   2.   5.]
 [ 10.   6.  15.  59.  12.]
 [  8.  17.  18.   8. 133.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 162
I - Training: 
	I - Batch: 50 | Loss: 0.081 | Acc: 94.125% | Wgt Acc: 98.024%
	I - Batch: 100 | Loss: 0.084 | Acc: 94.812% | Wgt Acc: 98.260%
	I - Batch: 150 | Loss: 0.079 | Acc: 95.000% | Wgt Acc: 98.344%
	I - Batch: 200 | Loss: 0.081 | Acc: 94.875% | Wgt Acc: 98.290%
I - num batch: 222
I - Train -- Loss: 0.083 | Acc: 94.643% | Wgt Acc: 98.116% | LR: 1.250000e-04 | Dur: 141.89s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   1.  69.]
 [  0. 577.   0.   0.  20.]
 [  0.   0. 731.   0.  43.]
 [  0.   0.   0. 534.  43.]
 [  7.   1.   3.   3. 825.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.163 | Acc: 65.089% | Wgt Acc: 59.774% | Dur: 15.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   0.   8.   6.]
 [  0.  37.   2.   0.   2.]
 [  1.   9.  32.   1.  11.]
 [ 23.  10.  15.  65.  18.]
 [ 11.  21.  26.  12. 143.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 163
I - Training: 
	I - Batch: 50 | Loss: 0.087 | Acc: 94.500% | Wgt Acc: 98.129%
	I - Batch: 100 | Loss: 0.097 | Acc: 93.875% | Wgt Acc: 97.742%
	I - Batch: 150 | Loss: 0.096 | Acc: 94.208% | Wgt Acc: 97.817%
	I - Batch: 200 | Loss: 0.093 | Acc: 94.406% | Wgt Acc: 97.941%
I - num batch: 222
I - Train -- Loss: 0.094 | Acc: 94.305% | Wgt Acc: 97.911% | LR: 1.250000e-04 | Dur: 134.08s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   1.   0.   2.  70.]
 [  1. 577.   0.   0.  27.]
 [  0.   0. 732.   0.  47.]
 [  2.   0.   0. 533.  39.]
 [  8.   0.   2.   3. 817.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 65.089% | Wgt Acc: 57.825% | Dur: 17.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   4.  14.  16.]
 [  0.  28.   2.   1.   1.]
 [  0.  13.  28.   0.   6.]
 [  7.   5.  12.  57.   6.]
 [ 15.  28.  29.  14. 151.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 164
I - Training: 
	I - Batch: 50 | Loss: 0.077 | Acc: 95.000% | Wgt Acc: 98.401%
	I - Batch: 100 | Loss: 0.078 | Acc: 95.062% | Wgt Acc: 98.340%
	I - Batch: 150 | Loss: 0.078 | Acc: 95.208% | Wgt Acc: 98.404%
	I - Batch: 200 | Loss: 0.079 | Acc: 95.000% | Wgt Acc: 98.392%
I - num batch: 222
I - Train -- Loss: 0.078 | Acc: 95.066% | Wgt Acc: 98.380% | LR: 1.250000e-04 | Dur: 136.48s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   1.   1.  68.]
 [  0. 578.   0.   0.  20.]
 [  0.   0. 733.   0.  40.]
 [  2.   0.   0. 537.  37.]
 [  6.   0.   0.   0. 835.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 66.864% | Wgt Acc: 57.467% | Dur: 14.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   1.  11.   3.]
 [  0.  32.   2.   1.   3.]
 [  2.  13.  36.   0.   8.]
 [ 17.   1.   8.  53.   2.]
 [ 15.  30.  28.  21. 164.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 165
I - Training: 
	I - Batch: 50 | Loss: 0.074 | Acc: 95.000% | Wgt Acc: 98.258%
	I - Batch: 100 | Loss: 0.076 | Acc: 94.750% | Wgt Acc: 98.238%
	I - Batch: 150 | Loss: 0.079 | Acc: 94.792% | Wgt Acc: 98.148%
	I - Batch: 200 | Loss: 0.082 | Acc: 94.844% | Wgt Acc: 98.149%
I - num batch: 222
I - Train -- Loss: 0.084 | Acc: 94.643% | Wgt Acc: 98.110% | LR: 1.250000e-04 | Dur: 134.72s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  60.]
 [  0. 575.   0.   0.  30.]
 [  0.   0. 732.   1.  52.]
 [  1.   1.   0. 533.  33.]
 [  4.   2.   2.   4. 825.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.099 | Acc: 67.061% | Wgt Acc: 60.535% | Dur: 14.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   8.   2.  13.  11.]
 [  0.  33.   4.   0.   4.]
 [  0.  10.  34.   0.   4.]
 [ 10.   8.  12.  57.  10.]
 [ 13.  19.  23.  16. 151.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 166
I - Training: 
	I - Batch: 50 | Loss: 0.082 | Acc: 95.500% | Wgt Acc: 98.263%
	I - Batch: 100 | Loss: 0.082 | Acc: 94.812% | Wgt Acc: 98.195%
	I - Batch: 150 | Loss: 0.083 | Acc: 94.625% | Wgt Acc: 98.103%
	I - Batch: 200 | Loss: 0.085 | Acc: 94.531% | Wgt Acc: 98.127%
I - num batch: 222
I - Train -- Loss: 0.086 | Acc: 94.474% | Wgt Acc: 98.095% | LR: 1.250000e-04 | Dur: 134.81s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   1.   1.   0.  74.]
 [  0. 575.   1.   0.  27.]
 [  0.   0. 729.   0.  42.]
 [  0.   1.   0. 534.  39.]
 [  2.   1.   3.   4. 818.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.089 | Acc: 68.245% | Wgt Acc: 61.873% | Dur: 13.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   1.  14.   8.]
 [  0.  35.   2.   0.   2.]
 [  0.  14.  42.   2.  13.]
 [  9.   1.   7.  54.   5.]
 [ 16.  25.  23.  16. 152.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 167
I - Training: 
	I - Batch: 50 | Loss: 0.083 | Acc: 95.625% | Wgt Acc: 98.670%
	I - Batch: 100 | Loss: 0.089 | Acc: 94.625% | Wgt Acc: 98.263%
	I - Batch: 150 | Loss: 0.084 | Acc: 94.917% | Wgt Acc: 98.399%
	I - Batch: 200 | Loss: 0.083 | Acc: 94.969% | Wgt Acc: 98.467%
I - num batch: 222
I - Train -- Loss: 0.081 | Acc: 94.982% | Wgt Acc: 98.467% | LR: 1.250000e-04 | Dur: 131.41s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  62.]
 [  0. 578.   0.   0.  21.]
 [  0.   0. 732.   0.  56.]
 [  0.   0.   0. 537.  33.]
 [  3.   0.   2.   1. 828.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.083 | Acc: 68.047% | Wgt Acc: 60.743% | Dur: 13.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   2.   3.  14.   8.]
 [  0.  38.   4.   0.   3.]
 [  0.  10.  32.   1.   9.]
 [  7.   2.   8.  51.   4.]
 [ 13.  26.  28.  20. 156.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 168
I - Training: 
	I - Batch: 50 | Loss: 0.074 | Acc: 95.375% | Wgt Acc: 98.513%
	I - Batch: 100 | Loss: 0.083 | Acc: 94.500% | Wgt Acc: 98.205%
	I - Batch: 150 | Loss: 0.083 | Acc: 94.625% | Wgt Acc: 98.231%
	I - Batch: 200 | Loss: 0.081 | Acc: 94.719% | Wgt Acc: 98.260%
I - num batch: 222
I - Train -- Loss: 0.082 | Acc: 94.756% | Wgt Acc: 98.265% | LR: 1.250000e-04 | Dur: 142.38s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   0.  74.]
 [  0. 576.   0.   0.  25.]
 [  1.   0. 734.   0.  37.]
 [  0.   0.   0. 537.  39.]
 [  7.   2.   0.   1. 825.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.055 | Acc: 68.245% | Wgt Acc: 61.654% | Dur: 14.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   0.  10.   9.]
 [  1.  38.   4.   0.   4.]
 [  0.   8.  33.   1.   4.]
 [ 14.   4.  11.  61.   9.]
 [ 13.  25.  27.  14. 154.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 169
I - Training: 
	I - Batch: 50 | Loss: 0.085 | Acc: 95.000% | Wgt Acc: 98.487%
	I - Batch: 100 | Loss: 0.076 | Acc: 95.750% | Wgt Acc: 98.459%
	I - Batch: 150 | Loss: 0.079 | Acc: 95.375% | Wgt Acc: 98.363%
	I - Batch: 200 | Loss: 0.082 | Acc: 95.281% | Wgt Acc: 98.271%
I - num batch: 222
I - Train -- Loss: 0.082 | Acc: 95.207% | Wgt Acc: 98.294% | LR: 1.250000e-04 | Dur: 139.49s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  51.]
 [  0. 575.   1.   0.  30.]
 [  0.   0. 732.   1.  44.]
 [  0.   0.   0. 534.  31.]
 [  5.   3.   1.   3. 844.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.109 | Acc: 67.456% | Wgt Acc: 59.013% | Dur: 14.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   3.   0.  12.   7.]
 [  0.  31.   2.   1.   2.]
 [  0.  11.  35.   0.   7.]
 [ 15.   6.  10.  59.   3.]
 [ 17.  27.  28.  14. 161.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 170
I - Training: 
	I - Batch: 50 | Loss: 0.084 | Acc: 94.250% | Wgt Acc: 97.813%
	I - Batch: 100 | Loss: 0.085 | Acc: 94.688% | Wgt Acc: 97.975%
	I - Batch: 150 | Loss: 0.081 | Acc: 94.917% | Wgt Acc: 98.234%
	I - Batch: 200 | Loss: 0.086 | Acc: 94.500% | Wgt Acc: 98.032%
I - num batch: 222
I - Train -- Loss: 0.085 | Acc: 94.700% | Wgt Acc: 98.072% | LR: 1.250000e-04 | Dur: 135.82s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   2.  54.]
 [  1. 574.   1.   1.  33.]
 [  0.   1. 731.   0.  47.]
 [  0.   0.   0. 534.  37.]
 [  5.   3.   2.   1. 829.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.166 | Acc: 66.864% | Wgt Acc: 57.006% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  10.   4.]
 [  0.  36.   2.   0.   4.]
 [  2.  10.  34.   0.   4.]
 [  9.   1.   3.  49.   2.]
 [ 23.  29.  34.  27. 166.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 171
I - Training: 
	I - Batch: 50 | Loss: 0.071 | Acc: 95.375% | Wgt Acc: 98.600%
	I - Batch: 100 | Loss: 0.081 | Acc: 94.938% | Wgt Acc: 98.188%
	I - Batch: 150 | Loss: 0.086 | Acc: 94.583% | Wgt Acc: 98.055%
	I - Batch: 200 | Loss: 0.088 | Acc: 94.375% | Wgt Acc: 97.921%
I - num batch: 222
I - Train -- Loss: 0.086 | Acc: 94.418% | Wgt Acc: 97.992% | LR: 1.250000e-04 | Dur: 140.62s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   4.  61.]
 [  0. 576.   0.   0.  31.]
 [  1.   0. 731.   0.  53.]
 [  1.   1.   2. 532.  36.]
 [  4.   1.   1.   2. 819.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.057 | Acc: 68.639% | Wgt Acc: 63.372% | Dur: 15.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   4.  18.   7.]
 [  1.  45.   5.   1.   6.]
 [  0.  11.  43.   6.  14.]
 [  5.   2.   6.  46.   6.]
 [ 15.  17.  17.  15. 147.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 172
I - Training: 
	I - Batch: 50 | Loss: 0.072 | Acc: 96.250% | Wgt Acc: 98.839%
	I - Batch: 100 | Loss: 0.069 | Acc: 96.312% | Wgt Acc: 98.862%
	I - Batch: 150 | Loss: 0.072 | Acc: 95.917% | Wgt Acc: 98.593%
	I - Batch: 200 | Loss: 0.075 | Acc: 95.594% | Wgt Acc: 98.525%
I - num batch: 222
I - Train -- Loss: 0.076 | Acc: 95.461% | Wgt Acc: 98.487% | LR: 1.250000e-04 | Dur: 145.43s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  51.]
 [  0. 576.   1.   0.  29.]
 [  0.   0. 731.   0.  40.]
 [  0.   0.   0. 538.  31.]
 [  5.   2.   2.   0. 849.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.058 | Acc: 66.667% | Wgt Acc: 59.105% | Dur: 15.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   5.   2.  12.  11.]
 [  0.  33.   3.   0.   2.]
 [  0.  14.  35.   2.   8.]
 [ 10.   3.   9.  54.   4.]
 [ 17.  23.  26.  18. 155.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 173
I - Training: 
	I - Batch: 50 | Loss: 0.090 | Acc: 94.000% | Wgt Acc: 98.111%
	I - Batch: 100 | Loss: 0.092 | Acc: 94.125% | Wgt Acc: 97.845%
	I - Batch: 150 | Loss: 0.088 | Acc: 94.208% | Wgt Acc: 98.021%
	I - Batch: 200 | Loss: 0.086 | Acc: 94.375% | Wgt Acc: 98.006%
I - num batch: 222
I - Train -- Loss: 0.086 | Acc: 94.305% | Wgt Acc: 98.000% | LR: 1.250000e-04 | Dur: 140.53s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   2.  71.]
 [  0. 575.   0.   0.  24.]
 [  0.   0. 730.   0.  48.]
 [  2.   0.   1. 536.  43.]
 [  5.   3.   3.   0. 814.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.087 | Acc: 67.456% | Wgt Acc: 58.355% | Dur: 17.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   0.   0.  10.   4.]
 [  0.  38.   2.   1.   3.]
 [  1.  10.  35.   0.   5.]
 [ 15.   1.   8.  54.   4.]
 [ 21.  29.  30.  21. 164.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 174
I - Training: 
	I - Batch: 50 | Loss: 0.075 | Acc: 94.750% | Wgt Acc: 98.088%
	I - Batch: 100 | Loss: 0.075 | Acc: 94.938% | Wgt Acc: 98.317%
	I - Batch: 150 | Loss: 0.078 | Acc: 94.792% | Wgt Acc: 98.337%
	I - Batch: 200 | Loss: 0.079 | Acc: 95.000% | Wgt Acc: 98.415%
I - num batch: 222
I - Train -- Loss: 0.080 | Acc: 94.841% | Wgt Acc: 98.367% | LR: 1.250000e-04 | Dur: 134.67s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   0.  60.]
 [  0. 574.   0.   0.  33.]
 [  0.   1. 732.   0.  44.]
 [  0.   0.   0. 537.  38.]
 [  1.   3.   2.   1. 825.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 69.034% | Wgt Acc: 62.842% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   0.   8.   7.]
 [  1.  39.   5.   0.   9.]
 [  0.  15.  37.   0.   7.]
 [  9.   3.   8.  57.   4.]
 [ 14.  19.  25.  21. 153.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 175
I - Training: 
	I - Batch: 50 | Loss: 0.074 | Acc: 95.000% | Wgt Acc: 98.169%
	I - Batch: 100 | Loss: 0.077 | Acc: 95.312% | Wgt Acc: 98.280%
	I - Batch: 150 | Loss: 0.083 | Acc: 94.833% | Wgt Acc: 98.040%
	I - Batch: 200 | Loss: 0.086 | Acc: 94.719% | Wgt Acc: 98.027%
I - num batch: 222
I - Train -- Loss: 0.088 | Acc: 94.728% | Wgt Acc: 97.993% | LR: 1.250000e-04 | Dur: 141.97s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   1.   1.   1.  50.]
 [  0. 574.   1.   0.  29.]
 [  1.   2. 729.   0.  51.]
 [  0.   0.   0. 533.  37.]
 [  5.   1.   3.   4. 833.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.154 | Acc: 66.469% | Wgt Acc: 60.501% | Dur: 16.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   2.   9.   9.]
 [  0.  43.   7.   1.   9.]
 [  1.   9.  28.   0.   6.]
 [ 14.   3.  13.  58.   8.]
 [ 13.  20.  25.  18. 148.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 176
I - Training: 
	I - Batch: 50 | Loss: 0.084 | Acc: 94.875% | Wgt Acc: 98.181%
	I - Batch: 100 | Loss: 0.084 | Acc: 94.812% | Wgt Acc: 98.263%
	I - Batch: 150 | Loss: 0.083 | Acc: 94.958% | Wgt Acc: 98.300%
	I - Batch: 200 | Loss: 0.085 | Acc: 94.719% | Wgt Acc: 98.128%
I - num batch: 222
I - Train -- Loss: 0.088 | Acc: 94.418% | Wgt Acc: 97.943% | LR: 1.250000e-04 | Dur: 136.53s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   1.  68.]
 [  0. 576.   0.   0.  29.]
 [  1.   2. 731.   0.  51.]
 [  2.   0.   0. 534.  31.]
 [  7.   0.   3.   3. 821.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.047 | Acc: 68.836% | Wgt Acc: 64.203% | Dur: 18.77s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   2.   2.  14.  10.]
 [  1.  42.   5.   0.   4.]
 [  0.  10.  36.   2.  13.]
 [  4.   2.   7.  54.   8.]
 [ 11.  22.  25.  16. 145.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 177
I - Training: 
	I - Batch: 50 | Loss: 0.097 | Acc: 94.375% | Wgt Acc: 97.698%
	I - Batch: 100 | Loss: 0.091 | Acc: 94.938% | Wgt Acc: 98.004%
	I - Batch: 150 | Loss: 0.086 | Acc: 94.917% | Wgt Acc: 98.123%
	I - Batch: 200 | Loss: 0.087 | Acc: 95.156% | Wgt Acc: 98.238%
I - num batch: 222
I - Train -- Loss: 0.088 | Acc: 94.982% | Wgt Acc: 98.208% | LR: 1.250000e-04 | Dur: 133.10s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   1.  64.]
 [  0. 575.   0.   0.  22.]
 [  0.   0. 731.   0.  45.]
 [  0.   0.   0. 535.  32.]
 [  6.   3.   2.   2. 837.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.170 | Acc: 66.272% | Wgt Acc: 57.191% | Dur: 14.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   2.   2.  16.   8.]
 [  0.  33.   2.   0.   2.]
 [  0.  14.  27.   0.   5.]
 [  8.   1.   5.  48.   4.]
 [ 13.  28.  39.  22. 161.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 178
I - Training: 
	I - Batch: 50 | Loss: 0.093 | Acc: 95.625% | Wgt Acc: 98.414%
	I - Batch: 100 | Loss: 0.088 | Acc: 95.312% | Wgt Acc: 98.480%
	I - Batch: 150 | Loss: 0.083 | Acc: 95.417% | Wgt Acc: 98.466%
	I - Batch: 200 | Loss: 0.083 | Acc: 95.438% | Wgt Acc: 98.417%
I - num batch: 222
I - Train -- Loss: 0.082 | Acc: 95.574% | Wgt Acc: 98.478% | LR: 1.250000e-04 | Dur: 137.25s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   1.   2.  54.]
 [  0. 578.   0.   0.  24.]
 [  0.   0. 730.   0.  39.]
 [  1.   0.   0. 533.  29.]
 [  1.   0.   3.   3. 854.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.104 | Acc: 67.850% | Wgt Acc: 60.362% | Dur: 18.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   2.   2.  19.   6.]
 [  0.  40.   2.   1.   4.]
 [  0.  10.  36.   0.   9.]
 [  6.   2.   9.  45.   5.]
 [ 15.  24.  26.  21. 156.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 179
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 95.625% | Wgt Acc: 98.542%
	I - Batch: 100 | Loss: 0.077 | Acc: 95.625% | Wgt Acc: 98.412%
	I - Batch: 150 | Loss: 0.080 | Acc: 95.542% | Wgt Acc: 98.390%
	I - Batch: 200 | Loss: 0.078 | Acc: 95.469% | Wgt Acc: 98.378%
I - num batch: 222
I - Train -- Loss: 0.080 | Acc: 95.376% | Wgt Acc: 98.335% | LR: 1.250000e-04 | Dur: 145.03s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.  51.]
 [  0. 576.   1.   0.  24.]
 [  0.   1. 731.   0.  31.]
 [  0.   0.   1. 532.  44.]
 [  3.   1.   1.   5. 850.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.074 | Acc: 66.469% | Wgt Acc: 61.585% | Dur: 16.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   6.   2.  12.  14.]
 [  0.  41.   4.   0.   4.]
 [  1.   8.  34.   0.   7.]
 [ 16.   4.  13.  61.  12.]
 [ 13.  19.  22.  13. 143.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 180
I - Training: 
	I - Batch: 50 | Loss: 0.076 | Acc: 95.375% | Wgt Acc: 98.597%
	I - Batch: 100 | Loss: 0.079 | Acc: 95.562% | Wgt Acc: 98.597%
	I - Batch: 150 | Loss: 0.075 | Acc: 95.667% | Wgt Acc: 98.605%
	I - Batch: 200 | Loss: 0.074 | Acc: 95.594% | Wgt Acc: 98.545%
I - num batch: 222
I - Train -- Loss: 0.073 | Acc: 95.686% | Wgt Acc: 98.573% | LR: 1.250000e-04 | Dur: 139.08s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  47.]
 [  0. 578.   0.   0.  20.]
 [  0.   0. 731.   0.  40.]
 [  0.   0.   0. 536.  37.]
 [  4.   0.   3.   1. 856.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.167 | Acc: 67.850% | Wgt Acc: 60.224% | Dur: 15.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   3.  16.  13.]
 [  0.  36.   3.   0.   3.]
 [  0.   9.  34.   0.   5.]
 [  6.   2.   8.  50.   2.]
 [ 15.  27.  27.  20. 157.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 181
I - Training: 
	I - Batch: 50 | Loss: 0.062 | Acc: 96.625% | Wgt Acc: 99.083%
	I - Batch: 100 | Loss: 0.070 | Acc: 95.625% | Wgt Acc: 98.739%
	I - Batch: 150 | Loss: 0.072 | Acc: 95.375% | Wgt Acc: 98.650%
	I - Batch: 200 | Loss: 0.072 | Acc: 95.406% | Wgt Acc: 98.626%
I - num batch: 222
I - Train -- Loss: 0.073 | Acc: 95.405% | Wgt Acc: 98.604% | LR: 1.250000e-04 | Dur: 139.04s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  51.]
 [  0. 578.   0.   0.  27.]
 [  0.   0. 734.   0.  47.]
 [  0.   0.   0. 535.  33.]
 [  2.   0.   0.   3. 842.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.165 | Acc: 65.680% | Wgt Acc: 58.644% | Dur: 15.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   5.   4.  22.  17.]
 [  0.  38.   3.   2.   3.]
 [  0.   8.  32.   0.   7.]
 [  5.   2.   8.  44.   3.]
 [ 14.  25.  28.  18. 150.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 182
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 95.875% | Wgt Acc: 98.777%
	I - Batch: 100 | Loss: 0.073 | Acc: 95.312% | Wgt Acc: 98.434%
	I - Batch: 150 | Loss: 0.073 | Acc: 95.333% | Wgt Acc: 98.415%
	I - Batch: 200 | Loss: 0.072 | Acc: 95.469% | Wgt Acc: 98.520%
I - num batch: 222
I - Train -- Loss: 0.074 | Acc: 95.433% | Wgt Acc: 98.443% | LR: 1.250000e-04 | Dur: 144.06s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   1.  55.]
 [  0. 578.   0.   1.  23.]
 [  0.   0. 732.   0.  38.]
 [  3.   0.   0. 534.  35.]
 [  2.   0.   2.   2. 849.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.120 | Acc: 66.864% | Wgt Acc: 59.532% | Dur: 14.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   1.   8.  11.]
 [  0.  37.   4.   1.   5.]
 [  0.  14.  30.   1.   2.]
 [ 15.   5.  10.  59.   7.]
 [ 15.  19.  30.  17. 155.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 183
I - Training: 
	I - Batch: 50 | Loss: 0.086 | Acc: 94.000% | Wgt Acc: 98.097%
	I - Batch: 100 | Loss: 0.079 | Acc: 95.125% | Wgt Acc: 98.416%
	I - Batch: 150 | Loss: 0.084 | Acc: 94.625% | Wgt Acc: 98.245%
	I - Batch: 200 | Loss: 0.083 | Acc: 94.781% | Wgt Acc: 98.269%
I - num batch: 222
I - Train -- Loss: 0.082 | Acc: 94.897% | Wgt Acc: 98.303% | LR: 1.250000e-04 | Dur: 133.62s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  74.]
 [  0. 576.   2.   0.  23.]
 [  0.   1. 731.   0.  38.]
 [  0.   0.   0. 537.  35.]
 [  5.   1.   1.   1. 830.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.111 | Acc: 64.892% | Wgt Acc: 58.736% | Dur: 14.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   6.   3.  20.  20.]
 [  0.  36.   4.   0.   6.]
 [  0.   8.  30.   1.   4.]
 [ 12.   7.  11.  50.   5.]
 [  8.  21.  27.  15. 145.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 184
I - Training: 
	I - Batch: 50 | Loss: 0.080 | Acc: 94.000% | Wgt Acc: 97.614%
	I - Batch: 100 | Loss: 0.079 | Acc: 94.438% | Wgt Acc: 98.042%
	I - Batch: 150 | Loss: 0.077 | Acc: 94.667% | Wgt Acc: 98.124%
	I - Batch: 200 | Loss: 0.078 | Acc: 94.875% | Wgt Acc: 98.168%
I - num batch: 222
I - Train -- Loss: 0.078 | Acc: 94.784% | Wgt Acc: 98.154% | LR: 1.250000e-04 | Dur: 142.44s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   1.   0.  57.]
 [  0. 577.   0.   0.  30.]
 [  0.   0. 731.   0.  45.]
 [  3.   0.   0. 534.  38.]
 [  4.   1.   2.   4. 830.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.097 | Acc: 69.625% | Wgt Acc: 61.562% | Dur: 17.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   3.   2.  20.  13.]
 [  0.  37.   3.   1.   1.]
 [  0.  10.  34.   0.   2.]
 [  5.   3.   7.  49.   2.]
 [ 12.  25.  29.  16. 162.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 185
I - Training: 
	I - Batch: 50 | Loss: 0.078 | Acc: 95.375% | Wgt Acc: 98.613%
	I - Batch: 100 | Loss: 0.080 | Acc: 95.188% | Wgt Acc: 98.505%
	I - Batch: 150 | Loss: 0.078 | Acc: 95.458% | Wgt Acc: 98.558%
	I - Batch: 200 | Loss: 0.079 | Acc: 95.281% | Wgt Acc: 98.492%
I - num batch: 222
I - Train -- Loss: 0.079 | Acc: 95.320% | Wgt Acc: 98.472% | LR: 1.250000e-04 | Dur: 134.88s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   1.  61.]
 [  0. 577.   1.   0.  25.]
 [  0.   0. 733.   1.  36.]
 [  0.   0.   0. 536.  35.]
 [  5.   1.   0.   0. 843.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.194 | Acc: 66.667% | Wgt Acc: 58.897% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   4.  13.  12.]
 [  0.  31.   3.   1.   3.]
 [  0.   7.  28.   0.   4.]
 [  7.   4.  10.  55.   5.]
 [ 13.  32.  30.  17. 156.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 186
I - Training: 
	I - Batch: 50 | Loss: 0.097 | Acc: 93.375% | Wgt Acc: 97.692%
	I - Batch: 100 | Loss: 0.084 | Acc: 94.625% | Wgt Acc: 98.246%
	I - Batch: 150 | Loss: 0.085 | Acc: 94.792% | Wgt Acc: 98.220%
	I - Batch: 200 | Loss: 0.085 | Acc: 94.656% | Wgt Acc: 98.144%
I - num batch: 222
I - Train -- Loss: 0.086 | Acc: 94.756% | Wgt Acc: 98.121% | LR: 1.250000e-04 | Dur: 141.00s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   1.   1.  67.]
 [  0. 576.   0.   0.  33.]
 [  0.   0. 733.   0.  33.]
 [  2.   0.   0. 535.  37.]
 [  8.   2.   0.   2. 830.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.090 | Acc: 67.456% | Wgt Acc: 60.178% | Dur: 19.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   5.   2.  19.  16.]
 [  1.  32.   2.   1.   0.]
 [  1.   8.  38.   1.   6.]
 [  4.   4.   9.  46.   4.]
 [ 10.  29.  24.  19. 154.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 187
I - Training: 
	I - Batch: 50 | Loss: 0.081 | Acc: 94.500% | Wgt Acc: 97.991%
	I - Batch: 100 | Loss: 0.084 | Acc: 94.500% | Wgt Acc: 98.163%
	I - Batch: 150 | Loss: 0.081 | Acc: 94.667% | Wgt Acc: 98.168%
	I - Batch: 200 | Loss: 0.083 | Acc: 94.594% | Wgt Acc: 98.053%
I - num batch: 222
I - Train -- Loss: 0.084 | Acc: 94.700% | Wgt Acc: 98.076% | LR: 1.250000e-04 | Dur: 138.16s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   1.   0.  57.]
 [  1. 573.   0.   1.  28.]
 [  0.   0. 731.   0.  52.]
 [  0.   1.   0. 536.  34.]
 [  6.   4.   2.   1. 829.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.180 | Acc: 66.469% | Wgt Acc: 57.744% | Dur: 14.81s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   1.   1.   6.   3.]
 [  0.  34.   2.   1.   3.]
 [  2.   7.  24.   0.   8.]
 [ 10.   3.  15.  60.   5.]
 [ 18.  33.  33.  19. 161.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 188
I - Training: 
	I - Batch: 50 | Loss: 0.072 | Acc: 95.125% | Wgt Acc: 98.476%
	I - Batch: 100 | Loss: 0.075 | Acc: 95.312% | Wgt Acc: 98.537%
	I - Batch: 150 | Loss: 0.070 | Acc: 95.375% | Wgt Acc: 98.596%
	I - Batch: 200 | Loss: 0.069 | Acc: 95.500% | Wgt Acc: 98.681%
I - num batch: 208
I - Train -- Loss: 0.070 | Acc: 95.373% | Wgt Acc: 98.625% | LR: 1.250000e-04 | Dur: 130.94s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  42.]
 [  0. 576.   0.   0.  30.]
 [  0.   1. 734.   0.  38.]
 [  1.   0.   0. 537.  38.]
 [  2.   1.   0.   1. 633.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 66.469% | Wgt Acc: 59.047% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   3.   1.  13.  13.]
 [  0.  37.   5.   1.   4.]
 [  1.  13.  30.   0.   5.]
 [  9.   2.  12.  52.   4.]
 [ 14.  23.  27.  20. 154.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 189
I - Training: 
	I - Batch: 50 | Loss: 0.068 | Acc: 96.500% | Wgt Acc: 99.050%
	I - Batch: 100 | Loss: 0.077 | Acc: 95.312% | Wgt Acc: 98.407%
	I - Batch: 150 | Loss: 0.076 | Acc: 95.583% | Wgt Acc: 98.445%
	I - Batch: 200 | Loss: 0.073 | Acc: 95.688% | Wgt Acc: 98.498%
I - num batch: 222
I - Train -- Loss: 0.073 | Acc: 95.630% | Wgt Acc: 98.459% | LR: 1.250000e-04 | Dur: 136.06s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  49.]
 [  0. 574.   0.   1.  21.]
 [  0.   2. 734.   0.  30.]
 [  0.   0.   0. 533.  43.]
 [  3.   2.   0.   4. 857.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.026 | Acc: 69.428% | Wgt Acc: 62.738% | Dur: 18.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   3.   1.  13.   7.]
 [  0.  40.   3.   1.   3.]
 [  0.  14.  36.   0.   9.]
 [  8.   3.   8.  56.   5.]
 [ 16.  18.  27.  16. 156.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 190
I - Training: 
	I - Batch: 50 | Loss: 0.062 | Acc: 96.000% | Wgt Acc: 98.804%
	I - Batch: 100 | Loss: 0.069 | Acc: 95.875% | Wgt Acc: 98.707%
	I - Batch: 150 | Loss: 0.071 | Acc: 95.833% | Wgt Acc: 98.616%
	I - Batch: 200 | Loss: 0.074 | Acc: 95.625% | Wgt Acc: 98.554%
I - num batch: 222
I - Train -- Loss: 0.075 | Acc: 95.517% | Wgt Acc: 98.493% | LR: 1.250000e-04 | Dur: 143.24s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   1.  63.]
 [  1. 576.   0.   0.  22.]
 [  0.   0. 734.   0.  33.]
 [  0.   0.   0. 535.  31.]
 [  4.   2.   0.   2. 851.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.090 | Acc: 69.231% | Wgt Acc: 61.769% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   3.   1.   8.   9.]
 [  0.  39.   2.   0.   3.]
 [  1.   9.  36.   1.   5.]
 [ 10.   1.   6.  53.   4.]
 [ 13.  26.  30.  24. 159.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 191
I - Training: 
	I - Batch: 50 | Loss: 0.077 | Acc: 95.375% | Wgt Acc: 98.379%
	I - Batch: 100 | Loss: 0.072 | Acc: 95.375% | Wgt Acc: 98.440%
	I - Batch: 150 | Loss: 0.075 | Acc: 95.375% | Wgt Acc: 98.489%
	I - Batch: 200 | Loss: 0.077 | Acc: 95.125% | Wgt Acc: 98.411%
I - num batch: 222
I - Train -- Loss: 0.077 | Acc: 95.151% | Wgt Acc: 98.424% | LR: 1.250000e-04 | Dur: 141.19s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  57.]
 [  0. 576.   0.   0.  18.]
 [  0.   0. 733.   0.  56.]
 [  1.   0.   0. 536.  32.]
 [  3.   2.   1.   1. 837.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.034 | Acc: 68.836% | Wgt Acc: 63.810% | Dur: 16.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   5.   3.  13.  17.]
 [  0.  42.   3.   0.   6.]
 [  0.   9.  33.   0.   6.]
 [  6.   2.   5.  55.   4.]
 [ 10.  20.  31.  18. 147.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 192
I - Training: 
	I - Batch: 50 | Loss: 0.083 | Acc: 95.375% | Wgt Acc: 98.216%
	I - Batch: 100 | Loss: 0.082 | Acc: 94.688% | Wgt Acc: 98.224%
	I - Batch: 150 | Loss: 0.076 | Acc: 95.458% | Wgt Acc: 98.548%
	I - Batch: 200 | Loss: 0.077 | Acc: 95.312% | Wgt Acc: 98.473%
I - num batch: 222
I - Train -- Loss: 0.079 | Acc: 95.151% | Wgt Acc: 98.366% | LR: 1.250000e-04 | Dur: 143.09s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  61.]
 [  0. 576.   0.   0.  26.]
 [  0.   0. 732.   0.  47.]
 [  0.   0.   0. 535.  27.]
 [  4.   2.   2.   2. 839.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.098 | Acc: 69.428% | Wgt Acc: 61.435% | Dur: 16.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   3.  12.   5.]
 [  0.  37.   2.   0.   2.]
 [  1.  11.  31.   0.   6.]
 [ 12.   4.  10.  55.   5.]
 [  8.  22.  29.  19. 162.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 193
I - Training: 
	I - Batch: 50 | Loss: 0.090 | Acc: 94.125% | Wgt Acc: 98.074%
	I - Batch: 100 | Loss: 0.087 | Acc: 94.438% | Wgt Acc: 98.173%
	I - Batch: 150 | Loss: 0.083 | Acc: 94.792% | Wgt Acc: 98.337%
	I - Batch: 200 | Loss: 0.081 | Acc: 94.844% | Wgt Acc: 98.321%
I - num batch: 222
I - Train -- Loss: 0.084 | Acc: 94.615% | Wgt Acc: 98.226% | LR: 1.250000e-04 | Dur: 138.84s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   0.  73.]
 [  1. 576.   0.   0.  23.]
 [  0.   0. 732.   0.  49.]
 [  1.   0.   0. 537.  35.]
 [  4.   2.   2.   1. 820.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.125 | Acc: 67.061% | Wgt Acc: 57.121% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   1.   1.  17.   5.]
 [  0.  33.   2.   1.   1.]
 [  1.  12.  36.   2.   7.]
 [  5.   2.   4.  44.   1.]
 [ 21.  30.  32.  22. 166.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 194
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 95.250% | Wgt Acc: 98.687%
	I - Batch: 100 | Loss: 0.089 | Acc: 95.312% | Wgt Acc: 98.460%
	I - Batch: 150 | Loss: 0.084 | Acc: 95.208% | Wgt Acc: 98.438%
	I - Batch: 200 | Loss: 0.082 | Acc: 95.375% | Wgt Acc: 98.486%
I - num batch: 222
I - Train -- Loss: 0.081 | Acc: 95.235% | Wgt Acc: 98.475% | LR: 1.250000e-04 | Dur: 136.30s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  57.]
 [  0. 578.   0.   0.  25.]
 [  0.   0. 733.   0.  47.]
 [  1.   0.   0. 535.  32.]
 [  3.   0.   1.   3. 839.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.234 | Acc: 65.089% | Wgt Acc: 55.230% | Dur: 16.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   3.   3.  12.   8.]
 [  0.  29.   2.   1.   1.]
 [  2.  14.  32.   0.   4.]
 [ 11.   2.   7.  52.   4.]
 [ 21.  30.  31.  21. 163.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 195
I - Training: 
	I - Batch: 50 | Loss: 0.075 | Acc: 95.500% | Wgt Acc: 98.686%
	I - Batch: 100 | Loss: 0.072 | Acc: 95.375% | Wgt Acc: 98.555%
	I - Batch: 150 | Loss: 0.070 | Acc: 95.667% | Wgt Acc: 98.657%
	I - Batch: 200 | Loss: 0.073 | Acc: 95.531% | Wgt Acc: 98.592%
I - num batch: 222
I - Train -- Loss: 0.072 | Acc: 95.658% | Wgt Acc: 98.620% | LR: 1.250000e-04 | Dur: 133.27s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  50.]
 [  0. 578.   0.   0.  21.]
 [  0.   0. 731.   0.  48.]
 [  0.   0.   0. 536.  28.]
 [  2.   0.   3.   2. 853.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.090 | Acc: 68.836% | Wgt Acc: 61.342% | Dur: 14.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   4.   3.  19.  10.]
 [  0.  40.   4.   1.   4.]
 [  0.   9.  33.   0.   5.]
 [  6.   5.  11.  48.   3.]
 [ 12.  20.  24.  18. 158.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 196
I - Training: 
	I - Batch: 50 | Loss: 0.062 | Acc: 95.750% | Wgt Acc: 98.590%
	I - Batch: 100 | Loss: 0.061 | Acc: 96.000% | Wgt Acc: 98.551%
	I - Batch: 150 | Loss: 0.063 | Acc: 95.958% | Wgt Acc: 98.619%
	I - Batch: 200 | Loss: 0.069 | Acc: 95.562% | Wgt Acc: 98.508%
I - num batch: 222
I - Train -- Loss: 0.071 | Acc: 95.461% | Wgt Acc: 98.482% | LR: 1.250000e-04 | Dur: 140.22s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  50.]
 [  0. 575.   0.   0.  25.]
 [  0.   1. 732.   0.  43.]
 [  0.   0.   1. 537.  33.]
 [  4.   2.   1.   1. 849.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.044 | Acc: 67.653% | Wgt Acc: 60.189% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   1.   0.  13.   6.]
 [  0.  40.   3.   0.   3.]
 [  1.  17.  39.   5.  13.]
 [ 10.   1.   6.  49.   2.]
 [ 18.  19.  27.  19. 156.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 197
I - Training: 
	I - Batch: 50 | Loss: 0.081 | Acc: 94.250% | Wgt Acc: 97.913%
	I - Batch: 100 | Loss: 0.072 | Acc: 95.188% | Wgt Acc: 98.389%
	I - Batch: 150 | Loss: 0.072 | Acc: 95.458% | Wgt Acc: 98.472%
	I - Batch: 200 | Loss: 0.073 | Acc: 95.594% | Wgt Acc: 98.421%
I - num batch: 222
I - Train -- Loss: 0.072 | Acc: 95.658% | Wgt Acc: 98.478% | LR: 1.250000e-04 | Dur: 134.26s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  45.]
 [  0. 575.   0.   1.  13.]
 [  0.   0. 731.   0.  45.]
 [  0.   0.   0. 536.  39.]
 [  4.   3.   3.   0. 858.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.022 | Acc: 71.598% | Wgt Acc: 68.550% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   1.   4.   7.   9.]
 [  1.  55.   8.   1.   8.]
 [  0.   7.  33.   1.   9.]
 [ 13.   5.  12.  66.  10.]
 [  9.  10.  18.  11. 144.]]

I - Local maximum validation set accuracy:  71.60

I - Validation set results: 
[14-1-2-0.82][50-3-4-0.54][124-2-3-1.15][127-0-0-7.28][443-2-2-1.98][567-0-0-4.56][573-1-1-6.93][615-0-0-3.78][695-1-2-3.03][722-3-3-4.88]
[826-0-0-5.53][878-0-0-4.04][1103-0-0-3.04][1212-3-3-2.11][1368-0-0-7.23][2181-2-3-1.32][2476-2-2-2.99][2721-2-2-2.39][2818-1-1-5.66][2886-2-1-3.39]
[3231-2-2-8.58][3333-2-2-0.45][3482-2-2-2.65][3536-3-4-1.58][3625-1-1-3.47][3909-0-0-3.52][4035-0-0-4.06][4140-0-0-2.86][4214-1-3-1.21][4346-1-4-1.64]
[4581-2-4-1.73][4708-3-4-1.08][4838-3-3-0.89][4845-1-1-1.39][4868-0-0-3.25][4939-0-3-1.55][4984-2-2-1.46][5078-1-3-0.71][5396-0-0-7.03][5479-1-1-8.44]
[5717-0-0-3.41][5843-1-1-1.08][5949-3-3-1.30][5987-2-4-3.02][6014-3-3-2.73][6033-3-3-0.34][6313-0-0-3.05][6421-3-3-5.71][6500-1-1-3.87][6583-3-3-4.33]
[6683-3-3-2.90][6825-2-1-5.49][6998-3-3-0.16][7049-3-3-3.09][7517-1-1-6.75][7521-1-1-1.72][7528-1-1-3.23][7949-1-2-3.38][8135-1-1-2.34][8185-3-0-6.45]
[8269-3-1-5.21][8273-3-3-6.83][8543-3-0-5.96][8666-1-1-8.90][8672-0-0-7.68][8903-1-1-4.34][9001-2-2-1.29][9036-2-2-6.82][9281-3-3-1.02][9300-2-2-10.21]
[9571-0-3-1.23][9617-1-4-1.77][9644-2-2-1.85][9705-2-1-1.96][9801-0-3-3.43][9803-3-3-3.12][9865-3-3-7.34][9896-2-4-2.42][10314-1-1-0.70][10337-3-3-5.22]
[10403-0-4-1.46][10653-2-4-1.51][10704-2-2-1.92][10719-1-1-5.75][10727-1-1-2.52][10836-0-0-11.95][10969-2-3-3.06][11042-0-0-2.10][11088-1-1-11.48][11322-0-0-6.52]
[11398-2-4-2.87][11499-0-0-4.85][11502-3-3-1.53][11512-3-3-3.52][11608-1-1-1.41][11610-0-0-3.29][11692-0-0-5.17][11905-0-0-4.42][11993-1-1-6.47][12002-2-0-0.90]
[12052-0-0-3.89][12201-0-3-3.59][12235-2-4-2.74][12320-1-4-3.55][12377-2-4-3.03][12398-2-3-2.69][12503-1-4-2.23][12617-0-0-1.53][12685-3-3-1.43][12738-2-0-2.08]
[12742-2-2-8.87][12823-0-0-3.25][13110-1-2-1.12][13240-3-3-3.20][13253-1-1-4.55][13273-0-0-7.51][13634-1-3-1.92][13763-2-3-2.67][13905-3-3-1.95][14060-2-1-2.93]
[14065-3-0-3.12][14147-3-3-5.00][14595-2-2-1.68][14687-2-2-5.35][14788-2-2-5.07][14869-1-1-3.69][14872-3-3-2.22][14877-1-1-6.34][14927-0-3-1.60][15066-0-0-7.00]
[15175-1-1-2.91][15178-2-3-4.20][15375-3-3-2.02][15389-3-3-4.84][15568-2-1-1.84][15675-3-3-7.01][15869-1-1-1.19][16207-3-0-0.91][16236-0-0-4.85][16302-3-3-3.02]
[16331-2-2-9.16][16381-0-3-3.78][16488-1-1-4.76][16495-0-0-3.68][16650-0-0-8.23][16719-1-2-1.39][16801-0-0-7.69][16828-0-0-3.67][17137-3-3-2.53][17245-1-1-0.62]
[17278-3-0-0.93][17282-0-0-2.38][17311-2-2-2.29][17336-2-1-4.84][17608-3-3-5.92][17627-0-0-1.68][17877-3-4-1.44][17924-1-2-0.26][17984-3-3-3.93][18211-0-3-2.51]
[18276-3-3-3.77][18287-1-1-3.83][18394-0-0-6.71][18428-0-0-4.34][18442-0-3-6.04][18478-3-3-3.76][18607-0-0-4.06][18616-0-0-4.37][18663-0-0-6.41][18718-0-0-7.78]
[18766-2-2-5.75][18824-2-4-1.96][18890-3-3-4.39][18930-3-4-1.96][18938-3-3-1.95][19817-1-1-3.23][19839-0-1-0.67][19930-3-3-4.23][19944-0-4-3.26][20036-2-2-8.09]
[20101-3-3-4.20][20474-1-1-1.60][20547-3-3-2.11][20929-2-2-2.27][21245-1-1-1.55][21257-3-3-0.28][21293-1-1-7.48][21316-1-1-9.22][21384-1-1-6.59][21448-1-1-6.43]
[21483-0-0-4.98][21487-2-2-5.22][21714-0-0-1.37][21943-3-3-1.76][21947-0-4-0.93][21948-0-0-10.36][21965-2-2-2.82][21998-1-1-7.98][22025-0-4-3.93][22228-3-3-8.86]
[22446-1-1-7.64][22494-3-3-4.96][22757-0-0-4.76][22811-3-3-6.84][22976-3-2-0.78][22985-3-3-2.36][23014-0-0-4.76][23112-1-1-9.26][23144-3-3-7.06][23168-2-3-1.14]
[23219-0-0-2.44][23363-3-3-3.30][23470-0-0-1.77][23486-2-2-1.38][23497-0-3-5.89][23516-0-0-4.63][23690-1-3-3.09][23921-2-1-1.66][23936-1-0-4.01][24040-3-4-1.03]
[24111-1-4-2.28][24182-0-0-6.21][24238-3-3-4.49][24290-2-0-3.09][24345-0-0-4.40][24364-1-2-1.48][24427-3-0-3.44][24477-2-4-3.62][24495-2-4-1.83][24893-2-4-0.46]
[25012-1-1-2.53][25121-2-2-4.67][25165-3-3-4.50][25183-0-0-3.33][25297-3-3-4.69][25398-0-0-8.22][25574-2-4-2.52][25644-1-1-3.56][25718-1-4-0.95][25774-2-4-1.84]
[26032-3-3-5.60][26051-3-3-5.90][26120-0-4-2.02][26321-1-4-0.71][26732-1-1-5.22][26784-3-3-8.89][26827-3-3-4.77][26833-0-3-3.09][26838-2-4-0.52][26860-1-1-1.53]
[26948-0-0-4.41][27049-3-0-4.63][27098-1-1-1.58][27526-0-3-3.64][27639-3-3-3.46][27698-3-3-3.65][27772-0-0-5.60][27890-1-1-6.38][28040-0-4-2.12][28503-2-2-4.19]
[28577-1-1-8.47][28959-0-0-6.72][29198-3-4-2.20][29777-0-0-9.50][29877-2-3-1.47][30035-1-1-8.44][30098-0-0-1.71][30326-1-1-8.94][30572-2-2-3.22][30716-0-4-3.92]
[30806-2-3-1.96][30906-1-1-11.82][31007-0-0-5.03][31181-3-4-2.00][31238-0-3-2.67][31347-0-0-5.94][31422-2-4-1.83][31429-3-3-2.87][31431-0-0-0.71][31432-1-1-5.01]
[31477-0-3-4.21][31524-1-1-1.17][31597-1-1-4.11][31619-1-4-1.36][31701-0-0-3.39][31755-0-0-5.76][31854-3-3-3.48][32074-1-1-4.72][32078-3-3-5.94][32111-1-1-8.01]
[32127-1-1-2.71][32140-3-3-4.11][32263-2-2--0.34][32365-0-0-5.58][32411-2-3-7.25][32429-3-3-1.21][32473-3-3-3.74][32574-3-3-4.01][32584-0-4-2.36][32622-0-4-2.71]
[32858-3-3-2.87][32969-3-3-5.03][33016-2-2-5.73][33031-1-3-5.25][33035-2-2-2.85][33133-2-2-3.20][33173-2-1-1.32][33175-3-4-3.64][33306-3-3-4.64][33309-2-3-3.52]
[33474-0-0-1.46][33478-2-0-0.88][33618-1-4-3.09][33712-0-0-1.92][33782-2-4-2.70][33914-3-3-4.18][34076-3-4-2.31][34112-2-2-6.30][34138-2-3-2.22][34239-1-1-3.87]
[34364-2-2-5.75][34617-1-4-2.89][34751-3-3-3.17][34783-2-4-2.77][35015-3-4-2.60][35018-1-1-2.96][35288-2-4-0.69][0-4-4-4.71][1-4-4-3.81][2-4-0-1.06]
[3-4-4-2.17][4-4-4-2.52][5-4-1-0.80][6-4-4-4.25][7-4-0-2.27][8-4-4-0.95][9-4-4-2.09][10-4-4-5.64][11-4-4-6.55][12-4-4-3.11]
[14-4-4-3.41][15-4-3-4.54][16-4-4-2.35][17-4-4-2.94][18-4-4-5.59][19-4-3-1.58][20-4-4-2.29][21-4-4-3.00][22-4-4-2.98][23-4-4-1.42]
[24-4-4-6.73][25-4-4-3.19][26-4-4-1.16][27-4-4-3.40][28-4-4-3.28][29-4-1-2.74][30-4-4-1.28][31-4-4-2.16][32-4-4-2.06][33-4-4-1.81]
[34-4-4-3.51][35-4-4-4.38][37-4-4-2.84][39-4-0-2.45][40-4-4-1.90][41-4-4-1.99][42-4-4-1.77][43-4-4-2.73][45-4-2-1.51][46-4-4-4.48]
[47-4-4-4.81][48-4-2-0.74][51-4-4-3.42][52-4-4-2.43][53-4-4-2.49][54-4-4-3.83][55-4-4-3.78][56-4-1-3.33][57-4-3-3.94][58-4-2-2.80]
[59-4-0-3.65][60-4-4-3.97][61-4-4-3.10][62-4-4-2.43][63-4-4-4.71][64-4-4-2.78][65-4-4-5.11][66-4-4-4.68][67-4-2-1.81][68-4-1-2.61]
[69-4-0-0.56][70-4-4-1.94][72-4-1-4.24][73-4-1-4.56][74-4-4-2.39][75-4-3-2.57][77-4-4-3.39][78-4-4-1.23][79-4-4-4.18][80-4-4-2.11]
[81-4-4-4.37][82-4-4-1.83][83-4-4-2.35][84-4-4-4.55][85-4-4-5.16][86-4-4-1.55][87-4-4-3.76][88-4-4-3.72][89-4-4-0.74][90-4-4-2.03]
[91-4-4-3.29][92-4-4-1.00][93-4-3-3.53][94-4-4-1.44][95-4-4-1.62][96-4-4-1.55][97-4-4-4.40][98-4-2-2.15][99-4-4-1.67][100-4-4-3.30]
[101-4-4-6.21][102-4-4-3.18][103-4-4-1.34][104-4-4-3.77][105-4-4-3.76][106-4-4-4.47][107-4-4-3.84][108-4-4-2.03][109-4-4-3.58][110-4-4-3.09]
[111-4-0-4.03][112-4-4-1.53][113-4-4-0.78][114-4-3-1.21][115-4-4-2.35][116-4-4-0.85][117-4-4-3.57][119-4-4-6.34][121-4-4-2.48][122-4-4-3.22]
[124-4-4-2.38][125-4-4-4.92][126-4-4-4.96][127-4-2-0.76][128-4-4-0.80][129-4-4-2.70][130-4-4-1.76][131-4-3-1.22][132-4-4-3.10][133-4-4-4.70]
[135-4-4-3.51][136-4-4-1.67][137-4-4-2.09][138-4-4-3.00][139-4-4-4.39][140-4-4-3.51][141-4-4-0.84][142-4-4-4.30][143-4-4-5.56][144-4-4-4.06]
[145-4-2-3.62][148-4-0-4.03][149-4-4-3.26][150-4-4-6.85][151-4-4-4.97][152-4-4-2.80][153-4-4-3.30][154-4-4-7.85][155-4-4-5.03][156-4-4-1.08]
[157-4-0-1.12][158-4-4-2.76][160-4-4-0.24][161-4-4-1.47][162-4-4-0.43][164-4-4-1.67][165-4-4-2.52][167-4-0-3.88][168-4-2-1.12][170-4-4-3.38]
[171-4-4-5.07][172-4-4-4.77][173-4-4-5.35][174-4-3-1.52][175-4-4-2.26][177-4-4-5.07][178-4-4-2.47][179-4-4-2.41][180-4-4-1.97][181-4-4-2.95]
[182-4-3-4.78][183-4-4-3.12][184-4-4-3.26][186-4-4-0.90][187-4-4-1.50][188-4-4-5.51][189-4-1-2.60][190-4-3-0.32][191-4-4-3.32][192-4-4-4.11]
[193-4-1-2.57][194-4-4-1.73][195-4-4-1.53][196-4-4-2.21][197-4-4-3.83][198-4-4-7.81][199-4-2-1.01]
---------------------------
I - Global maximum validation set accuracy:  71.60
I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 198
I - Training: 
	I - Batch: 50 | Loss: 0.057 | Acc: 96.250% | Wgt Acc: 98.984%
	I - Batch: 100 | Loss: 0.064 | Acc: 96.250% | Wgt Acc: 98.718%
	I - Batch: 150 | Loss: 0.069 | Acc: 95.667% | Wgt Acc: 98.480%
	I - Batch: 200 | Loss: 0.068 | Acc: 95.719% | Wgt Acc: 98.519%
I - num batch: 222
I - Train -- Loss: 0.070 | Acc: 95.799% | Wgt Acc: 98.574% | LR: 1.250000e-04 | Dur: 142.67s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  48.]
 [  0. 575.   0.   0.  27.]
 [  0.   0. 733.   0.  35.]
 [  2.   0.   0. 537.  29.]
 [  3.   3.   1.   1. 861.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.126 | Acc: 67.258% | Wgt Acc: 58.194% | Dur: 16.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   1.  18.   7.]
 [  0.  35.   2.   1.   5.]
 [  0.   8.  24.   0.   1.]
 [  6.   4.   8.  51.   4.]
 [ 14.  27.  40.  16. 163.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 199
I - Training: 
	I - Batch: 50 | Loss: 0.075 | Acc: 94.750% | Wgt Acc: 98.163%
	I - Batch: 100 | Loss: 0.074 | Acc: 95.062% | Wgt Acc: 98.337%
	I - Batch: 150 | Loss: 0.071 | Acc: 95.292% | Wgt Acc: 98.422%
	I - Batch: 200 | Loss: 0.071 | Acc: 95.562% | Wgt Acc: 98.578%
I - num batch: 222
I - Train -- Loss: 0.071 | Acc: 95.658% | Wgt Acc: 98.596% | LR: 1.250000e-04 | Dur: 132.56s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  54.]
 [  0. 578.   0.   0.  27.]
 [  0.   0. 730.   0.  35.]
 [  0.   0.   1. 537.  30.]
 [  3.   0.   3.   1. 854.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.083 | Acc: 68.836% | Wgt Acc: 61.873% | Dur: 13.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   1.   1.  10.   5.]
 [  0.  42.   5.   1.   6.]
 [  0.  16.  39.   4.  10.]
 [  7.   1.   6.  52.   3.]
 [ 21.  18.  24.  19. 156.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 200
I - Training: 
	I - Batch: 50 | Loss: 0.062 | Acc: 96.000% | Wgt Acc: 98.910%
	I - Batch: 100 | Loss: 0.068 | Acc: 96.000% | Wgt Acc: 98.848%
	I - Batch: 150 | Loss: 0.072 | Acc: 95.750% | Wgt Acc: 98.673%
	I - Batch: 200 | Loss: 0.071 | Acc: 96.062% | Wgt Acc: 98.772%
I - num batch: 222
I - Train -- Loss: 0.072 | Acc: 95.912% | Wgt Acc: 98.715% | LR: 1.250000e-04 | Dur: 138.10s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   0.  48.]
 [  0. 575.   0.   0.  25.]
 [  0.   1. 733.   0.  34.]
 [  0.   1.   0. 537.  32.]
 [  1.   1.   1.   1. 861.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.173 | Acc: 67.653% | Wgt Acc: 59.128% | Dur: 27.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   3.  17.  10.]
 [  0.  34.   4.   0.   2.]
 [  0.   6.  31.   0.   3.]
 [ 10.   5.  10.  51.   4.]
 [ 12.  29.  27.  18. 161.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 201
I - Training: 
	I - Batch: 50 | Loss: 0.071 | Acc: 96.125% | Wgt Acc: 98.708%
	I - Batch: 100 | Loss: 0.077 | Acc: 95.625% | Wgt Acc: 98.494%
	I - Batch: 150 | Loss: 0.074 | Acc: 95.667% | Wgt Acc: 98.483%
	I - Batch: 200 | Loss: 0.080 | Acc: 95.250% | Wgt Acc: 98.359%
I - num batch: 222
I - Train -- Loss: 0.079 | Acc: 95.292% | Wgt Acc: 98.409% | LR: 1.250000e-04 | Dur: 135.66s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  56.]
 [  0. 577.   0.   0.  30.]
 [  0.   0. 730.   0.  47.]
 [  1.   0.   0. 536.  23.]
 [  3.   1.   4.   2. 844.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.075 | Acc: 69.428% | Wgt Acc: 62.553% | Dur: 15.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   3.  11.  12.]
 [  2.  36.   3.   1.   1.]
 [  1.   8.  34.   1.   3.]
 [  8.   7.   7.  59.   7.]
 [ 11.  24.  28.  14. 157.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 202
I - Training: 
	I - Batch: 50 | Loss: 0.090 | Acc: 94.375% | Wgt Acc: 98.075%
	I - Batch: 100 | Loss: 0.079 | Acc: 95.000% | Wgt Acc: 98.387%
	I - Batch: 150 | Loss: 0.081 | Acc: 95.167% | Wgt Acc: 98.340%
	I - Batch: 200 | Loss: 0.082 | Acc: 95.062% | Wgt Acc: 98.337%
I - num batch: 222
I - Train -- Loss: 0.084 | Acc: 94.869% | Wgt Acc: 98.231% | LR: 1.250000e-04 | Dur: 137.13s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  59.]
 [  1. 574.   0.   0.  32.]
 [  0.   0. 732.   0.  36.]
 [  1.   1.   1. 535.  42.]
 [  2.   3.   1.   2. 831.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.117 | Acc: 66.469% | Wgt Acc: 59.105% | Dur: 14.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   2.  14.   8.]
 [  0.  36.   4.   1.   9.]
 [  0.   8.  28.   0.   2.]
 [ 18.   8.  14.  55.   7.]
 [  6.  21.  27.  16. 154.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 203
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 96.125% | Wgt Acc: 98.810%
	I - Batch: 100 | Loss: 0.066 | Acc: 96.250% | Wgt Acc: 98.911%
	I - Batch: 150 | Loss: 0.066 | Acc: 96.083% | Wgt Acc: 98.804%
	I - Batch: 200 | Loss: 0.066 | Acc: 96.062% | Wgt Acc: 98.832%
I - num batch: 222
I - Train -- Loss: 0.065 | Acc: 96.194% | Wgt Acc: 98.876% | LR: 1.250000e-04 | Dur: 136.01s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   0.  41.]
 [  0. 576.   0.   0.  25.]
 [  0.   0. 734.   0.  36.]
 [  0.   0.   0. 537.  30.]
 [  0.   2.   0.   1. 868.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.118 | Acc: 69.822% | Wgt Acc: 60.339% | Dur: 14.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   0.   1.  12.   3.]
 [  0.  36.   2.   0.   3.]
 [  1.   7.  36.   1.   2.]
 [  4.   3.   3.  48.   3.]
 [ 18.  32.  33.  25. 169.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 204
I - Training: 
	I - Batch: 50 | Loss: 0.067 | Acc: 95.875% | Wgt Acc: 98.241%
	I - Batch: 100 | Loss: 0.066 | Acc: 96.000% | Wgt Acc: 98.473%
	I - Batch: 150 | Loss: 0.072 | Acc: 95.750% | Wgt Acc: 98.380%
	I - Batch: 200 | Loss: 0.070 | Acc: 96.062% | Wgt Acc: 98.549%
I - num batch: 222
I - Train -- Loss: 0.070 | Acc: 96.053% | Wgt Acc: 98.553% | LR: 1.250000e-04 | Dur: 141.52s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   2.   0.   1.  46.]
 [  1. 574.   0.   0.  17.]
 [  0.   0. 732.   0.  36.]
 [  0.   0.   1. 535.  28.]
 [  3.   2.   1.   2. 873.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.136 | Acc: 67.258% | Wgt Acc: 58.621% | Dur: 14.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   2.  22.   7.]
 [  0.  36.   6.   1.   4.]
 [  0.  11.  34.   0.   7.]
 [  3.   2.   5.  42.   2.]
 [ 16.  25.  28.  21. 160.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 205
I - Training: 
	I - Batch: 50 | Loss: 0.061 | Acc: 96.625% | Wgt Acc: 99.086%
	I - Batch: 100 | Loss: 0.061 | Acc: 96.312% | Wgt Acc: 98.989%
	I - Batch: 150 | Loss: 0.064 | Acc: 96.042% | Wgt Acc: 98.795%
	I - Batch: 200 | Loss: 0.067 | Acc: 96.125% | Wgt Acc: 98.722%
I - num batch: 222
I - Train -- Loss: 0.067 | Acc: 96.025% | Wgt Acc: 98.720% | LR: 1.250000e-04 | Dur: 136.32s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   1.   0.  48.]
 [  0. 578.   0.   0.  17.]
 [  0.   0. 731.   0.  34.]
 [  1.   0.   0. 536.  35.]
 [  1.   0.   2.   2. 866.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.195 | Acc: 64.694% | Wgt Acc: 54.757% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   6.   3.  24.  10.]
 [  0.  28.   1.   0.   2.]
 [  0.  10.  34.   1.   4.]
 [  3.   2.   3.  37.   3.]
 [ 17.  32.  34.  24. 161.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 206
I - Training: 
	I - Batch: 50 | Loss: 0.078 | Acc: 94.750% | Wgt Acc: 98.289%
	I - Batch: 100 | Loss: 0.073 | Acc: 95.438% | Wgt Acc: 98.562%
	I - Batch: 150 | Loss: 0.071 | Acc: 95.542% | Wgt Acc: 98.627%
	I - Batch: 200 | Loss: 0.071 | Acc: 95.656% | Wgt Acc: 98.657%
I - num batch: 222
I - Train -- Loss: 0.072 | Acc: 95.546% | Wgt Acc: 98.611% | LR: 1.250000e-04 | Dur: 135.84s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   0.  54.]
 [  0. 576.   0.   0.  19.]
 [  0.   0. 734.   0.  33.]
 [  0.   0.   0. 535.  46.]
 [  1.   2.   0.   3. 848.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 68.639% | Wgt Acc: 62.242% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   2.  14.  10.]
 [  3.  49.   7.   3.   6.]
 [  1.   3.  28.   0.   4.]
 [  6.   1.   7.  49.   7.]
 [  9.  21.  31.  20. 153.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 207
I - Training: 
	I - Batch: 50 | Loss: 0.061 | Acc: 96.125% | Wgt Acc: 98.694%
	I - Batch: 100 | Loss: 0.069 | Acc: 95.562% | Wgt Acc: 98.540%
	I - Batch: 150 | Loss: 0.070 | Acc: 95.792% | Wgt Acc: 98.611%
	I - Batch: 200 | Loss: 0.072 | Acc: 95.625% | Wgt Acc: 98.559%
I - num batch: 222
I - Train -- Loss: 0.073 | Acc: 95.574% | Wgt Acc: 98.568% | LR: 1.250000e-04 | Dur: 134.77s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   0.  47.]
 [  0. 577.   0.   1.  29.]
 [  0.   0. 730.   0.  37.]
 [  0.   0.   0. 536.  36.]
 [  1.   1.   4.   1. 851.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.131 | Acc: 68.245% | Wgt Acc: 59.509% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   3.  16.   6.]
 [  0.  38.   2.   1.   2.]
 [  0.  12.  30.   3.   6.]
 [  4.   2.   6.  49.   3.]
 [ 18.  22.  34.  17. 163.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 208
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 95.625% | Wgt Acc: 98.396%
	I - Batch: 100 | Loss: 0.083 | Acc: 94.875% | Wgt Acc: 98.020%
	I - Batch: 150 | Loss: 0.079 | Acc: 95.208% | Wgt Acc: 98.272%
	I - Batch: 200 | Loss: 0.077 | Acc: 95.188% | Wgt Acc: 98.372%
I - num batch: 222
I - Train -- Loss: 0.076 | Acc: 95.292% | Wgt Acc: 98.403% | LR: 1.250000e-04 | Dur: 136.78s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   1.   1.  57.]
 [  0. 577.   0.   0.  28.]
 [  1.   1. 733.   0.  34.]
 [  0.   0.   0. 534.  37.]
 [  4.   0.   0.   3. 844.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.036 | Acc: 69.231% | Wgt Acc: 61.965% | Dur: 26.83s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   2.   8.   4.]
 [  0.  38.   3.   1.   1.]
 [  0.   7.  32.   0.   1.]
 [ 15.   7.  10.  62.  15.]
 [ 13.  24.  28.  15. 159.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 209
I - Training: 
	I - Batch: 50 | Loss: 0.059 | Acc: 96.125% | Wgt Acc: 98.702%
	I - Batch: 100 | Loss: 0.063 | Acc: 96.188% | Wgt Acc: 98.721%
	I - Batch: 150 | Loss: 0.064 | Acc: 96.208% | Wgt Acc: 98.679%
	I - Batch: 200 | Loss: 0.068 | Acc: 95.906% | Wgt Acc: 98.538%
I - num batch: 222
I - Train -- Loss: 0.067 | Acc: 95.940% | Wgt Acc: 98.579% | LR: 1.250000e-04 | Dur: 134.38s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  44.]
 [  0. 577.   0.   0.  17.]
 [  0.   0. 732.   0.  40.]
 [  0.   0.   0. 534.  32.]
 [  4.   1.   2.   4. 867.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.221 | Acc: 66.667% | Wgt Acc: 57.906% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   9.   5.  19.  12.]
 [  0.  35.   3.   0.   3.]
 [  1.   6.  27.   1.   2.]
 [  6.   1.   6.  46.   3.]
 [ 11.  27.  34.  20. 160.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 210
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 95.625% | Wgt Acc: 98.586%
	I - Batch: 100 | Loss: 0.072 | Acc: 94.875% | Wgt Acc: 98.300%
	I - Batch: 150 | Loss: 0.074 | Acc: 95.250% | Wgt Acc: 98.368%
	I - Batch: 200 | Loss: 0.075 | Acc: 95.281% | Wgt Acc: 98.426%
I - num batch: 222
I - Train -- Loss: 0.076 | Acc: 95.292% | Wgt Acc: 98.369% | LR: 1.250000e-04 | Dur: 134.67s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  65.]
 [  0. 577.   0.   2.  20.]
 [  0.   0. 733.   0.  39.]
 [  1.   0.   0. 532.  31.]
 [  3.   1.   1.   3. 845.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.157 | Acc: 67.061% | Wgt Acc: 58.540% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   2.   1.   5.   6.]
 [  0.  28.   2.   0.   0.]
 [  0.  11.  35.   0.   5.]
 [ 17.   9.   9.  63.   8.]
 [ 18.  28.  28.  18. 161.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 211
I - Training: 
	I - Batch: 50 | Loss: 0.067 | Acc: 95.125% | Wgt Acc: 98.545%
	I - Batch: 100 | Loss: 0.064 | Acc: 95.750% | Wgt Acc: 98.710%
	I - Batch: 150 | Loss: 0.069 | Acc: 95.500% | Wgt Acc: 98.433%
	I - Batch: 200 | Loss: 0.069 | Acc: 95.562% | Wgt Acc: 98.476%
I - num batch: 222
I - Train -- Loss: 0.068 | Acc: 95.517% | Wgt Acc: 98.446% | LR: 1.250000e-04 | Dur: 134.60s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   0.  58.]
 [  0. 577.   0.   0.  17.]
 [  1.   0. 732.   0.  32.]
 [  0.   0.   0. 537.  40.]
 [  7.   1.   2.   1. 853.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.195 | Acc: 67.258% | Wgt Acc: 57.767% | Dur: 16.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   3.   2.  14.   8.]
 [  1.  34.   3.   0.   1.]
 [  1.   6.  27.   0.   3.]
 [ 10.   3.   8.  51.   3.]
 [ 12.  32.  35.  21. 165.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 212
I - Training: 
	I - Batch: 50 | Loss: 0.068 | Acc: 95.000% | Wgt Acc: 98.147%
	I - Batch: 100 | Loss: 0.067 | Acc: 95.312% | Wgt Acc: 98.488%
	I - Batch: 150 | Loss: 0.068 | Acc: 95.542% | Wgt Acc: 98.577%
	I - Batch: 200 | Loss: 0.067 | Acc: 95.562% | Wgt Acc: 98.611%
I - num batch: 222
I - Train -- Loss: 0.067 | Acc: 95.658% | Wgt Acc: 98.625% | LR: 1.250000e-04 | Dur: 134.41s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   0.  63.]
 [  0. 577.   0.   0.  24.]
 [  0.   0. 734.   0.  33.]
 [  1.   0.   0. 538.  27.]
 [  5.   1.   0.   0. 853.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.204 | Acc: 67.061% | Wgt Acc: 59.889% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 74.   2.   5.  22.  16.]
 [  0.  37.   2.   0.   2.]
 [  0.   7.  31.   1.   7.]
 [  2.   2.   7.  45.   2.]
 [ 12.  30.  30.  18. 153.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 213
I - Training: 
	I - Batch: 50 | Loss: 0.058 | Acc: 96.500% | Wgt Acc: 98.822%
	I - Batch: 100 | Loss: 0.061 | Acc: 96.812% | Wgt Acc: 99.008%
	I - Batch: 150 | Loss: 0.060 | Acc: 96.667% | Wgt Acc: 98.931%
	I - Batch: 200 | Loss: 0.062 | Acc: 96.500% | Wgt Acc: 98.893%
I - num batch: 222
I - Train -- Loss: 0.064 | Acc: 96.307% | Wgt Acc: 98.827% | LR: 1.250000e-04 | Dur: 137.41s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  46.]
 [  0. 578.   0.   0.  13.]
 [  0.   0. 731.   0.  34.]
 [  0.   0.   0. 537.  32.]
 [  2.   0.   3.   1. 875.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.124 | Acc: 67.456% | Wgt Acc: 57.848% | Dur: 30.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   1.   2.   6.   6.]
 [  0.  30.   1.   0.   3.]
 [  0.  11.  34.   0.   3.]
 [ 10.   3.   7.  54.   2.]
 [ 20.  33.  31.  26. 166.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 214
I - Training: 
	I - Batch: 50 | Loss: 0.071 | Acc: 95.750% | Wgt Acc: 98.834%
	I - Batch: 100 | Loss: 0.071 | Acc: 95.188% | Wgt Acc: 98.490%
	I - Batch: 150 | Loss: 0.068 | Acc: 95.625% | Wgt Acc: 98.555%
	I - Batch: 200 | Loss: 0.068 | Acc: 95.750% | Wgt Acc: 98.594%
I - num batch: 222
I - Train -- Loss: 0.069 | Acc: 95.715% | Wgt Acc: 98.576% | LR: 1.250000e-04 | Dur: 134.62s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  60.]
 [  0. 577.   0.   0.  18.]
 [  0.   0. 733.   0.  34.]
 [  0.   0.   0. 535.  31.]
 [  4.   1.   1.   3. 857.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.072 | Acc: 67.653% | Wgt Acc: 60.085% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 74.   3.   2.  22.  11.]
 [  1.  37.   5.   0.   7.]
 [  0.   7.  30.   0.   2.]
 [  2.   1.   8.  46.   4.]
 [ 11.  30.  30.  18. 156.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 215
I - Training: 
	I - Batch: 50 | Loss: 0.065 | Acc: 96.000% | Wgt Acc: 98.950%
	I - Batch: 100 | Loss: 0.058 | Acc: 96.125% | Wgt Acc: 98.934%
	I - Batch: 150 | Loss: 0.058 | Acc: 96.125% | Wgt Acc: 98.912%
	I - Batch: 200 | Loss: 0.059 | Acc: 96.219% | Wgt Acc: 98.867%
I - num batch: 208
I - Train -- Loss: 0.058 | Acc: 96.304% | Wgt Acc: 98.896% | LR: 1.250000e-04 | Dur: 127.10s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  40.]
 [  0. 577.   0.   0.  20.]
 [  1.   0. 734.   0.  32.]
 [  0.   0.   0. 537.  26.]
 [  2.   1.   0.   1. 663.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.064 | Acc: 69.231% | Wgt Acc: 62.496% | Dur: 14.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   3.   9.   5.]
 [  0.  40.   4.   0.   3.]
 [  0.   9.  35.   2.  10.]
 [ 13.   5.  13.  57.   6.]
 [ 12.  21.  20.  18. 156.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 216
I - Training: 
	I - Batch: 50 | Loss: 0.058 | Acc: 96.750% | Wgt Acc: 99.115%
	I - Batch: 100 | Loss: 0.066 | Acc: 96.188% | Wgt Acc: 98.845%
	I - Batch: 150 | Loss: 0.067 | Acc: 95.917% | Wgt Acc: 98.677%
	I - Batch: 200 | Loss: 0.070 | Acc: 95.688% | Wgt Acc: 98.611%
I - num batch: 222
I - Train -- Loss: 0.073 | Acc: 95.489% | Wgt Acc: 98.459% | LR: 1.250000e-04 | Dur: 135.58s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  58.]
 [  1. 577.   0.   0.  16.]
 [  0.   0. 732.   0.  40.]
 [  1.   0.   0. 535.  35.]
 [  3.   1.   2.   3. 851.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.159 | Acc: 67.456% | Wgt Acc: 61.642% | Dur: 25.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   2.   4.  23.   7.]
 [  0.  35.   2.   0.   4.]
 [  0.  15.  46.   1.  18.]
 [  4.   5.   8.  45.   4.]
 [ 15.  21.  15.  17. 147.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 217
I - Training: 
	I - Batch: 50 | Loss: 0.071 | Acc: 96.250% | Wgt Acc: 98.708%
	I - Batch: 100 | Loss: 0.066 | Acc: 96.312% | Wgt Acc: 98.750%
	I - Batch: 150 | Loss: 0.066 | Acc: 96.208% | Wgt Acc: 98.798%
	I - Batch: 200 | Loss: 0.064 | Acc: 96.250% | Wgt Acc: 98.759%
I - num batch: 222
I - Train -- Loss: 0.065 | Acc: 96.194% | Wgt Acc: 98.680% | LR: 1.250000e-04 | Dur: 133.73s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   0.  41.]
 [  0. 578.   1.   0.  15.]
 [  0.   0. 733.   0.  37.]
 [  0.   0.   0. 535.  32.]
 [  6.   0.   0.   3. 875.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.110 | Acc: 68.442% | Wgt Acc: 58.621% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   3.  19.   4.]
 [  0.  36.   2.   1.   2.]
 [  0.   9.  30.   0.   3.]
 [  2.   1.   7.  45.   3.]
 [ 18.  28.  33.  21. 168.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 218
I - Training: 
	I - Batch: 50 | Loss: 0.071 | Acc: 95.750% | Wgt Acc: 98.590%
	I - Batch: 100 | Loss: 0.068 | Acc: 95.938% | Wgt Acc: 98.720%
	I - Batch: 150 | Loss: 0.073 | Acc: 95.667% | Wgt Acc: 98.491%
	I - Batch: 200 | Loss: 0.072 | Acc: 95.938% | Wgt Acc: 98.612%
I - num batch: 222
I - Train -- Loss: 0.072 | Acc: 95.856% | Wgt Acc: 98.619% | LR: 1.250000e-04 | Dur: 135.00s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   2.  49.]
 [  0. 578.   0.   0.  18.]
 [  0.   0. 733.   0.  40.]
 [  0.   0.   0. 536.  31.]
 [  6.   0.   0.   0. 862.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.193 | Acc: 70.020% | Wgt Acc: 60.604% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   6.   3.  21.   7.]
 [  0.  42.   5.   1.   1.]
 [  0.   7.  30.   0.   2.]
 [  3.   1.   9.  45.   1.]
 [ 16.  22.  28.  19. 169.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 219
I - Training: 
	I - Batch: 50 | Loss: 0.088 | Acc: 93.875% | Wgt Acc: 98.044%
	I - Batch: 100 | Loss: 0.084 | Acc: 94.688% | Wgt Acc: 98.230%
	I - Batch: 150 | Loss: 0.080 | Acc: 94.917% | Wgt Acc: 98.365%
	I - Batch: 200 | Loss: 0.080 | Acc: 95.156% | Wgt Acc: 98.423%
I - num batch: 222
I - Train -- Loss: 0.078 | Acc: 95.320% | Wgt Acc: 98.496% | LR: 1.250000e-04 | Dur: 138.33s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  52.]
 [  0. 577.   0.   0.  27.]
 [  0.   0. 733.   1.  56.]
 [  0.   0.   0. 535.  23.]
 [  3.   1.   1.   2. 842.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 65.483% | Wgt Acc: 57.825% | Dur: 15.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   1.   9.   4.]
 [  0.  36.   2.   1.   2.]
 [  0.   7.  27.   0.   9.]
 [ 14.   5.  16.  57.  11.]
 [ 16.  27.  29.  19. 154.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 220
I - Training: 
	I - Batch: 50 | Loss: 0.069 | Acc: 95.375% | Wgt Acc: 98.362%
	I - Batch: 100 | Loss: 0.072 | Acc: 95.500% | Wgt Acc: 98.353%
	I - Batch: 150 | Loss: 0.074 | Acc: 95.583% | Wgt Acc: 98.471%
	I - Batch: 200 | Loss: 0.077 | Acc: 95.312% | Wgt Acc: 98.351%
I - num batch: 222
I - Train -- Loss: 0.084 | Acc: 94.925% | Wgt Acc: 98.136% | LR: 1.250000e-04 | Dur: 135.17s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   2.  56.]
 [  0. 574.   1.   0.  27.]
 [  1.   2. 731.   0.  45.]
 [  2.   1.   0. 535.  35.]
 [  4.   1.   2.   1. 837.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.061 | Acc: 68.442% | Wgt Acc: 63.418% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   4.   9.   6.]
 [  1.  50.  11.   5.  14.]
 [  0.   5.  31.   1.   5.]
 [ 15.   4.   8.  57.   8.]
 [ 10.  16.  21.  14. 147.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 221
I - Training: 
	I - Batch: 50 | Loss: 0.112 | Acc: 93.750% | Wgt Acc: 97.554%
	I - Batch: 100 | Loss: 0.107 | Acc: 94.062% | Wgt Acc: 97.807%
	I - Batch: 150 | Loss: 0.099 | Acc: 94.292% | Wgt Acc: 97.903%
	I - Batch: 200 | Loss: 0.093 | Acc: 94.625% | Wgt Acc: 97.998%
I - num batch: 222
I - Train -- Loss: 0.091 | Acc: 94.728% | Wgt Acc: 98.084% | LR: 1.250000e-04 | Dur: 133.88s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   2.  58.]
 [  0. 575.   0.   0.  34.]
 [  0.   1. 730.   0.  52.]
 [  3.   0.   0. 535.  26.]
 [  4.   2.   4.   1. 830.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.164 | Acc: 68.245% | Wgt Acc: 59.566% | Dur: 14.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   3.   9.   6.]
 [  0.  35.   4.   1.   3.]
 [  0.  12.  30.   0.   6.]
 [  8.   2.   6.  53.   2.]
 [ 15.  25.  32.  23. 163.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 222
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 95.625% | Wgt Acc: 98.438%
	I - Batch: 100 | Loss: 0.074 | Acc: 95.438% | Wgt Acc: 98.517%
	I - Batch: 150 | Loss: 0.069 | Acc: 95.708% | Wgt Acc: 98.622%
	I - Batch: 200 | Loss: 0.071 | Acc: 95.719% | Wgt Acc: 98.485%
I - num batch: 222
I - Train -- Loss: 0.071 | Acc: 95.574% | Wgt Acc: 98.453% | LR: 1.250000e-04 | Dur: 136.46s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   1.  49.]
 [  0. 576.   0.   0.  20.]
 [  0.   0. 732.   0.  41.]
 [  0.   0.   0. 535.  35.]
 [  5.   2.   2.   2. 855.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.129 | Acc: 67.653% | Wgt Acc: 57.825% | Dur: 17.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   2.   1.  12.   5.]
 [  1.  37.   1.   0.   2.]
 [  0.  13.  34.   0.   4.]
 [ 10.   2.   8.  47.   2.]
 [ 19.  24.  31.  27. 167.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 223
I - Training: 
	I - Batch: 50 | Loss: 0.066 | Acc: 96.250% | Wgt Acc: 98.836%
	I - Batch: 100 | Loss: 0.065 | Acc: 96.000% | Wgt Acc: 98.793%
	I - Batch: 150 | Loss: 0.068 | Acc: 96.083% | Wgt Acc: 98.812%
	I - Batch: 200 | Loss: 0.070 | Acc: 95.719% | Wgt Acc: 98.676%
I - num batch: 222
I - Train -- Loss: 0.071 | Acc: 95.771% | Wgt Acc: 98.620% | LR: 1.250000e-04 | Dur: 131.94s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.  45.]
 [  0. 578.   0.   0.  25.]
 [  1.   0. 734.   0.  48.]
 [  3.   0.   0. 535.  24.]
 [  1.   0.   0.   1. 858.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.143 | Acc: 67.653% | Wgt Acc: 58.632% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   1.   9.   3.]
 [  1.  37.   5.   1.   2.]
 [  0.  11.  30.   0.   4.]
 [ 11.   4.   6.  55.   7.]
 [ 19.  23.  33.  21. 164.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 224
I - Training: 
	I - Batch: 50 | Loss: 0.074 | Acc: 94.750% | Wgt Acc: 97.967%
	I - Batch: 100 | Loss: 0.074 | Acc: 94.938% | Wgt Acc: 98.195%
	I - Batch: 150 | Loss: 0.075 | Acc: 95.042% | Wgt Acc: 98.139%
	I - Batch: 200 | Loss: 0.074 | Acc: 95.250% | Wgt Acc: 98.316%
I - num batch: 222
I - Train -- Loss: 0.073 | Acc: 95.376% | Wgt Acc: 98.372% | LR: 1.250000e-04 | Dur: 133.51s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   1.   1.  57.]
 [  0. 576.   0.   0.  21.]
 [  0.   0. 731.   0.  38.]
 [  1.   0.   0. 535.  35.]
 [  4.   2.   2.   2. 849.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.167 | Acc: 67.061% | Wgt Acc: 58.286% | Dur: 17.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   1.   0.  14.   7.]
 [  0.  34.   4.   2.   3.]
 [  0.  14.  36.   0.   8.]
 [  9.   3.   7.  49.   1.]
 [ 19.  26.  28.  21. 161.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 225
I - Training: 
	I - Batch: 50 | Loss: 0.068 | Acc: 95.000% | Wgt Acc: 98.290%
	I - Batch: 100 | Loss: 0.065 | Acc: 95.625% | Wgt Acc: 98.623%
	I - Batch: 150 | Loss: 0.065 | Acc: 95.542% | Wgt Acc: 98.631%
	I - Batch: 200 | Loss: 0.068 | Acc: 95.656% | Wgt Acc: 98.632%
I - num batch: 222
I - Train -- Loss: 0.069 | Acc: 95.574% | Wgt Acc: 98.596% | LR: 1.250000e-04 | Dur: 135.78s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  56.]
 [  1. 577.   0.   0.  22.]
 [  0.   0. 734.   0.  36.]
 [  0.   0.   0. 536.  36.]
 [  3.   1.   0.   2. 850.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.174 | Acc: 67.061% | Wgt Acc: 57.652% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   1.   1.  13.   5.]
 [  1.  33.   3.   0.   1.]
 [  0.   7.  33.   0.   8.]
 [  7.   1.   6.  49.   2.]
 [ 19.  36.  32.  24. 164.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 226
I - Training: 
	I - Batch: 50 | Loss: 0.074 | Acc: 95.750% | Wgt Acc: 98.585%
	I - Batch: 100 | Loss: 0.077 | Acc: 95.000% | Wgt Acc: 98.258%
	I - Batch: 150 | Loss: 0.072 | Acc: 95.625% | Wgt Acc: 98.563%
	I - Batch: 200 | Loss: 0.068 | Acc: 96.000% | Wgt Acc: 98.693%
I - num batch: 222
I - Train -- Loss: 0.066 | Acc: 96.138% | Wgt Acc: 98.723% | LR: 1.250000e-04 | Dur: 134.52s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  48.]
 [  0. 576.   1.   0.  22.]
 [  0.   1. 732.   0.  32.]
 [  0.   0.   0. 537.  27.]
 [  3.   1.   1.   1. 871.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.168 | Acc: 67.456% | Wgt Acc: 59.739% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   2.  10.   7.]
 [  0.  30.   1.   1.   1.]
 [  2.  14.  39.   0.   9.]
 [ 10.   5.   7.  55.   6.]
 [ 15.  26.  26.  20. 157.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 227
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 96.125% | Wgt Acc: 98.657%
	I - Batch: 100 | Loss: 0.060 | Acc: 96.562% | Wgt Acc: 98.925%
	I - Batch: 150 | Loss: 0.063 | Acc: 96.292% | Wgt Acc: 98.762%
	I - Batch: 200 | Loss: 0.065 | Acc: 96.094% | Wgt Acc: 98.677%
I - num batch: 222
I - Train -- Loss: 0.065 | Acc: 96.081% | Wgt Acc: 98.700% | LR: 1.250000e-04 | Dur: 135.34s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   1.  44.]
 [  0. 577.   1.   1.  14.]
 [  0.   1. 732.   0.  41.]
 [  0.   0.   0. 534.  32.]
 [  1.   0.   1.   2. 869.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.120 | Acc: 67.258% | Wgt Acc: 58.240% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   1.   9.   3.]
 [  0.  35.   5.   2.   5.]
 [  0.   7.  29.   0.   5.]
 [  6.   2.   7.  53.   4.]
 [ 21.  32.  33.  22. 163.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 228
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 95.875% | Wgt Acc: 98.616%
	I - Batch: 100 | Loss: 0.068 | Acc: 95.312% | Wgt Acc: 98.403%
	I - Batch: 150 | Loss: 0.069 | Acc: 95.833% | Wgt Acc: 98.607%
	I - Batch: 200 | Loss: 0.066 | Acc: 95.906% | Wgt Acc: 98.663%
I - num batch: 222
I - Train -- Loss: 0.067 | Acc: 95.884% | Wgt Acc: 98.622% | LR: 1.250000e-04 | Dur: 136.14s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  50.]
 [  0. 577.   0.   0.  16.]
 [  0.   0. 733.   0.  37.]
 [  0.   0.   0. 535.  34.]
 [  4.   1.   1.   3. 863.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.163 | Acc: 67.850% | Wgt Acc: 59.866% | Dur: 15.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   0.   2.   6.   6.]
 [  1.  37.   2.   1.   4.]
 [  0.  15.  38.   3.   8.]
 [ 12.   4.   6.  52.   3.]
 [ 17.  22.  27.  24. 159.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 229
I - Training: 
	I - Batch: 50 | Loss: 0.065 | Acc: 96.250% | Wgt Acc: 98.603%
	I - Batch: 100 | Loss: 0.070 | Acc: 95.500% | Wgt Acc: 98.518%
	I - Batch: 150 | Loss: 0.075 | Acc: 95.167% | Wgt Acc: 98.330%
	I - Batch: 200 | Loss: 0.076 | Acc: 95.219% | Wgt Acc: 98.341%
I - num batch: 222
I - Train -- Loss: 0.077 | Acc: 95.123% | Wgt Acc: 98.323% | LR: 1.250000e-04 | Dur: 134.65s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   3.  56.]
 [  0. 575.   0.   0.  32.]
 [  0.   1. 733.   1.  40.]
 [  0.   0.   0. 533.  33.]
 [  3.   2.   1.   1. 839.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.154 | Acc: 67.258% | Wgt Acc: 62.830% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   2.   3.   4.   9.]
 [  1.  43.   7.   0.   6.]
 [  0.  10.  35.   1.   7.]
 [ 27.   8.  15.  68.  15.]
 [  8.  15.  15.  13. 143.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 230
I - Training: 
	I - Batch: 50 | Loss: 0.076 | Acc: 95.500% | Wgt Acc: 98.301%
	I - Batch: 100 | Loss: 0.071 | Acc: 95.688% | Wgt Acc: 98.537%
	I - Batch: 150 | Loss: 0.068 | Acc: 96.042% | Wgt Acc: 98.682%
	I - Batch: 200 | Loss: 0.072 | Acc: 95.688% | Wgt Acc: 98.577%
I - num batch: 222
I - Train -- Loss: 0.074 | Acc: 95.630% | Wgt Acc: 98.524% | LR: 1.250000e-04 | Dur: 135.89s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.  52.]
 [  0. 576.   0.   0.  26.]
 [  0.   0. 733.   0.  39.]
 [  1.   0.   0. 535.  28.]
 [  3.   2.   1.   1. 855.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.189 | Acc: 65.878% | Wgt Acc: 59.416% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   6.  20.  12.]
 [  1.  36.   2.   1.   5.]
 [  2.   6.  32.   1.   9.]
 [  8.   5.   6.  49.   6.]
 [  8.  27.  29.  15. 148.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 231
I - Training: 
	I - Batch: 50 | Loss: 0.087 | Acc: 94.125% | Wgt Acc: 97.761%
	I - Batch: 100 | Loss: 0.079 | Acc: 94.750% | Wgt Acc: 98.267%
	I - Batch: 150 | Loss: 0.080 | Acc: 94.917% | Wgt Acc: 98.286%
	I - Batch: 200 | Loss: 0.078 | Acc: 95.156% | Wgt Acc: 98.371%
I - num batch: 222
I - Train -- Loss: 0.079 | Acc: 95.179% | Wgt Acc: 98.401% | LR: 1.250000e-04 | Dur: 136.18s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   1.  57.]
 [  0. 576.   0.   0.  27.]
 [  0.   0. 733.   0.  42.]
 [  2.   0.   0. 535.  35.]
 [  2.   2.   0.   2. 839.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.134 | Acc: 67.850% | Wgt Acc: 57.490% | Dur: 16.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   0.   0.   9.   2.]
 [  0.  35.   2.   1.   1.]
 [  2.   7.  33.   0.   3.]
 [  9.   1.   3.  49.   4.]
 [ 20.  35.  37.  27. 170.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 232
I - Training: 
	I - Batch: 50 | Loss: 0.064 | Acc: 96.375% | Wgt Acc: 98.792%
	I - Batch: 100 | Loss: 0.063 | Acc: 96.188% | Wgt Acc: 98.654%
	I - Batch: 150 | Loss: 0.062 | Acc: 96.125% | Wgt Acc: 98.698%
	I - Batch: 200 | Loss: 0.066 | Acc: 96.062% | Wgt Acc: 98.583%
I - num batch: 222
I - Train -- Loss: 0.067 | Acc: 95.997% | Wgt Acc: 98.567% | LR: 1.250000e-04 | Dur: 134.21s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  53.]
 [  0. 577.   0.   0.  11.]
 [  0.   0. 731.   0.  34.]
 [  1.   1.   0. 534.  32.]
 [  3.   0.   3.   3. 870.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.227 | Acc: 67.653% | Wgt Acc: 59.128% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   3.   3.  15.   8.]
 [  0.  36.   1.   1.   2.]
 [  2.   5.  26.   0.   6.]
 [  5.   3.   9.  50.   3.]
 [ 11.  31.  36.  20. 161.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 233
I - Training: 
	I - Batch: 50 | Loss: 0.064 | Acc: 95.625% | Wgt Acc: 98.544%
	I - Batch: 100 | Loss: 0.064 | Acc: 95.875% | Wgt Acc: 98.672%
	I - Batch: 150 | Loss: 0.063 | Acc: 96.042% | Wgt Acc: 98.796%
	I - Batch: 200 | Loss: 0.063 | Acc: 96.156% | Wgt Acc: 98.799%
I - num batch: 222
I - Train -- Loss: 0.062 | Acc: 96.307% | Wgt Acc: 98.855% | LR: 1.250000e-04 | Dur: 138.39s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  44.]
 [  0. 576.   0.   0.  14.]
 [  0.   0. 733.   0.  32.]
 [  0.   0.   0. 538.  36.]
 [  2.   2.   1.   0. 874.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 69.625% | Wgt Acc: 62.023% | Dur: 17.12s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   1.   9.   4.]
 [  0.  37.   3.   2.   3.]
 [  0.  11.  39.   1.  10.]
 [  9.   1.   6.  51.   3.]
 [ 13.  27.  26.  23. 160.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 234
I - Training: 
	I - Batch: 50 | Loss: 0.058 | Acc: 97.125% | Wgt Acc: 99.207%
	I - Batch: 100 | Loss: 0.058 | Acc: 96.438% | Wgt Acc: 98.904%
	I - Batch: 150 | Loss: 0.059 | Acc: 96.417% | Wgt Acc: 98.869%
	I - Batch: 200 | Loss: 0.060 | Acc: 96.375% | Wgt Acc: 98.831%
I - num batch: 222
I - Train -- Loss: 0.063 | Acc: 96.222% | Wgt Acc: 98.775% | LR: 1.250000e-04 | Dur: 137.08s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  41.]
 [  0. 577.   0.   0.  21.]
 [  0.   0. 733.   0.  31.]
 [  0.   0.   0. 537.  34.]
 [  4.   1.   1.   1. 873.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.103 | Acc: 69.428% | Wgt Acc: 62.796% | Dur: 14.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   3.  19.   8.]
 [  1.  41.   2.   0.   4.]
 [  0.  11.  39.   0.   7.]
 [  8.   2.   7.  49.   6.]
 [ 11.  20.  24.  18. 155.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 235
I - Training: 
	I - Batch: 50 | Loss: 0.067 | Acc: 95.875% | Wgt Acc: 98.720%
	I - Batch: 100 | Loss: 0.063 | Acc: 96.125% | Wgt Acc: 98.881%
	I - Batch: 150 | Loss: 0.060 | Acc: 96.333% | Wgt Acc: 98.921%
	I - Batch: 200 | Loss: 0.061 | Acc: 96.281% | Wgt Acc: 98.862%
I - num batch: 222
I - Train -- Loss: 0.062 | Acc: 96.307% | Wgt Acc: 98.850% | LR: 1.250000e-04 | Dur: 136.81s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   1.  47.]
 [  0. 577.   0.   0.  23.]
 [  0.   0. 732.   0.  27.]
 [  0.   0.   0. 536.  29.]
 [  0.   1.   2.   1. 874.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.137 | Acc: 68.836% | Wgt Acc: 60.743% | Dur: 15.12s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   1.   3.  14.   4.]
 [  1.  40.   7.   1.   9.]
 [  1.   8.  34.   0.   3.]
 [  8.   5.   9.  49.   3.]
 [ 13.  24.  22.  22. 161.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 236
I - Training: 
	I - Batch: 50 | Loss: 0.074 | Acc: 95.375% | Wgt Acc: 98.633%
	I - Batch: 100 | Loss: 0.066 | Acc: 95.812% | Wgt Acc: 98.727%
	I - Batch: 150 | Loss: 0.064 | Acc: 95.708% | Wgt Acc: 98.743%
	I - Batch: 200 | Loss: 0.065 | Acc: 95.750% | Wgt Acc: 98.748%
I - num batch: 222
I - Train -- Loss: 0.064 | Acc: 95.771% | Wgt Acc: 98.761% | LR: 1.250000e-04 | Dur: 134.03s
I - Confusion Matrix: [row->prediction - col->label]
[[697.   0.   0.   0.  49.]
 [  0. 578.   0.   0.  23.]
 [  0.   0. 733.   0.  46.]
 [  0.   0.   0. 536.  29.]
 [  0.   0.   1.   2. 853.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.215 | Acc: 66.272% | Wgt Acc: 55.899% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   1.   1.  14.   5.]
 [  0.  37.   3.   0.   2.]
 [  0.   5.  26.   2.   3.]
 [  8.   4.   7.  44.   3.]
 [ 18.  31.  38.  26. 167.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 237
I - Training: 
	I - Batch: 50 | Loss: 0.061 | Acc: 95.875% | Wgt Acc: 98.643%
	I - Batch: 100 | Loss: 0.065 | Acc: 95.750% | Wgt Acc: 98.480%
	I - Batch: 150 | Loss: 0.065 | Acc: 95.958% | Wgt Acc: 98.582%
	I - Batch: 200 | Loss: 0.071 | Acc: 95.625% | Wgt Acc: 98.276%
I - num batch: 222
I - Train -- Loss: 0.070 | Acc: 95.827% | Wgt Acc: 98.384% | LR: 1.250000e-04 | Dur: 133.45s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   0.   0.  60.]
 [  0. 576.   0.   0.  18.]
 [  0.   1. 733.   0.  23.]
 [  1.   1.   0. 535.  30.]
 [ 10.   0.   1.   3. 869.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.210 | Acc: 65.483% | Wgt Acc: 56.487% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   1.   1.  10.   8.]
 [  0.  32.   3.   0.   3.]
 [  0.  12.  33.   1.   8.]
 [  9.   1.   5.  46.   2.]
 [ 17.  32.  33.  29. 159.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 238
I - Training: 
	I - Batch: 50 | Loss: 0.050 | Acc: 97.500% | Wgt Acc: 99.180%
	I - Batch: 100 | Loss: 0.056 | Acc: 96.875% | Wgt Acc: 99.071%
	I - Batch: 150 | Loss: 0.058 | Acc: 96.667% | Wgt Acc: 99.006%
	I - Batch: 200 | Loss: 0.060 | Acc: 96.344% | Wgt Acc: 98.843%
I - num batch: 222
I - Train -- Loss: 0.061 | Acc: 96.307% | Wgt Acc: 98.735% | LR: 1.250000e-04 | Dur: 134.29s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   0.  46.]
 [  1. 578.   0.   0.  19.]
 [  0.   0. 730.   1.  30.]
 [  0.   0.   3. 534.  27.]
 [  0.   0.   1.   3. 878.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 68.047% | Wgt Acc: 60.408% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   3.  13.   8.]
 [  0.  37.   1.   1.   2.]
 [  1.  13.  39.   3.   9.]
 [  3.   2.   4.  46.   4.]
 [ 18.  22.  28.  23. 157.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 239
I - Training: 
	I - Batch: 50 | Loss: 0.062 | Acc: 96.625% | Wgt Acc: 98.973%
	I - Batch: 100 | Loss: 0.066 | Acc: 96.188% | Wgt Acc: 98.785%
	I - Batch: 150 | Loss: 0.068 | Acc: 95.917% | Wgt Acc: 98.721%
	I - Batch: 200 | Loss: 0.067 | Acc: 95.969% | Wgt Acc: 98.712%
I - num batch: 222
I - Train -- Loss: 0.068 | Acc: 95.771% | Wgt Acc: 98.653% | LR: 1.250000e-04 | Dur: 139.93s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  54.]
 [  0. 577.   0.   0.  32.]
 [  0.   0. 732.   0.  24.]
 [  0.   0.   0. 537.  33.]
 [  3.   1.   2.   1. 857.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.061 | Acc: 68.639% | Wgt Acc: 60.258% | Dur: 15.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   2.   8.   5.]
 [  0.  39.   4.   0.   2.]
 [  0.  10.  29.   0.   5.]
 [ 13.   4.  10.  58.   5.]
 [ 16.  23.  30.  20. 163.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 240
I - Training: 
	I - Batch: 50 | Loss: 0.077 | Acc: 94.875% | Wgt Acc: 98.313%
	I - Batch: 100 | Loss: 0.068 | Acc: 95.750% | Wgt Acc: 98.647%
	I - Batch: 150 | Loss: 0.070 | Acc: 95.583% | Wgt Acc: 98.500%
	I - Batch: 200 | Loss: 0.070 | Acc: 95.781% | Wgt Acc: 98.570%
I - num batch: 222
I - Train -- Loss: 0.070 | Acc: 95.630% | Wgt Acc: 98.501% | LR: 1.250000e-04 | Dur: 136.06s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   1.  59.]
 [  0. 577.   0.   0.  28.]
 [  1.   0. 732.   0.  26.]
 [  0.   0.   0. 536.  31.]
 [  5.   1.   2.   1. 856.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 68.047% | Wgt Acc: 60.074% | Dur: 16.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   2.   2.  16.  10.]
 [  0.  32.   3.   1.   1.]
 [  0.  15.  33.   2.   7.]
 [ 10.   2.   7.  52.   3.]
 [  9.  27.  30.  15. 159.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 241
I - Training: 
	I - Batch: 50 | Loss: 0.064 | Acc: 96.750% | Wgt Acc: 98.984%
	I - Batch: 100 | Loss: 0.067 | Acc: 96.062% | Wgt Acc: 98.741%
	I - Batch: 150 | Loss: 0.066 | Acc: 96.000% | Wgt Acc: 98.780%
	I - Batch: 200 | Loss: 0.066 | Acc: 96.156% | Wgt Acc: 98.826%
I - num batch: 222
I - Train -- Loss: 0.066 | Acc: 96.109% | Wgt Acc: 98.798% | LR: 1.250000e-04 | Dur: 137.27s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.  39.]
 [  0. 578.   0.   0.  19.]
 [  0.   0. 734.   1.  42.]
 [  2.   0.   0. 536.  33.]
 [  1.   0.   0.   0. 867.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.097 | Acc: 67.850% | Wgt Acc: 59.889% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   1.   3.  21.   5.]
 [  0.  47.   8.   1.   6.]
 [  0.  14.  32.   0.   9.]
 [  1.   1.   3.  39.   2.]
 [ 19.  15.  29.  25. 158.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 242
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 97.000% | Wgt Acc: 98.972%
	I - Batch: 100 | Loss: 0.059 | Acc: 96.688% | Wgt Acc: 98.756%
	I - Batch: 150 | Loss: 0.060 | Acc: 96.375% | Wgt Acc: 98.715%
	I - Batch: 200 | Loss: 0.060 | Acc: 96.281% | Wgt Acc: 98.751%
I - num batch: 208
I - Train -- Loss: 0.059 | Acc: 96.334% | Wgt Acc: 98.778% | LR: 1.250000e-04 | Dur: 127.43s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  36.]
 [  1. 576.   0.   0.  20.]
 [  0.   0. 734.   0.  30.]
 [  0.   0.   0. 533.  27.]
 [  1.   2.   0.   5. 668.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.127 | Acc: 67.258% | Wgt Acc: 58.056% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   4.  14.   9.]
 [  0.  31.   3.   0.   2.]
 [  0.  13.  32.   2.   3.]
 [  5.   5.   6.  47.   3.]
 [ 15.  25.  30.  23. 163.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 243
I - Training: 
	I - Batch: 50 | Loss: 0.051 | Acc: 97.375% | Wgt Acc: 99.158%
	I - Batch: 100 | Loss: 0.057 | Acc: 97.000% | Wgt Acc: 98.860%
	I - Batch: 150 | Loss: 0.063 | Acc: 96.583% | Wgt Acc: 98.764%
	I - Batch: 200 | Loss: 0.064 | Acc: 96.406% | Wgt Acc: 98.732%
I - num batch: 222
I - Train -- Loss: 0.066 | Acc: 96.194% | Wgt Acc: 98.627% | LR: 1.250000e-04 | Dur: 135.72s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  48.]
 [  0. 575.   0.   0.  24.]
 [  0.   0. 730.   0.  31.]
 [  1.   0.   0. 537.  20.]
 [  3.   3.   4.   0. 877.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 68.639% | Wgt Acc: 59.636% | Dur: 17.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   1.   1.  11.   3.]
 [  1.  37.   3.   0.   3.]
 [  1.  12.  36.   3.   8.]
 [  9.   1.   5.  50.   1.]
 [ 17.  27.  30.  22. 165.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 244
I - Training: 
	I - Batch: 50 | Loss: 0.072 | Acc: 95.125% | Wgt Acc: 98.172%
	I - Batch: 100 | Loss: 0.064 | Acc: 95.812% | Wgt Acc: 98.541%
	I - Batch: 150 | Loss: 0.063 | Acc: 96.000% | Wgt Acc: 98.659%
	I - Batch: 200 | Loss: 0.060 | Acc: 96.406% | Wgt Acc: 98.839%
I - num batch: 222
I - Train -- Loss: 0.061 | Acc: 96.279% | Wgt Acc: 98.789% | LR: 1.250000e-04 | Dur: 133.47s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  46.]
 [  0. 578.   0.   0.  19.]
 [  0.   0. 733.   0.  33.]
 [  0.   0.   0. 536.  27.]
 [  4.   0.   1.   2. 875.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.260 | Acc: 66.864% | Wgt Acc: 57.029% | Dur: 14.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   3.  12.   8.]
 [  0.  34.   1.   0.   0.]
 [  1.  10.  24.   0.   2.]
 [  7.   4.   9.  51.   4.]
 [ 16.  26.  38.  23. 166.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 245
I - Training: 
	I - Batch: 50 | Loss: 0.059 | Acc: 96.500% | Wgt Acc: 99.059%
	I - Batch: 100 | Loss: 0.066 | Acc: 95.688% | Wgt Acc: 98.770%
	I - Batch: 150 | Loss: 0.065 | Acc: 95.875% | Wgt Acc: 98.755%
	I - Batch: 200 | Loss: 0.065 | Acc: 95.875% | Wgt Acc: 98.751%
I - num batch: 222
I - Train -- Loss: 0.065 | Acc: 95.856% | Wgt Acc: 98.732% | LR: 1.250000e-04 | Dur: 136.19s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  63.]
 [  0. 578.   0.   0.  17.]
 [  0.   0. 734.   0.  33.]
 [  0.   0.   0. 537.  29.]
 [  4.   0.   0.   1. 858.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.048 | Acc: 69.034% | Wgt Acc: 61.100% | Dur: 16.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   1.   0.  21.   9.]
 [  0.  40.   2.   1.   5.]
 [  0.   8.  35.   0.   5.]
 [  6.   2.   7.  44.   1.]
 [ 11.  27.  31.  20. 160.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 246
I - Training: 
	I - Batch: 50 | Loss: 0.060 | Acc: 96.750% | Wgt Acc: 98.991%
	I - Batch: 100 | Loss: 0.060 | Acc: 96.438% | Wgt Acc: 98.911%
	I - Batch: 150 | Loss: 0.061 | Acc: 96.417% | Wgt Acc: 98.861%
	I - Batch: 200 | Loss: 0.060 | Acc: 96.438% | Wgt Acc: 98.908%
I - num batch: 222
I - Train -- Loss: 0.060 | Acc: 96.532% | Wgt Acc: 98.915% | LR: 1.250000e-04 | Dur: 135.74s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   0.  43.]
 [  0. 577.   0.   0.  16.]
 [  0.   0. 732.   0.  34.]
 [  0.   0.   0. 537.  25.]
 [  1.   1.   2.   1. 882.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.135 | Acc: 68.245% | Wgt Acc: 60.328% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   3.  13.   9.]
 [  0.  33.   3.   0.   0.]
 [  1.  10.  38.   2.   5.]
 [ 12.   2.   6.  51.   7.]
 [ 10.  29.  25.  20. 159.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 247
I - Training: 
	I - Batch: 50 | Loss: 0.066 | Acc: 95.750% | Wgt Acc: 98.589%
	I - Batch: 100 | Loss: 0.068 | Acc: 95.688% | Wgt Acc: 98.613%
	I - Batch: 150 | Loss: 0.066 | Acc: 95.917% | Wgt Acc: 98.681%
	I - Batch: 200 | Loss: 0.069 | Acc: 95.531% | Wgt Acc: 98.502%
I - num batch: 222
I - Train -- Loss: 0.068 | Acc: 95.602% | Wgt Acc: 98.548% | LR: 1.250000e-04 | Dur: 140.22s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  54.]
 [  1. 577.   0.   0.  16.]
 [  0.   0. 733.   0.  38.]
 [  0.   0.   0. 536.  39.]
 [  4.   1.   1.   2. 853.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.162 | Acc: 66.864% | Wgt Acc: 58.033% | Dur: 16.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   3.  14.   8.]
 [  1.  33.   3.   0.   3.]
 [  0.  13.  33.   0.   4.]
 [  9.   3.   5.  50.   4.]
 [ 16.  25.  31.  22. 161.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 248
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 96.375% | Wgt Acc: 98.642%
	I - Batch: 100 | Loss: 0.061 | Acc: 96.250% | Wgt Acc: 98.722%
	I - Batch: 150 | Loss: 0.059 | Acc: 96.583% | Wgt Acc: 98.811%
	I - Batch: 200 | Loss: 0.060 | Acc: 96.438% | Wgt Acc: 98.779%
I - num batch: 222
I - Train -- Loss: 0.059 | Acc: 96.448% | Wgt Acc: 98.755% | LR: 1.250000e-04 | Dur: 137.50s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   0.  51.]
 [  1. 578.   0.   0.  13.]
 [  0.   0. 732.   0.  35.]
 [  0.   0.   0. 537.  17.]
 [  6.   0.   2.   1. 884.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.272 | Acc: 66.075% | Wgt Acc: 57.433% | Dur: 14.89s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   1.   8.   6.]
 [  1.  33.   2.   0.   2.]
 [  0.   7.  20.   0.   3.]
 [ 14.   7.  15.  61.   9.]
 [ 12.  29.  37.  17. 160.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 249
I - Training: 
	I - Batch: 50 | Loss: 0.055 | Acc: 96.625% | Wgt Acc: 98.975%
	I - Batch: 100 | Loss: 0.057 | Acc: 96.438% | Wgt Acc: 98.966%
	I - Batch: 150 | Loss: 0.061 | Acc: 96.292% | Wgt Acc: 98.698%
	I - Batch: 200 | Loss: 0.062 | Acc: 96.344% | Wgt Acc: 98.664%
I - num batch: 222
I - Train -- Loss: 0.061 | Acc: 96.391% | Wgt Acc: 98.708% | LR: 1.250000e-04 | Dur: 132.86s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   1.   0.   0.  44.]
 [  0. 575.   0.   0.  15.]
 [  0.   0. 731.   0.  28.]
 [  0.   0.   0. 537.  30.]
 [  4.   2.   3.   1. 883.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.147 | Acc: 68.836% | Wgt Acc: 59.751% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   3.  11.   5.]
 [  0.  32.   2.   0.   1.]
 [  0.   8.  27.   0.   2.]
 [  8.   2.   9.  55.   6.]
 [ 11.  32.  34.  20. 166.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Maximum validation set accuracy in current training:  71.60
