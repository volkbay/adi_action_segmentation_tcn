Mon Oct 31 00:41:49 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'blacklistFile': './blacklists/blacklist30.yaml', 'classWeights': [0.23, 0.24, 0.23, 0.25, 0.05], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'singleBackgroundPath': 'new_background', 'singleBackgroundPickle': True, 'tossFirstLastFrames': True}, 'dataPath': '/data_ssd/processed/kinetics400/', 'dropoutRate': 0.5, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat', 'background'], 'lastLayerInitUniform': False, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 71.20315581854044, 'maxValidationTrainNo': 64, 'modelVersion': 17, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 70, 'validationAccThr': 75, 'warmStartConfig': {'checkpointFile': './sav/model17_trainNo60_at_epoch_197_with_acc_71_60_checkpoint.pth.tar', 'checkpointModelNo': 17, 'freezeSpatialCNN': False, 'warmStartFlag': False}, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [], 'pull ups': [1466, 12635, 131478, 185449, 218783, 35469, 67801, 74587, 75408], 'push up': [2377, 2750, 117373, 173206, 187541, 209276, 213350, 83353, 85540, 99142], 'situp': [7337, 108111, 108505, 111111, 117511, 119784, 127277, 132060, 136812, 145544, 145953, 149066, 150259, 150302, 153122, 153691, 171290, 174857, 177150, 181684, 181706, 183368, 187731, 188206, 190974, 19414, 20909, 216500, 23537, 33814, 34065, 35811, 39060, 42035, 46880, 47767, 56415, 58072, 62391, 64925, 66736, 84326, 91813, 95689, 98098, 99540], 'squat': [3874, 4780, 6372, 9404, 101769, 103303, 109807, 113334, 118191, 118592, 119202, 120752, 122418, 126727, 127111, 131213, 137236, 141399, 142777, 143512, 143925, 146153, 147886, 149752, 151859, 152117, 15417, 159204, 159443, 169907, 173139, 173316, 174855, 175040, 175771, 184090, 186723, 188017, 190146, 191780, 192239, 20076, 203580, 203768, 204978, 208375, 212870, 215592, 218504, 218729, 24358, 28403, 36106, 38864, 50865, 51112, 51544, 61813, 62637, 63445, 69578, 83151, 85369, 91785, 94764, 94929, 95719]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Train set length:  4362
I - Label distribution: [ 797.  760. 1032.  773. 1000.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test
I - Loading file: dataset_test_background00_no_samples180.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test/new_background
I - New label distribution: [ 88.  78.  75.  86. 180.]

I - Test set length:  507
I - Label distribution: [ 88.  78.  75.  86. 180.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(1) tensor(4) data number in dataset:  tensor([157956,   1045, 102813,  54339, 133290,   1493, 188987,    307,    300,
          2585, 127142, 104099,  65426, 173429,    340,   5491])
I - Initializing model TCNv17
I - Number of Model Parameters: 640096
I - Model output shape:  torch.Size([16, 5])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv17                                   [16, 5]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 128, 64, 64]         6
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Conv2d: 2-2                       --                        6,272
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─OutputShiftSqueeze: 2-4           --                        --
│    └─One: 2-5                          [1]                       --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─OutputScale: 2-7                  --                        --
│    └─Empty: 2-8                        [128, 48, 1, 1]           --
│    └─Empty: 2-9                        [128, 48, 1, 1]           --
│    └─Empty: 2-10                       [128]                     --
│    └─Empty: 2-11                       [128]                     --
│    └─BatchNorm2d: 2-12                 [16, 128, 64, 64]         --
│    └─Scaler: 2-13                      [16, 128, 64, 64]         --
│    └─ReLU: 2-14                        [16, 128, 64, 64]         --
│    └─Empty: 2-15                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Conv2d: 2-18                      --                        147,584
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         (recursive)
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-20                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-3          [16, 128, 32, 32]         147,846
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-22                   [16, 128, 32, 32]         --
│    └─Empty: 2-23                       [16, 128, 32, 32]         --
│    └─Empty: 2-24                       [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [128, 128, 3, 3]          --
│    └─Empty: 2-29                       [128, 128, 3, 3]          --
│    └─Empty: 2-30                       [128]                     --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-33                      --                        147,584
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         (recursive)
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-35                       [128]                     --
│    └─BatchNorm2d: 2-36                 [16, 128, 32, 32]         256
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-38                      [16, 128, 32, 32]         --
│    └─ReLU: 2-39                        [16, 128, 32, 32]         --
│    └─Empty: 2-40                       [16, 128, 32, 32]         --
│    └─Clamp: 2-41                       [16, 128, 32, 32]         --
├─Dropout2d: 1-5                         [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-6          [16, 128, 16, 16]         131,078
│    └─MaxPool2d: 2-42                   [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─Empty: 2-45                       [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Conv2d: 2-47                      --                        16,512
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         (recursive)
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-49                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-50          --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-52                         [1]                       --
│    └─OutputScale: 2-53                 --                        --
│    └─Empty: 2-54                       [128, 128, 3, 3]          --
│    └─Empty: 2-55                       [128, 128, 3, 3]          --
│    └─Empty: 2-56                       [128]                     --
│    └─Empty: 2-57                       [128]                     --
│    └─BatchNorm2d: 2-58                 [16, 128, 16, 16]         256
│    └─Scaler: 2-59                      [16, 128, 16, 16]         --
│    └─ReLU: 2-60                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-63                      --                        147,584
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         (recursive)
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-65                       [16, 128, 16, 16]         --
│    └─Clamp: 2-66                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-8                 [16, 128, 16, 16]         16,518
│    └─OutputShiftSqueeze: 2-68          --                        --
│    └─One: 2-69                         [1]                       --
│    └─OutputScale: 2-70                 --                        --
│    └─Empty: 2-71                       [128, 128, 1, 1]          --
│    └─Empty: 2-72                       [128, 128, 1, 1]          --
│    └─Empty: 2-73                       [128]                     --
│    └─Empty: 2-74                       [128]                     --
│    └─BatchNorm2d: 2-75                 [16, 128, 16, 16]         256
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-78                      --                        147,584
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           (recursive)
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-80                      [16, 128, 16, 16]         --
│    └─ReLU: 2-81                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-83                       [16, 128, 16, 16]         --
│    └─Clamp: 2-84                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-10         [16, 128, 16, 16]         145,526
│    └─MaxPool2d: 2-85                   [16, 128, 16, 16]         --
│    └─Empty: 2-86                       [16, 128, 16, 16]         --
│    └─Empty: 2-87                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-88          --                        --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-91                         [1]                       --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Conv2d: 2-93                      --                        2,064
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            (recursive)
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-95                 --                        --
│    └─Empty: 2-96                       [128, 128, 3, 3]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-98                       [128, 128, 3, 3]          --
│    └─Empty: 2-99                       [128]                     --
│    └─Empty: 2-100                      [128]                     --
│    └─BatchNorm2d: 2-101                [16, 128, 16, 16]         256
│    └─Scaler: 2-102                     [16, 128, 16, 16]         --
│    └─ReLU: 2-103                       [16, 128, 16, 16]         --
│    └─Empty: 2-104                      [16, 128, 16, 16]         --
│    └─Clamp: 2-105                      [16, 128, 16, 16]         --
├─Dropout2d: 1-11                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-108                     --                        18,448
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            (recursive)
├─FusedMaxPoolConv2dBNReLU: 1-13         [16, 128, 8, 8]           147,590
│    └─MaxPool2d: 2-110                  [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-112                      [16, 128, 8, 8]           --
│    └─Empty: 2-113                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-114         --                        --
│    └─One: 2-115                        [1]                       --
│    └─OutputScale: 2-116                --                        --
│    └─Empty: 2-117                      [128, 128, 3, 3]          --
│    └─Empty: 2-118                      [128, 128, 3, 3]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-121                      [128]                     --
│    └─Empty: 2-122                      [128]                     --
│    └─BatchNorm2d: 2-123                [16, 128, 8, 8]           256
│    └─Scaler: 2-124                     [16, 128, 8, 8]           --
│    └─ReLU: 2-125                       [16, 128, 8, 8]           --
│    └─Empty: 2-126                      [16, 128, 8, 8]           --
│    └─Clamp: 2-127                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-14                [16, 16, 8, 8]            2,070
├─Linear: 1                              --                        --
│    └─Scaler: 2-128                     --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-129         --                        --
│    └─One: 2-130                        [1]                       --
│    └─OutputScale: 2-131                --                        --
│    └─Empty: 2-132                      [16, 128, 1, 1]           --
│    └─Empty: 2-133                      [16, 128, 1, 1]           --
│    └─Empty: 2-134                      [16]                      --
│    └─Empty: 2-135                      [16]                      --
│    └─BatchNorm2d: 2-136                [16, 16, 8, 8]            32
│    └─Scaler: 2-137                     [16, 16, 8, 8]            --
│    └─ReLU: 2-138                       [16, 16, 8, 8]            --
│    └─Empty: 2-139                      [16, 16, 8, 8]            --
│    └─Clamp: 2-140                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 16, 8, 8]            18,454
│    └─MaxPool2d: 2-141                  [16, 128, 8, 8]           --
│    └─Empty: 2-142                      [16, 128, 8, 8]           --
│    └─Empty: 2-143                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-144         --                        --
│    └─One: 2-145                        [1]                       --
│    └─OutputScale: 2-146                --                        --
│    └─Empty: 2-147                      [16, 128, 3, 3]           --
│    └─Empty: 2-148                      [16, 128, 3, 3]           --
│    └─Empty: 2-149                      [16]                      --
│    └─Empty: 2-150                      [16]                      --
│    └─BatchNorm2d: 2-151                [16, 16, 8, 8]            32
│    └─Scaler: 2-152                     [16, 16, 8, 8]            --
│    └─ReLU: 2-153                       [16, 16, 8, 8]            --
│    └─Empty: 2-154                      [16, 16, 8, 8]            --
│    └─Clamp: 2-155                      [16, 16, 8, 8]            --
├─Dropout2d: 1-16                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-17                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-156         --                        --
│    └─One: 2-157                        [1]                       --
│    └─OutputScale: 2-158                --                        --
│    └─Empty: 2-159                      [128, 48, 1, 1]           --
│    └─Empty: 2-160                      [128, 48, 1, 1]           --
│    └─Empty: 2-161                      [128]                     --
│    └─Empty: 2-162                      [128]                     --
│    └─BatchNorm2d: 2-163                [16, 128, 64, 64]         --
│    └─Scaler: 2-164                     [16, 128, 64, 64]         --
│    └─ReLU: 2-165                       [16, 128, 64, 64]         --
│    └─Empty: 2-166                      [16, 128, 64, 64]         --
│    └─Clamp: 2-167                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-18         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-168                  [16, 128, 32, 32]         --
│    └─Empty: 2-169                      [16, 128, 32, 32]         --
│    └─Empty: 2-170                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-171         --                        --
│    └─One: 2-172                        [1]                       --
│    └─OutputScale: 2-173                --                        --
│    └─Empty: 2-174                      [128, 128, 3, 3]          --
│    └─Empty: 2-175                      [128, 128, 3, 3]          --
│    └─Empty: 2-176                      [128]                     --
│    └─Empty: 2-177                      [128]                     --
│    └─BatchNorm2d: 2-178                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-179                     [16, 128, 32, 32]         --
│    └─ReLU: 2-180                       [16, 128, 32, 32]         --
│    └─Empty: 2-181                      [16, 128, 32, 32]         --
│    └─Clamp: 2-182                      [16, 128, 32, 32]         --
├─Dropout2d: 1-19                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-20         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-183                  [16, 128, 16, 16]         --
│    └─Empty: 2-184                      [16, 128, 16, 16]         --
│    └─Empty: 2-185                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-186         --                        --
│    └─One: 2-187                        [1]                       --
│    └─OutputScale: 2-188                --                        --
│    └─Empty: 2-189                      [128, 128, 3, 3]          --
│    └─Empty: 2-190                      [128, 128, 3, 3]          --
│    └─Empty: 2-191                      [128]                     --
│    └─Empty: 2-192                      [128]                     --
│    └─BatchNorm2d: 2-193                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-194                     [16, 128, 16, 16]         --
│    └─ReLU: 2-195                       [16, 128, 16, 16]         --
│    └─Empty: 2-196                      [16, 128, 16, 16]         --
│    └─Clamp: 2-197                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-21                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-198         --                        --
│    └─One: 2-199                        [1]                       --
│    └─OutputScale: 2-200                --                        --
│    └─Empty: 2-201                      [128, 128, 1, 1]          --
│    └─Empty: 2-202                      [128, 128, 1, 1]          --
│    └─Empty: 2-203                      [128]                     --
│    └─Empty: 2-204                      [128]                     --
│    └─BatchNorm2d: 2-205                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-206                     [16, 128, 16, 16]         --
│    └─ReLU: 2-207                       [16, 128, 16, 16]         --
│    └─Empty: 2-208                      [16, 128, 16, 16]         --
│    └─Clamp: 2-209                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-22         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-210                  [16, 128, 16, 16]         --
│    └─Empty: 2-211                      [16, 128, 16, 16]         --
│    └─Empty: 2-212                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-213         --                        --
│    └─One: 2-214                        [1]                       --
│    └─OutputScale: 2-215                --                        --
│    └─Empty: 2-216                      [128, 128, 3, 3]          --
│    └─Empty: 2-217                      [128, 128, 3, 3]          --
│    └─Empty: 2-218                      [128]                     --
│    └─Empty: 2-219                      [128]                     --
│    └─BatchNorm2d: 2-220                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-221                     [16, 128, 16, 16]         --
│    └─ReLU: 2-222                       [16, 128, 16, 16]         --
│    └─Empty: 2-223                      [16, 128, 16, 16]         --
│    └─Clamp: 2-224                      [16, 128, 16, 16]         --
├─Dropout2d: 1-23                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-24         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-225                  [16, 128, 8, 8]           --
│    └─Empty: 2-226                      [16, 128, 8, 8]           --
│    └─Empty: 2-227                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-228         --                        --
│    └─One: 2-229                        [1]                       --
│    └─OutputScale: 2-230                --                        --
│    └─Empty: 2-231                      [128, 128, 3, 3]          --
│    └─Empty: 2-232                      [128, 128, 3, 3]          --
│    └─Empty: 2-233                      [128]                     --
│    └─Empty: 2-234                      [128]                     --
│    └─BatchNorm2d: 2-235                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-236                     [16, 128, 8, 8]           --
│    └─ReLU: 2-237                       [16, 128, 8, 8]           --
│    └─Empty: 2-238                      [16, 128, 8, 8]           --
│    └─Clamp: 2-239                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-25                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-240         --                        --
│    └─One: 2-241                        [1]                       --
│    └─OutputScale: 2-242                --                        --
│    └─Empty: 2-243                      [16, 128, 1, 1]           --
│    └─Empty: 2-244                      [16, 128, 1, 1]           --
│    └─Empty: 2-245                      [16]                      --
│    └─Empty: 2-246                      [16]                      --
│    └─BatchNorm2d: 2-247                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-248                     [16, 16, 8, 8]            --
│    └─ReLU: 2-249                       [16, 16, 8, 8]            --
│    └─Empty: 2-250                      [16, 16, 8, 8]            --
│    └─Clamp: 2-251                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-252                  [16, 128, 8, 8]           --
│    └─Empty: 2-253                      [16, 128, 8, 8]           --
│    └─Empty: 2-254                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-255         --                        --
│    └─One: 2-256                        [1]                       --
│    └─OutputScale: 2-257                --                        --
│    └─Empty: 2-258                      [16, 128, 3, 3]           --
│    └─Empty: 2-259                      [16, 128, 3, 3]           --
│    └─Empty: 2-260                      [16]                      --
│    └─Empty: 2-261                      [16]                      --
│    └─BatchNorm2d: 2-262                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-263                     [16, 16, 8, 8]            --
│    └─ReLU: 2-264                       [16, 16, 8, 8]            --
│    └─Empty: 2-265                      [16, 16, 8, 8]            --
│    └─Clamp: 2-266                      [16, 16, 8, 8]            --
├─Dropout2d: 1-27                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-28                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-267         --                        --
│    └─One: 2-268                        [1]                       --
│    └─OutputScale: 2-269                --                        --
│    └─Empty: 2-270                      [128, 48, 1, 1]           --
│    └─Empty: 2-271                      [128, 48, 1, 1]           --
│    └─Empty: 2-272                      [128]                     --
│    └─Empty: 2-273                      [128]                     --
│    └─BatchNorm2d: 2-274                [16, 128, 64, 64]         --
│    └─Scaler: 2-275                     [16, 128, 64, 64]         --
│    └─ReLU: 2-276                       [16, 128, 64, 64]         --
│    └─Empty: 2-277                      [16, 128, 64, 64]         --
│    └─Clamp: 2-278                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-29         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-279                  [16, 128, 32, 32]         --
│    └─Empty: 2-280                      [16, 128, 32, 32]         --
│    └─Empty: 2-281                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-282         --                        --
│    └─One: 2-283                        [1]                       --
│    └─OutputScale: 2-284                --                        --
│    └─Empty: 2-285                      [128, 128, 3, 3]          --
│    └─Empty: 2-286                      [128, 128, 3, 3]          --
│    └─Empty: 2-287                      [128]                     --
│    └─Empty: 2-288                      [128]                     --
│    └─BatchNorm2d: 2-289                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-290                     [16, 128, 32, 32]         --
│    └─ReLU: 2-291                       [16, 128, 32, 32]         --
│    └─Empty: 2-292                      [16, 128, 32, 32]         --
│    └─Clamp: 2-293                      [16, 128, 32, 32]         --
├─Dropout2d: 1-30                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-294                  [16, 128, 16, 16]         --
│    └─Empty: 2-295                      [16, 128, 16, 16]         --
│    └─Empty: 2-296                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-297         --                        --
│    └─One: 2-298                        [1]                       --
│    └─OutputScale: 2-299                --                        --
│    └─Empty: 2-300                      [128, 128, 3, 3]          --
│    └─Empty: 2-301                      [128, 128, 3, 3]          --
│    └─Empty: 2-302                      [128]                     --
│    └─Empty: 2-303                      [128]                     --
│    └─BatchNorm2d: 2-304                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-305                     [16, 128, 16, 16]         --
│    └─ReLU: 2-306                       [16, 128, 16, 16]         --
│    └─Empty: 2-307                      [16, 128, 16, 16]         --
│    └─Clamp: 2-308                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-32                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-309         --                        --
│    └─One: 2-310                        [1]                       --
│    └─OutputScale: 2-311                --                        --
│    └─Empty: 2-312                      [128, 128, 1, 1]          --
│    └─Empty: 2-313                      [128, 128, 1, 1]          --
│    └─Empty: 2-314                      [128]                     --
│    └─Empty: 2-315                      [128]                     --
│    └─BatchNorm2d: 2-316                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-317                     [16, 128, 16, 16]         --
│    └─ReLU: 2-318                       [16, 128, 16, 16]         --
│    └─Empty: 2-319                      [16, 128, 16, 16]         --
│    └─Clamp: 2-320                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-33         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-321                  [16, 128, 16, 16]         --
│    └─Empty: 2-322                      [16, 128, 16, 16]         --
│    └─Empty: 2-323                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-324         --                        --
│    └─One: 2-325                        [1]                       --
│    └─OutputScale: 2-326                --                        --
│    └─Empty: 2-327                      [128, 128, 3, 3]          --
│    └─Empty: 2-328                      [128, 128, 3, 3]          --
│    └─Empty: 2-329                      [128]                     --
│    └─Empty: 2-330                      [128]                     --
│    └─BatchNorm2d: 2-331                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-332                     [16, 128, 16, 16]         --
│    └─ReLU: 2-333                       [16, 128, 16, 16]         --
│    └─Empty: 2-334                      [16, 128, 16, 16]         --
│    └─Clamp: 2-335                      [16, 128, 16, 16]         --
├─Dropout2d: 1-34                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-35         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-336                  [16, 128, 8, 8]           --
│    └─Empty: 2-337                      [16, 128, 8, 8]           --
│    └─Empty: 2-338                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-339         --                        --
│    └─One: 2-340                        [1]                       --
│    └─OutputScale: 2-341                --                        --
│    └─Empty: 2-342                      [128, 128, 3, 3]          --
│    └─Empty: 2-343                      [128, 128, 3, 3]          --
│    └─Empty: 2-344                      [128]                     --
│    └─Empty: 2-345                      [128]                     --
│    └─BatchNorm2d: 2-346                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-347                     [16, 128, 8, 8]           --
│    └─ReLU: 2-348                       [16, 128, 8, 8]           --
│    └─Empty: 2-349                      [16, 128, 8, 8]           --
│    └─Clamp: 2-350                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-36                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-351         --                        --
│    └─One: 2-352                        [1]                       --
│    └─OutputScale: 2-353                --                        --
│    └─Empty: 2-354                      [16, 128, 1, 1]           --
│    └─Empty: 2-355                      [16, 128, 1, 1]           --
│    └─Empty: 2-356                      [16]                      --
│    └─Empty: 2-357                      [16]                      --
│    └─BatchNorm2d: 2-358                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-359                     [16, 16, 8, 8]            --
│    └─ReLU: 2-360                       [16, 16, 8, 8]            --
│    └─Empty: 2-361                      [16, 16, 8, 8]            --
│    └─Clamp: 2-362                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-37         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-363                  [16, 128, 8, 8]           --
│    └─Empty: 2-364                      [16, 128, 8, 8]           --
│    └─Empty: 2-365                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-366         --                        --
│    └─One: 2-367                        [1]                       --
│    └─OutputScale: 2-368                --                        --
│    └─Empty: 2-369                      [16, 128, 3, 3]           --
│    └─Empty: 2-370                      [16, 128, 3, 3]           --
│    └─Empty: 2-371                      [16]                      --
│    └─Empty: 2-372                      [16]                      --
│    └─BatchNorm2d: 2-373                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-374                     [16, 16, 8, 8]            --
│    └─ReLU: 2-375                       [16, 16, 8, 8]            --
│    └─Empty: 2-376                      [16, 16, 8, 8]            --
│    └─Clamp: 2-377                      [16, 16, 8, 8]            --
├─Dropout2d: 1-38                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-39                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-378         --                        --
│    └─One: 2-379                        [1]                       --
│    └─OutputScale: 2-380                --                        --
│    └─Empty: 2-381                      [128, 48, 1, 1]           --
│    └─Empty: 2-382                      [128, 48, 1, 1]           --
│    └─Empty: 2-383                      [128]                     --
│    └─Empty: 2-384                      [128]                     --
│    └─BatchNorm2d: 2-385                [16, 128, 64, 64]         --
│    └─Scaler: 2-386                     [16, 128, 64, 64]         --
│    └─ReLU: 2-387                       [16, 128, 64, 64]         --
│    └─Empty: 2-388                      [16, 128, 64, 64]         --
│    └─Clamp: 2-389                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-40         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-390                  [16, 128, 32, 32]         --
│    └─Empty: 2-391                      [16, 128, 32, 32]         --
│    └─Empty: 2-392                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-393         --                        --
│    └─One: 2-394                        [1]                       --
│    └─OutputScale: 2-395                --                        --
│    └─Empty: 2-396                      [128, 128, 3, 3]          --
│    └─Empty: 2-397                      [128, 128, 3, 3]          --
│    └─Empty: 2-398                      [128]                     --
│    └─Empty: 2-399                      [128]                     --
│    └─BatchNorm2d: 2-400                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-401                     [16, 128, 32, 32]         --
│    └─ReLU: 2-402                       [16, 128, 32, 32]         --
│    └─Empty: 2-403                      [16, 128, 32, 32]         --
│    └─Clamp: 2-404                      [16, 128, 32, 32]         --
├─Dropout2d: 1-41                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-42         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-405                  [16, 128, 16, 16]         --
│    └─Empty: 2-406                      [16, 128, 16, 16]         --
│    └─Empty: 2-407                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-408         --                        --
│    └─One: 2-409                        [1]                       --
│    └─OutputScale: 2-410                --                        --
│    └─Empty: 2-411                      [128, 128, 3, 3]          --
│    └─Empty: 2-412                      [128, 128, 3, 3]          --
│    └─Empty: 2-413                      [128]                     --
│    └─Empty: 2-414                      [128]                     --
│    └─BatchNorm2d: 2-415                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-416                     [16, 128, 16, 16]         --
│    └─ReLU: 2-417                       [16, 128, 16, 16]         --
│    └─Empty: 2-418                      [16, 128, 16, 16]         --
│    └─Clamp: 2-419                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-43                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-420         --                        --
│    └─One: 2-421                        [1]                       --
│    └─OutputScale: 2-422                --                        --
│    └─Empty: 2-423                      [128, 128, 1, 1]          --
│    └─Empty: 2-424                      [128, 128, 1, 1]          --
│    └─Empty: 2-425                      [128]                     --
│    └─Empty: 2-426                      [128]                     --
│    └─BatchNorm2d: 2-427                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-428                     [16, 128, 16, 16]         --
│    └─ReLU: 2-429                       [16, 128, 16, 16]         --
│    └─Empty: 2-430                      [16, 128, 16, 16]         --
│    └─Clamp: 2-431                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-44         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-432                  [16, 128, 16, 16]         --
│    └─Empty: 2-433                      [16, 128, 16, 16]         --
│    └─Empty: 2-434                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-435         --                        --
│    └─One: 2-436                        [1]                       --
│    └─OutputScale: 2-437                --                        --
│    └─Empty: 2-438                      [128, 128, 3, 3]          --
│    └─Empty: 2-439                      [128, 128, 3, 3]          --
│    └─Empty: 2-440                      [128]                     --
│    └─Empty: 2-441                      [128]                     --
│    └─BatchNorm2d: 2-442                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-443                     [16, 128, 16, 16]         --
│    └─ReLU: 2-444                       [16, 128, 16, 16]         --
│    └─Empty: 2-445                      [16, 128, 16, 16]         --
│    └─Clamp: 2-446                      [16, 128, 16, 16]         --
├─Dropout2d: 1-45                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-447                  [16, 128, 8, 8]           --
│    └─Empty: 2-448                      [16, 128, 8, 8]           --
│    └─Empty: 2-449                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-450         --                        --
│    └─One: 2-451                        [1]                       --
│    └─OutputScale: 2-452                --                        --
│    └─Empty: 2-453                      [128, 128, 3, 3]          --
│    └─Empty: 2-454                      [128, 128, 3, 3]          --
│    └─Empty: 2-455                      [128]                     --
│    └─Empty: 2-456                      [128]                     --
│    └─BatchNorm2d: 2-457                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-458                     [16, 128, 8, 8]           --
│    └─ReLU: 2-459                       [16, 128, 8, 8]           --
│    └─Empty: 2-460                      [16, 128, 8, 8]           --
│    └─Clamp: 2-461                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-47                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-462         --                        --
│    └─One: 2-463                        [1]                       --
│    └─OutputScale: 2-464                --                        --
│    └─Empty: 2-465                      [16, 128, 1, 1]           --
│    └─Empty: 2-466                      [16, 128, 1, 1]           --
│    └─Empty: 2-467                      [16]                      --
│    └─Empty: 2-468                      [16]                      --
│    └─BatchNorm2d: 2-469                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-470                     [16, 16, 8, 8]            --
│    └─ReLU: 2-471                       [16, 16, 8, 8]            --
│    └─Empty: 2-472                      [16, 16, 8, 8]            --
│    └─Clamp: 2-473                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-474                  [16, 128, 8, 8]           --
│    └─Empty: 2-475                      [16, 128, 8, 8]           --
│    └─Empty: 2-476                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-477         --                        --
│    └─One: 2-478                        [1]                       --
│    └─OutputScale: 2-479                --                        --
│    └─Empty: 2-480                      [16, 128, 3, 3]           --
│    └─Empty: 2-481                      [16, 128, 3, 3]           --
│    └─Empty: 2-482                      [16]                      --
│    └─Empty: 2-483                      [16]                      --
│    └─BatchNorm2d: 2-484                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-485                     [16, 16, 8, 8]            --
│    └─ReLU: 2-486                       [16, 16, 8, 8]            --
│    └─Empty: 2-487                      [16, 16, 8, 8]            --
│    └─Clamp: 2-488                      [16, 16, 8, 8]            --
├─Dropout2d: 1-49                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-50                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-489         --                        --
│    └─One: 2-490                        [1]                       --
│    └─OutputScale: 2-491                --                        --
│    └─Empty: 2-492                      [128, 48, 1, 1]           --
│    └─Empty: 2-493                      [128, 48, 1, 1]           --
│    └─Empty: 2-494                      [128]                     --
│    └─Empty: 2-495                      [128]                     --
│    └─BatchNorm2d: 2-496                [16, 128, 64, 64]         --
│    └─Scaler: 2-497                     [16, 128, 64, 64]         --
│    └─ReLU: 2-498                       [16, 128, 64, 64]         --
│    └─Empty: 2-499                      [16, 128, 64, 64]         --
│    └─Clamp: 2-500                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-501                  [16, 128, 32, 32]         --
│    └─Empty: 2-502                      [16, 128, 32, 32]         --
│    └─Empty: 2-503                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-504         --                        --
│    └─One: 2-505                        [1]                       --
│    └─OutputScale: 2-506                --                        --
│    └─Empty: 2-507                      [128, 128, 3, 3]          --
│    └─Empty: 2-508                      [128, 128, 3, 3]          --
│    └─Empty: 2-509                      [128]                     --
│    └─Empty: 2-510                      [128]                     --
│    └─BatchNorm2d: 2-511                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-512                     [16, 128, 32, 32]         --
│    └─ReLU: 2-513                       [16, 128, 32, 32]         --
│    └─Empty: 2-514                      [16, 128, 32, 32]         --
│    └─Clamp: 2-515                      [16, 128, 32, 32]         --
├─Dropout2d: 1-52                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-53         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-516                  [16, 128, 16, 16]         --
│    └─Empty: 2-517                      [16, 128, 16, 16]         --
│    └─Empty: 2-518                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-519         --                        --
│    └─One: 2-520                        [1]                       --
│    └─OutputScale: 2-521                --                        --
│    └─Empty: 2-522                      [128, 128, 3, 3]          --
│    └─Empty: 2-523                      [128, 128, 3, 3]          --
│    └─Empty: 2-524                      [128]                     --
│    └─Empty: 2-525                      [128]                     --
│    └─BatchNorm2d: 2-526                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-527                     [16, 128, 16, 16]         --
│    └─ReLU: 2-528                       [16, 128, 16, 16]         --
│    └─Empty: 2-529                      [16, 128, 16, 16]         --
│    └─Clamp: 2-530                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-54                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-531         --                        --
│    └─One: 2-532                        [1]                       --
│    └─OutputScale: 2-533                --                        --
│    └─Empty: 2-534                      [128, 128, 1, 1]          --
│    └─Empty: 2-535                      [128, 128, 1, 1]          --
│    └─Empty: 2-536                      [128]                     --
│    └─Empty: 2-537                      [128]                     --
│    └─BatchNorm2d: 2-538                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-539                     [16, 128, 16, 16]         --
│    └─ReLU: 2-540                       [16, 128, 16, 16]         --
│    └─Empty: 2-541                      [16, 128, 16, 16]         --
│    └─Clamp: 2-542                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-55         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-543                  [16, 128, 16, 16]         --
│    └─Empty: 2-544                      [16, 128, 16, 16]         --
│    └─Empty: 2-545                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-546         --                        --
│    └─One: 2-547                        [1]                       --
│    └─OutputScale: 2-548                --                        --
│    └─Empty: 2-549                      [128, 128, 3, 3]          --
│    └─Empty: 2-550                      [128, 128, 3, 3]          --
│    └─Empty: 2-551                      [128]                     --
│    └─Empty: 2-552                      [128]                     --
│    └─BatchNorm2d: 2-553                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-554                     [16, 128, 16, 16]         --
│    └─ReLU: 2-555                       [16, 128, 16, 16]         --
│    └─Empty: 2-556                      [16, 128, 16, 16]         --
│    └─Clamp: 2-557                      [16, 128, 16, 16]         --
├─Dropout2d: 1-56                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-57         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-558                  [16, 128, 8, 8]           --
│    └─Empty: 2-559                      [16, 128, 8, 8]           --
│    └─Empty: 2-560                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-561         --                        --
│    └─One: 2-562                        [1]                       --
│    └─OutputScale: 2-563                --                        --
│    └─Empty: 2-564                      [128, 128, 3, 3]          --
│    └─Empty: 2-565                      [128, 128, 3, 3]          --
│    └─Empty: 2-566                      [128]                     --
│    └─Empty: 2-567                      [128]                     --
│    └─BatchNorm2d: 2-568                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-569                     [16, 128, 8, 8]           --
│    └─ReLU: 2-570                       [16, 128, 8, 8]           --
│    └─Empty: 2-571                      [16, 128, 8, 8]           --
│    └─Clamp: 2-572                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-58                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-573         --                        --
│    └─One: 2-574                        [1]                       --
│    └─OutputScale: 2-575                --                        --
│    └─Empty: 2-576                      [16, 128, 1, 1]           --
│    └─Empty: 2-577                      [16, 128, 1, 1]           --
│    └─Empty: 2-578                      [16]                      --
│    └─Empty: 2-579                      [16]                      --
│    └─BatchNorm2d: 2-580                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-581                     [16, 16, 8, 8]            --
│    └─ReLU: 2-582                       [16, 16, 8, 8]            --
│    └─Empty: 2-583                      [16, 16, 8, 8]            --
│    └─Clamp: 2-584                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-59         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-585                  [16, 128, 8, 8]           --
│    └─Empty: 2-586                      [16, 128, 8, 8]           --
│    └─Empty: 2-587                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-588         --                        --
│    └─One: 2-589                        [1]                       --
│    └─OutputScale: 2-590                --                        --
│    └─Empty: 2-591                      [16, 128, 3, 3]           --
│    └─Empty: 2-592                      [16, 128, 3, 3]           --
│    └─Empty: 2-593                      [16]                      --
│    └─Empty: 2-594                      [16]                      --
│    └─BatchNorm2d: 2-595                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-596                     [16, 16, 8, 8]            --
│    └─ReLU: 2-597                       [16, 16, 8, 8]            --
│    └─Empty: 2-598                      [16, 16, 8, 8]            --
│    └─Clamp: 2-599                      [16, 16, 8, 8]            --
├─Dropout2d: 1-60                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-61                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-600         --                        --
│    └─One: 2-601                        [1]                       --
│    └─OutputScale: 2-602                --                        --
│    └─Empty: 2-603                      [128, 48, 1, 1]           --
│    └─Empty: 2-604                      [128, 48, 1, 1]           --
│    └─Empty: 2-605                      [128]                     --
│    └─Empty: 2-606                      [128]                     --
│    └─BatchNorm2d: 2-607                [16, 128, 64, 64]         --
│    └─Scaler: 2-608                     [16, 128, 64, 64]         --
│    └─ReLU: 2-609                       [16, 128, 64, 64]         --
│    └─Empty: 2-610                      [16, 128, 64, 64]         --
│    └─Clamp: 2-611                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-62         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-612                  [16, 128, 32, 32]         --
│    └─Empty: 2-613                      [16, 128, 32, 32]         --
│    └─Empty: 2-614                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-615         --                        --
│    └─One: 2-616                        [1]                       --
│    └─OutputScale: 2-617                --                        --
│    └─Empty: 2-618                      [128, 128, 3, 3]          --
│    └─Empty: 2-619                      [128, 128, 3, 3]          --
│    └─Empty: 2-620                      [128]                     --
│    └─Empty: 2-621                      [128]                     --
│    └─BatchNorm2d: 2-622                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-623                     [16, 128, 32, 32]         --
│    └─ReLU: 2-624                       [16, 128, 32, 32]         --
│    └─Empty: 2-625                      [16, 128, 32, 32]         --
│    └─Clamp: 2-626                      [16, 128, 32, 32]         --
├─Dropout2d: 1-63                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-64         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-627                  [16, 128, 16, 16]         --
│    └─Empty: 2-628                      [16, 128, 16, 16]         --
│    └─Empty: 2-629                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-630         --                        --
│    └─One: 2-631                        [1]                       --
│    └─OutputScale: 2-632                --                        --
│    └─Empty: 2-633                      [128, 128, 3, 3]          --
│    └─Empty: 2-634                      [128, 128, 3, 3]          --
│    └─Empty: 2-635                      [128]                     --
│    └─Empty: 2-636                      [128]                     --
│    └─BatchNorm2d: 2-637                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-638                     [16, 128, 16, 16]         --
│    └─ReLU: 2-639                       [16, 128, 16, 16]         --
│    └─Empty: 2-640                      [16, 128, 16, 16]         --
│    └─Clamp: 2-641                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-65                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-642         --                        --
│    └─One: 2-643                        [1]                       --
│    └─OutputScale: 2-644                --                        --
│    └─Empty: 2-645                      [128, 128, 1, 1]          --
│    └─Empty: 2-646                      [128, 128, 1, 1]          --
│    └─Empty: 2-647                      [128]                     --
│    └─Empty: 2-648                      [128]                     --
│    └─BatchNorm2d: 2-649                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-650                     [16, 128, 16, 16]         --
│    └─ReLU: 2-651                       [16, 128, 16, 16]         --
│    └─Empty: 2-652                      [16, 128, 16, 16]         --
│    └─Clamp: 2-653                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-654                  [16, 128, 16, 16]         --
│    └─Empty: 2-655                      [16, 128, 16, 16]         --
│    └─Empty: 2-656                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-657         --                        --
│    └─One: 2-658                        [1]                       --
│    └─OutputScale: 2-659                --                        --
│    └─Empty: 2-660                      [128, 128, 3, 3]          --
│    └─Empty: 2-661                      [128, 128, 3, 3]          --
│    └─Empty: 2-662                      [128]                     --
│    └─Empty: 2-663                      [128]                     --
│    └─BatchNorm2d: 2-664                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-665                     [16, 128, 16, 16]         --
│    └─ReLU: 2-666                       [16, 128, 16, 16]         --
│    └─Empty: 2-667                      [16, 128, 16, 16]         --
│    └─Clamp: 2-668                      [16, 128, 16, 16]         --
├─Dropout2d: 1-67                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-669                  [16, 128, 8, 8]           --
│    └─Empty: 2-670                      [16, 128, 8, 8]           --
│    └─Empty: 2-671                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-672         --                        --
│    └─One: 2-673                        [1]                       --
│    └─OutputScale: 2-674                --                        --
│    └─Empty: 2-675                      [128, 128, 3, 3]          --
│    └─Empty: 2-676                      [128, 128, 3, 3]          --
│    └─Empty: 2-677                      [128]                     --
│    └─Empty: 2-678                      [128]                     --
│    └─BatchNorm2d: 2-679                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-680                     [16, 128, 8, 8]           --
│    └─ReLU: 2-681                       [16, 128, 8, 8]           --
│    └─Empty: 2-682                      [16, 128, 8, 8]           --
│    └─Clamp: 2-683                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-69                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-684         --                        --
│    └─One: 2-685                        [1]                       --
│    └─OutputScale: 2-686                --                        --
│    └─Empty: 2-687                      [16, 128, 1, 1]           --
│    └─Empty: 2-688                      [16, 128, 1, 1]           --
│    └─Empty: 2-689                      [16]                      --
│    └─Empty: 2-690                      [16]                      --
│    └─BatchNorm2d: 2-691                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-692                     [16, 16, 8, 8]            --
│    └─ReLU: 2-693                       [16, 16, 8, 8]            --
│    └─Empty: 2-694                      [16, 16, 8, 8]            --
│    └─Clamp: 2-695                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-70         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-696                  [16, 128, 8, 8]           --
│    └─Empty: 2-697                      [16, 128, 8, 8]           --
│    └─Empty: 2-698                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-699         --                        --
│    └─One: 2-700                        [1]                       --
│    └─OutputScale: 2-701                --                        --
│    └─Empty: 2-702                      [16, 128, 3, 3]           --
│    └─Empty: 2-703                      [16, 128, 3, 3]           --
│    └─Empty: 2-704                      [16]                      --
│    └─Empty: 2-705                      [16]                      --
│    └─BatchNorm2d: 2-706                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-707                     [16, 16, 8, 8]            --
│    └─ReLU: 2-708                       [16, 16, 8, 8]            --
│    └─Empty: 2-709                      [16, 16, 8, 8]            --
│    └─Clamp: 2-710                      [16, 16, 8, 8]            --
├─Dropout2d: 1-71                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-72                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-711         --                        --
│    └─One: 2-712                        [1]                       --
│    └─OutputScale: 2-713                --                        --
│    └─Empty: 2-714                      [128, 48, 1, 1]           --
│    └─Empty: 2-715                      [128, 48, 1, 1]           --
│    └─Empty: 2-716                      [128]                     --
│    └─Empty: 2-717                      [128]                     --
│    └─BatchNorm2d: 2-718                [16, 128, 64, 64]         --
│    └─Scaler: 2-719                     [16, 128, 64, 64]         --
│    └─ReLU: 2-720                       [16, 128, 64, 64]         --
│    └─Empty: 2-721                      [16, 128, 64, 64]         --
│    └─Clamp: 2-722                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-723                  [16, 128, 32, 32]         --
│    └─Empty: 2-724                      [16, 128, 32, 32]         --
│    └─Empty: 2-725                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-726         --                        --
│    └─One: 2-727                        [1]                       --
│    └─OutputScale: 2-728                --                        --
│    └─Empty: 2-729                      [128, 128, 3, 3]          --
│    └─Empty: 2-730                      [128, 128, 3, 3]          --
│    └─Empty: 2-731                      [128]                     --
│    └─Empty: 2-732                      [128]                     --
│    └─BatchNorm2d: 2-733                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-734                     [16, 128, 32, 32]         --
│    └─ReLU: 2-735                       [16, 128, 32, 32]         --
│    └─Empty: 2-736                      [16, 128, 32, 32]         --
│    └─Clamp: 2-737                      [16, 128, 32, 32]         --
├─Dropout2d: 1-74                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-738                  [16, 128, 16, 16]         --
│    └─Empty: 2-739                      [16, 128, 16, 16]         --
│    └─Empty: 2-740                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-741         --                        --
│    └─One: 2-742                        [1]                       --
│    └─OutputScale: 2-743                --                        --
│    └─Empty: 2-744                      [128, 128, 3, 3]          --
│    └─Empty: 2-745                      [128, 128, 3, 3]          --
│    └─Empty: 2-746                      [128]                     --
│    └─Empty: 2-747                      [128]                     --
│    └─BatchNorm2d: 2-748                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-749                     [16, 128, 16, 16]         --
│    └─ReLU: 2-750                       [16, 128, 16, 16]         --
│    └─Empty: 2-751                      [16, 128, 16, 16]         --
│    └─Clamp: 2-752                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-76                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-753         --                        --
│    └─One: 2-754                        [1]                       --
│    └─OutputScale: 2-755                --                        --
│    └─Empty: 2-756                      [128, 128, 1, 1]          --
│    └─Empty: 2-757                      [128, 128, 1, 1]          --
│    └─Empty: 2-758                      [128]                     --
│    └─Empty: 2-759                      [128]                     --
│    └─BatchNorm2d: 2-760                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-761                     [16, 128, 16, 16]         --
│    └─ReLU: 2-762                       [16, 128, 16, 16]         --
│    └─Empty: 2-763                      [16, 128, 16, 16]         --
│    └─Clamp: 2-764                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-77         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-765                  [16, 128, 16, 16]         --
│    └─Empty: 2-766                      [16, 128, 16, 16]         --
│    └─Empty: 2-767                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-768         --                        --
│    └─One: 2-769                        [1]                       --
│    └─OutputScale: 2-770                --                        --
│    └─Empty: 2-771                      [128, 128, 3, 3]          --
│    └─Empty: 2-772                      [128, 128, 3, 3]          --
│    └─Empty: 2-773                      [128]                     --
│    └─Empty: 2-774                      [128]                     --
│    └─BatchNorm2d: 2-775                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-776                     [16, 128, 16, 16]         --
│    └─ReLU: 2-777                       [16, 128, 16, 16]         --
│    └─Empty: 2-778                      [16, 128, 16, 16]         --
│    └─Clamp: 2-779                      [16, 128, 16, 16]         --
├─Dropout2d: 1-78                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-79         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-780                  [16, 128, 8, 8]           --
│    └─Empty: 2-781                      [16, 128, 8, 8]           --
│    └─Empty: 2-782                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-783         --                        --
│    └─One: 2-784                        [1]                       --
│    └─OutputScale: 2-785                --                        --
│    └─Empty: 2-786                      [128, 128, 3, 3]          --
│    └─Empty: 2-787                      [128, 128, 3, 3]          --
│    └─Empty: 2-788                      [128]                     --
│    └─Empty: 2-789                      [128]                     --
│    └─BatchNorm2d: 2-790                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-791                     [16, 128, 8, 8]           --
│    └─ReLU: 2-792                       [16, 128, 8, 8]           --
│    └─Empty: 2-793                      [16, 128, 8, 8]           --
│    └─Clamp: 2-794                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-80                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-795         --                        --
│    └─One: 2-796                        [1]                       --
│    └─OutputScale: 2-797                --                        --
│    └─Empty: 2-798                      [16, 128, 1, 1]           --
│    └─Empty: 2-799                      [16, 128, 1, 1]           --
│    └─Empty: 2-800                      [16]                      --
│    └─Empty: 2-801                      [16]                      --
│    └─BatchNorm2d: 2-802                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-803                     [16, 16, 8, 8]            --
│    └─ReLU: 2-804                       [16, 16, 8, 8]            --
│    └─Empty: 2-805                      [16, 16, 8, 8]            --
│    └─Clamp: 2-806                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-807                  [16, 128, 8, 8]           --
│    └─Empty: 2-808                      [16, 128, 8, 8]           --
│    └─Empty: 2-809                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-810         --                        --
│    └─One: 2-811                        [1]                       --
│    └─OutputScale: 2-812                --                        --
│    └─Empty: 2-813                      [16, 128, 3, 3]           --
│    └─Empty: 2-814                      [16, 128, 3, 3]           --
│    └─Empty: 2-815                      [16]                      --
│    └─Empty: 2-816                      [16]                      --
│    └─BatchNorm2d: 2-817                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-818                     [16, 16, 8, 8]            --
│    └─ReLU: 2-819                       [16, 16, 8, 8]            --
│    └─Empty: 2-820                      [16, 16, 8, 8]            --
│    └─Clamp: 2-821                      [16, 16, 8, 8]            --
├─Dropout2d: 1-82                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-83                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-822         --                        --
│    └─One: 2-823                        [1]                       --
│    └─OutputScale: 2-824                --                        --
│    └─Empty: 2-825                      [128, 48, 1, 1]           --
│    └─Empty: 2-826                      [128, 48, 1, 1]           --
│    └─Empty: 2-827                      [128]                     --
│    └─Empty: 2-828                      [128]                     --
│    └─BatchNorm2d: 2-829                [16, 128, 64, 64]         --
│    └─Scaler: 2-830                     [16, 128, 64, 64]         --
│    └─ReLU: 2-831                       [16, 128, 64, 64]         --
│    └─Empty: 2-832                      [16, 128, 64, 64]         --
│    └─Clamp: 2-833                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-84         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-834                  [16, 128, 32, 32]         --
│    └─Empty: 2-835                      [16, 128, 32, 32]         --
│    └─Empty: 2-836                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-837         --                        --
│    └─One: 2-838                        [1]                       --
│    └─OutputScale: 2-839                --                        --
│    └─Empty: 2-840                      [128, 128, 3, 3]          --
│    └─Empty: 2-841                      [128, 128, 3, 3]          --
│    └─Empty: 2-842                      [128]                     --
│    └─Empty: 2-843                      [128]                     --
│    └─BatchNorm2d: 2-844                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-845                     [16, 128, 32, 32]         --
│    └─ReLU: 2-846                       [16, 128, 32, 32]         --
│    └─Empty: 2-847                      [16, 128, 32, 32]         --
│    └─Clamp: 2-848                      [16, 128, 32, 32]         --
├─Dropout2d: 1-85                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-86         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-849                  [16, 128, 16, 16]         --
│    └─Empty: 2-850                      [16, 128, 16, 16]         --
│    └─Empty: 2-851                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-852         --                        --
│    └─One: 2-853                        [1]                       --
│    └─OutputScale: 2-854                --                        --
│    └─Empty: 2-855                      [128, 128, 3, 3]          --
│    └─Empty: 2-856                      [128, 128, 3, 3]          --
│    └─Empty: 2-857                      [128]                     --
│    └─Empty: 2-858                      [128]                     --
│    └─BatchNorm2d: 2-859                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-860                     [16, 128, 16, 16]         --
│    └─ReLU: 2-861                       [16, 128, 16, 16]         --
│    └─Empty: 2-862                      [16, 128, 16, 16]         --
│    └─Clamp: 2-863                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-87                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-864         --                        --
│    └─One: 2-865                        [1]                       --
│    └─OutputScale: 2-866                --                        --
│    └─Empty: 2-867                      [128, 128, 1, 1]          --
│    └─Empty: 2-868                      [128, 128, 1, 1]          --
│    └─Empty: 2-869                      [128]                     --
│    └─Empty: 2-870                      [128]                     --
│    └─BatchNorm2d: 2-871                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-872                     [16, 128, 16, 16]         --
│    └─ReLU: 2-873                       [16, 128, 16, 16]         --
│    └─Empty: 2-874                      [16, 128, 16, 16]         --
│    └─Clamp: 2-875                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-88         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-876                  [16, 128, 16, 16]         --
│    └─Empty: 2-877                      [16, 128, 16, 16]         --
│    └─Empty: 2-878                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-879         --                        --
│    └─One: 2-880                        [1]                       --
│    └─OutputScale: 2-881                --                        --
│    └─Empty: 2-882                      [128, 128, 3, 3]          --
│    └─Empty: 2-883                      [128, 128, 3, 3]          --
│    └─Empty: 2-884                      [128]                     --
│    └─Empty: 2-885                      [128]                     --
│    └─BatchNorm2d: 2-886                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-887                     [16, 128, 16, 16]         --
│    └─ReLU: 2-888                       [16, 128, 16, 16]         --
│    └─Empty: 2-889                      [16, 128, 16, 16]         --
│    └─Clamp: 2-890                      [16, 128, 16, 16]         --
├─Dropout2d: 1-89                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-90         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-891                  [16, 128, 8, 8]           --
│    └─Empty: 2-892                      [16, 128, 8, 8]           --
│    └─Empty: 2-893                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-894         --                        --
│    └─One: 2-895                        [1]                       --
│    └─OutputScale: 2-896                --                        --
│    └─Empty: 2-897                      [128, 128, 3, 3]          --
│    └─Empty: 2-898                      [128, 128, 3, 3]          --
│    └─Empty: 2-899                      [128]                     --
│    └─Empty: 2-900                      [128]                     --
│    └─BatchNorm2d: 2-901                [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-902                     [16, 128, 8, 8]           --
│    └─ReLU: 2-903                       [16, 128, 8, 8]           --
│    └─Empty: 2-904                      [16, 128, 8, 8]           --
│    └─Clamp: 2-905                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-91                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-906         --                        --
│    └─One: 2-907                        [1]                       --
│    └─OutputScale: 2-908                --                        --
│    └─Empty: 2-909                      [16, 128, 1, 1]           --
│    └─Empty: 2-910                      [16, 128, 1, 1]           --
│    └─Empty: 2-911                      [16]                      --
│    └─Empty: 2-912                      [16]                      --
│    └─BatchNorm2d: 2-913                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-914                     [16, 16, 8, 8]            --
│    └─ReLU: 2-915                       [16, 16, 8, 8]            --
│    └─Empty: 2-916                      [16, 16, 8, 8]            --
│    └─Clamp: 2-917                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-92         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-918                  [16, 128, 8, 8]           --
│    └─Empty: 2-919                      [16, 128, 8, 8]           --
│    └─Empty: 2-920                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-921         --                        --
│    └─One: 2-922                        [1]                       --
│    └─OutputScale: 2-923                --                        --
│    └─Empty: 2-924                      [16, 128, 3, 3]           --
│    └─Empty: 2-925                      [16, 128, 3, 3]           --
│    └─Empty: 2-926                      [16]                      --
│    └─Empty: 2-927                      [16]                      --
│    └─BatchNorm2d: 2-928                [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-929                     [16, 16, 8, 8]            --
│    └─ReLU: 2-930                       [16, 16, 8, 8]            --
│    └─Empty: 2-931                      [16, 16, 8, 8]            --
│    └─Clamp: 2-932                      [16, 16, 8, 8]            --
├─Dropout2d: 1-93                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-94                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-933         --                        --
│    └─One: 2-934                        [1]                       --
│    └─OutputScale: 2-935                --                        --
│    └─Empty: 2-936                      [128, 48, 1, 1]           --
│    └─Empty: 2-937                      [128, 48, 1, 1]           --
│    └─Empty: 2-938                      [128]                     --
│    └─Empty: 2-939                      [128]                     --
│    └─BatchNorm2d: 2-940                [16, 128, 64, 64]         --
│    └─Scaler: 2-941                     [16, 128, 64, 64]         --
│    └─ReLU: 2-942                       [16, 128, 64, 64]         --
│    └─Empty: 2-943                      [16, 128, 64, 64]         --
│    └─Clamp: 2-944                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-95         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-945                  [16, 128, 32, 32]         --
│    └─Empty: 2-946                      [16, 128, 32, 32]         --
│    └─Empty: 2-947                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-948         --                        --
│    └─One: 2-949                        [1]                       --
│    └─OutputScale: 2-950                --                        --
│    └─Empty: 2-951                      [128, 128, 3, 3]          --
│    └─Empty: 2-952                      [128, 128, 3, 3]          --
│    └─Empty: 2-953                      [128]                     --
│    └─Empty: 2-954                      [128]                     --
│    └─BatchNorm2d: 2-955                [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-956                     [16, 128, 32, 32]         --
│    └─ReLU: 2-957                       [16, 128, 32, 32]         --
│    └─Empty: 2-958                      [16, 128, 32, 32]         --
│    └─Clamp: 2-959                      [16, 128, 32, 32]         --
├─Dropout2d: 1-96                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-97         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-960                  [16, 128, 16, 16]         --
│    └─Empty: 2-961                      [16, 128, 16, 16]         --
│    └─Empty: 2-962                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-963         --                        --
│    └─One: 2-964                        [1]                       --
│    └─OutputScale: 2-965                --                        --
│    └─Empty: 2-966                      [128, 128, 3, 3]          --
│    └─Empty: 2-967                      [128, 128, 3, 3]          --
│    └─Empty: 2-968                      [128]                     --
│    └─Empty: 2-969                      [128]                     --
│    └─BatchNorm2d: 2-970                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-971                     [16, 128, 16, 16]         --
│    └─ReLU: 2-972                       [16, 128, 16, 16]         --
│    └─Empty: 2-973                      [16, 128, 16, 16]         --
│    └─Clamp: 2-974                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-98                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-975         --                        --
│    └─One: 2-976                        [1]                       --
│    └─OutputScale: 2-977                --                        --
│    └─Empty: 2-978                      [128, 128, 1, 1]          --
│    └─Empty: 2-979                      [128, 128, 1, 1]          --
│    └─Empty: 2-980                      [128]                     --
│    └─Empty: 2-981                      [128]                     --
│    └─BatchNorm2d: 2-982                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-983                     [16, 128, 16, 16]         --
│    └─ReLU: 2-984                       [16, 128, 16, 16]         --
│    └─Empty: 2-985                      [16, 128, 16, 16]         --
│    └─Clamp: 2-986                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-99         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-987                  [16, 128, 16, 16]         --
│    └─Empty: 2-988                      [16, 128, 16, 16]         --
│    └─Empty: 2-989                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-990         --                        --
│    └─One: 2-991                        [1]                       --
│    └─OutputScale: 2-992                --                        --
│    └─Empty: 2-993                      [128, 128, 3, 3]          --
│    └─Empty: 2-994                      [128, 128, 3, 3]          --
│    └─Empty: 2-995                      [128]                     --
│    └─Empty: 2-996                      [128]                     --
│    └─BatchNorm2d: 2-997                [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-998                     [16, 128, 16, 16]         --
│    └─ReLU: 2-999                       [16, 128, 16, 16]         --
│    └─Empty: 2-1000                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1001                     [16, 128, 16, 16]         --
├─Dropout2d: 1-100                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-101        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1002                 [16, 128, 8, 8]           --
│    └─Empty: 2-1003                     [16, 128, 8, 8]           --
│    └─Empty: 2-1004                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1005        --                        --
│    └─One: 2-1006                       [1]                       --
│    └─OutputScale: 2-1007               --                        --
│    └─Empty: 2-1008                     [128, 128, 3, 3]          --
│    └─Empty: 2-1009                     [128, 128, 3, 3]          --
│    └─Empty: 2-1010                     [128]                     --
│    └─Empty: 2-1011                     [128]                     --
│    └─BatchNorm2d: 2-1012               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1013                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1014                      [16, 128, 8, 8]           --
│    └─Empty: 2-1015                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1016                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-102               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1017        --                        --
│    └─One: 2-1018                       [1]                       --
│    └─OutputScale: 2-1019               --                        --
│    └─Empty: 2-1020                     [16, 128, 1, 1]           --
│    └─Empty: 2-1021                     [16, 128, 1, 1]           --
│    └─Empty: 2-1022                     [16]                      --
│    └─Empty: 2-1023                     [16]                      --
│    └─BatchNorm2d: 2-1024               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1025                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1026                      [16, 16, 8, 8]            --
│    └─Empty: 2-1027                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1028                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-103        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1029                 [16, 128, 8, 8]           --
│    └─Empty: 2-1030                     [16, 128, 8, 8]           --
│    └─Empty: 2-1031                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1032        --                        --
│    └─One: 2-1033                       [1]                       --
│    └─OutputScale: 2-1034               --                        --
│    └─Empty: 2-1035                     [16, 128, 3, 3]           --
│    └─Empty: 2-1036                     [16, 128, 3, 3]           --
│    └─Empty: 2-1037                     [16]                      --
│    └─Empty: 2-1038                     [16]                      --
│    └─BatchNorm2d: 2-1039               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1040                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1041                      [16, 16, 8, 8]            --
│    └─Empty: 2-1042                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1043                     [16, 16, 8, 8]            --
├─Dropout2d: 1-104                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-105               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1044        --                        --
│    └─One: 2-1045                       [1]                       --
│    └─OutputScale: 2-1046               --                        --
│    └─Empty: 2-1047                     [128, 48, 1, 1]           --
│    └─Empty: 2-1048                     [128, 48, 1, 1]           --
│    └─Empty: 2-1049                     [128]                     --
│    └─Empty: 2-1050                     [128]                     --
│    └─BatchNorm2d: 2-1051               [16, 128, 64, 64]         --
│    └─Scaler: 2-1052                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1053                      [16, 128, 64, 64]         --
│    └─Empty: 2-1054                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1055                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-106        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1056                 [16, 128, 32, 32]         --
│    └─Empty: 2-1057                     [16, 128, 32, 32]         --
│    └─Empty: 2-1058                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1059        --                        --
│    └─One: 2-1060                       [1]                       --
│    └─OutputScale: 2-1061               --                        --
│    └─Empty: 2-1062                     [128, 128, 3, 3]          --
│    └─Empty: 2-1063                     [128, 128, 3, 3]          --
│    └─Empty: 2-1064                     [128]                     --
│    └─Empty: 2-1065                     [128]                     --
│    └─BatchNorm2d: 2-1066               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1067                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1068                      [16, 128, 32, 32]         --
│    └─Empty: 2-1069                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1070                     [16, 128, 32, 32]         --
├─Dropout2d: 1-107                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-108        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1071                 [16, 128, 16, 16]         --
│    └─Empty: 2-1072                     [16, 128, 16, 16]         --
│    └─Empty: 2-1073                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1074        --                        --
│    └─One: 2-1075                       [1]                       --
│    └─OutputScale: 2-1076               --                        --
│    └─Empty: 2-1077                     [128, 128, 3, 3]          --
│    └─Empty: 2-1078                     [128, 128, 3, 3]          --
│    └─Empty: 2-1079                     [128]                     --
│    └─Empty: 2-1080                     [128]                     --
│    └─BatchNorm2d: 2-1081               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1082                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1083                      [16, 128, 16, 16]         --
│    └─Empty: 2-1084                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1085                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-109               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1086        --                        --
│    └─One: 2-1087                       [1]                       --
│    └─OutputScale: 2-1088               --                        --
│    └─Empty: 2-1089                     [128, 128, 1, 1]          --
│    └─Empty: 2-1090                     [128, 128, 1, 1]          --
│    └─Empty: 2-1091                     [128]                     --
│    └─Empty: 2-1092                     [128]                     --
│    └─BatchNorm2d: 2-1093               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1094                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1095                      [16, 128, 16, 16]         --
│    └─Empty: 2-1096                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1097                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-110        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1098                 [16, 128, 16, 16]         --
│    └─Empty: 2-1099                     [16, 128, 16, 16]         --
│    └─Empty: 2-1100                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1101        --                        --
│    └─One: 2-1102                       [1]                       --
│    └─OutputScale: 2-1103               --                        --
│    └─Empty: 2-1104                     [128, 128, 3, 3]          --
│    └─Empty: 2-1105                     [128, 128, 3, 3]          --
│    └─Empty: 2-1106                     [128]                     --
│    └─Empty: 2-1107                     [128]                     --
│    └─BatchNorm2d: 2-1108               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1109                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1110                      [16, 128, 16, 16]         --
│    └─Empty: 2-1111                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1112                     [16, 128, 16, 16]         --
├─Dropout2d: 1-111                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-112        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1113                 [16, 128, 8, 8]           --
│    └─Empty: 2-1114                     [16, 128, 8, 8]           --
│    └─Empty: 2-1115                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1116        --                        --
│    └─One: 2-1117                       [1]                       --
│    └─OutputScale: 2-1118               --                        --
│    └─Empty: 2-1119                     [128, 128, 3, 3]          --
│    └─Empty: 2-1120                     [128, 128, 3, 3]          --
│    └─Empty: 2-1121                     [128]                     --
│    └─Empty: 2-1122                     [128]                     --
│    └─BatchNorm2d: 2-1123               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1124                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1125                      [16, 128, 8, 8]           --
│    └─Empty: 2-1126                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1127                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-113               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1128        --                        --
│    └─One: 2-1129                       [1]                       --
│    └─OutputScale: 2-1130               --                        --
│    └─Empty: 2-1131                     [16, 128, 1, 1]           --
│    └─Empty: 2-1132                     [16, 128, 1, 1]           --
│    └─Empty: 2-1133                     [16]                      --
│    └─Empty: 2-1134                     [16]                      --
│    └─BatchNorm2d: 2-1135               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1136                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1137                      [16, 16, 8, 8]            --
│    └─Empty: 2-1138                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1139                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-114        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1140                 [16, 128, 8, 8]           --
│    └─Empty: 2-1141                     [16, 128, 8, 8]           --
│    └─Empty: 2-1142                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1143        --                        --
│    └─One: 2-1144                       [1]                       --
│    └─OutputScale: 2-1145               --                        --
│    └─Empty: 2-1146                     [16, 128, 3, 3]           --
│    └─Empty: 2-1147                     [16, 128, 3, 3]           --
│    └─Empty: 2-1148                     [16]                      --
│    └─Empty: 2-1149                     [16]                      --
│    └─BatchNorm2d: 2-1150               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1151                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1152                      [16, 16, 8, 8]            --
│    └─Empty: 2-1153                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1154                     [16, 16, 8, 8]            --
├─Dropout2d: 1-115                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-116               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1155        --                        --
│    └─One: 2-1156                       [1]                       --
│    └─OutputScale: 2-1157               --                        --
│    └─Empty: 2-1158                     [128, 48, 1, 1]           --
│    └─Empty: 2-1159                     [128, 48, 1, 1]           --
│    └─Empty: 2-1160                     [128]                     --
│    └─Empty: 2-1161                     [128]                     --
│    └─BatchNorm2d: 2-1162               [16, 128, 64, 64]         --
│    └─Scaler: 2-1163                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1164                      [16, 128, 64, 64]         --
│    └─Empty: 2-1165                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1166                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-117        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1167                 [16, 128, 32, 32]         --
│    └─Empty: 2-1168                     [16, 128, 32, 32]         --
│    └─Empty: 2-1169                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1170        --                        --
│    └─One: 2-1171                       [1]                       --
│    └─OutputScale: 2-1172               --                        --
│    └─Empty: 2-1173                     [128, 128, 3, 3]          --
│    └─Empty: 2-1174                     [128, 128, 3, 3]          --
│    └─Empty: 2-1175                     [128]                     --
│    └─Empty: 2-1176                     [128]                     --
│    └─BatchNorm2d: 2-1177               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1178                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1179                      [16, 128, 32, 32]         --
│    └─Empty: 2-1180                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1181                     [16, 128, 32, 32]         --
├─Dropout2d: 1-118                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-119        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1182                 [16, 128, 16, 16]         --
│    └─Empty: 2-1183                     [16, 128, 16, 16]         --
│    └─Empty: 2-1184                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1185        --                        --
│    └─One: 2-1186                       [1]                       --
│    └─OutputScale: 2-1187               --                        --
│    └─Empty: 2-1188                     [128, 128, 3, 3]          --
│    └─Empty: 2-1189                     [128, 128, 3, 3]          --
│    └─Empty: 2-1190                     [128]                     --
│    └─Empty: 2-1191                     [128]                     --
│    └─BatchNorm2d: 2-1192               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1193                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1194                      [16, 128, 16, 16]         --
│    └─Empty: 2-1195                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1196                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-120               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1197        --                        --
│    └─One: 2-1198                       [1]                       --
│    └─OutputScale: 2-1199               --                        --
│    └─Empty: 2-1200                     [128, 128, 1, 1]          --
│    └─Empty: 2-1201                     [128, 128, 1, 1]          --
│    └─Empty: 2-1202                     [128]                     --
│    └─Empty: 2-1203                     [128]                     --
│    └─BatchNorm2d: 2-1204               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1205                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1206                      [16, 128, 16, 16]         --
│    └─Empty: 2-1207                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1208                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-121        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1209                 [16, 128, 16, 16]         --
│    └─Empty: 2-1210                     [16, 128, 16, 16]         --
│    └─Empty: 2-1211                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1212        --                        --
│    └─One: 2-1213                       [1]                       --
│    └─OutputScale: 2-1214               --                        --
│    └─Empty: 2-1215                     [128, 128, 3, 3]          --
│    └─Empty: 2-1216                     [128, 128, 3, 3]          --
│    └─Empty: 2-1217                     [128]                     --
│    └─Empty: 2-1218                     [128]                     --
│    └─BatchNorm2d: 2-1219               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1220                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1221                      [16, 128, 16, 16]         --
│    └─Empty: 2-1222                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1223                     [16, 128, 16, 16]         --
├─Dropout2d: 1-122                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-123        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1224                 [16, 128, 8, 8]           --
│    └─Empty: 2-1225                     [16, 128, 8, 8]           --
│    └─Empty: 2-1226                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1227        --                        --
│    └─One: 2-1228                       [1]                       --
│    └─OutputScale: 2-1229               --                        --
│    └─Empty: 2-1230                     [128, 128, 3, 3]          --
│    └─Empty: 2-1231                     [128, 128, 3, 3]          --
│    └─Empty: 2-1232                     [128]                     --
│    └─Empty: 2-1233                     [128]                     --
│    └─BatchNorm2d: 2-1234               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1235                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1236                      [16, 128, 8, 8]           --
│    └─Empty: 2-1237                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1238                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-124               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1239        --                        --
│    └─One: 2-1240                       [1]                       --
│    └─OutputScale: 2-1241               --                        --
│    └─Empty: 2-1242                     [16, 128, 1, 1]           --
│    └─Empty: 2-1243                     [16, 128, 1, 1]           --
│    └─Empty: 2-1244                     [16]                      --
│    └─Empty: 2-1245                     [16]                      --
│    └─BatchNorm2d: 2-1246               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1247                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1248                      [16, 16, 8, 8]            --
│    └─Empty: 2-1249                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1250                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-125        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1251                 [16, 128, 8, 8]           --
│    └─Empty: 2-1252                     [16, 128, 8, 8]           --
│    └─Empty: 2-1253                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1254        --                        --
│    └─One: 2-1255                       [1]                       --
│    └─OutputScale: 2-1256               --                        --
│    └─Empty: 2-1257                     [16, 128, 3, 3]           --
│    └─Empty: 2-1258                     [16, 128, 3, 3]           --
│    └─Empty: 2-1259                     [16]                      --
│    └─Empty: 2-1260                     [16]                      --
│    └─BatchNorm2d: 2-1261               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1262                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1263                      [16, 16, 8, 8]            --
│    └─Empty: 2-1264                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1265                     [16, 16, 8, 8]            --
├─Dropout2d: 1-126                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-127               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1266        --                        --
│    └─One: 2-1267                       [1]                       --
│    └─OutputScale: 2-1268               --                        --
│    └─Empty: 2-1269                     [128, 48, 1, 1]           --
│    └─Empty: 2-1270                     [128, 48, 1, 1]           --
│    └─Empty: 2-1271                     [128]                     --
│    └─Empty: 2-1272                     [128]                     --
│    └─BatchNorm2d: 2-1273               [16, 128, 64, 64]         --
│    └─Scaler: 2-1274                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1275                      [16, 128, 64, 64]         --
│    └─Empty: 2-1276                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1277                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-128        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1278                 [16, 128, 32, 32]         --
│    └─Empty: 2-1279                     [16, 128, 32, 32]         --
│    └─Empty: 2-1280                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1281        --                        --
│    └─One: 2-1282                       [1]                       --
│    └─OutputScale: 2-1283               --                        --
│    └─Empty: 2-1284                     [128, 128, 3, 3]          --
│    └─Empty: 2-1285                     [128, 128, 3, 3]          --
│    └─Empty: 2-1286                     [128]                     --
│    └─Empty: 2-1287                     [128]                     --
│    └─BatchNorm2d: 2-1288               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1289                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1290                      [16, 128, 32, 32]         --
│    └─Empty: 2-1291                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1292                     [16, 128, 32, 32]         --
├─Dropout2d: 1-129                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-130        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1293                 [16, 128, 16, 16]         --
│    └─Empty: 2-1294                     [16, 128, 16, 16]         --
│    └─Empty: 2-1295                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1296        --                        --
│    └─One: 2-1297                       [1]                       --
│    └─OutputScale: 2-1298               --                        --
│    └─Empty: 2-1299                     [128, 128, 3, 3]          --
│    └─Empty: 2-1300                     [128, 128, 3, 3]          --
│    └─Empty: 2-1301                     [128]                     --
│    └─Empty: 2-1302                     [128]                     --
│    └─BatchNorm2d: 2-1303               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1304                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1305                      [16, 128, 16, 16]         --
│    └─Empty: 2-1306                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1307                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-131               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1308        --                        --
│    └─One: 2-1309                       [1]                       --
│    └─OutputScale: 2-1310               --                        --
│    └─Empty: 2-1311                     [128, 128, 1, 1]          --
│    └─Empty: 2-1312                     [128, 128, 1, 1]          --
│    └─Empty: 2-1313                     [128]                     --
│    └─Empty: 2-1314                     [128]                     --
│    └─BatchNorm2d: 2-1315               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1316                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1317                      [16, 128, 16, 16]         --
│    └─Empty: 2-1318                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1319                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-132        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1320                 [16, 128, 16, 16]         --
│    └─Empty: 2-1321                     [16, 128, 16, 16]         --
│    └─Empty: 2-1322                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1323        --                        --
│    └─One: 2-1324                       [1]                       --
│    └─OutputScale: 2-1325               --                        --
│    └─Empty: 2-1326                     [128, 128, 3, 3]          --
│    └─Empty: 2-1327                     [128, 128, 3, 3]          --
│    └─Empty: 2-1328                     [128]                     --
│    └─Empty: 2-1329                     [128]                     --
│    └─BatchNorm2d: 2-1330               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1331                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1332                      [16, 128, 16, 16]         --
│    └─Empty: 2-1333                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1334                     [16, 128, 16, 16]         --
├─Dropout2d: 1-133                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-134        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1335                 [16, 128, 8, 8]           --
│    └─Empty: 2-1336                     [16, 128, 8, 8]           --
│    └─Empty: 2-1337                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1338        --                        --
│    └─One: 2-1339                       [1]                       --
│    └─OutputScale: 2-1340               --                        --
│    └─Empty: 2-1341                     [128, 128, 3, 3]          --
│    └─Empty: 2-1342                     [128, 128, 3, 3]          --
│    └─Empty: 2-1343                     [128]                     --
│    └─Empty: 2-1344                     [128]                     --
│    └─BatchNorm2d: 2-1345               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1346                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1347                      [16, 128, 8, 8]           --
│    └─Empty: 2-1348                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1349                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-135               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1350        --                        --
│    └─One: 2-1351                       [1]                       --
│    └─OutputScale: 2-1352               --                        --
│    └─Empty: 2-1353                     [16, 128, 1, 1]           --
│    └─Empty: 2-1354                     [16, 128, 1, 1]           --
│    └─Empty: 2-1355                     [16]                      --
│    └─Empty: 2-1356                     [16]                      --
│    └─BatchNorm2d: 2-1357               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1358                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1359                      [16, 16, 8, 8]            --
│    └─Empty: 2-1360                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1361                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-136        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1362                 [16, 128, 8, 8]           --
│    └─Empty: 2-1363                     [16, 128, 8, 8]           --
│    └─Empty: 2-1364                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1365        --                        --
│    └─One: 2-1366                       [1]                       --
│    └─OutputScale: 2-1367               --                        --
│    └─Empty: 2-1368                     [16, 128, 3, 3]           --
│    └─Empty: 2-1369                     [16, 128, 3, 3]           --
│    └─Empty: 2-1370                     [16]                      --
│    └─Empty: 2-1371                     [16]                      --
│    └─BatchNorm2d: 2-1372               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1373                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1374                      [16, 16, 8, 8]            --
│    └─Empty: 2-1375                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1376                     [16, 16, 8, 8]            --
├─Dropout2d: 1-137                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-138               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1377        --                        --
│    └─One: 2-1378                       [1]                       --
│    └─OutputScale: 2-1379               --                        --
│    └─Empty: 2-1380                     [128, 48, 1, 1]           --
│    └─Empty: 2-1381                     [128, 48, 1, 1]           --
│    └─Empty: 2-1382                     [128]                     --
│    └─Empty: 2-1383                     [128]                     --
│    └─BatchNorm2d: 2-1384               [16, 128, 64, 64]         --
│    └─Scaler: 2-1385                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1386                      [16, 128, 64, 64]         --
│    └─Empty: 2-1387                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1388                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-139        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1389                 [16, 128, 32, 32]         --
│    └─Empty: 2-1390                     [16, 128, 32, 32]         --
│    └─Empty: 2-1391                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1392        --                        --
│    └─One: 2-1393                       [1]                       --
│    └─OutputScale: 2-1394               --                        --
│    └─Empty: 2-1395                     [128, 128, 3, 3]          --
│    └─Empty: 2-1396                     [128, 128, 3, 3]          --
│    └─Empty: 2-1397                     [128]                     --
│    └─Empty: 2-1398                     [128]                     --
│    └─BatchNorm2d: 2-1399               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1400                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1401                      [16, 128, 32, 32]         --
│    └─Empty: 2-1402                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1403                     [16, 128, 32, 32]         --
├─Dropout2d: 1-140                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-141        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1404                 [16, 128, 16, 16]         --
│    └─Empty: 2-1405                     [16, 128, 16, 16]         --
│    └─Empty: 2-1406                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1407        --                        --
│    └─One: 2-1408                       [1]                       --
│    └─OutputScale: 2-1409               --                        --
│    └─Empty: 2-1410                     [128, 128, 3, 3]          --
│    └─Empty: 2-1411                     [128, 128, 3, 3]          --
│    └─Empty: 2-1412                     [128]                     --
│    └─Empty: 2-1413                     [128]                     --
│    └─BatchNorm2d: 2-1414               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1415                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1416                      [16, 128, 16, 16]         --
│    └─Empty: 2-1417                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1418                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-142               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1419        --                        --
│    └─One: 2-1420                       [1]                       --
│    └─OutputScale: 2-1421               --                        --
│    └─Empty: 2-1422                     [128, 128, 1, 1]          --
│    └─Empty: 2-1423                     [128, 128, 1, 1]          --
│    └─Empty: 2-1424                     [128]                     --
│    └─Empty: 2-1425                     [128]                     --
│    └─BatchNorm2d: 2-1426               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1427                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1428                      [16, 128, 16, 16]         --
│    └─Empty: 2-1429                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1430                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-143        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1431                 [16, 128, 16, 16]         --
│    └─Empty: 2-1432                     [16, 128, 16, 16]         --
│    └─Empty: 2-1433                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1434        --                        --
│    └─One: 2-1435                       [1]                       --
│    └─OutputScale: 2-1436               --                        --
│    └─Empty: 2-1437                     [128, 128, 3, 3]          --
│    └─Empty: 2-1438                     [128, 128, 3, 3]          --
│    └─Empty: 2-1439                     [128]                     --
│    └─Empty: 2-1440                     [128]                     --
│    └─BatchNorm2d: 2-1441               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1442                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1443                      [16, 128, 16, 16]         --
│    └─Empty: 2-1444                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1445                     [16, 128, 16, 16]         --
├─Dropout2d: 1-144                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-145        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1446                 [16, 128, 8, 8]           --
│    └─Empty: 2-1447                     [16, 128, 8, 8]           --
│    └─Empty: 2-1448                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1449        --                        --
│    └─One: 2-1450                       [1]                       --
│    └─OutputScale: 2-1451               --                        --
│    └─Empty: 2-1452                     [128, 128, 3, 3]          --
│    └─Empty: 2-1453                     [128, 128, 3, 3]          --
│    └─Empty: 2-1454                     [128]                     --
│    └─Empty: 2-1455                     [128]                     --
│    └─BatchNorm2d: 2-1456               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1457                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1458                      [16, 128, 8, 8]           --
│    └─Empty: 2-1459                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1460                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-146               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1461        --                        --
│    └─One: 2-1462                       [1]                       --
│    └─OutputScale: 2-1463               --                        --
│    └─Empty: 2-1464                     [16, 128, 1, 1]           --
│    └─Empty: 2-1465                     [16, 128, 1, 1]           --
│    └─Empty: 2-1466                     [16]                      --
│    └─Empty: 2-1467                     [16]                      --
│    └─BatchNorm2d: 2-1468               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1469                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1470                      [16, 16, 8, 8]            --
│    └─Empty: 2-1471                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1472                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-147        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1473                 [16, 128, 8, 8]           --
│    └─Empty: 2-1474                     [16, 128, 8, 8]           --
│    └─Empty: 2-1475                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1476        --                        --
│    └─One: 2-1477                       [1]                       --
│    └─OutputScale: 2-1478               --                        --
│    └─Empty: 2-1479                     [16, 128, 3, 3]           --
│    └─Empty: 2-1480                     [16, 128, 3, 3]           --
│    └─Empty: 2-1481                     [16]                      --
│    └─Empty: 2-1482                     [16]                      --
│    └─BatchNorm2d: 2-1483               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1484                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1485                      [16, 16, 8, 8]            --
│    └─Empty: 2-1486                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1487                     [16, 16, 8, 8]            --
├─Dropout2d: 1-148                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-149               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1488        --                        --
│    └─One: 2-1489                       [1]                       --
│    └─OutputScale: 2-1490               --                        --
│    └─Empty: 2-1491                     [128, 48, 1, 1]           --
│    └─Empty: 2-1492                     [128, 48, 1, 1]           --
│    └─Empty: 2-1493                     [128]                     --
│    └─Empty: 2-1494                     [128]                     --
│    └─BatchNorm2d: 2-1495               [16, 128, 64, 64]         --
│    └─Scaler: 2-1496                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1497                      [16, 128, 64, 64]         --
│    └─Empty: 2-1498                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1499                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-150        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1500                 [16, 128, 32, 32]         --
│    └─Empty: 2-1501                     [16, 128, 32, 32]         --
│    └─Empty: 2-1502                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1503        --                        --
│    └─One: 2-1504                       [1]                       --
│    └─OutputScale: 2-1505               --                        --
│    └─Empty: 2-1506                     [128, 128, 3, 3]          --
│    └─Empty: 2-1507                     [128, 128, 3, 3]          --
│    └─Empty: 2-1508                     [128]                     --
│    └─Empty: 2-1509                     [128]                     --
│    └─BatchNorm2d: 2-1510               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1511                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1512                      [16, 128, 32, 32]         --
│    └─Empty: 2-1513                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1514                     [16, 128, 32, 32]         --
├─Dropout2d: 1-151                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-152        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1515                 [16, 128, 16, 16]         --
│    └─Empty: 2-1516                     [16, 128, 16, 16]         --
│    └─Empty: 2-1517                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1518        --                        --
│    └─One: 2-1519                       [1]                       --
│    └─OutputScale: 2-1520               --                        --
│    └─Empty: 2-1521                     [128, 128, 3, 3]          --
│    └─Empty: 2-1522                     [128, 128, 3, 3]          --
│    └─Empty: 2-1523                     [128]                     --
│    └─Empty: 2-1524                     [128]                     --
│    └─BatchNorm2d: 2-1525               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1526                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1527                      [16, 128, 16, 16]         --
│    └─Empty: 2-1528                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1529                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-153               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1530        --                        --
│    └─One: 2-1531                       [1]                       --
│    └─OutputScale: 2-1532               --                        --
│    └─Empty: 2-1533                     [128, 128, 1, 1]          --
│    └─Empty: 2-1534                     [128, 128, 1, 1]          --
│    └─Empty: 2-1535                     [128]                     --
│    └─Empty: 2-1536                     [128]                     --
│    └─BatchNorm2d: 2-1537               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1538                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1539                      [16, 128, 16, 16]         --
│    └─Empty: 2-1540                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1541                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-154        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1542                 [16, 128, 16, 16]         --
│    └─Empty: 2-1543                     [16, 128, 16, 16]         --
│    └─Empty: 2-1544                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1545        --                        --
│    └─One: 2-1546                       [1]                       --
│    └─OutputScale: 2-1547               --                        --
│    └─Empty: 2-1548                     [128, 128, 3, 3]          --
│    └─Empty: 2-1549                     [128, 128, 3, 3]          --
│    └─Empty: 2-1550                     [128]                     --
│    └─Empty: 2-1551                     [128]                     --
│    └─BatchNorm2d: 2-1552               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1553                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1554                      [16, 128, 16, 16]         --
│    └─Empty: 2-1555                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1556                     [16, 128, 16, 16]         --
├─Dropout2d: 1-155                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-156        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1557                 [16, 128, 8, 8]           --
│    └─Empty: 2-1558                     [16, 128, 8, 8]           --
│    └─Empty: 2-1559                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1560        --                        --
│    └─One: 2-1561                       [1]                       --
│    └─OutputScale: 2-1562               --                        --
│    └─Empty: 2-1563                     [128, 128, 3, 3]          --
│    └─Empty: 2-1564                     [128, 128, 3, 3]          --
│    └─Empty: 2-1565                     [128]                     --
│    └─Empty: 2-1566                     [128]                     --
│    └─BatchNorm2d: 2-1567               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1568                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1569                      [16, 128, 8, 8]           --
│    └─Empty: 2-1570                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1571                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-157               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1572        --                        --
│    └─One: 2-1573                       [1]                       --
│    └─OutputScale: 2-1574               --                        --
│    └─Empty: 2-1575                     [16, 128, 1, 1]           --
│    └─Empty: 2-1576                     [16, 128, 1, 1]           --
│    └─Empty: 2-1577                     [16]                      --
│    └─Empty: 2-1578                     [16]                      --
│    └─BatchNorm2d: 2-1579               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1580                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1581                      [16, 16, 8, 8]            --
│    └─Empty: 2-1582                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1583                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-158        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1584                 [16, 128, 8, 8]           --
│    └─Empty: 2-1585                     [16, 128, 8, 8]           --
│    └─Empty: 2-1586                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1587        --                        --
│    └─One: 2-1588                       [1]                       --
│    └─OutputScale: 2-1589               --                        --
│    └─Empty: 2-1590                     [16, 128, 3, 3]           --
│    └─Empty: 2-1591                     [16, 128, 3, 3]           --
│    └─Empty: 2-1592                     [16]                      --
│    └─Empty: 2-1593                     [16]                      --
│    └─BatchNorm2d: 2-1594               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1595                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1596                      [16, 16, 8, 8]            --
│    └─Empty: 2-1597                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1598                     [16, 16, 8, 8]            --
├─Dropout2d: 1-159                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-160               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1599        --                        --
│    └─One: 2-1600                       [1]                       --
│    └─OutputScale: 2-1601               --                        --
│    └─Empty: 2-1602                     [128, 48, 1, 1]           --
│    └─Empty: 2-1603                     [128, 48, 1, 1]           --
│    └─Empty: 2-1604                     [128]                     --
│    └─Empty: 2-1605                     [128]                     --
│    └─BatchNorm2d: 2-1606               [16, 128, 64, 64]         --
│    └─Scaler: 2-1607                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1608                      [16, 128, 64, 64]         --
│    └─Empty: 2-1609                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1610                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-161        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1611                 [16, 128, 32, 32]         --
│    └─Empty: 2-1612                     [16, 128, 32, 32]         --
│    └─Empty: 2-1613                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1614        --                        --
│    └─One: 2-1615                       [1]                       --
│    └─OutputScale: 2-1616               --                        --
│    └─Empty: 2-1617                     [128, 128, 3, 3]          --
│    └─Empty: 2-1618                     [128, 128, 3, 3]          --
│    └─Empty: 2-1619                     [128]                     --
│    └─Empty: 2-1620                     [128]                     --
│    └─BatchNorm2d: 2-1621               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1622                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1623                      [16, 128, 32, 32]         --
│    └─Empty: 2-1624                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1625                     [16, 128, 32, 32]         --
├─Dropout2d: 1-162                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-163        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1626                 [16, 128, 16, 16]         --
│    └─Empty: 2-1627                     [16, 128, 16, 16]         --
│    └─Empty: 2-1628                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1629        --                        --
│    └─One: 2-1630                       [1]                       --
│    └─OutputScale: 2-1631               --                        --
│    └─Empty: 2-1632                     [128, 128, 3, 3]          --
│    └─Empty: 2-1633                     [128, 128, 3, 3]          --
│    └─Empty: 2-1634                     [128]                     --
│    └─Empty: 2-1635                     [128]                     --
│    └─BatchNorm2d: 2-1636               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1637                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1638                      [16, 128, 16, 16]         --
│    └─Empty: 2-1639                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1640                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-164               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1641        --                        --
│    └─One: 2-1642                       [1]                       --
│    └─OutputScale: 2-1643               --                        --
│    └─Empty: 2-1644                     [128, 128, 1, 1]          --
│    └─Empty: 2-1645                     [128, 128, 1, 1]          --
│    └─Empty: 2-1646                     [128]                     --
│    └─Empty: 2-1647                     [128]                     --
│    └─BatchNorm2d: 2-1648               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1649                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1650                      [16, 128, 16, 16]         --
│    └─Empty: 2-1651                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1652                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-165        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1653                 [16, 128, 16, 16]         --
│    └─Empty: 2-1654                     [16, 128, 16, 16]         --
│    └─Empty: 2-1655                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1656        --                        --
│    └─One: 2-1657                       [1]                       --
│    └─OutputScale: 2-1658               --                        --
│    └─Empty: 2-1659                     [128, 128, 3, 3]          --
│    └─Empty: 2-1660                     [128, 128, 3, 3]          --
│    └─Empty: 2-1661                     [128]                     --
│    └─Empty: 2-1662                     [128]                     --
│    └─BatchNorm2d: 2-1663               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1664                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1665                      [16, 128, 16, 16]         --
│    └─Empty: 2-1666                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1667                     [16, 128, 16, 16]         --
├─Dropout2d: 1-166                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-167        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1668                 [16, 128, 8, 8]           --
│    └─Empty: 2-1669                     [16, 128, 8, 8]           --
│    └─Empty: 2-1670                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1671        --                        --
│    └─One: 2-1672                       [1]                       --
│    └─OutputScale: 2-1673               --                        --
│    └─Empty: 2-1674                     [128, 128, 3, 3]          --
│    └─Empty: 2-1675                     [128, 128, 3, 3]          --
│    └─Empty: 2-1676                     [128]                     --
│    └─Empty: 2-1677                     [128]                     --
│    └─BatchNorm2d: 2-1678               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1679                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1680                      [16, 128, 8, 8]           --
│    └─Empty: 2-1681                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1682                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-168               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1683        --                        --
│    └─One: 2-1684                       [1]                       --
│    └─OutputScale: 2-1685               --                        --
│    └─Empty: 2-1686                     [16, 128, 1, 1]           --
│    └─Empty: 2-1687                     [16, 128, 1, 1]           --
│    └─Empty: 2-1688                     [16]                      --
│    └─Empty: 2-1689                     [16]                      --
│    └─BatchNorm2d: 2-1690               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1691                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1692                      [16, 16, 8, 8]            --
│    └─Empty: 2-1693                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1694                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-169        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1695                 [16, 128, 8, 8]           --
│    └─Empty: 2-1696                     [16, 128, 8, 8]           --
│    └─Empty: 2-1697                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1698        --                        --
│    └─One: 2-1699                       [1]                       --
│    └─OutputScale: 2-1700               --                        --
│    └─Empty: 2-1701                     [16, 128, 3, 3]           --
│    └─Empty: 2-1702                     [16, 128, 3, 3]           --
│    └─Empty: 2-1703                     [16]                      --
│    └─Empty: 2-1704                     [16]                      --
│    └─BatchNorm2d: 2-1705               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1706                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1707                      [16, 16, 8, 8]            --
│    └─Empty: 2-1708                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1709                     [16, 16, 8, 8]            --
├─Dropout2d: 1-170                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-171               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1710        --                        --
│    └─One: 2-1711                       [1]                       --
│    └─OutputScale: 2-1712               --                        --
│    └─Empty: 2-1713                     [128, 48, 1, 1]           --
│    └─Empty: 2-1714                     [128, 48, 1, 1]           --
│    └─Empty: 2-1715                     [128]                     --
│    └─Empty: 2-1716                     [128]                     --
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Empty: 2-1720                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1721                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Empty: 2-1723                     [16, 128, 32, 32]         --
│    └─Empty: 2-1724                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1725        --                        --
│    └─One: 2-1726                       [1]                       --
│    └─OutputScale: 2-1727               --                        --
│    └─Empty: 2-1728                     [128, 128, 3, 3]          --
│    └─Empty: 2-1729                     [128, 128, 3, 3]          --
│    └─Empty: 2-1730                     [128]                     --
│    └─Empty: 2-1731                     [128]                     --
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         (recursive)
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─Empty: 2-1735                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1736                     [16, 128, 32, 32]         --
├─Dropout2d: 1-173                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1740        --                        --
│    └─One: 2-1741                       [1]                       --
│    └─OutputScale: 2-1742               --                        --
│    └─Empty: 2-1743                     [128, 128, 3, 3]          --
│    └─Empty: 2-1744                     [128, 128, 3, 3]          --
│    └─Empty: 2-1745                     [128]                     --
│    └─Empty: 2-1746                     [128]                     --
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─Empty: 2-1750                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1751                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-175               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1752        --                        --
│    └─One: 2-1753                       [1]                       --
│    └─OutputScale: 2-1754               --                        --
│    └─Empty: 2-1755                     [128, 128, 1, 1]          --
│    └─Empty: 2-1756                     [128, 128, 1, 1]          --
│    └─Empty: 2-1757                     [128]                     --
│    └─Empty: 2-1758                     [128]                     --
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Empty: 2-1762                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1763                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Empty: 2-1765                     [16, 128, 16, 16]         --
│    └─Empty: 2-1766                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1767        --                        --
│    └─One: 2-1768                       [1]                       --
│    └─OutputScale: 2-1769               --                        --
│    └─Empty: 2-1770                     [128, 128, 3, 3]          --
│    └─Empty: 2-1771                     [128, 128, 3, 3]          --
│    └─Empty: 2-1772                     [128]                     --
│    └─Empty: 2-1773                     [128]                     --
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         (recursive)
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─Empty: 2-1777                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1778                     [16, 128, 16, 16]         --
├─Dropout2d: 1-177                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1782        --                        --
│    └─One: 2-1783                       [1]                       --
│    └─OutputScale: 2-1784               --                        --
│    └─Empty: 2-1785                     [128, 128, 3, 3]          --
│    └─Empty: 2-1786                     [128, 128, 3, 3]          --
│    └─Empty: 2-1787                     [128]                     --
│    └─Empty: 2-1788                     [128]                     --
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           (recursive)
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─Empty: 2-1792                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1793                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-179               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1794        --                        --
│    └─One: 2-1795                       [1]                       --
│    └─OutputScale: 2-1796               --                        --
│    └─Empty: 2-1797                     [16, 128, 1, 1]           --
│    └─Empty: 2-1798                     [16, 128, 1, 1]           --
│    └─Empty: 2-1799                     [16]                      --
│    └─Empty: 2-1800                     [16]                      --
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Empty: 2-1804                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1805                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1809        --                        --
│    └─One: 2-1810                       [1]                       --
│    └─OutputScale: 2-1811               --                        --
│    └─Empty: 2-1812                     [16, 128, 3, 3]           --
│    └─Empty: 2-1813                     [16, 128, 3, 3]           --
│    └─Empty: 2-1814                     [16]                      --
│    └─Empty: 2-1815                     [16]                      --
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            (recursive)
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─Empty: 2-1819                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1820                     [16, 16, 8, 8]            --
├─Dropout2d: 1-181                       [16, 16, 8, 8]            --
├─Linear: 1-182                          [16, 5]                   5,126
│    └─OutputShiftSqueeze: 2-1821        --                        --
│    └─One: 2-1822                       [1]                       --
│    └─OutputScale: 2-1823               --                        --
│    └─Empty: 2-1824                     [5, 1024]                 --
│    └─Empty: 2-1825                     [5, 1024]                 --
│    └─Empty: 2-1826                     [16, 5]                   --
│    └─Empty: 2-1827                     [16, 5]                   --
│    └─Clamp: 2-1828                     [16, 5]                   --
├─Linear: 1-183                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1829        --                        --
│    └─One: 2-1830                       [1]                       --
│    └─OutputScale: 2-1831               --                        --
│    └─Empty: 2-1832                     [5, 1024]                 --
│    └─Empty: 2-1833                     [5, 1024]                 --
│    └─Empty: 2-1834                     [16, 5]                   --
│    └─Empty: 2-1835                     [16, 5]                   --
│    └─Clamp: 2-1836                     [16, 5]                   --
├─Linear: 1-184                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1837        --                        --
│    └─One: 2-1838                       [1]                       --
│    └─OutputScale: 2-1839               --                        --
│    └─Empty: 2-1840                     [5, 1024]                 --
│    └─Empty: 2-1841                     [5, 1024]                 --
│    └─Empty: 2-1842                     [16, 5]                   --
│    └─Empty: 2-1843                     [16, 5]                   --
│    └─Clamp: 2-1844                     [16, 5]                   --
├─Linear: 1-185                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1845        --                        --
│    └─One: 2-1846                       [1]                       --
│    └─OutputScale: 2-1847               --                        --
│    └─Empty: 2-1848                     [5, 1024]                 --
│    └─Empty: 2-1849                     [5, 1024]                 --
│    └─Empty: 2-1850                     [16, 5]                   --
│    └─Empty: 2-1851                     [16, 5]                   --
│    └─Clamp: 2-1852                     [16, 5]                   --
├─Linear: 1-186                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1853        --                        --
│    └─One: 2-1854                       [1]                       --
│    └─OutputScale: 2-1855               --                        --
│    └─Empty: 2-1856                     [5, 1024]                 --
│    └─Empty: 2-1857                     [5, 1024]                 --
│    └─Empty: 2-1858                     [16, 5]                   --
│    └─Empty: 2-1859                     [16, 5]                   --
│    └─Clamp: 2-1860                     [16, 5]                   --
├─Linear: 1-187                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1861        --                        --
│    └─One: 2-1862                       [1]                       --
│    └─OutputScale: 2-1863               --                        --
│    └─Empty: 2-1864                     [5, 1024]                 --
│    └─Empty: 2-1865                     [5, 1024]                 --
│    └─Empty: 2-1866                     [16, 5]                   --
│    └─Empty: 2-1867                     [16, 5]                   --
│    └─Clamp: 2-1868                     [16, 5]                   --
├─Linear: 1-188                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1869        --                        --
│    └─One: 2-1870                       [1]                       --
│    └─OutputScale: 2-1871               --                        --
│    └─Empty: 2-1872                     [5, 1024]                 --
│    └─Empty: 2-1873                     [5, 1024]                 --
│    └─Empty: 2-1874                     [16, 5]                   --
│    └─Empty: 2-1875                     [16, 5]                   --
│    └─Clamp: 2-1876                     [16, 5]                   --
├─Linear: 1-189                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1877        --                        --
│    └─One: 2-1878                       [1]                       --
│    └─OutputScale: 2-1879               --                        --
│    └─Empty: 2-1880                     [5, 1024]                 --
│    └─Empty: 2-1881                     [5, 1024]                 --
│    └─Empty: 2-1882                     [16, 5]                   --
│    └─Empty: 2-1883                     [16, 5]                   --
│    └─Clamp: 2-1884                     [16, 5]                   --
├─Linear: 1-190                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1885        --                        --
│    └─One: 2-1886                       [1]                       --
│    └─OutputScale: 2-1887               --                        --
│    └─Empty: 2-1888                     [5, 1024]                 --
│    └─Empty: 2-1889                     [5, 1024]                 --
│    └─Empty: 2-1890                     [16, 5]                   --
│    └─Empty: 2-1891                     [16, 5]                   --
│    └─Clamp: 2-1892                     [16, 5]                   --
├─Linear: 1-191                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1893        --                        --
│    └─One: 2-1894                       [1]                       --
│    └─OutputScale: 2-1895               --                        --
│    └─Empty: 2-1896                     [5, 1024]                 --
│    └─Empty: 2-1897                     [5, 1024]                 --
│    └─Empty: 2-1898                     [16, 5]                   --
│    └─Empty: 2-1899                     [16, 5]                   --
│    └─Clamp: 2-1900                     [16, 5]                   --
├─Linear: 1-192                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1901        --                        --
│    └─One: 2-1902                       [1]                       --
│    └─OutputScale: 2-1903               --                        --
│    └─Empty: 2-1904                     [5, 1024]                 --
│    └─Empty: 2-1905                     [5, 1024]                 --
│    └─Empty: 2-1906                     [16, 5]                   --
│    └─Empty: 2-1907                     [16, 5]                   --
│    └─Clamp: 2-1908                     [16, 5]                   --
├─Linear: 1-193                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1909        --                        --
│    └─One: 2-1910                       [1]                       --
│    └─OutputScale: 2-1911               --                        --
│    └─Empty: 2-1912                     [5, 1024]                 --
│    └─Empty: 2-1913                     [5, 1024]                 --
│    └─Empty: 2-1914                     [16, 5]                   --
│    └─Empty: 2-1915                     [16, 5]                   --
│    └─Clamp: 2-1916                     [16, 5]                   --
├─Linear: 1-194                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1917        --                        --
│    └─One: 2-1918                       [1]                       --
│    └─OutputScale: 2-1919               --                        --
│    └─Empty: 2-1920                     [5, 1024]                 --
│    └─Empty: 2-1921                     [5, 1024]                 --
│    └─Empty: 2-1922                     [16, 5]                   --
│    └─Empty: 2-1923                     [16, 5]                   --
│    └─Clamp: 2-1924                     [16, 5]                   --
├─Linear: 1-195                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1925        --                        --
│    └─One: 2-1926                       [1]                       --
│    └─OutputScale: 2-1927               --                        --
│    └─Empty: 2-1928                     [5, 1024]                 --
│    └─Empty: 2-1929                     [5, 1024]                 --
│    └─Empty: 2-1930                     [16, 5]                   --
│    └─Empty: 2-1931                     [16, 5]                   --
│    └─Clamp: 2-1932                     [16, 5]                   --
├─Linear: 1-196                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1933        --                        --
│    └─One: 2-1934                       [1]                       --
│    └─OutputScale: 2-1935               --                        --
│    └─Empty: 2-1936                     [5, 1024]                 --
│    └─Empty: 2-1937                     [5, 1024]                 --
│    └─Empty: 2-1938                     [16, 5]                   --
│    └─Empty: 2-1939                     [16, 5]                   --
│    └─Clamp: 2-1940                     [16, 5]                   --
├─Linear: 1-197                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1941        --                        --
│    └─One: 2-1942                       [1]                       --
│    └─OutputScale: 2-1943               --                        --
│    └─Empty: 2-1944                     [5, 1024]                 --
│    └─Empty: 2-1945                     [5, 1024]                 --
│    └─Empty: 2-1946                     [16, 5]                   --
│    └─Empty: 2-1947                     [16, 5]                   --
│    └─Clamp: 2-1948                     [16, 5]                   --
==========================================================================================
Total params: 640,150
Trainable params: 640,096
Non-trainable params: 54
Total mult-adds (M): 0.37
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 30.67
Params size (MB): 2.54
Estimated Total Size (MB): 234.54
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.483 | Acc: 25.500% | Wgt Acc: 30.992%
	I - Batch: 100 | Loss: 1.405 | Acc: 30.438% | Wgt Acc: 36.534%
	I - Batch: 150 | Loss: 1.369 | Acc: 32.333% | Wgt Acc: 39.063%
	I - Batch: 200 | Loss: 1.338 | Acc: 34.125% | Wgt Acc: 41.185%
	I - Batch: 250 | Loss: 1.313 | Acc: 35.225% | Wgt Acc: 42.595%
I - num batch: 273
I - Train -- Loss: 1.311 | Acc: 35.351% | Wgt Acc: 42.831% | LR: 1.000000e-03 | Dur: 165.27s
I - Confusion Matrix: [row->prediction - col->label]
[[395.  52.  89. 210. 135.]
 [ 48. 165. 162.  67. 146.]
 [159. 441. 673. 187. 593.]
 [195. 101. 107. 309. 126.]
 [  0.   1.   1.   0.   0.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.628 | Acc: 31.755% | Wgt Acc: 43.974% | Dur: 16.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   3.   4.  25.  28.]
 [  2.  18.   5.   1.  11.]
 [ 10.  41.  51.  14. 115.]
 [ 30.  16.  15.  46.  26.]
 [  0.   0.   0.   0.   0.]]

I - Local maximum validation set accuracy:  31.76

I - Validation set results: 
[14-1-2-0.65][50-3-1-0.11][124-2-2-0.61][127-0-0-1.41][443-2-2-1.34][567-0-0-1.61][573-1-1-0.31][615-0-3-1.02][695-1-2-0.59][722-3-3-1.31]
[826-0-0-1.61][878-0-0-1.33][1103-0-0-0.43][1212-3-3-0.61][1368-0-0-0.81][2181-2-3-0.48][2476-2-2-0.33][2721-2-2-0.88][2818-1-3-0.27][2886-2-2-0.87]
[3231-2-2-1.72][3333-2-2-0.71][3482-2-2-0.80][3536-3-3-0.90][3625-1-1-0.68][3909-0-3-0.20][4035-0-3-1.54][4140-0-0-0.92][4214-1-3-0.26][4346-1-0-0.93]
[4581-2-2-1.74][4708-3-3-0.34][4838-3-2-0.64][4845-1-3-0.35][4868-0-0-0.86][4939-0-2-0.44][4984-2-0-0.60][5078-1-2-1.03][5396-0-0-3.26][5479-1-2-0.41]
[5717-0-0-0.46][5843-1-1-0.59][5949-3-0-0.86][5987-2-2-0.71][6014-3-3-0.46][6033-3-0-1.83][6313-0-0-0.98][6421-3-3-1.46][6500-1-1-0.40][6583-3-3-0.85]
[6683-3-3-0.43][6825-2-3-0.72][6998-3-3-0.63][7049-3-3-0.62][7517-1-1-0.86][7521-1-3-0.32][7528-1-2-0.63][7949-1-2-1.26][8135-1-3-0.69][8185-3-0-1.87]
[8269-3-2-0.29][8273-3-3-0.26][8543-3-0-1.65][8666-1-1-0.37][8672-0-0-2.20][8903-1-3-0.35][9001-2-1-0.79][9036-2-2-1.37][9281-3-2-0.80][9300-2-2-0.75]
[9571-0-3-0.53][9617-1-1-0.26][9644-2-2-0.70][9705-2-2-0.41][9801-0-3-1.00][9803-3-3-0.50][9865-3-0-2.31][9896-2-2-0.90][10314-1-2-0.95][10337-3-0-0.63]
[10403-0-2-0.32][10653-2-2-0.54][10704-2-3-0.60][10719-1-2-0.91][10727-1-2-0.73][10836-0-0-2.20][10969-2-3-0.74][11042-0-0-0.66][11088-1-2-1.55][11322-0-0-0.65]
[11398-2-2-1.34][11499-0-3-0.21][11502-3-3-1.03][11512-3-2-0.57][11608-1-1-1.10][11610-0-3-0.53][11692-0-0-0.69][11905-0-0-2.67][11993-1-2-0.90][12002-2-3-0.33]
[12052-0-0-0.86][12201-0-0-1.73][12235-2-2-1.20][12320-1-0-0.73][12377-2-2-0.40][12398-2-3-0.18][12503-1-2-1.18][12617-0-1-1.29][12685-3-3-0.48][12738-2-3-0.68]
[12742-2-2-1.22][12823-0-3-1.32][13110-1-2-1.26][13240-3-3-0.70][13253-1-2-1.02][13273-0-0-1.90][13634-1-2-0.58][13763-2-3-0.46][13905-3-0-1.22][14060-2-1-0.98]
[14065-3-0-0.67][14147-3-3-0.93][14595-2-2-0.85][14687-2-2-0.65][14788-2-2-0.61][14869-1-2-0.47][14872-3-0-0.46][14877-1-2-0.31][14927-0-3-1.47][15066-0-0-2.07]
[15175-1-2-0.45][15178-2-0-0.69][15375-3-0-0.50][15389-3-3-1.43][15568-2-1-0.73][15675-3-3-0.69][15869-1-2-0.36][16207-3-0-1.02][16236-0-3-0.80][16302-3-0-1.04]
[16331-2-2-1.52][16381-0-0-1.04][16488-1-2-0.65][16495-0-0-0.75][16650-0-0-2.42][16719-1-2-0.73][16801-0-0-1.97][16828-0-3-0.38][17137-3-0-1.09][17245-1-3-0.08]
[17278-3-2-0.26][17282-0-3-0.22][17311-2-2-1.09][17336-2-2-0.87][17608-3-3-1.75][17627-0-3-0.66][17877-3-2-0.20][17924-1-3-0.46][17984-3-0-2.40][18211-0-3-0.40]
[18276-3-0-1.87][18287-1-2-0.42][18394-0-0-1.48][18428-0-1-0.27][18442-0-3-1.08][18478-3-0-2.08][18607-0-3--0.01][18616-0-3-0.09][18663-0-0-1.12][18718-0-0-1.25]
[18766-2-2-0.66][18824-2-2-1.32][18890-3-2-0.31][18930-3-2-0.49][18938-3-3-0.67][19817-1-2-0.94][19839-0-2-0.42][19930-3-0-1.21][19944-0-2-0.41][20036-2-2-0.94]
[20101-3-2-0.04][20474-1-2-0.81][20547-3-3-0.12][20929-2-2-1.85][21245-1-2-0.46][21257-3-3-0.55][21293-1-1-0.74][21316-1-1-0.65][21384-1-2-0.78][21448-1-2-0.95]
[21483-0-0-1.49][21487-2-2-0.60][21714-0-0-0.55][21943-3-2-0.46][21947-0-0-0.57][21948-0-0-2.40][21965-2-2-0.62][21998-1-3-0.34][22025-0-2-0.56][22228-3-3-1.22]
[22446-1-1-0.70][22494-3-0-1.58][22757-0-3-0.94][22811-3-3-1.75][22976-3-2-0.96][22985-3-0-1.42][23014-0-0-1.54][23112-1-2-0.89][23144-3-0-1.97][23168-2-3-0.39]
[23219-0-3-0.55][23363-3-3-0.66][23470-0-2-0.39][23486-2-3-0.46][23497-0-3-1.54][23516-0-0-1.81][23690-1-2-1.58][23921-2-2-0.73][23936-1-3-0.56][24040-3-3-0.39]
[24111-1-2-0.93][24182-0-3-1.87][24238-3-3-1.38][24290-2-0-0.56][24345-0-0-1.38][24364-1-3-0.18][24427-3-3-0.51][24477-2-2-0.61][24495-2-1-0.39][24893-2-2-0.55]
[25012-1-2-0.48][25121-2-2-1.07][25165-3-3-0.57][25183-0-0-0.26][25297-3-3-0.67][25398-0-0-1.31][25574-2-2-0.51][25644-1-2-0.71][25718-1-1-0.27][25774-2-3-0.47]
[26032-3-3-1.09][26051-3-3-1.36][26120-0-0-0.76][26321-1-3-0.42][26732-1-1-0.38][26784-3-3-1.93][26827-3-3-0.66][26833-0-3-1.71][26838-2-2-0.38][26860-1-3-0.41]
[26948-0-2-0.14][27049-3-0-1.63][27098-1-3-0.54][27526-0-0-2.07][27639-3-3-0.31][27698-3-3-1.45][27772-0-0-1.55][27890-1-1-0.36][28040-0-2-0.23][28503-2-2-1.70]
[28577-1-2-0.45][28959-0-0-2.89][29198-3-2-0.49][29777-0-0-2.51][29877-2-2-0.64][30035-1-2-0.82][30098-0-3-1.06][30326-1-1-0.85][30572-2-2-0.60][30716-0-2-0.21]
[30806-2-3-0.32][30906-1-2-0.69][31007-0-0-0.51][31181-3-3-1.20][31238-0-3-0.81][31347-0-3-1.45][31422-2-2-0.45][31429-3-0-0.76][31431-0-3-1.47][31432-1-1-0.19]
[31477-0-0-2.06][31524-1-2-0.80][31597-1-2-0.96][31619-1-0-0.42][31701-0-0-1.61][31755-0-0-1.45][31854-3-3-0.41][32074-1-3-0.34][32078-3-3-1.07][32111-1-2-0.65]
[32127-1-2-1.94][32140-3-3-1.21][32263-2-2-0.15][32365-0-3-0.39][32411-2-0-1.40][32429-3-3-1.50][32473-3-0-0.92][32574-3-3-1.12][32584-0-3-0.28][32622-0-2-0.50]
[32858-3-0-0.79][32969-3-0-1.63][33016-2-2-1.96][33031-1-3-0.76][33035-2-2-1.00][33133-2-2-0.50][33173-2-2-0.67][33175-3-2-1.17][33306-3-2-0.50][33309-2-3-0.36]
[33474-0-3-0.22][33478-2-3-0.38][33618-1-1-0.36][33712-0-3-0.51][33782-2-2-1.25][33914-3-3-0.51][34076-3-3-1.58][34112-2-1-0.68][34138-2-3-0.61][34239-1-2-0.58]
[34364-2-2-1.16][34617-1-2-1.12][34751-3-3-1.86][34783-2-2-0.81][35015-3-3-0.45][35018-1-1-0.92][35288-2-2-0.12][0-4-2-1.10][1-4-0-0.49][2-4-2-0.47]
[3-4-2-0.99][4-4-2-0.54][5-4-1-1.06][6-4-2-0.35][7-4-2-0.33][8-4-2-0.38][9-4-2-0.55][10-4-0--0.01][11-4-2-0.72][12-4-3-0.53]
[14-4-3-0.50][15-4-0-1.45][16-4-3-0.10][17-4-0-0.20][18-4-2-0.70][19-4-3-1.21][20-4-2-0.29][21-4-2-1.23][22-4-2-1.01][23-4-3-0.47]
[24-4-2-0.41][25-4-3-0.51][26-4-3-0.47][27-4-0-0.67][28-4-2-0.29][29-4-2-0.58][30-4-0-0.59][31-4-2-0.59][32-4-2-1.00][33-4-2-0.51]
[34-4-0-0.57][35-4-3-1.08][37-4-2-0.29][39-4-0-1.05][40-4-0-0.50][41-4-2-0.99][42-4-2-0.78][43-4-2-0.85][45-4-3-0.25][46-4-2-1.74]
[47-4-2-1.14][48-4-2-0.91][51-4-2-1.19][52-4-2-0.51][53-4-2-0.84][54-4-3-0.04][55-4-3-0.36][56-4-2-0.89][57-4-3-0.78][58-4-2-0.76]
[59-4-2-0.18][60-4-2-0.25][61-4-2-0.60][62-4-3-0.61][63-4-2-0.57][64-4-2-0.52][65-4-2-0.62][66-4-2-1.13][67-4-2-0.54][68-4-3-0.32]
[69-4-0-1.19][70-4-2-0.66][72-4-2-0.92][73-4-1-0.74][74-4-2-0.92][75-4-2-0.20][77-4-2-0.77][78-4-0-0.25][79-4-2-0.64][80-4-1-0.72]
[81-4-2-0.95][82-4-2-0.85][83-4-1-0.23][84-4-0-0.38][85-4-2-0.72][86-4-2-0.56][87-4-0-0.44][88-4-2-0.56][89-4-2-0.79][90-4-3-0.10]
[91-4-2-0.49][92-4-2-0.33][93-4-0-0.09][94-4-2-1.11][95-4-2-0.17][96-4-1-0.49][97-4-2-0.71][98-4-2-0.96][99-4-2-0.42][100-4-2-0.94]
[101-4-2-0.65][102-4-2-0.97][103-4-3-0.89][104-4-2-1.42][105-4-2-0.98][106-4-2-0.80][107-4-0-0.62][108-4-2-0.39][109-4-1-0.47][110-4-2-0.98]
[111-4-0-0.82][112-4-2-0.96][113-4-3-0.52][114-4-0-0.77][115-4-0-0.01][116-4-2-0.44][117-4-1-0.69][119-4-2-1.01][121-4-1-1.08][122-4-0-0.54]
[124-4-2-0.66][125-4-2-0.88][126-4-3-0.33][127-4-2-1.12][128-4-2-0.45][129-4-1-0.37][130-4-2-1.49][131-4-3-0.49][132-4-3-1.08][133-4-0-1.85]
[135-4-2-1.04][136-4-2-0.11][137-4-2-0.05][138-4-3-0.83][139-4-0-0.63][140-4-2-0.30][141-4-3-0.85][142-4-2-1.37][143-4-2-0.92][144-4-2-0.67]
[145-4-2-0.95][148-4-0-1.50][149-4-3-0.35][150-4-2-0.79][151-4-2-0.83][152-4-2-1.05][153-4-2-0.79][154-4-2-0.40][155-4-1-0.43][156-4-2-0.29]
[157-4-0-0.67][158-4-2-0.14][160-4-2-0.65][161-4-2-0.92][162-4-2-0.12][164-4-2-0.78][165-4-2-0.56][167-4-0-0.43][168-4-2-0.22][170-4-3-1.55]
[171-4-1-0.38][172-4-2-0.52][173-4-0-0.30][174-4-0-1.07][175-4-2-0.48][177-4-0-1.05][178-4-2-1.09][179-4-2-0.47][180-4-2-0.20][181-4-2-0.42]
[182-4-3-0.92][183-4-2-0.67][184-4-2-0.44][186-4-2-0.20][187-4-2-0.41][188-4-2-0.41][189-4-2-0.65][190-4-2-0.46][191-4-2-1.32][192-4-2-0.58]
[193-4-2-0.95][194-4-3-0.68][195-4-0-0.60][196-4-2-0.55][197-4-2-1.63][198-4-2-0.65][199-4-2-0.63]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.224 | Acc: 40.750% | Wgt Acc: 50.013%
	I - Batch: 100 | Loss: 1.213 | Acc: 41.062% | Wgt Acc: 49.563%
	I - Batch: 150 | Loss: 1.197 | Acc: 41.292% | Wgt Acc: 50.184%
	I - Batch: 200 | Loss: 1.186 | Acc: 42.125% | Wgt Acc: 51.209%
	I - Batch: 250 | Loss: 1.182 | Acc: 42.225% | Wgt Acc: 51.213%
I - num batch: 273
I - Train -- Loss: 1.188 | Acc: 41.930% | Wgt Acc: 50.932% | LR: 1.000000e-03 | Dur: 167.44s
I - Confusion Matrix: [row->prediction - col->label]
[[451.  44.  53. 195. 110.]
 [ 40. 318. 174.  86. 224.]
 [120. 315. 690. 123. 541.]
 [186.  83. 115. 369. 124.]
 [  0.   0.   0.   0.   1.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.530 | Acc: 33.925% | Wgt Acc: 46.477% | Dur: 15.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   4.  27.  19.]
 [  7.  54.  38.  16. 102.]
 [  3.  16.  25.   6.  41.]
 [ 25.   7.   8.  37.  15.]
 [  0.   0.   0.   0.   3.]]

I - Local maximum validation set accuracy:  33.93

I - Validation set results: 
[14-1-1-1.07][50-3-3-0.36][124-2-2-1.34][127-0-0-2.01][443-2-1-1.14][567-0-0-2.21][573-1-1-0.91][615-0-3-1.22][695-1-2-0.78][722-3-0-2.32]
[826-0-0-0.59][878-0-0-2.26][1103-0-0-0.34][1212-3-3-0.30][1368-0-0-1.43][2181-2-0-0.60][2476-2-3--0.00][2721-2-2-0.96][2818-1-0-0.25][2886-2-1-1.46]
[3231-2-1-1.63][3333-2-1-1.19][3482-2-2-0.55][3536-3-3-0.69][3625-1-1-1.49][3909-0-3-0.14][4035-0-3-2.03][4140-0-0-2.32][4214-1-1-0.45][4346-1-3-0.37]
[4581-2-2-1.75][4708-3-3-0.33][4838-3-0-0.37][4845-1-1-1.05][4868-0-0-1.16][4939-0-1-0.60][4984-2-2-0.87][5078-1-1-0.41][5396-0-0-3.19][5479-1-1-1.37]
[5717-0-0-0.69][5843-1-1-1.66][5949-3-0-2.00][5987-2-1-1.67][6014-3-1-0.66][6033-3-0-1.50][6313-0-3-1.34][6421-3-3-1.46][6500-1-1-1.02][6583-3-3-0.56]
[6683-3-3-0.59][6825-2-3-0.64][6998-3-3-1.04][7049-3-1-0.42][7517-1-1-1.90][7521-1-3-0.60][7528-1-2-0.39][7949-1-2-0.76][8135-1-3-0.64][8185-3-0-1.46]
[8269-3-1-0.73][8273-3-3-0.53][8543-3-0-3.10][8666-1-1-0.88][8672-0-0-2.64][8903-1-2-0.44][9001-2-1-1.52][9036-2-2-1.14][9281-3-1-0.61][9300-2-2-0.63]
[9571-0-0-0.73][9617-1-1-0.65][9644-2-2-1.19][9705-2-1-0.75][9801-0-3-0.55][9803-3-0-0.70][9865-3-0-2.00][9896-2-1-1.19][10314-1-1-0.94][10337-3-0-1.38]
[10403-0-0--0.08][10653-2-1-1.07][10704-2-1-0.77][10719-1-1-1.90][10727-1-1-1.67][10836-0-0-3.55][10969-2-3-0.23][11042-0-0-0.78][11088-1-2-1.87][11322-0-0-1.68]
[11398-2-2-1.26][11499-0-3-0.40][11502-3-3-0.89][11512-3-1-0.91][11608-1-1-1.87][11610-0-3-0.82][11692-0-3-1.01][11905-0-0-4.00][11993-1-2-0.78][12002-2-3-0.16]
[12052-0-0-1.58][12201-0-0-1.24][12235-2-2-0.82][12320-1-1-0.30][12377-2-1-1.08][12398-2-1-0.22][12503-1-2-1.83][12617-0-1-1.47][12685-3-3-0.39][12738-2-3-0.69]
[12742-2-2-1.39][12823-0-3-1.81][13110-1-3-0.31][13240-3-3-0.75][13253-1-1-1.37][13273-0-0-3.35][13634-1-1-1.41][13763-2-3-1.09][13905-3-3-1.11][14060-2-1-1.88]
[14065-3-0-1.70][14147-3-1-0.42][14595-2-1-1.28][14687-2-2-0.80][14788-2-2-0.81][14869-1-1-1.14][14872-3-0-1.02][14877-1-2-0.32][14927-0-3-1.01][15066-0-0-3.28]
[15175-1-1-0.85][15178-2-3-0.52][15375-3-3-1.02][15389-3-3-0.89][15568-2-1-1.41][15675-3-3-0.77][15869-1-2-0.95][16207-3-0-0.82][16236-0-2-0.68][16302-3-2-0.43]
[16331-2-2-1.33][16381-0-0-1.65][16488-1-1-1.89][16495-0-0-2.15][16650-0-0-2.94][16719-1-1-0.57][16801-0-0-3.71][16828-0-0-1.99][17137-3-0-1.29][17245-1-1-0.25]
[17278-3-1-0.60][17282-0-0-0.57][17311-2-2-1.34][17336-2-1-0.73][17608-3-3-2.30][17627-0-1-0.66][17877-3-1-0.95][17924-1-1-0.21][17984-3-0-3.21][18211-0-3-1.02]
[18276-3-0-2.29][18287-1-1-0.45][18394-0-0-1.05][18428-0-3-0.31][18442-0-0-0.81][18478-3-0-1.90][18607-0-0--0.00][18616-0-0-0.21][18663-0-0-2.23][18718-0-0-1.27]
[18766-2-1-1.76][18824-2-1-0.79][18890-3-2-0.61][18930-3-2-0.38][18938-3-3-0.30][19817-1-1-0.84][19839-0-2-0.74][19930-3-3-1.17][19944-0-1-1.38][20036-2-2-1.22]
[20101-3-3-0.22][20474-1-1-1.00][20547-3-1-0.87][20929-2-1-1.34][21245-1-1-1.35][21257-3-1-0.73][21293-1-1-1.67][21316-1-1-2.15][21384-1-2-0.84][21448-1-1-1.30]
[21483-0-3-0.80][21487-2-1-1.24][21714-0-3-0.61][21943-3-1-0.97][21947-0-0-2.23][21948-0-0-4.89][21965-2-1-0.88][21998-1-3-0.24][22025-0-3-0.50][22228-3-0-1.67]
[22446-1-1-2.31][22494-3-0-1.62][22757-0-0-2.31][22811-3-3-1.95][22976-3-1-0.92][22985-3-0-1.27][23014-0-3-1.32][23112-1-1-1.56][23144-3-0-1.53][23168-2-0-1.69]
[23219-0-0-0.99][23363-3-3-1.55][23470-0-1-0.02][23486-2-1-0.55][23497-0-3-2.02][23516-0-0-4.10][23690-1-1-0.67][23921-2-2-0.80][23936-1-2-0.53][24040-3-1-0.63]
[24111-1-1-1.54][24182-0-3-2.59][24238-3-3-1.12][24290-2-0-1.34][24345-0-0-2.23][24364-1-1-0.10][24427-3-3-0.74][24477-2-2-1.09][24495-2-1-0.74][24893-2-1-0.75]
[25012-1-1-0.40][25121-2-2-1.19][25165-3-3-0.60][25183-0-0-1.04][25297-3-3-0.73][25398-0-0-1.35][25574-2-1-1.56][25644-1-1-1.25][25718-1-1-1.17][25774-2-2-0.74]
[26032-3-3-0.63][26051-3-3-1.73][26120-0-0-1.10][26321-1-2-0.93][26732-1-1-0.73][26784-3-3-2.38][26827-3-3-0.64][26833-0-3-1.75][26838-2-1-0.42][26860-1-2-0.93]
[26948-0-0-0.87][27049-3-0-2.36][27098-1-1-0.55][27526-0-0-2.52][27639-3-3-0.86][27698-3-3-1.43][27772-0-0-3.03][27890-1-1-0.42][28040-0-0-0.58][28503-2-2-1.67]
[28577-1-1-1.44][28959-0-0-3.71][29198-3-0-0.22][29777-0-0-3.16][29877-2-1-0.33][30035-1-1-1.28][30098-0-3-0.93][30326-1-1-1.57][30572-2-2-0.57][30716-0-1-0.03]
[30806-2-2-0.33][30906-1-1-1.44][31007-0-0-2.18][31181-3-3-0.52][31238-0-0-0.87][31347-0-3-1.10][31422-2-2-0.22][31429-3-0-0.59][31431-0-3-0.50][31432-1-1-1.10]
[31477-0-0-2.14][31524-1-1-0.36][31597-1-2-1.18][31619-1-3-0.02][31701-0-0-0.63][31755-0-3-0.67][31854-3-3-0.46][32074-1-1-0.54][32078-3-2-0.68][32111-1-1-1.45]
[32127-1-2-1.57][32140-3-3-0.72][32263-2-1-0.16][32365-0-0-0.41][32411-2-0-2.49][32429-3-0-1.54][32473-3-0-1.02][32574-3-3-1.76][32584-0-2-0.48][32622-0-1-0.38]
[32858-3-0-0.21][32969-3-0-1.71][33016-2-1-0.95][33031-1-3-1.56][33035-2-2-1.47][33133-2-1-1.28][33173-2-1-0.77][33175-3-1-1.09][33306-3-2-0.66][33309-2-1-0.22]
[33474-0-3-0.29][33478-2-3-0.23][33618-1-1-1.21][33712-0-3-0.49][33782-2-1-1.45][33914-3-1-0.72][34076-3-1-0.37][34112-2-1-1.69][34138-2-1-1.02][34239-1-2-0.90]
[34364-2-1-1.70][34617-1-1-0.65][34751-3-3-1.65][34783-2-1-0.97][35015-3-2-0.51][35018-1-1-1.33][35288-2-1-0.14][0-4-1-0.91][1-4-2-0.75][2-4-1-0.10]
[3-4-2-0.76][4-4-3-0.42][5-4-1-1.61][6-4-0-2.06][7-4-1-0.99][8-4-2-0.89][9-4-1-1.21][10-4-1-0.33][11-4-1-0.78][12-4-1-1.44]
[14-4-0-1.31][15-4-3-1.68][16-4-1-0.40][17-4-1-0.97][18-4-1-1.33][19-4-0-2.13][20-4-0-0.86][21-4-2-1.13][22-4-1-0.67][23-4-1-0.51]
[24-4-1-0.23][25-4-2-0.80][26-4-3-0.80][27-4-0-1.16][28-4-1-0.48][29-4-1-0.18][30-4-0-0.31][31-4-2-1.19][32-4-1-1.44][33-4-2-0.84]
[34-4-3-0.00][35-4-3-1.11][37-4-0-0.23][39-4-0-1.90][40-4-0-0.21][41-4-1-0.12][42-4-2-0.72][43-4-2-1.24][45-4-1-0.36][46-4-2-0.94]
[47-4-4-0.65][48-4-1-1.05][51-4-1-0.90][52-4-2-0.54][53-4-1-1.12][54-4-1-0.81][55-4-2-0.11][56-4-1-1.94][57-4-3-0.57][58-4-1-1.43]
[59-4-3-0.45][60-4-1-0.48][61-4-1-1.23][62-4-3-0.33][63-4-2-1.06][64-4-1-0.97][65-4-1-1.49][66-4-1-1.70][67-4-2-0.97][68-4-1-1.67]
[69-4-2-0.49][70-4-2-0.85][72-4-1-1.27][73-4-1-1.19][74-4-2-1.16][75-4-3-0.56][77-4-1-1.49][78-4-2-0.46][79-4-1-1.15][80-4-1-0.97]
[81-4-1-1.07][82-4-1-0.62][83-4-1-0.86][84-4-2-1.30][85-4-1-0.33][86-4-1-1.18][87-4-0-0.46][88-4-1-0.95][89-4-2-0.60][90-4-1--0.14]
[91-4-2-0.86][92-4-1-0.31][93-4-1-0.24][94-4-1-1.19][95-4-1-0.60][96-4-1-1.86][97-4-4-0.28][98-4-2-0.92][99-4-1-0.33][100-4-1-1.23]
[101-4-2-0.60][102-4-2-0.84][103-4-2-0.40][104-4-4-0.98][105-4-1-1.74][106-4-1-1.74][107-4-1-0.81][108-4-1-0.10][109-4-1-0.58][110-4-2-0.87]
[111-4-0-2.01][112-4-2--0.04][113-4-3-0.18][114-4-3-0.45][115-4-0-0.23][116-4-1-0.77][117-4-1-2.12][119-4-2-1.52][121-4-1-0.58][122-4-3-0.35]
[124-4-1-0.80][125-4-1-1.62][126-4-1-0.31][127-4-1-0.96][128-4-1-0.80][129-4-1-1.04][130-4-1-1.03][131-4-2-0.70][132-4-2-0.84][133-4-0-1.36]
[135-4-2-1.04][136-4-1-0.39][137-4-1-0.53][138-4-1-0.66][139-4-3-0.21][140-4-1-0.67][141-4-2-0.56][142-4-1-0.57][143-4-1-0.94][144-4-1-0.88]
[145-4-1-1.83][148-4-0-2.23][149-4-2-0.80][150-4-2-0.83][151-4-1-1.11][152-4-1-1.40][153-4-1-1.85][154-4-1-1.90][155-4-1-0.67][156-4-3-0.16]
[157-4-2-0.87][158-4-1-0.30][160-4-1-1.12][161-4-1-0.80][162-4-1-0.01][164-4-2-0.76][165-4-1-0.47][167-4-0-1.00][168-4-1-0.08][170-4-3-1.16]
[171-4-1-1.21][172-4-1-0.77][173-4-0-0.76][174-4-0-1.61][175-4-1-0.67][177-4-0-0.77][178-4-1-0.98][179-4-1-0.65][180-4-1-0.29][181-4-1-0.39]
[182-4-1-0.68][183-4-1-0.53][184-4-2-0.92][186-4-1-0.25][187-4-1-1.61][188-4-2-0.78][189-4-2-0.70][190-4-1-1.45][191-4-2-0.53][192-4-2-0.85]
[193-4-1-1.44][194-4-1-0.44][195-4-0-1.41][196-4-1-0.53][197-4-1-1.11][198-4-1-0.43][199-4-1-1.52]
---------------------------
I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.169 | Acc: 43.750% | Wgt Acc: 52.508%
	I - Batch: 100 | Loss: 1.136 | Acc: 44.938% | Wgt Acc: 53.899%
	I - Batch: 150 | Loss: 1.123 | Acc: 45.167% | Wgt Acc: 54.208%
	I - Batch: 200 | Loss: 1.103 | Acc: 46.438% | Wgt Acc: 55.882%
	I - Batch: 250 | Loss: 1.108 | Acc: 45.925% | Wgt Acc: 55.295%
I - num batch: 273
I - Train -- Loss: 1.105 | Acc: 46.171% | Wgt Acc: 55.592% | LR: 1.000000e-03 | Dur: 167.77s
I - Confusion Matrix: [row->prediction - col->label]
[[510.  27.  38. 194. 174.]
 [ 32. 375. 177.  73. 215.]
 [ 85. 271. 723. 123. 479.]
 [169.  87.  94. 383. 109.]
 [  1.   0.   0.   0.  23.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.552 | Acc: 34.122% | Wgt Acc: 45.404% | Dur: 14.69s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   5.  28.  25.]
 [  5.  30.   9.  11.  27.]
 [ 15.  44.  59.  29. 121.]
 [  6.   1.   2.  18.   1.]
 [  2.   1.   0.   0.   6.]]

I - Local maximum validation set accuracy:  34.12

I - Validation set results: 
[14-1-2-1.95][50-3-0-0.01][124-2-2-2.58][127-0-0-3.13][443-2-2-2.54][567-0-0-1.25][573-1-1-1.85][615-0-0-1.54][695-1-2-1.50][722-3-0-1.77]
[826-0-0-1.58][878-0-0-2.29][1103-0-0-0.13][1212-3-2-0.47][1368-0-0-2.17][2181-2-3-0.11][2476-2-2-1.15][2721-2-2-2.06][2818-1-2-1.13][2886-2-2-1.48]
[3231-2-2-2.68][3333-2-2-1.78][3482-2-2-2.03][3536-3-3-0.16][3625-1-1-2.48][3909-0-1-0.56][4035-0-3-0.88][4140-0-0-0.78][4214-1-2-0.58][4346-1-0-1.86]
[4581-2-2-3.32][4708-3-2-0.46][4838-3-0-0.46][4845-1-1-1.36][4868-0-0-1.82][4939-0-2-0.90][4984-2-2-2.05][5078-1-2-0.78][5396-0-0-4.21][5479-1-1-1.53]
[5717-0-0-1.02][5843-1-2-0.83][5949-3-0-1.36][5987-2-1-1.15][6014-3-1-0.45][6033-3-0-1.30][6313-0-0-1.29][6421-3-3-0.77][6500-1-1-1.41][6583-3-2-0.96]
[6683-3-1-0.32][6825-2-1-0.69][6998-3-2-0.75][7049-3-2-0.78][7517-1-1-2.21][7521-1-1-0.49][7528-1-2-1.80][7949-1-2-1.65][8135-1-0-0.55][8185-3-0-1.90]
[8269-3-2-1.37][8273-3-3-0.62][8543-3-0-3.45][8666-1-1-0.74][8672-0-0-3.83][8903-1-2-2.12][9001-2-1-0.99][9036-2-2-2.97][9281-3-2-0.74][9300-2-2-2.44]
[9571-0-0-0.97][9617-1-2-0.35][9644-2-2-2.73][9705-2-2-1.40][9801-0-2-0.47][9803-3-1-0.36][9865-3-3-1.55][9896-2-2-2.05][10314-1-2-2.00][10337-3-0-0.50]
[10403-0-2-1.51][10653-2-1-0.74][10704-2-1-0.90][10719-1-1-1.92][10727-1-2-1.09][10836-0-0-3.70][10969-2-2-0.93][11042-0-0-0.16][11088-1-1-2.08][11322-0-0-1.03]
[11398-2-2-2.12][11499-0-2-0.57][11502-3-0-0.37][11512-3-2-1.40][11608-1-1-2.11][11610-0-0-0.98][11692-0-0-0.47][11905-0-0-3.72][11993-1-2-2.26][12002-2-3--0.03]
[12052-0-0-0.36][12201-0-0-1.27][12235-2-2-1.21][12320-1-2-0.35][12377-2-2-0.98][12398-2-2-0.12][12503-1-2-2.12][12617-0-1-0.33][12685-3-1-0.21][12738-2-2-1.24]
[12742-2-2-2.88][12823-0-0-1.35][13110-1-2-1.23][13240-3-2--0.01][13253-1-1-1.67][13273-0-0-2.34][13634-1-2-1.84][13763-2-1-0.50][13905-3-3--0.08][14060-2-1-1.19]
[14065-3-0-1.86][14147-3-2-0.64][14595-2-2-2.00][14687-2-2-2.56][14788-2-2-2.58][14869-1-1-2.03][14872-3-0-0.57][14877-1-1-1.26][14927-0-3-0.66][15066-0-0-3.26]
[15175-1-2-0.61][15178-2-0-0.12][15375-3-1-0.34][15389-3-0-1.35][15568-2-1-0.06][15675-3-1-0.64][15869-1-2-1.53][16207-3-2-0.07][16236-0-2-0.78][16302-3-2-1.18]
[16331-2-2-3.04][16381-0-0-1.57][16488-1-1-2.32][16495-0-0-0.27][16650-0-0-2.36][16719-1-2-0.83][16801-0-0-3.77][16828-0-0-1.16][17137-3-0-1.10][17245-1-2-1.15]
[17278-3-1-0.76][17282-0-1-0.14][17311-2-2-2.86][17336-2-2-2.16][17608-3-3-1.73][17627-0-2-0.01][17877-3-1-1.20][17924-1-2-1.21][17984-3-0-2.39][18211-0-3-0.12]
[18276-3-0-1.91][18287-1-1-1.23][18394-0-0-0.97][18428-0-0-0.99][18442-0-0-0.55][18478-3-0-1.34][18607-0-4-0.19][18616-0-0-0.28][18663-0-0-1.38][18718-0-0-1.53]
[18766-2-2-2.34][18824-2-2-1.71][18890-3-2-2.01][18930-3-2-0.91][18938-3-2-0.27][19817-1-2-1.82][19839-0-2-1.45][19930-3-3-0.49][19944-0-2-1.96][20036-2-2-2.46]
[20101-3-1-0.24][20474-1-2-1.56][20547-3-3-0.27][20929-2-2-2.48][21245-1-2-1.64][21257-3-2-0.75][21293-1-1-2.35][21316-1-1-1.97][21384-1-2-2.62][21448-1-2-2.25]
[21483-0-0-0.23][21487-2-2-1.83][21714-0-2-0.28][21943-3-2-1.43][21947-0-0-2.16][21948-0-0-4.02][21965-2-2-2.83][21998-1-1-1.10][22025-0-2-0.99][22228-3-3-0.30]
[22446-1-1-1.46][22494-3-0-1.86][22757-0-3-1.34][22811-3-3-1.42][22976-3-2-1.82][22985-3-0-1.40][23014-0-0-1.52][23112-1-2-1.71][23144-3-0-0.91][23168-2-0-1.91]
[23219-0-0-0.69][23363-3-3-0.51][23470-0-2-0.80][23486-2-2-0.99][23497-0-0-2.71][23516-0-0-3.47][23690-1-2-0.90][23921-2-2-1.89][23936-1-2-1.75][24040-3-2-1.01]
[24111-1-4-0.95][24182-0-0-2.40][24238-3-3-0.65][24290-2-0-0.26][24345-0-2-1.00][24364-1-2-1.30][24427-3-0-0.01][24477-2-2-2.40][24495-2-2-1.13][24893-2-2-2.37]
[25012-1-2-0.82][25121-2-2-1.46][25165-3-0-0.21][25183-0-0-0.73][25297-3-2-0.31][25398-0-0-1.49][25574-2-2-1.34][25644-1-2-2.28][25718-1-1-0.40][25774-2-2-1.74]
[26032-3-2-0.27][26051-3-3-1.10][26120-0-2-0.84][26321-1-1-1.37][26732-1-1-1.25][26784-3-3-1.50][26827-3-3-0.26][26833-0-3-0.66][26838-2-2-0.93][26860-1-2-2.24]
[26948-0-0--0.04][27049-3-0-1.85][27098-1-2-0.90][27526-0-0-2.56][27639-3-0-0.60][27698-3-3-0.58][27772-0-0-2.53][27890-1-1-1.59][28040-0-2-0.77][28503-2-2-3.67]
[28577-1-1-1.75][28959-0-0-4.44][29198-3-0-0.17][29777-0-0-3.43][29877-2-2-1.83][30035-1-2-2.13][30098-0-0-1.06][30326-1-1-1.94][30572-2-2-2.43][30716-0-4-0.32]
[30806-2-2-1.02][30906-1-1-1.17][31007-0-0-0.78][31181-3-2-0.48][31238-0-1-0.02][31347-0-0-0.93][31422-2-2-0.92][31429-3-1-0.53][31431-0-0-0.98][31432-1-1-1.38]
[31477-0-0-2.56][31524-1-2-0.79][31597-1-2-2.52][31619-1-2-1.07][31701-0-0-0.18][31755-0-0-0.95][31854-3-1-0.91][32074-1-2-2.07][32078-3-2-1.20][32111-1-1-1.58]
[32127-1-2-2.62][32140-3-3-0.50][32263-2-0-0.03][32365-0-0--0.07][32411-2-0-3.65][32429-3-0-1.32][32473-3-0-1.22][32574-3-0-1.22][32584-0-2-0.14][32622-0-1-0.90]
[32858-3-2-0.66][32969-3-0-2.44][33016-2-2-2.54][33031-1-3-0.93][33035-2-2-3.32][33133-2-2-2.16][33173-2-2-1.83][33175-3-2-3.01][33306-3-2-1.98][33309-2-1-0.40]
[33474-0-3--0.05][33478-2-2-0.33][33618-1-1-1.26][33712-0-0-0.13][33782-2-2-2.73][33914-3-2-0.34][34076-3-2-1.16][34112-2-2-1.59][34138-2-2-2.31][34239-1-2-1.74]
[34364-2-2-2.78][34617-1-2-1.45][34751-3-3-0.86][34783-2-2-1.51][35015-3-2-1.30][35018-1-2-2.15][35288-2-2-1.78][0-4-2-1.51][1-4-4-0.85][2-4-4-0.25]
[3-4-2-0.41][4-4-2-0.59][5-4-1-2.16][6-4-0-1.57][7-4-2-0.38][8-4-2-1.95][9-4-2-1.23][10-4-2-0.71][11-4-2-2.73][12-4-2-1.03]
[14-4-1-0.57][15-4-0-1.28][16-4-0-0.13][17-4-1-0.03][18-4-1-0.72][19-4-0-2.99][20-4-1--0.01][21-4-2-1.73][22-4-2-0.88][23-4-2-0.40]
[24-4-2-0.70][25-4-2-1.56][26-4-2--0.04][27-4-2-1.50][28-4-2-0.35][29-4-2-1.95][30-4-0-0.44][31-4-2-1.64][32-4-2-2.17][33-4-2-1.32]
[34-4-2-1.12][35-4-0-1.82][37-4-2-1.52][39-4-0-1.54][40-4-2-0.93][41-4-2-0.81][42-4-2-1.49][43-4-2-1.38][45-4-2-2.27][46-4-2-1.79]
[47-4-2-1.27][48-4-2-0.67][51-4-2-0.51][52-4-2-1.30][53-4-1-1.23][54-4-0-0.50][55-4-2-0.22][56-4-2-1.00][57-4-0-1.57][58-4-2-2.08]
[59-4-2-0.27][60-4-1-0.36][61-4-2-1.04][62-4-2-1.64][63-4-2-2.41][64-4-2-0.93][65-4-2-1.17][66-4-1-1.54][67-4-2-1.72][68-4-1-1.77]
[69-4-0-1.09][70-4-2-0.57][72-4-2-1.57][73-4-2-0.81][74-4-2-2.59][75-4-2-0.14][77-4-2-1.42][78-4-2-0.98][79-4-2-1.18][80-4-2-0.76]
[81-4-2-2.26][82-4-1-0.46][83-4-1-1.20][84-4-2-0.69][85-4-2-1.28][86-4-1-1.03][87-4-1-0.59][88-4-1-0.88][89-4-2-1.53][90-4-2-0.16]
[91-4-2-1.83][92-4-2-0.90][93-4-0--0.07][94-4-2-0.92][95-4-2-1.11][96-4-1-0.65][97-4-2-1.19][98-4-2-2.68][99-4-1-0.59][100-4-1-1.58]
[101-4-4-0.55][102-4-2-1.63][103-4-2-0.38][104-4-2-1.33][105-4-2-1.24][106-4-2-1.20][107-4-4-0.82][108-4-2-1.11][109-4-1--0.05][110-4-2-2.07]
[111-4-0-2.54][112-4-2-0.64][113-4-2-0.60][114-4-2-0.97][115-4-0-0.23][116-4-2-0.41][117-4-2-1.13][119-4-2-3.76][121-4-1-1.11][122-4-0-0.38]
[124-4-2-1.62][125-4-2-1.66][126-4-0-0.77][127-4-2-1.61][128-4-2-1.98][129-4-1-1.41][130-4-2-1.55][131-4-2-1.48][132-4-2-0.99][133-4-0-1.51]
[135-4-2-1.69][136-4-1-0.84][137-4-2-1.04][138-4-1-0.52][139-4-2-1.05][140-4-1-1.18][141-4-2-0.47][142-4-2-0.78][143-4-2-1.14][144-4-4-0.84]
[145-4-2-1.38][148-4-0-2.40][149-4-2-1.91][150-4-2-1.84][151-4-2-1.57][152-4-2-1.63][153-4-2-1.93][154-4-2-2.00][155-4-0-0.24][156-4-0-0.13]
[157-4-2-0.18][158-4-2-0.77][160-4-1-1.32][161-4-2-2.41][162-4-2-1.14][164-4-2-1.82][165-4-2-1.31][167-4-0-0.64][168-4-2-0.20][170-4-0-2.00]
[171-4-2-0.88][172-4-4-0.77][173-4-2-0.42][174-4-0-1.03][175-4-2-0.83][177-4-0-1.26][178-4-2-1.53][179-4-1-0.22][180-4-0-0.39][181-4-3-0.07]
[182-4-2-1.47][183-4-2-0.53][184-4-2-2.44][186-4-0-0.28][187-4-1-2.09][188-4-2-1.72][189-4-2-1.53][190-4-2-0.30][191-4-2-2.18][192-4-2-1.58]
[193-4-2-3.67][194-4-2-2.28][195-4-2-0.69][196-4-2-1.17][197-4-1-1.00][198-4-2-0.56][199-4-2-1.24]
---------------------------
I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.097 | Acc: 46.750% | Wgt Acc: 56.615%
	I - Batch: 100 | Loss: 1.089 | Acc: 47.375% | Wgt Acc: 56.775%
	I - Batch: 150 | Loss: 1.072 | Acc: 47.833% | Wgt Acc: 57.199%
	I - Batch: 200 | Loss: 1.060 | Acc: 48.719% | Wgt Acc: 57.961%
	I - Batch: 250 | Loss: 1.065 | Acc: 48.675% | Wgt Acc: 57.810%
I - num batch: 273
I - Train -- Loss: 1.063 | Acc: 48.395% | Wgt Acc: 57.622% | LR: 1.000000e-03 | Dur: 165.65s
I - Confusion Matrix: [row->prediction - col->label]
[[510.  34.  34. 177. 145.]
 [ 36. 392. 143.  65. 232.]
 [ 74. 259. 748. 124. 440.]
 [174.  72.  98. 406. 128.]
 [  3.   3.   9.   1.  55.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.506 | Acc: 40.039% | Wgt Acc: 51.620% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[47.  8.  4. 10. 36.]
 [ 2. 39. 10.  9. 45.]
 [10. 25. 53. 21. 71.]
 [29.  4.  7. 46. 10.]
 [ 0.  2.  1.  0. 18.]]

I - Local maximum validation set accuracy:  40.04

I - Validation set results: 
[14-1-2-1.19][50-3-0-0.58][124-2-2-2.07][127-0-0-1.80][443-2-2-0.88][567-0-0-0.57][573-1-1-1.53][615-0-3-1.20][695-1-2-1.38][722-3-3-2.16]
[826-0-3-0.16][878-0-0-1.93][1103-0-0-0.66][1212-3-2-0.42][1368-0-0-2.73][2181-2-3-0.48][2476-2-2-0.09][2721-2-2-2.06][2818-1-3-0.08][2886-2-2-0.85]
[3231-2-2-2.03][3333-2-1-1.31][3482-2-2-1.52][3536-3-1-1.15][3625-1-1-1.28][3909-0-0-1.02][4035-0-3-1.94][4140-0-0-1.89][4214-1-1-0.49][4346-1-0-1.91]
[4581-2-2-2.05][4708-3-2-0.58][4838-3-0-1.53][4845-1-1-1.06][4868-0-0-1.61][4939-0-0--0.03][4984-2-2-1.02][5078-1-1-0.18][5396-0-0-1.61][5479-1-1-1.39]
[5717-0-0-1.94][5843-1-1-0.61][5949-3-3-1.34][5987-2-1-0.70][6014-3-3-1.24][6033-3-3-0.35][6313-0-3-1.64][6421-3-3-1.37][6500-1-1-1.14][6583-3-2-0.42]
[6683-3-3-0.40][6825-2-3-0.71][6998-3-3-1.20][7049-3-2-1.30][7517-1-1-1.95][7521-1-0-0.24][7528-1-2-0.80][7949-1-2-2.06][8135-1-0-1.12][8185-3-3-1.46]
[8269-3-1--0.11][8273-3-3-0.72][8543-3-0-1.85][8666-1-1-1.49][8672-0-3-1.29][8903-1-2-1.22][9001-2-1-1.06][9036-2-2-1.55][9281-3-1-0.21][9300-2-2-1.68]
[9571-0-3-0.65][9617-1-1-0.39][9644-2-2-1.78][9705-2-2-0.90][9801-0-2-0.47][9803-3-3-0.72][9865-3-3-2.15][9896-2-2-1.16][10314-1-2-0.96][10337-3-3-1.29]
[10403-0-0-0.25][10653-2-0-0.38][10704-2-2-1.86][10719-1-1-1.64][10727-1-4-0.61][10836-0-0-3.63][10969-2-2-0.44][11042-0-0-1.47][11088-1-2-1.67][11322-0-0-1.50]
[11398-2-2-1.50][11499-0-0-0.41][11502-3-3-0.91][11512-3-1-0.01][11608-1-1-1.74][11610-0-0-0.18][11692-0-3-0.82][11905-0-0-1.82][11993-1-2-0.63][12002-2-3-1.95]
[12052-0-0-0.95][12201-0-3-1.24][12235-2-1-1.21][12320-1-0-0.22][12377-2-4-0.35][12398-2-1-0.75][12503-1-2-0.87][12617-0-1-0.59][12685-3-1-1.00][12738-2-2-0.79]
[12742-2-2-1.84][12823-0-3-1.27][13110-1-3-0.31][13240-3-3-0.49][13253-1-2-0.94][13273-0-0-1.48][13634-1-2-1.66][13763-2-3-0.85][13905-3-3-0.43][14060-2-1-1.92]
[14065-3-3-1.16][14147-3-2-0.70][14595-2-2-1.57][14687-2-2-1.75][14788-2-2-1.77][14869-1-1-1.51][14872-3-0-0.76][14877-1-1-0.67][14927-0-3-0.99][15066-0-0-3.57]
[15175-1-1-0.42][15178-2-3-0.40][15375-3-3-0.32][15389-3-3-0.92][15568-2-1-0.90][15675-3-3-0.72][15869-1-2-1.41][16207-3-3-0.28][16236-0-3-0.44][16302-3-2-0.79]
[16331-2-2-1.11][16381-0-3-0.30][16488-1-1-2.10][16495-0-0-1.99][16650-0-0-1.96][16719-1-0-0.73][16801-0-0-3.02][16828-0-0-1.25][17137-3-3-0.39][17245-1-1-0.61]
[17278-3-0-0.69][17282-0-0-1.55][17311-2-2-1.78][17336-2-2-1.30][17608-3-3-2.44][17627-0-2-0.13][17877-3-1-1.18][17924-1-2-1.49][17984-3-0-2.63][18211-0-3-0.95]
[18276-3-0-0.93][18287-1-1-0.99][18394-0-0-0.54][18428-0-3-0.79][18442-0-3-0.54][18478-3-3-0.41][18607-0-0-1.62][18616-0-0-1.26][18663-0-0-2.01][18718-0-0-0.79]
[18766-2-2-2.43][18824-2-2-0.48][18890-3-2-0.29][18930-3-2-0.45][18938-3-2-0.29][19817-1-2-0.47][19839-0-2-1.32][19930-3-3-1.60][19944-0-2-1.53][20036-2-2-2.53]
[20101-3-3-0.75][20474-1-1-1.44][20547-3-0-0.60][20929-2-2-1.45][21245-1-1-1.74][21257-3-1-0.62][21293-1-1-1.67][21316-1-1-1.90][21384-1-2-1.69][21448-1-2-0.87]
[21483-0-0-0.89][21487-2-2-1.70][21714-0-2-0.26][21943-3-2-1.47][21947-0-0-1.49][21948-0-0-5.31][21965-2-1-0.95][21998-1-1-0.04][22025-0-2-0.69][22228-3-3-1.33]
[22446-1-1-1.44][22494-3-3-1.37][22757-0-3-1.83][22811-3-3-1.26][22976-3-1-1.33][22985-3-3-1.60][23014-0-3-1.07][23112-1-1-0.51][23144-3-3-1.76][23168-2-0-2.16]
[23219-0-3-0.52][23363-3-3-1.86][23470-0-0-0.21][23486-2-2-1.18][23497-0-3-1.92][23516-0-0-2.26][23690-1-1-0.89][23921-2-2-1.19][23936-1-2-1.64][24040-3-2-0.88]
[24111-1-4-1.04][24182-0-3-2.04][24238-3-3-0.95][24290-2-0-0.95][24345-0-0-1.19][24364-1-3-0.54][24427-3-3-0.34][24477-2-2-1.93][24495-2-2-0.20][24893-2-2-1.87]
[25012-1-0-0.46][25121-2-2-1.13][25165-3-3-0.77][25183-0-0-2.25][25297-3-1-0.52][25398-0-0-1.65][25574-2-2-0.74][25644-1-1-0.19][25718-1-1-1.19][25774-2-2-0.88]
[26032-3-2-0.59][26051-3-3-1.64][26120-0-2-0.34][26321-1-1-0.62][26732-1-1-0.91][26784-3-3-2.05][26827-3-2-0.77][26833-0-3-1.81][26838-2-1-0.81][26860-1-2-1.05]
[26948-0-0-1.72][27049-3-0-1.42][27098-1-1-0.71][27526-0-0-1.76][27639-3-3-0.53][27698-3-3-1.05][27772-0-3-1.47][27890-1-0-0.73][28040-0-2-0.36][28503-2-2-2.22]
[28577-1-1-1.61][28959-0-0-3.05][29198-3-0-1.36][29777-0-3-1.23][29877-2-2-0.74][30035-1-2-1.29][30098-0-0-0.23][30326-1-1-2.27][30572-2-2-1.19][30716-0-1-0.21]
[30806-2-2-0.41][30906-1-1-1.03][31007-0-0-1.82][31181-3-2-0.86][31238-0-3-0.65][31347-0-2-0.67][31422-2-2-0.58][31429-3-3-0.43][31431-0-3-0.56][31432-1-1-1.16]
[31477-0-3-1.85][31524-1-2-1.73][31597-1-2-1.52][31619-1-2-0.68][31701-0-3-0.22][31755-0-3-0.43][31854-3-3-0.49][32074-1-1-0.47][32078-3-2-1.36][32111-1-0-0.17]
[32127-1-2-1.88][32140-3-3-0.97][32263-2-0-0.97][32365-0-0-2.42][32411-2-3-1.91][32429-3-3-1.80][32473-3-3-0.73][32574-3-3-1.35][32584-0-2-1.02][32622-0-0-0.22]
[32858-3-2-0.27][32969-3-3-1.58][33016-2-2-0.97][33031-1-3-2.02][33035-2-2-2.82][33133-2-2-2.18][33173-2-1-0.56][33175-3-2-1.02][33306-3-2-1.71][33309-2-3-0.09]
[33474-0-0-0.42][33478-2-2-0.80][33618-1-1-0.87][33712-0-3-0.15][33782-2-2-1.42][33914-3-2-0.39][34076-3-2-2.14][34112-2-2-1.41][34138-2-2-2.21][34239-1-2-1.23]
[34364-2-2-2.34][34617-1-1-0.71][34751-3-3-1.56][34783-2-2-0.61][35015-3-2-1.52][35018-1-2-1.75][35288-2-2-0.54][0-4-2-0.92][1-4-2-1.08][2-4-0-0.45]
[3-4-2-1.34][4-4-2-0.05][5-4-1-1.25][6-4-4-0.97][7-4-0-0.28][8-4-2-1.61][9-4-1-1.25][10-4-4-1.00][11-4-2-2.29][12-4-1-1.32]
[14-4-0-0.89][15-4-3-1.48][16-4-0-1.27][17-4-1-0.63][18-4-1-1.09][19-4-3-1.57][20-4-0-0.47][21-4-2-1.11][22-4-4-0.41][23-4-2-0.58]
[24-4-4-0.83][25-4-2-1.17][26-4-1-0.97][27-4-3-0.25][28-4-0--0.00][29-4-2--0.09][30-4-1--0.20][31-4-2-1.29][32-4-1-0.64][33-4-2-1.53]
[34-4-2-0.78][35-4-3-0.90][37-4-2-1.93][39-4-3-1.19][40-4-0--0.08][41-4-3--0.13][42-4-2-0.15][43-4-2-0.68][45-4-1--0.15][46-4-2-0.86]
[47-4-4-0.74][48-4-1-1.09][51-4-1-0.29][52-4-2-1.02][53-4-1-0.71][54-4-0-0.86][55-4-2-0.59][56-4-1-0.85][57-4-2-0.22][58-4-2-1.89]
[59-4-0-1.66][60-4-0-0.43][61-4-1-0.76][62-4-2-1.47][63-4-2-2.32][64-4-1-0.61][65-4-1-0.77][66-4-4-0.73][67-4-2-1.04][68-4-1-1.61]
[69-4-0-0.67][70-4-1-0.13][72-4-2-0.90][73-4-1-0.19][74-4-2-1.28][75-4-0-0.58][77-4-2-0.75][78-4-2-1.42][79-4-2-0.29][80-4-4-0.42]
[81-4-1-1.44][82-4-1-0.45][83-4-1-0.85][84-4-2-1.19][85-4-1-0.25][86-4-2-1.29][87-4-0-0.90][88-4-1-0.49][89-4-2-1.19][90-4-4-0.57]
[91-4-2-1.23][92-4-0-0.34][93-4-0-1.69][94-4-4-0.36][95-4-2-0.49][96-4-1-1.61][97-4-0-0.46][98-4-2-1.15][99-4-0-0.16][100-4-2-1.02]
[101-4-1-0.73][102-4-2-0.51][103-4-2-1.66][104-4-4-0.77][105-4-4-1.09][106-4-1-1.21][107-4-0-0.93][108-4-1-0.25][109-4-1-1.37][110-4-0-0.95]
[111-4-0-2.05][112-4-0-1.14][113-4-2-0.83][114-4-2-1.35][115-4-0-0.71][116-4-0-1.01][117-4-1-1.93][119-4-2-3.10][121-4-1-1.49][122-4-3-0.27]
[124-4-2-0.89][125-4-1-1.30][126-4-2-0.98][127-4-2-1.36][128-4-2-1.43][129-4-1-0.93][130-4-2-0.27][131-4-2-1.67][132-4-2-0.96][133-4-4-0.47]
[135-4-2-0.71][136-4-0-0.14][137-4-1-0.15][138-4-2-1.10][139-4-2-1.39][140-4-2-0.78][141-4-2-1.50][142-4-4-0.33][143-4-0-0.39][144-4-4-0.94]
[145-4-1-0.63][148-4-0-2.35][149-4-2-0.99][150-4-1-1.34][151-4-2-0.72][152-4-2-1.19][153-4-1-1.45][154-4-1-1.42][155-4-1-0.10][156-4-3-0.23]
[157-4-2-0.31][158-4-2-0.21][160-4-2-1.17][161-4-2-1.48][162-4-0-0.29][164-4-2-0.85][165-4-1-0.18][167-4-0-1.50][168-4-0-0.76][170-4-3-0.88]
[171-4-1-1.53][172-4-4-0.51][173-4-0-0.60][174-4-0-0.43][175-4-2--0.04][177-4-2-0.07][178-4-4-0.95][179-4-0-0.63][180-4-0-0.75][181-4-2-1.08]
[182-4-2-2.02][183-4-0-0.52][184-4-2-2.00][186-4-0-0.50][187-4-1-1.51][188-4-2-1.37][189-4-2-1.02][190-4-1-0.57][191-4-4-0.25][192-4-2-0.73]
[193-4-2-2.85][194-4-3-0.57][195-4-0-0.79][196-4-1-0.76][197-4-1-0.81][198-4-4-1.13][199-4-2-1.37]
---------------------------
I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 1.047 | Acc: 49.000% | Wgt Acc: 58.584%
	I - Batch: 100 | Loss: 1.037 | Acc: 49.500% | Wgt Acc: 60.087%
	I - Batch: 150 | Loss: 1.039 | Acc: 49.833% | Wgt Acc: 60.057%
	I - Batch: 200 | Loss: 1.019 | Acc: 51.031% | Wgt Acc: 60.736%
	I - Batch: 250 | Loss: 1.015 | Acc: 51.225% | Wgt Acc: 60.785%
I - num batch: 273
I - Train -- Loss: 1.021 | Acc: 50.963% | Wgt Acc: 60.469% | LR: 1.000000e-03 | Dur: 169.77s
I - Confusion Matrix: [row->prediction - col->label]
[[531.  30.  40. 161. 178.]
 [ 36. 441. 157.  53. 167.]
 [ 66. 226. 738. 114. 460.]
 [161.  61.  86. 442. 124.]
 [  3.   2.  11.   3.  71.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.489 | Acc: 40.039% | Wgt Acc: 53.915% | Dur: 15.28s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  6. 35. 28.]
 [ 1. 48. 24.  3. 53.]
 [ 3. 19. 37.  6. 72.]
 [15.  6.  8. 42. 20.]
 [ 0.  0.  0.  0.  7.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 0.999 | Acc: 50.375% | Wgt Acc: 59.927%
	I - Batch: 100 | Loss: 0.999 | Acc: 51.438% | Wgt Acc: 60.887%
	I - Batch: 150 | Loss: 1.009 | Acc: 51.375% | Wgt Acc: 60.787%
	I - Batch: 200 | Loss: 1.011 | Acc: 50.969% | Wgt Acc: 60.251%
	I - Batch: 250 | Loss: 1.014 | Acc: 50.425% | Wgt Acc: 59.760%
I - num batch: 273
I - Train -- Loss: 1.013 | Acc: 50.779% | Wgt Acc: 60.259% | LR: 1.000000e-03 | Dur: 168.04s
I - Confusion Matrix: [row->prediction - col->label]
[[533.  29.  47. 165. 145.]
 [ 32. 427. 140.  60. 192.]
 [ 71. 235. 752. 114. 466.]
 [156.  60.  81. 434. 128.]
 [  5.   9.  12.   0.  69.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.468 | Acc: 42.406% | Wgt Acc: 55.115% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[52.  1.  4. 12. 21.]
 [ 2. 40. 16.  3. 44.]
 [ 2. 13. 36.  2. 41.]
 [31. 21. 19. 68. 55.]
 [ 1.  3.  0.  1. 19.]]

I - Local maximum validation set accuracy:  42.41

I - Validation set results: 
[14-1-1-1.04][50-3-3-0.84][124-2-2-1.31][127-0-0-3.03][443-2-2-2.22][567-0-0-2.22][573-1-1-1.29][615-0-3-2.65][695-1-3-0.29][722-3-3-2.35]
[826-0-0-2.86][878-0-0-3.78][1103-0-0-0.21][1212-3-3-2.20][1368-0-0-3.24][2181-2-3-1.52][2476-2-2-0.54][2721-2-2-1.13][2818-1-3-0.55][2886-2-1-1.62]
[3231-2-1-2.44][3333-2-1-0.87][3482-2-2-0.98][3536-3-3-1.53][3625-1-1-2.54][3909-0-0-1.49][4035-0-3-2.90][4140-0-0-2.45][4214-1-3-2.60][4346-1-3-2.10]
[4581-2-2-3.01][4708-3-3-2.22][4838-3-3-1.17][4845-1-3-0.91][4868-0-0-2.41][4939-0-1-0.82][4984-2-3-0.94][5078-1-1-0.87][5396-0-0-5.22][5479-1-1-1.34]
[5717-0-0-1.20][5843-1-1-1.40][5949-3-3-2.77][5987-2-1-1.71][6014-3-3-1.21][6033-3-0-1.05][6313-0-3-2.36][6421-3-3-3.00][6500-1-3-0.51][6583-3-3-1.08]
[6683-3-3-0.90][6825-2-0-1.23][6998-3-3-1.39][7049-3-3-2.04][7517-1-1-1.98][7521-1-3-0.98][7528-1-3-1.85][7949-1-2-1.48][8135-1-0-0.84][8185-3-0-2.45]
[8269-3-1-1.21][8273-3-3-2.01][8543-3-0-3.58][8666-1-1-1.40][8672-0-0-2.70][8903-1-2-1.22][9001-2-2-0.33][9036-2-2-3.43][9281-3-3-0.60][9300-2-2-2.56]
[9571-0-3-1.93][9617-1-4-0.37][9644-2-2-1.25][9705-2-2--0.03][9801-0-3-2.07][9803-3-3-1.63][9865-3-3-3.33][9896-2-1-0.99][10314-1-2-1.27][10337-3-3-3.58]
[10403-0-0-0.17][10653-2-1-1.60][10704-2-2-0.86][10719-1-1-2.71][10727-1-1-1.31][10836-0-0-4.50][10969-2-3-1.29][11042-0-0-1.22][11088-1-2-1.76][11322-0-0-1.61]
[11398-2-2-1.69][11499-0-0-1.34][11502-3-3-2.25][11512-3-3-1.30][11608-1-1-2.63][11610-0-3-2.18][11692-0-3-2.11][11905-0-0-3.54][11993-1-2-1.22][12002-2-0-2.45]
[12052-0-0-1.82][12201-0-3-2.93][12235-2-1-2.21][12320-1-4-0.46][12377-2-1-0.56][12398-2-3-0.81][12503-1-1-1.22][12617-0-3-1.12][12685-3-3-1.18][12738-2-3-1.11]
[12742-2-2-2.14][12823-0-3-2.92][13110-1-3-0.61][13240-3-3-1.79][13253-1-1-1.85][13273-0-0-4.27][13634-1-3-0.92][13763-2-3-1.43][13905-3-3-0.95][14060-2-1-0.44]
[14065-3-3-2.40][14147-3-3-1.91][14595-2-2-1.09][14687-2-2-1.39][14788-2-2-0.62][14869-1-1-1.83][14872-3-4-0.47][14877-1-1-0.90][14927-0-3-2.38][15066-0-0-3.76]
[15175-1-1-0.52][15178-2-0-1.71][15375-3-3-2.58][15389-3-3-2.99][15568-2-1-1.08][15675-3-3-2.27][15869-1-2-0.59][16207-3-3-0.80][16236-0-3-1.25][16302-3-3-1.08]
[16331-2-2-1.98][16381-0-0-2.18][16488-1-1-2.60][16495-0-0-2.22][16650-0-0-4.64][16719-1-2-0.19][16801-0-0-4.50][16828-0-3-1.87][17137-3-0-1.85][17245-1-3-0.41]
[17278-3-3-0.57][17282-0-0-1.73][17311-2-2-1.29][17336-2-2-1.58][17608-3-3-3.53][17627-0-3-0.82][17877-3-1-1.10][17924-1-3-0.35][17984-3-0-2.99][18211-0-3-2.08]
[18276-3-3-2.26][18287-1-1-0.07][18394-0-0-2.36][18428-0-0-3.39][18442-0-3-2.74][18478-3-0-2.24][18607-0-0-2.06][18616-0-3-0.88][18663-0-0-2.50][18718-0-0-3.13]
[18766-2-1-1.55][18824-2-1-0.64][18890-3-3-0.87][18930-3-0-0.65][18938-3-3-1.56][19817-1-2-0.68][19839-0-2-0.16][19930-3-3-2.53][19944-0-2-0.45][20036-2-2-3.11]
[20101-3-3-2.55][20474-1-1-1.95][20547-3-3-1.10][20929-2-2-1.81][21245-1-1-0.93][21257-3-3-0.89][21293-1-1-2.37][21316-1-1-3.27][21384-1-2-1.63][21448-1-1-1.30]
[21483-0-0-2.34][21487-2-2-0.88][21714-0-0-1.50][21943-3-2-0.51][21947-0-0-1.00][21948-0-0-4.74][21965-2-2-3.55][21998-1-1-0.41][22025-0-3-1.50][22228-3-0-2.89]
[22446-1-1-1.59][22494-3-3-2.15][22757-0-3-2.87][22811-3-3-3.75][22976-3-1-1.57][22985-3-3-3.43][23014-0-3-2.87][23112-1-1-1.72][23144-3-3-3.06][23168-2-3-0.77]
[23219-0-0-1.43][23363-3-3-2.59][23470-0-1-0.12][23486-2-3-1.88][23497-0-0-3.05][23516-0-0-3.10][23690-1-2-1.12][23921-2-2-1.31][23936-1-3-1.33][24040-3-3-0.21]
[24111-1-4-1.47][24182-0-3-3.75][24238-3-3-3.51][24290-2-0-1.69][24345-0-0-1.99][24364-1-3-1.43][24427-3-0-1.77][24477-2-2-1.65][24495-2-1-0.00][24893-2-1-1.05]
[25012-1-3-0.54][25121-2-2-1.43][25165-3-3-1.75][25183-0-0-0.24][25297-3-3-2.52][25398-0-3-1.93][25574-2-3-0.35][25644-1-1-1.74][25718-1-3-0.77][25774-2-3-0.73]
[26032-3-3-1.40][26051-3-3-2.75][26120-0-0-0.58][26321-1-1-0.80][26732-1-1-0.57][26784-3-3-4.16][26827-3-3-2.12][26833-0-3-2.90][26838-2-3-1.07][26860-1-2-0.40]
[26948-0-0-0.75][27049-3-0-1.59][27098-1-3-0.92][27526-0-0-2.83][27639-3-3-1.39][27698-3-3-2.81][27772-0-3-2.13][27890-1-1-1.26][28040-0-0-1.83][28503-2-2-3.18]
[28577-1-1-1.61][28959-0-0-3.86][29198-3-3-0.87][29777-0-0-4.10][29877-2-2-0.74][30035-1-1-1.76][30098-0-3-1.69][30326-1-1-3.06][30572-2-2-2.09][30716-0-4-0.14]
[30806-2-3-1.47][30906-1-1-2.09][31007-0-0-0.99][31181-3-3-2.44][31238-0-3-1.99][31347-0-3-2.39][31422-2-2-0.46][31429-3-3-0.78][31431-0-3-1.29][31432-1-1-1.84]
[31477-0-3-3.25][31524-1-3-1.05][31597-1-2-2.10][31619-1-3-0.77][31701-0-0-1.35][31755-0-0-1.36][31854-3-3-0.93][32074-1-3-1.36][32078-3-3-1.73][32111-1-1-1.82]
[32127-1-2-2.40][32140-3-3-2.53][32263-2-3-0.30][32365-0-0-1.84][32411-2-3-3.22][32429-3-3-2.47][32473-3-3-1.50][32574-3-3-2.52][32584-0-0-1.39][32622-0-3-0.36]
[32858-3-0-1.44][32969-3-0-2.71][33016-2-2-1.44][33031-1-3-2.33][33035-2-2-1.36][33133-2-2-1.61][33173-2-3-0.57][33175-3-2-0.80][33306-3-3-0.94][33309-2-3-0.68]
[33474-0-3-1.27][33478-2-3-0.74][33618-1-1-2.03][33712-0-0-1.48][33782-2-2-1.74][33914-3-3-2.41][34076-3-3-1.77][34112-2-1-1.01][34138-2-3-1.34][34239-1-1-0.60]
[34364-2-1-2.28][34617-1-1-0.93][34751-3-3-3.12][34783-2-2-1.12][35015-3-3-1.20][35018-1-1-1.57][35288-2-3-1.23][0-4-2-0.90][1-4-0-1.13][2-4-4-0.48]
[3-4-2-1.12][4-4-3-0.32][5-4-3-1.13][6-4-4-0.59][7-4-1-0.18][8-4-2-0.54][9-4-1-1.32][10-4-3-0.45][11-4-2-1.01][12-4-3-2.22]
[14-4-3-1.92][15-4-3-2.93][16-4-2-0.82][17-4-3-0.86][18-4-1-0.61][19-4-3-3.25][20-4-0-0.72][21-4-2-1.38][22-4-4-1.37][23-4-3-0.33]
[24-4-4-0.89][25-4-3-1.18][26-4-3-0.77][27-4-3-1.08][28-4-1-0.15][29-4-1-0.40][30-4-3-0.35][31-4-2-0.75][32-4-1-1.05][33-4-3-1.26]
[34-4-3-0.67][35-4-3-2.43][37-4-3-0.92][39-4-0-2.49][40-4-0-0.18][41-4-2-0.42][42-4-3-2.50][43-4-2-1.03][45-4-1-0.72][46-4-4-0.89]
[47-4-4-1.85][48-4-1-1.17][51-4-1-0.49][52-4-2-0.77][53-4-1-0.74][54-4-3-0.44][55-4-3-1.30][56-4-1-0.82][57-4-3-2.07][58-4-2-2.05]
[59-4-0-0.74][60-4-3-0.17][61-4-4-0.97][62-4-3-0.27][63-4-2-1.06][64-4-1-0.63][65-4-1-1.23][66-4-1-3.21][67-4-3-0.44][68-4-1-1.03]
[69-4-3-0.66][70-4-2-0.50][72-4-1-1.40][73-4-2-0.22][74-4-2-0.79][75-4-3-1.00][77-4-1-1.71][78-4-2-0.99][79-4-2-0.78][80-4-1-0.87]
[81-4-4-2.03][82-4-1-0.97][83-4-1--0.07][84-4-0-0.60][85-4-1-0.59][86-4-2-0.60][87-4-4-0.73][88-4-1-1.00][89-4-3-1.29][90-4-0-0.42]
[91-4-2-0.35][92-4-3-0.35][93-4-0-0.85][94-4-1-1.18][95-4-3-0.71][96-4-1-1.64][97-4-0-0.47][98-4-2-1.24][99-4-4-0.10][100-4-2-0.90]
[101-4-4-0.08][102-4-2-0.64][103-4-3-1.60][104-4-4-1.63][105-4-1-1.97][106-4-1-1.56][107-4-1-1.64][108-4-2--0.01][109-4-3-0.53][110-4-4-0.61]
[111-4-0-2.28][112-4-2-0.76][113-4-3-1.16][114-4-3-1.24][115-4-0-0.74][116-4-1-0.73][117-4-1-0.84][119-4-2-1.35][121-4-1-1.69][122-4-3-1.61]
[124-4-3-0.99][125-4-1-1.77][126-4-3-0.19][127-4-2-1.63][128-4-2-0.16][129-4-1-0.59][130-4-2-0.85][131-4-3-1.54][132-4-1-1.10][133-4-4-0.69]
[135-4-2-0.58][136-4-0-0.30][137-4-0--0.02][138-4-3-0.35][139-4-3-0.91][140-4-3-0.12][141-4-3-1.91][142-4-2-0.98][143-4-4-0.98][144-4-4-1.76]
[145-4-1-1.48][148-4-0-2.63][149-4-3-0.39][150-4-2-1.54][151-4-2-0.97][152-4-2-1.50][153-4-1-1.81][154-4-1-0.43][155-4-3-0.13][156-4-3-0.97]
[157-4-0-0.24][158-4-3-1.81][160-4-1-0.79][161-4-2-0.36][162-4-1-0.15][164-4-2-0.45][165-4-2-0.41][167-4-0-1.00][168-4-3-0.03][170-4-0-2.10]
[171-4-3-0.05][172-4-1-0.90][173-4-0-2.13][174-4-0-2.02][175-4-1-0.71][177-4-0-1.82][178-4-4-1.49][179-4-1-0.50][180-4-4-0.92][181-4-3-2.48]
[182-4-3-1.32][183-4-3-0.13][184-4-2-0.86][186-4-3-0.95][187-4-1-1.52][188-4-2-1.06][189-4-2-0.43][190-4-1-0.09][191-4-3-0.50][192-4-2-0.33]
[193-4-1-2.21][194-4-3-1.12][195-4-0-0.49][196-4-3-0.38][197-4-2-2.22][198-4-4-1.44][199-4-3-0.77]
---------------------------
I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 0.918 | Acc: 53.125% | Wgt Acc: 63.344%
	I - Batch: 100 | Loss: 0.936 | Acc: 53.062% | Wgt Acc: 62.963%
	I - Batch: 150 | Loss: 0.962 | Acc: 52.000% | Wgt Acc: 61.977%
	I - Batch: 200 | Loss: 0.971 | Acc: 51.875% | Wgt Acc: 61.608%
	I - Batch: 250 | Loss: 0.963 | Acc: 52.425% | Wgt Acc: 62.173%
I - num batch: 273
I - Train -- Loss: 0.967 | Acc: 52.270% | Wgt Acc: 61.969% | LR: 1.000000e-03 | Dur: 165.12s
I - Confusion Matrix: [row->prediction - col->label]
[[536.  28.  36. 145. 181.]
 [ 39. 448. 139.  64. 172.]
 [ 62. 228. 757.  99. 446.]
 [153.  55.  91. 463. 125.]
 [  7.   1.   9.   2.  76.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.428 | Acc: 41.617% | Wgt Acc: 52.382% | Dur: 14.61s
I - Confusion Matrix: [row->prediction - col->label]
[[49.  2.  2.  9. 14.]
 [ 2. 47. 15. 18. 38.]
 [10. 22. 54. 22. 93.]
 [24.  3.  4. 37. 11.]
 [ 3.  4.  0.  0. 24.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 0.976 | Acc: 52.000% | Wgt Acc: 63.031%
	I - Batch: 100 | Loss: 0.960 | Acc: 52.188% | Wgt Acc: 62.646%
	I - Batch: 150 | Loss: 0.949 | Acc: 52.667% | Wgt Acc: 62.883%
	I - Batch: 200 | Loss: 0.950 | Acc: 52.719% | Wgt Acc: 62.940%
	I - Batch: 250 | Loss: 0.949 | Acc: 52.975% | Wgt Acc: 63.269%
I - num batch: 273
I - Train -- Loss: 0.955 | Acc: 52.682% | Wgt Acc: 62.893% | LR: 1.000000e-03 | Dur: 166.95s
I - Confusion Matrix: [row->prediction - col->label]
[[535.  30.  34. 146. 151.]
 [ 24. 464. 140.  52. 178.]
 [ 68. 215. 764.  93. 463.]
 [164.  47.  82. 477. 150.]
 [  6.   4.  12.   5.  58.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.392 | Acc: 42.604% | Wgt Acc: 55.253% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  1.  4. 15. 20.]
 [ 3. 33.  8.  3. 21.]
 [12. 33. 53. 10. 97.]
 [18.  9. 10. 57. 24.]
 [ 0.  2.  0.  1. 18.]]

I - Local maximum validation set accuracy:  42.60

I - Validation set results: 
[14-1-1-1.40][50-3-1-0.24][124-2-2-1.87][127-0-0-2.04][443-2-2-1.88][567-0-0-0.91][573-1-1-1.74][615-0-3-1.38][695-1-2-0.93][722-3-3-2.05]
[826-0-0-3.78][878-0-0-2.36][1103-0-0-0.31][1212-3-3-0.58][1368-0-0-1.60][2181-2-0-1.03][2476-2-2-0.93][2721-2-2-1.51][2818-1-1-0.50][2886-2-1-1.17]
[3231-2-2-2.77][3333-2-1-1.14][3482-2-2-1.27][3536-3-3-1.43][3625-1-1-2.69][3909-0-3-0.61][4035-0-3-2.15][4140-0-0-0.79][4214-1-3-1.27][4346-1-3-1.43]
[4581-2-2-3.08][4708-3-3-1.40][4838-3-3-0.32][4845-1-2-0.78][4868-0-0-2.81][4939-0-2-1.23][4984-2-2-1.33][5078-1-2-0.91][5396-0-0-5.10][5479-1-1-1.15]
[5717-0-0-1.31][5843-1-2-1.32][5949-3-0-1.39][5987-2-2-1.73][6014-3-3-1.20][6033-3-0-0.20][6313-0-3-1.75][6421-3-3-2.15][6500-1-3-0.39][6583-3-2-0.55]
[6683-3-3-0.72][6825-2-3-0.95][6998-3-3-0.57][7049-3-3-1.47][7517-1-2-2.09][7521-1-1-0.59][7528-1-3-1.48][7949-1-2-2.72][8135-1-0-1.95][8185-3-0-2.41]
[8269-3-2-0.82][8273-3-3-0.90][8543-3-0-4.38][8666-1-1-0.94][8672-0-0-4.10][8903-1-2-1.66][9001-2-1-1.35][9036-2-2-3.56][9281-3-2-0.34][9300-2-2-3.35]
[9571-0-3-1.06][9617-1-4-0.26][9644-2-2-2.63][9705-2-2-0.23][9801-0-3-1.12][9803-3-3-0.95][9865-3-3-2.68][9896-2-2-1.88][10314-1-2-1.42][10337-3-3-2.79]
[10403-0-2-0.28][10653-2-2-0.64][10704-2-1-1.18][10719-1-1-2.03][10727-1-2-0.89][10836-0-0-4.87][10969-2-0-0.79][11042-0-3-1.06][11088-1-2-2.18][11322-0-0-2.30]
[11398-2-2-1.23][11499-0-0-0.73][11502-3-3-0.52][11512-3-3-0.77][11608-1-2-2.10][11610-0-0-2.53][11692-0-3-1.15][11905-0-0-1.16][11993-1-2-1.68][12002-2-3-1.01]
[12052-0-0-0.72][12201-0-3-2.51][12235-2-1-1.40][12320-1-2-0.88][12377-2-2-1.68][12398-2-3-0.38][12503-1-2-2.26][12617-0-2-0.27][12685-3-3-0.58][12738-2-3-1.07]
[12742-2-2-1.93][12823-0-0-2.59][13110-1-1-1.16][13240-3-3-1.39][13253-1-1-1.87][13273-0-0-4.77][13634-1-2-1.02][13763-2-2-0.56][13905-3-3-0.47][14060-2-2-0.62]
[14065-3-3-1.03][14147-3-3-0.99][14595-2-2-1.93][14687-2-2-2.18][14788-2-2-2.07][14869-1-1-2.38][14872-3-4-0.42][14877-1-1-0.64][14927-0-3-0.93][15066-0-0-3.11]
[15175-1-1-0.77][15178-2-3-0.82][15375-3-3--0.08][15389-3-3-2.20][15568-2-1-1.62][15675-3-3-1.59][15869-1-2-1.01][16207-3-0-0.32][16236-0-2-0.69][16302-3-0-1.40]
[16331-2-2-2.10][16381-0-0-1.67][16488-1-1-2.59][16495-0-2-0.35][16650-0-0-3.55][16719-1-2-1.40][16801-0-0-3.24][16828-0-0-1.12][17137-3-3-0.90][17245-1-2-1.09]
[17278-3-0-0.13][17282-0-0-0.71][17311-2-2-1.92][17336-2-2-0.68][17608-3-3-3.04][17627-0-3-0.86][17877-3-1-1.49][17924-1-3-0.29][17984-3-0-2.79][18211-0-3-0.95]
[18276-3-3-1.37][18287-1-1-1.48][18394-0-0-2.25][18428-0-0-4.68][18442-0-3-2.10][18478-3-3-1.30][18607-0-0-0.18][18616-0-1-0.19][18663-0-0-0.68][18718-0-0-2.52]
[18766-2-2-2.19][18824-2-2-0.98][18890-3-2-0.79][18930-3-2-0.43][18938-3-3-1.44][19817-1-2-2.26][19839-0-2-0.93][19930-3-3-1.90][19944-0-2-0.73][20036-2-2-3.51]
[20101-3-3-1.33][20474-1-1-1.76][20547-3-3-0.69][20929-2-2-3.03][21245-1-2-1.32][21257-3-3-0.62][21293-1-1-1.68][21316-1-1-2.39][21384-1-1-1.33][21448-1-1-1.26]
[21483-0-0-1.83][21487-2-2-1.64][21714-0-0-0.27][21943-3-2-1.12][21947-0-3-1.18][21948-0-0-3.57][21965-2-2-4.35][21998-1-1-0.59][22025-0-2-1.00][22228-3-3-1.76]
[22446-1-2-1.34][22494-3-3-1.48][22757-0-0-2.19][22811-3-3-2.92][22976-3-2-0.84][22985-3-3-2.16][23014-0-0-2.22][23112-1-1-1.83][23144-3-3-2.81][23168-2-0-0.57]
[23219-0-3-0.99][23363-3-3-2.27][23470-0-1-0.13][23486-2-2-0.92][23497-0-0-3.03][23516-0-0-2.82][23690-1-2-0.98][23921-2-2-1.53][23936-1-2-1.51][24040-3-2-0.40]
[24111-1-4-1.40][24182-0-0-3.26][24238-3-3-2.40][24290-2-0-2.12][24345-0-2-0.60][24364-1-2-1.75][24427-3-3-1.15][24477-2-2-1.88][24495-2-1-0.56][24893-2-2-1.44]
[25012-1-2-0.91][25121-2-2-1.66][25165-3-3-0.98][25183-0-0-1.71][25297-3-3-1.03][25398-0-0-0.77][25574-2-2-1.39][25644-1-1-3.61][25718-1-3-0.32][25774-2-3-0.79]
[26032-3-0-1.95][26051-3-3-1.73][26120-0-0-1.26][26321-1-1-1.54][26732-1-1-1.36][26784-3-3-2.66][26827-3-3-1.30][26833-0-3-1.85][26838-2-1-0.58][26860-1-2-1.98]
[26948-0-0-1.56][27049-3-0-2.63][27098-1-3-1.02][27526-0-3-1.78][27639-3-3--0.06][27698-3-3-1.84][27772-0-0-1.15][27890-1-1-1.21][28040-0-2-0.99][28503-2-2-3.39]
[28577-1-1-2.06][28959-0-0-3.71][29198-3-3-0.53][29777-0-0-4.57][29877-2-2-0.59][30035-1-1-1.82][30098-0-0-1.06][30326-1-1-3.02][30572-2-2-3.19][30716-0-1-0.07]
[30806-2-3-1.23][30906-1-1-1.90][31007-0-0-1.11][31181-3-3-1.20][31238-0-3-0.98][31347-0-0-2.74][31422-2-2-1.58][31429-3-3-0.50][31431-0-0-1.46][31432-1-1-1.78]
[31477-0-0-3.07][31524-1-2-1.28][31597-1-2-2.62][31619-1-3-0.48][31701-0-0-1.27][31755-0-0-1.86][31854-3-3-1.20][32074-1-2-1.22][32078-3-3-1.56][32111-1-1-2.02]
[32127-1-2-2.91][32140-3-3-1.83][32263-2-2-0.26][32365-0-0-0.75][32411-2-3-2.37][32429-3-0-1.65][32473-3-0-0.90][32574-3-0-2.45][32584-0-0-1.61][32622-0-2-0.57]
[32858-3-0-1.54][32969-3-0-1.86][33016-2-2-2.28][33031-1-3-2.67][33035-2-2-2.95][33133-2-2-1.53][33173-2-2-0.24][33175-3-1-0.96][33306-3-2-1.11][33309-2-3-0.40]
[33474-0-2-0.24][33478-2-3-0.58][33618-1-1-1.74][33712-0-0-0.85][33782-2-2-1.55][33914-3-3-1.30][34076-3-3-0.75][34112-2-2-1.77][34138-2-2-1.08][34239-1-2-0.54]
[34364-2-2-2.78][34617-1-2-0.39][34751-3-3-1.62][34783-2-2-1.52][35015-3-2-0.96][35018-1-2-1.76][35288-2-2-0.51][0-4-2-1.85][1-4-0-0.87][2-4-4-0.01]
[3-4-2-0.58][4-4-2-0.59][5-4-1-1.25][6-4-0-0.19][7-4-2-0.23][8-4-2-0.47][9-4-2-0.25][10-4-3-0.31][11-4-2-2.38][12-4-2-1.16]
[14-4-3-1.26][15-4-3-2.09][16-4-2--0.08][17-4-1-0.89][18-4-2-1.23][19-4-0-2.28][20-4-0-0.73][21-4-2-2.89][22-4-1-0.50][23-4-0-0.25]
[24-4-4-1.30][25-4-3-1.02][26-4-3-0.45][27-4-3-0.54][28-4-2-0.39][29-4-1-0.79][30-4-2-0.81][31-4-2-1.10][32-4-1-1.43][33-4-2-1.43]
[34-4-3-0.65][35-4-3-1.03][37-4-2-0.63][39-4-0-2.58][40-4-0-0.07][41-4-2-0.46][42-4-2-0.74][43-4-2-2.19][45-4-3-0.31][46-4-2-1.32]
[47-4-2-2.01][48-4-2-1.20][51-4-2-1.34][52-4-2-1.51][53-4-1-0.33][54-4-3-0.74][55-4-3-1.07][56-4-2-1.90][57-4-3-0.59][58-4-2-1.78]
[59-4-4-0.21][60-4-1-0.41][61-4-2-0.49][62-4-3-0.96][63-4-2-1.15][64-4-1-0.80][65-4-2-1.09][66-4-1-1.61][67-4-2-0.73][68-4-1-0.65]
[69-4-3-0.81][70-4-2-1.11][72-4-2-0.71][73-4-1-0.83][74-4-2-1.45][75-4-2-0.59][77-4-2-1.65][78-4-2-0.83][79-4-2-1.91][80-4-1-0.41]
[81-4-4-1.62][82-4-1-1.07][83-4-2-0.59][84-4-2-0.42][85-4-2-0.73][86-4-2--0.11][87-4-4-1.27][88-4-4-0.59][89-4-2-1.09][90-4-2-0.50]
[91-4-2-1.36][92-4-2-0.98][93-4-0-1.99][94-4-4-1.51][95-4-2-0.85][96-4-1-1.66][97-4-2-0.92][98-4-2-1.50][99-4-4-0.90][100-4-2-1.61]
[101-4-2-1.33][102-4-2-1.43][103-4-3-0.89][104-4-4-1.18][105-4-2-2.01][106-4-1-1.36][107-4-4-0.34][108-4-2-0.19][109-4-3-0.28][110-4-2-1.08]
[111-4-0-2.75][112-4-2-1.55][113-4-2-1.17][114-4-0-0.48][115-4-2-0.23][116-4-2-0.66][117-4-2-1.27][119-4-2-2.52][121-4-2-0.99][122-4-3-1.03]
[124-4-2-0.96][125-4-1-1.30][126-4-0-0.21][127-4-2-2.55][128-4-2-0.64][129-4-1-0.29][130-4-2-1.33][131-4-3-1.03][132-4-3-0.31][133-4-0-2.75]
[135-4-2-0.81][136-4-0--0.04][137-4-1-0.39][138-4-2-0.33][139-4-2-0.43][140-4-2-0.59][141-4-0-1.21][142-4-4-1.77][143-4-4-1.07][144-4-4-1.07]
[145-4-2-2.30][148-4-0-2.51][149-4-2-0.68][150-4-2-1.54][151-4-2-1.32][152-4-2-1.39][153-4-2-1.67][154-4-1-2.09][155-4-4-0.61][156-4-3-0.59]
[157-4-0-0.54][158-4-3-1.09][160-4-2-1.13][161-4-1-1.83][162-4-2--0.07][164-4-2-0.61][165-4-2-0.74][167-4-0-1.09][168-4-4-0.14][170-4-3-1.64]
[171-4-2-0.83][172-4-2-0.98][173-4-2-0.64][174-4-0-2.93][175-4-1-0.73][177-4-0-2.17][178-4-2--0.03][179-4-2-0.36][180-4-4-1.29][181-4-2-0.61]
[182-4-2-0.90][183-4-4-0.19][184-4-2-1.46][186-4-3-0.54][187-4-2-0.78][188-4-2-1.53][189-4-2-0.33][190-4-2-0.98][191-4-2-0.92][192-4-3--0.04]
[193-4-2-2.48][194-4-2-1.99][195-4-0-0.43][196-4-2-1.25][197-4-2-1.10][198-4-4-1.78][199-4-2-1.17]
---------------------------
I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 0.912 | Acc: 55.875% | Wgt Acc: 66.080%
	I - Batch: 100 | Loss: 0.914 | Acc: 55.562% | Wgt Acc: 66.114%
	I - Batch: 150 | Loss: 0.907 | Acc: 55.333% | Wgt Acc: 66.189%
	I - Batch: 200 | Loss: 0.917 | Acc: 54.906% | Wgt Acc: 65.482%
	I - Batch: 250 | Loss: 0.915 | Acc: 55.325% | Wgt Acc: 65.681%
I - num batch: 273
I - Train -- Loss: 0.919 | Acc: 55.181% | Wgt Acc: 65.602% | LR: 1.000000e-03 | Dur: 166.45s
I - Confusion Matrix: [row->prediction - col->label]
[[561.  28.  29. 141. 155.]
 [ 25. 485. 104.  59. 151.]
 [ 60. 179. 804.  87. 465.]
 [145.  64.  87. 485. 157.]
 [  6.   4.   8.   1.  72.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.365 | Acc: 46.351% | Wgt Acc: 59.243% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  1.  2. 16. 16.]
 [ 6. 52. 13.  7. 48.]
 [ 6. 17. 47.  7. 71.]
 [20.  7. 12. 56. 21.]
 [ 0.  1.  1.  0. 24.]]

I - Local maximum validation set accuracy:  46.35

I - Validation set results: 
[14-1-1-1.13][50-3-1-0.11][124-2-2-1.87][127-0-0-3.19][443-2-2-2.09][567-0-0-1.16][573-1-1-2.58][615-0-3-1.68][695-1-2-1.32][722-3-3-1.78]
[826-0-0-3.32][878-0-0-3.82][1103-0-0-0.42][1212-3-3-1.58][1368-0-0-2.56][2181-2-3-0.72][2476-2-2-0.99][2721-2-2-1.59][2818-1-1-0.87][2886-2-1-0.88]
[3231-2-2-2.57][3333-2-2-0.56][3482-2-2-1.19][3536-3-3-1.20][3625-1-1-3.08][3909-0-0-0.59][4035-0-3-2.34][4140-0-0-0.16][4214-1-3-2.32][4346-1-3-1.12]
[4581-2-2-2.31][4708-3-3-1.34][4838-3-3-0.58][4845-1-1-1.10][4868-0-0-2.88][4939-0-1-0.58][4984-2-2-1.59][5078-1-2-0.31][5396-0-0-4.13][5479-1-1-1.44]
[5717-0-0-1.02][5843-1-1-1.05][5949-3-0-1.83][5987-2-1-1.48][6014-3-3-1.19][6033-3-0-0.58][6313-0-0-2.11][6421-3-3-2.13][6500-1-3-0.13][6583-3-2-1.20]
[6683-3-3-0.49][6825-2-3-0.71][6998-3-3-0.74][7049-3-3-1.90][7517-1-2-1.54][7521-1-1-1.03][7528-1-3-0.99][7949-1-2-1.75][8135-1-0-1.10][8185-3-0-2.47]
[8269-3-1-2.89][8273-3-3-1.15][8543-3-0-4.31][8666-1-1-2.02][8672-0-0-4.62][8903-1-1-0.69][9001-2-1-1.68][9036-2-2-3.77][9281-3-1-1.16][9300-2-2-3.80]
[9571-0-3-0.69][9617-1-1-0.41][9644-2-2-2.94][9705-2-2-0.61][9801-0-3-1.05][9803-3-3-1.27][9865-3-3-3.07][9896-2-2-1.13][10314-1-2-2.07][10337-3-3-3.82]
[10403-0-2-0.28][10653-2-1-0.96][10704-2-1-0.98][10719-1-1-2.53][10727-1-1-0.61][10836-0-0-5.03][10969-2-3-0.66][11042-0-3-1.06][11088-1-2-2.98][11322-0-0-2.15]
[11398-2-2-2.02][11499-0-0-0.29][11502-3-0-0.63][11512-3-2-0.72][11608-1-1-2.68][11610-0-0-1.19][11692-0-3-0.68][11905-0-0-3.01][11993-1-2-1.72][12002-2-3-0.26]
[12052-0-0-1.23][12201-0-3-2.80][12235-2-1-1.27][12320-1-2--0.12][12377-2-4-0.86][12398-2-3-0.41][12503-1-1-1.40][12617-0-3-0.50][12685-3-3-1.36][12738-2-3-0.25]
[12742-2-2-3.20][12823-0-0-2.75][13110-1-1-1.49][13240-3-3-1.67][13253-1-1-2.09][13273-0-0-4.72][13634-1-1-1.45][13763-2-3-1.00][13905-3-3-0.14][14060-2-1-1.15]
[14065-3-0-2.26][14147-3-3-1.16][14595-2-2-2.18][14687-2-2-2.90][14788-2-2-1.90][14869-1-1-3.81][14872-3-0--0.09][14877-1-1-2.22][14927-0-3-0.87][15066-0-0-3.81]
[15175-1-1-1.28][15178-2-2-0.04][15375-3-3-0.96][15389-3-3-1.80][15568-2-1-1.34][15675-3-3-1.28][15869-1-1-0.94][16207-3-3-0.13][16236-0-2-0.35][16302-3-3-0.97]
[16331-2-2-1.95][16381-0-0-1.79][16488-1-1-3.14][16495-0-0-2.33][16650-0-0-2.94][16719-1-1-1.55][16801-0-0-3.67][16828-0-0-1.52][17137-3-3-0.82][17245-1-1-0.18]
[17278-3-0-0.18][17282-0-0-0.50][17311-2-2-1.74][17336-2-1-2.32][17608-3-3-2.90][17627-0-3-0.72][17877-3-1-0.92][17924-1-2-0.83][17984-3-0-2.24][18211-0-3-1.41]
[18276-3-3-0.95][18287-1-1-2.16][18394-0-0-1.68][18428-0-0-2.47][18442-0-3-2.25][18478-3-3-0.97][18607-0-0-0.70][18616-0-1-0.23][18663-0-0-2.99][18718-0-0-2.79]
[18766-2-2-2.35][18824-2-2-2.00][18890-3-3-1.22][18930-3-3-0.38][18938-3-3-0.95][19817-1-2-1.25][19839-0-2-1.01][19930-3-3-1.82][19944-0-2-0.99][20036-2-2-3.51]
[20101-3-3-1.83][20474-1-1-2.47][20547-3-1--0.22][20929-2-1-2.41][21245-1-1-1.86][21257-3-3-0.31][21293-1-2-1.77][21316-1-1-3.95][21384-1-2-2.30][21448-1-1-0.95]
[21483-0-0-2.11][21487-2-2-1.66][21714-0-0-0.45][21943-3-2-0.76][21947-0-0-0.70][21948-0-0-4.96][21965-2-2-2.71][21998-1-1-1.06][22025-0-1-0.11][22228-3-3-2.17]
[22446-1-1-2.20][22494-3-0-1.53][22757-0-3-1.94][22811-3-3-3.06][22976-3-2-0.58][22985-3-3-2.79][23014-0-0-2.51][23112-1-1-1.25][23144-3-3-2.67][23168-2-3-0.70]
[23219-0-3-0.76][23363-3-3-1.60][23470-0-0-0.16][23486-2-3-0.69][23497-0-3-2.94][23516-0-0-3.33][23690-1-1-1.45][23921-2-2-2.00][23936-1-2-1.89][24040-3-1-0.51]
[24111-1-4-0.95][24182-0-3-3.15][24238-3-3-2.71][24290-2-0-2.11][24345-0-0-1.23][24364-1-2-1.19][24427-3-0-1.53][24477-2-2-2.78][24495-2-1-0.19][24893-2-2-1.34]
[25012-1-1-0.13][25121-2-2-1.99][25165-3-3-1.23][25183-0-0-0.54][25297-3-3-2.06][25398-0-0-1.45][25574-2-1-1.44][25644-1-1-3.31][25718-1-3-0.39][25774-2-2-0.97]
[26032-3-3-0.74][26051-3-3-1.91][26120-0-2-0.22][26321-1-1-1.59][26732-1-1-1.59][26784-3-3-2.73][26827-3-3-1.53][26833-0-3-1.65][26838-2-2-0.18][26860-1-2-1.35]
[26948-0-0-0.76][27049-3-0-1.46][27098-1-1-1.07][27526-0-0-2.31][27639-3-3-0.11][27698-3-3-2.56][27772-0-0-2.31][27890-1-1-1.88][28040-0-2-0.68][28503-2-2-3.76]
[28577-1-1-3.14][28959-0-0-3.85][29198-3-3-1.43][29777-0-0-4.74][29877-2-2-1.06][30035-1-1-2.63][30098-0-0-1.64][30326-1-1-2.46][30572-2-2-2.48][30716-0-1-0.27]
[30806-2-3-1.42][30906-1-1-2.05][31007-0-1--0.10][31181-3-2-0.24][31238-0-3-0.99][31347-0-0-1.46][31422-2-2-0.94][31429-3-3-0.59][31431-0-3-0.24][31432-1-1-2.13]
[31477-0-0-2.72][31524-1-2--0.09][31597-1-2-2.16][31619-1-3-0.05][31701-0-0-1.10][31755-0-0-0.94][31854-3-3-1.02][32074-1-1-0.50][32078-3-3-0.97][32111-1-1-1.21]
[32127-1-1-2.49][32140-3-3-2.30][32263-2-2-0.19][32365-0-0-0.96][32411-2-0-2.59][32429-3-0-1.93][32473-3-0-0.70][32574-3-3-2.27][32584-0-0-1.38][32622-0-1-1.44]
[32858-3-0-0.87][32969-3-0-2.93][33016-2-1-1.66][33031-1-3-2.74][33035-2-2-2.18][33133-2-2-2.27][33173-2-2-1.57][33175-3-1-1.00][33306-3-2-0.98][33309-2-3-0.08]
[33474-0-3-0.66][33478-2-3-0.24][33618-1-1-2.78][33712-0-0-0.65][33782-2-2-2.44][33914-3-3-1.79][34076-3-3-0.80][34112-2-2-1.50][34138-2-2-1.49][34239-1-1-1.09]
[34364-2-2-2.63][34617-1-1-0.35][34751-3-3-2.16][34783-2-2-1.38][35015-3-2-1.00][35018-1-1-1.92][35288-2-2-1.69][0-4-2-1.38][1-4-3-0.10][2-4-0-0.51]
[3-4-1-1.19][4-4-2-0.21][5-4-1-0.91][6-4-4-0.30][7-4-2-0.20][8-4-2-0.86][9-4-2-0.38][10-4-3-0.89][11-4-2-2.52][12-4-2-0.60]
[14-4-3-2.22][15-4-0-2.04][16-4-1-0.34][17-4-1-1.94][18-4-2-0.09][19-4-0-3.28][20-4-1-0.23][21-4-2-1.31][22-4-1-0.47][23-4-1--0.07]
[24-4-1-0.49][25-4-3-0.97][26-4-3-0.22][27-4-3-0.70][28-4-1-0.01][29-4-1-0.46][30-4-1-0.38][31-4-1-1.28][32-4-1-1.31][33-4-2-1.53]
[34-4-3-0.02][35-4-3-1.21][37-4-2-0.83][39-4-0-3.04][40-4-3--0.05][41-4-1-0.59][42-4-3-1.54][43-4-1-1.41][45-4-2-0.08][46-4-2-1.07]
[47-4-2-1.89][48-4-1-1.65][51-4-4-1.13][52-4-2-0.71][53-4-1-1.46][54-4-3-1.01][55-4-2-1.35][56-4-1-0.78][57-4-0-1.49][58-4-2-1.88]
[59-4-0-0.71][60-4-1-0.99][61-4-1-0.97][62-4-2-0.57][63-4-2-1.81][64-4-1-1.26][65-4-4-1.25][66-4-4-0.63][67-4-2-0.48][68-4-1-1.24]
[69-4-0-1.38][70-4-2-0.72][72-4-4-0.14][73-4-1-1.11][74-4-2-1.98][75-4-3-0.55][77-4-4-1.40][78-4-2-1.34][79-4-2-1.90][80-4-4-0.57]
[81-4-2-1.32][82-4-1-1.50][83-4-2-0.35][84-4-4-0.42][85-4-2-1.09][86-4-2-1.23][87-4-4-1.03][88-4-1-0.75][89-4-2-0.99][90-4-1-0.64]
[91-4-2-0.78][92-4-3-0.31][93-4-0-1.22][94-4-1-0.81][95-4-2-0.85][96-4-1-2.18][97-4-2-0.99][98-4-2-1.63][99-4-4-0.54][100-4-2-0.90]
[101-4-4-1.27][102-4-2-1.04][103-4-3-0.62][104-4-1-1.02][105-4-2-1.48][106-4-4-0.82][107-4-1-0.58][108-4-2-0.74][109-4-2-0.18][110-4-2-0.61]
[111-4-0-3.40][112-4-2-1.03][113-4-2-0.05][114-4-3-0.46][115-4-1-0.57][116-4-2-0.60][117-4-1-1.73][119-4-2-2.70][121-4-2-0.92][122-4-3-1.17]
[124-4-3-0.89][125-4-1-1.95][126-4-1-0.65][127-4-2-1.15][128-4-2-0.32][129-4-1-0.36][130-4-1-0.61][131-4-2-0.98][132-4-2-0.63][133-4-4-0.80]
[135-4-2-1.05][136-4-4--0.13][137-4-1-0.53][138-4-1-0.16][139-4-0-0.51][140-4-2-0.37][141-4-2-0.77][142-4-4-1.04][143-4-4-0.59][144-4-4-0.96]
[145-4-1-2.28][148-4-0-3.03][149-4-2-0.83][150-4-2-1.93][151-4-2-1.40][152-4-4-0.21][153-4-1-1.52][154-4-1-2.47][155-4-4-0.57][156-4-3-0.60]
[157-4-0-0.85][158-4-3-0.75][160-4-1-1.60][161-4-2-1.30][162-4-2-0.96][164-4-2-1.06][165-4-2-0.74][167-4-0-0.69][168-4-4-0.12][170-4-3-0.86]
[171-4-2-0.40][172-4-4-0.66][173-4-4--0.06][174-4-0-1.48][175-4-2-0.50][177-4-0-0.51][178-4-2-0.75][179-4-1--0.05][180-4-4-0.79][181-4-3-1.24]
[182-4-1-1.59][183-4-1-0.19][184-4-2-1.22][186-4-2--0.09][187-4-1-2.34][188-4-2-1.68][189-4-2-0.57][190-4-1-0.64][191-4-2-1.61][192-4-1-0.35]
[193-4-2-2.46][194-4-2-2.02][195-4-0-0.83][196-4-2-0.60][197-4-2-1.31][198-4-4-1.15][199-4-2-0.50]
---------------------------
I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 0.864 | Acc: 55.250% | Wgt Acc: 64.885%
	I - Batch: 100 | Loss: 0.854 | Acc: 57.188% | Wgt Acc: 66.924%
	I - Batch: 150 | Loss: 0.895 | Acc: 55.917% | Wgt Acc: 65.598%
	I - Batch: 200 | Loss: 0.889 | Acc: 55.812% | Wgt Acc: 65.504%
	I - Batch: 250 | Loss: 0.896 | Acc: 55.375% | Wgt Acc: 65.344%
I - num batch: 273
I - Train -- Loss: 0.894 | Acc: 55.227% | Wgt Acc: 65.140% | LR: 1.000000e-03 | Dur: 165.10s
I - Confusion Matrix: [row->prediction - col->label]
[[559.  31.  40. 150. 173.]
 [ 31. 490. 140.  58. 156.]
 [ 63. 190. 774.  74. 440.]
 [139.  46.  70. 489. 134.]
 [  5.   3.   8.   2.  97.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.572 | Acc: 39.645% | Wgt Acc: 51.586% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  3.  7. 10. 20.]
 [ 3. 30.  8. 12. 34.]
 [12. 38. 57. 19. 96.]
 [19.  5.  3. 45. 15.]
 [ 0.  2.  0.  0. 15.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 0.834 | Acc: 58.875% | Wgt Acc: 68.332%
	I - Batch: 100 | Loss: 0.818 | Acc: 59.062% | Wgt Acc: 69.517%
	I - Batch: 150 | Loss: 0.811 | Acc: 59.917% | Wgt Acc: 70.441%
	I - Batch: 200 | Loss: 0.818 | Acc: 59.625% | Wgt Acc: 69.776%
	I - Batch: 250 | Loss: 0.813 | Acc: 59.675% | Wgt Acc: 69.778%
I - num batch: 273
I - Train -- Loss: 0.817 | Acc: 59.354% | Wgt Acc: 69.496% | LR: 5.000000e-04 | Dur: 164.84s
I - Confusion Matrix: [row->prediction - col->label]
[[581.  29.  33. 137. 168.]
 [ 21. 549.  91.  59. 143.]
 [ 54. 140. 819.  63. 408.]
 [130.  36.  79. 512. 153.]
 [ 11.   6.  10.   2. 128.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.360 | Acc: 46.548% | Wgt Acc: 58.125% | Dur: 13.91s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  5.  8. 25. 29.]
 [ 0. 34.  4.  2. 13.]
 [ 4. 34. 57.  7. 83.]
 [17.  3.  6. 50. 26.]
 [ 1.  2.  0.  2. 29.]]

I - Local maximum validation set accuracy:  46.55

I - Validation set results: 
[14-1-2-1.38][50-3-3-0.06][124-2-2-2.81][127-0-0-3.67][443-2-2-1.76][567-0-0-1.64][573-1-1-2.61][615-0-3-0.84][695-1-2-1.51][722-3-3-2.29]
[826-0-0-2.65][878-0-0-4.16][1103-0-0-1.45][1212-3-3-0.52][1368-0-0-3.78][2181-2-0-1.06][2476-2-2-1.34][2721-2-2-2.75][2818-1-0-0.62][2886-2-2-1.06]
[3231-2-2-2.50][3333-2-2-1.22][3482-2-2-1.87][3536-3-3-0.68][3625-1-1-1.98][3909-0-0-2.17][4035-0-3-2.43][4140-0-0-0.78][4214-1-3-1.68][4346-1-0-1.88]
[4581-2-2-3.20][4708-3-3-0.59][4838-3-0-1.24][4845-1-2-0.78][4868-0-0-2.81][4939-0-0-0.41][4984-2-2-2.27][5078-1-2-0.41][5396-0-0-4.81][5479-1-1-0.94]
[5717-0-0-3.26][5843-1-2-1.13][5949-3-3-1.23][5987-2-2-1.67][6014-3-1-1.06][6033-3-0-1.17][6313-0-0-2.63][6421-3-3-1.87][6500-1-1-0.17][6583-3-2-1.74]
[6683-3-3-0.56][6825-2-3-0.55][6998-3-3-0.79][7049-3-3-0.96][7517-1-2-2.12][7521-1-0--0.11][7528-1-3-0.96][7949-1-2-2.41][8135-1-0-1.07][8185-3-0-3.49]
[8269-3-4-0.10][8273-3-3-1.10][8543-3-0-4.24][8666-1-1-2.17][8672-0-0-3.20][8903-1-2-2.31][9001-2-2-0.83][9036-2-2-2.53][9281-3-3-0.77][9300-2-2-4.45]
[9571-0-3-1.09][9617-1-1--0.22][9644-2-2-2.68][9705-2-0-0.17][9801-0-3-1.56][9803-3-0-0.83][9865-3-3-3.95][9896-2-2-2.03][10314-1-2-1.25][10337-3-3-4.47]
[10403-0-0-0.63][10653-2-2-0.10][10704-2-1-1.65][10719-1-1-2.37][10727-1-4-0.45][10836-0-0-6.83][10969-2-0-1.03][11042-0-3-1.27][11088-1-1-1.60][11322-0-0-2.58]
[11398-2-2-1.23][11499-0-0-1.95][11502-3-0-1.56][11512-3-3-1.02][11608-1-2-1.83][11610-0-0-1.78][11692-0-3-1.23][11905-0-0-2.86][11993-1-1-0.59][12002-2-3-0.91]
[12052-0-0-2.81][12201-0-3-2.73][12235-2-1-1.36][12320-1-0-0.48][12377-2-2-1.04][12398-2-2-0.02][12503-1-2-1.21][12617-0-3-1.22][12685-3-3-0.77][12738-2-3-0.51]
[12742-2-2-4.22][12823-0-0-3.62][13110-1-2-1.55][13240-3-0-1.66][13253-1-2-1.23][13273-0-0-5.61][13634-1-1-1.04][13763-2-3-1.48][13905-3-0-0.76][14060-2-1-1.96]
[14065-3-0-1.02][14147-3-3-0.76][14595-2-2-2.00][14687-2-2-3.30][14788-2-2-1.62][14869-1-1-3.01][14872-3-0-0.48][14877-1-1-0.72][14927-0-3-1.02][15066-0-0-4.83]
[15175-1-1-1.24][15178-2-3-0.43][15375-3-0-1.78][15389-3-3-2.27][15568-2-1-0.77][15675-3-3-0.98][15869-1-2-1.11][16207-3-0-0.79][16236-0-2-0.68][16302-3-0-1.04]
[16331-2-2-2.57][16381-0-3-1.31][16488-1-1-3.08][16495-0-0-4.00][16650-0-0-4.22][16719-1-2-1.04][16801-0-0-4.62][16828-0-0-1.74][17137-3-3-0.67][17245-1-2-0.39]
[17278-3-0-0.18][17282-0-0-2.01][17311-2-2-1.92][17336-2-2-1.25][17608-3-3-3.48][17627-0-0-0.18][17877-3-0--0.12][17924-1-2-0.81][17984-3-0-4.33][18211-0-3-1.59]
[18276-3-3-2.09][18287-1-1-0.85][18394-0-0-2.41][18428-0-0-5.31][18442-0-3-2.28][18478-3-0-1.11][18607-0-0-1.94][18616-0-0-0.37][18663-0-0-3.14][18718-0-0-3.21]
[18766-2-2-2.86][18824-2-2-1.12][18890-3-3-1.05][18930-3-3-0.58][18938-3-3-0.76][19817-1-2-1.42][19839-0-2-1.25][19930-3-3-1.84][19944-0-2-1.10][20036-2-2-4.11]
[20101-3-3-1.40][20474-1-1-1.64][20547-3-3-0.90][20929-2-2-2.36][21245-1-1-2.35][21257-3-2-0.15][21293-1-2-1.62][21316-1-1-0.55][21384-1-2-1.90][21448-1-1-0.90]
[21483-0-0-3.89][21487-2-2-1.83][21714-0-0-1.47][21943-3-2-0.55][21947-0-0-1.20][21948-0-0-6.16][21965-2-2-3.17][21998-1-1-0.57][22025-0-3-0.71][22228-3-3-2.63]
[22446-1-1-1.49][22494-3-3-2.70][22757-0-0-2.63][22811-3-3-3.47][22976-3-1-1.12][22985-3-3-3.62][23014-0-0-3.50][23112-1-1-1.31][23144-3-3-3.15][23168-2-0-1.68]
[23219-0-3-0.86][23363-3-3-4.08][23470-0-0-0.24][23486-2-2-0.78][23497-0-0-2.74][23516-0-0-2.93][23690-1-2--0.12][23921-2-2-1.28][23936-1-2-1.25][24040-3-2-0.39]
[24111-1-4-1.30][24182-0-0-4.53][24238-3-3-3.10][24290-2-0-3.05][24345-0-0-1.35][24364-1-2-0.72][24427-3-0-2.31][24477-2-2-3.30][24495-2-2-0.06][24893-2-2-1.14]
[25012-1-2-0.33][25121-2-2-3.14][25165-3-3-1.93][25183-0-0-1.41][25297-3-3-2.03][25398-0-0-2.19][25574-2-2-1.18][25644-1-2-3.17][25718-1-1-2.02][25774-2-2-0.36]
[26032-3-0-0.93][26051-3-3-2.56][26120-0-0-0.72][26321-1-1-2.49][26732-1-1-1.75][26784-3-3-4.16][26827-3-3-1.98][26833-0-3-1.71][26838-2-2--0.02][26860-1-2-1.25]
[26948-0-0-1.48][27049-3-0-3.05][27098-1-1-0.15][27526-0-0-2.61][27639-3-3-1.88][27698-3-3-2.55][27772-0-0-3.86][27890-1-1-1.11][28040-0-2-0.39][28503-2-2-4.50]
[28577-1-2-1.89][28959-0-0-4.22][29198-3-3-1.42][29777-0-0-4.46][29877-2-2-0.44][30035-1-1-2.76][30098-0-0-1.75][30326-1-1-2.75][30572-2-2-2.89][30716-0-4-0.43]
[30806-2-2-0.77][30906-1-1-0.99][31007-0-0-0.99][31181-3-3-0.57][31238-0-0-1.76][31347-0-0-2.51][31422-2-2-1.35][31429-3-3-0.98][31431-0-0-0.69][31432-1-1-1.89]
[31477-0-3-2.88][31524-1-2-0.91][31597-1-2-2.96][31619-1-2-0.42][31701-0-0-1.70][31755-0-0-1.75][31854-3-0-1.16][32074-1-2--0.24][32078-3-3-1.18][32111-1-1-0.65]
[32127-1-2-2.17][32140-3-3-2.25][32263-2-0-0.36][32365-0-0-3.19][32411-2-0-3.67][32429-3-0-1.68][32473-3-0-0.65][32574-3-0-2.38][32584-0-0-1.25][32622-0-3-0.17]
[32858-3-0-1.30][32969-3-3-2.30][33016-2-2-1.90][33031-1-3-2.79][33035-2-2-3.18][33133-2-2-1.77][33173-2-2-0.31][33175-3-4-0.51][33306-3-3-0.74][33309-2-0--0.15]
[33474-0-0-0.95][33478-2-3-0.43][33618-1-1-1.97][33712-0-0-1.37][33782-2-2-1.09][33914-3-2-1.10][34076-3-2-1.74][34112-2-2-2.36][34138-2-2-1.99][34239-1-1-0.93]
[34364-2-2-3.64][34617-1-2-1.46][34751-3-3-1.20][34783-2-2-1.28][35015-3-2-1.35][35018-1-2-2.47][35288-2-2-0.30][0-4-3-0.57][1-4-4-0.72][2-4-0-0.86]
[3-4-2-1.03][4-4-2-0.40][5-4-1-1.73][6-4-4-1.70][7-4-4-0.63][8-4-2-1.27][9-4-1-0.41][10-4-3-1.05][11-4-2-2.63][12-4-2-1.25]
[14-4-3-1.92][15-4-3-2.41][16-4-4--0.06][17-4-0-0.21][18-4-2-0.98][19-4-3-1.80][20-4-0-0.59][21-4-2-2.68][22-4-0-0.64][23-4-1-0.04]
[24-4-4-2.22][25-4-3-0.91][26-4-3-1.49][27-4-3-1.33][28-4-4-0.93][29-4-1-0.26][30-4-1-0.60][31-4-2-1.29][32-4-4-1.08][33-4-2-1.05]
[34-4-2-0.02][35-4-3-0.97][37-4-2-1.69][39-4-0-1.44][40-4-0-0.43][41-4-3--0.18][42-4-3-2.04][43-4-2-0.76][45-4-2-2.21][46-4-2-1.53]
[47-4-2-1.88][48-4-2-0.49][51-4-4-0.38][52-4-0-1.43][53-4-0-0.55][54-4-3-0.39][55-4-3-0.51][56-4-2-1.04][57-4-3-1.40][58-4-2-2.46]
[59-4-0-1.92][60-4-2-0.14][61-4-4-0.86][62-4-2-0.96][63-4-2-2.52][64-4-4-0.26][65-4-4-1.35][66-4-1-2.13][67-4-2-1.69][68-4-1-0.72]
[69-4-3-0.92][70-4-2-0.84][72-4-2-0.59][73-4-1-0.69][74-4-2-1.86][75-4-3-1.19][77-4-2-1.46][78-4-2-1.51][79-4-2-2.08][80-4-4-1.01]
[81-4-2-0.90][82-4-1-1.00][83-4-2-0.29][84-4-2-1.68][85-4-4-1.07][86-4-4--0.02][87-4-4-1.68][88-4-1-0.48][89-4-2-1.70][90-4-2-0.91]
[91-4-2-0.43][92-4-3-1.24][93-4-0-2.31][94-4-2-0.95][95-4-2-0.35][96-4-1-1.78][97-4-2-0.52][98-4-2-1.74][99-4-4-0.48][100-4-2-1.33]
[101-4-2-1.43][102-4-2-1.34][103-4-3-0.69][104-4-4-1.20][105-4-2-1.51][106-4-1-1.13][107-4-0-0.41][108-4-2-0.27][109-4-3-0.15][110-4-2-1.03]
[111-4-0-3.61][112-4-0-0.59][113-4-2-0.77][114-4-3-0.97][115-4-0--0.20][116-4-2-0.31][117-4-2-1.28][119-4-4-0.99][121-4-2-0.85][122-4-3-0.96]
[124-4-3-0.52][125-4-2-1.86][126-4-4-1.28][127-4-2-2.97][128-4-2-0.60][129-4-3-0.57][130-4-2-0.98][131-4-2-1.19][132-4-0-0.47][133-4-4-1.27]
[135-4-2-1.33][136-4-0-0.57][137-4-4--0.04][138-4-1-0.60][139-4-0-0.72][140-4-2-0.42][141-4-2-1.62][142-4-2-0.83][143-4-4-0.79][144-4-4-1.98]
[145-4-2-2.65][148-4-0-3.33][149-4-2-0.39][150-4-2-2.29][151-4-2-1.15][152-4-2-1.15][153-4-2-0.91][154-4-2-0.25][155-4-4-0.94][156-4-3-1.36]
[157-4-0-0.84][158-4-3-0.79][160-4-2-1.16][161-4-2-3.08][162-4-0--0.04][164-4-2-0.61][165-4-2-0.71][167-4-0-0.83][168-4-0-0.78][170-4-0-1.05]
[171-4-4-0.25][172-4-4-0.87][173-4-0-1.43][174-4-0-2.74][175-4-4-0.38][177-4-0-2.02][178-4-2-0.61][179-4-0-0.29][180-4-4-1.30][181-4-2-0.23]
[182-4-2-1.25][183-4-2-0.55][184-4-2-2.04][186-4-3--0.05][187-4-2-0.75][188-4-2-2.41][189-4-2-0.80][190-4-2-0.69][191-4-2-1.31][192-4-0-0.22]
[193-4-2-2.58][194-4-2-0.68][195-4-0-1.01][196-4-2-1.49][197-4-2-1.65][198-4-4-1.50][199-4-2-1.33]
---------------------------
I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 0.801 | Acc: 60.250% | Wgt Acc: 70.490%
	I - Batch: 100 | Loss: 0.777 | Acc: 61.375% | Wgt Acc: 71.902%
	I - Batch: 150 | Loss: 0.777 | Acc: 61.708% | Wgt Acc: 71.853%
	I - Batch: 200 | Loss: 0.773 | Acc: 61.750% | Wgt Acc: 72.205%
	I - Batch: 250 | Loss: 0.778 | Acc: 60.850% | Wgt Acc: 71.438%
I - num batch: 273
I - Train -- Loss: 0.771 | Acc: 61.050% | Wgt Acc: 71.735% | LR: 5.000000e-04 | Dur: 165.38s
I - Confusion Matrix: [row->prediction - col->label]
[[590.  19.  32. 121. 170.]
 [ 29. 554.  79.  43. 142.]
 [ 41. 142. 855.  62. 418.]
 [123.  33.  54. 543. 149.]
 [ 14.  12.  12.   4. 121.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.436 | Acc: 45.168% | Wgt Acc: 56.891% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  3. 11. 28. 45.]
 [ 1. 34.  5.  3. 18.]
 [ 7. 33. 50.  5. 67.]
 [11.  7.  9. 50. 24.]
 [ 0.  1.  0.  0. 26.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 0.806 | Acc: 59.750% | Wgt Acc: 68.939%
	I - Batch: 100 | Loss: 0.779 | Acc: 60.188% | Wgt Acc: 70.226%
	I - Batch: 150 | Loss: 0.774 | Acc: 60.917% | Wgt Acc: 70.742%
	I - Batch: 200 | Loss: 0.757 | Acc: 62.281% | Wgt Acc: 72.119%
	I - Batch: 250 | Loss: 0.755 | Acc: 62.425% | Wgt Acc: 72.439%
I - num batch: 273
I - Train -- Loss: 0.759 | Acc: 62.127% | Wgt Acc: 72.334% | LR: 5.000000e-04 | Dur: 164.21s
I - Confusion Matrix: [row->prediction - col->label]
[[598.  24.  27. 116. 156.]
 [ 22. 576.  82.  44. 164.]
 [ 38. 125. 840.  66. 392.]
 [130.  29.  71. 542. 134.]
 [  9.   6.  12.   5. 154.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.319 | Acc: 47.535% | Wgt Acc: 59.924% | Dur: 14.05s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  4. 15. 33.]
 [ 1. 39.  7.  4. 23.]
 [ 3. 29. 54.  6. 66.]
 [23.  5. 10. 59. 30.]
 [ 0.  0.  0.  2. 28.]]

I - Local maximum validation set accuracy:  47.53

I - Validation set results: 
[14-1-2-2.14][50-3-3--0.01][124-2-2-0.72][127-0-0-3.51][443-2-2-1.81][567-0-0-2.00][573-1-1-3.13][615-0-0-1.24][695-1-2-1.00][722-3-0-2.19]
[826-0-0-2.06][878-0-0-4.51][1103-0-0-1.03][1212-3-3-0.69][1368-0-0-5.05][2181-2-3-0.65][2476-2-2-1.29][2721-2-2-2.35][2818-1-0-0.76][2886-2-1-1.31]
[3231-2-2-2.77][3333-2-2-2.09][3482-2-2-1.99][3536-3-3-1.00][3625-1-1-3.80][3909-0-0-2.03][4035-0-3-0.96][4140-0-0-1.28][4214-1-2-0.84][4346-1-0-1.28]
[4581-2-2-3.30][4708-3-2-0.93][4838-3-0-1.64][4845-1-3-1.07][4868-0-0-2.74][4939-0-1-0.21][4984-2-2-2.38][5078-1-2--0.06][5396-0-0-4.33][5479-1-1-1.02]
[5717-0-0-0.78][5843-1-1-1.14][5949-3-3-1.83][5987-2-2-0.70][6014-3-3-1.19][6033-3-1--0.38][6313-0-3-2.28][6421-3-3-2.06][6500-1-1-0.51][6583-3-2-1.32]
[6683-3-3-1.21][6825-2-3-0.76][6998-3-3-0.40][7049-3-3-1.30][7517-1-1-2.39][7521-1-1--0.18][7528-1-3-1.13][7949-1-2-2.57][8135-1-0-0.60][8185-3-0-2.00]
[8269-3-1-4.06][8273-3-3-1.06][8543-3-0-4.32][8666-1-1-2.64][8672-0-0-3.89][8903-1-2-1.87][9001-2-1-2.67][9036-2-2-2.37][9281-3-3-0.93][9300-2-2-4.18]
[9571-0-3-0.98][9617-1-2--0.39][9644-2-2-3.05][9705-2-0-0.32][9801-0-3-0.25][9803-3-3-1.23][9865-3-3-3.65][9896-2-2-1.68][10314-1-2-1.76][10337-3-3-4.07]
[10403-0-0-0.95][10653-2-1--0.35][10704-2-2-1.52][10719-1-1-3.51][10727-1-2-1.27][10836-0-0-6.69][10969-2-3-1.33][11042-0-0-1.05][11088-1-1-3.67][11322-0-0-1.37]
[11398-2-2-0.91][11499-0-0-1.02][11502-3-3-1.32][11512-3-3-0.69][11608-1-1-2.68][11610-0-0-2.00][11692-0-3-0.92][11905-0-0-2.10][11993-1-1-2.56][12002-2-2-1.05]
[12052-0-0-2.87][12201-0-3-3.41][12235-2-2-1.36][12320-1-0-0.41][12377-2-2-0.79][12398-2-3-0.21][12503-1-2--0.31][12617-0-3-0.45][12685-3-3-1.26][12738-2-2-0.28]
[12742-2-2-3.36][12823-0-0-2.12][13110-1-2-0.39][13240-3-0-2.01][13253-1-1-2.92][13273-0-0-4.23][13634-1-1-1.47][13763-2-3-1.66][13905-3-3-0.06][14060-2-1-1.81]
[14065-3-0-1.24][14147-3-3-1.22][14595-2-2-2.30][14687-2-2-3.27][14788-2-2-1.34][14869-1-1-4.48][14872-3-4--0.03][14877-1-1-2.34][14927-0-2-0.13][15066-0-0-4.01]
[15175-1-1-0.75][15178-2-0-0.51][15375-3-3-0.66][15389-3-3-2.14][15568-2-1-1.37][15675-3-3-1.03][15869-1-2-1.04][16207-3-0-0.13][16236-0-3-0.65][16302-3-2-0.36]
[16331-2-2-2.98][16381-0-3-1.50][16488-1-1-3.85][16495-0-0-2.17][16650-0-0-3.47][16719-1-1-0.65][16801-0-0-4.41][16828-0-0-2.16][17137-3-3-0.19][17245-1-2-0.89]
[17278-3-0-0.20][17282-0-0-1.75][17311-2-2-2.38][17336-2-2-1.98][17608-3-3-2.78][17627-0-2-0.26][17877-3-1-0.87][17924-1-2-0.46][17984-3-0-3.94][18211-0-3-1.81]
[18276-3-3-1.94][18287-1-1-2.10][18394-0-0-2.67][18428-0-0-2.72][18442-0-3-1.96][18478-3-3-1.54][18607-0-0-1.56][18616-0-0-0.78][18663-0-0-2.91][18718-0-0-2.54]
[18766-2-2-2.61][18824-2-2-0.30][18890-3-3-1.46][18930-3-0-0.35][18938-3-3-1.19][19817-1-2-1.34][19839-0-2-1.30][19930-3-3-2.37][19944-0-0-0.89][20036-2-2-3.29]
[20101-3-3-0.34][20474-1-1-2.55][20547-3-3-0.70][20929-2-2-2.73][21245-1-2-2.19][21257-3-3-0.82][21293-1-2-2.34][21316-1-1-1.99][21384-1-2-0.95][21448-1-2-1.95]
[21483-0-0-3.65][21487-2-2-1.88][21714-0-0-0.72][21943-3-3-0.88][21947-0-0-0.93][21948-0-0-5.57][21965-2-2-2.30][21998-1-1-1.15][22025-0-3-0.54][22228-3-3-2.60]
[22446-1-1-3.70][22494-3-3-1.67][22757-0-3-1.35][22811-3-3-2.20][22976-3-1-0.86][22985-3-3-3.08][23014-0-0-2.92][23112-1-1-1.91][23144-3-3-2.95][23168-2-3-1.18]
[23219-0-3-1.11][23363-3-3-3.07][23470-0-0-0.03][23486-2-3-0.69][23497-0-0-2.08][23516-0-0-2.88][23690-1-1-1.21][23921-2-2-1.88][23936-1-2-2.44][24040-3-2-0.98]
[24111-1-2-0.72][24182-0-0-4.63][24238-3-3-2.58][24290-2-0-2.43][24345-0-0-1.30][24364-1-2-0.70][24427-3-3-1.55][24477-2-2-1.81][24495-2-2-0.46][24893-2-2-1.77]
[25012-1-2-1.03][25121-2-2-1.26][25165-3-3-2.56][25183-0-0-1.20][25297-3-3-2.20][25398-0-0-1.66][25574-2-2-1.53][25644-1-1-4.17][25718-1-3-0.03][25774-2-2-0.59]
[26032-3-0-1.06][26051-3-3-2.13][26120-0-0-0.83][26321-1-1-1.56][26732-1-1-2.37][26784-3-3-3.10][26827-3-3-1.06][26833-0-3-1.35][26838-2-2-0.58][26860-1-2-1.21]
[26948-0-0-1.76][27049-3-0-2.00][27098-1-2-0.35][27526-0-0-2.90][27639-3-3-1.56][27698-3-3-1.94][27772-0-0-1.04][27890-1-1-1.76][28040-0-3-0.23][28503-2-2-4.77]
[28577-1-1-2.72][28959-0-0-3.83][29198-3-3-2.08][29777-0-0-4.65][29877-2-2-1.57][30035-1-1-1.92][30098-0-0-1.39][30326-1-1-4.76][30572-2-2-2.72][30716-0-3--0.13]
[30806-2-3-0.83][30906-1-1-2.00][31007-0-0-2.13][31181-3-3-0.60][31238-0-3-1.19][31347-0-0-2.81][31422-2-2-2.06][31429-3-3-1.51][31431-0-3-0.69][31432-1-1-2.62]
[31477-0-3-2.34][31524-1-0-0.19][31597-1-2-2.08][31619-1-3-1.28][31701-0-0-1.53][31755-0-0-1.87][31854-3-3-1.42][32074-1-2-1.08][32078-3-3-0.69][32111-1-1-2.51]
[32127-1-2-1.14][32140-3-3-2.57][32263-2-0-0.34][32365-0-0-2.17][32411-2-3-2.32][32429-3-0-1.61][32473-3-0-0.87][32574-3-3-2.55][32584-0-3-0.61][32622-0-3-0.44]
[32858-3-0-1.42][32969-3-3-2.73][33016-2-2-2.91][33031-1-3-3.66][33035-2-2-2.96][33133-2-2-2.24][33173-2-1-0.63][33175-3-4-0.25][33306-3-2-0.77][33309-2-3-0.04]
[33474-0-3-0.86][33478-2-1-0.38][33618-1-1-2.65][33712-0-0-1.35][33782-2-2-1.09][33914-3-3-1.58][34076-3-2-1.04][34112-2-2-2.41][34138-2-2-1.67][34239-1-1-1.68]
[34364-2-2-2.54][34617-1-2-0.42][34751-3-3-2.07][34783-2-2-1.75][35015-3-3-0.99][35018-1-1-1.51][35288-2-2-1.30][0-4-2-1.64][1-4-0-0.82][2-4-0-1.20]
[3-4-1--0.20][4-4-0--0.36][5-4-1-2.99][6-4-4-0.22][7-4-4-1.42][8-4-2-0.64][9-4-4--0.19][10-4-3-0.66][11-4-2-2.53][12-4-1-1.32]
[14-4-3-1.64][15-4-3-2.05][16-4-0--0.09][17-4-3--0.15][18-4-4-0.66][19-4-0-2.61][20-4-0-0.08][21-4-2-2.35][22-4-4-0.21][23-4-0--0.26]
[24-4-4-1.12][25-4-2-1.03][26-4-3-0.90][27-4-2-1.22][28-4-0-0.79][29-4-3-0.62][30-4-0-0.41][31-4-2-0.31][32-4-1-1.91][33-4-2-1.14]
[34-4-2-0.44][35-4-3-1.34][37-4-3-0.94][39-4-0-1.23][40-4-4-0.55][41-4-3-0.37][42-4-3-0.69][43-4-2-2.90][45-4-3-0.56][46-4-2-1.08]
[47-4-4-1.59][48-4-1-1.55][51-4-2-0.27][52-4-2-0.70][53-4-0-0.26][54-4-3-1.00][55-4-3-0.87][56-4-2-1.43][57-4-3-0.89][58-4-1-2.18]
[59-4-0-2.48][60-4-3--0.16][61-4-4-0.71][62-4-2-0.85][63-4-2-1.90][64-4-4--0.11][65-4-1-0.96][66-4-1-0.76][67-4-2-1.01][68-4-3-1.49]
[69-4-3-1.26][70-4-0--0.05][72-4-2-1.09][73-4-1-1.35][74-4-2-1.72][75-4-0-1.04][77-4-2-1.50][78-4-2--0.03][79-4-2-1.02][80-4-4-0.53]
[81-4-2-1.02][82-4-1-1.17][83-4-4--0.26][84-4-4-0.51][85-4-4-1.08][86-4-2-1.34][87-4-4-1.57][88-4-1-0.20][89-4-2-0.37][90-4-3--0.28]
[91-4-2-1.52][92-4-3-0.43][93-4-0-2.28][94-4-2-0.78][95-4-3--0.05][96-4-1-1.92][97-4-2-0.87][98-4-2-1.93][99-4-2-0.31][100-4-2-1.53]
[101-4-4-0.63][102-4-2-0.21][103-4-2-0.36][104-4-4-0.68][105-4-2-1.14][106-4-1-1.18][107-4-0-0.92][108-4-2-0.75][109-4-3-0.47][110-4-3-1.15]
[111-4-0-1.77][112-4-0-1.18][113-4-2-0.64][114-4-3-1.26][115-4-0-0.39][116-4-0--0.11][117-4-1-1.08][119-4-2-2.94][121-4-4--0.07][122-4-0-0.81]
[124-4-2-0.89][125-4-1-1.06][126-4-4-0.56][127-4-2-1.42][128-4-0-0.55][129-4-1-0.41][130-4-2-0.85][131-4-3-1.12][132-4-2-0.44][133-4-4-1.28]
[135-4-2-1.47][136-4-1-0.12][137-4-3--0.31][138-4-1-0.45][139-4-2-0.81][140-4-2-1.05][141-4-3-0.99][142-4-2-0.29][143-4-2-1.16][144-4-4-1.58]
[145-4-2-0.89][148-4-0-3.54][149-4-2-0.82][150-4-2-1.85][151-4-2-0.84][152-4-3-0.62][153-4-2-1.45][154-4-2-0.32][155-4-4-0.62][156-4-3-1.37]
[157-4-0-1.44][158-4-3-0.95][160-4-1-1.40][161-4-2-1.51][162-4-0-0.26][164-4-2-1.06][165-4-2-0.72][167-4-0-1.49][168-4-2-0.47][170-4-0-1.01]
[171-4-4-0.05][172-4-4-0.28][173-4-1-0.28][174-4-0-2.14][175-4-2-0.54][177-4-0-1.23][178-4-2-0.27][179-4-0-0.20][180-4-4-0.87][181-4-3-0.64]
[182-4-1-0.80][183-4-2--0.24][184-4-4-1.03][186-4-0-0.60][187-4-1-0.22][188-4-2-2.30][189-4-4-1.07][190-4-0-0.17][191-4-2-0.63][192-4-2-1.05]
[193-4-1-3.69][194-4-2-2.56][195-4-0-0.92][196-4-2-1.56][197-4-2-0.51][198-4-4-1.61][199-4-2-0.01]
---------------------------
I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 0.738 | Acc: 62.500% | Wgt Acc: 73.896%
	I - Batch: 100 | Loss: 0.723 | Acc: 64.812% | Wgt Acc: 74.627%
	I - Batch: 150 | Loss: 0.722 | Acc: 64.083% | Wgt Acc: 74.585%
	I - Batch: 200 | Loss: 0.721 | Acc: 64.500% | Wgt Acc: 74.895%
	I - Batch: 250 | Loss: 0.723 | Acc: 64.525% | Wgt Acc: 74.775%
I - num batch: 273
I - Train -- Loss: 0.723 | Acc: 64.420% | Wgt Acc: 74.706% | LR: 5.000000e-04 | Dur: 165.83s
I - Confusion Matrix: [row->prediction - col->label]
[[617.  24.  25. 104. 151.]
 [ 24. 583.  70.  41. 150.]
 [ 47. 108. 874.  61. 366.]
 [101.  38.  52. 563. 160.]
 [  8.   7.  11.   4. 173.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.464 | Acc: 48.323% | Wgt Acc: 58.759% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  9. 10. 32. 54.]
 [ 0. 32.  1.  2. 15.]
 [ 3. 23. 55.  4. 49.]
 [11. 12.  8. 47. 25.]
 [ 0.  2.  1.  1. 37.]]

I - Local maximum validation set accuracy:  48.32

I - Validation set results: 
[14-1-2-1.08][50-3-3-0.66][124-2-2-0.28][127-0-0-5.21][443-2-2-1.81][567-0-0-3.07][573-1-1-1.98][615-0-0-1.84][695-1-2-0.99][722-3-0-3.10]
[826-0-0-4.42][878-0-0-3.55][1103-0-0-1.72][1212-3-3-0.30][1368-0-0-5.09][2181-2-3-0.83][2476-2-2-0.79][2721-2-2-2.96][2818-1-3-0.40][2886-2-2-1.40]
[3231-2-2-2.90][3333-2-2-2.05][3482-2-2-1.32][3536-3-3-0.74][3625-1-1-1.90][3909-0-0-3.41][4035-0-0-2.30][4140-0-0-2.41][4214-1-3-3.07][4346-1-0-2.71]
[4581-2-2-2.49][4708-3-3-1.30][4838-3-0-2.06][4845-1-3-1.43][4868-0-0-3.62][4939-0-0-0.18][4984-2-2-2.29][5078-1-3-0.31][5396-0-0-6.35][5479-1-1-1.02]
[5717-0-0-1.54][5843-1-1-0.92][5949-3-0-1.85][5987-2-2-1.16][6014-3-3-1.85][6033-3-0-0.79][6313-0-0-2.62][6421-3-3-2.13][6500-1-2-0.32][6583-3-2-1.29]
[6683-3-3-1.37][6825-2-0-2.09][6998-3-3-1.87][7049-3-3-1.59][7517-1-1-1.04][7521-1-0-0.70][7528-1-2-1.07][7949-1-2-1.82][8135-1-0-1.46][8185-3-0-4.71]
[8269-3-4-0.02][8273-3-0-1.08][8543-3-0-5.72][8666-1-1-2.06][8672-0-0-2.99][8903-1-2-2.69][9001-2-2-0.98][9036-2-2-2.39][9281-3-3-1.12][9300-2-2-3.74]
[9571-0-0-1.65][9617-1-1-0.14][9644-2-2-2.41][9705-2-0-1.34][9801-0-0-1.76][9803-3-0-2.50][9865-3-3-3.41][9896-2-2-1.54][10314-1-1-0.24][10337-3-3-3.86]
[10403-0-0-0.80][10653-2-2--0.10][10704-2-2-0.21][10719-1-1-2.94][10727-1-4-0.11][10836-0-0-8.12][10969-2-0-2.29][11042-0-3-1.18][11088-1-1-2.64][11322-0-0-2.24]
[11398-2-2-2.65][11499-0-0-2.80][11502-3-3-1.95][11512-3-3-1.83][11608-1-1-1.72][11610-0-0-4.15][11692-0-0-1.77][11905-0-0-3.30][11993-1-1-2.92][12002-2-2-0.52]
[12052-0-0-4.08][12201-0-3-3.85][12235-2-2-2.40][12320-1-0-0.66][12377-2-4-0.55][12398-2-2-0.43][12503-1-2-0.70][12617-0-3-0.82][12685-3-3-0.89][12738-2-3-0.54]
[12742-2-2-4.11][12823-0-0-3.96][13110-1-2-1.63][13240-3-0-2.29][13253-1-1-1.90][13273-0-0-7.59][13634-1-1-1.23][13763-2-3-1.59][13905-3-0-1.64][14060-2-2-1.19]
[14065-3-0-1.91][14147-3-3-2.30][14595-2-2-2.06][14687-2-2-3.53][14788-2-2-1.59][14869-1-1-3.68][14872-3-0-1.46][14877-1-1-1.38][14927-0-3-0.64][15066-0-0-5.70]
[15175-1-1-0.21][15178-2-3-1.05][15375-3-3-0.92][15389-3-0-2.75][15568-2-1-2.07][15675-3-3-2.93][15869-1-3-0.56][16207-3-0-1.49][16236-0-2-0.51][16302-3-0-1.19]
[16331-2-2-2.80][16381-0-0-2.64][16488-1-1-2.51][16495-0-0-2.21][16650-0-0-4.45][16719-1-2-0.94][16801-0-0-6.56][16828-0-0-3.47][17137-3-0-2.34][17245-1-3-0.51]
[17278-3-0-0.84][17282-0-0-2.45][17311-2-2-1.59][17336-2-2-1.72][17608-3-3-2.73][17627-0-0-0.60][17877-3-0-0.63][17924-1-0--0.30][17984-3-0-4.98][18211-0-3-1.87]
[18276-3-3-2.54][18287-1-0-0.42][18394-0-0-4.32][18428-0-0-7.20][18442-0-3-2.15][18478-3-3-1.56][18607-0-0-3.33][18616-0-0-1.15][18663-0-0-3.66][18718-0-0-3.73]
[18766-2-2-2.15][18824-2-2-1.26][18890-3-3-2.13][18930-3-0-1.50][18938-3-3-1.47][19817-1-2-1.74][19839-0-0-0.64][19930-3-3-2.27][19944-0-2-0.60][20036-2-2-3.57]
[20101-3-0-2.03][20474-1-1-1.89][20547-3-3-0.72][20929-2-2-3.32][21245-1-2-1.15][21257-3-2-0.68][21293-1-2-2.44][21316-1-1-4.14][21384-1-2-1.93][21448-1-2-1.66]
[21483-0-0-5.28][21487-2-2-3.21][21714-0-0-1.37][21943-3-3-1.45][21947-0-0-1.50][21948-0-0-6.44][21965-2-2-2.00][21998-1-1-1.38][22025-0-3-0.97][22228-3-3-3.11]
[22446-1-1-1.57][22494-3-3-1.36][22757-0-0-4.37][22811-3-3-2.10][22976-3-1-0.41][22985-3-3-3.43][23014-0-0-4.01][23112-1-1-1.08][23144-3-3-3.40][23168-2-0-2.85]
[23219-0-3-1.29][23363-3-3-3.57][23470-0-0-1.29][23486-2-2-1.08][23497-0-0-4.49][23516-0-0-4.20][23690-1-2-0.44][23921-2-2-1.89][23936-1-2-1.56][24040-3-0-0.56]
[24111-1-4-0.77][24182-0-0-4.86][24238-3-3-3.06][24290-2-0-3.21][24345-0-0-1.01][24364-1-3-1.34][24427-3-0-2.49][24477-2-2-2.27][24495-2-0-0.15][24893-2-2-1.77]
[25012-1-3-0.32][25121-2-2-0.62][25165-3-3-2.82][25183-0-0-1.56][25297-3-3-2.49][25398-0-0-2.07][25574-2-2-1.40][25644-1-2-3.71][25718-1-3-1.46][25774-2-3-0.96]
[26032-3-0-3.58][26051-3-3-2.20][26120-0-0-1.74][26321-1-1-1.11][26732-1-1-0.40][26784-3-3-3.93][26827-3-3-1.55][26833-0-3-1.55][26838-2-3-0.61][26860-1-2-0.99]
[26948-0-0-2.79][27049-3-0-3.66][27098-1-0-1.45][27526-0-0-3.72][27639-3-3-2.47][27698-3-0-3.21][27772-0-0-2.49][27890-1-1-1.02][28040-0-2-0.49][28503-2-2-4.48]
[28577-1-2-0.54][28959-0-0-5.00][29198-3-3-2.87][29777-0-0-6.17][29877-2-2-1.60][30035-1-1-2.07][30098-0-0-2.54][30326-1-1-2.85][30572-2-2-3.41][30716-0-0-0.68]
[30806-2-3-1.28][30906-1-1-0.77][31007-0-0-1.57][31181-3-0-2.33][31238-0-0-2.93][31347-0-0-4.41][31422-2-0-0.85][31429-3-3-1.34][31431-0-0-3.54][31432-1-0-0.38]
[31477-0-0-3.14][31524-1-2-1.47][31597-1-2-2.34][31619-1-3-1.16][31701-0-0-3.65][31755-0-0-2.67][31854-3-0-2.52][32074-1-2--0.21][32078-3-3-2.40][32111-1-0-1.15]
[32127-1-2-2.44][32140-3-3-3.01][32263-2-0-0.79][32365-0-0-3.77][32411-2-0-4.36][32429-3-0-2.10][32473-3-0-1.46][32574-3-0-4.93][32584-0-0-1.49][32622-0-3-1.34]
[32858-3-0-2.48][32969-3-3-3.34][33016-2-2-2.69][33031-1-3-4.12][33035-2-2-2.82][33133-2-2-0.89][33173-2-2-0.47][33175-3-1-0.79][33306-3-3-0.34][33309-2-0-0.93]
[33474-0-3-1.08][33478-2-3-1.14][33618-1-1-0.90][33712-0-0-2.23][33782-2-2-1.50][33914-3-3-1.99][34076-3-2-1.13][34112-2-2-2.34][34138-2-2-1.29][34239-1-1-1.56]
[34364-2-2-3.23][34617-1-3-2.03][34751-3-3-2.70][34783-2-2-0.83][35015-3-2-1.58][35018-1-1-1.06][35288-2-2-0.69][0-4-2-2.18][1-4-4-0.86][2-4-0-0.68]
[3-4-0-0.01][4-4-1-0.31][5-4-1-1.49][6-4-4-0.14][7-4-0-0.10][8-4-2-0.62][9-4-2-0.63][10-4-3-1.35][11-4-2-3.57][12-4-2-1.42]
[14-4-0-2.02][15-4-3-2.70][16-4-4-0.32][17-4-0-0.06][18-4-2-0.89][19-4-0-4.16][20-4-0-2.21][21-4-2-1.11][22-4-4-0.48][23-4-0-1.31]
[24-4-4-2.16][25-4-3-1.00][26-4-3-0.81][27-4-2-1.60][28-4-4-1.23][29-4-2--0.41][30-4-0-0.67][31-4-2-1.20][32-4-1-0.63][33-4-2-2.11]
[34-4-0-0.52][35-4-3-1.55][37-4-0-0.75][39-4-0-4.08][40-4-0-0.56][41-4-3-0.55][42-4-0--0.20][43-4-1-1.55][45-4-2-0.97][46-4-4-0.77]
[47-4-4-1.83][48-4-1-0.14][51-4-0-0.23][52-4-0-1.56][53-4-0-0.72][54-4-3-1.39][55-4-2-0.90][56-4-3-0.18][57-4-0-1.91][58-4-2-2.11]
[59-4-0-2.02][60-4-3-0.72][61-4-4-0.67][62-4-2-0.67][63-4-2-1.77][64-4-0-0.92][65-4-4-1.47][66-4-4-0.66][67-4-1-0.77][68-4-1-1.32]
[69-4-0-1.12][70-4-2-1.09][72-4-1-0.06][73-4-0-0.47][74-4-2-2.29][75-4-0-0.89][77-4-2-0.68][78-4-0-0.42][79-4-2-1.34][80-4-4-1.23]
[81-4-2-1.30][82-4-1-0.48][83-4-4-0.43][84-4-4-1.23][85-4-4-1.40][86-4-1-0.69][87-4-4-0.86][88-4-4-0.65][89-4-2-2.13][90-4-0-1.43]
[91-4-2-1.39][92-4-0-0.19][93-4-0-2.22][94-4-2-1.05][95-4-2-0.10][96-4-1-0.82][97-4-1-0.68][98-4-2-1.73][99-4-4-0.22][100-4-2-1.09]
[101-4-4-1.12][102-4-2-1.23][103-4-0-1.38][104-4-4-1.13][105-4-4-1.04][106-4-4-0.66][107-4-0-1.60][108-4-3-0.28][109-4-3-0.28][110-4-3-1.23]
[111-4-0-3.03][112-4-0-1.46][113-4-3-0.20][114-4-3-1.55][115-4-0-1.11][116-4-0-0.65][117-4-4-0.65][119-4-2-0.79][121-4-0-0.26][122-4-4-0.56]
[124-4-3-0.88][125-4-4-1.32][126-4-4-0.30][127-4-2-0.66][128-4-3-0.32][129-4-3-1.75][130-4-2-0.66][131-4-2-1.37][132-4-0-2.03][133-4-4-1.25]
[135-4-2-1.14][136-4-0-0.94][137-4-3-0.52][138-4-0-0.79][139-4-0-0.36][140-4-2--0.12][141-4-0-1.49][142-4-4-0.92][143-4-4-0.83][144-4-4-1.23]
[145-4-2-1.63][148-4-0-5.31][149-4-2-0.98][150-4-2-3.07][151-4-2-0.25][152-4-3-1.51][153-4-2-2.54][154-4-4-0.56][155-4-4-0.63][156-4-3-1.46]
[157-4-0-3.22][158-4-0-0.77][160-4-1-0.74][161-4-2-0.75][162-4-0-0.79][164-4-3-0.58][165-4-0-0.41][167-4-0-2.11][168-4-0-1.18][170-4-0-2.82]
[171-4-2-0.78][172-4-4-0.58][173-4-0-2.01][174-4-0-4.42][175-4-4-0.25][177-4-0-0.31][178-4-0-0.52][179-4-3-0.56][180-4-4-0.90][181-4-3-1.95]
[182-4-2-0.63][183-4-4-0.21][184-4-2-0.75][186-4-0-0.81][187-4-2-0.58][188-4-2-1.63][189-4-1-1.24][190-4-3-0.30][191-4-2-0.69][192-4-2-0.28]
[193-4-1-1.84][194-4-3-0.83][195-4-0-0.10][196-4-2-1.42][197-4-2-0.46][198-4-4-1.77][199-4-0-0.18]
---------------------------
I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 0.691 | Acc: 65.375% | Wgt Acc: 76.072%
	I - Batch: 100 | Loss: 0.683 | Acc: 66.312% | Wgt Acc: 76.532%
	I - Batch: 150 | Loss: 0.688 | Acc: 65.917% | Wgt Acc: 76.239%
	I - Batch: 200 | Loss: 0.692 | Acc: 65.781% | Wgt Acc: 75.713%
	I - Batch: 250 | Loss: 0.695 | Acc: 65.725% | Wgt Acc: 75.856%
I - num batch: 273
I - Train -- Loss: 0.694 | Acc: 65.566% | Wgt Acc: 75.807% | LR: 5.000000e-04 | Dur: 165.12s
I - Confusion Matrix: [row->prediction - col->label]
[[619.  19.  24. 111. 165.]
 [ 22. 603.  65.  42. 132.]
 [ 41.  99. 891.  53. 370.]
 [105.  31.  39. 561. 147.]
 [ 10.   8.  13.   6. 186.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.280 | Acc: 48.323% | Wgt Acc: 56.983% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[44.  0.  3.  7. 12.]
 [ 4. 42. 17.  7. 29.]
 [ 8. 22. 47.  7. 66.]
 [29. 10.  6. 64. 25.]
 [ 3.  4.  2.  1. 48.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 0.624 | Acc: 63.500% | Wgt Acc: 76.443%
	I - Batch: 100 | Loss: 0.661 | Acc: 64.250% | Wgt Acc: 75.291%
	I - Batch: 150 | Loss: 0.655 | Acc: 65.125% | Wgt Acc: 75.645%
	I - Batch: 200 | Loss: 0.652 | Acc: 65.500% | Wgt Acc: 76.193%
	I - Batch: 250 | Loss: 0.650 | Acc: 65.950% | Wgt Acc: 76.449%
I - num batch: 273
I - Train -- Loss: 0.657 | Acc: 65.589% | Wgt Acc: 76.072% | LR: 5.000000e-04 | Dur: 163.96s
I - Confusion Matrix: [row->prediction - col->label]
[[617.  22.  26. 100. 167.]
 [ 24. 610.  64.  32. 139.]
 [ 41.  95. 880.  57. 360.]
 [103.  27.  47. 577. 157.]
 [ 12.   6.  15.   7. 177.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.273 | Acc: 48.323% | Wgt Acc: 56.429% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[51.  1.  2. 13.  7.]
 [ 7. 42. 12. 11. 19.]
 [ 7. 28. 54. 11. 89.]
 [16.  4.  6. 49. 16.]
 [ 7.  3.  1.  2. 49.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 0.598 | Acc: 68.625% | Wgt Acc: 80.093%
	I - Batch: 100 | Loss: 0.610 | Acc: 67.938% | Wgt Acc: 78.739%
	I - Batch: 150 | Loss: 0.611 | Acc: 68.458% | Wgt Acc: 79.091%
	I - Batch: 200 | Loss: 0.614 | Acc: 67.938% | Wgt Acc: 78.417%
	I - Batch: 250 | Loss: 0.622 | Acc: 67.575% | Wgt Acc: 77.897%
I - num batch: 273
I - Train -- Loss: 0.622 | Acc: 67.813% | Wgt Acc: 78.083% | LR: 5.000000e-04 | Dur: 163.24s
I - Confusion Matrix: [row->prediction - col->label]
[[628.  14.  18.  97. 148.]
 [ 17. 645.  56.  40. 140.]
 [ 38.  70. 893.  49. 373.]
 [106.  23.  51. 583. 130.]
 [  8.   8.  14.   4. 209.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.294 | Acc: 50.099% | Wgt Acc: 59.589% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  6. 15. 38.]
 [ 0. 43. 12.  6. 19.]
 [ 5. 21. 49.  8. 56.]
 [19.  5.  6. 55. 21.]
 [ 3.  4.  2.  2. 46.]]

I - Local maximum validation set accuracy:  50.10

I - Validation set results: 
[14-1-1-1.21][50-3-3--0.15][124-2-2-0.86][127-0-0-4.17][443-2-2-1.34][567-0-0-1.01][573-1-1-1.43][615-0-0-1.35][695-1-2-2.16][722-3-0-2.47]
[826-0-0-2.46][878-0-0-2.54][1103-0-0-1.56][1212-3-2-0.44][1368-0-0-4.36][2181-2-0-0.36][2476-2-2-0.65][2721-2-2-3.42][2818-1-3-0.42][2886-2-2-1.10]
[3231-2-2-2.38][3333-2-1-1.40][3482-2-2-1.50][3536-3-3-0.31][3625-1-1-3.76][3909-0-0-1.92][4035-0-3-0.85][4140-0-0-0.70][4214-1-3-1.13][4346-1-0-1.24]
[4581-2-2-1.92][4708-3-3-0.53][4838-3-0-2.16][4845-1-2-0.77][4868-0-0-3.32][4939-0-0-0.01][4984-2-2-2.79][5078-1-2-1.07][5396-0-0-4.23][5479-1-1-0.68]
[5717-0-0-2.16][5843-1-1-1.00][5949-3-3-1.00][5987-2-4-1.25][6014-3-3-1.79][6033-3-0--0.19][6313-0-0-2.34][6421-3-1-0.72][6500-1-1-0.87][6583-3-2-1.69]
[6683-3-3-1.26][6825-2-1-1.70][6998-3-3-1.15][7049-3-3-1.48][7517-1-1-2.67][7521-1-1-1.57][7528-1-3-1.50][7949-1-2-2.75][8135-1-0-1.29][8185-3-3-1.52]
[8269-3-4-0.23][8273-3-3-1.82][8543-3-0-4.39][8666-1-1-0.85][8672-0-0-3.01][8903-1-2-2.33][9001-2-2-1.29][9036-2-2-3.00][9281-3-3-0.46][9300-2-2-5.04]
[9571-0-3--0.12][9617-1-0--0.17][9644-2-2-3.77][9705-2-2-0.08][9801-0-3-0.99][9803-3-0-1.63][9865-3-3-3.03][9896-2-2-0.58][10314-1-1-0.13][10337-3-3-3.51]
[10403-0-0-0.46][10653-2-1--0.26][10704-2-1-3.32][10719-1-1-3.88][10727-1-4-0.13][10836-0-0-6.23][10969-2-3-1.12][11042-0-3-0.84][11088-1-1-2.48][11322-0-0-1.97]
[11398-2-2-1.25][11499-0-0-1.37][11502-3-3-0.76][11512-3-3-1.10][11608-1-1-2.40][11610-0-0-2.33][11692-0-3-1.79][11905-0-0-2.27][11993-1-1-2.97][12002-2-3-0.43]
[12052-0-0-3.20][12201-0-3-3.39][12235-2-1-1.20][12320-1-4-1.31][12377-2-4-1.16][12398-2-1-0.04][12503-1-4-1.04][12617-0-2-0.33][12685-3-2--0.09][12738-2-3-0.76]
[12742-2-2-3.86][12823-0-0-2.30][13110-1-2-1.26][13240-3-3-1.74][13253-1-1-1.84][13273-0-0-4.57][13634-1-1-1.29][13763-2-1-0.72][13905-3-3-0.23][14060-2-1-1.43]
[14065-3-0-0.83][14147-3-3-0.94][14595-2-2-1.28][14687-2-2-5.03][14788-2-2-2.94][14869-1-1-4.08][14872-3-1-0.69][14877-1-1-1.63][14927-0-3-0.33][15066-0-0-4.14]
[15175-1-1-0.63][15178-2-2-1.80][15375-3-1-0.05][15389-3-3-0.85][15568-2-1-1.89][15675-3-3-1.35][15869-1-3-0.29][16207-3-3--0.69][16236-0-2-1.25][16302-3-3-0.51]
[16331-2-2-3.40][16381-0-3-0.87][16488-1-1-1.92][16495-0-0-3.01][16650-0-0-2.75][16719-1-2-0.10][16801-0-0-4.45][16828-0-0-2.37][17137-3-0-0.36][17245-1-1-0.55]
[17278-3-0--0.14][17282-0-0-1.60][17311-2-2-1.57][17336-2-1-1.49][17608-3-3-3.73][17627-0-3--0.28][17877-3-0-0.09][17924-1-2-1.55][17984-3-0-1.81][18211-0-3-0.70]
[18276-3-3-1.08][18287-1-1-1.58][18394-0-0-2.07][18428-0-0-3.84][18442-0-3-0.73][18478-3-2-1.14][18607-0-0-2.27][18616-0-0-2.11][18663-0-0-3.45][18718-0-0-2.26]
[18766-2-2-3.06][18824-2-2-0.82][18890-3-3-2.49][18930-3-4-0.53][18938-3-3-1.14][19817-1-2-1.02][19839-0-2-0.85][19930-3-3-2.32][19944-0-4-0.97][20036-2-2-3.14]
[20101-3-3-1.22][20474-1-1-3.28][20547-3-1--0.00][20929-2-2-3.76][21245-1-2-1.61][21257-3-3-0.28][21293-1-2-3.21][21316-1-1-4.10][21384-1-2-1.72][21448-1-1-1.45]
[21483-0-0-2.90][21487-2-2-2.40][21714-0-3-0.15][21943-3-3-0.07][21947-0-0-0.63][21948-0-0-5.31][21965-2-2-3.84][21998-1-1-1.72][22025-0-2-1.07][22228-3-3-2.52]
[22446-1-1-2.16][22494-3-3-0.49][22757-0-0-2.45][22811-3-3-2.70][22976-3-1-0.34][22985-3-3-2.95][23014-0-3-2.08][23112-1-1-2.09][23144-3-3-2.62][23168-2-0-1.81]
[23219-0-0-0.70][23363-3-3-1.11][23470-0-0-0.39][23486-2-2-1.18][23497-0-0-1.83][23516-0-0-2.64][23690-1-1-0.99][23921-2-1-1.14][23936-1-2-2.70][24040-3-3-0.60]
[24111-1-4-1.05][24182-0-0-3.38][24238-3-3-2.27][24290-2-0-2.99][24345-0-4-0.62][24364-1-2-1.34][24427-3-3-0.94][24477-2-2-2.75][24495-2-1-0.59][24893-2-2-2.70]
[25012-1-2-0.24][25121-2-2-1.92][25165-3-3-2.30][25183-0-0-2.50][25297-3-3-1.61][25398-0-0-1.75][25574-2-2-1.13][25644-1-2-1.87][25718-1-0--0.31][25774-2-2-1.40]
[26032-3-3-0.46][26051-3-3-2.27][26120-0-0-0.48][26321-1-1-1.44][26732-1-1-3.19][26784-3-3-3.18][26827-3-3-0.32][26833-0-3-1.79][26838-2-3-0.42][26860-1-2-1.22]
[26948-0-0-1.65][27049-3-0-2.34][27098-1-0-0.67][27526-0-0-2.72][27639-3-0-0.41][27698-3-3-2.11][27772-0-3-0.43][27890-1-1-2.58][28040-0-4-0.42][28503-2-2-3.36]
[28577-1-2-1.58][28959-0-0-3.59][29198-3-0-2.81][29777-0-0-4.23][29877-2-2-1.04][30035-1-1-2.99][30098-0-0-1.12][30326-1-1-4.93][30572-2-2-2.48][30716-0-0-0.27]
[30806-2-3-0.78][30906-1-1-1.81][31007-0-0-1.43][31181-3-2-0.63][31238-0-3-1.04][31347-0-0-0.98][31422-2-2-1.31][31429-3-3-1.24][31431-0-2-2.05][31432-1-1-1.29]
[31477-0-3-1.70][31524-1-2-0.75][31597-1-1-1.55][31619-1-2-1.36][31701-0-0-0.82][31755-0-0-0.96][31854-3-3-1.98][32074-1-1-1.79][32078-3-3-1.60][32111-1-1-0.67]
[32127-1-1-1.41][32140-3-3-1.78][32263-2-0-0.97][32365-0-0-2.19][32411-2-0-2.88][32429-3-0-1.82][32473-3-0-0.67][32574-3-3-1.78][32584-0-0-1.34][32622-0-3-0.55]
[32858-3-3-1.76][32969-3-3-2.57][33016-2-2-1.08][33031-1-3-3.19][33035-2-2-3.03][33133-2-2-1.87][33173-2-2-0.68][33175-3-1-0.65][33306-3-2-0.99][33309-2-0--0.01]
[33474-0-3-0.68][33478-2-3-0.69][33618-1-1-2.49][33712-0-0-0.75][33782-2-2-1.11][33914-3-3-1.17][34076-3-2-1.80][34112-2-2-2.74][34138-2-2-1.84][34239-1-1-0.83]
[34364-2-2-1.96][34617-1-2-0.78][34751-3-3-0.94][34783-2-2-1.26][35015-3-2-1.28][35018-1-1-0.90][35288-2-2-1.20][0-4-2-1.44][1-4-4-1.06][2-4-0-1.39]
[3-4-4-0.94][4-4-1-0.78][5-4-3-0.41][6-4-0-0.35][7-4-0-0.04][8-4-2-1.32][9-4-2-0.57][10-4-3-1.01][11-4-2-2.87][12-4-1-1.10]
[14-4-3-2.48][15-4-3-1.21][16-4-0-1.72][17-4-0-0.31][18-4-4-1.56][19-4-0-2.36][20-4-0-0.99][21-4-2-1.18][22-4-0-1.25][23-4-0-1.48]
[24-4-4-3.03][25-4-2-1.87][26-4-3-0.40][27-4-2-1.67][28-4-4-1.10][29-4-1-2.52][30-4-4--0.11][31-4-1-1.79][32-4-1-1.38][33-4-2-2.87]
[34-4-3--0.12][35-4-1-0.72][37-4-2-0.39][39-4-0-1.53][40-4-0--0.34][41-4-2--0.25][42-4-3-0.10][43-4-3-0.75][45-4-2--0.46][46-4-4-1.08]
[47-4-2-1.57][48-4-1-1.27][51-4-4-1.13][52-4-0-0.51][53-4-4-0.11][54-4-3-1.05][55-4-2-0.58][56-4-1-0.86][57-4-3-0.97][58-4-2-2.08]
[59-4-0-2.09][60-4-2--0.17][61-4-4-0.52][62-4-2-0.53][63-4-2-1.91][64-4-0-0.66][65-4-4-2.46][66-4-1-1.16][67-4-2-0.93][68-4-3-1.02]
[69-4-3-0.32][70-4-4-1.10][72-4-4-0.10][73-4-1-0.85][74-4-2-2.44][75-4-0-0.35][77-4-4-1.33][78-4-2--0.25][79-4-4-0.46][80-4-4-0.48]
[81-4-1-2.08][82-4-1-0.97][83-4-4-0.19][84-4-4-1.55][85-4-4-0.65][86-4-4-0.22][87-4-4-1.35][88-4-4-0.80][89-4-2-0.33][90-4-3--0.25]
[91-4-2-1.32][92-4-0-0.49][93-4-0-2.77][94-4-4--0.16][95-4-2-0.26][96-4-1-0.92][97-4-0-0.23][98-4-2-2.21][99-4-4-0.52][100-4-2-0.61]
[101-4-4-2.77][102-4-0--0.56][103-4-2-0.23][104-4-4-0.35][105-4-2-1.46][106-4-1-2.24][107-4-0-1.77][108-4-3-0.24][109-4-1--0.10][110-4-0-0.82]
[111-4-0-2.90][112-4-0-1.71][113-4-2--0.24][114-4-2-0.72][115-4-0-0.68][116-4-0-0.88][117-4-4-0.83][119-4-2-2.27][121-4-0-0.68][122-4-0-0.90]
[124-4-3-0.08][125-4-4-1.62][126-4-4-0.90][127-4-2-2.27][128-4-3--0.07][129-4-1-0.53][130-4-4--0.25][131-4-2-1.68][132-4-4-0.53][133-4-4-1.43]
[135-4-2-1.48][136-4-2-1.09][137-4-1-0.33][138-4-2-0.19][139-4-2-1.17][140-4-1-0.46][141-4-2-1.64][142-4-4-1.21][143-4-0-0.81][144-4-4-1.80]
[145-4-2-1.40][148-4-0-3.47][149-4-3-0.76][150-4-2-1.14][151-4-4-1.11][152-4-2-0.72][153-4-4-1.37][154-4-4-2.98][155-4-4-0.55][156-4-3-0.97]
[157-4-2-1.77][158-4-3-1.07][160-4-2-0.21][161-4-2-2.00][162-4-4--0.09][164-4-2-0.68][165-4-2-0.54][167-4-0-1.24][168-4-0-0.34][170-4-3-0.33]
[171-4-4-0.75][172-4-4-1.41][173-4-4-0.98][174-4-0-1.60][175-4-4-0.18][177-4-0-1.92][178-4-2-0.49][179-4-0-1.77][180-4-4-1.33][181-4-3-1.24]
[182-4-1-1.01][183-4-0-1.32][184-4-2-1.16][186-4-0-0.05][187-4-2-0.95][188-4-2-1.86][189-4-2-0.82][190-4-2--0.28][191-4-2-0.19][192-4-0--0.01]
[193-4-2-2.40][194-4-2-2.14][195-4-0-0.21][196-4-2-2.51][197-4-4-0.30][198-4-4-3.92][199-4-2-0.90]
---------------------------
I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 0.605 | Acc: 69.375% | Wgt Acc: 79.301%
	I - Batch: 100 | Loss: 0.586 | Acc: 70.688% | Wgt Acc: 80.356%
	I - Batch: 150 | Loss: 0.583 | Acc: 70.292% | Wgt Acc: 80.698%
	I - Batch: 200 | Loss: 0.591 | Acc: 69.844% | Wgt Acc: 80.070%
	I - Batch: 250 | Loss: 0.596 | Acc: 69.500% | Wgt Acc: 79.855%
I - num batch: 273
I - Train -- Loss: 0.597 | Acc: 69.303% | Wgt Acc: 79.754% | LR: 5.000000e-04 | Dur: 167.13s
I - Confusion Matrix: [row->prediction - col->label]
[[634.  14.  19.  90. 180.]
 [ 18. 644.  48.  31. 137.]
 [ 30.  68. 923.  41. 327.]
 [103.  24.  32. 606. 140.]
 [ 12.  10.  10.   5. 216.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.404 | Acc: 49.901% | Wgt Acc: 60.777% | Dur: 14.58s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  5.  5. 12. 30.]
 [ 0. 43.  7.  4. 30.]
 [ 5. 20. 49.  5. 50.]
 [26.  8. 13. 64. 30.]
 [ 0.  2.  1.  1. 40.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 0.593 | Acc: 68.125% | Wgt Acc: 78.904%
	I - Batch: 100 | Loss: 0.574 | Acc: 69.812% | Wgt Acc: 80.073%
	I - Batch: 150 | Loss: 0.585 | Acc: 69.083% | Wgt Acc: 79.831%
	I - Batch: 200 | Loss: 0.577 | Acc: 69.094% | Wgt Acc: 79.916%
	I - Batch: 250 | Loss: 0.575 | Acc: 69.050% | Wgt Acc: 79.893%
I - num batch: 273
I - Train -- Loss: 0.571 | Acc: 69.578% | Wgt Acc: 80.284% | LR: 5.000000e-04 | Dur: 166.89s
I - Confusion Matrix: [row->prediction - col->label]
[[654.  14.  18.  91. 156.]
 [ 13. 647.  45.  25. 145.]
 [ 27.  71. 917.  42. 335.]
 [ 86.  18.  33. 610. 157.]
 [ 17.  10.  19.   5. 207.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.290 | Acc: 49.310% | Wgt Acc: 58.609% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  0.  2. 18. 13.]
 [ 2. 40. 10.  7. 20.]
 [ 6. 30. 54.  9. 87.]
 [17.  6.  9. 51. 15.]
 [ 3.  2.  0.  1. 45.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 0.534 | Acc: 72.375% | Wgt Acc: 82.979%
	I - Batch: 100 | Loss: 0.562 | Acc: 70.562% | Wgt Acc: 81.319%
	I - Batch: 150 | Loss: 0.545 | Acc: 71.833% | Wgt Acc: 82.178%
	I - Batch: 200 | Loss: 0.540 | Acc: 71.812% | Wgt Acc: 82.217%
	I - Batch: 250 | Loss: 0.533 | Acc: 71.825% | Wgt Acc: 82.376%
I - num batch: 273
I - Train -- Loss: 0.540 | Acc: 71.435% | Wgt Acc: 81.852% | LR: 5.000000e-04 | Dur: 164.83s
I - Confusion Matrix: [row->prediction - col->label]
[[650.  18.  20.  70. 159.]
 [ 22. 667.  41.  29. 148.]
 [ 36.  48. 930.  37. 306.]
 [ 75.  18.  24. 629. 147.]
 [ 14.   9.  17.   8. 240.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.319 | Acc: 51.479% | Wgt Acc: 62.969% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  1.  3. 13. 26.]
 [ 0. 45.  9.  4. 22.]
 [ 2. 25. 54. 11. 72.]
 [18.  4.  9. 57. 21.]
 [ 2.  3.  0.  1. 39.]]

I - Local maximum validation set accuracy:  51.48

I - Validation set results: 
[14-1-2-1.70][50-3-1-0.42][124-2-2-0.92][127-0-0-4.79][443-2-2-2.56][567-0-0-1.58][573-1-1-1.73][615-0-0-2.07][695-1-2-1.80][722-3-0-2.64]
[826-0-0-3.87][878-0-0-3.23][1103-0-0-1.10][1212-3-3-0.18][1368-0-0-4.92][2181-2-3-1.57][2476-2-2-1.28][2721-2-2-2.16][2818-1-1-1.29][2886-2-1-2.27]
[3231-2-2-2.14][3333-2-2-1.77][3482-2-2-1.83][3536-3-2-0.45][3625-1-1-3.15][3909-0-0-2.39][4035-0-3-1.96][4140-0-0-2.28][4214-1-3-1.67][4346-1-0-2.01]
[4581-2-2-2.45][4708-3-3-1.41][4838-3-0-1.45][4845-1-3-0.71][4868-0-0-4.31][4939-0-4-0.18][4984-2-2-3.13][5078-1-2-0.20][5396-0-0-5.07][5479-1-1-2.72]
[5717-0-0-2.55][5843-1-2-1.99][5949-3-3-1.94][5987-2-2-1.63][6014-3-3-2.24][6033-3-2-0.01][6313-0-0-2.96][6421-3-3-2.42][6500-1-2-1.46][6583-3-2-2.62]
[6683-3-3-1.51][6825-2-1-2.22][6998-3-3-1.06][7049-3-3-1.99][7517-1-1-3.65][7521-1-1-1.91][7528-1-2-1.43][7949-1-2-4.13][8135-1-2-1.70][8185-3-0-3.40]
[8269-3-1-4.85][8273-3-3-3.02][8543-3-0-4.28][8666-1-1-3.22][8672-0-0-4.73][8903-1-2-1.29][9001-2-1-2.02][9036-2-2-3.53][9281-3-3-0.23][9300-2-2-5.53]
[9571-0-3-0.34][9617-1-4--0.21][9644-2-2-3.49][9705-2-1-0.67][9801-0-3-1.25][9803-3-3-1.46][9865-3-3-4.01][9896-2-2-3.28][10314-1-2-1.22][10337-3-3-4.84]
[10403-0-0-1.06][10653-2-1-0.19][10704-2-1-1.12][10719-1-1-3.83][10727-1-2-1.47][10836-0-0-8.43][10969-2-3-1.77][11042-0-3-0.74][11088-1-1-4.12][11322-0-0-1.72]
[11398-2-2-1.01][11499-0-0-1.19][11502-3-3-1.31][11512-3-3-1.70][11608-1-1-4.09][11610-0-0-0.47][11692-0-3-1.89][11905-0-0-3.87][11993-1-1-4.00][12002-2-2-2.68]
[12052-0-0-3.28][12201-0-3-4.30][12235-2-2-3.15][12320-1-4-1.77][12377-2-2-1.59][12398-2-3-0.47][12503-1-1-1.27][12617-0-3-0.51][12685-3-3-1.30][12738-2-0-0.99]
[12742-2-2-4.55][12823-0-0-2.35][13110-1-2-3.19][13240-3-3-2.57][13253-1-1-3.30][13273-0-0-4.92][13634-1-1-2.03][13763-2-2-1.08][13905-3-0-1.05][14060-2-1-3.10]
[14065-3-3-1.05][14147-3-3-2.70][14595-2-2-2.37][14687-2-2-5.75][14788-2-2-3.16][14869-1-1-2.32][14872-3-0-1.13][14877-1-1-1.52][14927-0-3-1.21][15066-0-0-5.05]
[15175-1-1-1.17][15178-2-2-1.33][15375-3-3-0.81][15389-3-3-3.86][15568-2-1-2.77][15675-3-3-2.51][15869-1-1-0.39][16207-3-0-0.58][16236-0-0-1.13][16302-3-3-0.93]
[16331-2-2-4.84][16381-0-3-2.11][16488-1-1-4.74][16495-0-0-3.94][16650-0-0-3.58][16719-1-2-1.85][16801-0-0-6.50][16828-0-0-2.23][17137-3-3-0.14][17245-1-1-0.02]
[17278-3-0-0.93][17282-0-0-2.11][17311-2-2-1.88][17336-2-2-2.04][17608-3-3-4.34][17627-0-0-0.52][17877-3-3-0.22][17924-1-2-0.20][17984-3-0-4.21][18211-0-3-0.84]
[18276-3-3-1.83][18287-1-1-0.39][18394-0-0-3.01][18428-0-0-5.76][18442-0-3-1.87][18478-3-3-1.17][18607-0-0-1.91][18616-0-0-0.43][18663-0-0-3.47][18718-0-0-3.97]
[18766-2-2-3.12][18824-2-2-1.42][18890-3-3-2.86][18930-3-4-0.81][18938-3-2-1.47][19817-1-2-1.74][19839-0-2-1.02][19930-3-3-3.26][19944-0-0-0.81][20036-2-2-3.36]
[20101-3-1-0.62][20474-1-1-3.73][20547-3-3-0.80][20929-2-2-3.96][21245-1-2-1.72][21257-3-3-0.51][21293-1-1-2.29][21316-1-1-3.85][21384-1-4-1.61][21448-1-1-1.87]
[21483-0-0-5.00][21487-2-2-2.38][21714-0-0-1.17][21943-3-2-0.45][21947-0-0-1.38][21948-0-0-6.74][21965-2-2-3.19][21998-1-1-2.04][22025-0-3-0.60][22228-3-3-4.14]
[22446-1-1-2.99][22494-3-3-2.42][22757-0-3-2.78][22811-3-3-1.31][22976-3-2-1.95][22985-3-3-4.36][23014-0-0-3.81][23112-1-1-2.13][23144-3-3-4.51][23168-2-3-1.37]
[23219-0-0-0.86][23363-3-3-3.24][23470-0-0-0.59][23486-2-2-1.47][23497-0-3-3.21][23516-0-0-4.07][23690-1-1-0.91][23921-2-2-2.04][23936-1-2-1.94][24040-3-2-0.93]
[24111-1-2-1.08][24182-0-0-6.80][24238-3-3-3.76][24290-2-0-3.84][24345-0-0-0.80][24364-1-2-0.87][24427-3-0-2.59][24477-2-2-2.13][24495-2-1-0.81][24893-2-2-1.76]
[25012-1-2-1.82][25121-2-2-3.31][25165-3-3-2.34][25183-0-0-1.35][25297-3-3-2.66][25398-0-0-3.29][25574-2-2-1.16][25644-1-1-4.93][25718-1-3-0.36][25774-2-2-1.13]
[26032-3-3-1.30][26051-3-3-3.10][26120-0-0-0.05][26321-1-1-3.82][26732-1-1-2.47][26784-3-3-5.38][26827-3-3-0.82][26833-0-3-2.13][26838-2-3-0.91][26860-1-2-1.90]
[26948-0-0-2.63][27049-3-0-3.35][27098-1-1-1.33][27526-0-3-2.37][27639-3-3-1.44][27698-3-3-2.28][27772-0-0-2.47][27890-1-1-1.89][28040-0-0-2.48][28503-2-2-4.33]
[28577-1-1-2.66][28959-0-0-4.69][29198-3-3-2.35][29777-0-0-5.87][29877-2-2-1.32][30035-1-1-2.41][30098-0-0-1.13][30326-1-1-6.80][30572-2-2-3.37][30716-0-4-0.33]
[30806-2-3-0.94][30906-1-1-3.53][31007-0-0-0.84][31181-3-3-2.33][31238-0-3-1.74][31347-0-0-3.03][31422-2-2-0.11][31429-3-3-1.76][31431-0-0-2.56][31432-1-1-3.01]
[31477-0-3-3.20][31524-1-2-1.24][31597-1-1-1.99][31619-1-2-0.60][31701-0-0-3.20][31755-0-0-2.85][31854-3-3-1.96][32074-1-1-1.29][32078-3-3-1.65][32111-1-1-0.37]
[32127-1-2-1.69][32140-3-3-4.12][32263-2-0-0.03][32365-0-0-2.71][32411-2-3-4.10][32429-3-0-2.46][32473-3-0-0.58][32574-3-3-1.92][32584-0-0-0.81][32622-0-2-0.15]
[32858-3-3-1.46][32969-3-3-4.54][33016-2-2-2.88][33031-1-3-4.06][33035-2-2-3.42][33133-2-2-2.35][33173-2-2-1.51][33175-3-1-0.59][33306-3-2-0.92][33309-2-3--0.02]
[33474-0-0-1.91][33478-2-3-1.61][33618-1-1-0.53][33712-0-0-1.20][33782-2-2-1.38][33914-3-2-0.84][34076-3-2-1.27][34112-2-2-3.16][34138-2-2-1.66][34239-1-1-1.04]
[34364-2-2-2.11][34617-1-2-1.06][34751-3-3-2.40][34783-2-2-2.09][35015-3-2-0.76][35018-1-1-2.37][35288-2-2-1.47][0-4-2-2.75][1-4-4-0.98][2-4-0-1.08]
[3-4-4-0.86][4-4-4-0.48][5-4-3-0.40][6-4-0-1.55][7-4-2-1.41][8-4-2-1.67][9-4-2-0.66][10-4-2-1.24][11-4-2-2.53][12-4-2-0.08]
[14-4-3-2.62][15-4-3-2.63][16-4-4-0.42][17-4-3-0.13][18-4-4-2.55][19-4-0-2.49][20-4-2-0.17][21-4-2-2.26][22-4-0-0.65][23-4-1--0.47]
[24-4-4-3.65][25-4-2-2.49][26-4-1-0.84][27-4-2-1.72][28-4-4-1.09][29-4-1-1.60][30-4-0-1.12][31-4-2-0.84][32-4-1-2.16][33-4-2-4.17]
[34-4-3--0.12][35-4-0-1.48][37-4-2-0.95][39-4-0-2.42][40-4-0--0.18][41-4-2--0.19][42-4-0--0.41][43-4-1-0.93][45-4-2-1.65][46-4-4-1.59]
[47-4-4-2.51][48-4-4-0.72][51-4-4-0.33][52-4-4-0.15][53-4-2--0.11][54-4-3-1.83][55-4-2-2.35][56-4-1-1.72][57-4-3-0.98][58-4-2-2.48]
[59-4-0-2.06][60-4-1-0.51][61-4-4-1.11][62-4-3-0.65][63-4-2-1.61][64-4-4--0.05][65-4-4-2.06][66-4-4--0.02][67-4-2-0.80][68-4-3-2.24]
[69-4-3-0.74][70-4-4-1.48][72-4-1-1.24][73-4-1-2.55][74-4-2-2.39][75-4-0-0.49][77-4-4-2.39][78-4-2-0.88][79-4-2-1.76][80-4-4-1.80]
[81-4-2-2.87][82-4-0-0.90][83-4-4--0.33][84-4-4-1.17][85-4-4-2.20][86-4-2--0.04][87-4-4-1.04][88-4-4-0.04][89-4-2-0.67][90-4-2--0.23]
[91-4-1-1.78][92-4-2-0.72][93-4-0-1.63][94-4-2-1.36][95-4-3-0.04][96-4-1-1.05][97-4-0-1.23][98-4-2-1.75][99-4-0-0.07][100-4-2-1.69]
[101-4-4-2.27][102-4-2-0.86][103-4-2--0.09][104-4-2-0.26][105-4-1-1.77][106-4-4-1.38][107-4-1-0.43][108-4-2-0.71][109-4-3-0.13][110-4-4-1.43]
[111-4-0-2.35][112-4-2-0.46][113-4-3--0.02][114-4-3-1.45][115-4-3--0.12][116-4-2-0.70][117-4-1-1.05][119-4-2-0.78][121-4-2-0.36][122-4-3-0.72]
[124-4-3-0.42][125-4-1-1.71][126-4-4-3.09][127-4-2-0.64][128-4-0--0.07][129-4-1-0.36][130-4-2-2.29][131-4-2-1.16][132-4-2-0.49][133-4-4-1.86]
[135-4-2-1.10][136-4-2-0.31][137-4-2-0.54][138-4-2--0.37][139-4-2-0.47][140-4-2-0.52][141-4-2-2.33][142-4-4-0.71][143-4-2-0.94][144-4-4-2.42]
[145-4-2-2.12][148-4-0-4.71][149-4-3-0.91][150-4-2-2.91][151-4-2-1.06][152-4-4-0.83][153-4-2-1.02][154-4-4-3.36][155-4-1-0.73][156-4-0-0.83]
[157-4-0-1.05][158-4-2-1.04][160-4-1-1.17][161-4-2-1.84][162-4-0--0.28][164-4-2-1.07][165-4-2-1.58][167-4-0-2.00][168-4-0-0.63][170-4-3-0.73]
[171-4-2-1.28][172-4-4-0.97][173-4-4-1.33][174-4-0-3.85][175-4-2-0.99][177-4-0-1.66][178-4-4-0.91][179-4-0-0.51][180-4-4-1.61][181-4-3-0.93]
[182-4-3-0.79][183-4-4-0.59][184-4-2-2.10][186-4-2-0.62][187-4-2-0.66][188-4-2-3.36][189-4-2-1.12][190-4-2-1.36][191-4-2-1.64][192-4-1-0.59]
[193-4-1-3.05][194-4-2-2.50][195-4-1-0.84][196-4-2-2.46][197-4-1-2.04][198-4-4-3.62][199-4-2-2.15]
---------------------------
I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.481 | Acc: 73.500% | Wgt Acc: 85.235%
	I - Batch: 100 | Loss: 0.468 | Acc: 73.875% | Wgt Acc: 85.465%
	I - Batch: 150 | Loss: 0.480 | Acc: 73.625% | Wgt Acc: 85.070%
	I - Batch: 200 | Loss: 0.463 | Acc: 74.469% | Wgt Acc: 85.811%
	I - Batch: 250 | Loss: 0.469 | Acc: 74.150% | Wgt Acc: 85.525%
I - num batch: 273
I - Train -- Loss: 0.466 | Acc: 74.278% | Wgt Acc: 85.562% | LR: 2.500000e-04 | Dur: 165.52s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   9.   8.  66. 176.]
 [ 11. 693.  24.  18. 125.]
 [ 23.  35. 970.  22. 325.]
 [ 61.  15.  18. 661. 145.]
 [ 15.   8.  12.   6. 229.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.271 | Acc: 52.071% | Wgt Acc: 61.804% | Dur: 14.29s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  3.  4. 16. 33.]
 [ 1. 41.  8.  2. 18.]
 [ 4. 20. 47.  4. 55.]
 [20. 11. 14. 64. 25.]
 [ 0.  3.  2.  0. 49.]]

I - Local maximum validation set accuracy:  52.07

I - Validation set results: 
[14-1-1-0.74][50-3-1-1.95][124-2-2-1.82][127-0-0-4.57][443-2-2-2.32][567-0-0-2.91][573-1-1-3.83][615-0-3-2.56][695-1-2-1.30][722-3-0-2.00]
[826-0-0-4.09][878-0-0-6.12][1103-0-0-1.94][1212-3-3-0.49][1368-0-0-4.62][2181-2-3-2.01][2476-2-2-1.16][2721-2-2-3.59][2818-1-1-1.37][2886-2-1-0.85]
[3231-2-2-3.10][3333-2-3-1.23][3482-2-2-1.68][3536-3-3-0.27][3625-1-1-3.52][3909-0-0-2.30][4035-0-0-2.06][4140-0-0-1.14][4214-1-3-2.43][4346-1-3-1.26]
[4581-2-2-2.15][4708-3-3-1.49][4838-3-0-1.27][4845-1-3-0.65][4868-0-0-3.21][4939-0-3-0.78][4984-2-2-2.23][5078-1-2-1.06][5396-0-0-5.63][5479-1-1-1.40]
[5717-0-0-1.11][5843-1-2-0.41][5949-3-3-1.94][5987-2-2-2.34][6014-3-3-2.36][6033-3-0-1.44][6313-0-0-1.92][6421-3-3-2.45][6500-1-2-0.30][6583-3-2-1.30]
[6683-3-3-2.57][6825-2-1-2.13][6998-3-3-1.05][7049-3-3-2.46][7517-1-1-1.79][7521-1-1-2.37][7528-1-3-2.21][7949-1-2-4.01][8135-1-0-1.12][8185-3-0-3.82]
[8269-3-1-1.51][8273-3-3-3.07][8543-3-0-4.66][8666-1-1-1.27][8672-0-0-3.65][8903-1-1-1.89][9001-2-2-1.45][9036-2-2-3.28][9281-3-3-1.08][9300-2-2-5.18]
[9571-0-3-1.02][9617-1-1-2.37][9644-2-2-4.40][9705-2-0-0.17][9801-0-3-1.99][9803-3-3-1.59][9865-3-3-4.76][9896-2-2-3.08][10314-1-2-1.19][10337-3-3-5.33]
[10403-0-0-0.52][10653-2-1-0.04][10704-2-1-1.08][10719-1-1-3.81][10727-1-2-0.38][10836-0-0-6.90][10969-2-3-3.01][11042-0-0-1.34][11088-1-1-3.64][11322-0-0-2.16]
[11398-2-2-0.80][11499-0-0-2.62][11502-3-0-2.31][11512-3-3-1.96][11608-1-1-1.44][11610-0-0-1.23][11692-0-3-2.27][11905-0-0-4.32][11993-1-1-2.33][12002-2-3-0.73]
[12052-0-0-4.30][12201-0-3-4.44][12235-2-1-0.56][12320-1-0-0.85][12377-2-4-0.60][12398-2-3-0.61][12503-1-2-0.78][12617-0-3-0.48][12685-3-3-1.13][12738-2-3-1.17]
[12742-2-2-5.00][12823-0-0-2.57][13110-1-2-3.40][13240-3-3-2.41][13253-1-1-3.11][13273-0-0-6.56][13634-1-1-2.26][13763-2-2-1.37][13905-3-3-1.28][14060-2-1-0.49]
[14065-3-0-1.49][14147-3-3-2.61][14595-2-2-2.23][14687-2-2-5.14][14788-2-2-2.53][14869-1-1-5.18][14872-3-0-0.69][14877-1-1-1.73][14927-0-3-0.90][15066-0-0-5.41]
[15175-1-1-1.97][15178-2-0-0.72][15375-3-3-1.48][15389-3-3-3.09][15568-2-1-2.65][15675-3-3-3.41][15869-1-3-0.84][16207-3-0--0.04][16236-0-2-0.99][16302-3-3-1.72]
[16331-2-2-5.28][16381-0-3-2.02][16488-1-1-1.64][16495-0-0-1.70][16650-0-0-4.53][16719-1-4-0.10][16801-0-0-5.17][16828-0-3-1.63][17137-3-0-1.29][17245-1-3-1.17]
[17278-3-0-0.60][17282-0-0-1.98][17311-2-2-1.84][17336-2-1-1.90][17608-3-3-4.84][17627-0-0-1.65][17877-3-3-0.91][17924-1-2-1.23][17984-3-0-2.60][18211-0-3-1.56]
[18276-3-3-2.81][18287-1-1-2.06][18394-0-0-3.16][18428-0-0-2.15][18442-0-3-3.47][18478-3-3-1.54][18607-0-0-2.84][18616-0-0-1.76][18663-0-0-4.04][18718-0-0-4.18]
[18766-2-2-2.89][18824-2-2-2.03][18890-3-3-2.48][18930-3-3-0.89][18938-3-3-1.98][19817-1-2-2.24][19839-0-2-0.92][19930-3-3-3.56][19944-0-2-0.89][20036-2-2-4.12]
[20101-3-3-2.92][20474-1-1-4.01][20547-3-0-0.95][20929-2-2-4.93][21245-1-2-1.40][21257-3-3-1.45][21293-1-2-2.22][21316-1-1-5.50][21384-1-1-3.08][21448-1-2-1.94]
[21483-0-0-5.64][21487-2-2-2.43][21714-0-3-1.06][21943-3-3-2.24][21947-0-0-1.80][21948-0-0-6.40][21965-2-2-5.20][21998-1-1-3.11][22025-0-2-0.29][22228-3-3-4.70]
[22446-1-1-2.02][22494-3-3-1.97][22757-0-0-3.79][22811-3-3-4.68][22976-3-2-1.70][22985-3-3-4.98][23014-0-3-3.69][23112-1-1-2.40][23144-3-3-4.83][23168-2-3-0.89]
[23219-0-0-0.74][23363-3-3-4.84][23470-0-0-1.38][23486-2-3-0.99][23497-0-3-3.45][23516-0-0-2.87][23690-1-3-2.01][23921-2-2-3.24][23936-1-2-1.01][24040-3-2-1.14]
[24111-1-4-1.22][24182-0-0-4.21][24238-3-3-3.72][24290-2-0-3.18][24345-0-0-3.18][24364-1-3-0.26][24427-3-0-3.32][24477-2-2-2.58][24495-2-4-0.22][24893-2-2-1.75]
[25012-1-2-1.57][25121-2-2-2.79][25165-3-3-3.32][25183-0-0-2.36][25297-3-3-3.81][25398-0-0-2.97][25574-2-2-1.51][25644-1-1-2.76][25718-1-3-0.39][25774-2-2-0.45]
[26032-3-3-1.72][26051-3-3-4.66][26120-0-0-2.22][26321-1-1-2.72][26732-1-1-2.16][26784-3-3-5.09][26827-3-3-0.56][26833-0-3-3.07][26838-2-3-1.49][26860-1-2-0.82]
[26948-0-0-2.35][27049-3-0-3.36][27098-1-0-1.46][27526-0-0-1.72][27639-3-3-0.91][27698-3-3-3.17][27772-0-0-3.28][27890-1-1-2.76][28040-0-0-2.95][28503-2-2-4.88]
[28577-1-1-2.73][28959-0-0-3.80][29198-3-3-2.47][29777-0-0-5.34][29877-2-2-1.53][30035-1-1-3.25][30098-0-3-2.43][30326-1-1-5.27][30572-2-2-3.20][30716-0-0-0.36]
[30806-2-3-1.79][30906-1-1-2.59][31007-0-0-1.83][31181-3-3-2.09][31238-0-3-2.02][31347-0-0-4.28][31422-2-2-0.34][31429-3-3-2.08][31431-0-0--0.12][31432-1-1-1.95]
[31477-0-3-3.66][31524-1-2-1.36][31597-1-1-1.73][31619-1-3-0.82][31701-0-0-3.30][31755-0-0-3.06][31854-3-3-3.17][32074-1-1--0.08][32078-3-3-2.57][32111-1-4--0.46]
[32127-1-2-1.97][32140-3-3-4.55][32263-2-0-1.40][32365-0-0-3.02][32411-2-3-3.70][32429-3-3-2.63][32473-3-2-0.53][32574-3-3-2.83][32584-0-0-0.97][32622-0-1-0.08]
[32858-3-0-1.71][32969-3-3-3.87][33016-2-2-1.99][33031-1-3-3.49][33035-2-2-2.23][33133-2-2-1.58][33173-2-2-0.97][33175-3-3-0.57][33306-3-3-0.63][33309-2-3-0.55]
[33474-0-3-0.42][33478-2-3-2.09][33618-1-1-2.14][33712-0-0-1.33][33782-2-2-1.38][33914-3-3-0.56][34076-3-3-0.73][34112-2-2-2.70][34138-2-2-1.07][34239-1-1-0.23]
[34364-2-2-2.74][34617-1-2-2.43][34751-3-3-3.23][34783-2-2-1.32][35015-3-3-1.19][35018-1-1-2.45][35288-2-3-0.81][0-4-2-2.41][1-4-4-1.24][2-4-0-1.33]
[3-4-1-1.08][4-4-0--0.06][5-4-3-1.77][6-4-4-1.71][7-4-4-0.69][8-4-2-1.03][9-4-2-1.36][10-4-4-1.85][11-4-2-1.91][12-4-1-0.60]
[14-4-3-2.92][15-4-3-2.61][16-4-4-0.82][17-4-0--0.32][18-4-4-1.17][19-4-0-3.05][20-4-0-1.58][21-4-2-1.27][22-4-4-1.34][23-4-0-0.90]
[24-4-4-2.92][25-4-2-1.49][26-4-3-0.47][27-4-2-2.01][28-4-4-0.20][29-4-1-1.97][30-4-0-0.09][31-4-2-0.31][32-4-1-0.61][33-4-2-2.44]
[34-4-4--0.26][35-4-0-0.45][37-4-3-0.28][39-4-0-3.22][40-4-4--0.18][41-4-1--0.16][42-4-2-0.76][43-4-3-0.62][45-4-2-1.42][46-4-4-2.50]
[47-4-4-2.79][48-4-4-0.12][51-4-4-0.92][52-4-0-0.30][53-4-0-0.86][54-4-3-1.34][55-4-2-2.08][56-4-2-0.78][57-4-3-1.18][58-4-2-3.91]
[59-4-0-2.49][60-4-1-0.46][61-4-4-1.62][62-4-2-0.72][63-4-2-2.56][64-4-2-0.05][65-4-4-2.43][66-4-4-0.47][67-4-2-0.50][68-4-3-1.37]
[69-4-3-0.77][70-4-4-1.74][72-4-4-0.26][73-4-1-0.15][74-4-2-2.01][75-4-0-1.11][77-4-4-3.58][78-4-2-0.13][79-4-2-1.05][80-4-4-2.14]
[81-4-1-1.40][82-4-1-1.07][83-4-1-1.29][84-4-4-1.79][85-4-4-1.86][86-4-2-0.35][87-4-4-1.57][88-4-1-0.49][89-4-3-0.24][90-4-3--0.35]
[91-4-3--0.24][92-4-0--0.04][93-4-0-2.50][94-4-2-0.70][95-4-4--0.51][96-4-1-1.91][97-4-4-1.85][98-4-2-2.98][99-4-2-0.22][100-4-2-2.23]
[101-4-4-2.59][102-4-2-0.00][103-4-3-0.39][104-4-2-0.94][105-4-2-1.83][106-4-4-1.34][107-4-0-1.48][108-4-4-0.92][109-4-4-0.88][110-4-4-1.34]
[111-4-0-4.09][112-4-2-0.07][113-4-3-0.72][114-4-3-1.15][115-4-0--0.10][116-4-4-0.06][117-4-1-0.86][119-4-2-3.10][121-4-2-1.76][122-4-0-0.61]
[124-4-3-0.53][125-4-4-2.18][126-4-4-1.27][127-4-1-0.93][128-4-0-0.31][129-4-1-0.56][130-4-2-2.47][131-4-3-1.75][132-4-3-0.19][133-4-4-1.84]
[135-4-2-1.99][136-4-0--0.16][137-4-2-0.16][138-4-2--0.19][139-4-3-0.15][140-4-1-0.70][141-4-0-0.28][142-4-4-1.60][143-4-4-1.43][144-4-4-3.09]
[145-4-2-3.46][148-4-0-4.03][149-4-2-0.89][150-4-2-3.41][151-4-2-0.82][152-4-4-0.98][153-4-2-1.31][154-4-3-0.78][155-4-4-1.08][156-4-3-0.88]
[157-4-0-1.36][158-4-3-0.61][160-4-2-0.02][161-4-2-2.70][162-4-0--0.03][164-4-2-1.54][165-4-2-1.64][167-4-0-2.49][168-4-4-1.36][170-4-0-0.38]
[171-4-4-1.04][172-4-4-0.67][173-4-4-1.61][174-4-0-3.79][175-4-2-0.19][177-4-0-2.97][178-4-4-1.90][179-4-0-0.32][180-4-4-1.43][181-4-3-0.61]
[182-4-3-1.35][183-4-4-0.48][184-4-2-2.09][186-4-0--0.19][187-4-2-1.40][188-4-2-1.29][189-4-2-1.37][190-4-1--0.13][191-4-2-2.58][192-4-0-0.26]
[193-4-2-2.20][194-4-2-2.15][195-4-0-1.40][196-4-2-2.54][197-4-1-0.68][198-4-4-2.95][199-4-2-1.95]
---------------------------
I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 0.409 | Acc: 77.125% | Wgt Acc: 88.118%
	I - Batch: 100 | Loss: 0.416 | Acc: 77.375% | Wgt Acc: 88.060%
	I - Batch: 150 | Loss: 0.424 | Acc: 76.500% | Wgt Acc: 87.588%
	I - Batch: 200 | Loss: 0.420 | Acc: 76.844% | Wgt Acc: 87.657%
	I - Batch: 250 | Loss: 0.429 | Acc: 76.500% | Wgt Acc: 87.270%
I - num batch: 273
I - Train -- Loss: 0.430 | Acc: 76.547% | Wgt Acc: 87.325% | LR: 2.500000e-04 | Dur: 164.96s
I - Confusion Matrix: [row->prediction - col->label]
[[699.   9.   8.  47. 166.]
 [ 13. 698.  15.  18. 115.]
 [ 22.  29. 987.  21. 304.]
 [ 51.  14.  10. 680. 140.]
 [ 12.  10.  12.   7. 275.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.208 | Acc: 56.016% | Wgt Acc: 63.511% | Dur: 14.01s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  1.  4. 19. 24.]
 [ 0. 41.  3.  1. 15.]
 [ 2. 24. 53.  4. 48.]
 [22.  8. 12. 61. 27.]
 [ 1.  4.  3.  1. 66.]]

I - Local maximum validation set accuracy:  56.02

I - Validation set results: 
[14-1-1-1.43][50-3-3--0.20][124-2-2-2.95][127-0-0-4.13][443-2-2-1.97][567-0-0-2.36][573-1-1-4.29][615-0-3-2.04][695-1-2-1.84][722-3-0-2.34]
[826-0-0-3.87][878-0-3-3.82][1103-0-0-0.90][1212-3-3-1.31][1368-0-0-4.89][2181-2-3-1.80][2476-2-2-1.60][2721-2-2-3.89][2818-1-1-0.52][2886-2-1-0.91]
[3231-2-2-4.15][3333-2-2-2.07][3482-2-2-1.81][3536-3-3-1.06][3625-1-1-3.57][3909-0-0-3.39][4035-0-0-2.53][4140-0-0-2.90][4214-1-2-0.53][4346-1-3-2.37]
[4581-2-2-2.84][4708-3-3-1.56][4838-3-0-1.92][4845-1-3-0.57][4868-0-0-3.32][4939-0-3--0.22][4984-2-2-2.73][5078-1-2-0.76][5396-0-0-6.14][5479-1-1-1.74]
[5717-0-0-2.02][5843-1-2-0.10][5949-3-3-3.50][5987-2-4-1.69][6014-3-3-2.59][6033-3-3-0.23][6313-0-3-2.67][6421-3-3-2.29][6500-1-1-0.46][6583-3-2-1.21]
[6683-3-3-2.62][6825-2-1-1.04][6998-3-3-1.14][7049-3-3-1.70][7517-1-1-1.84][7521-1-1--0.28][7528-1-3-2.50][7949-1-2-3.68][8135-1-0-1.69][8185-3-0-4.20]
[8269-3-1-0.03][8273-3-3-3.76][8543-3-0-5.22][8666-1-1-3.02][8672-0-0-3.13][8903-1-2-2.14][9001-2-2-1.19][9036-2-2-3.03][9281-3-3-0.91][9300-2-2-5.37]
[9571-0-3-1.33][9617-1-4-0.92][9644-2-2-2.22][9705-2-0-1.21][9801-0-3-1.71][9803-3-3-2.11][9865-3-3-5.15][9896-2-2-2.55][10314-1-2-0.94][10337-3-3-4.83]
[10403-0-0-0.43][10653-2-2--0.22][10704-2-1-0.71][10719-1-1-2.65][10727-1-1-0.08][10836-0-0-7.59][10969-2-3-2.28][11042-0-0-1.62][11088-1-1-3.52][11322-0-0-2.95]
[11398-2-2-1.21][11499-0-0-2.78][11502-3-3-1.33][11512-3-3-2.77][11608-1-1-1.37][11610-0-0-1.88][11692-0-3-2.25][11905-0-0-3.70][11993-1-1-2.96][12002-2-2-3.71]
[12052-0-0-3.98][12201-0-3-5.48][12235-2-2-0.45][12320-1-4-1.24][12377-2-4-1.66][12398-2-2-0.14][12503-1-1-0.78][12617-0-2-0.53][12685-3-3-1.19][12738-2-3-0.44]
[12742-2-2-5.90][12823-0-3-2.79][13110-1-2-3.38][13240-3-0-2.77][13253-1-1-3.55][13273-0-0-6.30][13634-1-1-1.84][13763-2-3-1.34][13905-3-3-1.60][14060-2-3-0.85]
[14065-3-0-1.60][14147-3-3-3.02][14595-2-2-1.75][14687-2-2-5.30][14788-2-2-2.11][14869-1-1-3.24][14872-3-0-1.42][14877-1-1-2.53][14927-0-3-0.90][15066-0-0-4.46]
[15175-1-1-1.66][15178-2-0-1.18][15375-3-0--0.78][15389-3-3-3.48][15568-2-3-0.06][15675-3-3-3.05][15869-1-2-0.09][16207-3-0-0.59][16236-0-0-0.80][16302-3-3-1.76]
[16331-2-2-5.32][16381-0-0-1.75][16488-1-1-5.12][16495-0-0-3.30][16650-0-0-4.12][16719-1-2-0.69][16801-0-0-6.44][16828-0-3-1.87][17137-3-0-2.07][17245-1-4-0.17]
[17278-3-0-0.62][17282-0-0-2.20][17311-2-2-1.83][17336-2-2-1.26][17608-3-3-5.92][17627-0-0-0.11][17877-3-0-0.49][17924-1-2-0.79][17984-3-0-3.47][18211-0-3-0.94]
[18276-3-3-3.40][18287-1-1-2.32][18394-0-0-3.33][18428-0-0-2.49][18442-0-3-2.94][18478-3-3-2.25][18607-0-0-2.59][18616-0-0-1.41][18663-0-0-3.03][18718-0-0-3.10]
[18766-2-2-2.58][18824-2-2-1.47][18890-3-3-2.73][18930-3-4-0.83][18938-3-3-2.11][19817-1-2-1.96][19839-0-0-0.84][19930-3-3-3.10][19944-0-0-0.15][20036-2-2-4.38]
[20101-3-3-2.52][20474-1-1-2.28][20547-3-0-1.67][20929-2-2-2.55][21245-1-2-2.52][21257-3-3-1.88][21293-1-2-2.58][21316-1-1-5.73][21384-1-1-2.14][21448-1-2-0.78]
[21483-0-0-4.84][21487-2-2-2.41][21714-0-0-1.52][21943-3-3-1.39][21947-0-0-2.07][21948-0-0-6.64][21965-2-2-3.65][21998-1-1-2.97][22025-0-2-0.45][22228-3-3-4.71]
[22446-1-1-1.86][22494-3-3-2.02][22757-0-0-3.44][22811-3-3-4.55][22976-3-2-2.06][22985-3-3-4.88][23014-0-0-3.81][23112-1-1-2.53][23144-3-3-4.13][23168-2-3-1.80]
[23219-0-0-1.10][23363-3-3-4.33][23470-0-0-2.10][23486-2-2-0.78][23497-0-3-3.50][23516-0-0-3.04][23690-1-1-1.09][23921-2-2-2.37][23936-1-2-2.69][24040-3-0-1.21]
[24111-1-4-0.87][24182-0-0-4.00][24238-3-3-3.94][24290-2-0-3.95][24345-0-0-0.56][24364-1-2-0.30][24427-3-0-3.07][24477-2-2-2.90][24495-2-4--0.27][24893-2-2-1.63]
[25012-1-2-1.16][25121-2-2-2.00][25165-3-3-3.27][25183-0-0-2.68][25297-3-3-3.98][25398-0-0-2.32][25574-2-2-1.24][25644-1-2-3.66][25718-1-3-1.16][25774-2-2-1.40]
[26032-3-3-1.86][26051-3-3-4.71][26120-0-0-1.40][26321-1-1-3.24][26732-1-1-2.60][26784-3-3-5.50][26827-3-3-0.42][26833-0-3-2.80][26838-2-3-0.57][26860-1-2-0.55]
[26948-0-0-2.38][27049-3-0-3.06][27098-1-1-0.71][27526-0-3-3.37][27639-3-3-1.04][27698-3-3-2.96][27772-0-0-3.71][27890-1-1-4.44][28040-0-0-2.33][28503-2-2-4.62]
[28577-1-1-1.96][28959-0-0-4.79][29198-3-3-3.39][29777-0-0-6.03][29877-2-2-2.29][30035-1-1-4.41][30098-0-0-1.92][30326-1-1-5.36][30572-2-2-3.49][30716-0-4-0.05]
[30806-2-2-1.10][30906-1-1-2.60][31007-0-0-1.00][31181-3-3-2.29][31238-0-3-2.06][31347-0-0-4.10][31422-2-2-1.15][31429-3-3-2.16][31431-0-3-0.14][31432-1-1-2.76]
[31477-0-3-4.14][31524-1-3-1.09][31597-1-2-2.02][31619-1-3-0.55][31701-0-0-3.41][31755-0-0-3.28][31854-3-3-3.58][32074-1-3-1.37][32078-3-3-3.17][32111-1-1-2.52]
[32127-1-2-1.80][32140-3-3-4.91][32263-2-0-0.11][32365-0-0-2.76][32411-2-3-3.36][32429-3-3-2.94][32473-3-0-0.97][32574-3-3-2.74][32584-0-0-0.83][32622-0-3-0.65]
[32858-3-0-1.90][32969-3-3-3.61][33016-2-2-3.70][33031-1-3-3.78][33035-2-2-2.07][33133-2-2-1.78][33173-2-2-0.53][33175-3-3-0.23][33306-3-3-1.27][33309-2-3-0.57]
[33474-0-3-1.26][33478-2-3-1.14][33618-1-2-0.26][33712-0-3-2.06][33782-2-2-0.34][33914-3-2-1.22][34076-3-2-0.95][34112-2-2-3.64][34138-2-2-1.62][34239-1-1-0.58]
[34364-2-2-3.30][34617-1-2-1.78][34751-3-3-3.77][34783-2-2-1.16][35015-3-3-1.48][35018-1-1-1.13][35288-2-3-0.99][0-4-2-1.94][1-4-4-0.63][2-4-4-0.64]
[3-4-1-1.35][4-4-1-0.22][5-4-1-1.42][6-4-0-2.00][7-4-2-0.53][8-4-2-1.12][9-4-2-0.40][10-4-4-1.46][11-4-2-2.19][12-4-2--0.05]
[14-4-3-1.04][15-4-3-2.59][16-4-4--0.12][17-4-1--0.05][18-4-4-3.31][19-4-3-2.51][20-4-0-1.43][21-4-2-1.53][22-4-0-0.88][23-4-0-0.46]
[24-4-4-3.13][25-4-2-0.53][26-4-3-0.63][27-4-2-2.58][28-4-4-1.31][29-4-1-1.34][30-4-4--0.21][31-4-2--0.03][32-4-1-0.64][33-4-2-1.71]
[34-4-4-0.24][35-4-0-1.37][37-4-4-0.40][39-4-0-0.93][40-4-4--0.02][41-4-1--0.50][42-4-3-0.54][43-4-3-0.32][45-4-2-0.66][46-4-4-2.67]
[47-4-4-2.77][48-4-4-0.89][51-4-4-0.95][52-4-4-1.19][53-4-0-0.74][54-4-3-1.41][55-4-3-0.72][56-4-1-0.98][57-4-3-0.81][58-4-2-2.79]
[59-4-0-2.00][60-4-1--0.07][61-4-4-1.44][62-4-2-0.49][63-4-2-2.05][64-4-2-0.78][65-4-4-2.88][66-4-2-1.08][67-4-2--0.16][68-4-3-0.75]
[69-4-2-0.76][70-4-4-1.02][72-4-1-0.82][73-4-1-0.97][74-4-2-2.38][75-4-0-0.68][77-4-4-3.14][78-4-2--0.17][79-4-2-1.59][80-4-4-1.39]
[81-4-4-1.76][82-4-1-0.86][83-4-4--0.28][84-4-4-1.98][85-4-2-1.21][86-4-2-0.76][87-4-4-1.89][88-4-4-0.88][89-4-2-1.03][90-4-4-0.27]
[91-4-4--0.40][92-4-4--0.27][93-4-0-2.72][94-4-2-1.44][95-4-2-0.04][96-4-4-0.47][97-4-4-1.75][98-4-2-2.77][99-4-4-0.74][100-4-2-0.51]
[101-4-4-2.64][102-4-4-0.01][103-4-0-0.21][104-4-4-0.66][105-4-2-1.79][106-4-4-3.44][107-4-0-0.70][108-4-4-0.81][109-4-3-0.87][110-4-4-1.16]
[111-4-0-2.75][112-4-0-0.59][113-4-3-0.41][114-4-3-1.02][115-4-3--0.12][116-4-0-0.90][117-4-4-1.24][119-4-4-1.00][121-4-4-0.68][122-4-4-0.96]
[124-4-3-0.05][125-4-2-2.03][126-4-4-1.96][127-4-1-0.80][128-4-3-0.13][129-4-3-0.23][130-4-2-1.46][131-4-2-1.01][132-4-2-0.06][133-4-4-1.97]
[135-4-2-1.18][136-4-0--0.26][137-4-4-0.14][138-4-0-0.15][139-4-4-0.59][140-4-1-0.55][141-4-0-0.55][142-4-4-2.39][143-4-4-1.35][144-4-4-2.59]
[145-4-2-1.47][148-4-0-4.77][149-4-3-1.09][150-4-2-2.80][151-4-4-0.56][152-4-4-1.00][153-4-2-3.08][154-4-4-1.11][155-4-4-1.18][156-4-3-1.55]
[157-4-2-1.40][158-4-3-1.98][160-4-1-0.88][161-4-2-2.32][162-4-4--0.06][164-4-2-1.34][165-4-2-0.73][167-4-3-1.09][168-4-0-0.93][170-4-3-0.61]
[171-4-4-0.52][172-4-4-1.55][173-4-4-1.70][174-4-0-2.58][175-4-4-0.29][177-4-0-1.84][178-4-4-0.39][179-4-4-0.40][180-4-4-1.97][181-4-3-0.57]
[182-4-3-1.73][183-4-4-0.77][184-4-2-1.74][186-4-3-0.11][187-4-2-0.53][188-4-4-0.83][189-4-2-0.84][190-4-3-0.30][191-4-4-0.76][192-4-0-0.81]
[193-4-2-2.11][194-4-3-0.06][195-4-0-0.57][196-4-2-2.53][197-4-4-0.33][198-4-4-4.97][199-4-2-1.54]
---------------------------
I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.408 | Acc: 75.500% | Wgt Acc: 86.518%
	I - Batch: 100 | Loss: 0.418 | Acc: 76.250% | Wgt Acc: 86.740%
	I - Batch: 150 | Loss: 0.419 | Acc: 76.583% | Wgt Acc: 87.036%
	I - Batch: 200 | Loss: 0.413 | Acc: 77.375% | Wgt Acc: 87.486%
	I - Batch: 250 | Loss: 0.414 | Acc: 77.150% | Wgt Acc: 87.415%
I - num batch: 273
I - Train -- Loss: 0.413 | Acc: 77.098% | Wgt Acc: 87.479% | LR: 2.500000e-04 | Dur: 165.02s
I - Confusion Matrix: [row->prediction - col->label]
[[698.   7.  14.  48. 166.]
 [  9. 704.  13.  17.  98.]
 [ 24.  28. 980.  16. 288.]
 [ 53.  14.  12. 682. 149.]
 [ 13.   7.  13.  10. 299.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.200 | Acc: 55.621% | Wgt Acc: 62.646% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  2.  4. 20. 33.]
 [ 0. 44.  6.  4. 20.]
 [ 1. 18. 48.  6. 39.]
 [15.  9. 13. 54. 21.]
 [ 3.  5.  4.  2. 67.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 0.412 | Acc: 78.000% | Wgt Acc: 87.972%
	I - Batch: 100 | Loss: 0.407 | Acc: 77.625% | Wgt Acc: 87.812%
	I - Batch: 150 | Loss: 0.411 | Acc: 76.917% | Wgt Acc: 87.547%
	I - Batch: 200 | Loss: 0.403 | Acc: 77.688% | Wgt Acc: 87.838%
	I - Batch: 250 | Loss: 0.399 | Acc: 77.625% | Wgt Acc: 88.112%
I - num batch: 273
I - Train -- Loss: 0.395 | Acc: 77.671% | Wgt Acc: 88.211% | LR: 2.500000e-04 | Dur: 164.66s
I - Confusion Matrix: [row->prediction - col->label]
[[712.   5.  10.  47. 154.]
 [ 10. 715.  13.  17. 123.]
 [ 19.  24. 983.  20. 292.]
 [ 50.   7.  16. 681. 134.]
 [  6.   9.  10.   8. 297.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.232 | Acc: 53.057% | Wgt Acc: 60.166% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  3.  5. 26. 30.]
 [ 1. 42.  8.  2. 18.]
 [ 5. 19. 42.  4. 48.]
 [10. 10. 17. 52. 22.]
 [ 1.  4.  3.  2. 62.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.401 | Acc: 77.250% | Wgt Acc: 88.137%
	I - Batch: 100 | Loss: 0.383 | Acc: 78.438% | Wgt Acc: 89.001%
	I - Batch: 150 | Loss: 0.378 | Acc: 78.042% | Wgt Acc: 88.632%
	I - Batch: 200 | Loss: 0.383 | Acc: 77.812% | Wgt Acc: 88.382%
	I - Batch: 250 | Loss: 0.385 | Acc: 77.725% | Wgt Acc: 88.404%
I - num batch: 273
I - Train -- Loss: 0.383 | Acc: 77.831% | Wgt Acc: 88.506% | LR: 2.500000e-04 | Dur: 164.49s
I - Confusion Matrix: [row->prediction - col->label]
[[714.   4.   8.  40. 177.]
 [  8. 713.  15.  20. 114.]
 [ 27.  20. 985.  16. 274.]
 [ 36.  13.  14. 690. 142.]
 [ 12.  10.  10.   7. 293.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.272 | Acc: 55.424% | Wgt Acc: 63.672% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  2.  4.  9. 20.]
 [ 1. 40.  8.  1. 12.]
 [ 1. 26. 51.  6. 57.]
 [21.  7. 10. 67. 29.]
 [ 4.  3.  2.  3. 62.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.309 | Acc: 81.625% | Wgt Acc: 91.231%
	I - Batch: 100 | Loss: 0.317 | Acc: 81.250% | Wgt Acc: 91.189%
	I - Batch: 150 | Loss: 0.324 | Acc: 80.708% | Wgt Acc: 91.268%
	I - Batch: 200 | Loss: 0.325 | Acc: 81.031% | Wgt Acc: 91.373%
	I - Batch: 250 | Loss: 0.332 | Acc: 80.625% | Wgt Acc: 91.104%
I - num batch: 273
I - Train -- Loss: 0.330 | Acc: 80.674% | Wgt Acc: 91.085% | LR: 1.250000e-04 | Dur: 164.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 736.    4.    5.   33.  153.]
 [   7.  734.    6.    8.  103.]
 [  11.   11. 1001.    9.  285.]
 [  29.    4.    7.  714.  125.]
 [  14.    7.   13.    9.  334.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.202 | Acc: 57.199% | Wgt Acc: 63.591% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  1.  3. 11. 21.]
 [ 0. 40.  7.  1. 15.]
 [ 1. 20. 45.  3. 38.]
 [19. 12. 17. 68. 32.]
 [ 5.  5.  3.  3. 74.]]

I - Local maximum validation set accuracy:  57.20

I - Validation set results: 
[14-1-2-0.50][50-3-1-2.72][124-2-3-0.40][127-0-0-5.10][443-2-2-2.39][567-0-0-2.04][573-1-1-2.74][615-0-0-1.84][695-1-2-0.89][722-3-3-3.48]
[826-0-0-3.08][878-0-0-6.07][1103-0-0-2.02][1212-3-3-0.22][1368-0-0-5.31][2181-2-3-2.32][2476-2-2-1.69][2721-2-2-2.66][2818-1-3-0.99][2886-2-2-1.59]
[3231-2-2-3.13][3333-2-3-1.43][3482-2-2-2.26][3536-3-3-0.51][3625-1-1-2.36][3909-0-0-3.33][4035-0-0-2.36][4140-0-0-2.02][4214-1-3-1.80][4346-1-3-1.75]
[4581-2-2-1.95][4708-3-3-2.15][4838-3-0-1.48][4845-1-3-1.11][4868-0-0-4.02][4939-0-4-0.56][4984-2-2-1.17][5078-1-2-1.18][5396-0-0-5.87][5479-1-1-2.32]
[5717-0-0-2.30][5843-1-2-0.45][5949-3-3-1.76][5987-2-2-2.77][6014-3-3-3.52][6033-3-3-0.79][6313-0-0-3.00][6421-3-3-2.56][6500-1-2-0.86][6583-3-3-1.87]
[6683-3-3-3.30][6825-2-1-0.96][6998-3-3-1.34][7049-3-3-2.54][7517-1-3-0.76][7521-1-1--0.57][7528-1-3-2.61][7949-1-2-4.10][8135-1-0-0.80][8185-3-3-3.43]
[8269-3-2-1.19][8273-3-3-3.49][8543-3-0-4.76][8666-1-1-3.20][8672-0-0-5.67][8903-1-2-1.41][9001-2-1-1.62][9036-2-2-2.65][9281-3-3-1.54][9300-2-2-5.37]
[9571-0-3-1.18][9617-1-3-0.78][9644-2-2-3.41][9705-2-0-0.60][9801-0-3-2.06][9803-3-3-1.76][9865-3-3-5.46][9896-2-2-1.24][10314-1-1-0.48][10337-3-3-5.60]
[10403-0-4-0.43][10653-2-4--0.16][10704-2-1-1.80][10719-1-2-2.34][10727-1-4-0.27][10836-0-0-7.73][10969-2-3-2.79][11042-0-0-1.07][11088-1-1-4.18][11322-0-0-2.86]
[11398-2-2-0.52][11499-0-0-3.12][11502-3-3-0.83][11512-3-3-2.02][11608-1-1-2.27][11610-0-0-1.08][11692-0-3-3.44][11905-0-0-4.35][11993-1-1-4.28][12002-2-2-1.78]
[12052-0-0-3.05][12201-0-3-4.76][12235-2-2-1.48][12320-1-4-2.80][12377-2-4-2.40][12398-2-2-0.48][12503-1-1-0.82][12617-0-3-0.45][12685-3-3-1.95][12738-2-3-2.41]
[12742-2-2-4.78][12823-0-3-3.29][13110-1-2-2.41][13240-3-3-2.66][13253-1-1-2.27][13273-0-0-5.40][13634-1-1-2.25][13763-2-3-2.17][13905-3-3-2.14][14060-2-1-1.59]
[14065-3-0-1.12][14147-3-3-3.56][14595-2-1-1.81][14687-2-2-4.87][14788-2-2-3.01][14869-1-1-4.45][14872-3-4--0.05][14877-1-1-2.02][14927-0-3-2.23][15066-0-0-5.97]
[15175-1-1-1.93][15178-2-2-1.61][15375-3-2--0.60][15389-3-3-3.35][15568-2-1-0.56][15675-3-3-4.06][15869-1-3-1.62][16207-3-0-0.95][16236-0-0-2.11][16302-3-3-1.68]
[16331-2-2-4.01][16381-0-3-1.92][16488-1-1-6.34][16495-0-0-3.38][16650-0-0-3.68][16719-1-4-0.65][16801-0-0-6.14][16828-0-0-3.47][17137-3-0-1.86][17245-1-4-0.46]
[17278-3-0-0.97][17282-0-0-2.11][17311-2-2-2.01][17336-2-3-0.38][17608-3-3-5.05][17627-0-0-0.74][17877-3-3-0.74][17924-1-2-1.41][17984-3-0-3.55][18211-0-3-1.56]
[18276-3-3-2.80][18287-1-1-0.81][18394-0-0-2.48][18428-0-0-2.72][18442-0-3-2.85][18478-3-3-1.64][18607-0-0-3.50][18616-0-0-1.49][18663-0-0-3.25][18718-0-0-3.50]
[18766-2-2-1.98][18824-2-2-1.08][18890-3-3-2.73][18930-3-4-1.32][18938-3-3-2.74][19817-1-2-3.76][19839-0-2-0.45][19930-3-3-3.26][19944-0-0-1.46][20036-2-2-3.88]
[20101-3-3-1.46][20474-1-1-2.26][20547-3-3-0.44][20929-2-2-3.21][21245-1-2-2.09][21257-3-3-1.14][21293-1-2-2.09][21316-1-1-5.95][21384-1-1-2.21][21448-1-2-1.04]
[21483-0-0-4.35][21487-2-2-2.92][21714-0-3-0.96][21943-3-3-1.34][21947-0-0-1.91][21948-0-0-6.41][21965-2-2-4.09][21998-1-1-4.63][22025-0-4-0.91][22228-3-3-5.87]
[22446-1-1-2.89][22494-3-3-2.90][22757-0-0-3.51][22811-3-3-5.68][22976-3-4-0.50][22985-3-3-4.85][23014-0-0-4.40][23112-1-1-1.79][23144-3-3-4.91][23168-2-3-2.12]
[23219-0-0-1.40][23363-3-3-5.07][23470-0-0-2.38][23486-2-3-0.77][23497-0-3-4.74][23516-0-0-4.24][23690-1-1-1.75][23921-2-1-0.74][23936-1-2-0.42][24040-3-0-0.58]
[24111-1-4-1.49][24182-0-0-3.83][24238-3-3-4.42][24290-2-0-3.75][24345-0-0-3.16][24364-1-2--0.00][24427-3-0-2.68][24477-2-2-3.38][24495-2-4-0.96][24893-2-2-1.95]
[25012-1-2-1.13][25121-2-2-2.29][25165-3-3-3.97][25183-0-0-4.32][25297-3-3-4.67][25398-0-0-2.54][25574-2-2-1.23][25644-1-1-2.37][25718-1-3-0.01][25774-2-2-0.36]
[26032-3-3-2.32][26051-3-3-6.11][26120-0-4-2.24][26321-1-1-0.24][26732-1-1-1.36][26784-3-3-5.99][26827-3-3-1.26][26833-0-3-2.06][26838-2-3-1.58][26860-1-1-0.71]
[26948-0-0-3.47][27049-3-0-4.15][27098-1-1-1.75][27526-0-3-2.37][27639-3-3-1.60][27698-3-3-4.49][27772-0-0-1.94][27890-1-1-2.22][28040-0-0-0.47][28503-2-2-3.48]
[28577-1-1-1.91][28959-0-0-3.52][29198-3-3-4.06][29777-0-0-5.21][29877-2-2-1.04][30035-1-1-2.98][30098-0-3-2.21][30326-1-1-6.17][30572-2-2-1.74][30716-0-4-1.13]
[30806-2-3-2.48][30906-1-1-2.25][31007-0-0-0.57][31181-3-3-2.98][31238-0-0-2.11][31347-0-0-2.94][31422-2-2-0.08][31429-3-3-2.99][31431-0-3--0.03][31432-1-1-2.58]
[31477-0-3-4.09][31524-1-3-0.33][31597-1-2-1.21][31619-1-3-0.66][31701-0-0-2.92][31755-0-0-3.05][31854-3-3-4.04][32074-1-1-1.05][32078-3-3-4.29][32111-1-1-0.20]
[32127-1-1-1.11][32140-3-3-5.64][32263-2-0-0.71][32365-0-0-3.68][32411-2-3-3.19][32429-3-3-3.38][32473-3-0-1.55][32574-3-3-3.42][32584-0-0-1.59][32622-0-3-0.30]
[32858-3-3-2.32][32969-3-3-4.89][33016-2-2-3.20][33031-1-3-4.35][33035-2-2-2.20][33133-2-2-1.25][33173-2-3--0.09][33175-3-3-0.41][33306-3-2-1.26][33309-2-3-0.76]
[33474-0-3-1.69][33478-2-3-1.12][33618-1-2-0.32][33712-0-0-1.70][33782-2-2-0.83][33914-3-3-4.25][34076-3-3-1.22][34112-2-2-2.06][34138-2-3-2.03][34239-1-1-1.18]
[34364-2-2-3.20][34617-1-2-1.45][34751-3-3-4.41][34783-2-2-2.14][35015-3-3-1.31][35018-1-1-2.92][35288-2-3-0.39][0-4-2-2.29][1-4-4-1.35][2-4-0-1.96]
[3-4-4-1.59][4-4-1-0.25][5-4-3-2.02][6-4-4-2.12][7-4-4-0.62][8-4-2-0.82][9-4-2-0.79][10-4-4-2.55][11-4-2-3.42][12-4-1-0.71]
[14-4-3-1.80][15-4-3-2.49][16-4-4-1.56][17-4-1-0.90][18-4-4-3.91][19-4-3-2.37][20-4-0-1.84][21-4-2-0.86][22-4-4-1.86][23-4-0--0.08]
[24-4-4-5.61][25-4-3-1.37][26-4-3-0.88][27-4-2-2.61][28-4-4-2.82][29-4-1-1.61][30-4-4--0.12][31-4-1-0.14][32-4-1-0.43][33-4-2-2.55]
[34-4-4-0.40][35-4-3-1.03][37-4-3-1.47][39-4-0-0.97][40-4-1-0.58][41-4-3-0.54][42-4-3-1.37][43-4-3--0.21][45-4-2-0.50][46-4-4-3.41]
[47-4-4-3.48][48-4-4-1.69][51-4-4-1.68][52-4-4-1.42][53-4-4--0.16][54-4-3-1.71][55-4-3-2.25][56-4-1-0.80][57-4-3-1.20][58-4-2-0.79]
[59-4-0-2.38][60-4-4-0.51][61-4-4-1.88][62-4-3-2.14][63-4-4-1.87][64-4-2-0.52][65-4-4-3.71][66-4-4-2.49][67-4-0-0.57][68-4-3-1.74]
[69-4-3-0.67][70-4-4-2.65][72-4-4-0.91][73-4-1-2.70][74-4-2-1.60][75-4-0-0.08][77-4-4-1.36][78-4-3-0.01][79-4-2-2.70][80-4-4-1.38]
[81-4-2-2.50][82-4-1-0.00][83-4-4-1.29][84-4-4-0.90][85-4-4-2.85][86-4-4-1.00][87-4-4-2.36][88-4-4-0.96][89-4-2-0.42][90-4-2-1.02]
[91-4-2-0.54][92-4-4-0.27][93-4-0-3.02][94-4-2-1.27][95-4-4-0.57][96-4-4-0.66][97-4-4-2.16][98-4-2-2.13][99-4-4-0.78][100-4-2-1.57]
[101-4-4-3.59][102-4-4-0.66][103-4-3-0.05][104-4-4-0.98][105-4-4-2.08][106-4-4-2.46][107-4-0-2.07][108-4-4-0.90][109-4-3-1.01][110-4-0-0.92]
[111-4-0-4.15][112-4-0-0.09][113-4-4--0.07][114-4-3-1.54][115-4-4-0.26][116-4-0-0.28][117-4-4-1.42][119-4-2-3.44][121-4-4-1.58][122-4-4-1.47]
[124-4-3-0.62][125-4-4-2.52][126-4-4-3.00][127-4-2-0.72][128-4-3-1.03][129-4-3-0.76][130-4-2-1.37][131-4-3-0.95][132-4-2-0.69][133-4-4-4.17]
[135-4-2-1.23][136-4-1-0.79][137-4-4-0.77][138-4-2-0.40][139-4-3-0.61][140-4-1-0.50][141-4-0-0.65][142-4-4-2.11][143-4-2-2.47][144-4-4-3.01]
[145-4-2-3.02][148-4-0-4.00][149-4-4-1.26][150-4-4-2.21][151-4-4-1.10][152-4-4-1.83][153-4-2-2.15][154-4-4-4.40][155-4-4-1.98][156-4-3-1.71]
[157-4-0-2.19][158-4-3-1.47][160-4-1-1.61][161-4-2-1.83][162-4-4--0.21][164-4-4-1.35][165-4-2-0.50][167-4-0-1.84][168-4-4-1.01][170-4-3-0.67]
[171-4-4-1.72][172-4-4-2.61][173-4-4-3.29][174-4-0-3.28][175-4-4-1.98][177-4-0-2.88][178-4-2-0.21][179-4-0-1.07][180-4-4-2.09][181-4-4-0.94]
[182-4-3-2.96][183-4-4-0.73][184-4-2-0.91][186-4-3--0.31][187-4-2-0.33][188-4-2-2.26][189-4-1-0.21][190-4-3-0.42][191-4-4-1.52][192-4-4-0.35]
[193-4-2-1.46][194-4-2--0.52][195-4-0-1.52][196-4-2-2.90][197-4-1-0.53][198-4-4-5.10][199-4-2-2.10]
---------------------------
I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773.  781.]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.281 | Acc: 84.125% | Wgt Acc: 92.021%
	I - Batch: 100 | Loss: 0.294 | Acc: 82.500% | Wgt Acc: 91.379%
	I - Batch: 150 | Loss: 0.290 | Acc: 83.083% | Wgt Acc: 91.902%
	I - Batch: 200 | Loss: 0.290 | Acc: 82.969% | Wgt Acc: 92.178%
	I - Batch: 250 | Loss: 0.285 | Acc: 83.225% | Wgt Acc: 92.450%
I - num batch: 259
I - Train -- Loss: 0.287 | Acc: 83.128% | Wgt Acc: 92.389% | LR: 1.250000e-04 | Dur: 156.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 737.    4.    7.   22.  110.]
 [   4.  735.   10.    5.   82.]
 [  10.   13. 1006.   11.  238.]
 [  39.    6.    3.  731.  116.]
 [   7.    2.    6.    4.  235.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.267 | Acc: 54.043% | Wgt Acc: 61.319% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  0.  4. 10. 13.]
 [ 0. 37.  8.  3. 18.]
 [ 6. 27. 49.  5. 62.]
 [20. 11. 13. 65. 23.]
 [ 3.  3.  1.  3. 64.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.330 | Acc: 79.750% | Wgt Acc: 90.261%
	I - Batch: 100 | Loss: 0.314 | Acc: 80.188% | Wgt Acc: 91.122%
	I - Batch: 150 | Loss: 0.317 | Acc: 80.458% | Wgt Acc: 91.021%
	I - Batch: 200 | Loss: 0.313 | Acc: 80.656% | Wgt Acc: 91.310%
	I - Batch: 250 | Loss: 0.309 | Acc: 80.975% | Wgt Acc: 91.483%
I - num batch: 273
I - Train -- Loss: 0.307 | Acc: 81.270% | Wgt Acc: 91.631% | LR: 1.250000e-04 | Dur: 167.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 733.    6.    6.   22.  158.]
 [   4.  726.    5.    8.  104.]
 [  17.   13. 1012.    7.  257.]
 [  28.    9.    6.  731.  138.]
 [  15.    6.    3.    5.  343.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.291 | Acc: 55.621% | Wgt Acc: 62.023% | Dur: 14.68s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  3.  6. 15. 26.]
 [ 1. 36.  3.  2. 11.]
 [ 1. 26. 46.  2. 36.]
 [21.  8. 16. 67. 36.]
 [ 3.  5.  4.  0. 71.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.279 | Acc: 82.750% | Wgt Acc: 92.283%
	I - Batch: 100 | Loss: 0.273 | Acc: 82.938% | Wgt Acc: 92.518%
	I - Batch: 150 | Loss: 0.274 | Acc: 82.750% | Wgt Acc: 92.412%
	I - Batch: 200 | Loss: 0.285 | Acc: 82.344% | Wgt Acc: 92.011%
	I - Batch: 250 | Loss: 0.293 | Acc: 82.100% | Wgt Acc: 91.709%
I - num batch: 273
I - Train -- Loss: 0.295 | Acc: 81.935% | Wgt Acc: 91.621% | LR: 1.250000e-04 | Dur: 164.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 735.    4.    4.   23.  135.]
 [   7.  723.    5.   11.   91.]
 [  16.   17. 1008.    6.  257.]
 [  26.    7.    6.  728.  137.]
 [  13.    9.    9.    5.  380.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.183 | Acc: 57.791% | Wgt Acc: 65.137% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  2.  5. 16. 36.]
 [ 2. 47.  7.  3. 21.]
 [ 2. 16. 50.  5. 37.]
 [16. 11.  9. 61. 16.]
 [ 3.  2.  4.  1. 70.]]

I - Local maximum validation set accuracy:  57.79

I - Validation set results: 
[14-1-1-1.10][50-3-1-3.62][124-2-2-2.70][127-0-0-4.78][443-2-2-3.02][567-0-0-3.05][573-1-1-5.12][615-0-3-1.79][695-1-2-0.71][722-3-3-3.48]
[826-0-0-4.39][878-0-0-6.17][1103-0-0-0.67][1212-3-3-0.58][1368-0-0-5.86][2181-2-3-1.21][2476-2-2-1.99][2721-2-2-3.34][2818-1-3-1.27][2886-2-2-1.45]
[3231-2-2-3.65][3333-2-3-1.54][3482-2-2-2.43][3536-3-2-0.46][3625-1-1-4.10][3909-0-0-2.78][4035-0-0-2.18][4140-0-0-1.73][4214-1-4-1.70][4346-1-3-0.67]
[4581-2-2-2.74][4708-3-3-2.08][4838-3-0-1.74][4845-1-2-1.19][4868-0-0-2.43][4939-0-0-0.97][4984-2-2-1.59][5078-1-2-1.44][5396-0-0-7.23][5479-1-1-3.19]
[5717-0-0-3.51][5843-1-1-0.93][5949-3-3-2.71][5987-2-4-1.20][6014-3-3-2.14][6033-3-3-0.70][6313-0-0-2.51][6421-3-3-2.50][6500-1-2-0.77][6583-3-3-0.81]
[6683-3-3-3.01][6825-2-1-2.06][6998-3-3-0.53][7049-3-3-1.34][7517-1-1-2.05][7521-1-1-2.13][7528-1-3-2.81][7949-1-2-3.82][8135-1-0-0.43][8185-3-0-4.25]
[8269-3-1-2.18][8273-3-3-2.52][8543-3-0-4.58][8666-1-1-1.20][8672-0-0-2.89][8903-1-2-1.11][9001-2-2-1.21][9036-2-2-2.83][9281-3-3-1.94][9300-2-2-4.75]
[9571-0-3-0.31][9617-1-1-2.88][9644-2-2-4.10][9705-2-1-1.01][9801-0-3-2.11][9803-3-3-1.48][9865-3-3-4.00][9896-2-1-0.66][10314-1-1-0.98][10337-3-3-3.60]
[10403-0-4-0.61][10653-2-2-0.84][10704-2-2-2.51][10719-1-1-1.82][10727-1-2--0.11][10836-0-0-7.19][10969-2-3-1.81][11042-0-0-1.06][11088-1-1-5.94][11322-0-0-4.24]
[11398-2-2-1.95][11499-0-0-1.46][11502-3-3-1.30][11512-3-3-1.83][11608-1-1-1.88][11610-0-0-2.12][11692-0-3-2.12][11905-0-0-3.60][11993-1-1-4.19][12002-2-0-2.41]
[12052-0-0-3.33][12201-0-3-4.35][12235-2-4-0.68][12320-1-4-2.00][12377-2-4-2.17][12398-2-2-1.08][12503-1-1-0.21][12617-0-2-0.61][12685-3-3-0.24][12738-2-0-0.70]
[12742-2-2-5.24][12823-0-0-4.45][13110-1-1-1.80][13240-3-0-2.63][13253-1-1-4.89][13273-0-0-6.49][13634-1-1-2.36][13763-2-3-0.27][13905-3-3-0.95][14060-2-2-2.96]
[14065-3-3-0.82][14147-3-3-3.67][14595-2-1-2.12][14687-2-2-5.63][14788-2-2-2.65][14869-1-1-3.87][14872-3-0-0.89][14877-1-1-2.91][14927-0-3-0.66][15066-0-0-6.25]
[15175-1-1-1.90][15178-2-0-1.16][15375-3-0--0.11][15389-3-3-3.51][15568-2-1-1.06][15675-3-3-3.28][15869-1-3-1.12][16207-3-0-0.97][16236-0-0-0.76][16302-3-3-1.46]
[16331-2-2-5.27][16381-0-0-1.58][16488-1-1-4.41][16495-0-0-3.50][16650-0-0-4.63][16719-1-2-1.95][16801-0-0-4.42][16828-0-0-2.35][17137-3-0-1.48][17245-1-3-0.37]
[17278-3-0-0.86][17282-0-0-2.21][17311-2-2-2.30][17336-2-2-1.59][17608-3-3-4.54][17627-0-0-1.44][17877-3-3-0.51][17924-1-2-0.17][17984-3-0-2.49][18211-0-3-1.16]
[18276-3-3-3.26][18287-1-1-2.72][18394-0-0-3.65][18428-0-0-5.10][18442-0-3-3.18][18478-3-3-1.36][18607-0-0-3.06][18616-0-0-1.18][18663-0-0-4.66][18718-0-0-3.73]
[18766-2-2-3.16][18824-2-2-1.88][18890-3-3-3.42][18930-3-4-0.67][18938-3-3-1.48][19817-1-2-3.06][19839-0-0-0.60][19930-3-3-2.48][19944-0-4-0.12][20036-2-2-5.15]
[20101-3-3-2.67][20474-1-1-4.18][20547-3-3-1.66][20929-2-2-4.60][21245-1-1-0.52][21257-3-3-0.20][21293-1-2-3.32][21316-1-1-4.29][21384-1-1-2.58][21448-1-1-1.67]
[21483-0-0-4.90][21487-2-2-2.71][21714-0-3-0.71][21943-3-3-1.30][21947-0-0-2.76][21948-0-0-5.99][21965-2-2-3.50][21998-1-1-4.37][22025-0-2-1.68][22228-3-3-4.76]
[22446-1-1-3.69][22494-3-0-2.02][22757-0-0-3.81][22811-3-3-5.15][22976-3-2-2.15][22985-3-3-3.86][23014-0-0-4.67][23112-1-1-4.86][23144-3-3-4.06][23168-2-3-0.70]
[23219-0-0-1.01][23363-3-3-2.70][23470-0-1-0.45][23486-2-2-2.47][23497-0-3-3.73][23516-0-0-3.42][23690-1-3-1.14][23921-2-2-1.63][23936-1-2-2.46][24040-3-0-0.91]
[24111-1-2-0.65][24182-0-0-3.70][24238-3-3-3.97][24290-2-0-3.56][24345-0-0-0.89][24364-1-3-0.25][24427-3-0-2.59][24477-2-2-2.96][24495-2-1-1.11][24893-2-2-2.53]
[25012-1-3-0.16][25121-2-2-2.60][25165-3-3-2.95][25183-0-0-2.98][25297-3-3-4.13][25398-0-0-3.40][25574-2-2-3.06][25644-1-1-2.57][25718-1-0--0.23][25774-2-2-1.29]
[26032-3-3-1.93][26051-3-3-4.84][26120-0-0-1.07][26321-1-1-0.96][26732-1-1-2.26][26784-3-3-5.44][26827-3-3-0.26][26833-0-3-3.07][26838-2-2-0.50][26860-1-2-0.38]
[26948-0-0-2.00][27049-3-0-3.30][27098-1-1-1.48][27526-0-0-2.00][27639-3-3-1.17][27698-3-3-3.06][27772-0-0-1.66][27890-1-1-2.77][28040-0-0-2.68][28503-2-2-4.60]
[28577-1-1-2.43][28959-0-0-3.95][29198-3-3-2.91][29777-0-0-6.85][29877-2-1-1.10][30035-1-1-4.89][30098-0-3-2.05][30326-1-1-8.06][30572-2-2-2.72][30716-0-4-0.25]
[30806-2-2-1.71][30906-1-1-2.97][31007-0-0-2.08][31181-3-2-0.82][31238-0-3-1.76][31347-0-0-2.94][31422-2-2-0.53][31429-3-3-1.97][31431-0-3-0.48][31432-1-1-1.87]
[31477-0-3-3.10][31524-1-2-1.09][31597-1-1-2.53][31619-1-3-0.62][31701-0-0-3.43][31755-0-0-4.57][31854-3-3-3.14][32074-1-3-0.73][32078-3-3-2.87][32111-1-1-1.84]
[32127-1-1-1.39][32140-3-3-3.56][32263-2-0-0.64][32365-0-0-2.94][32411-2-3-4.29][32429-3-3-2.14][32473-3-0-2.04][32574-3-0-2.69][32584-0-0-1.50][32622-0-1-0.78]
[32858-3-3-1.27][32969-3-3-4.90][33016-2-2-4.22][33031-1-3-3.88][33035-2-2-1.84][33133-2-2-2.46][33173-2-2-0.04][33175-3-1-0.56][33306-3-2-0.79][33309-2-3-0.04]
[33474-0-3-0.61][33478-2-3-0.74][33618-1-1-0.50][33712-0-0-1.21][33782-2-4-0.83][33914-3-3-4.89][34076-3-2-1.32][34112-2-2-4.34][34138-2-3-0.24][34239-1-1-2.14]
[34364-2-2-3.80][34617-1-2-2.04][34751-3-3-3.86][34783-2-2-2.20][35015-3-3-0.72][35018-1-1-2.31][35288-2-2-0.30][0-4-2-2.06][1-4-4-2.11][2-4-0-2.14]
[3-4-4-0.87][4-4-1-0.26][5-4-1-0.13][6-4-0-2.17][7-4-2--0.09][8-4-2-0.81][9-4-2-0.74][10-4-4-2.52][11-4-2-3.15][12-4-1-1.30]
[14-4-3-0.15][15-4-3-2.22][16-4-4-0.20][17-4-0-0.39][18-4-4-4.13][19-4-0-1.89][20-4-0-1.16][21-4-2-0.84][22-4-4-0.59][23-4-0-0.98]
[24-4-4-4.21][25-4-3-1.03][26-4-1--0.11][27-4-2-1.69][28-4-4-0.97][29-4-1-1.47][30-4-3-0.45][31-4-4-0.30][32-4-1-1.78][33-4-2-1.10]
[34-4-4-0.20][35-4-0-1.08][37-4-4-1.18][39-4-0-2.94][40-4-0-0.13][41-4-1--0.12][42-4-3-0.48][43-4-2-0.53][45-4-3--0.00][46-4-4-2.28]
[47-4-4-3.18][48-4-4-1.11][51-4-4-1.34][52-4-4-0.82][53-4-0-1.08][54-4-3-1.75][55-4-0--0.81][56-4-2-0.43][57-4-0-1.08][58-4-2-2.43]
[59-4-0-2.29][60-4-4-0.71][61-4-4-1.28][62-4-2-0.80][63-4-2-2.49][64-4-2-1.12][65-4-4-2.66][66-4-0-0.73][67-4-2-0.07][68-4-3-0.47]
[69-4-0-0.76][70-4-4-1.58][72-4-1-1.06][73-4-1-2.12][74-4-2-2.15][75-4-0-0.94][77-4-4-1.48][78-4-0-0.02][79-4-2-1.91][80-4-4-0.98]
[81-4-4-1.94][82-4-0-0.34][83-4-1-0.05][84-4-4-2.08][85-4-4-2.90][86-4-1-0.55][87-4-4-1.55][88-4-4-0.57][89-4-3-1.10][90-4-0-0.61]
[91-4-1-0.58][92-4-4--0.05][93-4-0-1.83][94-4-2-0.36][95-4-3-0.51][96-4-4-0.48][97-4-4-1.31][98-4-2-2.36][99-4-4-0.55][100-4-2-2.56]
[101-4-4-2.92][102-4-4-0.61][103-4-0--0.07][104-4-4-0.55][105-4-4-1.39][106-4-1-1.86][107-4-0-0.70][108-4-4-0.84][109-4-3-0.48][110-4-4-0.88]
[111-4-0-4.33][112-4-0-0.70][113-4-3--0.02][114-4-3-0.94][115-4-0-0.15][116-4-0-0.48][117-4-4-1.28][119-4-4-1.30][121-4-2-1.20][122-4-4-1.35]
[124-4-1-0.11][125-4-4-1.79][126-4-4-2.16][127-4-2-1.36][128-4-2--0.42][129-4-3-0.28][130-4-2-1.17][131-4-2-0.40][132-4-2-0.23][133-4-4-2.89]
[135-4-2-1.43][136-4-1--0.25][137-4-4-0.34][138-4-4-0.48][139-4-4-0.78][140-4-1-0.67][141-4-4-0.58][142-4-4-1.24][143-4-2-3.28][144-4-4-0.90]
[145-4-2-0.97][148-4-0-3.30][149-4-4-1.51][150-4-4-2.79][151-4-4-0.63][152-4-4-1.71][153-4-4-1.60][154-4-4-2.14][155-4-4-0.81][156-4-0-0.53]
[157-4-0-2.52][158-4-4-1.03][160-4-1-1.93][161-4-2-3.63][162-4-4--0.31][164-4-4-1.60][165-4-2-0.60][167-4-0-1.94][168-4-0-0.94][170-4-4-0.81]
[171-4-4-1.90][172-4-4-2.43][173-4-4-2.09][174-4-0-1.95][175-4-4-1.31][177-4-0-0.11][178-4-4-1.15][179-4-0-0.78][180-4-4-1.24][181-4-4-1.24]
[182-4-3-3.23][183-4-4-0.32][184-4-2-1.74][186-4-0-0.14][187-4-2-0.72][188-4-2-2.58][189-4-1-0.96][190-4-3-0.35][191-4-4-2.43][192-4-1-0.01]
[193-4-1-3.59][194-4-2-2.68][195-4-0-0.93][196-4-2-3.97][197-4-1-1.19][198-4-4-4.37][199-4-2-2.42]
---------------------------
I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.265 | Acc: 84.500% | Wgt Acc: 93.760%
	I - Batch: 100 | Loss: 0.296 | Acc: 82.500% | Wgt Acc: 92.287%
	I - Batch: 150 | Loss: 0.297 | Acc: 82.292% | Wgt Acc: 92.326%
	I - Batch: 200 | Loss: 0.293 | Acc: 82.250% | Wgt Acc: 92.373%
	I - Batch: 250 | Loss: 0.291 | Acc: 82.800% | Wgt Acc: 92.519%
I - num batch: 273
I - Train -- Loss: 0.290 | Acc: 82.760% | Wgt Acc: 92.439% | LR: 1.250000e-04 | Dur: 163.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 748.    5.    2.   24.  157.]
 [   6.  731.    5.    6.   77.]
 [  10.   14. 1015.   10.  250.]
 [  19.    5.    2.  728.  128.]
 [  14.    5.    8.    5.  388.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.177 | Acc: 58.383% | Wgt Acc: 62.057% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  1.  2.  8.  9.]
 [ 1. 37.  3.  2. 11.]
 [ 7. 31. 57.  9. 60.]
 [19.  5.  6. 60. 12.]
 [ 7.  4.  7.  7. 88.]]

I - Local maximum validation set accuracy:  58.38

I - Validation set results: 
[14-1-2-1.50][50-3-1-1.64][124-2-2-1.34][127-0-0-4.37][443-2-2-4.03][567-0-0-1.61][573-1-1-0.38][615-0-3-1.83][695-1-2-1.79][722-3-3-3.61]
[826-0-0-3.54][878-0-0-5.92][1103-0-0-2.39][1212-3-2-1.28][1368-0-0-4.57][2181-2-3-1.45][2476-2-2-2.62][2721-2-2-3.04][2818-1-3-1.00][2886-2-2-2.07]
[3231-2-2-4.79][3333-2-2-2.92][3482-2-2-3.29][3536-3-2-0.55][3625-1-1-4.14][3909-0-0-1.90][4035-0-0-2.84][4140-0-0-1.21][4214-1-2-0.03][4346-1-3-1.00]
[4581-2-2-2.29][4708-3-3-1.34][4838-3-3-0.97][4845-1-2-0.97][4868-0-0-3.30][4939-0-2-0.20][4984-2-2-2.02][5078-1-2-1.38][5396-0-0-4.95][5479-1-1-2.84]
[5717-0-0-2.73][5843-1-2-0.87][5949-3-3-1.43][5987-2-2-2.82][6014-3-3-2.61][6033-3-3-0.61][6313-0-0-2.36][6421-3-3-1.76][6500-1-2-1.76][6583-3-2-2.23]
[6683-3-3-2.78][6825-2-1-1.56][6998-3-3-1.29][7049-3-3-1.38][7517-1-2-3.55][7521-1-1-2.96][7528-1-3-1.69][7949-1-2-4.35][8135-1-2-0.35][8185-3-0-3.23]
[8269-3-2-1.77][8273-3-3-2.65][8543-3-0-4.84][8666-1-1-3.37][8672-0-0-5.24][8903-1-2-1.62][9001-2-1-0.80][9036-2-2-4.04][9281-3-3-1.41][9300-2-2-5.46]
[9571-0-3-0.59][9617-1-4-0.53][9644-2-2-5.01][9705-2-1-0.34][9801-0-3-1.23][9803-3-3-1.62][9865-3-3-4.20][9896-2-2-2.37][10314-1-2-2.20][10337-3-3-4.13]
[10403-0-4-0.89][10653-2-2-0.95][10704-2-2-3.02][10719-1-2-1.97][10727-1-2-1.02][10836-0-0-7.33][10969-2-3-1.60][11042-0-0-0.60][11088-1-1-4.79][11322-0-0-3.48]
[11398-2-2-2.07][11499-0-3-1.23][11502-3-3-1.07][11512-3-3-1.23][11608-1-2-3.68][11610-0-0-1.98][11692-0-3-2.05][11905-0-0-4.11][11993-1-1-4.12][12002-2-0-1.51]
[12052-0-0-2.90][12201-0-3-4.00][12235-2-4-1.43][12320-1-4-2.58][12377-2-4-2.39][12398-2-2-0.82][12503-1-2-0.19][12617-0-2-0.13][12685-3-4-0.76][12738-2-2-0.45]
[12742-2-2-7.53][12823-0-0-4.48][13110-1-2-3.88][13240-3-3-2.56][13253-1-1-3.88][13273-0-0-5.77][13634-1-1-2.05][13763-2-2-2.37][13905-3-3-1.25][14060-2-4-1.48]
[14065-3-0-0.50][14147-3-3-3.25][14595-2-2-2.80][14687-2-2-5.96][14788-2-2-4.15][14869-1-1-2.83][14872-3-4-0.16][14877-1-1-2.04][14927-0-3-1.22][15066-0-0-5.67]
[15175-1-1-1.86][15178-2-2-2.42][15375-3-3-0.08][15389-3-3-2.00][15568-2-4-0.69][15675-3-3-2.52][15869-1-2-0.06][16207-3-0-1.30][16236-0-2-1.67][16302-3-3-1.29]
[16331-2-2-6.28][16381-0-3-0.97][16488-1-1-6.10][16495-0-0-1.66][16650-0-0-2.73][16719-1-2-2.19][16801-0-0-5.43][16828-0-0-2.03][17137-3-3-0.22][17245-1-2-1.00]
[17278-3-4-0.54][17282-0-0-1.10][17311-2-2-3.58][17336-2-2-2.05][17608-3-3-6.00][17627-0-0-0.83][17877-3-4-1.97][17924-1-2-1.61][17984-3-0-3.55][18211-0-3-1.22]
[18276-3-3-2.59][18287-1-1-1.27][18394-0-0-1.61][18428-0-0-5.15][18442-0-3-3.38][18478-3-3-1.65][18607-0-0-2.30][18616-0-4-1.46][18663-0-0-3.87][18718-0-0-2.70]
[18766-2-2-5.13][18824-2-4-2.21][18890-3-3-2.67][18930-3-4-0.99][18938-3-2-1.56][19817-1-2-2.78][19839-0-2-0.18][19930-3-3-3.00][19944-0-4-1.64][20036-2-2-5.50]
[20101-3-3-2.94][20474-1-1-2.63][20547-3-3-1.68][20929-2-2-5.95][21245-1-2-3.18][21257-3-4--0.02][21293-1-2-4.03][21316-1-1-7.07][21384-1-1-2.48][21448-1-1-2.07]
[21483-0-0-3.88][21487-2-2-3.37][21714-0-3-1.75][21943-3-3-1.54][21947-0-0-1.45][21948-0-0-5.69][21965-2-2-5.12][21998-1-1-4.81][22025-0-2-1.29][22228-3-3-5.86]
[22446-1-1-2.33][22494-3-3-2.09][22757-0-0-2.75][22811-3-3-5.05][22976-3-2-1.23][22985-3-3-4.32][23014-0-0-3.48][23112-1-1-2.02][23144-3-3-5.17][23168-2-3-1.51]
[23219-0-2-1.86][23363-3-3-4.41][23470-0-0-0.02][23486-2-2-2.95][23497-0-3-4.27][23516-0-0-3.79][23690-1-1-1.15][23921-2-2-3.57][23936-1-2-2.35][24040-3-3-0.47]
[24111-1-4-1.36][24182-0-0-5.12][24238-3-3-3.80][24290-2-0-3.31][24345-0-0-1.95][24364-1-2-0.44][24427-3-0-1.46][24477-2-2-4.25][24495-2-2-0.92][24893-2-2-3.54]
[25012-1-2-1.85][25121-2-2-3.37][25165-3-3-2.95][25183-0-0-3.18][25297-3-3-3.42][25398-0-0-1.88][25574-2-2-3.58][25644-1-4--0.00][25718-1-3-0.25][25774-2-2-2.08]
[26032-3-3-2.03][26051-3-3-4.35][26120-0-4-3.17][26321-1-1-2.39][26732-1-1-2.54][26784-3-3-6.13][26827-3-2-0.31][26833-0-3-4.04][26838-2-2-0.79][26860-1-1-0.63]
[26948-0-0-0.67][27049-3-0-2.26][27098-1-0-0.92][27526-0-3-2.85][27639-3-4-0.47][27698-3-3-2.80][27772-0-0-0.88][27890-1-1-2.11][28040-0-4-1.28][28503-2-2-5.38]
[28577-1-1-2.58][28959-0-0-3.92][29198-3-3-3.59][29777-0-0-5.38][29877-2-2-2.02][30035-1-1-4.34][30098-0-3-1.83][30326-1-1-6.71][30572-2-2-3.85][30716-0-4-1.37]
[30806-2-2-1.90][30906-1-1-2.17][31007-0-0-1.53][31181-3-3-1.93][31238-0-3-2.25][31347-0-0-2.21][31422-2-2-2.79][31429-3-3-2.72][31431-0-2-1.09][31432-1-1-3.36]
[31477-0-3-2.71][31524-1-2-2.31][31597-1-1-1.81][31619-1-2-0.66][31701-0-0-1.22][31755-0-0-1.82][31854-3-3-3.28][32074-1-1-0.75][32078-3-3-2.54][32111-1-1-4.49]
[32127-1-1-1.58][32140-3-3-3.96][32263-2-4--0.05][32365-0-0-2.69][32411-2-3-3.72][32429-3-3-2.70][32473-3-0-2.15][32574-3-3-2.17][32584-0-4-1.87][32622-0-1--0.07]
[32858-3-3-1.27][32969-3-3-4.93][33016-2-2-3.52][33031-1-3-4.05][33035-2-2-4.12][33133-2-2-3.10][33173-2-2-1.99][33175-3-1-0.74][33306-3-2-2.04][33309-2-3-0.52]
[33474-0-3-0.47][33478-2-3-1.01][33618-1-2-0.40][33712-0-3-1.75][33782-2-4-1.14][33914-3-3-0.98][34076-3-2-1.01][34112-2-2-2.44][34138-2-2-1.00][34239-1-1-0.99]
[34364-2-2-4.41][34617-1-2-3.05][34751-3-3-3.61][34783-2-2-2.26][35015-3-3-0.78][35018-1-1-2.00][35288-2-2-1.82][0-4-2-3.65][1-4-4-2.62][2-4-0-1.95]
[3-4-4-2.31][4-4-4-2.07][5-4-1-2.15][6-4-4-1.82][7-4-4-1.82][8-4-2-1.09][9-4-2-2.21][10-4-4-4.33][11-4-2-3.92][12-4-1-0.77]
[14-4-3-1.01][15-4-3-1.36][16-4-4-2.47][17-4-1-1.01][18-4-4-3.11][19-4-3-1.76][20-4-4-0.83][21-4-2-1.19][22-4-4-1.55][23-4-4--0.34]
[24-4-4-5.12][25-4-2-1.44][26-4-4-0.18][27-4-2-2.84][28-4-4-3.36][29-4-1-1.44][30-4-4-0.29][31-4-4-0.73][32-4-1-0.69][33-4-2-5.25]
[34-4-4-0.14][35-4-3-0.74][37-4-2-1.63][39-4-0-0.09][40-4-1-0.50][41-4-2-1.89][42-4-2-1.68][43-4-1-0.98][45-4-2-0.23][46-4-4-3.40]
[47-4-4-4.33][48-4-4-2.25][51-4-4-2.52][52-4-4-1.52][53-4-4-1.30][54-4-3-2.01][55-4-2-1.39][56-4-2-1.08][57-4-3-0.89][58-4-2-3.32]
[59-4-4-1.81][60-4-4-1.21][61-4-4-2.87][62-4-2-2.99][63-4-2-2.97][64-4-2-1.96][65-4-4-4.20][66-4-4-1.51][67-4-2-0.19][68-4-3-0.83]
[69-4-3-0.85][70-4-4-2.55][72-4-4-1.07][73-4-1-2.27][74-4-2-3.24][75-4-4--0.17][77-4-4-3.06][78-4-2-1.56][79-4-2-3.25][80-4-4-1.40]
[81-4-2-2.62][82-4-4-0.37][83-4-4-0.59][84-4-4-2.97][85-4-4-2.27][86-4-4-1.03][87-4-4-2.15][88-4-4-2.10][89-4-3-1.11][90-4-4-0.07]
[91-4-2-2.09][92-4-4-0.74][93-4-0-1.46][94-4-4-1.27][95-4-4-1.14][96-4-4-1.01][97-4-4-1.84][98-4-2-2.96][99-4-4-1.20][100-4-2-3.48]
[101-4-4-4.08][102-4-2-1.94][103-4-2-0.09][104-4-4-1.04][105-4-2-2.34][106-4-4-1.42][107-4-0-1.60][108-4-4-0.81][109-4-4-2.17][110-4-4-2.17]
[111-4-0-3.53][112-4-2-0.55][113-4-2-0.56][114-4-2-2.52][115-4-4-0.44][116-4-4-0.12][117-4-4-2.62][119-4-2-4.19][121-4-4-1.84][122-4-4-1.82]
[124-4-2-0.35][125-4-4-2.83][126-4-4-4.94][127-4-2-0.65][128-4-2-0.46][129-4-4-1.35][130-4-4-1.67][131-4-2-1.88][132-4-2-0.57][133-4-4-3.99]
[135-4-2-1.69][136-4-4--0.34][137-4-4-0.63][138-4-2-1.04][139-4-2-1.58][140-4-2-0.37][141-4-2-2.60][142-4-4-3.23][143-4-2-3.90][144-4-4-2.91]
[145-4-2-3.66][148-4-0-3.75][149-4-4-1.32][150-4-4-3.39][151-4-4-0.96][152-4-4-2.37][153-4-2-2.45][154-4-4-2.71][155-4-4-2.61][156-4-3-0.38]
[157-4-0-1.94][158-4-4-1.26][160-4-1-0.15][161-4-2-3.75][162-4-2-0.72][164-4-4-1.92][165-4-2-1.50][167-4-3-1.78][168-4-4-1.21][170-4-1-0.69]
[171-4-4-2.12][172-4-4-2.80][173-4-4-3.47][174-4-0-2.56][175-4-4-0.93][177-4-4-2.64][178-4-2-1.14][179-4-4-0.17][180-4-4-2.41][181-4-4-1.01]
[182-4-3-2.25][183-4-4-1.83][184-4-2-2.37][186-4-2-0.76][187-4-2-0.52][188-4-2-1.70][189-4-2-0.82][190-4-4-0.35][191-4-2-2.56][192-4-2-1.14]
[193-4-1-2.25][194-4-2-2.22][195-4-0-1.66][196-4-2-5.29][197-4-4-1.58][198-4-4-6.40][199-4-2-2.30]
---------------------------
I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.238 | Acc: 85.000% | Wgt Acc: 94.235%
	I - Batch: 100 | Loss: 0.266 | Acc: 84.188% | Wgt Acc: 93.239%
	I - Batch: 150 | Loss: 0.280 | Acc: 82.958% | Wgt Acc: 92.584%
	I - Batch: 200 | Loss: 0.278 | Acc: 83.094% | Wgt Acc: 92.805%
	I - Batch: 250 | Loss: 0.273 | Acc: 83.325% | Wgt Acc: 93.080%
I - num batch: 273
I - Train -- Loss: 0.272 | Acc: 83.242% | Wgt Acc: 93.105% | LR: 1.250000e-04 | Dur: 169.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 751.    6.    1.   19.  167.]
 [   7.  740.    3.    3.   96.]
 [  10.    9. 1018.    7.  237.]
 [  17.    3.    4.  737.  115.]
 [  12.    2.    6.    7.  385.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.229 | Acc: 56.016% | Wgt Acc: 61.723% | Dur: 14.76s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  7.  7. 21. 35.]
 [ 2. 40.  3.  1. 10.]
 [ 1. 20. 46.  5. 42.]
 [13.  7. 15. 56. 19.]
 [ 4.  4.  4.  3. 74.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.235 | Acc: 86.875% | Wgt Acc: 94.722%
	I - Batch: 100 | Loss: 0.245 | Acc: 85.250% | Wgt Acc: 94.001%
	I - Batch: 150 | Loss: 0.250 | Acc: 84.875% | Wgt Acc: 93.837%
	I - Batch: 200 | Loss: 0.253 | Acc: 84.969% | Wgt Acc: 93.871%
	I - Batch: 250 | Loss: 0.256 | Acc: 84.675% | Wgt Acc: 93.660%
I - num batch: 273
I - Train -- Loss: 0.258 | Acc: 84.663% | Wgt Acc: 93.670% | LR: 1.250000e-04 | Dur: 165.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 759.    3.    4.   19.  161.]
 [   5.  746.    4.    4.   81.]
 [   7.    8. 1013.    5.  211.]
 [  13.    1.    1.  737.  109.]
 [  13.    2.   10.    8.  438.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.374 | Acc: 54.635% | Wgt Acc: 62.519% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  5.  7. 17. 36.]
 [ 0. 36.  3.  3. 10.]
 [ 1. 24. 45.  1. 37.]
 [18. 10. 17. 65. 35.]
 [ 0.  3.  3.  0. 62.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.250 | Acc: 83.500% | Wgt Acc: 93.487%
	I - Batch: 100 | Loss: 0.258 | Acc: 84.125% | Wgt Acc: 93.046%
	I - Batch: 150 | Loss: 0.258 | Acc: 83.917% | Wgt Acc: 93.115%
	I - Batch: 200 | Loss: 0.261 | Acc: 83.781% | Wgt Acc: 93.076%
	I - Batch: 250 | Loss: 0.257 | Acc: 83.850% | Wgt Acc: 93.126%
I - num batch: 273
I - Train -- Loss: 0.258 | Acc: 83.746% | Wgt Acc: 93.043% | LR: 1.250000e-04 | Dur: 163.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 750.    7.    5.   14.  137.]
 [   3.  737.    2.   10.  103.]
 [   8.    8. 1012.    5.  223.]
 [  22.    2.    7.  738.  121.]
 [  14.    6.    6.    6.  416.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.198 | Acc: 57.791% | Wgt Acc: 62.450% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  3.  7. 17. 24.]
 [ 1. 36.  1.  1. 15.]
 [ 1. 24. 47.  6. 37.]
 [14.  9. 11. 58. 22.]
 [ 2.  6.  9.  4. 82.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.236 | Acc: 86.875% | Wgt Acc: 94.612%
	I - Batch: 100 | Loss: 0.242 | Acc: 85.750% | Wgt Acc: 94.238%
	I - Batch: 150 | Loss: 0.249 | Acc: 84.542% | Wgt Acc: 93.697%
	I - Batch: 200 | Loss: 0.242 | Acc: 85.250% | Wgt Acc: 94.055%
	I - Batch: 250 | Loss: 0.248 | Acc: 84.825% | Wgt Acc: 93.728%
I - num batch: 273
I - Train -- Loss: 0.249 | Acc: 84.663% | Wgt Acc: 93.627% | LR: 1.250000e-04 | Dur: 163.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 758.    4.    2.   20.  143.]
 [   5.  745.    3.    3.   77.]
 [   8.    3. 1013.    8.  220.]
 [  13.    2.    4.  737.  120.]
 [  13.    6.   10.    5.  440.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.243 | Acc: 54.635% | Wgt Acc: 61.435% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  1.  8. 21. 32.]
 [ 1. 46.  8.  1. 22.]
 [ 2. 17. 41.  3. 40.]
 [17. 10. 15. 58. 19.]
 [ 3.  4.  3.  3. 67.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.224 | Acc: 86.500% | Wgt Acc: 95.057%
	I - Batch: 100 | Loss: 0.226 | Acc: 85.812% | Wgt Acc: 94.364%
	I - Batch: 150 | Loss: 0.227 | Acc: 85.500% | Wgt Acc: 94.346%
	I - Batch: 200 | Loss: 0.233 | Acc: 85.281% | Wgt Acc: 94.182%
	I - Batch: 250 | Loss: 0.237 | Acc: 85.250% | Wgt Acc: 94.067%
I - num batch: 273
I - Train -- Loss: 0.238 | Acc: 85.053% | Wgt Acc: 94.003% | LR: 1.250000e-04 | Dur: 166.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 771.    1.    6.    8.  120.]
 [   2.  743.    5.    9.   93.]
 [   5.    9. 1004.    1.  217.]
 [  12.    5.    8.  747.  125.]
 [   7.    2.    9.    8.  445.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.176 | Acc: 58.185% | Wgt Acc: 63.614% | Dur: 14.63s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  2.  6. 18. 19.]
 [ 1. 39.  5.  2. 19.]
 [ 1. 24. 52.  5. 55.]
 [11.  8.  8. 57.  8.]
 [ 7.  5.  4.  4. 79.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.229 | Acc: 85.750% | Wgt Acc: 94.984%
	I - Batch: 100 | Loss: 0.225 | Acc: 85.250% | Wgt Acc: 94.654%
	I - Batch: 150 | Loss: 0.228 | Acc: 85.708% | Wgt Acc: 94.759%
	I - Batch: 200 | Loss: 0.232 | Acc: 85.344% | Wgt Acc: 94.535%
	I - Batch: 250 | Loss: 0.233 | Acc: 85.500% | Wgt Acc: 94.457%
I - num batch: 273
I - Train -- Loss: 0.231 | Acc: 85.511% | Wgt Acc: 94.467% | LR: 1.250000e-04 | Dur: 168.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 769.    5.    0.   12.  147.]
 [   1.  745.    2.    3.   75.]
 [  11.    6. 1019.    6.  213.]
 [   7.    2.    5.  748.  116.]
 [   9.    2.    6.    4.  449.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.128 | Acc: 60.355% | Wgt Acc: 62.715% | Dur: 14.81s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  3.  5. 11. 15.]
 [ 0. 36.  3.  2. 10.]
 [ 1. 29. 46.  4. 42.]
 [19.  6. 12. 64. 15.]
 [ 6.  4.  9.  5. 98.]]

I - Local maximum validation set accuracy:  60.36

I - Validation set results: 
[14-1-2-0.92][50-3-1-1.53][124-2-2-2.82][127-0-0-5.31][443-2-2-3.29][567-0-0-2.09][573-1-1-3.30][615-0-3-0.40][695-1-2-2.01][722-3-3-4.36]
[826-0-0-4.35][878-0-0-5.93][1103-0-0-3.43][1212-3-3-0.16][1368-0-0-5.33][2181-2-3-1.51][2476-2-2-1.95][2721-2-2-3.72][2818-1-1-1.39][2886-2-4-1.18]
[3231-2-2-5.82][3333-2-3-1.73][3482-2-2-2.98][3536-3-4-0.29][3625-1-1-4.90][3909-0-0-3.43][4035-0-0-3.55][4140-0-0-3.52][4214-1-3-2.24][4346-1-3-1.01]
[4581-2-2-2.20][4708-3-3-2.12][4838-3-3-1.54][4845-1-2-0.85][4868-0-0-4.70][4939-0-4-0.28][4984-2-3-1.91][5078-1-2-1.65][5396-0-0-5.52][5479-1-1-2.92]
[5717-0-0-2.62][5843-1-1-2.79][5949-3-3-1.36][5987-2-4-2.48][6014-3-3-3.37][6033-3-3-1.17][6313-0-0-2.76][6421-3-3-1.99][6500-1-2-1.39][6583-3-2-0.25]
[6683-3-3-2.77][6825-2-3-0.91][6998-3-3-1.59][7049-3-3-2.28][7517-1-2-2.26][7521-1-1-2.21][7528-1-3-1.96][7949-1-2-3.57][8135-1-0-1.31][8185-3-0-3.78]
[8269-3-1-3.79][8273-3-3-3.48][8543-3-0-3.82][8666-1-1-3.20][8672-0-0-6.15][8903-1-2-1.54][9001-2-1-1.23][9036-2-2-4.20][9281-3-3-2.63][9300-2-2-7.28]
[9571-0-3-0.89][9617-1-1-1.60][9644-2-2-3.94][9705-2-0-0.92][9801-0-0-2.18][9803-3-3-1.81][9865-3-3-4.44][9896-2-2-1.10][10314-1-2-1.58][10337-3-3-5.48]
[10403-0-4-1.11][10653-2-1-0.96][10704-2-2-3.19][10719-1-2-2.24][10727-1-4-0.42][10836-0-0-8.07][10969-2-3-2.40][11042-0-0-0.91][11088-1-1-3.38][11322-0-0-4.23]
[11398-2-2-2.62][11499-0-0-3.55][11502-3-3-1.14][11512-3-3-2.58][11608-1-2-4.39][11610-0-0-3.03][11692-0-3-2.26][11905-0-0-2.30][11993-1-1-4.10][12002-2-0-0.65]
[12052-0-0-2.75][12201-0-3-4.41][12235-2-2-3.66][12320-1-4-3.07][12377-2-4-2.91][12398-2-4-0.20][12503-1-2-1.00][12617-0-2--0.01][12685-3-3-0.86][12738-2-0-1.03]
[12742-2-2-7.09][12823-0-0-2.74][13110-1-2-3.32][13240-3-0-2.53][13253-1-1-3.59][13273-0-0-6.19][13634-1-1-1.87][13763-2-2-1.75][13905-3-3-1.30][14060-2-4-1.84]
[14065-3-3-1.72][14147-3-3-4.04][14595-2-1-2.08][14687-2-2-6.32][14788-2-2-4.05][14869-1-1-3.76][14872-3-0-1.05][14877-1-1-2.74][14927-0-3-2.03][15066-0-0-4.63]
[15175-1-4-2.34][15178-2-2-1.43][15375-3-3-0.80][15389-3-3-2.42][15568-2-4-0.58][15675-3-3-4.38][15869-1-0--0.06][16207-3-0-2.43][16236-0-0-0.47][16302-3-3-1.52]
[16331-2-2-5.89][16381-0-3-2.18][16488-1-1-4.08][16495-0-0-2.57][16650-0-0-3.67][16719-1-2-1.04][16801-0-0-4.54][16828-0-0-2.14][17137-3-3--0.27][17245-1-2-1.69]
[17278-3-0-0.84][17282-0-0-1.90][17311-2-2-1.94][17336-2-2-2.28][17608-3-3-4.63][17627-0-0-1.87][17877-3-3-0.90][17924-1-2-0.69][17984-3-3-2.28][18211-0-3-2.45]
[18276-3-3-3.07][18287-1-1-1.43][18394-0-0-2.90][18428-0-0-2.39][18442-0-3-4.24][18478-3-3-1.54][18607-0-0-2.94][18616-0-4-1.94][18663-0-0-3.50][18718-0-0-3.26]
[18766-2-2-3.65][18824-2-4-2.16][18890-3-3-3.00][18930-3-4-1.55][18938-3-3-1.56][19817-1-2-2.61][19839-0-0-0.81][19930-3-3-3.31][19944-0-3-0.54][20036-2-2-5.91]
[20101-3-3-3.36][20474-1-1-2.34][20547-3-3-1.47][20929-2-2-4.15][21245-1-2-3.40][21257-3-3-0.46][21293-1-2-2.70][21316-1-1-7.28][21384-1-1-3.69][21448-1-2-1.05]
[21483-0-0-5.47][21487-2-2-4.65][21714-0-3-1.36][21943-3-3-2.67][21947-0-0-2.53][21948-0-0-6.87][21965-2-2-2.49][21998-1-1-4.67][22025-0-4-1.47][22228-3-3-5.97]
[22446-1-1-1.80][22494-3-0-1.86][22757-0-0-3.10][22811-3-3-5.17][22976-3-2-1.18][22985-3-3-5.56][23014-0-3-3.30][23112-1-1-2.47][23144-3-3-4.17][23168-2-3-0.99]
[23219-0-0-1.21][23363-3-3-5.20][23470-0-0-2.80][23486-2-2-1.88][23497-0-3-3.85][23516-0-0-3.31][23690-1-2-1.38][23921-2-2-1.21][23936-1-2-2.67][24040-3-3-0.68]
[24111-1-4-1.02][24182-0-3-3.35][24238-3-3-4.05][24290-2-0-2.61][24345-0-0-0.86][24364-1-2-1.11][24427-3-0-2.19][24477-2-2-2.73][24495-2-4-2.18][24893-2-2-3.47]
[25012-1-2-2.00][25121-2-2-2.70][25165-3-3-4.13][25183-0-0-3.87][25297-3-3-4.20][25398-0-0-2.84][25574-2-2-3.43][25644-1-2-2.42][25718-1-3-0.28][25774-2-2-0.79]
[26032-3-3-2.14][26051-3-3-4.94][26120-0-0-0.48][26321-1-1-4.25][26732-1-1-2.51][26784-3-3-5.82][26827-3-2-0.55][26833-0-3-2.22][26838-2-3-1.63][26860-1-1-1.13]
[26948-0-0-1.31][27049-3-0-3.44][27098-1-0-1.58][27526-0-3-1.97][27639-3-4-0.51][27698-3-3-3.04][27772-0-0-1.68][27890-1-1-3.97][28040-0-0-1.26][28503-2-2-3.72]
[28577-1-2-2.27][28959-0-0-3.68][29198-3-3-4.99][29777-0-0-6.20][29877-2-2-1.38][30035-1-1-4.84][30098-0-3-3.02][30326-1-1-5.30][30572-2-2-4.48][30716-0-4-0.83]
[30806-2-2-1.67][30906-1-1-2.77][31007-0-0-2.10][31181-3-3-1.41][31238-0-0-2.27][31347-0-0-3.93][31422-2-2--0.22][31429-3-3-2.85][31431-0-0-1.24][31432-1-1-3.04]
[31477-0-3-2.95][31524-1-2-1.10][31597-1-1-2.41][31619-1-3-1.01][31701-0-0-4.92][31755-0-0-3.38][31854-3-3-3.62][32074-1-2-1.26][32078-3-3-4.29][32111-1-1-4.63]
[32127-1-1-1.15][32140-3-3-3.71][32263-2-0-1.05][32365-0-0-5.26][32411-2-3-3.11][32429-3-3-3.66][32473-3-0-1.63][32574-3-0-2.59][32584-0-4-3.12][32622-0-3-0.96]
[32858-3-3-2.73][32969-3-3-5.46][33016-2-2-6.31][33031-1-3-4.83][33035-2-2-2.93][33133-2-2-2.35][33173-2-3-1.01][33175-3-4-0.42][33306-3-2-0.92][33309-2-3-0.40]
[33474-0-3-2.74][33478-2-3-1.82][33618-1-1-1.53][33712-0-0-1.41][33782-2-4-1.34][33914-3-3-5.83][34076-3-4-1.68][34112-2-2-3.92][34138-2-3-0.90][34239-1-1-1.02]
[34364-2-2-3.83][34617-1-2-2.18][34751-3-3-4.38][34783-2-2-2.89][35015-3-3-1.79][35018-1-1-3.82][35288-2-2-0.38][0-4-2-2.23][1-4-4-1.75][2-4-0-1.76]
[3-4-4-1.72][4-4-1-0.78][5-4-1-2.27][6-4-4-4.27][7-4-4-0.91][8-4-2-0.99][9-4-2-1.90][10-4-4-3.87][11-4-2-2.76][12-4-4-0.12]
[14-4-4-0.63][15-4-3-2.04][16-4-4-1.35][17-4-4-0.12][18-4-4-4.84][19-4-3-2.39][20-4-0-1.17][21-4-2-0.35][22-4-4-1.25][23-4-0--0.16]
[24-4-4-6.18][25-4-3-1.38][26-4-3-0.35][27-4-2-1.75][28-4-4-2.32][29-4-1-1.84][30-4-3-0.10][31-4-4-0.63][32-4-1-0.74][33-4-2-3.47]
[34-4-4-1.45][35-4-4-1.14][37-4-4-1.98][39-4-0-2.51][40-4-0-0.88][41-4-4-1.93][42-4-2-1.45][43-4-4-0.27][45-4-2-0.93][46-4-4-4.54]
[47-4-4-4.64][48-4-4-2.01][51-4-4-2.94][52-4-4-2.24][53-4-4-0.67][54-4-3-1.58][55-4-4-1.93][56-4-4-0.35][57-4-4-0.55][58-4-2-3.29]
[59-4-0-2.30][60-4-4-2.48][61-4-4-3.32][62-4-4-1.66][63-4-2-2.12][64-4-2-2.69][65-4-4-4.91][66-4-4-2.80][67-4-0-0.42][68-4-4--0.09]
[69-4-3-0.52][70-4-4-2.44][72-4-4-1.70][73-4-1-0.64][74-4-2-2.52][75-4-0-0.47][77-4-4-3.35][78-4-2-1.01][79-4-2-2.75][80-4-4-2.99]
[81-4-1-3.86][82-4-4-0.31][83-4-4-1.23][84-4-4-2.42][85-4-4-3.42][86-4-4-0.78][87-4-1-2.76][88-4-2-1.67][89-4-3-1.05][90-4-4-0.58]
[91-4-2-0.51][92-4-4-0.61][93-4-0-2.91][94-4-4-1.27][95-4-4-1.18][96-4-4-1.07][97-4-4-3.29][98-4-2-3.00][99-4-4-0.28][100-4-2-3.37]
[101-4-4-5.22][102-4-2-0.47][103-4-2-0.31][104-4-2-0.90][105-4-2-1.82][106-4-1-2.74][107-4-0-1.29][108-4-4-1.85][109-4-4-2.56][110-4-4-1.53]
[111-4-0-3.36][112-4-4-1.04][113-4-4-0.93][114-4-2-1.93][115-4-4-0.12][116-4-4-0.08][117-4-4-2.08][119-4-2-2.07][121-4-4-2.22][122-4-4-2.95]
[124-4-3-1.06][125-4-4-2.57][126-4-4-2.95][127-4-2-1.85][128-4-0-0.91][129-4-4-0.98][130-4-4-0.45][131-4-3-0.43][132-4-4-0.50][133-4-4-3.83]
[135-4-4-1.14][136-4-4-0.35][137-4-4-0.18][138-4-4-0.30][139-4-4-1.90][140-4-3-0.62][141-4-2-2.77][142-4-4-3.26][143-4-4-2.94][144-4-1-1.63]
[145-4-2-3.14][148-4-0-3.18][149-4-4-1.97][150-4-2-3.72][151-4-4-1.09][152-4-4-2.69][153-4-2-4.13][154-4-4-7.20][155-4-4-2.88][156-4-4--0.05]
[157-4-2-2.18][158-4-4-1.44][160-4-4-0.38][161-4-2-3.99][162-4-4-0.22][164-4-4-2.79][165-4-2-0.80][167-4-0-2.11][168-4-4-1.33][170-4-4-1.87]
[171-4-4-2.08][172-4-4-2.83][173-4-4-3.89][174-4-3-2.36][175-4-2-1.20][177-4-4-2.79][178-4-4-2.31][179-4-4-0.83][180-4-4-2.02][181-4-3-1.95]
[182-4-2-1.21][183-4-4-2.03][184-4-2-1.53][186-4-2--0.14][187-4-2-0.19][188-4-4-1.83][189-4-2-1.17][190-4-3-0.40][191-4-4-2.17][192-4-4-0.39]
[193-4-2-1.16][194-4-3--0.22][195-4-0-2.07][196-4-2-2.26][197-4-1-1.93][198-4-4-8.37][199-4-2-1.69]
---------------------------
I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.221 | Acc: 84.750% | Wgt Acc: 93.997%
	I - Batch: 100 | Loss: 0.227 | Acc: 84.562% | Wgt Acc: 94.071%
	I - Batch: 150 | Loss: 0.231 | Acc: 85.042% | Wgt Acc: 94.036%
	I - Batch: 200 | Loss: 0.226 | Acc: 85.438% | Wgt Acc: 94.368%
	I - Batch: 250 | Loss: 0.227 | Acc: 85.500% | Wgt Acc: 94.406%
I - num batch: 273
I - Train -- Loss: 0.227 | Acc: 85.534% | Wgt Acc: 94.436% | LR: 1.250000e-04 | Dur: 163.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 764.    3.    3.   13.  161.]
 [   4.  752.    4.    3.   84.]
 [   5.    2. 1016.    3.  197.]
 [  12.    0.    1.  747.  106.]
 [  12.    3.    8.    7.  452.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.272 | Acc: 57.396% | Wgt Acc: 62.357% | Dur: 14.08s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  1.  6. 16. 26.]
 [ 0. 30.  3.  2.  8.]
 [ 1. 35. 52.  4. 47.]
 [16.  8.  9. 62. 19.]
 [ 4.  4.  5.  2. 80.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.208 | Acc: 87.125% | Wgt Acc: 94.757%
	I - Batch: 100 | Loss: 0.211 | Acc: 86.562% | Wgt Acc: 94.608%
	I - Batch: 150 | Loss: 0.211 | Acc: 86.458% | Wgt Acc: 94.722%
	I - Batch: 200 | Loss: 0.211 | Acc: 86.375% | Wgt Acc: 94.695%
	I - Batch: 250 | Loss: 0.211 | Acc: 86.300% | Wgt Acc: 94.676%
I - num batch: 273
I - Train -- Loss: 0.213 | Acc: 86.382% | Wgt Acc: 94.672% | LR: 1.250000e-04 | Dur: 162.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 765.    0.    0.   16.  134.]
 [   4.  747.    2.    3.   71.]
 [   4.    2. 1020.    3.  203.]
 [   9.    3.    4.  748.  104.]
 [  15.    8.    6.    3.  488.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.206 | Acc: 58.777% | Wgt Acc: 62.842% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  2.  4. 24. 32.]
 [ 0. 35.  3.  2. 11.]
 [ 0. 28. 54.  4. 38.]
 [14.  9.  9. 54. 13.]
 [ 5.  4.  5.  2. 86.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.220 | Acc: 84.500% | Wgt Acc: 94.117%
	I - Batch: 100 | Loss: 0.208 | Acc: 86.438% | Wgt Acc: 94.673%
	I - Batch: 150 | Loss: 0.211 | Acc: 86.042% | Wgt Acc: 94.883%
	I - Batch: 200 | Loss: 0.216 | Acc: 85.688% | Wgt Acc: 94.239%
	I - Batch: 250 | Loss: 0.217 | Acc: 85.800% | Wgt Acc: 94.285%
I - num batch: 273
I - Train -- Loss: 0.218 | Acc: 85.672% | Wgt Acc: 94.267% | LR: 1.250000e-04 | Dur: 163.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 767.    2.    1.   15.  138.]
 [   2.  745.    6.    2.   72.]
 [   6.    4. 1013.    6.  205.]
 [  11.    0.    3.  745.  118.]
 [  11.    9.    9.    5.  467.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.244 | Acc: 58.383% | Wgt Acc: 63.995% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  2.  4. 14. 21.]
 [ 2. 41.  1.  4. 20.]
 [ 0. 23. 53.  4. 42.]
 [17.  7. 12. 61. 18.]
 [ 7.  5.  5.  3. 79.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.185 | Acc: 87.250% | Wgt Acc: 95.780%
	I - Batch: 100 | Loss: 0.198 | Acc: 87.688% | Wgt Acc: 95.331%
	I - Batch: 150 | Loss: 0.205 | Acc: 86.875% | Wgt Acc: 94.869%
	I - Batch: 200 | Loss: 0.211 | Acc: 86.719% | Wgt Acc: 94.777%
	I - Batch: 250 | Loss: 0.207 | Acc: 87.025% | Wgt Acc: 94.981%
I - num batch: 273
I - Train -- Loss: 0.209 | Acc: 86.795% | Wgt Acc: 94.887% | LR: 1.250000e-04 | Dur: 166.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 767.    5.    3.    8.  124.]
 [   3.  743.    1.    6.   76.]
 [   7.    7. 1024.    2.  187.]
 [  11.    2.    1.  751.  112.]
 [   9.    3.    3.    6.  501.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.163 | Acc: 59.566% | Wgt Acc: 62.300% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  4.  5. 14. 27.]
 [ 3. 36.  4.  2.  8.]
 [ 1. 19. 41.  3. 30.]
 [15. 10. 13. 65. 20.]
 [ 4.  9. 12.  2. 95.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.212 | Acc: 85.500% | Wgt Acc: 94.205%
	I - Batch: 100 | Loss: 0.211 | Acc: 87.000% | Wgt Acc: 94.698%
	I - Batch: 150 | Loss: 0.203 | Acc: 86.708% | Wgt Acc: 94.785%
	I - Batch: 200 | Loss: 0.205 | Acc: 86.531% | Wgt Acc: 94.678%
	I - Batch: 250 | Loss: 0.203 | Acc: 86.800% | Wgt Acc: 94.841%
I - num batch: 273
I - Train -- Loss: 0.205 | Acc: 86.657% | Wgt Acc: 94.839% | LR: 1.250000e-04 | Dur: 169.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 766.    0.    4.    5.  125.]
 [   2.  752.    2.    5.   78.]
 [   2.    6. 1016.    4.  185.]
 [   9.    1.    5.  750.  116.]
 [  18.    1.    5.    9.  496.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.173 | Acc: 61.341% | Wgt Acc: 65.955% | Dur: 14.80s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  2.  6. 12. 29.]
 [ 0. 42.  4.  1.  5.]
 [ 1. 21. 51.  4. 42.]
 [17.  7. 10. 63. 15.]
 [ 4.  6.  4.  6. 89.]]

I - Local maximum validation set accuracy:  61.34

I - Validation set results: 
[14-1-1--0.04][50-3-1-2.39][124-2-2-3.90][127-0-0-6.25][443-2-2-2.98][567-0-0-3.25][573-1-1-3.95][615-0-0-2.88][695-1-2-0.42][722-3-3-3.37]
[826-0-0-3.84][878-0-0-7.88][1103-0-0-2.05][1212-3-2--0.22][1368-0-0-6.19][2181-2-3-2.58][2476-2-2-0.28][2721-2-2-3.78][2818-1-3-0.86][2886-2-2-0.70]
[3231-2-2-4.86][3333-2-3-1.46][3482-2-2-3.37][3536-3-4-0.12][3625-1-1-3.96][3909-0-0-3.88][4035-0-0-3.81][4140-0-0-2.79][4214-1-3-0.86][4346-1-3-0.76]
[4581-2-2-2.33][4708-3-3-3.52][4838-3-0-2.08][4845-1-1-1.44][4868-0-0-5.17][4939-0-0-0.77][4984-2-3-1.59][5078-1-2-1.50][5396-0-0-5.72][5479-1-1-3.67]
[5717-0-0-3.52][5843-1-2-0.70][5949-3-3-2.12][5987-2-2-2.00][6014-3-3-3.79][6033-3-3-2.15][6313-0-0-3.79][6421-3-3-2.91][6500-1-2-1.07][6583-3-3-0.98]
[6683-3-3-3.43][6825-2-1-4.51][6998-3-3-1.71][7049-3-3-3.40][7517-1-1-4.11][7521-1-1-0.94][7528-1-3-1.93][7949-1-2-6.59][8135-1-2-1.19][8185-3-3-3.46]
[8269-3-4-0.20][8273-3-3-4.07][8543-3-0-5.12][8666-1-1-3.17][8672-0-0-7.01][8903-1-2-1.44][9001-2-2-1.44][9036-2-2-4.60][9281-3-3-2.10][9300-2-2-8.80]
[9571-0-3-0.90][9617-1-1-1.82][9644-2-2-4.51][9705-2-0-1.62][9801-0-3-2.82][9803-3-0-2.26][9865-3-3-5.65][9896-2-2-1.31][10314-1-1-0.39][10337-3-3-4.48]
[10403-0-0-1.94][10653-2-2-0.93][10704-2-2-2.91][10719-1-2-1.72][10727-1-0-1.01][10836-0-0-8.66][10969-2-3-1.74][11042-0-0-1.07][11088-1-1-4.19][11322-0-0-3.46]
[11398-2-2-0.57][11499-0-0-4.07][11502-3-3-1.55][11512-3-3-2.54][11608-1-2-1.81][11610-0-0-1.92][11692-0-3-2.53][11905-0-0-4.50][11993-1-1-3.92][12002-2-0-1.98]
[12052-0-0-4.15][12201-0-3-3.38][12235-2-2-1.32][12320-1-4-2.11][12377-2-4-1.40][12398-2-3-0.51][12503-1-1--0.27][12617-0-3-0.13][12685-3-3-1.60][12738-2-2-1.49]
[12742-2-2-5.74][12823-0-3-3.19][13110-1-2-0.76][13240-3-3-2.47][13253-1-1-2.14][13273-0-0-7.38][13634-1-4-1.24][13763-2-3-0.04][13905-3-3-2.24][14060-2-2-2.44]
[14065-3-0-1.40][14147-3-3-3.38][14595-2-1-1.21][14687-2-2-3.95][14788-2-2-2.90][14869-1-1-3.58][14872-3-0-0.69][14877-1-1-2.68][14927-0-3-3.11][15066-0-0-6.61]
[15175-1-4-1.91][15178-2-2-1.64][15375-3-0-0.00][15389-3-3-4.51][15568-2-2-0.41][15675-3-3-3.39][15869-1-3-0.67][16207-3-0-1.70][16236-0-0-1.32][16302-3-3-1.93]
[16331-2-2-3.45][16381-0-3-2.22][16488-1-1-3.41][16495-0-0-3.32][16650-0-0-4.25][16719-1-2-3.66][16801-0-0-5.61][16828-0-0-3.65][17137-3-3-0.54][17245-1-3-0.61]
[17278-3-0-1.16][17282-0-0-2.44][17311-2-2-1.39][17336-2-1-1.41][17608-3-3-5.76][17627-0-0-2.04][17877-3-4-1.99][17924-1-2-0.07][17984-3-3-1.54][18211-0-0-0.66]
[18276-3-3-3.70][18287-1-1-2.95][18394-0-0-3.36][18428-0-0-2.82][18442-0-3-3.53][18478-3-3-1.75][18607-0-0-3.23][18616-0-4-1.45][18663-0-0-4.97][18718-0-0-4.59]
[18766-2-2-2.57][18824-2-4-1.35][18890-3-3-4.56][18930-3-4-0.89][18938-3-3-1.53][19817-1-2-2.08][19839-0-0-1.06][19930-3-3-2.78][19944-0-0-2.33][20036-2-2-6.88]
[20101-3-3-2.73][20474-1-1-2.57][20547-3-3-1.25][20929-2-2-3.37][21245-1-2-3.77][21257-3-3-1.61][21293-1-2-3.23][21316-1-1-7.31][21384-1-4-3.04][21448-1-1-1.29]
[21483-0-0-5.16][21487-2-2-4.73][21714-0-3-1.20][21943-3-3-3.00][21947-0-0-3.13][21948-0-0-7.21][21965-2-2-3.72][21998-1-1-3.63][22025-0-4-1.55][22228-3-3-5.85]
[22446-1-1-1.92][22494-3-3-1.58][22757-0-0-2.83][22811-3-3-7.19][22976-3-4-1.09][22985-3-3-4.29][23014-0-3-3.70][23112-1-1-2.63][23144-3-3-5.38][23168-2-3-2.31]
[23219-0-0-2.48][23363-3-3-5.02][23470-0-0-1.41][23486-2-2-3.57][23497-0-3-4.38][23516-0-0-4.61][23690-1-1-0.81][23921-2-2-2.69][23936-1-2-1.37][24040-3-2-0.52]
[24111-1-4-0.60][24182-0-0-7.22][24238-3-3-5.19][24290-2-0-3.56][24345-0-0-3.98][24364-1-2-1.51][24427-3-0-5.11][24477-2-2-3.70][24495-2-2-0.28][24893-2-2-3.53]
[25012-1-2-0.25][25121-2-2-4.98][25165-3-3-3.70][25183-0-0-2.33][25297-3-3-4.28][25398-0-0-3.82][25574-2-2-3.31][25644-1-2-2.22][25718-1-4--0.12][25774-2-4-0.41]
[26032-3-3-3.04][26051-3-3-5.79][26120-0-0-0.57][26321-1-1-5.08][26732-1-1-2.45][26784-3-3-6.96][26827-3-3-1.80][26833-0-3-3.13][26838-2-3-0.87][26860-1-1-1.78]
[26948-0-0-1.34][27049-3-0-4.07][27098-1-1-1.45][27526-0-0-2.09][27639-3-3--0.26][27698-3-3-3.83][27772-0-0-3.68][27890-1-1-2.81][28040-0-4-1.65][28503-2-2-3.17]
[28577-1-1-1.22][28959-0-0-5.56][29198-3-3-5.63][29777-0-0-7.29][29877-2-2-0.28][30035-1-1-4.97][30098-0-3-2.60][30326-1-1-5.67][30572-2-2-3.53][30716-0-4-0.89]
[30806-2-3-1.53][30906-1-1-3.83][31007-0-0-3.64][31181-3-3-1.51][31238-0-3-2.35][31347-0-0-3.32][31422-2-2-1.65][31429-3-3-2.01][31431-0-0-1.22][31432-1-1-3.82]
[31477-0-3-3.48][31524-1-0-0.31][31597-1-1-2.40][31619-1-2-0.83][31701-0-0-3.11][31755-0-0-6.12][31854-3-3-2.16][32074-1-1-1.77][32078-3-3-3.36][32111-1-1-1.41]
[32127-1-1-2.13][32140-3-3-4.71][32263-2-0-1.46][32365-0-0-3.82][32411-2-0-4.15][32429-3-3-3.37][32473-3-0-2.35][32574-3-0-3.00][32584-0-0-2.00][32622-0-2-0.10]
[32858-3-3-3.04][32969-3-3-5.21][33016-2-2-3.70][33031-1-3-4.56][33035-2-2-3.60][33133-2-2-2.70][33173-2-2-0.91][33175-3-4-0.62][33306-3-2-2.17][33309-2-0-0.48]
[33474-0-3-0.64][33478-2-3-1.80][33618-1-1-1.04][33712-0-0-1.32][33782-2-4-0.89][33914-3-3-2.87][34076-3-2-2.26][34112-2-2-4.82][34138-2-1-0.54][34239-1-1-1.23]
[34364-2-2-3.64][34617-1-2-2.03][34751-3-3-3.78][34783-2-2-2.35][35015-3-3-1.72][35018-1-1-1.65][35288-2-2-0.57][0-4-2-1.97][1-4-4-2.42][2-4-4-0.70]
[3-4-4-1.10][4-4-1-0.43][5-4-3-1.03][6-4-4-5.40][7-4-0-1.84][8-4-2-1.69][9-4-4-1.94][10-4-4-3.18][11-4-2-3.49][12-4-2-0.63]
[14-4-4-0.43][15-4-3-4.54][16-4-4-1.78][17-4-4-0.40][18-4-4-4.22][19-4-3-2.11][20-4-0-0.63][21-4-2-1.93][22-4-0-0.98][23-4-0-0.29]
[24-4-4-7.34][25-4-3-2.14][26-4-4-0.70][27-4-2-1.78][28-4-4-2.91][29-4-1-1.31][30-4-0--0.08][31-4-4-0.78][32-4-4-0.77][33-4-2-3.36]
[34-4-4-0.30][35-4-4-0.92][37-4-4-1.15][39-4-0-1.21][40-4-0-0.20][41-4-3-0.19][42-4-2-0.89][43-4-2-0.27][45-4-4-0.38][46-4-4-3.20]
[47-4-4-4.30][48-4-4-1.44][51-4-4-1.36][52-4-0-2.17][53-4-4-1.02][54-4-4-1.72][55-4-2-2.74][56-4-2-0.33][57-4-0-0.97][58-4-2-2.46]
[59-4-0-3.24][60-4-4-0.88][61-4-4-3.50][62-4-2-2.12][63-4-2-2.33][64-4-2-1.03][65-4-4-4.55][66-4-2-1.41][67-4-3-1.88][68-4-3-1.44]
[69-4-4--0.30][70-4-4-2.12][72-4-4-0.98][73-4-1-4.10][74-4-2-3.76][75-4-0-1.45][77-4-4-4.94][78-4-2--0.08][79-4-2-4.29][80-4-4-4.54]
[81-4-2-1.83][82-4-0-0.78][83-4-4-1.51][84-4-4-2.15][85-4-4-3.55][86-4-4-0.79][87-4-4-2.17][88-4-0-0.19][89-4-3-1.22][90-4-3--0.14]
[91-4-4-0.19][92-4-4-0.03][93-4-0-2.50][94-4-4-0.55][95-4-4-0.35][96-4-4-0.86][97-4-4-1.22][98-4-2-2.20][99-4-0-1.06][100-4-2-4.35]
[101-4-4-3.72][102-4-4-0.73][103-4-2-1.67][104-4-4-1.11][105-4-2-1.56][106-4-4-2.46][107-4-0-1.70][108-4-4-0.44][109-4-4-1.41][110-4-4-1.51]
[111-4-0-3.11][112-4-0-0.57][113-4-4-0.67][114-4-3-1.57][115-4-4-0.49][116-4-4-0.40][117-4-2-3.07][119-4-2-4.39][121-4-4-1.77][122-4-4-2.86]
[124-4-4-0.63][125-4-4-2.47][126-4-4-5.96][127-4-4--0.27][128-4-3-0.02][129-4-2-0.77][130-4-4-1.31][131-4-2-0.69][132-4-4-0.28][133-4-4-3.83]
[135-4-2-1.16][136-4-2-0.73][137-4-4-0.36][138-4-2-2.85][139-4-4-1.68][140-4-3-1.02][141-4-4-1.04][142-4-4-1.86][143-4-4-3.61][144-4-4-1.19]
[145-4-2-4.76][148-4-0-3.89][149-4-3-1.56][150-4-4-4.22][151-4-4-1.79][152-4-4-2.77][153-4-2-2.54][154-4-4-7.10][155-4-4-3.47][156-4-0-0.53]
[157-4-0-2.27][158-4-4-0.82][160-4-1-1.90][161-4-2-3.13][162-4-4-0.00][164-4-2-1.58][165-4-2-1.11][167-4-0-2.02][168-4-0-0.79][170-4-4-0.57]
[171-4-4-2.53][172-4-4-3.22][173-4-4-3.07][174-4-0-1.99][175-4-4-1.41][177-4-0-2.65][178-4-4-2.30][179-4-0-0.15][180-4-4-2.91][181-4-4-2.00]
[182-4-3-4.03][183-4-4-2.88][184-4-2-1.50][186-4-0-0.07][187-4-2-1.94][188-4-2-1.04][189-4-2-0.29][190-4-3-0.30][191-4-4-1.69][192-4-2--0.15]
[193-4-1-3.09][194-4-0-0.48][195-4-0-0.84][196-4-2-3.39][197-4-4-1.94][198-4-4-6.30][199-4-4-0.89]
---------------------------
I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.191 | Acc: 87.125% | Wgt Acc: 95.165%
	I - Batch: 100 | Loss: 0.183 | Acc: 87.625% | Wgt Acc: 95.582%
	I - Batch: 150 | Loss: 0.191 | Acc: 87.125% | Wgt Acc: 95.241%
	I - Batch: 200 | Loss: 0.192 | Acc: 87.469% | Wgt Acc: 95.296%
	I - Batch: 250 | Loss: 0.200 | Acc: 86.800% | Wgt Acc: 94.917%
I - num batch: 273
I - Train -- Loss: 0.201 | Acc: 86.841% | Wgt Acc: 94.916% | LR: 1.250000e-04 | Dur: 164.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 764.    3.    4.    9.  151.]
 [   2.  749.    1.    2.   60.]
 [   7.    3. 1017.    1.  186.]
 [  10.    3.    1.  755.  100.]
 [  14.    2.    9.    6.  503.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.162 | Acc: 60.355% | Wgt Acc: 63.407% | Dur: 14.03s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  1.  2. 13. 17.]
 [ 1. 40.  6.  1. 10.]
 [ 3. 26. 56. 10. 50.]
 [17.  4.  3. 56.  9.]
 [ 7.  7.  8.  6. 94.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.198 | Acc: 86.250% | Wgt Acc: 95.402%
	I - Batch: 100 | Loss: 0.196 | Acc: 86.812% | Wgt Acc: 95.453%
	I - Batch: 150 | Loss: 0.191 | Acc: 86.958% | Wgt Acc: 95.560%
	I - Batch: 200 | Loss: 0.190 | Acc: 87.250% | Wgt Acc: 95.637%
	I - Batch: 250 | Loss: 0.193 | Acc: 87.050% | Wgt Acc: 95.452%
I - num batch: 273
I - Train -- Loss: 0.195 | Acc: 86.887% | Wgt Acc: 95.404% | LR: 1.250000e-04 | Dur: 164.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 778.    2.    1.    4.  141.]
 [   3.  746.    1.    2.   78.]
 [   3.    5. 1023.    2.  189.]
 [   6.    3.    3.  760.  109.]
 [   7.    4.    4.    5.  483.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.141 | Acc: 60.552% | Wgt Acc: 62.311% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   1.   4.  15.   7.]
 [  1.  42.   7.   3.  16.]
 [  3.  20.  48.   4.  42.]
 [ 19.   9.   8.  59.  14.]
 [  8.   6.   8.   5. 101.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.180 | Acc: 87.375% | Wgt Acc: 94.309%
	I - Batch: 100 | Loss: 0.176 | Acc: 87.938% | Wgt Acc: 95.140%
	I - Batch: 150 | Loss: 0.179 | Acc: 87.542% | Wgt Acc: 95.179%
	I - Batch: 200 | Loss: 0.179 | Acc: 87.875% | Wgt Acc: 95.377%
	I - Batch: 250 | Loss: 0.183 | Acc: 87.325% | Wgt Acc: 95.253%
I - num batch: 273
I - Train -- Loss: 0.185 | Acc: 87.322% | Wgt Acc: 95.249% | LR: 1.250000e-04 | Dur: 166.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 768.    0.    0.   11.  118.]
 [   2.  756.    1.    4.   71.]
 [   7.    3. 1021.    2.  190.]
 [   9.    0.    5.  750.  107.]
 [  11.    1.    5.    6.  514.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.185 | Acc: 61.736% | Wgt Acc: 64.514% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  5.  7. 21. 34.]
 [ 2. 43.  3.  2.  5.]
 [ 1. 13. 43.  4. 25.]
 [12.  8. 12. 58. 18.]
 [ 2.  9. 10.  1. 98.]]

I - Local maximum validation set accuracy:  61.74

I - Validation set results: 
[14-1-1-1.43][50-3-1-1.09][124-2-2-3.54][127-0-0-6.33][443-2-2-2.26][567-0-0-3.53][573-1-1-3.52][615-0-0-2.52][695-1-2-1.82][722-3-0-3.48]
[826-0-0-5.77][878-0-0-4.01][1103-0-0-1.52][1212-3-3-0.37][1368-0-0-6.13][2181-2-3-2.61][2476-2-2-0.32][2721-2-2-3.81][2818-1-1-1.12][2886-2-1-0.93]
[3231-2-2-3.43][3333-2-3-1.95][3482-2-2-2.84][3536-3-0-0.84][3625-1-1-4.66][3909-0-0-5.63][4035-0-0-4.97][4140-0-0-2.91][4214-1-3-1.90][4346-1-3-0.65]
[4581-2-4-1.28][4708-3-3-1.88][4838-3-0-2.02][4845-1-1-0.45][4868-0-0-3.72][4939-0-0-2.10][4984-2-3-2.34][5078-1-3-0.05][5396-0-0-7.41][5479-1-1-2.86]
[5717-0-0-5.32][5843-1-4--0.52][5949-3-0-1.66][5987-2-4-2.07][6014-3-3-3.87][6033-3-3-3.11][6313-0-0-4.25][6421-3-3-1.70][6500-1-2-0.07][6583-3-3-1.34]
[6683-3-3-2.93][6825-2-3-0.50][6998-3-3-1.32][7049-3-3-2.32][7517-1-1-2.36][7521-1-1-2.54][7528-1-3-3.30][7949-1-2-5.47][8135-1-0-1.26][8185-3-0-5.30]
[8269-3-1-6.37][8273-3-3-3.28][8543-3-0-5.64][8666-1-1-2.02][8672-0-0-6.62][8903-1-3-1.23][9001-2-1-1.50][9036-2-2-3.25][9281-3-3-1.91][9300-2-2-7.30]
[9571-0-0-0.22][9617-1-4-1.71][9644-2-2-4.50][9705-2-0-2.76][9801-0-0-1.66][9803-3-0-2.19][9865-3-3-4.59][9896-2-4-0.34][10314-1-4-0.53][10337-3-3-4.72]
[10403-0-0-1.03][10653-2-2-0.19][10704-2-2-2.60][10719-1-1-0.82][10727-1-0-0.81][10836-0-0-10.49][10969-2-3-2.57][11042-0-0-1.07][11088-1-1-2.94][11322-0-0-4.38]
[11398-2-2-1.56][11499-0-0-4.73][11502-3-3-1.63][11512-3-3-2.99][11608-1-2-1.89][11610-0-0-3.78][11692-0-3-3.12][11905-0-0-3.62][11993-1-1-2.81][12002-2-0-5.12]
[12052-0-0-5.38][12201-0-3-4.93][12235-2-2-2.76][12320-1-0-1.88][12377-2-4-1.76][12398-2-3-1.00][12503-1-2-1.77][12617-0-3-0.66][12685-3-3-1.15][12738-2-3-0.95]
[12742-2-2-6.08][12823-0-0-5.96][13110-1-2-1.27][13240-3-0-3.71][13253-1-1-2.67][13273-0-0-7.39][13634-1-1-2.56][13763-2-2-1.12][13905-3-3-3.55][14060-2-4-1.91]
[14065-3-0-1.78][14147-3-3-3.81][14595-2-2-0.61][14687-2-2-5.25][14788-2-2-4.17][14869-1-1-3.27][14872-3-0-1.25][14877-1-1-2.13][14927-0-3-2.62][15066-0-0-6.71]
[15175-1-4-1.95][15178-2-2-1.54][15375-3-0-2.77][15389-3-3-2.40][15568-2-4-0.89][15675-3-3-2.73][15869-1-0-1.30][16207-3-0-3.38][16236-0-0-2.92][16302-3-3-1.91]
[16331-2-2-4.05][16381-0-0-1.76][16488-1-1-5.61][16495-0-0-5.01][16650-0-0-4.83][16719-1-4-1.85][16801-0-0-6.32][16828-0-0-4.37][17137-3-3-0.52][17245-1-4-1.50]
[17278-3-0-1.71][17282-0-0-2.94][17311-2-2-1.23][17336-2-1-1.54][17608-3-3-6.04][17627-0-0-2.08][17877-3-3-2.56][17924-1-3-0.43][17984-3-0-4.03][18211-0-3-1.24]
[18276-3-3-3.31][18287-1-1-2.02][18394-0-0-3.82][18428-0-0-3.21][18442-0-3-3.94][18478-3-3-2.43][18607-0-0-4.12][18616-0-0-2.83][18663-0-0-5.77][18718-0-0-4.16]
[18766-2-2-1.30][18824-2-4-2.20][18890-3-3-3.17][18930-3-4-1.06][18938-3-3-2.44][19817-1-2-1.64][19839-0-0-1.31][19930-3-3-3.16][19944-0-1-1.83][20036-2-2-4.65]
[20101-3-3-3.08][20474-1-1-2.45][20547-3-3-1.86][20929-2-2-4.49][21245-1-1-0.48][21257-3-3-0.64][21293-1-2-1.66][21316-1-1-6.07][21384-1-1-5.09][21448-1-1-0.84]
[21483-0-0-6.04][21487-2-2-2.39][21714-0-3-1.68][21943-3-3-1.26][21947-0-0-3.28][21948-0-0-6.86][21965-2-2-4.46][21998-1-1-4.18][22025-0-2-1.63][22228-3-3-6.56]
[22446-1-1-1.67][22494-3-0-2.93][22757-0-0-4.01][22811-3-3-5.11][22976-3-2-2.22][22985-3-3-4.45][23014-0-0-3.75][23112-1-1-1.69][23144-3-3-5.01][23168-2-0-1.01]
[23219-0-0-2.77][23363-3-3-3.55][23470-0-0-2.07][23486-2-2-1.63][23497-0-3-4.75][23516-0-0-3.61][23690-1-4-0.91][23921-2-2-1.81][23936-1-2-1.28][24040-3-0-1.03]
[24111-1-4-1.00][24182-0-0-4.82][24238-3-3-4.09][24290-2-0-6.26][24345-0-0-4.18][24364-1-2-1.04][24427-3-0-5.02][24477-2-2-0.89][24495-2-4-0.69][24893-2-2-2.13]
[25012-1-2-0.44][25121-2-2-3.65][25165-3-3-4.06][25183-0-0-4.54][25297-3-3-3.85][25398-0-0-3.75][25574-2-2-1.46][25644-1-1-0.30][25718-1-3-1.76][25774-2-2-1.08]
[26032-3-3-2.90][26051-3-3-5.18][26120-0-4-3.22][26321-1-1-3.24][26732-1-1-1.32][26784-3-3-6.67][26827-3-2-0.28][26833-0-3-2.71][26838-2-2-0.09][26860-1-1-2.23]
[26948-0-0-2.84][27049-3-0-4.50][27098-1-0-0.97][27526-0-0-3.09][27639-3-3-0.90][27698-3-3-4.17][27772-0-0-4.17][27890-1-1-5.16][28040-0-0-0.91][28503-2-2-3.80]
[28577-1-1-1.14][28959-0-0-5.41][29198-3-3-3.79][29777-0-0-7.51][29877-2-2-1.26][30035-1-1-5.28][30098-0-3-2.94][30326-1-1-6.67][30572-2-2-3.56][30716-0-4-0.52]
[30806-2-2-2.14][30906-1-1-2.20][31007-0-0-4.71][31181-3-3-1.50][31238-0-0-3.49][31347-0-0-3.86][31422-2-0-1.46][31429-3-3-3.03][31431-0-0-3.06][31432-1-1-2.25]
[31477-0-3-2.78][31524-1-4--0.00][31597-1-1-2.24][31619-1-2-0.62][31701-0-0-3.00][31755-0-0-6.18][31854-3-3-3.36][32074-1-1--0.24][32078-3-3-4.49][32111-1-1-3.31]
[32127-1-1-0.98][32140-3-3-3.62][32263-2-0-1.06][32365-0-0-6.24][32411-2-0-4.07][32429-3-0-4.31][32473-3-0-2.40][32574-3-0-4.26][32584-0-0-2.38][32622-0-1--0.05]
[32858-3-3-3.44][32969-3-3-5.14][33016-2-2-3.50][33031-1-3-4.81][33035-2-2-1.65][33133-2-2-3.36][33173-2-3-0.20][33175-3-3-0.38][33306-3-2-0.91][33309-2-3-1.00]
[33474-0-3-1.71][33478-2-3-1.66][33618-1-1-0.86][33712-0-0-2.80][33782-2-4-1.67][33914-3-3-6.59][34076-3-2-1.13][34112-2-2-0.67][34138-2-3-0.99][34239-1-1-0.93]
[34364-2-2-4.59][34617-1-2-1.16][34751-3-3-4.86][34783-2-4-2.33][35015-3-3-1.49][35018-1-1-0.99][35288-2-3--0.11][0-4-3-0.97][1-4-4-1.09][2-4-0-2.09]
[3-4-4-1.29][4-4-4-1.58][5-4-3-3.13][6-4-4-4.81][7-4-0-3.30][8-4-2-0.44][9-4-4-1.62][10-4-4-2.79][11-4-4-2.75][12-4-0--0.07]
[14-4-0-1.37][15-4-0-1.85][16-4-4-1.81][17-4-0-1.26][18-4-4-3.41][19-4-0-3.71][20-4-0-2.17][21-4-2-0.77][22-4-4-1.86][23-4-0-1.19]
[24-4-4-5.42][25-4-3-0.40][26-4-3-0.76][27-4-4-1.34][28-4-4-3.49][29-4-1-0.55][30-4-0-0.54][31-4-0--0.09][32-4-4-2.05][33-4-2-1.13]
[34-4-4-0.92][35-4-4-1.21][37-4-4-1.39][39-4-0-4.09][40-4-0-0.58][41-4-3--0.79][42-4-4-0.08][43-4-4-0.98][45-4-4-0.63][46-4-4-2.86]
[47-4-4-4.63][48-4-4-2.12][51-4-4-1.77][52-4-4-2.16][53-4-4-0.64][54-4-4-2.04][55-4-2-1.63][56-4-2-0.02][57-4-0-1.82][58-4-4-1.17]
[59-4-0-3.68][60-4-4-0.32][61-4-4-3.01][62-4-3-2.50][63-4-4-2.94][64-4-2-2.30][65-4-4-4.07][66-4-4-2.16][67-4-0-1.08][68-4-3-1.90]
[69-4-3-0.15][70-4-4-2.25][72-4-4-1.65][73-4-1-1.50][74-4-4-1.55][75-4-0-1.49][77-4-4-2.48][78-4-2-0.07][79-4-4-1.25][80-4-4-1.77]
[81-4-2-2.08][82-4-4-0.77][83-4-4-0.36][84-4-4-1.79][85-4-4-2.16][86-4-4-0.76][87-4-4-1.98][88-4-4-1.52][89-4-2-0.57][90-4-4-0.65]
[91-4-4--0.18][92-4-0-0.59][93-4-0-1.82][94-4-4-0.78][95-4-4-1.09][96-4-4-1.38][97-4-4-3.21][98-4-2-2.18][99-4-4-0.47][100-4-2-1.88]
[101-4-4-2.93][102-4-4-0.93][103-4-2-1.16][104-4-4-0.75][105-4-2-0.70][106-4-1-1.40][107-4-0-1.79][108-4-4-1.22][109-4-3-1.45][110-4-0-1.67]
[111-4-0-4.24][112-4-4-1.13][113-4-3-0.48][114-4-2-0.94][115-4-0-0.56][116-4-4-0.99][117-4-4-2.56][119-4-2-1.52][121-4-4-1.49][122-4-4-2.47]
[124-4-3-0.21][125-4-4-2.66][126-4-4-2.77][127-4-4-0.68][128-4-0-0.11][129-4-4-1.06][130-4-4-1.23][131-4-2-0.80][132-4-4-0.37][133-4-4-3.31]
[135-4-2-1.12][136-4-0-0.77][137-4-4-0.99][138-4-2-0.39][139-4-3-1.34][140-4-4-0.28][141-4-4-0.25][142-4-4-2.89][143-4-4-3.65][144-4-4-3.30]
[145-4-2-1.83][148-4-0-3.30][149-4-4-1.43][150-4-4-3.24][151-4-4-1.08][152-4-4-1.85][153-4-2-3.39][154-4-4-5.41][155-4-4-3.44][156-4-3-1.70]
[157-4-0-1.39][158-4-2-0.87][160-4-1-0.08][161-4-2-2.98][162-4-4-0.60][164-4-4-2.30][165-4-4-0.60][167-4-0-2.57][168-4-0-2.07][170-4-4-0.53]
[171-4-4-1.43][172-4-4-3.81][173-4-4-3.74][174-4-0-2.35][175-4-4-0.85][177-4-0-2.79][178-4-4-2.02][179-4-0-1.54][180-4-4-2.76][181-4-3-1.92]
[182-4-3-2.82][183-4-4-1.91][184-4-2-0.77][186-4-3-0.01][187-4-3-0.42][188-4-4-1.55][189-4-2-1.01][190-4-3-0.65][191-4-4-1.56][192-4-4-0.38]
[193-4-1-2.52][194-4-0-1.41][195-4-0-2.63][196-4-2-2.71][197-4-4-1.08][198-4-4-5.52][199-4-4-0.64]
---------------------------
I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.175 | Acc: 88.375% | Wgt Acc: 95.763%
	I - Batch: 100 | Loss: 0.193 | Acc: 88.188% | Wgt Acc: 95.264%
	I - Batch: 150 | Loss: 0.184 | Acc: 88.542% | Wgt Acc: 95.434%
	I - Batch: 200 | Loss: 0.183 | Acc: 88.375% | Wgt Acc: 95.582%
	I - Batch: 250 | Loss: 0.183 | Acc: 88.050% | Wgt Acc: 95.558%
I - num batch: 273
I - Train -- Loss: 0.181 | Acc: 88.171% | Wgt Acc: 95.584% | LR: 1.250000e-04 | Dur: 165.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 776.    2.    1.    3.  130.]
 [   3.  745.    3.    2.   64.]
 [   2.    3. 1019.    5.  159.]
 [   5.    3.    2.  760.  101.]
 [  11.    7.    7.    3.  546.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 62.722% | Wgt Acc: 63.464% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   3.  12.   9.]
 [  1.  37.   2.   4.  10.]
 [  2.  23.  52.   5.  40.]
 [ 16.   5.   6.  57.  12.]
 [  6.  11.  12.   8. 109.]]

I - Local maximum validation set accuracy:  62.72

I - Validation set results: 
[14-1-1-1.47][50-3-1-0.32][124-2-2-1.51][127-0-0-5.09][443-2-2-3.77][567-0-0-2.99][573-1-1-4.16][615-0-0-2.84][695-1-2-1.83][722-3-0-1.98]
[826-0-0-3.83][878-0-0-3.50][1103-0-0-3.90][1212-3-4--0.12][1368-0-0-5.38][2181-2-2-1.88][2476-2-2-2.19][2721-2-2-5.28][2818-1-3-0.60][2886-2-2-1.73]
[3231-2-2-5.12][3333-2-2-2.20][3482-2-2-3.87][3536-3-4-0.54][3625-1-1-3.15][3909-0-0-3.64][4035-0-0-4.06][4140-0-0-1.92][4214-1-3-1.44][4346-1-3-1.01]
[4581-2-2-3.28][4708-3-4-2.71][4838-3-0-0.77][4845-1-1-1.35][4868-0-0-3.97][4939-0-0-0.51][4984-2-3-1.62][5078-1-2-1.36][5396-0-0-5.40][5479-1-1-4.49]
[5717-0-0-2.02][5843-1-4-0.71][5949-3-3-1.06][5987-2-4-2.44][6014-3-3-3.47][6033-3-3-1.40][6313-0-0-2.87][6421-3-2--0.08][6500-1-2-1.20][6583-3-3-0.87]
[6683-3-3-2.68][6825-2-1-4.03][6998-3-3-0.67][7049-3-3-1.52][7517-1-1-1.83][7521-1-1-1.00][7528-1-3-1.71][7949-1-2-5.76][8135-1-2-1.07][8185-3-3-2.28]
[8269-3-1-2.00][8273-3-3-3.62][8543-3-0-2.49][8666-1-1-4.63][8672-0-0-5.64][8903-1-2-3.33][9001-2-2-0.62][9036-2-2-3.42][9281-3-3-0.68][9300-2-2-8.69]
[9571-0-4-0.67][9617-1-1-0.46][9644-2-2-4.59][9705-2-0-0.67][9801-0-3-1.19][9803-3-3-1.04][9865-3-3-3.54][9896-2-2-2.65][10314-1-1-0.96][10337-3-3-4.67]
[10403-0-4-1.41][10653-2-4-1.03][10704-2-2-2.66][10719-1-2-3.35][10727-1-0-0.33][10836-0-0-7.39][10969-2-3-2.22][11042-0-0-1.18][11088-1-1-4.90][11322-0-0-3.83]
[11398-2-2-1.97][11499-0-0-2.76][11502-3-3-0.49][11512-3-3-2.75][11608-1-2-5.11][11610-0-0-1.59][11692-0-3-1.49][11905-0-0-2.86][11993-1-1-5.66][12002-2-2-0.72]
[12052-0-0-2.75][12201-0-3-4.38][12235-2-4-1.92][12320-1-4-3.78][12377-2-4-3.60][12398-2-4-1.17][12503-1-2-2.18][12617-0-3--0.09][12685-3-3-1.48][12738-2-2-0.47]
[12742-2-2-6.48][12823-0-3-2.76][13110-1-2-2.46][13240-3-0-2.93][13253-1-1-2.93][13273-0-0-6.49][13634-1-1-2.60][13763-2-3--0.33][13905-3-3-0.60][14060-2-4-2.34]
[14065-3-0-2.31][14147-3-3-3.98][14595-2-1-1.93][14687-2-2-6.46][14788-2-2-5.00][14869-1-4-2.65][14872-3-1-1.08][14877-1-1-4.54][14927-0-3-2.88][15066-0-0-6.38]
[15175-1-4-2.36][15178-2-2-1.62][15375-3-2-0.68][15389-3-3-1.86][15568-2-4-2.19][15675-3-3-3.16][15869-1-0-1.24][16207-3-0-1.22][16236-0-0-1.50][16302-3-3-1.06]
[16331-2-2-5.85][16381-0-0-1.44][16488-1-1-6.00][16495-0-0-3.18][16650-0-0-3.94][16719-1-4-2.13][16801-0-0-5.16][16828-0-0-3.84][17137-3-0-0.08][17245-1-4-1.36]
[17278-3-4-0.78][17282-0-0-1.80][17311-2-2-2.58][17336-2-2-2.62][17608-3-3-3.76][17627-0-0-0.28][17877-3-3-1.58][17924-1-2-0.91][17984-3-0-3.11][18211-0-3-0.44]
[18276-3-3-2.30][18287-1-1-1.44][18394-0-0-2.57][18428-0-0-1.05][18442-0-3-5.13][18478-3-3-1.78][18607-0-0-3.56][18616-0-0-2.45][18663-0-0-3.88][18718-0-0-3.57]
[18766-2-2-4.62][18824-2-2-2.66][18890-3-3-4.52][18930-3-4-1.63][18938-3-2-1.74][19817-1-2-4.73][19839-0-0-0.04][19930-3-3-2.26][19944-0-1-0.01][20036-2-2-5.91]
[20101-3-3-3.28][20474-1-1-2.99][20547-3-3-0.51][20929-2-2-4.47][21245-1-2-4.52][21257-3-4-0.82][21293-1-2-3.45][21316-1-1-6.53][21384-1-1-3.18][21448-1-2-1.84]
[21483-0-0-4.23][21487-2-2-3.06][21714-0-3-0.68][21943-3-3-1.31][21947-0-0-2.34][21948-0-0-5.07][21965-2-2-2.91][21998-1-1-5.52][22025-0-4-2.62][22228-3-3-5.80]
[22446-1-1-2.38][22494-3-3-1.87][22757-0-0-2.65][22811-3-3-3.34][22976-3-2-2.05][22985-3-3-4.19][23014-0-0-3.52][23112-1-1-2.14][23144-3-3-4.60][23168-2-3-1.10]
[23219-0-2-0.79][23363-3-3-3.55][23470-0-0-1.67][23486-2-2-2.25][23497-0-3-4.40][23516-0-0-3.60][23690-1-1-1.65][23921-2-2-3.43][23936-1-2-1.94][24040-3-3-0.56]
[24111-1-4-1.68][24182-0-0-6.02][24238-3-3-4.84][24290-2-0-3.98][24345-0-0-0.07][24364-1-2-1.27][24427-3-0-2.87][24477-2-2-0.89][24495-2-4-1.12][24893-2-2-3.92]
[25012-1-2-1.72][25121-2-2-3.90][25165-3-3-3.99][25183-0-0-3.90][25297-3-3-2.42][25398-0-0-2.44][25574-2-2-3.59][25644-1-2-3.10][25718-1-4-1.48][25774-2-4-1.25]
[26032-3-3-2.41][26051-3-3-4.83][26120-0-4-3.60][26321-1-4-0.68][26732-1-1-3.83][26784-3-3-7.08][26827-3-0-0.48][26833-0-3-2.57][26838-2-2-1.13][26860-1-1-2.82]
[26948-0-0-1.42][27049-3-0-2.86][27098-1-1-2.31][27526-0-0-1.96][27639-3-4-1.62][27698-3-3-4.24][27772-0-3-1.32][27890-1-1-3.57][28040-0-0-1.05][28503-2-2-4.52]
[28577-1-1-2.02][28959-0-0-5.44][29198-3-3-5.08][29777-0-0-6.08][29877-2-2-1.25][30035-1-1-4.63][30098-0-3-2.93][30326-1-1-6.37][30572-2-2-3.79][30716-0-4-2.03]
[30806-2-2-1.84][30906-1-1-3.41][31007-0-0-3.27][31181-3-3-2.39][31238-0-0-2.40][31347-0-0-3.79][31422-2-2-2.40][31429-3-3-2.62][31431-0-0-1.26][31432-1-1-3.46]
[31477-0-3-2.99][31524-1-4-0.48][31597-1-2-1.83][31619-1-2-1.73][31701-0-0-2.12][31755-0-0-3.60][31854-3-3-3.35][32074-1-4--0.14][32078-3-3-4.24][32111-1-1-3.93]
[32127-1-1-2.09][32140-3-3-4.58][32263-2-4-0.74][32365-0-0-3.71][32411-2-0-2.93][32429-3-3-4.05][32473-3-0-1.58][32574-3-3-3.73][32584-0-4-3.55][32622-0-2-0.16]
[32858-3-3-3.81][32969-3-3-4.57][33016-2-2-4.41][33031-1-3-3.88][33035-2-2-4.12][33133-2-2-4.23][33173-2-2-0.32][33175-3-1-0.79][33306-3-2-1.19][33309-2-4-0.24]
[33474-0-3-2.28][33478-2-3-0.88][33618-1-2-1.29][33712-0-3-0.89][33782-2-4-1.41][33914-3-3-5.73][34076-3-4-1.53][34112-2-2-1.61][34138-2-3-0.56][34239-1-1-1.66]
[34364-2-2-4.93][34617-1-2-2.00][34751-3-3-4.20][34783-2-2-3.20][35015-3-3-1.23][35018-1-1-1.58][35288-2-2-2.00][0-4-2-1.81][1-4-4-3.12][2-4-0-2.13]
[3-4-4-3.56][4-4-1-0.79][5-4-1-1.93][6-4-4-5.87][7-4-4-3.15][8-4-2-1.54][9-4-4-1.83][10-4-4-4.35][11-4-2-4.12][12-4-4-0.44]
[14-4-3-1.31][15-4-3-1.40][16-4-4-2.33][17-4-0-0.68][18-4-4-6.95][19-4-3-1.89][20-4-4-1.20][21-4-2-2.61][22-4-4-1.33][23-4-4-0.51]
[24-4-4-8.41][25-4-3-1.85][26-4-4-1.29][27-4-2-1.84][28-4-4-3.98][29-4-1-2.42][30-4-4--0.23][31-4-1-1.40][32-4-4-1.95][33-4-2-2.99]
[34-4-4-0.52][35-4-4-2.06][37-4-4-1.85][39-4-4-0.79][40-4-4-0.28][41-4-4-0.16][42-4-3-1.51][43-4-4-1.09][45-4-2-1.94][46-4-4-3.80]
[47-4-4-5.20][48-4-4-3.42][51-4-4-2.39][52-4-0-2.58][53-4-4-2.44][54-4-4-1.78][55-4-2-2.83][56-4-2-2.03][57-4-4--0.27][58-4-2-2.55]
[59-4-4-2.90][60-4-4-2.99][61-4-4-3.23][62-4-3-0.91][63-4-4-3.12][64-4-2-3.18][65-4-4-5.52][66-4-4-3.36][67-4-0-0.27][68-4-3-0.89]
[69-4-4-0.71][70-4-4-3.11][72-4-4-1.75][73-4-1-3.74][74-4-2-3.96][75-4-2-0.36][77-4-4-5.24][78-4-4--0.02][79-4-2-2.68][80-4-4-4.12]
[81-4-2-2.03][82-4-4-0.92][83-4-1-0.96][84-4-4-3.77][85-4-4-5.62][86-4-4-0.96][87-4-4-3.07][88-4-4-2.59][89-4-3-1.09][90-4-4-0.08]
[91-4-4-0.76][92-4-4-0.43][93-4-0-2.87][94-4-4-1.31][95-4-4-1.83][96-4-4-1.91][97-4-4-2.44][98-4-2-3.85][99-4-4-1.18][100-4-2-2.65]
[101-4-4-5.72][102-4-4-1.58][103-4-2-0.81][104-4-4-0.96][105-4-4-3.80][106-4-4-4.56][107-4-0-2.41][108-4-4-1.06][109-4-4-1.59][110-4-4-2.28]
[111-4-0-3.65][112-4-4-0.36][113-4-4-2.17][114-4-2-2.30][115-4-1-1.40][116-4-4-1.24][117-4-4-2.85][119-4-2-3.02][121-4-4-1.07][122-4-4-4.24]
[124-4-3-0.68][125-4-4-2.42][126-4-4-7.32][127-4-4-0.75][128-4-2-0.45][129-4-4-1.11][130-4-4-1.06][131-4-4--0.03][132-4-2-2.76][133-4-4-4.38]
[135-4-2-2.24][136-4-4-1.13][137-4-2-0.64][138-4-2-0.81][139-4-4-2.79][140-4-4-0.28][141-4-2-3.09][142-4-4-3.04][143-4-4-4.20][144-4-4-4.10]
[145-4-4-1.34][148-4-0-2.75][149-4-4-0.43][150-4-4-3.94][151-4-4-3.03][152-4-4-3.61][153-4-2-4.01][154-4-4-4.65][155-4-4-4.69][156-4-4-0.76]
[157-4-2-1.74][158-4-4-0.76][160-4-1-1.27][161-4-2-2.56][162-4-2-1.04][164-4-4-2.01][165-4-4-1.13][167-4-4-2.20][168-4-4-1.12][170-4-4-1.03]
[171-4-4-2.23][172-4-4-4.99][173-4-4-4.87][174-4-0-2.08][175-4-4-2.82][177-4-4-3.41][178-4-2-1.15][179-4-4-2.20][180-4-4-3.52][181-4-3-1.52]
[182-4-3-1.98][183-4-4-3.20][184-4-2-2.85][186-4-2-0.44][187-4-1-1.86][188-4-2-0.67][189-4-2-1.46][190-4-3--0.10][191-4-4-1.75][192-4-4--0.05]
[193-4-2-3.86][194-4-2-2.44][195-4-2-0.23][196-4-2-5.08][197-4-1-2.76][198-4-4-6.07][199-4-2-1.56]
---------------------------
I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.183 | Acc: 86.625% | Wgt Acc: 95.093%
	I - Batch: 100 | Loss: 0.182 | Acc: 86.812% | Wgt Acc: 95.239%
	I - Batch: 150 | Loss: 0.179 | Acc: 87.292% | Wgt Acc: 95.400%
	I - Batch: 200 | Loss: 0.179 | Acc: 87.188% | Wgt Acc: 95.315%
	I - Batch: 250 | Loss: 0.178 | Acc: 87.550% | Wgt Acc: 95.386%
I - num batch: 273
I - Train -- Loss: 0.178 | Acc: 87.735% | Wgt Acc: 95.484% | LR: 1.250000e-04 | Dur: 163.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 777.    1.    1.    6.  119.]
 [   2.  749.    3.    3.   75.]
 [   3.    1. 1021.    2.  162.]
 [   8.    2.    2.  754.  118.]
 [   7.    7.    5.    8.  526.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.285 | Acc: 58.185% | Wgt Acc: 63.165% | Dur: 14.12s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  0.  2. 13. 19.]
 [ 2. 36.  5.  2. 11.]
 [ 5. 31. 60.  8. 58.]
 [15.  7.  4. 57. 11.]
 [ 5.  4.  4.  6. 81.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.165 | Acc: 87.375% | Wgt Acc: 95.572%
	I - Batch: 100 | Loss: 0.170 | Acc: 88.000% | Wgt Acc: 95.776%
	I - Batch: 150 | Loss: 0.170 | Acc: 88.458% | Wgt Acc: 95.920%
	I - Batch: 200 | Loss: 0.169 | Acc: 88.688% | Wgt Acc: 95.962%
	I - Batch: 250 | Loss: 0.169 | Acc: 88.650% | Wgt Acc: 96.033%
I - num batch: 273
I - Train -- Loss: 0.167 | Acc: 88.973% | Wgt Acc: 96.149% | LR: 1.250000e-04 | Dur: 163.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 778.    2.    0.    2.  119.]
 [   4.  750.    1.    2.   58.]
 [   2.    4. 1023.    2.  149.]
 [   5.    1.    0.  765.  109.]
 [   8.    3.    8.    2.  565.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.138 | Acc: 63.511% | Wgt Acc: 64.272% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   5.   6.  15.  22.]
 [  0.  37.   3.   2.   4.]
 [  1.  18.  45.   1.  23.]
 [ 18.   7.  12.  64.  20.]
 [  4.  11.   9.   4. 111.]]

I - Local maximum validation set accuracy:  63.51

I - Validation set results: 
[14-1-0-0.44][50-3-1-0.60][124-2-2-3.87][127-0-0-6.54][443-2-2-2.89][567-0-0-4.29][573-1-1-5.97][615-0-0-2.49][695-1-2-1.28][722-3-3-2.61]
[826-0-0-6.81][878-0-0-8.28][1103-0-0-1.76][1212-3-3-0.88][1368-0-0-6.44][2181-2-3-2.21][2476-2-2-0.79][2721-2-2-3.02][2818-1-3-0.98][2886-2-2-1.80]
[3231-2-2-4.49][3333-2-3-2.20][3482-2-2-2.83][3536-3-3-0.57][3625-1-1-3.30][3909-0-0-2.69][4035-0-0-5.59][4140-0-0-3.40][4214-1-3-0.71][4346-1-4-0.70]
[4581-2-2-2.05][4708-3-3-2.78][4838-3-0-1.92][4845-1-2-0.44][4868-0-0-3.96][4939-0-0-1.13][4984-2-3-2.08][5078-1-2-0.69][5396-0-0-7.45][5479-1-1-3.98]
[5717-0-0-5.10][5843-1-1-0.46][5949-3-3-1.29][5987-2-4-3.31][6014-3-3-3.17][6033-3-3-3.50][6313-0-0-3.63][6421-3-3-2.39][6500-1-2-2.37][6583-3-3-1.40]
[6683-3-3-3.14][6825-2-1-4.18][6998-3-3-1.02][7049-3-3-1.99][7517-1-1-4.00][7521-1-1--0.44][7528-1-3-1.24][7949-1-2-4.43][8135-1-0-1.11][8185-3-0-6.21]
[8269-3-4-0.88][8273-3-3-3.50][8543-3-0-4.49][8666-1-1-3.08][8672-0-0-4.92][8903-1-3-1.32][9001-2-1-1.30][9036-2-2-2.95][9281-3-3-2.42][9300-2-2-7.85]
[9571-0-3-1.12][9617-1-4-0.91][9644-2-2-4.10][9705-2-0-2.27][9801-0-0-3.24][9803-3-0-2.00][9865-3-3-4.86][9896-2-2-2.47][10314-1-2-0.88][10337-3-3-5.58]
[10403-0-0-1.67][10653-2-4--0.02][10704-2-2-1.55][10719-1-1-1.79][10727-1-4-0.42][10836-0-0-9.04][10969-2-3-2.22][11042-0-0-1.60][11088-1-1-5.16][11322-0-0-5.51]
[11398-2-2-0.87][11499-0-0-4.93][11502-3-0-2.42][11512-3-3-2.00][11608-1-2-2.39][11610-0-0-4.04][11692-0-0-3.34][11905-0-0-3.88][11993-1-1-4.47][12002-2-2-1.28]
[12052-0-0-3.59][12201-0-3-4.11][12235-2-2-3.03][12320-1-4-4.53][12377-2-4-3.74][12398-2-4-0.85][12503-1-2-0.87][12617-0-3-0.86][12685-3-3-1.13][12738-2-0-1.85]
[12742-2-2-6.98][12823-0-0-6.32][13110-1-2-2.16][13240-3-0-3.56][13253-1-1-2.82][13273-0-0-7.71][13634-1-1-2.11][13763-2-3-0.29][13905-3-3-1.84][14060-2-4-2.03]
[14065-3-3-1.84][14147-3-3-4.81][14595-2-1-3.00][14687-2-2-5.77][14788-2-2-4.67][14869-1-1-3.82][14872-3-1-0.70][14877-1-1-3.59][14927-0-3-3.58][15066-0-0-7.39]
[15175-1-4-3.62][15178-2-0-0.81][15375-3-3-0.73][15389-3-3-4.30][15568-2-4-0.98][15675-3-3-2.83][15869-1-0-0.94][16207-3-0-1.92][16236-0-0-3.39][16302-3-3-1.19]
[16331-2-2-4.56][16381-0-3-2.33][16488-1-1-6.01][16495-0-0-3.97][16650-0-0-5.67][16719-1-4-2.26][16801-0-0-6.97][16828-0-0-3.91][17137-3-3-0.23][17245-1-4-1.29]
[17278-3-0-0.77][17282-0-0-2.98][17311-2-2-1.92][17336-2-2-1.59][17608-3-3-6.00][17627-0-0-2.01][17877-3-3-1.74][17924-1-4-0.07][17984-3-3-1.82][18211-0-3-1.29]
[18276-3-3-3.25][18287-1-1-2.71][18394-0-0-4.03][18428-0-0-7.56][18442-0-3-5.23][18478-3-3-2.72][18607-0-0-3.59][18616-0-0-1.91][18663-0-0-4.55][18718-0-0-5.30]
[18766-2-2-2.10][18824-2-4-2.77][18890-3-3-4.25][18930-3-4-2.33][18938-3-3-2.59][19817-1-2-2.36][19839-0-2--0.07][19930-3-3-3.17][19944-0-3-1.55][20036-2-2-5.12]
[20101-3-3-3.19][20474-1-1-0.75][20547-3-3-1.08][20929-2-2-4.26][21245-1-2-4.24][21257-3-3-1.10][21293-1-1-2.52][21316-1-1-6.66][21384-1-4-2.73][21448-1-2-0.80]
[21483-0-0-5.80][21487-2-2-2.85][21714-0-3-1.71][21943-3-3-2.01][21947-0-0-3.55][21948-0-0-7.26][21965-2-2-4.56][21998-1-1-3.93][22025-0-4-1.68][22228-3-3-5.61]
[22446-1-1-1.31][22494-3-0-2.33][22757-0-0-4.49][22811-3-3-6.97][22976-3-2-3.03][22985-3-3-5.26][23014-0-0-4.93][23112-1-1-3.61][23144-3-3-6.16][23168-2-3-1.86]
[23219-0-0-1.78][23363-3-3-4.08][23470-0-0-1.54][23486-2-2-2.09][23497-0-3-5.64][23516-0-0-4.61][23690-1-4-0.81][23921-2-2-2.85][23936-1-2-1.64][24040-3-0-1.04]
[24111-1-4-1.15][24182-0-0-5.68][24238-3-3-5.50][24290-2-0-3.75][24345-0-0-3.14][24364-1-2-1.45][24427-3-0-6.25][24477-2-2-2.15][24495-2-4-1.59][24893-2-2-1.57]
[25012-1-2-1.46][25121-2-2-4.46][25165-3-3-3.96][25183-0-0-2.60][25297-3-3-3.23][25398-0-0-3.97][25574-2-2-3.00][25644-1-1-2.28][25718-1-3-0.57][25774-2-2-0.98]
[26032-3-0-2.46][26051-3-3-7.08][26120-0-0-1.78][26321-1-1-2.62][26732-1-1-3.29][26784-3-3-7.35][26827-3-3-1.15][26833-0-3-3.58][26838-2-2-0.24][26860-1-1-0.28]
[26948-0-0-2.21][27049-3-0-4.95][27098-1-0-2.52][27526-0-3-2.98][27639-3-3-0.70][27698-3-3-3.21][27772-0-0-5.23][27890-1-1-1.20][28040-0-4-2.42][28503-2-2-3.19]
[28577-1-1-0.43][28959-0-0-5.81][29198-3-3-4.54][29777-0-0-7.76][29877-2-2-0.02][30035-1-1-4.76][30098-0-3-2.53][30326-1-1-7.65][30572-2-2-4.51][30716-0-4-2.13]
[30806-2-2-2.01][30906-1-1-4.47][31007-0-0-2.83][31181-3-3-3.52][31238-0-0-2.95][31347-0-0-4.99][31422-2-2-1.58][31429-3-3-2.52][31431-0-3-0.53][31432-1-1-2.64]
[31477-0-3-3.75][31524-1-2-0.33][31597-1-1-2.99][31619-1-0-0.49][31701-0-0-4.79][31755-0-0-6.16][31854-3-3-3.60][32074-1-3-1.38][32078-3-3-5.18][32111-1-1-6.37]
[32127-1-1-1.78][32140-3-3-4.18][32263-2-0-2.20][32365-0-0-5.05][32411-2-0-3.30][32429-3-3-6.27][32473-3-0-2.87][32574-3-0-3.51][32584-0-4-3.97][32622-0-3-0.58]
[32858-3-3-3.72][32969-3-3-5.70][33016-2-2-4.69][33031-1-3-4.05][33035-2-2-2.86][33133-2-2-2.15][33173-2-3-1.23][33175-3-4-0.69][33306-3-3-0.91][33309-2-3-0.02]
[33474-0-3-1.90][33478-2-3-0.89][33618-1-2-0.64][33712-0-3-1.62][33782-2-4-1.88][33914-3-3-5.40][34076-3-4-1.89][34112-2-3-0.17][34138-2-3-1.57][34239-1-1-1.65]
[34364-2-2-4.21][34617-1-2-2.02][34751-3-3-4.60][34783-2-2-1.23][35015-3-3-2.45][35018-1-1-2.22][35288-2-3-0.49][0-4-2-2.85][1-4-4-2.61][2-4-0-2.18]
[3-4-4-2.70][4-4-4-2.59][5-4-3-1.25][6-4-4-3.58][7-4-4-1.58][8-4-2-0.80][9-4-4-2.38][10-4-4-5.42][11-4-4-2.68][12-4-4-0.55]
[14-4-3-0.94][15-4-3-5.30][16-4-4-2.81][17-4-0-0.06][18-4-4-7.01][19-4-0-3.60][20-4-0-1.90][21-4-4-1.00][22-4-0-1.51][23-4-0-0.31]
[24-4-4-6.72][25-4-3-1.39][26-4-4-1.57][27-4-4-2.79][28-4-4-3.03][29-4-1-1.19][30-4-1-0.09][31-4-4-1.64][32-4-4-1.45][33-4-3-1.02]
[34-4-4-0.89][35-4-4-2.52][37-4-3-2.38][39-4-0-2.40][40-4-4-0.59][41-4-4-1.12][42-4-4-1.35][43-4-4-0.99][45-4-2-1.59][46-4-4-5.20]
[47-4-4-5.32][48-4-4-2.91][51-4-4-2.96][52-4-4-2.59][53-4-4-0.35][54-4-4-1.80][55-4-4-2.09][56-4-2-0.74][57-4-0-1.07][58-4-2-3.02]
[59-4-4-3.09][60-4-4-3.05][61-4-4-3.79][62-4-3-2.21][63-4-4-4.28][64-4-2-2.73][65-4-4-6.49][66-4-4-1.23][67-4-0-1.44][68-4-2-0.66]
[69-4-3-1.60][70-4-4-3.52][72-4-4-2.33][73-4-1-1.38][74-4-4-1.56][75-4-4-0.56][77-4-4-3.50][78-4-2-0.19][79-4-4-2.13][80-4-4-1.88]
[81-4-4-2.72][82-4-4-0.99][83-4-4-1.80][84-4-4-2.73][85-4-4-5.39][86-4-4-0.92][87-4-4-3.50][88-4-4-3.32][89-4-3-2.14][90-4-2-0.92]
[91-4-4-2.11][92-4-4--0.16][93-4-0-2.76][94-4-4-1.79][95-4-4-1.24][96-4-4-1.57][97-4-4-2.09][98-4-2-2.41][99-4-4-1.48][100-4-2-1.86]
[101-4-4-6.54][102-4-4-1.43][103-4-0-0.67][104-4-4-1.77][105-4-4-1.28][106-4-4-3.41][107-4-0-1.79][108-4-4-0.33][109-4-4-3.28][110-4-4-1.54]
[111-4-0-4.31][112-4-4--0.38][113-4-4-1.09][114-4-3-0.17][115-4-4-0.94][116-4-0-0.61][117-4-4-3.51][119-4-4-1.42][121-4-4-1.86][122-4-4-4.10]
[124-4-3-1.00][125-4-4-3.14][126-4-4-4.44][127-4-2-1.51][128-4-3-1.30][129-4-4-1.77][130-4-2-1.03][131-4-3-1.56][132-4-4-0.81][133-4-4-4.85]
[135-4-3-1.49][136-4-4-0.88][137-4-4-0.81][138-4-2-0.65][139-4-4-2.83][140-4-4-0.42][141-4-0-1.66][142-4-4-2.59][143-4-4-3.59][144-4-4-2.11]
[145-4-2-1.18][148-4-0-3.35][149-4-3-2.36][150-4-4-3.84][151-4-4-2.93][152-4-4-2.73][153-4-4-2.67][154-4-4-5.58][155-4-4-3.58][156-4-3-2.55]
[157-4-0-2.67][158-4-4-0.97][160-4-4--0.04][161-4-2-3.11][162-4-4-0.69][164-4-4-2.12][165-4-4-1.06][167-4-4-2.20][168-4-0-2.01][170-4-4-0.80]
[171-4-4-3.02][172-4-4-4.73][173-4-4-5.51][174-4-0-1.53][175-4-4-2.85][177-4-0-3.78][178-4-4-1.27][179-4-0-1.08][180-4-4-4.22][181-4-3-1.53]
[182-4-3-3.66][183-4-4-3.16][184-4-2-2.01][186-4-4-0.64][187-4-2-0.39][188-4-4-1.90][189-4-2-1.32][190-4-3-2.06][191-4-4-2.96][192-4-3-0.66]
[193-4-2-2.76][194-4-2-2.47][195-4-0-1.56][196-4-2-4.66][197-4-1-1.11][198-4-4-6.42][199-4-2-0.09]
---------------------------
I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.154 | Acc: 90.125% | Wgt Acc: 96.360%
	I - Batch: 100 | Loss: 0.154 | Acc: 89.438% | Wgt Acc: 96.490%
	I - Batch: 150 | Loss: 0.164 | Acc: 88.708% | Wgt Acc: 96.001%
	I - Batch: 200 | Loss: 0.163 | Acc: 89.000% | Wgt Acc: 96.097%
	I - Batch: 250 | Loss: 0.163 | Acc: 89.200% | Wgt Acc: 96.195%
I - num batch: 273
I - Train -- Loss: 0.165 | Acc: 88.927% | Wgt Acc: 96.082% | LR: 1.250000e-04 | Dur: 166.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 778.    1.    2.    4.  117.]
 [   3.  753.    1.    1.   67.]
 [   6.    0. 1025.    4.  142.]
 [   1.    3.    1.  758.  109.]
 [   9.    3.    3.    6.  565.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.153 | Acc: 62.919% | Wgt Acc: 60.881% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   7.  15.  17.]
 [  0.  34.   3.   2.   3.]
 [  1.  18.  39.   2.  19.]
 [ 17.  10.  15.  61.  18.]
 [  8.  14.  11.   6. 123.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.158 | Acc: 89.500% | Wgt Acc: 95.995%
	I - Batch: 100 | Loss: 0.168 | Acc: 88.375% | Wgt Acc: 95.821%
	I - Batch: 150 | Loss: 0.171 | Acc: 88.042% | Wgt Acc: 95.713%
	I - Batch: 200 | Loss: 0.166 | Acc: 88.438% | Wgt Acc: 95.806%
	I - Batch: 250 | Loss: 0.166 | Acc: 88.600% | Wgt Acc: 95.930%
I - num batch: 273
I - Train -- Loss: 0.167 | Acc: 88.537% | Wgt Acc: 95.869% | LR: 1.250000e-04 | Dur: 167.56s
I - Confusion Matrix: [row->prediction - col->label]
[[ 779.    3.    2.    6.  120.]
 [   0.  750.    1.    3.   53.]
 [   4.    5. 1023.    0.  170.]
 [   5.    0.    2.  757.  104.]
 [   9.    2.    4.    7.  553.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.097 | Acc: 62.919% | Wgt Acc: 64.122% | Dur: 14.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   1.   4.  14.  17.]
 [  1.  43.   9.   3.  15.]
 [  1.  18.  40.   1.  26.]
 [ 17.   7.  11.  62.  14.]
 [  3.   9.  11.   6. 108.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.162 | Acc: 88.250% | Wgt Acc: 95.372%
	I - Batch: 100 | Loss: 0.158 | Acc: 88.625% | Wgt Acc: 95.809%
	I - Batch: 150 | Loss: 0.165 | Acc: 88.583% | Wgt Acc: 95.863%
	I - Batch: 200 | Loss: 0.163 | Acc: 88.656% | Wgt Acc: 95.705%
	I - Batch: 250 | Loss: 0.163 | Acc: 89.025% | Wgt Acc: 95.918%
I - num batch: 273
I - Train -- Loss: 0.162 | Acc: 89.042% | Wgt Acc: 95.958% | LR: 1.250000e-04 | Dur: 163.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 778.    3.    0.    2.  125.]
 [   0.  747.    3.    4.   58.]
 [   3.    2. 1023.    3.  131.]
 [   5.    0.    1.  759.  109.]
 [  11.    8.    5.    5.  577.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.226 | Acc: 61.736% | Wgt Acc: 59.416% | Dur: 13.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   7.  21.  19.]
 [  0.  31.   4.   2.   6.]
 [  0.  21.  42.   1.  26.]
 [ 12.   9.  10.  50.   8.]
 [  7.  13.  12.  12. 121.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.169 | Acc: 88.750% | Wgt Acc: 95.789%
	I - Batch: 100 | Loss: 0.154 | Acc: 89.875% | Wgt Acc: 96.362%
	I - Batch: 150 | Loss: 0.154 | Acc: 89.750% | Wgt Acc: 96.376%
	I - Batch: 200 | Loss: 0.157 | Acc: 89.375% | Wgt Acc: 96.039%
	I - Batch: 250 | Loss: 0.159 | Acc: 89.400% | Wgt Acc: 96.036%
I - num batch: 273
I - Train -- Loss: 0.159 | Acc: 89.409% | Wgt Acc: 96.007% | LR: 1.250000e-04 | Dur: 165.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 775.    0.    1.    9.  114.]
 [   2.  755.    1.    2.   64.]
 [   4.    2. 1021.    0.  141.]
 [   8.    1.    1.  754.   86.]
 [   8.    2.    8.    8.  595.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.142 | Acc: 64.892% | Wgt Acc: 64.006% | Dur: 14.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   5.   6.  17.  23.]
 [  0.  43.   5.   2.   8.]
 [  1.  14.  39.   0.  15.]
 [ 14.   6.  13.  59.  13.]
 [  6.  10.  12.   8. 121.]]

I - Local maximum validation set accuracy:  64.89

I - Validation set results: 
[14-1-1-0.17][50-3-1-2.78][124-2-2-0.34][127-0-0-6.76][443-2-2-1.84][567-0-0-3.46][573-1-1-3.18][615-0-0-2.41][695-1-0-0.28][722-3-3-3.54]
[826-0-0-7.02][878-0-0-4.12][1103-0-0-2.13][1212-3-3-0.21][1368-0-0-6.78][2181-2-3-2.56][2476-2-2-0.32][2721-2-2-3.44][2818-1-1-0.90][2886-2-4-1.61]
[3231-2-2-5.00][3333-2-3-1.54][3482-2-2-3.33][3536-3-4-1.28][3625-1-1-5.49][3909-0-0-5.03][4035-0-0-5.39][4140-0-0-4.46][4214-1-3-3.01][4346-1-4-0.80]
[4581-2-2-2.86][4708-3-3-2.76][4838-3-0-1.55][4845-1-1-0.95][4868-0-0-4.93][4939-0-4-0.63][4984-2-3-1.99][5078-1-2-1.92][5396-0-0-6.85][5479-1-1-2.79]
[5717-0-0-3.51][5843-1-4-0.55][5949-3-3-1.69][5987-2-4-3.25][6014-3-3-2.41][6033-3-3-2.38][6313-0-0-4.63][6421-3-3-2.49][6500-1-2-0.94][6583-3-3-1.87]
[6683-3-3-2.71][6825-2-1-2.09][6998-3-3-0.99][7049-3-3-1.57][7517-1-1-1.96][7521-1-1-2.32][7528-1-3-3.11][7949-1-2-4.74][8135-1-0--0.49][8185-3-0-5.76]
[8269-3-1-7.52][8273-3-3-4.41][8543-3-0-4.25][8666-1-1-3.87][8672-0-0-7.68][8903-1-2-0.64][9001-2-2-0.99][9036-2-2-4.78][9281-3-3-1.41][9300-2-2-7.99]
[9571-0-3-1.20][9617-1-1-0.62][9644-2-2-4.46][9705-2-0-3.67][9801-0-0-3.14][9803-3-3-0.90][9865-3-3-1.90][9896-2-4-1.09][10314-1-1-0.94][10337-3-3-4.61]
[10403-0-4-1.43][10653-2-4-1.57][10704-2-2-2.69][10719-1-1-1.12][10727-1-4-1.07][10836-0-0-11.31][10969-2-3-0.51][11042-0-2-1.18][11088-1-1-5.47][11322-0-0-4.79]
[11398-2-2-1.22][11499-0-0-4.11][11502-3-0-3.20][11512-3-3-2.83][11608-1-2-4.81][11610-0-0-3.45][11692-0-0-2.07][11905-0-0-5.18][11993-1-1-5.31][12002-2-0-5.06]
[12052-0-0-4.33][12201-0-3-4.37][12235-2-4-2.40][12320-1-4-3.54][12377-2-4-3.71][12398-2-4-1.11][12503-1-2-3.02][12617-0-3-1.47][12685-3-3-1.82][12738-2-0-0.62]
[12742-2-2-6.83][12823-0-0-6.49][13110-1-2-1.63][13240-3-0-4.07][13253-1-1-2.13][13273-0-0-7.49][13634-1-1-2.16][13763-2-2--0.06][13905-3-0-1.94][14060-2-4-2.84]
[14065-3-0-2.15][14147-3-3-4.51][14595-2-1-1.92][14687-2-2-5.42][14788-2-2-3.46][14869-1-1-2.71][14872-3-4-0.24][14877-1-1-3.08][14927-0-3-2.21][15066-0-0-7.40]
[15175-1-4-3.12][15178-2-2-1.28][15375-3-0-3.42][15389-3-3-3.65][15568-2-4-0.96][15675-3-3-1.89][15869-1-0-1.94][16207-3-0-2.41][16236-0-0-1.07][16302-3-3-2.52]
[16331-2-2-7.36][16381-0-3-2.00][16488-1-1-6.56][16495-0-0-4.64][16650-0-0-5.07][16719-1-4-2.07][16801-0-0-6.26][16828-0-0-3.86][17137-3-3-0.54][17245-1-3-0.82]
[17278-3-3-0.38][17282-0-0-2.61][17311-2-2-1.85][17336-2-1-0.92][17608-3-3-4.70][17627-0-0-1.97][17877-3-4-2.98][17924-1-4-0.44][17984-3-0-3.92][18211-0-3-1.07]
[18276-3-3-3.40][18287-1-1-3.82][18394-0-0-4.71][18428-0-0-6.72][18442-0-3-5.01][18478-3-3-2.69][18607-0-0-3.44][18616-0-0-1.70][18663-0-0-6.90][18718-0-0-5.40]
[18766-2-2-1.75][18824-2-2-2.22][18890-3-3-3.92][18930-3-4-1.83][18938-3-3-2.61][19817-1-2-2.26][19839-0-0-1.28][19930-3-3-4.07][19944-0-4-0.61][20036-2-2-5.79]
[20101-3-3-2.64][20474-1-1-3.22][20547-3-3-0.74][20929-2-2-2.72][21245-1-2-3.57][21257-3-3-1.37][21293-1-1-2.38][21316-1-1-4.62][21384-1-1-3.69][21448-1-1-1.54]
[21483-0-0-7.54][21487-2-2-2.92][21714-0-3-1.82][21943-3-3-2.68][21947-0-0-3.69][21948-0-0-6.90][21965-2-2-4.79][21998-1-1-4.92][22025-0-4-2.20][22228-3-3-5.98]
[22446-1-1-2.84][22494-3-0-2.84][22757-0-0-4.09][22811-3-3-6.85][22976-3-4-2.05][22985-3-3-4.31][23014-0-0-4.49][23112-1-1-3.47][23144-3-3-5.55][23168-2-3-0.95]
[23219-0-0-1.61][23363-3-3-3.45][23470-0-0-2.77][23486-2-2-2.11][23497-0-3-4.52][23516-0-0-3.81][23690-1-4-1.59][23921-2-1-0.21][23936-1-3-1.62][24040-3-0-0.85]
[24111-1-4-1.66][24182-0-0-6.00][24238-3-3-4.57][24290-2-0-5.64][24345-0-0-1.22][24364-1-2-1.39][24427-3-0-3.36][24477-2-2-1.52][24495-2-4-0.99][24893-2-2-2.89]
[25012-1-2-1.13][25121-2-2-4.38][25165-3-3-3.26][25183-0-0-3.55][25297-3-3-5.03][25398-0-0-5.07][25574-2-2-2.74][25644-1-1-3.13][25718-1-3-1.67][25774-2-3-0.45]
[26032-3-3-2.94][26051-3-3-6.25][26120-0-0-0.77][26321-1-1-6.27][26732-1-1-1.40][26784-3-3-6.87][26827-3-3-2.35][26833-0-3-4.14][26838-2-3-0.81][26860-1-0-0.89]
[26948-0-0-2.30][27049-3-0-4.38][27098-1-0-1.47][27526-0-0-3.93][27639-3-3-1.13][27698-3-3-2.83][27772-0-0-4.86][27890-1-1-5.64][28040-0-0-2.15][28503-2-2-4.09]
[28577-1-1-4.05][28959-0-0-5.20][29198-3-3-3.99][29777-0-0-7.35][29877-2-1-0.37][30035-1-1-6.75][30098-0-0-3.36][30326-1-1-7.71][30572-2-2-2.69][30716-0-4-1.44]
[30806-2-2-2.07][30906-1-1-3.38][31007-0-0-3.68][31181-3-3-2.98][31238-0-0-4.52][31347-0-0-6.10][31422-2-0-0.52][31429-3-3-3.05][31431-0-0-1.44][31432-1-1-4.38]
[31477-0-3-3.40][31524-1-4-0.38][31597-1-1-3.76][31619-1-2-0.71][31701-0-0-5.54][31755-0-0-6.28][31854-3-3-3.99][32074-1-1-1.15][32078-3-3-5.17][32111-1-1-1.72]
[32127-1-1-3.49][32140-3-3-5.55][32263-2-0-0.70][32365-0-0-4.42][32411-2-3-3.52][32429-3-0-3.27][32473-3-0-2.81][32574-3-0-3.63][32584-0-4-2.96][32622-0-3-0.16]
[32858-3-3-2.45][32969-3-3-5.94][33016-2-2-5.63][33031-1-3-2.79][33035-2-2-2.16][33133-2-2-1.42][33173-2-3-0.55][33175-3-4-2.00][33306-3-3-1.64][33309-2-4-0.46]
[33474-0-3-1.65][33478-2-3-1.58][33618-1-2-0.41][33712-0-3-1.97][33782-2-4-2.60][33914-3-3-7.35][34076-3-4-1.44][34112-2-3-0.39][34138-2-3-1.80][34239-1-1-2.53]
[34364-2-2-3.06][34617-1-2-2.24][34751-3-3-4.59][34783-2-2-3.01][35015-3-4-0.97][35018-1-1-3.66][35288-2-3-0.44][0-4-4-1.45][1-4-4-3.01][2-4-0-1.78]
[3-4-4-3.09][4-4-4-1.72][5-4-3-0.34][6-4-4-4.11][7-4-4-2.64][8-4-0-0.76][9-4-4-2.06][10-4-4-4.88][11-4-2-3.66][12-4-4-0.61]
[14-4-4-0.39][15-4-3-3.57][16-4-4-1.25][17-4-4-1.39][18-4-4-3.06][19-4-0-4.11][20-4-0-1.89][21-4-4-1.27][22-4-4-2.09][23-4-0-1.65]
[24-4-4-6.82][25-4-4-1.00][26-4-4-1.27][27-4-2-1.22][28-4-4-3.81][29-4-1-1.66][30-4-4-2.25][31-4-4-1.19][32-4-4-3.10][33-4-4-1.43]
[34-4-4-0.51][35-4-4-2.65][37-4-4-2.39][39-4-0-3.12][40-4-0-0.74][41-4-4-0.99][42-4-2-1.35][43-4-1-0.79][45-4-4-1.15][46-4-4-5.15]
[47-4-4-5.79][48-4-4-2.83][51-4-4-3.78][52-4-4-2.77][53-4-4-1.04][54-4-4-1.96][55-4-2-2.70][56-4-1-1.57][57-4-3-1.55][58-4-4-1.89]
[59-4-0-2.96][60-4-4-3.01][61-4-4-3.00][62-4-4-1.60][63-4-4-4.99][64-4-2-2.72][65-4-4-5.66][66-4-4-3.77][67-4-0-1.72][68-4-3-1.89]
[69-4-3-0.87][70-4-4-3.93][72-4-4-2.30][73-4-1-2.49][74-4-4-1.68][75-4-4-0.91][77-4-4-3.67][78-4-0--0.14][79-4-2-2.67][80-4-4-4.43]
[81-4-4-3.65][82-4-4-1.57][83-4-4-1.42][84-4-4-3.61][85-4-4-5.16][86-4-4-0.63][87-4-4-3.31][88-4-4-3.41][89-4-3-2.71][90-4-4-1.17]
[91-4-4-1.62][92-4-4-0.45][93-4-0-1.54][94-4-4-2.26][95-4-4-2.00][96-4-4-2.60][97-4-4-3.69][98-4-4-1.10][99-4-4-0.96][100-4-2-3.33]
[101-4-4-5.35][102-4-4-1.58][103-4-0-1.25][104-4-4-1.25][105-4-2-1.66][106-4-4-2.26][107-4-0-1.65][108-4-1-0.19][109-4-4-3.53][110-4-4-2.39]
[111-4-0-4.36][112-4-4-1.01][113-4-4-1.21][114-4-3-0.46][115-4-4-1.13][116-4-0-0.77][117-4-4-4.40][119-4-4-1.26][121-4-4-2.79][122-4-4-2.83]
[124-4-4-1.03][125-4-4-2.65][126-4-4-6.01][127-4-4-1.15][128-4-0-0.20][129-4-4-1.51][130-4-4-1.95][131-4-4-0.55][132-4-0-1.87][133-4-4-4.12]
[135-4-4-1.82][136-4-0-0.45][137-4-4-0.50][138-4-4-1.18][139-4-4-2.87][140-4-4-0.54][141-4-2-0.79][142-4-4-3.92][143-4-4-2.68][144-4-4-2.71]
[145-4-4-3.02][148-4-0-3.57][149-4-0-1.22][150-4-4-4.32][151-4-4-2.63][152-4-4-2.61][153-4-4-4.51][154-4-4-6.25][155-4-4-3.55][156-4-3-0.91]
[157-4-0-1.13][158-4-4-1.84][160-4-1--0.08][161-4-2-2.65][162-4-2-0.85][164-4-4-2.36][165-4-4-1.24][167-4-4-2.40][168-4-4-1.76][170-4-4-1.15]
[171-4-4-2.08][172-4-4-4.94][173-4-4-4.64][174-4-3-1.59][175-4-4-2.54][177-4-0-2.13][178-4-4-2.05][179-4-4-1.94][180-4-4-3.29][181-4-3-2.33]
[182-4-3-3.47][183-4-4-3.66][184-4-2-2.31][186-4-0-0.18][187-4-4-0.24][188-4-4-2.16][189-4-2-0.67][190-4-3-0.26][191-4-4-3.24][192-4-2-0.54]
[193-4-1-2.09][194-4-3-0.41][195-4-4-0.79][196-4-2-3.19][197-4-1-1.41][198-4-4-5.80][199-4-4-1.10]
---------------------------
I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.152 | Acc: 88.875% | Wgt Acc: 96.357%
	I - Batch: 100 | Loss: 0.153 | Acc: 88.438% | Wgt Acc: 96.287%
	I - Batch: 150 | Loss: 0.158 | Acc: 88.583% | Wgt Acc: 96.120%
	I - Batch: 200 | Loss: 0.155 | Acc: 89.188% | Wgt Acc: 96.288%
	I - Batch: 250 | Loss: 0.151 | Acc: 89.300% | Wgt Acc: 96.399%
I - num batch: 273
I - Train -- Loss: 0.151 | Acc: 89.202% | Wgt Acc: 96.360% | LR: 1.250000e-04 | Dur: 167.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 782.    1.    1.    3.  112.]
 [   1.  752.    1.    0.   64.]
 [   4.    1. 1024.    1.  147.]
 [   2.    2.    0.  765.  109.]
 [   8.    4.    6.    4.  568.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 62.722% | Wgt Acc: 61.100% | Dur: 14.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   2.  12.   5.]
 [  1.  36.   4.   2.  10.]
 [  4.  28.  51.   5.  38.]
 [ 14.   4.   7.  54.   7.]
 [ 12.   8.  11.  13. 120.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.149 | Acc: 89.250% | Wgt Acc: 96.330%
	I - Batch: 100 | Loss: 0.150 | Acc: 89.562% | Wgt Acc: 96.441%
	I - Batch: 150 | Loss: 0.149 | Acc: 89.417% | Wgt Acc: 96.231%
	I - Batch: 200 | Loss: 0.152 | Acc: 89.188% | Wgt Acc: 96.093%
	I - Batch: 250 | Loss: 0.148 | Acc: 89.575% | Wgt Acc: 96.317%
I - num batch: 273
I - Train -- Loss: 0.149 | Acc: 89.546% | Wgt Acc: 96.355% | LR: 1.250000e-04 | Dur: 164.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 785.    0.    0.    5.  104.]
 [   0.  749.    1.    1.   60.]
 [   3.    3. 1022.    0.  156.]
 [   2.    0.    1.  763.   93.]
 [   7.    8.    8.    4.  587.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.141 | Acc: 62.919% | Wgt Acc: 65.333% | Dur: 13.77s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   4.  17.  26.]
 [  0.  42.   2.   2.   7.]
 [  1.  16.  47.   3.  33.]
 [ 14.   7.  12.  61.  12.]
 [  6.  10.  10.   3. 102.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773.  781.]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.132 | Acc: 90.500% | Wgt Acc: 96.392%
	I - Batch: 100 | Loss: 0.139 | Acc: 90.062% | Wgt Acc: 96.507%
	I - Batch: 150 | Loss: 0.138 | Acc: 90.917% | Wgt Acc: 96.955%
	I - Batch: 200 | Loss: 0.137 | Acc: 91.031% | Wgt Acc: 96.909%
	I - Batch: 250 | Loss: 0.140 | Acc: 90.850% | Wgt Acc: 96.869%
I - num batch: 259
I - Train -- Loss: 0.140 | Acc: 90.755% | Wgt Acc: 96.811% | LR: 1.250000e-04 | Dur: 155.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 782.    3.    1.    4.  102.]
 [   2.  751.    0.    2.   44.]
 [   4.    0. 1026.    3.  119.]
 [   2.    3.    1.  763.   78.]
 [   7.    3.    4.    1.  438.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.119 | Acc: 61.736% | Wgt Acc: 63.983% | Dur: 14.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   5.  15.  25.]
 [  1.  41.   3.   2.   8.]
 [  0.  20.  45.   3.  32.]
 [ 18.   6.  12.  63.  14.]
 [  6.   9.  10.   3. 101.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.157 | Acc: 88.875% | Wgt Acc: 95.923%
	I - Batch: 100 | Loss: 0.152 | Acc: 89.500% | Wgt Acc: 96.207%
	I - Batch: 150 | Loss: 0.155 | Acc: 89.000% | Wgt Acc: 95.964%
	I - Batch: 200 | Loss: 0.156 | Acc: 89.094% | Wgt Acc: 96.088%
	I - Batch: 250 | Loss: 0.156 | Acc: 88.750% | Wgt Acc: 96.065%
I - num batch: 273
I - Train -- Loss: 0.156 | Acc: 88.767% | Wgt Acc: 96.023% | LR: 1.250000e-04 | Dur: 167.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 780.    1.    1.    5.  119.]
 [   0.  752.    3.    0.   61.]
 [   4.    1. 1021.    0.  154.]
 [   1.    3.    0.  760.  107.]
 [  12.    3.    7.    8.  559.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.107 | Acc: 65.483% | Wgt Acc: 63.614% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   1.   4.  13.  15.]
 [  0.  42.   4.   2.   6.]
 [  2.  14.  42.   1.  20.]
 [ 17.   7.  10.  62.  12.]
 [ 10.  14.  15.   8. 127.]]

I - Local maximum validation set accuracy:  65.48

I - Validation set results: 
[14-1-2-1.43][50-3-1-1.43][124-2-2-2.23][127-0-0-6.59][443-2-2-3.45][567-0-0-2.35][573-1-1-3.67][615-0-3-1.23][695-1-2-1.79][722-3-3-2.67]
[826-0-0-4.82][878-0-0-2.98][1103-0-4-2.08][1212-3-3--0.52][1368-0-0-5.84][2181-2-3-2.18][2476-2-2-1.63][2721-2-2-4.78][2818-1-1-0.78][2886-2-4-2.12]
[3231-2-2-4.87][3333-2-3-1.77][3482-2-2-2.75][3536-3-4-0.88][3625-1-1-3.68][3909-0-0-3.93][4035-0-0-5.01][4140-0-0-2.77][4214-1-3-3.53][4346-1-4-1.23]
[4581-2-2-4.07][4708-3-3-4.07][4838-3-3-0.83][4845-1-2-1.21][4868-0-0-3.98][4939-0-4-1.26][4984-2-3-2.59][5078-1-2-2.33][5396-0-0-7.17][5479-1-1-3.22]
[5717-0-0-1.95][5843-1-1-0.90][5949-3-0-1.27][5987-2-4-3.76][6014-3-3-4.46][6033-3-3-2.75][6313-0-0-2.25][6421-3-3-3.20][6500-1-1-1.55][6583-3-3-1.55]
[6683-3-3-2.09][6825-2-1-2.26][6998-3-3-1.10][7049-3-3-2.61][7517-1-1-4.24][7521-1-1--0.43][7528-1-3-2.98][7949-1-2-5.36][8135-1-4-1.40][8185-3-0-4.98]
[8269-3-1-4.24][8273-3-3-2.67][8543-3-0-6.09][8666-1-1-3.50][8672-0-0-5.59][8903-1-1-3.29][9001-2-4-2.38][9036-2-2-4.10][9281-3-3-1.45][9300-2-2-8.47]
[9571-0-4-0.59][9617-1-1-2.50][9644-2-2-5.82][9705-2-0-2.68][9801-0-0-2.09][9803-3-3-1.30][9865-3-3-5.59][9896-2-4-1.66][10314-1-4-2.75][10337-3-3-4.74]
[10403-0-4-1.39][10653-2-4-0.96][10704-2-2-1.30][10719-1-2-1.58][10727-1-4-0.80][10836-0-0-7.95][10969-2-3-2.36][11042-0-4-1.17][11088-1-1-5.24][11322-0-0-5.22]
[11398-2-2-1.96][11499-0-0-3.41][11502-3-3-1.59][11512-3-3-2.05][11608-1-2-2.79][11610-0-0-2.58][11692-0-3-2.35][11905-0-0-3.03][11993-1-1-4.39][12002-2-0-0.35]
[12052-0-0-4.66][12201-0-3-4.84][12235-2-4-2.15][12320-1-4-5.00][12377-2-4-3.69][12398-2-4-0.65][12503-1-1-0.58][12617-0-3-1.90][12685-3-3-1.74][12738-2-3-0.41]
[12742-2-2-6.93][12823-0-0-5.34][13110-1-2-4.31][13240-3-0-2.57][13253-1-1-1.74][13273-0-0-7.71][13634-1-1-2.73][13763-2-3-0.60][13905-3-3-0.75][14060-2-4-2.82]
[14065-3-0-1.57][14147-3-3-4.47][14595-2-1-1.72][14687-2-2-5.65][14788-2-2-5.81][14869-1-1-4.10][14872-3-4-0.52][14877-1-1-3.44][14927-0-3-2.02][15066-0-0-6.96]
[15175-1-4-3.68][15178-2-2-2.30][15375-3-2--0.80][15389-3-3-4.01][15568-2-4-1.01][15675-3-3-2.51][15869-1-3-1.36][16207-3-0-0.87][16236-0-0-2.45][16302-3-3-2.77]
[16331-2-2-6.21][16381-0-3-2.19][16488-1-1-4.66][16495-0-0-3.60][16650-0-0-4.30][16719-1-4-3.03][16801-0-0-3.90][16828-0-0-3.86][17137-3-3--0.28][17245-1-4-1.42]
[17278-3-4-1.76][17282-0-0-1.31][17311-2-2-2.52][17336-2-2-1.43][17608-3-3-7.01][17627-0-0-1.25][17877-3-3-2.26][17924-1-4--0.06][17984-3-3-1.58][18211-0-3-1.45]
[18276-3-3-3.40][18287-1-1-2.75][18394-0-0-3.17][18428-0-0-7.28][18442-0-3-4.96][18478-3-3-3.39][18607-0-0-4.86][18616-0-4-2.59][18663-0-0-5.78][18718-0-0-4.77]
[18766-2-2-3.69][18824-2-2-2.05][18890-3-3-3.12][18930-3-4-1.39][18938-3-3-2.30][19817-1-2-3.89][19839-0-0-0.65][19930-3-3-4.07][19944-0-4-0.36][20036-2-2-5.11]
[20101-3-3-3.98][20474-1-1-1.70][20547-3-3-1.00][20929-2-2-3.05][21245-1-2-2.15][21257-3-4-1.19][21293-1-1-2.82][21316-1-1-6.49][21384-1-1-5.25][21448-1-1-1.06]
[21483-0-0-4.60][21487-2-2-2.31][21714-0-3-1.90][21943-3-3-2.63][21947-0-0-2.12][21948-0-0-5.91][21965-2-2-1.47][21998-1-1-4.38][22025-0-2-0.91][22228-3-3-6.23]
[22446-1-1-2.90][22494-3-0-3.86][22757-0-0-2.85][22811-3-3-7.31][22976-3-4-2.22][22985-3-3-5.00][23014-0-3-3.61][23112-1-1-4.17][23144-3-3-5.33][23168-2-3-2.84]
[23219-0-0-1.48][23363-3-3-3.41][23470-0-0-1.42][23486-2-2-2.26][23497-0-3-4.12][23516-0-0-4.22][23690-1-1-1.74][23921-2-2-0.22][23936-1-2-0.76][24040-3-0-0.43]
[24111-1-4-2.21][24182-0-0-5.58][24238-3-3-4.24][24290-2-0-2.80][24345-0-0-2.81][24364-1-3-0.99][24427-3-0-2.64][24477-2-2-4.00][24495-2-4-1.22][24893-2-2-3.58]
[25012-1-4-1.99][25121-2-2-4.07][25165-3-3-4.59][25183-0-0-3.56][25297-3-3-6.18][25398-0-0-4.14][25574-2-4-1.61][25644-1-1-0.23][25718-1-4-1.09][25774-2-2-0.20]
[26032-3-0-1.66][26051-3-3-6.99][26120-0-4-2.02][26321-1-1-5.11][26732-1-1-2.72][26784-3-3-7.98][26827-3-3-3.47][26833-0-3-1.07][26838-2-3-0.79][26860-1-1-1.80]
[26948-0-0-0.53][27049-3-0-3.30][27098-1-0-1.71][27526-0-3-2.04][27639-3-3-3.05][27698-3-3-2.13][27772-0-0-3.05][27890-1-1-3.16][28040-0-2-0.93][28503-2-2-4.27]
[28577-1-1-2.18][28959-0-0-3.73][29198-3-3-5.64][29777-0-0-6.44][29877-2-3-0.80][30035-1-1-4.77][30098-0-3-2.67][30326-1-1-7.62][30572-2-2-3.03][30716-0-4-1.42]
[30806-2-2-2.59][30906-1-1-4.43][31007-0-0-1.45][31181-3-3-2.50][31238-0-0-3.11][31347-0-0-2.94][31422-2-2-0.61][31429-3-3-3.21][31431-0-0-0.36][31432-1-1-3.51]
[31477-0-3-3.04][31524-1-4-0.38][31597-1-2-1.28][31619-1-3-2.55][31701-0-0-3.75][31755-0-0-4.30][31854-3-3-2.31][32074-1-3-1.31][32078-3-3-3.81][32111-1-1-2.95]
[32127-1-1-1.88][32140-3-3-5.87][32263-2-4-1.13][32365-0-0-3.98][32411-2-0-3.38][32429-3-3-3.90][32473-3-0-1.42][32574-3-0-3.19][32584-0-4-2.68][32622-0-3-0.60]
[32858-3-3-2.76][32969-3-3-6.52][33016-2-2-3.72][33031-1-3-4.27][33035-2-2-3.27][33133-2-2-2.73][33173-2-2-0.50][33175-3-4-1.53][33306-3-3-2.02][33309-2-4-0.70]
[33474-0-3-0.65][33478-2-3-1.53][33618-1-2-1.46][33712-0-0-1.53][33782-2-4-1.92][33914-3-3-1.16][34076-3-4-1.55][34112-2-2-3.52][34138-2-1-0.59][34239-1-1-1.18]
[34364-2-2-3.88][34617-1-2-2.50][34751-3-3-4.31][34783-2-1-1.45][35015-3-3-2.29][35018-1-4-2.86][35288-2-2-0.01][0-4-2-2.56][1-4-4-2.35][2-4-4-1.46]
[3-4-4-1.93][4-4-4-1.47][5-4-3-1.83][6-4-4-3.32][7-4-4-1.87][8-4-2-0.74][9-4-4-2.22][10-4-4-4.55][11-4-2-4.17][12-4-0--0.16]
[14-4-4-1.31][15-4-3-4.48][16-4-4-3.83][17-4-4-0.41][18-4-4-5.14][19-4-0-2.26][20-4-0-1.25][21-4-4-1.19][22-4-4-2.56][23-4-4-1.81]
[24-4-4-8.01][25-4-2-0.75][26-4-4-0.22][27-4-2-1.77][28-4-4-4.60][29-4-1-0.81][30-4-4-0.77][31-4-4-1.16][32-4-4-1.93][33-4-2-2.73]
[34-4-4-0.79][35-4-4-1.69][37-4-4-1.67][39-4-0-0.91][40-4-4-0.98][41-4-3-0.25][42-4-4-0.35][43-4-4-0.36][45-4-4-2.03][46-4-4-4.11]
[47-4-4-6.14][48-4-4-3.16][51-4-4-3.87][52-4-4-2.20][53-4-4-3.25][54-4-4-3.22][55-4-4-2.00][56-4-4-1.51][57-4-0-0.39][58-4-4-3.10]
[59-4-4-2.71][60-4-4-1.08][61-4-4-4.68][62-4-4-1.64][63-4-4-2.34][64-4-4-1.96][65-4-4-6.62][66-4-4-2.20][67-4-0-1.55][68-4-3-1.33]
[69-4-3-0.35][70-4-4-3.25][72-4-4-2.50][73-4-1-2.67][74-4-4-1.51][75-4-0-0.53][77-4-4-7.78][78-4-2-1.37][79-4-4-3.22][80-4-4-5.37]
[81-4-2-3.05][82-4-4-1.77][83-4-4-2.28][84-4-4-5.27][85-4-4-4.65][86-4-4-1.00][87-4-4-3.76][88-4-4-3.66][89-4-3-2.33][90-4-4-1.80]
[91-4-4-1.83][92-4-4-0.42][93-4-4-0.10][94-4-4-2.07][95-4-4-1.84][96-4-4-2.65][97-4-4-2.80][98-4-2-2.42][99-4-4-1.98][100-4-2-2.76]
[101-4-4-5.26][102-4-4-1.66][103-4-4--0.12][104-4-4-2.05][105-4-4-2.99][106-4-4-3.93][107-4-0-2.16][108-4-4-0.77][109-4-4-3.29][110-4-4-2.56]
[111-4-0-3.28][112-4-4-0.47][113-4-4-0.88][114-4-2-2.28][115-4-4-1.73][116-4-4-2.60][117-4-4-4.26][119-4-4-0.91][121-4-4-2.48][122-4-4-3.42]
[124-4-4-1.28][125-4-4-4.68][126-4-4-8.22][127-4-4-1.99][128-4-4-0.16][129-4-3-0.90][130-4-4-1.85][131-4-3-1.03][132-4-4-1.40][133-4-4-4.74]
[135-4-2-1.55][136-4-4-0.97][137-4-4-0.83][138-4-4-1.28][139-4-4-2.50][140-4-4-0.59][141-4-2-2.66][142-4-4-3.34][143-4-4-7.10][144-4-4-5.80]
[145-4-2-5.05][148-4-0-2.13][149-4-4-1.90][150-4-4-4.56][151-4-4-3.99][152-4-4-2.37][153-4-4-3.86][154-4-4-5.45][155-4-4-5.59][156-4-3-1.86]
[157-4-2-0.46][158-4-4-2.06][160-4-1-1.81][161-4-2-3.63][162-4-4-0.54][164-4-4-2.16][165-4-4-2.63][167-4-0-2.55][168-4-4-1.64][170-4-4-1.50]
[171-4-4-2.68][172-4-4-5.88][173-4-4-4.94][174-4-3-1.34][175-4-4-4.02][177-4-0-2.50][178-4-4-4.21][179-4-0-0.76][180-4-4-4.35][181-4-4-2.16]
[182-4-3-3.89][183-4-4-4.42][184-4-2-2.34][186-4-4-1.85][187-4-1-0.98][188-4-4-2.74][189-4-1-1.44][190-4-3-0.38][191-4-4-1.86][192-4-2-1.15]
[193-4-2-2.19][194-4-0-1.33][195-4-0-0.83][196-4-2-4.06][197-4-4-1.96][198-4-4-8.45][199-4-1-1.22]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.166 | Acc: 89.500% | Wgt Acc: 95.896%
	I - Batch: 100 | Loss: 0.162 | Acc: 88.562% | Wgt Acc: 95.845%
	I - Batch: 150 | Loss: 0.159 | Acc: 88.875% | Wgt Acc: 95.713%
	I - Batch: 200 | Loss: 0.153 | Acc: 89.875% | Wgt Acc: 96.104%
	I - Batch: 250 | Loss: 0.151 | Acc: 89.750% | Wgt Acc: 96.183%
I - num batch: 273
I - Train -- Loss: 0.152 | Acc: 89.798% | Wgt Acc: 96.227% | LR: 1.250000e-04 | Dur: 163.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 777.    3.    2.    2.  109.]
 [   1.  746.    1.    0.   50.]
 [   3.    3. 1023.    1.  141.]
 [   5.    1.    0.  764.   93.]
 [  11.    7.    6.    6.  607.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.146 | Acc: 65.483% | Wgt Acc: 66.198% | Dur: 13.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   4.  11.  18.]
 [  0.  40.   4.   2.   8.]
 [  1.  19.  46.   2.  25.]
 [ 18.   7.  11.  67.  14.]
 [  5.  10.  10.   4. 115.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.134 | Acc: 90.125% | Wgt Acc: 96.384%
	I - Batch: 100 | Loss: 0.134 | Acc: 90.500% | Wgt Acc: 96.694%
	I - Batch: 150 | Loss: 0.137 | Acc: 90.792% | Wgt Acc: 96.824%
	I - Batch: 200 | Loss: 0.141 | Acc: 90.250% | Wgt Acc: 96.621%
	I - Batch: 250 | Loss: 0.144 | Acc: 90.000% | Wgt Acc: 96.457%
I - num batch: 273
I - Train -- Loss: 0.143 | Acc: 90.028% | Wgt Acc: 96.500% | LR: 1.250000e-04 | Dur: 165.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 781.    3.    0.    3.  130.]
 [   2.  753.    0.    2.   41.]
 [   5.    1. 1025.    0.  142.]
 [   0.    1.    0.  761.   80.]
 [   9.    2.    7.    7.  607.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.192 | Acc: 62.722% | Wgt Acc: 62.104% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   1.   4.  16.  14.]
 [  0.  39.   6.   1.   8.]
 [  1.  16.  39.   1.  27.]
 [ 15.   8.  11.  60.  15.]
 [  8.  14.  15.   8. 116.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.114 | Acc: 91.500% | Wgt Acc: 97.318%
	I - Batch: 100 | Loss: 0.132 | Acc: 90.812% | Wgt Acc: 96.818%
	I - Batch: 150 | Loss: 0.133 | Acc: 90.500% | Wgt Acc: 96.784%
	I - Batch: 200 | Loss: 0.134 | Acc: 90.500% | Wgt Acc: 96.711%
	I - Batch: 250 | Loss: 0.137 | Acc: 90.225% | Wgt Acc: 96.591%
I - num batch: 273
I - Train -- Loss: 0.138 | Acc: 90.188% | Wgt Acc: 96.547% | LR: 1.250000e-04 | Dur: 164.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 778.    0.    0.    3.  115.]
 [   0.  752.    0.    1.   58.]
 [   3.    2. 1026.    0.  144.]
 [   6.    1.    1.  764.   69.]
 [  10.    5.    5.    5.  614.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.118 | Acc: 65.089% | Wgt Acc: 63.314% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   2.   9.  10.]
 [  0.  36.   4.   2.  10.]
 [  2.  16.  42.   1.  19.]
 [ 18.   9.  10.  66.  15.]
 [  8.  15.  17.   8. 126.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.136 | Acc: 91.125% | Wgt Acc: 97.319%
	I - Batch: 100 | Loss: 0.140 | Acc: 90.625% | Wgt Acc: 97.112%
	I - Batch: 150 | Loss: 0.137 | Acc: 90.792% | Wgt Acc: 97.106%
	I - Batch: 200 | Loss: 0.139 | Acc: 90.781% | Wgt Acc: 96.961%
	I - Batch: 250 | Loss: 0.139 | Acc: 90.775% | Wgt Acc: 96.986%
I - num batch: 273
I - Train -- Loss: 0.138 | Acc: 90.922% | Wgt Acc: 97.085% | LR: 1.250000e-04 | Dur: 166.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    1.    1.    1.  114.]
 [   0.  755.    0.    0.   58.]
 [   1.    0. 1027.    1.  114.]
 [   1.    0.    0.  766.   84.]
 [   7.    4.    4.    5.  630.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.247 | Acc: 59.172% | Wgt Acc: 60.904% | Dur: 14.87s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  3.  4. 16. 25.]
 [ 0. 35.  3.  0.  8.]
 [ 1. 16. 38.  3. 27.]
 [19. 12. 17. 64. 21.]
 [ 4. 12. 13.  3. 99.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.124 | Acc: 90.125% | Wgt Acc: 97.090%
	I - Batch: 100 | Loss: 0.128 | Acc: 90.750% | Wgt Acc: 97.140%
	I - Batch: 150 | Loss: 0.123 | Acc: 91.208% | Wgt Acc: 97.122%
	I - Batch: 200 | Loss: 0.122 | Acc: 91.219% | Wgt Acc: 97.161%
	I - Batch: 250 | Loss: 0.126 | Acc: 91.025% | Wgt Acc: 96.976%
I - num batch: 273
I - Train -- Loss: 0.129 | Acc: 90.853% | Wgt Acc: 96.917% | LR: 1.250000e-04 | Dur: 165.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 784.    0.    1.    3.   99.]
 [   0.  754.    2.    2.   57.]
 [   2.    4. 1025.    0.  113.]
 [   3.    0.    0.  766.   97.]
 [   8.    2.    4.    2.  634.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.134 | Acc: 63.905% | Wgt Acc: 60.800% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   4.  18.  17.]
 [  1.  32.   1.   1.   5.]
 [  1.  18.  41.   1.  22.]
 [ 13.  11.  14.  55.   7.]
 [  6.  14.  15.  11. 129.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.135 | Acc: 90.750% | Wgt Acc: 96.653%
	I - Batch: 100 | Loss: 0.134 | Acc: 91.000% | Wgt Acc: 96.764%
	I - Batch: 150 | Loss: 0.136 | Acc: 90.958% | Wgt Acc: 96.814%
	I - Batch: 200 | Loss: 0.135 | Acc: 90.656% | Wgt Acc: 96.668%
	I - Batch: 250 | Loss: 0.132 | Acc: 90.850% | Wgt Acc: 96.753%
I - num batch: 273
I - Train -- Loss: 0.133 | Acc: 90.646% | Wgt Acc: 96.747% | LR: 1.250000e-04 | Dur: 164.12s
I - Confusion Matrix: [row->prediction - col->label]
[[ 787.    3.    1.    1.   93.]
 [   0.  751.    2.    3.   53.]
 [   1.    1. 1023.    1.  137.]
 [   2.    1.    1.  763.   87.]
 [   7.    4.    5.    5.  630.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.108 | Acc: 65.089% | Wgt Acc: 64.848% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   5.   5.  15.  14.]
 [  0.  35.   3.   2.  10.]
 [  1.  17.  48.   2.  30.]
 [ 13.   9.   8.  61.   8.]
 [  6.  12.  11.   6. 118.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.121 | Acc: 91.125% | Wgt Acc: 96.809%
	I - Batch: 100 | Loss: 0.123 | Acc: 91.062% | Wgt Acc: 97.029%
	I - Batch: 150 | Loss: 0.123 | Acc: 91.417% | Wgt Acc: 97.073%
	I - Batch: 200 | Loss: 0.125 | Acc: 91.625% | Wgt Acc: 96.978%
	I - Batch: 250 | Loss: 0.125 | Acc: 91.375% | Wgt Acc: 97.013%
I - num batch: 273
I - Train -- Loss: 0.125 | Acc: 91.426% | Wgt Acc: 97.027% | LR: 1.250000e-04 | Dur: 167.59s
I - Confusion Matrix: [row->prediction - col->label]
[[ 779.    0.    0.    1.   92.]
 [   0.  756.    1.    1.   55.]
 [   0.    0. 1025.    0.  112.]
 [   4.    1.    2.  767.   80.]
 [  14.    3.    4.    4.  661.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.108 | Acc: 65.878% | Wgt Acc: 64.710% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   4.  11.  11.]
 [  1.  39.   2.   2.   9.]
 [  3.  22.  51.   4.  28.]
 [ 16.   5.   8.  61.   8.]
 [  9.  10.  10.   8. 124.]]

I - Local maximum validation set accuracy:  65.88

I - Validation set results: 
[14-1-2-1.90][50-3-1-1.05][124-2-2-1.16][127-0-0-6.85][443-2-2-3.15][567-0-0-4.35][573-1-1-3.78][615-0-3-1.03][695-1-2-1.64][722-3-3-2.17]
[826-0-0-4.31][878-0-0-4.12][1103-0-0-2.26][1212-3-4-0.20][1368-0-0-5.32][2181-2-2-2.50][2476-2-2-1.16][2721-2-2-4.40][2818-1-1-1.51][2886-2-2-1.93]
[3231-2-2-6.90][3333-2-2-1.50][3482-2-2-4.40][3536-3-4-0.64][3625-1-1-5.73][3909-0-0-3.64][4035-0-0-3.91][4140-0-0-2.87][4214-1-3-0.38][4346-1-4-0.39]
[4581-2-2-2.21][4708-3-4-2.93][4838-3-3--0.10][4845-1-2-2.29][4868-0-0-5.35][4939-0-4-0.62][4984-2-3-2.11][5078-1-2-2.87][5396-0-0-8.33][5479-1-1-4.80]
[5717-0-0-2.37][5843-1-1-0.86][5949-3-3-1.21][5987-2-4-3.21][6014-3-3-3.79][6033-3-3-2.37][6313-0-0-2.74][6421-3-3-2.87][6500-1-2-1.70][6583-3-3-0.75]
[6683-3-3-2.53][6825-2-1-4.70][6998-3-3-0.35][7049-3-3-3.21][7517-1-1-3.55][7521-1-3-0.66][7528-1-3-0.85][7949-1-2-4.88][8135-1-2-0.44][8185-3-0-6.19]
[8269-3-4-1.11][8273-3-3-3.21][8543-3-0-5.72][8666-1-1-4.28][8672-0-0-7.89][8903-1-2-1.38][9001-2-4-1.10][9036-2-2-6.52][9281-3-3-2.53][9300-2-2-8.34]
[9571-0-4-1.10][9617-1-1-0.89][9644-2-2-5.00][9705-2-0-2.03][9801-0-0-2.37][9803-3-3-1.00][9865-3-3-4.04][9896-2-2-3.35][10314-1-1-0.93][10337-3-3-4.82]
[10403-0-4-1.37][10653-2-1-1.10][10704-2-2-6.04][10719-1-1-1.79][10727-1-4-0.82][10836-0-0-9.05][10969-2-3-1.42][11042-0-2-1.54][11088-1-1-6.54][11322-0-0-5.92]
[11398-2-2-2.21][11499-0-0-2.60][11502-3-3-0.87][11512-3-3-0.88][11608-1-2-6.36][11610-0-3-0.18][11692-0-0-2.71][11905-0-0-4.27][11993-1-1-4.14][12002-2-0-2.18]
[12052-0-0-3.45][12201-0-3-4.17][12235-2-2-4.71][12320-1-4-3.05][12377-2-4-2.42][12398-2-4-0.87][12503-1-2-0.34][12617-0-4--0.10][12685-3-4-1.95][12738-2-3-0.64]
[12742-2-2-8.03][12823-0-0-5.61][13110-1-2-2.23][13240-3-3-2.01][13253-1-1-3.03][13273-0-0-8.25][13634-1-1-4.23][13763-2-2-1.61][13905-3-3-2.13][14060-2-4-2.97]
[14065-3-3-1.84][14147-3-3-4.77][14595-2-2-1.75][14687-2-2-5.94][14788-2-2-5.77][14869-1-1-4.14][14872-3-0-0.89][14877-1-1-2.87][14927-0-3-2.95][15066-0-0-6.87]
[15175-1-4-3.28][15178-2-2-2.48][15375-3-0-2.52][15389-3-3-3.91][15568-2-4-2.04][15675-3-3-3.27][15869-1-0-0.05][16207-3-0-1.72][16236-0-0-1.34][16302-3-3-2.24]
[16331-2-2-6.50][16381-0-3-2.32][16488-1-1-5.05][16495-0-0-4.19][16650-0-0-4.48][16719-1-4-1.71][16801-0-0-7.61][16828-0-0-3.02][17137-3-3-2.80][17245-1-4-2.01]
[17278-3-4-0.08][17282-0-2-0.89][17311-2-2-3.02][17336-2-2-2.69][17608-3-3-8.28][17627-0-0-1.24][17877-3-4-3.69][17924-1-2-0.13][17984-3-3-2.81][18211-0-0-0.84]
[18276-3-3-2.79][18287-1-1-4.10][18394-0-0-2.96][18428-0-0-4.21][18442-0-3-3.59][18478-3-3-2.91][18607-0-0-2.76][18616-0-0-1.44][18663-0-0-6.02][18718-0-0-4.32]
[18766-2-2-5.04][18824-2-2-4.45][18890-3-3-5.75][18930-3-4-2.12][18938-3-3-2.33][19817-1-2-4.25][19839-0-3--0.24][19930-3-3-3.60][19944-0-3-1.17][20036-2-2-6.99]
[20101-3-3-4.67][20474-1-1-2.02][20547-3-3-0.85][20929-2-2-6.22][21245-1-1-1.59][21257-3-2-1.03][21293-1-2-4.07][21316-1-1-2.46][21384-1-1-4.01][21448-1-1-1.62]
[21483-0-0-5.53][21487-2-2-3.99][21714-0-0-1.36][21943-3-3-0.88][21947-0-0-3.29][21948-0-0-8.97][21965-2-2-4.97][21998-1-1-6.95][22025-0-4-3.01][22228-3-3-6.67]
[22446-1-1-2.27][22494-3-0-1.69][22757-0-0-3.37][22811-3-3-8.48][22976-3-2-2.79][22985-3-3-4.89][23014-0-3-4.18][23112-1-1-4.31][23144-3-3-5.46][23168-2-3-1.82]
[23219-0-0-2.83][23363-3-3-5.31][23470-0-0-2.10][23486-2-2-3.18][23497-0-3-5.64][23516-0-0-6.05][23690-1-4-1.84][23921-2-2-3.45][23936-1-2-0.89][24040-3-0-1.06]
[24111-1-4-0.91][24182-0-0-7.93][24238-3-3-5.12][24290-2-0-6.13][24345-0-0-1.40][24364-1-2-2.25][24427-3-0-2.55][24477-2-4-0.82][24495-2-4-2.04][24893-2-2-3.25]
[25012-1-2-1.41][25121-2-2-5.97][25165-3-3-4.04][25183-0-0-4.11][25297-3-3-3.89][25398-0-0-4.33][25574-2-2-4.00][25644-1-2-1.26][25718-1-4-0.70][25774-2-2-3.07]
[26032-3-3-2.58][26051-3-3-6.22][26120-0-4-0.82][26321-1-1-5.95][26732-1-1-2.65][26784-3-3-9.09][26827-3-2-0.81][26833-0-3-4.13][26838-2-3-0.71][26860-1-1--0.04]
[26948-0-0-3.01][27049-3-0-4.45][27098-1-0-1.33][27526-0-0-2.03][27639-3-3-1.20][27698-3-3-4.06][27772-0-0-5.13][27890-1-1-5.11][28040-0-4-1.34][28503-2-2-3.39]
[28577-1-2-2.04][28959-0-0-5.95][29198-3-3-4.56][29777-0-0-8.16][29877-2-3-0.48][30035-1-1-6.01][30098-0-3-2.85][30326-1-1-8.33][30572-2-2-4.15][30716-0-4-1.55]
[30806-2-2-4.19][30906-1-1-3.93][31007-0-1-0.68][31181-3-3-1.81][31238-0-0-4.40][31347-0-0-3.47][31422-2-2-2.31][31429-3-3-3.72][31431-0-3-1.10][31432-1-1-3.17]
[31477-0-3-2.64][31524-1-4-0.57][31597-1-2-1.96][31619-1-3-1.41][31701-0-0-5.69][31755-0-0-6.00][31854-3-3-2.65][32074-1-1-2.15][32078-3-3-4.14][32111-1-1-3.27]
[32127-1-1-2.37][32140-3-3-3.89][32263-2-0-1.08][32365-0-0-3.84][32411-2-3-3.74][32429-3-3-3.88][32473-3-0-2.42][32574-3-0-3.28][32584-0-4-4.14][32622-0-2-0.41]
[32858-3-3-3.27][32969-3-3-6.41][33016-2-2-5.16][33031-1-3-4.84][33035-2-2-3.32][33133-2-2-4.29][33173-2-2-1.44][33175-3-1-1.91][33306-3-3-1.74][33309-2-2-0.65]
[33474-0-3-0.68][33478-2-3-1.46][33618-1-2-1.80][33712-0-3-2.35][33782-2-4-2.39][33914-3-3-5.66][34076-3-2-2.82][34112-2-2-1.79][34138-2-2-1.48][34239-1-1-1.82]
[34364-2-2-4.39][34617-1-2-3.88][34751-3-3-4.83][34783-2-4-2.44][35015-3-3-2.12][35018-1-1-3.87][35288-2-2--0.17][0-4-4-3.43][1-4-4-3.69][2-4-0-1.38]
[3-4-4-4.03][4-4-1-0.83][5-4-1-2.54][6-4-4-6.63][7-4-4-1.83][8-4-4-0.25][9-4-2-3.01][10-4-4-4.15][11-4-2-4.63][12-4-4-0.59]
[14-4-4-0.09][15-4-3-2.41][16-4-4-2.33][17-4-4-0.78][18-4-4-4.35][19-4-0-2.87][20-4-0-0.67][21-4-2-1.19][22-4-4-2.02][23-4-4-1.11]
[24-4-4-8.31][25-4-3-2.12][26-4-3-0.76][27-4-4-1.64][28-4-4-5.19][29-4-1-2.34][30-4-3-0.42][31-4-4-2.69][32-4-4-2.51][33-4-4-2.61]
[34-4-4-1.18][35-4-4-2.47][37-4-4-1.89][39-4-0-3.28][40-4-4-0.78][41-4-4-2.18][42-4-4-0.49][43-4-4-1.89][45-4-2-2.16][46-4-4-6.16]
[47-4-4-6.03][48-4-4-2.86][51-4-4-4.06][52-4-4-2.96][53-4-4-2.75][54-4-3-1.95][55-4-2-2.24][56-4-4-1.07][57-4-0-0.28][58-4-2-3.62]
[59-4-4-3.19][60-4-4-3.20][61-4-4-2.31][62-4-4-1.52][63-4-2-2.95][64-4-2-2.93][65-4-4-4.61][66-4-4-1.98][67-4-0-1.42][68-4-2-2.79]
[69-4-4-0.75][70-4-4-3.35][72-4-4-1.96][73-4-1-3.87][74-4-4-2.26][75-4-4-1.50][77-4-4-6.46][78-4-2-0.69][79-4-2-6.16][80-4-4-4.18]
[81-4-1-2.81][82-4-4-1.69][83-4-4-1.65][84-4-4-3.55][85-4-4-5.86][86-4-4-0.70][87-4-4-3.20][88-4-4-1.99][89-4-4-1.24][90-4-4-1.20]
[91-4-4-1.48][92-4-4-2.00][93-4-4-1.49][94-4-4-1.58][95-4-4-1.74][96-4-4-2.40][97-4-4-4.83][98-4-2-1.81][99-4-4-0.37][100-4-2-4.44]
[101-4-4-6.76][102-4-4-1.55][103-4-2-2.02][104-4-4-1.54][105-4-2-1.13][106-4-4-2.48][107-4-4-1.18][108-4-4-0.57][109-4-4-4.00][110-4-4-3.13]
[111-4-0-5.37][112-4-4--0.14][113-4-4-1.66][114-4-2-1.51][115-4-1-0.93][116-4-4-0.36][117-4-4-3.34][119-4-2-2.52][121-4-4-2.77][122-4-4-4.09]
[124-4-4-1.99][125-4-4-3.28][126-4-4-7.56][127-4-4-0.85][128-4-4-0.43][129-4-4-2.14][130-4-4-0.92][131-4-2-0.71][132-4-4-1.63][133-4-4-5.48]
[135-4-2-2.14][136-4-4-0.19][137-4-4-1.93][138-4-4-1.41][139-4-4-3.32][140-4-4-0.43][141-4-0-2.10][142-4-4-3.87][143-4-4-4.53][144-4-4-1.32]
[145-4-2-5.57][148-4-0-3.35][149-4-4-1.37][150-4-4-4.63][151-4-4-2.19][152-4-4-3.62][153-4-4-3.68][154-4-4-8.52][155-4-4-4.13][156-4-3-1.31]
[157-4-0-5.36][158-4-2-2.00][160-4-1-0.09][161-4-2-3.11][162-4-2-1.98][164-4-4-2.02][165-4-4-0.95][167-4-4-2.75][168-4-4-1.11][170-4-4-1.66]
[171-4-4-1.53][172-4-4-5.40][173-4-4-6.13][174-4-0-1.37][175-4-4-2.25][177-4-4-3.86][178-4-4-1.41][179-4-4-1.66][180-4-4-2.70][181-4-4-3.03]
[182-4-3-3.33][183-4-4-4.41][184-4-2-2.15][186-4-4-1.92][187-4-1-0.74][188-4-4-1.24][189-4-2-0.94][190-4-3-1.36][191-4-4-2.13][192-4-4-0.63]
[193-4-2-4.29][194-4-2-2.85][195-4-4-1.75][196-4-2-4.34][197-4-4-2.38][198-4-4-6.46][199-4-1-0.66]
---------------------------
I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.119 | Acc: 92.375% | Wgt Acc: 97.595%
	I - Batch: 100 | Loss: 0.121 | Acc: 91.875% | Wgt Acc: 97.295%
	I - Batch: 150 | Loss: 0.124 | Acc: 91.750% | Wgt Acc: 97.303%
	I - Batch: 200 | Loss: 0.129 | Acc: 91.406% | Wgt Acc: 97.119%
	I - Batch: 250 | Loss: 0.130 | Acc: 91.125% | Wgt Acc: 97.006%
I - num batch: 273
I - Train -- Loss: 0.129 | Acc: 91.311% | Wgt Acc: 97.077% | LR: 1.250000e-04 | Dur: 164.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 785.    0.    1.    1.  108.]
 [   0.  755.    1.    1.   52.]
 [   1.    3. 1026.    1.  106.]
 [   1.    1.    1.  765.   82.]
 [  10.    1.    3.    5.  652.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.206 | Acc: 64.300% | Wgt Acc: 63.499% | Dur: 14.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   3.  12.  19.]
 [  0.  38.   7.   2.  10.]
 [  1.  10.  39.   3.  17.]
 [ 17.  12.  13.  65.  14.]
 [  6.  14.  13.   4. 120.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.141 | Acc: 89.750% | Wgt Acc: 96.035%
	I - Batch: 100 | Loss: 0.135 | Acc: 90.250% | Wgt Acc: 96.546%
	I - Batch: 150 | Loss: 0.136 | Acc: 90.500% | Wgt Acc: 96.706%
	I - Batch: 200 | Loss: 0.135 | Acc: 90.500% | Wgt Acc: 96.817%
	I - Batch: 250 | Loss: 0.129 | Acc: 90.950% | Wgt Acc: 96.996%
I - num batch: 273
I - Train -- Loss: 0.129 | Acc: 90.990% | Wgt Acc: 96.976% | LR: 1.250000e-04 | Dur: 163.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 783.    0.    1.    1.  105.]
 [   0.  756.    0.    0.   43.]
 [   2.    1. 1025.    1.  123.]
 [   3.    1.    0.  766.   90.]
 [   9.    2.    6.    5.  639.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.231 | Acc: 62.130% | Wgt Acc: 63.303% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   0.   7.  12.  15.]
 [  0.  42.   4.   1.  11.]
 [  0.  19.  42.   3.  31.]
 [ 20.  11.  15.  64.  16.]
 [  8.   6.   7.   6. 107.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.124 | Acc: 90.375% | Wgt Acc: 96.041%
	I - Batch: 100 | Loss: 0.119 | Acc: 91.062% | Wgt Acc: 96.729%
	I - Batch: 150 | Loss: 0.121 | Acc: 90.875% | Wgt Acc: 96.837%
	I - Batch: 200 | Loss: 0.121 | Acc: 91.125% | Wgt Acc: 96.898%
	I - Batch: 250 | Loss: 0.124 | Acc: 91.025% | Wgt Acc: 96.749%
I - num batch: 273
I - Train -- Loss: 0.125 | Acc: 91.174% | Wgt Acc: 96.825% | LR: 1.250000e-04 | Dur: 163.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 776.    0.    0.    1.  100.]
 [   0.  752.    0.    1.   48.]
 [   2.    1. 1028.    1.  118.]
 [   5.    1.    1.  765.   78.]
 [  14.    6.    3.    5.  656.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.170 | Acc: 63.511% | Wgt Acc: 63.845% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   5.   6.  20.  25.]
 [  0.  36.   2.   3.   8.]
 [  0.  18.  48.   3.  24.]
 [ 12.   7.   9.  55.  11.]
 [  5.  12.  10.   5. 112.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.118 | Acc: 92.000% | Wgt Acc: 97.735%
	I - Batch: 100 | Loss: 0.127 | Acc: 91.438% | Wgt Acc: 97.264%
	I - Batch: 150 | Loss: 0.125 | Acc: 91.708% | Wgt Acc: 97.426%
	I - Batch: 200 | Loss: 0.123 | Acc: 91.812% | Wgt Acc: 97.452%
	I - Batch: 250 | Loss: 0.122 | Acc: 91.725% | Wgt Acc: 97.368%
I - num batch: 273
I - Train -- Loss: 0.121 | Acc: 91.747% | Wgt Acc: 97.330% | LR: 1.250000e-04 | Dur: 164.83s
I - Confusion Matrix: [row->prediction - col->label]
[[ 784.    1.    0.    2.  102.]
 [   1.  756.    1.    0.   60.]
 [   1.    0. 1027.    0.   98.]
 [   4.    0.    0.  770.   75.]
 [   7.    3.    4.    1.  665.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.158 | Acc: 65.286% | Wgt Acc: 61.619% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   1.   6.  20.  19.]
 [  0.  34.   1.   0.   1.]
 [  0.  18.  40.   1.  17.]
 [ 13.   6.   9.  54.   9.]
 [  6.  19.  19.  11. 134.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.115 | Acc: 92.750% | Wgt Acc: 97.302%
	I - Batch: 100 | Loss: 0.112 | Acc: 92.000% | Wgt Acc: 97.185%
	I - Batch: 150 | Loss: 0.114 | Acc: 91.875% | Wgt Acc: 97.158%
	I - Batch: 200 | Loss: 0.117 | Acc: 91.500% | Wgt Acc: 97.008%
	I - Batch: 250 | Loss: 0.115 | Acc: 91.850% | Wgt Acc: 97.166%
I - num batch: 273
I - Train -- Loss: 0.117 | Acc: 91.862% | Wgt Acc: 97.162% | LR: 1.250000e-04 | Dur: 166.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 780.    0.    0.    2.  103.]
 [   0.  755.    1.    0.   55.]
 [   1.    1. 1025.    0.  107.]
 [   6.    0.    1.  768.   56.]
 [  10.    4.    5.    3.  679.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.155 | Acc: 64.497% | Wgt Acc: 64.306% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   1.   5.  14.  19.]
 [  0.  35.   4.   0.   5.]
 [  0.  24.  45.   1.  27.]
 [ 14.   7.   9.  63.  12.]
 [  7.  11.  12.   8. 117.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.114 | Acc: 92.375% | Wgt Acc: 97.414%
	I - Batch: 100 | Loss: 0.116 | Acc: 91.875% | Wgt Acc: 97.252%
	I - Batch: 150 | Loss: 0.113 | Acc: 92.042% | Wgt Acc: 97.413%
	I - Batch: 200 | Loss: 0.117 | Acc: 91.625% | Wgt Acc: 97.336%
	I - Batch: 250 | Loss: 0.118 | Acc: 91.500% | Wgt Acc: 97.231%
I - num batch: 273
I - Train -- Loss: 0.118 | Acc: 91.541% | Wgt Acc: 97.229% | LR: 1.250000e-04 | Dur: 164.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 784.    0.    0.    2.   89.]
 [   3.  756.    0.    1.   57.]
 [   3.    0. 1027.    1.  107.]
 [   1.    2.    1.  768.   89.]
 [   6.    2.    4.    1.  658.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.222 | Acc: 62.327% | Wgt Acc: 61.861% | Dur: 14.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   2.   2.  10.  15.]
 [  1.  34.   5.   3.   5.]
 [  3.  21.  43.   3.  29.]
 [ 17.   6.  10.  66.  16.]
 [  9.  15.  15.   4. 115.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.100 | Acc: 92.250% | Wgt Acc: 97.874%
	I - Batch: 100 | Loss: 0.102 | Acc: 91.938% | Wgt Acc: 97.679%
	I - Batch: 150 | Loss: 0.104 | Acc: 92.042% | Wgt Acc: 97.700%
	I - Batch: 200 | Loss: 0.105 | Acc: 92.094% | Wgt Acc: 97.624%
	I - Batch: 250 | Loss: 0.106 | Acc: 92.175% | Wgt Acc: 97.643%
I - num batch: 273
I - Train -- Loss: 0.107 | Acc: 92.068% | Wgt Acc: 97.624% | LR: 1.250000e-04 | Dur: 162.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    1.   89.]
 [   1.  759.    1.    1.   55.]
 [   0.    0. 1027.    0.  119.]
 [   1.    0.    0.  768.   68.]
 [   2.    1.    4.    3.  669.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.177 | Acc: 63.116% | Wgt Acc: 63.718% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   2.   6.  20.  23.]
 [  0.  41.   4.   5.   8.]
 [  0.  20.  47.   2.  31.]
 [ 12.   6.   7.  52.   8.]
 [  6.   9.  11.   7. 110.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.113 | Acc: 91.250% | Wgt Acc: 97.411%
	I - Batch: 100 | Loss: 0.114 | Acc: 91.562% | Wgt Acc: 97.359%
	I - Batch: 150 | Loss: 0.117 | Acc: 91.125% | Wgt Acc: 97.050%
	I - Batch: 200 | Loss: 0.114 | Acc: 91.281% | Wgt Acc: 97.219%
	I - Batch: 250 | Loss: 0.114 | Acc: 91.275% | Wgt Acc: 97.223%
I - num batch: 273
I - Train -- Loss: 0.116 | Acc: 91.288% | Wgt Acc: 97.136% | LR: 1.250000e-04 | Dur: 166.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 786.    1.    1.    4.   97.]
 [   1.  756.    0.    1.   51.]
 [   2.    1. 1027.    1.  128.]
 [   1.    0.    1.  765.   76.]
 [   7.    2.    3.    2.  648.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.118 | Acc: 64.300% | Wgt Acc: 60.650% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   4.   3.  13.  15.]
 [  1.  35.   3.   1.   9.]
 [  2.  12.  37.   2.  14.]
 [ 15.   9.   9.  60.   9.]
 [  9.  18.  23.  10. 133.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.110 | Acc: 93.625% | Wgt Acc: 97.866%
	I - Batch: 100 | Loss: 0.114 | Acc: 92.375% | Wgt Acc: 97.236%
	I - Batch: 150 | Loss: 0.114 | Acc: 92.500% | Wgt Acc: 97.447%
	I - Batch: 200 | Loss: 0.116 | Acc: 92.188% | Wgt Acc: 97.312%
	I - Batch: 250 | Loss: 0.111 | Acc: 92.525% | Wgt Acc: 97.488%
I - num batch: 273
I - Train -- Loss: 0.110 | Acc: 92.641% | Wgt Acc: 97.549% | LR: 1.250000e-04 | Dur: 164.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    0.    0.    4.   83.]
 [   1.  757.    0.    0.   40.]
 [   1.    1. 1027.    1.  105.]
 [   3.    0.    2.  765.   68.]
 [   4.    2.    3.    3.  704.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.110 | Acc: 69.428% | Wgt Acc: 67.616% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   4.   6.  16.  22.]
 [  0.  41.   1.   2.   5.]
 [  0.  17.  46.   0.  16.]
 [ 11.   4.   8.  60.   4.]
 [  5.  12.  14.   8. 133.]]

I - Local maximum validation set accuracy:  69.43

I - Validation set results: 
[14-1-2-1.99][50-3-1-4.23][124-2-2-2.95][127-0-0-7.09][443-2-2-3.49][567-0-0-4.39][573-1-1-3.71][615-0-0-3.45][695-1-2-2.99][722-3-3-2.07]
[826-0-0-7.07][878-0-0-7.08][1103-0-0-2.36][1212-3-0-1.24][1368-0-0-7.23][2181-2-3-2.85][2476-2-2-0.40][2721-2-2-4.36][2818-1-1-1.39][2886-2-4-2.45]
[3231-2-2-7.59][3333-2-2-1.23][3482-2-2-3.76][3536-3-4-0.51][3625-1-1-7.74][3909-0-0-6.17][4035-0-0-7.06][4140-0-0-4.42][4214-1-3-1.39][4346-1-0-1.25]
[4581-2-4-2.39][4708-3-4-2.58][4838-3-0-1.26][4845-1-1-0.46][4868-0-0-5.36][4939-0-0-1.02][4984-2-3-1.86][5078-1-2-2.65][5396-0-0-8.19][5479-1-1-2.92]
[5717-0-0-4.88][5843-1-4-1.54][5949-3-3-2.11][5987-2-4-4.03][6014-3-3-6.19][6033-3-3-3.27][6313-0-0-4.43][6421-3-3-0.81][6500-1-2-2.60][6583-3-3-1.13]
[6683-3-3-3.87][6825-2-3-0.96][6998-3-3-0.07][7049-3-3-2.64][7517-1-1-2.36][7521-1-1--0.08][7528-1-3-4.04][7949-1-2-5.18][8135-1-4-0.42][8185-3-3-3.55]
[8269-3-1-4.79][8273-3-3-4.55][8543-3-0-2.58][8666-1-1-4.51][8672-0-0-8.26][8903-1-2-2.63][9001-2-4-1.86][9036-2-2-4.82][9281-3-3-3.43][9300-2-2-8.66]
[9571-0-4-0.56][9617-1-4-2.25][9644-2-2-4.06][9705-2-0-2.09][9801-0-0-2.85][9803-3-3-2.66][9865-3-3-4.22][9896-2-2-3.06][10314-1-4-1.85][10337-3-3-5.76]
[10403-0-0-1.20][10653-2-2-1.01][10704-2-2-3.56][10719-1-1-2.87][10727-1-0--0.02][10836-0-0-11.97][10969-2-3-0.55][11042-0-0-1.32][11088-1-1-6.21][11322-0-0-5.23]
[11398-2-2-0.38][11499-0-0-5.15][11502-3-0-2.34][11512-3-3-1.47][11608-1-2-5.43][11610-0-0-2.79][11692-0-0-2.83][11905-0-0-5.24][11993-1-1-3.09][12002-2-0-1.31]
[12052-0-0-4.38][12201-0-3-4.10][12235-2-4-3.27][12320-1-4-3.81][12377-2-2-2.33][12398-2-4-1.02][12503-1-2-1.58][12617-0-3-0.20][12685-3-3-0.83][12738-2-0-1.59]
[12742-2-2-7.61][12823-0-0-4.89][13110-1-2-3.36][13240-3-0-3.67][13253-1-1-3.66][13273-0-0-8.33][13634-1-1-1.98][13763-2-2-1.53][13905-3-3-2.12][14060-2-4-2.59]
[14065-3-0-2.18][14147-3-3-4.43][14595-2-2-1.69][14687-2-2-8.28][14788-2-2-5.35][14869-1-1-3.54][14872-3-0-1.74][14877-1-1-3.45][14927-0-3-3.51][15066-0-0-6.84]
[15175-1-4-3.61][15178-2-2-1.70][15375-3-3-1.50][15389-3-3-2.47][15568-2-4-1.36][15675-3-3-2.69][15869-1-0-0.44][16207-3-0-1.98][16236-0-0-1.40][16302-3-3-0.77]
[16331-2-2-6.66][16381-0-0-1.82][16488-1-1-6.45][16495-0-0-4.53][16650-0-0-6.37][16719-1-4-2.70][16801-0-0-5.73][16828-0-0-5.86][17137-3-3-0.90][17245-1-4-2.18]
[17278-3-4-1.28][17282-0-0-2.02][17311-2-2-2.74][17336-2-2-1.22][17608-3-3-5.19][17627-0-0-3.08][17877-3-4-2.27][17924-1-3-2.00][17984-3-3-3.19][18211-0-0-1.71]
[18276-3-3-3.38][18287-1-1-4.87][18394-0-0-4.22][18428-0-0-2.32][18442-0-3-4.84][18478-3-3-4.55][18607-0-0-6.02][18616-0-0-2.22][18663-0-0-7.18][18718-0-0-5.66]
[18766-2-2-3.19][18824-2-4-3.54][18890-3-3-4.90][18930-3-4-1.56][18938-3-3-2.83][19817-1-2-1.64][19839-0-0-0.73][19930-3-3-4.16][19944-0-3-2.72][20036-2-2-7.95]
[20101-3-3-4.19][20474-1-1-3.98][20547-3-3-1.38][20929-2-2-6.91][21245-1-1-1.17][21257-3-3-2.19][21293-1-2-3.54][21316-1-1-7.76][21384-1-1-4.16][21448-1-1-1.75]
[21483-0-0-6.89][21487-2-2-2.79][21714-0-0-1.18][21943-3-3-2.40][21947-0-0-4.40][21948-0-0-7.38][21965-2-2-6.36][21998-1-1-5.79][22025-0-4-2.54][22228-3-3-6.96]
[22446-1-1-1.02][22494-3-0-3.77][22757-0-0-4.25][22811-3-3-7.07][22976-3-4-2.88][22985-3-3-5.09][23014-0-0-4.72][23112-1-1-3.09][23144-3-3-4.61][23168-2-3-1.31]
[23219-0-0-2.14][23363-3-3-5.51][23470-0-0-2.42][23486-2-2-3.21][23497-0-3-3.93][23516-0-0-5.02][23690-1-4-1.83][23921-2-2-1.95][23936-1-2-1.04][24040-3-0-1.14]
[24111-1-4-2.67][24182-0-0-7.42][24238-3-3-4.07][24290-2-0-5.88][24345-0-0-3.45][24364-1-2-3.03][24427-3-0-6.27][24477-2-2-3.01][24495-2-4-1.94][24893-2-2-2.22]
[25012-1-2-1.47][25121-2-2-4.00][25165-3-3-4.20][25183-0-0-3.99][25297-3-3-4.19][25398-0-0-6.19][25574-2-2-2.79][25644-1-2-2.99][25718-1-2-0.07][25774-2-4--0.24]
[26032-3-3-2.52][26051-3-3-6.26][26120-0-4-4.53][26321-1-1-2.94][26732-1-1-2.03][26784-3-3-7.94][26827-3-3-1.46][26833-0-3-0.91][26838-2-4-0.51][26860-1-0-0.17]
[26948-0-0-1.69][27049-3-0-6.96][27098-1-1-1.29][27526-0-0-2.39][27639-3-3-2.03][27698-3-3-3.95][27772-0-0-5.48][27890-1-1-5.82][28040-0-0-1.22][28503-2-2-3.64]
[28577-1-1-2.65][28959-0-0-6.32][29198-3-3-4.59][29777-0-0-7.95][29877-2-2-2.19][30035-1-1-5.54][30098-0-3-2.53][30326-1-1-6.12][30572-2-2-2.60][30716-0-4-1.59]
[30806-2-2-4.25][30906-1-1-2.81][31007-0-0-4.69][31181-3-0-2.12][31238-0-0-3.58][31347-0-0-5.51][31422-2-0--0.12][31429-3-3-3.01][31431-0-0-2.77][31432-1-1-3.34]
[31477-0-0-2.61][31524-1-1-1.48][31597-1-1-2.06][31619-1-4-0.53][31701-0-0-7.44][31755-0-0-4.79][31854-3-3-3.26][32074-1-1-0.24][32078-3-3-4.32][32111-1-1-4.23]
[32127-1-1-1.76][32140-3-3-4.40][32263-2-0-1.59][32365-0-0-5.14][32411-2-3-5.47][32429-3-3-4.32][32473-3-0-3.72][32574-3-0-3.33][32584-0-4-3.76][32622-0-3-0.48]
[32858-3-0-3.25][32969-3-3-6.10][33016-2-2-4.21][33031-1-3-3.86][33035-2-2-1.67][33133-2-2-2.47][33173-2-2-1.04][33175-3-4-2.35][33306-3-3-2.49][33309-2-2-0.32]
[33474-0-3-1.73][33478-2-3-0.70][33618-1-1-0.83][33712-0-3-1.84][33782-2-4-3.29][33914-3-3-1.49][34076-3-4-2.30][34112-2-2-3.02][34138-2-1-0.35][34239-1-1-1.62]
[34364-2-2-5.69][34617-1-2-2.93][34751-3-3-5.42][34783-2-4-2.77][35015-3-3-1.43][35018-1-4-2.81][35288-2-3--0.23][0-4-4-2.80][1-4-4-2.00][2-4-4-1.65]
[3-4-4-3.06][4-4-4-1.64][5-4-3-0.33][6-4-4-5.21][7-4-4-2.06][8-4-0-0.66][9-4-4-3.61][10-4-4-4.27][11-4-4-4.28][12-4-4-0.86]
[14-4-0-0.48][15-4-0-2.98][16-4-4-2.31][17-4-4-1.32][18-4-4-2.74][19-4-0-4.71][20-4-0-1.62][21-4-2-3.10][22-4-4-3.13][23-4-4-1.01]
[24-4-4-8.98][25-4-4-2.03][26-4-4-1.18][27-4-4-1.62][28-4-4-3.62][29-4-4-0.48][30-4-4-0.68][31-4-4-2.37][32-4-4-2.36][33-4-4-2.56]
[34-4-4-1.72][35-4-4-2.42][37-4-4-2.65][39-4-0-1.12][40-4-4-1.32][41-4-4-1.34][42-4-4-1.82][43-4-4-2.44][45-4-1-2.08][46-4-4-4.51]
[47-4-4-6.65][48-4-4-2.27][51-4-4-2.92][52-4-4-3.75][53-4-4-2.18][54-4-4-2.28][55-4-4-2.16][56-4-4-1.25][57-4-0-1.69][58-4-4-3.47]
[59-4-4-3.12][60-4-4-4.13][61-4-4-4.68][62-4-4-2.64][63-4-2-4.39][64-4-2-3.27][65-4-4-5.90][66-4-0-1.59][67-4-0-1.51][68-4-4-1.29]
[69-4-0-0.81][70-4-4-2.78][72-4-4-2.84][73-4-1-2.12][74-4-4-2.11][75-4-0-1.29][77-4-4-5.02][78-4-2-0.39][79-4-2-3.32][80-4-4-2.96]
[81-4-2-3.01][82-4-4-1.89][83-4-4-2.11][84-4-4-2.62][85-4-4-5.59][86-4-4-1.16][87-4-4-3.02][88-4-4-2.79][89-4-4-1.00][90-4-4-1.75]
[91-4-4-1.51][92-4-0-0.50][93-4-0-1.18][94-4-4-2.07][95-4-4-2.44][96-4-4-3.52][97-4-4-5.61][98-4-2-3.49][99-4-4-0.45][100-4-2-4.48]
[101-4-4-5.41][102-4-4-3.28][103-4-0-0.04][104-4-4-2.52][105-4-4-3.07][106-4-4-3.17][107-4-4-2.76][108-4-4-0.76][109-4-4-3.72][110-4-4-3.64]
[111-4-0-4.55][112-4-4-1.49][113-4-4-0.53][114-4-2-1.56][115-4-4-1.41][116-4-4-1.96][117-4-4-4.82][119-4-4-2.37][121-4-4-2.77][122-4-4-2.53]
[124-4-4-1.91][125-4-4-3.23][126-4-4-4.33][127-4-4-2.29][128-4-4-0.59][129-4-4-1.52][130-4-4-1.51][131-4-4-1.45][132-4-4-0.82][133-4-4-5.07]
[135-4-4-2.09][136-4-4-0.34][137-4-4-1.80][138-4-4-2.09][139-4-4-2.66][140-4-4-0.86][141-4-0-3.85][142-4-4-4.10][143-4-4-6.00][144-4-4-5.00]
[145-4-4-2.94][148-4-0-3.10][149-4-4-3.24][150-4-4-5.30][151-4-4-2.59][152-4-4-2.76][153-4-2-4.90][154-4-4-8.08][155-4-4-4.45][156-4-3-2.02]
[157-4-0-4.86][158-4-4-1.31][160-4-2-1.83][161-4-2-4.00][162-4-4-0.97][164-4-4-2.74][165-4-4-1.70][167-4-0-2.30][168-4-0-1.45][170-4-4-2.64]
[171-4-4-2.19][172-4-4-5.34][173-4-4-4.75][174-4-0-2.71][175-4-4-2.94][177-4-0-2.22][178-4-4-2.09][179-4-4-0.01][180-4-4-4.06][181-4-4-2.39]
[182-4-3-2.63][183-4-4-4.66][184-4-2-2.80][186-4-4-1.59][187-4-1-1.31][188-4-4-2.72][189-4-2-0.53][190-4-3-2.33][191-4-4-2.89][192-4-4-0.19]
[193-4-1-4.42][194-4-2-2.77][195-4-4-1.19][196-4-2-5.39][197-4-1-3.29][198-4-4-8.21][199-4-4-2.20]
---------------------------
I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.104 | Acc: 93.375% | Wgt Acc: 97.448%
	I - Batch: 100 | Loss: 0.109 | Acc: 92.688% | Wgt Acc: 97.434%
	I - Batch: 150 | Loss: 0.116 | Acc: 92.333% | Wgt Acc: 97.184%
	I - Batch: 200 | Loss: 0.117 | Acc: 92.062% | Wgt Acc: 97.081%
	I - Batch: 250 | Loss: 0.116 | Acc: 91.925% | Wgt Acc: 97.122%
I - num batch: 273
I - Train -- Loss: 0.115 | Acc: 91.884% | Wgt Acc: 97.111% | LR: 1.250000e-04 | Dur: 166.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 783.    1.    0.    7.   99.]
 [   0.  753.    0.    1.   45.]
 [   4.    0. 1027.    1.  101.]
 [   0.    1.    0.  763.   73.]
 [  10.    5.    5.    1.  682.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.285 | Acc: 63.116% | Wgt Acc: 62.230% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   6.   7.  25.  23.]
 [  0.  36.   2.   1.   6.]
 [  2.  18.  44.   0.  17.]
 [ 12.   8.   8.  53.  17.]
 [  4.  10.  14.   7. 117.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.115 | Acc: 90.875% | Wgt Acc: 97.255%
	I - Batch: 100 | Loss: 0.111 | Acc: 91.438% | Wgt Acc: 97.488%
	I - Batch: 150 | Loss: 0.106 | Acc: 92.125% | Wgt Acc: 97.580%
	I - Batch: 200 | Loss: 0.109 | Acc: 91.938% | Wgt Acc: 97.481%
	I - Batch: 250 | Loss: 0.108 | Acc: 92.050% | Wgt Acc: 97.473%
I - num batch: 273
I - Train -- Loss: 0.110 | Acc: 92.045% | Wgt Acc: 97.450% | LR: 1.250000e-04 | Dur: 163.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 784.    1.    0.    0.  101.]
 [   0.  755.    0.    0.   49.]
 [   0.    1. 1029.    1.   96.]
 [   1.    1.    1.  771.   78.]
 [  12.    2.    2.    1.  676.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.179 | Acc: 65.483% | Wgt Acc: 63.672% | Dur: 13.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   5.  12.  11.]
 [  1.  37.   2.   1.   5.]
 [  2.  17.  41.   1.  16.]
 [ 18.  13.  11.  67.  21.]
 [  7.   8.  16.   5. 127.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.104 | Acc: 92.000% | Wgt Acc: 97.787%
	I - Batch: 100 | Loss: 0.110 | Acc: 91.812% | Wgt Acc: 97.387%
	I - Batch: 150 | Loss: 0.108 | Acc: 92.000% | Wgt Acc: 97.379%
	I - Batch: 200 | Loss: 0.108 | Acc: 91.844% | Wgt Acc: 97.342%
	I - Batch: 250 | Loss: 0.112 | Acc: 91.750% | Wgt Acc: 97.245%
I - num batch: 273
I - Train -- Loss: 0.112 | Acc: 91.884% | Wgt Acc: 97.289% | LR: 1.250000e-04 | Dur: 162.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    1.    2.    2.   94.]
 [   1.  756.    0.    0.   47.]
 [   2.    0. 1025.    0.  104.]
 [   1.    1.    0.  765.   81.]
 [   5.    2.    5.    6.  674.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.208 | Acc: 64.497% | Wgt Acc: 62.519% | Dur: 14.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   2.   6.  23.  24.]
 [  0.  40.   3.   1.   7.]
 [  1.  10.  39.   1.  10.]
 [ 15.   7.   9.  55.  14.]
 [  4.  19.  18.   6. 125.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.106 | Acc: 92.250% | Wgt Acc: 97.711%
	I - Batch: 100 | Loss: 0.108 | Acc: 92.250% | Wgt Acc: 97.585%
	I - Batch: 150 | Loss: 0.110 | Acc: 92.000% | Wgt Acc: 97.466%
	I - Batch: 200 | Loss: 0.107 | Acc: 92.406% | Wgt Acc: 97.590%
	I - Batch: 250 | Loss: 0.106 | Acc: 92.300% | Wgt Acc: 97.630%
I - num batch: 273
I - Train -- Loss: 0.105 | Acc: 92.435% | Wgt Acc: 97.678% | LR: 1.250000e-04 | Dur: 162.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    1.    0.    0.   74.]
 [   0.  755.    1.    0.   55.]
 [   2.    2. 1029.    0.  108.]
 [   1.    0.    0.  771.   76.]
 [   4.    2.    2.    2.  687.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.221 | Acc: 64.694% | Wgt Acc: 61.066% | Dur: 13.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   1.   5.   8.  11.]
 [  0.  33.   4.   2.   7.]
 [  0.  17.  36.   1.  15.]
 [ 17.  11.  11.  65.  13.]
 [ 11.  16.  19.  10. 134.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.082 | Acc: 94.500% | Wgt Acc: 98.482%
	I - Batch: 100 | Loss: 0.095 | Acc: 93.562% | Wgt Acc: 97.941%
	I - Batch: 150 | Loss: 0.100 | Acc: 93.250% | Wgt Acc: 97.877%
	I - Batch: 200 | Loss: 0.108 | Acc: 92.844% | Wgt Acc: 97.676%
	I - Batch: 250 | Loss: 0.113 | Acc: 92.600% | Wgt Acc: 97.518%
I - num batch: 273
I - Train -- Loss: 0.114 | Acc: 92.366% | Wgt Acc: 97.482% | LR: 1.250000e-04 | Dur: 166.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 785.    2.    0.    2.   90.]
 [   2.  754.    1.    0.   49.]
 [   2.    1. 1030.    1.   93.]
 [   1.    2.    0.  768.   76.]
 [   7.    1.    1.    2.  692.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.088 | Acc: 66.272% | Wgt Acc: 62.646% | Dur: 14.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   5.  19.  15.]
 [  2.  36.   6.   1.   6.]
 [  1.  14.  40.   1.  13.]
 [ 11.   8.   7.  58.  10.]
 [  8.  18.  17.   7. 136.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.115 | Acc: 92.625% | Wgt Acc: 97.334%
	I - Batch: 100 | Loss: 0.110 | Acc: 92.000% | Wgt Acc: 97.256%
	I - Batch: 150 | Loss: 0.111 | Acc: 92.250% | Wgt Acc: 97.236%
	I - Batch: 200 | Loss: 0.116 | Acc: 92.094% | Wgt Acc: 97.063%
	I - Batch: 250 | Loss: 0.115 | Acc: 92.075% | Wgt Acc: 97.066%
I - num batch: 273
I - Train -- Loss: 0.114 | Acc: 92.228% | Wgt Acc: 97.142% | LR: 1.250000e-04 | Dur: 164.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 781.    3.    1.    1.   89.]
 [   0.  750.    1.    1.   39.]
 [   2.    2. 1025.    0.   99.]
 [   2.    1.    0.  767.   73.]
 [  12.    4.    5.    4.  700.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.190 | Acc: 64.300% | Wgt Acc: 63.038% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   1.   4.  11.  13.]
 [  1.  42.   0.   2.   9.]
 [  3.  17.  45.   4.  20.]
 [ 19.   8.  12.  61.  16.]
 [  9.  10.  14.   8. 122.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.109 | Acc: 91.625% | Wgt Acc: 97.686%
	I - Batch: 100 | Loss: 0.103 | Acc: 92.500% | Wgt Acc: 97.880%
	I - Batch: 150 | Loss: 0.104 | Acc: 92.667% | Wgt Acc: 97.802%
	I - Batch: 200 | Loss: 0.103 | Acc: 92.594% | Wgt Acc: 97.733%
	I - Batch: 250 | Loss: 0.107 | Acc: 92.600% | Wgt Acc: 97.593%
I - num batch: 273
I - Train -- Loss: 0.109 | Acc: 92.412% | Wgt Acc: 97.538% | LR: 1.250000e-04 | Dur: 163.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    1.    0.    1.   82.]
 [   2.  757.    1.    1.   57.]
 [   2.    0. 1026.    0.  103.]
 [   0.    0.    2.  767.   66.]
 [   4.    2.    3.    4.  692.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.201 | Acc: 65.089% | Wgt Acc: 64.572% | Dur: 14.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   2.   5.  11.  18.]
 [  0.  37.   1.   1.   7.]
 [  1.  20.  42.   1.  23.]
 [ 17.   8.  13.  66.  12.]
 [  5.  11.  14.   7. 120.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.111 | Acc: 92.500% | Wgt Acc: 97.402%
	I - Batch: 100 | Loss: 0.113 | Acc: 92.250% | Wgt Acc: 97.378%
	I - Batch: 150 | Loss: 0.105 | Acc: 92.875% | Wgt Acc: 97.650%
	I - Batch: 200 | Loss: 0.108 | Acc: 92.531% | Wgt Acc: 97.619%
	I - Batch: 250 | Loss: 0.109 | Acc: 92.325% | Wgt Acc: 97.583%
I - num batch: 273
I - Train -- Loss: 0.109 | Acc: 92.366% | Wgt Acc: 97.588% | LR: 1.250000e-04 | Dur: 165.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    1.    0.    4.  104.]
 [   1.  754.    0.    0.   56.]
 [   1.    2. 1032.    0.   79.]
 [   1.    1.    0.  768.   74.]
 [   6.    2.    0.    1.  687.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.220 | Acc: 64.497% | Wgt Acc: 63.199% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   4.  15.  20.]
 [  1.  35.   0.   1.   5.]
 [  3.  14.  46.   2.  21.]
 [ 14.   8.   9.  60.  12.]
 [  6.  16.  16.   8. 122.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.099 | Acc: 92.875% | Wgt Acc: 97.764%
	I - Batch: 100 | Loss: 0.102 | Acc: 93.188% | Wgt Acc: 97.798%
	I - Batch: 150 | Loss: 0.101 | Acc: 92.833% | Wgt Acc: 97.819%
	I - Batch: 200 | Loss: 0.101 | Acc: 93.000% | Wgt Acc: 97.761%
	I - Batch: 250 | Loss: 0.100 | Acc: 92.900% | Wgt Acc: 97.757%
I - num batch: 273
I - Train -- Loss: 0.100 | Acc: 92.733% | Wgt Acc: 97.709% | LR: 1.250000e-04 | Dur: 164.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    2.    2.   93.]
 [   0.  756.    0.    0.   52.]
 [   2.    0. 1028.    0.   99.]
 [   1.    0.    0.  769.   54.]
 [   4.    4.    2.    2.  702.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.219 | Acc: 64.103% | Wgt Acc: 61.850% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   0.   4.  11.  12.]
 [  1.  36.   3.   3.   8.]
 [  2.  21.  45.   3.  25.]
 [ 18.   5.   7.  60.   9.]
 [  9.  16.  16.   9. 126.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773.  781.]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.079 | Acc: 94.750% | Wgt Acc: 98.237%
	I - Batch: 100 | Loss: 0.089 | Acc: 93.688% | Wgt Acc: 97.674%
	I - Batch: 150 | Loss: 0.087 | Acc: 93.708% | Wgt Acc: 97.892%
	I - Batch: 200 | Loss: 0.087 | Acc: 93.906% | Wgt Acc: 97.993%
	I - Batch: 250 | Loss: 0.090 | Acc: 93.800% | Wgt Acc: 97.999%
I - num batch: 259
I - Train -- Loss: 0.090 | Acc: 93.748% | Wgt Acc: 97.976% | LR: 1.250000e-04 | Dur: 159.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    1.    1.    2.   55.]
 [   0.  756.    1.    2.   36.]
 [   0.    1. 1028.    1.   85.]
 [   0.    1.    1.  766.   62.]
 [   6.    1.    1.    2.  543.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.144 | Acc: 65.878% | Wgt Acc: 64.030% | Dur: 14.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   6.   6.  18.  18.]
 [  0.  42.   3.   3.  11.]
 [  1.   7.  43.   1.  16.]
 [ 16.   6.  10.  57.   8.]
 [  6.  17.  13.   7. 127.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.101 | Acc: 92.875% | Wgt Acc: 97.678%
	I - Batch: 100 | Loss: 0.095 | Acc: 92.938% | Wgt Acc: 97.696%
	I - Batch: 150 | Loss: 0.096 | Acc: 93.000% | Wgt Acc: 97.746%
	I - Batch: 200 | Loss: 0.100 | Acc: 92.906% | Wgt Acc: 97.778%
	I - Batch: 250 | Loss: 0.099 | Acc: 92.950% | Wgt Acc: 97.846%
I - num batch: 273
I - Train -- Loss: 0.098 | Acc: 93.077% | Wgt Acc: 97.890% | LR: 1.250000e-04 | Dur: 165.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    0.    2.   83.]
 [   0.  758.    0.    0.   35.]
 [   0.    0. 1029.    0.   86.]
 [   3.    0.    1.  771.   83.]
 [   5.    2.    2.    0.  713.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.137 | Acc: 66.469% | Wgt Acc: 60.766% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   4.  16.  16.]
 [  0.  32.   1.   2.   4.]
 [  0.  14.  36.   1.  11.]
 [ 16.   8.  11.  57.   3.]
 [  6.  20.  23.  10. 146.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.100 | Acc: 93.500% | Wgt Acc: 98.303%
	I - Batch: 100 | Loss: 0.099 | Acc: 93.375% | Wgt Acc: 97.904%
	I - Batch: 150 | Loss: 0.100 | Acc: 93.208% | Wgt Acc: 97.866%
	I - Batch: 200 | Loss: 0.096 | Acc: 93.469% | Wgt Acc: 97.919%
	I - Batch: 250 | Loss: 0.097 | Acc: 93.375% | Wgt Acc: 97.923%
I - num batch: 273
I - Train -- Loss: 0.095 | Acc: 93.398% | Wgt Acc: 97.938% | LR: 1.250000e-04 | Dur: 164.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    2.   88.]
 [   0.  757.    0.    0.   40.]
 [   0.    0. 1031.    1.   79.]
 [   0.    0.    0.  766.   65.]
 [   5.    3.    1.    4.  728.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 66.075% | Wgt Acc: 61.735% | Dur: 13.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   3.  13.  15.]
 [  0.  34.   5.   0.   4.]
 [  1.  13.  37.   1.  13.]
 [ 12.  10.   9.  58.   9.]
 [  8.  17.  21.  14. 139.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.088 | Acc: 94.375% | Wgt Acc: 97.954%
	I - Batch: 100 | Loss: 0.097 | Acc: 93.438% | Wgt Acc: 97.616%
	I - Batch: 150 | Loss: 0.095 | Acc: 93.208% | Wgt Acc: 97.703%
	I - Batch: 200 | Loss: 0.093 | Acc: 93.375% | Wgt Acc: 97.855%
	I - Batch: 250 | Loss: 0.092 | Acc: 93.550% | Wgt Acc: 97.880%
I - num batch: 273
I - Train -- Loss: 0.093 | Acc: 93.512% | Wgt Acc: 97.839% | LR: 1.250000e-04 | Dur: 165.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    2.    0.   84.]
 [   1.  758.    0.    0.   35.]
 [   0.    1. 1028.    1.   76.]
 [   1.    0.    0.  765.   66.]
 [   6.    1.    2.    7.  739.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.290 | Acc: 64.694% | Wgt Acc: 56.718% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   5.  17.   9.]
 [  0.  30.   2.   1.   3.]
 [  0.  16.  30.   1.   7.]
 [ 11.   5.   7.  49.   8.]
 [ 11.  23.  31.  18. 153.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.104 | Acc: 92.125% | Wgt Acc: 97.600%
	I - Batch: 100 | Loss: 0.097 | Acc: 93.375% | Wgt Acc: 97.853%
	I - Batch: 150 | Loss: 0.094 | Acc: 93.542% | Wgt Acc: 97.962%
	I - Batch: 200 | Loss: 0.092 | Acc: 93.781% | Wgt Acc: 97.959%
	I - Batch: 250 | Loss: 0.091 | Acc: 94.050% | Wgt Acc: 98.096%
I - num batch: 273
I - Train -- Loss: 0.094 | Acc: 94.039% | Wgt Acc: 98.061% | LR: 1.250000e-04 | Dur: 163.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    1.    2.   76.]
 [   0.  757.    0.    0.   32.]
 [   1.    1. 1029.    1.   80.]
 [   0.    0.    0.  766.   54.]
 [   4.    2.    2.    4.  758.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.180 | Acc: 66.667% | Wgt Acc: 62.807% | Dur: 13.97s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   3.  13.  12.]
 [  0.  34.   3.   3.   1.]
 [  1.  23.  39.   1.  23.]
 [ 12.   7.  11.  61.   6.]
 [  9.  12.  19.   8. 138.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.084 | Acc: 93.500% | Wgt Acc: 98.109%
	I - Batch: 100 | Loss: 0.082 | Acc: 93.500% | Wgt Acc: 98.141%
	I - Batch: 150 | Loss: 0.090 | Acc: 93.042% | Wgt Acc: 97.936%
	I - Batch: 200 | Loss: 0.097 | Acc: 92.844% | Wgt Acc: 97.878%
	I - Batch: 250 | Loss: 0.098 | Acc: 92.700% | Wgt Acc: 97.665%
I - num batch: 273
I - Train -- Loss: 0.098 | Acc: 92.756% | Wgt Acc: 97.647% | LR: 1.250000e-04 | Dur: 166.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 786.    0.    0.    0.   95.]
 [   0.  755.    0.    1.   46.]
 [   1.    0. 1031.    0.   93.]
 [   2.    1.    0.  768.   60.]
 [   8.    4.    1.    4.  706.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.218 | Acc: 62.722% | Wgt Acc: 58.944% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   3.   7.  18.  19.]
 [  0.  37.   6.   3.   5.]
 [  2.  12.  27.   1.  16.]
 [ 13.  10.  12.  59.   9.]
 [  9.  16.  23.   5. 131.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.092 | Acc: 93.750% | Wgt Acc: 97.849%
	I - Batch: 100 | Loss: 0.093 | Acc: 93.250% | Wgt Acc: 97.816%
	I - Batch: 150 | Loss: 0.093 | Acc: 93.333% | Wgt Acc: 97.832%
	I - Batch: 200 | Loss: 0.094 | Acc: 93.094% | Wgt Acc: 97.774%
	I - Batch: 250 | Loss: 0.095 | Acc: 93.375% | Wgt Acc: 97.888%
I - num batch: 273
I - Train -- Loss: 0.095 | Acc: 93.237% | Wgt Acc: 97.886% | LR: 1.250000e-04 | Dur: 164.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    0.    0.    1.   78.]
 [   0.  756.    0.    0.   42.]
 [   0.    0. 1030.    0.   83.]
 [   1.    1.    0.  771.   75.]
 [   8.    3.    2.    1.  722.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.222 | Acc: 61.736% | Wgt Acc: 61.181% | Dur: 14.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   4.  16.  22.]
 [  0.  34.   2.   4.   2.]
 [  0.  14.  37.   1.  20.]
 [ 18.  12.  15.  62.  22.]
 [  4.  15.  17.   3. 114.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.077 | Acc: 94.875% | Wgt Acc: 98.101%
	I - Batch: 100 | Loss: 0.085 | Acc: 93.625% | Wgt Acc: 97.932%
	I - Batch: 150 | Loss: 0.095 | Acc: 93.000% | Wgt Acc: 97.628%
	I - Batch: 200 | Loss: 0.097 | Acc: 93.062% | Wgt Acc: 97.570%
	I - Batch: 250 | Loss: 0.099 | Acc: 93.025% | Wgt Acc: 97.596%
I - num batch: 273
I - Train -- Loss: 0.099 | Acc: 92.916% | Wgt Acc: 97.555% | LR: 1.250000e-04 | Dur: 166.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    1.    1.    1.   92.]
 [   1.  752.    0.    0.   32.]
 [   2.    0. 1026.    1.   89.]
 [   1.    0.    0.  767.   68.]
 [   4.    7.    5.    4.  719.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.140 | Acc: 64.694% | Wgt Acc: 61.469% | Dur: 14.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   2.   6.   9.]
 [  1.  36.   4.   1.   6.]
 [  0.  13.  37.   4.  22.]
 [ 16.   9.   9.  63.  11.]
 [ 11.  18.  23.  12. 132.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.100 | Acc: 91.625% | Wgt Acc: 96.880%
	I - Batch: 100 | Loss: 0.104 | Acc: 92.062% | Wgt Acc: 97.095%
	I - Batch: 150 | Loss: 0.098 | Acc: 92.625% | Wgt Acc: 97.370%
	I - Batch: 200 | Loss: 0.095 | Acc: 92.781% | Wgt Acc: 97.555%
	I - Batch: 250 | Loss: 0.094 | Acc: 92.925% | Wgt Acc: 97.574%
I - num batch: 273
I - Train -- Loss: 0.094 | Acc: 92.870% | Wgt Acc: 97.522% | LR: 1.250000e-04 | Dur: 166.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 787.    1.    1.    2.   67.]
 [   0.  752.    0.    0.   51.]
 [   2.    3. 1027.    0.   86.]
 [   2.    0.    0.  767.   78.]
 [   6.    4.    4.    4.  718.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.190 | Acc: 66.272% | Wgt Acc: 59.785% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   2.   4.  15.  12.]
 [  0.  28.   2.   3.   2.]
 [  0.  14.  35.   0.  10.]
 [  8.   8.   8.  55.   7.]
 [ 11.  26.  26.  13. 149.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.087 | Acc: 93.375% | Wgt Acc: 98.055%
	I - Batch: 100 | Loss: 0.084 | Acc: 93.438% | Wgt Acc: 97.952%
	I - Batch: 150 | Loss: 0.085 | Acc: 93.583% | Wgt Acc: 97.947%
	I - Batch: 200 | Loss: 0.084 | Acc: 93.719% | Wgt Acc: 97.931%
	I - Batch: 250 | Loss: 0.085 | Acc: 93.800% | Wgt Acc: 97.918%
I - num batch: 273
I - Train -- Loss: 0.084 | Acc: 93.764% | Wgt Acc: 97.950% | LR: 1.250000e-04 | Dur: 167.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 787.    1.    0.    2.   69.]
 [   1.  757.    0.    0.   39.]
 [   1.    0. 1031.    1.   71.]
 [   0.    0.    0.  767.   73.]
 [   8.    2.    1.    3.  748.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.169 | Acc: 64.497% | Wgt Acc: 60.258% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   0.   4.  20.  20.]
 [  0.  32.   3.   2.   9.]
 [  0.  13.  41.   2.  10.]
 [ 12.   9.  10.  51.   6.]
 [  8.  24.  17.  11. 135.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.088 | Acc: 92.750% | Wgt Acc: 97.640%
	I - Batch: 100 | Loss: 0.084 | Acc: 93.625% | Wgt Acc: 97.912%
	I - Batch: 150 | Loss: 0.083 | Acc: 93.917% | Wgt Acc: 98.063%
	I - Batch: 200 | Loss: 0.086 | Acc: 93.656% | Wgt Acc: 97.896%
	I - Batch: 250 | Loss: 0.089 | Acc: 93.525% | Wgt Acc: 97.818%
I - num batch: 273
I - Train -- Loss: 0.089 | Acc: 93.558% | Wgt Acc: 97.848% | LR: 1.250000e-04 | Dur: 168.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    1.   71.]
 [   1.  756.    0.    1.   33.]
 [   0.    0. 1028.    0.   86.]
 [   0.    2.    0.  765.   69.]
 [   5.    2.    4.    6.  741.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 67.061% | Wgt Acc: 66.578% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   2.  11.  13.]
 [  1.  39.   4.   1.   8.]
 [  2.  22.  51.   2.  30.]
 [ 14.   5.   6.  64.   6.]
 [  8.  10.  12.   8. 123.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.080 | Acc: 94.250% | Wgt Acc: 98.128%
	I - Batch: 100 | Loss: 0.084 | Acc: 94.438% | Wgt Acc: 98.312%
	I - Batch: 150 | Loss: 0.081 | Acc: 94.583% | Wgt Acc: 98.371%
	I - Batch: 200 | Loss: 0.080 | Acc: 94.500% | Wgt Acc: 98.320%
	I - Batch: 250 | Loss: 0.084 | Acc: 94.225% | Wgt Acc: 98.216%
I - num batch: 273
I - Train -- Loss: 0.084 | Acc: 94.131% | Wgt Acc: 98.086% | LR: 1.250000e-04 | Dur: 164.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    1.    0.    1.   72.]
 [   0.  756.    1.    0.   36.]
 [   0.    0. 1030.    1.   80.]
 [   2.    0.    0.  767.   50.]
 [   4.    3.    1.    4.  762.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.159 | Acc: 65.680% | Wgt Acc: 62.392% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   5.   3.  17.  17.]
 [  1.  39.   2.   2.   6.]
 [  0.  14.  42.   3.  16.]
 [ 13.   6.   9.  53.   8.]
 [  8.  14.  19.  11. 133.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.106 | Acc: 91.875% | Wgt Acc: 96.925%
	I - Batch: 100 | Loss: 0.096 | Acc: 92.688% | Wgt Acc: 97.541%
	I - Batch: 150 | Loss: 0.096 | Acc: 92.833% | Wgt Acc: 97.527%
	I - Batch: 200 | Loss: 0.092 | Acc: 93.188% | Wgt Acc: 97.736%
	I - Batch: 250 | Loss: 0.093 | Acc: 93.275% | Wgt Acc: 97.782%
I - num batch: 273
I - Train -- Loss: 0.094 | Acc: 93.168% | Wgt Acc: 97.763% | LR: 1.250000e-04 | Dur: 165.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 785.    0.    0.    0.   87.]
 [   0.  757.    0.    1.   39.]
 [   0.    0. 1027.    0.   89.]
 [   4.    1.    0.  771.   61.]
 [   8.    2.    5.    1.  724.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.225 | Acc: 65.483% | Wgt Acc: 63.418% | Dur: 14.99s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   3.  11.  23.]
 [  0.  41.   4.   3.   6.]
 [  1.  11.  38.   2.  11.]
 [ 19.   7.  13.  63.  12.]
 [  6.  15.  17.   7. 128.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.103 | Acc: 92.125% | Wgt Acc: 97.561%
	I - Batch: 100 | Loss: 0.101 | Acc: 92.688% | Wgt Acc: 97.680%
	I - Batch: 150 | Loss: 0.098 | Acc: 93.083% | Wgt Acc: 97.739%
	I - Batch: 200 | Loss: 0.098 | Acc: 93.031% | Wgt Acc: 97.686%
	I - Batch: 250 | Loss: 0.097 | Acc: 93.150% | Wgt Acc: 97.723%
I - num batch: 273
I - Train -- Loss: 0.097 | Acc: 93.168% | Wgt Acc: 97.729% | LR: 1.250000e-04 | Dur: 171.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    1.    0.    4.   77.]
 [   0.  754.    0.    0.   51.]
 [   1.    0. 1030.    1.   84.]
 [   0.    1.    0.  767.   63.]
 [   8.    4.    2.    1.  725.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.217 | Acc: 64.497% | Wgt Acc: 62.623% | Dur: 14.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   1.   3.  16.  16.]
 [  1.  39.   6.   2.   7.]
 [  0.  17.  40.   2.  18.]
 [ 18.  11.  15.  60.  14.]
 [  6.  10.  11.   6. 125.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 92.500% | Wgt Acc: 97.770%
	I - Batch: 100 | Loss: 0.084 | Acc: 93.562% | Wgt Acc: 98.081%
	I - Batch: 150 | Loss: 0.082 | Acc: 93.833% | Wgt Acc: 98.214%
	I - Batch: 200 | Loss: 0.084 | Acc: 93.875% | Wgt Acc: 98.154%
	I - Batch: 250 | Loss: 0.088 | Acc: 93.650% | Wgt Acc: 98.100%
I - num batch: 273
I - Train -- Loss: 0.088 | Acc: 93.650% | Wgt Acc: 98.101% | LR: 1.250000e-04 | Dur: 163.97s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    1.   66.]
 [   2.  758.    0.    0.   46.]
 [   0.    0. 1030.    1.   81.]
 [   0.    0.    0.  771.   72.]
 [   4.    2.    2.    0.  735.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.149 | Acc: 65.286% | Wgt Acc: 58.644% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   3.  17.   8.]
 [  0.  28.   2.   3.   1.]
 [  2.  14.  38.   0.  13.]
 [ 14.   5.   7.  54.  10.]
 [  9.  27.  25.  12. 148.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.082 | Acc: 93.750% | Wgt Acc: 98.005%
	I - Batch: 100 | Loss: 0.085 | Acc: 93.875% | Wgt Acc: 97.825%
	I - Batch: 150 | Loss: 0.084 | Acc: 94.083% | Wgt Acc: 97.950%
	I - Batch: 200 | Loss: 0.084 | Acc: 94.094% | Wgt Acc: 98.053%
	I - Batch: 250 | Loss: 0.086 | Acc: 93.975% | Wgt Acc: 98.030%
I - num batch: 273
I - Train -- Loss: 0.085 | Acc: 94.131% | Wgt Acc: 98.086% | LR: 1.250000e-04 | Dur: 163.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    2.   74.]
 [   0.  758.    1.    1.   35.]
 [   0.    1. 1029.    0.   76.]
 [   1.    0.    0.  766.   53.]
 [   5.    1.    2.    4.  762.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.184 | Acc: 67.258% | Wgt Acc: 63.718% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   3.  14.   9.]
 [  0.  36.   2.   1.   6.]
 [  1.  17.  45.   1.  20.]
 [ 12.   7.  11.  56.   8.]
 [  8.  14.  14.  14. 137.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.099 | Acc: 92.500% | Wgt Acc: 97.372%
	I - Batch: 100 | Loss: 0.105 | Acc: 92.188% | Wgt Acc: 97.104%
	I - Batch: 150 | Loss: 0.101 | Acc: 92.333% | Wgt Acc: 97.217%
	I - Batch: 200 | Loss: 0.094 | Acc: 92.938% | Wgt Acc: 97.534%
	I - Batch: 250 | Loss: 0.090 | Acc: 93.475% | Wgt Acc: 97.809%
I - num batch: 273
I - Train -- Loss: 0.090 | Acc: 93.466% | Wgt Acc: 97.847% | LR: 1.250000e-04 | Dur: 165.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    3.   72.]
 [   0.  755.    1.    1.   38.]
 [   3.    0. 1029.    1.   90.]
 [   2.    1.    0.  766.   64.]
 [   1.    4.    2.    2.  736.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.190 | Acc: 66.272% | Wgt Acc: 61.931% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   3.   3.   7.   6.]
 [  0.  36.   3.   2.   9.]
 [  5.  17.  44.   2.  17.]
 [ 22.   5.   7.  63.   8.]
 [  8.  17.  18.  12. 140.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.072 | Acc: 95.000% | Wgt Acc: 98.389%
	I - Batch: 100 | Loss: 0.076 | Acc: 94.750% | Wgt Acc: 98.426%
	I - Batch: 150 | Loss: 0.075 | Acc: 94.875% | Wgt Acc: 98.482%
	I - Batch: 200 | Loss: 0.080 | Acc: 94.594% | Wgt Acc: 98.303%
	I - Batch: 250 | Loss: 0.082 | Acc: 94.600% | Wgt Acc: 98.244%
I - num batch: 273
I - Train -- Loss: 0.080 | Acc: 94.681% | Wgt Acc: 98.299% | LR: 1.250000e-04 | Dur: 165.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    1.    1.   52.]
 [   0.  760.    0.    0.   30.]
 [   0.    0. 1029.    1.   78.]
 [   0.    0.    0.  768.   57.]
 [   7.    0.    2.    3.  783.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.250 | Acc: 63.314% | Wgt Acc: 62.450% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   8.   7.  20.  24.]
 [  0.  38.   2.   0.   6.]
 [  2.  13.  39.   1.  16.]
 [ 17.   9.  15.  59.  16.]
 [  2.  10.  12.   6. 118.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.081 | Acc: 94.625% | Wgt Acc: 98.467%
	I - Batch: 100 | Loss: 0.080 | Acc: 94.500% | Wgt Acc: 98.327%
	I - Batch: 150 | Loss: 0.082 | Acc: 94.000% | Wgt Acc: 98.003%
	I - Batch: 200 | Loss: 0.085 | Acc: 93.781% | Wgt Acc: 97.908%
	I - Batch: 250 | Loss: 0.088 | Acc: 93.675% | Wgt Acc: 97.878%
I - num batch: 273
I - Train -- Loss: 0.087 | Acc: 93.764% | Wgt Acc: 97.926% | LR: 1.250000e-04 | Dur: 167.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    1.    0.    1.   76.]
 [   1.  757.    0.    0.   39.]
 [   0.    0. 1030.    1.   76.]
 [   1.    0.    0.  766.   60.]
 [   7.    2.    2.    5.  749.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.154 | Acc: 64.497% | Wgt Acc: 61.977% | Dur: 14.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   1.   4.  19.  13.]
 [  0.  41.   5.   1.   8.]
 [  1.  12.  36.   1.  19.]
 [ 17.   8.  12.  58.  12.]
 [  6.  16.  18.   7. 128.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.074 | Acc: 95.375% | Wgt Acc: 98.572%
	I - Batch: 100 | Loss: 0.081 | Acc: 94.375% | Wgt Acc: 98.249%
	I - Batch: 150 | Loss: 0.084 | Acc: 93.875% | Wgt Acc: 97.944%
	I - Batch: 200 | Loss: 0.083 | Acc: 93.969% | Wgt Acc: 98.032%
	I - Batch: 250 | Loss: 0.082 | Acc: 94.075% | Wgt Acc: 98.124%
I - num batch: 273
I - Train -- Loss: 0.083 | Acc: 93.971% | Wgt Acc: 98.100% | LR: 1.250000e-04 | Dur: 166.63s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    0.    1.    0.   77.]
 [   0.  757.    0.    0.   45.]
 [   1.    1. 1029.    0.   65.]
 [   1.    0.    0.  772.   60.]
 [   7.    2.    2.    1.  753.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.318 | Acc: 63.314% | Wgt Acc: 59.209% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   5.   7.  20.  22.]
 [  0.  35.   4.   3.   3.]
 [  1.  11.  33.   1.  14.]
 [ 13.   9.  10.  55.   8.]
 [  9.  18.  21.   7. 133.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.073 | Acc: 95.750% | Wgt Acc: 98.663%
	I - Batch: 100 | Loss: 0.077 | Acc: 94.938% | Wgt Acc: 98.379%
	I - Batch: 150 | Loss: 0.080 | Acc: 94.708% | Wgt Acc: 98.219%
	I - Batch: 200 | Loss: 0.078 | Acc: 94.688% | Wgt Acc: 98.239%
	I - Batch: 250 | Loss: 0.076 | Acc: 94.975% | Wgt Acc: 98.372%
I - num batch: 273
I - Train -- Loss: 0.077 | Acc: 94.888% | Wgt Acc: 98.373% | LR: 1.250000e-04 | Dur: 165.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    1.   63.]
 [   0.  758.    0.    1.   30.]
 [   0.    0. 1030.    0.   66.]
 [   0.    0.    0.  769.   50.]
 [   6.    2.    2.    2.  791.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.107 | Acc: 69.034% | Wgt Acc: 65.437% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   2.   8.  10.]
 [  0.  34.   1.   1.   4.]
 [  4.  20.  48.   3.  21.]
 [ 13.   2.   5.  64.   4.]
 [  8.  19.  19.  10. 141.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.079 | Acc: 95.000% | Wgt Acc: 98.353%
	I - Batch: 100 | Loss: 0.078 | Acc: 94.500% | Wgt Acc: 98.274%
	I - Batch: 150 | Loss: 0.079 | Acc: 94.458% | Wgt Acc: 98.140%
	I - Batch: 200 | Loss: 0.081 | Acc: 94.469% | Wgt Acc: 98.188%
	I - Batch: 250 | Loss: 0.081 | Acc: 94.550% | Wgt Acc: 98.216%
I - num batch: 273
I - Train -- Loss: 0.081 | Acc: 94.475% | Wgt Acc: 98.205% | LR: 1.250000e-04 | Dur: 163.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    1.    2.   72.]
 [   0.  756.    1.    0.   26.]
 [   0.    0. 1027.    0.   70.]
 [   0.    1.    0.  771.   56.]
 [   6.    3.    3.    0.  776.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.168 | Acc: 65.286% | Wgt Acc: 60.224% | Dur: 13.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   5.  16.  16.]
 [  1.  29.   4.   2.   3.]
 [  1.  17.  40.   1.  12.]
 [ 13.   6.   5.  59.   8.]
 [ 11.  23.  21.   8. 141.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 93.875% | Wgt Acc: 97.951%
	I - Batch: 100 | Loss: 0.092 | Acc: 93.750% | Wgt Acc: 98.014%
	I - Batch: 150 | Loss: 0.090 | Acc: 93.917% | Wgt Acc: 98.065%
	I - Batch: 200 | Loss: 0.087 | Acc: 93.969% | Wgt Acc: 98.114%
	I - Batch: 250 | Loss: 0.085 | Acc: 94.200% | Wgt Acc: 98.167%
I - num batch: 273
I - Train -- Loss: 0.083 | Acc: 94.269% | Wgt Acc: 98.221% | LR: 1.250000e-04 | Dur: 163.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    1.    0.    0.   74.]
 [   2.  758.    0.    0.   42.]
 [   0.    0. 1030.    0.   67.]
 [   1.    0.    0.  772.   53.]
 [   6.    1.    2.    1.  764.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.158 | Acc: 66.075% | Wgt Acc: 59.128% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   1.   5.  22.  11.]
 [  1.  42.   6.   3.   7.]
 [  0.   8.  31.   0.   9.]
 [ 14.   6.  10.  49.   2.]
 [ 11.  21.  23.  12. 151.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.069 | Acc: 94.375% | Wgt Acc: 98.420%
	I - Batch: 100 | Loss: 0.070 | Acc: 94.688% | Wgt Acc: 98.449%
	I - Batch: 150 | Loss: 0.071 | Acc: 94.625% | Wgt Acc: 98.498%
	I - Batch: 200 | Loss: 0.072 | Acc: 94.719% | Wgt Acc: 98.424%
	I - Batch: 250 | Loss: 0.074 | Acc: 94.800% | Wgt Acc: 98.437%
I - num batch: 273
I - Train -- Loss: 0.076 | Acc: 94.704% | Wgt Acc: 98.385% | LR: 1.250000e-04 | Dur: 162.69s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    1.   62.]
 [   0.  758.    0.    0.   37.]
 [   0.    0. 1030.    1.   67.]
 [   0.    0.    0.  767.   54.]
 [   1.    2.    2.    4.  780.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.195 | Acc: 65.483% | Wgt Acc: 60.178% | Dur: 14.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.   6.   5.  23.  18.]
 [  0.  34.   1.   0.   3.]
 [  0.   9.  31.   1.   9.]
 [  9.   5.  11.  52.   8.]
 [  6.  24.  27.  10. 142.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.075 | Acc: 94.375% | Wgt Acc: 98.541%
	I - Batch: 100 | Loss: 0.077 | Acc: 94.188% | Wgt Acc: 98.318%
	I - Batch: 150 | Loss: 0.079 | Acc: 94.000% | Wgt Acc: 98.221%
	I - Batch: 200 | Loss: 0.081 | Acc: 94.250% | Wgt Acc: 98.263%
	I - Batch: 250 | Loss: 0.081 | Acc: 94.150% | Wgt Acc: 98.209%
I - num batch: 273
I - Train -- Loss: 0.081 | Acc: 94.177% | Wgt Acc: 98.213% | LR: 1.250000e-04 | Dur: 164.78s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    1.   69.]
 [   0.  758.    0.    0.   43.]
 [   0.    0. 1030.    0.   69.]
 [   0.    0.    0.  770.   60.]
 [   6.    2.    2.    2.  759.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.206 | Acc: 67.258% | Wgt Acc: 63.545% | Dur: 14.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   5.  12.   9.]
 [  0.  39.   2.   2.   5.]
 [  2.  15.  40.   0.  12.]
 [ 23.   8.  10.  65.  15.]
 [  5.  13.  18.   7. 139.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 93.000% | Wgt Acc: 97.430%
	I - Batch: 100 | Loss: 0.088 | Acc: 93.125% | Wgt Acc: 97.663%
	I - Batch: 150 | Loss: 0.086 | Acc: 93.417% | Wgt Acc: 97.770%
	I - Batch: 200 | Loss: 0.083 | Acc: 93.531% | Wgt Acc: 97.885%
	I - Batch: 250 | Loss: 0.083 | Acc: 93.800% | Wgt Acc: 97.941%
I - num batch: 273
I - Train -- Loss: 0.083 | Acc: 93.696% | Wgt Acc: 97.938% | LR: 1.250000e-04 | Dur: 165.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 785.    0.    0.    1.   81.]
 [   0.  758.    0.    0.   44.]
 [   1.    0. 1030.    0.   65.]
 [   0.    0.    1.  769.   65.]
 [  11.    2.    1.    3.  745.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.250 | Acc: 65.878% | Wgt Acc: 63.061% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   3.   5.   8.  11.]
 [  0.  40.   8.   0.   3.]
 [  4.  17.  39.   2.  19.]
 [ 25.  10.  11.  70.  14.]
 [  7.   8.  12.   6. 133.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.085 | Acc: 94.125% | Wgt Acc: 98.154%
	I - Batch: 100 | Loss: 0.079 | Acc: 94.875% | Wgt Acc: 98.495%
	I - Batch: 150 | Loss: 0.076 | Acc: 94.833% | Wgt Acc: 98.464%
	I - Batch: 200 | Loss: 0.079 | Acc: 94.500% | Wgt Acc: 98.370%
	I - Batch: 250 | Loss: 0.077 | Acc: 94.675% | Wgt Acc: 98.415%
I - num batch: 273
I - Train -- Loss: 0.077 | Acc: 94.635% | Wgt Acc: 98.402% | LR: 1.250000e-04 | Dur: 163.56s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    0.    0.   68.]
 [   0.  760.    0.    1.   36.]
 [   0.    0. 1030.    0.   71.]
 [   0.    0.    0.  772.   49.]
 [   7.    0.    2.    0.  776.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.173 | Acc: 65.089% | Wgt Acc: 61.504% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   1.   3.  12.   8.]
 [  0.  37.   3.   2.   5.]
 [  2.  18.  41.   1.  21.]
 [ 20.   7.  11.  59.  12.]
 [  7.  15.  17.  12. 134.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773.  781.]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 95.625% | Wgt Acc: 98.800%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.688% | Wgt Acc: 98.687%
	I - Batch: 150 | Loss: 0.062 | Acc: 95.833% | Wgt Acc: 98.774%
	I - Batch: 200 | Loss: 0.064 | Acc: 95.562% | Wgt Acc: 98.721%
	I - Batch: 250 | Loss: 0.065 | Acc: 95.325% | Wgt Acc: 98.581%
I - num batch: 259
I - Train -- Loss: 0.065 | Acc: 95.414% | Wgt Acc: 98.614% | LR: 1.250000e-04 | Dur: 153.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    1.    1.   58.]
 [   0.  760.    0.    0.   22.]
 [   0.    0. 1031.    0.   48.]
 [   0.    0.    0.  768.   51.]
 [   5.    0.    0.    4.  602.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.216 | Acc: 65.483% | Wgt Acc: 60.893% | Dur: 13.78s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   2.   2.  17.  16.]
 [  0.  28.   4.   1.   4.]
 [  0.  19.  39.   3.  14.]
 [ 14.  12.  10.  59.   7.]
 [  7.  17.  20.   6. 139.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.064 | Acc: 95.125% | Wgt Acc: 98.632%
	I - Batch: 100 | Loss: 0.065 | Acc: 95.875% | Wgt Acc: 98.836%
	I - Batch: 150 | Loss: 0.068 | Acc: 95.167% | Wgt Acc: 98.644%
	I - Batch: 200 | Loss: 0.069 | Acc: 95.188% | Wgt Acc: 98.620%
	I - Batch: 250 | Loss: 0.068 | Acc: 95.225% | Wgt Acc: 98.630%
I - num batch: 273
I - Train -- Loss: 0.069 | Acc: 95.277% | Wgt Acc: 98.609% | LR: 1.250000e-04 | Dur: 161.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    1.    0.    0.   67.]
 [   0.  757.    0.    0.   24.]
 [   0.    0. 1030.    0.   58.]
 [   0.    0.    0.  773.   49.]
 [   3.    2.    2.    0.  802.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.155 | Acc: 68.245% | Wgt Acc: 62.542% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   1.   4.  12.   9.]
 [  1.  37.   4.   2.   3.]
 [  0.  23.  38.   4.  16.]
 [ 13.   6.  10.  55.   3.]
 [  7.  11.  19.  13. 149.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.076 | Acc: 94.750% | Wgt Acc: 98.162%
	I - Batch: 100 | Loss: 0.072 | Acc: 94.938% | Wgt Acc: 98.277%
	I - Batch: 150 | Loss: 0.072 | Acc: 94.708% | Wgt Acc: 98.243%
	I - Batch: 200 | Loss: 0.076 | Acc: 94.625% | Wgt Acc: 98.113%
	I - Batch: 250 | Loss: 0.075 | Acc: 94.575% | Wgt Acc: 98.124%
I - num batch: 273
I - Train -- Loss: 0.073 | Acc: 94.704% | Wgt Acc: 98.202% | LR: 1.250000e-04 | Dur: 163.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 786.    0.    0.    0.   71.]
 [   0.  757.    0.    0.   33.]
 [   0.    1. 1028.    0.   61.]
 [   2.    0.    0.  771.   46.]
 [   9.    2.    4.    2.  789.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.135 | Acc: 69.428% | Wgt Acc: 65.483% | Dur: 14.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   3.  15.   8.]
 [  0.  38.   2.   1.   2.]
 [  1.  14.  49.   2.  20.]
 [ 15.   4.   6.  59.   7.]
 [  9.  18.  15.   9. 143.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.068 | Acc: 95.000% | Wgt Acc: 98.607%
	I - Batch: 100 | Loss: 0.069 | Acc: 95.000% | Wgt Acc: 98.521%
	I - Batch: 150 | Loss: 0.073 | Acc: 94.583% | Wgt Acc: 98.317%
	I - Batch: 200 | Loss: 0.072 | Acc: 94.688% | Wgt Acc: 98.391%
	I - Batch: 250 | Loss: 0.074 | Acc: 94.450% | Wgt Acc: 98.280%
I - num batch: 273
I - Train -- Loss: 0.073 | Acc: 94.658% | Wgt Acc: 98.340% | LR: 1.250000e-04 | Dur: 168.59s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    0.    0.   81.]
 [   0.  758.    0.    0.   23.]
 [   0.    0. 1030.    1.   63.]
 [   0.    0.    1.  771.   53.]
 [   7.    2.    1.    1.  780.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.202 | Acc: 65.483% | Wgt Acc: 58.978% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   7.   5.  17.  13.]
 [  0.  30.   2.   0.   1.]
 [  2.  13.  34.   1.  11.]
 [ 13.   5.  10.  56.   7.]
 [  9.  23.  24.  12. 148.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.079 | Acc: 94.125% | Wgt Acc: 97.880%
	I - Batch: 100 | Loss: 0.078 | Acc: 94.438% | Wgt Acc: 98.152%
	I - Batch: 150 | Loss: 0.081 | Acc: 94.208% | Wgt Acc: 98.149%
	I - Batch: 200 | Loss: 0.080 | Acc: 94.312% | Wgt Acc: 98.206%
	I - Batch: 250 | Loss: 0.081 | Acc: 94.425% | Wgt Acc: 98.199%
I - num batch: 273
I - Train -- Loss: 0.080 | Acc: 94.567% | Wgt Acc: 98.250% | LR: 1.250000e-04 | Dur: 166.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    0.    0.    0.   67.]
 [   1.  756.    0.    0.   39.]
 [   0.    0. 1031.    0.   73.]
 [   0.    0.    0.  771.   42.]
 [   8.    4.    1.    2.  779.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.234 | Acc: 63.708% | Wgt Acc: 60.570% | Dur: 14.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   6.  24.  20.]
 [  0.  33.   2.   0.   5.]
 [  2.  22.  49.   3.  18.]
 [ 14.   5.   5.  47.   9.]
 [  6.  15.  13.  12. 128.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.099 | Acc: 93.375% | Wgt Acc: 97.639%
	I - Batch: 100 | Loss: 0.092 | Acc: 93.625% | Wgt Acc: 97.478%
	I - Batch: 150 | Loss: 0.092 | Acc: 93.583% | Wgt Acc: 97.529%
	I - Batch: 200 | Loss: 0.092 | Acc: 93.375% | Wgt Acc: 97.510%
	I - Batch: 250 | Loss: 0.092 | Acc: 93.550% | Wgt Acc: 97.557%
I - num batch: 273
I - Train -- Loss: 0.092 | Acc: 93.581% | Wgt Acc: 97.549% | LR: 1.250000e-04 | Dur: 165.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 785.    1.    0.    3.   77.]
 [   2.  752.    1.    1.   36.]
 [   0.    2. 1025.    0.   77.]
 [   2.    1.    1.  764.   54.]
 [   8.    4.    5.    5.  756.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.225 | Acc: 63.708% | Wgt Acc: 62.680% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   4.  12.  12.]
 [  1.  36.   3.   2.  11.]
 [  3.  16.  47.   1.  23.]
 [ 25.  11.   6.  65.  14.]
 [  4.  13.  15.   6. 120.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 0.086 | Acc: 93.875% | Wgt Acc: 98.067%
	I - Batch: 100 | Loss: 0.087 | Acc: 94.125% | Wgt Acc: 98.066%
	I - Batch: 150 | Loss: 0.086 | Acc: 94.125% | Wgt Acc: 98.164%
	I - Batch: 200 | Loss: 0.082 | Acc: 94.344% | Wgt Acc: 98.220%
	I - Batch: 250 | Loss: 0.081 | Acc: 94.350% | Wgt Acc: 98.258%
I - num batch: 273
I - Train -- Loss: 0.080 | Acc: 94.360% | Wgt Acc: 98.262% | LR: 1.250000e-04 | Dur: 164.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    0.    1.   69.]
 [   1.  759.    0.    0.   34.]
 [   1.    0. 1030.    0.   73.]
 [   0.    0.    0.  770.   57.]
 [   5.    1.    2.    2.  767.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.162 | Acc: 67.653% | Wgt Acc: 64.606% | Dur: 13.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   3.   5.  16.  11.]
 [  0.  41.   5.   1.   6.]
 [  1.  16.  41.   1.  19.]
 [ 13.   6.   7.  60.   8.]
 [  9.  12.  17.   8. 136.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 0.067 | Acc: 94.250% | Wgt Acc: 98.176%
	I - Batch: 100 | Loss: 0.080 | Acc: 94.250% | Wgt Acc: 97.925%
	I - Batch: 150 | Loss: 0.080 | Acc: 94.000% | Wgt Acc: 97.819%
	I - Batch: 200 | Loss: 0.079 | Acc: 94.125% | Wgt Acc: 97.978%
	I - Batch: 250 | Loss: 0.080 | Acc: 94.025% | Wgt Acc: 97.927%
I - num batch: 273
I - Train -- Loss: 0.081 | Acc: 94.085% | Wgt Acc: 97.968% | LR: 1.250000e-04 | Dur: 163.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 784.    0.    1.    1.   77.]
 [   0.  758.    0.    1.   35.]
 [   1.    0. 1031.    0.   66.]
 [   2.    1.    0.  766.   57.]
 [  10.    1.    0.    5.  765.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.268 | Acc: 65.680% | Wgt Acc: 63.361% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   4.   4.  18.  17.]
 [  0.  35.   3.   1.   4.]
 [  2.  14.  38.   2.  17.]
 [ 11.  12.  13.  61.  13.]
 [  5.  13.  17.   4. 129.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 0.080 | Acc: 93.625% | Wgt Acc: 98.242%
	I - Batch: 100 | Loss: 0.081 | Acc: 93.938% | Wgt Acc: 98.131%
	I - Batch: 150 | Loss: 0.080 | Acc: 94.250% | Wgt Acc: 98.192%
	I - Batch: 200 | Loss: 0.079 | Acc: 94.062% | Wgt Acc: 98.140%
	I - Batch: 250 | Loss: 0.076 | Acc: 94.175% | Wgt Acc: 98.228%
I - num batch: 273
I - Train -- Loss: 0.075 | Acc: 94.337% | Wgt Acc: 98.271% | LR: 1.250000e-04 | Dur: 162.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   64.]
 [   0.  758.    0.    0.   40.]
 [   0.    0. 1031.    0.   73.]
 [   1.    1.    0.  768.   58.]
 [   3.    1.    1.    5.  765.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.119 | Acc: 68.047% | Wgt Acc: 64.352% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   3.  11.  13.]
 [  0.  39.   3.   1.   7.]
 [  1.  14.  42.   1.  11.]
 [ 19.   5.   5.  63.   9.]
 [  7.  17.  22.  10. 140.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 95.125% | Wgt Acc: 98.274%
	I - Batch: 100 | Loss: 0.069 | Acc: 94.875% | Wgt Acc: 98.371%
	I - Batch: 150 | Loss: 0.065 | Acc: 95.125% | Wgt Acc: 98.416%
	I - Batch: 200 | Loss: 0.065 | Acc: 95.250% | Wgt Acc: 98.502%
	I - Batch: 250 | Loss: 0.066 | Acc: 95.275% | Wgt Acc: 98.490%
I - num batch: 273
I - Train -- Loss: 0.066 | Acc: 95.277% | Wgt Acc: 98.518% | LR: 1.250000e-04 | Dur: 163.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    1.    0.   60.]
 [   0.  758.    0.    0.   33.]
 [   1.    1. 1031.    0.   55.]
 [   0.    0.    0.  770.   46.]
 [   5.    1.    0.    3.  806.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.161 | Acc: 66.864% | Wgt Acc: 62.323% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   0.   2.  18.  16.]
 [  0.  39.   4.   2.   5.]
 [  4.  16.  44.   1.  14.]
 [ 14.   4.   5.  53.   4.]
 [  8.  19.  20.  12. 141.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 0.055 | Acc: 96.250% | Wgt Acc: 99.037%
	I - Batch: 100 | Loss: 0.060 | Acc: 95.375% | Wgt Acc: 98.698%
	I - Batch: 150 | Loss: 0.059 | Acc: 95.667% | Wgt Acc: 98.810%
	I - Batch: 200 | Loss: 0.067 | Acc: 95.250% | Wgt Acc: 98.600%
	I - Batch: 250 | Loss: 0.072 | Acc: 95.100% | Wgt Acc: 98.448%
I - num batch: 273
I - Train -- Loss: 0.073 | Acc: 95.209% | Wgt Acc: 98.459% | LR: 1.250000e-04 | Dur: 164.89s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    1.    2.   65.]
 [   0.  759.    1.    0.   31.]
 [   1.    0. 1028.    0.   58.]
 [   2.    0.    0.  770.   41.]
 [   3.    1.    2.    1.  805.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.240 | Acc: 64.694% | Wgt Acc: 61.908% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   3.  15.  13.]
 [  0.  28.   2.   1.   3.]
 [  1.  26.  49.   4.  25.]
 [ 15.   7.   6.  59.  10.]
 [  9.  15.  15.   7. 129.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 0.077 | Acc: 94.875% | Wgt Acc: 98.183%
	I - Batch: 100 | Loss: 0.091 | Acc: 94.000% | Wgt Acc: 97.767%
	I - Batch: 150 | Loss: 0.087 | Acc: 94.167% | Wgt Acc: 97.971%
	I - Batch: 200 | Loss: 0.085 | Acc: 94.219% | Wgt Acc: 98.026%
	I - Batch: 250 | Loss: 0.083 | Acc: 94.300% | Wgt Acc: 98.027%
I - num batch: 273
I - Train -- Loss: 0.084 | Acc: 94.131% | Wgt Acc: 97.959% | LR: 1.250000e-04 | Dur: 164.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    1.    1.   65.]
 [   0.  755.    1.    0.   29.]
 [   1.    1. 1026.    2.   73.]
 [   1.    0.    1.  768.   65.]
 [   6.    4.    3.    2.  768.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.250 | Acc: 65.483% | Wgt Acc: 56.833% | Dur: 14.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   3.  13.   3.]
 [  1.  28.   1.   1.   7.]
 [  1.  18.  40.   1.  10.]
 [  9.   5.   4.  45.   3.]
 [ 15.  25.  27.  26. 157.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 0.083 | Acc: 93.750% | Wgt Acc: 98.158%
	I - Batch: 100 | Loss: 0.072 | Acc: 94.688% | Wgt Acc: 98.542%
	I - Batch: 150 | Loss: 0.072 | Acc: 94.833% | Wgt Acc: 98.481%
	I - Batch: 200 | Loss: 0.073 | Acc: 94.938% | Wgt Acc: 98.490%
	I - Batch: 250 | Loss: 0.073 | Acc: 94.850% | Wgt Acc: 98.406%
I - num batch: 273
I - Train -- Loss: 0.074 | Acc: 94.865% | Wgt Acc: 98.339% | LR: 1.250000e-04 | Dur: 166.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    1.   61.]
 [   0.  758.    0.    0.   39.]
 [   0.    0. 1032.    1.   53.]
 [   1.    0.    0.  766.   56.]
 [   5.    2.    0.    5.  791.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.153 | Acc: 67.258% | Wgt Acc: 62.288% | Dur: 15.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   2.  10.   8.]
 [  1.  32.   2.   0.   4.]
 [  0.  19.  43.   1.  18.]
 [ 12.   7.   6.  59.   6.]
 [ 12.  18.  22.  16. 144.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 0.067 | Acc: 95.125% | Wgt Acc: 98.709%
	I - Batch: 100 | Loss: 0.071 | Acc: 94.750% | Wgt Acc: 98.429%
	I - Batch: 150 | Loss: 0.073 | Acc: 94.667% | Wgt Acc: 98.313%
	I - Batch: 200 | Loss: 0.072 | Acc: 94.844% | Wgt Acc: 98.357%
	I - Batch: 250 | Loss: 0.075 | Acc: 94.825% | Wgt Acc: 98.301%
I - num batch: 273
I - Train -- Loss: 0.074 | Acc: 94.819% | Wgt Acc: 98.315% | LR: 1.250000e-04 | Dur: 164.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    0.    1.   71.]
 [   0.  758.    0.    0.   38.]
 [   1.    1. 1029.    0.   59.]
 [   0.    0.    0.  770.   42.]
 [   7.    1.    3.    2.  790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.241 | Acc: 66.667% | Wgt Acc: 63.499% | Dur: 13.99s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   1.   3.  10.   9.]
 [  0.  34.   3.   1.   4.]
 [  1.  17.  45.   1.  18.]
 [ 20.   8.   7.  64.  14.]
 [  7.  18.  17.  10. 135.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 0.062 | Acc: 95.375% | Wgt Acc: 98.614%
	I - Batch: 100 | Loss: 0.075 | Acc: 95.000% | Wgt Acc: 98.332%
	I - Batch: 150 | Loss: 0.076 | Acc: 94.750% | Wgt Acc: 98.311%
	I - Batch: 200 | Loss: 0.072 | Acc: 95.125% | Wgt Acc: 98.456%
	I - Batch: 250 | Loss: 0.072 | Acc: 95.175% | Wgt Acc: 98.477%
I - num batch: 273
I - Train -- Loss: 0.071 | Acc: 95.186% | Wgt Acc: 98.478% | LR: 1.250000e-04 | Dur: 167.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    1.    1.   54.]
 [   1.  758.    0.    0.   34.]
 [   1.    0. 1029.    0.   59.]
 [   0.    0.    0.  772.   50.]
 [   5.    2.    2.    0.  803.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.179 | Acc: 67.456% | Wgt Acc: 61.850% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   4.  11.  12.]
 [  0.  35.   3.   1.   2.]
 [  2.  11.  33.   3.  11.]
 [ 15.   7.   9.  63.   7.]
 [  8.  21.  26.   8. 148.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 0.083 | Acc: 94.750% | Wgt Acc: 98.168%
	I - Batch: 100 | Loss: 0.077 | Acc: 95.000% | Wgt Acc: 98.408%
	I - Batch: 150 | Loss: 0.080 | Acc: 94.625% | Wgt Acc: 98.171%
	I - Batch: 200 | Loss: 0.078 | Acc: 94.906% | Wgt Acc: 98.350%
	I - Batch: 250 | Loss: 0.077 | Acc: 94.875% | Wgt Acc: 98.297%
I - num batch: 273
I - Train -- Loss: 0.077 | Acc: 94.796% | Wgt Acc: 98.286% | LR: 1.250000e-04 | Dur: 167.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    1.    1.   52.]
 [   1.  756.    0.    0.   40.]
 [   1.    1. 1028.    0.   66.]
 [   1.    0.    0.  770.   52.]
 [   3.    3.    3.    2.  790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.212 | Acc: 67.258% | Wgt Acc: 61.804% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   4.  12.  14.]
 [  1.  38.   3.   2.   6.]
 [  0.   9.  31.   2.   7.]
 [ 17.   7.   9.  62.   6.]
 [  7.  20.  28.   8. 147.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 0.060 | Acc: 95.875% | Wgt Acc: 98.716%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.812% | Wgt Acc: 98.697%
	I - Batch: 150 | Loss: 0.064 | Acc: 95.042% | Wgt Acc: 98.431%
	I - Batch: 200 | Loss: 0.066 | Acc: 94.969% | Wgt Acc: 98.450%
	I - Batch: 250 | Loss: 0.067 | Acc: 94.850% | Wgt Acc: 98.396%
I - num batch: 273
I - Train -- Loss: 0.068 | Acc: 94.773% | Wgt Acc: 98.352% | LR: 1.250000e-04 | Dur: 168.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    0.    0.    0.   77.]
 [   0.  757.    0.    0.   34.]
 [   1.    0. 1030.    0.   57.]
 [   0.    0.    0.  773.   46.]
 [   8.    3.    2.    0.  786.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.238 | Acc: 65.680% | Wgt Acc: 59.693% | Dur: 14.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.   2.   6.  28.  19.]
 [  0.  39.   5.   1.   5.]
 [  0.   8.  32.   0.   8.]
 [ 11.   7.   7.  44.   3.]
 [  4.  22.  25.  13. 145.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 0.068 | Acc: 94.875% | Wgt Acc: 98.307%
	I - Batch: 100 | Loss: 0.070 | Acc: 94.938% | Wgt Acc: 98.372%
	I - Batch: 150 | Loss: 0.070 | Acc: 94.792% | Wgt Acc: 98.285%
	I - Batch: 200 | Loss: 0.067 | Acc: 95.031% | Wgt Acc: 98.411%
	I - Batch: 250 | Loss: 0.066 | Acc: 95.275% | Wgt Acc: 98.491%
I - num batch: 273
I - Train -- Loss: 0.066 | Acc: 95.346% | Wgt Acc: 98.536% | LR: 1.250000e-04 | Dur: 169.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    0.   55.]
 [   1.  756.    0.    0.   32.]
 [   0.    0. 1031.    0.   58.]
 [   0.    0.    0.  771.   46.]
 [   4.    4.    1.    2.  809.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.260 | Acc: 62.130% | Wgt Acc: 56.453% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   5.  22.  21.]
 [  0.  34.   3.   1.   5.]
 [  1.   7.  25.   1.   8.]
 [ 12.   7.  10.  50.   8.]
 [  7.  27.  32.  12. 138.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 0.055 | Acc: 95.875% | Wgt Acc: 98.728%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.188% | Wgt Acc: 98.643%
	I - Batch: 150 | Loss: 0.062 | Acc: 95.458% | Wgt Acc: 98.742%
	I - Batch: 200 | Loss: 0.063 | Acc: 95.281% | Wgt Acc: 98.572%
	I - Batch: 250 | Loss: 0.063 | Acc: 95.350% | Wgt Acc: 98.561%
I - num batch: 273
I - Train -- Loss: 0.064 | Acc: 95.323% | Wgt Acc: 98.577% | LR: 1.250000e-04 | Dur: 165.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   63.]
 [   0.  758.    0.    0.   24.]
 [   2.    0. 1029.    0.   51.]
 [   0.    0.    0.  772.   56.]
 [   2.    2.    3.    1.  806.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.202 | Acc: 66.272% | Wgt Acc: 58.967% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   7.  24.  13.]
 [  0.  39.   4.   2.   4.]
 [  0.   8.  27.   0.   4.]
 [ 11.   5.   8.  50.   6.]
 [ 10.  22.  29.  10. 153.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 126
I - Training: 
	I - Batch: 50 | Loss: 0.078 | Acc: 94.125% | Wgt Acc: 98.239%
	I - Batch: 100 | Loss: 0.075 | Acc: 94.625% | Wgt Acc: 98.376%
	I - Batch: 150 | Loss: 0.070 | Acc: 95.208% | Wgt Acc: 98.605%
	I - Batch: 200 | Loss: 0.069 | Acc: 95.250% | Wgt Acc: 98.625%
	I - Batch: 250 | Loss: 0.067 | Acc: 95.300% | Wgt Acc: 98.581%
I - num batch: 273
I - Train -- Loss: 0.066 | Acc: 95.415% | Wgt Acc: 98.602% | LR: 1.250000e-04 | Dur: 167.78s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    0.   51.]
 [   1.  759.    0.    0.   33.]
 [   0.    0. 1030.    0.   53.]
 [   0.    1.    0.  772.   53.]
 [   5.    0.    2.    1.  810.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.282 | Acc: 66.075% | Wgt Acc: 58.194% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   0.   5.  10.   7.]
 [  0.  31.   4.   2.   3.]
 [  0.  11.  31.   1.   6.]
 [ 19.   8.  12.  59.   8.]
 [ 11.  28.  23.  14. 156.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 127
I - Training: 
	I - Batch: 50 | Loss: 0.057 | Acc: 96.000% | Wgt Acc: 98.853%
	I - Batch: 100 | Loss: 0.063 | Acc: 95.812% | Wgt Acc: 98.566%
	I - Batch: 150 | Loss: 0.065 | Acc: 95.667% | Wgt Acc: 98.490%
	I - Batch: 200 | Loss: 0.067 | Acc: 95.469% | Wgt Acc: 98.444%
	I - Batch: 250 | Loss: 0.067 | Acc: 95.275% | Wgt Acc: 98.446%
I - num batch: 273
I - Train -- Loss: 0.068 | Acc: 95.209% | Wgt Acc: 98.330% | LR: 1.250000e-04 | Dur: 163.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 787.    0.    0.    1.   68.]
 [   0.  760.    1.    1.   29.]
 [   0.    0. 1026.    0.   55.]
 [   0.    0.    0.  769.   37.]
 [  10.    0.    5.    2.  811.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.324 | Acc: 64.497% | Wgt Acc: 58.367% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   6.   3.  15.  22.]
 [  0.  30.   1.   1.   1.]
 [  0.   8.  28.   0.   6.]
 [ 20.   9.  11.  60.   6.]
 [  4.  25.  32.  10. 145.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 128
I - Training: 
	I - Batch: 50 | Loss: 0.069 | Acc: 93.750% | Wgt Acc: 98.397%
	I - Batch: 100 | Loss: 0.068 | Acc: 94.750% | Wgt Acc: 98.522%
	I - Batch: 150 | Loss: 0.067 | Acc: 95.125% | Wgt Acc: 98.584%
	I - Batch: 200 | Loss: 0.065 | Acc: 95.375% | Wgt Acc: 98.604%
	I - Batch: 250 | Loss: 0.064 | Acc: 95.450% | Wgt Acc: 98.591%
I - num batch: 273
I - Train -- Loss: 0.064 | Acc: 95.415% | Wgt Acc: 98.601% | LR: 1.250000e-04 | Dur: 169.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    0.   62.]
 [   0.  758.    0.    0.   26.]
 [   0.    0. 1031.    0.   51.]
 [   1.    0.    0.  772.   51.]
 [   5.    2.    1.    1.  810.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.318 | Acc: 64.103% | Wgt Acc: 59.532% | Dur: 14.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   7.   5.  14.  14.]
 [  0.  33.   4.   1.   5.]
 [  0.  17.  35.   0.  11.]
 [ 20.   5.   7.  60.  13.]
 [  8.  16.  24.  11. 137.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 129
I - Training: 
	I - Batch: 50 | Loss: 0.060 | Acc: 95.625% | Wgt Acc: 98.667%
	I - Batch: 100 | Loss: 0.061 | Acc: 95.750% | Wgt Acc: 98.740%
	I - Batch: 150 | Loss: 0.064 | Acc: 95.292% | Wgt Acc: 98.517%
	I - Batch: 200 | Loss: 0.064 | Acc: 95.438% | Wgt Acc: 98.588%
	I - Batch: 250 | Loss: 0.064 | Acc: 95.525% | Wgt Acc: 98.606%
I - num batch: 273
I - Train -- Loss: 0.063 | Acc: 95.598% | Wgt Acc: 98.621% | LR: 1.250000e-04 | Dur: 167.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    1.    0.    1.   63.]
 [   0.  757.    1.    0.   22.]
 [   0.    0. 1031.    0.   48.]
 [   1.    0.    0.  770.   48.]
 [   3.    2.    0.    2.  819.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.324 | Acc: 63.905% | Wgt Acc: 57.179% | Dur: 14.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   4.  18.  13.]
 [  0.  34.   4.   1.   2.]
 [  1.  14.  31.   1.  11.]
 [ 15.   4.   9.  50.   8.]
 [  9.  22.  27.  16. 146.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 130
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 95.500% | Wgt Acc: 98.488%
	I - Batch: 100 | Loss: 0.074 | Acc: 94.500% | Wgt Acc: 98.208%
	I - Batch: 150 | Loss: 0.069 | Acc: 94.875% | Wgt Acc: 98.267%
	I - Batch: 200 | Loss: 0.067 | Acc: 95.125% | Wgt Acc: 98.434%
	I - Batch: 250 | Loss: 0.067 | Acc: 95.000% | Wgt Acc: 98.375%
I - num batch: 273
I - Train -- Loss: 0.065 | Acc: 95.071% | Wgt Acc: 98.424% | LR: 1.250000e-04 | Dur: 164.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    1.    0.    1.   59.]
 [   0.  757.    0.    1.   33.]
 [   0.    0. 1028.    0.   57.]
 [   1.    0.    0.  771.   52.]
 [   4.    2.    4.    0.  799.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 65.680% | Wgt Acc: 59.670% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   1.   4.  14.  14.]
 [  0.  33.   3.   0.   2.]
 [  0.  14.  35.   2.  13.]
 [ 13.   6.   4.  55.   5.]
 [ 11.  24.  29.  15. 146.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 131
I - Training: 
	I - Batch: 50 | Loss: 0.058 | Acc: 95.625% | Wgt Acc: 98.852%
	I - Batch: 100 | Loss: 0.065 | Acc: 95.062% | Wgt Acc: 98.477%
	I - Batch: 150 | Loss: 0.063 | Acc: 95.375% | Wgt Acc: 98.639%
	I - Batch: 200 | Loss: 0.061 | Acc: 95.562% | Wgt Acc: 98.733%
	I - Batch: 250 | Loss: 0.060 | Acc: 95.625% | Wgt Acc: 98.731%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 95.621% | Wgt Acc: 98.719% | LR: 1.250000e-04 | Dur: 166.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    0.   54.]
 [   0.  759.    0.    0.   31.]
 [   0.    0. 1030.    0.   46.]
 [   2.    0.    0.  772.   53.]
 [   1.    1.    2.    1.  816.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.201 | Acc: 66.272% | Wgt Acc: 59.059% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   3.  11.   4.]
 [  0.  35.   4.   1.   3.]
 [  0.  13.  32.   2.  17.]
 [ 16.   5.   6.  56.   3.]
 [ 12.  22.  30.  16. 153.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 132
I - Training: 
	I - Batch: 50 | Loss: 0.064 | Acc: 95.125% | Wgt Acc: 98.572%
	I - Batch: 100 | Loss: 0.064 | Acc: 95.000% | Wgt Acc: 98.512%
	I - Batch: 150 | Loss: 0.061 | Acc: 95.542% | Wgt Acc: 98.720%
	I - Batch: 200 | Loss: 0.060 | Acc: 95.469% | Wgt Acc: 98.710%
	I - Batch: 250 | Loss: 0.060 | Acc: 95.625% | Wgt Acc: 98.751%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 95.552% | Wgt Acc: 98.722% | LR: 1.250000e-04 | Dur: 165.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    0.   67.]
 [   0.  760.    0.    0.   24.]
 [   0.    0. 1031.    0.   57.]
 [   0.    0.    0.  771.   40.]
 [   3.    0.    1.    2.  812.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.188 | Acc: 66.075% | Wgt Acc: 61.446% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   2.  10.  11.]
 [  0.  37.   2.   2.   4.]
 [  0.   9.  33.   2.  15.]
 [ 20.   7.   9.  62.   9.]
 [  6.  23.  29.  10. 141.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 133
I - Training: 
	I - Batch: 50 | Loss: 0.064 | Acc: 95.125% | Wgt Acc: 98.394%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.562% | Wgt Acc: 98.666%
	I - Batch: 150 | Loss: 0.060 | Acc: 95.917% | Wgt Acc: 98.828%
	I - Batch: 200 | Loss: 0.062 | Acc: 95.812% | Wgt Acc: 98.796%
	I - Batch: 250 | Loss: 0.062 | Acc: 95.700% | Wgt Acc: 98.725%
I - num batch: 273
I - Train -- Loss: 0.062 | Acc: 95.736% | Wgt Acc: 98.750% | LR: 1.250000e-04 | Dur: 167.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    1.   40.]
 [   0.  760.    0.    0.   30.]
 [   0.    0. 1030.    0.   54.]
 [   0.    0.    0.  772.   55.]
 [   4.    0.    2.    0.  821.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.164 | Acc: 65.878% | Wgt Acc: 60.858% | Dur: 14.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   4.  20.   8.]
 [  1.  43.   4.   0.   8.]
 [  2.  11.  29.   1.  15.]
 [ 14.   7.   8.  54.   7.]
 [  5.  14.  30.  11. 142.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773.  781.]

I - Epoch: 134
I - Training: 
	I - Batch: 50 | Loss: 0.051 | Acc: 96.750% | Wgt Acc: 98.862%
	I - Batch: 100 | Loss: 0.059 | Acc: 96.250% | Wgt Acc: 98.667%
	I - Batch: 150 | Loss: 0.060 | Acc: 96.083% | Wgt Acc: 98.720%
	I - Batch: 200 | Loss: 0.061 | Acc: 95.906% | Wgt Acc: 98.726%
	I - Batch: 250 | Loss: 0.059 | Acc: 96.000% | Wgt Acc: 98.777%
I - num batch: 259
I - Train -- Loss: 0.059 | Acc: 95.993% | Wgt Acc: 98.759% | LR: 1.250000e-04 | Dur: 157.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    1.    0.    0.   41.]
 [   0.  757.    1.    0.   30.]
 [   0.    0. 1030.    1.   46.]
 [   0.    1.    0.  770.   38.]
 [   3.    1.    1.    2.  626.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.223 | Acc: 65.680% | Wgt Acc: 58.517% | Dur: 14.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   1.   5.  11.  10.]
 [  0.  31.   2.   0.   4.]
 [  1.  12.  29.   1.   8.]
 [ 15.   5.   9.  60.   6.]
 [ 11.  29.  30.  14. 152.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 135
I - Training: 
	I - Batch: 50 | Loss: 0.057 | Acc: 94.875% | Wgt Acc: 98.312%
	I - Batch: 100 | Loss: 0.060 | Acc: 95.375% | Wgt Acc: 98.484%
	I - Batch: 150 | Loss: 0.059 | Acc: 95.542% | Wgt Acc: 98.638%
	I - Batch: 200 | Loss: 0.062 | Acc: 95.344% | Wgt Acc: 98.553%
	I - Batch: 250 | Loss: 0.062 | Acc: 95.400% | Wgt Acc: 98.571%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 95.484% | Wgt Acc: 98.615% | LR: 1.250000e-04 | Dur: 164.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    0.   63.]
 [   0.  757.    0.    0.   28.]
 [   0.    0. 1030.    1.   54.]
 [   1.    0.    0.  771.   42.]
 [   2.    3.    2.    1.  813.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.211 | Acc: 66.075% | Wgt Acc: 58.367% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   1.   3.  13.   3.]
 [  0.  32.   3.   0.   2.]
 [  2.  18.  35.   1.  12.]
 [ 15.   5.   6.  57.   8.]
 [ 15.  22.  28.  15. 155.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 136
I - Training: 
	I - Batch: 50 | Loss: 0.056 | Acc: 96.125% | Wgt Acc: 98.508%
	I - Batch: 100 | Loss: 0.061 | Acc: 95.875% | Wgt Acc: 98.510%
	I - Batch: 150 | Loss: 0.062 | Acc: 95.583% | Wgt Acc: 98.579%
	I - Batch: 200 | Loss: 0.060 | Acc: 95.562% | Wgt Acc: 98.614%
	I - Batch: 250 | Loss: 0.059 | Acc: 95.725% | Wgt Acc: 98.633%
I - num batch: 273
I - Train -- Loss: 0.059 | Acc: 95.713% | Wgt Acc: 98.609% | LR: 1.250000e-04 | Dur: 166.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    0.    0.   60.]
 [   1.  758.    0.    0.   27.]
 [   0.    0. 1032.    0.   41.]
 [   0.    0.    0.  770.   46.]
 [   7.    2.    0.    3.  826.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.212 | Acc: 65.878% | Wgt Acc: 61.285% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   4.  13.   9.]
 [  0.  32.   2.   1.   3.]
 [  1.  17.  41.   3.  19.]
 [ 18.   7.  12.  60.   9.]
 [  8.  19.  16.   9. 140.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 137
I - Training: 
	I - Batch: 50 | Loss: 0.058 | Acc: 96.125% | Wgt Acc: 99.013%
	I - Batch: 100 | Loss: 0.057 | Acc: 96.250% | Wgt Acc: 99.051%
	I - Batch: 150 | Loss: 0.061 | Acc: 95.708% | Wgt Acc: 98.864%
	I - Batch: 200 | Loss: 0.063 | Acc: 95.594% | Wgt Acc: 98.780%
	I - Batch: 250 | Loss: 0.062 | Acc: 95.725% | Wgt Acc: 98.782%
I - num batch: 273
I - Train -- Loss: 0.062 | Acc: 95.736% | Wgt Acc: 98.794% | LR: 1.250000e-04 | Dur: 164.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   61.]
 [   1.  759.    0.    0.   21.]
 [   0.    0. 1032.    0.   55.]
 [   0.    0.    0.  773.   44.]
 [   3.    1.    0.    0.  819.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.319 | Acc: 62.919% | Wgt Acc: 56.556% | Dur: 15.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   4.   6.  25.  22.]
 [  0.  27.   3.   1.   3.]
 [  1.  13.  30.   1.   8.]
 [  9.   5.  11.  48.   5.]
 [  6.  29.  25.  11. 142.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 138
I - Training: 
	I - Batch: 50 | Loss: 0.055 | Acc: 96.000% | Wgt Acc: 98.846%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.875% | Wgt Acc: 98.804%
	I - Batch: 150 | Loss: 0.060 | Acc: 95.917% | Wgt Acc: 98.783%
	I - Batch: 200 | Loss: 0.060 | Acc: 96.031% | Wgt Acc: 98.848%
	I - Batch: 250 | Loss: 0.061 | Acc: 96.075% | Wgt Acc: 98.820%
I - num batch: 273
I - Train -- Loss: 0.062 | Acc: 95.965% | Wgt Acc: 98.805% | LR: 1.250000e-04 | Dur: 165.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    1.   58.]
 [   0.  759.    0.    0.   22.]
 [   0.    1. 1031.    0.   49.]
 [   1.    0.    0.  771.   40.]
 [   2.    0.    1.    1.  831.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.159 | Acc: 66.272% | Wgt Acc: 60.120% | Dur: 13.88s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.  11.   4.]
 [  0.  37.   3.   1.   4.]
 [  2.  14.  41.   3.  15.]
 [ 23.   5.   6.  56.   9.]
 [  9.  20.  23.  15. 148.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 139
I - Training: 
	I - Batch: 50 | Loss: 0.069 | Acc: 95.000% | Wgt Acc: 98.574%
	I - Batch: 100 | Loss: 0.067 | Acc: 95.125% | Wgt Acc: 98.497%
	I - Batch: 150 | Loss: 0.074 | Acc: 94.708% | Wgt Acc: 98.336%
	I - Batch: 200 | Loss: 0.074 | Acc: 94.969% | Wgt Acc: 98.369%
	I - Batch: 250 | Loss: 0.073 | Acc: 95.075% | Wgt Acc: 98.398%
I - num batch: 273
I - Train -- Loss: 0.072 | Acc: 94.911% | Wgt Acc: 98.387% | LR: 1.250000e-04 | Dur: 165.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    1.    0.   59.]
 [   0.  757.    0.    0.   32.]
 [   2.    0. 1029.    0.   64.]
 [   0.    0.    0.  773.   53.]
 [   6.    3.    2.    0.  792.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.168 | Acc: 68.639% | Wgt Acc: 64.848% | Dur: 14.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   5.   3.  11.   7.]
 [  0.  39.   1.   1.   5.]
 [  2.  13.  54.   2.  18.]
 [ 20.   8.   5.  59.   9.]
 [ 11.  13.  12.  13. 141.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 140
I - Training: 
	I - Batch: 50 | Loss: 0.071 | Acc: 95.250% | Wgt Acc: 98.289%
	I - Batch: 100 | Loss: 0.066 | Acc: 95.312% | Wgt Acc: 98.441%
	I - Batch: 150 | Loss: 0.069 | Acc: 95.167% | Wgt Acc: 98.425%
	I - Batch: 200 | Loss: 0.065 | Acc: 95.438% | Wgt Acc: 98.550%
	I - Batch: 250 | Loss: 0.066 | Acc: 95.425% | Wgt Acc: 98.556%
I - num batch: 273
I - Train -- Loss: 0.067 | Acc: 95.300% | Wgt Acc: 98.545% | LR: 1.250000e-04 | Dur: 166.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   71.]
 [   0.  758.    0.    1.   30.]
 [   0.    1. 1030.    0.   55.]
 [   1.    0.    0.  770.   38.]
 [   3.    1.    2.    2.  806.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.264 | Acc: 64.694% | Wgt Acc: 58.690% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   4.   3.  13.  10.]
 [  0.  33.   1.   1.   2.]
 [  0.  10.  28.   1.  12.]
 [ 18.   6.  11.  61.  11.]
 [  9.  25.  32.  10. 145.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 141
I - Training: 
	I - Batch: 50 | Loss: 0.067 | Acc: 95.750% | Wgt Acc: 98.411%
	I - Batch: 100 | Loss: 0.066 | Acc: 95.875% | Wgt Acc: 98.571%
	I - Batch: 150 | Loss: 0.069 | Acc: 95.417% | Wgt Acc: 98.543%
	I - Batch: 200 | Loss: 0.067 | Acc: 95.406% | Wgt Acc: 98.524%
	I - Batch: 250 | Loss: 0.071 | Acc: 95.300% | Wgt Acc: 98.454%
I - num batch: 273
I - Train -- Loss: 0.072 | Acc: 95.186% | Wgt Acc: 98.408% | LR: 1.250000e-04 | Dur: 164.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    2.   57.]
 [   0.  757.    0.    0.   30.]
 [   0.    0. 1028.    1.   59.]
 [   2.    0.    1.  770.   48.]
 [   4.    3.    3.    0.  806.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.224 | Acc: 66.075% | Wgt Acc: 60.547% | Dur: 14.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   1.   0.   8.   6.]
 [  0.  35.   6.   2.   6.]
 [  4.  16.  38.   3.  17.]
 [ 16.   7.   6.  60.   6.]
 [ 11.  19.  25.  13. 145.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 142
I - Training: 
	I - Batch: 50 | Loss: 0.057 | Acc: 95.625% | Wgt Acc: 98.794%
	I - Batch: 100 | Loss: 0.061 | Acc: 95.500% | Wgt Acc: 98.547%
	I - Batch: 150 | Loss: 0.059 | Acc: 95.833% | Wgt Acc: 98.745%
	I - Batch: 200 | Loss: 0.062 | Acc: 95.750% | Wgt Acc: 98.705%
	I - Batch: 250 | Loss: 0.065 | Acc: 95.575% | Wgt Acc: 98.576%
I - num batch: 273
I - Train -- Loss: 0.066 | Acc: 95.575% | Wgt Acc: 98.576% | LR: 1.250000e-04 | Dur: 168.88s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    1.   34.]
 [   2.  758.    0.    0.   37.]
 [   0.    1. 1028.    0.   63.]
 [   0.    0.    0.  771.   46.]
 [   3.    1.    4.    1.  820.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.184 | Acc: 64.300% | Wgt Acc: 58.886% | Dur: 14.77s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   3.  13.  12.]
 [  1.  36.   2.   2.   3.]
 [  1.  17.  33.   2.  13.]
 [ 17.   7.   8.  55.  11.]
 [  8.  15.  29.  14. 141.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 143
I - Training: 
	I - Batch: 50 | Loss: 0.062 | Acc: 95.750% | Wgt Acc: 98.785%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.438% | Wgt Acc: 98.526%
	I - Batch: 150 | Loss: 0.060 | Acc: 95.625% | Wgt Acc: 98.559%
	I - Batch: 200 | Loss: 0.059 | Acc: 95.469% | Wgt Acc: 98.541%
	I - Batch: 250 | Loss: 0.064 | Acc: 95.175% | Wgt Acc: 98.445%
I - num batch: 273
I - Train -- Loss: 0.064 | Acc: 95.186% | Wgt Acc: 98.452% | LR: 1.250000e-04 | Dur: 166.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    1.    0.   61.]
 [   0.  758.    0.    0.   26.]
 [   0.    0. 1026.    0.   58.]
 [   0.    0.    0.  770.   51.]
 [   3.    2.    5.    3.  804.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.230 | Acc: 65.286% | Wgt Acc: 61.077% | Dur: 14.80s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   5.  18.  15.]
 [  1.  35.   2.   0.   6.]
 [  3.  13.  41.   1.  15.]
 [ 17.   8.   6.  57.   7.]
 [  6.  20.  21.  10. 137.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 144
I - Training: 
	I - Batch: 50 | Loss: 0.061 | Acc: 96.375% | Wgt Acc: 98.695%
	I - Batch: 100 | Loss: 0.067 | Acc: 95.250% | Wgt Acc: 98.474%
	I - Batch: 150 | Loss: 0.065 | Acc: 95.417% | Wgt Acc: 98.538%
	I - Batch: 200 | Loss: 0.063 | Acc: 95.469% | Wgt Acc: 98.589%
	I - Batch: 250 | Loss: 0.063 | Acc: 95.550% | Wgt Acc: 98.565%
I - num batch: 273
I - Train -- Loss: 0.063 | Acc: 95.438% | Wgt Acc: 98.560% | LR: 1.250000e-04 | Dur: 166.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    1.    0.    0.   64.]
 [   0.  758.    1.    0.   33.]
 [   0.    0. 1029.    0.   46.]
 [   0.    0.    0.  770.   44.]
 [   4.    1.    2.    3.  813.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.206 | Acc: 65.089% | Wgt Acc: 56.187% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   1.  13.   5.]
 [  0.  34.   3.   0.   2.]
 [  1.  15.  38.   4.  12.]
 [ 14.   4.   5.  46.   3.]
 [ 19.  23.  28.  23. 158.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 145
I - Training: 
	I - Batch: 50 | Loss: 0.053 | Acc: 96.000% | Wgt Acc: 98.734%
	I - Batch: 100 | Loss: 0.055 | Acc: 95.750% | Wgt Acc: 98.675%
	I - Batch: 150 | Loss: 0.056 | Acc: 95.708% | Wgt Acc: 98.619%
	I - Batch: 200 | Loss: 0.056 | Acc: 95.906% | Wgt Acc: 98.671%
	I - Batch: 250 | Loss: 0.056 | Acc: 96.050% | Wgt Acc: 98.759%
I - num batch: 273
I - Train -- Loss: 0.056 | Acc: 96.034% | Wgt Acc: 98.755% | LR: 1.250000e-04 | Dur: 165.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    1.   52.]
 [   0.  759.    0.    0.   21.]
 [   0.    0. 1031.    0.   51.]
 [   1.    0.    0.  769.   39.]
 [   3.    1.    1.    3.  837.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.236 | Acc: 67.258% | Wgt Acc: 64.087% | Dur: 14.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   2.  11.  13.]
 [  1.  40.   9.   2.   7.]
 [  2.  13.  42.   3.  13.]
 [ 17.   5.   5.  61.  11.]
 [  6.  17.  17.   9. 136.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 146
I - Training: 
	I - Batch: 50 | Loss: 0.047 | Acc: 97.125% | Wgt Acc: 99.143%
	I - Batch: 100 | Loss: 0.049 | Acc: 96.688% | Wgt Acc: 99.027%
	I - Batch: 150 | Loss: 0.054 | Acc: 96.458% | Wgt Acc: 98.888%
	I - Batch: 200 | Loss: 0.055 | Acc: 96.344% | Wgt Acc: 98.849%
	I - Batch: 250 | Loss: 0.057 | Acc: 96.025% | Wgt Acc: 98.762%
I - num batch: 273
I - Train -- Loss: 0.058 | Acc: 95.873% | Wgt Acc: 98.697% | LR: 1.250000e-04 | Dur: 170.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    1.   55.]
 [   0.  759.    0.    0.   27.]
 [   0.    0. 1029.    0.   42.]
 [   0.    0.    0.  771.   45.]
 [   5.    1.    3.    1.  831.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.245 | Acc: 63.905% | Wgt Acc: 58.990% | Dur: 14.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   5.  11.  11.]
 [  0.  31.   2.   0.   4.]
 [  1.  12.  38.   2.  13.]
 [ 21.   4.   4.  58.  14.]
 [  7.  29.  26.  15. 138.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 147
I - Training: 
	I - Batch: 50 | Loss: 0.065 | Acc: 96.000% | Wgt Acc: 98.461%
	I - Batch: 100 | Loss: 0.069 | Acc: 95.062% | Wgt Acc: 98.172%
	I - Batch: 150 | Loss: 0.074 | Acc: 94.833% | Wgt Acc: 98.127%
	I - Batch: 200 | Loss: 0.070 | Acc: 95.188% | Wgt Acc: 98.242%
	I - Batch: 250 | Loss: 0.068 | Acc: 95.375% | Wgt Acc: 98.401%
I - num batch: 273
I - Train -- Loss: 0.068 | Acc: 95.346% | Wgt Acc: 98.405% | LR: 1.250000e-04 | Dur: 169.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 788.    1.    1.    0.   57.]
 [   0.  757.    0.    0.   34.]
 [   1.    0. 1030.    1.   52.]
 [   0.    0.    1.  769.   42.]
 [   8.    2.    0.    3.  815.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.237 | Acc: 63.116% | Wgt Acc: 57.560% | Dur: 15.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   1.   3.  17.  15.]
 [  0.  37.   9.   3.   5.]
 [  1.   9.  27.   0.   5.]
 [ 20.  11.   9.  57.  15.]
 [  8.  20.  27.   9. 140.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 148
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 95.750% | Wgt Acc: 98.623%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.812% | Wgt Acc: 98.675%
	I - Batch: 150 | Loss: 0.058 | Acc: 95.958% | Wgt Acc: 98.800%
	I - Batch: 200 | Loss: 0.060 | Acc: 95.562% | Wgt Acc: 98.681%
	I - Batch: 250 | Loss: 0.058 | Acc: 95.750% | Wgt Acc: 98.737%
I - num batch: 273
I - Train -- Loss: 0.058 | Acc: 95.805% | Wgt Acc: 98.763% | LR: 1.250000e-04 | Dur: 165.72s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    0.    0.   44.]
 [   0.  758.    0.    0.   39.]
 [   0.    0. 1031.    0.   44.]
 [   1.    0.    0.  771.   49.]
 [   1.    2.    1.    2.  824.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.218 | Acc: 68.639% | Wgt Acc: 63.361% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   5.   6.  12.  13.]
 [  1.  38.   7.   1.   4.]
 [  0.  11.  34.   1.   9.]
 [ 11.   4.   6.  58.   6.]
 [  6.  20.  22.  14. 148.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 149
I - Training: 
	I - Batch: 50 | Loss: 0.062 | Acc: 95.000% | Wgt Acc: 98.382%
	I - Batch: 100 | Loss: 0.061 | Acc: 95.250% | Wgt Acc: 98.376%
	I - Batch: 150 | Loss: 0.059 | Acc: 95.708% | Wgt Acc: 98.627%
	I - Batch: 200 | Loss: 0.061 | Acc: 95.531% | Wgt Acc: 98.561%
	I - Batch: 250 | Loss: 0.061 | Acc: 95.575% | Wgt Acc: 98.601%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 95.575% | Wgt Acc: 98.622% | LR: 1.250000e-04 | Dur: 162.83s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    1.    0.   54.]
 [   0.  757.    0.    0.   40.]
 [   1.    1. 1031.    0.   44.]
 [   0.    0.    0.  773.   44.]
 [   6.    2.    0.    0.  818.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.186 | Acc: 66.667% | Wgt Acc: 61.054% | Dur: 14.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   2.  12.   5.]
 [  0.  34.   1.   1.   4.]
 [  2.  17.  41.   3.  18.]
 [ 16.   8.   8.  57.   7.]
 [ 10.  16.  23.  13. 146.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 150
I - Training: 
	I - Batch: 50 | Loss: 0.042 | Acc: 97.750% | Wgt Acc: 99.436%
	I - Batch: 100 | Loss: 0.047 | Acc: 96.438% | Wgt Acc: 98.969%
	I - Batch: 150 | Loss: 0.052 | Acc: 96.250% | Wgt Acc: 98.918%
	I - Batch: 200 | Loss: 0.057 | Acc: 95.875% | Wgt Acc: 98.736%
	I - Batch: 250 | Loss: 0.060 | Acc: 95.675% | Wgt Acc: 98.672%
I - num batch: 273
I - Train -- Loss: 0.062 | Acc: 95.575% | Wgt Acc: 98.644% | LR: 1.250000e-04 | Dur: 165.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    0.   61.]
 [   0.  759.    0.    0.   30.]
 [   0.    0. 1030.    0.   57.]
 [   1.    0.    0.  772.   35.]
 [   5.    1.    2.    1.  817.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.273 | Acc: 65.680% | Wgt Acc: 60.281% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   4.   9.   7.]
 [  0.  38.   3.   2.   5.]
 [  2.  12.  32.   1.  12.]
 [ 16.   8.   8.  61.  12.]
 [ 12.  17.  28.  13. 144.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 151
I - Training: 
	I - Batch: 50 | Loss: 0.056 | Acc: 96.375% | Wgt Acc: 98.946%
	I - Batch: 100 | Loss: 0.056 | Acc: 96.188% | Wgt Acc: 98.838%
	I - Batch: 150 | Loss: 0.056 | Acc: 95.875% | Wgt Acc: 98.775%
	I - Batch: 200 | Loss: 0.056 | Acc: 96.156% | Wgt Acc: 98.791%
	I - Batch: 250 | Loss: 0.054 | Acc: 96.200% | Wgt Acc: 98.832%
I - num batch: 273
I - Train -- Loss: 0.054 | Acc: 96.171% | Wgt Acc: 98.843% | LR: 1.250000e-04 | Dur: 169.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    0.    0.   52.]
 [   1.  760.    0.    0.   25.]
 [   1.    0. 1031.    0.   47.]
 [   1.    0.    0.  773.   35.]
 [   4.    0.    1.    0.  841.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.236 | Acc: 67.258% | Wgt Acc: 62.496% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   4.   8.  17.  12.]
 [  2.  32.   5.   2.   2.]
 [  0.  17.  37.   0.  17.]
 [ 11.   8.   5.  59.   6.]
 [  5.  17.  20.   8. 143.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 152
I - Training: 
	I - Batch: 50 | Loss: 0.055 | Acc: 96.750% | Wgt Acc: 98.927%
	I - Batch: 100 | Loss: 0.050 | Acc: 96.438% | Wgt Acc: 98.968%
	I - Batch: 150 | Loss: 0.053 | Acc: 96.167% | Wgt Acc: 98.905%
	I - Batch: 200 | Loss: 0.054 | Acc: 96.000% | Wgt Acc: 98.788%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.100% | Wgt Acc: 98.800%
I - num batch: 273
I - Train -- Loss: 0.056 | Acc: 96.011% | Wgt Acc: 98.751% | LR: 1.250000e-04 | Dur: 173.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    1.    1.   56.]
 [   0.  757.    0.    0.   26.]
 [   0.    1. 1031.    1.   45.]
 [   1.    0.    0.  771.   37.]
 [   3.    2.    0.    0.  836.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.183 | Acc: 68.639% | Wgt Acc: 62.657% | Dur: 14.77s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   3.   5.  16.  15.]
 [  0.  37.   2.   1.   2.]
 [  0.   8.  34.   1.   6.]
 [ 11.  11.  10.  55.   6.]
 [  6.  19.  24.  13. 151.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 153
I - Training: 
	I - Batch: 50 | Loss: 0.049 | Acc: 96.625% | Wgt Acc: 99.128%
	I - Batch: 100 | Loss: 0.058 | Acc: 95.438% | Wgt Acc: 98.766%
	I - Batch: 150 | Loss: 0.064 | Acc: 95.250% | Wgt Acc: 98.520%
	I - Batch: 200 | Loss: 0.062 | Acc: 95.500% | Wgt Acc: 98.564%
	I - Batch: 250 | Loss: 0.062 | Acc: 95.450% | Wgt Acc: 98.516%
I - num batch: 273
I - Train -- Loss: 0.062 | Acc: 95.507% | Wgt Acc: 98.554% | LR: 1.250000e-04 | Dur: 175.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    1.    0.    1.   59.]
 [   1.  758.    0.    1.   25.]
 [   1.    0. 1029.    0.   51.]
 [   0.    0.    0.  769.   48.]
 [   2.    1.    3.    2.  817.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.210 | Acc: 67.653% | Wgt Acc: 62.945% | Dur: 15.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   3.   3.  19.  13.]
 [  1.  39.   4.   1.   4.]
 [  1.  14.  40.   1.  13.]
 [  7.   6.   7.  52.   7.]
 [ 10.  16.  21.  13. 143.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 154
I - Training: 
	I - Batch: 50 | Loss: 0.054 | Acc: 96.125% | Wgt Acc: 98.890%
	I - Batch: 100 | Loss: 0.050 | Acc: 96.875% | Wgt Acc: 99.077%
	I - Batch: 150 | Loss: 0.054 | Acc: 96.250% | Wgt Acc: 98.865%
	I - Batch: 200 | Loss: 0.054 | Acc: 96.375% | Wgt Acc: 98.945%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.200% | Wgt Acc: 98.899%
I - num batch: 273
I - Train -- Loss: 0.055 | Acc: 96.194% | Wgt Acc: 98.862% | LR: 1.250000e-04 | Dur: 165.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    0.   39.]
 [   0.  757.    0.    0.   30.]
 [   0.    0. 1031.    0.   51.]
 [   0.    0.    0.  771.   39.]
 [   1.    3.    1.    2.  841.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.243 | Acc: 65.878% | Wgt Acc: 62.415% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   5.  12.  15.]
 [  0.  36.   7.   2.   4.]
 [  1.  16.  37.   3.  16.]
 [ 17.   9.   7.  62.  10.]
 [  6.  15.  19.   7. 135.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 155
I - Training: 
	I - Batch: 50 | Loss: 0.056 | Acc: 96.500% | Wgt Acc: 99.110%
	I - Batch: 100 | Loss: 0.056 | Acc: 95.875% | Wgt Acc: 98.890%
	I - Batch: 150 | Loss: 0.057 | Acc: 95.958% | Wgt Acc: 98.887%
	I - Batch: 200 | Loss: 0.057 | Acc: 96.031% | Wgt Acc: 98.863%
	I - Batch: 250 | Loss: 0.060 | Acc: 95.650% | Wgt Acc: 98.663%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 95.598% | Wgt Acc: 98.668% | LR: 1.250000e-04 | Dur: 162.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    1.    0.   49.]
 [   0.  759.    2.    2.   33.]
 [   0.    0. 1029.    0.   52.]
 [   0.    1.    0.  771.   49.]
 [   3.    0.    0.    0.  817.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.279 | Acc: 65.680% | Wgt Acc: 57.352% | Dur: 13.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   6.   3.  11.   5.]
 [  0.  28.   1.   0.   3.]
 [  2.  14.  37.   0.  11.]
 [ 13.   5.   3.  56.   4.]
 [ 18.  25.  31.  19. 157.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 156
I - Training: 
	I - Batch: 50 | Loss: 0.058 | Acc: 95.750% | Wgt Acc: 98.905%
	I - Batch: 100 | Loss: 0.056 | Acc: 96.000% | Wgt Acc: 98.858%
	I - Batch: 150 | Loss: 0.056 | Acc: 96.042% | Wgt Acc: 98.832%
	I - Batch: 200 | Loss: 0.058 | Acc: 96.000% | Wgt Acc: 98.799%
	I - Batch: 250 | Loss: 0.060 | Acc: 95.750% | Wgt Acc: 98.687%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 95.782% | Wgt Acc: 98.692% | LR: 1.250000e-04 | Dur: 164.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    0.    0.   56.]
 [   0.  757.    0.    0.   30.]
 [   0.    0. 1029.    0.   46.]
 [   0.    0.    0.  771.   42.]
 [   2.    3.    3.    2.  826.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.260 | Acc: 67.456% | Wgt Acc: 61.527% | Dur: 13.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   5.  12.  10.]
 [  0.  31.   1.   1.   1.]
 [  2.  14.  39.   1.   9.]
 [ 19.   7.   8.  60.  11.]
 [  4.  23.  22.  12. 149.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 157
I - Training: 
	I - Batch: 50 | Loss: 0.054 | Acc: 96.375% | Wgt Acc: 98.825%
	I - Batch: 100 | Loss: 0.054 | Acc: 96.312% | Wgt Acc: 98.802%
	I - Batch: 150 | Loss: 0.053 | Acc: 96.500% | Wgt Acc: 98.856%
	I - Batch: 200 | Loss: 0.055 | Acc: 96.156% | Wgt Acc: 98.770%
	I - Batch: 250 | Loss: 0.058 | Acc: 95.925% | Wgt Acc: 98.685%
I - num batch: 273
I - Train -- Loss: 0.057 | Acc: 96.057% | Wgt Acc: 98.742% | LR: 1.250000e-04 | Dur: 164.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    0.   42.]
 [   0.  757.    1.    0.   22.]
 [   0.    0. 1029.    0.   50.]
 [   0.    1.    0.  771.   47.]
 [   3.    2.    2.    2.  839.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.259 | Acc: 64.892% | Wgt Acc: 59.601% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   3.   7.  22.  15.]
 [  0.  33.   3.   0.   3.]
 [  0.  13.  33.   2.  12.]
 [ 13.  10.   6.  53.   9.]
 [  6.  19.  26.   9. 141.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 158
I - Training: 
	I - Batch: 50 | Loss: 0.056 | Acc: 96.000% | Wgt Acc: 98.712%
	I - Batch: 100 | Loss: 0.059 | Acc: 95.875% | Wgt Acc: 98.605%
	I - Batch: 150 | Loss: 0.060 | Acc: 95.708% | Wgt Acc: 98.607%
	I - Batch: 200 | Loss: 0.061 | Acc: 95.719% | Wgt Acc: 98.598%
	I - Batch: 250 | Loss: 0.062 | Acc: 95.725% | Wgt Acc: 98.559%
I - num batch: 273
I - Train -- Loss: 0.063 | Acc: 95.736% | Wgt Acc: 98.592% | LR: 1.250000e-04 | Dur: 164.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    0.   53.]
 [   0.  756.    0.    1.   31.]
 [   1.    0. 1030.    0.   47.]
 [   2.    0.    0.  770.   41.]
 [   2.    4.    2.    2.  828.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.304 | Acc: 63.314% | Wgt Acc: 60.039% | Dur: 14.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   6.  23.  20.]
 [  1.  33.   2.   3.   9.]
 [  1.  20.  44.   2.  18.]
 [ 11.   8.   5.  47.   5.]
 [  6.  13.  18.  11. 128.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 159
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 94.750% | Wgt Acc: 98.170%
	I - Batch: 100 | Loss: 0.065 | Acc: 95.125% | Wgt Acc: 98.368%
	I - Batch: 150 | Loss: 0.061 | Acc: 95.458% | Wgt Acc: 98.590%
	I - Batch: 200 | Loss: 0.061 | Acc: 95.344% | Wgt Acc: 98.471%
	I - Batch: 250 | Loss: 0.062 | Acc: 95.525% | Wgt Acc: 98.581%
I - num batch: 273
I - Train -- Loss: 0.062 | Acc: 95.438% | Wgt Acc: 98.538% | LR: 1.250000e-04 | Dur: 164.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   57.]
 [   0.  758.    0.    1.   26.]
 [   0.    1. 1028.    0.   55.]
 [   0.    0.    1.  770.   48.]
 [   4.    1.    3.    2.  814.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.248 | Acc: 65.286% | Wgt Acc: 59.751% | Dur: 14.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   2.  12.   9.]
 [  0.  34.   4.   1.   6.]
 [  2.  15.  34.   0.  14.]
 [ 21.   7.  11.  63.   7.]
 [  9.  20.  24.  10. 144.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 160
I - Training: 
	I - Batch: 50 | Loss: 0.057 | Acc: 95.250% | Wgt Acc: 98.677%
	I - Batch: 100 | Loss: 0.056 | Acc: 95.938% | Wgt Acc: 98.854%
	I - Batch: 150 | Loss: 0.054 | Acc: 96.167% | Wgt Acc: 98.867%
	I - Batch: 200 | Loss: 0.055 | Acc: 96.188% | Wgt Acc: 98.813%
	I - Batch: 250 | Loss: 0.059 | Acc: 95.825% | Wgt Acc: 98.682%
I - num batch: 273
I - Train -- Loss: 0.059 | Acc: 95.759% | Wgt Acc: 98.664% | LR: 1.250000e-04 | Dur: 163.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    1.   62.]
 [   0.  758.    0.    0.   31.]
 [   0.    0. 1029.    0.   48.]
 [   0.    0.    0.  770.   33.]
 [   3.    2.    3.    2.  826.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.208 | Acc: 67.850% | Wgt Acc: 61.089% | Dur: 14.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   1.   4.   6.   6.]
 [  0.  33.   1.   1.   2.]
 [  1.  15.  35.   1.  10.]
 [ 17.   6.   6.  62.   8.]
 [ 10.  23.  29.  16. 154.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773.  781.]

I - Epoch: 161
I - Training: 
	I - Batch: 50 | Loss: 0.043 | Acc: 97.375% | Wgt Acc: 99.354%
	I - Batch: 100 | Loss: 0.044 | Acc: 97.438% | Wgt Acc: 99.258%
	I - Batch: 150 | Loss: 0.045 | Acc: 97.208% | Wgt Acc: 99.240%
	I - Batch: 200 | Loss: 0.045 | Acc: 97.125% | Wgt Acc: 99.232%
	I - Batch: 250 | Loss: 0.044 | Acc: 97.300% | Wgt Acc: 99.286%
I - num batch: 259
I - Train -- Loss: 0.045 | Acc: 97.273% | Wgt Acc: 99.281% | LR: 1.250000e-04 | Dur: 157.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    0.   32.]
 [   0.  760.    0.    0.   12.]
 [   0.    0. 1031.    0.   37.]
 [   0.    0.    0.  773.   30.]
 [   1.    0.    1.    0.  670.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.288 | Acc: 67.850% | Wgt Acc: 60.247% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   1.   3.  12.   3.]
 [  0.  28.   1.   2.   3.]
 [  1.  22.  43.   2.  11.]
 [ 14.   6.   5.  55.   6.]
 [ 12.  21.  23.  15. 157.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 162
I - Training: 
	I - Batch: 50 | Loss: 0.054 | Acc: 96.250% | Wgt Acc: 98.799%
	I - Batch: 100 | Loss: 0.052 | Acc: 96.438% | Wgt Acc: 98.961%
	I - Batch: 150 | Loss: 0.051 | Acc: 96.625% | Wgt Acc: 99.050%
	I - Batch: 200 | Loss: 0.054 | Acc: 96.406% | Wgt Acc: 98.953%
	I - Batch: 250 | Loss: 0.053 | Acc: 96.425% | Wgt Acc: 98.961%
I - num batch: 273
I - Train -- Loss: 0.052 | Acc: 96.447% | Wgt Acc: 98.977% | LR: 1.250000e-04 | Dur: 164.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   48.]
 [   0.  759.    0.    0.   24.]
 [   0.    0. 1032.    0.   40.]
 [   0.    0.    0.  773.   38.]
 [   4.    1.    0.    0.  850.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.162 | Acc: 66.469% | Wgt Acc: 60.028% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   3.  16.   6.]
 [  0.  30.   5.   3.   2.]
 [  1.  18.  42.   2.  17.]
 [ 15.   6.   3.  53.   6.]
 [  9.  21.  22.  12. 149.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 163
I - Training: 
	I - Batch: 50 | Loss: 0.051 | Acc: 96.500% | Wgt Acc: 98.983%
	I - Batch: 100 | Loss: 0.051 | Acc: 96.938% | Wgt Acc: 99.105%
	I - Batch: 150 | Loss: 0.054 | Acc: 96.833% | Wgt Acc: 99.029%
	I - Batch: 200 | Loss: 0.053 | Acc: 96.719% | Wgt Acc: 99.008%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.375% | Wgt Acc: 98.858%
I - num batch: 273
I - Train -- Loss: 0.056 | Acc: 96.309% | Wgt Acc: 98.811% | LR: 1.250000e-04 | Dur: 170.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    0.   49.]
 [   0.  759.    0.    0.   25.]
 [   0.    0. 1029.    0.   40.]
 [   1.    0.    0.  772.   36.]
 [   5.    1.    3.    1.  850.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.224 | Acc: 64.694% | Wgt Acc: 60.697% | Dur: 14.97s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   4.  11.  12.]
 [  0.  37.   7.   0.   5.]
 [  1.  15.  35.   0.  14.]
 [ 21.   7.  14.  65.  13.]
 [ 11.  17.  15.  10. 136.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 164
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 95.125% | Wgt Acc: 98.498%
	I - Batch: 100 | Loss: 0.052 | Acc: 96.062% | Wgt Acc: 98.877%
	I - Batch: 150 | Loss: 0.051 | Acc: 96.167% | Wgt Acc: 98.858%
	I - Batch: 200 | Loss: 0.054 | Acc: 96.125% | Wgt Acc: 98.829%
	I - Batch: 250 | Loss: 0.056 | Acc: 95.850% | Wgt Acc: 98.720%
I - num batch: 273
I - Train -- Loss: 0.057 | Acc: 95.851% | Wgt Acc: 98.716% | LR: 1.250000e-04 | Dur: 165.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    0.    0.   75.]
 [   1.  758.    0.    0.   19.]
 [   1.    0. 1032.    0.   43.]
 [   2.    0.    0.  773.   34.]
 [   4.    2.    0.    0.  829.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.232 | Acc: 65.878% | Wgt Acc: 58.067% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   4.   9.   6.]
 [  0.  33.   1.   0.   2.]
 [  1.  17.  30.   0.  12.]
 [ 15.   4.   8.  55.   5.]
 [ 11.  21.  32.  22. 155.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 165
I - Training: 
	I - Batch: 50 | Loss: 0.059 | Acc: 96.375% | Wgt Acc: 98.866%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.688% | Wgt Acc: 98.551%
	I - Batch: 150 | Loss: 0.060 | Acc: 95.833% | Wgt Acc: 98.693%
	I - Batch: 200 | Loss: 0.062 | Acc: 95.906% | Wgt Acc: 98.685%
	I - Batch: 250 | Loss: 0.061 | Acc: 96.000% | Wgt Acc: 98.711%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 96.034% | Wgt Acc: 98.740% | LR: 1.250000e-04 | Dur: 164.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    1.    0.   57.]
 [   0.  759.    1.    0.   27.]
 [   0.    0. 1029.    0.   50.]
 [   0.    0.    0.  772.   28.]
 [   6.    1.    1.    1.  838.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.254 | Acc: 65.286% | Wgt Acc: 57.191% | Dur: 13.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   4.  19.   7.]
 [  0.  28.   3.   1.   5.]
 [  3.  14.  30.   0.   8.]
 [ 12.   5.   7.  54.   5.]
 [  9.  26.  31.  12. 155.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 166
I - Training: 
	I - Batch: 50 | Loss: 0.060 | Acc: 95.500% | Wgt Acc: 98.581%
	I - Batch: 100 | Loss: 0.059 | Acc: 95.875% | Wgt Acc: 98.753%
	I - Batch: 150 | Loss: 0.058 | Acc: 95.667% | Wgt Acc: 98.721%
	I - Batch: 200 | Loss: 0.055 | Acc: 95.844% | Wgt Acc: 98.808%
	I - Batch: 250 | Loss: 0.053 | Acc: 96.000% | Wgt Acc: 98.804%
I - num batch: 273
I - Train -- Loss: 0.053 | Acc: 95.988% | Wgt Acc: 98.813% | LR: 1.250000e-04 | Dur: 165.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    1.   47.]
 [   0.  758.    0.    0.   25.]
 [   0.    0. 1031.    0.   47.]
 [   0.    0.    0.  772.   49.]
 [   3.    2.    1.    0.  832.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.161 | Acc: 69.034% | Wgt Acc: 60.904% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   2.   3.  13.   4.]
 [  1.  33.   5.   1.   3.]
 [  2.  19.  33.   1.   7.]
 [ 12.   6.   6.  57.   4.]
 [  8.  18.  28.  14. 162.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 167
I - Training: 
	I - Batch: 50 | Loss: 0.048 | Acc: 96.750% | Wgt Acc: 99.184%
	I - Batch: 100 | Loss: 0.050 | Acc: 96.500% | Wgt Acc: 99.038%
	I - Batch: 150 | Loss: 0.053 | Acc: 96.250% | Wgt Acc: 98.875%
	I - Batch: 200 | Loss: 0.056 | Acc: 96.000% | Wgt Acc: 98.760%
	I - Batch: 250 | Loss: 0.058 | Acc: 96.150% | Wgt Acc: 98.812%
I - num batch: 273
I - Train -- Loss: 0.059 | Acc: 96.103% | Wgt Acc: 98.777% | LR: 1.250000e-04 | Dur: 168.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    2.   50.]
 [   0.  759.    0.    0.   26.]
 [   0.    0. 1031.    0.   41.]
 [   0.    0.    0.  771.   43.]
 [   6.    1.    1.    0.  840.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.250 | Acc: 64.694% | Wgt Acc: 61.331% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.   3.   4.  22.  19.]
 [  0.  34.   6.   4.   8.]
 [  2.  16.  41.   2.  13.]
 [  7.   8.   3.  49.   9.]
 [  6.  17.  21.   9. 131.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 168
I - Training: 
	I - Batch: 50 | Loss: 0.056 | Acc: 96.000% | Wgt Acc: 98.866%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.312% | Wgt Acc: 98.503%
	I - Batch: 150 | Loss: 0.064 | Acc: 95.292% | Wgt Acc: 98.461%
	I - Batch: 200 | Loss: 0.062 | Acc: 95.625% | Wgt Acc: 98.600%
	I - Batch: 250 | Loss: 0.060 | Acc: 95.875% | Wgt Acc: 98.673%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 95.690% | Wgt Acc: 98.580% | LR: 1.250000e-04 | Dur: 164.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    1.   48.]
 [   1.  758.    0.    0.   24.]
 [   0.    1. 1030.    1.   48.]
 [   0.    0.    1.  769.   54.]
 [   5.    1.    1.    2.  826.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.244 | Acc: 63.905% | Wgt Acc: 60.454% | Dur: 14.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   3.  16.  13.]
 [  2.  38.   4.   2.  12.]
 [  0.  12.  35.   2.  14.]
 [ 15.   7.   9.  55.  10.]
 [  6.  17.  24.  11. 131.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 169
I - Training: 
	I - Batch: 50 | Loss: 0.051 | Acc: 96.375% | Wgt Acc: 98.965%
	I - Batch: 100 | Loss: 0.051 | Acc: 96.312% | Wgt Acc: 98.896%
	I - Batch: 150 | Loss: 0.053 | Acc: 96.500% | Wgt Acc: 98.948%
	I - Batch: 200 | Loss: 0.053 | Acc: 96.406% | Wgt Acc: 98.897%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.300% | Wgt Acc: 98.876%
I - num batch: 273
I - Train -- Loss: 0.055 | Acc: 96.424% | Wgt Acc: 98.925% | LR: 1.250000e-04 | Dur: 163.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    1.   44.]
 [   0.  760.    0.    0.   25.]
 [   0.    0. 1031.    0.   42.]
 [   1.    0.    0.  771.   38.]
 [   3.    0.    1.    1.  851.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.262 | Acc: 64.497% | Wgt Acc: 62.300% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   5.   7.  12.  20.]
 [  0.  35.   5.   3.   7.]
 [  0.  11.  31.   1.   6.]
 [ 15.   9.  12.  66.  20.]
 [  5.  18.  20.   4. 127.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 170
I - Training: 
	I - Batch: 50 | Loss: 0.045 | Acc: 96.250% | Wgt Acc: 98.836%
	I - Batch: 100 | Loss: 0.047 | Acc: 96.500% | Wgt Acc: 98.998%
	I - Batch: 150 | Loss: 0.051 | Acc: 96.417% | Wgt Acc: 98.921%
	I - Batch: 200 | Loss: 0.052 | Acc: 96.344% | Wgt Acc: 98.886%
	I - Batch: 250 | Loss: 0.053 | Acc: 96.275% | Wgt Acc: 98.876%
I - num batch: 273
I - Train -- Loss: 0.053 | Acc: 96.240% | Wgt Acc: 98.860% | LR: 1.250000e-04 | Dur: 161.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    0.   44.]
 [   0.  759.    0.    0.   26.]
 [   0.    0. 1030.    0.   43.]
 [   0.    0.    0.  773.   43.]
 [   5.    1.    2.    0.  844.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.184 | Acc: 68.047% | Wgt Acc: 64.018% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   6.   3.  16.   9.]
 [  0.  40.   4.   1.   5.]
 [  0.  10.  38.   1.  12.]
 [ 15.   6.   8.  57.  13.]
 [  4.  16.  22.  11. 141.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 171
I - Training: 
	I - Batch: 50 | Loss: 0.047 | Acc: 96.375% | Wgt Acc: 98.815%
	I - Batch: 100 | Loss: 0.054 | Acc: 95.812% | Wgt Acc: 98.740%
	I - Batch: 150 | Loss: 0.051 | Acc: 96.250% | Wgt Acc: 98.917%
	I - Batch: 200 | Loss: 0.051 | Acc: 96.125% | Wgt Acc: 98.883%
	I - Batch: 250 | Loss: 0.050 | Acc: 96.400% | Wgt Acc: 98.978%
I - num batch: 273
I - Train -- Loss: 0.050 | Acc: 96.447% | Wgt Acc: 98.994% | LR: 1.250000e-04 | Dur: 166.12s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    0.    0.   43.]
 [   0.  760.    0.    0.   28.]
 [   0.    0. 1030.    1.   41.]
 [   0.    0.    0.  771.   39.]
 [   0.    0.    2.    1.  849.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.225 | Acc: 63.708% | Wgt Acc: 58.079% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   2.  12.  10.]
 [  0.  29.   1.   0.   3.]
 [  2.  16.  40.   1.  18.]
 [ 22.   8.  12.  58.   8.]
 [  9.  23.  20.  15. 141.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 172
I - Training: 
	I - Batch: 50 | Loss: 0.056 | Acc: 96.500% | Wgt Acc: 98.738%
	I - Batch: 100 | Loss: 0.055 | Acc: 96.688% | Wgt Acc: 98.898%
	I - Batch: 150 | Loss: 0.054 | Acc: 96.667% | Wgt Acc: 98.857%
	I - Batch: 200 | Loss: 0.055 | Acc: 96.375% | Wgt Acc: 98.764%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.275% | Wgt Acc: 98.704%
I - num batch: 273
I - Train -- Loss: 0.056 | Acc: 96.263% | Wgt Acc: 98.662% | LR: 1.250000e-04 | Dur: 165.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    1.    2.   43.]
 [   0.  759.    1.    2.   20.]
 [   0.    0. 1028.    0.   48.]
 [   0.    0.    0.  768.   35.]
 [   7.    1.    2.    1.  854.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.307 | Acc: 66.864% | Wgt Acc: 59.935% | Dur: 14.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   3.   2.   8.   8.]
 [  0.  28.   1.   3.   4.]
 [  1.  13.  31.   2.   9.]
 [ 12.   7.  12.  63.   6.]
 [ 11.  27.  29.  10. 153.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 173
I - Training: 
	I - Batch: 50 | Loss: 0.060 | Acc: 95.500% | Wgt Acc: 98.825%
	I - Batch: 100 | Loss: 0.055 | Acc: 96.312% | Wgt Acc: 98.929%
	I - Batch: 150 | Loss: 0.052 | Acc: 96.625% | Wgt Acc: 99.018%
	I - Batch: 200 | Loss: 0.055 | Acc: 96.219% | Wgt Acc: 98.879%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.350% | Wgt Acc: 98.897%
I - num batch: 273
I - Train -- Loss: 0.055 | Acc: 96.378% | Wgt Acc: 98.847% | LR: 1.250000e-04 | Dur: 167.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    1.    0.    0.   42.]
 [   1.  758.    0.    1.   20.]
 [   0.    0. 1030.    0.   46.]
 [   0.    1.    0.  771.   40.]
 [   3.    0.    2.    1.  852.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.302 | Acc: 63.708% | Wgt Acc: 54.492% | Dur: 14.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   2.   9.   4.]
 [  0.  24.   3.   2.   7.]
 [  3.  19.  29.   3.   5.]
 [ 18.   7.   7.  58.   6.]
 [ 13.  26.  34.  14. 158.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 174
I - Training: 
	I - Batch: 50 | Loss: 0.057 | Acc: 95.375% | Wgt Acc: 98.459%
	I - Batch: 100 | Loss: 0.055 | Acc: 95.875% | Wgt Acc: 98.592%
	I - Batch: 150 | Loss: 0.057 | Acc: 95.792% | Wgt Acc: 98.521%
	I - Batch: 200 | Loss: 0.059 | Acc: 95.719% | Wgt Acc: 98.478%
	I - Batch: 250 | Loss: 0.060 | Acc: 95.700% | Wgt Acc: 98.497%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 95.667% | Wgt Acc: 98.492% | LR: 1.250000e-04 | Dur: 166.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    1.   49.]
 [   0.  755.    0.    0.   36.]
 [   0.    0. 1026.    0.   47.]
 [   1.    2.    0.  772.   39.]
 [   5.    3.    6.    0.  829.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.339 | Acc: 65.878% | Wgt Acc: 59.059% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   3.  16.   9.]
 [  0.  31.   4.   2.   6.]
 [  2.  20.  38.   1.  10.]
 [ 16.   7.   8.  54.   5.]
 [  9.  18.  22.  13. 150.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 175
I - Training: 
	I - Batch: 50 | Loss: 0.052 | Acc: 96.500% | Wgt Acc: 99.108%
	I - Batch: 100 | Loss: 0.059 | Acc: 96.438% | Wgt Acc: 98.961%
	I - Batch: 150 | Loss: 0.058 | Acc: 96.125% | Wgt Acc: 98.878%
	I - Batch: 200 | Loss: 0.057 | Acc: 96.219% | Wgt Acc: 98.936%
	I - Batch: 250 | Loss: 0.056 | Acc: 96.175% | Wgt Acc: 98.895%
I - num batch: 273
I - Train -- Loss: 0.058 | Acc: 96.126% | Wgt Acc: 98.893% | LR: 1.250000e-04 | Dur: 164.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    1.   43.]
 [   1.  760.    0.    0.   27.]
 [   0.    0. 1032.    0.   57.]
 [   0.    0.    0.  772.   37.]
 [   3.    0.    0.    0.  836.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.371 | Acc: 61.341% | Wgt Acc: 56.833% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 74.   7.   7.  25.  28.]
 [  0.  31.   2.   2.   4.]
 [  1.  13.  24.   0.  10.]
 [  8.   5.  11.  51.   7.]
 [  5.  22.  31.   8. 131.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 176
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 95.250% | Wgt Acc: 98.606%
	I - Batch: 100 | Loss: 0.061 | Acc: 95.688% | Wgt Acc: 98.751%
	I - Batch: 150 | Loss: 0.060 | Acc: 95.792% | Wgt Acc: 98.743%
	I - Batch: 200 | Loss: 0.057 | Acc: 95.906% | Wgt Acc: 98.758%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.175% | Wgt Acc: 98.869%
I - num batch: 273
I - Train -- Loss: 0.055 | Acc: 96.126% | Wgt Acc: 98.848% | LR: 1.250000e-04 | Dur: 166.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    0.    0.   47.]
 [   0.  758.    0.    0.   33.]
 [   0.    0. 1030.    0.   47.]
 [   0.    0.    0.  772.   35.]
 [   2.    2.    2.    1.  838.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.346 | Acc: 63.116% | Wgt Acc: 55.288% | Dur: 15.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   6.   4.  23.  19.]
 [  0.  22.   3.   0.   1.]
 [  1.  11.  30.   0.   7.]
 [  7.   4.   2.  47.   4.]
 [  8.  35.  36.  16. 149.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 177
I - Training: 
	I - Batch: 50 | Loss: 0.064 | Acc: 94.500% | Wgt Acc: 98.074%
	I - Batch: 100 | Loss: 0.059 | Acc: 95.438% | Wgt Acc: 98.459%
	I - Batch: 150 | Loss: 0.054 | Acc: 95.917% | Wgt Acc: 98.641%
	I - Batch: 200 | Loss: 0.055 | Acc: 95.969% | Wgt Acc: 98.633%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.050% | Wgt Acc: 98.721%
I - num batch: 273
I - Train -- Loss: 0.056 | Acc: 96.080% | Wgt Acc: 98.701% | LR: 1.250000e-04 | Dur: 166.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    1.   49.]
 [   0.  758.    0.    0.   27.]
 [   0.    0. 1031.    1.   51.]
 [   0.    0.    0.  769.   31.]
 [   6.    2.    1.    2.  842.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.188 | Acc: 68.245% | Wgt Acc: 63.142% | Dur: 14.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   3.   5.  14.  10.]
 [  0.  37.   5.   2.   5.]
 [  1.  18.  44.   1.  12.]
 [ 14.   5.   2.  54.   7.]
 [  8.  15.  19.  15. 146.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 178
I - Training: 
	I - Batch: 50 | Loss: 0.049 | Acc: 96.625% | Wgt Acc: 99.003%
	I - Batch: 100 | Loss: 0.052 | Acc: 96.688% | Wgt Acc: 99.025%
	I - Batch: 150 | Loss: 0.052 | Acc: 96.542% | Wgt Acc: 98.994%
	I - Batch: 200 | Loss: 0.052 | Acc: 96.500% | Wgt Acc: 98.950%
	I - Batch: 250 | Loss: 0.053 | Acc: 96.375% | Wgt Acc: 98.873%
I - num batch: 273
I - Train -- Loss: 0.053 | Acc: 96.378% | Wgt Acc: 98.868% | LR: 1.250000e-04 | Dur: 164.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    1.   51.]
 [   0.  758.    0.    0.   23.]
 [   0.    0. 1031.    0.   35.]
 [   0.    0.    0.  771.   40.]
 [   4.    2.    1.    1.  851.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.203 | Acc: 66.075% | Wgt Acc: 60.593% | Dur: 14.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   1.   1.  11.  10.]
 [  1.  31.   2.   0.   2.]
 [  1.  14.  33.   0.  12.]
 [ 17.   8.   9.  64.  11.]
 [  7.  24.  30.  11. 145.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 179
I - Training: 
	I - Batch: 50 | Loss: 0.054 | Acc: 95.500% | Wgt Acc: 98.587%
	I - Batch: 100 | Loss: 0.055 | Acc: 95.875% | Wgt Acc: 98.645%
	I - Batch: 150 | Loss: 0.055 | Acc: 96.000% | Wgt Acc: 98.694%
	I - Batch: 200 | Loss: 0.056 | Acc: 95.906% | Wgt Acc: 98.653%
	I - Batch: 250 | Loss: 0.056 | Acc: 95.975% | Wgt Acc: 98.675%
I - num batch: 273
I - Train -- Loss: 0.055 | Acc: 96.126% | Wgt Acc: 98.718% | LR: 1.250000e-04 | Dur: 164.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    0.    0.   36.]
 [   0.  758.    0.    0.   26.]
 [   0.    0. 1030.    0.   46.]
 [   1.    0.    0.  771.   48.]
 [   6.    2.    2.    2.  844.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.201 | Acc: 67.850% | Wgt Acc: 61.827% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   4.  13.   9.]
 [  1.  33.   3.   3.   5.]
 [  2.  16.  41.   0.   9.]
 [ 13.   6.   7.  58.   7.]
 [ 10.  19.  20.  12. 150.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 180
I - Training: 
	I - Batch: 50 | Loss: 0.050 | Acc: 96.625% | Wgt Acc: 98.982%
	I - Batch: 100 | Loss: 0.050 | Acc: 96.625% | Wgt Acc: 98.998%
	I - Batch: 150 | Loss: 0.057 | Acc: 96.458% | Wgt Acc: 98.846%
	I - Batch: 200 | Loss: 0.060 | Acc: 96.125% | Wgt Acc: 98.793%
	I - Batch: 250 | Loss: 0.059 | Acc: 96.100% | Wgt Acc: 98.761%
I - num batch: 273
I - Train -- Loss: 0.062 | Acc: 96.149% | Wgt Acc: 98.724% | LR: 1.250000e-04 | Dur: 164.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    2.    1.   44.]
 [   0.  758.    0.    0.   28.]
 [   1.    0. 1026.    0.   46.]
 [   1.    0.    1.  771.   37.]
 [   1.    2.    3.    1.  845.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.275 | Acc: 65.878% | Wgt Acc: 58.644% | Dur: 14.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   1.   1.   9.   6.]
 [  0.  41.   2.   2.   3.]
 [  1.  14.  30.   0.  13.]
 [ 22.   4.   8.  58.   5.]
 [ 13.  18.  34.  17. 153.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 181
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 95.875% | Wgt Acc: 98.464%
	I - Batch: 100 | Loss: 0.065 | Acc: 95.812% | Wgt Acc: 98.508%
	I - Batch: 150 | Loss: 0.065 | Acc: 95.708% | Wgt Acc: 98.575%
	I - Batch: 200 | Loss: 0.062 | Acc: 96.062% | Wgt Acc: 98.677%
	I - Batch: 250 | Loss: 0.060 | Acc: 96.100% | Wgt Acc: 98.678%
I - num batch: 273
I - Train -- Loss: 0.060 | Acc: 96.126% | Wgt Acc: 98.712% | LR: 1.250000e-04 | Dur: 165.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    1.    3.   45.]
 [   0.  759.    0.    1.   29.]
 [   0.    0. 1030.    0.   40.]
 [   0.    0.    0.  768.   42.]
 [   5.    1.    1.    1.  844.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.242 | Acc: 67.258% | Wgt Acc: 63.649% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   5.  11.  12.]
 [  0.  34.   4.   0.   7.]
 [  2.  14.  39.   1.  15.]
 [ 13.   8.   5.  63.   8.]
 [  6.  18.  22.  11. 138.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 182
I - Training: 
	I - Batch: 50 | Loss: 0.054 | Acc: 95.500% | Wgt Acc: 98.829%
	I - Batch: 100 | Loss: 0.049 | Acc: 96.312% | Wgt Acc: 99.003%
	I - Batch: 150 | Loss: 0.051 | Acc: 96.250% | Wgt Acc: 98.922%
	I - Batch: 200 | Loss: 0.050 | Acc: 96.438% | Wgt Acc: 98.998%
	I - Batch: 250 | Loss: 0.052 | Acc: 96.450% | Wgt Acc: 98.988%
I - num batch: 273
I - Train -- Loss: 0.052 | Acc: 96.355% | Wgt Acc: 98.952% | LR: 1.250000e-04 | Dur: 166.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   49.]
 [   0.  760.    0.    0.   24.]
 [   0.    0. 1032.    0.   41.]
 [   0.    0.    0.  772.   40.]
 [   4.    0.    0.    1.  846.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.268 | Acc: 67.653% | Wgt Acc: 60.996% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   5.  19.  13.]
 [  0.  34.   1.   1.   3.]
 [  1.  16.  38.   0.   7.]
 [ 12.   4.   7.  51.   5.]
 [  7.  21.  24.  15. 152.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 183
I - Training: 
	I - Batch: 50 | Loss: 0.054 | Acc: 96.500% | Wgt Acc: 98.730%
	I - Batch: 100 | Loss: 0.053 | Acc: 96.250% | Wgt Acc: 98.717%
	I - Batch: 150 | Loss: 0.058 | Acc: 95.292% | Wgt Acc: 98.450%
	I - Batch: 200 | Loss: 0.055 | Acc: 95.844% | Wgt Acc: 98.685%
	I - Batch: 250 | Loss: 0.054 | Acc: 96.000% | Wgt Acc: 98.747%
I - num batch: 273
I - Train -- Loss: 0.055 | Acc: 96.057% | Wgt Acc: 98.719% | LR: 1.250000e-04 | Dur: 166.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    0.    0.   52.]
 [   1.  758.    0.    0.   25.]
 [   1.    0. 1032.    0.   37.]
 [   1.    0.    0.  770.   46.]
 [   4.    2.    0.    3.  840.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.353 | Acc: 63.905% | Wgt Acc: 59.762% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   7.   7.  22.  25.]
 [  0.  36.   4.   1.   7.]
 [  1.  12.  30.   0.   8.]
 [ 13.   8.  12.  53.   6.]
 [  3.  15.  22.  10. 134.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 184
I - Training: 
	I - Batch: 50 | Loss: 0.063 | Acc: 96.625% | Wgt Acc: 98.913%
	I - Batch: 100 | Loss: 0.056 | Acc: 96.562% | Wgt Acc: 98.821%
	I - Batch: 150 | Loss: 0.055 | Acc: 96.292% | Wgt Acc: 98.775%
	I - Batch: 200 | Loss: 0.056 | Acc: 96.156% | Wgt Acc: 98.742%
	I - Batch: 250 | Loss: 0.057 | Acc: 96.050% | Wgt Acc: 98.670%
I - num batch: 273
I - Train -- Loss: 0.057 | Acc: 95.988% | Wgt Acc: 98.610% | LR: 1.250000e-04 | Dur: 164.83s
I - Confusion Matrix: [row->prediction - col->label]
[[ 790.    0.    2.    0.   43.]
 [   0.  759.    0.    0.   23.]
 [   0.    0. 1030.    0.   50.]
 [   0.    0.    0.  767.   43.]
 [   7.    1.    0.    6.  841.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.243 | Acc: 67.850% | Wgt Acc: 61.631% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   3.  18.  11.]
 [  0.  34.   3.   0.   2.]
 [  1.  11.  32.   0.   8.]
 [ 13.   7.  12.  58.   8.]
 [  5.  22.  25.  10. 151.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 185
I - Training: 
	I - Batch: 50 | Loss: 0.053 | Acc: 96.125% | Wgt Acc: 98.849%
	I - Batch: 100 | Loss: 0.054 | Acc: 96.438% | Wgt Acc: 98.908%
	I - Batch: 150 | Loss: 0.056 | Acc: 96.292% | Wgt Acc: 98.808%
	I - Batch: 200 | Loss: 0.057 | Acc: 96.031% | Wgt Acc: 98.773%
	I - Batch: 250 | Loss: 0.056 | Acc: 96.000% | Wgt Acc: 98.734%
I - num batch: 273
I - Train -- Loss: 0.056 | Acc: 96.011% | Wgt Acc: 98.756% | LR: 1.250000e-04 | Dur: 166.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    1.    0.    0.   55.]
 [   0.  759.    0.    0.   29.]
 [   1.    0. 1029.    0.   43.]
 [   1.    0.    0.  772.   37.]
 [   3.    0.    3.    1.  836.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.337 | Acc: 64.103% | Wgt Acc: 59.128% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   6.  17.  16.]
 [  0.  40.   3.   3.  12.]
 [  1.   8.  27.   2.   5.]
 [ 15.  11.  15.  57.   8.]
 [ 10.  17.  24.   7. 139.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 186
I - Training: 
	I - Batch: 50 | Loss: 0.055 | Acc: 96.875% | Wgt Acc: 99.096%
	I - Batch: 100 | Loss: 0.049 | Acc: 97.000% | Wgt Acc: 99.185%
	I - Batch: 150 | Loss: 0.046 | Acc: 97.125% | Wgt Acc: 99.230%
	I - Batch: 200 | Loss: 0.047 | Acc: 96.969% | Wgt Acc: 99.191%
	I - Batch: 250 | Loss: 0.048 | Acc: 96.900% | Wgt Acc: 99.153%
I - num batch: 273
I - Train -- Loss: 0.049 | Acc: 96.836% | Wgt Acc: 99.140% | LR: 1.250000e-04 | Dur: 165.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    0.   44.]
 [   0.  760.    0.    0.   25.]
 [   0.    0. 1032.    0.   34.]
 [   0.    0.    0.  772.   33.]
 [   1.    0.    0.    1.  864.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.239 | Acc: 67.456% | Wgt Acc: 61.515% | Dur: 14.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   2.   9.   5.]
 [  0.  28.   1.   2.   6.]
 [  3.  18.  42.   1.  14.]
 [ 15.   4.   7.  61.   6.]
 [  8.  24.  23.  13. 149.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 187
I - Training: 
	I - Batch: 50 | Loss: 0.054 | Acc: 96.000% | Wgt Acc: 98.611%
	I - Batch: 100 | Loss: 0.054 | Acc: 96.250% | Wgt Acc: 98.802%
	I - Batch: 150 | Loss: 0.058 | Acc: 96.042% | Wgt Acc: 98.743%
	I - Batch: 200 | Loss: 0.055 | Acc: 96.312% | Wgt Acc: 98.778%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.300% | Wgt Acc: 98.784%
I - num batch: 273
I - Train -- Loss: 0.054 | Acc: 96.378% | Wgt Acc: 98.784% | LR: 1.250000e-04 | Dur: 166.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    1.   40.]
 [   1.  759.    0.    0.   20.]
 [   0.    0. 1028.    1.   53.]
 [   1.    0.    0.  771.   32.]
 [   4.    1.    4.    0.  855.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.175 | Acc: 66.272% | Wgt Acc: 60.454% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   2.  11.   7.]
 [  0.  30.   2.   1.   5.]
 [  1.  18.  43.   2.  17.]
 [ 18.   3.   7.  56.   5.]
 [  8.  25.  21.  16. 146.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773.  781.]

I - Epoch: 188
I - Training: 
	I - Batch: 50 | Loss: 0.037 | Acc: 97.500% | Wgt Acc: 99.380%
	I - Batch: 100 | Loss: 0.040 | Acc: 97.312% | Wgt Acc: 99.282%
	I - Batch: 150 | Loss: 0.041 | Acc: 97.250% | Wgt Acc: 99.247%
	I - Batch: 200 | Loss: 0.041 | Acc: 97.188% | Wgt Acc: 99.245%
	I - Batch: 250 | Loss: 0.043 | Acc: 97.000% | Wgt Acc: 99.117%
I - num batch: 259
I - Train -- Loss: 0.043 | Acc: 97.055% | Wgt Acc: 99.137% | LR: 1.250000e-04 | Dur: 157.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    1.    1.   28.]
 [   0.  759.    0.    0.   22.]
 [   0.    0. 1031.    0.   33.]
 [   0.    0.    0.  772.   33.]
 [   3.    1.    0.    0.  665.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.201 | Acc: 65.680% | Wgt Acc: 61.827% | Dur: 14.80s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   4.  16.  13.]
 [  0.  34.   7.   2.   3.]
 [  1.  17.  41.   0.  15.]
 [ 17.   8.   4.  58.  13.]
 [  6.  17.  19.  10. 136.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 189
I - Training: 
	I - Batch: 50 | Loss: 0.053 | Acc: 95.500% | Wgt Acc: 98.582%
	I - Batch: 100 | Loss: 0.050 | Acc: 96.125% | Wgt Acc: 98.744%
	I - Batch: 150 | Loss: 0.050 | Acc: 96.083% | Wgt Acc: 98.785%
	I - Batch: 200 | Loss: 0.049 | Acc: 96.188% | Wgt Acc: 98.805%
	I - Batch: 250 | Loss: 0.049 | Acc: 96.400% | Wgt Acc: 98.879%
I - num batch: 273
I - Train -- Loss: 0.048 | Acc: 96.424% | Wgt Acc: 98.903% | LR: 1.250000e-04 | Dur: 166.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   47.]
 [   0.  760.    1.    0.   20.]
 [   0.    0. 1030.    0.   45.]
 [   0.    0.    0.  771.   36.]
 [   4.    0.    1.    2.  852.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.245 | Acc: 65.878% | Wgt Acc: 59.128% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   6.  13.  12.]
 [  0.  33.   4.   1.   5.]
 [  1.  14.  37.   1.   7.]
 [ 18.   7.   9.  56.   6.]
 [ 11.  21.  19.  15. 150.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 190
I - Training: 
	I - Batch: 50 | Loss: 0.050 | Acc: 97.125% | Wgt Acc: 99.133%
	I - Batch: 100 | Loss: 0.042 | Acc: 97.562% | Wgt Acc: 99.312%
	I - Batch: 150 | Loss: 0.042 | Acc: 97.250% | Wgt Acc: 99.211%
	I - Batch: 200 | Loss: 0.046 | Acc: 96.938% | Wgt Acc: 99.061%
	I - Batch: 250 | Loss: 0.047 | Acc: 96.675% | Wgt Acc: 98.975%
I - num batch: 273
I - Train -- Loss: 0.048 | Acc: 96.538% | Wgt Acc: 98.932% | LR: 1.250000e-04 | Dur: 162.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    1.   37.]
 [   0.  757.    0.    0.   31.]
 [   0.    1. 1029.    0.   41.]
 [   0.    0.    0.  772.   34.]
 [   1.    2.    3.    0.  857.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.140 | Acc: 66.667% | Wgt Acc: 61.804% | Dur: 14.12s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   1.  15.   8.]
 [  0.  33.   2.   2.   4.]
 [  1.  17.  46.   2.  17.]
 [ 15.   3.   8.  54.   9.]
 [  9.  22.  18.  13. 142.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 191
I - Training: 
	I - Batch: 50 | Loss: 0.047 | Acc: 97.250% | Wgt Acc: 99.066%
	I - Batch: 100 | Loss: 0.045 | Acc: 97.000% | Wgt Acc: 99.117%
	I - Batch: 150 | Loss: 0.050 | Acc: 96.708% | Wgt Acc: 98.962%
	I - Batch: 200 | Loss: 0.050 | Acc: 96.656% | Wgt Acc: 98.996%
	I - Batch: 250 | Loss: 0.050 | Acc: 96.700% | Wgt Acc: 98.987%
I - num batch: 273
I - Train -- Loss: 0.050 | Acc: 96.745% | Wgt Acc: 98.989% | LR: 1.250000e-04 | Dur: 166.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    0.   38.]
 [   2.  758.    0.    0.   19.]
 [   0.    0. 1031.    0.   48.]
 [   0.    0.    0.  773.   29.]
 [   3.    2.    1.    0.  866.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.303 | Acc: 67.456% | Wgt Acc: 59.036% | Dur: 15.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   4.  20.   6.]
 [  0.  33.   2.   1.   1.]
 [  1.  15.  33.   1.   9.]
 [ 10.   3.   3.  50.   4.]
 [ 11.  23.  33.  14. 160.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 192
I - Training: 
	I - Batch: 50 | Loss: 0.039 | Acc: 97.125% | Wgt Acc: 99.261%
	I - Batch: 100 | Loss: 0.042 | Acc: 97.125% | Wgt Acc: 99.200%
	I - Batch: 150 | Loss: 0.043 | Acc: 97.208% | Wgt Acc: 99.207%
	I - Batch: 200 | Loss: 0.045 | Acc: 97.000% | Wgt Acc: 99.146%
	I - Batch: 250 | Loss: 0.045 | Acc: 97.025% | Wgt Acc: 99.162%
I - num batch: 273
I - Train -- Loss: 0.046 | Acc: 96.997% | Wgt Acc: 99.117% | LR: 1.250000e-04 | Dur: 167.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    1.   41.]
 [   0.  760.    0.    0.   18.]
 [   0.    0. 1032.    0.   41.]
 [   0.    0.    0.  772.   26.]
 [   4.    0.    0.    0.  874.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 68.245% | Wgt Acc: 63.038% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   2.  13.   7.]
 [  0.  40.   4.   0.   5.]
 [  2.  12.  40.   1.  16.]
 [ 12.   2.   6.  57.   5.]
 [ 12.  20.  23.  15. 147.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 193
I - Training: 
	I - Batch: 50 | Loss: 0.047 | Acc: 97.625% | Wgt Acc: 99.153%
	I - Batch: 100 | Loss: 0.060 | Acc: 96.188% | Wgt Acc: 98.713%
	I - Batch: 150 | Loss: 0.061 | Acc: 95.708% | Wgt Acc: 98.609%
	I - Batch: 200 | Loss: 0.061 | Acc: 95.688% | Wgt Acc: 98.581%
	I - Batch: 250 | Loss: 0.061 | Acc: 95.575% | Wgt Acc: 98.565%
I - num batch: 273
I - Train -- Loss: 0.060 | Acc: 95.713% | Wgt Acc: 98.605% | LR: 1.250000e-04 | Dur: 165.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    1.   59.]
 [   1.  758.    0.    0.   23.]
 [   0.    0. 1030.    0.   48.]
 [   0.    0.    0.  768.   44.]
 [   3.    2.    2.    4.  826.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.171 | Acc: 66.075% | Wgt Acc: 63.684% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   1.   5.  12.  11.]
 [  0.  39.   7.   1.   5.]
 [  2.  17.  42.   2.  22.]
 [ 14.   7.   7.  59.  12.]
 [  7.  14.  14.  12. 130.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 194
I - Training: 
	I - Batch: 50 | Loss: 0.062 | Acc: 95.875% | Wgt Acc: 98.585%
	I - Batch: 100 | Loss: 0.056 | Acc: 96.188% | Wgt Acc: 98.841%
	I - Batch: 150 | Loss: 0.054 | Acc: 96.125% | Wgt Acc: 98.762%
	I - Batch: 200 | Loss: 0.054 | Acc: 96.250% | Wgt Acc: 98.771%
	I - Batch: 250 | Loss: 0.054 | Acc: 96.300% | Wgt Acc: 98.740%
I - num batch: 273
I - Train -- Loss: 0.055 | Acc: 96.286% | Wgt Acc: 98.718% | LR: 1.250000e-04 | Dur: 163.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    0.    0.   51.]
 [   0.  757.    0.    0.   21.]
 [   1.    1. 1029.    0.   40.]
 [   1.    1.    1.  772.   35.]
 [   6.    1.    2.    1.  853.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.196 | Acc: 67.456% | Wgt Acc: 60.454% | Dur: 13.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   4.  12.   8.]
 [  1.  32.   9.   3.   5.]
 [  0.  17.  32.   1.   8.]
 [ 13.   4.   1.  58.   5.]
 [  8.  23.  29.  12. 154.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 195
I - Training: 
	I - Batch: 50 | Loss: 0.050 | Acc: 96.250% | Wgt Acc: 99.046%
	I - Batch: 100 | Loss: 0.052 | Acc: 96.125% | Wgt Acc: 98.825%
	I - Batch: 150 | Loss: 0.055 | Acc: 95.833% | Wgt Acc: 98.719%
	I - Batch: 200 | Loss: 0.055 | Acc: 95.938% | Wgt Acc: 98.765%
	I - Batch: 250 | Loss: 0.057 | Acc: 96.025% | Wgt Acc: 98.703%
I - num batch: 273
I - Train -- Loss: 0.057 | Acc: 96.126% | Wgt Acc: 98.753% | LR: 1.250000e-04 | Dur: 164.72s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    0.    1.   35.]
 [   0.  756.    1.    0.   27.]
 [   0.    2. 1031.    0.   46.]
 [   0.    0.    0.  769.   50.]
 [   2.    2.    0.    3.  842.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.213 | Acc: 66.469% | Wgt Acc: 58.874% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   4.   1.   9.   2.]
 [  0.  34.   6.   1.   4.]
 [  3.  17.  39.   2.  16.]
 [ 12.   3.   5.  55.   3.]
 [ 19.  20.  24.  19. 155.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 196
I - Training: 
	I - Batch: 50 | Loss: 0.060 | Acc: 96.000% | Wgt Acc: 98.601%
	I - Batch: 100 | Loss: 0.058 | Acc: 96.312% | Wgt Acc: 98.743%
	I - Batch: 150 | Loss: 0.062 | Acc: 95.792% | Wgt Acc: 98.580%
	I - Batch: 200 | Loss: 0.063 | Acc: 95.875% | Wgt Acc: 98.658%
	I - Batch: 250 | Loss: 0.062 | Acc: 95.775% | Wgt Acc: 98.663%
I - num batch: 273
I - Train -- Loss: 0.061 | Acc: 95.782% | Wgt Acc: 98.693% | LR: 1.250000e-04 | Dur: 164.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    1.    0.    1.   50.]
 [   0.  758.    0.    0.   33.]
 [   0.    0. 1031.    0.   45.]
 [   0.    0.    0.  771.   46.]
 [   5.    1.    1.    1.  826.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.222 | Acc: 66.864% | Wgt Acc: 62.230% | Dur: 14.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   3.  10.   9.]
 [  0.  39.   8.   2.   9.]
 [  3.  20.  38.   4.  12.]
 [ 17.   4.   6.  58.   8.]
 [  6.  13.  20.  12. 142.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 197
I - Training: 
	I - Batch: 50 | Loss: 0.045 | Acc: 96.000% | Wgt Acc: 98.762%
	I - Batch: 100 | Loss: 0.045 | Acc: 96.875% | Wgt Acc: 99.080%
	I - Batch: 150 | Loss: 0.044 | Acc: 96.875% | Wgt Acc: 98.997%
	I - Batch: 200 | Loss: 0.046 | Acc: 96.656% | Wgt Acc: 98.894%
	I - Batch: 250 | Loss: 0.043 | Acc: 97.025% | Wgt Acc: 99.042%
I - num batch: 273
I - Train -- Loss: 0.044 | Acc: 96.905% | Wgt Acc: 99.025% | LR: 1.250000e-04 | Dur: 165.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    1.   35.]
 [   0.  760.    2.    0.   21.]
 [   1.    0. 1030.    0.   41.]
 [   0.    0.    0.  770.   30.]
 [   2.    0.    0.    2.  873.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.294 | Acc: 66.469% | Wgt Acc: 58.436% | Dur: 14.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   3.  15.   8.]
 [  1.  32.   2.   0.   5.]
 [  0.  11.  30.   2.   8.]
 [ 12.   6.   5.  55.   2.]
 [ 12.  26.  35.  14. 157.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 198
I - Training: 
	I - Batch: 50 | Loss: 0.040 | Acc: 97.000% | Wgt Acc: 99.106%
	I - Batch: 100 | Loss: 0.041 | Acc: 97.188% | Wgt Acc: 99.171%
	I - Batch: 150 | Loss: 0.044 | Acc: 96.833% | Wgt Acc: 99.075%
	I - Batch: 200 | Loss: 0.046 | Acc: 96.719% | Wgt Acc: 99.040%
	I - Batch: 250 | Loss: 0.047 | Acc: 96.600% | Wgt Acc: 99.031%
I - num batch: 273
I - Train -- Loss: 0.048 | Acc: 96.538% | Wgt Acc: 98.997% | LR: 1.250000e-04 | Dur: 168.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    0.    0.   42.]
 [   0.  758.    0.    0.   20.]
 [   0.    0. 1030.    0.   47.]
 [   0.    0.    0.  772.   37.]
 [   0.    2.    2.    1.  854.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.307 | Acc: 66.075% | Wgt Acc: 59.186% | Dur: 14.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   5.   2.  18.   8.]
 [  0.  33.   5.   1.   4.]
 [  1.  15.  30.   2.   8.]
 [ 17.   4.  12.  56.   9.]
 [  5.  21.  26.   9. 151.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 199
I - Training: 
	I - Batch: 50 | Loss: 0.059 | Acc: 95.625% | Wgt Acc: 98.462%
	I - Batch: 100 | Loss: 0.058 | Acc: 96.000% | Wgt Acc: 98.540%
	I - Batch: 150 | Loss: 0.059 | Acc: 96.042% | Wgt Acc: 98.659%
	I - Batch: 200 | Loss: 0.059 | Acc: 96.062% | Wgt Acc: 98.680%
	I - Batch: 250 | Loss: 0.058 | Acc: 96.125% | Wgt Acc: 98.713%
I - num batch: 273
I - Train -- Loss: 0.058 | Acc: 96.011% | Wgt Acc: 98.707% | LR: 1.250000e-04 | Dur: 165.94s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    1.    0.    1.   45.]
 [   0.  756.    1.    0.   30.]
 [   0.    1. 1030.    1.   47.]
 [   0.    0.    0.  771.   40.]
 [   4.    2.    1.    0.  838.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.389 | Acc: 65.483% | Wgt Acc: 56.118% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   7.  16.   9.]
 [  0.  30.   2.   1.   0.]
 [  2.  13.  28.   1.   7.]
 [ 11.   3.   5.  49.   3.]
 [ 11.  28.  33.  19. 161.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 200
I - Training: 
	I - Batch: 50 | Loss: 0.056 | Acc: 96.250% | Wgt Acc: 98.688%
	I - Batch: 100 | Loss: 0.053 | Acc: 96.688% | Wgt Acc: 98.842%
	I - Batch: 150 | Loss: 0.052 | Acc: 96.750% | Wgt Acc: 98.954%
	I - Batch: 200 | Loss: 0.052 | Acc: 96.844% | Wgt Acc: 98.943%
	I - Batch: 250 | Loss: 0.052 | Acc: 96.700% | Wgt Acc: 98.953%
I - num batch: 273
I - Train -- Loss: 0.051 | Acc: 96.699% | Wgt Acc: 98.948% | LR: 1.250000e-04 | Dur: 163.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    0.   38.]
 [   0.  758.    2.    1.   23.]
 [   0.    1. 1029.    0.   40.]
 [   0.    0.    0.  770.   34.]
 [   1.    1.    1.    2.  865.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.306 | Acc: 65.286% | Wgt Acc: 60.258% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   6.   3.  16.  12.]
 [  0.  32.   3.   1.   1.]
 [  4.  18.  34.   0.  19.]
 [ 17.   7.   7.  59.   7.]
 [  2.  15.  28.  10. 141.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 201
I - Training: 
	I - Batch: 50 | Loss: 0.060 | Acc: 96.125% | Wgt Acc: 98.874%
	I - Batch: 100 | Loss: 0.052 | Acc: 96.500% | Wgt Acc: 99.034%
	I - Batch: 150 | Loss: 0.049 | Acc: 96.708% | Wgt Acc: 99.111%
	I - Batch: 200 | Loss: 0.054 | Acc: 96.594% | Wgt Acc: 98.907%
	I - Batch: 250 | Loss: 0.053 | Acc: 96.650% | Wgt Acc: 98.918%
I - num batch: 273
I - Train -- Loss: 0.053 | Acc: 96.584% | Wgt Acc: 98.899% | LR: 1.250000e-04 | Dur: 168.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    2.   39.]
 [   0.  759.    0.    0.   32.]
 [   1.    0. 1029.    0.   36.]
 [   1.    0.    0.  770.   32.]
 [   1.    1.    3.    1.  861.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.268 | Acc: 65.286% | Wgt Acc: 59.901% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   5.   4.  22.  13.]
 [  0.  33.   4.   3.   7.]
 [  1.  12.  32.   2.  13.]
 [ 10.   8.  11.  52.   5.]
 [  5.  20.  24.   7. 142.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 202
I - Training: 
	I - Batch: 50 | Loss: 0.048 | Acc: 96.625% | Wgt Acc: 99.021%
	I - Batch: 100 | Loss: 0.046 | Acc: 96.875% | Wgt Acc: 99.141%
	I - Batch: 150 | Loss: 0.048 | Acc: 96.500% | Wgt Acc: 98.979%
	I - Batch: 200 | Loss: 0.048 | Acc: 96.375% | Wgt Acc: 98.919%
	I - Batch: 250 | Loss: 0.048 | Acc: 96.450% | Wgt Acc: 98.964%
I - num batch: 273
I - Train -- Loss: 0.048 | Acc: 96.447% | Wgt Acc: 98.976% | LR: 1.250000e-04 | Dur: 164.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    0.   35.]
 [   0.  760.    0.    0.   34.]
 [   0.    0. 1031.    0.   46.]
 [   0.    0.    0.  772.   35.]
 [   3.    0.    1.    1.  850.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.252 | Acc: 66.469% | Wgt Acc: 60.754% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   2.  16.   8.]
 [  0.  33.   2.   1.   4.]
 [  1.  13.  40.   1.  11.]
 [ 18.   6.   9.  56.  11.]
 [  7.  23.  22.  12. 146.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 203
I - Training: 
	I - Batch: 50 | Loss: 0.045 | Acc: 97.500% | Wgt Acc: 99.349%
	I - Batch: 100 | Loss: 0.049 | Acc: 96.875% | Wgt Acc: 99.071%
	I - Batch: 150 | Loss: 0.047 | Acc: 97.042% | Wgt Acc: 99.119%
	I - Batch: 200 | Loss: 0.047 | Acc: 97.031% | Wgt Acc: 99.089%
	I - Batch: 250 | Loss: 0.047 | Acc: 96.975% | Wgt Acc: 99.102%
I - num batch: 273
I - Train -- Loss: 0.046 | Acc: 97.043% | Wgt Acc: 99.129% | LR: 1.250000e-04 | Dur: 165.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    0.   40.]
 [   0.  760.    0.    0.   20.]
 [   0.    0. 1031.    0.   37.]
 [   0.    0.    0.  772.   27.]
 [   3.    0.    1.    1.  876.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.234 | Acc: 67.258% | Wgt Acc: 60.235% | Dur: 14.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   1.  11.   9.]
 [  0.  32.   2.   1.   5.]
 [  1.  11.  35.   0.   7.]
 [ 17.   7.   8.  60.   5.]
 [ 10.  26.  29.  14. 154.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 204
I - Training: 
	I - Batch: 50 | Loss: 0.046 | Acc: 97.500% | Wgt Acc: 99.099%
	I - Batch: 100 | Loss: 0.041 | Acc: 97.375% | Wgt Acc: 99.143%
	I - Batch: 150 | Loss: 0.041 | Acc: 97.208% | Wgt Acc: 99.129%
	I - Batch: 200 | Loss: 0.044 | Acc: 96.844% | Wgt Acc: 98.982%
	I - Batch: 250 | Loss: 0.044 | Acc: 96.800% | Wgt Acc: 98.966%
I - num batch: 273
I - Train -- Loss: 0.044 | Acc: 96.951% | Wgt Acc: 99.023% | LR: 1.250000e-04 | Dur: 162.12s
I - Confusion Matrix: [row->prediction - col->label]
[[ 791.    0.    0.    0.   41.]
 [   0.  760.    0.    0.   14.]
 [   0.    0. 1029.    0.   37.]
 [   0.    0.    0.  773.   32.]
 [   6.    0.    3.    0.  876.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.371 | Acc: 65.680% | Wgt Acc: 58.932% | Dur: 13.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   3.  10.   6.]
 [  0.  25.   3.   1.   3.]
 [  3.  20.  36.   2.  12.]
 [ 18.   6.   9.  63.   9.]
 [  8.  24.  24.  10. 150.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 205
I - Training: 
	I - Batch: 50 | Loss: 0.045 | Acc: 97.250% | Wgt Acc: 98.933%
	I - Batch: 100 | Loss: 0.047 | Acc: 96.625% | Wgt Acc: 98.899%
	I - Batch: 150 | Loss: 0.047 | Acc: 96.833% | Wgt Acc: 98.912%
	I - Batch: 200 | Loss: 0.047 | Acc: 96.719% | Wgt Acc: 98.916%
	I - Batch: 250 | Loss: 0.046 | Acc: 96.875% | Wgt Acc: 99.001%
I - num batch: 273
I - Train -- Loss: 0.045 | Acc: 96.882% | Wgt Acc: 99.019% | LR: 1.250000e-04 | Dur: 164.77s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    1.   42.]
 [   0.  758.    0.    0.   14.]
 [   1.    0. 1031.    0.   43.]
 [   0.    0.    0.  771.   29.]
 [   2.    2.    1.    1.  872.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.192 | Acc: 67.258% | Wgt Acc: 62.334% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   2.  12.   5.]
 [  1.  42.   7.   1.   8.]
 [  2.  14.  38.   1.  13.]
 [ 18.   5.   6.  56.  10.]
 [  6.  15.  22.  16. 144.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 206
I - Training: 
	I - Batch: 50 | Loss: 0.046 | Acc: 96.750% | Wgt Acc: 99.154%
	I - Batch: 100 | Loss: 0.046 | Acc: 96.562% | Wgt Acc: 99.059%
	I - Batch: 150 | Loss: 0.048 | Acc: 96.375% | Wgt Acc: 98.906%
	I - Batch: 200 | Loss: 0.048 | Acc: 96.438% | Wgt Acc: 98.936%
	I - Batch: 250 | Loss: 0.048 | Acc: 96.350% | Wgt Acc: 98.891%
I - num batch: 273
I - Train -- Loss: 0.050 | Acc: 96.309% | Wgt Acc: 98.849% | LR: 1.250000e-04 | Dur: 164.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    0.   43.]
 [   0.  759.    0.    0.   24.]
 [   0.    0. 1031.    0.   45.]
 [   1.    0.    0.  770.   40.]
 [   3.    1.    1.    3.  848.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.330 | Acc: 65.483% | Wgt Acc: 57.917% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   4.  14.   6.]
 [  0.  30.   4.   0.   4.]
 [  1.  13.  35.   0.   9.]
 [ 19.   5.   3.  55.   8.]
 [  9.  27.  29.  17. 153.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 207
I - Training: 
	I - Batch: 50 | Loss: 0.060 | Acc: 95.500% | Wgt Acc: 98.741%
	I - Batch: 100 | Loss: 0.053 | Acc: 96.250% | Wgt Acc: 98.982%
	I - Batch: 150 | Loss: 0.053 | Acc: 96.500% | Wgt Acc: 99.014%
	I - Batch: 200 | Loss: 0.050 | Acc: 96.531% | Wgt Acc: 99.050%
	I - Batch: 250 | Loss: 0.050 | Acc: 96.550% | Wgt Acc: 99.063%
I - num batch: 273
I - Train -- Loss: 0.050 | Acc: 96.561% | Wgt Acc: 99.025% | LR: 1.250000e-04 | Dur: 163.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    0.   42.]
 [   0.  759.    1.    0.   18.]
 [   0.    0. 1031.    0.   44.]
 [   0.    0.    0.  772.   42.]
 [   1.    1.    0.    1.  854.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.264 | Acc: 65.680% | Wgt Acc: 58.194% | Dur: 14.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   3.  13.   6.]
 [  1.  35.   4.   1.   3.]
 [  1.  15.  26.   3.  13.]
 [ 12.   4.   7.  53.   5.]
 [  8.  22.  35.  16. 153.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 208
I - Training: 
	I - Batch: 50 | Loss: 0.050 | Acc: 96.750% | Wgt Acc: 98.931%
	I - Batch: 100 | Loss: 0.044 | Acc: 97.250% | Wgt Acc: 99.181%
	I - Batch: 150 | Loss: 0.045 | Acc: 97.083% | Wgt Acc: 99.134%
	I - Batch: 200 | Loss: 0.047 | Acc: 96.969% | Wgt Acc: 99.133%
	I - Batch: 250 | Loss: 0.047 | Acc: 97.000% | Wgt Acc: 99.132%
I - num batch: 273
I - Train -- Loss: 0.046 | Acc: 97.066% | Wgt Acc: 99.155% | LR: 1.250000e-04 | Dur: 164.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    1.    0.   44.]
 [   0.  759.    0.    0.   18.]
 [   0.    0. 1031.    0.   33.]
 [   0.    0.    0.  772.   29.]
 [   1.    1.    0.    1.  876.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.375 | Acc: 66.075% | Wgt Acc: 57.986% | Dur: 14.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   4.  11.   9.]
 [  0.  29.   2.   2.   2.]
 [  0.  10.  28.   2.   4.]
 [ 19.   9.  10.  60.   8.]
 [  8.  27.  31.  11. 157.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 209
I - Training: 
	I - Batch: 50 | Loss: 0.049 | Acc: 97.000% | Wgt Acc: 99.091%
	I - Batch: 100 | Loss: 0.045 | Acc: 97.062% | Wgt Acc: 99.118%
	I - Batch: 150 | Loss: 0.045 | Acc: 96.875% | Wgt Acc: 99.072%
	I - Batch: 200 | Loss: 0.045 | Acc: 97.094% | Wgt Acc: 99.161%
	I - Batch: 250 | Loss: 0.046 | Acc: 97.000% | Wgt Acc: 99.131%
I - num batch: 273
I - Train -- Loss: 0.046 | Acc: 97.066% | Wgt Acc: 99.156% | LR: 1.250000e-04 | Dur: 163.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    0.    0.   42.]
 [   0.  760.    0.    0.   22.]
 [   0.    0. 1031.    0.   31.]
 [   0.    0.    0.  772.   29.]
 [   2.    0.    1.    1.  876.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.345 | Acc: 64.497% | Wgt Acc: 58.897% | Dur: 14.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   3.   4.  22.  21.]
 [  0.  36.   3.   1.   5.]
 [  0.   8.  27.   2.   7.]
 [ 10.   8.  10.  53.   5.]
 [  9.  23.  31.   8. 142.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 210
I - Training: 
	I - Batch: 50 | Loss: 0.051 | Acc: 96.000% | Wgt Acc: 98.712%
	I - Batch: 100 | Loss: 0.055 | Acc: 95.812% | Wgt Acc: 98.664%
	I - Batch: 150 | Loss: 0.053 | Acc: 96.250% | Wgt Acc: 98.863%
	I - Batch: 200 | Loss: 0.051 | Acc: 96.469% | Wgt Acc: 98.967%
	I - Batch: 250 | Loss: 0.050 | Acc: 96.525% | Wgt Acc: 99.002%
I - num batch: 273
I - Train -- Loss: 0.049 | Acc: 96.653% | Wgt Acc: 99.045% | LR: 1.250000e-04 | Dur: 165.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    0.   50.]
 [   0.  760.    0.    0.   28.]
 [   0.    0. 1032.    0.   31.]
 [   0.    0.    0.  770.   33.]
 [   1.    0.    0.    3.  858.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.208 | Acc: 67.258% | Wgt Acc: 61.285% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   2.   2.   8.   8.]
 [  0.  33.   2.   0.   2.]
 [  1.  16.  34.   1.  14.]
 [ 14.   7.   7.  60.   7.]
 [  8.  20.  30.  17. 149.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 211
I - Training: 
	I - Batch: 50 | Loss: 0.047 | Acc: 97.125% | Wgt Acc: 99.130%
	I - Batch: 100 | Loss: 0.046 | Acc: 96.938% | Wgt Acc: 99.024%
	I - Batch: 150 | Loss: 0.046 | Acc: 97.167% | Wgt Acc: 99.143%
	I - Batch: 200 | Loss: 0.046 | Acc: 97.156% | Wgt Acc: 99.140%
	I - Batch: 250 | Loss: 0.048 | Acc: 96.800% | Wgt Acc: 98.978%
I - num batch: 273
I - Train -- Loss: 0.048 | Acc: 96.882% | Wgt Acc: 98.999% | LR: 1.250000e-04 | Dur: 168.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    0.   37.]
 [   0.  759.    1.    0.   26.]
 [   0.    0. 1031.    0.   27.]
 [   0.    0.    0.  771.   37.]
 [   5.    1.    0.    2.  873.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.208 | Acc: 65.878% | Wgt Acc: 65.010% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   2.   3.  13.  18.]
 [  0.  39.   5.   3.  11.]
 [  1.  13.  40.   1.  14.]
 [ 16.  10.   8.  65.  14.]
 [  4.  14.  19.   4. 123.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 212
I - Training: 
	I - Batch: 50 | Loss: 0.044 | Acc: 97.125% | Wgt Acc: 99.262%
	I - Batch: 100 | Loss: 0.045 | Acc: 97.250% | Wgt Acc: 99.298%
	I - Batch: 150 | Loss: 0.044 | Acc: 97.000% | Wgt Acc: 99.152%
	I - Batch: 200 | Loss: 0.048 | Acc: 96.719% | Wgt Acc: 99.005%
	I - Batch: 250 | Loss: 0.048 | Acc: 96.650% | Wgt Acc: 98.972%
I - num batch: 273
I - Train -- Loss: 0.047 | Acc: 96.813% | Wgt Acc: 99.026% | LR: 1.250000e-04 | Dur: 164.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    0.    0.   42.]
 [   0.  759.    0.    0.   22.]
 [   0.    0. 1029.    1.   38.]
 [   1.    0.    1.  772.   30.]
 [   1.    1.    2.    0.  868.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.357 | Acc: 67.850% | Wgt Acc: 60.639% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.   4.   4.  21.  13.]
 [  0.  30.   3.   0.   2.]
 [  0.  11.  33.   0.   4.]
 [  9.   8.   6.  53.   6.]
 [  6.  25.  29.  12. 155.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 213
I - Training: 
	I - Batch: 50 | Loss: 0.049 | Acc: 96.500% | Wgt Acc: 98.835%
	I - Batch: 100 | Loss: 0.046 | Acc: 96.688% | Wgt Acc: 98.905%
	I - Batch: 150 | Loss: 0.047 | Acc: 96.583% | Wgt Acc: 98.919%
	I - Batch: 200 | Loss: 0.048 | Acc: 96.406% | Wgt Acc: 98.920%
	I - Batch: 250 | Loss: 0.048 | Acc: 96.550% | Wgt Acc: 98.965%
I - num batch: 273
I - Train -- Loss: 0.048 | Acc: 96.607% | Wgt Acc: 98.993% | LR: 1.250000e-04 | Dur: 168.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    1.    0.    0.   50.]
 [   0.  758.    0.    1.   27.]
 [   0.    0. 1029.    0.   33.]
 [   0.    0.    0.  772.   32.]
 [   0.    1.    3.    0.  858.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.262 | Acc: 65.680% | Wgt Acc: 62.461% | Dur: 14.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   1.  21.  17.]
 [  1.  43.   8.   2.   6.]
 [  1.   9.  36.   1.  18.]
 [ 15.   5.   8.  54.   6.]
 [  4.  18.  22.   8. 133.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 214
I - Training: 
	I - Batch: 50 | Loss: 0.045 | Acc: 96.750% | Wgt Acc: 99.043%
	I - Batch: 100 | Loss: 0.045 | Acc: 96.938% | Wgt Acc: 99.039%
	I - Batch: 150 | Loss: 0.046 | Acc: 96.833% | Wgt Acc: 98.950%
	I - Batch: 200 | Loss: 0.048 | Acc: 96.688% | Wgt Acc: 98.912%
	I - Batch: 250 | Loss: 0.050 | Acc: 96.450% | Wgt Acc: 98.753%
I - num batch: 273
I - Train -- Loss: 0.051 | Acc: 96.355% | Wgt Acc: 98.756% | LR: 1.250000e-04 | Dur: 165.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 789.    0.    0.    1.   50.]
 [   0.  758.    0.    0.   24.]
 [   0.    0. 1030.    0.   41.]
 [   0.    0.    0.  771.   30.]
 [   8.    2.    2.    1.  855.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.216 | Acc: 67.456% | Wgt Acc: 58.678% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   2.   2.  13.   6.]
 [  1.  36.   4.   0.   3.]
 [  0.   8.  28.   0.   5.]
 [  9.   4.   7.  51.   4.]
 [ 13.  28.  34.  22. 162.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773.  781.]

I - Epoch: 215
I - Training: 
	I - Batch: 50 | Loss: 0.038 | Acc: 97.500% | Wgt Acc: 99.266%
	I - Batch: 100 | Loss: 0.044 | Acc: 97.062% | Wgt Acc: 98.993%
	I - Batch: 150 | Loss: 0.047 | Acc: 96.875% | Wgt Acc: 99.004%
	I - Batch: 200 | Loss: 0.046 | Acc: 96.719% | Wgt Acc: 98.991%
	I - Batch: 250 | Loss: 0.046 | Acc: 96.850% | Wgt Acc: 99.036%
I - num batch: 259
I - Train -- Loss: 0.046 | Acc: 96.838% | Wgt Acc: 99.040% | LR: 1.250000e-04 | Dur: 158.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 792.    0.    0.    0.   43.]
 [   2.  759.    0.    0.   22.]
 [   1.    1. 1031.    0.   28.]
 [   0.    0.    0.  772.   30.]
 [   2.    0.    1.    1.  658.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.316 | Acc: 64.892% | Wgt Acc: 60.846% | Dur: 14.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 74.   6.   4.  24.  20.]
 [  0.  37.   5.   1.   8.]
 [  2.  14.  32.   0.   8.]
 [  8.   8.   6.  51.   9.]
 [  4.  13.  28.  10. 135.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 216
I - Training: 
	I - Batch: 50 | Loss: 0.047 | Acc: 96.875% | Wgt Acc: 99.185%
	I - Batch: 100 | Loss: 0.041 | Acc: 97.250% | Wgt Acc: 99.290%
	I - Batch: 150 | Loss: 0.044 | Acc: 96.917% | Wgt Acc: 99.198%
	I - Batch: 200 | Loss: 0.045 | Acc: 96.812% | Wgt Acc: 99.115%
	I - Batch: 250 | Loss: 0.044 | Acc: 96.925% | Wgt Acc: 99.135%
I - num batch: 273
I - Train -- Loss: 0.043 | Acc: 96.974% | Wgt Acc: 99.133% | LR: 1.250000e-04 | Dur: 166.88s
I - Confusion Matrix: [row->prediction - col->label]
[[ 794.    0.    0.    0.   47.]
 [   0.  760.    0.    0.   15.]
 [   0.    0. 1032.    0.   37.]
 [   0.    0.    0.  772.   29.]
 [   3.    0.    0.    1.  872.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.316 | Acc: 64.300% | Wgt Acc: 56.326% | Dur: 14.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   3.  11.   9.]
 [  0.  26.   1.   0.   2.]
 [  1.  19.  29.   2.  10.]
 [ 20.   6.   9.  57.   6.]
 [  6.  24.  33.  16. 153.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 217
I - Training: 
	I - Batch: 50 | Loss: 0.036 | Acc: 97.750% | Wgt Acc: 99.419%
	I - Batch: 100 | Loss: 0.039 | Acc: 97.250% | Wgt Acc: 99.170%
	I - Batch: 150 | Loss: 0.042 | Acc: 97.125% | Wgt Acc: 99.092%
	I - Batch: 200 | Loss: 0.041 | Acc: 97.281% | Wgt Acc: 99.178%
	I - Batch: 250 | Loss: 0.040 | Acc: 97.350% | Wgt Acc: 99.222%
I - num batch: 273
I - Train -- Loss: 0.041 | Acc: 97.364% | Wgt Acc: 99.212% | LR: 1.250000e-04 | Dur: 166.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    0.    1.   37.]
 [   0.  760.    0.    0.   15.]
 [   1.    0. 1032.    0.   23.]
 [   0.    0.    0.  772.   35.]
 [   3.    0.    0.    0.  890.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.176 | Acc: 67.456% | Wgt Acc: 61.758% | Dur: 14.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   1.   3.  16.  10.]
 [  1.  39.   3.   2.   4.]
 [  2.  16.  34.   1.  10.]
 [ 14.   6.   7.  57.   8.]
 [  7.  16.  28.  10. 148.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 218
I - Training: 
	I - Batch: 50 | Loss: 0.036 | Acc: 97.875% | Wgt Acc: 99.462%
	I - Batch: 100 | Loss: 0.036 | Acc: 97.625% | Wgt Acc: 99.395%
	I - Batch: 150 | Loss: 0.036 | Acc: 97.708% | Wgt Acc: 99.412%
	I - Batch: 200 | Loss: 0.036 | Acc: 97.719% | Wgt Acc: 99.388%
	I - Batch: 250 | Loss: 0.037 | Acc: 97.575% | Wgt Acc: 99.330%
I - num batch: 273
I - Train -- Loss: 0.039 | Acc: 97.455% | Wgt Acc: 99.299% | LR: 1.250000e-04 | Dur: 165.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 796.    0.    0.    0.   36.]
 [   0.  760.    0.    0.   11.]
 [   0.    0. 1032.    1.   28.]
 [   0.    0.    0.  772.   34.]
 [   1.    0.    0.    0.  891.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.324 | Acc: 64.103% | Wgt Acc: 54.988% | Dur: 14.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   3.   2.  11.   6.]
 [  0.  31.   2.   1.   2.]
 [  0.  14.  28.   1.   7.]
 [ 13.   6.   7.  53.   7.]
 [ 20.  24.  36.  20. 158.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 219
I - Training: 
	I - Batch: 50 | Loss: 0.036 | Acc: 96.875% | Wgt Acc: 99.060%
	I - Batch: 100 | Loss: 0.037 | Acc: 97.312% | Wgt Acc: 99.181%
	I - Batch: 150 | Loss: 0.036 | Acc: 97.292% | Wgt Acc: 99.177%
	I - Batch: 200 | Loss: 0.039 | Acc: 97.000% | Wgt Acc: 99.075%
	I - Batch: 250 | Loss: 0.041 | Acc: 96.875% | Wgt Acc: 99.049%
I - num batch: 273
I - Train -- Loss: 0.042 | Acc: 96.836% | Wgt Acc: 99.050% | LR: 1.250000e-04 | Dur: 163.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 795.    0.    0.    0.   37.]
 [   0.  758.    0.    0.   18.]
 [   0.    0. 1032.    0.   45.]
 [   0.    0.    0.  771.   32.]
 [   2.    2.    0.    2.  868.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.306 | Acc: 66.667% | Wgt Acc: 59.174% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   4.  15.  10.]
 [  0.  35.   1.   1.   3.]
 [  0.  12.  32.   0.   5.]
 [ 16.   8.   5.  56.   7.]
 [ 12.  19.  33.  14. 155.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 220
I - Training: 
	I - Batch: 50 | Loss: 0.039 | Acc: 97.625% | Wgt Acc: 99.388%
	I - Batch: 100 | Loss: 0.043 | Acc: 96.750% | Wgt Acc: 99.045%
	I - Batch: 150 | Loss: 0.050 | Acc: 96.542% | Wgt Acc: 98.817%
	I - Batch: 200 | Loss: 0.051 | Acc: 96.562% | Wgt Acc: 98.808%
	I - Batch: 250 | Loss: 0.055 | Acc: 96.200% | Wgt Acc: 98.656%
I - num batch: 273
I - Train -- Loss: 0.056 | Acc: 96.149% | Wgt Acc: 98.562% | LR: 1.250000e-04 | Dur: 165.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 793.    0.    1.    0.   45.]
 [   0.  759.    0.    0.   23.]
 [   1.    1. 1025.    4.   44.]
 [   1.    0.    3.  765.   36.]
 [   2.    0.    3.    4.  852.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.256 | Acc: 65.878% | Wgt Acc: 58.563% | Dur: 14.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   4.  18.  15.]
 [  0.  36.   3.   3.   2.]
 [  0.  15.  31.   2.   7.]
 [ 12.   7.   6.  48.   4.]
 [  9.  17.  31.  15. 152.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 221
I - Training: 
	I - Batch: 50 | Loss: 0.060 | Acc: 96.875% | Wgt Acc: 99.093%
	I - Batch: 100 | Loss: 0.062 | Acc: 95.938% | Wgt Acc: 98.771%
	I - Batch: 150 | Loss: 0.059 | Acc: 96.292% | Wgt Acc: 98.926%
	I - Batch: 200 | Loss: 0.055 | Acc: 96.531% | Wgt Acc: 99.015%
	I - Batch: 250 | Loss: 0.053 | Acc: 96.650% | Wgt Acc: 99.065%
I - num batch: 273
I - Train -- Loss: 0.053 | Acc: 96.584% | Wgt Acc: 99.010% | LR: 1.250000e-04 | Dur: 169.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 797.    0.    0.    0.   41.]
 [   0.  759.    0.    1.   19.]
 [   0.    0. 1029.    0.   41.]
 [   0.    0.    1.  772.   43.]
 [   0.    1.    2.    0.  856.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.216 | Acc: 65.878% | Wgt Acc: 60.085% | Dur: 14.63s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   4.   2.   9.  10.]
 [  0.  34.   1.   1.   3.]
 [  1.  16.  32.   1.  15.]
 [ 17.   4.   6.  61.   6.]
 [  9.  20.  34.  14. 146.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 797.  760. 1032.  773. 1000.]

I - Epoch: 222
I - Training: 
	I - Batch: 50 | Loss: 0.053 | Acc: 96.250% | Wgt Acc: 98.804%
	I - Batch: 100 | Loss: 0.054 | Acc: 96.125% | Wgt Acc: 98.820%
