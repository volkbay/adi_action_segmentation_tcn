Sun Oct 30 01:55:32 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.23, 0.24, 0.23, 0.25, 0.1], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'singleBackgroundPath': 'new_background', 'singleBackgroundPickle': True, 'tossFirstLastFrames': True}, 'dataPath': '/data_ssd/processed/kinetics400/', 'dropoutRate': 0.6, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat', 'background'], 'lastLayerInitUniform': False, 'learningRate': 0.01, 'logBatchAt': 50, 'maxValidationAcc': 71.20315581854044, 'maxValidationTrainNo': 64, 'modelVersion': 17, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [5, 10, 20, 25], 'trainNo': 67, 'validationAccThr': 75, 'warmStartConfig': {'checkpointFile': './sav/model17_trainNo60_at_epoch_197_with_acc_71_60_checkpoint.pth.tar', 'checkpointModelNo': 17, 'freezeSpatialCNN': False, 'warmStartFlag': False}, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Train set length:  3547
I - Label distribution: [ 697.  578.  734.  538. 1000.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test
I - Loading file: dataset_test_background00_no_samples180.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test/new_background
I - New label distribution: [ 88.  78.  75.  86. 180.]

I - Test set length:  507
I - Label distribution: [ 88.  78.  75.  86. 180.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(4) data number in dataset:  tensor([138520,  52779, 187266, 207664,   6863, 177705, 186264,    887,  61817,
        191266, 205486, 121594,  10213,     82, 193461, 109608])
I - Initializing model TCNv17
I - Number of Model Parameters: 638752
I - Model output shape:  torch.Size([16, 5])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv17                                   [16, 5]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 128, 64, 64]         6
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Conv2d: 2-2                       --                        6,272
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─OutputShiftSqueeze: 2-4           --                        --
│    └─One: 2-5                          [1]                       --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─OutputScale: 2-7                  --                        --
│    └─Empty: 2-8                        [128, 48, 1, 1]           --
│    └─Empty: 2-9                        [128, 48, 1, 1]           --
│    └─Empty: 2-10                       [128]                     --
│    └─Empty: 2-11                       [128]                     --
│    └─BatchNorm2d: 2-12                 [16, 128, 64, 64]         --
│    └─Scaler: 2-13                      [16, 128, 64, 64]         --
│    └─ReLU: 2-14                        [16, 128, 64, 64]         --
│    └─Empty: 2-15                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Conv2d: 2-18                      --                        147,584
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-20                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-3          [16, 128, 32, 32]         147,590
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-22                   [16, 128, 32, 32]         --
│    └─Empty: 2-23                       [16, 128, 32, 32]         --
│    └─Empty: 2-24                       [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [128, 128, 3, 3]          --
│    └─Empty: 2-29                       [128, 128, 3, 3]          --
│    └─Empty: 2-30                       [128]                     --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-33                      --                        147,584
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-35                       [128]                     --
│    └─BatchNorm2d: 2-36                 [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-38                      [16, 128, 32, 32]         --
│    └─ReLU: 2-39                        [16, 128, 32, 32]         --
│    └─Empty: 2-40                       [16, 128, 32, 32]         --
│    └─Clamp: 2-41                       [16, 128, 32, 32]         --
├─Dropout2d: 1-5                         [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-6          [16, 128, 16, 16]         131,078
│    └─MaxPool2d: 2-42                   [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─Empty: 2-45                       [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Conv2d: 2-47                      --                        16,512
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-49                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-50          --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-52                         [1]                       --
│    └─OutputScale: 2-53                 --                        --
│    └─Empty: 2-54                       [128, 128, 3, 3]          --
│    └─Empty: 2-55                       [128, 128, 3, 3]          --
│    └─Empty: 2-56                       [128]                     --
│    └─Empty: 2-57                       [128]                     --
│    └─BatchNorm2d: 2-58                 [16, 128, 16, 16]         --
│    └─Scaler: 2-59                      [16, 128, 16, 16]         --
│    └─ReLU: 2-60                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-63                      --                        147,584
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-65                       [16, 128, 16, 16]         --
│    └─Clamp: 2-66                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-8                 [16, 128, 16, 16]         16,518
│    └─OutputShiftSqueeze: 2-68          --                        --
│    └─One: 2-69                         [1]                       --
│    └─OutputScale: 2-70                 --                        --
│    └─Empty: 2-71                       [128, 128, 1, 1]          --
│    └─Empty: 2-72                       [128, 128, 1, 1]          --
│    └─Empty: 2-73                       [128]                     --
│    └─Empty: 2-74                       [128]                     --
│    └─BatchNorm2d: 2-75                 [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-78                      --                        147,584
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-80                      [16, 128, 16, 16]         --
│    └─ReLU: 2-81                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-83                       [16, 128, 16, 16]         --
│    └─Clamp: 2-84                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-10         [16, 128, 16, 16]         145,526
│    └─MaxPool2d: 2-85                   [16, 128, 16, 16]         --
│    └─Empty: 2-86                       [16, 128, 16, 16]         --
│    └─Empty: 2-87                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-88          --                        --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-91                         [1]                       --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Conv2d: 2-93                      --                        2,064
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-95                 --                        --
│    └─Empty: 2-96                       [128, 128, 3, 3]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-98                       [128, 128, 3, 3]          --
│    └─Empty: 2-99                       [128]                     --
│    └─Empty: 2-100                      [128]                     --
│    └─BatchNorm2d: 2-101                [16, 128, 16, 16]         --
│    └─Scaler: 2-102                     [16, 128, 16, 16]         --
│    └─ReLU: 2-103                       [16, 128, 16, 16]         --
│    └─Empty: 2-104                      [16, 128, 16, 16]         --
│    └─Clamp: 2-105                      [16, 128, 16, 16]         --
├─Dropout2d: 1-11                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-108                     --                        18,448
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-13         [16, 128, 8, 8]           147,590
│    └─MaxPool2d: 2-110                  [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-112                      [16, 128, 8, 8]           --
│    └─Empty: 2-113                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-114         --                        --
│    └─One: 2-115                        [1]                       --
│    └─OutputScale: 2-116                --                        --
│    └─Empty: 2-117                      [128, 128, 3, 3]          --
│    └─Empty: 2-118                      [128, 128, 3, 3]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-121                      [128]                     --
│    └─Empty: 2-122                      [128]                     --
│    └─BatchNorm2d: 2-123                [16, 128, 8, 8]           --
│    └─Scaler: 2-124                     [16, 128, 8, 8]           --
│    └─ReLU: 2-125                       [16, 128, 8, 8]           --
│    └─Empty: 2-126                      [16, 128, 8, 8]           --
│    └─Clamp: 2-127                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-14                [16, 16, 8, 8]            2,070
├─Linear: 1                              --                        --
│    └─Scaler: 2-128                     --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-129         --                        --
│    └─One: 2-130                        [1]                       --
│    └─OutputScale: 2-131                --                        --
│    └─Empty: 2-132                      [16, 128, 1, 1]           --
│    └─Empty: 2-133                      [16, 128, 1, 1]           --
│    └─Empty: 2-134                      [16]                      --
│    └─Empty: 2-135                      [16]                      --
│    └─BatchNorm2d: 2-136                [16, 16, 8, 8]            --
│    └─Scaler: 2-137                     [16, 16, 8, 8]            --
│    └─ReLU: 2-138                       [16, 16, 8, 8]            --
│    └─Empty: 2-139                      [16, 16, 8, 8]            --
│    └─Clamp: 2-140                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 16, 8, 8]            18,454
│    └─MaxPool2d: 2-141                  [16, 128, 8, 8]           --
│    └─Empty: 2-142                      [16, 128, 8, 8]           --
│    └─Empty: 2-143                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-144         --                        --
│    └─One: 2-145                        [1]                       --
│    └─OutputScale: 2-146                --                        --
│    └─Empty: 2-147                      [16, 128, 3, 3]           --
│    └─Empty: 2-148                      [16, 128, 3, 3]           --
│    └─Empty: 2-149                      [16]                      --
│    └─Empty: 2-150                      [16]                      --
│    └─BatchNorm2d: 2-151                [16, 16, 8, 8]            --
│    └─Scaler: 2-152                     [16, 16, 8, 8]            --
│    └─ReLU: 2-153                       [16, 16, 8, 8]            --
│    └─Empty: 2-154                      [16, 16, 8, 8]            --
│    └─Clamp: 2-155                      [16, 16, 8, 8]            --
├─Dropout2d: 1-16                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-17                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-156         --                        --
│    └─One: 2-157                        [1]                       --
│    └─OutputScale: 2-158                --                        --
│    └─Empty: 2-159                      [128, 48, 1, 1]           --
│    └─Empty: 2-160                      [128, 48, 1, 1]           --
│    └─Empty: 2-161                      [128]                     --
│    └─Empty: 2-162                      [128]                     --
│    └─BatchNorm2d: 2-163                [16, 128, 64, 64]         --
│    └─Scaler: 2-164                     [16, 128, 64, 64]         --
│    └─ReLU: 2-165                       [16, 128, 64, 64]         --
│    └─Empty: 2-166                      [16, 128, 64, 64]         --
│    └─Clamp: 2-167                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-18         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-168                  [16, 128, 32, 32]         --
│    └─Empty: 2-169                      [16, 128, 32, 32]         --
│    └─Empty: 2-170                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-171         --                        --
│    └─One: 2-172                        [1]                       --
│    └─OutputScale: 2-173                --                        --
│    └─Empty: 2-174                      [128, 128, 3, 3]          --
│    └─Empty: 2-175                      [128, 128, 3, 3]          --
│    └─Empty: 2-176                      [128]                     --
│    └─Empty: 2-177                      [128]                     --
│    └─BatchNorm2d: 2-178                [16, 128, 32, 32]         --
│    └─Scaler: 2-179                     [16, 128, 32, 32]         --
│    └─ReLU: 2-180                       [16, 128, 32, 32]         --
│    └─Empty: 2-181                      [16, 128, 32, 32]         --
│    └─Clamp: 2-182                      [16, 128, 32, 32]         --
├─Dropout2d: 1-19                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-20         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-183                  [16, 128, 16, 16]         --
│    └─Empty: 2-184                      [16, 128, 16, 16]         --
│    └─Empty: 2-185                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-186         --                        --
│    └─One: 2-187                        [1]                       --
│    └─OutputScale: 2-188                --                        --
│    └─Empty: 2-189                      [128, 128, 3, 3]          --
│    └─Empty: 2-190                      [128, 128, 3, 3]          --
│    └─Empty: 2-191                      [128]                     --
│    └─Empty: 2-192                      [128]                     --
│    └─BatchNorm2d: 2-193                [16, 128, 16, 16]         --
│    └─Scaler: 2-194                     [16, 128, 16, 16]         --
│    └─ReLU: 2-195                       [16, 128, 16, 16]         --
│    └─Empty: 2-196                      [16, 128, 16, 16]         --
│    └─Clamp: 2-197                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-21                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-198         --                        --
│    └─One: 2-199                        [1]                       --
│    └─OutputScale: 2-200                --                        --
│    └─Empty: 2-201                      [128, 128, 1, 1]          --
│    └─Empty: 2-202                      [128, 128, 1, 1]          --
│    └─Empty: 2-203                      [128]                     --
│    └─Empty: 2-204                      [128]                     --
│    └─BatchNorm2d: 2-205                [16, 128, 16, 16]         --
│    └─Scaler: 2-206                     [16, 128, 16, 16]         --
│    └─ReLU: 2-207                       [16, 128, 16, 16]         --
│    └─Empty: 2-208                      [16, 128, 16, 16]         --
│    └─Clamp: 2-209                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-22         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-210                  [16, 128, 16, 16]         --
│    └─Empty: 2-211                      [16, 128, 16, 16]         --
│    └─Empty: 2-212                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-213         --                        --
│    └─One: 2-214                        [1]                       --
│    └─OutputScale: 2-215                --                        --
│    └─Empty: 2-216                      [128, 128, 3, 3]          --
│    └─Empty: 2-217                      [128, 128, 3, 3]          --
│    └─Empty: 2-218                      [128]                     --
│    └─Empty: 2-219                      [128]                     --
│    └─BatchNorm2d: 2-220                [16, 128, 16, 16]         --
│    └─Scaler: 2-221                     [16, 128, 16, 16]         --
│    └─ReLU: 2-222                       [16, 128, 16, 16]         --
│    └─Empty: 2-223                      [16, 128, 16, 16]         --
│    └─Clamp: 2-224                      [16, 128, 16, 16]         --
├─Dropout2d: 1-23                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-24         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-225                  [16, 128, 8, 8]           --
│    └─Empty: 2-226                      [16, 128, 8, 8]           --
│    └─Empty: 2-227                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-228         --                        --
│    └─One: 2-229                        [1]                       --
│    └─OutputScale: 2-230                --                        --
│    └─Empty: 2-231                      [128, 128, 3, 3]          --
│    └─Empty: 2-232                      [128, 128, 3, 3]          --
│    └─Empty: 2-233                      [128]                     --
│    └─Empty: 2-234                      [128]                     --
│    └─BatchNorm2d: 2-235                [16, 128, 8, 8]           --
│    └─Scaler: 2-236                     [16, 128, 8, 8]           --
│    └─ReLU: 2-237                       [16, 128, 8, 8]           --
│    └─Empty: 2-238                      [16, 128, 8, 8]           --
│    └─Clamp: 2-239                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-25                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-240         --                        --
│    └─One: 2-241                        [1]                       --
│    └─OutputScale: 2-242                --                        --
│    └─Empty: 2-243                      [16, 128, 1, 1]           --
│    └─Empty: 2-244                      [16, 128, 1, 1]           --
│    └─Empty: 2-245                      [16]                      --
│    └─Empty: 2-246                      [16]                      --
│    └─BatchNorm2d: 2-247                [16, 16, 8, 8]            --
│    └─Scaler: 2-248                     [16, 16, 8, 8]            --
│    └─ReLU: 2-249                       [16, 16, 8, 8]            --
│    └─Empty: 2-250                      [16, 16, 8, 8]            --
│    └─Clamp: 2-251                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-252                  [16, 128, 8, 8]           --
│    └─Empty: 2-253                      [16, 128, 8, 8]           --
│    └─Empty: 2-254                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-255         --                        --
│    └─One: 2-256                        [1]                       --
│    └─OutputScale: 2-257                --                        --
│    └─Empty: 2-258                      [16, 128, 3, 3]           --
│    └─Empty: 2-259                      [16, 128, 3, 3]           --
│    └─Empty: 2-260                      [16]                      --
│    └─Empty: 2-261                      [16]                      --
│    └─BatchNorm2d: 2-262                [16, 16, 8, 8]            --
│    └─Scaler: 2-263                     [16, 16, 8, 8]            --
│    └─ReLU: 2-264                       [16, 16, 8, 8]            --
│    └─Empty: 2-265                      [16, 16, 8, 8]            --
│    └─Clamp: 2-266                      [16, 16, 8, 8]            --
├─Dropout2d: 1-27                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-28                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-267         --                        --
│    └─One: 2-268                        [1]                       --
│    └─OutputScale: 2-269                --                        --
│    └─Empty: 2-270                      [128, 48, 1, 1]           --
│    └─Empty: 2-271                      [128, 48, 1, 1]           --
│    └─Empty: 2-272                      [128]                     --
│    └─Empty: 2-273                      [128]                     --
│    └─BatchNorm2d: 2-274                [16, 128, 64, 64]         --
│    └─Scaler: 2-275                     [16, 128, 64, 64]         --
│    └─ReLU: 2-276                       [16, 128, 64, 64]         --
│    └─Empty: 2-277                      [16, 128, 64, 64]         --
│    └─Clamp: 2-278                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-29         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-279                  [16, 128, 32, 32]         --
│    └─Empty: 2-280                      [16, 128, 32, 32]         --
│    └─Empty: 2-281                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-282         --                        --
│    └─One: 2-283                        [1]                       --
│    └─OutputScale: 2-284                --                        --
│    └─Empty: 2-285                      [128, 128, 3, 3]          --
│    └─Empty: 2-286                      [128, 128, 3, 3]          --
│    └─Empty: 2-287                      [128]                     --
│    └─Empty: 2-288                      [128]                     --
│    └─BatchNorm2d: 2-289                [16, 128, 32, 32]         --
│    └─Scaler: 2-290                     [16, 128, 32, 32]         --
│    └─ReLU: 2-291                       [16, 128, 32, 32]         --
│    └─Empty: 2-292                      [16, 128, 32, 32]         --
│    └─Clamp: 2-293                      [16, 128, 32, 32]         --
├─Dropout2d: 1-30                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-294                  [16, 128, 16, 16]         --
│    └─Empty: 2-295                      [16, 128, 16, 16]         --
│    └─Empty: 2-296                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-297         --                        --
│    └─One: 2-298                        [1]                       --
│    └─OutputScale: 2-299                --                        --
│    └─Empty: 2-300                      [128, 128, 3, 3]          --
│    └─Empty: 2-301                      [128, 128, 3, 3]          --
│    └─Empty: 2-302                      [128]                     --
│    └─Empty: 2-303                      [128]                     --
│    └─BatchNorm2d: 2-304                [16, 128, 16, 16]         --
│    └─Scaler: 2-305                     [16, 128, 16, 16]         --
│    └─ReLU: 2-306                       [16, 128, 16, 16]         --
│    └─Empty: 2-307                      [16, 128, 16, 16]         --
│    └─Clamp: 2-308                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-32                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-309         --                        --
│    └─One: 2-310                        [1]                       --
│    └─OutputScale: 2-311                --                        --
│    └─Empty: 2-312                      [128, 128, 1, 1]          --
│    └─Empty: 2-313                      [128, 128, 1, 1]          --
│    └─Empty: 2-314                      [128]                     --
│    └─Empty: 2-315                      [128]                     --
│    └─BatchNorm2d: 2-316                [16, 128, 16, 16]         --
│    └─Scaler: 2-317                     [16, 128, 16, 16]         --
│    └─ReLU: 2-318                       [16, 128, 16, 16]         --
│    └─Empty: 2-319                      [16, 128, 16, 16]         --
│    └─Clamp: 2-320                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-33         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-321                  [16, 128, 16, 16]         --
│    └─Empty: 2-322                      [16, 128, 16, 16]         --
│    └─Empty: 2-323                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-324         --                        --
│    └─One: 2-325                        [1]                       --
│    └─OutputScale: 2-326                --                        --
│    └─Empty: 2-327                      [128, 128, 3, 3]          --
│    └─Empty: 2-328                      [128, 128, 3, 3]          --
│    └─Empty: 2-329                      [128]                     --
│    └─Empty: 2-330                      [128]                     --
│    └─BatchNorm2d: 2-331                [16, 128, 16, 16]         --
│    └─Scaler: 2-332                     [16, 128, 16, 16]         --
│    └─ReLU: 2-333                       [16, 128, 16, 16]         --
│    └─Empty: 2-334                      [16, 128, 16, 16]         --
│    └─Clamp: 2-335                      [16, 128, 16, 16]         --
├─Dropout2d: 1-34                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-35         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-336                  [16, 128, 8, 8]           --
│    └─Empty: 2-337                      [16, 128, 8, 8]           --
│    └─Empty: 2-338                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-339         --                        --
│    └─One: 2-340                        [1]                       --
│    └─OutputScale: 2-341                --                        --
│    └─Empty: 2-342                      [128, 128, 3, 3]          --
│    └─Empty: 2-343                      [128, 128, 3, 3]          --
│    └─Empty: 2-344                      [128]                     --
│    └─Empty: 2-345                      [128]                     --
│    └─BatchNorm2d: 2-346                [16, 128, 8, 8]           --
│    └─Scaler: 2-347                     [16, 128, 8, 8]           --
│    └─ReLU: 2-348                       [16, 128, 8, 8]           --
│    └─Empty: 2-349                      [16, 128, 8, 8]           --
│    └─Clamp: 2-350                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-36                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-351         --                        --
│    └─One: 2-352                        [1]                       --
│    └─OutputScale: 2-353                --                        --
│    └─Empty: 2-354                      [16, 128, 1, 1]           --
│    └─Empty: 2-355                      [16, 128, 1, 1]           --
│    └─Empty: 2-356                      [16]                      --
│    └─Empty: 2-357                      [16]                      --
│    └─BatchNorm2d: 2-358                [16, 16, 8, 8]            --
│    └─Scaler: 2-359                     [16, 16, 8, 8]            --
│    └─ReLU: 2-360                       [16, 16, 8, 8]            --
│    └─Empty: 2-361                      [16, 16, 8, 8]            --
│    └─Clamp: 2-362                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-37         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-363                  [16, 128, 8, 8]           --
│    └─Empty: 2-364                      [16, 128, 8, 8]           --
│    └─Empty: 2-365                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-366         --                        --
│    └─One: 2-367                        [1]                       --
│    └─OutputScale: 2-368                --                        --
│    └─Empty: 2-369                      [16, 128, 3, 3]           --
│    └─Empty: 2-370                      [16, 128, 3, 3]           --
│    └─Empty: 2-371                      [16]                      --
│    └─Empty: 2-372                      [16]                      --
│    └─BatchNorm2d: 2-373                [16, 16, 8, 8]            --
│    └─Scaler: 2-374                     [16, 16, 8, 8]            --
│    └─ReLU: 2-375                       [16, 16, 8, 8]            --
│    └─Empty: 2-376                      [16, 16, 8, 8]            --
│    └─Clamp: 2-377                      [16, 16, 8, 8]            --
├─Dropout2d: 1-38                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-39                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-378         --                        --
│    └─One: 2-379                        [1]                       --
│    └─OutputScale: 2-380                --                        --
│    └─Empty: 2-381                      [128, 48, 1, 1]           --
│    └─Empty: 2-382                      [128, 48, 1, 1]           --
│    └─Empty: 2-383                      [128]                     --
│    └─Empty: 2-384                      [128]                     --
│    └─BatchNorm2d: 2-385                [16, 128, 64, 64]         --
│    └─Scaler: 2-386                     [16, 128, 64, 64]         --
│    └─ReLU: 2-387                       [16, 128, 64, 64]         --
│    └─Empty: 2-388                      [16, 128, 64, 64]         --
│    └─Clamp: 2-389                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-40         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-390                  [16, 128, 32, 32]         --
│    └─Empty: 2-391                      [16, 128, 32, 32]         --
│    └─Empty: 2-392                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-393         --                        --
│    └─One: 2-394                        [1]                       --
│    └─OutputScale: 2-395                --                        --
│    └─Empty: 2-396                      [128, 128, 3, 3]          --
│    └─Empty: 2-397                      [128, 128, 3, 3]          --
│    └─Empty: 2-398                      [128]                     --
│    └─Empty: 2-399                      [128]                     --
│    └─BatchNorm2d: 2-400                [16, 128, 32, 32]         --
│    └─Scaler: 2-401                     [16, 128, 32, 32]         --
│    └─ReLU: 2-402                       [16, 128, 32, 32]         --
│    └─Empty: 2-403                      [16, 128, 32, 32]         --
│    └─Clamp: 2-404                      [16, 128, 32, 32]         --
├─Dropout2d: 1-41                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-42         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-405                  [16, 128, 16, 16]         --
│    └─Empty: 2-406                      [16, 128, 16, 16]         --
│    └─Empty: 2-407                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-408         --                        --
│    └─One: 2-409                        [1]                       --
│    └─OutputScale: 2-410                --                        --
│    └─Empty: 2-411                      [128, 128, 3, 3]          --
│    └─Empty: 2-412                      [128, 128, 3, 3]          --
│    └─Empty: 2-413                      [128]                     --
│    └─Empty: 2-414                      [128]                     --
│    └─BatchNorm2d: 2-415                [16, 128, 16, 16]         --
│    └─Scaler: 2-416                     [16, 128, 16, 16]         --
│    └─ReLU: 2-417                       [16, 128, 16, 16]         --
│    └─Empty: 2-418                      [16, 128, 16, 16]         --
│    └─Clamp: 2-419                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-43                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-420         --                        --
│    └─One: 2-421                        [1]                       --
│    └─OutputScale: 2-422                --                        --
│    └─Empty: 2-423                      [128, 128, 1, 1]          --
│    └─Empty: 2-424                      [128, 128, 1, 1]          --
│    └─Empty: 2-425                      [128]                     --
│    └─Empty: 2-426                      [128]                     --
│    └─BatchNorm2d: 2-427                [16, 128, 16, 16]         --
│    └─Scaler: 2-428                     [16, 128, 16, 16]         --
│    └─ReLU: 2-429                       [16, 128, 16, 16]         --
│    └─Empty: 2-430                      [16, 128, 16, 16]         --
│    └─Clamp: 2-431                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-44         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-432                  [16, 128, 16, 16]         --
│    └─Empty: 2-433                      [16, 128, 16, 16]         --
│    └─Empty: 2-434                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-435         --                        --
│    └─One: 2-436                        [1]                       --
│    └─OutputScale: 2-437                --                        --
│    └─Empty: 2-438                      [128, 128, 3, 3]          --
│    └─Empty: 2-439                      [128, 128, 3, 3]          --
│    └─Empty: 2-440                      [128]                     --
│    └─Empty: 2-441                      [128]                     --
│    └─BatchNorm2d: 2-442                [16, 128, 16, 16]         --
│    └─Scaler: 2-443                     [16, 128, 16, 16]         --
│    └─ReLU: 2-444                       [16, 128, 16, 16]         --
│    └─Empty: 2-445                      [16, 128, 16, 16]         --
│    └─Clamp: 2-446                      [16, 128, 16, 16]         --
├─Dropout2d: 1-45                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-447                  [16, 128, 8, 8]           --
│    └─Empty: 2-448                      [16, 128, 8, 8]           --
│    └─Empty: 2-449                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-450         --                        --
│    └─One: 2-451                        [1]                       --
│    └─OutputScale: 2-452                --                        --
│    └─Empty: 2-453                      [128, 128, 3, 3]          --
│    └─Empty: 2-454                      [128, 128, 3, 3]          --
│    └─Empty: 2-455                      [128]                     --
│    └─Empty: 2-456                      [128]                     --
│    └─BatchNorm2d: 2-457                [16, 128, 8, 8]           --
│    └─Scaler: 2-458                     [16, 128, 8, 8]           --
│    └─ReLU: 2-459                       [16, 128, 8, 8]           --
│    └─Empty: 2-460                      [16, 128, 8, 8]           --
│    └─Clamp: 2-461                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-47                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-462         --                        --
│    └─One: 2-463                        [1]                       --
│    └─OutputScale: 2-464                --                        --
│    └─Empty: 2-465                      [16, 128, 1, 1]           --
│    └─Empty: 2-466                      [16, 128, 1, 1]           --
│    └─Empty: 2-467                      [16]                      --
│    └─Empty: 2-468                      [16]                      --
│    └─BatchNorm2d: 2-469                [16, 16, 8, 8]            --
│    └─Scaler: 2-470                     [16, 16, 8, 8]            --
│    └─ReLU: 2-471                       [16, 16, 8, 8]            --
│    └─Empty: 2-472                      [16, 16, 8, 8]            --
│    └─Clamp: 2-473                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-474                  [16, 128, 8, 8]           --
│    └─Empty: 2-475                      [16, 128, 8, 8]           --
│    └─Empty: 2-476                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-477         --                        --
│    └─One: 2-478                        [1]                       --
│    └─OutputScale: 2-479                --                        --
│    └─Empty: 2-480                      [16, 128, 3, 3]           --
│    └─Empty: 2-481                      [16, 128, 3, 3]           --
│    └─Empty: 2-482                      [16]                      --
│    └─Empty: 2-483                      [16]                      --
│    └─BatchNorm2d: 2-484                [16, 16, 8, 8]            --
│    └─Scaler: 2-485                     [16, 16, 8, 8]            --
│    └─ReLU: 2-486                       [16, 16, 8, 8]            --
│    └─Empty: 2-487                      [16, 16, 8, 8]            --
│    └─Clamp: 2-488                      [16, 16, 8, 8]            --
├─Dropout2d: 1-49                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-50                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-489         --                        --
│    └─One: 2-490                        [1]                       --
│    └─OutputScale: 2-491                --                        --
│    └─Empty: 2-492                      [128, 48, 1, 1]           --
│    └─Empty: 2-493                      [128, 48, 1, 1]           --
│    └─Empty: 2-494                      [128]                     --
│    └─Empty: 2-495                      [128]                     --
│    └─BatchNorm2d: 2-496                [16, 128, 64, 64]         --
│    └─Scaler: 2-497                     [16, 128, 64, 64]         --
│    └─ReLU: 2-498                       [16, 128, 64, 64]         --
│    └─Empty: 2-499                      [16, 128, 64, 64]         --
│    └─Clamp: 2-500                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-501                  [16, 128, 32, 32]         --
│    └─Empty: 2-502                      [16, 128, 32, 32]         --
│    └─Empty: 2-503                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-504         --                        --
│    └─One: 2-505                        [1]                       --
│    └─OutputScale: 2-506                --                        --
│    └─Empty: 2-507                      [128, 128, 3, 3]          --
│    └─Empty: 2-508                      [128, 128, 3, 3]          --
│    └─Empty: 2-509                      [128]                     --
│    └─Empty: 2-510                      [128]                     --
│    └─BatchNorm2d: 2-511                [16, 128, 32, 32]         --
│    └─Scaler: 2-512                     [16, 128, 32, 32]         --
│    └─ReLU: 2-513                       [16, 128, 32, 32]         --
│    └─Empty: 2-514                      [16, 128, 32, 32]         --
│    └─Clamp: 2-515                      [16, 128, 32, 32]         --
├─Dropout2d: 1-52                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-53         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-516                  [16, 128, 16, 16]         --
│    └─Empty: 2-517                      [16, 128, 16, 16]         --
│    └─Empty: 2-518                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-519         --                        --
│    └─One: 2-520                        [1]                       --
│    └─OutputScale: 2-521                --                        --
│    └─Empty: 2-522                      [128, 128, 3, 3]          --
│    └─Empty: 2-523                      [128, 128, 3, 3]          --
│    └─Empty: 2-524                      [128]                     --
│    └─Empty: 2-525                      [128]                     --
│    └─BatchNorm2d: 2-526                [16, 128, 16, 16]         --
│    └─Scaler: 2-527                     [16, 128, 16, 16]         --
│    └─ReLU: 2-528                       [16, 128, 16, 16]         --
│    └─Empty: 2-529                      [16, 128, 16, 16]         --
│    └─Clamp: 2-530                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-54                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-531         --                        --
│    └─One: 2-532                        [1]                       --
│    └─OutputScale: 2-533                --                        --
│    └─Empty: 2-534                      [128, 128, 1, 1]          --
│    └─Empty: 2-535                      [128, 128, 1, 1]          --
│    └─Empty: 2-536                      [128]                     --
│    └─Empty: 2-537                      [128]                     --
│    └─BatchNorm2d: 2-538                [16, 128, 16, 16]         --
│    └─Scaler: 2-539                     [16, 128, 16, 16]         --
│    └─ReLU: 2-540                       [16, 128, 16, 16]         --
│    └─Empty: 2-541                      [16, 128, 16, 16]         --
│    └─Clamp: 2-542                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-55         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-543                  [16, 128, 16, 16]         --
│    └─Empty: 2-544                      [16, 128, 16, 16]         --
│    └─Empty: 2-545                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-546         --                        --
│    └─One: 2-547                        [1]                       --
│    └─OutputScale: 2-548                --                        --
│    └─Empty: 2-549                      [128, 128, 3, 3]          --
│    └─Empty: 2-550                      [128, 128, 3, 3]          --
│    └─Empty: 2-551                      [128]                     --
│    └─Empty: 2-552                      [128]                     --
│    └─BatchNorm2d: 2-553                [16, 128, 16, 16]         --
│    └─Scaler: 2-554                     [16, 128, 16, 16]         --
│    └─ReLU: 2-555                       [16, 128, 16, 16]         --
│    └─Empty: 2-556                      [16, 128, 16, 16]         --
│    └─Clamp: 2-557                      [16, 128, 16, 16]         --
├─Dropout2d: 1-56                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-57         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-558                  [16, 128, 8, 8]           --
│    └─Empty: 2-559                      [16, 128, 8, 8]           --
│    └─Empty: 2-560                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-561         --                        --
│    └─One: 2-562                        [1]                       --
│    └─OutputScale: 2-563                --                        --
│    └─Empty: 2-564                      [128, 128, 3, 3]          --
│    └─Empty: 2-565                      [128, 128, 3, 3]          --
│    └─Empty: 2-566                      [128]                     --
│    └─Empty: 2-567                      [128]                     --
│    └─BatchNorm2d: 2-568                [16, 128, 8, 8]           --
│    └─Scaler: 2-569                     [16, 128, 8, 8]           --
│    └─ReLU: 2-570                       [16, 128, 8, 8]           --
│    └─Empty: 2-571                      [16, 128, 8, 8]           --
│    └─Clamp: 2-572                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-58                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-573         --                        --
│    └─One: 2-574                        [1]                       --
│    └─OutputScale: 2-575                --                        --
│    └─Empty: 2-576                      [16, 128, 1, 1]           --
│    └─Empty: 2-577                      [16, 128, 1, 1]           --
│    └─Empty: 2-578                      [16]                      --
│    └─Empty: 2-579                      [16]                      --
│    └─BatchNorm2d: 2-580                [16, 16, 8, 8]            --
│    └─Scaler: 2-581                     [16, 16, 8, 8]            --
│    └─ReLU: 2-582                       [16, 16, 8, 8]            --
│    └─Empty: 2-583                      [16, 16, 8, 8]            --
│    └─Clamp: 2-584                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-59         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-585                  [16, 128, 8, 8]           --
│    └─Empty: 2-586                      [16, 128, 8, 8]           --
│    └─Empty: 2-587                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-588         --                        --
│    └─One: 2-589                        [1]                       --
│    └─OutputScale: 2-590                --                        --
│    └─Empty: 2-591                      [16, 128, 3, 3]           --
│    └─Empty: 2-592                      [16, 128, 3, 3]           --
│    └─Empty: 2-593                      [16]                      --
│    └─Empty: 2-594                      [16]                      --
│    └─BatchNorm2d: 2-595                [16, 16, 8, 8]            --
│    └─Scaler: 2-596                     [16, 16, 8, 8]            --
│    └─ReLU: 2-597                       [16, 16, 8, 8]            --
│    └─Empty: 2-598                      [16, 16, 8, 8]            --
│    └─Clamp: 2-599                      [16, 16, 8, 8]            --
├─Dropout2d: 1-60                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-61                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-600         --                        --
│    └─One: 2-601                        [1]                       --
│    └─OutputScale: 2-602                --                        --
│    └─Empty: 2-603                      [128, 48, 1, 1]           --
│    └─Empty: 2-604                      [128, 48, 1, 1]           --
│    └─Empty: 2-605                      [128]                     --
│    └─Empty: 2-606                      [128]                     --
│    └─BatchNorm2d: 2-607                [16, 128, 64, 64]         --
│    └─Scaler: 2-608                     [16, 128, 64, 64]         --
│    └─ReLU: 2-609                       [16, 128, 64, 64]         --
│    └─Empty: 2-610                      [16, 128, 64, 64]         --
│    └─Clamp: 2-611                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-62         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-612                  [16, 128, 32, 32]         --
│    └─Empty: 2-613                      [16, 128, 32, 32]         --
│    └─Empty: 2-614                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-615         --                        --
│    └─One: 2-616                        [1]                       --
│    └─OutputScale: 2-617                --                        --
│    └─Empty: 2-618                      [128, 128, 3, 3]          --
│    └─Empty: 2-619                      [128, 128, 3, 3]          --
│    └─Empty: 2-620                      [128]                     --
│    └─Empty: 2-621                      [128]                     --
│    └─BatchNorm2d: 2-622                [16, 128, 32, 32]         --
│    └─Scaler: 2-623                     [16, 128, 32, 32]         --
│    └─ReLU: 2-624                       [16, 128, 32, 32]         --
│    └─Empty: 2-625                      [16, 128, 32, 32]         --
│    └─Clamp: 2-626                      [16, 128, 32, 32]         --
├─Dropout2d: 1-63                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-64         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-627                  [16, 128, 16, 16]         --
│    └─Empty: 2-628                      [16, 128, 16, 16]         --
│    └─Empty: 2-629                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-630         --                        --
│    └─One: 2-631                        [1]                       --
│    └─OutputScale: 2-632                --                        --
│    └─Empty: 2-633                      [128, 128, 3, 3]          --
│    └─Empty: 2-634                      [128, 128, 3, 3]          --
│    └─Empty: 2-635                      [128]                     --
│    └─Empty: 2-636                      [128]                     --
│    └─BatchNorm2d: 2-637                [16, 128, 16, 16]         --
│    └─Scaler: 2-638                     [16, 128, 16, 16]         --
│    └─ReLU: 2-639                       [16, 128, 16, 16]         --
│    └─Empty: 2-640                      [16, 128, 16, 16]         --
│    └─Clamp: 2-641                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-65                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-642         --                        --
│    └─One: 2-643                        [1]                       --
│    └─OutputScale: 2-644                --                        --
│    └─Empty: 2-645                      [128, 128, 1, 1]          --
│    └─Empty: 2-646                      [128, 128, 1, 1]          --
│    └─Empty: 2-647                      [128]                     --
│    └─Empty: 2-648                      [128]                     --
│    └─BatchNorm2d: 2-649                [16, 128, 16, 16]         --
│    └─Scaler: 2-650                     [16, 128, 16, 16]         --
│    └─ReLU: 2-651                       [16, 128, 16, 16]         --
│    └─Empty: 2-652                      [16, 128, 16, 16]         --
│    └─Clamp: 2-653                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-654                  [16, 128, 16, 16]         --
│    └─Empty: 2-655                      [16, 128, 16, 16]         --
│    └─Empty: 2-656                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-657         --                        --
│    └─One: 2-658                        [1]                       --
│    └─OutputScale: 2-659                --                        --
│    └─Empty: 2-660                      [128, 128, 3, 3]          --
│    └─Empty: 2-661                      [128, 128, 3, 3]          --
│    └─Empty: 2-662                      [128]                     --
│    └─Empty: 2-663                      [128]                     --
│    └─BatchNorm2d: 2-664                [16, 128, 16, 16]         --
│    └─Scaler: 2-665                     [16, 128, 16, 16]         --
│    └─ReLU: 2-666                       [16, 128, 16, 16]         --
│    └─Empty: 2-667                      [16, 128, 16, 16]         --
│    └─Clamp: 2-668                      [16, 128, 16, 16]         --
├─Dropout2d: 1-67                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-669                  [16, 128, 8, 8]           --
│    └─Empty: 2-670                      [16, 128, 8, 8]           --
│    └─Empty: 2-671                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-672         --                        --
│    └─One: 2-673                        [1]                       --
│    └─OutputScale: 2-674                --                        --
│    └─Empty: 2-675                      [128, 128, 3, 3]          --
│    └─Empty: 2-676                      [128, 128, 3, 3]          --
│    └─Empty: 2-677                      [128]                     --
│    └─Empty: 2-678                      [128]                     --
│    └─BatchNorm2d: 2-679                [16, 128, 8, 8]           --
│    └─Scaler: 2-680                     [16, 128, 8, 8]           --
│    └─ReLU: 2-681                       [16, 128, 8, 8]           --
│    └─Empty: 2-682                      [16, 128, 8, 8]           --
│    └─Clamp: 2-683                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-69                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-684         --                        --
│    └─One: 2-685                        [1]                       --
│    └─OutputScale: 2-686                --                        --
│    └─Empty: 2-687                      [16, 128, 1, 1]           --
│    └─Empty: 2-688                      [16, 128, 1, 1]           --
│    └─Empty: 2-689                      [16]                      --
│    └─Empty: 2-690                      [16]                      --
│    └─BatchNorm2d: 2-691                [16, 16, 8, 8]            --
│    └─Scaler: 2-692                     [16, 16, 8, 8]            --
│    └─ReLU: 2-693                       [16, 16, 8, 8]            --
│    └─Empty: 2-694                      [16, 16, 8, 8]            --
│    └─Clamp: 2-695                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-70         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-696                  [16, 128, 8, 8]           --
│    └─Empty: 2-697                      [16, 128, 8, 8]           --
│    └─Empty: 2-698                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-699         --                        --
│    └─One: 2-700                        [1]                       --
│    └─OutputScale: 2-701                --                        --
│    └─Empty: 2-702                      [16, 128, 3, 3]           --
│    └─Empty: 2-703                      [16, 128, 3, 3]           --
│    └─Empty: 2-704                      [16]                      --
│    └─Empty: 2-705                      [16]                      --
│    └─BatchNorm2d: 2-706                [16, 16, 8, 8]            --
│    └─Scaler: 2-707                     [16, 16, 8, 8]            --
│    └─ReLU: 2-708                       [16, 16, 8, 8]            --
│    └─Empty: 2-709                      [16, 16, 8, 8]            --
│    └─Clamp: 2-710                      [16, 16, 8, 8]            --
├─Dropout2d: 1-71                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-72                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-711         --                        --
│    └─One: 2-712                        [1]                       --
│    └─OutputScale: 2-713                --                        --
│    └─Empty: 2-714                      [128, 48, 1, 1]           --
│    └─Empty: 2-715                      [128, 48, 1, 1]           --
│    └─Empty: 2-716                      [128]                     --
│    └─Empty: 2-717                      [128]                     --
│    └─BatchNorm2d: 2-718                [16, 128, 64, 64]         --
│    └─Scaler: 2-719                     [16, 128, 64, 64]         --
│    └─ReLU: 2-720                       [16, 128, 64, 64]         --
│    └─Empty: 2-721                      [16, 128, 64, 64]         --
│    └─Clamp: 2-722                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-723                  [16, 128, 32, 32]         --
│    └─Empty: 2-724                      [16, 128, 32, 32]         --
│    └─Empty: 2-725                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-726         --                        --
│    └─One: 2-727                        [1]                       --
│    └─OutputScale: 2-728                --                        --
│    └─Empty: 2-729                      [128, 128, 3, 3]          --
│    └─Empty: 2-730                      [128, 128, 3, 3]          --
│    └─Empty: 2-731                      [128]                     --
│    └─Empty: 2-732                      [128]                     --
│    └─BatchNorm2d: 2-733                [16, 128, 32, 32]         --
│    └─Scaler: 2-734                     [16, 128, 32, 32]         --
│    └─ReLU: 2-735                       [16, 128, 32, 32]         --
│    └─Empty: 2-736                      [16, 128, 32, 32]         --
│    └─Clamp: 2-737                      [16, 128, 32, 32]         --
├─Dropout2d: 1-74                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-738                  [16, 128, 16, 16]         --
│    └─Empty: 2-739                      [16, 128, 16, 16]         --
│    └─Empty: 2-740                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-741         --                        --
│    └─One: 2-742                        [1]                       --
│    └─OutputScale: 2-743                --                        --
│    └─Empty: 2-744                      [128, 128, 3, 3]          --
│    └─Empty: 2-745                      [128, 128, 3, 3]          --
│    └─Empty: 2-746                      [128]                     --
│    └─Empty: 2-747                      [128]                     --
│    └─BatchNorm2d: 2-748                [16, 128, 16, 16]         --
│    └─Scaler: 2-749                     [16, 128, 16, 16]         --
│    └─ReLU: 2-750                       [16, 128, 16, 16]         --
│    └─Empty: 2-751                      [16, 128, 16, 16]         --
│    └─Clamp: 2-752                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-76                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-753         --                        --
│    └─One: 2-754                        [1]                       --
│    └─OutputScale: 2-755                --                        --
│    └─Empty: 2-756                      [128, 128, 1, 1]          --
│    └─Empty: 2-757                      [128, 128, 1, 1]          --
│    └─Empty: 2-758                      [128]                     --
│    └─Empty: 2-759                      [128]                     --
│    └─BatchNorm2d: 2-760                [16, 128, 16, 16]         --
│    └─Scaler: 2-761                     [16, 128, 16, 16]         --
│    └─ReLU: 2-762                       [16, 128, 16, 16]         --
│    └─Empty: 2-763                      [16, 128, 16, 16]         --
│    └─Clamp: 2-764                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-77         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-765                  [16, 128, 16, 16]         --
│    └─Empty: 2-766                      [16, 128, 16, 16]         --
│    └─Empty: 2-767                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-768         --                        --
│    └─One: 2-769                        [1]                       --
│    └─OutputScale: 2-770                --                        --
│    └─Empty: 2-771                      [128, 128, 3, 3]          --
│    └─Empty: 2-772                      [128, 128, 3, 3]          --
│    └─Empty: 2-773                      [128]                     --
│    └─Empty: 2-774                      [128]                     --
│    └─BatchNorm2d: 2-775                [16, 128, 16, 16]         --
│    └─Scaler: 2-776                     [16, 128, 16, 16]         --
│    └─ReLU: 2-777                       [16, 128, 16, 16]         --
│    └─Empty: 2-778                      [16, 128, 16, 16]         --
│    └─Clamp: 2-779                      [16, 128, 16, 16]         --
├─Dropout2d: 1-78                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-79         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-780                  [16, 128, 8, 8]           --
│    └─Empty: 2-781                      [16, 128, 8, 8]           --
│    └─Empty: 2-782                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-783         --                        --
│    └─One: 2-784                        [1]                       --
│    └─OutputScale: 2-785                --                        --
│    └─Empty: 2-786                      [128, 128, 3, 3]          --
│    └─Empty: 2-787                      [128, 128, 3, 3]          --
│    └─Empty: 2-788                      [128]                     --
│    └─Empty: 2-789                      [128]                     --
│    └─BatchNorm2d: 2-790                [16, 128, 8, 8]           --
│    └─Scaler: 2-791                     [16, 128, 8, 8]           --
│    └─ReLU: 2-792                       [16, 128, 8, 8]           --
│    └─Empty: 2-793                      [16, 128, 8, 8]           --
│    └─Clamp: 2-794                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-80                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-795         --                        --
│    └─One: 2-796                        [1]                       --
│    └─OutputScale: 2-797                --                        --
│    └─Empty: 2-798                      [16, 128, 1, 1]           --
│    └─Empty: 2-799                      [16, 128, 1, 1]           --
│    └─Empty: 2-800                      [16]                      --
│    └─Empty: 2-801                      [16]                      --
│    └─BatchNorm2d: 2-802                [16, 16, 8, 8]            --
│    └─Scaler: 2-803                     [16, 16, 8, 8]            --
│    └─ReLU: 2-804                       [16, 16, 8, 8]            --
│    └─Empty: 2-805                      [16, 16, 8, 8]            --
│    └─Clamp: 2-806                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-807                  [16, 128, 8, 8]           --
│    └─Empty: 2-808                      [16, 128, 8, 8]           --
│    └─Empty: 2-809                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-810         --                        --
│    └─One: 2-811                        [1]                       --
│    └─OutputScale: 2-812                --                        --
│    └─Empty: 2-813                      [16, 128, 3, 3]           --
│    └─Empty: 2-814                      [16, 128, 3, 3]           --
│    └─Empty: 2-815                      [16]                      --
│    └─Empty: 2-816                      [16]                      --
│    └─BatchNorm2d: 2-817                [16, 16, 8, 8]            --
│    └─Scaler: 2-818                     [16, 16, 8, 8]            --
│    └─ReLU: 2-819                       [16, 16, 8, 8]            --
│    └─Empty: 2-820                      [16, 16, 8, 8]            --
│    └─Clamp: 2-821                      [16, 16, 8, 8]            --
├─Dropout2d: 1-82                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-83                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-822         --                        --
│    └─One: 2-823                        [1]                       --
│    └─OutputScale: 2-824                --                        --
│    └─Empty: 2-825                      [128, 48, 1, 1]           --
│    └─Empty: 2-826                      [128, 48, 1, 1]           --
│    └─Empty: 2-827                      [128]                     --
│    └─Empty: 2-828                      [128]                     --
│    └─BatchNorm2d: 2-829                [16, 128, 64, 64]         --
│    └─Scaler: 2-830                     [16, 128, 64, 64]         --
│    └─ReLU: 2-831                       [16, 128, 64, 64]         --
│    └─Empty: 2-832                      [16, 128, 64, 64]         --
│    └─Clamp: 2-833                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-84         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-834                  [16, 128, 32, 32]         --
│    └─Empty: 2-835                      [16, 128, 32, 32]         --
│    └─Empty: 2-836                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-837         --                        --
│    └─One: 2-838                        [1]                       --
│    └─OutputScale: 2-839                --                        --
│    └─Empty: 2-840                      [128, 128, 3, 3]          --
│    └─Empty: 2-841                      [128, 128, 3, 3]          --
│    └─Empty: 2-842                      [128]                     --
│    └─Empty: 2-843                      [128]                     --
│    └─BatchNorm2d: 2-844                [16, 128, 32, 32]         --
│    └─Scaler: 2-845                     [16, 128, 32, 32]         --
│    └─ReLU: 2-846                       [16, 128, 32, 32]         --
│    └─Empty: 2-847                      [16, 128, 32, 32]         --
│    └─Clamp: 2-848                      [16, 128, 32, 32]         --
├─Dropout2d: 1-85                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-86         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-849                  [16, 128, 16, 16]         --
│    └─Empty: 2-850                      [16, 128, 16, 16]         --
│    └─Empty: 2-851                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-852         --                        --
│    └─One: 2-853                        [1]                       --
│    └─OutputScale: 2-854                --                        --
│    └─Empty: 2-855                      [128, 128, 3, 3]          --
│    └─Empty: 2-856                      [128, 128, 3, 3]          --
│    └─Empty: 2-857                      [128]                     --
│    └─Empty: 2-858                      [128]                     --
│    └─BatchNorm2d: 2-859                [16, 128, 16, 16]         --
│    └─Scaler: 2-860                     [16, 128, 16, 16]         --
│    └─ReLU: 2-861                       [16, 128, 16, 16]         --
│    └─Empty: 2-862                      [16, 128, 16, 16]         --
│    └─Clamp: 2-863                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-87                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-864         --                        --
│    └─One: 2-865                        [1]                       --
│    └─OutputScale: 2-866                --                        --
│    └─Empty: 2-867                      [128, 128, 1, 1]          --
│    └─Empty: 2-868                      [128, 128, 1, 1]          --
│    └─Empty: 2-869                      [128]                     --
│    └─Empty: 2-870                      [128]                     --
│    └─BatchNorm2d: 2-871                [16, 128, 16, 16]         --
│    └─Scaler: 2-872                     [16, 128, 16, 16]         --
│    └─ReLU: 2-873                       [16, 128, 16, 16]         --
│    └─Empty: 2-874                      [16, 128, 16, 16]         --
│    └─Clamp: 2-875                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-88         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-876                  [16, 128, 16, 16]         --
│    └─Empty: 2-877                      [16, 128, 16, 16]         --
│    └─Empty: 2-878                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-879         --                        --
│    └─One: 2-880                        [1]                       --
│    └─OutputScale: 2-881                --                        --
│    └─Empty: 2-882                      [128, 128, 3, 3]          --
│    └─Empty: 2-883                      [128, 128, 3, 3]          --
│    └─Empty: 2-884                      [128]                     --
│    └─Empty: 2-885                      [128]                     --
│    └─BatchNorm2d: 2-886                [16, 128, 16, 16]         --
│    └─Scaler: 2-887                     [16, 128, 16, 16]         --
│    └─ReLU: 2-888                       [16, 128, 16, 16]         --
│    └─Empty: 2-889                      [16, 128, 16, 16]         --
│    └─Clamp: 2-890                      [16, 128, 16, 16]         --
├─Dropout2d: 1-89                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-90         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-891                  [16, 128, 8, 8]           --
│    └─Empty: 2-892                      [16, 128, 8, 8]           --
│    └─Empty: 2-893                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-894         --                        --
│    └─One: 2-895                        [1]                       --
│    └─OutputScale: 2-896                --                        --
│    └─Empty: 2-897                      [128, 128, 3, 3]          --
│    └─Empty: 2-898                      [128, 128, 3, 3]          --
│    └─Empty: 2-899                      [128]                     --
│    └─Empty: 2-900                      [128]                     --
│    └─BatchNorm2d: 2-901                [16, 128, 8, 8]           --
│    └─Scaler: 2-902                     [16, 128, 8, 8]           --
│    └─ReLU: 2-903                       [16, 128, 8, 8]           --
│    └─Empty: 2-904                      [16, 128, 8, 8]           --
│    └─Clamp: 2-905                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-91                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-906         --                        --
│    └─One: 2-907                        [1]                       --
│    └─OutputScale: 2-908                --                        --
│    └─Empty: 2-909                      [16, 128, 1, 1]           --
│    └─Empty: 2-910                      [16, 128, 1, 1]           --
│    └─Empty: 2-911                      [16]                      --
│    └─Empty: 2-912                      [16]                      --
│    └─BatchNorm2d: 2-913                [16, 16, 8, 8]            --
│    └─Scaler: 2-914                     [16, 16, 8, 8]            --
│    └─ReLU: 2-915                       [16, 16, 8, 8]            --
│    └─Empty: 2-916                      [16, 16, 8, 8]            --
│    └─Clamp: 2-917                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-92         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-918                  [16, 128, 8, 8]           --
│    └─Empty: 2-919                      [16, 128, 8, 8]           --
│    └─Empty: 2-920                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-921         --                        --
│    └─One: 2-922                        [1]                       --
│    └─OutputScale: 2-923                --                        --
│    └─Empty: 2-924                      [16, 128, 3, 3]           --
│    └─Empty: 2-925                      [16, 128, 3, 3]           --
│    └─Empty: 2-926                      [16]                      --
│    └─Empty: 2-927                      [16]                      --
│    └─BatchNorm2d: 2-928                [16, 16, 8, 8]            --
│    └─Scaler: 2-929                     [16, 16, 8, 8]            --
│    └─ReLU: 2-930                       [16, 16, 8, 8]            --
│    └─Empty: 2-931                      [16, 16, 8, 8]            --
│    └─Clamp: 2-932                      [16, 16, 8, 8]            --
├─Dropout2d: 1-93                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-94                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-933         --                        --
│    └─One: 2-934                        [1]                       --
│    └─OutputScale: 2-935                --                        --
│    └─Empty: 2-936                      [128, 48, 1, 1]           --
│    └─Empty: 2-937                      [128, 48, 1, 1]           --
│    └─Empty: 2-938                      [128]                     --
│    └─Empty: 2-939                      [128]                     --
│    └─BatchNorm2d: 2-940                [16, 128, 64, 64]         --
│    └─Scaler: 2-941                     [16, 128, 64, 64]         --
│    └─ReLU: 2-942                       [16, 128, 64, 64]         --
│    └─Empty: 2-943                      [16, 128, 64, 64]         --
│    └─Clamp: 2-944                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-95         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-945                  [16, 128, 32, 32]         --
│    └─Empty: 2-946                      [16, 128, 32, 32]         --
│    └─Empty: 2-947                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-948         --                        --
│    └─One: 2-949                        [1]                       --
│    └─OutputScale: 2-950                --                        --
│    └─Empty: 2-951                      [128, 128, 3, 3]          --
│    └─Empty: 2-952                      [128, 128, 3, 3]          --
│    └─Empty: 2-953                      [128]                     --
│    └─Empty: 2-954                      [128]                     --
│    └─BatchNorm2d: 2-955                [16, 128, 32, 32]         --
│    └─Scaler: 2-956                     [16, 128, 32, 32]         --
│    └─ReLU: 2-957                       [16, 128, 32, 32]         --
│    └─Empty: 2-958                      [16, 128, 32, 32]         --
│    └─Clamp: 2-959                      [16, 128, 32, 32]         --
├─Dropout2d: 1-96                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-97         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-960                  [16, 128, 16, 16]         --
│    └─Empty: 2-961                      [16, 128, 16, 16]         --
│    └─Empty: 2-962                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-963         --                        --
│    └─One: 2-964                        [1]                       --
│    └─OutputScale: 2-965                --                        --
│    └─Empty: 2-966                      [128, 128, 3, 3]          --
│    └─Empty: 2-967                      [128, 128, 3, 3]          --
│    └─Empty: 2-968                      [128]                     --
│    └─Empty: 2-969                      [128]                     --
│    └─BatchNorm2d: 2-970                [16, 128, 16, 16]         --
│    └─Scaler: 2-971                     [16, 128, 16, 16]         --
│    └─ReLU: 2-972                       [16, 128, 16, 16]         --
│    └─Empty: 2-973                      [16, 128, 16, 16]         --
│    └─Clamp: 2-974                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-98                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-975         --                        --
│    └─One: 2-976                        [1]                       --
│    └─OutputScale: 2-977                --                        --
│    └─Empty: 2-978                      [128, 128, 1, 1]          --
│    └─Empty: 2-979                      [128, 128, 1, 1]          --
│    └─Empty: 2-980                      [128]                     --
│    └─Empty: 2-981                      [128]                     --
│    └─BatchNorm2d: 2-982                [16, 128, 16, 16]         --
│    └─Scaler: 2-983                     [16, 128, 16, 16]         --
│    └─ReLU: 2-984                       [16, 128, 16, 16]         --
│    └─Empty: 2-985                      [16, 128, 16, 16]         --
│    └─Clamp: 2-986                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-99         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-987                  [16, 128, 16, 16]         --
│    └─Empty: 2-988                      [16, 128, 16, 16]         --
│    └─Empty: 2-989                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-990         --                        --
│    └─One: 2-991                        [1]                       --
│    └─OutputScale: 2-992                --                        --
│    └─Empty: 2-993                      [128, 128, 3, 3]          --
│    └─Empty: 2-994                      [128, 128, 3, 3]          --
│    └─Empty: 2-995                      [128]                     --
│    └─Empty: 2-996                      [128]                     --
│    └─BatchNorm2d: 2-997                [16, 128, 16, 16]         --
│    └─Scaler: 2-998                     [16, 128, 16, 16]         --
│    └─ReLU: 2-999                       [16, 128, 16, 16]         --
│    └─Empty: 2-1000                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1001                     [16, 128, 16, 16]         --
├─Dropout2d: 1-100                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-101        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1002                 [16, 128, 8, 8]           --
│    └─Empty: 2-1003                     [16, 128, 8, 8]           --
│    └─Empty: 2-1004                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1005        --                        --
│    └─One: 2-1006                       [1]                       --
│    └─OutputScale: 2-1007               --                        --
│    └─Empty: 2-1008                     [128, 128, 3, 3]          --
│    └─Empty: 2-1009                     [128, 128, 3, 3]          --
│    └─Empty: 2-1010                     [128]                     --
│    └─Empty: 2-1011                     [128]                     --
│    └─BatchNorm2d: 2-1012               [16, 128, 8, 8]           --
│    └─Scaler: 2-1013                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1014                      [16, 128, 8, 8]           --
│    └─Empty: 2-1015                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1016                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-102               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1017        --                        --
│    └─One: 2-1018                       [1]                       --
│    └─OutputScale: 2-1019               --                        --
│    └─Empty: 2-1020                     [16, 128, 1, 1]           --
│    └─Empty: 2-1021                     [16, 128, 1, 1]           --
│    └─Empty: 2-1022                     [16]                      --
│    └─Empty: 2-1023                     [16]                      --
│    └─BatchNorm2d: 2-1024               [16, 16, 8, 8]            --
│    └─Scaler: 2-1025                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1026                      [16, 16, 8, 8]            --
│    └─Empty: 2-1027                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1028                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-103        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1029                 [16, 128, 8, 8]           --
│    └─Empty: 2-1030                     [16, 128, 8, 8]           --
│    └─Empty: 2-1031                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1032        --                        --
│    └─One: 2-1033                       [1]                       --
│    └─OutputScale: 2-1034               --                        --
│    └─Empty: 2-1035                     [16, 128, 3, 3]           --
│    └─Empty: 2-1036                     [16, 128, 3, 3]           --
│    └─Empty: 2-1037                     [16]                      --
│    └─Empty: 2-1038                     [16]                      --
│    └─BatchNorm2d: 2-1039               [16, 16, 8, 8]            --
│    └─Scaler: 2-1040                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1041                      [16, 16, 8, 8]            --
│    └─Empty: 2-1042                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1043                     [16, 16, 8, 8]            --
├─Dropout2d: 1-104                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-105               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1044        --                        --
│    └─One: 2-1045                       [1]                       --
│    └─OutputScale: 2-1046               --                        --
│    └─Empty: 2-1047                     [128, 48, 1, 1]           --
│    └─Empty: 2-1048                     [128, 48, 1, 1]           --
│    └─Empty: 2-1049                     [128]                     --
│    └─Empty: 2-1050                     [128]                     --
│    └─BatchNorm2d: 2-1051               [16, 128, 64, 64]         --
│    └─Scaler: 2-1052                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1053                      [16, 128, 64, 64]         --
│    └─Empty: 2-1054                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1055                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-106        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1056                 [16, 128, 32, 32]         --
│    └─Empty: 2-1057                     [16, 128, 32, 32]         --
│    └─Empty: 2-1058                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1059        --                        --
│    └─One: 2-1060                       [1]                       --
│    └─OutputScale: 2-1061               --                        --
│    └─Empty: 2-1062                     [128, 128, 3, 3]          --
│    └─Empty: 2-1063                     [128, 128, 3, 3]          --
│    └─Empty: 2-1064                     [128]                     --
│    └─Empty: 2-1065                     [128]                     --
│    └─BatchNorm2d: 2-1066               [16, 128, 32, 32]         --
│    └─Scaler: 2-1067                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1068                      [16, 128, 32, 32]         --
│    └─Empty: 2-1069                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1070                     [16, 128, 32, 32]         --
├─Dropout2d: 1-107                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-108        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1071                 [16, 128, 16, 16]         --
│    └─Empty: 2-1072                     [16, 128, 16, 16]         --
│    └─Empty: 2-1073                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1074        --                        --
│    └─One: 2-1075                       [1]                       --
│    └─OutputScale: 2-1076               --                        --
│    └─Empty: 2-1077                     [128, 128, 3, 3]          --
│    └─Empty: 2-1078                     [128, 128, 3, 3]          --
│    └─Empty: 2-1079                     [128]                     --
│    └─Empty: 2-1080                     [128]                     --
│    └─BatchNorm2d: 2-1081               [16, 128, 16, 16]         --
│    └─Scaler: 2-1082                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1083                      [16, 128, 16, 16]         --
│    └─Empty: 2-1084                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1085                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-109               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1086        --                        --
│    └─One: 2-1087                       [1]                       --
│    └─OutputScale: 2-1088               --                        --
│    └─Empty: 2-1089                     [128, 128, 1, 1]          --
│    └─Empty: 2-1090                     [128, 128, 1, 1]          --
│    └─Empty: 2-1091                     [128]                     --
│    └─Empty: 2-1092                     [128]                     --
│    └─BatchNorm2d: 2-1093               [16, 128, 16, 16]         --
│    └─Scaler: 2-1094                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1095                      [16, 128, 16, 16]         --
│    └─Empty: 2-1096                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1097                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-110        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1098                 [16, 128, 16, 16]         --
│    └─Empty: 2-1099                     [16, 128, 16, 16]         --
│    └─Empty: 2-1100                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1101        --                        --
│    └─One: 2-1102                       [1]                       --
│    └─OutputScale: 2-1103               --                        --
│    └─Empty: 2-1104                     [128, 128, 3, 3]          --
│    └─Empty: 2-1105                     [128, 128, 3, 3]          --
│    └─Empty: 2-1106                     [128]                     --
│    └─Empty: 2-1107                     [128]                     --
│    └─BatchNorm2d: 2-1108               [16, 128, 16, 16]         --
│    └─Scaler: 2-1109                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1110                      [16, 128, 16, 16]         --
│    └─Empty: 2-1111                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1112                     [16, 128, 16, 16]         --
├─Dropout2d: 1-111                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-112        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1113                 [16, 128, 8, 8]           --
│    └─Empty: 2-1114                     [16, 128, 8, 8]           --
│    └─Empty: 2-1115                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1116        --                        --
│    └─One: 2-1117                       [1]                       --
│    └─OutputScale: 2-1118               --                        --
│    └─Empty: 2-1119                     [128, 128, 3, 3]          --
│    └─Empty: 2-1120                     [128, 128, 3, 3]          --
│    └─Empty: 2-1121                     [128]                     --
│    └─Empty: 2-1122                     [128]                     --
│    └─BatchNorm2d: 2-1123               [16, 128, 8, 8]           --
│    └─Scaler: 2-1124                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1125                      [16, 128, 8, 8]           --
│    └─Empty: 2-1126                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1127                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-113               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1128        --                        --
│    └─One: 2-1129                       [1]                       --
│    └─OutputScale: 2-1130               --                        --
│    └─Empty: 2-1131                     [16, 128, 1, 1]           --
│    └─Empty: 2-1132                     [16, 128, 1, 1]           --
│    └─Empty: 2-1133                     [16]                      --
│    └─Empty: 2-1134                     [16]                      --
│    └─BatchNorm2d: 2-1135               [16, 16, 8, 8]            --
│    └─Scaler: 2-1136                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1137                      [16, 16, 8, 8]            --
│    └─Empty: 2-1138                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1139                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-114        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1140                 [16, 128, 8, 8]           --
│    └─Empty: 2-1141                     [16, 128, 8, 8]           --
│    └─Empty: 2-1142                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1143        --                        --
│    └─One: 2-1144                       [1]                       --
│    └─OutputScale: 2-1145               --                        --
│    └─Empty: 2-1146                     [16, 128, 3, 3]           --
│    └─Empty: 2-1147                     [16, 128, 3, 3]           --
│    └─Empty: 2-1148                     [16]                      --
│    └─Empty: 2-1149                     [16]                      --
│    └─BatchNorm2d: 2-1150               [16, 16, 8, 8]            --
│    └─Scaler: 2-1151                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1152                      [16, 16, 8, 8]            --
│    └─Empty: 2-1153                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1154                     [16, 16, 8, 8]            --
├─Dropout2d: 1-115                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-116               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1155        --                        --
│    └─One: 2-1156                       [1]                       --
│    └─OutputScale: 2-1157               --                        --
│    └─Empty: 2-1158                     [128, 48, 1, 1]           --
│    └─Empty: 2-1159                     [128, 48, 1, 1]           --
│    └─Empty: 2-1160                     [128]                     --
│    └─Empty: 2-1161                     [128]                     --
│    └─BatchNorm2d: 2-1162               [16, 128, 64, 64]         --
│    └─Scaler: 2-1163                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1164                      [16, 128, 64, 64]         --
│    └─Empty: 2-1165                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1166                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-117        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1167                 [16, 128, 32, 32]         --
│    └─Empty: 2-1168                     [16, 128, 32, 32]         --
│    └─Empty: 2-1169                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1170        --                        --
│    └─One: 2-1171                       [1]                       --
│    └─OutputScale: 2-1172               --                        --
│    └─Empty: 2-1173                     [128, 128, 3, 3]          --
│    └─Empty: 2-1174                     [128, 128, 3, 3]          --
│    └─Empty: 2-1175                     [128]                     --
│    └─Empty: 2-1176                     [128]                     --
│    └─BatchNorm2d: 2-1177               [16, 128, 32, 32]         --
│    └─Scaler: 2-1178                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1179                      [16, 128, 32, 32]         --
│    └─Empty: 2-1180                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1181                     [16, 128, 32, 32]         --
├─Dropout2d: 1-118                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-119        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1182                 [16, 128, 16, 16]         --
│    └─Empty: 2-1183                     [16, 128, 16, 16]         --
│    └─Empty: 2-1184                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1185        --                        --
│    └─One: 2-1186                       [1]                       --
│    └─OutputScale: 2-1187               --                        --
│    └─Empty: 2-1188                     [128, 128, 3, 3]          --
│    └─Empty: 2-1189                     [128, 128, 3, 3]          --
│    └─Empty: 2-1190                     [128]                     --
│    └─Empty: 2-1191                     [128]                     --
│    └─BatchNorm2d: 2-1192               [16, 128, 16, 16]         --
│    └─Scaler: 2-1193                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1194                      [16, 128, 16, 16]         --
│    └─Empty: 2-1195                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1196                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-120               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1197        --                        --
│    └─One: 2-1198                       [1]                       --
│    └─OutputScale: 2-1199               --                        --
│    └─Empty: 2-1200                     [128, 128, 1, 1]          --
│    └─Empty: 2-1201                     [128, 128, 1, 1]          --
│    └─Empty: 2-1202                     [128]                     --
│    └─Empty: 2-1203                     [128]                     --
│    └─BatchNorm2d: 2-1204               [16, 128, 16, 16]         --
│    └─Scaler: 2-1205                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1206                      [16, 128, 16, 16]         --
│    └─Empty: 2-1207                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1208                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-121        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1209                 [16, 128, 16, 16]         --
│    └─Empty: 2-1210                     [16, 128, 16, 16]         --
│    └─Empty: 2-1211                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1212        --                        --
│    └─One: 2-1213                       [1]                       --
│    └─OutputScale: 2-1214               --                        --
│    └─Empty: 2-1215                     [128, 128, 3, 3]          --
│    └─Empty: 2-1216                     [128, 128, 3, 3]          --
│    └─Empty: 2-1217                     [128]                     --
│    └─Empty: 2-1218                     [128]                     --
│    └─BatchNorm2d: 2-1219               [16, 128, 16, 16]         --
│    └─Scaler: 2-1220                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1221                      [16, 128, 16, 16]         --
│    └─Empty: 2-1222                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1223                     [16, 128, 16, 16]         --
├─Dropout2d: 1-122                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-123        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1224                 [16, 128, 8, 8]           --
│    └─Empty: 2-1225                     [16, 128, 8, 8]           --
│    └─Empty: 2-1226                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1227        --                        --
│    └─One: 2-1228                       [1]                       --
│    └─OutputScale: 2-1229               --                        --
│    └─Empty: 2-1230                     [128, 128, 3, 3]          --
│    └─Empty: 2-1231                     [128, 128, 3, 3]          --
│    └─Empty: 2-1232                     [128]                     --
│    └─Empty: 2-1233                     [128]                     --
│    └─BatchNorm2d: 2-1234               [16, 128, 8, 8]           --
│    └─Scaler: 2-1235                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1236                      [16, 128, 8, 8]           --
│    └─Empty: 2-1237                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1238                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-124               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1239        --                        --
│    └─One: 2-1240                       [1]                       --
│    └─OutputScale: 2-1241               --                        --
│    └─Empty: 2-1242                     [16, 128, 1, 1]           --
│    └─Empty: 2-1243                     [16, 128, 1, 1]           --
│    └─Empty: 2-1244                     [16]                      --
│    └─Empty: 2-1245                     [16]                      --
│    └─BatchNorm2d: 2-1246               [16, 16, 8, 8]            --
│    └─Scaler: 2-1247                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1248                      [16, 16, 8, 8]            --
│    └─Empty: 2-1249                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1250                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-125        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1251                 [16, 128, 8, 8]           --
│    └─Empty: 2-1252                     [16, 128, 8, 8]           --
│    └─Empty: 2-1253                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1254        --                        --
│    └─One: 2-1255                       [1]                       --
│    └─OutputScale: 2-1256               --                        --
│    └─Empty: 2-1257                     [16, 128, 3, 3]           --
│    └─Empty: 2-1258                     [16, 128, 3, 3]           --
│    └─Empty: 2-1259                     [16]                      --
│    └─Empty: 2-1260                     [16]                      --
│    └─BatchNorm2d: 2-1261               [16, 16, 8, 8]            --
│    └─Scaler: 2-1262                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1263                      [16, 16, 8, 8]            --
│    └─Empty: 2-1264                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1265                     [16, 16, 8, 8]            --
├─Dropout2d: 1-126                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-127               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1266        --                        --
│    └─One: 2-1267                       [1]                       --
│    └─OutputScale: 2-1268               --                        --
│    └─Empty: 2-1269                     [128, 48, 1, 1]           --
│    └─Empty: 2-1270                     [128, 48, 1, 1]           --
│    └─Empty: 2-1271                     [128]                     --
│    └─Empty: 2-1272                     [128]                     --
│    └─BatchNorm2d: 2-1273               [16, 128, 64, 64]         --
│    └─Scaler: 2-1274                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1275                      [16, 128, 64, 64]         --
│    └─Empty: 2-1276                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1277                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-128        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1278                 [16, 128, 32, 32]         --
│    └─Empty: 2-1279                     [16, 128, 32, 32]         --
│    └─Empty: 2-1280                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1281        --                        --
│    └─One: 2-1282                       [1]                       --
│    └─OutputScale: 2-1283               --                        --
│    └─Empty: 2-1284                     [128, 128, 3, 3]          --
│    └─Empty: 2-1285                     [128, 128, 3, 3]          --
│    └─Empty: 2-1286                     [128]                     --
│    └─Empty: 2-1287                     [128]                     --
│    └─BatchNorm2d: 2-1288               [16, 128, 32, 32]         --
│    └─Scaler: 2-1289                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1290                      [16, 128, 32, 32]         --
│    └─Empty: 2-1291                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1292                     [16, 128, 32, 32]         --
├─Dropout2d: 1-129                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-130        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1293                 [16, 128, 16, 16]         --
│    └─Empty: 2-1294                     [16, 128, 16, 16]         --
│    └─Empty: 2-1295                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1296        --                        --
│    └─One: 2-1297                       [1]                       --
│    └─OutputScale: 2-1298               --                        --
│    └─Empty: 2-1299                     [128, 128, 3, 3]          --
│    └─Empty: 2-1300                     [128, 128, 3, 3]          --
│    └─Empty: 2-1301                     [128]                     --
│    └─Empty: 2-1302                     [128]                     --
│    └─BatchNorm2d: 2-1303               [16, 128, 16, 16]         --
│    └─Scaler: 2-1304                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1305                      [16, 128, 16, 16]         --
│    └─Empty: 2-1306                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1307                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-131               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1308        --                        --
│    └─One: 2-1309                       [1]                       --
│    └─OutputScale: 2-1310               --                        --
│    └─Empty: 2-1311                     [128, 128, 1, 1]          --
│    └─Empty: 2-1312                     [128, 128, 1, 1]          --
│    └─Empty: 2-1313                     [128]                     --
│    └─Empty: 2-1314                     [128]                     --
│    └─BatchNorm2d: 2-1315               [16, 128, 16, 16]         --
│    └─Scaler: 2-1316                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1317                      [16, 128, 16, 16]         --
│    └─Empty: 2-1318                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1319                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-132        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1320                 [16, 128, 16, 16]         --
│    └─Empty: 2-1321                     [16, 128, 16, 16]         --
│    └─Empty: 2-1322                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1323        --                        --
│    └─One: 2-1324                       [1]                       --
│    └─OutputScale: 2-1325               --                        --
│    └─Empty: 2-1326                     [128, 128, 3, 3]          --
│    └─Empty: 2-1327                     [128, 128, 3, 3]          --
│    └─Empty: 2-1328                     [128]                     --
│    └─Empty: 2-1329                     [128]                     --
│    └─BatchNorm2d: 2-1330               [16, 128, 16, 16]         --
│    └─Scaler: 2-1331                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1332                      [16, 128, 16, 16]         --
│    └─Empty: 2-1333                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1334                     [16, 128, 16, 16]         --
├─Dropout2d: 1-133                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-134        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1335                 [16, 128, 8, 8]           --
│    └─Empty: 2-1336                     [16, 128, 8, 8]           --
│    └─Empty: 2-1337                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1338        --                        --
│    └─One: 2-1339                       [1]                       --
│    └─OutputScale: 2-1340               --                        --
│    └─Empty: 2-1341                     [128, 128, 3, 3]          --
│    └─Empty: 2-1342                     [128, 128, 3, 3]          --
│    └─Empty: 2-1343                     [128]                     --
│    └─Empty: 2-1344                     [128]                     --
│    └─BatchNorm2d: 2-1345               [16, 128, 8, 8]           --
│    └─Scaler: 2-1346                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1347                      [16, 128, 8, 8]           --
│    └─Empty: 2-1348                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1349                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-135               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1350        --                        --
│    └─One: 2-1351                       [1]                       --
│    └─OutputScale: 2-1352               --                        --
│    └─Empty: 2-1353                     [16, 128, 1, 1]           --
│    └─Empty: 2-1354                     [16, 128, 1, 1]           --
│    └─Empty: 2-1355                     [16]                      --
│    └─Empty: 2-1356                     [16]                      --
│    └─BatchNorm2d: 2-1357               [16, 16, 8, 8]            --
│    └─Scaler: 2-1358                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1359                      [16, 16, 8, 8]            --
│    └─Empty: 2-1360                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1361                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-136        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1362                 [16, 128, 8, 8]           --
│    └─Empty: 2-1363                     [16, 128, 8, 8]           --
│    └─Empty: 2-1364                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1365        --                        --
│    └─One: 2-1366                       [1]                       --
│    └─OutputScale: 2-1367               --                        --
│    └─Empty: 2-1368                     [16, 128, 3, 3]           --
│    └─Empty: 2-1369                     [16, 128, 3, 3]           --
│    └─Empty: 2-1370                     [16]                      --
│    └─Empty: 2-1371                     [16]                      --
│    └─BatchNorm2d: 2-1372               [16, 16, 8, 8]            --
│    └─Scaler: 2-1373                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1374                      [16, 16, 8, 8]            --
│    └─Empty: 2-1375                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1376                     [16, 16, 8, 8]            --
├─Dropout2d: 1-137                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-138               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1377        --                        --
│    └─One: 2-1378                       [1]                       --
│    └─OutputScale: 2-1379               --                        --
│    └─Empty: 2-1380                     [128, 48, 1, 1]           --
│    └─Empty: 2-1381                     [128, 48, 1, 1]           --
│    └─Empty: 2-1382                     [128]                     --
│    └─Empty: 2-1383                     [128]                     --
│    └─BatchNorm2d: 2-1384               [16, 128, 64, 64]         --
│    └─Scaler: 2-1385                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1386                      [16, 128, 64, 64]         --
│    └─Empty: 2-1387                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1388                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-139        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1389                 [16, 128, 32, 32]         --
│    └─Empty: 2-1390                     [16, 128, 32, 32]         --
│    └─Empty: 2-1391                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1392        --                        --
│    └─One: 2-1393                       [1]                       --
│    └─OutputScale: 2-1394               --                        --
│    └─Empty: 2-1395                     [128, 128, 3, 3]          --
│    └─Empty: 2-1396                     [128, 128, 3, 3]          --
│    └─Empty: 2-1397                     [128]                     --
│    └─Empty: 2-1398                     [128]                     --
│    └─BatchNorm2d: 2-1399               [16, 128, 32, 32]         --
│    └─Scaler: 2-1400                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1401                      [16, 128, 32, 32]         --
│    └─Empty: 2-1402                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1403                     [16, 128, 32, 32]         --
├─Dropout2d: 1-140                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-141        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1404                 [16, 128, 16, 16]         --
│    └─Empty: 2-1405                     [16, 128, 16, 16]         --
│    └─Empty: 2-1406                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1407        --                        --
│    └─One: 2-1408                       [1]                       --
│    └─OutputScale: 2-1409               --                        --
│    └─Empty: 2-1410                     [128, 128, 3, 3]          --
│    └─Empty: 2-1411                     [128, 128, 3, 3]          --
│    └─Empty: 2-1412                     [128]                     --
│    └─Empty: 2-1413                     [128]                     --
│    └─BatchNorm2d: 2-1414               [16, 128, 16, 16]         --
│    └─Scaler: 2-1415                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1416                      [16, 128, 16, 16]         --
│    └─Empty: 2-1417                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1418                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-142               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1419        --                        --
│    └─One: 2-1420                       [1]                       --
│    └─OutputScale: 2-1421               --                        --
│    └─Empty: 2-1422                     [128, 128, 1, 1]          --
│    └─Empty: 2-1423                     [128, 128, 1, 1]          --
│    └─Empty: 2-1424                     [128]                     --
│    └─Empty: 2-1425                     [128]                     --
│    └─BatchNorm2d: 2-1426               [16, 128, 16, 16]         --
│    └─Scaler: 2-1427                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1428                      [16, 128, 16, 16]         --
│    └─Empty: 2-1429                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1430                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-143        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1431                 [16, 128, 16, 16]         --
│    └─Empty: 2-1432                     [16, 128, 16, 16]         --
│    └─Empty: 2-1433                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1434        --                        --
│    └─One: 2-1435                       [1]                       --
│    └─OutputScale: 2-1436               --                        --
│    └─Empty: 2-1437                     [128, 128, 3, 3]          --
│    └─Empty: 2-1438                     [128, 128, 3, 3]          --
│    └─Empty: 2-1439                     [128]                     --
│    └─Empty: 2-1440                     [128]                     --
│    └─BatchNorm2d: 2-1441               [16, 128, 16, 16]         --
│    └─Scaler: 2-1442                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1443                      [16, 128, 16, 16]         --
│    └─Empty: 2-1444                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1445                     [16, 128, 16, 16]         --
├─Dropout2d: 1-144                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-145        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1446                 [16, 128, 8, 8]           --
│    └─Empty: 2-1447                     [16, 128, 8, 8]           --
│    └─Empty: 2-1448                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1449        --                        --
│    └─One: 2-1450                       [1]                       --
│    └─OutputScale: 2-1451               --                        --
│    └─Empty: 2-1452                     [128, 128, 3, 3]          --
│    └─Empty: 2-1453                     [128, 128, 3, 3]          --
│    └─Empty: 2-1454                     [128]                     --
│    └─Empty: 2-1455                     [128]                     --
│    └─BatchNorm2d: 2-1456               [16, 128, 8, 8]           --
│    └─Scaler: 2-1457                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1458                      [16, 128, 8, 8]           --
│    └─Empty: 2-1459                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1460                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-146               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1461        --                        --
│    └─One: 2-1462                       [1]                       --
│    └─OutputScale: 2-1463               --                        --
│    └─Empty: 2-1464                     [16, 128, 1, 1]           --
│    └─Empty: 2-1465                     [16, 128, 1, 1]           --
│    └─Empty: 2-1466                     [16]                      --
│    └─Empty: 2-1467                     [16]                      --
│    └─BatchNorm2d: 2-1468               [16, 16, 8, 8]            --
│    └─Scaler: 2-1469                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1470                      [16, 16, 8, 8]            --
│    └─Empty: 2-1471                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1472                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-147        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1473                 [16, 128, 8, 8]           --
│    └─Empty: 2-1474                     [16, 128, 8, 8]           --
│    └─Empty: 2-1475                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1476        --                        --
│    └─One: 2-1477                       [1]                       --
│    └─OutputScale: 2-1478               --                        --
│    └─Empty: 2-1479                     [16, 128, 3, 3]           --
│    └─Empty: 2-1480                     [16, 128, 3, 3]           --
│    └─Empty: 2-1481                     [16]                      --
│    └─Empty: 2-1482                     [16]                      --
│    └─BatchNorm2d: 2-1483               [16, 16, 8, 8]            --
│    └─Scaler: 2-1484                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1485                      [16, 16, 8, 8]            --
│    └─Empty: 2-1486                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1487                     [16, 16, 8, 8]            --
├─Dropout2d: 1-148                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-149               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1488        --                        --
│    └─One: 2-1489                       [1]                       --
│    └─OutputScale: 2-1490               --                        --
│    └─Empty: 2-1491                     [128, 48, 1, 1]           --
│    └─Empty: 2-1492                     [128, 48, 1, 1]           --
│    └─Empty: 2-1493                     [128]                     --
│    └─Empty: 2-1494                     [128]                     --
│    └─BatchNorm2d: 2-1495               [16, 128, 64, 64]         --
│    └─Scaler: 2-1496                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1497                      [16, 128, 64, 64]         --
│    └─Empty: 2-1498                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1499                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-150        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1500                 [16, 128, 32, 32]         --
│    └─Empty: 2-1501                     [16, 128, 32, 32]         --
│    └─Empty: 2-1502                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1503        --                        --
│    └─One: 2-1504                       [1]                       --
│    └─OutputScale: 2-1505               --                        --
│    └─Empty: 2-1506                     [128, 128, 3, 3]          --
│    └─Empty: 2-1507                     [128, 128, 3, 3]          --
│    └─Empty: 2-1508                     [128]                     --
│    └─Empty: 2-1509                     [128]                     --
│    └─BatchNorm2d: 2-1510               [16, 128, 32, 32]         --
│    └─Scaler: 2-1511                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1512                      [16, 128, 32, 32]         --
│    └─Empty: 2-1513                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1514                     [16, 128, 32, 32]         --
├─Dropout2d: 1-151                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-152        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1515                 [16, 128, 16, 16]         --
│    └─Empty: 2-1516                     [16, 128, 16, 16]         --
│    └─Empty: 2-1517                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1518        --                        --
│    └─One: 2-1519                       [1]                       --
│    └─OutputScale: 2-1520               --                        --
│    └─Empty: 2-1521                     [128, 128, 3, 3]          --
│    └─Empty: 2-1522                     [128, 128, 3, 3]          --
│    └─Empty: 2-1523                     [128]                     --
│    └─Empty: 2-1524                     [128]                     --
│    └─BatchNorm2d: 2-1525               [16, 128, 16, 16]         --
│    └─Scaler: 2-1526                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1527                      [16, 128, 16, 16]         --
│    └─Empty: 2-1528                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1529                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-153               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1530        --                        --
│    └─One: 2-1531                       [1]                       --
│    └─OutputScale: 2-1532               --                        --
│    └─Empty: 2-1533                     [128, 128, 1, 1]          --
│    └─Empty: 2-1534                     [128, 128, 1, 1]          --
│    └─Empty: 2-1535                     [128]                     --
│    └─Empty: 2-1536                     [128]                     --
│    └─BatchNorm2d: 2-1537               [16, 128, 16, 16]         --
│    └─Scaler: 2-1538                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1539                      [16, 128, 16, 16]         --
│    └─Empty: 2-1540                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1541                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-154        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1542                 [16, 128, 16, 16]         --
│    └─Empty: 2-1543                     [16, 128, 16, 16]         --
│    └─Empty: 2-1544                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1545        --                        --
│    └─One: 2-1546                       [1]                       --
│    └─OutputScale: 2-1547               --                        --
│    └─Empty: 2-1548                     [128, 128, 3, 3]          --
│    └─Empty: 2-1549                     [128, 128, 3, 3]          --
│    └─Empty: 2-1550                     [128]                     --
│    └─Empty: 2-1551                     [128]                     --
│    └─BatchNorm2d: 2-1552               [16, 128, 16, 16]         --
│    └─Scaler: 2-1553                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1554                      [16, 128, 16, 16]         --
│    └─Empty: 2-1555                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1556                     [16, 128, 16, 16]         --
├─Dropout2d: 1-155                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-156        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1557                 [16, 128, 8, 8]           --
│    └─Empty: 2-1558                     [16, 128, 8, 8]           --
│    └─Empty: 2-1559                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1560        --                        --
│    └─One: 2-1561                       [1]                       --
│    └─OutputScale: 2-1562               --                        --
│    └─Empty: 2-1563                     [128, 128, 3, 3]          --
│    └─Empty: 2-1564                     [128, 128, 3, 3]          --
│    └─Empty: 2-1565                     [128]                     --
│    └─Empty: 2-1566                     [128]                     --
│    └─BatchNorm2d: 2-1567               [16, 128, 8, 8]           --
│    └─Scaler: 2-1568                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1569                      [16, 128, 8, 8]           --
│    └─Empty: 2-1570                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1571                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-157               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1572        --                        --
│    └─One: 2-1573                       [1]                       --
│    └─OutputScale: 2-1574               --                        --
│    └─Empty: 2-1575                     [16, 128, 1, 1]           --
│    └─Empty: 2-1576                     [16, 128, 1, 1]           --
│    └─Empty: 2-1577                     [16]                      --
│    └─Empty: 2-1578                     [16]                      --
│    └─BatchNorm2d: 2-1579               [16, 16, 8, 8]            --
│    └─Scaler: 2-1580                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1581                      [16, 16, 8, 8]            --
│    └─Empty: 2-1582                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1583                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-158        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1584                 [16, 128, 8, 8]           --
│    └─Empty: 2-1585                     [16, 128, 8, 8]           --
│    └─Empty: 2-1586                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1587        --                        --
│    └─One: 2-1588                       [1]                       --
│    └─OutputScale: 2-1589               --                        --
│    └─Empty: 2-1590                     [16, 128, 3, 3]           --
│    └─Empty: 2-1591                     [16, 128, 3, 3]           --
│    └─Empty: 2-1592                     [16]                      --
│    └─Empty: 2-1593                     [16]                      --
│    └─BatchNorm2d: 2-1594               [16, 16, 8, 8]            --
│    └─Scaler: 2-1595                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1596                      [16, 16, 8, 8]            --
│    └─Empty: 2-1597                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1598                     [16, 16, 8, 8]            --
├─Dropout2d: 1-159                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-160               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1599        --                        --
│    └─One: 2-1600                       [1]                       --
│    └─OutputScale: 2-1601               --                        --
│    └─Empty: 2-1602                     [128, 48, 1, 1]           --
│    └─Empty: 2-1603                     [128, 48, 1, 1]           --
│    └─Empty: 2-1604                     [128]                     --
│    └─Empty: 2-1605                     [128]                     --
│    └─BatchNorm2d: 2-1606               [16, 128, 64, 64]         --
│    └─Scaler: 2-1607                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1608                      [16, 128, 64, 64]         --
│    └─Empty: 2-1609                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1610                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-161        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1611                 [16, 128, 32, 32]         --
│    └─Empty: 2-1612                     [16, 128, 32, 32]         --
│    └─Empty: 2-1613                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1614        --                        --
│    └─One: 2-1615                       [1]                       --
│    └─OutputScale: 2-1616               --                        --
│    └─Empty: 2-1617                     [128, 128, 3, 3]          --
│    └─Empty: 2-1618                     [128, 128, 3, 3]          --
│    └─Empty: 2-1619                     [128]                     --
│    └─Empty: 2-1620                     [128]                     --
│    └─BatchNorm2d: 2-1621               [16, 128, 32, 32]         --
│    └─Scaler: 2-1622                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1623                      [16, 128, 32, 32]         --
│    └─Empty: 2-1624                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1625                     [16, 128, 32, 32]         --
├─Dropout2d: 1-162                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-163        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1626                 [16, 128, 16, 16]         --
│    └─Empty: 2-1627                     [16, 128, 16, 16]         --
│    └─Empty: 2-1628                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1629        --                        --
│    └─One: 2-1630                       [1]                       --
│    └─OutputScale: 2-1631               --                        --
│    └─Empty: 2-1632                     [128, 128, 3, 3]          --
│    └─Empty: 2-1633                     [128, 128, 3, 3]          --
│    └─Empty: 2-1634                     [128]                     --
│    └─Empty: 2-1635                     [128]                     --
│    └─BatchNorm2d: 2-1636               [16, 128, 16, 16]         --
│    └─Scaler: 2-1637                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1638                      [16, 128, 16, 16]         --
│    └─Empty: 2-1639                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1640                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-164               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1641        --                        --
│    └─One: 2-1642                       [1]                       --
│    └─OutputScale: 2-1643               --                        --
│    └─Empty: 2-1644                     [128, 128, 1, 1]          --
│    └─Empty: 2-1645                     [128, 128, 1, 1]          --
│    └─Empty: 2-1646                     [128]                     --
│    └─Empty: 2-1647                     [128]                     --
│    └─BatchNorm2d: 2-1648               [16, 128, 16, 16]         --
│    └─Scaler: 2-1649                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1650                      [16, 128, 16, 16]         --
│    └─Empty: 2-1651                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1652                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-165        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1653                 [16, 128, 16, 16]         --
│    └─Empty: 2-1654                     [16, 128, 16, 16]         --
│    └─Empty: 2-1655                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1656        --                        --
│    └─One: 2-1657                       [1]                       --
│    └─OutputScale: 2-1658               --                        --
│    └─Empty: 2-1659                     [128, 128, 3, 3]          --
│    └─Empty: 2-1660                     [128, 128, 3, 3]          --
│    └─Empty: 2-1661                     [128]                     --
│    └─Empty: 2-1662                     [128]                     --
│    └─BatchNorm2d: 2-1663               [16, 128, 16, 16]         --
│    └─Scaler: 2-1664                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1665                      [16, 128, 16, 16]         --
│    └─Empty: 2-1666                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1667                     [16, 128, 16, 16]         --
├─Dropout2d: 1-166                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-167        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1668                 [16, 128, 8, 8]           --
│    └─Empty: 2-1669                     [16, 128, 8, 8]           --
│    └─Empty: 2-1670                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1671        --                        --
│    └─One: 2-1672                       [1]                       --
│    └─OutputScale: 2-1673               --                        --
│    └─Empty: 2-1674                     [128, 128, 3, 3]          --
│    └─Empty: 2-1675                     [128, 128, 3, 3]          --
│    └─Empty: 2-1676                     [128]                     --
│    └─Empty: 2-1677                     [128]                     --
│    └─BatchNorm2d: 2-1678               [16, 128, 8, 8]           --
│    └─Scaler: 2-1679                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1680                      [16, 128, 8, 8]           --
│    └─Empty: 2-1681                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1682                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-168               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1683        --                        --
│    └─One: 2-1684                       [1]                       --
│    └─OutputScale: 2-1685               --                        --
│    └─Empty: 2-1686                     [16, 128, 1, 1]           --
│    └─Empty: 2-1687                     [16, 128, 1, 1]           --
│    └─Empty: 2-1688                     [16]                      --
│    └─Empty: 2-1689                     [16]                      --
│    └─BatchNorm2d: 2-1690               [16, 16, 8, 8]            --
│    └─Scaler: 2-1691                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1692                      [16, 16, 8, 8]            --
│    └─Empty: 2-1693                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1694                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-169        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1695                 [16, 128, 8, 8]           --
│    └─Empty: 2-1696                     [16, 128, 8, 8]           --
│    └─Empty: 2-1697                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1698        --                        --
│    └─One: 2-1699                       [1]                       --
│    └─OutputScale: 2-1700               --                        --
│    └─Empty: 2-1701                     [16, 128, 3, 3]           --
│    └─Empty: 2-1702                     [16, 128, 3, 3]           --
│    └─Empty: 2-1703                     [16]                      --
│    └─Empty: 2-1704                     [16]                      --
│    └─BatchNorm2d: 2-1705               [16, 16, 8, 8]            --
│    └─Scaler: 2-1706                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1707                      [16, 16, 8, 8]            --
│    └─Empty: 2-1708                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1709                     [16, 16, 8, 8]            --
├─Dropout2d: 1-170                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-171               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1710        --                        --
│    └─One: 2-1711                       [1]                       --
│    └─OutputScale: 2-1712               --                        --
│    └─Empty: 2-1713                     [128, 48, 1, 1]           --
│    └─Empty: 2-1714                     [128, 48, 1, 1]           --
│    └─Empty: 2-1715                     [128]                     --
│    └─Empty: 2-1716                     [128]                     --
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Empty: 2-1720                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1721                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Empty: 2-1723                     [16, 128, 32, 32]         --
│    └─Empty: 2-1724                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1725        --                        --
│    └─One: 2-1726                       [1]                       --
│    └─OutputScale: 2-1727               --                        --
│    └─Empty: 2-1728                     [128, 128, 3, 3]          --
│    └─Empty: 2-1729                     [128, 128, 3, 3]          --
│    └─Empty: 2-1730                     [128]                     --
│    └─Empty: 2-1731                     [128]                     --
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         --
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─Empty: 2-1735                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1736                     [16, 128, 32, 32]         --
├─Dropout2d: 1-173                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1740        --                        --
│    └─One: 2-1741                       [1]                       --
│    └─OutputScale: 2-1742               --                        --
│    └─Empty: 2-1743                     [128, 128, 3, 3]          --
│    └─Empty: 2-1744                     [128, 128, 3, 3]          --
│    └─Empty: 2-1745                     [128]                     --
│    └─Empty: 2-1746                     [128]                     --
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─Empty: 2-1750                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1751                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-175               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1752        --                        --
│    └─One: 2-1753                       [1]                       --
│    └─OutputScale: 2-1754               --                        --
│    └─Empty: 2-1755                     [128, 128, 1, 1]          --
│    └─Empty: 2-1756                     [128, 128, 1, 1]          --
│    └─Empty: 2-1757                     [128]                     --
│    └─Empty: 2-1758                     [128]                     --
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Empty: 2-1762                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1763                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Empty: 2-1765                     [16, 128, 16, 16]         --
│    └─Empty: 2-1766                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1767        --                        --
│    └─One: 2-1768                       [1]                       --
│    └─OutputScale: 2-1769               --                        --
│    └─Empty: 2-1770                     [128, 128, 3, 3]          --
│    └─Empty: 2-1771                     [128, 128, 3, 3]          --
│    └─Empty: 2-1772                     [128]                     --
│    └─Empty: 2-1773                     [128]                     --
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─Empty: 2-1777                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1778                     [16, 128, 16, 16]         --
├─Dropout2d: 1-177                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1782        --                        --
│    └─One: 2-1783                       [1]                       --
│    └─OutputScale: 2-1784               --                        --
│    └─Empty: 2-1785                     [128, 128, 3, 3]          --
│    └─Empty: 2-1786                     [128, 128, 3, 3]          --
│    └─Empty: 2-1787                     [128]                     --
│    └─Empty: 2-1788                     [128]                     --
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─Empty: 2-1792                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1793                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-179               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1794        --                        --
│    └─One: 2-1795                       [1]                       --
│    └─OutputScale: 2-1796               --                        --
│    └─Empty: 2-1797                     [16, 128, 1, 1]           --
│    └─Empty: 2-1798                     [16, 128, 1, 1]           --
│    └─Empty: 2-1799                     [16]                      --
│    └─Empty: 2-1800                     [16]                      --
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Empty: 2-1804                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1805                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1809        --                        --
│    └─One: 2-1810                       [1]                       --
│    └─OutputScale: 2-1811               --                        --
│    └─Empty: 2-1812                     [16, 128, 3, 3]           --
│    └─Empty: 2-1813                     [16, 128, 3, 3]           --
│    └─Empty: 2-1814                     [16]                      --
│    └─Empty: 2-1815                     [16]                      --
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─Empty: 2-1819                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1820                     [16, 16, 8, 8]            --
├─Dropout2d: 1-181                       [16, 16, 8, 8]            --
├─Linear: 1-182                          [16, 5]                   5,126
│    └─OutputShiftSqueeze: 2-1821        --                        --
│    └─One: 2-1822                       [1]                       --
│    └─OutputScale: 2-1823               --                        --
│    └─Empty: 2-1824                     [5, 1024]                 --
│    └─Empty: 2-1825                     [5, 1024]                 --
│    └─Empty: 2-1826                     [16, 5]                   --
│    └─Empty: 2-1827                     [16, 5]                   --
│    └─Clamp: 2-1828                     [16, 5]                   --
├─Linear: 1-183                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1829        --                        --
│    └─One: 2-1830                       [1]                       --
│    └─OutputScale: 2-1831               --                        --
│    └─Empty: 2-1832                     [5, 1024]                 --
│    └─Empty: 2-1833                     [5, 1024]                 --
│    └─Empty: 2-1834                     [16, 5]                   --
│    └─Empty: 2-1835                     [16, 5]                   --
│    └─Clamp: 2-1836                     [16, 5]                   --
├─Linear: 1-184                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1837        --                        --
│    └─One: 2-1838                       [1]                       --
│    └─OutputScale: 2-1839               --                        --
│    └─Empty: 2-1840                     [5, 1024]                 --
│    └─Empty: 2-1841                     [5, 1024]                 --
│    └─Empty: 2-1842                     [16, 5]                   --
│    └─Empty: 2-1843                     [16, 5]                   --
│    └─Clamp: 2-1844                     [16, 5]                   --
├─Linear: 1-185                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1845        --                        --
│    └─One: 2-1846                       [1]                       --
│    └─OutputScale: 2-1847               --                        --
│    └─Empty: 2-1848                     [5, 1024]                 --
│    └─Empty: 2-1849                     [5, 1024]                 --
│    └─Empty: 2-1850                     [16, 5]                   --
│    └─Empty: 2-1851                     [16, 5]                   --
│    └─Clamp: 2-1852                     [16, 5]                   --
├─Linear: 1-186                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1853        --                        --
│    └─One: 2-1854                       [1]                       --
│    └─OutputScale: 2-1855               --                        --
│    └─Empty: 2-1856                     [5, 1024]                 --
│    └─Empty: 2-1857                     [5, 1024]                 --
│    └─Empty: 2-1858                     [16, 5]                   --
│    └─Empty: 2-1859                     [16, 5]                   --
│    └─Clamp: 2-1860                     [16, 5]                   --
├─Linear: 1-187                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1861        --                        --
│    └─One: 2-1862                       [1]                       --
│    └─OutputScale: 2-1863               --                        --
│    └─Empty: 2-1864                     [5, 1024]                 --
│    └─Empty: 2-1865                     [5, 1024]                 --
│    └─Empty: 2-1866                     [16, 5]                   --
│    └─Empty: 2-1867                     [16, 5]                   --
│    └─Clamp: 2-1868                     [16, 5]                   --
├─Linear: 1-188                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1869        --                        --
│    └─One: 2-1870                       [1]                       --
│    └─OutputScale: 2-1871               --                        --
│    └─Empty: 2-1872                     [5, 1024]                 --
│    └─Empty: 2-1873                     [5, 1024]                 --
│    └─Empty: 2-1874                     [16, 5]                   --
│    └─Empty: 2-1875                     [16, 5]                   --
│    └─Clamp: 2-1876                     [16, 5]                   --
├─Linear: 1-189                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1877        --                        --
│    └─One: 2-1878                       [1]                       --
│    └─OutputScale: 2-1879               --                        --
│    └─Empty: 2-1880                     [5, 1024]                 --
│    └─Empty: 2-1881                     [5, 1024]                 --
│    └─Empty: 2-1882                     [16, 5]                   --
│    └─Empty: 2-1883                     [16, 5]                   --
│    └─Clamp: 2-1884                     [16, 5]                   --
├─Linear: 1-190                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1885        --                        --
│    └─One: 2-1886                       [1]                       --
│    └─OutputScale: 2-1887               --                        --
│    └─Empty: 2-1888                     [5, 1024]                 --
│    └─Empty: 2-1889                     [5, 1024]                 --
│    └─Empty: 2-1890                     [16, 5]                   --
│    └─Empty: 2-1891                     [16, 5]                   --
│    └─Clamp: 2-1892                     [16, 5]                   --
├─Linear: 1-191                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1893        --                        --
│    └─One: 2-1894                       [1]                       --
│    └─OutputScale: 2-1895               --                        --
│    └─Empty: 2-1896                     [5, 1024]                 --
│    └─Empty: 2-1897                     [5, 1024]                 --
│    └─Empty: 2-1898                     [16, 5]                   --
│    └─Empty: 2-1899                     [16, 5]                   --
│    └─Clamp: 2-1900                     [16, 5]                   --
├─Linear: 1-192                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1901        --                        --
│    └─One: 2-1902                       [1]                       --
│    └─OutputScale: 2-1903               --                        --
│    └─Empty: 2-1904                     [5, 1024]                 --
│    └─Empty: 2-1905                     [5, 1024]                 --
│    └─Empty: 2-1906                     [16, 5]                   --
│    └─Empty: 2-1907                     [16, 5]                   --
│    └─Clamp: 2-1908                     [16, 5]                   --
├─Linear: 1-193                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1909        --                        --
│    └─One: 2-1910                       [1]                       --
│    └─OutputScale: 2-1911               --                        --
│    └─Empty: 2-1912                     [5, 1024]                 --
│    └─Empty: 2-1913                     [5, 1024]                 --
│    └─Empty: 2-1914                     [16, 5]                   --
│    └─Empty: 2-1915                     [16, 5]                   --
│    └─Clamp: 2-1916                     [16, 5]                   --
├─Linear: 1-194                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1917        --                        --
│    └─One: 2-1918                       [1]                       --
│    └─OutputScale: 2-1919               --                        --
│    └─Empty: 2-1920                     [5, 1024]                 --
│    └─Empty: 2-1921                     [5, 1024]                 --
│    └─Empty: 2-1922                     [16, 5]                   --
│    └─Empty: 2-1923                     [16, 5]                   --
│    └─Clamp: 2-1924                     [16, 5]                   --
├─Linear: 1-195                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1925        --                        --
│    └─One: 2-1926                       [1]                       --
│    └─OutputScale: 2-1927               --                        --
│    └─Empty: 2-1928                     [5, 1024]                 --
│    └─Empty: 2-1929                     [5, 1024]                 --
│    └─Empty: 2-1930                     [16, 5]                   --
│    └─Empty: 2-1931                     [16, 5]                   --
│    └─Clamp: 2-1932                     [16, 5]                   --
├─Linear: 1-196                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1933        --                        --
│    └─One: 2-1934                       [1]                       --
│    └─OutputScale: 2-1935               --                        --
│    └─Empty: 2-1936                     [5, 1024]                 --
│    └─Empty: 2-1937                     [5, 1024]                 --
│    └─Empty: 2-1938                     [16, 5]                   --
│    └─Empty: 2-1939                     [16, 5]                   --
│    └─Clamp: 2-1940                     [16, 5]                   --
├─Linear: 1-197                          [16, 5]                   (recursive)
│    └─OutputShiftSqueeze: 2-1941        --                        --
│    └─One: 2-1942                       [1]                       --
│    └─OutputScale: 2-1943               --                        --
│    └─Empty: 2-1944                     [5, 1024]                 --
│    └─Empty: 2-1945                     [5, 1024]                 --
│    └─Empty: 2-1946                     [16, 5]                   --
│    └─Empty: 2-1947                     [16, 5]                   --
│    └─Clamp: 2-1948                     [16, 5]                   --
==========================================================================================
Total params: 638,806
Trainable params: 638,752
Non-trainable params: 54
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 0.00
Params size (MB): 2.53
Estimated Total Size (MB): 203.86
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.779 | Acc: 21.250% | Wgt Acc: 22.443%
	I - Batch: 100 | Loss: 1.700 | Acc: 22.000% | Wgt Acc: 23.197%
	I - Batch: 150 | Loss: 1.684 | Acc: 22.500% | Wgt Acc: 24.057%
	I - Batch: 200 | Loss: 1.675 | Acc: 23.312% | Wgt Acc: 25.256%
I - num batch: 222
I - Train -- Loss: 1.667 | Acc: 23.738% | Wgt Acc: 25.778% | LR: 1.000000e-02 | Dur: 135.22s
I - Confusion Matrix: [row->prediction - col->label]
[[199. 129. 173. 144. 233.]
 [ 80. 128. 130.  91. 194.]
 [234. 189. 272. 150. 336.]
 [117.  88.  87. 118. 112.]
 [ 67.  44.  72.  35. 125.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.773 | Acc: 23.669% | Wgt Acc: 30.122% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 13.   2.   2.   7.  10.]
 [ 44.  63.  53.  45. 128.]
 [  4.   6.  14.   4.  16.]
 [ 27.   7.   6.  30.  26.]
 [  0.   0.   0.   0.   0.]]

I - Local maximum validation set accuracy:  23.67

I - Validation set results: 
[14-1-1-0.55][50-3-1-0.63][124-2-1-0.96][127-0-3-1.25][443-2-2-0.47][567-0-0-1.06][573-1-1-0.62][615-0-0-1.36][695-1-1-0.40][722-3-3-0.97]
[826-0-3-0.86][878-0-3-1.46][1103-0-1-0.64][1212-3-3-1.03][1368-0-1-0.49][2181-2-3-0.77][2476-2-1-0.52][2721-2-1-0.73][2818-1-1-0.12][2886-2-1-0.78]
[3231-2-1-0.91][3333-2-1-0.84][3482-2-1-0.94][3536-3-1-1.04][3625-1-1-1.10][3909-0-1-0.83][4035-0-1-1.83][4140-0-1-0.63][4214-1-3-1.24][4346-1-3-0.90]
[4581-2-2-0.61][4708-3-2-0.42][4838-3-1-0.68][4845-1-1-0.83][4868-0-3-0.20][4939-0-1-0.59][4984-2-1-0.13][5078-1-1-0.88][5396-0-0-2.12][5479-1-1-0.64]
[5717-0-0-0.57][5843-1-1-0.46][5949-3-3-1.10][5987-2-1-0.57][6014-3-1-0.50][6033-3-0-1.15][6313-0-1-0.52][6421-3-3-0.85][6500-1-1-0.70][6583-3-3-1.04]
[6683-3-1-0.63][6825-2-1-0.70][6998-3-1-0.86][7049-3-1-0.77][7517-1-1-0.86][7521-1-1-0.70][7528-1-1-1.03][7949-1-1-0.31][8135-1-1-0.18][8185-3-3-0.78]
[8269-3-1-0.60][8273-3-2-0.30][8543-3-3-1.89][8666-1-3-0.42][8672-0-3-1.48][8903-1-0-1.35][9001-2-1-0.83][9036-2-2-0.51][9281-3-2-0.62][9300-2-1-0.65]
[9571-0-0-0.42][9617-1-1-0.70][9644-2-2-0.31][9705-2-1-0.68][9801-0-3-1.70][9803-3-1-0.83][9865-3-0-1.45][9896-2-1-0.62][10314-1-1-0.81][10337-3-0-0.65]
[10403-0-1-0.32][10653-2-2-0.30][10704-2-1-0.68][10719-1-1-0.60][10727-1-1-0.58][10836-0-3-1.52][10969-2-1-0.77][11042-0-0-0.91][11088-1-1-0.75][11322-0-3-1.37]
[11398-2-2-0.70][11499-0-2-0.38][11502-3-3-0.58][11512-3-1-0.79][11608-1-1-0.83][11610-0-1-0.63][11692-0-1-0.75][11905-0-0-1.71][11993-1-1-0.72][12002-2-3-1.29]
[12052-0-1-0.42][12201-0-3-1.01][12235-2-1-0.70][12320-1-2-0.37][12377-2-1-0.31][12398-2-3-1.40][12503-1-1-0.96][12617-0-1-0.97][12685-3-1-0.90][12738-2-1-0.67]
[12742-2-2-0.64][12823-0-3-1.08][13110-1-1-0.75][13240-3-3-0.73][13253-1-1-0.80][13273-0-0-2.79][13634-1-1-0.69][13763-2-1-0.95][13905-3-1-0.69][14060-2-1-0.79]
[14065-3-3-1.11][14147-3-1-0.43][14595-2-1-0.74][14687-2-1-0.41][14788-2-1-0.67][14869-1-1-1.35][14872-3-1-0.37][14877-1-2-0.43][14927-0-3-1.59][15066-0-3-1.51]
[15175-1-1-0.77][15178-2-0-0.67][15375-3-1-0.62][15389-3-3-2.30][15568-2-1-0.95][15675-3-1-0.65][15869-1-1-0.15][16207-3-1-0.96][16236-0-1-0.29][16302-3-3-0.71]
[16331-2-2-0.88][16381-0-0-1.26][16488-1-1-0.75][16495-0-1-0.67][16650-0-0-1.55][16719-1-3-0.40][16801-0-1-0.13][16828-0-1-0.81][17137-3-1-0.26][17245-1-1-0.63]
[17278-3-3-0.36][17282-0-1-0.54][17311-2-1-0.55][17336-2-1-0.88][17608-3-3-1.59][17627-0-1-0.95][17877-3-1-0.56][17924-1-1-0.60][17984-3-1-0.83][18211-0-1-0.45]
[18276-3-0-1.05][18287-1-1-0.61][18394-0-3-0.64][18428-0-1-0.90][18442-0-3-1.50][18478-3-3-1.74][18607-0-1-0.56][18616-0-3-0.92][18663-0-1-0.88][18718-0-3-1.51]
[18766-2-1-1.09][18824-2-1-0.58][18890-3-1-0.34][18930-3-0-0.17][18938-3-3-0.49][19817-1-1-0.50][19839-0-1-0.68][19930-3-1-0.39][19944-0-1-0.72][20036-2-1-0.61]
[20101-3-1-0.67][20474-1-1-0.87][20547-3-0-0.65][20929-2-3-0.77][21245-1-1-0.92][21257-3-3-0.36][21293-1-1-1.03][21316-1-1-0.62][21384-1-0-0.45][21448-1-1-0.80]
[21483-0-1-0.62][21487-2-1-0.72][21714-0-3-0.62][21943-3-1-0.90][21947-0-1-0.48][21948-0-1-0.41][21965-2-2-0.66][21998-1-2-0.49][22025-0-1-1.43][22228-3-1-1.02]
[22446-1-1-1.03][22494-3-2-0.75][22757-0-3-1.05][22811-3-1-0.69][22976-3-1-0.62][22985-3-3-1.29][23014-0-3-1.85][23112-1-2-0.43][23144-3-1-1.14][23168-2-1-1.01]
[23219-0-1-0.44][23363-3-3-1.14][23470-0-1-0.55][23486-2-1-0.70][23497-0-1-0.44][23516-0-3-1.73][23690-1-3-0.68][23921-2-1-0.70][23936-1-1-0.33][24040-3-1-0.61]
[24111-1-1-0.63][24182-0-3-1.66][24238-3-3-0.98][24290-2-1-0.54][24345-0-1-0.36][24364-1-2-0.34][24427-3-1-0.78][24477-2-1-0.60][24495-2-1-0.85][24893-2-1-0.82]
[25012-1-1-0.20][25121-2-2-0.75][25165-3-1-0.25][25183-0-2-0.30][25297-3-3-1.40][25398-0-1-0.80][25574-2-1-0.79][25644-1-1-0.56][25718-1-3-0.17][25774-2-3-0.69]
[26032-3-3-0.43][26051-3-3-1.66][26120-0-0-1.93][26321-1-1-0.69][26732-1-1-0.65][26784-3-3-2.07][26827-3-3-1.53][26833-0-1-0.41][26838-2-1-0.71][26860-1-1-0.73]
[26948-0-1-0.67][27049-3-1-1.12][27098-1-1-0.65][27526-0-1-0.27][27639-3-1-0.77][27698-3-3-0.21][27772-0-3-1.60][27890-1-1-0.52][28040-0-3-1.54][28503-2-1-0.71]
[28577-1-1-0.85][28959-0-1-0.88][29198-3-1-0.58][29777-0-0-1.12][29877-2-1-0.48][30035-1-1-0.73][30098-0-3-1.26][30326-1-1-0.88][30572-2-2-0.37][30716-0-1-0.28]
[30806-2-1-0.39][30906-1-1-0.58][31007-0-2-0.27][31181-3-1-0.46][31238-0-2-0.43][31347-0-3-1.80][31422-2-1-0.86][31429-3-1-0.60][31431-0-3-1.25][31432-1-1-0.87]
[31477-0-3-2.65][31524-1-1-0.58][31597-1-1-0.65][31619-1-1-0.55][31701-0-1-0.61][31755-0-1-0.60][31854-3-3-1.05][32074-1-3-1.44][32078-3-1-0.89][32111-1-1-0.72]
[32127-1-2-0.53][32140-3-3-0.89][32263-2-1-0.48][32365-0-1-0.32][32411-2-3-2.94][32429-3-1-0.22][32473-3-1-0.11][32574-3-3-0.88][32584-0-1-0.99][32622-0-1-0.55]
[32858-3-1-0.56][32969-3-0-1.88][33016-2-1-0.84][33031-1-1-0.42][33035-2-1-0.51][33133-2-1-0.68][33173-2-2-0.42][33175-3-1-0.73][33306-3-1-0.45][33309-2-1-0.85]
[33474-0-1-0.58][33478-2-0-0.44][33618-1-1-0.44][33712-0-0-0.51][33782-2-2-0.39][33914-3-3-0.75][34076-3-1-0.94][34112-2-1-1.11][34138-2-1-0.78][34239-1-1-0.89]
[34364-2-1-0.87][34617-1-1-0.55][34751-3-1-0.62][34783-2-2-0.46][35015-3-1-0.62][35018-1-1-0.80][35288-2-1-0.19][0-4-1-0.38][1-4-0-2.21][2-4-1-0.67]
[3-4-3-2.06][4-4-3-1.34][5-4-1-0.81][6-4-1-0.43][7-4-0-2.01][8-4-1-0.69][9-4-1-0.46][10-4-1-0.51][11-4-1-0.38][12-4-3-2.11]
[14-4-1-0.73][15-4-3-1.99][16-4-3-2.14][17-4-1-0.29][18-4-1-0.48][19-4-3-0.61][20-4-3-0.69][21-4-1-0.51][22-4-1-0.65][23-4-1-1.22]
[24-4-3-2.72][25-4-1-0.85][26-4-1-0.35][27-4-2--0.05][28-4-0-0.26][29-4-1-0.60][30-4-1-0.37][31-4-2-0.47][32-4-1-0.68][33-4-2-0.59]
[34-4-1-0.68][35-4-3-0.57][37-4-1-0.48][39-4-0-0.81][40-4-1-0.71][41-4-1-0.73][42-4-1-0.46][43-4-1-0.57][45-4-2-0.56][46-4-2-0.53]
[47-4-1-0.48][48-4-3-0.30][51-4-1-0.60][52-4-1-0.69][53-4-1-0.84][54-4-1-0.84][55-4-1-0.77][56-4-1-0.84][57-4-3-0.90][58-4-1-0.86]
[59-4-0-0.88][60-4-1-0.92][61-4-1-0.61][62-4-1-0.61][63-4-1-0.64][64-4-1-0.64][65-4-1-0.60][66-4-1-0.69][67-4-1-0.60][68-4-1-0.86]
[69-4-1-0.38][70-4-1-0.21][72-4-1-0.72][73-4-1-0.66][74-4-1-0.67][75-4-3-0.27][77-4-1-0.58][78-4-1-0.48][79-4-1-0.58][80-4-1-0.67]
[81-4-2-0.64][82-4-1-0.50][83-4-1-0.63][84-4-3-1.16][85-4-0-0.08][86-4-1-0.69][87-4-1-0.20][88-4-1-0.52][89-4-1-0.77][90-4-1-0.51]
[91-4-1-0.20][92-4-1-0.42][93-4-1-0.29][94-4-1-0.52][95-4-1-0.84][96-4-1-0.80][97-4-1-0.62][98-4-1-0.82][99-4-1-0.67][100-4-1-0.61]
[101-4-3-0.71][102-4-2-0.51][103-4-1-0.81][104-4-1-0.44][105-4-1-0.62][106-4-1-0.62][107-4-2-0.21][108-4-1-0.84][109-4-1-0.75][110-4-1-0.49]
[111-4-3-3.30][112-4-2-0.49][113-4-3-1.05][114-4-1-0.19][115-4-1-0.28][116-4-1-0.49][117-4-1-0.87][119-4-1-0.58][121-4-1-0.85][122-4-3-0.93]
[124-4-1-0.70][125-4-1-0.76][126-4-3-0.68][127-4-1-0.59][128-4-1-0.71][129-4-1-0.70][130-4-1-0.60][131-4-1-0.74][132-4-1-0.39][133-4-0-2.58]
[135-4-1-0.66][136-4-1-0.31][137-4-1-0.65][138-4-0-0.59][139-4-3-2.70][140-4-1-0.74][141-4-1-0.59][142-4-1-0.71][143-4-2-0.42][144-4-1-0.68]
[145-4-2-0.52][148-4-2-0.25][149-4-1-0.64][150-4-1-0.50][151-4-1-0.46][152-4-1-0.76][153-4-1-1.37][154-4-2-0.24][155-4-3-0.68][156-4-3-0.38]
[157-4-1-0.53][158-4-1-0.49][160-4-1-0.76][161-4-1-0.95][162-4-1-0.80][164-4-1-0.56][165-4-1-0.64][167-4-3-0.55][168-4-1-0.68][170-4-3-0.62]
[171-4-1-0.78][172-4-1-0.62][173-4-0-0.12][174-4-1-0.65][175-4-1-0.60][177-4-3-2.64][178-4-1-0.61][179-4-2-0.39][180-4-2-0.28][181-4-0-0.53]
[182-4-3-1.11][183-4-1-0.55][184-4-1-0.70][186-4-1-0.38][187-4-1-0.66][188-4-1-0.59][189-4-1-0.70][190-4-1-0.46][191-4-2-0.44][192-4-1-0.80]
[193-4-1-0.99][194-4-1-0.76][195-4-1-0.31][196-4-1-0.46][197-4-1-0.39][198-4-3-2.45][199-4-1-0.78]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.498 | Acc: 31.875% | Wgt Acc: 35.109%
	I - Batch: 100 | Loss: 1.507 | Acc: 31.375% | Wgt Acc: 35.293%
	I - Batch: 150 | Loss: 1.504 | Acc: 32.083% | Wgt Acc: 36.310%
	I - Batch: 200 | Loss: 1.484 | Acc: 32.406% | Wgt Acc: 36.495%
I - num batch: 222
I - Train -- Loss: 1.479 | Acc: 32.845% | Wgt Acc: 36.899% | LR: 1.000000e-02 | Dur: 136.73s
I - Confusion Matrix: [row->prediction - col->label]
[[310.  53.  83. 189. 152.]
 [ 72. 212. 165.  74. 248.]
 [120. 213. 356.  83. 383.]
 [165.  56.  64. 176. 106.]
 [ 30.  44.  66.  16. 111.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.457 | Acc: 36.686% | Wgt Acc: 31.188% | Dur: 16.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 25.   0.   1.  18.   6.]
 [ 23.  38.  31.  35.  64.]
 [  1.   0.   1.   2.   0.]
 [ 12.   2.   3.  17.   5.]
 [ 27.  38.  39.  14. 105.]]

I - Local maximum validation set accuracy:  36.69

I - Validation set results: 
[14-1-4-0.54][50-3-1-0.80][124-2-4-0.24][127-0-1-0.37][443-2-4-1.17][567-0-4-0.82][573-1-1-0.64][615-0-0-0.56][695-1-4-0.66][722-3-0-0.78]
[826-0-0-1.21][878-0-0-0.28][1103-0-4-0.80][1212-3-3-0.13][1368-0-4-0.49][2181-2-3-1.09][2476-2-4-0.71][2721-2-4-0.70][2818-1-4-0.50][2886-2-4-0.82]
[3231-2-1-0.64][3333-2-1-0.42][3482-2-2-0.37][3536-3-1-0.98][3625-1-1-0.86][3909-0-4-0.45][4035-0-1-0.57][4140-0-4-1.27][4214-1-3-0.14][4346-1-3-0.34]
[4581-2-4-1.01][4708-3-1-0.28][4838-3-1-1.00][4845-1-1-0.38][4868-0-0-0.14][4939-0-4-0.81][4984-2-1-0.64][5078-1-4-0.91][5396-0-0-1.57][5479-1-1-0.77]
[5717-0-1-0.65][5843-1-1-1.11][5949-3-0-0.65][5987-2-4-1.06][6014-3-1-0.46][6033-3-4-1.15][6313-0-1-0.25][6421-3-2-0.25][6500-1-1-0.89][6583-3-1-0.31]
[6683-3-4-0.57][6825-2-4-0.29][6998-3-4-0.59][7049-3-1-0.59][7517-1-1-0.62][7521-1-1-1.03][7528-1-1-0.85][7949-1-4-1.04][8135-1-1-0.50][8185-3-0-0.31]
[8269-3-4-0.77][8273-3-0-0.47][8543-3-0-0.51][8666-1-1-0.90][8672-0-0-1.61][8903-1-1-0.62][9001-2-1-0.97][9036-2-4-0.59][9281-3-1-0.56][9300-2-4-0.87]
[9571-0-4-0.28][9617-1-1-0.64][9644-2-4-0.94][9705-2-4-0.61][9801-0-3-1.07][9803-3-0-0.44][9865-3-3-0.32][9896-2-4-0.79][10314-1-4-0.84][10337-3-3-0.70]
[10403-0-4-0.74][10653-2-1-0.76][10704-2-1-1.17][10719-1-4-0.75][10727-1-1-0.92][10836-0-3-0.33][10969-2-1-0.18][11042-0-4-0.88][11088-1-4-0.75][11322-0-0-0.34]
[11398-2-4-1.00][11499-0-4-0.64][11502-3-1-0.61][11512-3-1-1.13][11608-1-1-0.87][11610-0-3-0.71][11692-0-0-0.51][11905-0-0-0.80][11993-1-4-0.98][12002-2-3-0.54]
[12052-0-4-0.63][12201-0-3-0.73][12235-2-4-0.62][12320-1-4-0.95][12377-2-4-0.92][12398-2-1-0.47][12503-1-4-1.02][12617-0-1-1.66][12685-3-1-0.65][12738-2-4-0.64]
[12742-2-4-0.53][12823-0-1-1.01][13110-1-1-0.64][13240-3-1-0.18][13253-1-4-0.74][13273-0-0-1.19][13634-1-1-0.84][13763-2-1-0.73][13905-3-1-0.11][14060-2-1-1.06]
[14065-3-3-0.44][14147-3-3-0.26][14595-2-4-0.72][14687-2-1-0.23][14788-2-4-0.74][14869-1-1-0.81][14872-3-4-1.15][14877-1-4-0.53][14927-0-3-0.54][15066-0-0-0.50]
[15175-1-4-0.59][15178-2-0-0.11][15375-3-1-0.75][15389-3-3-1.10][15568-2-1-0.96][15675-3-1-0.97][15869-1-4-0.59][16207-3-4-0.66][16236-0-0-0.24][16302-3-3-0.20]
[16331-2-4-0.90][16381-0-1-0.64][16488-1-4-0.56][16495-0-4-0.57][16650-0-0-0.91][16719-1-1-0.60][16801-0-0-0.25][16828-0-1-0.50][17137-3-4-0.35][17245-1-4-0.73]
[17278-3-1-0.46][17282-0-1-0.52][17311-2-4-0.96][17336-2-1-0.46][17608-3-3-0.74][17627-0-1-0.76][17877-3-4-0.73][17924-1-4-0.57][17984-3-0-0.47][18211-0-1-0.77]
[18276-3-4-0.59][18287-1-4-0.62][18394-0-0-0.71][18428-0-1-0.67][18442-0-1-0.95][18478-3-0-0.59][18607-0-4-0.70][18616-0-1-0.62][18663-0-1-0.92][18718-0-3-0.72]
[18766-2-1-0.54][18824-2-4-0.99][18890-3-1-0.37][18930-3-1-0.53][18938-3-0-0.34][19817-1-4-0.98][19839-0-4-0.68][19930-3-1-0.43][19944-0-4-0.36][20036-2-4-0.87]
[20101-3-1-0.22][20474-1-4-0.74][20547-3-4-0.63][20929-2-1-0.44][21245-1-1-0.84][21257-3-1-0.60][21293-1-1-0.88][21316-1-1-0.72][21384-1-1-0.98][21448-1-1-0.51]
[21483-0-1-0.08][21487-2-4-0.86][21714-0-0-0.27][21943-3-4-0.52][21947-0-4-0.53][21948-0-0-1.11][21965-2-4-1.09][21998-1-4-0.53][22025-0-1-1.03][22228-3-1-0.52]
[22446-1-1-0.98][22494-3-3-0.42][22757-0-0-0.71][22811-3-3-1.13][22976-3-1-0.72][22985-3-3-0.39][23014-0-3-0.83][23112-1-1-0.65][23144-3-0-0.54][23168-2-1-0.34]
[23219-0-4-0.81][23363-3-3-0.45][23470-0-4-0.78][23486-2-1-0.38][23497-0-3-1.16][23516-0-3-0.46][23690-1-4-1.08][23921-2-1-0.57][23936-1-4-0.40][24040-3-0-0.09]
[24111-1-1-0.93][24182-0-0-1.46][24238-3-3-1.36][24290-2-4-0.59][24345-0-4-0.45][24364-1-4-0.42][24427-3-1-0.14][24477-2-4-0.82][24495-2-1-0.81][24893-2-1-0.70]
[25012-1-1-0.88][25121-2-4-1.39][25165-3-1-0.35][25183-0-1-0.78][25297-3-1-0.85][25398-0-4-0.24][25574-2-1-0.55][25644-1-1-0.88][25718-1-1-0.53][25774-2-1-0.60]
[26032-3-1-0.30][26051-3-0-1.27][26120-0-4-0.54][26321-1-1-0.55][26732-1-1-0.80][26784-3-3-1.78][26827-3-3-0.50][26833-0-0-0.61][26838-2-1-0.71][26860-1-4-0.54]
[26948-0-4-0.55][27049-3-1--0.03][27098-1-4-0.77][27526-0-4-0.56][27639-3-1-0.62][27698-3-3-0.54][27772-0-0-1.16][27890-1-4-0.76][28040-0-1-0.34][28503-2-4-1.00]
[28577-1-1-0.93][28959-0-0-0.98][29198-3-1-1.01][29777-0-0-1.18][29877-2-1-0.81][30035-1-4-0.85][30098-0-1-0.10][30326-1-1-1.19][30572-2-1-0.55][30716-0-1-1.08]
[30806-2-1-1.06][30906-1-4-0.82][31007-0-4-0.56][31181-3-2-0.09][31238-0-3-0.17][31347-0-3-1.04][31422-2-1-0.80][31429-3-1-0.27][31431-0-1-0.45][31432-1-1-0.80]
[31477-0-3-1.89][31524-1-1-0.50][31597-1-4-0.65][31619-1-4-0.68][31701-0-0-0.91][31755-0-0-0.37][31854-3-1-0.48][32074-1-1-0.55][32078-3-1-0.28][32111-1-4-0.79]
[32127-1-4-0.81][32140-3-0-0.15][32263-2-4-0.67][32365-0-4-0.50][32411-2-3-0.64][32429-3-0-1.25][32473-3-0-0.74][32574-3-0-1.39][32584-0-2-0.43][32622-0-4-0.59]
[32858-3-0-0.18][32969-3-0-0.55][33016-2-4-0.87][33031-1-1-0.91][33035-2-4-0.58][33133-2-1-0.70][33173-2-1-0.93][33175-3-4-0.90][33306-3-4-0.63][33309-2-4-0.56]
[33474-0-1-0.49][33478-2-4-0.37][33618-1-4-0.48][33712-0-4-0.43][33782-2-4-0.94][33914-3-1-1.35][34076-3-1-0.15][34112-2-1-1.03][34138-2-1-0.15][34239-1-4-0.65]
[34364-2-4-0.60][34617-1-4-0.86][34751-3-3-0.81][34783-2-4-0.88][35015-3-4-0.24][35018-1-4-0.70][35288-2-4-0.18][0-4-4-0.97][1-4-4-0.34][2-4-4-0.55]
[3-4-4-0.38][4-4-3-0.24][5-4-1-0.74][6-4-1-0.31][7-4-1-0.92][8-4-1-0.28][9-4-4-0.96][10-4-4-1.15][11-4-4-0.94][12-4-1-1.30]
[14-4-1-0.31][15-4-3-0.82][16-4-1-0.56][17-4-4-0.66][18-4-4-0.87][19-4-3-0.95][20-4-4-0.50][21-4-1-0.75][22-4-4-0.95][23-4-1-1.14]
[24-4-1-0.55][25-4-1-0.23][26-4-1-0.55][27-4-4-0.76][28-4-1-0.79][29-4-4-0.75][30-4-4-0.62][31-4-4-0.38][32-4-4-1.09][33-4-1-0.38]
[34-4-4-0.20][35-4-1-0.87][37-4-0-0.37][39-4-0-0.27][40-4-4-0.68][41-4-1-0.68][42-4-1-0.63][43-4-4-1.02][45-4-4-0.71][46-4-4-1.13]
[47-4-4-1.02][48-4-4-0.96][51-4-4-1.09][52-4-4-0.64][53-4-1-0.58][54-4-1-0.52][55-4-4-0.26][56-4-1-0.82][57-4-0-0.61][58-4-1-0.39]
[59-4-4-1.11][60-4-1-0.73][61-4-4-0.77][62-4-4-0.40][63-4-4-0.37][64-4-4-0.79][65-4-1-0.84][66-4-4-1.12][67-4-1-0.55][68-4-1-0.56]
[69-4-1-0.50][70-4-4-1.01][72-4-4-0.84][73-4-1-0.90][74-4-4-0.75][75-4-4-0.47][77-4-1-0.96][78-4-4-0.81][79-4-4-0.87][80-4-1-0.86]
[81-4-4-0.95][82-4-4-0.81][83-4-4-0.74][84-4-4-0.69][85-4-1-0.79][86-4-4-0.58][87-4-4-1.11][88-4-4-1.23][89-4-4-0.58][90-4-4-0.88]
[91-4-4-0.70][92-4-1-0.87][93-4-4-0.82][94-4-4-0.95][95-4-4-0.77][96-4-4-0.66][97-4-1-0.82][98-4-4-0.56][99-4-4-0.58][100-4-4-0.67]
[101-4-4-0.73][102-4-4-0.91][103-4-1-0.45][104-4-4-0.99][105-4-1-0.89][106-4-1-0.94][107-4-4-1.04][108-4-4-0.91][109-4-1-0.88][110-4-1-1.06]
[111-4-3-1.57][112-4-4-1.19][113-4-1-0.94][114-4-4-0.63][115-4-4-0.78][116-4-1-0.69][117-4-1-0.95][119-4-4-0.61][121-4-1-0.84][122-4-1-0.83]
[124-4-1-0.54][125-4-4-0.98][126-4-1-0.55][127-4-4-0.87][128-4-1-0.38][129-4-1-0.54][130-4-4-1.12][131-4-1-0.32][132-4-1-0.58][133-4-0-0.30]
[135-4-4-0.53][136-4-1-0.98][137-4-4-0.72][138-4-4-0.50][139-4-3-0.12][140-4-4-0.26][141-4-4-0.19][142-4-4-1.14][143-4-4-1.05][144-4-4-0.85]
[145-4-4-1.01][148-4-0-0.16][149-4-4-0.28][150-4-1-0.91][151-4-4-0.85][152-4-1-1.15][153-4-1-0.73][154-4-1-0.91][155-4-1-0.53][156-4-4-0.27]
[157-4-4-0.38][158-4-1-0.58][160-4-4-0.67][161-4-1-0.65][162-4-4-0.57][164-4-4-0.76][165-4-4-0.73][167-4-4-0.40][168-4-4-0.91][170-4-4-0.12]
[171-4-1-0.63][172-4-4-1.00][173-4-4-0.27][174-4-4-0.27][175-4-1-0.68][177-4-0-0.68][178-4-4-1.15][179-4-1-0.77][180-4-4-1.22][181-4-1-0.39]
[182-4-1-0.29][183-4-1-0.65][184-4-4-0.74][186-4-4-0.41][187-4-1-0.80][188-4-4-0.56][189-4-4-0.63][190-4-1-0.86][191-4-4-1.06][192-4-4-0.95]
[193-4-4-0.44][194-4-4-0.75][195-4-4-0.45][196-4-4-0.38][197-4-4-1.07][198-4-4-1.03][199-4-4-0.54]
---------------------------
I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.437 | Acc: 35.125% | Wgt Acc: 38.421%
	I - Batch: 100 | Loss: 1.405 | Acc: 36.812% | Wgt Acc: 40.761%
	I - Batch: 150 | Loss: 1.406 | Acc: 36.792% | Wgt Acc: 40.583%
	I - Batch: 200 | Loss: 1.415 | Acc: 36.156% | Wgt Acc: 39.867%
I - num batch: 222
I - Train -- Loss: 1.422 | Acc: 35.749% | Wgt Acc: 39.344% | LR: 1.000000e-02 | Dur: 132.95s
I - Confusion Matrix: [row->prediction - col->label]
[[357.  49.  89. 193. 155.]
 [ 44. 223. 161.  56. 209.]
 [ 92. 201. 350.  94. 361.]
 [159.  45.  69. 176. 113.]
 [ 45.  60.  65.  19. 162.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.577 | Acc: 28.797% | Wgt Acc: 33.393% | Dur: 19.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 13.   1.   2.   8.   6.]
 [ 34.  62.  43.  32. 108.]
 [ 15.  10.  18.  14.  27.]
 [ 23.   3.   6.  31.  17.]
 [  3.   2.   6.   1.  22.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.466 | Acc: 34.375% | Wgt Acc: 37.796%
	I - Batch: 100 | Loss: 1.445 | Acc: 35.938% | Wgt Acc: 38.620%
	I - Batch: 150 | Loss: 1.463 | Acc: 35.667% | Wgt Acc: 38.209%
	I - Batch: 200 | Loss: 1.447 | Acc: 36.125% | Wgt Acc: 39.075%
I - num batch: 222
I - Train -- Loss: 1.455 | Acc: 35.777% | Wgt Acc: 38.518% | LR: 1.000000e-02 | Dur: 132.63s
I - Confusion Matrix: [row->prediction - col->label]
[[326.  54. 105. 185. 162.]
 [ 54. 239. 164.  74. 245.]
 [ 97. 194. 323.  90. 313.]
 [173.  37.  59. 172.  71.]
 [ 47.  54.  83.  17. 209.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.896 | Acc: 22.682% | Wgt Acc: 27.562% | Dur: 13.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 28.   9.   9.  23.  18.]
 [  0.   0.   0.   0.   0.]
 [ 22.  46.  38.  20. 101.]
 [ 35.  20.  24.  42.  54.]
 [  3.   3.   4.   1.   7.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 1.410 | Acc: 36.500% | Wgt Acc: 40.135%
	I - Batch: 100 | Loss: 1.388 | Acc: 38.188% | Wgt Acc: 41.220%
	I - Batch: 150 | Loss: 1.384 | Acc: 38.833% | Wgt Acc: 41.551%
	I - Batch: 200 | Loss: 1.411 | Acc: 37.312% | Wgt Acc: 39.948%
I - num batch: 222
I - Train -- Loss: 1.413 | Acc: 37.440% | Wgt Acc: 40.208% | LR: 1.000000e-02 | Dur: 133.54s
I - Confusion Matrix: [row->prediction - col->label]
[[351.  42.  84. 199. 155.]
 [ 59. 235. 158.  64. 198.]
 [ 87. 204. 339.  76. 332.]
 [157.  39.  74. 180.  92.]
 [ 43.  58.  79.  19. 223.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.611 | Acc: 29.980% | Wgt Acc: 37.405% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   9.   8.  40.  34.]
 [ 19.  64.  57.  21. 132.]
 [  1.   4.   4.   2.   5.]
 [  9.   1.   6.  23.   7.]
 [  0.   0.   0.   0.   2.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 1.282 | Acc: 42.875% | Wgt Acc: 46.868%
	I - Batch: 100 | Loss: 1.313 | Acc: 41.438% | Wgt Acc: 45.068%
	I - Batch: 150 | Loss: 1.306 | Acc: 40.875% | Wgt Acc: 44.291%
	I - Batch: 200 | Loss: 1.298 | Acc: 41.125% | Wgt Acc: 44.366%
I - num batch: 222
I - Train -- Loss: 1.303 | Acc: 40.908% | Wgt Acc: 44.123% | LR: 5.000000e-03 | Dur: 136.22s
I - Confusion Matrix: [row->prediction - col->label]
[[398.  38.  82. 212. 142.]
 [ 36. 269. 159.  40. 220.]
 [ 81. 182. 370.  90. 340.]
 [148.  41.  55. 182.  66.]
 [ 34.  48.  68.  14. 232.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.431 | Acc: 38.067% | Wgt Acc: 41.302% | Dur: 15.66s
I - Confusion Matrix: [row->prediction - col->label]
[[46.  2.  8. 28. 23.]
 [ 5. 34. 20.  9. 46.]
 [ 8. 23. 28.  9. 46.]
 [23.  8. 12. 39. 19.]
 [ 6. 11.  7.  1. 46.]]

I - Local maximum validation set accuracy:  38.07

I - Validation set results: 
[14-1-2-1.04][50-3-1-1.40][124-2-3-0.51][127-0-0-3.31][443-2-2-1.30][567-0-0-1.32][573-1-1-0.81][615-0-0-1.65][695-1-3-0.72][722-3-3-2.83]
[826-0-0-2.16][878-0-0-2.44][1103-0-4-0.35][1212-3-3-0.53][1368-0-0-1.05][2181-2-3-1.48][2476-2-0-0.54][2721-2-2-1.22][2818-1-4-0.56][2886-2-1-1.47]
[3231-2-2-1.87][3333-2-3-1.61][3482-2-2-1.36][3536-3-3-1.84][3625-1-1-1.74][3909-0-3-1.04][4035-0-3-3.08][4140-0-0-1.19][4214-1-3-1.04][4346-1-4-0.97]
[4581-2-2-1.09][4708-3-2-0.50][4838-3-1-0.69][4845-1-2-1.05][4868-0-0-1.26][4939-0-2-0.94][4984-2-2-0.62][5078-1-1-0.34][5396-0-0-4.20][5479-1-1-1.37]
[5717-0-0-0.44][5843-1-4-1.20][5949-3-0-1.58][5987-2-4-1.67][6014-3-0-0.82][6033-3-0-1.46][6313-0-0-2.85][6421-3-3-1.78][6500-1-1-1.21][6583-3-3-1.39]
[6683-3-1-0.93][6825-2-3-2.06][6998-3-2-1.18][7049-3-3-0.97][7517-1-1-1.83][7521-1-1-1.16][7528-1-3-1.07][7949-1-2-1.32][8135-1-0-1.02][8185-3-0-2.47]
[8269-3-2-1.35][8273-3-0-2.07][8543-3-0-3.34][8666-1-1-1.27][8672-0-0-3.27][8903-1-2-0.73][9001-2-1-1.22][9036-2-1-1.85][9281-3-1-0.55][9300-2-2-0.78]
[9571-0-3-0.53][9617-1-4-0.75][9644-2-3-0.73][9705-2-1-0.91][9801-0-3-1.65][9803-3-3-1.68][9865-3-0-2.97][9896-2-2-1.22][10314-1-2-1.56][10337-3-3-3.01]
[10403-0-4-0.73][10653-2-4-1.69][10704-2-1-1.36][10719-1-1-1.40][10727-1-1-0.74][10836-0-3-3.51][10969-2-3-1.60][11042-0-0-1.83][11088-1-1-1.18][11322-0-0-2.84]
[11398-2-2-1.82][11499-0-3-0.66][11502-3-3-0.88][11512-3-2-1.26][11608-1-1-2.39][11610-0-3-1.69][11692-0-0-1.49][11905-0-0-2.46][11993-1-2-1.88][12002-2-0-2.44]
[12052-0-0-2.31][12201-0-3-2.07][12235-2-1-1.09][12320-1-2-0.63][12377-2-4-1.61][12398-2-3-0.57][12503-1-4-1.59][12617-0-1-2.51][12685-3-2-0.81][12738-2-2-1.08]
[12742-2-2-1.40][12823-0-3-2.09][13110-1-2-1.19][13240-3-3-1.58][13253-1-1-1.52][13273-0-0-4.02][13634-1-1-2.31][13763-2-3-1.21][13905-3-0-1.75][14060-2-1-1.73]
[14065-3-3-1.56][14147-3-0-0.78][14595-2-2-1.45][14687-2-2-0.52][14788-2-2-1.28][14869-1-1-1.12][14872-3-0-0.79][14877-1-4-0.99][14927-0-3-3.07][15066-0-0-4.24]
[15175-1-2-1.26][15178-2-0-0.82][15375-3-3-1.80][15389-3-0-1.62][15568-2-1-1.50][15675-3-3-1.39][15869-1-2-1.10][16207-3-3-0.56][16236-0-0-0.93][16302-3-0-1.00]
[16331-2-2-1.22][16381-0-0-1.68][16488-1-1-0.60][16495-0-0-0.94][16650-0-0-3.47][16719-1-4-1.20][16801-0-0-1.82][16828-0-0-1.91][17137-3-0-1.85][17245-1-1-0.51]
[17278-3-3-1.37][17282-0-2-0.01][17311-2-2-1.81][17336-2-1-1.57][17608-3-3-3.12][17627-0-3-0.42][17877-3-1-0.50][17924-1-2-0.90][17984-3-3-2.49][18211-0-1-0.82]
[18276-3-0-2.23][18287-1-1-1.77][18394-0-0-1.82][18428-0-1-0.50][18442-0-3-1.43][18478-3-0-2.96][18607-0-0-0.77][18616-0-1-0.24][18663-0-2-1.07][18718-0-0-2.64]
[18766-2-1-1.84][18824-2-1-0.73][18890-3-2-0.68][18930-3-2-0.86][18938-3-0-2.25][19817-1-2-1.24][19839-0-2-1.06][19930-3-3-1.07][19944-0-2-1.21][20036-2-2-1.67]
[20101-3-3-0.43][20474-1-2-1.30][20547-3-0-1.08][20929-2-2-1.98][21245-1-2-1.03][21257-3-0-0.57][21293-1-1-2.57][21316-1-1-1.08][21384-1-1-1.58][21448-1-1-0.94]
[21483-0-0-1.98][21487-2-1-0.77][21714-0-0-0.96][21943-3-2-1.57][21947-0-0-1.26][21948-0-0-2.61][21965-2-2-0.83][21998-1-3-0.80][22025-0-3-1.18][22228-3-3-1.98]
[22446-1-1-3.03][22494-3-3-2.11][22757-0-0-2.86][22811-3-3-1.32][22976-3-1-0.66][22985-3-3-1.60][23014-0-3-1.52][23112-1-1-0.86][23144-3-3-2.50][23168-2-0-1.33]
[23219-0-3-0.59][23363-3-3-1.64][23470-0-4-0.70][23486-2-1-0.75][23497-0-3-4.02][23516-0-0-2.50][23690-1-4-1.26][23921-2-1-0.80][23936-1-3-1.02][24040-3-4-0.60]
[24111-1-4-2.16][24182-0-3-4.02][24238-3-3-1.91][24290-2-0-1.99][24345-0-0-2.19][24364-1-2-0.99][24427-3-0-0.89][24477-2-2-1.15][24495-2-1-1.20][24893-2-1-1.51]
[25012-1-2-0.28][25121-2-0-1.03][25165-3-3-2.16][25183-0-0-0.41][25297-3-3-0.99][25398-0-0-1.08][25574-2-1-1.09][25644-1-1-1.68][25718-1-0-0.41][25774-2-3-1.26]
[26032-3-3-3.26][26051-3-0-3.18][26120-0-2-0.67][26321-1-2-0.46][26732-1-1-0.56][26784-3-3-2.89][26827-3-3-1.14][26833-0-3-2.46][26838-2-2-0.65][26860-1-2-1.28]
[26948-0-0-0.88][27049-3-0-2.06][27098-1-3-0.51][27526-0-0-1.40][27639-3-3-0.98][27698-3-3-1.60][27772-0-3-1.62][27890-1-1-1.11][28040-0-4-0.34][28503-2-2-2.14]
[28577-1-2-1.45][28959-0-0-3.84][29198-3-1-0.52][29777-0-0-4.44][29877-2-2-0.70][30035-1-2-1.22][30098-0-0-2.61][30326-1-1-0.80][30572-2-3-0.70][30716-0-4-1.61]
[30806-2-3-0.81][30906-1-4-1.68][31007-0-4-0.21][31181-3-3-1.52][31238-0-3-1.76][31347-0-3-1.61][31422-2-4-0.70][31429-3-0-1.04][31431-0-0-2.18][31432-1-1-0.39]
[31477-0-0-2.96][31524-1-3-0.84][31597-1-2-1.98][31619-1-3-1.04][31701-0-0-1.44][31755-0-3-1.42][31854-3-0-1.68][32074-1-2-0.47][32078-3-3-0.99][32111-1-1-1.92]
[32127-1-2-2.07][32140-3-0-1.66][32263-2-4-0.80][32365-0-0-0.65][32411-2-0-3.83][32429-3-3-2.13][32473-3-0-1.02][32574-3-0-3.64][32584-0-2-0.33][32622-0-1-1.10]
[32858-3-0-1.36][32969-3-0-3.97][33016-2-2-2.34][33031-1-1-0.54][33035-2-2-1.64][33133-2-2-1.29][33173-2-1-0.66][33175-3-1-1.17][33306-3-1-0.74][33309-2-2-0.28]
[33474-0-2-0.67][33478-2-0-1.45][33618-1-4-1.29][33712-0-3-1.22][33782-2-4-1.91][33914-3-3-1.04][34076-3-2-0.93][34112-2-3-0.49][34138-2-1-1.21][34239-1-1-1.21]
[34364-2-2-1.37][34617-1-1-1.09][34751-3-3-2.86][34783-2-1-1.12][35015-3-3-0.97][35018-1-1-1.58][35288-2-4-0.49][0-4-0-0.39][1-4-3-0.53][2-4-0-0.62]
[3-4-1-0.94][4-4-2-0.79][5-4-1-2.45][6-4-4-0.83][7-4-1-0.90][8-4-2-0.88][9-4-1-1.21][10-4-4-2.14][11-4-2-1.12][12-4-1-2.40]
[14-4-3-0.38][15-4-3-2.45][16-4-2-0.29][17-4-3-0.40][18-4-4-0.48][19-4-3-2.06][20-4-0-0.56][21-4-1-1.90][22-4-4-1.70][23-4-1-0.60]
[24-4-2-1.09][25-4-3-0.60][26-4-3-1.04][27-4-0-2.49][28-4-4-1.11][29-4-1-0.33][30-4-0-0.89][31-4-2-0.67][32-4-4-2.18][33-4-3-1.03]
[34-4-2-0.43][35-4-0-2.28][37-4-2-0.69][39-4-3-2.14][40-4-2-0.57][41-4-1-0.46][42-4-1-1.03][43-4-2-1.18][45-4-3-0.92][46-4-2-1.66]
[47-4-4-1.84][48-4-4-0.08][51-4-4-1.80][52-4-2-0.96][53-4-2-0.80][54-4-0-0.39][55-4-2-0.81][56-4-1-1.15][57-4-3-1.94][58-4-1-1.37]
[59-4-4-0.78][60-4-1-0.80][61-4-4-1.66][62-4-2-0.53][63-4-4-0.86][64-4-1-1.17][65-4-4-1.43][66-4-4-1.78][67-4-2-0.63][68-4-1-0.98]
[69-4-2-1.00][70-4-2-0.87][72-4-2-0.98][73-4-1-1.21][74-4-2-0.71][75-4-0-1.08][77-4-4-2.45][78-4-1-0.97][79-4-1-1.38][80-4-1-1.41]
[81-4-4-1.31][82-4-2-0.71][83-4-1-1.02][84-4-2-0.32][85-4-4-1.88][86-4-4-0.84][87-4-4-2.25][88-4-1-0.35][89-4-1-0.69][90-4-0-0.49]
[91-4-0-0.33][92-4-0-0.72][93-4-4-0.50][94-4-4-1.41][95-4-1-0.73][96-4-4-1.15][97-4-2-1.55][98-4-2-0.88][99-4-4-1.58][100-4-1-1.52]
[101-4-1-1.28][102-4-4-0.47][103-4-3-1.55][104-4-4-1.60][105-4-4-1.84][106-4-1-1.11][107-4-4-1.12][108-4-2-1.18][109-4-1-1.52][110-4-4-1.41]
[111-4-0-2.56][112-4-2-0.84][113-4-3-0.78][114-4-3-1.03][115-4-2-0.52][116-4-4-1.13][117-4-1-2.17][119-4-2-1.38][121-4-1-1.64][122-4-1-0.14]
[124-4-2-0.62][125-4-2-1.22][126-4-1-1.58][127-4-2-1.03][128-4-2-0.34][129-4-3-0.33][130-4-4-1.65][131-4-2-0.49][132-4-1-1.17][133-4-0-3.34]
[135-4-2-1.38][136-4-1-0.13][137-4-1-0.41][138-4-2-0.15][139-4-0-0.48][140-4-2-1.01][141-4-3-1.69][142-4-4-1.37][143-4-4-1.91][144-4-4-1.77]
[145-4-2-1.20][148-4-0-1.43][149-4-2-1.39][150-4-1-1.83][151-4-4-1.18][152-4-1-2.32][153-4-1-1.91][154-4-2-1.12][155-4-4-1.54][156-4-0-1.39]
[157-4-0-0.89][158-4-3-0.73][160-4-1-1.10][161-4-3-1.38][162-4-4-1.37][164-4-2-1.04][165-4-1-0.97][167-4-0-1.33][168-4-4-1.56][170-4-1-0.34]
[171-4-4-0.72][172-4-4-2.02][173-4-0-0.51][174-4-0-1.86][175-4-4-1.14][177-4-0-1.60][178-4-2-0.75][179-4-1-0.31][180-4-2-0.94][181-4-1-0.72]
[182-4-1-0.59][183-4-4-1.08][184-4-4-1.16][186-4-2-0.58][187-4-1-1.80][188-4-4-0.99][189-4-4-1.22][190-4-1-1.73][191-4-0-0.43][192-4-2-0.83]
[193-4-1-1.68][194-4-3-0.31][195-4-0-1.34][196-4-2-0.59][197-4-4-1.67][198-4-4-1.75][199-4-2-0.78]
---------------------------
I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 1.275 | Acc: 43.000% | Wgt Acc: 45.048%
	I - Batch: 100 | Loss: 1.291 | Acc: 41.938% | Wgt Acc: 44.713%
	I - Batch: 150 | Loss: 1.291 | Acc: 41.750% | Wgt Acc: 44.818%
	I - Batch: 200 | Loss: 1.281 | Acc: 42.438% | Wgt Acc: 45.400%
I - num batch: 222
I - Train -- Loss: 1.281 | Acc: 42.317% | Wgt Acc: 45.479% | LR: 5.000000e-03 | Dur: 134.59s
I - Confusion Matrix: [row->prediction - col->label]
[[391.  32.  64. 186. 159.]
 [ 34. 283. 156.  50. 178.]
 [ 84. 180. 369.  71. 313.]
 [145.  42.  67. 206.  98.]
 [ 43.  41.  78.  25. 252.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.668 | Acc: 30.966% | Wgt Acc: 33.810% | Dur: 14.01s
I - Confusion Matrix: [row->prediction - col->label]
[[31.  0.  3. 12. 11.]
 [ 7. 30. 12. 16. 50.]
 [35. 40. 51. 44. 81.]
 [ 9.  1.  1. 12.  5.]
 [ 6.  7.  8.  2. 33.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 1.238 | Acc: 41.875% | Wgt Acc: 45.491%
	I - Batch: 100 | Loss: 1.250 | Acc: 42.938% | Wgt Acc: 45.991%
	I - Batch: 150 | Loss: 1.246 | Acc: 44.458% | Wgt Acc: 47.428%
	I - Batch: 200 | Loss: 1.263 | Acc: 43.531% | Wgt Acc: 46.337%
I - num batch: 222
I - Train -- Loss: 1.262 | Acc: 43.727% | Wgt Acc: 46.551% | LR: 5.000000e-03 | Dur: 132.77s
I - Confusion Matrix: [row->prediction - col->label]
[[391.  35.  74. 195. 167.]
 [ 35. 269. 121.  50. 166.]
 [ 83. 197. 422.  89. 304.]
 [149.  41.  48. 190.  84.]
 [ 39.  36.  69.  14. 279.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.462 | Acc: 40.434% | Wgt Acc: 39.745% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[45.  4.  5. 39. 20.]
 [10. 39. 17. 15. 36.]
 [11. 15. 26.  8. 26.]
 [10. 10. 13. 19. 22.]
 [12. 10. 14.  5. 76.]]

I - Local maximum validation set accuracy:  40.43

I - Validation set results: 
[14-1-2-0.96][50-3-1-2.64][124-2-0-1.46][127-0-0-0.67][443-2-2-1.61][567-0-2-1.16][573-1-1-1.54][615-0-3-2.23][695-1-0-0.78][722-3-3-0.30]
[826-0-0-4.08][878-0-0-2.22][1103-0-4-0.45][1212-3-3-0.67][1368-0-2-1.30][2181-2-0-1.79][2476-2-1-0.70][2721-2-3-0.54][2818-1-2-1.10][2886-2-1-0.93]
[3231-2-2-1.66][3333-2-2-1.23][3482-2-2-1.16][3536-3-0-2.01][3625-1-1-1.85][3909-0-3-0.35][4035-0-0-3.71][4140-0-2-1.04][4214-1-0-2.64][4346-1-0-1.75]
[4581-2-2-1.19][4708-3-2-0.99][4838-3-4-1.82][4845-1-2-0.64][4868-0-0-2.80][4939-0-1-1.82][4984-2-2-0.35][5078-1-1-1.83][5396-0-0-4.84][5479-1-3-0.42]
[5717-0-4-0.59][5843-1-1-1.23][5949-3-3-1.62][5987-2-4-2.27][6014-3-3-0.91][6033-3-0-0.37][6313-0-0-1.11][6421-3-1-0.94][6500-1-1-1.35][6583-3-0-1.61]
[6683-3-1-0.73][6825-2-2-0.29][6998-3-1-1.56][7049-3-0-1.03][7517-1-2-0.92][7521-1-1-1.09][7528-1-3-1.21][7949-1-4-1.99][8135-1-4-1.03][8185-3-0-1.66]
[8269-3-2-1.82][8273-3-0-3.40][8543-3-0-2.82][8666-1-1-1.43][8672-0-0-2.33][8903-1-3-0.58][9001-2-1-1.81][9036-2-2-1.77][9281-3-1-1.53][9300-2-4-1.63]
[9571-0-2-0.11][9617-1-1-1.64][9644-2-2-1.05][9705-2-3-2.14][9801-0-3-1.84][9803-3-3-0.73][9865-3-0-2.09][9896-2-4-1.29][10314-1-1-1.13][10337-3-0-3.93]
[10403-0-2-0.90][10653-2-2-1.29][10704-2-1-1.22][10719-1-4-0.91][10727-1-2-0.65][10836-0-0-2.53][10969-2-0-2.14][11042-0-4-0.59][11088-1-1-1.35][11322-0-0-2.34]
[11398-2-3-0.76][11499-0-0-0.97][11502-3-1-1.42][11512-3-2-0.53][11608-1-1-2.45][11610-0-3-2.41][11692-0-0-2.36][11905-0-2-1.00][11993-1-1-1.38][12002-2-0-1.47]
[12052-0-0-0.32][12201-0-0-3.14][12235-2-4-1.68][12320-1-4-1.45][12377-2-4-1.81][12398-2-3-1.00][12503-1-4-1.61][12617-0-1-1.33][12685-3-0-0.74][12738-2-3-1.20]
[12742-2-2-1.11][12823-0-0-2.33][13110-1-1-2.27][13240-3-0-1.54][13253-1-1-0.87][13273-0-0-3.36][13634-1-1-1.14][13763-2-1-0.60][13905-3-0-2.29][14060-2-1-1.51]
[14065-3-3-0.97][14147-3-0-1.66][14595-2-2-0.57][14687-2-2-1.46][14788-2-2-1.15][14869-1-1-1.93][14872-3-0-0.53][14877-1-1-1.23][14927-0-0-2.06][15066-0-0-1.88]
[15175-1-1-0.76][15178-2-4-1.14][15375-3-1-0.65][15389-3-0-2.37][15568-2-1-1.11][15675-3-1-0.47][15869-1-4-1.26][16207-3-2-0.84][16236-0-0-1.49][16302-3-0-3.47]
[16331-2-2-0.76][16381-0-1-1.01][16488-1-1-0.84][16495-0-2-1.00][16650-0-0-2.17][16719-1-1-1.49][16801-0-0-1.40][16828-0-3-1.36][17137-3-0-1.76][17245-1-1-1.07]
[17278-3-4-1.18][17282-0-1-0.33][17311-2-2-1.39][17336-2-2-0.62][17608-3-3-2.47][17627-0-2-0.42][17877-3-4-0.45][17924-1-4-0.55][17984-3-3-2.10][18211-0-1-1.24]
[18276-3-0-1.81][18287-1-1-1.16][18394-0-0-2.79][18428-0-0-4.90][18442-0-0-1.21][18478-3-0-0.77][18607-0-4-0.81][18616-0-1-0.93][18663-0-3-0.63][18718-0-0-3.32]
[18766-2-1-0.79][18824-2-4-1.81][18890-3-2-0.86][18930-3-2-1.37][18938-3-0-2.63][19817-1-2-1.20][19839-0-3-0.68][19930-3-1-0.95][19944-0-0-1.04][20036-2-4-2.00]
[20101-3-3-1.89][20474-1-1-0.98][20547-3-2-0.69][20929-2-2-1.08][21245-1-2-0.84][21257-3-2-0.49][21293-1-2-1.37][21316-1-3-0.68][21384-1-4-1.17][21448-1-1-0.85]
[21483-0-0-2.57][21487-2-3-0.86][21714-0-3-0.64][21943-3-1-0.84][21947-0-4-0.56][21948-0-0-1.37][21965-2-2-2.19][21998-1-3-1.09][22025-0-1-0.52][22228-3-3-1.18]
[22446-1-1-2.36][22494-3-0-2.71][22757-0-0-2.34][22811-3-0-3.24][22976-3-1-0.68][22985-3-0-2.02][23014-0-0-3.20][23112-1-2-1.72][23144-3-0-4.36][23168-2-4-1.43]
[23219-0-4-1.46][23363-3-0-1.42][23470-0-4-0.76][23486-2-1-0.59][23497-0-3-3.16][23516-0-3-0.85][23690-1-2-0.86][23921-2-4-0.75][23936-1-3-0.73][24040-3-3-0.94]
[24111-1-1-1.87][24182-0-0-5.32][24238-3-3-2.37][24290-2-3-0.66][24345-0-0-1.70][24364-1-2-1.47][24427-3-1-0.37][24477-2-2-0.34][24495-2-1-0.67][24893-2-2-0.69]
[25012-1-1-1.67][25121-2-4-2.22][25165-3-3-0.59][25183-0-1-1.31][25297-3-0-0.39][25398-0-2-0.83][25574-2-1-1.48][25644-1-1-1.31][25718-1-4-0.67][25774-2-1-0.47]
[26032-3-0-4.08][26051-3-0-2.69][26120-0-4-1.11][26321-1-3-0.57][26732-1-1-1.17][26784-3-0-4.37][26827-3-0-1.32][26833-0-0-2.57][26838-2-3-0.26][26860-1-3-0.84]
[26948-0-1-0.90][27049-3-0-1.53][27098-1-0-0.62][27526-0-4-0.74][27639-3-4-0.73][27698-3-0-2.74][27772-0-0-2.83][27890-1-1-1.76][28040-0-2-1.05][28503-2-2-1.32]
[28577-1-1-1.34][28959-0-0-1.96][29198-3-1-0.92][29777-0-0-5.54][29877-2-2-0.69][30035-1-1-1.35][30098-0-0-1.95][30326-1-1-1.76][30572-2-3-0.52][30716-0-4-1.46]
[30806-2-3-0.49][30906-1-2-1.45][31007-0-2-0.25][31181-3-0-1.64][31238-0-0-2.37][31347-0-0-3.34][31422-2-1-1.26][31429-3-1-0.42][31431-0-0-1.19][31432-1-1-1.58]
[31477-0-0-3.96][31524-1-3-1.90][31597-1-1-0.34][31619-1-4-0.44][31701-0-0-2.27][31755-0-0-2.79][31854-3-3-1.39][32074-1-2-0.97][32078-3-3-2.37][32111-1-1-1.16]
[32127-1-2-2.10][32140-3-0-1.04][32263-2-4-1.20][32365-0-4-0.86][32411-2-0-1.86][32429-3-0-2.76][32473-3-3-1.50][32574-3-0-4.15][32584-0-0-0.21][32622-0-1-1.32]
[32858-3-0-4.07][32969-3-0-1.74][33016-2-4-2.07][33031-1-2-1.80][33035-2-2-1.28][33133-2-2-0.73][33173-2-1-1.09][33175-3-4-0.73][33306-3-1-0.46][33309-2-1-0.49]
[33474-0-4-0.37][33478-2-3-0.39][33618-1-1-2.85][33712-0-0-1.16][33782-2-4-1.43][33914-3-3-0.94][34076-3-3-2.38][34112-2-3-1.41][34138-2-3-1.43][34239-1-3-0.46]
[34364-2-1-0.92][34617-1-1-1.30][34751-3-3-1.91][34783-2-1-1.10][35015-3-0-0.59][35018-1-1-1.81][35288-2-2-0.50][0-4-4-1.69][1-4-4-0.28][2-4-4-0.71]
[3-4-4-2.03][4-4-0-0.20][5-4-1-1.59][6-4-4-1.20][7-4-4-2.06][8-4-3-0.97][9-4-4-1.34][10-4-4-0.62][11-4-4-0.73][12-4-1-0.98]
[14-4-3-1.88][15-4-0-2.04][16-4-2-1.14][17-4-1-0.80][18-4-4-1.31][19-4-3-1.61][20-4-1-1.09][21-4-2-1.38][22-4-4-1.35][23-4-1-0.85]
[24-4-4-1.26][25-4-3-1.03][26-4-1-0.96][27-4-0-2.80][28-4-4-0.99][29-4-2-0.98][30-4-4-2.23][31-4-1-1.13][32-4-4-1.69][33-4-1-1.12]
[34-4-3-0.89][35-4-0-1.95][37-4-3-0.40][39-4-0-1.55][40-4-3-0.68][41-4-1-1.64][42-4-1-0.38][43-4-2-0.47][45-4-4-1.06][46-4-4-1.90]
[47-4-4-2.18][48-4-4-1.91][51-4-4-2.05][52-4-4-0.75][53-4-1-2.16][54-4-2-0.11][55-4-3-0.54][56-4-4-1.61][57-4-3-1.69][58-4-1-0.98]
[59-4-4-2.64][60-4-1-2.03][61-4-4-1.31][62-4-3-0.52][63-4-4-0.90][64-4-2-1.92][65-4-2-1.14][66-4-4-2.30][67-4-3-0.48][68-4-1-1.52]
[69-4-0-0.86][70-4-4-2.26][72-4-2-1.03][73-4-1-2.04][74-4-0-1.29][75-4-2-1.01][77-4-1-1.80][78-4-2-0.72][79-4-4-2.40][80-4-4-1.68]
[81-4-4-1.33][82-4-2-1.66][83-4-4-0.68][84-4-4-1.75][85-4-4-1.67][86-4-4-1.17][87-4-4-2.70][88-4-4-1.94][89-4-4-1.48][90-4-4-1.98]
[91-4-4-0.63][92-4-1-0.42][93-4-4-1.19][94-4-4-1.81][95-4-4-1.40][96-4-4-1.68][97-4-1-1.95][98-4-2-1.28][99-4-4-0.37][100-4-2-0.96]
[101-4-4-2.33][102-4-1-1.79][103-4-0-1.98][104-4-4-1.79][105-4-2-1.59][106-4-2-1.44][107-4-4-1.17][108-4-4-0.73][109-4-4-1.01][110-4-4-1.64]
[111-4-0-2.12][112-4-4-1.82][113-4-1-0.84][114-4-0-0.95][115-4-4-1.57][116-4-1-1.28][117-4-1-2.30][119-4-0-1.24][121-4-1-1.56][122-4-2-0.82]
[124-4-1-0.81][125-4-1-1.57][126-4-3-0.76][127-4-2-1.45][128-4-4-0.33][129-4-0-0.78][130-4-4-1.65][131-4-3-0.78][132-4-0-1.68][133-4-0-2.31]
[135-4-2-0.62][136-4-1-0.59][137-4-1-1.09][138-4-3-0.41][139-4-4-0.74][140-4-2-0.72][141-4-3-2.38][142-4-4-2.44][143-4-4-1.87][144-4-4-1.53]
[145-4-4-2.59][148-4-0-1.66][149-4-0-0.63][150-4-4-2.41][151-4-2-1.33][152-4-1-2.11][153-4-4-1.04][154-4-3-0.29][155-4-4-1.68][156-4-2-0.63]
[157-4-2-0.79][158-4-3-0.70][160-4-1-0.84][161-4-3-1.67][162-4-2-0.36][164-4-1-1.00][165-4-1-1.65][167-4-4-1.53][168-4-4-1.72][170-4-0-2.19]
[171-4-0-1.17][172-4-4-2.07][173-4-4-1.29][174-4-0-2.18][175-4-4-0.99][177-4-0-1.49][178-4-4-1.65][179-4-2-0.69][180-4-4-1.89][181-4-3-0.46]
[182-4-1-0.99][183-4-1-1.44][184-4-3-0.46][186-4-4-1.61][187-4-1-0.81][188-4-2-0.52][189-4-3-0.49][190-4-4-1.25][191-4-4-1.13][192-4-4-0.80]
[193-4-1-1.07][194-4-1-0.83][195-4-2-0.77][196-4-4-1.08][197-4-4-2.24][198-4-4-1.90][199-4-3-0.65]
---------------------------
I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 1.227 | Acc: 46.500% | Wgt Acc: 48.655%
	I - Batch: 100 | Loss: 1.250 | Acc: 45.750% | Wgt Acc: 47.838%
	I - Batch: 150 | Loss: 1.250 | Acc: 45.708% | Wgt Acc: 47.528%
	I - Batch: 200 | Loss: 1.251 | Acc: 45.312% | Wgt Acc: 47.584%
I - num batch: 222
I - Train -- Loss: 1.251 | Acc: 45.334% | Wgt Acc: 47.831% | LR: 5.000000e-03 | Dur: 135.83s
I - Confusion Matrix: [row->prediction - col->label]
[[411.  37.  61. 173. 135.]
 [ 37. 275. 142.  52. 158.]
 [ 78. 180. 392.  74. 309.]
 [139.  33.  54. 215.  83.]
 [ 32.  53.  85.  24. 315.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.451 | Acc: 37.673% | Wgt Acc: 40.842% | Dur: 15.41s
I - Confusion Matrix: [row->prediction - col->label]
[[48.  3.  5. 31. 19.]
 [12. 44. 25. 19. 74.]
 [ 8. 22. 33. 14. 39.]
 [18.  0.  2. 22.  4.]
 [ 2.  9. 10.  0. 44.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 1.216 | Acc: 47.625% | Wgt Acc: 50.640%
	I - Batch: 100 | Loss: 1.247 | Acc: 46.000% | Wgt Acc: 48.681%
	I - Batch: 150 | Loss: 1.255 | Acc: 44.750% | Wgt Acc: 47.847%
	I - Batch: 200 | Loss: 1.261 | Acc: 44.281% | Wgt Acc: 47.237%
I - num batch: 222
I - Train -- Loss: 1.255 | Acc: 44.883% | Wgt Acc: 47.745% | LR: 5.000000e-03 | Dur: 133.52s
I - Confusion Matrix: [row->prediction - col->label]
[[415.  35.  68. 161. 162.]
 [ 35. 276. 139.  55. 165.]
 [ 79. 176. 390.  83. 285.]
 [129.  50.  59. 219.  96.]
 [ 39.  41.  78.  20. 292.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.604 | Acc: 32.939% | Wgt Acc: 36.893% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[51.  2.  4. 33. 15.]
 [14. 40. 25. 25. 66.]
 [15. 31. 41. 21. 71.]
 [ 5.  0.  2.  7.  0.]
 [ 3.  5.  3.  0. 28.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 1.195 | Acc: 45.875% | Wgt Acc: 49.201%
	I - Batch: 100 | Loss: 1.160 | Acc: 49.188% | Wgt Acc: 52.406%
	I - Batch: 150 | Loss: 1.144 | Acc: 49.708% | Wgt Acc: 52.998%
	I - Batch: 200 | Loss: 1.155 | Acc: 49.156% | Wgt Acc: 52.298%
I - num batch: 222
I - Train -- Loss: 1.152 | Acc: 49.337% | Wgt Acc: 52.487% | LR: 2.500000e-03 | Dur: 134.33s
I - Confusion Matrix: [row->prediction - col->label]
[[445.  24.  53. 177. 175.]
 [ 27. 316. 121.  31. 139.]
 [ 50. 175. 440.  80. 297.]
 [139.  27.  45. 229.  69.]
 [ 36.  36.  75.  21. 320.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.351 | Acc: 45.365% | Wgt Acc: 48.657% | Dur: 15.93s
I - Confusion Matrix: [row->prediction - col->label]
[[47.  2.  4. 24. 18.]
 [ 8. 54. 26. 15. 56.]
 [ 6. 14. 30.  6. 33.]
 [22.  1.  7. 40. 14.]
 [ 5.  7.  8.  1. 59.]]

I - Local maximum validation set accuracy:  45.36

I - Validation set results: 
[14-1-1-1.72][50-3-1-1.19][124-2-2-0.90][127-0-0-3.06][443-2-2-1.66][567-0-0-1.06][573-1-1-1.40][615-0-0-1.86][695-1-2-0.75][722-3-3-2.24]
[826-0-0-3.45][878-0-0-2.43][1103-0-4-1.05][1212-3-2-0.68][1368-0-0-2.17][2181-2-3-1.31][2476-2-1-0.55][2721-2-2-1.99][2818-1-1-0.97][2886-2-1-2.40]
[3231-2-1-2.52][3333-2-1-1.34][3482-2-2-1.60][3536-3-0-1.82][3625-1-1-3.96][3909-0-3-1.55][4035-0-3-3.50][4140-0-0-1.62][4214-1-1-1.04][4346-1-4-1.41]
[4581-2-2-1.36][4708-3-2-0.46][4838-3-2-0.28][4845-1-1-0.50][4868-0-0-3.10][4939-0-1-1.30][4984-2-3-0.68][5078-1-1-1.56][5396-0-0-5.28][5479-1-1-1.92]
[5717-0-0-1.44][5843-1-1-1.55][5949-3-3-2.25][5987-2-4-2.81][6014-3-1-1.26][6033-3-0-2.21][6313-0-3-2.75][6421-3-3-1.75][6500-1-1-0.96][6583-3-3-1.51]
[6683-3-1-0.82][6825-2-3-0.81][6998-3-1-1.30][7049-3-3-1.29][7517-1-1-2.79][7521-1-1-2.47][7528-1-2-1.17][7949-1-2-1.90][8135-1-0-0.83][8185-3-0-2.63]
[8269-3-1-2.52][8273-3-3-1.57][8543-3-0-3.46][8666-1-1-1.34][8672-0-0-3.67][8903-1-2-0.69][9001-2-1-2.17][9036-2-2-2.56][9281-3-1-0.42][9300-2-2-1.34]
[9571-0-0-0.71][9617-1-1-0.53][9644-2-2-1.95][9705-2-1-1.49][9801-0-3-1.82][9803-3-3-1.50][9865-3-3-3.37][9896-2-4-1.31][10314-1-1-1.88][10337-3-3-3.71]
[10403-0-2-1.25][10653-2-4-1.57][10704-2-1-2.44][10719-1-1-2.68][10727-1-1-1.55][10836-0-3-4.45][10969-2-3-1.21][11042-0-3-1.35][11088-1-1-2.21][11322-0-0-2.73]
[11398-2-2-1.83][11499-0-3-1.22][11502-3-3-0.88][11512-3-1-1.37][11608-1-1-2.61][11610-0-0-0.78][11692-0-0-1.83][11905-0-0-2.36][11993-1-1-2.06][12002-2-3-1.87]
[12052-0-0-2.35][12201-0-3-2.51][12235-2-1-1.05][12320-1-2-0.78][12377-2-4-1.80][12398-2-3-1.04][12503-1-2-1.34][12617-0-1-1.78][12685-3-1-0.21][12738-2-2-0.71]
[12742-2-2-1.41][12823-0-3-3.33][13110-1-1-1.95][13240-3-3-1.76][13253-1-1-2.35][13273-0-0-3.68][13634-1-1-1.29][13763-2-2-0.86][13905-3-0-1.80][14060-2-1-2.10]
[14065-3-3-1.30][14147-3-1-1.30][14595-2-2-1.73][14687-2-2-1.14][14788-2-2-1.78][14869-1-1-2.19][14872-3-2-1.38][14877-1-4-1.32][14927-0-3-3.28][15066-0-0-3.79]
[15175-1-1-0.71][15178-2-0-0.47][15375-3-3-1.58][15389-3-3-2.07][15568-2-4-0.70][15675-3-1-1.36][15869-1-1-1.88][16207-3-0-0.90][16236-0-1-0.57][16302-3-0-2.54]
[16331-2-2-1.75][16381-0-0-1.32][16488-1-1-2.22][16495-0-0-1.22][16650-0-0-3.99][16719-1-4-1.32][16801-0-0-3.13][16828-0-0-1.23][17137-3-3-1.40][17245-1-2-0.85]
[17278-3-3-0.54][17282-0-2-0.28][17311-2-2-1.65][17336-2-1-2.53][17608-3-3-4.48][17627-0-1-0.74][17877-3-1-1.78][17924-1-2-0.37][17984-3-0-3.07][18211-0-3-1.54]
[18276-3-0-2.05][18287-1-1-1.29][18394-0-0-2.90][18428-0-0-2.13][18442-0-3-2.14][18478-3-0-1.86][18607-0-2-0.72][18616-0-1-0.38][18663-0-3-0.74][18718-0-0-3.05]
[18766-2-1-1.91][18824-2-4-1.14][18890-3-2-0.56][18930-3-2-1.17][18938-3-0-2.01][19817-1-1-1.19][19839-0-2-1.08][19930-3-3-1.02][19944-0-4-1.08][20036-2-2-1.80]
[20101-3-3-1.30][20474-1-1-2.23][20547-3-0-1.50][20929-2-2-2.33][21245-1-1-2.21][21257-3-0-0.14][21293-1-1-3.38][21316-1-1-2.82][21384-1-2-0.91][21448-1-1-2.65]
[21483-0-0-2.69][21487-2-1-1.71][21714-0-0-0.87][21943-3-1-1.41][21947-0-0-1.23][21948-0-0-2.23][21965-2-2-2.84][21998-1-1-0.79][22025-0-1-0.57][22228-3-3-3.26]
[22446-1-1-3.25][22494-3-0-2.57][22757-0-3-2.04][22811-3-3-3.00][22976-3-1-1.14][22985-3-3-1.79][23014-0-3-2.31][23112-1-1-2.08][23144-3-0-3.05][23168-2-1-0.77]
[23219-0-0-1.02][23363-3-3-1.94][23470-0-1-2.01][23486-2-2-0.79][23497-0-3-3.94][23516-0-0-3.15][23690-1-4-2.15][23921-2-2-0.84][23936-1-3-1.80][24040-3-4-0.54]
[24111-1-4-2.83][24182-0-3-4.93][24238-3-3-2.61][24290-2-0-1.60][24345-0-0-1.96][24364-1-2-1.39][24427-3-3-1.53][24477-2-2-1.59][24495-2-1-2.19][24893-2-1-2.50]
[25012-1-1-0.83][25121-2-2-1.84][25165-3-3-2.41][25183-0-0-0.59][25297-3-3-1.10][25398-0-0-0.81][25574-2-0-0.56][25644-1-1-2.68][25718-1-1-1.74][25774-2-2-1.23]
[26032-3-3-2.48][26051-3-3-3.05][26120-0-4-0.51][26321-1-1-2.10][26732-1-1-1.95][26784-3-3-4.25][26827-3-3-1.21][26833-0-3-3.47][26838-2-1-1.20][26860-1-4-0.76]
[26948-0-0-1.14][27049-3-0-1.44][27098-1-1-0.95][27526-0-0-0.80][27639-3-3-2.26][27698-3-3-2.11][27772-0-3-2.38][27890-1-1-1.43][28040-0-2-0.70][28503-2-2-1.78]
[28577-1-1-1.00][28959-0-3-3.62][29198-3-0-0.26][29777-0-0-4.36][29877-2-1-1.21][30035-1-1-1.99][30098-0-3-1.92][30326-1-1-3.48][30572-2-2-0.84][30716-0-4-1.87]
[30806-2-1-0.86][30906-1-4-2.24][31007-0-0-1.06][31181-3-3-1.40][31238-0-3-1.37][31347-0-0-1.69][31422-2-1-1.42][31429-3-0-0.81][31431-0-0-2.57][31432-1-1-2.61]
[31477-0-0-3.03][31524-1-2-0.38][31597-1-2-1.61][31619-1-0-1.13][31701-0-0-2.69][31755-0-0-2.05][31854-3-3-1.50][32074-1-2-0.46][32078-3-3-1.13][32111-1-1-2.41]
[32127-1-2-2.20][32140-3-0-1.68][32263-2-1-0.91][32365-0-2-0.93][32411-2-3-3.41][32429-3-0-3.32][32473-3-0-1.52][32574-3-0-3.70][32584-0-1-0.68][32622-0-4-0.64]
[32858-3-0-1.80][32969-3-0-3.30][33016-2-2-2.66][33031-1-1-1.91][33035-2-2-1.95][33133-2-1-1.19][33173-2-1-1.93][33175-3-1-1.49][33306-3-1-1.10][33309-2-1-0.23]
[33474-0-0-0.73][33478-2-0-1.21][33618-1-1-2.03][33712-0-0-1.54][33782-2-4-1.32][33914-3-3-1.09][34076-3-3-1.31][34112-2-1-1.78][34138-2-1-2.20][34239-1-1-2.38]
[34364-2-1-2.36][34617-1-1-0.59][34751-3-3-3.20][34783-2-4-1.54][35015-3-3-0.98][35018-1-1-2.64][35288-2-2-0.55][0-4-4-1.55][1-4-0-0.84][2-4-2-0.69]
[3-4-1-1.19][4-4-2-1.45][5-4-1-2.38][6-4-3-1.28][7-4-1-1.09][8-4-2-0.85][9-4-0-0.76][10-4-4-3.22][11-4-4-1.07][12-4-1-1.53]
[14-4-3-0.98][15-4-3-3.29][16-4-1-0.84][17-4-1-0.24][18-4-4-0.95][19-4-3-2.57][20-4-1-1.10][21-4-1-1.82][22-4-4-2.33][23-4-1-2.40]
[24-4-2-0.87][25-4-3-0.93][26-4-3-0.57][27-4-0-2.28][28-4-4-2.10][29-4-1-1.27][30-4-2-0.99][31-4-1-0.75][32-4-4-1.72][33-4-3-0.91]
[34-4-1-0.33][35-4-3-1.78][37-4-0-0.43][39-4-3-1.75][40-4-1-0.31][41-4-1-0.75][42-4-1-0.90][43-4-1-1.59][45-4-2-1.03][46-4-2-1.74]
[47-4-4-2.36][48-4-2-1.29][51-4-4-2.88][52-4-2-1.42][53-4-2-1.03][54-4-4-0.33][55-4-2-0.70][56-4-1-1.32][57-4-3-2.01][58-4-1-2.39]
[59-4-4-0.40][60-4-1-1.57][61-4-4-2.36][62-4-0-0.49][63-4-2-1.20][64-4-1-0.59][65-4-4-1.62][66-4-4-2.43][67-4-2-0.84][68-4-1-2.37]
[69-4-0-1.97][70-4-4-1.54][72-4-1-1.96][73-4-1-1.59][74-4-1-0.76][75-4-2-0.72][77-4-1-2.25][78-4-2-1.01][79-4-4-2.03][80-4-4-3.26]
[81-4-4-2.09][82-4-2-1.38][83-4-1-1.55][84-4-2-0.51][85-4-2-1.23][86-4-4-2.13][87-4-4-3.16][88-4-1-1.34][89-4-4-1.09][90-4-0-0.77]
[91-4-0-0.63][92-4-1-1.04][93-4-4-0.53][94-4-4-1.56][95-4-2-0.71][96-4-4-2.98][97-4-2-1.36][98-4-2-1.37][99-4-4-0.78][100-4-1-1.77]
[101-4-4-2.27][102-4-2-1.05][103-4-0-1.68][104-4-4-2.15][105-4-4-2.47][106-4-1-1.47][107-4-0-1.01][108-4-1-0.89][109-4-1-1.42][110-4-4-1.44]
[111-4-3-3.22][112-4-2-1.39][113-4-1-0.79][114-4-0-1.23][115-4-1-1.62][116-4-1-1.88][117-4-4-2.65][119-4-2-1.03][121-4-4-1.37][122-4-1-0.61]
[124-4-1-1.67][125-4-1-2.16][126-4-1-1.93][127-4-4-1.85][128-4-4-0.43][129-4-0-0.89][130-4-4-1.09][131-4-2-0.34][132-4-1-0.96][133-4-0-3.28]
[135-4-2-1.22][136-4-1-0.69][137-4-1-1.08][138-4-4-0.44][139-4-0-0.82][140-4-1-0.66][141-4-3-0.92][142-4-4-2.29][143-4-2-1.13][144-4-4-1.82]
[145-4-4-1.87][148-4-0-2.51][149-4-4-0.57][150-4-4-1.48][151-4-1-1.77][152-4-1-1.84][153-4-4-2.22][154-4-2-1.36][155-4-1-1.14][156-4-0-0.72]
[157-4-2-0.75][158-4-4-0.05][160-4-4-2.37][161-4-1-1.07][162-4-4-0.70][164-4-4-1.03][165-4-4-1.74][167-4-0-0.81][168-4-4-2.33][170-4-4-0.33]
[171-4-4-0.87][172-4-4-2.11][173-4-2-0.63][174-4-0-1.56][175-4-1-1.84][177-4-3-0.54][178-4-4-1.01][179-4-3-1.09][180-4-4-1.43][181-4-1-0.73]
[182-4-1-1.68][183-4-4-1.22][184-4-4-0.85][186-4-2-0.09][187-4-1-1.69][188-4-2-0.90][189-4-4-1.39][190-4-1-1.57][191-4-4-1.83][192-4-4-0.94]
[193-4-1-2.77][194-4-2-0.52][195-4-1-1.31][196-4-1-1.72][197-4-4-2.54][198-4-4-2.45][199-4-1-1.58]
---------------------------
I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 1.173 | Acc: 49.625% | Wgt Acc: 51.876%
	I - Batch: 100 | Loss: 1.138 | Acc: 51.062% | Wgt Acc: 53.681%
	I - Batch: 150 | Loss: 1.139 | Acc: 50.375% | Wgt Acc: 52.966%
	I - Batch: 200 | Loss: 1.134 | Acc: 50.562% | Wgt Acc: 53.370%
I - num batch: 222
I - Train -- Loss: 1.139 | Acc: 50.832% | Wgt Acc: 53.630% | LR: 2.500000e-03 | Dur: 134.38s
I - Confusion Matrix: [row->prediction - col->label]
[[440.  21.  41. 157. 138.]
 [ 22. 326. 114.  50. 148.]
 [ 56. 146. 446.  64. 282.]
 [136.  36.  56. 237.  78.]
 [ 43.  49.  77.  30. 354.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.464 | Acc: 40.237% | Wgt Acc: 41.187% | Dur: 14.12s
I - Confusion Matrix: [row->prediction - col->label]
[[51.  5.  7. 33. 24.]
 [ 8. 35. 11. 15. 44.]
 [ 9. 23. 38. 19. 44.]
 [17.  1.  3. 17.  5.]
 [ 3. 14. 16.  2. 63.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 1.131 | Acc: 52.250% | Wgt Acc: 55.240%
	I - Batch: 100 | Loss: 1.122 | Acc: 52.062% | Wgt Acc: 54.714%
	I - Batch: 150 | Loss: 1.115 | Acc: 52.792% | Wgt Acc: 55.365%
	I - Batch: 200 | Loss: 1.122 | Acc: 51.875% | Wgt Acc: 54.517%
I - num batch: 222
I - Train -- Loss: 1.119 | Acc: 52.241% | Wgt Acc: 54.795% | LR: 2.500000e-03 | Dur: 138.10s
I - Confusion Matrix: [row->prediction - col->label]
[[457.  29.  53. 171. 144.]
 [ 29. 325. 110.  42. 147.]
 [ 49. 154. 458.  69. 260.]
 [127.  31.  36. 234.  70.]
 [ 35.  39.  77.  22. 379.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.241 | Acc: 51.677% | Wgt Acc: 48.898% | Dur: 14.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   1.   7.  28.  15.]
 [  2.  32.  14.   6.  17.]
 [  4.  17.  27.   5.  19.]
 [ 18.   9.   6.  39.  17.]
 [ 12.  19.  21.   8. 112.]]

I - Local maximum validation set accuracy:  51.68

I - Validation set results: 
[14-1-2-1.66][50-3-4-1.34][124-2-4-0.96][127-0-0-3.66][443-2-2-1.60][567-0-0-1.10][573-1-1-1.06][615-0-0-2.32][695-1-2-0.20][722-3-3-2.61]
[826-0-0-2.41][878-0-0-2.82][1103-0-4-0.97][1212-3-3-0.93][1368-0-0-2.19][2181-2-0-0.79][2476-2-2-0.38][2721-2-4-1.51][2818-1-2-0.93][2886-2-1-2.06]
[3231-2-2-3.23][3333-2-1-1.68][3482-2-2-1.54][3536-3-3-1.56][3625-1-1-2.58][3909-0-3-1.80][4035-0-3-2.56][4140-0-0-1.92][4214-1-2-0.73][4346-1-4-2.31]
[4581-2-4-1.70][4708-3-3-1.05][4838-3-2-0.55][4845-1-1-1.03][4868-0-0-3.43][4939-0-4-1.72][4984-2-2-0.59][5078-1-1-1.25][5396-0-0-5.43][5479-1-1-1.29]
[5717-0-0-1.01][5843-1-1-1.46][5949-3-0-2.80][5987-2-4-3.82][6014-3-3-0.44][6033-3-0-2.00][6313-0-3-2.24][6421-3-3-1.95][6500-1-1-0.78][6583-3-2-1.48]
[6683-3-3-0.96][6825-2-3-1.09][6998-3-1-1.39][7049-3-3-1.26][7517-1-1-2.26][7521-1-1-2.06][7528-1-3-1.37][7949-1-4-1.63][8135-1-0-1.93][8185-3-0-2.08]
[8269-3-2-2.98][8273-3-0-1.87][8543-3-0-4.31][8666-1-3-0.88][8672-0-0-3.65][8903-1-2-0.84][9001-2-4-1.25][9036-2-2-2.46][9281-3-4-0.69][9300-2-4-1.24]
[9571-0-4-0.76][9617-1-1-0.61][9644-2-2-1.22][9705-2-1-1.37][9801-0-3-1.57][9803-3-3-1.99][9865-3-0-3.19][9896-2-4-1.72][10314-1-1-1.72][10337-3-3-3.00]
[10403-0-4-2.01][10653-2-4-2.45][10704-2-2-1.58][10719-1-1-2.67][10727-1-4-1.49][10836-0-0-3.42][10969-2-3-1.97][11042-0-0-0.85][11088-1-1-2.52][11322-0-0-3.26]
[11398-2-4-1.61][11499-0-0-0.99][11502-3-3-1.07][11512-3-1-1.54][11608-1-1-2.84][11610-0-0-1.80][11692-0-0-1.51][11905-0-0-3.35][11993-1-2-2.01][12002-2-0-1.23]
[12052-0-0-2.78][12201-0-3-1.96][12235-2-1-1.14][12320-1-4-1.57][12377-2-4-3.51][12398-2-3-1.04][12503-1-2-2.03][12617-0-1-0.70][12685-3-2-0.74][12738-2-4-0.91]
[12742-2-2-1.55][12823-0-3-1.79][13110-1-1-0.57][13240-3-3-2.06][13253-1-1-2.46][13273-0-0-4.77][13634-1-1-1.77][13763-2-2-0.90][13905-3-0-0.73][14060-2-1-0.74]
[14065-3-3-1.72][14147-3-0-1.04][14595-2-2-2.21][14687-2-2-1.65][14788-2-2-0.65][14869-1-1-1.77][14872-3-4-1.12][14877-1-4-1.52][14927-0-3-2.74][15066-0-0-5.14]
[15175-1-2-1.13][15178-2-0-0.60][15375-3-3-1.17][15389-3-0-1.74][15568-2-4-1.60][15675-3-3-1.50][15869-1-2-1.15][16207-3-3-0.60][16236-0-2-0.12][16302-3-0-1.29]
[16331-2-2-2.09][16381-0-0-2.68][16488-1-1-2.56][16495-0-4-0.91][16650-0-0-4.05][16719-1-4-2.07][16801-0-0-3.86][16828-0-0-1.70][17137-3-0-1.59][17245-1-2-1.33]
[17278-3-4-2.22][17282-0-2-0.54][17311-2-2-2.07][17336-2-1-1.80][17608-3-0-3.74][17627-0-1-1.41][17877-3-1-2.18][17924-1-4-0.77][17984-3-3-2.46][18211-0-3-1.82]
[18276-3-0-3.59][18287-1-4-0.01][18394-0-0-2.39][18428-0-0-3.01][18442-0-3-1.57][18478-3-0-1.94][18607-0-2-1.04][18616-0-3-0.96][18663-0-0-0.84][18718-0-0-3.00]
[18766-2-2-2.03][18824-2-4-1.48][18890-3-3-0.75][18930-3-4-1.24][18938-3-0-2.03][19817-1-4-1.96][19839-0-4-1.83][19930-3-3-1.42][19944-0-4-1.84][20036-2-2-1.77]
[20101-3-3-1.81][20474-1-1-2.66][20547-3-4-1.20][20929-2-2-2.33][21245-1-2-2.04][21257-3-3-0.38][21293-1-1-2.71][21316-1-3-1.03][21384-1-1-2.00][21448-1-1-1.84]
[21483-0-0-2.21][21487-2-4-1.47][21714-0-2-0.62][21943-3-1-1.59][21947-0-0-2.04][21948-0-0-3.22][21965-2-2-1.48][21998-1-3-0.71][22025-0-3-0.57][22228-3-3-2.83]
[22446-1-1-2.73][22494-3-0-2.30][22757-0-0-3.65][22811-3-3-2.08][22976-3-4-1.01][22985-3-0-2.38][23014-0-3-1.71][23112-1-2-1.30][23144-3-0-2.76][23168-2-0-0.35]
[23219-0-0-0.60][23363-3-3-1.91][23470-0-4-1.30][23486-2-4-1.12][23497-0-0-3.55][23516-0-0-3.42][23690-1-1-0.69][23921-2-4-1.22][23936-1-3-1.52][24040-3-4-1.29]
[24111-1-4-3.20][24182-0-0-4.03][24238-3-3-1.59][24290-2-0-2.28][24345-0-0-1.80][24364-1-3-0.70][24427-3-0-1.74][24477-2-4-1.32][24495-2-1-1.60][24893-2-1-1.82]
[25012-1-4-1.80][25121-2-4-1.63][25165-3-3-1.73][25183-0-3-0.67][25297-3-3-1.28][25398-0-3-0.98][25574-2-3-0.46][25644-1-1-2.39][25718-1-1-1.45][25774-2-2-0.74]
[26032-3-3-2.47][26051-3-0-2.59][26120-0-4-1.07][26321-1-1-1.92][26732-1-4-0.81][26784-3-0-3.30][26827-3-3-1.02][26833-0-0-3.24][26838-2-1-0.84][26860-1-4-1.26]
[26948-0-0-2.24][27049-3-3-1.56][27098-1-3-0.50][27526-0-0-1.80][27639-3-3-2.23][27698-3-0-1.56][27772-0-0-3.21][27890-1-4-1.02][28040-0-4-1.19][28503-2-2-1.87]
[28577-1-2-1.39][28959-0-0-3.95][29198-3-3-1.35][29777-0-0-4.27][29877-2-2-1.27][30035-1-1-1.11][30098-0-3-2.04][30326-1-2-1.11][30572-2-3-1.03][30716-0-4-1.84]
[30806-2-3-0.83][30906-1-4-2.73][31007-0-0-1.18][31181-3-3-0.92][31238-0-3-1.45][31347-0-3-1.64][31422-2-1-0.74][31429-3-0-0.41][31431-0-0-1.21][31432-1-1-2.39]
[31477-0-0-3.51][31524-1-3-1.44][31597-1-2-1.32][31619-1-4-1.00][31701-0-0-1.82][31755-0-3-1.21][31854-3-3-1.97][32074-1-3-1.12][32078-3-3-1.05][32111-1-4-2.12]
[32127-1-2-2.63][32140-3-3-0.94][32263-2-4-1.65][32365-0-0-0.64][32411-2-0-3.22][32429-3-0-3.44][32473-3-0-0.99][32574-3-0-4.45][32584-0-0-1.24][32622-0-4-0.80]
[32858-3-0-1.63][32969-3-0-3.32][33016-2-2-3.83][33031-1-1-0.93][33035-2-2-1.39][33133-2-1-1.85][33173-2-1-0.62][33175-3-2-1.87][33306-3-1-1.16][33309-2-2-0.63]
[33474-0-0-0.67][33478-2-0-1.31][33618-1-4-1.35][33712-0-0-1.31][33782-2-4-1.96][33914-3-3-1.81][34076-3-1-1.32][34112-2-2-1.67][34138-2-1-1.37][34239-1-1-1.80]
[34364-2-1-3.38][34617-1-4-0.76][34751-3-3-2.76][34783-2-4-3.36][35015-3-3-1.43][35018-1-2-1.84][35288-2-2-1.04][0-4-4-2.26][1-4-4-1.39][2-4-4-1.09]
[3-4-4-1.53][4-4-2-1.33][5-4-1-1.99][6-4-4-1.16][7-4-0-1.12][8-4-4-1.16][9-4-1-1.99][10-4-4-2.85][11-4-4-1.50][12-4-1-1.10]
[14-4-4-1.09][15-4-3-2.24][16-4-2-0.52][17-4-4-0.78][18-4-4-2.67][19-4-3-2.20][20-4-2-0.49][21-4-2-1.32][22-4-4-2.43][23-4-1-1.29]
[24-4-2-1.10][25-4-3-1.53][26-4-3-0.84][27-4-0-1.57][28-4-4-3.18][29-4-2-1.41][30-4-4-1.26][31-4-4-1.14][32-4-4-2.06][33-4-3-1.48]
[34-4-4-0.71][35-4-3-1.60][37-4-3-0.48][39-4-0-2.52][40-4-4-0.89][41-4-3-0.40][42-4-4-0.79][43-4-4-0.63][45-4-4-1.66][46-4-2-2.24]
[47-4-4-3.17][48-4-4-2.34][51-4-4-3.53][52-4-2-1.45][53-4-2-1.35][54-4-4-0.66][55-4-4-1.30][56-4-4-1.02][57-4-3-2.30][58-4-1-2.44]
[59-4-4-1.57][60-4-4-1.17][61-4-4-1.94][62-4-4-1.07][63-4-4-1.75][64-4-4-0.82][65-4-4-3.06][66-4-4-3.79][67-4-4-0.72][68-4-1-1.77]
[69-4-0-1.14][70-4-4-2.89][72-4-4-1.86][73-4-2-0.93][74-4-1-0.87][75-4-4-0.94][77-4-4-2.43][78-4-1-1.15][79-4-4-3.10][80-4-4-2.25]
[81-4-4-2.27][82-4-2-1.26][83-4-1-2.44][84-4-4-1.81][85-4-4-3.26][86-4-4-2.38][87-4-4-3.78][88-4-4-2.72][89-4-2-0.58][90-4-4-0.70]
[91-4-4-0.81][92-4-3-0.77][93-4-4-2.63][94-4-4-2.74][95-4-4-1.96][96-4-4-2.96][97-4-4-2.96][98-4-2-1.95][99-4-4-2.91][100-4-1-2.02]
[101-4-4-3.08][102-4-4-1.63][103-4-3-1.06][104-4-4-1.95][105-4-4-2.79][106-4-1-1.82][107-4-4-2.79][108-4-4-1.43][109-4-1-0.79][110-4-4-1.62]
[111-4-0-3.29][112-4-4-1.50][113-4-3-1.00][114-4-0-0.64][115-4-4-1.03][116-4-4-1.29][117-4-4-1.46][119-4-4-1.62][121-4-4-1.37][122-4-3-1.04]
[124-4-3-1.01][125-4-4-1.97][126-4-4-1.32][127-4-4-1.75][128-4-4-0.82][129-4-4-1.30][130-4-4-1.64][131-4-4-0.94][132-4-1-1.17][133-4-0-3.71]
[135-4-2-1.74][136-4-4-0.78][137-4-4-1.59][138-4-4-1.26][139-4-0-0.79][140-4-2-1.05][141-4-3-1.70][142-4-4-3.42][143-4-4-0.85][144-4-4-1.99]
[145-4-4-2.67][148-4-0-1.72][149-4-4-1.19][150-4-4-1.28][151-4-2-1.48][152-4-1-1.47][153-4-4-1.76][154-4-2-1.07][155-4-4-1.77][156-4-3-1.26]
[157-4-2-1.03][158-4-4-0.54][160-4-4-1.72][161-4-3-1.18][162-4-4-0.97][164-4-4-1.19][165-4-4-0.61][167-4-0-1.14][168-4-4-2.60][170-4-4-1.05]
[171-4-4-0.97][172-4-4-2.78][173-4-4-1.00][174-4-0-0.51][175-4-4-1.60][177-4-0-1.47][178-4-4-2.07][179-4-0-1.18][180-4-4-3.10][181-4-4-0.79]
[182-4-1-1.39][183-4-4-1.86][184-4-4-1.91][186-4-0-0.57][187-4-4-2.33][188-4-4-1.07][189-4-4-2.04][190-4-4-1.22][191-4-4-2.37][192-4-4-1.23]
[193-4-2-2.72][194-4-1-1.20][195-4-0-1.89][196-4-1-1.37][197-4-4-2.35][198-4-4-3.45][199-4-4-1.05]
---------------------------
I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 1.121 | Acc: 50.875% | Wgt Acc: 54.721%
	I - Batch: 100 | Loss: 1.131 | Acc: 50.562% | Wgt Acc: 53.914%
	I - Batch: 150 | Loss: 1.129 | Acc: 51.417% | Wgt Acc: 54.662%
	I - Batch: 200 | Loss: 1.137 | Acc: 51.000% | Wgt Acc: 54.142%
I - num batch: 222
I - Train -- Loss: 1.128 | Acc: 51.311% | Wgt Acc: 54.483% | LR: 2.500000e-03 | Dur: 135.82s
I - Confusion Matrix: [row->prediction - col->label]
[[437.  22.  50. 161. 139.]
 [ 21. 325. 103.  39. 161.]
 [ 60. 153. 460.  62. 259.]
 [140.  35.  45. 257. 100.]
 [ 39.  43.  76.  19. 341.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.394 | Acc: 42.406% | Wgt Acc: 42.347% | Dur: 17.08s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  8.  8. 44. 19.]
 [ 1. 16.  6.  0. 13.]
 [ 9. 42. 48. 24. 73.]
 [ 5.  1.  1. 14.  3.]
 [ 8. 11. 12.  4. 72.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 1.032 | Acc: 54.375% | Wgt Acc: 57.036%
	I - Batch: 100 | Loss: 1.067 | Acc: 53.500% | Wgt Acc: 56.449%
	I - Batch: 150 | Loss: 1.076 | Acc: 52.792% | Wgt Acc: 55.864%
	I - Batch: 200 | Loss: 1.081 | Acc: 52.750% | Wgt Acc: 55.807%
I - num batch: 222
I - Train -- Loss: 1.082 | Acc: 53.031% | Wgt Acc: 56.015% | LR: 2.500000e-03 | Dur: 137.04s
I - Confusion Matrix: [row->prediction - col->label]
[[467.  26.  45. 171. 139.]
 [ 24. 350. 107.  43. 145.]
 [ 42. 136. 462.  67. 270.]
 [123.  33.  45. 237.  81.]
 [ 41.  33.  75.  20. 365.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.406 | Acc: 43.590% | Wgt Acc: 44.812% | Dur: 14.66s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  9. 10. 32. 30.]
 [ 1. 19.  2.  3. 13.]
 [ 3. 28. 36.  8. 34.]
 [24. 11. 13. 42. 34.]
 [ 5. 11. 14.  1. 69.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 1.061 | Acc: 54.250% | Wgt Acc: 57.383%
	I - Batch: 100 | Loss: 1.062 | Acc: 53.938% | Wgt Acc: 56.784%
	I - Batch: 150 | Loss: 1.071 | Acc: 54.208% | Wgt Acc: 57.055%
	I - Batch: 200 | Loss: 1.083 | Acc: 52.844% | Wgt Acc: 55.698%
I - num batch: 222
I - Train -- Loss: 1.091 | Acc: 52.551% | Wgt Acc: 55.407% | LR: 2.500000e-03 | Dur: 135.30s
I - Confusion Matrix: [row->prediction - col->label]
[[449.  18.  37. 168. 134.]
 [ 28. 331. 104.  37. 160.]
 [ 61. 153. 468.  59. 256.]
 [122.  28.  49. 248.  82.]
 [ 37.  48.  76.  26. 368.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.391 | Acc: 44.181% | Wgt Acc: 44.447% | Dur: 14.87s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  7. 12. 49. 38.]
 [ 2. 30.  7.  3. 17.]
 [ 4. 20. 39.  8. 42.]
 [ 7.  2.  3. 17.  9.]
 [11. 19. 14.  9. 74.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 1.091 | Acc: 54.375% | Wgt Acc: 57.026%
	I - Batch: 100 | Loss: 1.062 | Acc: 55.188% | Wgt Acc: 57.568%
	I - Batch: 150 | Loss: 1.089 | Acc: 53.375% | Wgt Acc: 56.065%
	I - Batch: 200 | Loss: 1.087 | Acc: 53.031% | Wgt Acc: 55.795%
I - num batch: 222
I - Train -- Loss: 1.098 | Acc: 52.608% | Wgt Acc: 55.358% | LR: 2.500000e-03 | Dur: 134.75s
I - Confusion Matrix: [row->prediction - col->label]
[[460.  20.  40. 167. 135.]
 [ 36. 348. 103.  48. 141.]
 [ 51. 138. 453.  64. 264.]
 [118.  34.  59. 232.  87.]
 [ 32.  38.  79.  27. 373.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.491 | Acc: 43.393% | Wgt Acc: 42.681% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[73. 13. 15. 53. 59.]
 [ 1. 11.  1.  0.  5.]
 [ 2. 14. 29.  2. 22.]
 [ 8. 17. 12. 27. 14.]
 [ 4. 23. 18.  4. 80.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 1.056 | Acc: 56.375% | Wgt Acc: 59.007%
	I - Batch: 100 | Loss: 1.068 | Acc: 53.500% | Wgt Acc: 56.517%
	I - Batch: 150 | Loss: 1.072 | Acc: 53.125% | Wgt Acc: 56.290%
	I - Batch: 200 | Loss: 1.079 | Acc: 52.781% | Wgt Acc: 55.731%
I - num batch: 222
I - Train -- Loss: 1.073 | Acc: 52.749% | Wgt Acc: 55.848% | LR: 2.500000e-03 | Dur: 134.14s
I - Confusion Matrix: [row->prediction - col->label]
[[464.  22.  37. 178. 148.]
 [ 28. 339. 105.  36. 131.]
 [ 46. 140. 464.  53. 280.]
 [123.  35.  54. 247.  84.]
 [ 36.  42.  74.  24. 357.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.354 | Acc: 46.943% | Wgt Acc: 48.845% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  8. 12. 33. 37.]
 [ 3. 33. 11.  1. 26.]
 [ 2. 24. 38. 10. 29.]
 [20.  5.  8. 39. 18.]
 [ 5.  8.  6.  3. 70.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 1.041 | Acc: 53.000% | Wgt Acc: 55.925%
	I - Batch: 100 | Loss: 1.065 | Acc: 52.062% | Wgt Acc: 55.216%
	I - Batch: 150 | Loss: 1.072 | Acc: 52.917% | Wgt Acc: 55.609%
	I - Batch: 200 | Loss: 1.070 | Acc: 53.250% | Wgt Acc: 55.844%
I - num batch: 222
I - Train -- Loss: 1.069 | Acc: 53.341% | Wgt Acc: 56.067% | LR: 2.500000e-03 | Dur: 135.35s
I - Confusion Matrix: [row->prediction - col->label]
[[458.  23.  44. 165. 121.]
 [ 33. 337. 100.  46. 140.]
 [ 49. 139. 469.  60. 270.]
 [119.  35.  53. 246.  87.]
 [ 38.  44.  68.  21. 382.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.370 | Acc: 45.168% | Wgt Acc: 42.002% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 41.   1.   2.  16.   8.]
 [  8.  28.   8.  16.  12.]
 [ 13.  30.  40.  22.  52.]
 [ 11.   1.   3.  19.   7.]
 [ 15.  18.  22.  13. 101.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 1.015 | Acc: 55.750% | Wgt Acc: 58.996%
	I - Batch: 100 | Loss: 1.060 | Acc: 54.062% | Wgt Acc: 57.271%
	I - Batch: 150 | Loss: 1.069 | Acc: 54.083% | Wgt Acc: 56.986%
	I - Batch: 200 | Loss: 1.058 | Acc: 54.219% | Wgt Acc: 57.018%
I - num batch: 222
I - Train -- Loss: 1.054 | Acc: 54.497% | Wgt Acc: 57.328% | LR: 2.500000e-03 | Dur: 135.21s
I - Confusion Matrix: [row->prediction - col->label]
[[450.  23.  36. 161. 138.]
 [ 32. 340.  99.  43. 142.]
 [ 50. 138. 493.  50. 256.]
 [126.  27.  36. 261.  75.]
 [ 39.  50.  70.  23. 389.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.465 | Acc: 40.434% | Wgt Acc: 44.395% | Dur: 14.48s
I - Confusion Matrix: [row->prediction - col->label]
[[34.  1.  2. 11.  4.]
 [ 3. 31. 12.  7. 26.]
 [23. 36. 51. 23. 92.]
 [24.  2.  5. 44. 13.]
 [ 4.  8.  5.  1. 45.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.991 | Acc: 58.000% | Wgt Acc: 61.469%
	I - Batch: 100 | Loss: 0.985 | Acc: 57.812% | Wgt Acc: 60.838%
	I - Batch: 150 | Loss: 0.982 | Acc: 57.917% | Wgt Acc: 61.043%
	I - Batch: 200 | Loss: 0.974 | Acc: 58.094% | Wgt Acc: 61.053%
I - num batch: 222
I - Train -- Loss: 0.973 | Acc: 58.275% | Wgt Acc: 61.217% | LR: 1.250000e-03 | Dur: 136.01s
I - Confusion Matrix: [row->prediction - col->label]
[[478.  17.  34. 143. 126.]
 [ 26. 371.  88.  33. 119.]
 [ 45. 118. 511.  55. 239.]
 [111.  32.  41. 285.  94.]
 [ 37.  40.  60.  22. 422.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.269 | Acc: 48.915% | Wgt Acc: 49.284% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[50.  2.  4. 24. 15.]
 [ 2. 37. 16.  4. 13.]
 [ 3. 24. 28.  4. 43.]
 [25.  3. 16. 47. 23.]
 [ 8. 12. 11.  7. 86.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 0.904 | Acc: 60.250% | Wgt Acc: 64.318%
	I - Batch: 100 | Loss: 0.936 | Acc: 58.875% | Wgt Acc: 63.083%
	I - Batch: 150 | Loss: 0.963 | Acc: 57.833% | Wgt Acc: 61.640%
	I - Batch: 200 | Loss: 0.970 | Acc: 58.125% | Wgt Acc: 61.842%
I - num batch: 222
I - Train -- Loss: 0.970 | Acc: 58.077% | Wgt Acc: 61.689% | LR: 1.250000e-03 | Dur: 135.00s
I - Confusion Matrix: [row->prediction - col->label]
[[495.  13.  32. 144. 145.]
 [ 27. 374.  85.  36. 133.]
 [ 32. 126. 517.  46. 246.]
 [107.  28.  37. 289.  91.]
 [ 36.  37.  63.  23. 385.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.275 | Acc: 47.732% | Wgt Acc: 50.632% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[48.  4.  6. 23. 21.]
 [ 2. 36. 12.  5. 26.]
 [ 4. 20. 41.  4. 34.]
 [29.  8.  7. 51. 33.]
 [ 5. 10.  9.  3. 66.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.973 | Acc: 58.375% | Wgt Acc: 61.268%
	I - Batch: 100 | Loss: 0.939 | Acc: 60.312% | Wgt Acc: 63.620%
	I - Batch: 150 | Loss: 0.935 | Acc: 60.208% | Wgt Acc: 63.154%
	I - Batch: 200 | Loss: 0.938 | Acc: 60.250% | Wgt Acc: 63.151%
I - num batch: 222
I - Train -- Loss: 0.941 | Acc: 59.938% | Wgt Acc: 62.866% | LR: 1.250000e-03 | Dur: 133.35s
I - Confusion Matrix: [row->prediction - col->label]
[[489.  15.  28. 131. 126.]
 [ 21. 379.  80.  31. 116.]
 [ 32. 106. 512.  45. 235.]
 [115.  33.  40. 305.  82.]
 [ 40.  45.  74.  26. 441.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.271 | Acc: 48.521% | Wgt Acc: 50.538% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[43.  1.  1. 11. 11.]
 [ 3. 33.  9.  6. 16.]
 [ 4. 27. 42.  9. 53.]
 [30.  3. 14. 54. 26.]
 [ 8. 14.  9.  6. 74.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 0.902 | Acc: 62.375% | Wgt Acc: 65.063%
	I - Batch: 100 | Loss: 0.937 | Acc: 61.062% | Wgt Acc: 64.023%
	I - Batch: 150 | Loss: 0.957 | Acc: 60.125% | Wgt Acc: 63.209%
	I - Batch: 200 | Loss: 0.955 | Acc: 59.594% | Wgt Acc: 62.508%
I - num batch: 222
I - Train -- Loss: 0.949 | Acc: 59.882% | Wgt Acc: 62.856% | LR: 1.250000e-03 | Dur: 135.51s
I - Confusion Matrix: [row->prediction - col->label]
[[493.  20.  32. 133. 145.]
 [ 23. 375.  70.  38. 120.]
 [ 40. 100. 526.  51. 221.]
 [108.  31.  47. 294.  78.]
 [ 33.  52.  59.  22. 436.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.352 | Acc: 48.718% | Wgt Acc: 50.454% | Dur: 14.60s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  6.  8. 41. 32.]
 [ 2. 44. 23.  6. 40.]
 [ 3. 17. 30.  3. 21.]
 [14.  4.  7. 33. 13.]
 [ 3.  7.  7.  3. 74.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.934 | Acc: 60.750% | Wgt Acc: 65.137%
	I - Batch: 100 | Loss: 0.945 | Acc: 59.312% | Wgt Acc: 62.930%
	I - Batch: 150 | Loss: 0.959 | Acc: 59.208% | Wgt Acc: 62.461%
	I - Batch: 200 | Loss: 0.950 | Acc: 59.312% | Wgt Acc: 62.890%
I - num batch: 222
I - Train -- Loss: 0.953 | Acc: 59.261% | Wgt Acc: 62.707% | LR: 1.250000e-03 | Dur: 136.50s
I - Confusion Matrix: [row->prediction - col->label]
[[492.  25.  33. 131. 135.]
 [ 25. 386.  79.  43. 136.]
 [ 27.  99. 524.  46. 240.]
 [108.  32.  36. 294.  83.]
 [ 45.  36.  62.  24. 406.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.258 | Acc: 50.888% | Wgt Acc: 51.844% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  3.  6. 20. 16.]
 [ 1. 25. 10.  1. 11.]
 [ 4. 32. 37.  8. 41.]
 [21.  5.  9. 54. 27.]
 [ 5. 13. 13.  3. 85.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.897 | Acc: 61.500% | Wgt Acc: 65.394%
	I - Batch: 100 | Loss: 0.888 | Acc: 62.875% | Wgt Acc: 66.211%
	I - Batch: 150 | Loss: 0.877 | Acc: 63.167% | Wgt Acc: 66.633%
	I - Batch: 200 | Loss: 0.863 | Acc: 63.688% | Wgt Acc: 67.158%
I - num batch: 222
I - Train -- Loss: 0.863 | Acc: 63.659% | Wgt Acc: 67.321% | LR: 6.250000e-04 | Dur: 135.89s
I - Confusion Matrix: [row->prediction - col->label]
[[513.  17.  20. 120. 123.]
 [ 18. 412.  54.  27. 115.]
 [ 29.  81. 557.  30. 235.]
 [ 95.  30.  38. 335.  86.]
 [ 42.  38.  65.  26. 441.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.222 | Acc: 52.071% | Wgt Acc: 53.558% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  2.  5. 20. 16.]
 [ 2. 39. 14.  8. 22.]
 [ 5. 25. 40.  7. 43.]
 [18.  3.  5. 47. 16.]
 [ 8.  9. 11.  4. 83.]]

I - Local maximum validation set accuracy:  52.07

I - Validation set results: 
[14-1-2-1.62][50-3-1-0.89][124-2-4-0.54][127-0-0-3.96][443-2-2-1.29][567-0-0-1.98][573-1-1-1.51][615-0-3-2.76][695-1-2-1.41][722-3-3-3.28]
[826-0-0-2.98][878-0-0-3.01][1103-0-4-0.74][1212-3-4-0.29][1368-0-0-4.52][2181-2-2-1.95][2476-2-1-0.80][2721-2-2-2.35][2818-1-1-0.62][2886-2-2-1.67]
[3231-2-2-4.06][3333-2-1-3.57][3482-2-2-3.93][3536-3-3-2.26][3625-1-1-4.36][3909-0-0-1.01][4035-0-3-2.59][4140-0-0-2.40][4214-1-3-1.60][4346-1-4-1.58]
[4581-2-1-1.91][4708-3-2-1.48][4838-3-3-0.87][4845-1-1-1.12][4868-0-0-2.92][4939-0-4-1.61][4984-2-2-0.28][5078-1-2-0.99][5396-0-0-5.71][5479-1-1-4.56]
[5717-0-0-1.98][5843-1-1-3.14][5949-3-3-2.18][5987-2-4-2.93][6014-3-2-1.55][6033-3-0-2.13][6313-0-3-2.96][6421-3-3-2.27][6500-1-1-1.54][6583-3-3-0.99]
[6683-3-0-1.01][6825-2-3-1.35][6998-3-2-0.39][7049-3-3-1.55][7517-1-1-2.46][7521-1-1-2.62][7528-1-2-1.37][7949-1-2-2.02][8135-1-0-2.37][8185-3-0-3.61]
[8269-3-4-1.72][8273-3-3-2.14][8543-3-0-4.73][8666-1-1-2.72][8672-0-0-4.55][8903-1-2-0.49][9001-2-2-2.64][9036-2-2-3.73][9281-3-0-0.11][9300-2-4-1.87]
[9571-0-3-1.01][9617-1-4-0.57][9644-2-2-2.41][9705-2-4-0.99][9801-0-0-1.58][9803-3-3-3.16][9865-3-3-3.74][9896-2-2-1.90][10314-1-2-0.76][10337-3-3-4.39]
[10403-0-4-1.21][10653-2-1-2.84][10704-2-2-2.38][10719-1-1-3.33][10727-1-2-2.14][10836-0-0-6.08][10969-2-3-2.34][11042-0-3-0.92][11088-1-1-3.61][11322-0-0-4.55]
[11398-2-2-2.15][11499-0-0-2.33][11502-3-3-2.98][11512-3-1-1.55][11608-1-1-2.80][11610-0-0-2.45][11692-0-0-2.22][11905-0-0-2.60][11993-1-2-1.47][12002-2-0-1.91]
[12052-0-0-2.75][12201-0-3-2.30][12235-2-4-1.40][12320-1-4-0.83][12377-2-4-1.75][12398-2-3-1.95][12503-1-4-2.89][12617-0-1-1.67][12685-3-1-1.19][12738-2-2-0.95]
[12742-2-2-2.70][12823-0-3-3.15][13110-1-1-1.81][13240-3-3-1.93][13253-1-1-1.97][13273-0-0-6.34][13634-1-2-1.39][13763-2-2-1.97][13905-3-3-0.34][14060-2-1-2.09]
[14065-3-3-1.80][14147-3-3-1.16][14595-2-2-3.15][14687-2-2-1.49][14788-2-2-1.50][14869-1-1-1.97][14872-3-0-0.81][14877-1-1-2.04][14927-0-3-3.16][15066-0-0-6.05]
[15175-1-2-0.82][15178-2-3-0.71][15375-3-1-1.32][15389-3-3-2.60][15568-2-1-1.54][15675-3-3-1.54][15869-1-2-2.61][16207-3-3-1.08][16236-0-0-0.88][16302-3-0-2.26]
[16331-2-2-4.55][16381-0-4-0.68][16488-1-1-1.92][16495-0-0-1.66][16650-0-0-4.79][16719-1-2-0.99][16801-0-0-5.20][16828-0-0-2.48][17137-3-3-1.56][17245-1-1-1.55]
[17278-3-3-0.85][17282-0-2-0.76][17311-2-2-2.70][17336-2-2-1.25][17608-3-3-4.44][17627-0-2-1.19][17877-3-1-1.74][17924-1-4-0.61][17984-3-0-3.59][18211-0-3-2.81]
[18276-3-0-3.10][18287-1-1-0.78][18394-0-0-3.45][18428-0-0-3.78][18442-0-3-2.02][18478-3-0-2.45][18607-0-0-0.50][18616-0-3-0.68][18663-0-0-1.55][18718-0-0-4.84]
[18766-2-1-2.15][18824-2-4-2.07][18890-3-3-0.86][18930-3-0-0.75][18938-3-0-0.45][19817-1-2-2.03][19839-0-2-1.78][19930-3-3-1.95][19944-0-4-2.50][20036-2-2-4.32]
[20101-3-3-1.53][20474-1-1-2.67][20547-3-0-1.87][20929-2-2-3.36][21245-1-2-2.28][21257-3-0-0.58][21293-1-1-3.86][21316-1-1-3.91][21384-1-4-1.97][21448-1-2-2.06]
[21483-0-0-4.33][21487-2-2-2.59][21714-0-0-0.85][21943-3-2-1.69][21947-0-0-1.70][21948-0-0-4.32][21965-2-2-3.98][21998-1-1-1.71][22025-0-2-1.05][22228-3-3-3.88]
[22446-1-1-2.61][22494-3-3-1.75][22757-0-0-3.27][22811-3-3-1.97][22976-3-1-1.01][22985-3-3-2.52][23014-0-3-2.90][23112-1-1-3.16][23144-3-3-2.53][23168-2-0-1.13]
[23219-0-3-0.83][23363-3-3-3.19][23470-0-0-1.09][23486-2-2-1.05][23497-0-3-4.57][23516-0-0-5.06][23690-1-4-1.34][23921-2-2-1.67][23936-1-0-0.66][24040-3-4-1.05]
[24111-1-4-2.19][24182-0-0-5.29][24238-3-3-1.13][24290-2-0-1.35][24345-0-4-1.93][24364-1-2-1.38][24427-3-3-2.62][24477-2-2-3.02][24495-2-1-1.51][24893-2-1-1.97]
[25012-1-2-1.21][25121-2-2-2.13][25165-3-0-1.38][25183-0-0-1.88][25297-3-3-1.97][25398-0-0-2.15][25574-2-2-1.96][25644-1-1-2.80][25718-1-1-1.54][25774-2-2-1.54]
[26032-3-3-1.83][26051-3-3-3.27][26120-0-0-1.38][26321-1-1-2.35][26732-1-1-0.90][26784-3-3-3.95][26827-3-3-1.29][26833-0-0-3.58][26838-2-2-0.43][26860-1-4-1.22]
[26948-0-0-2.03][27049-3-0-2.92][27098-1-3-0.59][27526-0-0-2.27][27639-3-3-1.54][27698-3-3-2.59][27772-0-0-4.59][27890-1-1-1.46][28040-0-4-1.09][28503-2-2-4.02]
[28577-1-1-1.63][28959-0-0-4.92][29198-3-3-1.61][29777-0-0-4.96][29877-2-1-1.53][30035-1-2-2.22][30098-0-0-1.94][30326-1-1-4.15][30572-2-1-1.33][30716-0-4-1.70]
[30806-2-1-0.80][30906-1-1-2.18][31007-0-0-1.71][31181-3-3-1.39][31238-0-3-1.34][31347-0-0-2.85][31422-2-2-1.00][31429-3-1-0.88][31431-0-3-1.71][31432-1-1-3.73]
[31477-0-0-3.70][31524-1-2-1.09][31597-1-2-2.63][31619-1-2-2.48][31701-0-0-1.69][31755-0-0-1.52][31854-3-3-2.23][32074-1-3-2.01][32078-3-3-1.04][32111-1-1-2.38]
[32127-1-2-3.54][32140-3-3-1.98][32263-2-4-0.35][32365-0-0-1.74][32411-2-0-4.27][32429-3-0-3.85][32473-3-2-1.22][32574-3-0-4.99][32584-0-2-0.52][32622-0-1-1.20]
[32858-3-0-2.01][32969-3-0-3.35][33016-2-2-3.67][33031-1-1-0.72][33035-2-2-2.95][33133-2-1-1.65][33173-2-1-1.80][33175-3-4-1.50][33306-3-1-1.11][33309-2-3-0.68]
[33474-0-3-1.10][33478-2-0-1.04][33618-1-1-3.23][33712-0-3-2.15][33782-2-4-2.45][33914-3-3-3.13][34076-3-2-1.33][34112-2-2-3.55][34138-2-2-2.39][34239-1-1-1.84]
[34364-2-2-3.93][34617-1-2-1.78][34751-3-3-3.64][34783-2-4-1.77][35015-3-2-2.33][35018-1-2-2.59][35288-2-4-0.33][0-4-4-1.47][1-4-4-2.26][2-4-4-1.03]
[3-4-2-2.25][4-4-2-1.61][5-4-1-0.61][6-4-4-3.14][7-4-2-0.86][8-4-2-1.34][9-4-4-1.01][10-4-4-3.02][11-4-2-3.27][12-4-2-0.85]
[14-4-3-1.24][15-4-3-3.41][16-4-4-1.91][17-4-4-0.46][18-4-4-2.63][19-4-3-2.02][20-4-0-0.89][21-4-1-2.84][22-4-4-2.62][23-4-4-0.87]
[24-4-4-2.75][25-4-3-1.21][26-4-3-0.52][27-4-2-1.56][28-4-4-2.37][29-4-2-1.76][30-4-3-0.96][31-4-4-1.72][32-4-1-2.05][33-4-2-1.17]
[34-4-4-0.62][35-4-3-2.39][37-4-2-0.92][39-4-0-3.80][40-4-4-0.99][41-4-1-1.05][42-4-2-2.23][43-4-2-1.50][45-4-3-0.14][46-4-2-2.95]
[47-4-4-2.33][48-4-2-1.05][51-4-4-2.60][52-4-2-1.81][53-4-2-1.47][54-4-0-0.65][55-4-2-1.51][56-4-1-1.95][57-4-0-1.61][58-4-2-2.79]
[59-4-0-1.58][60-4-2-1.09][61-4-4-2.44][62-4-2-0.19][63-4-2-2.70][64-4-2-1.04][65-4-4-2.70][66-4-4-3.29][67-4-4-0.56][68-4-1-2.30]
[69-4-0-2.01][70-4-4-2.26][72-4-2-1.94][73-4-1-1.40][74-4-4-0.84][75-4-0-1.26][77-4-4-3.11][78-4-2-1.89][79-4-4-1.71][80-4-4-2.41]
[81-4-4-2.35][82-4-4-0.23][83-4-1-1.47][84-4-4-0.82][85-4-4-2.54][86-4-2-1.54][87-4-4-3.67][88-4-4-1.31][89-4-4-1.46][90-4-2-0.66]
[91-4-0-0.21][92-4-2-0.52][93-4-0-0.97][94-4-4-1.74][95-4-3-0.37][96-4-4-2.93][97-4-4-2.49][98-4-1-1.32][99-4-4-1.26][100-4-2-1.32]
[101-4-4-2.94][102-4-2-0.92][103-4-3-1.60][104-4-4-2.41][105-4-4-2.92][106-4-1-3.08][107-4-1-2.41][108-4-4-0.86][109-4-4-0.91][110-4-1-1.03]
[111-4-0-4.01][112-4-0-0.91][113-4-3-0.62][114-4-3-1.64][115-4-4-1.27][116-4-4-1.21][117-4-4-2.16][119-4-2-2.09][121-4-4-1.14][122-4-4-1.59]
[124-4-1-1.29][125-4-4-3.57][126-4-4-1.48][127-4-2-2.76][128-4-1-0.41][129-4-3-1.10][130-4-2-2.12][131-4-4-1.75][132-4-1-0.70][133-4-4-3.20]
[135-4-2-2.03][136-4-1-2.30][137-4-4-0.65][138-4-4-1.21][139-4-4-1.15][140-4-2-1.66][141-4-3-1.89][142-4-4-3.75][143-4-4-2.04][144-4-4-3.76]
[145-4-4-2.18][148-4-0-2.60][149-4-2-1.28][150-4-4-2.27][151-4-2-1.30][152-4-1-1.45][153-4-4-2.04][154-4-2-1.85][155-4-4-1.36][156-4-3-1.79]
[157-4-2-1.55][158-4-4-1.46][160-4-4-1.44][161-4-2-1.79][162-4-4-0.53][164-4-2-0.97][165-4-4-1.42][167-4-0-1.16][168-4-4-2.05][170-4-4-1.05]
[171-4-4-1.52][172-4-2-1.74][173-4-4-0.52][174-4-0-1.98][175-4-4-1.57][177-4-0-0.97][178-4-4-1.98][179-4-1-1.31][180-4-4-2.21][181-4-3-0.95]
[182-4-4-1.14][183-4-4-1.55][184-4-4-1.72][186-4-2-0.94][187-4-1-1.75][188-4-4-1.47][189-4-4-2.25][190-4-1-2.66][191-4-4-2.43][192-4-4-0.77]
[193-4-2-3.82][194-4-1-1.27][195-4-0-2.27][196-4-1-0.96][197-4-4-2.28][198-4-4-5.20][199-4-4-1.36]
---------------------------
I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.768 | Acc: 67.875% | Wgt Acc: 70.957%
	I - Batch: 100 | Loss: 0.816 | Acc: 65.625% | Wgt Acc: 69.155%
	I - Batch: 150 | Loss: 0.815 | Acc: 65.375% | Wgt Acc: 68.759%
	I - Batch: 200 | Loss: 0.817 | Acc: 64.812% | Wgt Acc: 68.497%
I - num batch: 208
I - Train -- Loss: 0.821 | Acc: 64.663% | Wgt Acc: 68.431% | LR: 6.250000e-04 | Dur: 126.56s
I - Confusion Matrix: [row->prediction - col->label]
[[516.  11.  16. 123.  94.]
 [ 16. 434.  74.  32. 101.]
 [ 36.  88. 560.  36. 204.]
 [101.  22.  35. 332.  72.]
 [ 28.  23.  49.  15. 310.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.298 | Acc: 48.323% | Wgt Acc: 52.743% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[52.  2.  4. 16. 22.]
 [ 1. 31.  9.  2. 20.]
 [ 6. 33. 51. 12. 62.]
 [26.  7.  9. 55. 20.]
 [ 3.  5.  2.  1. 56.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.802 | Acc: 65.875% | Wgt Acc: 69.644%
	I - Batch: 100 | Loss: 0.808 | Acc: 65.625% | Wgt Acc: 69.270%
	I - Batch: 150 | Loss: 0.810 | Acc: 65.917% | Wgt Acc: 69.435%
	I - Batch: 200 | Loss: 0.815 | Acc: 65.719% | Wgt Acc: 69.399%
I - num batch: 222
I - Train -- Loss: 0.818 | Acc: 65.858% | Wgt Acc: 69.501% | LR: 6.250000e-04 | Dur: 135.65s
I - Confusion Matrix: [row->prediction - col->label]
[[519.  10.  14. 111. 131.]
 [ 17. 428.  52.  27. 110.]
 [ 22.  90. 580.  33. 202.]
 [ 98.  16.  26. 345.  93.]
 [ 41.  34.  62.  22. 464.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.161 | Acc: 57.002% | Wgt Acc: 54.582% | Dur: 21.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   3.   4.  15.  11.]
 [  2.  36.   8.   3.  10.]
 [  8.  20.  40.  13.  35.]
 [ 11.   3.   5.  44.   5.]
 [ 17.  16.  18.  11. 119.]]

I - Local maximum validation set accuracy:  57.00

I - Validation set results: 
[14-1-2-1.72][50-3-1-1.13][124-2-2-2.70][127-0-0-4.57][443-2-2-1.89][567-0-2-0.89][573-1-2-2.23][615-0-0-2.22][695-1-2-1.58][722-3-0-2.67]
[826-0-0-1.92][878-0-0-2.02][1103-0-4-1.46][1212-3-4-1.33][1368-0-0-4.54][2181-2-3-1.60][2476-2-1-1.24][2721-2-2-2.25][2818-1-1-1.39][2886-2-2-1.38]
[3231-2-1-3.42][3333-2-1-2.58][3482-2-2-3.57][3536-3-3-2.45][3625-1-1-3.62][3909-0-0-1.45][4035-0-3-3.04][4140-0-0-1.22][4214-1-1-0.52][4346-1-4-3.09]
[4581-2-2-2.05][4708-3-2-1.96][4838-3-4-0.73][4845-1-2-0.90][4868-0-0-2.67][4939-0-2-1.10][4984-2-2-1.26][5078-1-4-1.19][5396-0-0-4.75][5479-1-1-2.69]
[5717-0-0-2.10][5843-1-1-2.23][5949-3-3-1.74][5987-2-4-3.13][6014-3-2-1.11][6033-3-4-1.01][6313-0-0-2.58][6421-3-3-1.93][6500-1-1-1.28][6583-3-2-1.10]
[6683-3-3-0.73][6825-2-3-1.41][6998-3-2-0.58][7049-3-3-0.54][7517-1-2-1.96][7521-1-1-2.52][7528-1-2-2.66][7949-1-2-2.68][8135-1-0-1.73][8185-3-0-2.77]
[8269-3-4-1.73][8273-3-3-0.93][8543-3-0-4.35][8666-1-1-2.25][8672-0-0-2.90][8903-1-4-0.89][9001-2-2-2.61][9036-2-2-4.90][9281-3-2-1.04][9300-2-2-3.69]
[9571-0-4-0.94][9617-1-1-2.65][9644-2-2-2.79][9705-2-4-1.32][9801-0-4-0.81][9803-3-3-2.33][9865-3-0-2.64][9896-2-4-1.66][10314-1-2-1.54][10337-3-3-3.56]
[10403-0-4-1.71][10653-2-4-1.60][10704-2-2-2.81][10719-1-1-1.99][10727-1-2-1.09][10836-0-0-4.78][10969-2-3-2.38][11042-0-4-1.04][11088-1-1-3.61][11322-0-0-2.80]
[11398-2-4-2.14][11499-0-3-1.93][11502-3-2-0.96][11512-3-3-1.03][11608-1-1-3.49][11610-0-3-2.26][11692-0-0-3.22][11905-0-0-1.52][11993-1-1-2.06][12002-2-0-2.67]
[12052-0-0-3.38][12201-0-3-2.26][12235-2-2-2.45][12320-1-4-1.77][12377-2-4-2.31][12398-2-3-1.42][12503-1-4-3.81][12617-0-1-1.57][12685-3-2-0.73][12738-2-4-1.14]
[12742-2-2-2.99][12823-0-3-1.83][13110-1-2-3.26][13240-3-3-1.91][13253-1-4-1.18][13273-0-0-4.09][13634-1-2-1.52][13763-2-2-2.19][13905-3-4-0.37][14060-2-1-2.50]
[14065-3-3-1.02][14147-3-3-0.84][14595-2-2-1.72][14687-2-2-2.89][14788-2-2-2.03][14869-1-1-2.34][14872-3-0-0.93][14877-1-4-1.62][14927-0-3-2.38][15066-0-0-5.91]
[15175-1-1-1.13][15178-2-4-2.08][15375-3-3-0.88][15389-3-0-0.99][15568-2-4-2.80][15675-3-3-2.05][15869-1-2-2.40][16207-3-0-0.36][16236-0-0-0.56][16302-3-0-1.70]
[16331-2-2-4.23][16381-0-4-1.06][16488-1-1-2.55][16495-0-0-1.54][16650-0-0-3.19][16719-1-4-1.79][16801-0-0-4.07][16828-0-0-2.73][17137-3-3-1.05][17245-1-4-1.30]
[17278-3-3--0.07][17282-0-2-2.19][17311-2-2-2.19][17336-2-2-1.56][17608-3-3-3.98][17627-0-2-0.66][17877-3-1-2.64][17924-1-4-1.00][17984-3-3-4.00][18211-0-3-2.17]
[18276-3-4-0.82][18287-1-1-0.62][18394-0-0-2.49][18428-0-0-3.27][18442-0-3-2.05][18478-3-2-0.65][18607-0-4-1.00][18616-0-4-1.17][18663-0-0-1.43][18718-0-0-3.05]
[18766-2-1-2.47][18824-2-2-1.28][18890-3-3-1.08][18930-3-4-1.59][18938-3-0-0.83][19817-1-2-2.07][19839-0-2-1.58][19930-3-1-1.05][19944-0-4-2.44][20036-2-2-3.88]
[20101-3-3-0.73][20474-1-1-2.98][20547-3-0-1.69][20929-2-2-5.06][21245-1-1-2.41][21257-3-4-0.48][21293-1-2-3.50][21316-1-1-2.26][21384-1-4-2.37][21448-1-1-3.18]
[21483-0-0-3.52][21487-2-4-2.28][21714-0-4-0.41][21943-3-2-2.84][21947-0-0-1.17][21948-0-0-4.48][21965-2-2-3.98][21998-1-1-1.50][22025-0-2-0.96][22228-3-3-2.79]
[22446-1-1-3.55][22494-3-3-1.24][22757-0-0-1.74][22811-3-3-2.82][22976-3-4-3.17][22985-3-3-1.80][23014-0-3-0.87][23112-1-1-3.03][23144-3-3-1.42][23168-2-4-1.56]
[23219-0-2-1.58][23363-3-3-1.70][23470-0-4-0.31][23486-2-2-2.12][23497-0-3-3.98][23516-0-0-3.20][23690-1-4-0.39][23921-2-2-2.15][23936-1-2-0.35][24040-3-4-1.13]
[24111-1-4-2.66][24182-0-0-3.58][24238-3-3-0.74][24290-2-0-2.07][24345-0-4-1.72][24364-1-1-0.81][24427-3-3-1.40][24477-2-2-3.51][24495-2-1-1.03][24893-2-2-3.34]
[25012-1-4-1.28][25121-2-4-2.95][25165-3-3-0.94][25183-0-0-1.83][25297-3-3-1.19][25398-0-0-2.19][25574-2-2-1.47][25644-1-1-2.76][25718-1-4-0.56][25774-2-2-1.65]
[26032-3-2-0.81][26051-3-3-1.89][26120-0-4-0.91][26321-1-1-1.73][26732-1-1-1.04][26784-3-3-2.49][26827-3-3-0.83][26833-0-0-3.32][26838-2-2-0.52][26860-1-4-2.18]
[26948-0-0-2.89][27049-3-0-2.63][27098-1-3-0.89][27526-0-0-2.71][27639-3-3-0.90][27698-3-3-2.89][27772-0-0-3.37][27890-1-1-1.72][28040-0-4-1.30][28503-2-2-3.22]
[28577-1-2-2.45][28959-0-0-4.56][29198-3-3-1.35][29777-0-0-4.70][29877-2-2-2.09][30035-1-2-2.68][30098-0-0-1.61][30326-1-1-4.88][30572-2-2-3.75][30716-0-4-2.69]
[30806-2-4-1.11][30906-1-1-2.64][31007-0-0-1.34][31181-3-0-1.20][31238-0-3-1.01][31347-0-0-1.79][31422-2-4-0.84][31429-3-2-0.71][31431-0-0-1.32][31432-1-1-3.94]
[31477-0-0-2.59][31524-1-3-1.30][31597-1-2-2.46][31619-1-0-0.92][31701-0-4-0.61][31755-0-0-1.04][31854-3-3-1.91][32074-1-3-1.37][32078-3-0-0.72][32111-1-1-1.90]
[32127-1-2-3.54][32140-3-3-1.31][32263-2-4-2.61][32365-0-0-2.05][32411-2-0-3.86][32429-3-0-3.83][32473-3-2-1.00][32574-3-3-3.97][32584-0-2-0.32][32622-0-1-1.18]
[32858-3-3-1.37][32969-3-3-2.98][33016-2-1-2.22][33031-1-0-0.37][33035-2-2-3.38][33133-2-1-1.79][33173-2-2-1.23][33175-3-4-2.37][33306-3-3-0.63][33309-2-3-0.40]
[33474-0-0-1.65][33478-2-0-0.31][33618-1-1-2.20][33712-0-4-1.07][33782-2-4-2.61][33914-3-3-0.77][34076-3-0-0.34][34112-2-4-0.67][34138-2-2-1.06][34239-1-1-1.01]
[34364-2-2-2.44][34617-1-2-3.19][34751-3-3-2.09][34783-2-4-2.90][35015-3-2-2.60][35018-1-1-1.99][35288-2-2-1.01][0-4-4-3.02][1-4-4-4.25][2-4-4-1.48]
[3-4-4-2.90][4-4-2-1.51][5-4-4-0.77][6-4-4-1.61][7-4-4-2.02][8-4-4-1.46][9-4-2-0.78][10-4-4-3.67][11-4-2-3.18][12-4-4-1.77]
[14-4-3-1.06][15-4-3-1.43][16-4-4-3.29][17-4-4-1.24][18-4-4-3.39][19-4-3-0.96][20-4-2-0.38][21-4-4-1.61][22-4-4-2.24][23-4-2-0.97]
[24-4-4-4.11][25-4-3-0.50][26-4-2-1.34][27-4-2-2.88][28-4-4-1.92][29-4-2-1.41][30-4-0-1.09][31-4-4-1.59][32-4-4-1.48][33-4-2-1.58]
[34-4-4-1.11][35-4-0-1.62][37-4-2-0.93][39-4-0-1.54][40-4-4-1.46][41-4-2-1.09][42-4-1-1.19][43-4-1-1.51][45-4-4-0.84][46-4-4-3.61]
[47-4-4-3.01][48-4-4-2.56][51-4-4-2.17][52-4-4-1.11][53-4-2-1.68][54-4-0-1.02][55-4-4-1.88][56-4-1-2.13][57-4-4-0.51][58-4-2-2.47]
[59-4-4-2.39][60-4-1-1.17][61-4-4-3.67][62-4-4-1.47][63-4-2-2.51][64-4-2-1.23][65-4-4-3.64][66-4-4-3.70][67-4-4-1.20][68-4-1-2.47]
[69-4-0-1.25][70-4-4-2.69][72-4-2-1.76][73-4-2-1.98][74-4-2-1.71][75-4-2-1.99][77-4-4-2.83][78-4-2-1.82][79-4-4-2.64][80-4-4-3.09]
[81-4-4-2.37][82-4-1-1.81][83-4-4-1.85][84-4-4-3.05][85-4-4-4.08][86-4-4-1.56][87-4-4-4.61][88-4-4-2.33][89-4-4-3.04][90-4-2-0.97]
[91-4-4-1.43][92-4-2-1.60][93-4-4-1.64][94-4-4-3.29][95-4-4-1.23][96-4-4-3.36][97-4-4-2.77][98-4-1-1.23][99-4-4-0.87][100-4-2-1.20]
[101-4-4-4.22][102-4-4-1.43][103-4-2-1.01][104-4-4-2.31][105-4-4-2.88][106-4-4-1.54][107-4-4-2.19][108-4-4-0.96][109-4-4-1.70][110-4-2-1.40]
[111-4-0-3.13][112-4-4-0.98][113-4-4-1.05][114-4-4-0.69][115-4-4-1.14][116-4-0-0.59][117-4-4-3.03][119-4-4-2.00][121-4-4-1.80][122-4-4-2.04]
[124-4-2-0.83][125-4-4-2.89][126-4-4-2.55][127-4-4-2.92][128-4-4-0.59][129-4-4-0.73][130-4-2-2.26][131-4-4-2.10][132-4-4-1.37][133-4-4-4.18]
[135-4-2-1.55][136-4-1-1.23][137-4-4-1.57][138-4-4-1.75][139-4-4-2.39][140-4-4-0.81][141-4-2-0.94][142-4-4-4.34][143-4-4-2.61][144-4-4-3.30]
[145-4-4-3.08][148-4-0-2.92][149-4-4-1.29][150-4-4-1.97][151-4-4-1.95][152-4-4-1.68][153-4-2-2.41][154-4-4-2.06][155-4-4-2.24][156-4-3-1.36]
[157-4-2-0.65][158-4-4-1.40][160-4-4-1.47][161-4-2-0.94][162-4-4-1.75][164-4-2-1.09][165-4-4-1.87][167-4-4-1.27][168-4-4-2.14][170-4-4-1.53]
[171-4-4-1.80][172-4-4-3.26][173-4-4-1.79][174-4-0-1.42][175-4-4-1.94][177-4-4-1.60][178-4-4-3.50][179-4-4-1.32][180-4-4-3.14][181-4-4-0.77]
[182-4-4-2.39][183-4-4-3.36][184-4-4-2.19][186-4-4-1.10][187-4-4-2.03][188-4-2-1.53][189-4-4-2.15][190-4-1-1.99][191-4-4-2.03][192-4-4-1.16]
[193-4-2-2.78][194-4-0-0.80][195-4-0-1.48][196-4-1-0.76][197-4-4-3.26][198-4-4-5.00][199-4-4-1.49]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.767 | Acc: 69.375% | Wgt Acc: 73.036%
	I - Batch: 100 | Loss: 0.804 | Acc: 67.750% | Wgt Acc: 70.899%
	I - Batch: 150 | Loss: 0.790 | Acc: 68.500% | Wgt Acc: 71.704%
	I - Batch: 200 | Loss: 0.791 | Acc: 67.781% | Wgt Acc: 71.123%
I - num batch: 222
I - Train -- Loss: 0.804 | Acc: 67.324% | Wgt Acc: 70.698% | LR: 6.250000e-04 | Dur: 135.64s
I - Confusion Matrix: [row->prediction - col->label]
[[536.   8.  32. 106. 107.]
 [ 16. 439.  43.  27. 109.]
 [ 29.  74. 562.  25. 217.]
 [ 82.  26.  32. 357.  73.]
 [ 34.  31.  65.  23. 494.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.176 | Acc: 55.030% | Wgt Acc: 56.013% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  5. 23. 23.]
 [ 1. 34.  9.  2. 15.]
 [ 3. 21. 37.  5. 35.]
 [14.  7. 11. 53. 15.]
 [ 7. 12. 13.  3. 92.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.803 | Acc: 65.375% | Wgt Acc: 69.241%
	I - Batch: 100 | Loss: 0.802 | Acc: 66.438% | Wgt Acc: 70.328%
	I - Batch: 150 | Loss: 0.785 | Acc: 67.417% | Wgt Acc: 71.161%
	I - Batch: 200 | Loss: 0.790 | Acc: 67.406% | Wgt Acc: 71.076%
I - num batch: 222
I - Train -- Loss: 0.796 | Acc: 67.212% | Wgt Acc: 70.948% | LR: 6.250000e-04 | Dur: 133.97s
I - Confusion Matrix: [row->prediction - col->label]
[[532.  11.  21. 107. 132.]
 [ 16. 450.  47.  21. 109.]
 [ 21.  66. 573.  31. 189.]
 [ 84.  19.  28. 355.  96.]
 [ 44.  32.  65.  24. 474.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.251 | Acc: 49.704% | Wgt Acc: 52.105% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[46.  1.  4. 18. 20.]
 [ 4. 40. 11.  7. 26.]
 [ 4. 23. 43. 10. 45.]
 [26.  6. 10. 50. 16.]
 [ 8.  8.  7.  1. 73.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.756 | Acc: 68.000% | Wgt Acc: 71.772%
	I - Batch: 100 | Loss: 0.758 | Acc: 67.625% | Wgt Acc: 71.472%
	I - Batch: 150 | Loss: 0.770 | Acc: 67.333% | Wgt Acc: 71.246%
	I - Batch: 200 | Loss: 0.767 | Acc: 67.500% | Wgt Acc: 71.199%
I - num batch: 222
I - Train -- Loss: 0.768 | Acc: 67.296% | Wgt Acc: 71.009% | LR: 6.250000e-04 | Dur: 136.17s
I - Confusion Matrix: [row->prediction - col->label]
[[526.  13.  19. 108. 119.]
 [ 13. 440.  46.  21. 123.]
 [ 25.  70. 585.  28. 201.]
 [ 92.  19.  30. 360.  81.]
 [ 41.  36.  54.  21. 476.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 55.621% | Wgt Acc: 55.741% | Dur: 14.79s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  6. 23. 19.]
 [ 0. 30. 11.  1. 18.]
 [ 1. 23. 37.  3. 25.]
 [13.  6.  9. 53. 19.]
 [11. 15. 12.  6. 99.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.726 | Acc: 70.625% | Wgt Acc: 75.054%
	I - Batch: 100 | Loss: 0.745 | Acc: 69.688% | Wgt Acc: 73.851%
	I - Batch: 150 | Loss: 0.759 | Acc: 69.250% | Wgt Acc: 73.023%
	I - Batch: 200 | Loss: 0.749 | Acc: 69.781% | Wgt Acc: 73.444%
I - num batch: 222
I - Train -- Loss: 0.750 | Acc: 69.524% | Wgt Acc: 73.244% | LR: 6.250000e-04 | Dur: 137.70s
I - Confusion Matrix: [row->prediction - col->label]
[[549.  10.  14.  91. 128.]
 [ 16. 459.  48.  23. 109.]
 [ 23.  58. 590.  28. 175.]
 [ 73.  19.  25. 370.  90.]
 [ 36.  32.  57.  26. 498.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.156 | Acc: 53.649% | Wgt Acc: 53.840% | Dur: 14.60s
I - Confusion Matrix: [row->prediction - col->label]
[[49.  1.  4. 16.  7.]
 [ 3. 32.  8.  4. 18.]
 [ 4. 31. 46. 12. 45.]
 [19.  2.  6. 50. 15.]
 [13. 12. 11.  4. 95.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.712 | Acc: 70.250% | Wgt Acc: 74.604%
	I - Batch: 100 | Loss: 0.747 | Acc: 70.000% | Wgt Acc: 73.860%
	I - Batch: 150 | Loss: 0.739 | Acc: 69.833% | Wgt Acc: 73.679%
	I - Batch: 200 | Loss: 0.729 | Acc: 70.031% | Wgt Acc: 73.759%
I - num batch: 222
I - Train -- Loss: 0.730 | Acc: 70.087% | Wgt Acc: 73.734% | LR: 6.250000e-04 | Dur: 135.55s
I - Confusion Matrix: [row->prediction - col->label]
[[550.  23.  19.  95.  94.]
 [ 14. 459.  44.  23. 109.]
 [ 22.  53. 592.  28. 194.]
 [ 77.  14.  26. 377.  95.]
 [ 34.  29.  53.  15. 508.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.194 | Acc: 53.649% | Wgt Acc: 53.328% | Dur: 14.60s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  4.  7. 21. 15.]
 [ 3. 44. 16. 13. 23.]
 [ 3. 14. 35.  5. 32.]
 [18.  2.  4. 39. 12.]
 [ 8. 14. 13.  8. 98.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.734 | Acc: 68.750% | Wgt Acc: 72.628%
	I - Batch: 100 | Loss: 0.739 | Acc: 69.000% | Wgt Acc: 72.674%
	I - Batch: 150 | Loss: 0.725 | Acc: 69.417% | Wgt Acc: 73.165%
	I - Batch: 200 | Loss: 0.715 | Acc: 69.812% | Wgt Acc: 73.454%
I - num batch: 222
I - Train -- Loss: 0.722 | Acc: 69.185% | Wgt Acc: 72.978% | LR: 6.250000e-04 | Dur: 136.92s
I - Confusion Matrix: [row->prediction - col->label]
[[540.  14.  15. 101. 110.]
 [ 14. 463.  42.  17.  98.]
 [ 20.  57. 593.  27. 204.]
 [ 82.  18.  23. 367.  97.]
 [ 41.  26.  61.  26. 491.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.145 | Acc: 52.860% | Wgt Acc: 52.931% | Dur: 22.63s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  3.  4. 25. 10.]
 [ 3. 37. 15.  6. 22.]
 [ 5. 25. 43. 15. 46.]
 [10.  5.  5. 37.  9.]
 [12.  8.  8.  3. 93.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.707 | Acc: 70.125% | Wgt Acc: 74.592%
	I - Batch: 100 | Loss: 0.687 | Acc: 71.500% | Wgt Acc: 75.729%
	I - Batch: 150 | Loss: 0.710 | Acc: 71.375% | Wgt Acc: 75.033%
	I - Batch: 200 | Loss: 0.702 | Acc: 71.344% | Wgt Acc: 74.791%
I - num batch: 222
I - Train -- Loss: 0.703 | Acc: 71.102% | Wgt Acc: 74.621% | LR: 6.250000e-04 | Dur: 134.27s
I - Confusion Matrix: [row->prediction - col->label]
[[556.  15.  16.  99.  98.]
 [ 13. 471.  31.  18.  81.]
 [ 22.  48. 606.  29. 195.]
 [ 77.  14.  28. 366. 103.]
 [ 29.  30.  53.  26. 523.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.135 | Acc: 55.819% | Wgt Acc: 55.501% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   4.  11.  10.]
 [  2.  36.  14.   3.  19.]
 [  4.  23.  41.  10.  33.]
 [ 22.   5.   4.  53.  15.]
 [ 10.  13.  12.   9. 103.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.625 | Acc: 76.625% | Wgt Acc: 79.337%
	I - Batch: 100 | Loss: 0.659 | Acc: 74.375% | Wgt Acc: 77.655%
	I - Batch: 150 | Loss: 0.660 | Acc: 74.208% | Wgt Acc: 77.412%
	I - Batch: 200 | Loss: 0.682 | Acc: 72.812% | Wgt Acc: 76.036%
I - num batch: 222
I - Train -- Loss: 0.686 | Acc: 72.427% | Wgt Acc: 75.714% | LR: 6.250000e-04 | Dur: 135.75s
I - Confusion Matrix: [row->prediction - col->label]
[[562.  15.  20.  85. 111.]
 [ 15. 474.  40.  15.  91.]
 [ 13.  47. 592.  26. 173.]
 [ 66.  13.  23. 390.  74.]
 [ 41.  29.  59.  22. 551.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.158 | Acc: 57.199% | Wgt Acc: 55.908% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   3.   2.  14.  12.]
 [  2.  31.   7.   1.   9.]
 [  5.  30.  47.   7.  34.]
 [ 22.   4.   7.  53.  13.]
 [ 12.  10.  12.  11. 112.]]

I - Local maximum validation set accuracy:  57.20

I - Validation set results: 
[14-1-2-2.39][50-3-4-0.92][124-2-2-2.60][127-0-0-4.51][443-2-2-2.86][567-0-0-1.62][573-1-1-1.71][615-0-0-2.85][695-1-2-1.00][722-3-0-3.24]
[826-0-0-2.01][878-0-0-2.64][1103-0-4-1.62][1212-3-4-0.77][1368-0-0-3.42][2181-2-3-1.91][2476-2-2-1.44][2721-2-2-3.82][2818-1-2-0.86][2886-2-2-1.64]
[3231-2-2-4.10][3333-2-1-3.10][3482-2-2-4.22][3536-3-3-2.76][3625-1-1-4.30][3909-0-0-2.16][4035-0-3-3.75][4140-0-0-1.63][4214-1-3-1.70][4346-1-4-2.03]
[4581-2-1-1.77][4708-3-2-1.14][4838-3-4-0.21][4845-1-2-1.83][4868-0-0-2.78][4939-0-2-0.93][4984-2-2-2.04][5078-1-2-0.76][5396-0-0-4.86][5479-1-1-4.17]
[5717-0-3-1.05][5843-1-1-2.79][5949-3-4-1.08][5987-2-4-4.10][6014-3-3-1.93][6033-3-0-1.42][6313-0-3-3.11][6421-3-3-3.17][6500-1-1-0.23][6583-3-3-1.71]
[6683-3-3-1.83][6825-2-3-2.48][6998-3-2-0.76][7049-3-3-0.74][7517-1-1-2.12][7521-1-1-1.25][7528-1-3-1.02][7949-1-2-2.82][8135-1-0-2.13][8185-3-0-2.86]
[8269-3-2-2.12][8273-3-3-1.99][8543-3-0-5.20][8666-1-1-4.97][8672-0-3-2.37][8903-1-2-1.80][9001-2-2-2.75][9036-2-2-4.75][9281-3-4-0.43][9300-2-2-4.36]
[9571-0-3-1.22][9617-1-1-0.55][9644-2-2-3.39][9705-2-4-1.20][9801-0-3-1.07][9803-3-3-3.15][9865-3-3-2.87][9896-2-2-2.48][10314-1-2-2.74][10337-3-3-4.16]
[10403-0-4-1.52][10653-2-4-0.60][10704-2-1-2.82][10719-1-1-4.06][10727-1-2-1.70][10836-0-0-6.11][10969-2-3-2.18][11042-0-4-1.49][11088-1-1-2.81][11322-0-0-4.78]
[11398-2-2-1.87][11499-0-0-3.48][11502-3-3-1.26][11512-3-3-1.19][11608-1-1-4.24][11610-0-3-1.83][11692-0-0-1.76][11905-0-0-2.32][11993-1-2-1.78][12002-2-2-0.73]
[12052-0-0-3.73][12201-0-3-2.92][12235-2-4-1.48][12320-1-4-2.51][12377-2-4-2.37][12398-2-3-2.40][12503-1-4-2.59][12617-0-1-2.45][12685-3-3-0.67][12738-2-2-1.35]
[12742-2-2-3.24][12823-0-3-2.48][13110-1-1-2.47][13240-3-3-2.85][13253-1-2-1.09][13273-0-0-5.64][13634-1-2-0.95][13763-2-2-0.79][13905-3-3-0.21][14060-2-1-2.73]
[14065-3-3-3.08][14147-3-3-0.98][14595-2-2-3.64][14687-2-2-3.07][14788-2-2-1.60][14869-1-1-2.39][14872-3-4-1.38][14877-1-1-2.17][14927-0-3-1.72][15066-0-0-7.07]
[15175-1-4-1.03][15178-2-4-1.93][15375-3-0-2.25][15389-3-0-2.29][15568-2-1-1.48][15675-3-3-3.12][15869-1-2-1.76][16207-3-3-0.88][16236-0-1-0.33][16302-3-0-2.09]
[16331-2-2-5.09][16381-0-3-1.22][16488-1-1-1.59][16495-0-0-1.43][16650-0-0-5.64][16719-1-2-1.35][16801-0-0-5.35][16828-0-0-2.45][17137-3-3-1.53][17245-1-2-2.11]
[17278-3-3-0.73][17282-0-2-2.77][17311-2-2-2.67][17336-2-2-2.49][17608-3-3-5.19][17627-0-2-1.41][17877-3-1-2.73][17924-1-4-0.78][17984-3-0-4.15][18211-0-3-2.39]
[18276-3-0-2.65][18287-1-0-0.48][18394-0-0-3.42][18428-0-4-0.74][18442-0-3-2.87][18478-3-0-2.34][18607-0-4-1.05][18616-0-4-1.50][18663-0-0-2.44][18718-0-0-4.25]
[18766-2-2-2.38][18824-2-4-2.50][18890-3-3-1.62][18930-3-4-1.39][18938-3-3-1.03][19817-1-2-1.20][19839-0-2-2.13][19930-3-3-1.90][19944-0-4-1.99][20036-2-2-5.44]
[20101-3-3-1.24][20474-1-1-3.20][20547-3-0-4.08][20929-2-2-5.54][21245-1-2-2.95][21257-3-2-0.99][21293-1-2-3.82][21316-1-1-3.79][21384-1-4-2.37][21448-1-1-2.95]
[21483-0-0-3.68][21487-2-2-2.22][21714-0-2-0.96][21943-3-2-2.46][21947-0-0-2.64][21948-0-0-5.24][21965-2-2-4.26][21998-1-1-1.64][22025-0-3-0.62][22228-3-3-4.49]
[22446-1-1-5.39][22494-3-3-2.79][22757-0-0-3.20][22811-3-3-4.20][22976-3-4-2.93][22985-3-3-2.83][23014-0-3-2.72][23112-1-1-3.32][23144-3-3-2.27][23168-2-4-0.14]
[23219-0-3-1.52][23363-3-3-2.34][23470-0-0-1.19][23486-2-4-1.31][23497-0-3-4.96][23516-0-0-4.31][23690-1-4-2.96][23921-2-2-1.95][23936-1-2-1.01][24040-3-4-1.19]
[24111-1-4-2.62][24182-0-0-5.08][24238-3-3-1.45][24290-2-0-1.79][24345-0-0-1.17][24364-1-2-1.55][24427-3-3-2.06][24477-2-2-3.48][24495-2-1-0.98][24893-2-2-3.86]
[25012-1-4-0.60][25121-2-2-3.75][25165-3-3-0.97][25183-0-0-2.58][25297-3-3-3.12][25398-0-0-2.88][25574-2-4-1.24][25644-1-2-3.03][25718-1-1-1.12][25774-2-2-1.53]
[26032-3-3-1.17][26051-3-3-3.56][26120-0-4-1.45][26321-1-1-3.13][26732-1-1-1.52][26784-3-3-3.49][26827-3-3-1.25][26833-0-3-4.96][26838-2-2-0.96][26860-1-4-1.84]
[26948-0-0-1.69][27049-3-0-2.89][27098-1-2-1.29][27526-0-0-2.85][27639-3-3-1.71][27698-3-3-3.43][27772-0-0-3.71][27890-1-1-0.93][28040-0-4-1.44][28503-2-2-3.79]
[28577-1-2-2.09][28959-0-0-5.45][29198-3-3-1.58][29777-0-0-5.08][29877-2-2-0.67][30035-1-2-2.87][30098-0-3-2.90][30326-1-1-4.92][30572-2-2-3.02][30716-0-4-2.40]
[30806-2-3-0.74][30906-1-1-2.97][31007-0-0-1.92][31181-3-3-1.25][31238-0-3-2.00][31347-0-0-2.03][31422-2-2-1.45][31429-3-4-0.07][31431-0-0-3.13][31432-1-1-4.32]
[31477-0-3-3.87][31524-1-0-0.70][31597-1-2-3.25][31619-1-3-1.59][31701-0-0-1.54][31755-0-0-1.70][31854-3-3-2.52][32074-1-2-1.20][32078-3-3-1.12][32111-1-1-1.49]
[32127-1-2-2.86][32140-3-3-2.75][32263-2-4-1.94][32365-0-0-2.97][32411-2-0-4.13][32429-3-0-2.96][32473-3-3-0.99][32574-3-3-3.90][32584-0-4-0.78][32622-0-4-1.60]
[32858-3-0-1.87][32969-3-3-3.31][33016-2-2-2.54][33031-1-3-0.18][33035-2-2-4.05][33133-2-2-2.26][33173-2-1-1.55][33175-3-4-2.56][33306-3-3-1.70][33309-2-3-1.23]
[33474-0-3-1.39][33478-2-3-1.00][33618-1-1-2.40][33712-0-0-1.69][33782-2-4-3.23][33914-3-3-3.80][34076-3-2-1.35][34112-2-2-3.82][34138-2-2-2.16][34239-1-2-0.86]
[34364-2-2-4.77][34617-1-2-1.39][34751-3-3-3.19][34783-2-2-1.51][35015-3-2-2.27][35018-1-2-1.90][35288-2-2-0.96][0-4-4-2.78][1-4-4-2.44][2-4-4-2.40]
[3-4-4-2.53][4-4-2-2.89][5-4-1-3.45][6-4-0-1.02][7-4-4-0.97][8-4-4-1.23][9-4-4-1.69][10-4-4-3.57][11-4-2-3.30][12-4-2-1.41]
[14-4-3-1.66][15-4-3-2.23][16-4-4-2.13][17-4-4-1.18][18-4-4-2.36][19-4-3-2.48][20-4-2-1.03][21-4-4-2.13][22-4-4-2.75][23-4-2-0.87]
[24-4-4-2.85][25-4-3-1.61][26-4-3-1.16][27-4-4-2.11][28-4-4-2.85][29-4-4-0.65][30-4-0-1.04][31-4-4-2.04][32-4-4-2.63][33-4-2-1.42]
[34-4-4-1.09][35-4-0-1.23][37-4-2-2.00][39-4-3-1.73][40-4-4-1.00][41-4-3-0.84][42-4-2-1.65][43-4-2-1.36][45-4-4-1.15][46-4-4-3.15]
[47-4-4-3.41][48-4-4-1.93][51-4-4-3.70][52-4-4-1.08][53-4-1-1.17][54-4-4-1.31][55-4-4-2.40][56-4-1-2.57][57-4-0-2.52][58-4-2-3.99]
[59-4-4-1.98][60-4-2-1.00][61-4-4-2.33][62-4-2-1.47][63-4-2-3.29][64-4-4-1.37][65-4-4-3.61][66-4-4-3.31][67-4-2-1.48][68-4-3-1.00]
[69-4-0-1.66][70-4-4-2.70][72-4-4-1.35][73-4-1-3.31][74-4-2-2.72][75-4-2-1.37][77-4-4-4.36][78-4-2-3.21][79-4-4-2.30][80-4-4-2.89]
[81-4-4-2.62][82-4-4-0.67][83-4-4-1.17][84-4-4-3.96][85-4-4-4.76][86-4-4-1.52][87-4-4-5.31][88-4-4-2.76][89-4-2-1.71][90-4-4-1.68]
[91-4-4-1.19][92-4-4-1.05][93-4-0-1.68][94-4-4-2.76][95-4-4-0.86][96-4-4-2.76][97-4-4-2.18][98-4-2-1.09][99-4-4-2.32][100-4-2-1.62]
[101-4-4-4.57][102-4-2-2.01][103-4-2-1.61][104-4-4-2.88][105-4-4-2.28][106-4-1-3.33][107-4-4-2.66][108-4-4-1.70][109-4-4-1.32][110-4-4-1.22]
[111-4-0-3.68][112-4-4-0.59][113-4-3-0.77][114-4-3-1.80][115-4-4-1.20][116-4-4-2.01][117-4-4-2.71][119-4-4-1.84][121-4-4-1.87][122-4-4-1.80]
[124-4-2-1.31][125-4-4-3.10][126-4-4-3.32][127-4-2-2.50][128-4-2-0.49][129-4-4-0.99][130-4-4-2.65][131-4-4-1.99][132-4-4-0.77][133-4-4-4.39]
[135-4-2-2.94][136-4-1-0.35][137-4-4-1.85][138-4-4-1.30][139-4-4-1.63][140-4-4-1.59][141-4-2-1.66][142-4-4-3.54][143-4-4-2.98][144-4-4-3.67]
[145-4-4-3.19][148-4-0-2.50][149-4-4-1.61][150-4-4-2.78][151-4-4-2.74][152-4-4-1.44][153-4-4-2.60][154-4-4-2.78][155-4-4-2.69][156-4-3-1.69]
[157-4-2-1.33][158-4-4-1.56][160-4-4-1.04][161-4-2-1.67][162-4-4-1.20][164-4-2-1.70][165-4-4-0.81][167-4-0-2.02][168-4-4-2.40][170-4-4-1.36]
[171-4-4-2.12][172-4-4-2.74][173-4-4-2.14][174-4-0-2.14][175-4-4-1.24][177-4-4-2.02][178-4-2-2.50][179-4-0-0.86][180-4-4-3.50][181-4-3-2.26]
[182-4-2-1.49][183-4-4-2.58][184-4-4-2.47][186-4-3-1.17][187-4-1-1.74][188-4-4-1.99][189-4-4-2.29][190-4-1-0.71][191-4-4-3.22][192-4-4-1.61]
[193-4-2-3.66][194-4-2-1.71][195-4-0-1.93][196-4-1-0.36][197-4-4-2.61][198-4-4-4.73][199-4-4-1.94]
---------------------------
I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.658 | Acc: 72.125% | Wgt Acc: 76.691%
	I - Batch: 100 | Loss: 0.661 | Acc: 72.812% | Wgt Acc: 76.905%
	I - Batch: 150 | Loss: 0.685 | Acc: 71.667% | Wgt Acc: 75.461%
	I - Batch: 200 | Loss: 0.673 | Acc: 71.906% | Wgt Acc: 75.756%
I - num batch: 222
I - Train -- Loss: 0.672 | Acc: 72.089% | Wgt Acc: 75.827% | LR: 6.250000e-04 | Dur: 136.45s
I - Confusion Matrix: [row->prediction - col->label]
[[550.   6.  18.  91. 124.]
 [ 14. 497.  40.  21.  91.]
 [ 18.  40. 613.  23. 175.]
 [ 66.  11.  19. 374.  87.]
 [ 49.  24.  44.  29. 523.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.170 | Acc: 56.016% | Wgt Acc: 54.320% | Dur: 17.72s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   2.   1.   9.  14.]
 [  1.  31.   8.   3.   7.]
 [  2.  23.  33.  10.  31.]
 [ 30.   4.  12.  59.  14.]
 [  8.  18.  21.   5. 114.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.656 | Acc: 74.500% | Wgt Acc: 78.212%
	I - Batch: 100 | Loss: 0.657 | Acc: 74.375% | Wgt Acc: 77.673%
	I - Batch: 150 | Loss: 0.655 | Acc: 74.083% | Wgt Acc: 77.497%
	I - Batch: 200 | Loss: 0.666 | Acc: 73.438% | Wgt Acc: 76.903%
I - num batch: 222
I - Train -- Loss: 0.660 | Acc: 73.640% | Wgt Acc: 77.093% | LR: 6.250000e-04 | Dur: 135.34s
I - Confusion Matrix: [row->prediction - col->label]
[[557.   9.  14.  78.  97.]
 [ 16. 495.  31.  18.  92.]
 [ 14.  39. 610.  23. 166.]
 [ 66.   8.  23. 395.  90.]
 [ 44.  27.  56.  24. 555.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.182 | Acc: 56.213% | Wgt Acc: 56.473% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   3.   3.  11.  19.]
 [  2.  28.  12.   1.  16.]
 [  3.  27.  42.   6.  29.]
 [ 23.   8.   9.  61.  16.]
 [  6.  12.   9.   7. 100.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.578 | Acc: 77.375% | Wgt Acc: 81.433%
	I - Batch: 100 | Loss: 0.647 | Acc: 73.062% | Wgt Acc: 76.642%
	I - Batch: 150 | Loss: 0.657 | Acc: 72.708% | Wgt Acc: 76.352%
	I - Batch: 200 | Loss: 0.651 | Acc: 73.094% | Wgt Acc: 76.592%
I - num batch: 222
I - Train -- Loss: 0.646 | Acc: 73.301% | Wgt Acc: 76.704% | LR: 6.250000e-04 | Dur: 136.34s
I - Confusion Matrix: [row->prediction - col->label]
[[549.   6.  17.  83. 115.]
 [  8. 489.  29.  14.  76.]
 [ 18.  46. 615.  20. 174.]
 [ 82.  11.  26. 393.  81.]
 [ 40.  26.  47.  28. 554.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.102 | Acc: 57.791% | Wgt Acc: 55.365% | Dur: 15.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   4.  19.  12.]
 [  2.  35.  13.   0.  17.]
 [  2.  20.  34.   6.  21.]
 [ 19.   3.   7.  49.   9.]
 [ 11.  18.  17.  12. 121.]]

I - Local maximum validation set accuracy:  57.79

I - Validation set results: 
[14-1-2-2.07][50-3-4-1.45][124-2-3-1.20][127-0-0-4.82][443-2-2-2.64][567-0-0-1.79][573-1-1-1.54][615-0-0-2.83][695-1-2-1.74][722-3-3-2.75]
[826-0-0-3.42][878-0-0-3.86][1103-0-0-1.18][1212-3-3-0.69][1368-0-0-4.88][2181-2-3-2.13][2476-2-1-1.26][2721-2-2-2.55][2818-1-4-1.91][2886-2-4-1.66]
[3231-2-2-4.63][3333-2-1-1.75][3482-2-2-3.02][3536-3-3-2.56][3625-1-1-3.58][3909-0-0-1.75][4035-0-3-2.94][4140-0-0-1.61][4214-1-0-1.95][4346-1-4-1.80]
[4581-2-1-2.29][4708-3-4-1.51][4838-3-0-0.49][4845-1-2-1.37][4868-0-0-3.05][4939-0-4-0.92][4984-2-2-1.66][5078-1-4-0.71][5396-0-0-5.61][5479-1-1-4.93]
[5717-0-0-3.17][5843-1-1-3.19][5949-3-0-2.20][5987-2-4-2.98][6014-3-3-1.03][6033-3-0-1.96][6313-0-3-2.55][6421-3-3-3.31][6500-1-4-0.86][6583-3-3-1.18]
[6683-3-3-0.85][6825-2-1-1.79][6998-3-2-0.89][7049-3-3-1.36][7517-1-1-2.86][7521-1-1-1.78][7528-1-3-0.97][7949-1-2-1.69][8135-1-4-1.56][8185-3-0-2.61]
[8269-3-4-1.85][8273-3-3-2.52][8543-3-0-5.14][8666-1-1-4.01][8672-0-0-5.47][8903-1-1-1.83][9001-2-4-2.03][9036-2-2-4.19][9281-3-4-0.71][9300-2-2-2.19]
[9571-0-3-0.97][9617-1-4-1.23][9644-2-2-2.25][9705-2-4-1.36][9801-0-3-1.51][9803-3-3-1.90][9865-3-3-4.06][9896-2-4-1.59][10314-1-4-1.20][10337-3-3-4.52]
[10403-0-4-1.82][10653-2-4-0.84][10704-2-2-2.35][10719-1-1-3.39][10727-1-1-1.05][10836-0-0-6.12][10969-2-3-2.30][11042-0-4-0.61][11088-1-1-2.96][11322-0-0-5.11]
[11398-2-2-2.01][11499-0-3-2.69][11502-3-3-1.69][11512-3-3-1.60][11608-1-2-3.52][11610-0-3-1.97][11692-0-0-2.72][11905-0-0-2.22][11993-1-1-2.36][12002-2-0-3.44]
[12052-0-0-3.76][12201-0-3-2.76][12235-2-1-1.56][12320-1-4-4.15][12377-2-4-1.37][12398-2-3-1.48][12503-1-4-1.44][12617-0-4-0.87][12685-3-4-0.72][12738-2-2-0.82]
[12742-2-2-3.42][12823-0-3-3.15][13110-1-1-2.23][13240-3-3-2.32][13253-1-4-1.51][13273-0-0-5.21][13634-1-4-1.08][13763-2-2-1.26][13905-3-3-0.22][14060-2-1-1.32]
[14065-3-0-1.32][14147-3-0-1.39][14595-2-2-2.86][14687-2-2-3.13][14788-2-2-1.66][14869-1-1-3.14][14872-3-4-0.98][14877-1-1-2.19][14927-0-3-1.69][15066-0-0-6.61]
[15175-1-4-1.52][15178-2-4-1.81][15375-3-0-2.65][15389-3-3-2.60][15568-2-4-1.38][15675-3-3-2.46][15869-1-2-1.64][16207-3-4-0.18][16236-0-1-0.59][16302-3-0-3.49]
[16331-2-2-6.14][16381-0-0-1.54][16488-1-1-5.30][16495-0-0-0.71][16650-0-0-4.80][16719-1-2-1.32][16801-0-0-6.01][16828-0-0-3.04][17137-3-3-0.96][17245-1-2-1.69]
[17278-3-0-1.07][17282-0-2-3.04][17311-2-2-4.68][17336-2-1-1.78][17608-3-3-4.82][17627-0-2-0.78][17877-3-4-1.96][17924-1-2-1.00][17984-3-3-2.54][18211-0-3-2.17]
[18276-3-0-3.12][18287-1-1-0.65][18394-0-0-3.53][18428-0-0-3.07][18442-0-3-1.71][18478-3-0-1.71][18607-0-0-1.14][18616-0-0-0.66][18663-0-0-2.63][18718-0-0-3.76]
[18766-2-2-2.38][18824-2-4-2.95][18890-3-3-1.04][18930-3-4-1.52][18938-3-0-0.56][19817-1-1-2.07][19839-0-4-1.44][19930-3-3-2.17][19944-0-4-2.71][20036-2-2-4.21]
[20101-3-3-2.54][20474-1-1-3.40][20547-3-0-2.39][20929-2-2-4.67][21245-1-1-1.98][21257-3-2-1.36][21293-1-2-4.66][21316-1-1-4.00][21384-1-4-3.30][21448-1-1-3.60]
[21483-0-0-3.86][21487-2-2-2.16][21714-0-3-0.70][21943-3-2-1.78][21947-0-0-2.86][21948-0-0-5.55][21965-2-2-2.70][21998-1-1-2.68][22025-0-4-1.16][22228-3-3-2.94]
[22446-1-1-5.38][22494-3-3-2.29][22757-0-0-3.19][22811-3-3-4.67][22976-3-2-1.24][22985-3-3-3.08][23014-0-3-2.56][23112-1-1-3.12][23144-3-3-3.13][23168-2-0-1.69]
[23219-0-3-0.82][23363-3-3-1.81][23470-0-1-0.99][23486-2-2-1.89][23497-0-3-4.74][23516-0-0-5.69][23690-1-3-0.82][23921-2-1-1.03][23936-1-2-1.10][24040-3-4-1.09]
[24111-1-4-2.74][24182-0-0-5.61][24238-3-3-1.68][24290-2-0-1.27][24345-0-4-1.16][24364-1-2-1.14][24427-3-3-2.30][24477-2-2-3.69][24495-2-2-1.42][24893-2-2-4.43]
[25012-1-2-0.46][25121-2-2-2.31][25165-3-3-0.97][25183-0-0-1.29][25297-3-3-3.07][25398-0-0-2.99][25574-2-4-2.01][25644-1-2-3.02][25718-1-4-1.05][25774-2-4-1.77]
[26032-3-3-1.04][26051-3-3-3.52][26120-0-0-1.60][26321-1-1-5.20][26732-1-1-2.06][26784-3-3-4.49][26827-3-3-2.13][26833-0-0-3.74][26838-2-3-0.98][26860-1-4-1.68]
[26948-0-0-3.14][27049-3-0-2.68][27098-1-4-0.41][27526-0-0-1.72][27639-3-3-2.44][27698-3-3-3.90][27772-0-0-2.57][27890-1-1-1.81][28040-0-0-2.13][28503-2-2-3.75]
[28577-1-2-2.05][28959-0-0-6.17][29198-3-4-1.85][29777-0-0-6.55][29877-2-1-2.10][30035-1-2-2.39][30098-0-3-2.67][30326-1-1-4.56][30572-2-2-3.13][30716-0-4-3.41]
[30806-2-4-0.85][30906-1-1-2.45][31007-0-0-1.39][31181-3-3-2.26][31238-0-3-1.99][31347-0-0-3.49][31422-2-4-1.30][31429-3-2-0.52][31431-0-0-0.94][31432-1-1-4.80]
[31477-0-0-3.30][31524-1-4-1.02][31597-1-2-3.59][31619-1-0-0.93][31701-0-0-2.32][31755-0-0-2.66][31854-3-3-3.28][32074-1-2-2.50][32078-3-3-1.78][32111-1-1-2.46]
[32127-1-2-2.52][32140-3-3-2.16][32263-2-4-1.85][32365-0-0-2.56][32411-2-0-5.42][32429-3-0-1.84][32473-3-0-0.60][32574-3-0-4.02][32584-0-4-1.30][32622-0-4-0.95]
[32858-3-3-1.66][32969-3-0-3.86][33016-2-2-2.88][33031-1-3-0.46][33035-2-2-3.66][33133-2-1-3.49][33173-2-1-1.41][33175-3-4-2.99][33306-3-3-1.35][33309-2-3-1.32]
[33474-0-3-0.71][33478-2-3-0.33][33618-1-1-4.13][33712-0-3-2.19][33782-2-4-2.74][33914-3-3-3.18][34076-3-3-0.82][34112-2-2-1.23][34138-2-1-1.94][34239-1-1-1.33]
[34364-2-2-4.66][34617-1-2-2.66][34751-3-3-2.23][34783-2-1-1.76][35015-3-2-1.90][35018-1-1-2.81][35288-2-4-0.42][0-4-4-3.82][1-4-4-3.22][2-4-4-2.34]
[3-4-4-1.57][4-4-2-1.42][5-4-1-3.17][6-4-4-3.78][7-4-4-1.39][8-4-4-1.51][9-4-4-2.14][10-4-4-2.41][11-4-4-3.94][12-4-4-0.93]
[14-4-3-1.13][15-4-3-3.08][16-4-4-2.05][17-4-4-0.91][18-4-4-3.06][19-4-0-1.84][20-4-4-0.96][21-4-4-2.00][22-4-4-2.96][23-4-4-1.35]
[24-4-4-5.46][25-4-3-1.31][26-4-2-0.87][27-4-4-2.45][28-4-4-3.56][29-4-2-1.61][30-4-0-1.33][31-4-4-2.53][32-4-4-2.55][33-4-2-1.84]
[34-4-4-1.15][35-4-0-1.52][37-4-4-2.27][39-4-0-2.53][40-4-4-1.54][41-4-4-0.47][42-4-2-1.36][43-4-1-1.62][45-4-4-2.79][46-4-4-2.78]
[47-4-4-3.65][48-4-4-1.72][51-4-4-2.20][52-4-4-2.37][53-4-2-0.98][54-4-0-1.50][55-4-4-1.29][56-4-1-1.78][57-4-3-1.91][58-4-2-2.73]
[59-4-4-2.19][60-4-4-1.37][61-4-4-2.80][62-4-4-1.44][63-4-2-2.86][64-4-2-2.14][65-4-4-4.34][66-4-4-1.81][67-4-4-1.32][68-4-4-1.16]
[69-4-0-1.17][70-4-4-2.24][72-4-4-1.75][73-4-1-2.99][74-4-2-3.55][75-4-2-1.14][77-4-4-4.27][78-4-2-2.43][79-4-4-2.69][80-4-4-3.01]
[81-4-4-2.95][82-4-1-1.30][83-4-1-1.58][84-4-4-3.57][85-4-4-3.51][86-4-4-1.38][87-4-4-4.90][88-4-1-1.59][89-4-2-2.12][90-4-4-1.64]
[91-4-4-1.72][92-4-2-1.15][93-4-0-1.49][94-4-4-2.48][95-4-4-2.01][96-4-4-2.20][97-4-4-2.39][98-4-4-1.94][99-4-4-2.20][100-4-1-1.26]
[101-4-4-5.32][102-4-4-1.68][103-4-3-0.74][104-4-4-3.00][105-4-4-3.73][106-4-1-2.24][107-4-1-1.65][108-4-4-1.75][109-4-4-2.17][110-4-4-1.54]
[111-4-0-5.14][112-4-4-0.50][113-4-3-1.26][114-4-3-0.87][115-4-4-1.79][116-4-4-2.33][117-4-4-3.01][119-4-4-2.33][121-4-4-1.56][122-4-4-2.59]
[124-4-2-0.96][125-4-4-3.27][126-4-4-4.94][127-4-1-1.66][128-4-4-0.65][129-4-4-0.87][130-4-2-2.21][131-4-4-1.87][132-4-4-2.28][133-4-4-3.88]
[135-4-2-2.39][136-4-1-1.65][137-4-4-1.56][138-4-2-1.64][139-4-4-2.47][140-4-4-0.95][141-4-3-0.91][142-4-4-3.23][143-4-4-2.32][144-4-4-4.22]
[145-4-4-2.37][148-4-0-3.44][149-4-4-1.60][150-4-4-2.60][151-4-4-2.02][152-4-4-1.79][153-4-4-2.12][154-4-4-6.00][155-4-4-2.96][156-4-3-2.08]
[157-4-2-2.00][158-4-4-2.54][160-4-4-2.45][161-4-2-1.97][162-4-1-0.81][164-4-4-2.31][165-4-4-1.61][167-4-0-1.51][168-4-4-2.68][170-4-4-2.10]
[171-4-4-3.43][172-4-4-3.71][173-4-4-2.48][174-4-0-2.58][175-4-4-2.13][177-4-0-2.23][178-4-4-2.50][179-4-4-2.37][180-4-4-2.59][181-4-4-1.78]
[182-4-1-2.01][183-4-4-2.71][184-4-4-2.99][186-4-4-1.42][187-4-1-1.75][188-4-4-1.78][189-4-4-2.60][190-4-1-1.31][191-4-4-2.58][192-4-4-0.87]
[193-4-2-4.74][194-4-1-1.92][195-4-4-0.84][196-4-4-0.74][197-4-4-2.30][198-4-4-5.27][199-4-4-2.02]
---------------------------
I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.634 | Acc: 75.500% | Wgt Acc: 79.482%
	I - Batch: 100 | Loss: 0.615 | Acc: 75.375% | Wgt Acc: 79.198%
	I - Batch: 150 | Loss: 0.611 | Acc: 75.750% | Wgt Acc: 79.526%
	I - Batch: 200 | Loss: 0.630 | Acc: 74.688% | Wgt Acc: 78.430%
I - num batch: 222
I - Train -- Loss: 0.629 | Acc: 74.598% | Wgt Acc: 78.412% | LR: 6.250000e-04 | Dur: 139.02s
I - Confusion Matrix: [row->prediction - col->label]
[[562.  10.  16.  74. 126.]
 [ 10. 508.  23.  11.  88.]
 [ 18.  30. 620.  18. 163.]
 [ 73.   8.  19. 409.  76.]
 [ 34.  22.  56.  26. 547.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 59.763% | Wgt Acc: 58.876% | Dur: 15.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   1.   3.  14.   8.]
 [  3.  38.  16.   2.  20.]
 [  1.  22.  41.   5.  25.]
 [ 18.   4.   6.  55.  13.]
 [ 11.  13.   9.  10. 114.]]

I - Local maximum validation set accuracy:  59.76

I - Validation set results: 
[14-1-2-1.89][50-3-4-0.98][124-2-2-2.86][127-0-0-3.46][443-2-2-3.69][567-0-0-2.87][573-1-1-2.30][615-0-3-3.17][695-1-2-1.43][722-3-3-4.76]
[826-0-0-3.04][878-0-0-3.28][1103-0-4-1.77][1212-3-3-0.77][1368-0-0-4.28][2181-2-2-1.83][2476-2-1-1.21][2721-2-2-1.90][2818-1-4-0.54][2886-2-1-2.05]
[3231-2-1-3.48][3333-2-1-2.51][3482-2-2-3.29][3536-3-3-2.21][3625-1-1-6.44][3909-0-0-2.05][4035-0-3-3.46][4140-0-0-0.73][4214-1-1-1.69][4346-1-3-1.74]
[4581-2-1-2.35][4708-3-2-2.01][4838-3-4-0.47][4845-1-2-1.31][4868-0-0-3.84][4939-0-1-0.74][4984-2-2-1.37][5078-1-4-1.10][5396-0-0-5.68][5479-1-1-3.49]
[5717-0-0-2.19][5843-1-1-2.77][5949-3-3-2.26][5987-2-4-3.14][6014-3-3-1.84][6033-3-3-1.78][6313-0-3-3.54][6421-3-3-3.62][6500-1-1-0.68][6583-3-3-2.39]
[6683-3-0-0.95][6825-2-3-2.15][6998-3-2-0.10][7049-3-3-0.92][7517-1-1-2.35][7521-1-1-2.05][7528-1-2-2.48][7949-1-2-1.68][8135-1-0-2.13][8185-3-0-3.69]
[8269-3-1-4.51][8273-3-3-4.25][8543-3-0-5.30][8666-1-1-4.24][8672-0-0-5.41][8903-1-2-1.15][9001-2-2-2.92][9036-2-2-4.33][9281-3-2-0.56][9300-2-2-2.95]
[9571-0-3-2.38][9617-1-1-0.85][9644-2-1-1.83][9705-2-4-0.66][9801-0-0-1.78][9803-3-3-2.60][9865-3-3-4.37][9896-2-2-1.61][10314-1-1-1.60][10337-3-3-4.35]
[10403-0-4-1.47][10653-2-1-1.25][10704-2-1-1.81][10719-1-1-3.22][10727-1-2-1.89][10836-0-0-6.94][10969-2-3-2.78][11042-0-0-2.02][11088-1-1-3.74][11322-0-0-4.88]
[11398-2-2-3.32][11499-0-0-3.49][11502-3-3-1.49][11512-3-3-1.45][11608-1-2-3.56][11610-0-0-2.24][11692-0-0-2.03][11905-0-0-2.37][11993-1-1-2.78][12002-2-0-3.41]
[12052-0-0-4.50][12201-0-3-3.40][12235-2-2-1.72][12320-1-4-3.14][12377-2-4-1.66][12398-2-3-2.11][12503-1-4-2.82][12617-0-1-2.12][12685-3-4-0.68][12738-2-4-0.74]
[12742-2-2-4.32][12823-0-3-3.31][13110-1-1-3.62][13240-3-3-2.92][13253-1-1-1.53][13273-0-0-6.59][13634-1-3-1.19][13763-2-2-1.56][13905-3-3-0.43][14060-2-1-2.14]
[14065-3-3-2.89][14147-3-3-2.30][14595-2-2-1.36][14687-2-2-4.37][14788-2-2-2.34][14869-1-1-3.51][14872-3-4-0.69][14877-1-1-2.41][14927-0-0-2.26][15066-0-0-6.87]
[15175-1-4-1.74][15178-2-4-1.36][15375-3-0-2.95][15389-3-3-2.83][15568-2-1-0.68][15675-3-3-3.18][15869-1-2-1.70][16207-3-0-0.56][16236-0-1-0.51][16302-3-0-2.04]
[16331-2-2-5.86][16381-0-3-1.19][16488-1-1-3.18][16495-0-0-2.57][16650-0-0-5.44][16719-1-2-0.85][16801-0-0-6.51][16828-0-0-2.50][17137-3-3-1.82][17245-1-2-1.70]
[17278-3-3-1.14][17282-0-0-0.54][17311-2-2-2.52][17336-2-1-2.37][17608-3-3-5.91][17627-0-0-0.64][17877-3-4-0.73][17924-1-4-1.21][17984-3-0-4.49][18211-0-3-2.47]
[18276-3-3-1.66][18287-1-1-1.50][18394-0-0-4.36][18428-0-0-5.38][18442-0-3-3.06][18478-3-3-1.78][18607-0-4-0.66][18616-0-0-1.42][18663-0-0-2.50][18718-0-0-5.07]
[18766-2-1-1.54][18824-2-4-2.27][18890-3-3-1.13][18930-3-4-1.41][18938-3-3-1.09][19817-1-1-2.67][19839-0-4-0.78][19930-3-3-2.73][19944-0-4-2.11][20036-2-2-5.42]
[20101-3-1-1.36][20474-1-1-3.65][20547-3-0-2.68][20929-2-2-1.87][21245-1-2-2.34][21257-3-3-0.61][21293-1-2-4.20][21316-1-1-4.34][21384-1-4-2.82][21448-1-1-4.06]
[21483-0-0-4.52][21487-2-2-1.96][21714-0-0-0.99][21943-3-2-2.79][21947-0-0-3.20][21948-0-0-5.10][21965-2-2-3.96][21998-1-1-1.26][22025-0-2-1.10][22228-3-3-3.86]
[22446-1-1-6.56][22494-3-3-3.27][22757-0-0-3.95][22811-3-3-4.29][22976-3-4-3.13][22985-3-3-2.71][23014-0-3-2.91][23112-1-1-4.30][23144-3-3-2.87][23168-2-2-0.69]
[23219-0-4-0.81][23363-3-3-3.68][23470-0-4-0.68][23486-2-2-1.40][23497-0-3-5.59][23516-0-0-4.63][23690-1-4-1.18][23921-2-2-2.03][23936-1-2-1.08][24040-3-4-1.04]
[24111-1-4-1.93][24182-0-0-4.48][24238-3-0-0.97][24290-2-0-1.80][24345-0-0-2.96][24364-1-2-0.87][24427-3-3-1.84][24477-2-2-3.31][24495-2-1-1.46][24893-2-2-2.98]
[25012-1-4-0.87][25121-2-2-2.62][25165-3-3-1.68][25183-0-0-1.05][25297-3-3-3.52][25398-0-0-2.75][25574-2-4-1.19][25644-1-1-1.57][25718-1-2-1.42][25774-2-2-1.08]
[26032-3-0-3.51][26051-3-3-4.20][26120-0-0-2.51][26321-1-1-3.33][26732-1-1-1.71][26784-3-3-3.83][26827-3-3-2.25][26833-0-3-5.29][26838-2-2-0.54][26860-1-4-0.88]
[26948-0-0-3.30][27049-3-0-3.54][27098-1-4-0.74][27526-0-4-1.38][27639-3-3-3.20][27698-3-3-3.20][27772-0-0-5.24][27890-1-1-2.02][28040-0-0-2.83][28503-2-2-3.39]
[28577-1-1-1.51][28959-0-0-5.77][29198-3-3-1.61][29777-0-0-6.14][29877-2-2-1.62][30035-1-2-2.31][30098-0-3-3.33][30326-1-1-5.01][30572-2-2-3.24][30716-0-4-2.32]
[30806-2-3-1.15][30906-1-1-2.69][31007-0-3-1.49][31181-3-3-2.10][31238-0-3-2.68][31347-0-0-3.83][31422-2-2-1.95][31429-3-4-0.63][31431-0-3-2.54][31432-1-1-4.80]
[31477-0-0-3.54][31524-1-3-0.96][31597-1-2-2.04][31619-1-3-1.05][31701-0-0-2.75][31755-0-0-2.52][31854-3-3-3.62][32074-1-2-2.09][32078-3-3-2.54][32111-1-1-3.23]
[32127-1-2-2.89][32140-3-3-3.67][32263-2-4-0.43][32365-0-0-2.96][32411-2-0-4.68][32429-3-0-2.53][32473-3-0-0.81][32574-3-3-5.10][32584-0-4-1.00][32622-0-4-1.62]
[32858-3-0-2.10][32969-3-3-4.00][33016-2-2-3.77][33031-1-2-0.76][33035-2-2-3.86][33133-2-1-1.54][33173-2-2-1.18][33175-3-4-1.23][33306-3-3-1.66][33309-2-3-0.86]
[33474-0-3-0.19][33478-2-3-2.26][33618-1-1-3.50][33712-0-3-3.05][33782-2-2-4.18][33914-3-3-4.81][34076-3-3-0.51][34112-2-2-2.66][34138-2-1-1.43][34239-1-4-0.97]
[34364-2-1-4.18][34617-1-2-3.02][34751-3-3-4.82][34783-2-4-2.02][35015-3-2-2.15][35018-1-1-2.03][35288-2-2-1.03][0-4-4-1.20][1-4-4-2.64][2-4-4-2.80]
[3-4-4-1.02][4-4-4-1.50][5-4-1-1.32][6-4-4-3.14][7-4-4-0.98][8-4-4-1.25][9-4-4-1.50][10-4-4-2.88][11-4-4-2.64][12-4-2-0.52]
[14-4-3-1.77][15-4-3-3.36][16-4-4-1.54][17-4-1-1.22][18-4-4-2.49][19-4-3-2.91][20-4-2-1.08][21-4-2-3.28][22-4-4-2.32][23-4-4-0.56]
[24-4-4-4.05][25-4-4-1.34][26-4-4-0.92][27-4-4-2.28][28-4-4-2.19][29-4-1-1.52][30-4-4-1.49][31-4-4-1.77][32-4-2-1.39][33-4-2-2.43]
[34-4-4-1.35][35-4-3-1.71][37-4-4-1.68][39-4-0-2.48][40-4-4-1.74][41-4-4-0.71][42-4-4-0.68][43-4-1-1.50][45-4-4-1.59][46-4-4-2.44]
[47-4-4-3.51][48-4-2-1.18][51-4-4-3.33][52-4-4-1.62][53-4-4-1.13][54-4-3-1.00][55-4-4-1.84][56-4-1-1.72][57-4-3-2.77][58-4-2-2.83]
[59-4-0-2.29][60-4-1-0.96][61-4-4-2.14][62-4-4-1.97][63-4-2-3.78][64-4-4-1.60][65-4-4-3.48][66-4-4-2.86][67-4-4-1.02][68-4-4-0.86]
[69-4-2-1.20][70-4-4-1.65][72-4-1-1.42][73-4-1-3.05][74-4-2-2.19][75-4-2-1.82][77-4-1-2.13][78-4-2-2.12][79-4-4-1.66][80-4-4-2.82]
[81-4-4-2.04][82-4-1-0.80][83-4-1-0.80][84-4-4-3.47][85-4-4-2.95][86-4-4-1.35][87-4-4-3.91][88-4-4-2.13][89-4-2-1.98][90-4-4-0.84]
[91-4-4-0.74][92-4-4-1.13][93-4-0-1.02][94-4-4-2.25][95-4-4-0.97][96-4-4-2.45][97-4-2-2.11][98-4-1-1.32][99-4-4-2.04][100-4-2-1.11]
[101-4-4-3.60][102-4-2-1.13][103-4-3-1.53][104-4-4-2.80][105-4-4-3.40][106-4-4-2.99][107-4-4-2.40][108-4-4-1.46][109-4-4-1.28][110-4-4-1.41]
[111-4-0-3.25][112-4-0-0.98][113-4-2-1.02][114-4-3-1.35][115-4-4-0.79][116-4-4-0.97][117-4-4-2.64][119-4-4-1.48][121-4-4-1.42][122-4-4-2.11]
[124-4-1-1.66][125-4-4-3.08][126-4-4-2.70][127-4-1-1.41][128-4-4-0.63][129-4-1-0.61][130-4-2-2.84][131-4-4-1.92][132-4-4-1.63][133-4-4-3.88]
[135-4-2-2.21][136-4-1-1.25][137-4-4-1.64][138-4-4-1.09][139-4-4-1.23][140-4-4-1.43][141-4-3-1.65][142-4-4-2.98][143-4-4-2.57][144-4-4-2.26]
[145-4-4-2.36][148-4-0-3.39][149-4-4-2.24][150-4-4-3.34][151-4-4-0.64][152-4-1-1.19][153-4-2-1.24][154-4-4-4.30][155-4-4-1.65][156-4-3-2.62]
[157-4-2-2.06][158-4-4-1.60][160-4-1-1.06][161-4-2-0.86][162-4-2-0.95][164-4-4-1.57][165-4-4-1.25][167-4-4-1.77][168-4-4-1.86][170-4-4-1.25]
[171-4-4-2.13][172-4-4-1.92][173-4-4-2.08][174-4-0-3.48][175-4-4-1.61][177-4-4-0.57][178-4-4-2.88][179-4-4-0.81][180-4-4-2.83][181-4-3-3.47]
[182-4-3-2.34][183-4-4-2.34][184-4-4-1.47][186-4-4-0.65][187-4-3-1.31][188-4-2-2.10][189-4-4-1.90][190-4-4-1.04][191-4-4-2.80][192-4-4-1.27]
[193-4-1-2.24][194-4-1-1.02][195-4-0-1.23][196-4-2-0.89][197-4-4-2.75][198-4-4-5.67][199-4-4-1.50]
---------------------------
I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.593 | Acc: 75.750% | Wgt Acc: 79.004%
	I - Batch: 100 | Loss: 0.595 | Acc: 75.438% | Wgt Acc: 78.751%
	I - Batch: 150 | Loss: 0.599 | Acc: 75.333% | Wgt Acc: 78.658%
	I - Batch: 200 | Loss: 0.611 | Acc: 75.250% | Wgt Acc: 78.627%
I - num batch: 222
I - Train -- Loss: 0.614 | Acc: 74.683% | Wgt Acc: 78.216% | LR: 6.250000e-04 | Dur: 135.90s
I - Confusion Matrix: [row->prediction - col->label]
[[569.   8.  10.  83. 113.]
 [ 10. 504.  32.  15.  96.]
 [ 14.  30. 624.  20. 139.]
 [ 65.   8.  21. 392.  92.]
 [ 39.  28.  47.  28. 560.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.150 | Acc: 57.002% | Wgt Acc: 57.518% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  1.  2. 17. 10.]
 [ 1. 40. 12.  6. 15.]
 [ 8. 27. 48. 10. 50.]
 [12.  2.  4. 46.  7.]
 [10.  8.  9.  7. 98.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.555 | Acc: 78.500% | Wgt Acc: 81.975%
	I - Batch: 100 | Loss: 0.552 | Acc: 77.625% | Wgt Acc: 81.364%
	I - Batch: 150 | Loss: 0.575 | Acc: 76.542% | Wgt Acc: 79.815%
	I - Batch: 200 | Loss: 0.591 | Acc: 76.062% | Wgt Acc: 79.304%
I - num batch: 222
I - Train -- Loss: 0.597 | Acc: 75.923% | Wgt Acc: 79.191% | LR: 6.250000e-04 | Dur: 134.79s
I - Confusion Matrix: [row->prediction - col->label]
[[577.  13.  12.  75.  99.]
 [ 13. 491.  26.  19.  71.]
 [ 17.  40. 635.  16. 158.]
 [ 55.  12.  13. 404.  86.]
 [ 35.  22.  48.  24. 586.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.082 | Acc: 59.566% | Wgt Acc: 54.853% | Dur: 16.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 47.   1.   2.  14.   7.]
 [  1.  30.   3.   3.   5.]
 [  1.  23.  38.   8.  19.]
 [ 18.   2.   8.  47.   9.]
 [ 21.  22.  24.  14. 140.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.574 | Acc: 77.500% | Wgt Acc: 81.642%
	I - Batch: 100 | Loss: 0.582 | Acc: 77.375% | Wgt Acc: 81.428%
	I - Batch: 150 | Loss: 0.572 | Acc: 77.792% | Wgt Acc: 81.689%
	I - Batch: 200 | Loss: 0.583 | Acc: 77.094% | Wgt Acc: 81.063%
I - num batch: 222
I - Train -- Loss: 0.582 | Acc: 77.023% | Wgt Acc: 80.844% | LR: 6.250000e-04 | Dur: 137.34s
I - Confusion Matrix: [row->prediction - col->label]
[[581.   6.  12.  58. 115.]
 [ 12. 502.  21.  10.  84.]
 [  7.  31. 645.  13. 157.]
 [ 53.  11.  12. 433.  73.]
 [ 44.  28.  44.  24. 571.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.126 | Acc: 57.396% | Wgt Acc: 58.280% | Dur: 14.12s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  5.  3. 28. 20.]
 [ 0. 45. 17.  3. 22.]
 [ 2. 16. 39.  5. 27.]
 [11.  1.  6. 44. 15.]
 [ 8. 11. 10.  6. 96.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.560 | Acc: 76.875% | Wgt Acc: 81.332%
	I - Batch: 100 | Loss: 0.577 | Acc: 75.875% | Wgt Acc: 79.945%
	I - Batch: 150 | Loss: 0.564 | Acc: 76.458% | Wgt Acc: 80.169%
	I - Batch: 200 | Loss: 0.576 | Acc: 76.094% | Wgt Acc: 79.872%
I - num batch: 222
I - Train -- Loss: 0.579 | Acc: 76.064% | Wgt Acc: 79.875% | LR: 6.250000e-04 | Dur: 135.91s
I - Confusion Matrix: [row->prediction - col->label]
[[586.   9.   9.  69. 118.]
 [ 10. 509.  20.  18.  70.]
 [  9.  34. 642.  25. 173.]
 [ 52.   6.  19. 402.  80.]
 [ 40.  20.  44.  24. 559.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.114 | Acc: 58.383% | Wgt Acc: 57.632% | Dur: 16.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   4.  20.  16.]
 [  3.  39.  13.   4.  16.]
 [  2.  19.  42.   7.  27.]
 [ 11.   3.   3.  43.  12.]
 [  9.  15.  13.  12. 109.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.485 | Acc: 79.750% | Wgt Acc: 83.938%
	I - Batch: 100 | Loss: 0.528 | Acc: 78.500% | Wgt Acc: 82.569%
	I - Batch: 150 | Loss: 0.565 | Acc: 76.792% | Wgt Acc: 80.769%
	I - Batch: 200 | Loss: 0.568 | Acc: 76.438% | Wgt Acc: 80.538%
I - num batch: 222
I - Train -- Loss: 0.570 | Acc: 76.346% | Wgt Acc: 80.384% | LR: 6.250000e-04 | Dur: 133.69s
I - Confusion Matrix: [row->prediction - col->label]
[[580.   8.   7.  62. 117.]
 [ 15. 511.  25.   9.  95.]
 [ 10.  28. 638.  18. 154.]
 [ 51.  12.  15. 426.  81.]
 [ 41.  19.  49.  23. 553.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.085 | Acc: 62.130% | Wgt Acc: 59.074% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   1.   3.  15.   8.]
 [  3.  34.  10.   2.   7.]
 [  3.  18.  35.   4.  18.]
 [  7.   3.   7.  52.  14.]
 [ 14.  22.  20.  13. 133.]]

I - Local maximum validation set accuracy:  62.13

I - Validation set results: 
[14-1-1-1.35][50-3-4-1.74][124-2-2-2.36][127-0-0-5.26][443-2-2-2.92][567-0-0-1.92][573-1-1-0.95][615-0-0-2.94][695-1-1-0.36][722-3-3-3.33]
[826-0-0-4.42][878-0-0-2.66][1103-0-4-1.73][1212-3-3-0.98][1368-0-0-3.37][2181-2-0-0.48][2476-2-4-1.41][2721-2-2-1.48][2818-1-4-2.20][2886-2-4-2.46]
[3231-2-1-2.40][3333-2-2-0.97][3482-2-2-3.14][3536-3-0-2.21][3625-1-1-2.78][3909-0-0-2.61][4035-0-0-4.49][4140-0-4-1.53][4214-1-1-1.48][4346-1-4-1.96]
[4581-2-1-2.73][4708-3-4-0.54][4838-3-2-0.17][4845-1-2-0.64][4868-0-0-3.34][4939-0-4-1.48][4984-2-3-1.16][5078-1-2-1.61][5396-0-0-6.52][5479-1-1-4.14]
[5717-0-0-3.81][5843-1-2-2.80][5949-3-3-2.86][5987-2-4-3.14][6014-3-2-0.92][6033-3-3-2.94][6313-0-3-3.23][6421-3-3-3.05][6500-1-2-1.00][6583-3-3-3.82]
[6683-3-3-1.81][6825-2-1-1.35][6998-3-3-1.02][7049-3-0-0.49][7517-1-1-2.03][7521-1-1-1.11][7528-1-3-0.65][7949-1-2-1.93][8135-1-4-2.57][8185-3-0-3.81]
[8269-3-4-1.90][8273-3-3-2.15][8543-3-0-5.35][8666-1-1-1.23][8672-0-0-5.36][8903-1-3-0.46][9001-2-2-2.46][9036-2-2-3.14][9281-3-4-0.90][9300-2-2-4.42]
[9571-0-3-1.59][9617-1-4-1.69][9644-2-2-0.51][9705-2-4-0.99][9801-0-0-2.43][9803-3-3-1.82][9865-3-3-4.37][9896-2-4-1.83][10314-1-4-1.67][10337-3-3-4.33]
[10403-0-4-2.20][10653-2-4-2.13][10704-2-2-0.96][10719-1-1-1.97][10727-1-4-1.54][10836-0-0-7.89][10969-2-3-1.91][11042-0-0-1.18][11088-1-1-2.88][11322-0-0-6.01]
[11398-2-2-1.34][11499-0-0-4.89][11502-3-3-1.99][11512-3-3-1.57][11608-1-2-3.28][11610-0-0-3.13][11692-0-0-3.76][11905-0-0-3.15][11993-1-1-1.26][12002-2-0-1.83]
[12052-0-0-3.18][12201-0-3-3.02][12235-2-4-1.25][12320-1-4-4.82][12377-2-4-3.46][12398-2-3-2.71][12503-1-1-2.15][12617-0-1-1.31][12685-3-3-0.60][12738-2-4-0.99]
[12742-2-2-2.99][12823-0-0-2.80][13110-1-1-0.76][13240-3-3-2.93][13253-1-4-2.43][13273-0-0-6.07][13634-1-2-0.83][13763-2-2-0.92][13905-3-3-0.32][14060-2-1-2.34]
[14065-3-3-1.87][14147-3-3-1.55][14595-2-4-1.39][14687-2-2-2.80][14788-2-2-1.52][14869-1-1-2.05][14872-3-4-0.46][14877-1-4-1.74][14927-0-0-4.00][15066-0-0-7.30]
[15175-1-1-1.25][15178-2-4-1.94][15375-3-0-3.09][15389-3-3-3.45][15568-2-4-1.09][15675-3-3-4.03][15869-1-2-1.48][16207-3-1-0.99][16236-0-0-0.56][16302-3-0-2.33]
[16331-2-2-4.77][16381-0-0-1.10][16488-1-1-3.24][16495-0-0-1.30][16650-0-0-5.90][16719-1-4-1.35][16801-0-0-6.73][16828-0-0-3.18][17137-3-3-1.40][17245-1-4-0.89]
[17278-3-3-0.86][17282-0-2-1.34][17311-2-2-2.64][17336-2-1-1.09][17608-3-3-6.35][17627-0-4-1.01][17877-3-4-1.85][17924-1-4-0.82][17984-3-3-5.02][18211-0-1-1.61]
[18276-3-0-1.77][18287-1-4-0.58][18394-0-0-5.05][18428-0-4-0.97][18442-0-0-1.82][18478-3-3-2.67][18607-0-0-1.20][18616-0-2-0.69][18663-0-0-2.56][18718-0-0-5.11]
[18766-2-2-1.09][18824-2-2-2.07][18890-3-3-2.62][18930-3-4-2.48][18938-3-3-1.93][19817-1-1-2.64][19839-0-4-1.12][19930-3-3-2.46][19944-0-4-1.56][20036-2-2-5.32]
[20101-3-3-1.99][20474-1-1-2.48][20547-3-0-2.27][20929-2-2-4.25][21245-1-2-1.12][21257-3-4-1.39][21293-1-2-4.30][21316-1-1-1.71][21384-1-4-3.88][21448-1-1-3.24]
[21483-0-0-4.37][21487-2-2-2.61][21714-0-0-1.12][21943-3-2-1.22][21947-0-0-1.75][21948-0-0-5.19][21965-2-1-2.56][21998-1-1-3.27][22025-0-2-1.88][22228-3-3-3.10]
[22446-1-1-5.12][22494-3-3-2.71][22757-0-0-5.36][22811-3-3-3.66][22976-3-2-1.59][22985-3-3-2.50][23014-0-0-3.00][23112-1-1-2.95][23144-3-3-2.71][23168-2-4-0.23]
[23219-0-0-1.77][23363-3-3-3.35][23470-0-4-1.20][23486-2-2-3.07][23497-0-3-4.80][23516-0-0-4.14][23690-1-4-1.97][23921-2-2-1.45][23936-1-2-0.84][24040-3-4-1.21]
[24111-1-4-3.51][24182-0-0-6.83][24238-3-0-1.04][24290-2-3-0.77][24345-0-0-2.78][24364-1-3-0.91][24427-3-0-0.99][24477-2-2-3.33][24495-2-1-1.18][24893-2-2-2.70]
[25012-1-4-1.29][25121-2-2-3.20][25165-3-3-0.85][25183-0-0-2.98][25297-3-3-4.06][25398-0-0-2.68][25574-2-2-2.64][25644-1-2-2.73][25718-1-4-0.44][25774-2-4-1.20]
[26032-3-3-0.70][26051-3-3-4.16][26120-0-4-2.46][26321-1-1-3.26][26732-1-1-1.95][26784-3-3-4.73][26827-3-3-2.32][26833-0-0-4.13][26838-2-4-0.95][26860-1-2-1.89]
[26948-0-0-1.49][27049-3-0-3.93][27098-1-4-1.42][27526-0-0-1.36][27639-3-3-2.87][27698-3-3-1.89][27772-0-0-4.29][27890-1-1-2.17][28040-0-4-2.52][28503-2-2-2.31]
[28577-1-1-1.02][28959-0-0-5.85][29198-3-4-0.93][29777-0-0-6.25][29877-2-4-0.78][30035-1-1-2.69][30098-0-0-2.98][30326-1-1-3.71][30572-2-2-2.31][30716-0-4-4.49]
[30806-2-3-1.11][30906-1-4-2.42][31007-0-0-1.50][31181-3-3-1.25][31238-0-3-2.80][31347-0-0-3.93][31422-2-4-1.34][31429-3-1-0.73][31431-0-1-1.52][31432-1-1-3.17]
[31477-0-0-3.75][31524-1-2-1.73][31597-1-2-3.64][31619-1-2-1.08][31701-0-0-2.92][31755-0-0-3.82][31854-3-3-2.63][32074-1-1-1.01][32078-3-3-2.65][32111-1-1-2.21]
[32127-1-2-1.96][32140-3-3-3.88][32263-2-4-2.33][32365-0-0-2.54][32411-2-0-4.39][32429-3-0-3.00][32473-3-3-1.15][32574-3-0-5.30][32584-0-4-0.71][32622-0-4-1.15]
[32858-3-0-3.52][32969-3-3-3.91][33016-2-2-3.54][33031-1-0-0.24][33035-2-2-3.92][33133-2-1-1.11][33173-2-2-0.22][33175-3-4-2.21][33306-3-0-0.56][33309-2-3-1.57]
[33474-0-3-1.06][33478-2-3-2.70][33618-1-1-2.76][33712-0-3-1.85][33782-2-2-2.24][33914-3-3-1.94][34076-3-4-0.56][34112-2-2-2.72][34138-2-1-0.77][34239-1-4-0.97]
[34364-2-1-3.23][34617-1-2-2.16][34751-3-3-4.56][34783-2-4-2.61][35015-3-4-1.21][35018-1-4-1.83][35288-2-4-1.26][0-4-4-3.52][1-4-4-3.56][2-4-4-1.92]
[3-4-4-2.16][4-4-4-2.39][5-4-1-1.05][6-4-4-2.57][7-4-4-3.50][8-4-4-1.08][9-4-4-1.57][10-4-4-3.20][11-4-4-3.69][12-4-1-1.28]
[14-4-4-1.38][15-4-3-3.39][16-4-4-2.87][17-4-4-2.08][18-4-4-4.16][19-4-3-1.18][20-4-4-1.30][21-4-4-1.74][22-4-4-3.52][23-4-4-1.75]
[24-4-4-6.96][25-4-3-1.81][26-4-2-1.40][27-4-4-2.69][28-4-4-3.97][29-4-1-1.11][30-4-3-1.08][31-4-4-2.46][32-4-4-2.72][33-4-2-2.02]
[34-4-4-1.39][35-4-3-0.63][37-4-4-1.79][39-4-0-3.88][40-4-4-1.15][41-4-1-1.68][42-4-4-1.23][43-4-4-1.38][45-4-4-2.50][46-4-4-3.21]
[47-4-4-3.87][48-4-4-3.19][51-4-4-3.33][52-4-4-1.78][53-4-4-1.07][54-4-4-1.89][55-4-4-2.15][56-4-1-2.01][57-4-3-2.74][58-4-2-1.93]
[59-4-4-2.79][60-4-4-2.98][61-4-4-3.11][62-4-2-3.10][63-4-2-2.83][64-4-4-1.45][65-4-4-5.70][66-4-4-4.05][67-4-4-1.16][68-4-1-1.30]
[69-4-3-1.73][70-4-4-2.81][72-4-4-2.76][73-4-4-1.82][74-4-4-1.61][75-4-2-1.78][77-4-4-3.76][78-4-4-0.76][79-4-4-3.23][80-4-4-3.58]
[81-4-4-3.12][82-4-4-1.17][83-4-4-1.23][84-4-4-4.96][85-4-4-3.50][86-4-4-2.51][87-4-4-4.80][88-4-4-1.90][89-4-3-1.66][90-4-4-1.82]
[91-4-4-2.12][92-4-4-1.71][93-4-0-1.76][94-4-4-2.90][95-4-4-0.92][96-4-4-1.86][97-4-4-2.09][98-4-4-1.61][99-4-4-1.07][100-4-4-1.83]
[101-4-4-6.15][102-4-4-2.09][103-4-0-1.18][104-4-4-2.99][105-4-4-3.95][106-4-4-3.12][107-4-4-3.15][108-4-4-1.91][109-4-4-1.63][110-4-2-1.80]
[111-4-0-5.28][112-4-4-0.89][113-4-3-1.83][114-4-3-1.86][115-4-4-2.13][116-4-4-2.27][117-4-2-3.58][119-4-4-2.01][121-4-2-2.25][122-4-4-2.22]
[124-4-4-0.57][125-4-4-3.22][126-4-4-5.11][127-4-4-1.62][128-4-0-0.89][129-4-4-1.00][130-4-4-1.95][131-4-2-2.12][132-4-4-1.31][133-4-4-4.83]
[135-4-2-1.37][136-4-4-1.14][137-4-4-2.13][138-4-2-2.53][139-4-4-2.34][140-4-4-1.23][141-4-3-1.02][142-4-4-3.24][143-4-4-3.25][144-4-4-4.85]
[145-4-4-2.38][148-4-0-3.03][149-4-4-1.00][150-4-2-2.42][151-4-4-4.24][152-4-4-1.52][153-4-4-1.76][154-4-4-6.31][155-4-4-4.14][156-4-3-2.30]
[157-4-4-1.09][158-4-4-2.59][160-4-4-1.70][161-4-2-1.28][162-4-4-1.47][164-4-4-1.66][165-4-4-2.26][167-4-4-2.32][168-4-4-2.98][170-4-4-1.48]
[171-4-4-2.42][172-4-4-4.52][173-4-4-3.38][174-4-0-2.51][175-4-4-3.13][177-4-4-2.63][178-4-4-2.79][179-4-4-2.49][180-4-4-4.06][181-4-3-1.83]
[182-4-3-3.19][183-4-4-2.47][184-4-4-2.12][186-4-2-0.91][187-4-2-2.50][188-4-4-2.02][189-4-4-1.62][190-4-2-1.14][191-4-4-2.70][192-4-4-1.58]
[193-4-2-2.28][194-4-1-2.08][195-4-0-2.30][196-4-4-1.40][197-4-4-2.82][198-4-4-7.07][199-4-4-1.85]
---------------------------
I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.564 | Acc: 77.125% | Wgt Acc: 80.754%
	I - Batch: 100 | Loss: 0.539 | Acc: 78.688% | Wgt Acc: 82.474%
	I - Batch: 150 | Loss: 0.551 | Acc: 77.875% | Wgt Acc: 81.593%
	I - Batch: 200 | Loss: 0.561 | Acc: 77.719% | Wgt Acc: 81.065%
I - num batch: 222
I - Train -- Loss: 0.561 | Acc: 77.559% | Wgt Acc: 81.143% | LR: 6.250000e-04 | Dur: 135.06s
I - Confusion Matrix: [row->prediction - col->label]
[[589.  10.  14.  64. 106.]
 [ 13. 504.  17.   8.  73.]
 [ 15.  27. 641.  16. 147.]
 [ 45.  12.  17. 429.  86.]
 [ 35.  25.  45.  21. 588.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.103 | Acc: 59.763% | Wgt Acc: 59.858% | Dur: 13.97s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   5.   8.  12.]
 [  1.  42.  10.   3.  20.]
 [  5.  19.  39.   8.  28.]
 [ 23.   5.   8.  61.  12.]
 [  6.  11.  13.   6. 108.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.527 | Acc: 79.125% | Wgt Acc: 83.110%
	I - Batch: 100 | Loss: 0.546 | Acc: 78.875% | Wgt Acc: 82.653%
	I - Batch: 150 | Loss: 0.539 | Acc: 78.667% | Wgt Acc: 82.326%
	I - Batch: 200 | Loss: 0.550 | Acc: 77.969% | Wgt Acc: 81.499%
I - num batch: 222
I - Train -- Loss: 0.550 | Acc: 77.925% | Wgt Acc: 81.421% | LR: 6.250000e-04 | Dur: 137.39s
I - Confusion Matrix: [row->prediction - col->label]
[[586.  11.  12.  55.  94.]
 [ 11. 512.  24.  12.  77.]
 [ 13.  25. 645.  23. 143.]
 [ 52.   4.  22. 425.  90.]
 [ 35.  26.  31.  23. 596.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 59.763% | Wgt Acc: 58.855% | Dur: 14.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   1.   3.  11.  13.]
 [  2.  34.   7.   1.   7.]
 [  3.  29.  45.   9.  33.]
 [ 19.   3.   6.  56.  13.]
 [ 10.  11.  14.   9. 114.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.500 | Acc: 80.125% | Wgt Acc: 83.342%
	I - Batch: 100 | Loss: 0.513 | Acc: 79.312% | Wgt Acc: 82.825%
	I - Batch: 150 | Loss: 0.506 | Acc: 79.583% | Wgt Acc: 83.326%
	I - Batch: 200 | Loss: 0.516 | Acc: 79.094% | Wgt Acc: 82.784%
I - num batch: 222
I - Train -- Loss: 0.522 | Acc: 78.827% | Wgt Acc: 82.660% | LR: 6.250000e-04 | Dur: 134.39s
I - Confusion Matrix: [row->prediction - col->label]
[[592.   5.  13.  56. 127.]
 [ 13. 517.  19.   7.  79.]
 [ 10.  27. 654.  11. 135.]
 [ 52.   6.  10. 444.  70.]
 [ 30.  23.  38.  20. 589.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.136 | Acc: 58.974% | Wgt Acc: 59.189% | Dur: 17.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   9.   4.  16.  19.]
 [  1.  33.  13.   1.  13.]
 [  2.  20.  38.   3.  20.]
 [ 15.  10.  14.  60.  23.]
 [  7.   6.   6.   6. 105.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.491 | Acc: 80.125% | Wgt Acc: 83.732%
	I - Batch: 100 | Loss: 0.499 | Acc: 80.250% | Wgt Acc: 84.366%
	I - Batch: 150 | Loss: 0.493 | Acc: 79.708% | Wgt Acc: 83.993%
	I - Batch: 200 | Loss: 0.520 | Acc: 78.688% | Wgt Acc: 82.625%
I - num batch: 222
I - Train -- Loss: 0.520 | Acc: 78.771% | Wgt Acc: 82.633% | LR: 6.250000e-04 | Dur: 136.52s
I - Confusion Matrix: [row->prediction - col->label]
[[590.   4.  12.  51. 129.]
 [  9. 528.  14.  12.  68.]
 [ 11.  22. 650.  18. 130.]
 [ 49.   5.  12. 439.  86.]
 [ 38.  19.  46.  18. 587.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.073 | Acc: 61.933% | Wgt Acc: 60.673% | Dur: 17.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   3.  18.  14.]
 [  0.  35.   7.   0.  11.]
 [  3.  19.  40.   3.  25.]
 [ 11.   3.  10.  55.  10.]
 [ 10.  19.  15.  10. 120.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.469 | Acc: 82.000% | Wgt Acc: 86.185%
	I - Batch: 100 | Loss: 0.490 | Acc: 79.500% | Wgt Acc: 84.010%
	I - Batch: 150 | Loss: 0.497 | Acc: 79.292% | Wgt Acc: 83.436%
	I - Batch: 200 | Loss: 0.509 | Acc: 78.594% | Wgt Acc: 82.585%
I - num batch: 222
I - Train -- Loss: 0.505 | Acc: 78.884% | Wgt Acc: 82.746% | LR: 6.250000e-04 | Dur: 137.15s
I - Confusion Matrix: [row->prediction - col->label]
[[596.   3.   8.  52. 121.]
 [ 11. 521.  22.   8.  82.]
 [ 14.  24. 650.  12. 136.]
 [ 39.   1.  10. 443.  73.]
 [ 37.  29.  44.  23. 588.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 62.130% | Wgt Acc: 59.743% | Dur: 17.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   1.  17.   7.]
 [  2.  35.   4.   0.   9.]
 [  3.  24.  46.   9.  24.]
 [ 15.   2.   8.  51.  12.]
 [ 13.  15.  16.   9. 128.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.499 | Acc: 80.125% | Wgt Acc: 84.008%
	I - Batch: 100 | Loss: 0.497 | Acc: 80.250% | Wgt Acc: 84.281%
	I - Batch: 150 | Loss: 0.491 | Acc: 81.042% | Wgt Acc: 84.724%
	I - Batch: 200 | Loss: 0.502 | Acc: 80.125% | Wgt Acc: 83.730%
I - num batch: 222
I - Train -- Loss: 0.502 | Acc: 80.011% | Wgt Acc: 83.567% | LR: 6.250000e-04 | Dur: 135.49s
I - Confusion Matrix: [row->prediction - col->label]
[[595.   6.   7.  49. 115.]
 [ 11. 526.  14.  12.  79.]
 [  8.  20. 658.   9. 123.]
 [ 47.   7.  13. 444.  68.]
 [ 36.  19.  42.  24. 615.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.098 | Acc: 60.552% | Wgt Acc: 60.088% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   4.  14.  13.]
 [  0.  37.   6.   2.   7.]
 [  2.  19.  39.   2.  30.]
 [ 15.   4.   9.  61.  17.]
 [ 14.  15.  17.   7. 113.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.484 | Acc: 80.750% | Wgt Acc: 85.366%
	I - Batch: 100 | Loss: 0.490 | Acc: 79.938% | Wgt Acc: 84.517%
	I - Batch: 150 | Loss: 0.502 | Acc: 79.708% | Wgt Acc: 83.827%
	I - Batch: 200 | Loss: 0.496 | Acc: 79.906% | Wgt Acc: 84.020%
I - num batch: 222
I - Train -- Loss: 0.490 | Acc: 80.011% | Wgt Acc: 84.136% | LR: 6.250000e-04 | Dur: 137.11s
I - Confusion Matrix: [row->prediction - col->label]
[[602.   5.  10.  37. 118.]
 [  5. 531.  14.  13.  81.]
 [  9.  13. 666.  13. 130.]
 [ 36.   6.  14. 453.  85.]
 [ 45.  23.  30.  22. 586.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.067 | Acc: 62.130% | Wgt Acc: 58.709% | Dur: 16.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   0.   2.  16.   6.]
 [  1.  40.  10.   3.  13.]
 [  3.  17.  38.   4.  15.]
 [ 12.   4.  12.  51.  10.]
 [ 22.  17.  13.  12. 136.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.496 | Acc: 80.125% | Wgt Acc: 83.855%
	I - Batch: 100 | Loss: 0.473 | Acc: 81.438% | Wgt Acc: 84.860%
	I - Batch: 150 | Loss: 0.484 | Acc: 80.583% | Wgt Acc: 84.199%
	I - Batch: 200 | Loss: 0.490 | Acc: 80.281% | Wgt Acc: 83.909%
I - num batch: 222
I - Train -- Loss: 0.490 | Acc: 80.039% | Wgt Acc: 83.669% | LR: 6.250000e-04 | Dur: 133.10s
I - Confusion Matrix: [row->prediction - col->label]
[[596.   3.  11.  53. 102.]
 [  4. 529.  17.  10.  71.]
 [ 15.  15. 662.   7. 136.]
 [ 40.   4.  10. 441.  80.]
 [ 42.  27.  34.  27. 611.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.084 | Acc: 59.566% | Wgt Acc: 57.747% | Dur: 14.70s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   0.   3.   7.   7.]
 [  3.  39.  12.   3.  19.]
 [  7.  24.  46.  11.  27.]
 [ 18.   2.   2.  51.   7.]
 [ 14.  13.  12.  14. 120.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.391 | Acc: 84.500% | Wgt Acc: 88.338%
	I - Batch: 100 | Loss: 0.425 | Acc: 82.500% | Wgt Acc: 86.523%
	I - Batch: 150 | Loss: 0.430 | Acc: 82.250% | Wgt Acc: 86.278%
	I - Batch: 200 | Loss: 0.430 | Acc: 82.188% | Wgt Acc: 86.261%
I - num batch: 208
I - Train -- Loss: 0.430 | Acc: 82.001% | Wgt Acc: 86.102% | LR: 6.250000e-04 | Dur: 125.78s
I - Confusion Matrix: [row->prediction - col->label]
[[620.   6.  11.  47.  84.]
 [  2. 540.   9.   7.  64.]
 [  6.  15. 681.   6. 128.]
 [ 45.   4.   8. 455.  72.]
 [ 24.  13.  25.  23. 433.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.103 | Acc: 60.158% | Wgt Acc: 60.527% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   1.   3.  21.  14.]
 [  0.  39.   7.   2.   8.]
 [  5.  23.  42.   5.  39.]
 [  9.   3.   8.  52.  14.]
 [  7.  12.  15.   6. 105.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.484 | Acc: 80.250% | Wgt Acc: 84.462%
	I - Batch: 100 | Loss: 0.452 | Acc: 81.250% | Wgt Acc: 85.561%
	I - Batch: 150 | Loss: 0.460 | Acc: 81.250% | Wgt Acc: 85.453%
	I - Batch: 200 | Loss: 0.460 | Acc: 81.375% | Wgt Acc: 85.353%
I - num batch: 222
I - Train -- Loss: 0.462 | Acc: 81.252% | Wgt Acc: 85.128% | LR: 6.250000e-04 | Dur: 134.13s
I - Confusion Matrix: [row->prediction - col->label]
[[611.   2.   6.  30. 110.]
 [  8. 537.  10.  11.  69.]
 [ 10.  18. 668.  15. 120.]
 [ 33.   2.   9. 455.  90.]
 [ 35.  19.  41.  27. 611.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 62.722% | Wgt Acc: 59.962% | Dur: 16.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   0.   4.  17.   9.]
 [  0.  35.   5.   1.   4.]
 [  0.  20.  37.   5.  22.]
 [ 15.   4.  10.  53.  13.]
 [ 12.  19.  19.  10. 132.]]

I - Local maximum validation set accuracy:  62.72

I - Validation set results: 
[14-1-1-1.33][50-3-4-1.27][124-2-2-1.77][127-0-0-4.62][443-2-2-2.15][567-0-0-2.02][573-1-1-2.18][615-0-0-3.65][695-1-2-1.63][722-3-3-2.68]
[826-0-0-4.39][878-0-0-2.67][1103-0-4-1.70][1212-3-3-1.72][1368-0-0-4.01][2181-2-3-2.07][2476-2-4-1.13][2721-2-2-2.53][2818-1-1-0.69][2886-2-1-1.18]
[3231-2-2-4.99][3333-2-1-1.71][3482-2-2-2.77][3536-3-3-1.88][3625-1-1-3.98][3909-0-0-2.80][4035-0-0-3.09][4140-0-0-2.28][4214-1-3-2.40][4346-1-4-2.23]
[4581-2-1-1.84][4708-3-4-1.31][4838-3-4-0.66][4845-1-4-0.81][4868-0-0-3.80][4939-0-4-0.10][4984-2-2-2.93][5078-1-2-2.32][5396-0-0-5.59][5479-1-1-4.42]
[5717-0-0-2.07][5843-1-1-1.86][5949-3-3-4.44][5987-2-4-3.41][6014-3-3-1.72][6033-3-3-1.95][6313-0-3-3.40][6421-3-3-3.12][6500-1-4-0.27][6583-3-3-2.62]
[6683-3-0-0.85][6825-2-3-1.44][6998-3-3-2.33][7049-3-3-2.05][7517-1-1-1.71][7521-1-1-1.71][7528-1-2-2.90][7949-1-2-2.43][8135-1-4-1.69][8185-3-0-4.00]
[8269-3-2-1.72][8273-3-3-3.99][8543-3-0-4.58][8666-1-1-4.46][8672-0-0-4.67][8903-1-2-2.81][9001-2-4-2.06][9036-2-2-5.37][9281-3-4-0.99][9300-2-2-4.01]
[9571-0-3-2.04][9617-1-1-3.16][9644-2-2-0.92][9705-2-4-1.49][9801-0-3-1.24][9803-3-3-2.31][9865-3-3-4.89][9896-2-4-2.20][10314-1-4-0.72][10337-3-3-5.67]
[10403-0-4-1.13][10653-2-4-1.41][10704-2-2-3.16][10719-1-1-2.06][10727-1-2-2.38][10836-0-0-6.15][10969-2-3-3.33][11042-0-0-2.41][11088-1-2-2.20][11322-0-0-4.93]
[11398-2-4-2.88][11499-0-0-3.07][11502-3-3-1.55][11512-3-3-3.90][11608-1-2-2.40][11610-0-0-2.35][11692-0-0-2.48][11905-0-0-2.80][11993-1-1-3.65][12002-2-0-4.42]
[12052-0-0-4.13][12201-0-3-3.05][12235-2-2-1.97][12320-1-4-3.02][12377-2-4-2.53][12398-2-3-1.89][12503-1-4-2.28][12617-0-4-0.70][12685-3-3-1.03][12738-2-4-0.65]
[12742-2-2-3.67][12823-0-3-2.60][13110-1-1-2.01][13240-3-3-2.62][13253-1-4-1.26][13273-0-0-4.98][13634-1-3-1.80][13763-2-2-0.81][13905-3-0--0.16][14060-2-1-2.30]
[14065-3-3-1.70][14147-3-3-1.90][14595-2-2-2.43][14687-2-2-2.62][14788-2-2-1.80][14869-1-1-2.85][14872-3-4-1.31][14877-1-1-2.11][14927-0-3-1.54][15066-0-0-7.55]
[15175-1-4-0.93][15178-2-4-2.35][15375-3-0-2.58][15389-3-0-2.16][15568-2-4-1.97][15675-3-3-4.74][15869-1-2-1.80][16207-3-1-1.61][16236-0-0-1.35][16302-3-0-2.81]
[16331-2-2-6.20][16381-0-0-1.64][16488-1-1-1.39][16495-0-0-3.24][16650-0-0-5.08][16719-1-4-1.16][16801-0-0-6.83][16828-0-0-4.06][17137-3-3-0.94][17245-1-2-2.77]
[17278-3-4-1.10][17282-0-0-0.66][17311-2-2-3.01][17336-2-2-1.14][17608-3-3-5.04][17627-0-4-1.31][17877-3-4-2.13][17924-1-4-1.02][17984-3-3-3.24][18211-0-3-2.44]
[18276-3-3-1.48][18287-1-1-0.36][18394-0-0-4.00][18428-0-0-5.22][18442-0-3-2.29][18478-3-0-2.28][18607-0-0-1.65][18616-0-4-0.71][18663-0-0-3.58][18718-0-0-3.92]
[18766-2-2-2.65][18824-2-2-1.51][18890-3-3-1.48][18930-3-2-1.20][18938-3-3-1.51][19817-1-1-3.60][19839-0-4-1.83][19930-3-3-2.91][19944-0-4-3.04][20036-2-2-5.86]
[20101-3-3-1.87][20474-1-1-1.47][20547-3-0-1.97][20929-2-2-3.47][21245-1-1-0.76][21257-3-3-1.79][21293-1-2-4.56][21316-1-1-3.59][21384-1-4-1.89][21448-1-1-4.59]
[21483-0-0-3.64][21487-2-2-3.32][21714-0-3-0.93][21943-3-2-1.40][21947-0-0-2.79][21948-0-0-5.90][21965-2-2-3.03][21998-1-1-3.13][22025-0-4-1.89][22228-3-3-3.41]
[22446-1-1-5.85][22494-3-3-2.54][22757-0-0-5.07][22811-3-3-3.26][22976-3-2-1.47][22985-3-3-3.22][23014-0-3-2.89][23112-1-1-3.53][23144-3-3-3.55][23168-2-0-1.57]
[23219-0-0-1.81][23363-3-3-4.69][23470-0-0-2.40][23486-2-2-2.84][23497-0-3-5.40][23516-0-0-4.45][23690-1-4-1.17][23921-2-4-1.41][23936-1-2-1.88][24040-3-4-1.76]
[24111-1-4-2.77][24182-0-0-5.08][24238-3-0-1.60][24290-2-0-3.28][24345-0-0-1.37][24364-1-1-1.70][24427-3-0-2.03][24477-2-2-3.44][24495-2-4-0.48][24893-2-2-2.50]
[25012-1-4-1.03][25121-2-2-3.53][25165-3-3-1.00][25183-0-0-1.65][25297-3-3-3.67][25398-0-0-3.25][25574-2-4-0.63][25644-1-2-3.53][25718-1-2-1.40][25774-2-4-1.31]
[26032-3-0-3.04][26051-3-3-4.39][26120-0-4-2.03][26321-1-1-3.12][26732-1-1-1.29][26784-3-3-4.64][26827-3-3-2.50][26833-0-0-4.30][26838-2-3-1.03][26860-1-4-2.25]
[26948-0-0-3.52][27049-3-0-3.37][27098-1-4-1.04][27526-0-0-2.17][27639-3-3-3.69][27698-3-3-3.37][27772-0-0-2.73][27890-1-1-3.83][28040-0-3-1.49][28503-2-2-2.99]
[28577-1-1-3.86][28959-0-0-5.17][29198-3-3-2.57][29777-0-0-6.83][29877-2-3-1.65][30035-1-2-2.62][30098-0-0-2.57][30326-1-1-5.82][30572-2-2-3.39][30716-0-4-3.98]
[30806-2-3-1.12][30906-1-1-3.35][31007-0-0-2.11][31181-3-3-1.62][31238-0-3-2.63][31347-0-0-3.74][31422-2-2-1.77][31429-3-4-0.17][31431-0-0-3.10][31432-1-1-3.87]
[31477-0-0-2.98][31524-1-2-1.34][31597-1-2-3.49][31619-1-2-1.87][31701-0-0-3.32][31755-0-0-2.19][31854-3-3-2.66][32074-1-3-1.58][32078-3-3-3.86][32111-1-1-4.10]
[32127-1-2-1.91][32140-3-3-3.87][32263-2-4-1.52][32365-0-0-3.65][32411-2-0-5.08][32429-3-0-2.76][32473-3-0-1.03][32574-3-3-4.84][32584-0-0-2.28][32622-0-4-1.40]
[32858-3-0-2.15][32969-3-0-3.88][33016-2-2-5.27][33031-1-3-1.76][33035-2-2-3.33][33133-2-1-1.26][33173-2-2-1.17][33175-3-4-2.84][33306-3-3-2.75][33309-2-3-1.95]
[33474-0-3-0.80][33478-2-3-0.53][33618-1-1-4.10][33712-0-3-2.70][33782-2-4-2.19][33914-3-3-1.74][34076-3-3-1.91][34112-2-2-4.00][34138-2-3-1.40][34239-1-4-0.56]
[34364-2-2-4.08][34617-1-4-1.51][34751-3-3-3.85][34783-2-4-2.58][35015-3-2-1.91][35018-1-2-1.52][35288-2-4-0.33][0-4-4-3.65][1-4-4-3.20][2-4-4-2.23]
[3-4-4-1.73][4-4-2-1.79][5-4-4-0.53][6-4-4-2.16][7-4-0-2.86][8-4-4-2.03][9-4-4-1.39][10-4-4-3.09][11-4-4-3.50][12-4-2-0.93]
[14-4-3-2.84][15-4-3-2.66][16-4-4-2.50][17-4-3-0.99][18-4-4-2.69][19-4-3-1.99][20-4-4-1.10][21-4-4-1.19][22-4-4-2.75][23-4-4-2.00]
[24-4-4-5.46][25-4-3-2.26][26-4-4-1.07][27-4-2-2.42][28-4-4-3.75][29-4-2-1.35][30-4-2-1.04][31-4-4-1.22][32-4-4-3.45][33-4-3-1.89]
[34-4-4-1.47][35-4-4-1.41][37-4-4-2.05][39-4-0-3.56][40-4-4-1.10][41-4-1-0.76][42-4-4-1.38][43-4-2-1.70][45-4-4-2.12][46-4-4-2.93]
[47-4-4-3.34][48-4-4-1.03][51-4-4-2.95][52-4-4-2.22][53-4-2-1.50][54-4-4-1.93][55-4-4-1.27][56-4-4-1.02][57-4-0-2.37][58-4-2-3.17]
[59-4-4-2.68][60-4-4-0.68][61-4-4-3.33][62-4-2-2.65][63-4-2-4.23][64-4-4-1.86][65-4-4-3.98][66-4-4-3.62][67-4-4-1.77][68-4-3-1.94]
[69-4-0-1.38][70-4-4-2.11][72-4-4-1.16][73-4-1-1.62][74-4-2-2.16][75-4-2-2.53][77-4-4-4.56][78-4-2-1.80][79-4-4-2.28][80-4-4-1.15]
[81-4-4-3.26][82-4-4-0.55][83-4-4-1.72][84-4-4-4.11][85-4-4-2.81][86-4-4-2.06][87-4-4-5.12][88-4-4-1.47][89-4-2-2.10][90-4-4-0.86]
[91-4-4-1.33][92-4-2-1.26][93-4-0-1.11][94-4-4-2.66][95-4-4-1.11][96-4-4-2.64][97-4-4-2.68][98-4-4-2.90][99-4-4-1.63][100-4-4-1.03]
[101-4-4-4.96][102-4-1-2.02][103-4-4-0.49][104-4-4-2.26][105-4-4-2.41][106-4-4-1.76][107-4-4-1.65][108-4-4-2.22][109-4-4-1.76][110-4-4-1.45]
[111-4-0-3.39][112-4-4-0.91][113-4-4-1.67][114-4-3-1.99][115-4-4-1.83][116-4-4-1.59][117-4-4-3.79][119-4-4-2.64][121-4-2-1.85][122-4-4-1.99]
[124-4-4-0.77][125-4-4-3.37][126-4-4-2.94][127-4-4-2.64][128-4-4-0.94][129-4-4-1.49][130-4-4-1.57][131-4-2-2.44][132-4-4-2.38][133-4-4-4.05]
[135-4-2-1.03][136-4-4-0.90][137-4-4-1.39][138-4-4-1.98][139-4-4-1.98][140-4-4-1.24][141-4-3-1.89][142-4-4-3.42][143-4-4-3.09][144-4-4-4.48]
[145-4-4-3.05][148-4-0-3.81][149-4-4-2.10][150-4-4-2.53][151-4-4-2.73][152-4-4-1.57][153-4-4-3.31][154-4-4-4.24][155-4-4-3.59][156-4-3-1.79]
[157-4-2-2.50][158-4-4-2.28][160-4-4-1.37][161-4-2-1.84][162-4-4-0.93][164-4-4-1.80][165-4-4-2.04][167-4-4-1.98][168-4-4-2.67][170-4-4-1.99]
[171-4-4-2.87][172-4-4-3.15][173-4-4-2.37][174-4-0-2.60][175-4-4-2.75][177-4-4-2.27][178-4-4-2.01][179-4-4-1.34][180-4-4-3.08][181-4-3-2.05]
[182-4-3-3.08][183-4-4-2.96][184-4-4-2.29][186-4-0-0.82][187-4-2-1.33][188-4-4-2.66][189-4-4-2.76][190-4-1-1.77][191-4-4-2.69][192-4-4-2.10]
[193-4-2-2.29][194-4-3-1.60][195-4-4-1.69][196-4-4-1.04][197-4-4-2.29][198-4-4-5.53][199-4-4-2.44]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.424 | Acc: 82.125% | Wgt Acc: 85.660%
	I - Batch: 100 | Loss: 0.411 | Acc: 83.062% | Wgt Acc: 86.668%
	I - Batch: 150 | Loss: 0.439 | Acc: 82.042% | Wgt Acc: 85.444%
	I - Batch: 200 | Loss: 0.454 | Acc: 81.312% | Wgt Acc: 84.895%
I - num batch: 222
I - Train -- Loss: 0.454 | Acc: 81.421% | Wgt Acc: 84.857% | LR: 6.250000e-04 | Dur: 136.51s
I - Confusion Matrix: [row->prediction - col->label]
[[608.   3.   4.  45.  92.]
 [  3. 540.  15.   8.  80.]
 [  7.  13. 660.  11. 125.]
 [ 36.   3.  12. 445.  68.]
 [ 43.  19.  43.  29. 635.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.098 | Acc: 58.580% | Wgt Acc: 58.301% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   4.  19.  11.]
 [  2.  40.  11.   2.  17.]
 [  4.  18.  41.   6.  38.]
 [ 14.   4.   7.  50.   7.]
 [  9.  14.  12.   9. 107.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.436 | Acc: 82.875% | Wgt Acc: 85.977%
	I - Batch: 100 | Loss: 0.437 | Acc: 82.500% | Wgt Acc: 86.030%
	I - Batch: 150 | Loss: 0.454 | Acc: 81.458% | Wgt Acc: 85.302%
	I - Batch: 200 | Loss: 0.451 | Acc: 81.875% | Wgt Acc: 85.692%
I - num batch: 222
I - Train -- Loss: 0.458 | Acc: 81.703% | Wgt Acc: 85.398% | LR: 6.250000e-04 | Dur: 136.76s
I - Confusion Matrix: [row->prediction - col->label]
[[607.   8.   6.  37. 121.]
 [ 12. 534.  12.   8.  56.]
 [  5.  16. 674.   5. 131.]
 [ 41.   6.   6. 458.  67.]
 [ 32.  14.  36.  30. 625.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.157 | Acc: 59.763% | Wgt Acc: 59.106% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   3.  18.  18.]
 [  2.  34.   9.   1.  12.]
 [  1.  24.  36.   6.  29.]
 [ 13.   4.   8.  55.   9.]
 [  6.  13.  19.   6. 112.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.461 | Acc: 82.500% | Wgt Acc: 85.150%
	I - Batch: 100 | Loss: 0.458 | Acc: 81.875% | Wgt Acc: 85.214%
	I - Batch: 150 | Loss: 0.449 | Acc: 81.750% | Wgt Acc: 85.292%
	I - Batch: 200 | Loss: 0.446 | Acc: 81.750% | Wgt Acc: 85.502%
I - num batch: 222
I - Train -- Loss: 0.443 | Acc: 81.787% | Wgt Acc: 85.570% | LR: 6.250000e-04 | Dur: 137.00s
I - Confusion Matrix: [row->prediction - col->label]
[[601.   6.  13.  30. 100.]
 [  7. 539.  19.   7.  67.]
 [  9.  14. 662.   7. 132.]
 [ 37.   3.   7. 475.  77.]
 [ 43.  16.  33.  19. 624.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.070 | Acc: 62.130% | Wgt Acc: 60.453% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   0.   3.  12.  10.]
 [  1.  36.   8.   3.  10.]
 [  3.  25.  44.   9.  28.]
 [ 12.   2.   4.  52.   9.]
 [ 12.  15.  16.  10. 123.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.426 | Acc: 84.375% | Wgt Acc: 87.707%
	I - Batch: 100 | Loss: 0.415 | Acc: 83.500% | Wgt Acc: 87.433%
	I - Batch: 150 | Loss: 0.417 | Acc: 83.000% | Wgt Acc: 87.041%
	I - Batch: 200 | Loss: 0.441 | Acc: 81.750% | Wgt Acc: 85.783%
I - num batch: 222
I - Train -- Loss: 0.447 | Acc: 81.365% | Wgt Acc: 85.390% | LR: 6.250000e-04 | Dur: 137.19s
I - Confusion Matrix: [row->prediction - col->label]
[[610.   6.   6.  34. 120.]
 [  5. 534.   7.  10.  75.]
 [ 11.  13. 666.   7. 124.]
 [ 29.   4.  14. 470.  75.]
 [ 42.  21.  41.  17. 606.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.104 | Acc: 58.974% | Wgt Acc: 57.382% | Dur: 14.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 44.   0.   1.   5.   5.]
 [  0.  34.   7.   1.   6.]
 [  5.  23.  38.   4.  35.]
 [ 23.   4.  12.  64.  15.]
 [ 16.  17.  17.  12. 119.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.411 | Acc: 82.125% | Wgt Acc: 87.077%
	I - Batch: 100 | Loss: 0.419 | Acc: 82.875% | Wgt Acc: 87.070%
	I - Batch: 150 | Loss: 0.431 | Acc: 82.417% | Wgt Acc: 86.226%
	I - Batch: 200 | Loss: 0.434 | Acc: 82.250% | Wgt Acc: 86.023%
I - num batch: 222
I - Train -- Loss: 0.440 | Acc: 82.239% | Wgt Acc: 85.852% | LR: 6.250000e-04 | Dur: 135.95s
I - Confusion Matrix: [row->prediction - col->label]
[[614.   2.   6.  37.  97.]
 [  5. 530.  10.   8.  77.]
 [  7.  14. 672.   7. 125.]
 [ 35.   2.   9. 466.  66.]
 [ 36.  30.  37.  20. 635.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.106 | Acc: 59.172% | Wgt Acc: 59.701% | Dur: 14.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   3.   1.  13.  14.]
 [  4.  43.  12.   4.  19.]
 [  3.  15.  45.   7.  28.]
 [ 20.   4.   9.  55.  16.]
 [  7.  13.   8.   7. 103.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.403 | Acc: 84.750% | Wgt Acc: 88.812%
	I - Batch: 100 | Loss: 0.405 | Acc: 84.688% | Wgt Acc: 88.559%
	I - Batch: 150 | Loss: 0.415 | Acc: 84.250% | Wgt Acc: 88.171%
	I - Batch: 200 | Loss: 0.415 | Acc: 84.000% | Wgt Acc: 87.778%
I - num batch: 222
I - Train -- Loss: 0.418 | Acc: 83.902% | Wgt Acc: 87.610% | LR: 6.250000e-04 | Dur: 134.75s
I - Confusion Matrix: [row->prediction - col->label]
[[622.   2.   7.  35. 101.]
 [  7. 546.   4.   3.  55.]
 [ 10.   9. 686.   7. 107.]
 [ 29.   5.   7. 475.  90.]
 [ 29.  16.  30.  18. 647.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.094 | Acc: 62.722% | Wgt Acc: 62.992% | Dur: 14.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   4.  14.  16.]
 [  0.  40.  11.   3.  14.]
 [  1.  18.  42.   2.  21.]
 [ 12.   8.   8.  59.  18.]
 [  9.   8.  10.   8. 111.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.437 | Acc: 82.125% | Wgt Acc: 85.567%
	I - Batch: 100 | Loss: 0.417 | Acc: 82.625% | Wgt Acc: 86.226%
	I - Batch: 150 | Loss: 0.428 | Acc: 82.500% | Wgt Acc: 85.936%
	I - Batch: 200 | Loss: 0.433 | Acc: 82.469% | Wgt Acc: 86.095%
I - num batch: 222
I - Train -- Loss: 0.434 | Acc: 82.464% | Wgt Acc: 85.980% | LR: 6.250000e-04 | Dur: 137.99s
I - Confusion Matrix: [row->prediction - col->label]
[[607.   6.   8.  32. 108.]
 [  7. 536.   9.  11.  60.]
 [  7.  13. 671.   9. 115.]
 [ 33.   5.   6. 468.  74.]
 [ 43.  18.  40.  18. 643.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 60.750% | Wgt Acc: 60.673% | Dur: 14.88s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   1.   4.  14.  12.]
 [  0.  43.  15.   4.  19.]
 [  6.  22.  42.   5.  28.]
 [ 18.   3.   4.  55.  11.]
 [  6.   9.  10.   8. 110.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.385 | Acc: 84.250% | Wgt Acc: 87.976%
	I - Batch: 100 | Loss: 0.383 | Acc: 84.062% | Wgt Acc: 87.382%
	I - Batch: 150 | Loss: 0.397 | Acc: 83.875% | Wgt Acc: 87.194%
	I - Batch: 200 | Loss: 0.396 | Acc: 83.938% | Wgt Acc: 87.345%
I - num batch: 222
I - Train -- Loss: 0.397 | Acc: 83.930% | Wgt Acc: 87.204% | LR: 6.250000e-04 | Dur: 135.39s
I - Confusion Matrix: [row->prediction - col->label]
[[621.   1.   3.  39.  84.]
 [  3. 542.  10.   6.  59.]
 [  9.  13. 679.   6. 113.]
 [ 24.   4.  10. 466.  75.]
 [ 40.  18.  32.  21. 669.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.121 | Acc: 62.327% | Wgt Acc: 63.316% | Dur: 17.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   5.   5.   9.  22.]
 [  0.  38.   7.   2.   9.]
 [  4.  14.  44.   3.  26.]
 [ 17.   8.  12.  66.  17.]
 [  5.  13.   7.   6. 106.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.420 | Acc: 84.125% | Wgt Acc: 86.967%
	I - Batch: 100 | Loss: 0.401 | Acc: 84.688% | Wgt Acc: 87.866%
	I - Batch: 150 | Loss: 0.402 | Acc: 84.250% | Wgt Acc: 87.824%
	I - Batch: 200 | Loss: 0.408 | Acc: 83.906% | Wgt Acc: 87.656%
I - num batch: 222
I - Train -- Loss: 0.409 | Acc: 83.705% | Wgt Acc: 87.377% | LR: 6.250000e-04 | Dur: 137.08s
I - Confusion Matrix: [row->prediction - col->label]
[[611.   2.  10.  30. 113.]
 [  4. 548.   9.   2.  57.]
 [  8.   9. 683.   6. 113.]
 [ 22.   2.  10. 479.  69.]
 [ 52.  17.  22.  21. 648.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.155 | Acc: 59.566% | Wgt Acc: 58.959% | Dur: 18.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   3.   5.  16.  14.]
 [  0.  40.   5.   6.  10.]
 [  4.  23.  46.   7.  35.]
 [ 17.   3.   7.  50.  10.]
 [ 12.   9.  12.   7. 111.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.389 | Acc: 84.000% | Wgt Acc: 86.842%
	I - Batch: 100 | Loss: 0.376 | Acc: 84.688% | Wgt Acc: 87.698%
	I - Batch: 150 | Loss: 0.374 | Acc: 84.708% | Wgt Acc: 88.048%
	I - Batch: 200 | Loss: 0.381 | Acc: 84.188% | Wgt Acc: 87.689%
I - num batch: 222
I - Train -- Loss: 0.385 | Acc: 83.986% | Wgt Acc: 87.461% | LR: 6.250000e-04 | Dur: 139.48s
I - Confusion Matrix: [row->prediction - col->label]
[[629.   7.   2.  29. 102.]
 [  7. 531.  12.   4.  50.]
 [  6.  15. 679.   8. 119.]
 [ 23.   3.   5. 480.  69.]
 [ 32.  22.  36.  17. 660.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.083 | Acc: 61.933% | Wgt Acc: 60.150% | Dur: 17.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   1.   2.   8.   5.]
 [  1.  44.   9.   4.  18.]
 [  4.  17.  38.   4.  22.]
 [ 17.   5.   7.  58.  10.]
 [ 17.  11.  19.  12. 125.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.368 | Acc: 85.125% | Wgt Acc: 88.779%
	I - Batch: 100 | Loss: 0.380 | Acc: 84.875% | Wgt Acc: 88.417%
	I - Batch: 150 | Loss: 0.395 | Acc: 83.958% | Wgt Acc: 87.610%
	I - Batch: 200 | Loss: 0.394 | Acc: 83.875% | Wgt Acc: 87.636%
I - num batch: 222
I - Train -- Loss: 0.390 | Acc: 84.015% | Wgt Acc: 87.802% | LR: 6.250000e-04 | Dur: 136.44s
I - Confusion Matrix: [row->prediction - col->label]
[[623.   2.   6.  24.  99.]
 [  7. 547.   5.   9.  54.]
 [  5.  10. 682.   4. 126.]
 [ 29.   4.   5. 483.  76.]
 [ 33.  15.  36.  18. 645.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.091 | Acc: 60.750% | Wgt Acc: 60.318% | Dur: 14.80s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   2.   2.   5.   9.]
 [  2.  47.  16.   4.  20.]
 [  5.  15.  35.   6.  18.]
 [ 23.   6.  12.  62.  19.]
 [  8.   8.  10.   9. 114.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.352 | Acc: 86.125% | Wgt Acc: 89.943%
	I - Batch: 100 | Loss: 0.368 | Acc: 85.438% | Wgt Acc: 89.338%
	I - Batch: 150 | Loss: 0.366 | Acc: 85.375% | Wgt Acc: 89.305%
	I - Batch: 200 | Loss: 0.371 | Acc: 85.188% | Wgt Acc: 89.049%
I - num batch: 222
I - Train -- Loss: 0.370 | Acc: 85.142% | Wgt Acc: 88.924% | LR: 6.250000e-04 | Dur: 137.47s
I - Confusion Matrix: [row->prediction - col->label]
[[639.   3.   9.  24. 112.]
 [  3. 552.   5.   3.  44.]
 [  3.   9. 687.   5. 124.]
 [ 17.   1.   6. 486.  64.]
 [ 35.  13.  27.  20. 656.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.153 | Acc: 60.158% | Wgt Acc: 60.213% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   4.  11.  15.]
 [  2.  41.  12.   5.  15.]
 [  2.  17.  36.   3.  24.]
 [ 21.   8.  10.  62.  17.]
 [  6.   9.  13.   5. 109.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.365 | Acc: 84.250% | Wgt Acc: 87.776%
	I - Batch: 100 | Loss: 0.358 | Acc: 84.625% | Wgt Acc: 88.703%
	I - Batch: 150 | Loss: 0.362 | Acc: 84.875% | Wgt Acc: 88.814%
	I - Batch: 200 | Loss: 0.376 | Acc: 84.219% | Wgt Acc: 88.124%
I - num batch: 222
I - Train -- Loss: 0.371 | Acc: 84.522% | Wgt Acc: 88.406% | LR: 6.250000e-04 | Dur: 138.26s
I - Confusion Matrix: [row->prediction - col->label]
[[627.   4.   4.  27. 108.]
 [  6. 547.   4.   0.  66.]
 [  4.   5. 691.   8. 107.]
 [ 22.   3.   3. 488.  74.]
 [ 38.  19.  32.  15. 645.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.103 | Acc: 62.722% | Wgt Acc: 61.634% | Dur: 14.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   4.  11.  15.]
 [  0.  36.   7.   2.   4.]
 [  3.  12.  39.   3.  20.]
 [ 14.   8.   9.  61.  20.]
 [ 10.  20.  16.   9. 121.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.409 | Acc: 84.125% | Wgt Acc: 87.001%
	I - Batch: 100 | Loss: 0.379 | Acc: 84.438% | Wgt Acc: 87.852%
	I - Batch: 150 | Loss: 0.372 | Acc: 84.875% | Wgt Acc: 88.331%
	I - Batch: 200 | Loss: 0.379 | Acc: 84.594% | Wgt Acc: 88.247%
I - num batch: 222
I - Train -- Loss: 0.379 | Acc: 84.438% | Wgt Acc: 88.126% | LR: 6.250000e-04 | Dur: 134.19s
I - Confusion Matrix: [row->prediction - col->label]
[[625.   4.   3.  24.  94.]
 [  3. 541.  10.   6.  60.]
 [  5.  11. 682.   1. 112.]
 [ 25.   2.   4. 492.  79.]
 [ 39.  20.  35.  15. 655.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.163 | Acc: 62.525% | Wgt Acc: 62.261% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   4.  15.  17.]
 [  0.  39.   6.   1.  14.]
 [  1.  17.  39.   3.  18.]
 [ 18.   8.  12.  62.  16.]
 [  7.  10.  14.   5. 115.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.343 | Acc: 86.375% | Wgt Acc: 89.698%
	I - Batch: 100 | Loss: 0.360 | Acc: 85.125% | Wgt Acc: 88.520%
	I - Batch: 150 | Loss: 0.368 | Acc: 84.667% | Wgt Acc: 88.373%
	I - Batch: 200 | Loss: 0.380 | Acc: 84.281% | Wgt Acc: 87.976%
I - num batch: 222
I - Train -- Loss: 0.384 | Acc: 84.297% | Wgt Acc: 87.933% | LR: 6.250000e-04 | Dur: 134.16s
I - Confusion Matrix: [row->prediction - col->label]
[[622.   4.   5.  21. 107.]
 [  6. 541.   9.  10.  52.]
 [ 10.   8. 690.   4. 120.]
 [ 23.   3.   4. 482.  66.]
 [ 36.  22.  26.  21. 655.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.110 | Acc: 63.314% | Wgt Acc: 61.268% | Dur: 15.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   6.   5.  23.  15.]
 [  1.  44.  11.   2.   9.]
 [  5.  10.  39.   6.  25.]
 [  6.   2.   3.  44.   4.]
 [  9.  16.  17.  11. 127.]]

I - Local maximum validation set accuracy:  63.31

I - Validation set results: 
[14-1-2-2.34][50-3-1-2.51][124-2-4-1.01][127-0-0-6.31][443-2-2-3.97][567-0-0-1.91][573-1-1-2.35][615-0-0-4.75][695-1-1-1.62][722-3-0-2.80]
[826-0-0-5.28][878-0-0-3.65][1103-0-0-2.29][1212-3-4-0.60][1368-0-0-6.76][2181-2-2-1.22][2476-2-2-2.33][2721-2-2-3.09][2818-1-4-0.96][2886-2-1-2.53]
[3231-2-2-5.25][3333-2-1-0.80][3482-2-2-3.38][3536-3-3-1.89][3625-1-1-4.40][3909-0-0-4.33][4035-0-0-3.75][4140-0-0-3.61][4214-1-1-3.23][4346-1-4-3.73]
[4581-2-1-0.92][4708-3-3-1.15][4838-3-3-1.83][4845-1-4-1.10][4868-0-0-4.75][4939-0-2-2.18][4984-2-2-1.35][5078-1-0-1.77][5396-0-0-6.28][5479-1-1-5.80]
[5717-0-0-2.60][5843-1-1-4.16][5949-3-3-2.39][5987-2-4-3.20][6014-3-3-1.22][6033-3-0-2.21][6313-0-0-3.59][6421-3-3-3.95][6500-1-1-0.74][6583-3-3-3.02]
[6683-3-0-1.26][6825-2-1-2.66][6998-3-0-0.27][7049-3-2-0.96][7517-1-1-4.48][7521-1-1-1.09][7528-1-2-2.33][7949-1-2-3.26][8135-1-0-2.01][8185-3-0-2.93]
[8269-3-4-2.06][8273-3-3-4.04][8543-3-0-4.52][8666-1-1-6.70][8672-0-0-5.57][8903-1-2-3.34][9001-2-2-3.39][9036-2-2-2.86][9281-3-4-0.74][9300-2-2-5.38]
[9571-0-4-0.87][9617-1-1-3.34][9644-2-1-1.26][9705-2-4-0.96][9801-0-0-1.21][9803-3-3-1.55][9865-3-3-5.59][9896-2-2-2.44][10314-1-0-2.23][10337-3-3-4.17]
[10403-0-1-0.96][10653-2-4-2.37][10704-2-2-4.70][10719-1-1-3.79][10727-1-1-1.95][10836-0-0-8.46][10969-2-3-0.32][11042-0-4-1.55][11088-1-1-6.38][11322-0-0-4.78]
[11398-2-2-3.19][11499-0-0-4.00][11502-3-3-1.17][11512-3-3-2.31][11608-1-1-4.43][11610-0-0-3.30][11692-0-0-3.25][11905-0-0-2.87][11993-1-1-3.45][12002-2-0-4.63]
[12052-0-0-5.37][12201-0-3-3.52][12235-2-4-1.26][12320-1-4-4.99][12377-2-4-2.18][12398-2-4-0.60][12503-1-4-2.05][12617-0-2-1.10][12685-3-2-0.95][12738-2-0-0.79]
[12742-2-2-3.21][12823-0-0-1.94][13110-1-1-3.64][13240-3-3-2.87][13253-1-4-1.33][13273-0-0-6.32][13634-1-1-1.12][13763-2-2-0.96][13905-3-2-0.93][14060-2-1-1.43]
[14065-3-3-0.97][14147-3-0-3.62][14595-2-2-3.03][14687-2-2-4.77][14788-2-2-3.25][14869-1-1-3.63][14872-3-4-1.59][14877-1-1-3.19][14927-0-0-4.21][15066-0-0-8.11]
[15175-1-1-1.91][15178-2-4-2.64][15375-3-0-3.44][15389-3-0-2.23][15568-2-4-1.38][15675-3-3-2.26][15869-1-2-4.45][16207-3-1-1.69][16236-0-2-1.14][16302-3-0-2.46]
[16331-2-2-9.08][16381-0-3-1.51][16488-1-1-7.20][16495-0-0-4.19][16650-0-0-5.63][16719-1-4-1.87][16801-0-0-7.29][16828-0-0-4.56][17137-3-3-1.01][17245-1-4-2.16]
[17278-3-0-1.90][17282-0-2-3.30][17311-2-2-3.19][17336-2-1-1.13][17608-3-3-5.34][17627-0-4-1.66][17877-3-4-2.53][17924-1-4-0.84][17984-3-3-4.64][18211-0-3-1.68]
[18276-3-0-1.89][18287-1-0-1.04][18394-0-0-4.74][18428-0-0-0.74][18442-0-3-2.12][18478-3-0-2.09][18607-0-0-2.02][18616-0-0-1.83][18663-0-0-5.48][18718-0-0-4.36]
[18766-2-2-2.09][18824-2-4-2.23][18890-3-3-2.01][18930-3-4-1.13][18938-3-3-0.51][19817-1-1-4.36][19839-0-0-1.67][19930-3-3-2.30][19944-0-4-2.98][20036-2-2-7.23]
[20101-3-3-1.99][20474-1-1-1.91][20547-3-0-2.11][20929-2-2-6.51][21245-1-1-2.16][21257-3-4-1.03][21293-1-2-5.88][21316-1-1-3.29][21384-1-4-1.49][21448-1-1-5.02]
[21483-0-0-5.00][21487-2-2-3.26][21714-0-0-1.29][21943-3-2-2.38][21947-0-0-2.64][21948-0-0-8.72][21965-2-2-2.24][21998-1-1-3.97][22025-0-4-1.84][22228-3-3-2.90]
[22446-1-1-5.09][22494-3-3-2.55][22757-0-0-6.15][22811-3-3-4.27][22976-3-2-1.16][22985-3-0-2.87][23014-0-0-3.03][23112-1-1-6.16][23144-3-3-4.41][23168-2-0-2.31]
[23219-0-0-2.47][23363-3-3-4.43][23470-0-0-1.62][23486-2-2-3.08][23497-0-3-5.46][23516-0-0-4.86][23690-1-3-1.54][23921-2-4-1.47][23936-1-2-3.29][24040-3-4-2.15]
[24111-1-4-3.14][24182-0-0-6.73][24238-3-3-1.56][24290-2-0-2.11][24345-0-0-3.35][24364-1-1-1.44][24427-3-0-1.68][24477-2-2-3.59][24495-2-1-2.70][24893-2-2-3.97]
[25012-1-4-1.78][25121-2-4-2.22][25165-3-4-0.51][25183-0-0-2.33][25297-3-3-3.81][25398-0-0-5.85][25574-2-2-1.65][25644-1-2-4.04][25718-1-0-0.47][25774-2-4-1.23]
[26032-3-3-0.63][26051-3-3-4.11][26120-0-4-4.37][26321-1-1-8.19][26732-1-1-3.90][26784-3-3-5.78][26827-3-0-1.68][26833-0-0-4.13][26838-2-3-0.48][26860-1-4-2.26]
[26948-0-0-3.72][27049-3-0-4.25][27098-1-0-1.59][27526-0-0-2.45][27639-3-3-4.32][27698-3-3-0.81][27772-0-0-5.20][27890-1-1-3.76][28040-0-0-1.48][28503-2-2-2.31]
[28577-1-1-2.12][28959-0-0-6.91][29198-3-3-1.85][29777-0-0-7.55][29877-2-2-1.09][30035-1-1-5.09][30098-0-0-3.38][30326-1-1-6.37][30572-2-2-4.63][30716-0-4-3.31]
[30806-2-4-1.14][30906-1-1-6.06][31007-0-0-2.51][31181-3-3-1.59][31238-0-0-2.46][31347-0-0-4.95][31422-2-2-2.16][31429-3-2-0.61][31431-0-0-1.82][31432-1-1-4.98]
[31477-0-0-3.58][31524-1-4-1.19][31597-1-2-3.54][31619-1-4-2.87][31701-0-0-3.67][31755-0-0-3.72][31854-3-3-2.62][32074-1-1-2.72][32078-3-3-2.60][32111-1-1-3.97]
[32127-1-1-1.46][32140-3-3-3.14][32263-2-4-1.80][32365-0-0-4.60][32411-2-0-5.48][32429-3-0-2.91][32473-3-0-1.98][32574-3-0-4.56][32584-0-2-0.96][32622-0-4-1.48]
[32858-3-0-2.87][32969-3-0-3.93][33016-2-2-2.35][33031-1-3-1.88][33035-2-2-4.50][33133-2-1-2.36][33173-2-2-2.15][33175-3-4-2.38][33306-3-3-3.16][33309-2-3-1.37]
[33474-0-3-0.20][33478-2-1--0.16][33618-1-1-4.95][33712-0-4-1.76][33782-2-4-2.73][33914-3-3-1.49][34076-3-4-0.94][34112-2-2-4.16][34138-2-2-0.59][34239-1-1-1.50]
[34364-2-2-4.53][34617-1-2-1.56][34751-3-3-4.01][34783-2-4-3.52][35015-3-3-1.76][35018-1-4-1.85][35288-2-1-1.45][0-4-4-2.96][1-4-4-2.81][2-4-0-2.97]
[3-4-4-1.45][4-4-2-1.92][5-4-4-0.23][6-4-4-4.83][7-4-2-1.26][8-4-4-1.89][9-4-0-2.64][10-4-4-4.91][11-4-4-4.42][12-4-4-1.20]
[14-4-4-1.78][15-4-3-2.29][16-4-4-3.09][17-4-4-1.76][18-4-4-3.91][19-4-3-1.61][20-4-0-0.56][21-4-2-1.80][22-4-4-3.15][23-4-4-1.31]
[24-4-4-5.66][25-4-4-1.28][26-4-4-1.10][27-4-2-3.59][28-4-4-4.55][29-4-2-1.05][30-4-0-1.68][31-4-4-2.27][32-4-4-3.12][33-4-2-4.05]
[34-4-4-1.97][35-4-4-1.84][37-4-4-3.14][39-4-0-3.99][40-4-0-1.79][41-4-2-2.62][42-4-4-1.80][43-4-4-2.72][45-4-1-2.09][46-4-2-4.25]
[47-4-4-3.72][48-4-4-1.37][51-4-4-2.97][52-4-4-2.79][53-4-4-0.93][54-4-4-2.92][55-4-4-3.28][56-4-1-3.52][57-4-0-3.65][58-4-2-3.90]
[59-4-4-2.13][60-4-4-3.09][61-4-4-3.82][62-4-2-3.16][63-4-4-2.77][64-4-2-2.50][65-4-4-5.06][66-4-4-3.91][67-4-0-1.01][68-4-4-1.34]
[69-4-0-0.70][70-4-4-1.74][72-4-4-1.95][73-4-1-2.31][74-4-2-2.78][75-4-0-1.34][77-4-4-5.84][78-4-2-1.72][79-4-4-2.05][80-4-4-1.10]
[81-4-4-2.86][82-4-4-1.76][83-4-4-1.43][84-4-4-4.37][85-4-4-2.97][86-4-4-2.85][87-4-4-5.46][88-4-4-3.84][89-4-4-1.27][90-4-4-2.83]
[91-4-4-1.24][92-4-4-1.08][93-4-4-1.66][94-4-4-2.77][95-4-4-1.54][96-4-4-3.19][97-4-4-2.81][98-4-4-2.72][99-4-4-3.08][100-4-1-2.37]
[101-4-4-5.40][102-4-4-2.13][103-4-2-0.74][104-4-4-3.11][105-4-4-2.60][106-4-4-2.33][107-4-4-3.74][108-4-4-2.19][109-4-4-2.24][110-4-1-1.17]
[111-4-0-3.99][112-4-4-1.04][113-4-4-0.84][114-4-2-0.82][115-4-4-2.67][116-4-4-1.84][117-4-1-2.28][119-4-2-2.23][121-4-4-2.23][122-4-4-2.49]
[124-4-4-2.06][125-4-4-3.88][126-4-4-5.90][127-4-4-1.04][128-4-0-1.12][129-4-4-1.90][130-4-4-2.31][131-4-2-1.70][132-4-4-2.38][133-4-4-4.54]
[135-4-2-1.60][136-4-1-1.52][137-4-4-2.19][138-4-4-2.39][139-4-2-3.73][140-4-4-1.40][141-4-4-0.74][142-4-4-3.83][143-4-4-4.17][144-4-4-4.72]
[145-4-4-2.98][148-4-0-4.08][149-4-4-2.33][150-4-4-3.34][151-4-4-2.64][152-4-4-1.74][153-4-4-3.74][154-4-4-6.11][155-4-4-4.14][156-4-3-2.38]
[157-4-2-4.71][158-4-4-3.11][160-4-4-1.52][161-4-2-1.59][162-4-1-0.79][164-4-4-1.59][165-4-4-1.41][167-4-4-2.46][168-4-4-2.74][170-4-4-3.68]
[171-4-4-3.25][172-4-4-3.77][173-4-4-3.28][174-4-0-2.77][175-4-4-3.05][177-4-4-2.25][178-4-4-2.03][179-4-4-2.24][180-4-4-4.02][181-4-3-2.71]
[182-4-4-2.61][183-4-4-2.16][184-4-4-2.17][186-4-4-1.48][187-4-2-2.49][188-4-4-2.35][189-4-4-2.29][190-4-1-0.81][191-4-4-3.21][192-4-4-1.59]
[193-4-2-3.76][194-4-0-2.35][195-4-2-1.78][196-4-2-2.39][197-4-4-2.66][198-4-4-6.45][199-4-4-2.55]
---------------------------
I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.401 | Acc: 83.625% | Wgt Acc: 87.608%
	I - Batch: 100 | Loss: 0.390 | Acc: 84.562% | Wgt Acc: 88.398%
	I - Batch: 150 | Loss: 0.374 | Acc: 85.125% | Wgt Acc: 88.751%
	I - Batch: 200 | Loss: 0.371 | Acc: 85.375% | Wgt Acc: 88.861%
I - num batch: 222
I - Train -- Loss: 0.371 | Acc: 85.227% | Wgt Acc: 88.647% | LR: 6.250000e-04 | Dur: 133.88s
I - Confusion Matrix: [row->prediction - col->label]
[[624.   1.   5.  20.  85.]
 [  5. 550.   9.   3.  60.]
 [  6.   4. 688.   9. 111.]
 [ 33.   4.   5. 485.  68.]
 [ 29.  19.  27.  21. 676.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.100 | Acc: 61.736% | Wgt Acc: 59.503% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   3.   2.  22.  13.]
 [  1.  39.   7.   2.   9.]
 [  4.  23.  44.   7.  28.]
 [ 12.   2.   4.  41.   5.]
 [  7.  11.  18.  14. 125.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.350 | Acc: 85.875% | Wgt Acc: 88.869%
	I - Batch: 100 | Loss: 0.363 | Acc: 85.188% | Wgt Acc: 88.655%
	I - Batch: 150 | Loss: 0.376 | Acc: 85.125% | Wgt Acc: 88.515%
	I - Batch: 200 | Loss: 0.379 | Acc: 85.000% | Wgt Acc: 88.416%
I - num batch: 222
I - Train -- Loss: 0.379 | Acc: 84.860% | Wgt Acc: 88.272% | LR: 6.250000e-04 | Dur: 136.26s
I - Confusion Matrix: [row->prediction - col->label]
[[628.   3.   4.  24. 101.]
 [  4. 548.   9.   8.  54.]
 [ 10.   8. 684.   5. 114.]
 [ 18.   2.   8. 478.  59.]
 [ 37.  17.  29.  23. 672.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.086 | Acc: 64.300% | Wgt Acc: 61.456% | Dur: 15.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   3.  16.  19.]
 [  1.  37.   6.   0.   5.]
 [  2.  16.  39.   3.  14.]
 [  9.   4.   8.  51.   7.]
 [ 12.  17.  19.  16. 135.]]

I - Local maximum validation set accuracy:  64.30

I - Validation set results: 
[14-1-2-3.07][50-3-4-1.22][124-2-3-1.58][127-0-0-5.50][443-2-2-4.11][567-0-0-5.09][573-1-1-2.82][615-0-0-5.83][695-1-2-0.86][722-3-3-6.14]
[826-0-0-4.86][878-0-0-5.00][1103-0-4-2.18][1212-3-4-2.21][1368-0-0-5.03][2181-2-2-1.89][2476-2-2-1.54][2721-2-2-2.88][2818-1-1-1.23][2886-2-1-2.20]
[3231-2-2-6.18][3333-2-2-1.59][3482-2-4-2.88][3536-3-3-2.56][3625-1-1-3.97][3909-0-0-3.90][4035-0-0-4.28][4140-0-0-3.42][4214-1-4-0.64][4346-1-4-2.35]
[4581-2-1-1.64][4708-3-4-2.29][4838-3-3-1.15][4845-1-4-1.07][4868-0-0-5.27][4939-0-4-0.19][4984-2-3-2.80][5078-1-0-1.82][5396-0-0-7.20][5479-1-1-3.54]
[5717-0-0-4.27][5843-1-1-4.30][5949-3-3-4.78][5987-2-4-3.13][6014-3-3-1.57][6033-3-3-2.36][6313-0-3-3.01][6421-3-3-2.37][6500-1-2-1.35][6583-3-3-4.90]
[6683-3-0-1.87][6825-2-3-1.90][6998-3-3-0.76][7049-3-3-1.35][7517-1-1-2.08][7521-1-1-0.44][7528-1-0-1.93][7949-1-2-2.24][8135-1-0-2.66][8185-3-0-4.39]
[8269-3-4-1.70][8273-3-3-4.95][8543-3-0-4.59][8666-1-1-2.97][8672-0-0-5.11][8903-1-2-2.02][9001-2-2-2.58][9036-2-2-6.54][9281-3-4-2.66][9300-2-2-4.97]
[9571-0-3-1.97][9617-1-1-2.33][9644-2-1-1.08][9705-2-4-1.15][9801-0-4-2.34][9803-3-3-2.68][9865-3-3-5.18][9896-2-2-2.63][10314-1-0-1.51][10337-3-3-4.89]
[10403-0-4-1.66][10653-2-4-2.22][10704-2-2-4.19][10719-1-1-5.32][10727-1-1-1.15][10836-0-0-9.42][10969-2-2-2.09][11042-0-0-2.91][11088-1-1-6.15][11322-0-0-6.75]
[11398-2-4-2.78][11499-0-0-4.88][11502-3-3-1.77][11512-3-3-3.25][11608-1-2-5.80][11610-0-0-2.01][11692-0-0-2.60][11905-0-0-4.72][11993-1-1-2.21][12002-2-2-1.67]
[12052-0-0-4.75][12201-0-3-3.90][12235-2-4-1.57][12320-1-4-3.41][12377-2-1-2.34][12398-2-2-1.77][12503-1-2-2.62][12617-0-4-1.23][12685-3-4-0.89][12738-2-4-1.27]
[12742-2-2-3.90][12823-0-3-3.17][13110-1-1-4.86][13240-3-3-3.89][13253-1-4-1.28][13273-0-0-6.82][13634-1-3-1.08][13763-2-2-2.87][13905-3-4-0.21][14060-2-1-1.94]
[14065-3-3-2.32][14147-3-0-2.11][14595-2-2-2.48][14687-2-2-4.96][14788-2-2-3.03][14869-1-1-3.46][14872-3-4-1.03][14877-1-1-4.38][14927-0-0-3.04][15066-0-0-7.89]
[15175-1-1-2.53][15178-2-4-2.80][15375-3-0-3.05][15389-3-0-2.60][15568-2-4-1.87][15675-3-3-4.14][15869-1-2-2.46][16207-3-0-1.69][16236-0-0-1.72][16302-3-0-2.42]
[16331-2-2-8.76][16381-0-0-2.42][16488-1-1-2.28][16495-0-0-4.89][16650-0-0-8.64][16719-1-4-1.40][16801-0-0-6.26][16828-0-0-3.35][17137-3-3-1.63][17245-1-4-0.37]
[17278-3-4-2.60][17282-0-2-2.31][17311-2-2-2.57][17336-2-4-0.76][17608-3-3-6.34][17627-0-4-1.02][17877-3-4-2.39][17924-1-2-0.96][17984-3-3-5.73][18211-0-3-2.35]
[18276-3-0-4.07][18287-1-1-0.36][18394-0-0-5.51][18428-0-0-8.23][18442-0-0-2.61][18478-3-3-2.64][18607-0-0-3.64][18616-0-0-3.15][18663-0-0-4.18][18718-0-0-5.39]
[18766-2-2-3.43][18824-2-2-2.35][18890-3-3-1.58][18930-3-4-1.87][18938-3-3-2.22][19817-1-1-3.13][19839-0-2-0.87][19930-3-3-3.19][19944-0-4-2.96][20036-2-2-6.79]
[20101-3-4-2.47][20474-1-1-4.28][20547-3-0-2.68][20929-2-2-3.08][21245-1-1-1.09][21257-3-3-1.16][21293-1-2-4.23][21316-1-1-4.06][21384-1-4-2.62][21448-1-1-3.85]
[21483-0-0-5.92][21487-2-2-2.90][21714-0-0-2.02][21943-3-2-3.06][21947-0-0-5.71][21948-0-0-8.99][21965-2-2-1.30][21998-1-1-4.37][22025-0-4-1.96][22228-3-3-4.28]
[22446-1-1-7.28][22494-3-3-4.20][22757-0-0-5.58][22811-3-3-4.76][22976-3-2-2.12][22985-3-3-3.20][23014-0-0-3.62][23112-1-1-5.60][23144-3-3-4.79][23168-2-0-2.09]
[23219-0-0-3.63][23363-3-3-6.07][23470-0-0-1.69][23486-2-2-2.30][23497-0-3-6.40][23516-0-0-5.41][23690-1-1-1.61][23921-2-2-1.85][23936-1-2-2.04][24040-3-4-2.26]
[24111-1-4-2.98][24182-0-0-7.91][24238-3-3-2.15][24290-2-0-3.29][24345-0-0-4.73][24364-1-4-0.44][24427-3-0-2.92][24477-2-2-2.07][24495-2-4-0.98][24893-2-2-2.67]
[25012-1-4-1.66][25121-2-4-3.37][25165-3-3-1.08][25183-0-0-3.92][25297-3-3-4.40][25398-0-0-6.12][25574-2-4-2.19][25644-1-2-2.25][25718-1-2-1.53][25774-2-4-2.45]
[26032-3-0-4.13][26051-3-3-5.66][26120-0-0-2.25][26321-1-4-1.59][26732-1-1-2.11][26784-3-3-5.10][26827-3-3-2.42][26833-0-0-4.89][26838-2-3-1.25][26860-1-4-1.64]
[26948-0-0-3.12][27049-3-0-5.80][27098-1-3-1.72][27526-0-0-3.47][27639-3-3-2.43][27698-3-3-3.35][27772-0-0-5.75][27890-1-1-3.53][28040-0-4-1.83][28503-2-2-3.01]
[28577-1-2-3.49][28959-0-0-6.62][29198-3-3-1.19][29777-0-0-8.04][29877-2-3-2.46][30035-1-1-4.45][30098-0-3-2.78][30326-1-1-5.21][30572-2-2-1.73][30716-0-4-4.69]
[30806-2-4-1.39][30906-1-1-3.40][31007-0-4-2.26][31181-3-3-2.00][31238-0-3-3.28][31347-0-0-5.12][31422-2-4-1.47][31429-3-2-0.98][31431-0-0-2.14][31432-1-1-3.36]
[31477-0-0-4.25][31524-1-4-1.18][31597-1-2-3.57][31619-1-4-2.25][31701-0-0-5.67][31755-0-0-4.92][31854-3-3-3.98][32074-1-3-1.84][32078-3-3-4.21][32111-1-1-3.55]
[32127-1-2-1.76][32140-3-3-4.29][32263-2-4-1.70][32365-0-0-4.46][32411-2-0-5.26][32429-3-3-2.47][32473-3-0-1.42][32574-3-0-4.65][32584-0-4-2.07][32622-0-1-1.23]
[32858-3-0-3.53][32969-3-3-4.27][33016-2-2-4.03][33031-1-3-1.77][33035-2-2-4.85][33133-2-3-0.12][33173-2-2-1.48][33175-3-4-3.10][33306-3-3-2.76][33309-2-3-1.46]
[33474-0-3-0.51][33478-2-3-2.45][33618-1-1-3.53][33712-0-0-2.51][33782-2-4-2.41][33914-3-3-2.26][34076-3-4-1.83][34112-2-2-3.41][34138-2-2-1.04][34239-1-1-1.34]
[34364-2-2-5.18][34617-1-4-1.38][34751-3-3-4.05][34783-2-1-1.54][35015-3-4-2.04][35018-1-4-1.90][35288-2-4-1.41][0-4-4-3.34][1-4-4-4.48][2-4-4-3.15]
[3-4-4-2.50][4-4-4-3.64][5-4-2-0.93][6-4-0-3.86][7-4-4-3.35][8-4-4-1.79][9-4-0-2.82][10-4-4-4.06][11-4-4-3.18][12-4-2-1.83]
[14-4-4-2.02][15-4-0-2.52][16-4-4-2.71][17-4-4-1.59][18-4-4-3.69][19-4-3-2.64][20-4-0-1.46][21-4-2-2.39][22-4-4-2.67][23-4-4-1.87]
[24-4-4-5.96][25-4-3-3.12][26-4-4-1.21][27-4-4-3.45][28-4-4-4.13][29-4-2-1.60][30-4-0-1.24][31-4-4-4.17][32-4-4-2.24][33-4-2-2.49]
[34-4-4-2.33][35-4-4-1.85][37-4-4-2.56][39-4-0-4.58][40-4-4-1.25][41-4-4-1.90][42-4-4-1.74][43-4-4-2.56][45-4-4-2.26][46-4-4-4.62]
[47-4-4-4.29][48-4-4-1.26][51-4-4-2.35][52-4-4-2.45][53-4-4-1.58][54-4-4-1.62][55-4-4-3.96][56-4-1-3.27][57-4-0-2.73][58-4-2-5.77]
[59-4-0-3.61][60-4-0-1.57][61-4-4-3.30][62-4-4-1.54][63-4-4-3.43][64-4-2-3.22][65-4-4-4.86][66-4-4-4.45][67-4-4-2.36][68-4-3-2.39]
[69-4-0-2.54][70-4-4-1.71][72-4-4-1.63][73-4-1-2.38][74-4-2-2.89][75-4-0-2.37][77-4-4-5.89][78-4-2-1.30][79-4-4-2.84][80-4-4-1.89]
[81-4-4-2.25][82-4-1-1.41][83-4-4-1.91][84-4-4-3.73][85-4-4-4.20][86-4-4-1.43][87-4-4-4.02][88-4-4-2.52][89-4-4-2.24][90-4-4-1.98]
[91-4-0-1.64][92-4-4-2.42][93-4-4-2.44][94-4-4-2.99][95-4-4-1.27][96-4-4-2.55][97-4-4-2.79][98-4-4-2.51][99-4-4-2.40][100-4-4-1.58]
[101-4-4-6.93][102-4-4-1.71][103-4-4-1.04][104-4-4-2.06][105-4-4-3.22][106-4-4-3.25][107-4-4-3.39][108-4-4-2.37][109-4-4-2.99][110-4-4-1.39]
[111-4-0-5.35][112-4-0-1.49][113-4-4-2.95][114-4-3-1.12][115-4-4-2.35][116-4-4-2.90][117-4-4-2.23][119-4-4-2.92][121-4-4-2.04][122-4-4-3.12]
[124-4-4-1.82][125-4-4-3.05][126-4-4-5.37][127-4-1-1.61][128-4-0-1.82][129-4-4-2.07][130-4-4-2.45][131-4-2-2.77][132-4-4-2.06][133-4-4-6.35]
[135-4-4-1.68][136-4-4-1.84][137-4-4-1.62][138-4-4-1.72][139-4-4-3.32][140-4-4-1.81][141-4-4-1.69][142-4-4-3.48][143-4-4-3.13][144-4-4-5.26]
[145-4-2-2.60][148-4-0-4.83][149-4-4-2.00][150-4-4-3.46][151-4-4-3.54][152-4-4-2.42][153-4-4-2.75][154-4-4-5.11][155-4-4-4.10][156-4-3-1.27]
[157-4-2-2.10][158-4-4-3.30][160-4-4-1.34][161-4-2-2.26][162-4-4-1.48][164-4-4-1.99][165-4-4-2.23][167-4-0-4.75][168-4-4-2.86][170-4-4-2.89]
[171-4-4-4.41][172-4-4-3.56][173-4-4-3.79][174-4-0-2.98][175-4-4-3.19][177-4-4-5.20][178-4-4-2.63][179-4-4-2.88][180-4-4-3.65][181-4-3-3.19]
[182-4-1-1.67][183-4-4-3.33][184-4-4-3.67][186-4-0-2.26][187-4-2-2.37][188-4-4-3.43][189-4-4-2.11][190-4-4-0.63][191-4-4-3.50][192-4-4-2.09]
[193-4-3-1.83][194-4-4-1.17][195-4-4-2.11][196-4-4-1.35][197-4-4-2.31][198-4-4-6.27][199-4-4-2.42]
---------------------------
I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.343 | Acc: 85.625% | Wgt Acc: 89.456%
	I - Batch: 100 | Loss: 0.344 | Acc: 85.875% | Wgt Acc: 89.506%
	I - Batch: 150 | Loss: 0.345 | Acc: 85.792% | Wgt Acc: 89.514%
	I - Batch: 200 | Loss: 0.350 | Acc: 85.562% | Wgt Acc: 89.381%
I - num batch: 222
I - Train -- Loss: 0.347 | Acc: 85.622% | Wgt Acc: 89.454% | LR: 6.250000e-04 | Dur: 133.48s
I - Confusion Matrix: [row->prediction - col->label]
[[637.   3.   2.  20. 103.]
 [  6. 554.   6.   4.  58.]
 [  4.   6. 692.   3. 109.]
 [ 15.   1.   3. 495.  71.]
 [ 35.  14.  31.  16. 659.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 63.511% | Wgt Acc: 59.012% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   5.   4.  21.   7.]
 [  0.  32.   2.   1.   4.]
 [  1.  18.  39.   6.  17.]
 [  8.   2.   9.  41.   8.]
 [ 13.  21.  21.  17. 144.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.319 | Acc: 87.750% | Wgt Acc: 90.941%
	I - Batch: 100 | Loss: 0.339 | Acc: 86.000% | Wgt Acc: 89.089%
	I - Batch: 150 | Loss: 0.335 | Acc: 86.167% | Wgt Acc: 89.310%
	I - Batch: 200 | Loss: 0.342 | Acc: 85.438% | Wgt Acc: 88.844%
I - num batch: 222
I - Train -- Loss: 0.342 | Acc: 85.481% | Wgt Acc: 88.798% | LR: 6.250000e-04 | Dur: 133.79s
I - Confusion Matrix: [row->prediction - col->label]
[[625.   4.   4.  21.  98.]
 [  9. 543.   4.   4.  55.]
 [  5.  11. 690.   4.  96.]
 [ 22.   3.   5. 490.  67.]
 [ 36.  17.  31.  19. 684.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.121 | Acc: 63.511% | Wgt Acc: 59.701% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   2.   4.  20.  16.]
 [  0.  34.   5.   2.   6.]
 [  0.  16.  34.   1.  12.]
 [ 10.   6.  10.  47.   6.]
 [ 11.  20.  22.  16. 140.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.355 | Acc: 86.000% | Wgt Acc: 89.441%
	I - Batch: 100 | Loss: 0.354 | Acc: 86.438% | Wgt Acc: 89.960%
	I - Batch: 150 | Loss: 0.359 | Acc: 85.833% | Wgt Acc: 89.563%
	I - Batch: 200 | Loss: 0.362 | Acc: 85.438% | Wgt Acc: 88.994%
I - num batch: 222
I - Train -- Loss: 0.359 | Acc: 85.424% | Wgt Acc: 88.990% | LR: 6.250000e-04 | Dur: 132.78s
I - Confusion Matrix: [row->prediction - col->label]
[[633.   5.   5.  24. 102.]
 [  6. 548.   8.   8.  60.]
 [  5.   9. 692.   5.  96.]
 [ 16.   4.   4. 487.  72.]
 [ 37.  12.  25.  14. 670.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.091 | Acc: 63.314% | Wgt Acc: 60.662% | Dur: 13.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   8.   2.  12.  10.]
 [  1.  32.   7.   1.   4.]
 [  2.  18.  39.   1.  20.]
 [ 17.   3.  10.  60.  13.]
 [ 11.  17.  17.  12. 133.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.332 | Acc: 87.000% | Wgt Acc: 90.550%
	I - Batch: 100 | Loss: 0.358 | Acc: 85.438% | Wgt Acc: 89.166%
	I - Batch: 150 | Loss: 0.351 | Acc: 85.125% | Wgt Acc: 88.907%
	I - Batch: 200 | Loss: 0.357 | Acc: 84.719% | Wgt Acc: 88.686%
I - num batch: 222
I - Train -- Loss: 0.357 | Acc: 84.804% | Wgt Acc: 88.566% | LR: 6.250000e-04 | Dur: 133.95s
I - Confusion Matrix: [row->prediction - col->label]
[[627.   4.   5.  21. 110.]
 [  4. 544.   4.   5.  62.]
 [  2.  11. 694.   3. 110.]
 [ 22.   3.   2. 489.  64.]
 [ 42.  16.  29.  20. 654.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.141 | Acc: 61.736% | Wgt Acc: 60.934% | Dur: 17.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   5.   5.  21.  19.]
 [  0.  45.  11.   2.  12.]
 [  1.  11.  37.   5.  23.]
 [  9.   6.  11.  48.  10.]
 [ 11.  11.  11.  10. 116.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.317 | Acc: 86.375% | Wgt Acc: 90.060%
	I - Batch: 100 | Loss: 0.335 | Acc: 86.688% | Wgt Acc: 90.261%
	I - Batch: 150 | Loss: 0.341 | Acc: 86.542% | Wgt Acc: 89.992%
	I - Batch: 200 | Loss: 0.353 | Acc: 86.094% | Wgt Acc: 89.615%
I - num batch: 222
I - Train -- Loss: 0.354 | Acc: 86.101% | Wgt Acc: 89.458% | LR: 6.250000e-04 | Dur: 132.36s
I - Confusion Matrix: [row->prediction - col->label]
[[633.   4.   7.  15.  94.]
 [  7. 550.   6.   1.  60.]
 [  7.   8. 684.   7.  92.]
 [ 15.   1.   6. 498.  65.]
 [ 35.  15.  31.  17. 689.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.125 | Acc: 64.497% | Wgt Acc: 61.143% | Dur: 15.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   4.  11.  12.]
 [  0.  31.   4.   2.   4.]
 [  2.  16.  36.   2.  10.]
 [ 13.   5.  12.  60.  14.]
 [ 13.  23.  19.  11. 140.]]

I - Local maximum validation set accuracy:  64.50

I - Validation set results: 
[14-1-2-2.30][50-3-3-1.29][124-2-3-0.73][127-0-0-6.74][443-2-2-3.70][567-0-0-3.51][573-1-3-1.78][615-0-0-4.10][695-1-0-2.11][722-3-3-7.02]
[826-0-0-2.28][878-0-0-4.09][1103-0-0-1.91][1212-3-4-1.12][1368-0-0-5.12][2181-2-3-1.14][2476-2-2-1.12][2721-2-2-2.96][2818-1-4-1.32][2886-2-4-1.54]
[3231-2-2-5.25][3333-2-2-3.35][3482-2-2-3.20][3536-3-3-4.49][3625-1-1-3.82][3909-0-0-4.04][4035-0-0-3.36][4140-0-0-3.56][4214-1-1-2.51][4346-1-4-2.81]
[4581-2-1-2.44][4708-3-4-1.74][4838-3-3-2.23][4845-1-2-1.13][4868-0-0-6.49][4939-0-4-0.84][4984-2-3-1.80][5078-1-4-1.26][5396-0-0-7.00][5479-1-1-3.82]
[5717-0-0-2.05][5843-1-2-2.59][5949-3-3-3.93][5987-2-4-3.85][6014-3-3-3.09][6033-3-3-3.31][6313-0-3-5.27][6421-3-3-3.44][6500-1-4-0.47][6583-3-3-5.29]
[6683-3-3-2.23][6825-2-1-1.59][6998-3-3-2.49][7049-3-3-1.56][7517-1-1-3.28][7521-1-1-1.82][7528-1-3-2.75][7949-1-2-2.66][8135-1-0-2.29][8185-3-0-4.47]
[8269-3-1-4.19][8273-3-3-5.75][8543-3-0-4.73][8666-1-1-3.04][8672-0-0-5.85][8903-1-1-3.21][9001-2-2-3.14][9036-2-2-4.84][9281-3-4-3.33][9300-2-2-6.80]
[9571-0-4-1.54][9617-1-4-1.70][9644-2-1-1.45][9705-2-0-0.76][9801-0-3-1.62][9803-3-3-3.59][9865-3-3-6.85][9896-2-2-1.51][10314-1-4-0.93][10337-3-3-5.50]
[10403-0-4-1.62][10653-2-4-1.89][10704-2-2-3.19][10719-1-2-1.96][10727-1-4-0.86][10836-0-0-8.64][10969-2-3-2.26][11042-0-0-2.16][11088-1-1-3.65][11322-0-0-5.87]
[11398-2-2-2.08][11499-0-0-5.00][11502-3-3-1.71][11512-3-3-4.13][11608-1-2-3.01][11610-0-0-2.69][11692-0-0-2.94][11905-0-0-4.54][11993-1-1-4.19][12002-2-0-4.40]
[12052-0-0-4.67][12201-0-3-3.84][12235-2-4-1.81][12320-1-4-3.71][12377-2-4-3.71][12398-2-3-1.35][12503-1-2-1.69][12617-0-4-0.65][12685-3-3-1.70][12738-2-3-1.11]
[12742-2-2-3.35][12823-0-3-4.41][13110-1-1-3.84][13240-3-3-3.80][13253-1-2-1.53][13273-0-0-6.78][13634-1-4-1.62][13763-2-4-0.14][13905-3-3-1.24][14060-2-2-1.37]
[14065-3-3-2.20][14147-3-3-2.15][14595-2-2-2.84][14687-2-2-4.43][14788-2-2-2.40][14869-1-4-2.72][14872-3-4-1.25][14877-1-4-2.02][14927-0-3-3.29][15066-0-0-6.52]
[15175-1-1-1.82][15178-2-4-2.77][15375-3-0-2.63][15389-3-3-3.08][15568-2-4-2.50][15675-3-3-5.67][15869-1-2-1.77][16207-3-1-2.62][16236-0-0-2.88][16302-3-0-1.85]
[16331-2-2-7.22][16381-0-3-1.80][16488-1-1-4.24][16495-0-0-4.82][16650-0-0-6.32][16719-1-4-1.65][16801-0-0-7.79][16828-0-0-3.72][17137-3-3-1.64][17245-1-4-1.78]
[17278-3-0-1.68][17282-0-2-3.74][17311-2-2-3.11][17336-2-2-2.53][17608-3-3-6.78][17627-0-0-0.68][17877-3-4-2.61][17924-1-4-0.58][17984-3-3-5.05][18211-0-0-2.26]
[18276-3-0-3.45][18287-1-4-0.72][18394-0-0-5.16][18428-0-0-6.00][18442-0-3-5.39][18478-3-3-4.30][18607-0-0-1.81][18616-0-4-2.39][18663-0-0-3.80][18718-0-0-5.82]
[18766-2-2-2.26][18824-2-4-2.99][18890-3-3-1.58][18930-3-4-2.82][18938-3-3-1.43][19817-1-1-1.81][19839-0-4-1.08][19930-3-3-2.99][19944-0-4-1.79][20036-2-2-5.80]
[20101-3-3-3.29][20474-1-1-1.82][20547-3-4-1.18][20929-2-2-5.27][21245-1-2-2.96][21257-3-3-1.06][21293-1-1-4.87][21316-1-3-0.72][21384-1-4-4.03][21448-1-1-3.76]
[21483-0-0-4.43][21487-2-4-2.33][21714-0-3-0.82][21943-3-2-1.16][21947-0-0-5.43][21948-0-0-8.39][21965-2-2-4.23][21998-1-1-3.73][22025-0-2-1.51][22228-3-3-5.65]
[22446-1-1-3.42][22494-3-3-4.00][22757-0-0-5.89][22811-3-3-6.19][22976-3-4-1.61][22985-3-3-2.56][23014-0-0-2.35][23112-1-1-4.76][23144-3-3-6.03][23168-2-4-1.97]
[23219-0-0-2.22][23363-3-3-3.56][23470-0-4-1.20][23486-2-2-2.41][23497-0-3-7.75][23516-0-0-5.34][23690-1-3-1.49][23921-2-4-1.13][23936-1-0-1.55][24040-3-4-1.77]
[24111-1-4-2.40][24182-0-0-7.25][24238-3-3-2.75][24290-2-0-2.46][24345-0-0-4.61][24364-1-1-0.76][24427-3-0-1.67][24477-2-2-5.20][24495-2-4-1.12][24893-2-2-1.75]
[25012-1-4-1.73][25121-2-4-3.54][25165-3-3-1.68][25183-0-0-2.73][25297-3-3-4.77][25398-0-0-6.95][25574-2-4-1.77][25644-1-2-4.10][25718-1-2-4.62][25774-2-4-1.88]
[26032-3-3-5.31][26051-3-3-5.35][26120-0-4-5.92][26321-1-4-1.65][26732-1-1-4.70][26784-3-3-6.89][26827-3-3-1.57][26833-0-3-5.09][26838-2-3-2.08][26860-1-2-1.42]
[26948-0-0-3.80][27049-3-0-5.87][27098-1-4-0.56][27526-0-0-2.81][27639-3-4-2.61][27698-3-3-5.04][27772-0-0-5.50][27890-1-1-3.61][28040-0-4-4.49][28503-2-2-2.32]
[28577-1-1-2.30][28959-0-0-6.84][29198-3-3-2.01][29777-0-0-6.48][29877-2-3-2.13][30035-1-1-5.10][30098-0-3-3.49][30326-1-1-6.83][30572-2-2-2.23][30716-0-4-3.72]
[30806-2-3-1.43][30906-1-1-4.71][31007-0-0-3.12][31181-3-3-3.41][31238-0-3-3.22][31347-0-0-3.62][31422-2-2-2.21][31429-3-3-2.78][31431-0-0-1.14][31432-1-1-3.56]
[31477-0-0-3.43][31524-1-2-1.91][31597-1-2-3.73][31619-1-4-2.07][31701-0-0-2.44][31755-0-0-3.45][31854-3-3-3.07][32074-1-1-3.19][32078-3-3-5.38][32111-1-1-4.18]
[32127-1-1-2.27][32140-3-3-5.54][32263-2-4-2.69][32365-0-0-4.70][32411-2-0-4.73][32429-3-3-3.94][32473-3-0-2.12][32574-3-0-4.73][32584-0-4-2.05][32622-0-4-1.35]
[32858-3-0-3.47][32969-3-3-6.35][33016-2-2-3.22][33031-1-3-1.62][33035-2-2-3.94][33133-2-2-2.18][33173-2-3-2.03][33175-3-4-2.42][33306-3-3-2.86][33309-2-3-2.07]
[33474-0-3-1.03][33478-2-3-4.58][33618-1-1-3.03][33712-0-0-1.88][33782-2-4-3.29][33914-3-3-4.19][34076-3-2-0.97][34112-2-2-3.14][34138-2-2-1.63][34239-1-4-0.51]
[34364-2-2-5.07][34617-1-2-1.59][34751-3-3-4.91][34783-2-4-2.90][35015-3-3-2.45][35018-1-4-2.02][35288-2-1-1.35][0-4-4-1.88][1-4-4-4.53][2-4-4-2.53]
[3-4-4-3.73][4-4-2-1.80][5-4-3-1.10][6-4-4-5.33][7-4-4-3.10][8-4-4-2.17][9-4-0-2.62][10-4-4-4.42][11-4-2-3.77][12-4-4-2.34]
[14-4-0-2.23][15-4-3-4.20][16-4-4-2.94][17-4-4-1.84][18-4-4-5.54][19-4-3-1.35][20-4-3-1.32][21-4-4-1.94][22-4-4-3.16][23-4-4-2.31]
[24-4-4-7.95][25-4-3-2.62][26-4-4-1.45][27-4-4-2.90][28-4-4-5.53][29-4-1-1.70][30-4-4-0.64][31-4-4-3.49][32-4-4-2.45][33-4-3-1.41]
[34-4-4-1.73][35-4-4-2.60][37-4-2-2.03][39-4-0-5.58][40-4-4-1.50][41-4-4-1.59][42-4-4-1.66][43-4-4-1.44][45-4-4-1.69][46-4-4-5.06]
[47-4-4-4.74][48-4-4-3.80][51-4-4-3.46][52-4-4-1.35][53-4-4-1.50][54-4-4-2.97][55-4-4-2.95][56-4-4-2.87][57-4-3-1.90][58-4-2-3.75]
[59-4-0-3.81][60-4-4-2.98][61-4-4-3.05][62-4-2-2.00][63-4-4-3.66][64-4-4-2.31][65-4-4-6.17][66-4-4-3.36][67-4-4-1.23][68-4-3-2.24]
[69-4-4-0.49][70-4-4-3.46][72-4-4-1.94][73-4-4-1.58][74-4-2-2.17][75-4-4-1.91][77-4-4-6.13][78-4-3-1.95][79-4-4-4.35][80-4-4-3.07]
[81-4-4-3.04][82-4-4-1.19][83-4-4-2.48][84-4-4-6.23][85-4-4-4.47][86-4-4-1.33][87-4-4-4.92][88-4-4-1.88][89-4-2-1.99][90-4-4-1.56]
[91-4-4-1.47][92-4-4-1.03][93-4-0-1.83][94-4-4-3.72][95-4-4-1.45][96-4-4-2.83][97-4-4-2.64][98-4-4-1.59][99-4-4-2.19][100-4-4-1.89]
[101-4-4-7.63][102-4-4-2.85][103-4-3-2.45][104-4-4-2.90][105-4-4-3.43][106-4-4-3.16][107-4-4-2.12][108-4-4-2.31][109-4-4-2.36][110-4-4-1.21]
[111-4-0-6.81][112-4-4-1.29][113-4-4-1.93][114-4-3-2.83][115-4-4-2.84][116-4-4-1.92][117-4-4-3.11][119-4-4-2.31][121-4-4-2.27][122-4-4-2.40]
[124-4-1-1.89][125-4-4-3.80][126-4-4-6.86][127-4-4-1.27][128-4-0-2.34][129-4-4-1.44][130-4-0-0.65][131-4-4-2.27][132-4-4-1.64][133-4-4-5.49]
[135-4-4-2.62][136-4-4-1.11][137-4-4-2.11][138-4-4-1.82][139-4-4-2.76][140-4-1-1.84][141-4-0-0.88][142-4-4-4.37][143-4-4-4.41][144-4-4-3.17]
[145-4-4-4.11][148-4-0-4.89][149-4-4-1.66][150-4-4-3.58][151-4-4-4.34][152-4-4-2.27][153-4-4-3.79][154-4-4-6.02][155-4-4-5.45][156-4-3-1.96]
[157-4-2-2.48][158-4-4-2.20][160-4-4-1.76][161-4-2-1.81][162-4-4-1.94][164-4-4-2.03][165-4-4-2.91][167-4-4-2.77][168-4-4-2.74][170-4-4-2.91]
[171-4-4-3.07][172-4-4-3.78][173-4-4-3.35][174-4-0-1.69][175-4-4-3.23][177-4-4-3.73][178-4-4-2.84][179-4-4-2.87][180-4-4-5.01][181-4-3-2.99]
[182-4-4-2.94][183-4-4-4.09][184-4-4-3.12][186-4-4-1.82][187-4-4-1.71][188-4-4-2.22][189-4-4-2.32][190-4-4-1.48][191-4-4-2.83][192-4-2-2.19]
[193-4-3-2.50][194-4-1-0.49][195-4-0-3.16][196-4-4-1.41][197-4-4-2.30][198-4-4-7.06][199-4-4-2.79]
---------------------------
I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.346 | Acc: 86.000% | Wgt Acc: 89.526%
	I - Batch: 100 | Loss: 0.348 | Acc: 85.500% | Wgt Acc: 89.234%
	I - Batch: 150 | Loss: 0.347 | Acc: 85.833% | Wgt Acc: 89.459%
	I - Batch: 200 | Loss: 0.351 | Acc: 85.719% | Wgt Acc: 89.295%
I - num batch: 222
I - Train -- Loss: 0.346 | Acc: 85.847% | Wgt Acc: 89.383% | LR: 6.250000e-04 | Dur: 137.12s
I - Confusion Matrix: [row->prediction - col->label]
[[636.   0.   4.  21. 103.]
 [  2. 549.   6.   4.  54.]
 [  8.   8. 693.   2. 101.]
 [ 19.   4.   7. 491.  66.]
 [ 32.  17.  24.  20. 676.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.117 | Acc: 62.130% | Wgt Acc: 60.244% | Dur: 15.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   6.   3.  27.  19.]
 [  0.  36.   2.   0.   4.]
 [  1.  20.  43.   6.  27.]
 [  3.   1.   8.  42.   7.]
 [ 13.  15.  19.  11. 123.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.356 | Acc: 84.875% | Wgt Acc: 89.022%
	I - Batch: 100 | Loss: 0.343 | Acc: 85.812% | Wgt Acc: 89.817%
	I - Batch: 150 | Loss: 0.343 | Acc: 86.333% | Wgt Acc: 90.012%
	I - Batch: 200 | Loss: 0.339 | Acc: 86.500% | Wgt Acc: 90.072%
I - num batch: 222
I - Train -- Loss: 0.338 | Acc: 86.693% | Wgt Acc: 90.164% | LR: 6.250000e-04 | Dur: 136.07s
I - Confusion Matrix: [row->prediction - col->label]
[[643.   3.  12.  19. 104.]
 [  3. 556.   4.   4.  48.]
 [  3.   7. 693.   3.  93.]
 [ 17.   4.   3. 495.  67.]
 [ 31.   8.  22.  17. 688.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.069 | Acc: 62.722% | Wgt Acc: 63.222% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   5.  11.  14.]
 [  0.  43.   8.   2.  13.]
 [  6.  23.  44.   1.  32.]
 [ 17.   0.   7.  62.  11.]
 [  6.  10.  11.  10. 110.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.288 | Acc: 87.500% | Wgt Acc: 90.861%
	I - Batch: 100 | Loss: 0.312 | Acc: 87.438% | Wgt Acc: 90.338%
	I - Batch: 150 | Loss: 0.322 | Acc: 86.667% | Wgt Acc: 89.970%
	I - Batch: 200 | Loss: 0.326 | Acc: 86.438% | Wgt Acc: 89.824%
I - num batch: 222
I - Train -- Loss: 0.329 | Acc: 86.439% | Wgt Acc: 89.736% | LR: 6.250000e-04 | Dur: 138.40s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   1.   5.  18.  79.]
 [  4. 545.   4.   5.  62.]
 [  3.  10. 698.   6. 101.]
 [ 16.   3.   2. 486.  65.]
 [ 30.  19.  25.  23. 693.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.131 | Acc: 64.694% | Wgt Acc: 61.164% | Dur: 14.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   3.  18.  18.]
 [  0.  29.   2.   1.   2.]
 [  2.  17.  38.   2.   9.]
 [ 12.   3.  11.  57.  10.]
 [ 11.  25.  21.   8. 141.]]

I - Local maximum validation set accuracy:  64.69

I - Validation set results: 
[14-1-2-1.62][50-3-3-1.77][124-2-4-1.44][127-0-0-6.35][443-2-2-3.04][567-0-0-3.05][573-1-3-1.06][615-0-0-4.16][695-1-2-2.31][722-3-3-6.53]
[826-0-0-6.28][878-0-0-5.82][1103-0-0-2.49][1212-3-3-1.20][1368-0-0-5.14][2181-2-3-0.69][2476-2-4-1.41][2721-2-2-3.45][2818-1-0-1.85][2886-2-4-1.26]
[3231-2-2-5.74][3333-2-1-0.69][3482-2-4-2.83][3536-3-3-3.76][3625-1-1-2.65][3909-0-0-3.45][4035-0-0-4.09][4140-0-0-3.21][4214-1-2-0.40][4346-1-4-1.82]
[4581-2-1-2.13][4708-3-3-2.88][4838-3-3-2.94][4845-1-4-1.76][4868-0-0-5.05][4939-0-2-1.29][4984-2-3-3.47][5078-1-4-1.45][5396-0-0-8.16][5479-1-1-2.46]
[5717-0-0-4.56][5843-1-1-3.44][5949-3-3-3.50][5987-2-4-4.65][6014-3-3-2.38][6033-3-3-2.56][6313-0-3-4.12][6421-3-3-4.82][6500-1-2-0.66][6583-3-3-5.16]
[6683-3-3-2.74][6825-2-3-2.07][6998-3-3-2.35][7049-3-3-1.75][7517-1-2-1.53][7521-1-0-0.97][7528-1-2-1.73][7949-1-2-2.45][8135-1-0-4.15][8185-3-0-7.02]
[8269-3-1-2.38][8273-3-3-5.46][8543-3-0-5.78][8666-1-1-3.66][8672-0-0-5.56][8903-1-2-2.01][9001-2-2-4.09][9036-2-2-2.50][9281-3-3-1.27][9300-2-2-5.59]
[9571-0-3-2.25][9617-1-1-2.31][9644-2-2-0.97][9705-2-0-1.15][9801-0-0-1.91][9803-3-3-2.82][9865-3-3-6.05][9896-2-2-2.17][10314-1-4-1.42][10337-3-3-4.16]
[10403-0-4-1.92][10653-2-4-2.26][10704-2-2-3.11][10719-1-1-3.03][10727-1-4-0.95][10836-0-0-9.33][10969-2-3-1.21][11042-0-0-2.80][11088-1-1-6.41][11322-0-0-7.08]
[11398-2-4-2.39][11499-0-0-4.82][11502-3-0-2.23][11512-3-3-2.92][11608-1-2-3.34][11610-0-0-3.48][11692-0-0-2.97][11905-0-0-3.93][11993-1-1-2.93][12002-2-2-3.40]
[12052-0-0-5.78][12201-0-3-4.15][12235-2-4-2.19][12320-1-4-3.67][12377-2-4-1.46][12398-2-4-1.43][12503-1-4-2.92][12617-0-4-1.81][12685-3-4-1.60][12738-2-3-1.54]
[12742-2-2-4.49][12823-0-3-4.47][13110-1-4-1.55][13240-3-3-4.72][13253-1-2-1.41][13273-0-0-7.98][13634-1-2-0.64][13763-2-2-1.10][13905-3-3-0.29][14060-2-4-2.73]
[14065-3-3-2.07][14147-3-0-4.11][14595-2-2-2.24][14687-2-2-3.04][14788-2-2-2.63][14869-1-1-3.48][14872-3-4-1.65][14877-1-1-2.81][14927-0-0-4.53][15066-0-0-7.75]
[15175-1-4-1.74][15178-2-4-2.56][15375-3-0-3.53][15389-3-3-3.90][15568-2-4-2.86][15675-3-3-6.55][15869-1-4-0.98][16207-3-0-1.82][16236-0-3-1.00][16302-3-3-2.82]
[16331-2-2-8.84][16381-0-3-3.07][16488-1-1-2.29][16495-0-0-3.32][16650-0-0-7.94][16719-1-4-1.38][16801-0-0-7.11][16828-0-0-2.76][17137-3-0-2.25][17245-1-4-1.94]
[17278-3-0-2.53][17282-0-2-3.78][17311-2-2-3.26][17336-2-2-2.56][17608-3-3-7.01][17627-0-0-1.77][17877-3-0-2.02][17924-1-4-0.73][17984-3-3-4.88][18211-0-3-2.80]
[18276-3-3-3.57][18287-1-4-0.41][18394-0-0-5.67][18428-0-0-7.60][18442-0-0-2.85][18478-3-0-3.22][18607-0-0-2.43][18616-0-0-2.45][18663-0-0-5.34][18718-0-0-7.37]
[18766-2-2-1.69][18824-2-4-2.89][18890-3-3-2.73][18930-3-4-1.57][18938-3-3-1.75][19817-1-1-2.10][19839-0-0-1.50][19930-3-3-2.51][19944-0-4-2.08][20036-2-2-5.90]
[20101-3-3-3.63][20474-1-1-2.23][20547-3-3-2.78][20929-2-2-5.04][21245-1-2-0.61][21257-3-2-1.09][21293-1-2-6.42][21316-1-1-4.30][21384-1-4-4.12][21448-1-1-3.06]
[21483-0-0-5.84][21487-2-2-4.95][21714-0-3-1.29][21943-3-2-2.47][21947-0-0-4.51][21948-0-0-7.96][21965-2-2-3.61][21998-1-1-2.04][22025-0-4-2.26][22228-3-3-6.33]
[22446-1-1-4.17][22494-3-3-2.87][22757-0-0-6.85][22811-3-3-4.49][22976-3-4-0.81][22985-3-3-3.67][23014-0-0-4.58][23112-1-1-4.62][23144-3-3-4.31][23168-2-0-1.95]
[23219-0-0-4.05][23363-3-3-6.25][23470-0-0-2.72][23486-2-2-1.94][23497-0-3-6.29][23516-0-0-6.84][23690-1-4-2.95][23921-2-2-2.17][23936-1-2-2.97][24040-3-0-2.16]
[24111-1-4-1.31][24182-0-0-6.84][24238-3-3-2.50][24290-2-0-3.47][24345-0-4-1.39][24364-1-4-1.03][24427-3-0-4.00][24477-2-2-3.77][24495-2-4-1.09][24893-2-2-2.44]
[25012-1-4-1.64][25121-2-2-3.40][25165-3-3-4.44][25183-0-0-2.46][25297-3-3-6.10][25398-0-0-5.03][25574-2-4-2.91][25644-1-1-2.85][25718-1-2-1.06][25774-2-4-2.04]
[26032-3-3-3.12][26051-3-3-4.76][26120-0-4-4.05][26321-1-1-3.91][26732-1-1-0.98][26784-3-3-5.72][26827-3-3-2.85][26833-0-3-4.78][26838-2-3-1.81][26860-1-4-1.34]
[26948-0-0-3.37][27049-3-0-4.54][27098-1-0-1.56][27526-0-4-2.56][27639-3-3-5.33][27698-3-3-4.10][27772-0-0-6.83][27890-1-4-1.46][28040-0-4-3.19][28503-2-2-2.97]
[28577-1-2-2.90][28959-0-0-6.84][29198-3-4-3.50][29777-0-0-7.50][29877-2-3-2.75][30035-1-1-3.92][30098-0-3-5.64][30326-1-1-5.61][30572-2-2-1.68][30716-0-4-2.97]
[30806-2-3-1.97][30906-1-1-3.50][31007-0-0-2.56][31181-3-0-2.24][31238-0-0-3.77][31347-0-0-5.31][31422-2-2-3.40][31429-3-3-2.18][31431-0-3-1.35][31432-1-1-3.32]
[31477-0-0-5.65][31524-1-3-0.93][31597-1-2-2.63][31619-1-4-2.43][31701-0-0-4.48][31755-0-0-5.36][31854-3-3-3.54][32074-1-1-2.76][32078-3-3-4.77][32111-1-1-5.99]
[32127-1-1-2.19][32140-3-3-4.35][32263-2-4-1.75][32365-0-0-5.27][32411-2-3-4.88][32429-3-0-3.40][32473-3-0-2.97][32574-3-0-5.73][32584-0-4-3.49][32622-0-4-2.29]
[32858-3-0-3.99][32969-3-3-5.60][33016-2-2-5.06][33031-1-3-2.72][33035-2-2-2.71][33133-2-2-0.99][33173-2-2-1.88][33175-3-4-2.26][33306-3-3-3.19][33309-2-3-1.53]
[33474-0-0-1.80][33478-2-3-3.03][33618-1-1-3.49][33712-0-0-4.07][33782-2-4-3.43][33914-3-3-4.17][34076-3-4-2.05][34112-2-2-4.43][34138-2-2-1.37][34239-1-4-1.18]
[34364-2-2-5.40][34617-1-4-3.25][34751-3-3-3.63][34783-2-4-3.17][35015-3-4-1.43][35018-1-4-1.71][35288-2-4-0.97][0-4-4-3.45][1-4-4-3.70][2-4-4-3.87]
[3-4-4-2.21][4-4-4-1.94][5-4-1-0.74][6-4-4-4.98][7-4-4-2.07][8-4-0-0.76][9-4-0-3.33][10-4-4-4.13][11-4-4-4.05][12-4-4-1.83]
[14-4-4-1.58][15-4-3-3.82][16-4-4-2.02][17-4-4-2.22][18-4-4-4.82][19-4-0-2.76][20-4-0-2.39][21-4-4-1.66][22-4-4-2.79][23-4-4-2.72]
[24-4-4-6.37][25-4-3-3.17][26-4-4-1.50][27-4-4-4.29][28-4-4-4.61][29-4-4-1.41][30-4-3-1.27][31-4-4-1.59][32-4-4-3.18][33-4-3-1.77]
[34-4-4-2.40][35-4-4-2.95][37-4-4-2.87][39-4-0-6.26][40-4-4-1.59][41-4-4-2.17][42-4-4-1.90][43-4-4-2.59][45-4-4-2.02][46-4-4-4.09]
[47-4-4-4.13][48-4-4-1.53][51-4-4-4.31][52-4-4-1.91][53-4-4-2.18][54-4-4-3.01][55-4-0-2.57][56-4-4-1.78][57-4-0-3.74][58-4-2-3.41]
[59-4-0-4.11][60-4-4-3.85][61-4-4-2.98][62-4-4-1.65][63-4-4-3.63][64-4-2-2.98][65-4-4-4.52][66-4-4-2.62][67-4-4-2.29][68-4-3-3.05]
[69-4-0-1.49][70-4-4-2.65][72-4-4-1.87][73-4-1-2.30][74-4-2-2.34][75-4-4-1.68][77-4-4-6.93][78-4-2-1.50][79-4-4-3.22][80-4-4-4.16]
[81-4-4-3.53][82-4-4-0.99][83-4-4-2.58][84-4-4-4.62][85-4-4-3.34][86-4-4-1.47][87-4-4-4.95][88-4-4-2.57][89-4-4-2.88][90-4-4-1.36]
[91-4-4-1.25][92-4-4-2.73][93-4-0-1.59][94-4-4-2.49][95-4-4-1.53][96-4-4-3.96][97-4-4-3.50][98-4-4-2.93][99-4-4-1.96][100-4-4-1.72]
[101-4-4-6.15][102-4-4-2.11][103-4-0-1.05][104-4-4-2.91][105-4-4-2.93][106-4-4-3.70][107-4-4-4.06][108-4-4-2.84][109-4-4-2.90][110-4-4-2.15]
[111-4-0-6.88][112-4-4-1.31][113-4-4-2.05][114-4-3-2.13][115-4-4-2.24][116-4-4-1.29][117-4-4-3.26][119-4-4-1.71][121-4-4-3.29][122-4-4-2.74]
[124-4-4-2.05][125-4-4-3.95][126-4-4-4.69][127-4-2-3.59][128-4-0-1.54][129-4-4-2.60][130-4-4-3.73][131-4-2-2.72][132-4-4-1.62][133-4-4-4.47]
[135-4-4-3.29][136-4-4-1.62][137-4-4-2.60][138-4-4-2.14][139-4-4-2.76][140-4-4-1.73][141-4-4-1.40][142-4-4-3.80][143-4-4-3.89][144-4-4-3.97]
[145-4-4-3.27][148-4-0-4.27][149-4-4-2.66][150-4-4-4.14][151-4-4-3.04][152-4-4-3.27][153-4-4-3.37][154-4-4-5.34][155-4-4-4.08][156-4-3-1.76]
[157-4-2-1.86][158-4-4-2.05][160-4-4-2.38][161-4-2-1.89][162-4-4-1.59][164-4-4-3.17][165-4-4-2.11][167-4-0-2.82][168-4-4-2.61][170-4-4-2.37]
[171-4-4-3.82][172-4-4-3.78][173-4-4-3.32][174-4-0-2.65][175-4-4-3.19][177-4-4-3.95][178-4-2-1.59][179-4-4-3.56][180-4-4-4.10][181-4-4-2.21]
[182-4-3-2.99][183-4-4-3.33][184-4-4-2.81][186-4-0-2.21][187-4-4-1.91][188-4-4-2.76][189-4-4-2.31][190-4-4-1.60][191-4-4-3.23][192-4-4-1.99]
[193-4-3-3.46][194-4-4-1.20][195-4-0-1.78][196-4-3-1.34][197-4-4-3.12][198-4-4-6.44][199-4-4-2.46]
---------------------------
I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.302 | Acc: 88.750% | Wgt Acc: 91.596%
	I - Batch: 100 | Loss: 0.297 | Acc: 88.188% | Wgt Acc: 91.490%
	I - Batch: 150 | Loss: 0.317 | Acc: 86.917% | Wgt Acc: 90.536%
	I - Batch: 200 | Loss: 0.311 | Acc: 87.375% | Wgt Acc: 90.819%
I - num batch: 208
I - Train -- Loss: 0.311 | Acc: 87.350% | Wgt Acc: 90.862% | LR: 6.250000e-04 | Dur: 126.11s
I - Confusion Matrix: [row->prediction - col->label]
[[649.   2.   1.  22.  67.]
 [  3. 558.   4.   3.  58.]
 [  5.   6. 701.   3.  80.]
 [ 12.   1.   5. 493.  70.]
 [ 28.  11.  23.  17. 506.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.091 | Acc: 63.314% | Wgt Acc: 59.356% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   2.   3.  20.  12.]
 [  0.  28.   6.   0.   6.]
 [  1.  23.  40.   9.  18.]
 [  7.   2.   5.  45.   4.]
 [ 12.  23.  21.  12. 140.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.301 | Acc: 87.625% | Wgt Acc: 90.879%
	I - Batch: 100 | Loss: 0.304 | Acc: 87.562% | Wgt Acc: 90.745%
	I - Batch: 150 | Loss: 0.315 | Acc: 87.167% | Wgt Acc: 90.551%
	I - Batch: 200 | Loss: 0.321 | Acc: 86.781% | Wgt Acc: 90.226%
I - num batch: 222
I - Train -- Loss: 0.324 | Acc: 86.580% | Wgt Acc: 90.119% | LR: 6.250000e-04 | Dur: 140.23s
I - Confusion Matrix: [row->prediction - col->label]
[[639.   7.   8.  19.  85.]
 [  1. 555.   3.   2.  58.]
 [  5.   5. 701.   2.  92.]
 [ 14.   0.   2. 493.  82.]
 [ 38.  11.  20.  22. 683.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.111 | Acc: 61.538% | Wgt Acc: 59.147% | Dur: 15.83s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   1.   1.  11.   9.]
 [  1.  37.   7.   2.   8.]
 [  2.  24.  37.   6.  26.]
 [ 19.   7.  12.  56.   9.]
 [ 12.   9.  18.  11. 128.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.329 | Acc: 88.250% | Wgt Acc: 91.374%
	I - Batch: 100 | Loss: 0.313 | Acc: 88.438% | Wgt Acc: 91.892%
	I - Batch: 150 | Loss: 0.303 | Acc: 88.292% | Wgt Acc: 91.606%
	I - Batch: 200 | Loss: 0.308 | Acc: 88.094% | Wgt Acc: 91.420%
I - num batch: 222
I - Train -- Loss: 0.306 | Acc: 88.328% | Wgt Acc: 91.563% | LR: 6.250000e-04 | Dur: 134.27s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   0.   0.  15.  85.]
 [  4. 554.   3.   3.  51.]
 [  5.   6. 706.   2.  87.]
 [  7.   4.   4. 502.  61.]
 [ 26.  14.  21.  16. 716.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.135 | Acc: 62.722% | Wgt Acc: 62.282% | Dur: 13.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   2.   3.   4.   6.]
 [  2.  50.  10.   2.  19.]
 [  3.  14.  41.   8.  25.]
 [ 16.   1.   4.  59.  13.]
 [ 16.  11.  17.  13. 117.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.313 | Acc: 88.375% | Wgt Acc: 91.585%
	I - Batch: 100 | Loss: 0.321 | Acc: 87.812% | Wgt Acc: 91.269%
	I - Batch: 150 | Loss: 0.333 | Acc: 86.917% | Wgt Acc: 90.446%
	I - Batch: 200 | Loss: 0.324 | Acc: 87.344% | Wgt Acc: 90.659%
I - num batch: 222
I - Train -- Loss: 0.325 | Acc: 87.285% | Wgt Acc: 90.572% | LR: 6.250000e-04 | Dur: 132.40s
I - Confusion Matrix: [row->prediction - col->label]
[[653.   2.   6.  16.  84.]
 [  1. 551.   8.   0.  38.]
 [  6.   8. 685.   4. 100.]
 [ 15.   1.   6. 503.  74.]
 [ 22.  16.  29.  15. 704.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.188 | Acc: 62.327% | Wgt Acc: 59.043% | Dur: 13.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   4.  13.  11.]
 [  2.  36.  13.   2.  10.]
 [  3.  16.  37.   5.  19.]
 [ 16.   5.   6.  51.   5.]
 [ 10.  19.  15.  15. 135.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.296 | Acc: 86.750% | Wgt Acc: 90.263%
	I - Batch: 100 | Loss: 0.318 | Acc: 85.938% | Wgt Acc: 89.689%
	I - Batch: 150 | Loss: 0.336 | Acc: 85.833% | Wgt Acc: 89.329%
	I - Batch: 200 | Loss: 0.337 | Acc: 85.938% | Wgt Acc: 89.434%
I - num batch: 222
I - Train -- Loss: 0.338 | Acc: 85.791% | Wgt Acc: 89.484% | LR: 6.250000e-04 | Dur: 135.05s
I - Confusion Matrix: [row->prediction - col->label]
[[642.   0.   0.  18.  99.]
 [  6. 545.   8.   2.  63.]
 [  3.  11. 696.   5. 109.]
 [ 16.   4.   2. 493.  62.]
 [ 30.  18.  28.  20. 667.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.124 | Acc: 63.905% | Wgt Acc: 61.174% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   3.  16.  18.]
 [  0.  38.   7.   1.   6.]
 [  2.  10.  26.   1.   7.]
 [ 13.   9.  14.  60.  14.]
 [  8.  17.  25.   8. 135.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.306 | Acc: 88.375% | Wgt Acc: 91.761%
	I - Batch: 100 | Loss: 0.304 | Acc: 88.125% | Wgt Acc: 91.491%
	I - Batch: 150 | Loss: 0.306 | Acc: 87.667% | Wgt Acc: 91.090%
	I - Batch: 200 | Loss: 0.332 | Acc: 86.594% | Wgt Acc: 89.997%
I - num batch: 222
I - Train -- Loss: 0.336 | Acc: 86.496% | Wgt Acc: 89.836% | LR: 6.250000e-04 | Dur: 134.26s
I - Confusion Matrix: [row->prediction - col->label]
[[643.   6.   4.  14.  94.]
 [  3. 547.   8.   4.  52.]
 [  5.   5. 688.   3.  98.]
 [ 17.   3.   5. 497.  63.]
 [ 29.  17.  29.  20. 693.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.148 | Acc: 63.511% | Wgt Acc: 60.328% | Dur: 16.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   5.   4.  19.  15.]
 [  0.  38.   4.   0.   2.]
 [  1.  10.  31.   1.  10.]
 [  8.  10.  14.  49.  17.]
 [ 11.  15.  22.  17. 136.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.288 | Acc: 89.500% | Wgt Acc: 92.798%
	I - Batch: 100 | Loss: 0.299 | Acc: 87.812% | Wgt Acc: 91.130%
	I - Batch: 150 | Loss: 0.309 | Acc: 86.958% | Wgt Acc: 90.257%
	I - Batch: 200 | Loss: 0.313 | Acc: 86.969% | Wgt Acc: 90.427%
I - num batch: 222
I - Train -- Loss: 0.312 | Acc: 87.003% | Wgt Acc: 90.368% | LR: 6.250000e-04 | Dur: 137.95s
I - Confusion Matrix: [row->prediction - col->label]
[[636.   3.   1.  15. 101.]
 [  2. 555.   4.   3.  51.]
 [  5.   2. 699.   3.  91.]
 [ 19.   3.   5. 499.  60.]
 [ 35.  15.  25.  18. 697.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.087 | Acc: 63.116% | Wgt Acc: 62.188% | Dur: 20.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   6.   4.  10.  16.]
 [  1.  43.   6.   2.  12.]
 [  1.   8.  32.   1.  13.]
 [ 14.   5.  12.  61.  18.]
 [  9.  16.  21.  12. 121.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.293 | Acc: 87.250% | Wgt Acc: 91.224%
	I - Batch: 100 | Loss: 0.290 | Acc: 88.000% | Wgt Acc: 91.603%
	I - Batch: 150 | Loss: 0.293 | Acc: 87.792% | Wgt Acc: 91.586%
	I - Batch: 200 | Loss: 0.304 | Acc: 87.531% | Wgt Acc: 91.137%
I - num batch: 222
I - Train -- Loss: 0.305 | Acc: 87.454% | Wgt Acc: 91.100% | LR: 6.250000e-04 | Dur: 134.92s
I - Confusion Matrix: [row->prediction - col->label]
[[643.   0.   3.  14.  93.]
 [  6. 561.   3.   4.  57.]
 [  2.   7. 707.   2.  93.]
 [ 15.   0.   2. 504.  70.]
 [ 31.  10.  19.  14. 687.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.151 | Acc: 62.722% | Wgt Acc: 59.910% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   2.   3.   7.   9.]
 [  0.  36.   8.   2.  10.]
 [  3.  12.  33.   3.  11.]
 [ 20.   7.  12.  63.  16.]
 [ 13.  21.  19.  11. 134.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.296 | Acc: 88.750% | Wgt Acc: 91.551%
	I - Batch: 100 | Loss: 0.289 | Acc: 88.250% | Wgt Acc: 91.277%
	I - Batch: 150 | Loss: 0.294 | Acc: 88.042% | Wgt Acc: 91.153%
	I - Batch: 200 | Loss: 0.302 | Acc: 87.875% | Wgt Acc: 91.072%
I - num batch: 222
I - Train -- Loss: 0.306 | Acc: 87.680% | Wgt Acc: 90.885% | LR: 6.250000e-04 | Dur: 135.25s
I - Confusion Matrix: [row->prediction - col->label]
[[648.   3.   6.  12.  83.]
 [  2. 553.  11.   3.  42.]
 [  4.   6. 694.   3. 101.]
 [ 13.   4.   3. 503.  62.]
 [ 30.  12.  20.  17. 712.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.107 | Acc: 61.341% | Wgt Acc: 61.070% | Dur: 16.29s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   1.   3.  12.  12.]
 [  1.  43.  13.   1.  14.]
 [  0.  17.  38.   4.  27.]
 [ 21.   4.   9.  59.  14.]
 [  8.  13.  12.  10. 113.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.295 | Acc: 88.625% | Wgt Acc: 92.083%
	I - Batch: 100 | Loss: 0.302 | Acc: 88.438% | Wgt Acc: 91.784%
	I - Batch: 150 | Loss: 0.305 | Acc: 87.917% | Wgt Acc: 91.241%
	I - Batch: 200 | Loss: 0.313 | Acc: 87.688% | Wgt Acc: 91.062%
I - num batch: 222
I - Train -- Loss: 0.312 | Acc: 87.849% | Wgt Acc: 91.150% | LR: 6.250000e-04 | Dur: 134.11s
I - Confusion Matrix: [row->prediction - col->label]
[[650.   3.   2.  12.  81.]
 [  1. 553.   2.   8.  62.]
 [  4.   7. 704.   3.  79.]
 [ 11.   2.   1. 501.  70.]
 [ 31.  13.  25.  14. 708.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.094 | Acc: 64.103% | Wgt Acc: 62.418% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   5.  21.  19.]
 [  1.  46.  10.   3.   7.]
 [  1.  15.  33.   3.  17.]
 [  7.   3.  10.  52.  10.]
 [ 12.  10.  17.   7. 127.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.308 | Acc: 86.500% | Wgt Acc: 89.771%
	I - Batch: 100 | Loss: 0.316 | Acc: 86.688% | Wgt Acc: 89.759%
	I - Batch: 150 | Loss: 0.312 | Acc: 86.833% | Wgt Acc: 89.941%
	I - Batch: 200 | Loss: 0.317 | Acc: 86.844% | Wgt Acc: 89.961%
I - num batch: 222
I - Train -- Loss: 0.321 | Acc: 86.749% | Wgt Acc: 89.803% | LR: 6.250000e-04 | Dur: 137.51s
I - Confusion Matrix: [row->prediction - col->label]
[[633.   5.   3.  18.  93.]
 [  9. 542.   6.   5.  59.]
 [  7.   5. 697.   2.  85.]
 [ 12.   3.   5. 495.  53.]
 [ 36.  23.  23.  18. 710.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.120 | Acc: 63.708% | Wgt Acc: 59.952% | Dur: 15.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   3.  19.  11.]
 [  0.  39.   6.   2.   9.]
 [  2.  19.  37.   3.  14.]
 [ 12.   3.   8.  45.   6.]
 [ 12.  15.  21.  17. 140.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.297 | Acc: 87.500% | Wgt Acc: 90.917%
	I - Batch: 100 | Loss: 0.287 | Acc: 88.625% | Wgt Acc: 92.093%
	I - Batch: 150 | Loss: 0.291 | Acc: 88.250% | Wgt Acc: 91.573%
	I - Batch: 200 | Loss: 0.292 | Acc: 87.969% | Wgt Acc: 91.375%
I - num batch: 222
I - Train -- Loss: 0.296 | Acc: 87.764% | Wgt Acc: 91.160% | LR: 6.250000e-04 | Dur: 135.70s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   2.   3.  11.  76.]
 [  3. 556.   3.   0.  51.]
 [  6.   3. 694.   4. 103.]
 [  9.   4.   6. 505.  67.]
 [ 24.  13.  28.  18. 703.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.123 | Acc: 62.525% | Wgt Acc: 60.004% | Dur: 14.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   4.  21.  12.]
 [  0.  38.   7.   0.   9.]
 [  2.  17.  35.   2.  16.]
 [ 13.   7.  13.  52.  13.]
 [ 11.  13.  16.  11. 130.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.250 | Acc: 90.125% | Wgt Acc: 93.018%
	I - Batch: 100 | Loss: 0.276 | Acc: 88.938% | Wgt Acc: 91.838%
	I - Batch: 150 | Loss: 0.292 | Acc: 87.958% | Wgt Acc: 90.924%
	I - Batch: 200 | Loss: 0.299 | Acc: 87.344% | Wgt Acc: 90.567%
I - num batch: 222
I - Train -- Loss: 0.304 | Acc: 87.229% | Wgt Acc: 90.424% | LR: 6.250000e-04 | Dur: 137.24s
I - Confusion Matrix: [row->prediction - col->label]
[[646.   4.   5.  21.  83.]
 [  4. 554.   5.   4.  51.]
 [  4.   6. 695.   4.  82.]
 [ 14.   2.   1. 492.  77.]
 [ 29.  12.  28.  17. 707.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.152 | Acc: 61.736% | Wgt Acc: 59.158% | Dur: 15.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   3.   4.  12.  17.]
 [  1.  41.  11.   3.   6.]
 [  2.  15.  36.   5.  17.]
 [ 20.   6.   8.  56.  10.]
 [ 15.  13.  16.  10. 130.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.326 | Acc: 86.875% | Wgt Acc: 90.372%
	I - Batch: 100 | Loss: 0.323 | Acc: 86.875% | Wgt Acc: 90.410%
	I - Batch: 150 | Loss: 0.313 | Acc: 87.208% | Wgt Acc: 90.742%
	I - Batch: 200 | Loss: 0.309 | Acc: 87.312% | Wgt Acc: 90.861%
I - num batch: 222
I - Train -- Loss: 0.309 | Acc: 87.257% | Wgt Acc: 90.747% | LR: 6.250000e-04 | Dur: 133.34s
I - Confusion Matrix: [row->prediction - col->label]
[[636.   0.   2.  11.  96.]
 [  2. 554.   2.   2.  52.]
 [  5.   7. 709.   3. 100.]
 [ 13.   3.   0. 503.  59.]
 [ 41.  14.  21.  19. 693.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.142 | Acc: 62.722% | Wgt Acc: 58.301% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   2.  14.  12.]
 [  0.  30.   7.   1.   4.]
 [  2.  12.  31.   2.  10.]
 [ 12.   3.  10.  54.  10.]
 [ 15.  31.  25.  15. 144.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.313 | Acc: 86.625% | Wgt Acc: 90.140%
	I - Batch: 100 | Loss: 0.310 | Acc: 87.000% | Wgt Acc: 90.241%
	I - Batch: 150 | Loss: 0.306 | Acc: 86.875% | Wgt Acc: 90.325%
	I - Batch: 200 | Loss: 0.304 | Acc: 87.500% | Wgt Acc: 90.746%
I - num batch: 222
I - Train -- Loss: 0.310 | Acc: 87.116% | Wgt Acc: 90.354% | LR: 6.250000e-04 | Dur: 134.57s
I - Confusion Matrix: [row->prediction - col->label]
[[638.   1.   3.  14.  91.]
 [  3. 550.   3.   5.  59.]
 [  4.   4. 702.   5.  92.]
 [ 18.   4.   1. 496.  54.]
 [ 34.  19.  25.  18. 704.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.220 | Acc: 65.483% | Wgt Acc: 60.798% | Dur: 15.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   0.   2.   8.   9.]
 [  0.  31.   3.   0.   2.]
 [  1.  13.  30.   2.   6.]
 [ 17.   7.  15.  64.  11.]
 [ 15.  27.  25.  12. 152.]]

I - Local maximum validation set accuracy:  65.48

I - Validation set results: 
[14-1-2-2.80][50-3-4-2.09][124-2-2-1.19][127-0-0-5.99][443-2-2-4.46][567-0-0-3.76][573-1-1-1.05][615-0-0-5.17][695-1-1-1.79][722-3-3-6.39]
[826-0-0-6.20][878-0-0-4.41][1103-0-4-3.24][1212-3-0-1.70][1368-0-0-3.95][2181-2-3-2.54][2476-2-2-2.24][2721-2-2-2.19][2818-1-4-2.41][2886-2-1-1.62]
[3231-2-2-5.40][3333-2-3-2.41][3482-2-4-3.56][3536-3-4-2.44][3625-1-1-2.32][3909-0-0-3.28][4035-0-0-4.34][4140-0-4-2.61][4214-1-3-2.98][4346-1-4-2.50]
[4581-2-2-1.97][4708-3-3-1.86][4838-3-3-2.51][4845-1-4-1.49][4868-0-0-5.43][4939-0-4-1.64][4984-2-3-4.28][5078-1-4-2.00][5396-0-0-8.66][5479-1-1-1.78]
[5717-0-0-4.01][5843-1-1-2.21][5949-3-3-4.24][5987-2-4-4.12][6014-3-3-2.80][6033-3-3-3.44][6313-0-3-3.82][6421-3-3-5.47][6500-1-2-0.91][6583-3-3-6.81]
[6683-3-3-2.36][6825-2-1-3.10][6998-3-3-2.69][7049-3-3-3.27][7517-1-1-2.28][7521-1-1-1.63][7528-1-3-2.04][7949-1-2-3.18][8135-1-4-2.23][8185-3-0-5.52]
[8269-3-4-2.05][8273-3-3-5.52][8543-3-0-6.18][8666-1-1-1.53][8672-0-0-5.31][8903-1-3-3.14][9001-2-4-3.21][9036-2-2-5.76][9281-3-4-2.59][9300-2-2-4.29]
[9571-0-3-2.19][9617-1-1-2.55][9644-2-4-0.74][9705-2-4-1.73][9801-0-0-2.12][9803-3-3-3.60][9865-3-3-8.34][9896-2-2-2.56][10314-1-4-1.07][10337-3-3-6.16]
[10403-0-4-2.09][10653-2-4-2.31][10704-2-2-2.19][10719-1-4-2.13][10727-1-2-1.23][10836-0-0-8.57][10969-2-3-3.45][11042-0-3-0.98][11088-1-1-5.74][11322-0-0-5.97]
[11398-2-4-1.95][11499-0-0-4.40][11502-3-3-4.03][11512-3-3-2.57][11608-1-2-2.97][11610-0-0-2.72][11692-0-0-2.75][11905-0-0-3.72][11993-1-1-2.14][12002-2-3-4.47]
[12052-0-0-4.10][12201-0-3-6.07][12235-2-4-2.23][12320-1-4-5.16][12377-2-4-3.54][12398-2-3-3.24][12503-1-4-1.97][12617-0-0-0.53][12685-3-3-1.56][12738-2-3-2.07]
[12742-2-2-5.07][12823-0-3-4.53][13110-1-1-3.37][13240-3-3-3.55][13253-1-2-2.13][13273-0-0-7.31][13634-1-4-2.62][13763-2-2-1.87][13905-3-3-1.65][14060-2-4-1.84]
[14065-3-3-3.67][14147-3-3-2.71][14595-2-2-2.16][14687-2-2-3.29][14788-2-2-1.62][14869-1-4-2.20][14872-3-4-2.40][14877-1-1-3.63][14927-0-3-3.57][15066-0-0-7.34]
[15175-1-4-3.22][15178-2-4-3.15][15375-3-3-2.23][15389-3-3-4.80][15568-2-4-2.32][15675-3-3-5.94][15869-1-4-1.33][16207-3-0-1.52][16236-0-0-2.43][16302-3-0-3.72]
[16331-2-2-9.32][16381-0-3-2.69][16488-1-1-5.24][16495-0-0-1.03][16650-0-0-8.24][16719-1-4-1.93][16801-0-0-5.32][16828-0-0-4.24][17137-3-3-3.11][17245-1-4-1.52]
[17278-3-4-2.78][17282-0-2-4.16][17311-2-2-3.06][17336-2-1-1.34][17608-3-3-7.59][17627-0-0-0.90][17877-3-4-2.78][17924-1-3-2.44][17984-3-3-4.28][18211-0-3-3.17]
[18276-3-0-3.45][18287-1-4-1.00][18394-0-0-6.16][18428-0-4-1.00][18442-0-3-3.23][18478-3-3-3.88][18607-0-0-2.08][18616-0-4-2.39][18663-0-0-4.34][18718-0-0-5.63]
[18766-2-4-1.01][18824-2-4-3.47][18890-3-2-1.99][18930-3-4-1.95][18938-3-3-4.50][19817-1-1-2.95][19839-0-4-1.43][19930-3-3-4.16][19944-0-4-3.38][20036-2-2-7.28]
[20101-3-3-2.93][20474-1-1-3.65][20547-3-3-1.63][20929-2-2-3.94][21245-1-4-0.82][21257-3-3-1.56][21293-1-2-4.24][21316-1-1-1.06][21384-1-4-3.63][21448-1-1-5.26]
[21483-0-0-3.82][21487-2-2-4.53][21714-0-3-2.39][21943-3-2-2.22][21947-0-0-5.13][21948-0-0-7.43][21965-2-2-2.15][21998-1-1-2.52][22025-0-4-3.03][22228-3-3-4.53]
[22446-1-1-6.13][22494-3-3-3.05][22757-0-0-7.16][22811-3-3-6.00][22976-3-4-3.40][22985-3-3-4.38][23014-0-0-3.86][23112-1-1-6.16][23144-3-3-6.12][23168-2-4-1.65]
[23219-0-3-1.95][23363-3-3-7.60][23470-0-0-2.06][23486-2-4-2.05][23497-0-3-8.40][23516-0-0-5.50][23690-1-4-3.17][23921-2-2-2.71][23936-1-2-3.05][24040-3-4-2.64]
[24111-1-4-3.56][24182-0-0-7.74][24238-3-3-5.17][24290-2-0-2.51][24345-0-0-4.36][24364-1-3-1.18][24427-3-3-2.80][24477-2-2-4.65][24495-2-4-1.32][24893-2-2-2.44]
[25012-1-4-2.22][25121-2-4-3.84][25165-3-3-2.52][25183-0-4-2.39][25297-3-3-5.18][25398-0-0-6.19][25574-2-4-2.34][25644-1-2-3.41][25718-1-2-1.92][25774-2-4-2.39]
[26032-3-3-4.85][26051-3-3-8.06][26120-0-4-3.26][26321-1-1-7.55][26732-1-1-1.83][26784-3-3-7.59][26827-3-3-1.69][26833-0-3-9.18][26838-2-3-2.98][26860-1-4-2.88]
[26948-0-0-1.76][27049-3-0-5.52][27098-1-4-0.97][27526-0-0-5.82][27639-3-3-4.10][27698-3-3-5.06][27772-0-0-5.22][27890-1-1-5.52][28040-0-0-2.23][28503-2-4-2.14]
[28577-1-2-2.47][28959-0-0-7.65][29198-3-4-2.19][29777-0-0-8.30][29877-2-3-0.77][30035-1-1-3.04][30098-0-3-4.56][30326-1-1-6.55][30572-2-2-2.63][30716-0-4-4.39]
[30806-2-3-2.44][30906-1-1-5.14][31007-0-0-2.49][31181-3-3-2.65][31238-0-3-4.21][31347-0-0-5.67][31422-2-2-4.03][31429-3-3-3.38][31431-0-3-1.83][31432-1-1-4.52]
[31477-0-0-4.34][31524-1-2-1.39][31597-1-2-3.02][31619-1-4-1.70][31701-0-0-3.80][31755-0-0-5.35][31854-3-3-4.29][32074-1-3-2.76][32078-3-3-5.49][32111-1-1-3.68]
[32127-1-1-3.31][32140-3-3-5.15][32263-2-4-3.07][32365-0-0-4.12][32411-2-0-5.23][32429-3-3-3.97][32473-3-0-1.94][32574-3-3-5.45][32584-0-4-2.44][32622-0-4-1.97]
[32858-3-3-3.18][32969-3-3-5.49][33016-2-2-5.66][33031-1-3-2.68][33035-2-2-2.99][33133-2-4-0.61][33173-2-3-1.93][33175-3-4-3.37][33306-3-3-5.90][33309-2-3-1.91]
[33474-0-4-1.23][33478-2-3-2.20][33618-1-1-2.25][33712-0-3-3.43][33782-2-4-2.51][33914-3-3-3.60][34076-3-3-1.87][34112-2-2-3.47][34138-2-3-3.29][34239-1-4-0.92]
[34364-2-2-4.64][34617-1-4-2.15][34751-3-3-6.07][34783-2-4-3.90][35015-3-3-3.15][35018-1-4-3.01][35288-2-3-2.22][0-4-4-4.07][1-4-4-2.99][2-4-4-3.47]
[3-4-4-2.86][4-4-4-2.45][5-4-4-0.99][6-4-4-3.39][7-4-4-4.39][8-4-4-1.35][9-4-0-2.57][10-4-4-4.00][11-4-4-3.71][12-4-4-1.81]
[14-4-4-1.85][15-4-3-3.63][16-4-4-2.61][17-4-4-1.56][18-4-4-5.28][19-4-3-3.09][20-4-4-1.55][21-4-4-3.06][22-4-4-3.32][23-4-4-2.40]
[24-4-4-8.28][25-4-3-3.62][26-4-4-2.22][27-4-4-4.75][28-4-4-5.30][29-4-1-1.79][30-4-4-1.74][31-4-4-2.56][32-4-4-4.03][33-4-4-2.41]
[34-4-4-2.38][35-4-4-2.39][37-4-4-3.88][39-4-0-4.23][40-4-4-2.19][41-4-4-2.60][42-4-4-1.77][43-4-2-2.68][45-4-4-3.18][46-4-4-4.61]
[47-4-4-5.65][48-4-4-4.76][51-4-4-3.70][52-4-4-2.38][53-4-4-2.76][54-4-4-3.32][55-4-4-3.07][56-4-1-2.02][57-4-0-2.93][58-4-2-3.83]
[59-4-4-2.99][60-4-4-1.78][61-4-4-5.37][62-4-3-4.87][63-4-4-4.07][64-4-4-2.82][65-4-4-6.25][66-4-4-4.51][67-4-4-1.66][68-4-4-1.87]
[69-4-0-1.73][70-4-4-2.51][72-4-4-2.50][73-4-4-2.36][74-4-4-1.48][75-4-2-2.01][77-4-4-6.51][78-4-2-1.50][79-4-4-4.28][80-4-4-4.69]
[81-4-4-3.27][82-4-4-1.74][83-4-4-2.94][84-4-4-2.94][85-4-4-4.19][86-4-4-4.45][87-4-4-5.63][88-4-4-2.23][89-4-3-2.56][90-4-4-1.34]
[91-4-4-1.63][92-4-4-2.07][93-4-4-2.91][94-4-4-3.34][95-4-4-2.99][96-4-4-3.36][97-4-4-3.17][98-4-4-1.82][99-4-4-2.02][100-4-4-2.10]
[101-4-4-7.25][102-4-4-1.74][103-4-3-1.36][104-4-4-3.65][105-4-4-3.27][106-4-4-4.81][107-4-4-3.23][108-4-4-2.71][109-4-4-3.14][110-4-4-1.97]
[111-4-0-4.40][112-4-4-1.81][113-4-4-2.33][114-4-3-3.09][115-4-4-3.33][116-4-4-2.58][117-4-4-3.39][119-4-4-2.04][121-4-4-3.11][122-4-4-3.51]
[124-4-4-1.29][125-4-4-3.47][126-4-4-5.82][127-4-4-1.72][128-4-4-1.39][129-4-4-2.31][130-4-4-2.13][131-4-2-2.22][132-4-4-2.75][133-4-4-4.68]
[135-4-4-2.00][136-4-4-1.94][137-4-4-3.16][138-4-4-3.11][139-4-4-3.33][140-4-4-1.91][141-4-3-1.54][142-4-4-4.77][143-4-4-3.85][144-4-4-4.79]
[145-4-4-2.67][148-4-4-2.57][149-4-4-2.53][150-4-4-4.07][151-4-4-4.85][152-4-4-2.90][153-4-4-3.66][154-4-4-8.03][155-4-4-5.30][156-4-4-1.16]
[157-4-0-1.58][158-4-4-2.07][160-4-4-2.98][161-4-4-1.06][162-4-4-1.80][164-4-4-2.92][165-4-4-3.47][167-4-4-3.38][168-4-4-3.36][170-4-4-2.72]
[171-4-4-3.67][172-4-4-3.94][173-4-4-4.29][174-4-0-4.26][175-4-4-3.35][177-4-4-3.46][178-4-4-3.19][179-4-4-3.01][180-4-4-4.84][181-4-3-3.28]
[182-4-3-3.55][183-4-4-4.12][184-4-4-2.86][186-4-0-1.99][187-4-4-3.14][188-4-4-3.61][189-4-4-2.12][190-4-4-1.38][191-4-4-4.22][192-4-4-2.22]
[193-4-2-1.83][194-4-3-2.50][195-4-0-2.81][196-4-4-1.33][197-4-4-3.24][198-4-4-7.17][199-4-4-2.59]
---------------------------
I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.314 | Acc: 86.625% | Wgt Acc: 89.794%
	I - Batch: 100 | Loss: 0.307 | Acc: 87.562% | Wgt Acc: 90.730%
	I - Batch: 150 | Loss: 0.302 | Acc: 87.625% | Wgt Acc: 90.812%
	I - Batch: 200 | Loss: 0.303 | Acc: 87.438% | Wgt Acc: 90.771%
I - num batch: 222
I - Train -- Loss: 0.304 | Acc: 87.426% | Wgt Acc: 90.649% | LR: 6.250000e-04 | Dur: 135.33s
I - Confusion Matrix: [row->prediction - col->label]
[[647.   3.   6.  16.  83.]
 [  4. 550.   3.   3.  41.]
 [  6.   2. 697.   5.  97.]
 [ 12.   3.   2. 499.  71.]
 [ 28.  20.  26.  15. 708.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.059 | Acc: 63.116% | Wgt Acc: 60.150% | Dur: 14.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   0.   2.  17.   9.]
 [  0.  46.   9.   3.  15.]
 [  1.  11.  41.   4.  19.]
 [  8.   2.   2.  40.   4.]
 [ 19.  19.  21.  22. 133.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.286 | Acc: 88.500% | Wgt Acc: 91.315%
	I - Batch: 100 | Loss: 0.296 | Acc: 88.125% | Wgt Acc: 91.211%
	I - Batch: 150 | Loss: 0.307 | Acc: 87.125% | Wgt Acc: 90.516%
	I - Batch: 200 | Loss: 0.300 | Acc: 87.094% | Wgt Acc: 90.506%
I - num batch: 222
I - Train -- Loss: 0.297 | Acc: 87.200% | Wgt Acc: 90.640% | LR: 6.250000e-04 | Dur: 135.74s
I - Confusion Matrix: [row->prediction - col->label]
[[647.   3.   3.  15.  91.]
 [  2. 551.   9.   2.  59.]
 [  5.   7. 697.   0. 100.]
 [ 13.   0.   3. 503.  55.]
 [ 30.  17.  22.  18. 695.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.079 | Acc: 63.905% | Wgt Acc: 62.512% | Dur: 14.62s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   5.   4.  23.  20.]
 [  1.  47.  10.   0.  12.]
 [  2.  12.  39.   3.  22.]
 [ 12.   2.   5.  48.   2.]
 [  7.  12.  17.  12. 124.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.271 | Acc: 90.000% | Wgt Acc: 93.131%
	I - Batch: 100 | Loss: 0.283 | Acc: 89.000% | Wgt Acc: 92.103%
	I - Batch: 150 | Loss: 0.277 | Acc: 89.500% | Wgt Acc: 92.536%
	I - Batch: 200 | Loss: 0.284 | Acc: 89.031% | Wgt Acc: 92.048%
I - num batch: 222
I - Train -- Loss: 0.291 | Acc: 88.554% | Wgt Acc: 91.715% | LR: 6.250000e-04 | Dur: 137.66s
I - Confusion Matrix: [row->prediction - col->label]
[[648.   3.   4.  10.  79.]
 [  0. 560.   4.   4.  45.]
 [  2.   2. 704.   1.  92.]
 [ 17.   3.   5. 506.  61.]
 [ 30.  10.  17.  17. 723.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.102 | Acc: 63.116% | Wgt Acc: 60.150% | Dur: 15.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   1.   3.  13.  11.]
 [  0.  36.   5.   3.   6.]
 [  4.  26.  41.   7.  24.]
 [ 10.   1.   5.  45.   6.]
 [  9.  14.  21.  18. 133.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.288 | Acc: 88.750% | Wgt Acc: 91.968%
	I - Batch: 100 | Loss: 0.265 | Acc: 89.750% | Wgt Acc: 92.867%
	I - Batch: 150 | Loss: 0.276 | Acc: 89.375% | Wgt Acc: 92.438%
	I - Batch: 200 | Loss: 0.278 | Acc: 89.031% | Wgt Acc: 92.231%
I - num batch: 222
I - Train -- Loss: 0.279 | Acc: 89.061% | Wgt Acc: 92.307% | LR: 6.250000e-04 | Dur: 135.48s
I - Confusion Matrix: [row->prediction - col->label]
[[659.   2.   5.   8.  86.]
 [  4. 559.   3.   1.  44.]
 [  1.   3. 703.   0.  84.]
 [  7.   1.   4. 514.  62.]
 [ 26.  13.  19.  15. 724.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.141 | Acc: 64.103% | Wgt Acc: 59.858% | Dur: 14.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   3.  15.  12.]
 [  0.  33.   3.   2.   7.]
 [  1.  15.  35.   2.  11.]
 [ 13.   6.   8.  53.   5.]
 [ 15.  21.  26.  14. 145.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.268 | Acc: 89.000% | Wgt Acc: 91.849%
	I - Batch: 100 | Loss: 0.277 | Acc: 88.250% | Wgt Acc: 91.600%
	I - Batch: 150 | Loss: 0.275 | Acc: 88.542% | Wgt Acc: 91.831%
	I - Batch: 200 | Loss: 0.276 | Acc: 88.531% | Wgt Acc: 91.825%
I - num batch: 222
I - Train -- Loss: 0.277 | Acc: 88.582% | Wgt Acc: 91.839% | LR: 6.250000e-04 | Dur: 135.15s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   1.   2.   3.  76.]
 [  2. 558.   4.   2.  53.]
 [  3.   7. 694.   1.  90.]
 [ 10.   3.   3. 513.  62.]
 [ 24.   9.  31.  19. 719.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.185 | Acc: 62.919% | Wgt Acc: 58.677% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   3.  15.  10.]
 [  0.  36.   4.   0.   3.]
 [  3.  13.  32.   5.  15.]
 [ 16.   5.  11.  51.   9.]
 [ 12.  21.  25.  15. 143.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.258 | Acc: 90.875% | Wgt Acc: 93.870%
	I - Batch: 100 | Loss: 0.268 | Acc: 89.688% | Wgt Acc: 92.729%
	I - Batch: 150 | Loss: 0.283 | Acc: 88.917% | Wgt Acc: 92.095%
	I - Batch: 200 | Loss: 0.283 | Acc: 88.844% | Wgt Acc: 91.923%
I - num batch: 222
I - Train -- Loss: 0.288 | Acc: 88.582% | Wgt Acc: 91.628% | LR: 6.250000e-04 | Dur: 134.93s
I - Confusion Matrix: [row->prediction - col->label]
[[649.   1.   4.  11.  79.]
 [  5. 558.   4.   1.  37.]
 [  4.   5. 702.   2.  88.]
 [ 11.   2.   3. 504.  67.]
 [ 28.  12.  21.  20. 729.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.143 | Acc: 63.905% | Wgt Acc: 60.986% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   5.   5.  21.  11.]
 [  0.  44.   8.   3.  10.]
 [  1.  10.  36.   4.  14.]
 [ 12.   4.   8.  48.  10.]
 [ 14.  15.  18.  10. 135.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.267 | Acc: 89.500% | Wgt Acc: 92.830%
	I - Batch: 100 | Loss: 0.270 | Acc: 89.812% | Wgt Acc: 92.915%
	I - Batch: 150 | Loss: 0.284 | Acc: 88.625% | Wgt Acc: 91.936%
	I - Batch: 200 | Loss: 0.295 | Acc: 87.719% | Wgt Acc: 91.156%
I - num batch: 222
I - Train -- Loss: 0.291 | Acc: 88.018% | Wgt Acc: 91.295% | LR: 6.250000e-04 | Dur: 136.12s
I - Confusion Matrix: [row->prediction - col->label]
[[653.   1.   2.  15.  84.]
 [  3. 552.   8.   2.  52.]
 [  2.   5. 703.   4.  88.]
 [ 13.   2.   1. 503.  65.]
 [ 26.  18.  20.  14. 711.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.120 | Acc: 62.919% | Wgt Acc: 60.339% | Dur: 14.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   5.  12.   8.]
 [  0.  35.   6.   3.   9.]
 [  2.  19.  41.   7.  26.]
 [ 14.   4.   6.  53.   6.]
 [ 13.  17.  17.  11. 131.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.266 | Acc: 88.500% | Wgt Acc: 92.624%
	I - Batch: 100 | Loss: 0.257 | Acc: 89.500% | Wgt Acc: 93.056%
	I - Batch: 150 | Loss: 0.269 | Acc: 89.417% | Wgt Acc: 92.727%
	I - Batch: 200 | Loss: 0.282 | Acc: 88.875% | Wgt Acc: 92.109%
I - num batch: 222
I - Train -- Loss: 0.284 | Acc: 88.751% | Wgt Acc: 92.069% | LR: 6.250000e-04 | Dur: 133.84s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   2.   2.  11.  91.]
 [  1. 555.   1.   5.  42.]
 [  1.   7. 712.   1.  80.]
 [  6.   3.   2. 507.  71.]
 [ 31.  11.  17.  14. 716.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.040 | Acc: 65.680% | Wgt Acc: 62.407% | Dur: 14.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   4.   1.   9.   8.]
 [  0.  35.   6.   2.   6.]
 [  2.  20.  42.   4.  17.]
 [ 13.   3.   7.  56.   8.]
 [ 14.  16.  19.  15. 141.]]

I - Local maximum validation set accuracy:  65.68

I - Validation set results: 
[14-1-2-2.79][50-3-1-2.19][124-2-2-3.28][127-0-0-7.30][443-2-2-3.86][567-0-0-2.97][573-1-1-3.37][615-0-0-3.86][695-1-0-0.36][722-3-3-5.85]
[826-0-0-2.54][878-0-0-4.30][1103-0-4-1.78][1212-3-4-1.20][1368-0-0-6.12][2181-2-2-1.58][2476-2-2-2.96][2721-2-2-2.52][2818-1-4-0.65][2886-2-1-1.86]
[3231-2-2-7.88][3333-2-2-2.08][3482-2-2-2.89][3536-3-4-2.04][3625-1-1-6.24][3909-0-0-4.20][4035-0-0-3.96][4140-0-0-3.47][4214-1-4-2.48][4346-1-4-2.75]
[4581-2-2-3.21][4708-3-4-2.96][4838-3-3-1.55][4845-1-4-1.57][4868-0-0-6.08][4939-0-3-1.20][4984-2-3-3.87][5078-1-4-2.35][5396-0-0-6.07][5479-1-1-5.37]
[5717-0-0-2.21][5843-1-2-2.74][5949-3-3-5.13][5987-2-4-4.13][6014-3-2-1.37][6033-3-3-3.03][6313-0-3-1.88][6421-3-3-2.80][6500-1-2-1.71][6583-3-3-3.28]
[6683-3-3-2.10][6825-2-1-2.94][6998-3-3-2.61][7049-3-3-3.16][7517-1-1-4.16][7521-1-1-2.50][7528-1-3-0.97][7949-1-2-3.46][8135-1-0-1.32][8185-3-0-3.41]
[8269-3-4-1.59][8273-3-3-4.08][8543-3-0-2.08][8666-1-1-6.98][8672-0-0-4.17][8903-1-2-2.55][9001-2-2-4.25][9036-2-2-6.65][9281-3-3-0.84][9300-2-2-4.71]
[9571-0-4-1.34][9617-1-1-2.90][9644-2-1-2.66][9705-2-4-1.51][9801-0-4-1.65][9803-3-3-3.11][9865-3-3-3.96][9896-2-2-2.16][10314-1-1-1.24][10337-3-3-3.94]
[10403-0-4-2.01][10653-2-4-1.58][10704-2-2-5.55][10719-1-1-3.83][10727-1-2-1.25][10836-0-0-9.25][10969-2-3-1.43][11042-0-0-2.92][11088-1-1-8.27][11322-0-0-6.45]
[11398-2-4-2.31][11499-0-0-4.91][11502-3-3-4.33][11512-3-3-3.29][11608-1-2-4.39][11610-0-0-2.18][11692-0-0-3.23][11905-0-0-4.40][11993-1-1-4.08][12002-2-2-3.00]
[12052-0-0-3.62][12201-0-3-3.17][12235-2-4-2.44][12320-1-0-3.71][12377-2-4-1.76][12398-2-2-1.14][12503-1-2-2.11][12617-0-4-1.27][12685-3-4-1.37][12738-2-4-2.00]
[12742-2-2-4.04][12823-0-0-3.08][13110-1-1-1.35][13240-3-3-4.34][13253-1-4-1.90][13273-0-0-6.84][13634-1-1-1.96][13763-2-2-2.79][13905-3-2-2.02][14060-2-1-2.77]
[14065-3-3-2.03][14147-3-0-1.29][14595-2-2-3.51][14687-2-2-4.86][14788-2-2-4.73][14869-1-4-1.79][14872-3-4-1.24][14877-1-1-4.48][14927-0-3-1.94][15066-0-0-6.29]
[15175-1-1-1.70][15178-2-4-2.27][15375-3-3-0.95][15389-3-3-3.39][15568-2-4-1.41][15675-3-3-7.40][15869-1-2-2.67][16207-3-1-1.58][16236-0-2-0.92][16302-3-3-3.24]
[16331-2-2-10.26][16381-0-4-1.66][16488-1-1-4.90][16495-0-0-1.31][16650-0-0-6.87][16719-1-2-2.62][16801-0-0-7.56][16828-0-0-2.71][17137-3-3-2.39][17245-1-2-0.97]
[17278-3-4-2.34][17282-0-2-5.47][17311-2-2-3.75][17336-2-1-3.01][17608-3-3-6.60][17627-0-0-3.05][17877-3-4-1.98][17924-1-2-1.39][17984-3-3-4.31][18211-0-3-1.37]
[18276-3-0-1.55][18287-1-0-0.61][18394-0-0-3.63][18428-0-0-3.08][18442-0-3-2.26][18478-3-3-2.57][18607-0-0-2.58][18616-0-0-2.74][18663-0-0-3.53][18718-0-0-4.55]
[18766-2-2-3.88][18824-2-4-2.12][18890-3-3-1.99][18930-3-4-0.88][18938-3-3-2.19][19817-1-1-4.24][19839-0-4-1.66][19930-3-3-3.66][19944-0-4-3.15][20036-2-2-7.18]
[20101-3-3-1.53][20474-1-2-2.64][20547-3-3-2.02][20929-2-2-5.57][21245-1-2-2.15][21257-3-0-0.47][21293-1-2-5.37][21316-1-1-3.81][21384-1-4-3.24][21448-1-1-4.81]
[21483-0-0-4.11][21487-2-2-4.78][21714-0-3-1.01][21943-3-2-3.07][21947-0-0-3.10][21948-0-0-9.66][21965-2-2-5.62][21998-1-1-1.98][22025-0-4-2.96][22228-3-3-5.04]
[22446-1-1-4.54][22494-3-3-3.92][22757-0-0-5.22][22811-3-3-5.01][22976-3-4-3.64][22985-3-3-2.93][23014-0-3-4.28][23112-1-1-5.73][23144-3-3-3.01][23168-2-4-0.96]
[23219-0-0-4.27][23363-3-3-6.19][23470-0-0-0.54][23486-2-2-3.35][23497-0-3-8.37][23516-0-0-5.91][23690-1-4-2.63][23921-2-2-2.18][23936-1-2-3.91][24040-3-4-2.38]
[24111-1-4-1.93][24182-0-0-5.14][24238-3-3-2.28][24290-2-0-2.55][24345-0-0-1.88][24364-1-1-1.33][24427-3-0-3.43][24477-2-2-4.40][24495-2-4-0.96][24893-2-2-2.03]
[25012-1-4-1.65][25121-2-2-3.36][25165-3-3-2.23][25183-0-0-1.14][25297-3-3-5.39][25398-0-0-4.77][25574-2-4-1.72][25644-1-2-4.71][25718-1-2-3.29][25774-2-4-2.07]
[26032-3-3-3.74][26051-3-3-4.67][26120-0-4-1.90][26321-1-4-1.29][26732-1-1-3.14][26784-3-3-5.61][26827-3-4-1.92][26833-0-3-5.52][26838-2-4-1.46][26860-1-4-2.22]
[26948-0-0-2.60][27049-3-0-5.09][27098-1-4-1.24][27526-0-0-4.13][27639-3-3-2.87][27698-3-3-2.40][27772-0-0-5.36][27890-1-1-4.24][28040-0-0-1.44][28503-2-2-3.88]
[28577-1-1-2.76][28959-0-0-7.12][29198-3-3-1.79][29777-0-0-7.72][29877-2-3-0.77][30035-1-1-5.30][30098-0-3-4.54][30326-1-1-6.86][30572-2-2-4.44][30716-0-4-3.20]
[30806-2-4-2.72][30906-1-1-4.65][31007-0-0-3.35][31181-3-2-1.37][31238-0-3-1.89][31347-0-0-4.16][31422-2-2-3.74][31429-3-3-2.46][31431-0-4-1.86][31432-1-1-5.90]
[31477-0-0-4.30][31524-1-4-1.64][31597-1-2-3.86][31619-1-4-2.14][31701-0-0-3.61][31755-0-0-2.42][31854-3-3-4.16][32074-1-3-1.88][32078-3-3-2.08][32111-1-1-2.11]
[32127-1-1-3.55][32140-3-3-3.96][32263-2-4-1.59][32365-0-0-4.13][32411-2-3-4.00][32429-3-3-2.72][32473-3-0-1.29][32574-3-3-3.94][32584-0-4-2.75][32622-0-4-1.75]
[32858-3-0-2.81][32969-3-3-4.28][33016-2-2-5.48][33031-1-3-2.60][33035-2-2-2.88][33133-2-2-1.68][33173-2-3-2.24][33175-3-4-3.06][33306-3-3-3.45][33309-2-3-2.16]
[33474-0-3-1.19][33478-2-3-1.57][33618-1-1-2.35][33712-0-0-2.00][33782-2-4-4.02][33914-3-3-3.45][34076-3-4-2.16][34112-2-2-4.88][34138-2-2-2.20][34239-1-1-1.33]
[34364-2-2-6.13][34617-1-2-2.82][34751-3-3-4.04][34783-2-4-1.50][35015-3-4-2.87][35018-1-1-1.93][35288-2-1-0.63][0-4-4-4.46][1-4-4-5.18][2-4-4-2.79]
[3-4-4-2.01][4-4-4-1.53][5-4-4-0.89][6-4-4-2.76][7-4-4-1.69][8-4-4-1.93][9-4-0-2.53][10-4-4-4.74][11-4-4-4.70][12-4-4-2.45]
[14-4-4-0.94][15-4-3-1.98][16-4-4-2.98][17-4-4-2.46][18-4-4-3.48][19-4-3-1.69][20-4-3-1.45][21-4-2-2.42][22-4-4-2.41][23-4-2-2.71]
[24-4-4-6.72][25-4-4-2.67][26-4-4-0.96][27-4-4-3.60][28-4-4-3.93][29-4-2-2.15][30-4-4-2.93][31-4-4-3.28][32-4-4-3.43][33-4-2-3.38]
[34-4-4-2.39][35-4-4-2.38][37-4-4-3.60][39-4-0-3.96][40-4-4-1.73][41-4-1-2.71][42-4-4-1.86][43-4-4-2.66][45-4-4-2.55][46-4-4-3.98]
[47-4-4-4.67][48-4-4-1.98][51-4-4-3.78][52-4-4-2.44][53-4-4-2.98][54-4-3-2.60][55-4-4-1.71][56-4-1-1.55][57-4-0-1.52][58-4-2-4.68]
[59-4-0-3.67][60-4-4-1.86][61-4-4-3.96][62-4-4-2.69][63-4-4-4.28][64-4-2-2.48][65-4-4-4.75][66-4-4-3.49][67-4-4-1.88][68-4-1-2.55]
[69-4-2-0.76][70-4-4-2.25][72-4-4-2.19][73-4-1-2.04][74-4-4-2.10][75-4-4-1.27][77-4-4-5.91][78-4-2-1.01][79-4-4-2.69][80-4-4-3.04]
[81-4-4-3.57][82-4-4-0.80][83-4-4-3.17][84-4-4-3.89][85-4-4-3.31][86-4-4-2.00][87-4-4-5.05][88-4-4-3.10][89-4-4-3.12][90-4-4-1.65]
[91-4-4-1.88][92-4-4-0.71][93-4-4-1.40][94-4-4-2.31][95-4-4-1.99][96-4-4-2.35][97-4-4-2.77][98-4-4-2.04][99-4-4-1.62][100-4-4-1.94]
[101-4-4-4.90][102-4-4-1.48][103-4-2-2.69][104-4-4-2.32][105-4-2-2.07][106-4-4-3.52][107-4-4-3.82][108-4-4-2.78][109-4-4-1.69][110-4-4-1.44]
[111-4-0-2.75][112-4-4-0.67][113-4-4-3.03][114-4-3-2.10][115-4-4-1.43][116-4-4-1.13][117-4-4-3.34][119-4-4-3.20][121-4-4-1.78][122-4-4-3.01]
[124-4-3-1.50][125-4-2-3.36][126-4-4-4.19][127-4-1-0.98][128-4-4-1.82][129-4-4-1.68][130-4-4-2.14][131-4-2-4.48][132-4-4-2.69][133-4-4-4.59]
[135-4-2-4.18][136-4-4-0.94][137-4-4-2.59][138-4-4-2.35][139-4-4-3.08][140-4-4-1.67][141-4-0-2.04][142-4-4-4.40][143-4-4-3.77][144-4-4-4.18]
[145-4-4-3.38][148-4-0-4.99][149-4-4-2.62][150-4-4-3.94][151-4-4-2.88][152-4-4-2.10][153-4-4-2.48][154-4-4-4.83][155-4-4-3.57][156-4-1-1.01]
[157-4-4-1.35][158-4-4-1.69][160-4-4-2.05][161-4-4-1.45][162-4-4-0.70][164-4-4-1.99][165-4-4-2.95][167-4-4-2.26][168-4-4-2.51][170-4-4-2.81]
[171-4-4-3.31][172-4-4-2.43][173-4-4-2.70][174-4-0-3.25][175-4-4-3.49][177-4-4-3.83][178-4-2-2.81][179-4-4-2.86][180-4-4-3.64][181-4-3-1.81]
[182-4-4-2.26][183-4-4-3.18][184-4-4-3.16][186-4-4-1.22][187-4-2-2.98][188-4-4-3.67][189-4-4-2.61][190-4-4-1.92][191-4-4-2.96][192-4-4-3.04]
[193-4-2-3.24][194-4-3-1.85][195-4-4-2.06][196-4-4-2.32][197-4-4-2.94][198-4-4-7.39][199-4-2-2.16]
---------------------------
I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.254 | Acc: 89.750% | Wgt Acc: 93.049%
	I - Batch: 100 | Loss: 0.265 | Acc: 89.438% | Wgt Acc: 92.483%
	I - Batch: 150 | Loss: 0.270 | Acc: 89.125% | Wgt Acc: 92.370%
	I - Batch: 200 | Loss: 0.264 | Acc: 89.188% | Wgt Acc: 92.475%
I - num batch: 222
I - Train -- Loss: 0.268 | Acc: 88.977% | Wgt Acc: 92.245% | LR: 6.250000e-04 | Dur: 137.37s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   2.   3.  11.  87.]
 [  1. 560.   7.   1.  48.]
 [  4.   1. 704.   2.  83.]
 [  5.   1.   2. 513.  60.]
 [ 30.  14.  18.  11. 722.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.065 | Acc: 63.116% | Wgt Acc: 60.965% | Dur: 13.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   1.   3.   5.   9.]
 [  2.  49.  21.   3.  19.]
 [  1.  12.  32.   2.  15.]
 [ 16.   0.   5.  58.   7.]
 [ 18.  16.  14.  18. 130.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.276 | Acc: 88.500% | Wgt Acc: 91.801%
	I - Batch: 100 | Loss: 0.277 | Acc: 88.625% | Wgt Acc: 91.970%
	I - Batch: 150 | Loss: 0.270 | Acc: 88.917% | Wgt Acc: 92.195%
	I - Batch: 200 | Loss: 0.282 | Acc: 88.375% | Wgt Acc: 91.838%
I - num batch: 222
I - Train -- Loss: 0.281 | Acc: 88.441% | Wgt Acc: 91.773% | LR: 6.250000e-04 | Dur: 135.68s
I - Confusion Matrix: [row->prediction - col->label]
[[652.   1.   1.  12. 104.]
 [  6. 557.   3.   5.  45.]
 [  6.   3. 706.   0.  83.]
 [  9.   1.   1. 509.  55.]
 [ 24.  16.  23.  12. 713.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.092 | Acc: 61.538% | Wgt Acc: 57.476% | Dur: 16.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   0.   3.   7.  10.]
 [  1.  34.   7.   2.  11.]
 [  2.  14.  36.   5.  12.]
 [ 12.   2.   5.  49.   8.]
 [ 19.  28.  24.  23. 139.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.272 | Acc: 89.125% | Wgt Acc: 92.129%
	I - Batch: 100 | Loss: 0.269 | Acc: 88.875% | Wgt Acc: 92.130%
	I - Batch: 150 | Loss: 0.278 | Acc: 88.083% | Wgt Acc: 91.287%
	I - Batch: 200 | Loss: 0.286 | Acc: 87.906% | Wgt Acc: 91.066%
I - num batch: 222
I - Train -- Loss: 0.288 | Acc: 87.849% | Wgt Acc: 90.997% | LR: 6.250000e-04 | Dur: 134.95s
I - Confusion Matrix: [row->prediction - col->label]
[[644.   3.   2.  12.  93.]
 [  6. 552.   2.   0.  56.]
 [  3.   7. 704.   2.  77.]
 [ 12.   1.   2. 500.  58.]
 [ 32.  15.  24.  24. 716.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.083 | Acc: 65.286% | Wgt Acc: 62.282% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   2.  11.  11.]
 [  0.  39.   5.   1.   6.]
 [  3.  12.  31.   2.   7.]
 [ 13.   2.  15.  58.  17.]
 [  8.  21.  22.  14. 139.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.265 | Acc: 89.750% | Wgt Acc: 93.036%
	I - Batch: 100 | Loss: 0.286 | Acc: 88.938% | Wgt Acc: 92.070%
	I - Batch: 150 | Loss: 0.290 | Acc: 88.625% | Wgt Acc: 91.838%
	I - Batch: 200 | Loss: 0.296 | Acc: 88.312% | Wgt Acc: 91.659%
I - num batch: 222
I - Train -- Loss: 0.297 | Acc: 88.385% | Wgt Acc: 91.659% | LR: 6.250000e-04 | Dur: 134.46s
I - Confusion Matrix: [row->prediction - col->label]
[[659.   2.   3.  12.  88.]
 [  4. 555.   5.   4.  51.]
 [  4.   6. 700.   4.  95.]
 [  8.   2.   2. 506.  51.]
 [ 22.  13.  24.  12. 715.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.067 | Acc: 65.483% | Wgt Acc: 61.394% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   2.  16.   7.]
 [  0.  42.  11.   1.   8.]
 [  3.  12.  38.   3.  14.]
 [ 13.   1.   7.  48.   5.]
 [ 14.  20.  17.  18. 146.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.242 | Acc: 90.625% | Wgt Acc: 93.811%
	I - Batch: 100 | Loss: 0.238 | Acc: 91.125% | Wgt Acc: 93.897%
	I - Batch: 150 | Loss: 0.243 | Acc: 90.667% | Wgt Acc: 93.727%
	I - Batch: 200 | Loss: 0.247 | Acc: 90.281% | Wgt Acc: 93.377%
I - num batch: 208
I - Train -- Loss: 0.249 | Acc: 90.024% | Wgt Acc: 93.241% | LR: 6.250000e-04 | Dur: 125.64s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   3.   3.   8.  57.]
 [  2. 561.   2.   2.  48.]
 [  1.   3. 713.   1.  75.]
 [ 12.   1.   3. 518.  58.]
 [ 21.  10.  13.   9. 543.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.123 | Acc: 61.933% | Wgt Acc: 59.962% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   6.   5.  18.  20.]
 [  1.  36.   6.   2.   6.]
 [  2.  18.  39.   4.  22.]
 [ 11.   4.   8.  53.   7.]
 [ 13.  14.  17.   9. 125.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.273 | Acc: 87.750% | Wgt Acc: 91.880%
	I - Batch: 100 | Loss: 0.281 | Acc: 88.312% | Wgt Acc: 91.804%
	I - Batch: 150 | Loss: 0.272 | Acc: 88.875% | Wgt Acc: 92.279%
	I - Batch: 200 | Loss: 0.276 | Acc: 88.469% | Wgt Acc: 91.876%
I - num batch: 222
I - Train -- Loss: 0.280 | Acc: 88.413% | Wgt Acc: 91.822% | LR: 6.250000e-04 | Dur: 134.46s
I - Confusion Matrix: [row->prediction - col->label]
[[650.   1.   4.  11.  87.]
 [  4. 560.   8.   1.  49.]
 [  2.   4. 707.   3.  84.]
 [  8.   1.   3. 510.  71.]
 [ 33.  12.  12.  13. 709.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.139 | Acc: 61.933% | Wgt Acc: 60.976% | Dur: 19.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   7.   6.  21.  29.]
 [  0.  38.   5.   0.   7.]
 [  2.  14.  40.   2.  10.]
 [ 16.   4.  10.  55.  16.]
 [  7.  15.  14.   8. 118.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.261 | Acc: 90.625% | Wgt Acc: 93.530%
	I - Batch: 100 | Loss: 0.262 | Acc: 90.062% | Wgt Acc: 93.059%
	I - Batch: 150 | Loss: 0.263 | Acc: 89.792% | Wgt Acc: 92.772%
	I - Batch: 200 | Loss: 0.279 | Acc: 88.969% | Wgt Acc: 92.070%
I - num batch: 222
I - Train -- Loss: 0.281 | Acc: 88.892% | Wgt Acc: 91.897% | LR: 6.250000e-04 | Dur: 135.47s
I - Confusion Matrix: [row->prediction - col->label]
[[651.   0.   2.  15.  76.]
 [  1. 557.   9.   3.  43.]
 [  7.   3. 706.   2.  80.]
 [  7.   2.   1. 505.  67.]
 [ 31.  16.  16.  13. 734.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.112 | Acc: 63.708% | Wgt Acc: 61.665% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   4.  14.  14.]
 [  1.  38.   7.   5.   6.]
 [  3.  15.  41.   2.  20.]
 [ 17.   4.   8.  56.  11.]
 [  8.  18.  15.   9. 129.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.261 | Acc: 89.375% | Wgt Acc: 92.566%
	I - Batch: 100 | Loss: 0.257 | Acc: 89.438% | Wgt Acc: 92.460%
	I - Batch: 150 | Loss: 0.267 | Acc: 88.958% | Wgt Acc: 92.081%
	I - Batch: 200 | Loss: 0.283 | Acc: 88.062% | Wgt Acc: 91.335%
I - num batch: 222
I - Train -- Loss: 0.283 | Acc: 88.159% | Wgt Acc: 91.319% | LR: 6.250000e-04 | Dur: 137.90s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   3.   4.   8.  90.]
 [  2. 550.   3.   3.  50.]
 [  3.   8. 696.   1.  81.]
 [ 12.   1.   2. 507.  60.]
 [ 25.  16.  29.  19. 719.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.164 | Acc: 62.130% | Wgt Acc: 60.579% | Dur: 15.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   4.   3.  14.   9.]
 [  1.  38.   5.   3.  10.]
 [  4.  18.  40.   3.  21.]
 [ 15.   4.   8.  57.  17.]
 [ 11.  14.  19.   9. 123.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.287 | Acc: 87.250% | Wgt Acc: 90.702%
	I - Batch: 100 | Loss: 0.277 | Acc: 88.000% | Wgt Acc: 91.526%
	I - Batch: 150 | Loss: 0.274 | Acc: 88.417% | Wgt Acc: 91.826%
	I - Batch: 200 | Loss: 0.271 | Acc: 88.844% | Wgt Acc: 92.034%
I - num batch: 222
I - Train -- Loss: 0.269 | Acc: 88.920% | Wgt Acc: 92.126% | LR: 6.250000e-04 | Dur: 138.12s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   2.   5.   9.  87.]
 [  1. 561.   2.   3.  47.]
 [  2.   3. 702.   4.  94.]
 [  8.   2.   1. 507.  48.]
 [ 26.  10.  24.  15. 724.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.105 | Acc: 65.089% | Wgt Acc: 61.885% | Dur: 14.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   5.   2.  11.  15.]
 [  0.  43.   7.   0.   5.]
 [  2.   8.  29.   2.   7.]
 [ 13.   4.  10.  55.  13.]
 [ 10.  18.  27.  18. 140.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.226 | Acc: 91.250% | Wgt Acc: 94.052%
	I - Batch: 100 | Loss: 0.244 | Acc: 90.812% | Wgt Acc: 93.563%
	I - Batch: 150 | Loss: 0.246 | Acc: 90.583% | Wgt Acc: 93.403%
	I - Batch: 200 | Loss: 0.264 | Acc: 89.781% | Wgt Acc: 92.908%
I - num batch: 222
I - Train -- Loss: 0.263 | Acc: 89.822% | Wgt Acc: 92.931% | LR: 6.250000e-04 | Dur: 135.43s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   0.   3.   7.  80.]
 [  2. 567.   4.   0.  44.]
 [  4.   4. 706.   1.  84.]
 [  7.   0.   1. 516.  53.]
 [ 26.   7.  20.  14. 739.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.105 | Acc: 64.892% | Wgt Acc: 62.752% | Dur: 14.56s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   4.  12.  13.]
 [  1.  42.   7.   2.   8.]
 [  1.  10.  32.   1.  16.]
 [ 17.   5.  14.  63.  10.]
 [ 10.  19.  18.   8. 133.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 0.226 | Acc: 91.625% | Wgt Acc: 94.151%
	I - Batch: 100 | Loss: 0.254 | Acc: 90.125% | Wgt Acc: 93.071%
	I - Batch: 150 | Loss: 0.263 | Acc: 89.917% | Wgt Acc: 92.815%
	I - Batch: 200 | Loss: 0.265 | Acc: 89.719% | Wgt Acc: 92.686%
I - num batch: 222
I - Train -- Loss: 0.268 | Acc: 89.569% | Wgt Acc: 92.620% | LR: 6.250000e-04 | Dur: 136.19s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   2.   2.  12.  80.]
 [  1. 557.   6.   4.  49.]
 [  1.   3. 706.   4.  84.]
 [  9.   2.   3. 509.  49.]
 [ 19.  14.  17.   9. 738.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.131 | Acc: 63.708% | Wgt Acc: 60.840% | Dur: 16.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   4.  19.  11.]
 [  0.  45.  17.   3.  16.]
 [  1.  12.  29.   3.  11.]
 [ 14.   2.   7.  52.   7.]
 [ 11.  16.  18.   9. 135.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 0.244 | Acc: 88.875% | Wgt Acc: 91.994%
	I - Batch: 100 | Loss: 0.244 | Acc: 89.875% | Wgt Acc: 93.132%
	I - Batch: 150 | Loss: 0.248 | Acc: 89.417% | Wgt Acc: 92.653%
	I - Batch: 200 | Loss: 0.258 | Acc: 89.312% | Wgt Acc: 92.593%
I - num batch: 222
I - Train -- Loss: 0.264 | Acc: 89.146% | Wgt Acc: 92.418% | LR: 6.250000e-04 | Dur: 135.91s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   3.   2.  12.  80.]
 [  3. 562.   6.   1.  40.]
 [  4.   2. 710.   1.  90.]
 [  6.   1.   1. 504.  68.]
 [ 20.  10.  15.  20. 722.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.141 | Acc: 60.158% | Wgt Acc: 55.762% | Dur: 15.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 46.   1.   3.  12.   3.]
 [  0.  35.   2.   1.   4.]
 [  4.  17.  38.   8.  26.]
 [ 14.   1.   2.  47.   8.]
 [ 24.  24.  30.  18. 139.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 0.264 | Acc: 88.250% | Wgt Acc: 91.857%
	I - Batch: 100 | Loss: 0.265 | Acc: 88.812% | Wgt Acc: 92.136%
	I - Batch: 150 | Loss: 0.267 | Acc: 89.000% | Wgt Acc: 92.247%
	I - Batch: 200 | Loss: 0.263 | Acc: 89.562% | Wgt Acc: 92.571%
I - num batch: 222
I - Train -- Loss: 0.269 | Acc: 89.118% | Wgt Acc: 92.250% | LR: 6.250000e-04 | Dur: 136.68s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   1.   5.   9.  77.]
 [  2. 555.   3.   2.  44.]
 [  1.   7. 702.   1.  78.]
 [  9.   2.   0. 512.  71.]
 [ 23.  13.  24.  14. 730.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.206 | Acc: 63.314% | Wgt Acc: 59.910% | Dur: 14.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.  11.   4.  22.  19.]
 [  0.  36.   7.   3.   7.]
 [  0.   8.  31.   1.   8.]
 [  9.   4.   9.  48.   9.]
 [ 10.  19.  24.  12. 137.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 0.231 | Acc: 91.000% | Wgt Acc: 93.551%
	I - Batch: 100 | Loss: 0.240 | Acc: 90.875% | Wgt Acc: 93.438%
	I - Batch: 150 | Loss: 0.242 | Acc: 90.833% | Wgt Acc: 93.538%
	I - Batch: 200 | Loss: 0.250 | Acc: 90.375% | Wgt Acc: 93.321%
I - num batch: 222
I - Train -- Loss: 0.253 | Acc: 90.358% | Wgt Acc: 93.284% | LR: 6.250000e-04 | Dur: 134.92s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   1.   3.   5.  79.]
 [  2. 558.   4.   3.  48.]
 [  0.   6. 712.   4.  70.]
 [  7.   0.   0. 517.  50.]
 [ 23.  13.  15.   9. 753.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.170 | Acc: 60.552% | Wgt Acc: 59.701% | Dur: 19.56s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   1.   3.  20.  18.]
 [  2.  49.  13.   2.  16.]
 [  3.  11.  36.   9.  24.]
 [  6.   2.   8.  43.   8.]
 [ 12.  15.  15.  12. 114.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 0.274 | Acc: 89.125% | Wgt Acc: 92.692%
	I - Batch: 100 | Loss: 0.283 | Acc: 88.438% | Wgt Acc: 91.826%
	I - Batch: 150 | Loss: 0.276 | Acc: 88.875% | Wgt Acc: 92.083%
	I - Batch: 200 | Loss: 0.275 | Acc: 88.938% | Wgt Acc: 92.094%
I - num batch: 222
I - Train -- Loss: 0.273 | Acc: 89.005% | Wgt Acc: 92.185% | LR: 6.250000e-04 | Dur: 134.96s
I - Confusion Matrix: [row->prediction - col->label]
[[656.   0.   4.   9.  82.]
 [  3. 558.   3.   0.  49.]
 [  2.   5. 702.   4.  86.]
 [  8.   1.   2. 514.  56.]
 [ 28.  14.  23.  11. 727.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.054 | Acc: 64.694% | Wgt Acc: 62.334% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   3.  15.  10.]
 [  2.  53.  19.   4.  23.]
 [  2.   6.  30.   2.  12.]
 [  7.   1.   5.  49.   2.]
 [ 14.  15.  18.  16. 133.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 0.240 | Acc: 90.625% | Wgt Acc: 92.623%
	I - Batch: 100 | Loss: 0.240 | Acc: 90.750% | Wgt Acc: 93.038%
	I - Batch: 150 | Loss: 0.249 | Acc: 90.333% | Wgt Acc: 92.902%
	I - Batch: 200 | Loss: 0.250 | Acc: 90.312% | Wgt Acc: 92.900%
I - num batch: 222
I - Train -- Loss: 0.250 | Acc: 90.386% | Wgt Acc: 92.908% | LR: 6.250000e-04 | Dur: 136.44s
I - Confusion Matrix: [row->prediction - col->label]
[[656.   1.   2.  10.  63.]
 [  4. 560.   2.   3.  43.]
 [  3.   5. 707.   3.  78.]
 [  6.   3.   5. 509.  42.]
 [ 28.   9.  18.  13. 774.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.090 | Acc: 65.483% | Wgt Acc: 64.560% | Dur: 14.62s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   6.  15.  10.]
 [  2.  49.   7.   2.  18.]
 [  0.  14.  40.   3.  16.]
 [ 12.   3.   9.  53.  12.]
 [  8.  10.  13.  13. 124.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 0.227 | Acc: 91.375% | Wgt Acc: 93.936%
	I - Batch: 100 | Loss: 0.236 | Acc: 90.438% | Wgt Acc: 93.498%
	I - Batch: 150 | Loss: 0.237 | Acc: 90.500% | Wgt Acc: 93.460%
	I - Batch: 200 | Loss: 0.248 | Acc: 89.875% | Wgt Acc: 92.920%
I - num batch: 222
I - Train -- Loss: 0.255 | Acc: 89.371% | Wgt Acc: 92.481% | LR: 6.250000e-04 | Dur: 136.31s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   2.   2.  10.  84.]
 [  1. 557.   1.   3.  41.]
 [  1.   6. 709.   1.  88.]
 [  5.   1.   0. 508.  54.]
 [ 27.  12.  22.  16. 733.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.129 | Acc: 64.892% | Wgt Acc: 61.916% | Dur: 16.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   4.  14.  14.]
 [  1.  41.   9.   2.  10.]
 [  2.  12.  31.   0.   8.]
 [ 11.   5.  11.  56.  10.]
 [ 11.  17.  20.  14. 138.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 0.277 | Acc: 88.000% | Wgt Acc: 91.259%
	I - Batch: 100 | Loss: 0.280 | Acc: 88.062% | Wgt Acc: 91.313%
	I - Batch: 150 | Loss: 0.276 | Acc: 88.792% | Wgt Acc: 91.920%
	I - Batch: 200 | Loss: 0.280 | Acc: 88.688% | Wgt Acc: 91.727%
I - num batch: 222
I - Train -- Loss: 0.282 | Acc: 88.807% | Wgt Acc: 91.763% | LR: 6.250000e-04 | Dur: 136.77s
I - Confusion Matrix: [row->prediction - col->label]
[[650.   2.   2.  11.  85.]
 [  2. 556.   4.   1.  50.]
 [  2.   3. 702.   2.  78.]
 [  8.   3.   4. 506.  51.]
 [ 35.  14.  22.  18. 736.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.112 | Acc: 63.905% | Wgt Acc: 61.467% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   6.   4.  15.  15.]
 [  0.  39.   5.   4.   8.]
 [  3.  11.  37.   3.  15.]
 [ 15.   2.   6.  54.  10.]
 [  8.  20.  23.  10. 132.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 0.232 | Acc: 90.500% | Wgt Acc: 93.672%
	I - Batch: 100 | Loss: 0.248 | Acc: 89.062% | Wgt Acc: 92.503%
	I - Batch: 150 | Loss: 0.254 | Acc: 89.083% | Wgt Acc: 92.357%
	I - Batch: 200 | Loss: 0.256 | Acc: 89.156% | Wgt Acc: 92.325%
I - num batch: 222
I - Train -- Loss: 0.267 | Acc: 88.723% | Wgt Acc: 91.926% | LR: 6.250000e-04 | Dur: 135.45s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   3.   6.   9.  80.]
 [  2. 559.   2.   3.  60.]
 [  4.   2. 706.   5.  74.]
 [ 10.   0.   3. 505.  64.]
 [ 26.  14.  17.  16. 722.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.106 | Acc: 62.919% | Wgt Acc: 62.073% | Dur: 15.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   2.  14.  11.]
 [  1.  45.  11.   2.  18.]
 [  3.  15.  41.   6.  21.]
 [ 11.   2.   7.  53.  11.]
 [ 12.  13.  14.  11. 119.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 0.252 | Acc: 90.125% | Wgt Acc: 93.155%
	I - Batch: 100 | Loss: 0.248 | Acc: 90.000% | Wgt Acc: 92.779%
	I - Batch: 150 | Loss: 0.246 | Acc: 90.625% | Wgt Acc: 93.317%
	I - Batch: 200 | Loss: 0.243 | Acc: 90.656% | Wgt Acc: 93.422%
I - num batch: 222
I - Train -- Loss: 0.244 | Acc: 90.696% | Wgt Acc: 93.476% | LR: 6.250000e-04 | Dur: 135.52s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   1.   0.   8.  67.]
 [  2. 562.   3.   1.  41.]
 [  3.   3. 711.   2.  73.]
 [  3.   1.   1. 516.  55.]
 [ 25.  11.  19.  11. 764.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.129 | Acc: 63.116% | Wgt Acc: 60.297% | Dur: 14.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   3.  28.  14.]
 [  5.  55.  21.   7.  22.]
 [  2.   6.  30.   2.   9.]
 [  2.   1.   1.  36.   3.]
 [ 12.  12.  20.  13. 132.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 0.227 | Acc: 90.875% | Wgt Acc: 93.351%
	I - Batch: 100 | Loss: 0.234 | Acc: 90.500% | Wgt Acc: 93.021%
	I - Batch: 150 | Loss: 0.231 | Acc: 90.833% | Wgt Acc: 93.611%
	I - Batch: 200 | Loss: 0.242 | Acc: 90.094% | Wgt Acc: 93.001%
I - num batch: 222
I - Train -- Loss: 0.247 | Acc: 89.963% | Wgt Acc: 92.853% | LR: 6.250000e-04 | Dur: 130.82s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   1.   0.  11.  85.]
 [  4. 567.   5.   2.  47.]
 [  3.   3. 707.   3.  59.]
 [ 10.   1.   3. 509.  58.]
 [ 23.   6.  19.  13. 751.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.169 | Acc: 64.694% | Wgt Acc: 61.718% | Dur: 16.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   4.  22.  17.]
 [  1.  41.   5.   0.   8.]
 [  1.  13.  40.   2.  14.]
 [  8.   2.   5.  45.   5.]
 [ 12.  18.  21.  17. 136.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 0.256 | Acc: 90.125% | Wgt Acc: 92.979%
	I - Batch: 100 | Loss: 0.237 | Acc: 90.562% | Wgt Acc: 93.355%
	I - Batch: 150 | Loss: 0.239 | Acc: 89.958% | Wgt Acc: 92.990%
	I - Batch: 200 | Loss: 0.256 | Acc: 89.219% | Wgt Acc: 92.270%
I - num batch: 222
I - Train -- Loss: 0.257 | Acc: 89.118% | Wgt Acc: 92.175% | LR: 6.250000e-04 | Dur: 136.22s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   0.   6.   8.  97.]
 [  3. 558.   4.   3.  38.]
 [  3.   7. 701.   3.  85.]
 [ 10.   3.   3. 510.  46.]
 [ 23.  10.  20.  14. 734.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.165 | Acc: 63.905% | Wgt Acc: 60.140% | Dur: 14.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   5.   4.  15.  14.]
 [  0.  38.   8.   1.   4.]
 [  2.  13.  38.   6.  16.]
 [  6.   3.   3.  43.   6.]
 [ 15.  19.  22.  21. 140.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 0.255 | Acc: 90.000% | Wgt Acc: 92.695%
	I - Batch: 100 | Loss: 0.255 | Acc: 89.750% | Wgt Acc: 92.726%
	I - Batch: 150 | Loss: 0.251 | Acc: 89.917% | Wgt Acc: 92.618%
	I - Batch: 200 | Loss: 0.266 | Acc: 89.188% | Wgt Acc: 91.922%
I - num batch: 222
I - Train -- Loss: 0.272 | Acc: 88.807% | Wgt Acc: 91.615% | LR: 6.250000e-04 | Dur: 135.20s
I - Confusion Matrix: [row->prediction - col->label]
[[638.   4.   4.  12.  88.]
 [  2. 553.   4.   1.  45.]
 [  5.   6. 700.   0.  66.]
 [ 15.   0.   0. 514.  56.]
 [ 37.  15.  26.  11. 745.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.159 | Acc: 61.933% | Wgt Acc: 62.094% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   6.   6.  17.  19.]
 [  1.  51.  14.   4.  32.]
 [  0.  10.  33.   5.  15.]
 [ 12.   4.   7.  50.   4.]
 [  5.   7.  15.  10. 110.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 126
I - Training: 
	I - Batch: 50 | Loss: 0.255 | Acc: 90.500% | Wgt Acc: 93.473%
	I - Batch: 100 | Loss: 0.243 | Acc: 90.750% | Wgt Acc: 93.771%
	I - Batch: 150 | Loss: 0.252 | Acc: 90.042% | Wgt Acc: 93.193%
	I - Batch: 200 | Loss: 0.249 | Acc: 90.188% | Wgt Acc: 93.305%
I - num batch: 222
I - Train -- Loss: 0.254 | Acc: 89.963% | Wgt Acc: 93.132% | LR: 6.250000e-04 | Dur: 133.67s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   2.   5.   6.  76.]
 [  2. 567.   1.   0.  40.]
 [  1.   3. 708.   0.  80.]
 [  9.   0.   0. 516.  67.]
 [ 22.   6.  20.  16. 737.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 62.919% | Wgt Acc: 58.949% | Dur: 14.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   5.  16.  11.]
 [  0.  33.   4.   1.   3.]
 [  5.  20.  38.   1.  20.]
 [  5.   1.   7.  46.   6.]
 [ 16.  22.  21.  22. 140.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 127
I - Training: 
	I - Batch: 50 | Loss: 0.279 | Acc: 88.875% | Wgt Acc: 91.927%
	I - Batch: 100 | Loss: 0.277 | Acc: 88.688% | Wgt Acc: 91.737%
	I - Batch: 150 | Loss: 0.264 | Acc: 89.125% | Wgt Acc: 92.224%
	I - Batch: 200 | Loss: 0.260 | Acc: 89.156% | Wgt Acc: 92.257%
I - num batch: 222
I - Train -- Loss: 0.260 | Acc: 88.977% | Wgt Acc: 92.183% | LR: 6.250000e-04 | Dur: 140.16s
I - Confusion Matrix: [row->prediction - col->label]
[[659.   1.   4.  14.  90.]
 [  4. 553.   2.   2.  54.]
 [  2.   6. 712.   2.  72.]
 [  8.   3.   1. 508.  60.]
 [ 24.  15.  15.  12. 724.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.184 | Acc: 63.116% | Wgt Acc: 60.819% | Dur: 15.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   5.   4.  18.  18.]
 [  2.  39.  10.   1.   7.]
 [  0.  12.  29.   3.  11.]
 [  9.   4.  10.  56.  14.]
 [ 11.  18.  22.   8. 130.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 128
I - Training: 
	I - Batch: 50 | Loss: 0.231 | Acc: 90.125% | Wgt Acc: 93.376%
	I - Batch: 100 | Loss: 0.240 | Acc: 89.375% | Wgt Acc: 92.481%
	I - Batch: 150 | Loss: 0.242 | Acc: 89.417% | Wgt Acc: 92.484%
	I - Batch: 200 | Loss: 0.243 | Acc: 89.469% | Wgt Acc: 92.653%
I - num batch: 222
I - Train -- Loss: 0.247 | Acc: 89.230% | Wgt Acc: 92.544% | LR: 6.250000e-04 | Dur: 134.69s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   3.   4.   6.  79.]
 [  2. 563.   3.   2.  48.]
 [  4.   2. 707.   0.  85.]
 [  8.   1.   1. 513.  66.]
 [ 23.   9.  19.  17. 722.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.175 | Acc: 64.103% | Wgt Acc: 58.750% | Dur: 14.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   1.   3.  11.   7.]
 [  0.  31.   7.   1.   5.]
 [  3.   8.  30.   0.   8.]
 [  9.   6.   9.  53.   7.]
 [ 18.  32.  26.  21. 153.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 129
I - Training: 
	I - Batch: 50 | Loss: 0.253 | Acc: 90.125% | Wgt Acc: 92.778%
	I - Batch: 100 | Loss: 0.250 | Acc: 89.938% | Wgt Acc: 92.777%
	I - Batch: 150 | Loss: 0.256 | Acc: 89.875% | Wgt Acc: 92.726%
	I - Batch: 200 | Loss: 0.263 | Acc: 89.188% | Wgt Acc: 92.251%
I - num batch: 222
I - Train -- Loss: 0.264 | Acc: 89.146% | Wgt Acc: 92.246% | LR: 6.250000e-04 | Dur: 133.00s
I - Confusion Matrix: [row->prediction - col->label]
[[657.   1.   2.   6.  82.]
 [  1. 559.   3.   4.  51.]
 [  0.   6. 704.   3.  75.]
 [  4.   3.   2. 510.  60.]
 [ 35.   9.  23.  15. 732.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.099 | Acc: 63.708% | Wgt Acc: 59.827% | Dur: 18.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   3.  17.  12.]
 [  0.  32.   4.   0.   4.]
 [  1.  16.  40.   2.  19.]
 [ 11.   2.   7.  49.   4.]
 [ 15.  25.  21.  18. 141.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 130
I - Training: 
	I - Batch: 50 | Loss: 0.239 | Acc: 89.625% | Wgt Acc: 92.815%
	I - Batch: 100 | Loss: 0.245 | Acc: 89.688% | Wgt Acc: 92.730%
	I - Batch: 150 | Loss: 0.245 | Acc: 89.667% | Wgt Acc: 92.829%
	I - Batch: 200 | Loss: 0.251 | Acc: 89.562% | Wgt Acc: 92.677%
I - num batch: 222
I - Train -- Loss: 0.252 | Acc: 89.710% | Wgt Acc: 92.744% | LR: 6.250000e-04 | Dur: 136.81s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   3.   2.  12.  81.]
 [  2. 563.   3.   1.  48.]
 [  1.   1. 710.   2.  72.]
 [  7.   1.   1. 505.  59.]
 [ 23.  10.  18.  18. 740.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.091 | Acc: 64.103% | Wgt Acc: 62.365% | Dur: 14.89s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   6.   3.  19.  12.]
 [  2.  43.  12.   1.  11.]
 [  2.  13.  38.   2.  24.]
 [ 10.   3.   5.  51.   6.]
 [  8.  13.  17.  13. 127.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 131
I - Training: 
	I - Batch: 50 | Loss: 0.217 | Acc: 90.875% | Wgt Acc: 94.171%
	I - Batch: 100 | Loss: 0.244 | Acc: 89.938% | Wgt Acc: 93.180%
	I - Batch: 150 | Loss: 0.247 | Acc: 90.125% | Wgt Acc: 93.197%
	I - Batch: 200 | Loss: 0.256 | Acc: 89.500% | Wgt Acc: 92.717%
I - num batch: 222
I - Train -- Loss: 0.258 | Acc: 89.484% | Wgt Acc: 92.706% | LR: 6.250000e-04 | Dur: 136.87s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   1.   2.   7.  84.]
 [  1. 559.   3.   5.  49.]
 [  1.   6. 710.   1.  81.]
 [  8.   2.   1. 514.  57.]
 [ 25.  10.  18.  11. 729.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.134 | Acc: 62.722% | Wgt Acc: 60.213% | Dur: 14.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   6.   3.  22.  12.]
 [  2.  42.   8.   4.  12.]
 [  2.  11.  41.   3.  22.]
 [  7.   0.   8.  42.   5.]
 [ 13.  19.  15.  15. 129.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 132
I - Training: 
	I - Batch: 50 | Loss: 0.227 | Acc: 91.625% | Wgt Acc: 94.627%
	I - Batch: 100 | Loss: 0.222 | Acc: 91.500% | Wgt Acc: 94.326%
	I - Batch: 150 | Loss: 0.246 | Acc: 90.875% | Wgt Acc: 93.559%
	I - Batch: 200 | Loss: 0.251 | Acc: 90.750% | Wgt Acc: 93.399%
I - num batch: 222
I - Train -- Loss: 0.259 | Acc: 89.992% | Wgt Acc: 92.890% | LR: 6.250000e-04 | Dur: 135.57s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   1.   0.   5.  73.]
 [  1. 554.   3.   1.  44.]
 [  0.   7. 712.   2.  72.]
 [  8.   4.   1. 517.  60.]
 [ 30.  12.  18.  13. 751.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.180 | Acc: 61.341% | Wgt Acc: 58.761% | Dur: 14.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   4.  15.  15.]
 [  0.  44.  10.   4.  13.]
 [  2.  13.  27.   3.  13.]
 [ 18.   5.  11.  52.  10.]
 [  9.  13.  23.  12. 129.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 133
I - Training: 
	I - Batch: 50 | Loss: 0.260 | Acc: 89.500% | Wgt Acc: 92.480%
	I - Batch: 100 | Loss: 0.256 | Acc: 88.938% | Wgt Acc: 92.096%
	I - Batch: 150 | Loss: 0.262 | Acc: 88.708% | Wgt Acc: 92.228%
	I - Batch: 200 | Loss: 0.255 | Acc: 89.344% | Wgt Acc: 92.624%
I - num batch: 222
I - Train -- Loss: 0.252 | Acc: 89.456% | Wgt Acc: 92.650% | LR: 6.250000e-04 | Dur: 137.18s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   0.   0.   6.  82.]
 [  3. 564.   5.   2.  52.]
 [  1.   3. 708.   4.  79.]
 [  9.   1.   2. 510.  57.]
 [ 23.  10.  19.  16. 730.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.118 | Acc: 64.892% | Wgt Acc: 62.157% | Dur: 15.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   4.  13.  10.]
 [  2.  44.   6.   2.  12.]
 [  1.  12.  41.   4.  14.]
 [ 17.   3.   9.  53.   8.]
 [ 13.  17.  15.  14. 136.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 134
I - Training: 
	I - Batch: 50 | Loss: 0.210 | Acc: 92.125% | Wgt Acc: 94.678%
	I - Batch: 100 | Loss: 0.209 | Acc: 91.812% | Wgt Acc: 94.605%
	I - Batch: 150 | Loss: 0.218 | Acc: 90.958% | Wgt Acc: 93.812%
	I - Batch: 200 | Loss: 0.221 | Acc: 90.844% | Wgt Acc: 93.767%
I - num batch: 208
I - Train -- Loss: 0.221 | Acc: 90.865% | Wgt Acc: 93.786% | LR: 6.250000e-04 | Dur: 126.72s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   3.   2.   9.  58.]
 [  4. 556.   2.   0.  42.]
 [  0.   8. 714.   2.  60.]
 [  6.   1.   1. 514.  58.]
 [ 10.  10.  15.  13. 563.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.154 | Acc: 62.130% | Wgt Acc: 61.028% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   6.  18.  17.]
 [  2.  54.  19.   4.  15.]
 [  1.  11.  32.   4.  15.]
 [ 19.   1.   8.  51.  13.]
 [  8.   9.  10.   9. 120.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 135
I - Training: 
	I - Batch: 50 | Loss: 0.214 | Acc: 90.875% | Wgt Acc: 94.206%
	I - Batch: 100 | Loss: 0.224 | Acc: 90.938% | Wgt Acc: 94.025%
	I - Batch: 150 | Loss: 0.225 | Acc: 91.125% | Wgt Acc: 94.056%
	I - Batch: 200 | Loss: 0.243 | Acc: 90.156% | Wgt Acc: 93.305%
I - num batch: 222
I - Train -- Loss: 0.246 | Acc: 90.048% | Wgt Acc: 93.234% | LR: 6.250000e-04 | Dur: 134.27s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   0.   4.   2.  82.]
 [  0. 566.   1.   3.  40.]
 [  2.   4. 713.   1.  76.]
 [  8.   0.   1. 518.  65.]
 [ 27.   8.  15.  14. 737.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.163 | Acc: 62.130% | Wgt Acc: 58.918% | Dur: 17.78s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   2.   3.  21.  10.]
 [  1.  37.   6.   1.  11.]
 [  3.  17.  40.   7.  20.]
 [ 12.   4.   8.  43.   6.]
 [ 10.  18.  18.  14. 133.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 136
I - Training: 
	I - Batch: 50 | Loss: 0.259 | Acc: 88.125% | Wgt Acc: 91.491%
	I - Batch: 100 | Loss: 0.243 | Acc: 89.812% | Wgt Acc: 92.790%
	I - Batch: 150 | Loss: 0.246 | Acc: 89.500% | Wgt Acc: 92.462%
	I - Batch: 200 | Loss: 0.247 | Acc: 89.781% | Wgt Acc: 92.622%
I - num batch: 222
I - Train -- Loss: 0.245 | Acc: 89.851% | Wgt Acc: 92.658% | LR: 6.250000e-04 | Dur: 136.80s
I - Confusion Matrix: [row->prediction - col->label]
[[652.   1.   3.   9.  78.]
 [  1. 561.   3.   1.  41.]
 [  4.   0. 711.   2.  74.]
 [ 10.   3.   2. 509.  53.]
 [ 30.  13.  15.  17. 754.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.163 | Acc: 62.919% | Wgt Acc: 59.095% | Dur: 15.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   3.  18.  13.]
 [  0.  33.   3.   1.   6.]
 [  1.  11.  31.   2.   9.]
 [ 15.   9.  11.  53.  12.]
 [ 10.  22.  27.  12. 140.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 137
I - Training: 
	I - Batch: 50 | Loss: 0.230 | Acc: 90.625% | Wgt Acc: 93.558%
	I - Batch: 100 | Loss: 0.235 | Acc: 90.625% | Wgt Acc: 93.563%
	I - Batch: 150 | Loss: 0.229 | Acc: 90.792% | Wgt Acc: 93.609%
	I - Batch: 200 | Loss: 0.236 | Acc: 90.250% | Wgt Acc: 93.257%
I - num batch: 222
I - Train -- Loss: 0.233 | Acc: 90.302% | Wgt Acc: 93.305% | LR: 6.250000e-04 | Dur: 145.95s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   3.   0.   8.  81.]
 [  4. 560.   0.   2.  33.]
 [  3.   2. 715.   0.  86.]
 [  4.   1.   1. 514.  52.]
 [ 20.  12.  18.  14. 748.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.109 | Acc: 63.708% | Wgt Acc: 60.297% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   3.  20.  12.]
 [  1.  48.  10.   3.  10.]
 [  4.   8.  33.   3.  13.]
 [ 11.   1.   6.  44.   7.]
 [ 12.  17.  23.  16. 138.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 138
I - Training: 
	I - Batch: 50 | Loss: 0.229 | Acc: 90.500% | Wgt Acc: 93.988%
	I - Batch: 100 | Loss: 0.224 | Acc: 91.188% | Wgt Acc: 94.269%
	I - Batch: 150 | Loss: 0.234 | Acc: 90.708% | Wgt Acc: 93.823%
	I - Batch: 200 | Loss: 0.241 | Acc: 90.312% | Wgt Acc: 93.397%
I - num batch: 222
I - Train -- Loss: 0.242 | Acc: 90.330% | Wgt Acc: 93.428% | LR: 6.250000e-04 | Dur: 139.50s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   3.   8.  79.]
 [  3. 569.   0.   1.  43.]
 [  0.   2. 708.   1.  88.]
 [  5.   1.   1. 515.  46.]
 [ 21.   5.  22.  13. 744.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.229 | Acc: 61.933% | Wgt Acc: 59.095% | Dur: 14.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   5.  21.  17.]
 [  0.  43.   5.   2.   5.]
 [  2.   8.  33.   1.  20.]
 [ 11.   3.   8.  47.   7.]
 [ 15.  20.  24.  15. 131.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 139
I - Training: 
	I - Batch: 50 | Loss: 0.207 | Acc: 92.625% | Wgt Acc: 95.417%
	I - Batch: 100 | Loss: 0.227 | Acc: 91.688% | Wgt Acc: 94.581%
	I - Batch: 150 | Loss: 0.242 | Acc: 90.208% | Wgt Acc: 93.548%
	I - Batch: 200 | Loss: 0.250 | Acc: 89.844% | Wgt Acc: 93.045%
I - num batch: 222
I - Train -- Loss: 0.251 | Acc: 89.907% | Wgt Acc: 93.013% | LR: 6.250000e-04 | Dur: 136.99s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   4.   1.   8.  80.]
 [  0. 562.   5.   1.  46.]
 [  1.   2. 712.   1.  78.]
 [  7.   1.   0. 513.  57.]
 [ 26.   9.  16.  15. 739.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.127 | Acc: 62.130% | Wgt Acc: 60.036% | Dur: 15.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   4.   3.  11.   8.]
 [  1.  51.  15.   2.  19.]
 [  4.  10.  28.   8.  16.]
 [ 19.   2.   9.  57.   9.]
 [ 13.  11.  20.   8. 128.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 140
I - Training: 
	I - Batch: 50 | Loss: 0.244 | Acc: 89.125% | Wgt Acc: 92.600%
	I - Batch: 100 | Loss: 0.242 | Acc: 89.062% | Wgt Acc: 92.166%
	I - Batch: 150 | Loss: 0.259 | Acc: 89.042% | Wgt Acc: 91.896%
	I - Batch: 200 | Loss: 0.256 | Acc: 89.219% | Wgt Acc: 92.296%
I - num batch: 222
I - Train -- Loss: 0.256 | Acc: 89.230% | Wgt Acc: 92.222% | LR: 6.250000e-04 | Dur: 135.61s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   4.   3.   9.  76.]
 [  3. 549.   3.   3.  51.]
 [  6.   7. 712.   3.  79.]
 [ 10.   1.   2. 511.  56.]
 [ 23.  17.  14.  12. 738.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.150 | Acc: 63.511% | Wgt Acc: 58.677% | Dur: 15.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   3.   4.  10.   9.]
 [  0.  34.   3.   1.   4.]
 [  2.   9.  36.   2.   9.]
 [ 15.   5.   4.  50.  10.]
 [ 17.  27.  28.  23. 148.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 141
I - Training: 
	I - Batch: 50 | Loss: 0.279 | Acc: 87.750% | Wgt Acc: 90.879%
	I - Batch: 100 | Loss: 0.276 | Acc: 88.562% | Wgt Acc: 91.789%
	I - Batch: 150 | Loss: 0.256 | Acc: 89.583% | Wgt Acc: 92.631%
	I - Batch: 200 | Loss: 0.262 | Acc: 89.531% | Wgt Acc: 92.538%
I - num batch: 222
I - Train -- Loss: 0.259 | Acc: 89.540% | Wgt Acc: 92.672% | LR: 6.250000e-04 | Dur: 133.65s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   2.   2.  10.  73.]
 [  1. 558.   8.   3.  41.]
 [  3.   4. 708.   1.  79.]
 [  7.   1.   0. 512.  73.]
 [ 22.  13.  16.  12. 734.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.135 | Acc: 63.708% | Wgt Acc: 59.607% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   2.   5.   8.   7.]
 [  0.  35.   4.   2.   6.]
 [  1.  13.  30.   0.  11.]
 [ 18.  11.  13.  63.  11.]
 [ 19.  17.  23.  13. 145.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 142
I - Training: 
	I - Batch: 50 | Loss: 0.234 | Acc: 91.500% | Wgt Acc: 94.137%
	I - Batch: 100 | Loss: 0.238 | Acc: 91.250% | Wgt Acc: 94.125%
	I - Batch: 150 | Loss: 0.232 | Acc: 91.750% | Wgt Acc: 94.553%
	I - Batch: 200 | Loss: 0.242 | Acc: 90.938% | Wgt Acc: 93.922%
I - num batch: 222
I - Train -- Loss: 0.244 | Acc: 90.753% | Wgt Acc: 93.797% | LR: 6.250000e-04 | Dur: 136.43s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   0.   4.  63.]
 [  1. 562.   3.   2.  48.]
 [  1.   3. 712.   2.  81.]
 [  3.   1.   2. 521.  57.]
 [ 19.  11.  17.   9. 751.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.124 | Acc: 61.933% | Wgt Acc: 59.973% | Dur: 18.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   5.   4.  14.  11.]
 [  0.  46.  16.   2.  12.]
 [  1.   9.  30.   3.  18.]
 [ 15.   2.  10.  55.  13.]
 [ 15.  16.  15.  12. 126.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 143
I - Training: 
	I - Batch: 50 | Loss: 0.220 | Acc: 90.875% | Wgt Acc: 93.796%
	I - Batch: 100 | Loss: 0.216 | Acc: 91.438% | Wgt Acc: 94.368%
	I - Batch: 150 | Loss: 0.223 | Acc: 91.083% | Wgt Acc: 93.953%
	I - Batch: 200 | Loss: 0.234 | Acc: 90.906% | Wgt Acc: 93.669%
I - num batch: 222
I - Train -- Loss: 0.234 | Acc: 90.781% | Wgt Acc: 93.579% | LR: 6.250000e-04 | Dur: 133.52s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   1.   3.   6.  73.]
 [  6. 565.   1.   2.  40.]
 [  3.   3. 713.   2.  61.]
 [  4.   1.   0. 516.  62.]
 [ 22.   8.  17.  12. 764.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.146 | Acc: 62.525% | Wgt Acc: 57.674% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   2.   4.  13.   9.]
 [  0.  34.   6.   0.   6.]
 [  1.  14.  25.   2.  13.]
 [ 15.   6.  11.  53.   5.]
 [ 14.  22.  29.  18. 147.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 144
I - Training: 
	I - Batch: 50 | Loss: 0.224 | Acc: 92.125% | Wgt Acc: 94.578%
	I - Batch: 100 | Loss: 0.220 | Acc: 91.375% | Wgt Acc: 94.182%
	I - Batch: 150 | Loss: 0.221 | Acc: 91.250% | Wgt Acc: 94.061%
	I - Batch: 200 | Loss: 0.234 | Acc: 90.750% | Wgt Acc: 93.569%
I - num batch: 222
I - Train -- Loss: 0.239 | Acc: 90.104% | Wgt Acc: 93.194% | LR: 6.250000e-04 | Dur: 136.51s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   1.   3.   5.  82.]
 [  1. 561.   2.   2.  41.]
 [  1.   2. 713.   1.  85.]
 [  7.   2.   2. 516.  50.]
 [ 24.  12.  14.  14. 742.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.181 | Acc: 63.511% | Wgt Acc: 58.656% | Dur: 16.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   0.   3.  20.   7.]
 [  1.  41.   7.   2.   6.]
 [  0.  10.  38.   3.  16.]
 [  8.   2.   2.  39.   4.]
 [ 22.  25.  25.  22. 147.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 145
I - Training: 
	I - Batch: 50 | Loss: 0.215 | Acc: 91.250% | Wgt Acc: 93.298%
	I - Batch: 100 | Loss: 0.212 | Acc: 91.312% | Wgt Acc: 93.653%
	I - Batch: 150 | Loss: 0.225 | Acc: 91.250% | Wgt Acc: 93.660%
	I - Batch: 200 | Loss: 0.229 | Acc: 91.031% | Wgt Acc: 93.464%
I - num batch: 222
I - Train -- Loss: 0.232 | Acc: 91.063% | Wgt Acc: 93.539% | LR: 6.250000e-04 | Dur: 136.71s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   2.   2.  11.  65.]
 [  1. 564.   1.   2.  37.]
 [  4.   2. 711.   1.  71.]
 [  7.   0.   2. 511.  44.]
 [ 24.  10.  18.  13. 783.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.198 | Acc: 61.538% | Wgt Acc: 56.504% | Dur: 14.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   6.   6.  20.  16.]
 [  0.  36.   3.   1.   5.]
 [  1.   7.  23.   6.   7.]
 [ 12.   3.  11.  47.   6.]
 [ 15.  26.  32.  12. 146.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 146
I - Training: 
	I - Batch: 50 | Loss: 0.203 | Acc: 92.875% | Wgt Acc: 94.982%
	I - Batch: 100 | Loss: 0.220 | Acc: 91.562% | Wgt Acc: 94.378%
	I - Batch: 150 | Loss: 0.232 | Acc: 90.583% | Wgt Acc: 93.504%
	I - Batch: 200 | Loss: 0.239 | Acc: 90.562% | Wgt Acc: 93.405%
I - num batch: 222
I - Train -- Loss: 0.243 | Acc: 90.471% | Wgt Acc: 93.335% | LR: 6.250000e-04 | Dur: 143.71s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   1.   3.   6.  69.]
 [  1. 560.   3.   2.  35.]
 [  2.   3. 714.   5.  76.]
 [ 12.   4.   2. 514.  63.]
 [ 18.  10.  12.  11. 757.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.064 | Acc: 64.497% | Wgt Acc: 60.516% | Dur: 21.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   5.   5.  17.   9.]
 [  1.  47.   9.   1.  10.]
 [  1.  10.  29.   1.   7.]
 [ 11.   5.  10.  48.  10.]
 [ 16.  11.  22.  19. 144.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 147
I - Training: 
	I - Batch: 50 | Loss: 0.236 | Acc: 90.000% | Wgt Acc: 93.052%
	I - Batch: 100 | Loss: 0.244 | Acc: 90.250% | Wgt Acc: 93.277%
	I - Batch: 150 | Loss: 0.249 | Acc: 90.375% | Wgt Acc: 93.333%
	I - Batch: 200 | Loss: 0.249 | Acc: 90.188% | Wgt Acc: 93.188%
I - num batch: 222
I - Train -- Loss: 0.251 | Acc: 90.048% | Wgt Acc: 93.043% | LR: 6.250000e-04 | Dur: 135.72s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   1.   3.   9.  91.]
 [  2. 560.   3.   1.  43.]
 [  3.   1. 707.   0.  71.]
 [  4.   0.   1. 519.  48.]
 [ 27.  16.  20.   9. 747.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.107 | Acc: 64.497% | Wgt Acc: 62.188% | Dur: 14.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   4.  10.   8.]
 [  0.  42.   6.   1.   7.]
 [  3.  14.  37.   7.  19.]
 [ 19.   4.   9.  59.  13.]
 [ 10.  16.  19.   9. 133.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 148
I - Training: 
	I - Batch: 50 | Loss: 0.230 | Acc: 89.125% | Wgt Acc: 92.700%
	I - Batch: 100 | Loss: 0.235 | Acc: 89.125% | Wgt Acc: 92.563%
	I - Batch: 150 | Loss: 0.234 | Acc: 89.833% | Wgt Acc: 93.016%
	I - Batch: 200 | Loss: 0.228 | Acc: 90.219% | Wgt Acc: 93.247%
I - num batch: 222
I - Train -- Loss: 0.232 | Acc: 90.076% | Wgt Acc: 93.066% | LR: 6.250000e-04 | Dur: 133.89s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   1.   1.   5.  71.]
 [  3. 563.   2.   3.  38.]
 [  1.   3. 709.   2.  84.]
 [  7.   1.   1. 514.  60.]
 [ 24.  10.  21.  14. 747.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.256 | Acc: 61.144% | Wgt Acc: 56.400% | Dur: 14.47s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   6.   7.  25.  16.]
 [  0.  29.   3.   1.   2.]
 [  2.   9.  28.   0.   6.]
 [ 12.  10.  14.  49.  13.]
 [ 13.  24.  23.  11. 143.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 149
I - Training: 
	I - Batch: 50 | Loss: 0.254 | Acc: 90.000% | Wgt Acc: 92.899%
	I - Batch: 100 | Loss: 0.242 | Acc: 90.312% | Wgt Acc: 93.177%
	I - Batch: 150 | Loss: 0.243 | Acc: 90.583% | Wgt Acc: 93.269%
	I - Batch: 200 | Loss: 0.243 | Acc: 90.469% | Wgt Acc: 93.288%
I - num batch: 222
I - Train -- Loss: 0.246 | Acc: 90.217% | Wgt Acc: 93.109% | LR: 6.250000e-04 | Dur: 134.95s
I - Confusion Matrix: [row->prediction - col->label]
[[659.   0.   1.   7.  78.]
 [  4. 557.   2.   1.  47.]
 [  3.   5. 710.   1.  75.]
 [  5.   1.   0. 520.  46.]
 [ 26.  15.  21.   9. 754.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.165 | Acc: 62.327% | Wgt Acc: 60.036% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   9.   7.  20.  24.]
 [  1.  34.   2.   1.   8.]
 [  3.  15.  38.   2.  12.]
 [ 15.   5.  12.  54.   8.]
 [  7.  15.  16.   9. 128.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 150
I - Training: 
	I - Batch: 50 | Loss: 0.241 | Acc: 90.500% | Wgt Acc: 93.145%
	I - Batch: 100 | Loss: 0.227 | Acc: 91.062% | Wgt Acc: 93.773%
	I - Batch: 150 | Loss: 0.221 | Acc: 91.417% | Wgt Acc: 94.002%
	I - Batch: 200 | Loss: 0.223 | Acc: 91.281% | Wgt Acc: 93.900%
I - num batch: 222
I - Train -- Loss: 0.224 | Acc: 91.119% | Wgt Acc: 93.767% | LR: 6.250000e-04 | Dur: 138.83s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   1.   3.   5.  75.]
 [  0. 564.   3.   0.  36.]
 [  5.   3. 713.   2.  65.]
 [  4.   1.   2. 516.  49.]
 [ 24.   9.  13.  15. 775.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.342 | Acc: 59.763% | Wgt Acc: 56.619% | Dur: 16.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   9.   8.  29.  30.]
 [  0.  31.   2.   1.   3.]
 [  0.   8.  27.   0.   7.]
 [ 10.   9.  12.  48.  11.]
 [ 10.  21.  26.   8. 129.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 151
I - Training: 
	I - Batch: 50 | Loss: 0.227 | Acc: 90.875% | Wgt Acc: 93.439%
	I - Batch: 100 | Loss: 0.222 | Acc: 91.250% | Wgt Acc: 94.018%
	I - Batch: 150 | Loss: 0.227 | Acc: 91.083% | Wgt Acc: 93.826%
	I - Batch: 200 | Loss: 0.233 | Acc: 90.750% | Wgt Acc: 93.555%
I - num batch: 222
I - Train -- Loss: 0.233 | Acc: 90.809% | Wgt Acc: 93.601% | LR: 6.250000e-04 | Dur: 136.07s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   2.   4.   7.  74.]
 [  3. 565.   2.   2.  39.]
 [  1.   1. 709.   0.  72.]
 [  5.   2.   2. 519.  50.]
 [ 25.   8.  17.  10. 765.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.125 | Acc: 66.667% | Wgt Acc: 63.995% | Dur: 14.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   5.   6.  13.  15.]
 [  1.  46.  10.   2.   9.]
 [  1.  11.  29.   3.  10.]
 [  8.   3.   7.  56.   7.]
 [ 10.  13.  23.  12. 139.]]

I - Local maximum validation set accuracy:  66.67

I - Validation set results: 
[14-1-2-2.06][50-3-1-1.92][124-2-4-2.07][127-0-0-10.53][443-2-2-2.52][567-0-0-5.99][573-1-1-4.20][615-0-0-5.13][695-1-0-2.13][722-3-3-9.29]
[826-0-0-6.15][878-0-0-7.21][1103-0-4-1.88][1212-3-4-1.24][1368-0-0-7.03][2181-2-3-2.26][2476-2-2-2.41][2721-2-2-2.03][2818-1-1-2.14][2886-2-1-1.71]
[3231-2-2-7.31][3333-2-2-0.52][3482-2-4-2.80][3536-3-3-1.55][3625-1-1-3.37][3909-0-0-6.91][4035-0-0-6.62][4140-0-0-2.70][4214-1-4-2.04][4346-1-4-3.58]
[4581-2-2-1.92][4708-3-4-2.51][4838-3-3-2.66][4845-1-2-1.27][4868-0-0-6.85][4939-0-2-1.70][4984-2-3-3.57][5078-1-2-2.80][5396-0-0-9.78][5479-1-1-3.97]
[5717-0-0-4.49][5843-1-1-4.04][5949-3-3-4.26][5987-2-4-4.34][6014-3-3-3.11][6033-3-3-1.84][6313-0-0-4.06][6421-3-3-6.06][6500-1-4-0.83][6583-3-3-5.27]
[6683-3-0-1.06][6825-2-1-1.20][6998-3-3-2.55][7049-3-3-1.60][7517-1-1-3.94][7521-1-1-1.47][7528-1-1-1.96][7949-1-2-2.38][8135-1-0-3.37][8185-3-0-5.54]
[8269-3-2-1.53][8273-3-3-5.56][8543-3-0-3.65][8666-1-1-8.16][8672-0-0-7.95][8903-1-2-3.42][9001-2-2-4.18][9036-2-2-4.43][9281-3-4-0.54][9300-2-2-5.74]
[9571-0-0-2.62][9617-1-4-3.62][9644-2-1-3.28][9705-2-0-1.47][9801-0-4-0.73][9803-3-3-4.30][9865-3-3-5.53][9896-2-4-1.63][10314-1-4-1.12][10337-3-3-3.94]
[10403-0-4-2.61][10653-2-4-2.89][10704-2-1-2.26][10719-1-1-3.14][10727-1-1-1.99][10836-0-0-10.81][10969-2-4-0.92][11042-0-4-2.37][11088-1-1-10.10][11322-0-0-7.06]
[11398-2-4-2.52][11499-0-0-6.00][11502-3-3-1.40][11512-3-3-2.29][11608-1-1-3.36][11610-0-3-3.60][11692-0-0-3.75][11905-0-0-6.59][11993-1-1-4.56][12002-2-0-6.93]
[12052-0-0-4.99][12201-0-3-3.79][12235-2-2-1.98][12320-1-4-3.17][12377-2-4-2.09][12398-2-4-1.34][12503-1-2-1.27][12617-0-0-1.79][12685-3-4-0.97][12738-2-3-1.22]
[12742-2-2-4.51][12823-0-0-5.43][13110-1-1-5.35][13240-3-3-4.42][13253-1-0-1.15][13273-0-0-8.88][13634-1-1-2.92][13763-2-2-1.56][13905-3-3-2.44][14060-2-1-3.81]
[14065-3-3-2.42][14147-3-3-2.93][14595-2-2-3.18][14687-2-2-4.23][14788-2-2-6.50][14869-1-1-1.91][14872-3-3-0.79][14877-1-1-4.64][14927-0-0-3.26][15066-0-0-9.26]
[15175-1-1-4.27][15178-2-4-2.22][15375-3-0-1.04][15389-3-0-3.35][15568-2-4-1.60][15675-3-3-4.30][15869-1-0-1.40][16207-3-1-2.02][16236-0-0-5.43][16302-3-4-3.29]
[16331-2-2-10.60][16381-0-3-2.28][16488-1-1-3.40][16495-0-0-6.15][16650-0-0-7.70][16719-1-1-1.99][16801-0-0-2.98][16828-0-0-4.93][17137-3-3-2.99][17245-1-4-2.94]
[17278-3-4-1.24][17282-0-0-2.03][17311-2-2-2.40][17336-2-1-3.20][17608-3-3-6.73][17627-0-0-2.30][17877-3-4-2.88][17924-1-3-1.64][17984-3-3-3.98][18211-0-1-2.12]
[18276-3-0-4.97][18287-1-2-0.27][18394-0-0-6.21][18428-0-0-2.00][18442-0-0-3.34][18478-3-3-4.02][18607-0-0-3.11][18616-0-0-3.04][18663-0-0-4.61][18718-0-0-5.37]
[18766-2-2-4.00][18824-2-4-3.84][18890-3-3-2.83][18930-3-4-1.18][18938-3-3-1.84][19817-1-1-6.16][19839-0-4-1.54][19930-3-0-3.62][19944-0-0-2.44][20036-2-2-5.63]
[20101-3-3-2.24][20474-1-1-6.07][20547-3-0-2.10][20929-2-2-4.94][21245-1-1-2.19][21257-3-2-1.31][21293-1-1-4.80][21316-1-1-5.21][21384-1-4-4.16][21448-1-1-4.03]
[21483-0-0-6.71][21487-2-2-4.96][21714-0-0-2.09][21943-3-2-2.02][21947-0-0-5.80][21948-0-0-10.83][21965-2-1-4.87][21998-1-1-4.88][22025-0-4-3.50][22228-3-3-4.77]
[22446-1-1-7.58][22494-3-3-3.74][22757-0-0-6.96][22811-3-3-4.56][22976-3-4-3.51][22985-3-3-3.73][23014-0-3-3.40][23112-1-1-6.28][23144-3-3-5.13][23168-2-0-2.36]
[23219-0-0-5.43][23363-3-3-5.95][23470-0-0-1.27][23486-2-4-2.11][23497-0-3-7.45][23516-0-0-6.67][23690-1-3-2.90][23921-2-4-1.93][23936-1-2-1.90][24040-3-0-2.82]
[24111-1-4-2.10][24182-0-0-8.23][24238-3-3-4.14][24290-2-0-4.09][24345-0-0-3.08][24364-1-1-2.72][24427-3-0-6.04][24477-2-2-3.91][24495-2-4-1.58][24893-2-1-2.34]
[25012-1-4-1.73][25121-2-4-2.98][25165-3-3-1.53][25183-0-0-2.89][25297-3-3-5.52][25398-0-0-7.80][25574-2-4-2.07][25644-1-2-4.70][25718-1-4-2.39][25774-2-4-3.09]
[26032-3-3-3.00][26051-3-3-4.86][26120-0-4-3.45][26321-1-1-6.44][26732-1-1-3.86][26784-3-3-6.34][26827-3-0-2.44][26833-0-3-7.27][26838-2-4-1.15][26860-1-4-1.66]
[26948-0-0-3.60][27049-3-0-6.48][27098-1-0-1.61][27526-0-0-5.57][27639-3-3-3.18][27698-3-3-2.59][27772-0-0-7.10][27890-1-1-6.25][28040-0-0-3.11][28503-2-2-2.24]
[28577-1-2-3.91][28959-0-0-7.28][29198-3-3-1.87][29777-0-0-6.56][29877-2-3-3.09][30035-1-1-7.12][30098-0-0-3.11][30326-1-1-6.19][30572-2-2-3.77][30716-0-4-4.08]
[30806-2-4-1.80][30906-1-1-4.45][31007-0-0-3.71][31181-3-3-3.25][31238-0-3-3.28][31347-0-0-6.23][31422-2-2-5.34][31429-3-3-3.78][31431-0-0-3.87][31432-1-1-6.39]
[31477-0-0-4.26][31524-1-2-1.01][31597-1-1-2.86][31619-1-4-2.20][31701-0-0-4.01][31755-0-0-6.22][31854-3-3-3.04][32074-1-1-3.40][32078-3-3-5.37][32111-1-1-2.08]
[32127-1-1-3.41][32140-3-3-4.16][32263-2-4-2.00][32365-0-0-6.61][32411-2-0-6.94][32429-3-3-4.46][32473-3-3-2.88][32574-3-0-5.79][32584-0-4-2.19][32622-0-4-2.01]
[32858-3-3-2.51][32969-3-3-6.16][33016-2-2-1.86][33031-1-3-4.40][33035-2-1-2.47][33133-2-2-2.54][33173-2-3-0.96][33175-3-4-4.08][33306-3-3-4.41][33309-2-3-2.10]
[33474-0-3-2.16][33478-2-3-2.40][33618-1-1-3.22][33712-0-0-4.39][33782-2-4-4.27][33914-3-3-2.57][34076-3-4-2.16][34112-2-2-4.59][34138-2-0-0.73][34239-1-1-1.91]
[34364-2-2-4.73][34617-1-1-2.25][34751-3-3-4.20][34783-2-4-3.87][35015-3-4-2.71][35018-1-1-3.13][35288-2-1-1.30][0-4-4-3.75][1-4-4-4.21][2-4-0-3.48]
[3-4-4-3.25][4-4-4-0.83][5-4-1-1.96][6-4-4-3.92][7-4-4-3.27][8-4-4-1.85][9-4-4-2.49][10-4-4-3.95][11-4-4-4.57][12-4-2-2.65]
[14-4-3-2.74][15-4-0-2.54][16-4-4-3.12][17-4-4-2.05][18-4-4-5.00][19-4-3-3.11][20-4-0-1.23][21-4-4-1.97][22-4-4-3.03][23-4-4-2.05]
[24-4-4-5.18][25-4-4-4.23][26-4-4-1.27][27-4-4-4.23][28-4-4-4.33][29-4-2-1.26][30-4-4-1.41][31-4-4-3.68][32-4-4-3.19][33-4-4-1.17]
[34-4-4-2.95][35-4-4-2.39][37-4-4-3.32][39-4-0-5.21][40-4-4-2.11][41-4-2-1.52][42-4-4-1.69][43-4-4-2.78][45-4-4-2.64][46-4-4-4.08]
[47-4-4-3.93][48-4-4-2.56][51-4-4-2.89][52-4-1-1.02][53-4-4-2.05][54-4-4-3.11][55-4-4-4.12][56-4-1-3.05][57-4-0-5.30][58-4-2-3.22]
[59-4-0-4.99][60-4-4-3.70][61-4-4-4.66][62-4-4-2.99][63-4-4-3.02][64-4-4-2.15][65-4-4-4.90][66-4-4-4.01][67-4-2-1.30][68-4-1-2.12]
[69-4-3-2.30][70-4-4-2.77][72-4-4-2.27][73-4-1-6.10][74-4-2-3.04][75-4-0-2.94][77-4-4-6.21][78-4-2-1.79][79-4-4-3.26][80-4-4-3.12]
[81-4-1-2.66][82-4-3-1.27][83-4-4-2.62][84-4-4-4.27][85-4-4-1.77][86-4-4-1.41][87-4-4-5.35][88-4-4-3.37][89-4-4-3.65][90-4-4-1.69]
[91-4-4-1.43][92-4-0-0.55][93-4-4-2.14][94-4-4-2.32][95-4-4-3.01][96-4-4-3.34][97-4-4-3.51][98-4-1-1.87][99-4-4-2.50][100-4-4-2.00]
[101-4-4-6.72][102-4-4-1.75][103-4-4-1.49][104-4-4-2.60][105-4-4-2.03][106-4-4-4.70][107-4-4-2.80][108-4-4-2.30][109-4-4-2.35][110-4-1-1.72]
[111-4-0-5.97][112-4-4-1.48][113-4-4-1.83][114-4-4-1.35][115-4-4-3.09][116-4-4-1.49][117-4-1-2.23][119-4-4-3.01][121-4-4-1.69][122-4-4-2.36]
[124-4-4-2.61][125-4-4-3.38][126-4-4-5.83][127-4-4-3.59][128-4-4-1.55][129-4-4-1.76][130-4-4-2.48][131-4-2-3.15][132-4-4-1.43][133-4-4-4.02]
[135-4-4-3.60][136-4-4-1.18][137-4-4-2.36][138-4-4-2.26][139-4-4-2.83][140-4-4-1.61][141-4-3-1.92][142-4-4-3.49][143-4-4-3.26][144-4-4-2.59]
[145-4-4-3.96][148-4-0-3.36][149-4-4-2.76][150-4-4-4.55][151-4-4-4.89][152-4-4-3.33][153-4-4-2.49][154-4-4-5.53][155-4-4-3.99][156-4-0-1.05]
[157-4-0-2.61][158-4-4-2.82][160-4-4-1.48][161-4-4-1.80][162-4-4-0.98][164-4-4-1.97][165-4-4-2.99][167-4-0-3.44][168-4-4-2.30][170-4-4-2.36]
[171-4-4-2.31][172-4-4-4.26][173-4-4-2.93][174-4-0-3.13][175-4-4-2.69][177-4-4-2.20][178-4-4-2.28][179-4-4-1.37][180-4-4-4.02][181-4-3-3.38]
[182-4-3-3.87][183-4-4-3.67][184-4-4-4.01][186-4-0-2.95][187-4-4-2.27][188-4-4-3.50][189-4-4-1.86][190-4-4-1.63][191-4-4-2.79][192-4-4-1.78]
[193-4-2-3.00][194-4-4-1.47][195-4-4-2.33][196-4-4-1.91][197-4-4-3.08][198-4-4-5.65][199-4-2-2.22]
---------------------------
I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 152
I - Training: 
	I - Batch: 50 | Loss: 0.210 | Acc: 91.750% | Wgt Acc: 94.731%
	I - Batch: 100 | Loss: 0.220 | Acc: 91.125% | Wgt Acc: 94.157%
	I - Batch: 150 | Loss: 0.233 | Acc: 90.833% | Wgt Acc: 93.899%
	I - Batch: 200 | Loss: 0.241 | Acc: 90.594% | Wgt Acc: 93.530%
I - num batch: 222
I - Train -- Loss: 0.243 | Acc: 90.584% | Wgt Acc: 93.442% | LR: 6.250000e-04 | Dur: 134.41s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   0.   4.   6.  69.]
 [  1. 557.   1.   3.  44.]
 [  2.   0. 713.   1.  72.]
 [  4.   5.   0. 520.  56.]
 [ 26.  16.  16.   8. 759.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.152 | Acc: 62.919% | Wgt Acc: 58.426% | Dur: 13.97s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   2.   4.  12.  10.]
 [  2.  30.   6.   1.   6.]
 [  2.  14.  31.   1.  10.]
 [ 14.   5.  12.  55.   9.]
 [ 12.  27.  22.  17. 145.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 153
I - Training: 
	I - Batch: 50 | Loss: 0.214 | Acc: 92.125% | Wgt Acc: 94.913%
	I - Batch: 100 | Loss: 0.221 | Acc: 91.500% | Wgt Acc: 94.213%
	I - Batch: 150 | Loss: 0.224 | Acc: 91.375% | Wgt Acc: 94.021%
	I - Batch: 200 | Loss: 0.235 | Acc: 90.750% | Wgt Acc: 93.379%
I - num batch: 222
I - Train -- Loss: 0.236 | Acc: 90.866% | Wgt Acc: 93.456% | LR: 6.250000e-04 | Dur: 139.22s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   2.   2.   8.  68.]
 [  2. 557.   3.   2.  33.]
 [  2.   4. 712.   1.  68.]
 [  3.   1.   2. 514.  56.]
 [ 25.  14.  15.  13. 775.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.161 | Acc: 63.511% | Wgt Acc: 60.203% | Dur: 18.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   4.  17.  15.]
 [  0.  38.   6.   1.   7.]
 [  4.  19.  40.   7.  19.]
 [  7.   1.   2.  43.   3.]
 [ 12.  16.  23.  18. 136.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 154
I - Training: 
	I - Batch: 50 | Loss: 0.215 | Acc: 91.375% | Wgt Acc: 94.450%
	I - Batch: 100 | Loss: 0.220 | Acc: 91.375% | Wgt Acc: 94.345%
	I - Batch: 150 | Loss: 0.226 | Acc: 91.208% | Wgt Acc: 94.158%
	I - Batch: 200 | Loss: 0.227 | Acc: 91.281% | Wgt Acc: 94.159%
I - num batch: 222
I - Train -- Loss: 0.225 | Acc: 91.514% | Wgt Acc: 94.333% | LR: 6.250000e-04 | Dur: 134.95s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   3.   1.   4.  65.]
 [  0. 567.   3.   1.  36.]
 [  3.   3. 717.   0.  77.]
 [  4.   2.   2. 520.  52.]
 [ 18.   3.  11.  13. 770.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.168 | Acc: 64.103% | Wgt Acc: 59.534% | Dur: 16.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   3.  15.   8.]
 [  1.  36.   8.   2.   7.]
 [  2.  14.  34.   3.  13.]
 [ 11.   7.   9.  49.   5.]
 [ 15.  19.  21.  17. 147.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 155
I - Training: 
	I - Batch: 50 | Loss: 0.244 | Acc: 89.875% | Wgt Acc: 92.334%
	I - Batch: 100 | Loss: 0.243 | Acc: 89.500% | Wgt Acc: 92.349%
	I - Batch: 150 | Loss: 0.237 | Acc: 89.750% | Wgt Acc: 92.560%
	I - Batch: 200 | Loss: 0.239 | Acc: 89.719% | Wgt Acc: 92.579%
I - num batch: 222
I - Train -- Loss: 0.242 | Acc: 89.710% | Wgt Acc: 92.592% | LR: 6.250000e-04 | Dur: 135.75s
I - Confusion Matrix: [row->prediction - col->label]
[[653.   2.   0.   9.  76.]
 [  6. 558.   2.   2.  47.]
 [  2.   3. 717.   3.  68.]
 [ 11.   1.   1. 506.  61.]
 [ 25.  14.  14.  18. 748.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.213 | Acc: 60.355% | Wgt Acc: 57.361% | Dur: 14.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   4.  15.   4.]
 [  2.  40.   6.   1.  17.]
 [  4.  19.  45.  10.  26.]
 [ 11.   2.   2.  38.   5.]
 [ 16.  15.  18.  22. 128.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 156
I - Training: 
	I - Batch: 50 | Loss: 0.227 | Acc: 90.750% | Wgt Acc: 94.221%
	I - Batch: 100 | Loss: 0.239 | Acc: 90.188% | Wgt Acc: 93.438%
	I - Batch: 150 | Loss: 0.239 | Acc: 89.958% | Wgt Acc: 93.318%
	I - Batch: 200 | Loss: 0.236 | Acc: 90.188% | Wgt Acc: 93.420%
I - num batch: 222
I - Train -- Loss: 0.236 | Acc: 90.358% | Wgt Acc: 93.479% | LR: 6.250000e-04 | Dur: 136.52s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   1.   4.   5.  86.]
 [  1. 561.   2.   0.  41.]
 [  3.   5. 713.   2.  67.]
 [  4.   0.   1. 519.  63.]
 [ 20.  11.  14.  12. 743.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.157 | Acc: 63.708% | Wgt Acc: 61.728% | Dur: 14.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   7.   6.  23.  24.]
 [  1.  43.   7.   1.   7.]
 [  2.   8.  33.   3.  12.]
 [  9.   3.  12.  50.   9.]
 [  7.  17.  17.   9. 128.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 157
I - Training: 
	I - Batch: 50 | Loss: 0.215 | Acc: 91.125% | Wgt Acc: 94.081%
	I - Batch: 100 | Loss: 0.227 | Acc: 91.062% | Wgt Acc: 93.931%
	I - Batch: 150 | Loss: 0.229 | Acc: 90.792% | Wgt Acc: 93.688%
	I - Batch: 200 | Loss: 0.242 | Acc: 90.344% | Wgt Acc: 93.154%
I - num batch: 222
I - Train -- Loss: 0.243 | Acc: 90.189% | Wgt Acc: 93.002% | LR: 6.250000e-04 | Dur: 137.54s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   0.   2.   5.  74.]
 [  2. 558.   4.   4.  35.]
 [  2.   7. 708.   0.  73.]
 [  9.   1.   3. 513.  61.]
 [ 21.  12.  17.  16. 757.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.158 | Acc: 62.919% | Wgt Acc: 60.370% | Dur: 14.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   0.   3.  12.   4.]
 [  5.  56.  25.  12.  27.]
 [  1.  11.  34.   5.  13.]
 [  9.   0.   3.  44.   5.]
 [ 19.  11.  10.  13. 131.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 158
I - Training: 
	I - Batch: 50 | Loss: 0.262 | Acc: 88.750% | Wgt Acc: 91.561%
	I - Batch: 100 | Loss: 0.254 | Acc: 89.375% | Wgt Acc: 92.515%
	I - Batch: 150 | Loss: 0.244 | Acc: 89.833% | Wgt Acc: 92.986%
	I - Batch: 200 | Loss: 0.250 | Acc: 89.562% | Wgt Acc: 92.648%
I - num batch: 222
I - Train -- Loss: 0.248 | Acc: 89.766% | Wgt Acc: 92.797% | LR: 6.250000e-04 | Dur: 135.27s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   2.   2.  11.  77.]
 [  3. 563.   5.   3.  55.]
 [  3.   2. 707.   2.  69.]
 [  6.   4.   2. 507.  58.]
 [ 19.   7.  18.  15. 741.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.124 | Acc: 62.919% | Wgt Acc: 58.918% | Dur: 14.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   2.   3.   8.   6.]
 [  2.  39.   7.   5.   8.]
 [  3.  17.  40.   4.  20.]
 [ 18.   1.   5.  48.   5.]
 [ 14.  19.  20.  21. 141.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 159
I - Training: 
	I - Batch: 50 | Loss: 0.199 | Acc: 92.000% | Wgt Acc: 94.687%
	I - Batch: 100 | Loss: 0.214 | Acc: 91.938% | Wgt Acc: 94.568%
	I - Batch: 150 | Loss: 0.221 | Acc: 91.458% | Wgt Acc: 94.109%
	I - Batch: 200 | Loss: 0.227 | Acc: 90.688% | Wgt Acc: 93.551%
I - num batch: 222
I - Train -- Loss: 0.229 | Acc: 90.584% | Wgt Acc: 93.508% | LR: 6.250000e-04 | Dur: 136.66s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   0.   2.   5.  79.]
 [  1. 561.   0.   1.  54.]
 [  1.   2. 720.   1.  61.]
 [  5.   1.   0. 515.  51.]
 [ 28.  14.  12.  16. 755.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.251 | Acc: 62.525% | Wgt Acc: 59.273% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   7.   4.   9.  10.]
 [  0.  32.   3.   1.   6.]
 [  3.  15.  39.   5.  19.]
 [ 20.   5.  12.  59.   9.]
 [ 14.  19.  17.  12. 136.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 160
I - Training: 
	I - Batch: 50 | Loss: 0.209 | Acc: 90.625% | Wgt Acc: 93.526%
	I - Batch: 100 | Loss: 0.215 | Acc: 90.938% | Wgt Acc: 93.857%
	I - Batch: 150 | Loss: 0.216 | Acc: 90.875% | Wgt Acc: 93.811%
	I - Batch: 200 | Loss: 0.220 | Acc: 91.000% | Wgt Acc: 93.797%
I - num batch: 222
I - Train -- Loss: 0.221 | Acc: 90.950% | Wgt Acc: 93.731% | LR: 6.250000e-04 | Dur: 135.52s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   4.   2.   9.  67.]
 [  2. 564.   1.   2.  40.]
 [  2.   2. 719.   0.  74.]
 [  6.   0.   1. 514.  53.]
 [ 24.   8.  11.  13. 766.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.139 | Acc: 63.314% | Wgt Acc: 60.777% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 52.   3.   4.   7.  10.]
 [  1.  43.   7.   2.  10.]
 [  4.  15.  33.   4.  15.]
 [ 20.   2.   8.  60.  12.]
 [ 11.  15.  23.  13. 133.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 161
I - Training: 
	I - Batch: 50 | Loss: 0.198 | Acc: 92.500% | Wgt Acc: 94.829%
	I - Batch: 100 | Loss: 0.199 | Acc: 92.688% | Wgt Acc: 95.411%
	I - Batch: 150 | Loss: 0.194 | Acc: 92.708% | Wgt Acc: 95.429%
	I - Batch: 200 | Loss: 0.192 | Acc: 92.844% | Wgt Acc: 95.438%
I - num batch: 208
I - Train -- Loss: 0.192 | Acc: 92.909% | Wgt Acc: 95.450% | LR: 6.250000e-04 | Dur: 128.45s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   0.   0.   5.  57.]
 [  0. 570.   1.   1.  30.]
 [  2.   0. 720.   0.  54.]
 [  1.   0.   1. 525.  41.]
 [ 16.   8.  12.   7. 599.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 64.694% | Wgt Acc: 59.597% | Dur: 14.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   3.  22.   8.]
 [  2.  37.  12.   3.  10.]
 [  1.  14.  35.   3.   8.]
 [  8.   2.   2.  43.   3.]
 [ 15.  22.  23.  15. 151.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 162
I - Training: 
	I - Batch: 50 | Loss: 0.204 | Acc: 92.000% | Wgt Acc: 94.991%
	I - Batch: 100 | Loss: 0.220 | Acc: 91.688% | Wgt Acc: 94.397%
	I - Batch: 150 | Loss: 0.218 | Acc: 91.667% | Wgt Acc: 94.458%
	I - Batch: 200 | Loss: 0.219 | Acc: 91.375% | Wgt Acc: 94.217%
I - num batch: 222
I - Train -- Loss: 0.225 | Acc: 91.119% | Wgt Acc: 93.980% | LR: 6.250000e-04 | Dur: 135.46s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   2.   6.  74.]
 [  1. 566.   2.   3.  43.]
 [  0.   3. 718.   1.  59.]
 [  6.   1.   2. 512.  61.]
 [ 17.   8.  10.  16. 763.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.335 | Acc: 60.947% | Wgt Acc: 55.146% | Dur: 16.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   6.   7.  17.  13.]
 [  0.  26.   2.   0.   5.]
 [  2.   6.  15.   1.   3.]
 [ 14.   8.  14.  54.   8.]
 [  9.  32.  37.  14. 151.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 163
I - Training: 
	I - Batch: 50 | Loss: 0.249 | Acc: 89.625% | Wgt Acc: 92.222%
	I - Batch: 100 | Loss: 0.230 | Acc: 90.750% | Wgt Acc: 93.558%
	I - Batch: 150 | Loss: 0.221 | Acc: 90.958% | Wgt Acc: 93.747%
	I - Batch: 200 | Loss: 0.220 | Acc: 91.219% | Wgt Acc: 93.962%
I - num batch: 222
I - Train -- Loss: 0.221 | Acc: 91.119% | Wgt Acc: 93.916% | LR: 6.250000e-04 | Dur: 134.94s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   4.   4.  64.]
 [  0. 561.   4.   0.  49.]
 [  0.   2. 712.   3.  79.]
 [  4.   2.   2. 518.  41.]
 [ 19.  13.  12.  13. 767.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.102 | Acc: 65.089% | Wgt Acc: 62.073% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   2.  13.  11.]
 [  0.  42.   5.   0.   8.]
 [  2.   9.  34.   3.   9.]
 [ 17.   4.  12.  58.  13.]
 [ 12.  20.  22.  12. 139.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 164
I - Training: 
	I - Batch: 50 | Loss: 0.217 | Acc: 90.625% | Wgt Acc: 93.925%
	I - Batch: 100 | Loss: 0.219 | Acc: 91.125% | Wgt Acc: 94.102%
	I - Batch: 150 | Loss: 0.219 | Acc: 91.458% | Wgt Acc: 94.246%
	I - Batch: 200 | Loss: 0.229 | Acc: 90.719% | Wgt Acc: 93.537%
I - num batch: 222
I - Train -- Loss: 0.231 | Acc: 90.668% | Wgt Acc: 93.463% | LR: 6.250000e-04 | Dur: 132.96s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   1.   3.   6.  74.]
 [  1. 563.   3.   3.  41.]
 [  2.   5. 704.   2.  69.]
 [  5.   1.   4. 516.  53.]
 [ 19.   8.  20.  11. 763.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.214 | Acc: 61.144% | Wgt Acc: 59.001% | Dur: 17.57s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   7.   6.  21.  16.]
 [  3.  44.   8.   4.  18.]
 [  2.  10.  31.   5.  12.]
 [ 13.   4.  15.  49.   9.]
 [  9.  13.  15.   7. 125.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 165
I - Training: 
	I - Batch: 50 | Loss: 0.250 | Acc: 89.000% | Wgt Acc: 92.115%
	I - Batch: 100 | Loss: 0.234 | Acc: 90.375% | Wgt Acc: 93.262%
	I - Batch: 150 | Loss: 0.226 | Acc: 90.583% | Wgt Acc: 93.376%
	I - Batch: 200 | Loss: 0.225 | Acc: 90.562% | Wgt Acc: 93.327%
I - num batch: 222
I - Train -- Loss: 0.226 | Acc: 90.696% | Wgt Acc: 93.432% | LR: 6.250000e-04 | Dur: 134.34s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   2.   0.   6.  76.]
 [  1. 561.   3.   3.  41.]
 [  2.   2. 707.   0.  79.]
 [  7.   1.   3. 514.  38.]
 [ 18.  12.  21.  15. 766.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.079 | Acc: 64.103% | Wgt Acc: 60.255% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   2.  12.  12.]
 [  1.  43.  10.   2.   4.]
 [  2.   8.  23.   0.  10.]
 [ 11.   7.   8.  54.  11.]
 [ 12.  16.  32.  18. 143.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 166
I - Training: 
	I - Batch: 50 | Loss: 0.223 | Acc: 91.375% | Wgt Acc: 94.509%
	I - Batch: 100 | Loss: 0.221 | Acc: 91.562% | Wgt Acc: 94.425%
	I - Batch: 150 | Loss: 0.233 | Acc: 91.125% | Wgt Acc: 94.082%
	I - Batch: 200 | Loss: 0.234 | Acc: 91.062% | Wgt Acc: 93.976%
I - num batch: 222
I - Train -- Loss: 0.233 | Acc: 90.978% | Wgt Acc: 93.929% | LR: 6.250000e-04 | Dur: 134.44s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   0.   1.   5.  81.]
 [  2. 566.   4.   2.  44.]
 [  1.   1. 713.   0.  71.]
 [  7.   1.   0. 519.  46.]
 [ 16.  10.  16.  12. 758.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.127 | Acc: 65.089% | Wgt Acc: 62.209% | Dur: 14.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   6.   7.  16.  14.]
 [  0.  44.   7.   2.   7.]
 [  2.   7.  30.   2.   9.]
 [ 14.   4.  12.  57.  12.]
 [ 11.  17.  19.   9. 138.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 167
I - Training: 
	I - Batch: 50 | Loss: 0.235 | Acc: 90.500% | Wgt Acc: 93.431%
	I - Batch: 100 | Loss: 0.233 | Acc: 90.500% | Wgt Acc: 93.329%
	I - Batch: 150 | Loss: 0.242 | Acc: 90.667% | Wgt Acc: 93.400%
	I - Batch: 200 | Loss: 0.237 | Acc: 90.875% | Wgt Acc: 93.554%
I - num batch: 222
I - Train -- Loss: 0.239 | Acc: 90.781% | Wgt Acc: 93.489% | LR: 6.250000e-04 | Dur: 136.82s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   0.   2.  10.  71.]
 [  2. 564.   0.   1.  43.]
 [  1.   0. 716.   0.  65.]
 [ 10.   0.   2. 511.  53.]
 [ 23.  14.  14.  16. 768.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.180 | Acc: 62.327% | Wgt Acc: 59.492% | Dur: 16.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   5.   3.  14.  10.]
 [  2.  39.   7.   2.   4.]
 [  4.  12.  32.   2.  18.]
 [ 18.   5.  11.  58.  15.]
 [ 10.  17.  22.  10. 133.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 168
I - Training: 
	I - Batch: 50 | Loss: 0.232 | Acc: 90.625% | Wgt Acc: 93.932%
	I - Batch: 100 | Loss: 0.223 | Acc: 90.938% | Wgt Acc: 94.130%
	I - Batch: 150 | Loss: 0.234 | Acc: 90.667% | Wgt Acc: 93.531%
	I - Batch: 200 | Loss: 0.242 | Acc: 90.031% | Wgt Acc: 93.084%
I - num batch: 222
I - Train -- Loss: 0.245 | Acc: 89.879% | Wgt Acc: 92.975% | LR: 6.250000e-04 | Dur: 137.02s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   1.   2.   6.  77.]
 [  2. 560.   0.   4.  36.]
 [  3.   4. 716.   0.  76.]
 [  2.   1.   4. 512.  72.]
 [ 29.  12.  12.  16. 739.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.072 | Acc: 66.075% | Wgt Acc: 62.512% | Dur: 14.56s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   4.  16.   8.]
 [  0.  50.  13.   4.  18.]
 [  3.  10.  35.   4.   6.]
 [ 11.   1.   4.  50.   4.]
 [ 18.  15.  19.  12. 144.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 169
I - Training: 
	I - Batch: 50 | Loss: 0.200 | Acc: 93.000% | Wgt Acc: 95.484%
	I - Batch: 100 | Loss: 0.228 | Acc: 91.312% | Wgt Acc: 94.100%
	I - Batch: 150 | Loss: 0.232 | Acc: 90.833% | Wgt Acc: 93.607%
	I - Batch: 200 | Loss: 0.238 | Acc: 90.562% | Wgt Acc: 93.449%
I - num batch: 222
I - Train -- Loss: 0.237 | Acc: 90.527% | Wgt Acc: 93.404% | LR: 6.250000e-04 | Dur: 137.08s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   0.   2.   7.  68.]
 [  2. 562.   4.   2.  37.]
 [  2.   3. 710.   4.  82.]
 [  4.   1.   1. 514.  56.]
 [ 21.  12.  17.  11. 757.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.138 | Acc: 65.089% | Wgt Acc: 60.088% | Dur: 16.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   2.   4.  16.  10.]
 [  0.  39.   4.   1.   3.]
 [  3.  12.  31.   2.   9.]
 [ 11.   2.   8.  49.   6.]
 [ 15.  23.  28.  18. 152.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 170
I - Training: 
	I - Batch: 50 | Loss: 0.209 | Acc: 92.250% | Wgt Acc: 94.770%
	I - Batch: 100 | Loss: 0.199 | Acc: 92.375% | Wgt Acc: 94.852%
	I - Batch: 150 | Loss: 0.210 | Acc: 92.042% | Wgt Acc: 94.538%
	I - Batch: 200 | Loss: 0.221 | Acc: 91.625% | Wgt Acc: 94.231%
I - num batch: 222
I - Train -- Loss: 0.225 | Acc: 91.570% | Wgt Acc: 94.154% | LR: 6.250000e-04 | Dur: 136.14s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   2.  11.  66.]
 [  2. 563.   2.   1.  26.]
 [  0.   1. 716.   3.  64.]
 [  7.   0.   4. 514.  62.]
 [ 15.  13.  10.   9. 782.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.052 | Acc: 64.103% | Wgt Acc: 61.216% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   1.   1.  16.  13.]
 [  3.  44.   8.   5.  11.]
 [  3.  13.  31.   2.  11.]
 [ 12.   5.   9.  54.   9.]
 [ 10.  15.  26.   9. 136.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 171
I - Training: 
	I - Batch: 50 | Loss: 0.200 | Acc: 90.875% | Wgt Acc: 94.154%
	I - Batch: 100 | Loss: 0.193 | Acc: 92.250% | Wgt Acc: 94.943%
	I - Batch: 150 | Loss: 0.201 | Acc: 92.042% | Wgt Acc: 94.629%
	I - Batch: 200 | Loss: 0.207 | Acc: 91.969% | Wgt Acc: 94.530%
I - num batch: 222
I - Train -- Loss: 0.208 | Acc: 91.909% | Wgt Acc: 94.450% | LR: 6.250000e-04 | Dur: 135.96s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   0.   1.   8.  74.]
 [  2. 570.   2.   0.  30.]
 [  3.   2. 718.   1.  64.]
 [  6.   0.   0. 522.  43.]
 [ 25.   6.  13.   7. 789.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.217 | Acc: 62.919% | Wgt Acc: 57.612% | Dur: 14.82s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   1.   4.  15.   9.]
 [  1.  35.   5.   1.   4.]
 [  3.  12.  32.   4.  10.]
 [ 11.   5.   7.  46.   7.]
 [ 17.  25.  27.  20. 150.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 172
I - Training: 
	I - Batch: 50 | Loss: 0.189 | Acc: 92.500% | Wgt Acc: 95.176%
	I - Batch: 100 | Loss: 0.206 | Acc: 91.500% | Wgt Acc: 94.345%
	I - Batch: 150 | Loss: 0.204 | Acc: 92.042% | Wgt Acc: 94.617%
	I - Batch: 200 | Loss: 0.209 | Acc: 92.000% | Wgt Acc: 94.497%
I - num batch: 222
I - Train -- Loss: 0.207 | Acc: 92.191% | Wgt Acc: 94.703% | LR: 6.250000e-04 | Dur: 135.04s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   2.   3.  64.]
 [  2. 568.   2.   1.  32.]
 [  1.   0. 713.   2.  58.]
 [  3.   0.   2. 523.  53.]
 [ 18.   9.  15.   9. 793.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.156 | Acc: 63.905% | Wgt Acc: 60.119% | Dur: 18.94s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   5.   3.   9.   9.]
 [  0.  40.   9.   3.   9.]
 [  4.   6.  32.   3.  11.]
 [ 15.   4.   6.  54.   9.]
 [ 13.  23.  25.  17. 142.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 173
I - Training: 
	I - Batch: 50 | Loss: 0.222 | Acc: 91.000% | Wgt Acc: 93.358%
	I - Batch: 100 | Loss: 0.200 | Acc: 91.938% | Wgt Acc: 94.418%
	I - Batch: 150 | Loss: 0.206 | Acc: 91.250% | Wgt Acc: 94.086%
	I - Batch: 200 | Loss: 0.210 | Acc: 91.812% | Wgt Acc: 94.435%
I - num batch: 222
I - Train -- Loss: 0.214 | Acc: 91.570% | Wgt Acc: 94.303% | LR: 6.250000e-04 | Dur: 135.61s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   0.   4.  67.]
 [  1. 565.   3.   2.  39.]
 [  1.   1. 712.   2.  72.]
 [  3.   0.   2. 520.  47.]
 [ 16.  12.  17.  10. 775.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.174 | Acc: 62.130% | Wgt Acc: 59.064% | Dur: 19.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 51.   0.   3.  10.   5.]
 [  2.  37.   7.   6.  14.]
 [  3.  22.  44.   5.  22.]
 [ 20.   3.   7.  50.   6.]
 [ 12.  16.  14.  15. 133.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 174
I - Training: 
	I - Batch: 50 | Loss: 0.220 | Acc: 91.250% | Wgt Acc: 94.308%
	I - Batch: 100 | Loss: 0.212 | Acc: 91.500% | Wgt Acc: 94.246%
	I - Batch: 150 | Loss: 0.224 | Acc: 91.000% | Wgt Acc: 93.836%
	I - Batch: 200 | Loss: 0.222 | Acc: 91.281% | Wgt Acc: 94.011%
I - num batch: 222
I - Train -- Loss: 0.223 | Acc: 91.260% | Wgt Acc: 93.965% | LR: 6.250000e-04 | Dur: 134.00s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   2.   5.  72.]
 [  2. 562.   2.   4.  50.]
 [  0.   4. 713.   2.  62.]
 [  4.   2.   2. 516.  43.]
 [ 18.   9.  15.  11. 773.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.207 | Acc: 63.511% | Wgt Acc: 60.339% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   4.  15.  11.]
 [  1.  44.   4.   1.   6.]
 [  2.  10.  33.   6.  14.]
 [ 15.   3.   7.  53.  12.]
 [ 15.  19.  27.  11. 137.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 175
I - Training: 
	I - Batch: 50 | Loss: 0.214 | Acc: 89.875% | Wgt Acc: 93.241%
	I - Batch: 100 | Loss: 0.220 | Acc: 89.938% | Wgt Acc: 93.125%
	I - Batch: 150 | Loss: 0.226 | Acc: 90.208% | Wgt Acc: 93.194%
	I - Batch: 200 | Loss: 0.229 | Acc: 90.281% | Wgt Acc: 93.174%
I - num batch: 222
I - Train -- Loss: 0.229 | Acc: 90.189% | Wgt Acc: 93.133% | LR: 6.250000e-04 | Dur: 133.80s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   1.   3.   8.  72.]
 [  3. 558.   0.   2.  53.]
 [  3.   3. 712.   1.  69.]
 [  3.   1.   2. 520.  55.]
 [ 30.  15.  17.   7. 751.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.230 | Acc: 61.736% | Wgt Acc: 58.521% | Dur: 14.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   2.   3.  21.  18.]
 [  0.  33.   5.   1.   5.]
 [  4.  16.  34.   3.  14.]
 [ 11.   5.  11.  49.  10.]
 [  9.  22.  22.  12. 133.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 176
I - Training: 
	I - Batch: 50 | Loss: 0.199 | Acc: 91.125% | Wgt Acc: 94.220%
	I - Batch: 100 | Loss: 0.207 | Acc: 91.312% | Wgt Acc: 93.866%
	I - Batch: 150 | Loss: 0.216 | Acc: 91.208% | Wgt Acc: 93.813%
	I - Batch: 200 | Loss: 0.220 | Acc: 91.000% | Wgt Acc: 93.683%
I - num batch: 222
I - Train -- Loss: 0.217 | Acc: 91.260% | Wgt Acc: 93.832% | LR: 6.250000e-04 | Dur: 135.42s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   4.   4.  66.]
 [  2. 564.   0.   2.  37.]
 [  1.   3. 711.   0.  69.]
 [  4.   1.   3. 514.  48.]
 [ 22.   9.  16.  18. 780.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.226 | Acc: 61.144% | Wgt Acc: 59.576% | Dur: 18.99s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   6.  12.  15.]
 [  0.  38.   7.   1.   8.]
 [  2.  12.  28.   4.  20.]
 [ 15.   8.  14.  60.  15.]
 [  9.  16.  20.   9. 122.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 177
I - Training: 
	I - Batch: 50 | Loss: 0.199 | Acc: 92.250% | Wgt Acc: 95.006%
	I - Batch: 100 | Loss: 0.213 | Acc: 91.500% | Wgt Acc: 94.265%
	I - Batch: 150 | Loss: 0.213 | Acc: 91.375% | Wgt Acc: 94.164%
	I - Batch: 200 | Loss: 0.216 | Acc: 91.406% | Wgt Acc: 94.091%
I - num batch: 222
I - Train -- Loss: 0.212 | Acc: 91.429% | Wgt Acc: 94.205% | LR: 6.250000e-04 | Dur: 134.74s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   3.   5.  60.]
 [  2. 565.   0.   2.  35.]
 [  0.   2. 716.   0.  80.]
 [  6.   2.   1. 517.  54.]
 [ 15.   9.  14.  14. 771.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.157 | Acc: 65.089% | Wgt Acc: 59.670% | Dur: 15.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   4.   5.  14.   6.]
 [  1.  34.   4.   1.   3.]
 [  1.   9.  29.   2.  11.]
 [  9.   2.   4.  51.   5.]
 [ 16.  29.  33.  18. 155.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 178
I - Training: 
	I - Batch: 50 | Loss: 0.209 | Acc: 91.875% | Wgt Acc: 94.624%
	I - Batch: 100 | Loss: 0.215 | Acc: 91.812% | Wgt Acc: 94.258%
	I - Batch: 150 | Loss: 0.219 | Acc: 91.500% | Wgt Acc: 93.981%
	I - Batch: 200 | Loss: 0.221 | Acc: 91.156% | Wgt Acc: 93.742%
I - num batch: 222
I - Train -- Loss: 0.221 | Acc: 91.232% | Wgt Acc: 93.895% | LR: 6.250000e-04 | Dur: 135.71s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   3.   3.   6.  72.]
 [  2. 560.   1.   0.  37.]
 [  1.   0. 723.   2.  65.]
 [  5.   2.   1. 517.  51.]
 [ 28.  13.   6.  13. 775.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.110 | Acc: 64.497% | Wgt Acc: 62.595% | Dur: 14.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   0.   4.   6.   7.]
 [  1.  44.   7.   3.  14.]
 [  2.  17.  40.   3.  19.]
 [ 17.   3.   4.  58.  10.]
 [ 13.  14.  20.  16. 130.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 179
I - Training: 
	I - Batch: 50 | Loss: 0.220 | Acc: 91.000% | Wgt Acc: 93.223%
	I - Batch: 100 | Loss: 0.216 | Acc: 91.500% | Wgt Acc: 93.898%
	I - Batch: 150 | Loss: 0.223 | Acc: 91.292% | Wgt Acc: 93.845%
	I - Batch: 200 | Loss: 0.222 | Acc: 91.281% | Wgt Acc: 93.925%
I - num batch: 222
I - Train -- Loss: 0.221 | Acc: 91.458% | Wgt Acc: 94.049% | LR: 6.250000e-04 | Dur: 135.91s
I - Confusion Matrix: [row->prediction - col->label]
[[661.   2.   2.   8.  73.]
 [  3. 565.   5.   3.  34.]
 [  5.   2. 714.   0.  58.]
 [  8.   1.   0. 522.  53.]
 [ 20.   8.  13.   5. 782.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.148 | Acc: 62.919% | Wgt Acc: 58.541% | Dur: 17.48s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   3.   4.  12.   6.]
 [  2.  42.  11.   4.  10.]
 [  2.  10.  31.   5.  14.]
 [ 15.   1.   6.  48.   6.]
 [ 15.  22.  23.  17. 144.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 180
I - Training: 
	I - Batch: 50 | Loss: 0.215 | Acc: 91.375% | Wgt Acc: 93.881%
	I - Batch: 100 | Loss: 0.221 | Acc: 90.625% | Wgt Acc: 93.178%
	I - Batch: 150 | Loss: 0.229 | Acc: 90.458% | Wgt Acc: 93.169%
	I - Batch: 200 | Loss: 0.223 | Acc: 90.906% | Wgt Acc: 93.506%
I - num batch: 222
I - Train -- Loss: 0.222 | Acc: 91.063% | Wgt Acc: 93.599% | LR: 6.250000e-04 | Dur: 136.98s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   0.   0.  12.  64.]
 [  3. 569.   0.   3.  40.]
 [  2.   2. 711.   0.  59.]
 [  9.   1.   1. 510.  57.]
 [ 23.   6.  22.  13. 780.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.114 | Acc: 66.075% | Wgt Acc: 63.045% | Dur: 18.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   5.   5.  16.  10.]
 [  1.  41.   7.   2.   6.]
 [  1.  12.  38.   1.  12.]
 [ 13.   3.  11.  54.  12.]
 [ 11.  17.  14.  13. 140.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 181
I - Training: 
	I - Batch: 50 | Loss: 0.207 | Acc: 92.250% | Wgt Acc: 94.667%
	I - Batch: 100 | Loss: 0.207 | Acc: 92.000% | Wgt Acc: 94.414%
	I - Batch: 150 | Loss: 0.211 | Acc: 91.958% | Wgt Acc: 94.476%
	I - Batch: 200 | Loss: 0.218 | Acc: 91.531% | Wgt Acc: 94.201%
I - num batch: 222
I - Train -- Loss: 0.222 | Acc: 91.458% | Wgt Acc: 94.088% | LR: 6.250000e-04 | Dur: 135.23s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   1.   1.   7.  74.]
 [  1. 562.   3.   0.  36.]
 [  1.   1. 713.   0.  66.]
 [  4.   1.   1. 518.  45.]
 [ 19.  13.  16.  13. 779.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.156 | Acc: 66.075% | Wgt Acc: 63.139% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   5.  12.  12.]
 [  1.  39.   4.   1.  11.]
 [  1.  14.  37.   7.  10.]
 [  8.   3.   7.  53.   8.]
 [ 11.  18.  22.  13. 139.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 182
I - Training: 
	I - Batch: 50 | Loss: 0.203 | Acc: 92.500% | Wgt Acc: 94.496%
	I - Batch: 100 | Loss: 0.206 | Acc: 92.188% | Wgt Acc: 94.441%
	I - Batch: 150 | Loss: 0.220 | Acc: 91.792% | Wgt Acc: 93.944%
	I - Batch: 200 | Loss: 0.222 | Acc: 91.688% | Wgt Acc: 93.943%
I - num batch: 222
I - Train -- Loss: 0.224 | Acc: 91.429% | Wgt Acc: 93.764% | LR: 6.250000e-04 | Dur: 139.92s
I - Confusion Matrix: [row->prediction - col->label]
[[659.   1.   3.  10.  65.]
 [  0. 568.   4.   2.  36.]
 [  4.   1. 712.   3.  58.]
 [  6.   1.   3. 510.  47.]
 [ 28.   7.  12.  13. 794.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.193 | Acc: 60.750% | Wgt Acc: 56.379% | Dur: 14.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   4.   9.   9.]
 [  1.  37.   7.   1.   5.]
 [  3.  16.  38.   8.  20.]
 [ 15.   0.   3.  41.   7.]
 [ 16.  24.  23.  27. 139.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 183
I - Training: 
	I - Batch: 50 | Loss: 0.213 | Acc: 91.375% | Wgt Acc: 93.956%
	I - Batch: 100 | Loss: 0.228 | Acc: 90.562% | Wgt Acc: 93.468%
	I - Batch: 150 | Loss: 0.233 | Acc: 90.375% | Wgt Acc: 93.231%
	I - Batch: 200 | Loss: 0.238 | Acc: 90.250% | Wgt Acc: 93.119%
I - num batch: 222
I - Train -- Loss: 0.238 | Acc: 90.161% | Wgt Acc: 93.060% | LR: 6.250000e-04 | Dur: 135.03s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   0.   4.   9.  89.]
 [  1. 565.   1.   3.  46.]
 [  3.   1. 714.   1.  62.]
 [ 11.   0.   1. 509.  51.]
 [ 24.  12.  14.  16. 752.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.257 | Acc: 60.947% | Wgt Acc: 55.543% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   2.   3.  13.   7.]
 [  0.  35.   1.   2.   6.]
 [  4.  14.  39.   8.  17.]
 [  9.   3.   5.  36.   4.]
 [ 22.  24.  27.  27. 146.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 184
I - Training: 
	I - Batch: 50 | Loss: 0.224 | Acc: 92.000% | Wgt Acc: 94.243%
	I - Batch: 100 | Loss: 0.226 | Acc: 91.562% | Wgt Acc: 94.256%
	I - Batch: 150 | Loss: 0.228 | Acc: 91.042% | Wgt Acc: 93.819%
	I - Batch: 200 | Loss: 0.215 | Acc: 91.531% | Wgt Acc: 94.233%
I - num batch: 222
I - Train -- Loss: 0.214 | Acc: 91.711% | Wgt Acc: 94.356% | LR: 6.250000e-04 | Dur: 138.34s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   0.   1.   6.  72.]
 [  2. 571.   0.   2.  31.]
 [  2.   1. 717.   2.  67.]
 [  5.   0.   0. 517.  49.]
 [ 21.   6.  16.  11. 781.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.200 | Acc: 60.158% | Wgt Acc: 54.185% | Dur: 17.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   1.   4.  14.  10.]
 [  0.  25.   4.   0.   3.]
 [  0.  12.  27.   2.  10.]
 [ 15.   7.   3.  48.   7.]
 [ 18.  33.  37.  22. 150.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 185
I - Training: 
	I - Batch: 50 | Loss: 0.201 | Acc: 92.375% | Wgt Acc: 94.582%
	I - Batch: 100 | Loss: 0.212 | Acc: 91.500% | Wgt Acc: 93.937%
	I - Batch: 150 | Loss: 0.208 | Acc: 91.917% | Wgt Acc: 94.422%
	I - Batch: 200 | Loss: 0.218 | Acc: 91.188% | Wgt Acc: 93.828%
I - num batch: 222
I - Train -- Loss: 0.225 | Acc: 90.922% | Wgt Acc: 93.613% | LR: 6.250000e-04 | Dur: 136.36s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   2.   1.   9.  70.]
 [  3. 559.   1.   0.  38.]
 [  4.   4. 721.   0.  62.]
 [ 10.   1.   1. 519.  59.]
 [ 25.  12.  10.  10. 771.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.216 | Acc: 62.525% | Wgt Acc: 57.956% | Dur: 14.78s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   5.   4.  17.   8.]
 [  0.  41.   4.   4.   9.]
 [  3.   9.  36.   8.  16.]
 [  8.   0.   4.  37.   4.]
 [ 17.  23.  27.  20. 143.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 186
I - Training: 
	I - Batch: 50 | Loss: 0.237 | Acc: 91.375% | Wgt Acc: 93.623%
	I - Batch: 100 | Loss: 0.227 | Acc: 91.125% | Wgt Acc: 93.967%
	I - Batch: 150 | Loss: 0.220 | Acc: 91.542% | Wgt Acc: 94.279%
	I - Batch: 200 | Loss: 0.220 | Acc: 91.625% | Wgt Acc: 94.256%
I - num batch: 222
I - Train -- Loss: 0.217 | Acc: 91.796% | Wgt Acc: 94.387% | LR: 6.250000e-04 | Dur: 136.04s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   0.   2.   6.  77.]
 [  3. 574.   1.   0.  28.]
 [  3.   0. 712.   0.  57.]
 [  3.   0.   1. 518.  53.]
 [ 21.   4.  18.  14. 785.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.155 | Acc: 62.130% | Wgt Acc: 59.482% | Dur: 20.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   4.   6.  13.  12.]
 [  1.  41.  11.   1.  12.]
 [  1.   9.  29.   4.  10.]
 [ 17.   6.  10.  55.  15.]
 [ 10.  18.  19.  13. 131.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 187
I - Training: 
	I - Batch: 50 | Loss: 0.197 | Acc: 92.625% | Wgt Acc: 95.003%
	I - Batch: 100 | Loss: 0.184 | Acc: 93.125% | Wgt Acc: 95.549%
	I - Batch: 150 | Loss: 0.206 | Acc: 92.042% | Wgt Acc: 94.502%
	I - Batch: 200 | Loss: 0.217 | Acc: 91.656% | Wgt Acc: 94.180%
I - num batch: 222
I - Train -- Loss: 0.215 | Acc: 91.909% | Wgt Acc: 94.333% | LR: 6.250000e-04 | Dur: 137.80s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   1.   2.   9.  60.]
 [  3. 567.   1.   1.  30.]
 [  1.   2. 715.   2.  62.]
 [  3.   2.   1. 515.  54.]
 [ 21.   6.  15.  11. 794.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.168 | Acc: 64.497% | Wgt Acc: 60.391% | Dur: 16.96s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   5.  20.  15.]
 [  0.  37.   4.   1.   2.]
 [  2.  12.  36.   1.  13.]
 [  8.   5.   6.  47.   6.]
 [ 15.  20.  24.  17. 144.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 188
I - Training: 
	I - Batch: 50 | Loss: 0.176 | Acc: 93.500% | Wgt Acc: 95.951%
	I - Batch: 100 | Loss: 0.186 | Acc: 92.750% | Wgt Acc: 95.232%
	I - Batch: 150 | Loss: 0.191 | Acc: 92.458% | Wgt Acc: 95.158%
	I - Batch: 200 | Loss: 0.193 | Acc: 92.250% | Wgt Acc: 95.019%
I - num batch: 208
I - Train -- Loss: 0.193 | Acc: 92.218% | Wgt Acc: 94.999% | LR: 6.250000e-04 | Dur: 125.25s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   1.   2.   6.  56.]
 [  1. 564.   0.   1.  32.]
 [  0.   1. 724.   0.  63.]
 [  3.   1.   2. 522.  49.]
 [ 15.  11.   6.   9. 581.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.113 | Acc: 63.116% | Wgt Acc: 60.589% | Dur: 14.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   6.   6.  21.   9.]
 [  2.  42.  11.   4.  17.]
 [  1.   9.  34.   2.  13.]
 [ 12.   6.   8.  50.  10.]
 [ 10.  15.  16.   9. 131.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 189
I - Training: 
	I - Batch: 50 | Loss: 0.203 | Acc: 91.750% | Wgt Acc: 94.759%
	I - Batch: 100 | Loss: 0.202 | Acc: 91.938% | Wgt Acc: 94.610%
	I - Batch: 150 | Loss: 0.199 | Acc: 92.042% | Wgt Acc: 94.757%
	I - Batch: 200 | Loss: 0.204 | Acc: 91.625% | Wgt Acc: 94.445%
I - num batch: 222
I - Train -- Loss: 0.205 | Acc: 91.542% | Wgt Acc: 94.346% | LR: 6.250000e-04 | Dur: 134.67s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   1.   2.   6.  60.]
 [  1. 564.   0.   1.  54.]
 [  0.   0. 719.   1.  63.]
 [  4.   1.   0. 521.  52.]
 [ 20.  12.  13.   9. 771.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.300 | Acc: 62.130% | Wgt Acc: 57.329% | Dur: 17.49s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   5.   5.  18.  13.]
 [  0.  32.   7.   1.   8.]
 [  1.  16.  36.   3.  14.]
 [ 10.   3.   4.  41.   1.]
 [ 15.  22.  23.  23. 144.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 190
I - Training: 
	I - Batch: 50 | Loss: 0.196 | Acc: 91.875% | Wgt Acc: 94.202%
	I - Batch: 100 | Loss: 0.211 | Acc: 91.750% | Wgt Acc: 93.994%
	I - Batch: 150 | Loss: 0.206 | Acc: 92.000% | Wgt Acc: 94.350%
	I - Batch: 200 | Loss: 0.207 | Acc: 92.219% | Wgt Acc: 94.485%
I - num batch: 222
I - Train -- Loss: 0.210 | Acc: 92.021% | Wgt Acc: 94.336% | LR: 6.250000e-04 | Dur: 135.24s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   1.   5.  60.]
 [  3. 564.   3.   1.  36.]
 [  1.   0. 710.   1.  63.]
 [  4.   0.   4. 517.  40.]
 [ 17.  14.  16.  14. 801.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.160 | Acc: 66.272% | Wgt Acc: 63.922% | Dur: 15.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   4.   4.  14.  14.]
 [  1.  46.   8.   3.  10.]
 [  2.   8.  39.   1.   9.]
 [ 16.   6.   7.  56.  11.]
 [ 10.  14.  17.  12. 136.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 191
I - Training: 
	I - Batch: 50 | Loss: 0.191 | Acc: 92.750% | Wgt Acc: 94.939%
	I - Batch: 100 | Loss: 0.208 | Acc: 92.125% | Wgt Acc: 94.636%
	I - Batch: 150 | Loss: 0.211 | Acc: 91.875% | Wgt Acc: 94.494%
	I - Batch: 200 | Loss: 0.218 | Acc: 91.375% | Wgt Acc: 93.920%
I - num batch: 222
I - Train -- Loss: 0.221 | Acc: 91.204% | Wgt Acc: 93.831% | LR: 6.250000e-04 | Dur: 136.72s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   2.   1.   7.  75.]
 [  1. 568.   4.   0.  36.]
 [  1.   1. 712.   3.  59.]
 [  9.   0.   2. 515.  53.]
 [ 23.   7.  15.  13. 777.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.231 | Acc: 59.369% | Wgt Acc: 54.582% | Dur: 15.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   3.   3.  17.  13.]
 [  1.  41.  10.   3.  16.]
 [  1.   6.  26.   3.  11.]
 [ 14.   3.  10.  40.   0.]
 [ 18.  25.  26.  23. 140.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 192
I - Training: 
	I - Batch: 50 | Loss: 0.235 | Acc: 90.875% | Wgt Acc: 93.312%
	I - Batch: 100 | Loss: 0.223 | Acc: 91.250% | Wgt Acc: 93.974%
	I - Batch: 150 | Loss: 0.226 | Acc: 91.458% | Wgt Acc: 94.001%
	I - Batch: 200 | Loss: 0.230 | Acc: 91.031% | Wgt Acc: 93.639%
I - num batch: 222
I - Train -- Loss: 0.231 | Acc: 91.063% | Wgt Acc: 93.703% | LR: 6.250000e-04 | Dur: 137.81s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   0.   3.   6.  62.]
 [  2. 569.   3.   2.  35.]
 [  4.   3. 703.   1.  80.]
 [  4.   0.   1. 514.  48.]
 [ 18.   6.  24.  15. 775.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.178 | Acc: 63.116% | Wgt Acc: 59.910% | Dur: 14.65s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   7.   4.  24.  14.]
 [  1.  38.   7.   2.   6.]
 [  2.  14.  42.   7.  18.]
 [  4.   1.   2.  39.   8.]
 [ 14.  18.  20.  14. 134.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 193
I - Training: 
	I - Batch: 50 | Loss: 0.250 | Acc: 90.500% | Wgt Acc: 93.433%
	I - Batch: 100 | Loss: 0.237 | Acc: 90.625% | Wgt Acc: 93.574%
	I - Batch: 150 | Loss: 0.231 | Acc: 90.750% | Wgt Acc: 93.591%
	I - Batch: 200 | Loss: 0.226 | Acc: 90.906% | Wgt Acc: 93.728%
I - num batch: 222
I - Train -- Loss: 0.226 | Acc: 90.950% | Wgt Acc: 93.747% | LR: 6.250000e-04 | Dur: 135.00s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   0.   1.   4.  84.]
 [  2. 565.   7.   0.  34.]
 [  4.   2. 710.   1.  68.]
 [  5.   0.   1. 519.  48.]
 [ 20.  11.  15.  14. 766.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.303 | Acc: 60.947% | Wgt Acc: 55.804% | Dur: 17.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   9.   5.  28.  20.]
 [  0.  34.   3.   0.   3.]
 [  0.   5.  25.   1.   6.]
 [  5.   5.  10.  36.   7.]
 [ 13.  25.  32.  21. 144.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 194
I - Training: 
	I - Batch: 50 | Loss: 0.216 | Acc: 92.125% | Wgt Acc: 94.915%
	I - Batch: 100 | Loss: 0.217 | Acc: 91.438% | Wgt Acc: 94.165%
	I - Batch: 150 | Loss: 0.210 | Acc: 92.083% | Wgt Acc: 94.500%
	I - Batch: 200 | Loss: 0.204 | Acc: 92.062% | Wgt Acc: 94.477%
I - num batch: 222
I - Train -- Loss: 0.208 | Acc: 91.739% | Wgt Acc: 94.276% | LR: 6.250000e-04 | Dur: 138.91s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   2.   1.   4.  72.]
 [  3. 564.   3.   0.  40.]
 [  3.   3. 719.   1.  55.]
 [  7.   0.   1. 520.  46.]
 [ 20.   9.  10.  13. 787.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.161 | Acc: 62.919% | Wgt Acc: 58.374% | Dur: 14.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   4.   4.  16.   6.]
 [  0.  35.   5.   1.   5.]
 [  2.   9.  31.   1.  11.]
 [ 15.   1.   6.  50.  13.]
 [ 13.  29.  29.  18. 145.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 195
I - Training: 
	I - Batch: 50 | Loss: 0.186 | Acc: 93.250% | Wgt Acc: 95.341%
	I - Batch: 100 | Loss: 0.203 | Acc: 92.562% | Wgt Acc: 95.026%
	I - Batch: 150 | Loss: 0.196 | Acc: 92.875% | Wgt Acc: 95.289%
	I - Batch: 200 | Loss: 0.196 | Acc: 92.875% | Wgt Acc: 95.346%
I - num batch: 222
I - Train -- Loss: 0.201 | Acc: 92.473% | Wgt Acc: 94.951% | LR: 6.250000e-04 | Dur: 136.96s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   1.   4.  57.]
 [  2. 570.   1.   1.  36.]
 [  2.   0. 720.   0.  55.]
 [  2.   0.   1. 520.  55.]
 [ 18.   8.  11.  13. 797.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.197 | Acc: 62.722% | Wgt Acc: 59.962% | Dur: 17.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   1.   5.  14.  12.]
 [  0.  37.   4.   2.   6.]
 [  2.  15.  40.   3.  22.]
 [ 18.   3.   7.  52.   8.]
 [ 11.  22.  19.  15. 132.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 196
I - Training: 
	I - Batch: 50 | Loss: 0.203 | Acc: 92.750% | Wgt Acc: 95.746%
	I - Batch: 100 | Loss: 0.191 | Acc: 92.812% | Wgt Acc: 95.474%
	I - Batch: 150 | Loss: 0.208 | Acc: 91.500% | Wgt Acc: 94.384%
	I - Batch: 200 | Loss: 0.205 | Acc: 91.938% | Wgt Acc: 94.691%
I - num batch: 222
I - Train -- Loss: 0.204 | Acc: 91.965% | Wgt Acc: 94.671% | LR: 6.250000e-04 | Dur: 133.47s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   0.   2.  65.]
 [  2. 568.   2.   1.  34.]
 [  1.   2. 721.   3.  62.]
 [  1.   0.   1. 519.  59.]
 [ 19.   8.  10.  13. 780.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.226 | Acc: 62.722% | Wgt Acc: 60.767% | Dur: 14.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   7.   5.  12.  17.]
 [  0.  44.  12.   3.  11.]
 [  2.   8.  26.   0.  14.]
 [ 14.   4.  11.  61.  10.]
 [ 13.  15.  21.  10. 128.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 197
I - Training: 
	I - Batch: 50 | Loss: 0.195 | Acc: 91.625% | Wgt Acc: 94.341%
	I - Batch: 100 | Loss: 0.216 | Acc: 91.750% | Wgt Acc: 94.272%
	I - Batch: 150 | Loss: 0.216 | Acc: 92.083% | Wgt Acc: 94.409%
	I - Batch: 200 | Loss: 0.215 | Acc: 92.281% | Wgt Acc: 94.522%
I - num batch: 222
I - Train -- Loss: 0.217 | Acc: 92.134% | Wgt Acc: 94.303% | LR: 6.250000e-04 | Dur: 134.57s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   6.   2.  10.  59.]
 [  1. 561.   4.   3.  33.]
 [  4.   4. 717.   1.  49.]
 [  4.   0.   2. 513.  50.]
 [ 20.   7.   9.  11. 809.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.224 | Acc: 64.103% | Wgt Acc: 60.328% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   5.   5.  17.  15.]
 [  0.  33.   2.   1.   5.]
 [  1.  10.  29.   2.   7.]
 [ 12.   4.  11.  56.  11.]
 [ 10.  26.  28.  10. 142.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 198
I - Training: 
	I - Batch: 50 | Loss: 0.222 | Acc: 91.125% | Wgt Acc: 93.861%
	I - Batch: 100 | Loss: 0.216 | Acc: 91.625% | Wgt Acc: 94.247%
	I - Batch: 150 | Loss: 0.214 | Acc: 91.792% | Wgt Acc: 94.477%
	I - Batch: 200 | Loss: 0.212 | Acc: 91.750% | Wgt Acc: 94.354%
I - num batch: 222
I - Train -- Loss: 0.215 | Acc: 91.796% | Wgt Acc: 94.312% | LR: 6.250000e-04 | Dur: 135.56s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   4.   1.  70.]
 [  2. 563.   5.   0.  40.]
 [  2.   3. 713.   1.  67.]
 [  2.   0.   0. 523.  34.]
 [ 23.  11.  12.  13. 789.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.170 | Acc: 63.708% | Wgt Acc: 58.364% | Dur: 13.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   3.  19.  10.]
 [  0.  37.   8.   2.   6.]
 [  0.   8.  29.   2.   8.]
 [ 13.   2.   9.  48.   4.]
 [ 18.  29.  26.  15. 152.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 199
I - Training: 
	I - Batch: 50 | Loss: 0.189 | Acc: 93.375% | Wgt Acc: 95.477%
	I - Batch: 100 | Loss: 0.198 | Acc: 92.812% | Wgt Acc: 94.936%
	I - Batch: 150 | Loss: 0.196 | Acc: 92.792% | Wgt Acc: 95.068%
	I - Batch: 200 | Loss: 0.193 | Acc: 92.875% | Wgt Acc: 95.110%
I - num batch: 222
I - Train -- Loss: 0.192 | Acc: 92.726% | Wgt Acc: 95.069% | LR: 6.250000e-04 | Dur: 134.24s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   1.   3.  54.]
 [  2. 565.   1.   1.  28.]
 [  1.   1. 725.   2.  67.]
 [  1.   0.   1. 519.  45.]
 [ 19.  12.   6.  13. 806.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.146 | Acc: 63.314% | Wgt Acc: 57.601% | Dur: 14.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   2.  18.  13.]
 [  1.  41.   7.   1.   7.]
 [  1.   8.  31.   2.   6.]
 [ 10.   2.   6.  39.   1.]
 [ 19.  25.  29.  26. 153.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 200
I - Training: 
	I - Batch: 50 | Loss: 0.205 | Acc: 94.250% | Wgt Acc: 95.796%
	I - Batch: 100 | Loss: 0.201 | Acc: 93.375% | Wgt Acc: 95.347%
	I - Batch: 150 | Loss: 0.212 | Acc: 92.375% | Wgt Acc: 94.712%
	I - Batch: 200 | Loss: 0.218 | Acc: 92.000% | Wgt Acc: 94.313%
I - num batch: 222
I - Train -- Loss: 0.220 | Acc: 91.683% | Wgt Acc: 94.115% | LR: 6.250000e-04 | Dur: 133.74s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   0.   0.  10.  65.]
 [  3. 566.   1.   0.  43.]
 [  3.   2. 714.   1.  56.]
 [  4.   1.   2. 518.  44.]
 [ 25.   9.  17.   9. 792.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.130 | Acc: 62.919% | Wgt Acc: 57.852% | Dur: 17.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   4.   4.  18.   7.]
 [  2.  38.   3.   1.   5.]
 [  3.  13.  33.   3.  15.]
 [ 12.   1.   4.  43.   5.]
 [ 14.  22.  31.  21. 148.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 201
I - Training: 
	I - Batch: 50 | Loss: 0.206 | Acc: 91.875% | Wgt Acc: 94.458%
	I - Batch: 100 | Loss: 0.210 | Acc: 91.938% | Wgt Acc: 94.630%
	I - Batch: 150 | Loss: 0.217 | Acc: 91.500% | Wgt Acc: 94.079%
	I - Batch: 200 | Loss: 0.221 | Acc: 91.625% | Wgt Acc: 94.133%
I - num batch: 222
I - Train -- Loss: 0.223 | Acc: 91.570% | Wgt Acc: 94.053% | LR: 6.250000e-04 | Dur: 132.01s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   1.   3.  14.  68.]
 [  1. 564.   0.   0.  41.]
 [  2.   1. 716.   0.  63.]
 [  8.   2.   1. 517.  40.]
 [ 23.  10.  14.   7. 788.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.243 | Acc: 64.497% | Wgt Acc: 60.150% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   5.   5.  24.  12.]
 [  0.  46.  10.   2.   8.]
 [  2.   8.  29.   2.  10.]
 [ 10.   4.   4.  44.   4.]
 [ 14.  15.  27.  14. 146.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 202
I - Training: 
	I - Batch: 50 | Loss: 0.197 | Acc: 93.000% | Wgt Acc: 95.368%
	I - Batch: 100 | Loss: 0.211 | Acc: 91.438% | Wgt Acc: 94.181%
	I - Batch: 150 | Loss: 0.227 | Acc: 91.000% | Wgt Acc: 93.651%
	I - Batch: 200 | Loss: 0.227 | Acc: 90.781% | Wgt Acc: 93.438%
I - num batch: 222
I - Train -- Loss: 0.226 | Acc: 90.922% | Wgt Acc: 93.546% | LR: 6.250000e-04 | Dur: 135.63s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   2.  13.  66.]
 [  0. 560.   1.   0.  48.]
 [  0.   3. 716.   1.  63.]
 [ 10.   0.   2. 508.  50.]
 [ 19.  14.  13.  16. 773.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.230 | Acc: 64.103% | Wgt Acc: 58.238% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   4.   3.  13.   8.]
 [  1.  33.   3.   1.   3.]
 [  2.  12.  28.   3.   9.]
 [ 10.   4.  11.  47.   4.]
 [ 14.  25.  30.  22. 156.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 203
I - Training: 
	I - Batch: 50 | Loss: 0.212 | Acc: 91.250% | Wgt Acc: 94.338%
	I - Batch: 100 | Loss: 0.215 | Acc: 91.125% | Wgt Acc: 94.167%
	I - Batch: 150 | Loss: 0.209 | Acc: 91.375% | Wgt Acc: 94.247%
	I - Batch: 200 | Loss: 0.208 | Acc: 91.469% | Wgt Acc: 94.370%
I - num batch: 222
I - Train -- Loss: 0.210 | Acc: 91.599% | Wgt Acc: 94.426% | LR: 6.250000e-04 | Dur: 135.28s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   1.   1.  65.]
 [  1. 564.   0.   2.  39.]
 [  0.   1. 715.   0.  77.]
 [  2.   0.   0. 526.  48.]
 [ 21.  12.  18.   9. 771.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.201 | Acc: 63.905% | Wgt Acc: 59.638% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   3.   2.  19.   6.]
 [  0.  42.  11.   3.  12.]
 [  0.  12.  34.   4.  14.]
 [ 10.   1.   5.  43.   4.]
 [ 17.  20.  23.  17. 144.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 204
I - Training: 
	I - Batch: 50 | Loss: 0.202 | Acc: 92.375% | Wgt Acc: 95.175%
	I - Batch: 100 | Loss: 0.214 | Acc: 91.500% | Wgt Acc: 94.410%
	I - Batch: 150 | Loss: 0.215 | Acc: 91.458% | Wgt Acc: 94.218%
	I - Batch: 200 | Loss: 0.205 | Acc: 91.750% | Wgt Acc: 94.435%
I - num batch: 222
I - Train -- Loss: 0.202 | Acc: 91.824% | Wgt Acc: 94.464% | LR: 6.250000e-04 | Dur: 138.87s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   0.   4.  66.]
 [  1. 571.   2.   2.  42.]
 [  2.   1. 713.   1.  63.]
 [  5.   0.   2. 522.  46.]
 [ 21.   5.  17.   9. 783.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.163 | Acc: 63.905% | Wgt Acc: 59.241% | Dur: 16.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   4.  18.   9.]
 [  0.  35.   2.   2.   2.]
 [  1.  12.  29.   3.  10.]
 [  8.   5.  12.  47.  12.]
 [ 13.  23.  28.  16. 147.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 205
I - Training: 
	I - Batch: 50 | Loss: 0.211 | Acc: 92.375% | Wgt Acc: 94.459%
	I - Batch: 100 | Loss: 0.206 | Acc: 91.688% | Wgt Acc: 93.918%
	I - Batch: 150 | Loss: 0.206 | Acc: 91.792% | Wgt Acc: 94.201%
	I - Batch: 200 | Loss: 0.213 | Acc: 91.500% | Wgt Acc: 93.894%
I - num batch: 222
I - Train -- Loss: 0.215 | Acc: 91.260% | Wgt Acc: 93.735% | LR: 6.250000e-04 | Dur: 136.63s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   0.   1.   6.  64.]
 [  3. 560.   5.   3.  34.]
 [  1.   4. 705.   1.  70.]
 [  4.   2.   3. 521.  46.]
 [ 24.  12.  20.   7. 786.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.159 | Acc: 64.497% | Wgt Acc: 59.377% | Dur: 14.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   7.  18.   6.]
 [  0.  37.   6.   2.   3.]
 [  1.   9.  35.   2.  11.]
 [  9.   4.   8.  44.   9.]
 [ 18.  24.  19.  20. 151.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 206
I - Training: 
	I - Batch: 50 | Loss: 0.220 | Acc: 90.375% | Wgt Acc: 93.253%
	I - Batch: 100 | Loss: 0.240 | Acc: 90.125% | Wgt Acc: 92.648%
	I - Batch: 150 | Loss: 0.238 | Acc: 90.542% | Wgt Acc: 92.933%
	I - Batch: 200 | Loss: 0.235 | Acc: 90.750% | Wgt Acc: 93.265%
I - num batch: 222
I - Train -- Loss: 0.234 | Acc: 90.640% | Wgt Acc: 93.157% | LR: 6.250000e-04 | Dur: 136.90s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   2.   5.  11.  67.]
 [  2. 560.   3.   4.  32.]
 [  0.   1. 708.   2.  64.]
 [  6.   2.   3. 506.  61.]
 [ 24.  13.  15.  15. 776.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.190 | Acc: 64.694% | Wgt Acc: 62.992% | Dur: 16.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   6.  15.  20.]
 [  0.  39.   4.   1.   9.]
 [  2.   9.  38.   5.  12.]
 [ 12.   5.  12.  55.  11.]
 [  6.  21.  15.  10. 128.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 207
I - Training: 
	I - Batch: 50 | Loss: 0.192 | Acc: 92.375% | Wgt Acc: 94.872%
	I - Batch: 100 | Loss: 0.189 | Acc: 93.000% | Wgt Acc: 95.297%
	I - Batch: 150 | Loss: 0.194 | Acc: 92.792% | Wgt Acc: 95.031%
	I - Batch: 200 | Loss: 0.195 | Acc: 92.625% | Wgt Acc: 94.911%
I - num batch: 222
I - Train -- Loss: 0.200 | Acc: 92.247% | Wgt Acc: 94.692% | LR: 6.250000e-04 | Dur: 136.37s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   2.   0.   2.  66.]
 [  0. 564.   1.   0.  32.]
 [  3.   2. 714.   0.  51.]
 [  0.   0.   1. 524.  54.]
 [ 21.  10.  18.  12. 797.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.209 | Acc: 60.947% | Wgt Acc: 53.923% | Dur: 14.42s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   3.  12.   6.]
 [  0.  33.   3.   1.   4.]
 [  1.  17.  32.   2.   8.]
 [  9.   1.   4.  31.   5.]
 [ 22.  25.  33.  40. 157.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 208
I - Training: 
	I - Batch: 50 | Loss: 0.199 | Acc: 92.125% | Wgt Acc: 94.775%
	I - Batch: 100 | Loss: 0.206 | Acc: 91.500% | Wgt Acc: 94.437%
	I - Batch: 150 | Loss: 0.201 | Acc: 91.833% | Wgt Acc: 94.557%
	I - Batch: 200 | Loss: 0.200 | Acc: 92.094% | Wgt Acc: 94.785%
I - num batch: 222
I - Train -- Loss: 0.201 | Acc: 92.021% | Wgt Acc: 94.768% | LR: 6.250000e-04 | Dur: 134.98s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   0.   2.  69.]
 [  3. 569.   1.   0.  33.]
 [  2.   1. 720.   1.  67.]
 [  3.   0.   2. 523.  52.]
 [ 16.   7.  11.  12. 779.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.175 | Acc: 62.327% | Wgt Acc: 58.103% | Dur: 15.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   1.   4.  10.  10.]
 [  0.  39.   7.   4.   8.]
 [  1.  14.  35.   1.  13.]
 [ 23.   5.   6.  50.   7.]
 [ 14.  19.  23.  21. 142.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 209
I - Training: 
	I - Batch: 50 | Loss: 0.194 | Acc: 92.750% | Wgt Acc: 95.211%
	I - Batch: 100 | Loss: 0.201 | Acc: 91.938% | Wgt Acc: 94.587%
	I - Batch: 150 | Loss: 0.208 | Acc: 91.875% | Wgt Acc: 94.529%
	I - Batch: 200 | Loss: 0.209 | Acc: 91.625% | Wgt Acc: 94.319%
I - num batch: 222
I - Train -- Loss: 0.211 | Acc: 91.599% | Wgt Acc: 94.207% | LR: 6.250000e-04 | Dur: 136.42s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   0.   0.   6.  78.]
 [  4. 565.   3.   0.  35.]
 [  3.   5. 717.   0.  61.]
 [  4.   0.   1. 520.  44.]
 [ 21.   8.  13.  12. 782.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.256 | Acc: 63.116% | Wgt Acc: 58.615% | Dur: 14.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   5.  21.  13.]
 [  0.  35.   8.   2.   8.]
 [  1.  11.  38.   4.  12.]
 [  5.   3.   4.  37.   4.]
 [ 15.  26.  20.  22. 143.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 210
I - Training: 
	I - Batch: 50 | Loss: 0.202 | Acc: 92.250% | Wgt Acc: 95.066%
	I - Batch: 100 | Loss: 0.196 | Acc: 92.500% | Wgt Acc: 95.262%
	I - Batch: 150 | Loss: 0.193 | Acc: 92.458% | Wgt Acc: 95.179%
	I - Batch: 200 | Loss: 0.207 | Acc: 92.000% | Wgt Acc: 94.563%
I - num batch: 222
I - Train -- Loss: 0.215 | Acc: 91.514% | Wgt Acc: 94.261% | LR: 6.250000e-04 | Dur: 135.27s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   2.   3.  73.]
 [  3. 562.   1.   1.  44.]
 [  2.   2. 715.   1.  56.]
 [  3.   1.   0. 523.  53.]
 [ 17.  13.  16.  10. 774.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.247 | Acc: 62.722% | Wgt Acc: 57.873% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   7.   5.  16.  17.]
 [  0.  29.   4.   0.   3.]
 [  1.   9.  33.   4.  10.]
 [ 12.   5.   7.  47.   4.]
 [ 12.  28.  26.  19. 146.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 211
I - Training: 
	I - Batch: 50 | Loss: 0.212 | Acc: 92.500% | Wgt Acc: 94.786%
	I - Batch: 100 | Loss: 0.207 | Acc: 92.312% | Wgt Acc: 94.760%
	I - Batch: 150 | Loss: 0.202 | Acc: 92.292% | Wgt Acc: 94.768%
	I - Batch: 200 | Loss: 0.206 | Acc: 92.125% | Wgt Acc: 94.588%
I - num batch: 222
I - Train -- Loss: 0.202 | Acc: 92.191% | Wgt Acc: 94.689% | LR: 6.250000e-04 | Dur: 135.85s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   2.   8.  63.]
 [  1. 566.   1.   0.  36.]
 [  0.   2. 716.   1.  68.]
 [  4.   0.   1. 519.  40.]
 [ 16.  10.  14.  10. 793.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.086 | Acc: 63.708% | Wgt Acc: 59.283% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   4.   4.  15.   9.]
 [  0.  38.   9.   1.   6.]
 [  1.  12.  38.   3.  15.]
 [ 12.   3.   2.  46.   5.]
 [ 19.  21.  22.  21. 145.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 212
I - Training: 
	I - Batch: 50 | Loss: 0.211 | Acc: 90.875% | Wgt Acc: 93.839%
	I - Batch: 100 | Loss: 0.232 | Acc: 89.875% | Wgt Acc: 93.017%
	I - Batch: 150 | Loss: 0.227 | Acc: 90.167% | Wgt Acc: 93.211%
	I - Batch: 200 | Loss: 0.220 | Acc: 90.750% | Wgt Acc: 93.578%
I - num batch: 222
I - Train -- Loss: 0.220 | Acc: 90.696% | Wgt Acc: 93.480% | LR: 6.250000e-04 | Dur: 140.93s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   2.   1.   7.  70.]
 [  1. 558.   1.   0.  45.]
 [  1.   2. 715.   2.  64.]
 [  6.   1.   1. 513.  58.]
 [ 21.  15.  16.  16. 763.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.174 | Acc: 64.892% | Wgt Acc: 62.136% | Dur: 14.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   5.  16.  13.]
 [  2.  47.   8.   3.  13.]
 [  1.  11.  39.   3.  15.]
 [  8.   1.   3.  44.   4.]
 [ 13.  15.  20.  20. 135.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 213
I - Training: 
	I - Batch: 50 | Loss: 0.214 | Acc: 91.625% | Wgt Acc: 93.801%
	I - Batch: 100 | Loss: 0.208 | Acc: 91.812% | Wgt Acc: 94.255%
	I - Batch: 150 | Loss: 0.210 | Acc: 91.625% | Wgt Acc: 94.219%
	I - Batch: 200 | Loss: 0.214 | Acc: 91.656% | Wgt Acc: 94.171%
I - num batch: 222
I - Train -- Loss: 0.213 | Acc: 91.824% | Wgt Acc: 94.283% | LR: 6.250000e-04 | Dur: 139.73s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   2.   1.  63.]
 [  2. 561.   2.   2.  33.]
 [  0.   1. 710.   3.  58.]
 [  3.   2.   2. 522.  54.]
 [ 20.  14.  18.  10. 792.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.147 | Acc: 65.680% | Wgt Acc: 62.355% | Dur: 14.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   5.   3.  18.  10.]
 [  1.  43.   5.   2.   7.]
 [  2.  15.  41.   6.  16.]
 [  6.   1.   4.  43.   7.]
 [ 13.  14.  22.  17. 140.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 214
I - Training: 
	I - Batch: 50 | Loss: 0.179 | Acc: 93.500% | Wgt Acc: 95.555%
	I - Batch: 100 | Loss: 0.190 | Acc: 93.125% | Wgt Acc: 95.213%
	I - Batch: 150 | Loss: 0.208 | Acc: 91.792% | Wgt Acc: 94.310%
	I - Batch: 200 | Loss: 0.213 | Acc: 91.438% | Wgt Acc: 94.011%
I - num batch: 222
I - Train -- Loss: 0.215 | Acc: 91.486% | Wgt Acc: 94.073% | LR: 6.250000e-04 | Dur: 135.60s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   1.   3.  65.]
 [  2. 559.   2.   0.  37.]
 [  2.   5. 709.   2.  64.]
 [  3.   0.   2. 522.  52.]
 [ 17.  14.  20.  11. 782.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.228 | Acc: 63.511% | Wgt Acc: 58.500% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   5.   7.  21.  12.]
 [  0.  31.   4.   1.   4.]
 [  1.  12.  31.   4.  12.]
 [  4.   4.   7.  43.   4.]
 [ 14.  26.  26.  17. 148.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 215
I - Training: 
	I - Batch: 50 | Loss: 0.179 | Acc: 93.625% | Wgt Acc: 95.779%
	I - Batch: 100 | Loss: 0.176 | Acc: 93.438% | Wgt Acc: 95.680%
	I - Batch: 150 | Loss: 0.169 | Acc: 93.750% | Wgt Acc: 95.984%
	I - Batch: 200 | Loss: 0.167 | Acc: 93.781% | Wgt Acc: 95.926%
I - num batch: 208
I - Train -- Loss: 0.169 | Acc: 93.690% | Wgt Acc: 95.832% | LR: 6.250000e-04 | Dur: 125.70s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   2.   1.   6.  32.]
 [  0. 567.   1.   0.  31.]
 [  3.   1. 722.   2.  47.]
 [  2.   2.   1. 520.  47.]
 [  7.   6.   9.  10. 624.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.252 | Acc: 62.327% | Wgt Acc: 58.123% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   7.  26.  19.]
 [  0.  31.   4.   2.   3.]
 [  1.  16.  34.   1.  13.]
 [ 10.   4.   7.  42.   5.]
 [  8.  23.  23.  15. 140.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 216
I - Training: 
	I - Batch: 50 | Loss: 0.186 | Acc: 92.375% | Wgt Acc: 94.729%
	I - Batch: 100 | Loss: 0.201 | Acc: 92.312% | Wgt Acc: 94.730%
	I - Batch: 150 | Loss: 0.227 | Acc: 90.625% | Wgt Acc: 93.361%
	I - Batch: 200 | Loss: 0.228 | Acc: 90.656% | Wgt Acc: 93.453%
I - num batch: 222
I - Train -- Loss: 0.228 | Acc: 90.668% | Wgt Acc: 93.446% | LR: 6.250000e-04 | Dur: 135.64s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   1.   1.   7.  78.]
 [  1. 559.   3.   1.  46.]
 [  4.   3. 712.   1.  59.]
 [  7.   1.   1. 512.  54.]
 [ 15.  14.  17.  17. 763.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.215 | Acc: 62.525% | Wgt Acc: 59.482% | Dur: 15.85s
I - Confusion Matrix: [row->prediction - col->label]
[[ 50.   4.   1.  11.   4.]
 [  3.  42.   6.  10.  12.]
 [  3.  15.  40.   4.  21.]
 [ 18.   3.   9.  51.   9.]
 [ 14.  14.  19.  10. 134.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 217
I - Training: 
	I - Batch: 50 | Loss: 0.207 | Acc: 91.375% | Wgt Acc: 94.106%
	I - Batch: 100 | Loss: 0.207 | Acc: 91.438% | Wgt Acc: 94.220%
	I - Batch: 150 | Loss: 0.209 | Acc: 91.792% | Wgt Acc: 94.414%
	I - Batch: 200 | Loss: 0.207 | Acc: 92.031% | Wgt Acc: 94.519%
I - num batch: 222
I - Train -- Loss: 0.207 | Acc: 91.993% | Wgt Acc: 94.436% | LR: 6.250000e-04 | Dur: 134.25s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   2.   1.   6.  63.]
 [  0. 554.   4.   0.  47.]
 [  0.   5. 717.   0.  49.]
 [  4.   1.   0. 523.  47.]
 [ 18.  16.  12.   9. 794.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.221 | Acc: 63.116% | Wgt Acc: 60.704% | Dur: 14.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   5.   6.  20.  19.]
 [  1.  44.  11.   3.   9.]
 [  0.   8.  27.   2.  14.]
 [  8.   5.  10.  48.   8.]
 [  8.  16.  21.  13. 130.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 218
I - Training: 
	I - Batch: 50 | Loss: 0.202 | Acc: 92.125% | Wgt Acc: 94.150%
	I - Batch: 100 | Loss: 0.199 | Acc: 92.000% | Wgt Acc: 94.250%
	I - Batch: 150 | Loss: 0.205 | Acc: 92.000% | Wgt Acc: 94.356%
	I - Batch: 200 | Loss: 0.199 | Acc: 92.281% | Wgt Acc: 94.607%
I - num batch: 222
I - Train -- Loss: 0.203 | Acc: 92.162% | Wgt Acc: 94.537% | LR: 6.250000e-04 | Dur: 134.16s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   1.   4.  76.]
 [  0. 565.   3.   2.  26.]
 [  1.   2. 713.   0.  58.]
 [  4.   1.   0. 523.  40.]
 [ 24.   9.  17.   9. 800.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.149 | Acc: 64.300% | Wgt Acc: 60.892% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 53.   1.   3.  13.   6.]
 [  0.  46.  14.   2.  13.]
 [  4.  15.  35.   4.  12.]
 [ 15.   3.   5.  52.   9.]
 [ 16.  13.  18.  15. 140.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 219
I - Training: 
	I - Batch: 50 | Loss: 0.180 | Acc: 93.000% | Wgt Acc: 95.723%
	I - Batch: 100 | Loss: 0.196 | Acc: 92.062% | Wgt Acc: 94.947%
	I - Batch: 150 | Loss: 0.193 | Acc: 92.458% | Wgt Acc: 95.273%
	I - Batch: 200 | Loss: 0.196 | Acc: 92.656% | Wgt Acc: 95.263%
I - num batch: 222
I - Train -- Loss: 0.198 | Acc: 92.501% | Wgt Acc: 95.185% | LR: 6.250000e-04 | Dur: 135.57s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   1.   0.   1.  68.]
 [  1. 567.   0.   0.  36.]
 [  0.   1. 724.   0.  63.]
 [  4.   0.   1. 527.  46.]
 [ 16.   9.   9.  10. 787.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.124 | Acc: 65.286% | Wgt Acc: 59.941% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   3.  14.   5.]
 [  1.  40.   5.   4.   8.]
 [  2.  12.  30.   5.  10.]
 [  5.   1.   7.  43.   3.]
 [ 16.  21.  30.  20. 154.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 220
I - Training: 
	I - Batch: 50 | Loss: 0.194 | Acc: 92.125% | Wgt Acc: 94.585%
	I - Batch: 100 | Loss: 0.198 | Acc: 92.500% | Wgt Acc: 94.966%
	I - Batch: 150 | Loss: 0.210 | Acc: 91.833% | Wgt Acc: 94.590%
	I - Batch: 200 | Loss: 0.211 | Acc: 91.375% | Wgt Acc: 94.152%
I - num batch: 222
I - Train -- Loss: 0.212 | Acc: 91.260% | Wgt Acc: 94.104% | LR: 6.250000e-04 | Dur: 135.87s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   1.   1.   5.  70.]
 [  0. 559.   2.   2.  43.]
 [  3.   2. 720.   1.  61.]
 [  3.   4.   0. 521.  60.]
 [ 20.  12.  11.   9. 766.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.207 | Acc: 62.130% | Wgt Acc: 58.928% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   4.   2.  16.  14.]
 [  1.  45.  10.   4.  14.]
 [  1.   9.  32.   5.  10.]
 [ 17.   1.   6.  46.   8.]
 [ 11.  19.  25.  15. 134.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 221
I - Training: 
	I - Batch: 50 | Loss: 0.174 | Acc: 93.000% | Wgt Acc: 95.264%
	I - Batch: 100 | Loss: 0.186 | Acc: 92.562% | Wgt Acc: 95.001%
	I - Batch: 150 | Loss: 0.200 | Acc: 92.167% | Wgt Acc: 94.620%
	I - Batch: 200 | Loss: 0.210 | Acc: 91.688% | Wgt Acc: 94.128%
I - num batch: 222
I - Train -- Loss: 0.212 | Acc: 91.570% | Wgt Acc: 94.113% | LR: 6.250000e-04 | Dur: 135.30s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   0.   1.   7.  59.]
 [  3. 567.   1.   1.  43.]
 [  2.   2. 709.   1.  71.]
 [  5.   1.   2. 517.  42.]
 [ 17.   8.  21.  12. 785.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 65.089% | Wgt Acc: 61.331% | Dur: 14.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   4.  14.   6.]
 [  0.  44.  10.   3.   9.]
 [  0.   7.  28.   2.  11.]
 [ 14.   7.   7.  54.  10.]
 [ 14.  16.  26.  13. 144.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 222
I - Training: 
	I - Batch: 50 | Loss: 0.199 | Acc: 93.000% | Wgt Acc: 95.346%
	I - Batch: 100 | Loss: 0.212 | Acc: 92.500% | Wgt Acc: 94.854%
	I - Batch: 150 | Loss: 0.213 | Acc: 92.458% | Wgt Acc: 94.844%
	I - Batch: 200 | Loss: 0.219 | Acc: 91.938% | Wgt Acc: 94.419%
I - num batch: 222
I - Train -- Loss: 0.219 | Acc: 91.909% | Wgt Acc: 94.412% | LR: 6.250000e-04 | Dur: 135.05s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   3.   1.   3.  71.]
 [  0. 558.   3.   4.  38.]
 [  2.   2. 719.   2.  56.]
 [  5.   3.   0. 521.  45.]
 [ 18.  12.  11.   8. 790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.175 | Acc: 65.483% | Wgt Acc: 61.227% | Dur: 14.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   1.   4.  18.   9.]
 [  1.  46.   3.   2.   9.]
 [  4.  12.  37.   7.  13.]
 [  4.   1.   5.  38.   3.]
 [ 14.  18.  26.  21. 146.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 223
I - Training: 
	I - Batch: 50 | Loss: 0.227 | Acc: 91.375% | Wgt Acc: 93.874%
	I - Batch: 100 | Loss: 0.220 | Acc: 91.438% | Wgt Acc: 94.100%
	I - Batch: 150 | Loss: 0.222 | Acc: 91.250% | Wgt Acc: 93.896%
	I - Batch: 200 | Loss: 0.216 | Acc: 91.438% | Wgt Acc: 94.026%
I - num batch: 222
I - Train -- Loss: 0.217 | Acc: 91.232% | Wgt Acc: 93.809% | LR: 6.250000e-04 | Dur: 135.84s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   2.   1.   6.  62.]
 [  1. 561.   0.   4.  47.]
 [  2.   5. 713.   0.  65.]
 [  9.   1.   0. 519.  46.]
 [ 22.   9.  20.   9. 780.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.168 | Acc: 64.497% | Wgt Acc: 61.112% | Dur: 13.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 74.   5.   7.  26.  18.]
 [  0.  44.  11.   2.  11.]
 [  0.   4.  32.   0.  12.]
 [  8.   1.   5.  39.   1.]
 [  6.  24.  20.  19. 138.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 224
I - Training: 
	I - Batch: 50 | Loss: 0.193 | Acc: 92.625% | Wgt Acc: 95.128%
	I - Batch: 100 | Loss: 0.212 | Acc: 91.812% | Wgt Acc: 94.290%
	I - Batch: 150 | Loss: 0.210 | Acc: 91.375% | Wgt Acc: 94.055%
	I - Batch: 200 | Loss: 0.211 | Acc: 91.812% | Wgt Acc: 94.340%
I - num batch: 222
I - Train -- Loss: 0.212 | Acc: 91.824% | Wgt Acc: 94.318% | LR: 6.250000e-04 | Dur: 133.48s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   1.   2.   3.  77.]
 [  2. 565.   3.   1.  22.]
 [  0.   1. 714.   4.  66.]
 [  4.   1.   1. 519.  45.]
 [ 22.  10.  14.  11. 790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.201 | Acc: 65.089% | Wgt Acc: 61.624% | Dur: 16.67s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   7.   6.  24.  16.]
 [  0.  39.   7.   2.   5.]
 [  2.  11.  33.   1.  10.]
 [  8.   3.   9.  51.   8.]
 [ 12.  18.  20.   8. 141.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 225
I - Training: 
	I - Batch: 50 | Loss: 0.182 | Acc: 92.125% | Wgt Acc: 94.584%
	I - Batch: 100 | Loss: 0.189 | Acc: 92.375% | Wgt Acc: 94.824%
	I - Batch: 150 | Loss: 0.191 | Acc: 92.458% | Wgt Acc: 94.736%
	I - Batch: 200 | Loss: 0.192 | Acc: 92.375% | Wgt Acc: 94.716%
I - num batch: 222
I - Train -- Loss: 0.190 | Acc: 92.613% | Wgt Acc: 94.911% | LR: 6.250000e-04 | Dur: 132.82s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   1.   1.   6.  54.]
 [  0. 569.   2.   3.  30.]
 [  3.   2. 720.   0.  63.]
 [  2.   0.   0. 514.  46.]
 [ 17.   6.  11.  15. 807.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.174 | Acc: 63.905% | Wgt Acc: 60.537% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   3.  17.  10.]
 [  1.  41.  10.   1.   5.]
 [  2.   8.  30.   5.  12.]
 [ 15.   5.   8.  54.  14.]
 [ 10.  21.  24.   9. 139.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 226
I - Training: 
	I - Batch: 50 | Loss: 0.192 | Acc: 92.625% | Wgt Acc: 95.153%
	I - Batch: 100 | Loss: 0.192 | Acc: 92.500% | Wgt Acc: 94.914%
	I - Batch: 150 | Loss: 0.193 | Acc: 92.500% | Wgt Acc: 94.881%
	I - Batch: 200 | Loss: 0.184 | Acc: 92.969% | Wgt Acc: 95.202%
I - num batch: 222
I - Train -- Loss: 0.185 | Acc: 93.008% | Wgt Acc: 95.277% | LR: 6.250000e-04 | Dur: 135.13s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   0.   4.  55.]
 [  1. 566.   0.   0.  26.]
 [  0.   2. 726.   0.  58.]
 [  4.   0.   0. 522.  48.]
 [ 20.  10.   8.  12. 813.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.160 | Acc: 64.103% | Wgt Acc: 60.015% | Dur: 17.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   4.  23.  13.]
 [  3.  46.   6.   2.   8.]
 [  2.   9.  31.   3.  11.]
 [  6.   2.   4.  41.   5.]
 [ 13.  16.  30.  17. 143.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 227
I - Training: 
	I - Batch: 50 | Loss: 0.201 | Acc: 92.625% | Wgt Acc: 94.740%
	I - Batch: 100 | Loss: 0.198 | Acc: 92.750% | Wgt Acc: 95.037%
	I - Batch: 150 | Loss: 0.202 | Acc: 92.500% | Wgt Acc: 94.831%
	I - Batch: 200 | Loss: 0.206 | Acc: 92.281% | Wgt Acc: 94.558%
I - num batch: 222
I - Train -- Loss: 0.207 | Acc: 92.162% | Wgt Acc: 94.443% | LR: 6.250000e-04 | Dur: 134.06s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   1.   1.   3.  65.]
 [  1. 566.   1.   2.  34.]
 [  2.   1. 712.   0.  57.]
 [  6.   0.   4. 522.  39.]
 [ 24.  10.  16.  11. 805.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.220 | Acc: 62.722% | Wgt Acc: 60.453% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   5.   6.  15.  20.]
 [  1.  43.  10.   4.   6.]
 [  2.  11.  33.   2.  15.]
 [ 17.   5.   8.  53.  10.]
 [  8.  14.  18.  12. 129.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 228
I - Training: 
	I - Batch: 50 | Loss: 0.186 | Acc: 92.750% | Wgt Acc: 95.058%
	I - Batch: 100 | Loss: 0.188 | Acc: 92.875% | Wgt Acc: 95.197%
	I - Batch: 150 | Loss: 0.204 | Acc: 91.917% | Wgt Acc: 94.351%
	I - Batch: 200 | Loss: 0.207 | Acc: 91.688% | Wgt Acc: 94.299%
I - num batch: 222
I - Train -- Loss: 0.205 | Acc: 91.965% | Wgt Acc: 94.501% | LR: 6.250000e-04 | Dur: 134.69s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   2.   1.   3.  72.]
 [  2. 564.   3.   1.  43.]
 [  1.   4. 721.   1.  54.]
 [  3.   0.   0. 520.  42.]
 [ 23.   8.   9.  13. 789.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.207 | Acc: 62.919% | Wgt Acc: 58.479% | Dur: 16.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   4.  20.  10.]
 [  0.  38.   6.   2.   7.]
 [  3.  11.  33.   2.  14.]
 [ 14.   5.   7.  47.   5.]
 [ 14.  22.  25.  15. 144.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 229
I - Training: 
	I - Batch: 50 | Loss: 0.191 | Acc: 93.125% | Wgt Acc: 95.791%
	I - Batch: 100 | Loss: 0.187 | Acc: 93.375% | Wgt Acc: 95.916%
	I - Batch: 150 | Loss: 0.195 | Acc: 92.708% | Wgt Acc: 95.354%
	I - Batch: 200 | Loss: 0.200 | Acc: 92.406% | Wgt Acc: 95.119%
I - num batch: 222
I - Train -- Loss: 0.201 | Acc: 92.388% | Wgt Acc: 95.112% | LR: 6.250000e-04 | Dur: 134.16s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   0.   3.   3.  57.]
 [  1. 565.   0.   1.  44.]
 [  1.   1. 725.   0.  56.]
 [  3.   2.   0. 529.  59.]
 [ 18.  10.   6.   5. 784.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.194 | Acc: 64.694% | Wgt Acc: 60.077% | Dur: 13.94s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   6.   6.  20.  14.]
 [  0.  42.  12.   2.   8.]
 [  1.  10.  23.   1.   7.]
 [  7.   3.   6.  44.   3.]
 [  9.  17.  28.  19. 148.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 230
I - Training: 
	I - Batch: 50 | Loss: 0.171 | Acc: 93.000% | Wgt Acc: 95.588%
	I - Batch: 100 | Loss: 0.183 | Acc: 92.812% | Wgt Acc: 95.412%
	I - Batch: 150 | Loss: 0.191 | Acc: 92.542% | Wgt Acc: 95.159%
	I - Batch: 200 | Loss: 0.197 | Acc: 92.406% | Wgt Acc: 94.911%
I - num batch: 222
I - Train -- Loss: 0.200 | Acc: 92.388% | Wgt Acc: 94.847% | LR: 6.250000e-04 | Dur: 133.16s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   2.   1.   6.  77.]
 [  3. 568.   1.   2.  35.]
 [  0.   0. 721.   1.  53.]
 [  4.   1.   0. 519.  38.]
 [ 18.   7.  11.  10. 797.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.285 | Acc: 61.736% | Wgt Acc: 57.434% | Dur: 13.77s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   4.  25.  11.]
 [  0.  37.   3.   1.   6.]
 [  1.  12.  31.   2.  14.]
 [ 16.   6.  10.  47.   8.]
 [ 14.  20.  27.  11. 141.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 231
I - Training: 
	I - Batch: 50 | Loss: 0.169 | Acc: 93.375% | Wgt Acc: 95.459%
	I - Batch: 100 | Loss: 0.191 | Acc: 92.312% | Wgt Acc: 94.673%
	I - Batch: 150 | Loss: 0.206 | Acc: 91.625% | Wgt Acc: 94.164%
	I - Batch: 200 | Loss: 0.207 | Acc: 91.688% | Wgt Acc: 94.219%
I - num batch: 222
I - Train -- Loss: 0.212 | Acc: 91.345% | Wgt Acc: 93.861% | LR: 6.250000e-04 | Dur: 133.83s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   1.   1.  10.  73.]
 [  1. 565.   0.   0.  38.]
 [  2.   2. 714.   1.  57.]
 [ 10.   1.   0. 515.  48.]
 [ 22.   9.  19.  12. 784.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.275 | Acc: 63.905% | Wgt Acc: 60.213% | Dur: 14.94s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   5.   8.  20.  19.]
 [  0.  34.   8.   0.   5.]
 [  0.   6.  25.   0.   7.]
 [  8.   5.   7.  55.   8.]
 [ 11.  28.  27.  11. 141.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 232
I - Training: 
	I - Batch: 50 | Loss: 0.202 | Acc: 92.750% | Wgt Acc: 95.002%
	I - Batch: 100 | Loss: 0.203 | Acc: 92.875% | Wgt Acc: 95.040%
	I - Batch: 150 | Loss: 0.207 | Acc: 92.333% | Wgt Acc: 94.676%
	I - Batch: 200 | Loss: 0.213 | Acc: 91.594% | Wgt Acc: 94.129%
I - num batch: 222
I - Train -- Loss: 0.209 | Acc: 91.796% | Wgt Acc: 94.349% | LR: 6.250000e-04 | Dur: 136.89s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   1.   5.   5.  64.]
 [  3. 567.   1.   1.  33.]
 [  0.   1. 708.   1.  64.]
 [  3.   0.   0. 521.  52.]
 [ 18.   9.  20.  10. 787.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.224 | Acc: 62.722% | Wgt Acc: 57.591% | Dur: 17.62s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   7.  18.  10.]
 [  0.  28.   0.   1.   3.]
 [  1.  13.  34.   2.  12.]
 [ 12.   2.   8.  47.   7.]
 [ 14.  33.  26.  18. 148.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 233
I - Training: 
	I - Batch: 50 | Loss: 0.188 | Acc: 92.625% | Wgt Acc: 94.765%
	I - Batch: 100 | Loss: 0.180 | Acc: 93.500% | Wgt Acc: 95.610%
	I - Batch: 150 | Loss: 0.184 | Acc: 92.833% | Wgt Acc: 95.014%
	I - Batch: 200 | Loss: 0.192 | Acc: 92.375% | Wgt Acc: 94.711%
I - num batch: 222
I - Train -- Loss: 0.196 | Acc: 92.219% | Wgt Acc: 94.580% | LR: 6.250000e-04 | Dur: 134.89s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   1.   2.   1.  78.]
 [  5. 566.   0.   2.  27.]
 [  4.   1. 721.   0.  47.]
 [  4.   1.   2. 521.  47.]
 [ 22.   9.   9.  14. 801.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.236 | Acc: 63.116% | Wgt Acc: 59.680% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   8.   7.  22.  18.]
 [  4.  39.   7.   3.   6.]
 [  0.  11.  29.   2.  11.]
 [  9.   2.   8.  47.   8.]
 [  7.  18.  24.  12. 137.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 234
I - Training: 
	I - Batch: 50 | Loss: 0.182 | Acc: 92.750% | Wgt Acc: 95.678%
	I - Batch: 100 | Loss: 0.194 | Acc: 92.250% | Wgt Acc: 94.766%
	I - Batch: 150 | Loss: 0.199 | Acc: 92.000% | Wgt Acc: 94.481%
	I - Batch: 200 | Loss: 0.198 | Acc: 92.312% | Wgt Acc: 94.776%
I - num batch: 222
I - Train -- Loss: 0.198 | Acc: 92.501% | Wgt Acc: 94.887% | LR: 6.250000e-04 | Dur: 136.91s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   1.   1.   2.  53.]
 [  2. 564.   3.   0.  44.]
 [  1.   4. 717.   1.  53.]
 [  5.   0.   0. 528.  47.]
 [ 20.   9.  13.   7. 803.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.155 | Acc: 63.905% | Wgt Acc: 59.774% | Dur: 14.89s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   4.   3.  16.  14.]
 [  0.  43.  10.   2.   8.]
 [  3.   9.  29.   3.   8.]
 [ 11.   1.   6.  49.   6.]
 [ 15.  21.  27.  16. 144.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 235
I - Training: 
	I - Batch: 50 | Loss: 0.197 | Acc: 92.000% | Wgt Acc: 94.422%
	I - Batch: 100 | Loss: 0.201 | Acc: 92.062% | Wgt Acc: 94.365%
	I - Batch: 150 | Loss: 0.203 | Acc: 91.667% | Wgt Acc: 94.193%
	I - Batch: 200 | Loss: 0.209 | Acc: 91.531% | Wgt Acc: 94.138%
I - num batch: 222
I - Train -- Loss: 0.211 | Acc: 91.429% | Wgt Acc: 94.135% | LR: 6.250000e-04 | Dur: 136.17s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   3.   6.  64.]
 [  0. 564.   2.   1.  34.]
 [  0.   1. 713.   1.  77.]
 [  3.   2.   2. 519.  50.]
 [ 22.  11.  14.  11. 775.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.116 | Acc: 65.089% | Wgt Acc: 60.318% | Dur: 16.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   3.  12.   5.]
 [  1.  41.   4.   2.   4.]
 [  3.  13.  38.   4.  15.]
 [ 12.   2.   4.  46.   6.]
 [ 17.  20.  26.  22. 150.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 236
I - Training: 
	I - Batch: 50 | Loss: 0.173 | Acc: 93.375% | Wgt Acc: 95.454%
	I - Batch: 100 | Loss: 0.194 | Acc: 92.062% | Wgt Acc: 94.488%
	I - Batch: 150 | Loss: 0.203 | Acc: 91.708% | Wgt Acc: 94.289%
	I - Batch: 200 | Loss: 0.204 | Acc: 91.969% | Wgt Acc: 94.395%
I - num batch: 222
I - Train -- Loss: 0.202 | Acc: 92.191% | Wgt Acc: 94.484% | LR: 6.250000e-04 | Dur: 135.32s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   2.   1.   7.  53.]
 [  3. 563.   1.   0.  33.]
 [  2.   2. 721.   1.  57.]
 [  5.   0.   0. 520.  53.]
 [ 25.  11.  11.  10. 804.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.219 | Acc: 61.933% | Wgt Acc: 57.162% | Dur: 15.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   3.   5.  21.  15.]
 [  0.  29.   2.   1.   5.]
 [  1.  15.  30.   3.  12.]
 [  7.   5.   7.  46.   4.]
 [ 15.  26.  31.  15. 144.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 237
I - Training: 
	I - Batch: 50 | Loss: 0.178 | Acc: 92.750% | Wgt Acc: 95.252%
	I - Batch: 100 | Loss: 0.192 | Acc: 92.125% | Wgt Acc: 94.755%
	I - Batch: 150 | Loss: 0.206 | Acc: 91.583% | Wgt Acc: 94.317%
	I - Batch: 200 | Loss: 0.207 | Acc: 91.812% | Wgt Acc: 94.454%
I - num batch: 222
I - Train -- Loss: 0.212 | Acc: 91.486% | Wgt Acc: 94.316% | LR: 6.250000e-04 | Dur: 136.71s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   2.   1.   3.  78.]
 [  2. 557.   3.   0.  47.]
 [  1.   6. 720.   0.  54.]
 [  4.   1.   0. 524.  52.]
 [ 15.  12.  10.  11. 769.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.265 | Acc: 63.116% | Wgt Acc: 57.570% | Dur: 14.54s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   7.  14.   6.]
 [  0.  37.   6.   1.   5.]
 [  1.   3.  25.   1.   8.]
 [ 13.   4.   5.  51.   8.]
 [ 20.  32.  32.  19. 153.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 238
I - Training: 
	I - Batch: 50 | Loss: 0.243 | Acc: 90.125% | Wgt Acc: 92.770%
	I - Batch: 100 | Loss: 0.231 | Acc: 90.375% | Wgt Acc: 93.136%
	I - Batch: 150 | Loss: 0.217 | Acc: 91.292% | Wgt Acc: 93.911%
	I - Batch: 200 | Loss: 0.212 | Acc: 91.156% | Wgt Acc: 93.887%
I - num batch: 222
I - Train -- Loss: 0.210 | Acc: 91.317% | Wgt Acc: 94.004% | LR: 6.250000e-04 | Dur: 139.01s
I - Confusion Matrix: [row->prediction - col->label]
[[662.   0.   2.   5.  79.]
 [  1. 570.   0.   1.  35.]
 [  2.   1. 716.   2.  63.]
 [  9.   0.   0. 516.  48.]
 [ 23.   7.  16.  14. 775.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.288 | Acc: 61.933% | Wgt Acc: 57.100% | Dur: 14.44s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   4.   3.  17.  16.]
 [  0.  30.   4.   0.   4.]
 [  2.  13.  31.   3.   9.]
 [  9.   4.   8.  49.   6.]
 [ 18.  27.  29.  17. 145.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 239
I - Training: 
	I - Batch: 50 | Loss: 0.167 | Acc: 93.375% | Wgt Acc: 95.496%
	I - Batch: 100 | Loss: 0.193 | Acc: 92.625% | Wgt Acc: 94.868%
	I - Batch: 150 | Loss: 0.199 | Acc: 92.333% | Wgt Acc: 94.697%
	I - Batch: 200 | Loss: 0.205 | Acc: 92.094% | Wgt Acc: 94.500%
I - num batch: 222
I - Train -- Loss: 0.205 | Acc: 92.106% | Wgt Acc: 94.497% | LR: 6.250000e-04 | Dur: 132.51s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   1.   0.   4.  66.]
 [  1. 563.   1.   1.  34.]
 [  2.   2. 720.   0.  59.]
 [  2.   2.   2. 520.  43.]
 [ 26.  10.  11.  13. 798.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.220 | Acc: 63.116% | Wgt Acc: 57.988% | Dur: 13.80s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   5.  14.  10.]
 [  0.  35.   4.   1.   4.]
 [  1.  18.  30.   2.  11.]
 [ 11.   4.   6.  46.   6.]
 [ 16.  17.  30.  23. 149.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 240
I - Training: 
	I - Batch: 50 | Loss: 0.210 | Acc: 91.250% | Wgt Acc: 94.292%
	I - Batch: 100 | Loss: 0.184 | Acc: 92.500% | Wgt Acc: 95.208%
	I - Batch: 150 | Loss: 0.182 | Acc: 92.583% | Wgt Acc: 95.104%
	I - Batch: 200 | Loss: 0.194 | Acc: 92.219% | Wgt Acc: 94.812%
I - num batch: 222
I - Train -- Loss: 0.195 | Acc: 92.191% | Wgt Acc: 94.703% | LR: 6.250000e-04 | Dur: 135.75s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   1.   0.   5.  74.]
 [  0. 566.   1.   2.  31.]
 [  2.   1. 715.   1.  53.]
 [  2.   0.   1. 524.  49.]
 [ 21.  10.  17.   6. 793.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.145 | Acc: 62.327% | Wgt Acc: 58.970% | Dur: 15.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   6.  17.  13.]
 [  0.  34.  10.   1.   8.]
 [  2.  15.  29.   0.  13.]
 [ 13.   8.   9.  55.  10.]
 [ 11.  18.  21.  13. 136.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 241
I - Training: 
	I - Batch: 50 | Loss: 0.211 | Acc: 91.875% | Wgt Acc: 94.701%
	I - Batch: 100 | Loss: 0.194 | Acc: 92.625% | Wgt Acc: 95.071%
	I - Batch: 150 | Loss: 0.197 | Acc: 92.458% | Wgt Acc: 94.901%
	I - Batch: 200 | Loss: 0.201 | Acc: 92.250% | Wgt Acc: 94.737%
I - num batch: 222
I - Train -- Loss: 0.200 | Acc: 92.078% | Wgt Acc: 94.625% | LR: 6.250000e-04 | Dur: 136.58s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   0.   1.   3.  63.]
 [  0. 568.   2.   1.  34.]
 [  0.   2. 713.   1.  65.]
 [  3.   0.   2. 522.  48.]
 [ 21.   8.  16.  11. 790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.174 | Acc: 62.327% | Wgt Acc: 59.377% | Dur: 17.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   5.  17.  15.]
 [  0.  42.  10.   2.  13.]
 [  2.  10.  30.   0.   9.]
 [ 14.   6.   9.  51.  10.]
 [ 12.  18.  21.  16. 133.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 242
I - Training: 
	I - Batch: 50 | Loss: 0.166 | Acc: 93.750% | Wgt Acc: 95.714%
	I - Batch: 100 | Loss: 0.173 | Acc: 93.375% | Wgt Acc: 95.565%
	I - Batch: 150 | Loss: 0.173 | Acc: 93.292% | Wgt Acc: 95.724%
	I - Batch: 200 | Loss: 0.172 | Acc: 93.094% | Wgt Acc: 95.561%
I - num batch: 208
I - Train -- Loss: 0.173 | Acc: 92.969% | Wgt Acc: 95.479% | LR: 6.250000e-04 | Dur: 128.43s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   0.   7.  43.]
 [  1. 571.   1.   2.  42.]
 [  1.   0. 724.   2.  49.]
 [  4.   0.   3. 518.  47.]
 [ 10.   7.   6.   9. 600.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.370 | Acc: 61.341% | Wgt Acc: 55.491% | Dur: 14.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.  10.   7.  32.  17.]
 [  0.  29.   2.   1.   5.]
 [  1.   7.  32.   2.   3.]
 [  6.   3.   5.  33.   6.]
 [ 13.  29.  29.  18. 149.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 243
I - Training: 
	I - Batch: 50 | Loss: 0.188 | Acc: 92.125% | Wgt Acc: 94.664%
	I - Batch: 100 | Loss: 0.185 | Acc: 92.625% | Wgt Acc: 95.176%
	I - Batch: 150 | Loss: 0.191 | Acc: 92.208% | Wgt Acc: 94.883%
	I - Batch: 200 | Loss: 0.202 | Acc: 91.750% | Wgt Acc: 94.351%
I - num batch: 222
I - Train -- Loss: 0.206 | Acc: 91.683% | Wgt Acc: 94.276% | LR: 6.250000e-04 | Dur: 136.58s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   2.   3.   8.  62.]
 [  3. 562.   1.   0.  39.]
 [  1.   1. 717.   1.  66.]
 [  4.   0.   2. 518.  50.]
 [ 17.  13.  11.  11. 783.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.181 | Acc: 62.327% | Wgt Acc: 59.816% | Dur: 16.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   5.   6.  20.  16.]
 [  2.  38.   6.   2.   7.]
 [  2.  16.  41.   3.  16.]
 [ 13.   4.  10.  48.  12.]
 [ 11.  15.  12.  13. 129.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 244
I - Training: 
	I - Batch: 50 | Loss: 0.220 | Acc: 90.625% | Wgt Acc: 93.256%
	I - Batch: 100 | Loss: 0.208 | Acc: 91.500% | Wgt Acc: 94.078%
	I - Batch: 150 | Loss: 0.212 | Acc: 91.708% | Wgt Acc: 94.387%
	I - Batch: 200 | Loss: 0.211 | Acc: 92.000% | Wgt Acc: 94.411%
I - num batch: 222
I - Train -- Loss: 0.210 | Acc: 91.965% | Wgt Acc: 94.413% | LR: 6.250000e-04 | Dur: 136.48s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   3.   3.   7.  61.]
 [  0. 567.   1.   3.  37.]
 [  1.   1. 713.   0.  63.]
 [  6.   1.   1. 520.  45.]
 [ 22.   6.  16.   8. 794.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.088 | Acc: 65.483% | Wgt Acc: 61.143% | Dur: 15.50s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   4.  14.   9.]
 [  0.  42.   8.   2.   6.]
 [  1.  12.  36.   5.  12.]
 [ 15.   2.   5.  49.   5.]
 [ 15.  19.  22.  16. 148.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 245
I - Training: 
	I - Batch: 50 | Loss: 0.198 | Acc: 92.125% | Wgt Acc: 95.068%
	I - Batch: 100 | Loss: 0.188 | Acc: 92.188% | Wgt Acc: 94.849%
	I - Batch: 150 | Loss: 0.187 | Acc: 92.292% | Wgt Acc: 94.912%
	I - Batch: 200 | Loss: 0.197 | Acc: 92.250% | Wgt Acc: 94.656%
I - num batch: 222
I - Train -- Loss: 0.205 | Acc: 92.021% | Wgt Acc: 94.463% | LR: 6.250000e-04 | Dur: 136.41s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   1.   3.  74.]
 [  3. 561.   3.   1.  28.]
 [  1.   3. 716.   0.  58.]
 [  5.   0.   2. 524.  45.]
 [ 20.  13.  12.  10. 795.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.208 | Acc: 64.497% | Wgt Acc: 59.941% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   7.   6.  24.  17.]
 [  0.  35.   4.   2.   2.]
 [  0.   9.  29.   3.   5.]
 [  8.   5.  10.  46.   9.]
 [ 10.  22.  26.  11. 147.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 246
I - Training: 
	I - Batch: 50 | Loss: 0.243 | Acc: 90.625% | Wgt Acc: 92.932%
	I - Batch: 100 | Loss: 0.228 | Acc: 91.250% | Wgt Acc: 93.631%
	I - Batch: 150 | Loss: 0.210 | Acc: 91.917% | Wgt Acc: 94.149%
	I - Batch: 200 | Loss: 0.207 | Acc: 91.750% | Wgt Acc: 94.225%
I - num batch: 222
I - Train -- Loss: 0.209 | Acc: 91.880% | Wgt Acc: 94.295% | LR: 6.250000e-04 | Dur: 133.67s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   1.   0.   3.  68.]
 [  2. 566.   3.   1.  36.]
 [  2.   4. 712.   4.  56.]
 [  3.   0.   2. 520.  45.]
 [ 24.   7.  17.  10. 795.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.224 | Acc: 61.341% | Wgt Acc: 59.649% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[ 54.   2.   3.  10.  12.]
 [  0.  39.  10.   2.  12.]
 [  2.  22.  37.   5.  21.]
 [ 21.   4.   6.  58.  12.]
 [ 11.  11.  19.  11. 123.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 247
I - Training: 
	I - Batch: 50 | Loss: 0.206 | Acc: 92.375% | Wgt Acc: 95.120%
	I - Batch: 100 | Loss: 0.198 | Acc: 92.562% | Wgt Acc: 95.185%
	I - Batch: 150 | Loss: 0.191 | Acc: 92.667% | Wgt Acc: 95.298%
	I - Batch: 200 | Loss: 0.195 | Acc: 92.344% | Wgt Acc: 94.924%
I - num batch: 222
I - Train -- Loss: 0.199 | Acc: 92.219% | Wgt Acc: 94.796% | LR: 6.250000e-04 | Dur: 135.07s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   2.   2.   1.  63.]
 [  1. 565.   2.   3.  39.]
 [  0.   1. 715.   1.  63.]
 [  2.   1.   1. 526.  45.]
 [ 19.   9.  14.   7. 790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.228 | Acc: 60.355% | Wgt Acc: 55.835% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 49.   1.   3.  12.   5.]
 [  2.  40.   6.   5.   6.]
 [  3.  14.  34.   9.  23.]
 [ 17.   4.   6.  43.   6.]
 [ 17.  19.  26.  17. 140.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 248
I - Training: 
	I - Batch: 50 | Loss: 0.206 | Acc: 91.875% | Wgt Acc: 94.515%
	I - Batch: 100 | Loss: 0.203 | Acc: 92.375% | Wgt Acc: 94.913%
	I - Batch: 150 | Loss: 0.198 | Acc: 92.542% | Wgt Acc: 95.125%
	I - Batch: 200 | Loss: 0.202 | Acc: 91.969% | Wgt Acc: 94.606%
I - num batch: 222
I - Train -- Loss: 0.201 | Acc: 92.021% | Wgt Acc: 94.645% | LR: 6.250000e-04 | Dur: 137.62s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   0.   1.   4.  75.]
 [  1. 565.   2.   1.  35.]
 [  1.   0. 717.   1.  68.]
 [  7.   1.   0. 521.  37.]
 [ 12.  12.  14.  11. 785.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.167 | Acc: 63.314% | Wgt Acc: 58.876% | Dur: 15.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   2.   3.  16.  11.]
 [  1.  39.   7.   2.   7.]
 [  2.   6.  27.   2.   8.]
 [  8.   5.   9.  49.   9.]
 [ 16.  26.  29.  17. 145.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 249
I - Training: 
	I - Batch: 50 | Loss: 0.187 | Acc: 92.875% | Wgt Acc: 94.859%
	I - Batch: 100 | Loss: 0.176 | Acc: 93.000% | Wgt Acc: 95.213%
	I - Batch: 150 | Loss: 0.179 | Acc: 92.667% | Wgt Acc: 94.988%
	I - Batch: 200 | Loss: 0.188 | Acc: 92.250% | Wgt Acc: 94.752%
I - num batch: 222
I - Train -- Loss: 0.195 | Acc: 91.993% | Wgt Acc: 94.471% | LR: 6.250000e-04 | Dur: 135.58s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   0.   0.   5.  66.]
 [  1. 561.   1.   3.  39.]
 [  1.   4. 719.   0.  52.]
 [  2.   0.   0. 519.  51.]
 [ 21.  13.  14.  11. 792.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.244 | Acc: 63.905% | Wgt Acc: 60.265% | Dur: 14.46s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   8.   5.  18.  17.]
 [  0.  34.   5.   1.   7.]
 [  1.   6.  31.   1.   9.]
 [  6.   3.   8.  51.   7.]
 [ 13.  27.  26.  15. 140.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Maximum validation set accuracy in current training:  66.67
