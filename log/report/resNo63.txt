Fri Oct 28 18:47:00 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.23, 0.24, 0.23, 0.25, 0.05], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'singleBackgroundPath': 'new_background', 'singleBackgroundPickle': True, 'tossFirstLastFrames': True}, 'dataPath': '/data_ssd/processed/kinetics400/', 'dropoutRate': 0.6, 'epochNo': 250, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat', 'background'], 'lastLayerInitUniform': False, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 70.80867850098619, 'maxValidationTrainNo': 61, 'modelVersion': 18, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 63, 'validationAccThr': 70, 'warmStartConfig': {'checkpointFile': './sav/model17_trainNo60_at_epoch_197_with_acc_71_60_checkpoint.pth.tar', 'checkpointModelNo': 17, 'freezeSpatialCNN': False, 'warmStartFlag': True}, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Train set length:  3547
I - Label distribution: [ 697.  578.  734.  538. 1000.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test
I - Loading file: dataset_test_background00_no_samples180.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/test/new_background
I - New label distribution: [ 88.  78.  75.  86. 180.]

I - Test set length:  507
I - Label distribution: [ 88.  78.  75.  86. 180.]
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(4) data number in dataset:  tensor([  1071, 178652,    460, 212968, 183602, 209370, 178511, 186564, 171577,
           979, 130293,    225,   8260, 159761,     63, 199741])
I - Initializing model TCNv18
I - Number of Model Parameters: 659237
I - Warm start initiated
I - Initializing model TCNv17
I - Warm Start: Missing Keys ['tcn0.output_shift', 'tcn0.weight_bits', 'tcn0.bias_bits', 'tcn0.quantize_activation', 'tcn0.adjust_output_shift', 'tcn0.shift_quantile', 'tcn0.op.weight', 'tcn0.op.bias']
I - Warm Start: Unexpected Keys ['fc.output_shift', 'fc.weight_bits', 'fc.bias_bits', 'fc.quantize_activation', 'fc.adjust_output_shift', 'fc.shift_quantile', 'fc.op.weight']
I - Model output shape:  torch.Size([16, 5])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv18                                   [16, 5]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 128, 64, 64]         6
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Conv2d: 2-2                       --                        6,272
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─OutputShiftSqueeze: 2-4           --                        --
│    └─One: 2-5                          [1]                       --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─OutputScale: 2-7                  --                        --
│    └─Empty: 2-8                        [128, 48, 1, 1]           --
│    └─Empty: 2-9                        [128, 48, 1, 1]           --
│    └─Empty: 2-10                       [128]                     --
│    └─Empty: 2-11                       [128]                     --
│    └─BatchNorm2d: 2-12                 [16, 128, 64, 64]         --
│    └─Scaler: 2-13                      [16, 128, 64, 64]         --
│    └─ReLU: 2-14                        [16, 128, 64, 64]         --
│    └─Empty: 2-15                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Conv2d: 2-18                      --                        147,584
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Clamp: 2-20                       [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-3          [16, 128, 32, 32]         147,590
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─MaxPool2d: 2-22                   [16, 128, 32, 32]         --
│    └─Empty: 2-23                       [16, 128, 32, 32]         --
│    └─Empty: 2-24                       [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [128, 128, 3, 3]          --
│    └─Empty: 2-29                       [128, 128, 3, 3]          --
│    └─Empty: 2-30                       [128]                     --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-33                      --                        147,584
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-35                       [128]                     --
│    └─BatchNorm2d: 2-36                 [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-38                      [16, 128, 32, 32]         --
│    └─ReLU: 2-39                        [16, 128, 32, 32]         --
│    └─Empty: 2-40                       [16, 128, 32, 32]         --
│    └─Clamp: 2-41                       [16, 128, 32, 32]         --
├─Dropout2d: 1-5                         [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-6          [16, 128, 16, 16]         131,078
│    └─MaxPool2d: 2-42                   [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─Empty: 2-45                       [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Conv2d: 2-47                      --                        16,512
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-49                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-50          --                        --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-52                         [1]                       --
│    └─OutputScale: 2-53                 --                        --
│    └─Empty: 2-54                       [128, 128, 3, 3]          --
│    └─Empty: 2-55                       [128, 128, 3, 3]          --
│    └─Empty: 2-56                       [128]                     --
│    └─Empty: 2-57                       [128]                     --
│    └─BatchNorm2d: 2-58                 [16, 128, 16, 16]         --
│    └─Scaler: 2-59                      [16, 128, 16, 16]         --
│    └─ReLU: 2-60                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Conv2d: 2-63                      --                        147,584
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-65                       [16, 128, 16, 16]         --
│    └─Clamp: 2-66                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-8                 [16, 128, 16, 16]         16,518
│    └─OutputShiftSqueeze: 2-68          --                        --
│    └─One: 2-69                         [1]                       --
│    └─OutputScale: 2-70                 --                        --
│    └─Empty: 2-71                       [128, 128, 1, 1]          --
│    └─Empty: 2-72                       [128, 128, 1, 1]          --
│    └─Empty: 2-73                       [128]                     --
│    └─Empty: 2-74                       [128]                     --
│    └─BatchNorm2d: 2-75                 [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-78                      --                        147,584
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-80                      [16, 128, 16, 16]         --
│    └─ReLU: 2-81                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Empty: 2-83                       [16, 128, 16, 16]         --
│    └─Clamp: 2-84                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-10         [16, 128, 16, 16]         145,526
│    └─MaxPool2d: 2-85                   [16, 128, 16, 16]         --
│    └─Empty: 2-86                       [16, 128, 16, 16]         --
│    └─Empty: 2-87                       [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-88          --                        --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─One: 2-91                         [1]                       --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Conv2d: 2-93                      --                        2,064
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─OutputScale: 2-95                 --                        --
│    └─Empty: 2-96                       [128, 128, 3, 3]          --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-98                       [128, 128, 3, 3]          --
│    └─Empty: 2-99                       [128]                     --
│    └─Empty: 2-100                      [128]                     --
│    └─BatchNorm2d: 2-101                [16, 128, 16, 16]         --
│    └─Scaler: 2-102                     [16, 128, 16, 16]         --
│    └─ReLU: 2-103                       [16, 128, 16, 16]         --
│    └─Empty: 2-104                      [16, 128, 16, 16]         --
│    └─Clamp: 2-105                      [16, 128, 16, 16]         --
├─Dropout2d: 1-11                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Conv2d: 2-108                     --                        18,448
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-13         [16, 128, 8, 8]           147,590
│    └─MaxPool2d: 2-110                  [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-112                      [16, 128, 8, 8]           --
│    └─Empty: 2-113                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-114         --                        --
│    └─One: 2-115                        [1]                       --
│    └─OutputScale: 2-116                --                        --
│    └─Empty: 2-117                      [128, 128, 3, 3]          --
│    └─Empty: 2-118                      [128, 128, 3, 3]          --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
├─FusedMaxPoolConv2dBNReLU: 1            --                        --
│    └─Empty: 2-121                      [128]                     --
│    └─Empty: 2-122                      [128]                     --
│    └─BatchNorm2d: 2-123                [16, 128, 8, 8]           --
│    └─Scaler: 2-124                     [16, 128, 8, 8]           --
│    └─ReLU: 2-125                       [16, 128, 8, 8]           --
│    └─Empty: 2-126                      [16, 128, 8, 8]           --
│    └─Clamp: 2-127                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-14                [16, 16, 8, 8]            2,070
├─Conv1d: 1                              --                        --
│    └─Scaler: 2-1828                    [16, 5, 16]               --
├─FusedConv2dBNReLU: 1                   --                        --
│    └─OutputShiftSqueeze: 2-129         --                        --
│    └─One: 2-130                        [1]                       --
│    └─OutputScale: 2-131                --                        --
│    └─Empty: 2-132                      [16, 128, 1, 1]           --
│    └─Empty: 2-133                      [16, 128, 1, 1]           --
│    └─Empty: 2-134                      [16]                      --
│    └─Empty: 2-135                      [16]                      --
│    └─BatchNorm2d: 2-136                [16, 16, 8, 8]            --
│    └─Scaler: 2-137                     [16, 16, 8, 8]            --
│    └─ReLU: 2-138                       [16, 16, 8, 8]            --
│    └─Empty: 2-139                      [16, 16, 8, 8]            --
│    └─Clamp: 2-140                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 16, 8, 8]            18,454
│    └─MaxPool2d: 2-141                  [16, 128, 8, 8]           --
│    └─Empty: 2-142                      [16, 128, 8, 8]           --
│    └─Empty: 2-143                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-144         --                        --
│    └─One: 2-145                        [1]                       --
│    └─OutputScale: 2-146                --                        --
│    └─Empty: 2-147                      [16, 128, 3, 3]           --
│    └─Empty: 2-148                      [16, 128, 3, 3]           --
│    └─Empty: 2-149                      [16]                      --
│    └─Empty: 2-150                      [16]                      --
│    └─BatchNorm2d: 2-151                [16, 16, 8, 8]            --
│    └─Scaler: 2-152                     [16, 16, 8, 8]            --
│    └─ReLU: 2-153                       [16, 16, 8, 8]            --
│    └─Empty: 2-154                      [16, 16, 8, 8]            --
│    └─Clamp: 2-155                      [16, 16, 8, 8]            --
├─Dropout2d: 1-16                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-17                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-156         --                        --
│    └─One: 2-157                        [1]                       --
│    └─OutputScale: 2-158                --                        --
│    └─Empty: 2-159                      [128, 48, 1, 1]           --
│    └─Empty: 2-160                      [128, 48, 1, 1]           --
│    └─Empty: 2-161                      [128]                     --
│    └─Empty: 2-162                      [128]                     --
│    └─BatchNorm2d: 2-163                [16, 128, 64, 64]         --
│    └─Scaler: 2-164                     [16, 128, 64, 64]         --
│    └─ReLU: 2-165                       [16, 128, 64, 64]         --
│    └─Empty: 2-166                      [16, 128, 64, 64]         --
│    └─Clamp: 2-167                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-18         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-168                  [16, 128, 32, 32]         --
│    └─Empty: 2-169                      [16, 128, 32, 32]         --
│    └─Empty: 2-170                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-171         --                        --
│    └─One: 2-172                        [1]                       --
│    └─OutputScale: 2-173                --                        --
│    └─Empty: 2-174                      [128, 128, 3, 3]          --
│    └─Empty: 2-175                      [128, 128, 3, 3]          --
│    └─Empty: 2-176                      [128]                     --
│    └─Empty: 2-177                      [128]                     --
│    └─BatchNorm2d: 2-178                [16, 128, 32, 32]         --
│    └─Scaler: 2-179                     [16, 128, 32, 32]         --
│    └─ReLU: 2-180                       [16, 128, 32, 32]         --
│    └─Empty: 2-181                      [16, 128, 32, 32]         --
│    └─Clamp: 2-182                      [16, 128, 32, 32]         --
├─Dropout2d: 1-19                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-20         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-183                  [16, 128, 16, 16]         --
│    └─Empty: 2-184                      [16, 128, 16, 16]         --
│    └─Empty: 2-185                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-186         --                        --
│    └─One: 2-187                        [1]                       --
│    └─OutputScale: 2-188                --                        --
│    └─Empty: 2-189                      [128, 128, 3, 3]          --
│    └─Empty: 2-190                      [128, 128, 3, 3]          --
│    └─Empty: 2-191                      [128]                     --
│    └─Empty: 2-192                      [128]                     --
│    └─BatchNorm2d: 2-193                [16, 128, 16, 16]         --
│    └─Scaler: 2-194                     [16, 128, 16, 16]         --
│    └─ReLU: 2-195                       [16, 128, 16, 16]         --
│    └─Empty: 2-196                      [16, 128, 16, 16]         --
│    └─Clamp: 2-197                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-21                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-198         --                        --
│    └─One: 2-199                        [1]                       --
│    └─OutputScale: 2-200                --                        --
│    └─Empty: 2-201                      [128, 128, 1, 1]          --
│    └─Empty: 2-202                      [128, 128, 1, 1]          --
│    └─Empty: 2-203                      [128]                     --
│    └─Empty: 2-204                      [128]                     --
│    └─BatchNorm2d: 2-205                [16, 128, 16, 16]         --
│    └─Scaler: 2-206                     [16, 128, 16, 16]         --
│    └─ReLU: 2-207                       [16, 128, 16, 16]         --
│    └─Empty: 2-208                      [16, 128, 16, 16]         --
│    └─Clamp: 2-209                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-22         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-210                  [16, 128, 16, 16]         --
│    └─Empty: 2-211                      [16, 128, 16, 16]         --
│    └─Empty: 2-212                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-213         --                        --
│    └─One: 2-214                        [1]                       --
│    └─OutputScale: 2-215                --                        --
│    └─Empty: 2-216                      [128, 128, 3, 3]          --
│    └─Empty: 2-217                      [128, 128, 3, 3]          --
│    └─Empty: 2-218                      [128]                     --
│    └─Empty: 2-219                      [128]                     --
│    └─BatchNorm2d: 2-220                [16, 128, 16, 16]         --
│    └─Scaler: 2-221                     [16, 128, 16, 16]         --
│    └─ReLU: 2-222                       [16, 128, 16, 16]         --
│    └─Empty: 2-223                      [16, 128, 16, 16]         --
│    └─Clamp: 2-224                      [16, 128, 16, 16]         --
├─Dropout2d: 1-23                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-24         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-225                  [16, 128, 8, 8]           --
│    └─Empty: 2-226                      [16, 128, 8, 8]           --
│    └─Empty: 2-227                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-228         --                        --
│    └─One: 2-229                        [1]                       --
│    └─OutputScale: 2-230                --                        --
│    └─Empty: 2-231                      [128, 128, 3, 3]          --
│    └─Empty: 2-232                      [128, 128, 3, 3]          --
│    └─Empty: 2-233                      [128]                     --
│    └─Empty: 2-234                      [128]                     --
│    └─BatchNorm2d: 2-235                [16, 128, 8, 8]           --
│    └─Scaler: 2-236                     [16, 128, 8, 8]           --
│    └─ReLU: 2-237                       [16, 128, 8, 8]           --
│    └─Empty: 2-238                      [16, 128, 8, 8]           --
│    └─Clamp: 2-239                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-25                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-240         --                        --
│    └─One: 2-241                        [1]                       --
│    └─OutputScale: 2-242                --                        --
│    └─Empty: 2-243                      [16, 128, 1, 1]           --
│    └─Empty: 2-244                      [16, 128, 1, 1]           --
│    └─Empty: 2-245                      [16]                      --
│    └─Empty: 2-246                      [16]                      --
│    └─BatchNorm2d: 2-247                [16, 16, 8, 8]            --
│    └─Scaler: 2-248                     [16, 16, 8, 8]            --
│    └─ReLU: 2-249                       [16, 16, 8, 8]            --
│    └─Empty: 2-250                      [16, 16, 8, 8]            --
│    └─Clamp: 2-251                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-252                  [16, 128, 8, 8]           --
│    └─Empty: 2-253                      [16, 128, 8, 8]           --
│    └─Empty: 2-254                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-255         --                        --
│    └─One: 2-256                        [1]                       --
│    └─OutputScale: 2-257                --                        --
│    └─Empty: 2-258                      [16, 128, 3, 3]           --
│    └─Empty: 2-259                      [16, 128, 3, 3]           --
│    └─Empty: 2-260                      [16]                      --
│    └─Empty: 2-261                      [16]                      --
│    └─BatchNorm2d: 2-262                [16, 16, 8, 8]            --
│    └─Scaler: 2-263                     [16, 16, 8, 8]            --
│    └─ReLU: 2-264                       [16, 16, 8, 8]            --
│    └─Empty: 2-265                      [16, 16, 8, 8]            --
│    └─Clamp: 2-266                      [16, 16, 8, 8]            --
├─Dropout2d: 1-27                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-28                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-267         --                        --
│    └─One: 2-268                        [1]                       --
│    └─OutputScale: 2-269                --                        --
│    └─Empty: 2-270                      [128, 48, 1, 1]           --
│    └─Empty: 2-271                      [128, 48, 1, 1]           --
│    └─Empty: 2-272                      [128]                     --
│    └─Empty: 2-273                      [128]                     --
│    └─BatchNorm2d: 2-274                [16, 128, 64, 64]         --
│    └─Scaler: 2-275                     [16, 128, 64, 64]         --
│    └─ReLU: 2-276                       [16, 128, 64, 64]         --
│    └─Empty: 2-277                      [16, 128, 64, 64]         --
│    └─Clamp: 2-278                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-29         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-279                  [16, 128, 32, 32]         --
│    └─Empty: 2-280                      [16, 128, 32, 32]         --
│    └─Empty: 2-281                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-282         --                        --
│    └─One: 2-283                        [1]                       --
│    └─OutputScale: 2-284                --                        --
│    └─Empty: 2-285                      [128, 128, 3, 3]          --
│    └─Empty: 2-286                      [128, 128, 3, 3]          --
│    └─Empty: 2-287                      [128]                     --
│    └─Empty: 2-288                      [128]                     --
│    └─BatchNorm2d: 2-289                [16, 128, 32, 32]         --
│    └─Scaler: 2-290                     [16, 128, 32, 32]         --
│    └─ReLU: 2-291                       [16, 128, 32, 32]         --
│    └─Empty: 2-292                      [16, 128, 32, 32]         --
│    └─Clamp: 2-293                      [16, 128, 32, 32]         --
├─Dropout2d: 1-30                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-294                  [16, 128, 16, 16]         --
│    └─Empty: 2-295                      [16, 128, 16, 16]         --
│    └─Empty: 2-296                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-297         --                        --
│    └─One: 2-298                        [1]                       --
│    └─OutputScale: 2-299                --                        --
│    └─Empty: 2-300                      [128, 128, 3, 3]          --
│    └─Empty: 2-301                      [128, 128, 3, 3]          --
│    └─Empty: 2-302                      [128]                     --
│    └─Empty: 2-303                      [128]                     --
│    └─BatchNorm2d: 2-304                [16, 128, 16, 16]         --
│    └─Scaler: 2-305                     [16, 128, 16, 16]         --
│    └─ReLU: 2-306                       [16, 128, 16, 16]         --
│    └─Empty: 2-307                      [16, 128, 16, 16]         --
│    └─Clamp: 2-308                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-32                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-309         --                        --
│    └─One: 2-310                        [1]                       --
│    └─OutputScale: 2-311                --                        --
│    └─Empty: 2-312                      [128, 128, 1, 1]          --
│    └─Empty: 2-313                      [128, 128, 1, 1]          --
│    └─Empty: 2-314                      [128]                     --
│    └─Empty: 2-315                      [128]                     --
│    └─BatchNorm2d: 2-316                [16, 128, 16, 16]         --
│    └─Scaler: 2-317                     [16, 128, 16, 16]         --
│    └─ReLU: 2-318                       [16, 128, 16, 16]         --
│    └─Empty: 2-319                      [16, 128, 16, 16]         --
│    └─Clamp: 2-320                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-33         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-321                  [16, 128, 16, 16]         --
│    └─Empty: 2-322                      [16, 128, 16, 16]         --
│    └─Empty: 2-323                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-324         --                        --
│    └─One: 2-325                        [1]                       --
│    └─OutputScale: 2-326                --                        --
│    └─Empty: 2-327                      [128, 128, 3, 3]          --
│    └─Empty: 2-328                      [128, 128, 3, 3]          --
│    └─Empty: 2-329                      [128]                     --
│    └─Empty: 2-330                      [128]                     --
│    └─BatchNorm2d: 2-331                [16, 128, 16, 16]         --
│    └─Scaler: 2-332                     [16, 128, 16, 16]         --
│    └─ReLU: 2-333                       [16, 128, 16, 16]         --
│    └─Empty: 2-334                      [16, 128, 16, 16]         --
│    └─Clamp: 2-335                      [16, 128, 16, 16]         --
├─Dropout2d: 1-34                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-35         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-336                  [16, 128, 8, 8]           --
│    └─Empty: 2-337                      [16, 128, 8, 8]           --
│    └─Empty: 2-338                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-339         --                        --
│    └─One: 2-340                        [1]                       --
│    └─OutputScale: 2-341                --                        --
│    └─Empty: 2-342                      [128, 128, 3, 3]          --
│    └─Empty: 2-343                      [128, 128, 3, 3]          --
│    └─Empty: 2-344                      [128]                     --
│    └─Empty: 2-345                      [128]                     --
│    └─BatchNorm2d: 2-346                [16, 128, 8, 8]           --
│    └─Scaler: 2-347                     [16, 128, 8, 8]           --
│    └─ReLU: 2-348                       [16, 128, 8, 8]           --
│    └─Empty: 2-349                      [16, 128, 8, 8]           --
│    └─Clamp: 2-350                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-36                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-351         --                        --
│    └─One: 2-352                        [1]                       --
│    └─OutputScale: 2-353                --                        --
│    └─Empty: 2-354                      [16, 128, 1, 1]           --
│    └─Empty: 2-355                      [16, 128, 1, 1]           --
│    └─Empty: 2-356                      [16]                      --
│    └─Empty: 2-357                      [16]                      --
│    └─BatchNorm2d: 2-358                [16, 16, 8, 8]            --
│    └─Scaler: 2-359                     [16, 16, 8, 8]            --
│    └─ReLU: 2-360                       [16, 16, 8, 8]            --
│    └─Empty: 2-361                      [16, 16, 8, 8]            --
│    └─Clamp: 2-362                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-37         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-363                  [16, 128, 8, 8]           --
│    └─Empty: 2-364                      [16, 128, 8, 8]           --
│    └─Empty: 2-365                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-366         --                        --
│    └─One: 2-367                        [1]                       --
│    └─OutputScale: 2-368                --                        --
│    └─Empty: 2-369                      [16, 128, 3, 3]           --
│    └─Empty: 2-370                      [16, 128, 3, 3]           --
│    └─Empty: 2-371                      [16]                      --
│    └─Empty: 2-372                      [16]                      --
│    └─BatchNorm2d: 2-373                [16, 16, 8, 8]            --
│    └─Scaler: 2-374                     [16, 16, 8, 8]            --
│    └─ReLU: 2-375                       [16, 16, 8, 8]            --
│    └─Empty: 2-376                      [16, 16, 8, 8]            --
│    └─Clamp: 2-377                      [16, 16, 8, 8]            --
├─Dropout2d: 1-38                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-39                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-378         --                        --
│    └─One: 2-379                        [1]                       --
│    └─OutputScale: 2-380                --                        --
│    └─Empty: 2-381                      [128, 48, 1, 1]           --
│    └─Empty: 2-382                      [128, 48, 1, 1]           --
│    └─Empty: 2-383                      [128]                     --
│    └─Empty: 2-384                      [128]                     --
│    └─BatchNorm2d: 2-385                [16, 128, 64, 64]         --
│    └─Scaler: 2-386                     [16, 128, 64, 64]         --
│    └─ReLU: 2-387                       [16, 128, 64, 64]         --
│    └─Empty: 2-388                      [16, 128, 64, 64]         --
│    └─Clamp: 2-389                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-40         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-390                  [16, 128, 32, 32]         --
│    └─Empty: 2-391                      [16, 128, 32, 32]         --
│    └─Empty: 2-392                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-393         --                        --
│    └─One: 2-394                        [1]                       --
│    └─OutputScale: 2-395                --                        --
│    └─Empty: 2-396                      [128, 128, 3, 3]          --
│    └─Empty: 2-397                      [128, 128, 3, 3]          --
│    └─Empty: 2-398                      [128]                     --
│    └─Empty: 2-399                      [128]                     --
│    └─BatchNorm2d: 2-400                [16, 128, 32, 32]         --
│    └─Scaler: 2-401                     [16, 128, 32, 32]         --
│    └─ReLU: 2-402                       [16, 128, 32, 32]         --
│    └─Empty: 2-403                      [16, 128, 32, 32]         --
│    └─Clamp: 2-404                      [16, 128, 32, 32]         --
├─Dropout2d: 1-41                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-42         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-405                  [16, 128, 16, 16]         --
│    └─Empty: 2-406                      [16, 128, 16, 16]         --
│    └─Empty: 2-407                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-408         --                        --
│    └─One: 2-409                        [1]                       --
│    └─OutputScale: 2-410                --                        --
│    └─Empty: 2-411                      [128, 128, 3, 3]          --
│    └─Empty: 2-412                      [128, 128, 3, 3]          --
│    └─Empty: 2-413                      [128]                     --
│    └─Empty: 2-414                      [128]                     --
│    └─BatchNorm2d: 2-415                [16, 128, 16, 16]         --
│    └─Scaler: 2-416                     [16, 128, 16, 16]         --
│    └─ReLU: 2-417                       [16, 128, 16, 16]         --
│    └─Empty: 2-418                      [16, 128, 16, 16]         --
│    └─Clamp: 2-419                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-43                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-420         --                        --
│    └─One: 2-421                        [1]                       --
│    └─OutputScale: 2-422                --                        --
│    └─Empty: 2-423                      [128, 128, 1, 1]          --
│    └─Empty: 2-424                      [128, 128, 1, 1]          --
│    └─Empty: 2-425                      [128]                     --
│    └─Empty: 2-426                      [128]                     --
│    └─BatchNorm2d: 2-427                [16, 128, 16, 16]         --
│    └─Scaler: 2-428                     [16, 128, 16, 16]         --
│    └─ReLU: 2-429                       [16, 128, 16, 16]         --
│    └─Empty: 2-430                      [16, 128, 16, 16]         --
│    └─Clamp: 2-431                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-44         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-432                  [16, 128, 16, 16]         --
│    └─Empty: 2-433                      [16, 128, 16, 16]         --
│    └─Empty: 2-434                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-435         --                        --
│    └─One: 2-436                        [1]                       --
│    └─OutputScale: 2-437                --                        --
│    └─Empty: 2-438                      [128, 128, 3, 3]          --
│    └─Empty: 2-439                      [128, 128, 3, 3]          --
│    └─Empty: 2-440                      [128]                     --
│    └─Empty: 2-441                      [128]                     --
│    └─BatchNorm2d: 2-442                [16, 128, 16, 16]         --
│    └─Scaler: 2-443                     [16, 128, 16, 16]         --
│    └─ReLU: 2-444                       [16, 128, 16, 16]         --
│    └─Empty: 2-445                      [16, 128, 16, 16]         --
│    └─Clamp: 2-446                      [16, 128, 16, 16]         --
├─Dropout2d: 1-45                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-447                  [16, 128, 8, 8]           --
│    └─Empty: 2-448                      [16, 128, 8, 8]           --
│    └─Empty: 2-449                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-450         --                        --
│    └─One: 2-451                        [1]                       --
│    └─OutputScale: 2-452                --                        --
│    └─Empty: 2-453                      [128, 128, 3, 3]          --
│    └─Empty: 2-454                      [128, 128, 3, 3]          --
│    └─Empty: 2-455                      [128]                     --
│    └─Empty: 2-456                      [128]                     --
│    └─BatchNorm2d: 2-457                [16, 128, 8, 8]           --
│    └─Scaler: 2-458                     [16, 128, 8, 8]           --
│    └─ReLU: 2-459                       [16, 128, 8, 8]           --
│    └─Empty: 2-460                      [16, 128, 8, 8]           --
│    └─Clamp: 2-461                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-47                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-462         --                        --
│    └─One: 2-463                        [1]                       --
│    └─OutputScale: 2-464                --                        --
│    └─Empty: 2-465                      [16, 128, 1, 1]           --
│    └─Empty: 2-466                      [16, 128, 1, 1]           --
│    └─Empty: 2-467                      [16]                      --
│    └─Empty: 2-468                      [16]                      --
│    └─BatchNorm2d: 2-469                [16, 16, 8, 8]            --
│    └─Scaler: 2-470                     [16, 16, 8, 8]            --
│    └─ReLU: 2-471                       [16, 16, 8, 8]            --
│    └─Empty: 2-472                      [16, 16, 8, 8]            --
│    └─Clamp: 2-473                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-474                  [16, 128, 8, 8]           --
│    └─Empty: 2-475                      [16, 128, 8, 8]           --
│    └─Empty: 2-476                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-477         --                        --
│    └─One: 2-478                        [1]                       --
│    └─OutputScale: 2-479                --                        --
│    └─Empty: 2-480                      [16, 128, 3, 3]           --
│    └─Empty: 2-481                      [16, 128, 3, 3]           --
│    └─Empty: 2-482                      [16]                      --
│    └─Empty: 2-483                      [16]                      --
│    └─BatchNorm2d: 2-484                [16, 16, 8, 8]            --
│    └─Scaler: 2-485                     [16, 16, 8, 8]            --
│    └─ReLU: 2-486                       [16, 16, 8, 8]            --
│    └─Empty: 2-487                      [16, 16, 8, 8]            --
│    └─Clamp: 2-488                      [16, 16, 8, 8]            --
├─Dropout2d: 1-49                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-50                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-489         --                        --
│    └─One: 2-490                        [1]                       --
│    └─OutputScale: 2-491                --                        --
│    └─Empty: 2-492                      [128, 48, 1, 1]           --
│    └─Empty: 2-493                      [128, 48, 1, 1]           --
│    └─Empty: 2-494                      [128]                     --
│    └─Empty: 2-495                      [128]                     --
│    └─BatchNorm2d: 2-496                [16, 128, 64, 64]         --
│    └─Scaler: 2-497                     [16, 128, 64, 64]         --
│    └─ReLU: 2-498                       [16, 128, 64, 64]         --
│    └─Empty: 2-499                      [16, 128, 64, 64]         --
│    └─Clamp: 2-500                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-501                  [16, 128, 32, 32]         --
│    └─Empty: 2-502                      [16, 128, 32, 32]         --
│    └─Empty: 2-503                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-504         --                        --
│    └─One: 2-505                        [1]                       --
│    └─OutputScale: 2-506                --                        --
│    └─Empty: 2-507                      [128, 128, 3, 3]          --
│    └─Empty: 2-508                      [128, 128, 3, 3]          --
│    └─Empty: 2-509                      [128]                     --
│    └─Empty: 2-510                      [128]                     --
│    └─BatchNorm2d: 2-511                [16, 128, 32, 32]         --
│    └─Scaler: 2-512                     [16, 128, 32, 32]         --
│    └─ReLU: 2-513                       [16, 128, 32, 32]         --
│    └─Empty: 2-514                      [16, 128, 32, 32]         --
│    └─Clamp: 2-515                      [16, 128, 32, 32]         --
├─Dropout2d: 1-52                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-53         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-516                  [16, 128, 16, 16]         --
│    └─Empty: 2-517                      [16, 128, 16, 16]         --
│    └─Empty: 2-518                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-519         --                        --
│    └─One: 2-520                        [1]                       --
│    └─OutputScale: 2-521                --                        --
│    └─Empty: 2-522                      [128, 128, 3, 3]          --
│    └─Empty: 2-523                      [128, 128, 3, 3]          --
│    └─Empty: 2-524                      [128]                     --
│    └─Empty: 2-525                      [128]                     --
│    └─BatchNorm2d: 2-526                [16, 128, 16, 16]         --
│    └─Scaler: 2-527                     [16, 128, 16, 16]         --
│    └─ReLU: 2-528                       [16, 128, 16, 16]         --
│    └─Empty: 2-529                      [16, 128, 16, 16]         --
│    └─Clamp: 2-530                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-54                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-531         --                        --
│    └─One: 2-532                        [1]                       --
│    └─OutputScale: 2-533                --                        --
│    └─Empty: 2-534                      [128, 128, 1, 1]          --
│    └─Empty: 2-535                      [128, 128, 1, 1]          --
│    └─Empty: 2-536                      [128]                     --
│    └─Empty: 2-537                      [128]                     --
│    └─BatchNorm2d: 2-538                [16, 128, 16, 16]         --
│    └─Scaler: 2-539                     [16, 128, 16, 16]         --
│    └─ReLU: 2-540                       [16, 128, 16, 16]         --
│    └─Empty: 2-541                      [16, 128, 16, 16]         --
│    └─Clamp: 2-542                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-55         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-543                  [16, 128, 16, 16]         --
│    └─Empty: 2-544                      [16, 128, 16, 16]         --
│    └─Empty: 2-545                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-546         --                        --
│    └─One: 2-547                        [1]                       --
│    └─OutputScale: 2-548                --                        --
│    └─Empty: 2-549                      [128, 128, 3, 3]          --
│    └─Empty: 2-550                      [128, 128, 3, 3]          --
│    └─Empty: 2-551                      [128]                     --
│    └─Empty: 2-552                      [128]                     --
│    └─BatchNorm2d: 2-553                [16, 128, 16, 16]         --
│    └─Scaler: 2-554                     [16, 128, 16, 16]         --
│    └─ReLU: 2-555                       [16, 128, 16, 16]         --
│    └─Empty: 2-556                      [16, 128, 16, 16]         --
│    └─Clamp: 2-557                      [16, 128, 16, 16]         --
├─Dropout2d: 1-56                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-57         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-558                  [16, 128, 8, 8]           --
│    └─Empty: 2-559                      [16, 128, 8, 8]           --
│    └─Empty: 2-560                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-561         --                        --
│    └─One: 2-562                        [1]                       --
│    └─OutputScale: 2-563                --                        --
│    └─Empty: 2-564                      [128, 128, 3, 3]          --
│    └─Empty: 2-565                      [128, 128, 3, 3]          --
│    └─Empty: 2-566                      [128]                     --
│    └─Empty: 2-567                      [128]                     --
│    └─BatchNorm2d: 2-568                [16, 128, 8, 8]           --
│    └─Scaler: 2-569                     [16, 128, 8, 8]           --
│    └─ReLU: 2-570                       [16, 128, 8, 8]           --
│    └─Empty: 2-571                      [16, 128, 8, 8]           --
│    └─Clamp: 2-572                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-58                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-573         --                        --
│    └─One: 2-574                        [1]                       --
│    └─OutputScale: 2-575                --                        --
│    └─Empty: 2-576                      [16, 128, 1, 1]           --
│    └─Empty: 2-577                      [16, 128, 1, 1]           --
│    └─Empty: 2-578                      [16]                      --
│    └─Empty: 2-579                      [16]                      --
│    └─BatchNorm2d: 2-580                [16, 16, 8, 8]            --
│    └─Scaler: 2-581                     [16, 16, 8, 8]            --
│    └─ReLU: 2-582                       [16, 16, 8, 8]            --
│    └─Empty: 2-583                      [16, 16, 8, 8]            --
│    └─Clamp: 2-584                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-59         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-585                  [16, 128, 8, 8]           --
│    └─Empty: 2-586                      [16, 128, 8, 8]           --
│    └─Empty: 2-587                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-588         --                        --
│    └─One: 2-589                        [1]                       --
│    └─OutputScale: 2-590                --                        --
│    └─Empty: 2-591                      [16, 128, 3, 3]           --
│    └─Empty: 2-592                      [16, 128, 3, 3]           --
│    └─Empty: 2-593                      [16]                      --
│    └─Empty: 2-594                      [16]                      --
│    └─BatchNorm2d: 2-595                [16, 16, 8, 8]            --
│    └─Scaler: 2-596                     [16, 16, 8, 8]            --
│    └─ReLU: 2-597                       [16, 16, 8, 8]            --
│    └─Empty: 2-598                      [16, 16, 8, 8]            --
│    └─Clamp: 2-599                      [16, 16, 8, 8]            --
├─Dropout2d: 1-60                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-61                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-600         --                        --
│    └─One: 2-601                        [1]                       --
│    └─OutputScale: 2-602                --                        --
│    └─Empty: 2-603                      [128, 48, 1, 1]           --
│    └─Empty: 2-604                      [128, 48, 1, 1]           --
│    └─Empty: 2-605                      [128]                     --
│    └─Empty: 2-606                      [128]                     --
│    └─BatchNorm2d: 2-607                [16, 128, 64, 64]         --
│    └─Scaler: 2-608                     [16, 128, 64, 64]         --
│    └─ReLU: 2-609                       [16, 128, 64, 64]         --
│    └─Empty: 2-610                      [16, 128, 64, 64]         --
│    └─Clamp: 2-611                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-62         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-612                  [16, 128, 32, 32]         --
│    └─Empty: 2-613                      [16, 128, 32, 32]         --
│    └─Empty: 2-614                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-615         --                        --
│    └─One: 2-616                        [1]                       --
│    └─OutputScale: 2-617                --                        --
│    └─Empty: 2-618                      [128, 128, 3, 3]          --
│    └─Empty: 2-619                      [128, 128, 3, 3]          --
│    └─Empty: 2-620                      [128]                     --
│    └─Empty: 2-621                      [128]                     --
│    └─BatchNorm2d: 2-622                [16, 128, 32, 32]         --
│    └─Scaler: 2-623                     [16, 128, 32, 32]         --
│    └─ReLU: 2-624                       [16, 128, 32, 32]         --
│    └─Empty: 2-625                      [16, 128, 32, 32]         --
│    └─Clamp: 2-626                      [16, 128, 32, 32]         --
├─Dropout2d: 1-63                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-64         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-627                  [16, 128, 16, 16]         --
│    └─Empty: 2-628                      [16, 128, 16, 16]         --
│    └─Empty: 2-629                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-630         --                        --
│    └─One: 2-631                        [1]                       --
│    └─OutputScale: 2-632                --                        --
│    └─Empty: 2-633                      [128, 128, 3, 3]          --
│    └─Empty: 2-634                      [128, 128, 3, 3]          --
│    └─Empty: 2-635                      [128]                     --
│    └─Empty: 2-636                      [128]                     --
│    └─BatchNorm2d: 2-637                [16, 128, 16, 16]         --
│    └─Scaler: 2-638                     [16, 128, 16, 16]         --
│    └─ReLU: 2-639                       [16, 128, 16, 16]         --
│    └─Empty: 2-640                      [16, 128, 16, 16]         --
│    └─Clamp: 2-641                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-65                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-642         --                        --
│    └─One: 2-643                        [1]                       --
│    └─OutputScale: 2-644                --                        --
│    └─Empty: 2-645                      [128, 128, 1, 1]          --
│    └─Empty: 2-646                      [128, 128, 1, 1]          --
│    └─Empty: 2-647                      [128]                     --
│    └─Empty: 2-648                      [128]                     --
│    └─BatchNorm2d: 2-649                [16, 128, 16, 16]         --
│    └─Scaler: 2-650                     [16, 128, 16, 16]         --
│    └─ReLU: 2-651                       [16, 128, 16, 16]         --
│    └─Empty: 2-652                      [16, 128, 16, 16]         --
│    └─Clamp: 2-653                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-654                  [16, 128, 16, 16]         --
│    └─Empty: 2-655                      [16, 128, 16, 16]         --
│    └─Empty: 2-656                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-657         --                        --
│    └─One: 2-658                        [1]                       --
│    └─OutputScale: 2-659                --                        --
│    └─Empty: 2-660                      [128, 128, 3, 3]          --
│    └─Empty: 2-661                      [128, 128, 3, 3]          --
│    └─Empty: 2-662                      [128]                     --
│    └─Empty: 2-663                      [128]                     --
│    └─BatchNorm2d: 2-664                [16, 128, 16, 16]         --
│    └─Scaler: 2-665                     [16, 128, 16, 16]         --
│    └─ReLU: 2-666                       [16, 128, 16, 16]         --
│    └─Empty: 2-667                      [16, 128, 16, 16]         --
│    └─Clamp: 2-668                      [16, 128, 16, 16]         --
├─Dropout2d: 1-67                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-669                  [16, 128, 8, 8]           --
│    └─Empty: 2-670                      [16, 128, 8, 8]           --
│    └─Empty: 2-671                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-672         --                        --
│    └─One: 2-673                        [1]                       --
│    └─OutputScale: 2-674                --                        --
│    └─Empty: 2-675                      [128, 128, 3, 3]          --
│    └─Empty: 2-676                      [128, 128, 3, 3]          --
│    └─Empty: 2-677                      [128]                     --
│    └─Empty: 2-678                      [128]                     --
│    └─BatchNorm2d: 2-679                [16, 128, 8, 8]           --
│    └─Scaler: 2-680                     [16, 128, 8, 8]           --
│    └─ReLU: 2-681                       [16, 128, 8, 8]           --
│    └─Empty: 2-682                      [16, 128, 8, 8]           --
│    └─Clamp: 2-683                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-69                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-684         --                        --
│    └─One: 2-685                        [1]                       --
│    └─OutputScale: 2-686                --                        --
│    └─Empty: 2-687                      [16, 128, 1, 1]           --
│    └─Empty: 2-688                      [16, 128, 1, 1]           --
│    └─Empty: 2-689                      [16]                      --
│    └─Empty: 2-690                      [16]                      --
│    └─BatchNorm2d: 2-691                [16, 16, 8, 8]            --
│    └─Scaler: 2-692                     [16, 16, 8, 8]            --
│    └─ReLU: 2-693                       [16, 16, 8, 8]            --
│    └─Empty: 2-694                      [16, 16, 8, 8]            --
│    └─Clamp: 2-695                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-70         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-696                  [16, 128, 8, 8]           --
│    └─Empty: 2-697                      [16, 128, 8, 8]           --
│    └─Empty: 2-698                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-699         --                        --
│    └─One: 2-700                        [1]                       --
│    └─OutputScale: 2-701                --                        --
│    └─Empty: 2-702                      [16, 128, 3, 3]           --
│    └─Empty: 2-703                      [16, 128, 3, 3]           --
│    └─Empty: 2-704                      [16]                      --
│    └─Empty: 2-705                      [16]                      --
│    └─BatchNorm2d: 2-706                [16, 16, 8, 8]            --
│    └─Scaler: 2-707                     [16, 16, 8, 8]            --
│    └─ReLU: 2-708                       [16, 16, 8, 8]            --
│    └─Empty: 2-709                      [16, 16, 8, 8]            --
│    └─Clamp: 2-710                      [16, 16, 8, 8]            --
├─Dropout2d: 1-71                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-72                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-711         --                        --
│    └─One: 2-712                        [1]                       --
│    └─OutputScale: 2-713                --                        --
│    └─Empty: 2-714                      [128, 48, 1, 1]           --
│    └─Empty: 2-715                      [128, 48, 1, 1]           --
│    └─Empty: 2-716                      [128]                     --
│    └─Empty: 2-717                      [128]                     --
│    └─BatchNorm2d: 2-718                [16, 128, 64, 64]         --
│    └─Scaler: 2-719                     [16, 128, 64, 64]         --
│    └─ReLU: 2-720                       [16, 128, 64, 64]         --
│    └─Empty: 2-721                      [16, 128, 64, 64]         --
│    └─Clamp: 2-722                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-723                  [16, 128, 32, 32]         --
│    └─Empty: 2-724                      [16, 128, 32, 32]         --
│    └─Empty: 2-725                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-726         --                        --
│    └─One: 2-727                        [1]                       --
│    └─OutputScale: 2-728                --                        --
│    └─Empty: 2-729                      [128, 128, 3, 3]          --
│    └─Empty: 2-730                      [128, 128, 3, 3]          --
│    └─Empty: 2-731                      [128]                     --
│    └─Empty: 2-732                      [128]                     --
│    └─BatchNorm2d: 2-733                [16, 128, 32, 32]         --
│    └─Scaler: 2-734                     [16, 128, 32, 32]         --
│    └─ReLU: 2-735                       [16, 128, 32, 32]         --
│    └─Empty: 2-736                      [16, 128, 32, 32]         --
│    └─Clamp: 2-737                      [16, 128, 32, 32]         --
├─Dropout2d: 1-74                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-738                  [16, 128, 16, 16]         --
│    └─Empty: 2-739                      [16, 128, 16, 16]         --
│    └─Empty: 2-740                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-741         --                        --
│    └─One: 2-742                        [1]                       --
│    └─OutputScale: 2-743                --                        --
│    └─Empty: 2-744                      [128, 128, 3, 3]          --
│    └─Empty: 2-745                      [128, 128, 3, 3]          --
│    └─Empty: 2-746                      [128]                     --
│    └─Empty: 2-747                      [128]                     --
│    └─BatchNorm2d: 2-748                [16, 128, 16, 16]         --
│    └─Scaler: 2-749                     [16, 128, 16, 16]         --
│    └─ReLU: 2-750                       [16, 128, 16, 16]         --
│    └─Empty: 2-751                      [16, 128, 16, 16]         --
│    └─Clamp: 2-752                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-76                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-753         --                        --
│    └─One: 2-754                        [1]                       --
│    └─OutputScale: 2-755                --                        --
│    └─Empty: 2-756                      [128, 128, 1, 1]          --
│    └─Empty: 2-757                      [128, 128, 1, 1]          --
│    └─Empty: 2-758                      [128]                     --
│    └─Empty: 2-759                      [128]                     --
│    └─BatchNorm2d: 2-760                [16, 128, 16, 16]         --
│    └─Scaler: 2-761                     [16, 128, 16, 16]         --
│    └─ReLU: 2-762                       [16, 128, 16, 16]         --
│    └─Empty: 2-763                      [16, 128, 16, 16]         --
│    └─Clamp: 2-764                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-77         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-765                  [16, 128, 16, 16]         --
│    └─Empty: 2-766                      [16, 128, 16, 16]         --
│    └─Empty: 2-767                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-768         --                        --
│    └─One: 2-769                        [1]                       --
│    └─OutputScale: 2-770                --                        --
│    └─Empty: 2-771                      [128, 128, 3, 3]          --
│    └─Empty: 2-772                      [128, 128, 3, 3]          --
│    └─Empty: 2-773                      [128]                     --
│    └─Empty: 2-774                      [128]                     --
│    └─BatchNorm2d: 2-775                [16, 128, 16, 16]         --
│    └─Scaler: 2-776                     [16, 128, 16, 16]         --
│    └─ReLU: 2-777                       [16, 128, 16, 16]         --
│    └─Empty: 2-778                      [16, 128, 16, 16]         --
│    └─Clamp: 2-779                      [16, 128, 16, 16]         --
├─Dropout2d: 1-78                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-79         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-780                  [16, 128, 8, 8]           --
│    └─Empty: 2-781                      [16, 128, 8, 8]           --
│    └─Empty: 2-782                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-783         --                        --
│    └─One: 2-784                        [1]                       --
│    └─OutputScale: 2-785                --                        --
│    └─Empty: 2-786                      [128, 128, 3, 3]          --
│    └─Empty: 2-787                      [128, 128, 3, 3]          --
│    └─Empty: 2-788                      [128]                     --
│    └─Empty: 2-789                      [128]                     --
│    └─BatchNorm2d: 2-790                [16, 128, 8, 8]           --
│    └─Scaler: 2-791                     [16, 128, 8, 8]           --
│    └─ReLU: 2-792                       [16, 128, 8, 8]           --
│    └─Empty: 2-793                      [16, 128, 8, 8]           --
│    └─Clamp: 2-794                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-80                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-795         --                        --
│    └─One: 2-796                        [1]                       --
│    └─OutputScale: 2-797                --                        --
│    └─Empty: 2-798                      [16, 128, 1, 1]           --
│    └─Empty: 2-799                      [16, 128, 1, 1]           --
│    └─Empty: 2-800                      [16]                      --
│    └─Empty: 2-801                      [16]                      --
│    └─BatchNorm2d: 2-802                [16, 16, 8, 8]            --
│    └─Scaler: 2-803                     [16, 16, 8, 8]            --
│    └─ReLU: 2-804                       [16, 16, 8, 8]            --
│    └─Empty: 2-805                      [16, 16, 8, 8]            --
│    └─Clamp: 2-806                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-81         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-807                  [16, 128, 8, 8]           --
│    └─Empty: 2-808                      [16, 128, 8, 8]           --
│    └─Empty: 2-809                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-810         --                        --
│    └─One: 2-811                        [1]                       --
│    └─OutputScale: 2-812                --                        --
│    └─Empty: 2-813                      [16, 128, 3, 3]           --
│    └─Empty: 2-814                      [16, 128, 3, 3]           --
│    └─Empty: 2-815                      [16]                      --
│    └─Empty: 2-816                      [16]                      --
│    └─BatchNorm2d: 2-817                [16, 16, 8, 8]            --
│    └─Scaler: 2-818                     [16, 16, 8, 8]            --
│    └─ReLU: 2-819                       [16, 16, 8, 8]            --
│    └─Empty: 2-820                      [16, 16, 8, 8]            --
│    └─Clamp: 2-821                      [16, 16, 8, 8]            --
├─Dropout2d: 1-82                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-83                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-822         --                        --
│    └─One: 2-823                        [1]                       --
│    └─OutputScale: 2-824                --                        --
│    └─Empty: 2-825                      [128, 48, 1, 1]           --
│    └─Empty: 2-826                      [128, 48, 1, 1]           --
│    └─Empty: 2-827                      [128]                     --
│    └─Empty: 2-828                      [128]                     --
│    └─BatchNorm2d: 2-829                [16, 128, 64, 64]         --
│    └─Scaler: 2-830                     [16, 128, 64, 64]         --
│    └─ReLU: 2-831                       [16, 128, 64, 64]         --
│    └─Empty: 2-832                      [16, 128, 64, 64]         --
│    └─Clamp: 2-833                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-84         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-834                  [16, 128, 32, 32]         --
│    └─Empty: 2-835                      [16, 128, 32, 32]         --
│    └─Empty: 2-836                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-837         --                        --
│    └─One: 2-838                        [1]                       --
│    └─OutputScale: 2-839                --                        --
│    └─Empty: 2-840                      [128, 128, 3, 3]          --
│    └─Empty: 2-841                      [128, 128, 3, 3]          --
│    └─Empty: 2-842                      [128]                     --
│    └─Empty: 2-843                      [128]                     --
│    └─BatchNorm2d: 2-844                [16, 128, 32, 32]         --
│    └─Scaler: 2-845                     [16, 128, 32, 32]         --
│    └─ReLU: 2-846                       [16, 128, 32, 32]         --
│    └─Empty: 2-847                      [16, 128, 32, 32]         --
│    └─Clamp: 2-848                      [16, 128, 32, 32]         --
├─Dropout2d: 1-85                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-86         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-849                  [16, 128, 16, 16]         --
│    └─Empty: 2-850                      [16, 128, 16, 16]         --
│    └─Empty: 2-851                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-852         --                        --
│    └─One: 2-853                        [1]                       --
│    └─OutputScale: 2-854                --                        --
│    └─Empty: 2-855                      [128, 128, 3, 3]          --
│    └─Empty: 2-856                      [128, 128, 3, 3]          --
│    └─Empty: 2-857                      [128]                     --
│    └─Empty: 2-858                      [128]                     --
│    └─BatchNorm2d: 2-859                [16, 128, 16, 16]         --
│    └─Scaler: 2-860                     [16, 128, 16, 16]         --
│    └─ReLU: 2-861                       [16, 128, 16, 16]         --
│    └─Empty: 2-862                      [16, 128, 16, 16]         --
│    └─Clamp: 2-863                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-87                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-864         --                        --
│    └─One: 2-865                        [1]                       --
│    └─OutputScale: 2-866                --                        --
│    └─Empty: 2-867                      [128, 128, 1, 1]          --
│    └─Empty: 2-868                      [128, 128, 1, 1]          --
│    └─Empty: 2-869                      [128]                     --
│    └─Empty: 2-870                      [128]                     --
│    └─BatchNorm2d: 2-871                [16, 128, 16, 16]         --
│    └─Scaler: 2-872                     [16, 128, 16, 16]         --
│    └─ReLU: 2-873                       [16, 128, 16, 16]         --
│    └─Empty: 2-874                      [16, 128, 16, 16]         --
│    └─Clamp: 2-875                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-88         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-876                  [16, 128, 16, 16]         --
│    └─Empty: 2-877                      [16, 128, 16, 16]         --
│    └─Empty: 2-878                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-879         --                        --
│    └─One: 2-880                        [1]                       --
│    └─OutputScale: 2-881                --                        --
│    └─Empty: 2-882                      [128, 128, 3, 3]          --
│    └─Empty: 2-883                      [128, 128, 3, 3]          --
│    └─Empty: 2-884                      [128]                     --
│    └─Empty: 2-885                      [128]                     --
│    └─BatchNorm2d: 2-886                [16, 128, 16, 16]         --
│    └─Scaler: 2-887                     [16, 128, 16, 16]         --
│    └─ReLU: 2-888                       [16, 128, 16, 16]         --
│    └─Empty: 2-889                      [16, 128, 16, 16]         --
│    └─Clamp: 2-890                      [16, 128, 16, 16]         --
├─Dropout2d: 1-89                        [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-90         [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-891                  [16, 128, 8, 8]           --
│    └─Empty: 2-892                      [16, 128, 8, 8]           --
│    └─Empty: 2-893                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-894         --                        --
│    └─One: 2-895                        [1]                       --
│    └─OutputScale: 2-896                --                        --
│    └─Empty: 2-897                      [128, 128, 3, 3]          --
│    └─Empty: 2-898                      [128, 128, 3, 3]          --
│    └─Empty: 2-899                      [128]                     --
│    └─Empty: 2-900                      [128]                     --
│    └─BatchNorm2d: 2-901                [16, 128, 8, 8]           --
│    └─Scaler: 2-902                     [16, 128, 8, 8]           --
│    └─ReLU: 2-903                       [16, 128, 8, 8]           --
│    └─Empty: 2-904                      [16, 128, 8, 8]           --
│    └─Clamp: 2-905                      [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-91                [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-906         --                        --
│    └─One: 2-907                        [1]                       --
│    └─OutputScale: 2-908                --                        --
│    └─Empty: 2-909                      [16, 128, 1, 1]           --
│    └─Empty: 2-910                      [16, 128, 1, 1]           --
│    └─Empty: 2-911                      [16]                      --
│    └─Empty: 2-912                      [16]                      --
│    └─BatchNorm2d: 2-913                [16, 16, 8, 8]            --
│    └─Scaler: 2-914                     [16, 16, 8, 8]            --
│    └─ReLU: 2-915                       [16, 16, 8, 8]            --
│    └─Empty: 2-916                      [16, 16, 8, 8]            --
│    └─Clamp: 2-917                      [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-92         [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-918                  [16, 128, 8, 8]           --
│    └─Empty: 2-919                      [16, 128, 8, 8]           --
│    └─Empty: 2-920                      [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-921         --                        --
│    └─One: 2-922                        [1]                       --
│    └─OutputScale: 2-923                --                        --
│    └─Empty: 2-924                      [16, 128, 3, 3]           --
│    └─Empty: 2-925                      [16, 128, 3, 3]           --
│    └─Empty: 2-926                      [16]                      --
│    └─Empty: 2-927                      [16]                      --
│    └─BatchNorm2d: 2-928                [16, 16, 8, 8]            --
│    └─Scaler: 2-929                     [16, 16, 8, 8]            --
│    └─ReLU: 2-930                       [16, 16, 8, 8]            --
│    └─Empty: 2-931                      [16, 16, 8, 8]            --
│    └─Clamp: 2-932                      [16, 16, 8, 8]            --
├─Dropout2d: 1-93                        [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-94                [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-933         --                        --
│    └─One: 2-934                        [1]                       --
│    └─OutputScale: 2-935                --                        --
│    └─Empty: 2-936                      [128, 48, 1, 1]           --
│    └─Empty: 2-937                      [128, 48, 1, 1]           --
│    └─Empty: 2-938                      [128]                     --
│    └─Empty: 2-939                      [128]                     --
│    └─BatchNorm2d: 2-940                [16, 128, 64, 64]         --
│    └─Scaler: 2-941                     [16, 128, 64, 64]         --
│    └─ReLU: 2-942                       [16, 128, 64, 64]         --
│    └─Empty: 2-943                      [16, 128, 64, 64]         --
│    └─Clamp: 2-944                      [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-95         [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-945                  [16, 128, 32, 32]         --
│    └─Empty: 2-946                      [16, 128, 32, 32]         --
│    └─Empty: 2-947                      [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-948         --                        --
│    └─One: 2-949                        [1]                       --
│    └─OutputScale: 2-950                --                        --
│    └─Empty: 2-951                      [128, 128, 3, 3]          --
│    └─Empty: 2-952                      [128, 128, 3, 3]          --
│    └─Empty: 2-953                      [128]                     --
│    └─Empty: 2-954                      [128]                     --
│    └─BatchNorm2d: 2-955                [16, 128, 32, 32]         --
│    └─Scaler: 2-956                     [16, 128, 32, 32]         --
│    └─ReLU: 2-957                       [16, 128, 32, 32]         --
│    └─Empty: 2-958                      [16, 128, 32, 32]         --
│    └─Clamp: 2-959                      [16, 128, 32, 32]         --
├─Dropout2d: 1-96                        [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-97         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-960                  [16, 128, 16, 16]         --
│    └─Empty: 2-961                      [16, 128, 16, 16]         --
│    └─Empty: 2-962                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-963         --                        --
│    └─One: 2-964                        [1]                       --
│    └─OutputScale: 2-965                --                        --
│    └─Empty: 2-966                      [128, 128, 3, 3]          --
│    └─Empty: 2-967                      [128, 128, 3, 3]          --
│    └─Empty: 2-968                      [128]                     --
│    └─Empty: 2-969                      [128]                     --
│    └─BatchNorm2d: 2-970                [16, 128, 16, 16]         --
│    └─Scaler: 2-971                     [16, 128, 16, 16]         --
│    └─ReLU: 2-972                       [16, 128, 16, 16]         --
│    └─Empty: 2-973                      [16, 128, 16, 16]         --
│    └─Clamp: 2-974                      [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-98                [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-975         --                        --
│    └─One: 2-976                        [1]                       --
│    └─OutputScale: 2-977                --                        --
│    └─Empty: 2-978                      [128, 128, 1, 1]          --
│    └─Empty: 2-979                      [128, 128, 1, 1]          --
│    └─Empty: 2-980                      [128]                     --
│    └─Empty: 2-981                      [128]                     --
│    └─BatchNorm2d: 2-982                [16, 128, 16, 16]         --
│    └─Scaler: 2-983                     [16, 128, 16, 16]         --
│    └─ReLU: 2-984                       [16, 128, 16, 16]         --
│    └─Empty: 2-985                      [16, 128, 16, 16]         --
│    └─Clamp: 2-986                      [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-99         [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-987                  [16, 128, 16, 16]         --
│    └─Empty: 2-988                      [16, 128, 16, 16]         --
│    └─Empty: 2-989                      [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-990         --                        --
│    └─One: 2-991                        [1]                       --
│    └─OutputScale: 2-992                --                        --
│    └─Empty: 2-993                      [128, 128, 3, 3]          --
│    └─Empty: 2-994                      [128, 128, 3, 3]          --
│    └─Empty: 2-995                      [128]                     --
│    └─Empty: 2-996                      [128]                     --
│    └─BatchNorm2d: 2-997                [16, 128, 16, 16]         --
│    └─Scaler: 2-998                     [16, 128, 16, 16]         --
│    └─ReLU: 2-999                       [16, 128, 16, 16]         --
│    └─Empty: 2-1000                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1001                     [16, 128, 16, 16]         --
├─Dropout2d: 1-100                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-101        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1002                 [16, 128, 8, 8]           --
│    └─Empty: 2-1003                     [16, 128, 8, 8]           --
│    └─Empty: 2-1004                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1005        --                        --
│    └─One: 2-1006                       [1]                       --
│    └─OutputScale: 2-1007               --                        --
│    └─Empty: 2-1008                     [128, 128, 3, 3]          --
│    └─Empty: 2-1009                     [128, 128, 3, 3]          --
│    └─Empty: 2-1010                     [128]                     --
│    └─Empty: 2-1011                     [128]                     --
│    └─BatchNorm2d: 2-1012               [16, 128, 8, 8]           --
│    └─Scaler: 2-1013                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1014                      [16, 128, 8, 8]           --
│    └─Empty: 2-1015                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1016                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-102               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1017        --                        --
│    └─One: 2-1018                       [1]                       --
│    └─OutputScale: 2-1019               --                        --
│    └─Empty: 2-1020                     [16, 128, 1, 1]           --
│    └─Empty: 2-1021                     [16, 128, 1, 1]           --
│    └─Empty: 2-1022                     [16]                      --
│    └─Empty: 2-1023                     [16]                      --
│    └─BatchNorm2d: 2-1024               [16, 16, 8, 8]            --
│    └─Scaler: 2-1025                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1026                      [16, 16, 8, 8]            --
│    └─Empty: 2-1027                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1028                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-103        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1029                 [16, 128, 8, 8]           --
│    └─Empty: 2-1030                     [16, 128, 8, 8]           --
│    └─Empty: 2-1031                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1032        --                        --
│    └─One: 2-1033                       [1]                       --
│    └─OutputScale: 2-1034               --                        --
│    └─Empty: 2-1035                     [16, 128, 3, 3]           --
│    └─Empty: 2-1036                     [16, 128, 3, 3]           --
│    └─Empty: 2-1037                     [16]                      --
│    └─Empty: 2-1038                     [16]                      --
│    └─BatchNorm2d: 2-1039               [16, 16, 8, 8]            --
│    └─Scaler: 2-1040                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1041                      [16, 16, 8, 8]            --
│    └─Empty: 2-1042                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1043                     [16, 16, 8, 8]            --
├─Dropout2d: 1-104                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-105               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1044        --                        --
│    └─One: 2-1045                       [1]                       --
│    └─OutputScale: 2-1046               --                        --
│    └─Empty: 2-1047                     [128, 48, 1, 1]           --
│    └─Empty: 2-1048                     [128, 48, 1, 1]           --
│    └─Empty: 2-1049                     [128]                     --
│    └─Empty: 2-1050                     [128]                     --
│    └─BatchNorm2d: 2-1051               [16, 128, 64, 64]         --
│    └─Scaler: 2-1052                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1053                      [16, 128, 64, 64]         --
│    └─Empty: 2-1054                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1055                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-106        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1056                 [16, 128, 32, 32]         --
│    └─Empty: 2-1057                     [16, 128, 32, 32]         --
│    └─Empty: 2-1058                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1059        --                        --
│    └─One: 2-1060                       [1]                       --
│    └─OutputScale: 2-1061               --                        --
│    └─Empty: 2-1062                     [128, 128, 3, 3]          --
│    └─Empty: 2-1063                     [128, 128, 3, 3]          --
│    └─Empty: 2-1064                     [128]                     --
│    └─Empty: 2-1065                     [128]                     --
│    └─BatchNorm2d: 2-1066               [16, 128, 32, 32]         --
│    └─Scaler: 2-1067                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1068                      [16, 128, 32, 32]         --
│    └─Empty: 2-1069                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1070                     [16, 128, 32, 32]         --
├─Dropout2d: 1-107                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-108        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1071                 [16, 128, 16, 16]         --
│    └─Empty: 2-1072                     [16, 128, 16, 16]         --
│    └─Empty: 2-1073                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1074        --                        --
│    └─One: 2-1075                       [1]                       --
│    └─OutputScale: 2-1076               --                        --
│    └─Empty: 2-1077                     [128, 128, 3, 3]          --
│    └─Empty: 2-1078                     [128, 128, 3, 3]          --
│    └─Empty: 2-1079                     [128]                     --
│    └─Empty: 2-1080                     [128]                     --
│    └─BatchNorm2d: 2-1081               [16, 128, 16, 16]         --
│    └─Scaler: 2-1082                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1083                      [16, 128, 16, 16]         --
│    └─Empty: 2-1084                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1085                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-109               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1086        --                        --
│    └─One: 2-1087                       [1]                       --
│    └─OutputScale: 2-1088               --                        --
│    └─Empty: 2-1089                     [128, 128, 1, 1]          --
│    └─Empty: 2-1090                     [128, 128, 1, 1]          --
│    └─Empty: 2-1091                     [128]                     --
│    └─Empty: 2-1092                     [128]                     --
│    └─BatchNorm2d: 2-1093               [16, 128, 16, 16]         --
│    └─Scaler: 2-1094                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1095                      [16, 128, 16, 16]         --
│    └─Empty: 2-1096                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1097                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-110        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1098                 [16, 128, 16, 16]         --
│    └─Empty: 2-1099                     [16, 128, 16, 16]         --
│    └─Empty: 2-1100                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1101        --                        --
│    └─One: 2-1102                       [1]                       --
│    └─OutputScale: 2-1103               --                        --
│    └─Empty: 2-1104                     [128, 128, 3, 3]          --
│    └─Empty: 2-1105                     [128, 128, 3, 3]          --
│    └─Empty: 2-1106                     [128]                     --
│    └─Empty: 2-1107                     [128]                     --
│    └─BatchNorm2d: 2-1108               [16, 128, 16, 16]         --
│    └─Scaler: 2-1109                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1110                      [16, 128, 16, 16]         --
│    └─Empty: 2-1111                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1112                     [16, 128, 16, 16]         --
├─Dropout2d: 1-111                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-112        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1113                 [16, 128, 8, 8]           --
│    └─Empty: 2-1114                     [16, 128, 8, 8]           --
│    └─Empty: 2-1115                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1116        --                        --
│    └─One: 2-1117                       [1]                       --
│    └─OutputScale: 2-1118               --                        --
│    └─Empty: 2-1119                     [128, 128, 3, 3]          --
│    └─Empty: 2-1120                     [128, 128, 3, 3]          --
│    └─Empty: 2-1121                     [128]                     --
│    └─Empty: 2-1122                     [128]                     --
│    └─BatchNorm2d: 2-1123               [16, 128, 8, 8]           --
│    └─Scaler: 2-1124                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1125                      [16, 128, 8, 8]           --
│    └─Empty: 2-1126                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1127                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-113               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1128        --                        --
│    └─One: 2-1129                       [1]                       --
│    └─OutputScale: 2-1130               --                        --
│    └─Empty: 2-1131                     [16, 128, 1, 1]           --
│    └─Empty: 2-1132                     [16, 128, 1, 1]           --
│    └─Empty: 2-1133                     [16]                      --
│    └─Empty: 2-1134                     [16]                      --
│    └─BatchNorm2d: 2-1135               [16, 16, 8, 8]            --
│    └─Scaler: 2-1136                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1137                      [16, 16, 8, 8]            --
│    └─Empty: 2-1138                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1139                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-114        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1140                 [16, 128, 8, 8]           --
│    └─Empty: 2-1141                     [16, 128, 8, 8]           --
│    └─Empty: 2-1142                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1143        --                        --
│    └─One: 2-1144                       [1]                       --
│    └─OutputScale: 2-1145               --                        --
│    └─Empty: 2-1146                     [16, 128, 3, 3]           --
│    └─Empty: 2-1147                     [16, 128, 3, 3]           --
│    └─Empty: 2-1148                     [16]                      --
│    └─Empty: 2-1149                     [16]                      --
│    └─BatchNorm2d: 2-1150               [16, 16, 8, 8]            --
│    └─Scaler: 2-1151                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1152                      [16, 16, 8, 8]            --
│    └─Empty: 2-1153                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1154                     [16, 16, 8, 8]            --
├─Dropout2d: 1-115                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-116               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1155        --                        --
│    └─One: 2-1156                       [1]                       --
│    └─OutputScale: 2-1157               --                        --
│    └─Empty: 2-1158                     [128, 48, 1, 1]           --
│    └─Empty: 2-1159                     [128, 48, 1, 1]           --
│    └─Empty: 2-1160                     [128]                     --
│    └─Empty: 2-1161                     [128]                     --
│    └─BatchNorm2d: 2-1162               [16, 128, 64, 64]         --
│    └─Scaler: 2-1163                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1164                      [16, 128, 64, 64]         --
│    └─Empty: 2-1165                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1166                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-117        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1167                 [16, 128, 32, 32]         --
│    └─Empty: 2-1168                     [16, 128, 32, 32]         --
│    └─Empty: 2-1169                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1170        --                        --
│    └─One: 2-1171                       [1]                       --
│    └─OutputScale: 2-1172               --                        --
│    └─Empty: 2-1173                     [128, 128, 3, 3]          --
│    └─Empty: 2-1174                     [128, 128, 3, 3]          --
│    └─Empty: 2-1175                     [128]                     --
│    └─Empty: 2-1176                     [128]                     --
│    └─BatchNorm2d: 2-1177               [16, 128, 32, 32]         --
│    └─Scaler: 2-1178                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1179                      [16, 128, 32, 32]         --
│    └─Empty: 2-1180                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1181                     [16, 128, 32, 32]         --
├─Dropout2d: 1-118                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-119        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1182                 [16, 128, 16, 16]         --
│    └─Empty: 2-1183                     [16, 128, 16, 16]         --
│    └─Empty: 2-1184                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1185        --                        --
│    └─One: 2-1186                       [1]                       --
│    └─OutputScale: 2-1187               --                        --
│    └─Empty: 2-1188                     [128, 128, 3, 3]          --
│    └─Empty: 2-1189                     [128, 128, 3, 3]          --
│    └─Empty: 2-1190                     [128]                     --
│    └─Empty: 2-1191                     [128]                     --
│    └─BatchNorm2d: 2-1192               [16, 128, 16, 16]         --
│    └─Scaler: 2-1193                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1194                      [16, 128, 16, 16]         --
│    └─Empty: 2-1195                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1196                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-120               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1197        --                        --
│    └─One: 2-1198                       [1]                       --
│    └─OutputScale: 2-1199               --                        --
│    └─Empty: 2-1200                     [128, 128, 1, 1]          --
│    └─Empty: 2-1201                     [128, 128, 1, 1]          --
│    └─Empty: 2-1202                     [128]                     --
│    └─Empty: 2-1203                     [128]                     --
│    └─BatchNorm2d: 2-1204               [16, 128, 16, 16]         --
│    └─Scaler: 2-1205                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1206                      [16, 128, 16, 16]         --
│    └─Empty: 2-1207                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1208                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-121        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1209                 [16, 128, 16, 16]         --
│    └─Empty: 2-1210                     [16, 128, 16, 16]         --
│    └─Empty: 2-1211                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1212        --                        --
│    └─One: 2-1213                       [1]                       --
│    └─OutputScale: 2-1214               --                        --
│    └─Empty: 2-1215                     [128, 128, 3, 3]          --
│    └─Empty: 2-1216                     [128, 128, 3, 3]          --
│    └─Empty: 2-1217                     [128]                     --
│    └─Empty: 2-1218                     [128]                     --
│    └─BatchNorm2d: 2-1219               [16, 128, 16, 16]         --
│    └─Scaler: 2-1220                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1221                      [16, 128, 16, 16]         --
│    └─Empty: 2-1222                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1223                     [16, 128, 16, 16]         --
├─Dropout2d: 1-122                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-123        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1224                 [16, 128, 8, 8]           --
│    └─Empty: 2-1225                     [16, 128, 8, 8]           --
│    └─Empty: 2-1226                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1227        --                        --
│    └─One: 2-1228                       [1]                       --
│    └─OutputScale: 2-1229               --                        --
│    └─Empty: 2-1230                     [128, 128, 3, 3]          --
│    └─Empty: 2-1231                     [128, 128, 3, 3]          --
│    └─Empty: 2-1232                     [128]                     --
│    └─Empty: 2-1233                     [128]                     --
│    └─BatchNorm2d: 2-1234               [16, 128, 8, 8]           --
│    └─Scaler: 2-1235                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1236                      [16, 128, 8, 8]           --
│    └─Empty: 2-1237                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1238                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-124               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1239        --                        --
│    └─One: 2-1240                       [1]                       --
│    └─OutputScale: 2-1241               --                        --
│    └─Empty: 2-1242                     [16, 128, 1, 1]           --
│    └─Empty: 2-1243                     [16, 128, 1, 1]           --
│    └─Empty: 2-1244                     [16]                      --
│    └─Empty: 2-1245                     [16]                      --
│    └─BatchNorm2d: 2-1246               [16, 16, 8, 8]            --
│    └─Scaler: 2-1247                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1248                      [16, 16, 8, 8]            --
│    └─Empty: 2-1249                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1250                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-125        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1251                 [16, 128, 8, 8]           --
│    └─Empty: 2-1252                     [16, 128, 8, 8]           --
│    └─Empty: 2-1253                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1254        --                        --
│    └─One: 2-1255                       [1]                       --
│    └─OutputScale: 2-1256               --                        --
│    └─Empty: 2-1257                     [16, 128, 3, 3]           --
│    └─Empty: 2-1258                     [16, 128, 3, 3]           --
│    └─Empty: 2-1259                     [16]                      --
│    └─Empty: 2-1260                     [16]                      --
│    └─BatchNorm2d: 2-1261               [16, 16, 8, 8]            --
│    └─Scaler: 2-1262                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1263                      [16, 16, 8, 8]            --
│    └─Empty: 2-1264                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1265                     [16, 16, 8, 8]            --
├─Dropout2d: 1-126                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-127               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1266        --                        --
│    └─One: 2-1267                       [1]                       --
│    └─OutputScale: 2-1268               --                        --
│    └─Empty: 2-1269                     [128, 48, 1, 1]           --
│    └─Empty: 2-1270                     [128, 48, 1, 1]           --
│    └─Empty: 2-1271                     [128]                     --
│    └─Empty: 2-1272                     [128]                     --
│    └─BatchNorm2d: 2-1273               [16, 128, 64, 64]         --
│    └─Scaler: 2-1274                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1275                      [16, 128, 64, 64]         --
│    └─Empty: 2-1276                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1277                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-128        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1278                 [16, 128, 32, 32]         --
│    └─Empty: 2-1279                     [16, 128, 32, 32]         --
│    └─Empty: 2-1280                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1281        --                        --
│    └─One: 2-1282                       [1]                       --
│    └─OutputScale: 2-1283               --                        --
│    └─Empty: 2-1284                     [128, 128, 3, 3]          --
│    └─Empty: 2-1285                     [128, 128, 3, 3]          --
│    └─Empty: 2-1286                     [128]                     --
│    └─Empty: 2-1287                     [128]                     --
│    └─BatchNorm2d: 2-1288               [16, 128, 32, 32]         --
│    └─Scaler: 2-1289                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1290                      [16, 128, 32, 32]         --
│    └─Empty: 2-1291                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1292                     [16, 128, 32, 32]         --
├─Dropout2d: 1-129                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-130        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1293                 [16, 128, 16, 16]         --
│    └─Empty: 2-1294                     [16, 128, 16, 16]         --
│    └─Empty: 2-1295                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1296        --                        --
│    └─One: 2-1297                       [1]                       --
│    └─OutputScale: 2-1298               --                        --
│    └─Empty: 2-1299                     [128, 128, 3, 3]          --
│    └─Empty: 2-1300                     [128, 128, 3, 3]          --
│    └─Empty: 2-1301                     [128]                     --
│    └─Empty: 2-1302                     [128]                     --
│    └─BatchNorm2d: 2-1303               [16, 128, 16, 16]         --
│    └─Scaler: 2-1304                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1305                      [16, 128, 16, 16]         --
│    └─Empty: 2-1306                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1307                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-131               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1308        --                        --
│    └─One: 2-1309                       [1]                       --
│    └─OutputScale: 2-1310               --                        --
│    └─Empty: 2-1311                     [128, 128, 1, 1]          --
│    └─Empty: 2-1312                     [128, 128, 1, 1]          --
│    └─Empty: 2-1313                     [128]                     --
│    └─Empty: 2-1314                     [128]                     --
│    └─BatchNorm2d: 2-1315               [16, 128, 16, 16]         --
│    └─Scaler: 2-1316                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1317                      [16, 128, 16, 16]         --
│    └─Empty: 2-1318                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1319                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-132        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1320                 [16, 128, 16, 16]         --
│    └─Empty: 2-1321                     [16, 128, 16, 16]         --
│    └─Empty: 2-1322                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1323        --                        --
│    └─One: 2-1324                       [1]                       --
│    └─OutputScale: 2-1325               --                        --
│    └─Empty: 2-1326                     [128, 128, 3, 3]          --
│    └─Empty: 2-1327                     [128, 128, 3, 3]          --
│    └─Empty: 2-1328                     [128]                     --
│    └─Empty: 2-1329                     [128]                     --
│    └─BatchNorm2d: 2-1330               [16, 128, 16, 16]         --
│    └─Scaler: 2-1331                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1332                      [16, 128, 16, 16]         --
│    └─Empty: 2-1333                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1334                     [16, 128, 16, 16]         --
├─Dropout2d: 1-133                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-134        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1335                 [16, 128, 8, 8]           --
│    └─Empty: 2-1336                     [16, 128, 8, 8]           --
│    └─Empty: 2-1337                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1338        --                        --
│    └─One: 2-1339                       [1]                       --
│    └─OutputScale: 2-1340               --                        --
│    └─Empty: 2-1341                     [128, 128, 3, 3]          --
│    └─Empty: 2-1342                     [128, 128, 3, 3]          --
│    └─Empty: 2-1343                     [128]                     --
│    └─Empty: 2-1344                     [128]                     --
│    └─BatchNorm2d: 2-1345               [16, 128, 8, 8]           --
│    └─Scaler: 2-1346                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1347                      [16, 128, 8, 8]           --
│    └─Empty: 2-1348                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1349                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-135               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1350        --                        --
│    └─One: 2-1351                       [1]                       --
│    └─OutputScale: 2-1352               --                        --
│    └─Empty: 2-1353                     [16, 128, 1, 1]           --
│    └─Empty: 2-1354                     [16, 128, 1, 1]           --
│    └─Empty: 2-1355                     [16]                      --
│    └─Empty: 2-1356                     [16]                      --
│    └─BatchNorm2d: 2-1357               [16, 16, 8, 8]            --
│    └─Scaler: 2-1358                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1359                      [16, 16, 8, 8]            --
│    └─Empty: 2-1360                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1361                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-136        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1362                 [16, 128, 8, 8]           --
│    └─Empty: 2-1363                     [16, 128, 8, 8]           --
│    └─Empty: 2-1364                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1365        --                        --
│    └─One: 2-1366                       [1]                       --
│    └─OutputScale: 2-1367               --                        --
│    └─Empty: 2-1368                     [16, 128, 3, 3]           --
│    └─Empty: 2-1369                     [16, 128, 3, 3]           --
│    └─Empty: 2-1370                     [16]                      --
│    └─Empty: 2-1371                     [16]                      --
│    └─BatchNorm2d: 2-1372               [16, 16, 8, 8]            --
│    └─Scaler: 2-1373                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1374                      [16, 16, 8, 8]            --
│    └─Empty: 2-1375                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1376                     [16, 16, 8, 8]            --
├─Dropout2d: 1-137                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-138               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1377        --                        --
│    └─One: 2-1378                       [1]                       --
│    └─OutputScale: 2-1379               --                        --
│    └─Empty: 2-1380                     [128, 48, 1, 1]           --
│    └─Empty: 2-1381                     [128, 48, 1, 1]           --
│    └─Empty: 2-1382                     [128]                     --
│    └─Empty: 2-1383                     [128]                     --
│    └─BatchNorm2d: 2-1384               [16, 128, 64, 64]         --
│    └─Scaler: 2-1385                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1386                      [16, 128, 64, 64]         --
│    └─Empty: 2-1387                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1388                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-139        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1389                 [16, 128, 32, 32]         --
│    └─Empty: 2-1390                     [16, 128, 32, 32]         --
│    └─Empty: 2-1391                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1392        --                        --
│    └─One: 2-1393                       [1]                       --
│    └─OutputScale: 2-1394               --                        --
│    └─Empty: 2-1395                     [128, 128, 3, 3]          --
│    └─Empty: 2-1396                     [128, 128, 3, 3]          --
│    └─Empty: 2-1397                     [128]                     --
│    └─Empty: 2-1398                     [128]                     --
│    └─BatchNorm2d: 2-1399               [16, 128, 32, 32]         --
│    └─Scaler: 2-1400                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1401                      [16, 128, 32, 32]         --
│    └─Empty: 2-1402                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1403                     [16, 128, 32, 32]         --
├─Dropout2d: 1-140                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-141        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1404                 [16, 128, 16, 16]         --
│    └─Empty: 2-1405                     [16, 128, 16, 16]         --
│    └─Empty: 2-1406                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1407        --                        --
│    └─One: 2-1408                       [1]                       --
│    └─OutputScale: 2-1409               --                        --
│    └─Empty: 2-1410                     [128, 128, 3, 3]          --
│    └─Empty: 2-1411                     [128, 128, 3, 3]          --
│    └─Empty: 2-1412                     [128]                     --
│    └─Empty: 2-1413                     [128]                     --
│    └─BatchNorm2d: 2-1414               [16, 128, 16, 16]         --
│    └─Scaler: 2-1415                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1416                      [16, 128, 16, 16]         --
│    └─Empty: 2-1417                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1418                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-142               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1419        --                        --
│    └─One: 2-1420                       [1]                       --
│    └─OutputScale: 2-1421               --                        --
│    └─Empty: 2-1422                     [128, 128, 1, 1]          --
│    └─Empty: 2-1423                     [128, 128, 1, 1]          --
│    └─Empty: 2-1424                     [128]                     --
│    └─Empty: 2-1425                     [128]                     --
│    └─BatchNorm2d: 2-1426               [16, 128, 16, 16]         --
│    └─Scaler: 2-1427                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1428                      [16, 128, 16, 16]         --
│    └─Empty: 2-1429                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1430                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-143        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1431                 [16, 128, 16, 16]         --
│    └─Empty: 2-1432                     [16, 128, 16, 16]         --
│    └─Empty: 2-1433                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1434        --                        --
│    └─One: 2-1435                       [1]                       --
│    └─OutputScale: 2-1436               --                        --
│    └─Empty: 2-1437                     [128, 128, 3, 3]          --
│    └─Empty: 2-1438                     [128, 128, 3, 3]          --
│    └─Empty: 2-1439                     [128]                     --
│    └─Empty: 2-1440                     [128]                     --
│    └─BatchNorm2d: 2-1441               [16, 128, 16, 16]         --
│    └─Scaler: 2-1442                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1443                      [16, 128, 16, 16]         --
│    └─Empty: 2-1444                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1445                     [16, 128, 16, 16]         --
├─Dropout2d: 1-144                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-145        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1446                 [16, 128, 8, 8]           --
│    └─Empty: 2-1447                     [16, 128, 8, 8]           --
│    └─Empty: 2-1448                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1449        --                        --
│    └─One: 2-1450                       [1]                       --
│    └─OutputScale: 2-1451               --                        --
│    └─Empty: 2-1452                     [128, 128, 3, 3]          --
│    └─Empty: 2-1453                     [128, 128, 3, 3]          --
│    └─Empty: 2-1454                     [128]                     --
│    └─Empty: 2-1455                     [128]                     --
│    └─BatchNorm2d: 2-1456               [16, 128, 8, 8]           --
│    └─Scaler: 2-1457                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1458                      [16, 128, 8, 8]           --
│    └─Empty: 2-1459                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1460                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-146               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1461        --                        --
│    └─One: 2-1462                       [1]                       --
│    └─OutputScale: 2-1463               --                        --
│    └─Empty: 2-1464                     [16, 128, 1, 1]           --
│    └─Empty: 2-1465                     [16, 128, 1, 1]           --
│    └─Empty: 2-1466                     [16]                      --
│    └─Empty: 2-1467                     [16]                      --
│    └─BatchNorm2d: 2-1468               [16, 16, 8, 8]            --
│    └─Scaler: 2-1469                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1470                      [16, 16, 8, 8]            --
│    └─Empty: 2-1471                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1472                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-147        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1473                 [16, 128, 8, 8]           --
│    └─Empty: 2-1474                     [16, 128, 8, 8]           --
│    └─Empty: 2-1475                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1476        --                        --
│    └─One: 2-1477                       [1]                       --
│    └─OutputScale: 2-1478               --                        --
│    └─Empty: 2-1479                     [16, 128, 3, 3]           --
│    └─Empty: 2-1480                     [16, 128, 3, 3]           --
│    └─Empty: 2-1481                     [16]                      --
│    └─Empty: 2-1482                     [16]                      --
│    └─BatchNorm2d: 2-1483               [16, 16, 8, 8]            --
│    └─Scaler: 2-1484                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1485                      [16, 16, 8, 8]            --
│    └─Empty: 2-1486                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1487                     [16, 16, 8, 8]            --
├─Dropout2d: 1-148                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-149               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1488        --                        --
│    └─One: 2-1489                       [1]                       --
│    └─OutputScale: 2-1490               --                        --
│    └─Empty: 2-1491                     [128, 48, 1, 1]           --
│    └─Empty: 2-1492                     [128, 48, 1, 1]           --
│    └─Empty: 2-1493                     [128]                     --
│    └─Empty: 2-1494                     [128]                     --
│    └─BatchNorm2d: 2-1495               [16, 128, 64, 64]         --
│    └─Scaler: 2-1496                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1497                      [16, 128, 64, 64]         --
│    └─Empty: 2-1498                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1499                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-150        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1500                 [16, 128, 32, 32]         --
│    └─Empty: 2-1501                     [16, 128, 32, 32]         --
│    └─Empty: 2-1502                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1503        --                        --
│    └─One: 2-1504                       [1]                       --
│    └─OutputScale: 2-1505               --                        --
│    └─Empty: 2-1506                     [128, 128, 3, 3]          --
│    └─Empty: 2-1507                     [128, 128, 3, 3]          --
│    └─Empty: 2-1508                     [128]                     --
│    └─Empty: 2-1509                     [128]                     --
│    └─BatchNorm2d: 2-1510               [16, 128, 32, 32]         --
│    └─Scaler: 2-1511                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1512                      [16, 128, 32, 32]         --
│    └─Empty: 2-1513                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1514                     [16, 128, 32, 32]         --
├─Dropout2d: 1-151                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-152        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1515                 [16, 128, 16, 16]         --
│    └─Empty: 2-1516                     [16, 128, 16, 16]         --
│    └─Empty: 2-1517                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1518        --                        --
│    └─One: 2-1519                       [1]                       --
│    └─OutputScale: 2-1520               --                        --
│    └─Empty: 2-1521                     [128, 128, 3, 3]          --
│    └─Empty: 2-1522                     [128, 128, 3, 3]          --
│    └─Empty: 2-1523                     [128]                     --
│    └─Empty: 2-1524                     [128]                     --
│    └─BatchNorm2d: 2-1525               [16, 128, 16, 16]         --
│    └─Scaler: 2-1526                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1527                      [16, 128, 16, 16]         --
│    └─Empty: 2-1528                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1529                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-153               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1530        --                        --
│    └─One: 2-1531                       [1]                       --
│    └─OutputScale: 2-1532               --                        --
│    └─Empty: 2-1533                     [128, 128, 1, 1]          --
│    └─Empty: 2-1534                     [128, 128, 1, 1]          --
│    └─Empty: 2-1535                     [128]                     --
│    └─Empty: 2-1536                     [128]                     --
│    └─BatchNorm2d: 2-1537               [16, 128, 16, 16]         --
│    └─Scaler: 2-1538                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1539                      [16, 128, 16, 16]         --
│    └─Empty: 2-1540                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1541                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-154        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1542                 [16, 128, 16, 16]         --
│    └─Empty: 2-1543                     [16, 128, 16, 16]         --
│    └─Empty: 2-1544                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1545        --                        --
│    └─One: 2-1546                       [1]                       --
│    └─OutputScale: 2-1547               --                        --
│    └─Empty: 2-1548                     [128, 128, 3, 3]          --
│    └─Empty: 2-1549                     [128, 128, 3, 3]          --
│    └─Empty: 2-1550                     [128]                     --
│    └─Empty: 2-1551                     [128]                     --
│    └─BatchNorm2d: 2-1552               [16, 128, 16, 16]         --
│    └─Scaler: 2-1553                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1554                      [16, 128, 16, 16]         --
│    └─Empty: 2-1555                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1556                     [16, 128, 16, 16]         --
├─Dropout2d: 1-155                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-156        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1557                 [16, 128, 8, 8]           --
│    └─Empty: 2-1558                     [16, 128, 8, 8]           --
│    └─Empty: 2-1559                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1560        --                        --
│    └─One: 2-1561                       [1]                       --
│    └─OutputScale: 2-1562               --                        --
│    └─Empty: 2-1563                     [128, 128, 3, 3]          --
│    └─Empty: 2-1564                     [128, 128, 3, 3]          --
│    └─Empty: 2-1565                     [128]                     --
│    └─Empty: 2-1566                     [128]                     --
│    └─BatchNorm2d: 2-1567               [16, 128, 8, 8]           --
│    └─Scaler: 2-1568                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1569                      [16, 128, 8, 8]           --
│    └─Empty: 2-1570                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1571                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-157               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1572        --                        --
│    └─One: 2-1573                       [1]                       --
│    └─OutputScale: 2-1574               --                        --
│    └─Empty: 2-1575                     [16, 128, 1, 1]           --
│    └─Empty: 2-1576                     [16, 128, 1, 1]           --
│    └─Empty: 2-1577                     [16]                      --
│    └─Empty: 2-1578                     [16]                      --
│    └─BatchNorm2d: 2-1579               [16, 16, 8, 8]            --
│    └─Scaler: 2-1580                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1581                      [16, 16, 8, 8]            --
│    └─Empty: 2-1582                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1583                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-158        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1584                 [16, 128, 8, 8]           --
│    └─Empty: 2-1585                     [16, 128, 8, 8]           --
│    └─Empty: 2-1586                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1587        --                        --
│    └─One: 2-1588                       [1]                       --
│    └─OutputScale: 2-1589               --                        --
│    └─Empty: 2-1590                     [16, 128, 3, 3]           --
│    └─Empty: 2-1591                     [16, 128, 3, 3]           --
│    └─Empty: 2-1592                     [16]                      --
│    └─Empty: 2-1593                     [16]                      --
│    └─BatchNorm2d: 2-1594               [16, 16, 8, 8]            --
│    └─Scaler: 2-1595                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1596                      [16, 16, 8, 8]            --
│    └─Empty: 2-1597                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1598                     [16, 16, 8, 8]            --
├─Dropout2d: 1-159                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-160               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1599        --                        --
│    └─One: 2-1600                       [1]                       --
│    └─OutputScale: 2-1601               --                        --
│    └─Empty: 2-1602                     [128, 48, 1, 1]           --
│    └─Empty: 2-1603                     [128, 48, 1, 1]           --
│    └─Empty: 2-1604                     [128]                     --
│    └─Empty: 2-1605                     [128]                     --
│    └─BatchNorm2d: 2-1606               [16, 128, 64, 64]         --
│    └─Scaler: 2-1607                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1608                      [16, 128, 64, 64]         --
│    └─Empty: 2-1609                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1610                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-161        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1611                 [16, 128, 32, 32]         --
│    └─Empty: 2-1612                     [16, 128, 32, 32]         --
│    └─Empty: 2-1613                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1614        --                        --
│    └─One: 2-1615                       [1]                       --
│    └─OutputScale: 2-1616               --                        --
│    └─Empty: 2-1617                     [128, 128, 3, 3]          --
│    └─Empty: 2-1618                     [128, 128, 3, 3]          --
│    └─Empty: 2-1619                     [128]                     --
│    └─Empty: 2-1620                     [128]                     --
│    └─BatchNorm2d: 2-1621               [16, 128, 32, 32]         --
│    └─Scaler: 2-1622                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1623                      [16, 128, 32, 32]         --
│    └─Empty: 2-1624                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1625                     [16, 128, 32, 32]         --
├─Dropout2d: 1-162                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-163        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1626                 [16, 128, 16, 16]         --
│    └─Empty: 2-1627                     [16, 128, 16, 16]         --
│    └─Empty: 2-1628                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1629        --                        --
│    └─One: 2-1630                       [1]                       --
│    └─OutputScale: 2-1631               --                        --
│    └─Empty: 2-1632                     [128, 128, 3, 3]          --
│    └─Empty: 2-1633                     [128, 128, 3, 3]          --
│    └─Empty: 2-1634                     [128]                     --
│    └─Empty: 2-1635                     [128]                     --
│    └─BatchNorm2d: 2-1636               [16, 128, 16, 16]         --
│    └─Scaler: 2-1637                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1638                      [16, 128, 16, 16]         --
│    └─Empty: 2-1639                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1640                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-164               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1641        --                        --
│    └─One: 2-1642                       [1]                       --
│    └─OutputScale: 2-1643               --                        --
│    └─Empty: 2-1644                     [128, 128, 1, 1]          --
│    └─Empty: 2-1645                     [128, 128, 1, 1]          --
│    └─Empty: 2-1646                     [128]                     --
│    └─Empty: 2-1647                     [128]                     --
│    └─BatchNorm2d: 2-1648               [16, 128, 16, 16]         --
│    └─Scaler: 2-1649                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1650                      [16, 128, 16, 16]         --
│    └─Empty: 2-1651                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1652                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-165        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1653                 [16, 128, 16, 16]         --
│    └─Empty: 2-1654                     [16, 128, 16, 16]         --
│    └─Empty: 2-1655                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1656        --                        --
│    └─One: 2-1657                       [1]                       --
│    └─OutputScale: 2-1658               --                        --
│    └─Empty: 2-1659                     [128, 128, 3, 3]          --
│    └─Empty: 2-1660                     [128, 128, 3, 3]          --
│    └─Empty: 2-1661                     [128]                     --
│    └─Empty: 2-1662                     [128]                     --
│    └─BatchNorm2d: 2-1663               [16, 128, 16, 16]         --
│    └─Scaler: 2-1664                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1665                      [16, 128, 16, 16]         --
│    └─Empty: 2-1666                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1667                     [16, 128, 16, 16]         --
├─Dropout2d: 1-166                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-167        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1668                 [16, 128, 8, 8]           --
│    └─Empty: 2-1669                     [16, 128, 8, 8]           --
│    └─Empty: 2-1670                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1671        --                        --
│    └─One: 2-1672                       [1]                       --
│    └─OutputScale: 2-1673               --                        --
│    └─Empty: 2-1674                     [128, 128, 3, 3]          --
│    └─Empty: 2-1675                     [128, 128, 3, 3]          --
│    └─Empty: 2-1676                     [128]                     --
│    └─Empty: 2-1677                     [128]                     --
│    └─BatchNorm2d: 2-1678               [16, 128, 8, 8]           --
│    └─Scaler: 2-1679                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1680                      [16, 128, 8, 8]           --
│    └─Empty: 2-1681                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1682                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-168               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1683        --                        --
│    └─One: 2-1684                       [1]                       --
│    └─OutputScale: 2-1685               --                        --
│    └─Empty: 2-1686                     [16, 128, 1, 1]           --
│    └─Empty: 2-1687                     [16, 128, 1, 1]           --
│    └─Empty: 2-1688                     [16]                      --
│    └─Empty: 2-1689                     [16]                      --
│    └─BatchNorm2d: 2-1690               [16, 16, 8, 8]            --
│    └─Scaler: 2-1691                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1692                      [16, 16, 8, 8]            --
│    └─Empty: 2-1693                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1694                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-169        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1695                 [16, 128, 8, 8]           --
│    └─Empty: 2-1696                     [16, 128, 8, 8]           --
│    └─Empty: 2-1697                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1698        --                        --
│    └─One: 2-1699                       [1]                       --
│    └─OutputScale: 2-1700               --                        --
│    └─Empty: 2-1701                     [16, 128, 3, 3]           --
│    └─Empty: 2-1702                     [16, 128, 3, 3]           --
│    └─Empty: 2-1703                     [16]                      --
│    └─Empty: 2-1704                     [16]                      --
│    └─BatchNorm2d: 2-1705               [16, 16, 8, 8]            --
│    └─Scaler: 2-1706                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1707                      [16, 16, 8, 8]            --
│    └─Empty: 2-1708                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1709                     [16, 16, 8, 8]            --
├─Dropout2d: 1-170                       [16, 16, 8, 8]            --
├─FusedConv2dBNReLU: 1-171               [16, 128, 64, 64]         (recursive)
│    └─OutputShiftSqueeze: 2-1710        --                        --
│    └─One: 2-1711                       [1]                       --
│    └─OutputScale: 2-1712               --                        --
│    └─Empty: 2-1713                     [128, 48, 1, 1]           --
│    └─Empty: 2-1714                     [128, 48, 1, 1]           --
│    └─Empty: 2-1715                     [128]                     --
│    └─Empty: 2-1716                     [128]                     --
│    └─BatchNorm2d: 2-1717               [16, 128, 64, 64]         --
│    └─Scaler: 2-1718                    [16, 128, 64, 64]         --
│    └─ReLU: 2-1719                      [16, 128, 64, 64]         --
│    └─Empty: 2-1720                     [16, 128, 64, 64]         --
│    └─Clamp: 2-1721                     [16, 128, 64, 64]         --
├─FusedMaxPoolConv2dBNReLU: 1-172        [16, 128, 32, 32]         (recursive)
│    └─MaxPool2d: 2-1722                 [16, 128, 32, 32]         --
│    └─Empty: 2-1723                     [16, 128, 32, 32]         --
│    └─Empty: 2-1724                     [16, 128, 32, 32]         --
│    └─OutputShiftSqueeze: 2-1725        --                        --
│    └─One: 2-1726                       [1]                       --
│    └─OutputScale: 2-1727               --                        --
│    └─Empty: 2-1728                     [128, 128, 3, 3]          --
│    └─Empty: 2-1729                     [128, 128, 3, 3]          --
│    └─Empty: 2-1730                     [128]                     --
│    └─Empty: 2-1731                     [128]                     --
│    └─BatchNorm2d: 2-1732               [16, 128, 32, 32]         --
│    └─Scaler: 2-1733                    [16, 128, 32, 32]         --
│    └─ReLU: 2-1734                      [16, 128, 32, 32]         --
│    └─Empty: 2-1735                     [16, 128, 32, 32]         --
│    └─Clamp: 2-1736                     [16, 128, 32, 32]         --
├─Dropout2d: 1-173                       [16, 128, 32, 32]         --
├─FusedMaxPoolConv2dBNReLU: 1-174        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1737                 [16, 128, 16, 16]         --
│    └─Empty: 2-1738                     [16, 128, 16, 16]         --
│    └─Empty: 2-1739                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1740        --                        --
│    └─One: 2-1741                       [1]                       --
│    └─OutputScale: 2-1742               --                        --
│    └─Empty: 2-1743                     [128, 128, 3, 3]          --
│    └─Empty: 2-1744                     [128, 128, 3, 3]          --
│    └─Empty: 2-1745                     [128]                     --
│    └─Empty: 2-1746                     [128]                     --
│    └─BatchNorm2d: 2-1747               [16, 128, 16, 16]         --
│    └─Scaler: 2-1748                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1749                      [16, 128, 16, 16]         --
│    └─Empty: 2-1750                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1751                     [16, 128, 16, 16]         --
├─FusedConv2dBNReLU: 1-175               [16, 128, 16, 16]         (recursive)
│    └─OutputShiftSqueeze: 2-1752        --                        --
│    └─One: 2-1753                       [1]                       --
│    └─OutputScale: 2-1754               --                        --
│    └─Empty: 2-1755                     [128, 128, 1, 1]          --
│    └─Empty: 2-1756                     [128, 128, 1, 1]          --
│    └─Empty: 2-1757                     [128]                     --
│    └─Empty: 2-1758                     [128]                     --
│    └─BatchNorm2d: 2-1759               [16, 128, 16, 16]         --
│    └─Scaler: 2-1760                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1761                      [16, 128, 16, 16]         --
│    └─Empty: 2-1762                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1763                     [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-176        [16, 128, 16, 16]         (recursive)
│    └─MaxPool2d: 2-1764                 [16, 128, 16, 16]         --
│    └─Empty: 2-1765                     [16, 128, 16, 16]         --
│    └─Empty: 2-1766                     [16, 128, 16, 16]         --
│    └─OutputShiftSqueeze: 2-1767        --                        --
│    └─One: 2-1768                       [1]                       --
│    └─OutputScale: 2-1769               --                        --
│    └─Empty: 2-1770                     [128, 128, 3, 3]          --
│    └─Empty: 2-1771                     [128, 128, 3, 3]          --
│    └─Empty: 2-1772                     [128]                     --
│    └─Empty: 2-1773                     [128]                     --
│    └─BatchNorm2d: 2-1774               [16, 128, 16, 16]         --
│    └─Scaler: 2-1775                    [16, 128, 16, 16]         --
│    └─ReLU: 2-1776                      [16, 128, 16, 16]         --
│    └─Empty: 2-1777                     [16, 128, 16, 16]         --
│    └─Clamp: 2-1778                     [16, 128, 16, 16]         --
├─Dropout2d: 1-177                       [16, 128, 16, 16]         --
├─FusedMaxPoolConv2dBNReLU: 1-178        [16, 128, 8, 8]           (recursive)
│    └─MaxPool2d: 2-1779                 [16, 128, 8, 8]           --
│    └─Empty: 2-1780                     [16, 128, 8, 8]           --
│    └─Empty: 2-1781                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1782        --                        --
│    └─One: 2-1783                       [1]                       --
│    └─OutputScale: 2-1784               --                        --
│    └─Empty: 2-1785                     [128, 128, 3, 3]          --
│    └─Empty: 2-1786                     [128, 128, 3, 3]          --
│    └─Empty: 2-1787                     [128]                     --
│    └─Empty: 2-1788                     [128]                     --
│    └─BatchNorm2d: 2-1789               [16, 128, 8, 8]           --
│    └─Scaler: 2-1790                    [16, 128, 8, 8]           --
│    └─ReLU: 2-1791                      [16, 128, 8, 8]           --
│    └─Empty: 2-1792                     [16, 128, 8, 8]           --
│    └─Clamp: 2-1793                     [16, 128, 8, 8]           --
├─FusedConv2dBNReLU: 1-179               [16, 16, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1794        --                        --
│    └─One: 2-1795                       [1]                       --
│    └─OutputScale: 2-1796               --                        --
│    └─Empty: 2-1797                     [16, 128, 1, 1]           --
│    └─Empty: 2-1798                     [16, 128, 1, 1]           --
│    └─Empty: 2-1799                     [16]                      --
│    └─Empty: 2-1800                     [16]                      --
│    └─BatchNorm2d: 2-1801               [16, 16, 8, 8]            --
│    └─Scaler: 2-1802                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1803                      [16, 16, 8, 8]            --
│    └─Empty: 2-1804                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1805                     [16, 16, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-180        [16, 16, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1806                 [16, 128, 8, 8]           --
│    └─Empty: 2-1807                     [16, 128, 8, 8]           --
│    └─Empty: 2-1808                     [16, 128, 8, 8]           --
│    └─OutputShiftSqueeze: 2-1809        --                        --
│    └─One: 2-1810                       [1]                       --
│    └─OutputScale: 2-1811               --                        --
│    └─Empty: 2-1812                     [16, 128, 3, 3]           --
│    └─Empty: 2-1813                     [16, 128, 3, 3]           --
│    └─Empty: 2-1814                     [16]                      --
│    └─Empty: 2-1815                     [16]                      --
│    └─BatchNorm2d: 2-1816               [16, 16, 8, 8]            --
│    └─Scaler: 2-1817                    [16, 16, 8, 8]            --
│    └─ReLU: 2-1818                      [16, 16, 8, 8]            --
│    └─Empty: 2-1819                     [16, 16, 8, 8]            --
│    └─Clamp: 2-1820                     [16, 16, 8, 8]            --
├─Dropout2d: 1-181                       [16, 16, 8, 8]            --
├─Conv1d: 1-182                          [16, 5, 16]               25,611
│    └─OutputShiftSqueeze: 2-1821        --                        --
│    └─One: 2-1822                       [1]                       --
│    └─OutputScale: 2-1823               --                        --
│    └─Empty: 2-1824                     [5, 1024, 5]              --
│    └─Empty: 2-1825                     [5, 1024, 5]              --
│    └─Empty: 2-1826                     [5]                       --
│    └─Empty: 2-1827                     [5]                       --
│    └─Scaler: 2-1828                    [16, 5, 16]               --
│    └─Empty: 2-1829                     [16, 5, 16]               --
│    └─Empty: 2-1830                     [16, 5, 16]               --
│    └─Clamp: 2-1831                     [16, 5, 16]               --
==========================================================================================
Total params: 659,291
Trainable params: 659,237
Non-trainable params: 54
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 0.00
Params size (MB): 2.53
Estimated Total Size (MB): 203.86
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.240 | Acc: 39.500% | Wgt Acc: 49.410%
	I - Batch: 100 | Loss: 1.199 | Acc: 42.062% | Wgt Acc: 52.714%
	I - Batch: 150 | Loss: 1.172 | Acc: 44.208% | Wgt Acc: 54.934%
	I - Batch: 200 | Loss: 1.163 | Acc: 44.969% | Wgt Acc: 55.735%
I - num batch: 222
I - Train -- Loss: 1.156 | Acc: 45.729% | Wgt Acc: 56.289% | LR: 1.000000e-03 | Dur: 134.78s
I - Confusion Matrix: [row->prediction - col->label]
[[488.  30.  42. 181. 193.]
 [ 32. 348. 154.  44. 242.]
 [ 59. 160. 473.  73. 387.]
 [114.  34.  53. 235. 100.]
 [  4.   6.  12.   5.  78.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.425 | Acc: 40.631% | Wgt Acc: 53.719% | Dur: 13.92s
I - Confusion Matrix: [row->prediction - col->label]
[[45.  2.  4. 12. 20.]
 [ 6. 52. 24.  7. 70.]
 [ 2. 16. 35.  7. 43.]
 [32.  8. 12. 60. 33.]
 [ 3.  0.  0.  0. 14.]]

I - Local maximum validation set accuracy:  40.63

I - Validation set results: 
[14-1-1-0.71][50-3-3-0.55][124-2-2-0.92][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.75][615-0-3-0.99][695-1-2-0.76][722-3-0-0.99]
[826-0-3-0.90][878-0-3-0.99][1103-0-0-0.35][1212-3-3-0.98][1368-0-0-0.99][2181-2-3-0.99][2476-2-2-0.08][2721-2-2-0.89][2818-1-1-0.71][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-2-0.86][3482-2-2-0.99][3536-3-3-0.99][3625-1-1-0.99][3909-0-3-0.41][4035-0-3-0.99][4140-0-0-0.98][4214-1-1-0.82][4346-1-3-0.68]
[4581-2-1-0.92][4708-3-2-0.46][4838-3-0-0.14][4845-1-2-0.92][4868-0-0-0.99][4939-0-1-0.99][4984-2-2-0.96][5078-1-1-0.40][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.60][5843-1-1-0.99][5949-3-3-0.99][5987-2-1-0.97][6014-3-3-0.99][6033-3-0-0.98][6313-0-3-0.99][6421-3-3-0.99][6500-1-1-0.97][6583-3-2-0.94]
[6683-3-3-0.98][6825-2-3-0.79][6998-3-3-0.36][7049-3-3-0.89][7517-1-1-0.99][7521-1-1-0.88][7528-1-3-0.99][7949-1-2-0.99][8135-1-0-0.44][8185-3-0-0.99]
[8269-3-1-0.99][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.78][8672-0-0-0.99][8903-1-2-0.34][9001-2-1-0.99][9036-2-2-0.99][9281-3-2-0.53][9300-2-2-0.99]
[9571-0-3-0.95][9617-1-1-0.91][9644-2-1-0.95][9705-2-1-0.98][9801-0-3-0.99][9803-3-3-0.99][9865-3-3-0.99][9896-2-1-0.91][10314-1-1-0.98][10337-3-3-0.99]
[10403-0-1--0.29][10653-2-1-0.98][10704-2-1-0.99][10719-1-1-0.99][10727-1-1-0.99][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.96][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0--0.04][11502-3-3-0.99][11512-3-3-0.42][11608-1-1-0.99][11610-0-3-0.33][11692-0-3-0.99][11905-0-0-0.99][11993-1-1-0.99][12002-2-2-0.39]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-1-0.99][12320-1-0-0.03][12377-2-1-0.92][12398-2-3-0.83][12503-1-2-0.80][12617-0-2-0.82][12685-3-2-0.32][12738-2-3-0.60]
[12742-2-2-0.90][12823-0-3-0.99][13110-1-1-0.93][13240-3-3-0.97][13253-1-1-0.99][13273-0-0-0.99][13634-1-2-0.99][13763-2-3-0.99][13905-3-3-0.99][14060-2-1-0.99]
[14065-3-3-0.99][14147-3-3-0.96][14595-2-2-0.98][14687-2-2-0.98][14788-2-2-0.99][14869-1-1-0.96][14872-3-0-0.42][14877-1-1-0.99][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-1-0.99][15178-2-3-0.99][15375-3-3-0.86][15389-3-3-0.99][15568-2-1-0.83][15675-3-3-0.99][15869-1-3-0.43][16207-3-0-0.77][16236-0-3-0.96][16302-3-3-0.94]
[16331-2-2-0.99][16381-0-3-0.94][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-1-0.13][16801-0-0-0.99][16828-0-0-0.98][17137-3-0-0.99][17245-1-3--0.15]
[17278-3-3-0.39][17282-0-0-0.79][17311-2-2-0.88][17336-2-1-0.75][17608-3-3-0.99][17627-0-3-0.81][17877-3-1-0.99][17924-1-1-0.88][17984-3-0-0.99][18211-0-3-0.99]
[18276-3-0-0.99][18287-1-1-0.99][18394-0-3-0.98][18428-0-0-0.99][18442-0-3-0.99][18478-3-3-0.99][18607-0-0-0.85][18616-0-3-0.98][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-1-0.97][18824-2-2-0.99][18890-3-2-0.75][18930-3-1--0.10][18938-3-3-0.90][19817-1-2-0.95][19839-0-4-0.90][19930-3-3-0.99][19944-0-4-0.67][20036-2-2-0.99]
[20101-3-1-0.71][20474-1-1-0.99][20547-3-3-0.97][20929-2-2-0.99][21245-1-1-0.99][21257-3-3-0.93][21293-1-1-0.99][21316-1-3-0.39][21384-1-2-0.98][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-3-0.19][21943-3-2-0.90][21947-0-0-0.40][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.10][22025-0-3-0.84][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-2-0.35][22985-3-3-0.99][23014-0-3-0.99][23112-1-1-0.99][23144-3-3-0.99][23168-2-0-0.95]
[23219-0-0-0.99][23363-3-3-0.99][23470-0-1-0.35][23486-2-2-0.77][23497-0-3-0.99][23516-0-0-0.99][23690-1-2-0.99][23921-2-1-0.99][23936-1-2-0.60][24040-3-1-0.78]
[24111-1-1-0.99][24182-0-3-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-4-0.08][24364-1-2-0.93][24427-3-3-0.99][24477-2-2-0.96][24495-2-1-0.90][24893-2-1-0.99]
[25012-1-3--0.25][25121-2-2-0.93][25165-3-3-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.98][25644-1-1-0.99][25718-1-1-0.29][25774-2-3-0.40]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-1--0.15][26321-1-1-0.43][26732-1-1-0.97][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-1-0.65][26860-1-2-0.89]
[26948-0-0-0.78][27049-3-0-0.99][27098-1-1-0.99][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-3-0.96][27890-1-1-0.82][28040-0-2-0.17][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3-0.74][29777-0-0-0.99][29877-2-1-0.81][30035-1-1-0.99][30098-0-0-0.96][30326-1-1-0.99][30572-2-2-0.99][30716-0-1-0.69]
[30806-2-3-0.48][30906-1-1-0.99][31007-0-0-0.99][31181-3-3-0.99][31238-0-3-0.99][31347-0-3-0.99][31422-2-1-0.86][31429-3-3-0.60][31431-0-3-0.91][31432-1-1-0.99]
[31477-0-0-0.99][31524-1-2-0.96][31597-1-2-0.99][31619-1-3-0.43][31701-0-0-0.99][31755-0-0-0.98][31854-3-3-0.99][32074-1-1-0.78][32078-3-3-0.99][32111-1-1-0.88]
[32127-1-2-0.99][32140-3-3-0.99][32263-2-0--0.01][32365-0-0-0.98][32411-2-0-0.99][32429-3-3-0.99][32473-3-3-0.93][32574-3-3-0.99][32584-0-0-0.26][32622-0-1-0.40]
[32858-3-3-0.92][32969-3-0-0.99][33016-2-1-0.80][33031-1-3-0.99][33035-2-2-0.98][33133-2-1-0.85][33173-2-1-0.92][33175-3-1-0.67][33306-3-1-0.96][33309-2-3-0.97]
[33474-0-3-0.97][33478-2-3-0.82][33618-1-1-0.99][33712-0-3-0.99][33782-2-2-0.98][33914-3-3-0.95][34076-3-3-0.94][34112-2-2-0.99][34138-2-2-0.99][34239-1-1-0.98]
[34364-2-2-0.98][34617-1-2-0.99][34751-3-3-0.99][34783-2-2-0.35][35015-3-3-0.99][35018-1-1-0.99][35288-2-3-0.61][0-4-1-0.39][1-4-4-0.65][2-4-0--0.04]
[3-4-2-0.42][4-4-2-0.76][5-4-1-0.99][6-4-3-0.42][7-4-2-0.38][8-4-3-0.44][9-4-1--0.60][10-4-1-0.10][11-4-2-0.99][12-4-2-0.99]
[14-4-3-0.99][15-4-3-0.99][16-4-3-0.07][17-4-1-0.46][18-4-2-0.63][19-4-0-0.99][20-4-3-0.22][21-4-2-0.97][22-4-1-0.85][23-4-1-0.95]
[24-4-4-0.99][25-4-3-0.99][26-4-3-0.63][27-4-3-0.07][28-4-1-0.40][29-4-1-0.66][30-4-0-0.61][31-4-2-0.99][32-4-1-0.99][33-4-3-0.99]
[34-4-3-0.41][35-4-0-0.99][37-4-2-0.72][39-4-0-0.97][40-4-1-0.82][41-4-1-0.74][42-4-3-0.99][43-4-2-0.85][45-4-3-0.34][46-4-2-0.65]
[47-4-2-0.96][48-4-2-0.78][51-4-1-0.65][52-4-1-0.69][53-4-1--0.04][54-4-1-0.88][55-4-2-0.31][56-4-1-0.99][57-4-3-0.99][58-4-2-0.99]
[59-4-0-0.99][60-4-1-0.89][61-4-4-0.88][62-4-3-0.87][63-4-2-0.96][64-4-1-0.97][65-4-1-0.99][66-4-1-0.99][67-4-2-0.76][68-4-3-0.98]
[69-4-3-0.31][70-4-2-0.53][72-4-1-0.99][73-4-1-0.98][74-4-1-0.55][75-4-3-0.97][77-4-1-0.99][78-4-2-0.71][79-4-1-0.83][80-4-1-0.78]
[81-4-1-0.99][82-4-1-0.99][83-4-1-0.33][84-4-2-0.99][85-4-2-0.42][86-4-1-0.98][87-4-1-0.07][88-4-1-0.96][89-4-2-0.97][90-4-0-0.12]
[91-4-1-0.36][92-4-2-0.14][93-4-0-0.46][94-4-4-0.90][95-4-3-0.26][96-4-1-0.53][97-4-0-0.67][98-4-1-0.78][99-4-4-0.54][100-4-1-0.99]
[101-4-2-0.95][102-4-2-0.51][103-4-3-0.98][104-4-1-0.96][105-4-1-0.99][106-4-1-0.99][107-4-0-0.65][108-4-1--0.12][109-4-1-0.57][110-4-2-0.39]
[111-4-3-0.99][112-4-2-0.27][113-4-3-0.99][114-4-3-0.97][115-4-1--0.19][116-4-1-0.95][117-4-1-0.99][119-4-2-0.99][121-4-1-0.62][122-4-3-0.94]
[124-4-1-0.75][125-4-1-0.99][126-4-2-0.64][127-4-1-0.99][128-4-4--0.14][129-4-1-0.56][130-4-4-0.56][131-4-2-0.99][132-4-2-0.29][133-4-0-0.99]
[135-4-2-0.81][136-4-1-0.26][137-4-1--0.06][138-4-1-0.83][139-4-0-0.11][140-4-1-0.90][141-4-3-0.99][142-4-2-0.61][143-4-4-0.71][144-4-4-0.99]
[145-4-1-0.99][148-4-0-0.99][149-4-2-0.61][150-4-2-0.99][151-4-2-0.56][152-4-2-0.37][153-4-2-0.68][154-4-1-0.99][155-4-1-0.75][156-4-3-0.99]
[157-4-0-0.93][158-4-3-0.95][160-4-1-0.85][161-4-2-0.99][162-4-3--0.05][164-4-1-0.70][165-4-1-0.96][167-4-0-0.59][168-4-1-0.58][170-4-3-0.99]
[171-4-2-0.69][172-4-1-0.98][173-4-0-0.37][174-4-0-0.85][175-4-1-0.52][177-4-0-0.95][178-4-4-0.94][179-4-1-0.56][180-4-4-0.01][181-4-3-0.92]
[182-4-3-0.99][183-4-4-0.40][184-4-1-0.71][186-4-0-0.66][187-4-1-0.98][188-4-2-0.53][189-4-4-0.77][190-4-2-0.57][191-4-1-0.74][192-4-1-0.97]
[193-4-1-0.99][194-4-3-0.62][195-4-0-0.27][196-4-3-0.94][197-4-1-0.99][198-4-4-0.77][199-4-2-0.69]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.089 | Acc: 49.875% | Wgt Acc: 58.926%
	I - Batch: 100 | Loss: 1.069 | Acc: 51.062% | Wgt Acc: 61.648%
	I - Batch: 150 | Loss: 1.052 | Acc: 52.000% | Wgt Acc: 63.093%
	I - Batch: 200 | Loss: 1.047 | Acc: 53.156% | Wgt Acc: 63.712%
I - num batch: 222
I - Train -- Loss: 1.049 | Acc: 53.341% | Wgt Acc: 63.685% | LR: 1.000000e-03 | Dur: 136.40s
I - Confusion Matrix: [row->prediction - col->label]
[[534.  24.  42. 156. 178.]
 [ 31. 393. 117.  30. 198.]
 [ 32. 119. 507.  50. 376.]
 [ 89.  32.  41. 294.  84.]
 [ 11.  10.  27.   8. 164.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.310 | Acc: 48.126% | Wgt Acc: 54.907% | Dur: 16.61s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  2.  7. 27. 21.]
 [ 2. 41. 12.  4. 28.]
 [ 5. 26. 49. 12. 64.]
 [16.  5.  4. 40. 13.]
 [ 5.  4.  3.  3. 54.]]

I - Local maximum validation set accuracy:  48.13

I - Validation set results: 
[14-1-2-0.76][50-3-4-0.11][124-2-2-0.99][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.99][615-0-3-0.99][695-1-2-0.97][722-3-0-0.99]
[826-0-0-0.97][878-0-0-0.99][1103-0-0-0.47][1212-3-3-0.97][1368-0-0-0.99][2181-2-3-0.76][2476-2-2-0.89][2721-2-2-0.95][2818-1-3-0.91][2886-2-1-0.99]
[3231-2-2-0.99][3333-2-2-0.83][3482-2-2-0.99][3536-3-2-0.56][3625-1-1-0.99][3909-0-0-0.99][4035-0-0-0.99][4140-0-0-0.99][4214-1-3-0.72][4346-1-0-0.99]
[4581-2-2-0.98][4708-3-2-0.95][4838-3-0-0.57][4845-1-1-0.76][4868-0-0-0.99][4939-0-1-0.65][4984-2-2-0.99][5078-1-1-0.25][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.99][5843-1-1-0.88][5949-3-0-0.99][5987-2-1-0.92][6014-3-3-0.99][6033-3-2-0.67][6313-0-3-0.99][6421-3-3-0.99][6500-1-1-0.53][6583-3-2-0.99]
[6683-3-3-0.83][6825-2-0-0.56][6998-3-3-0.93][7049-3-3-0.92][7517-1-1-0.98][7521-1-1-0.54][7528-1-2-0.99][7949-1-2-0.99][8135-1-0-0.58][8185-3-0-0.99]
[8269-3-4-0.67][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.99][8903-1-2-0.85][9001-2-1-0.99][9036-2-2-0.99][9281-3-2-0.59][9300-2-2-0.99]
[9571-0-3-0.60][9617-1-1-0.73][9644-2-2-0.99][9705-2-1-0.82][9801-0-0-0.74][9803-3-3-0.73][9865-3-0-0.99][9896-2-1-0.88][10314-1-2-0.99][10337-3-3-0.99]
[10403-0-2-0.42][10653-2-2-0.51][10704-2-2-0.99][10719-1-1-0.99][10727-1-1-0.71][10836-0-0-0.99][10969-2-3-0.97][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.69][11499-0-3-0.72][11502-3-3-0.99][11512-3-3-0.90][11608-1-1-0.99][11610-0-0-0.92][11692-0-3-0.94][11905-0-0-0.99][11993-1-1-0.99][12002-2-0-0.78]
[12052-0-0-0.99][12201-0-0-0.96][12235-2-2-0.84][12320-1-4-0.84][12377-2-4-0.12][12398-2-2-0.52][12503-1-2-0.84][12617-0-2-0.32][12685-3-0--0.00][12738-2-0-0.41]
[12742-2-2-0.99][12823-0-0-0.99][13110-1-1-0.51][13240-3-3-0.95][13253-1-1-0.99][13273-0-0-0.99][13634-1-2-0.99][13763-2-2-0.97][13905-3-3--0.21][14060-2-1-0.99]
[14065-3-0-0.85][14147-3-0-0.92][14595-2-1-0.99][14687-2-2-0.99][14788-2-2-0.99][14869-1-1-0.98][14872-3-4-0.86][14877-1-1-0.99][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-1-0.98][15178-2-3-0.98][15375-3-3-0.97][15389-3-3-0.93][15568-2-1-0.89][15675-3-3-0.99][15869-1-2-0.97][16207-3-0-0.78][16236-0-2-0.93][16302-3-0-0.29]
[16331-2-2-0.99][16381-0-0-0.94][16488-1-1-0.99][16495-0-0-0.98][16650-0-0-0.99][16719-1-4-0.50][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.62][17245-1-2-0.22]
[17278-3-0-0.31][17282-0-0-0.81][17311-2-2-0.99][17336-2-2-0.98][17608-3-3-0.99][17627-0-0-0.84][17877-3-1-0.99][17924-1-2-0.97][17984-3-0-0.99][18211-0-3-0.99]
[18276-3-0-0.99][18287-1-1-0.94][18394-0-0-0.99][18428-0-0-0.99][18442-0-3-0.91][18478-3-0-0.78][18607-0-0-0.93][18616-0-4-0.85][18663-0-0-0.97][18718-0-0-0.99]
[18766-2-1-0.99][18824-2-4-0.98][18890-3-3-0.41][18930-3-2-0.51][18938-3-1--0.10][19817-1-1-0.97][19839-0-4-0.72][19930-3-3-0.99][19944-0-4-0.91][20036-2-2-0.99]
[20101-3-3-0.78][20474-1-2-0.99][20547-3-3-0.56][20929-2-2-0.99][21245-1-2-0.96][21257-3-2-0.99][21293-1-2-0.99][21316-1-1-0.94][21384-1-2-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-3-0.97][21943-3-2-0.99][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.99][21998-1-2-0.36][22025-0-3-0.99][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-2-0.97][22985-3-0-0.99][23014-0-0-0.99][23112-1-1-0.76][23144-3-3-0.99][23168-2-0-0.73]
[23219-0-0-0.75][23363-3-3-0.99][23470-0-1-0.51][23486-2-2-0.72][23497-0-3-0.99][23516-0-0-0.99][23690-1-4-0.58][23921-2-2-0.79][23936-1-2-0.91][24040-3-1-0.46]
[24111-1-4-0.99][24182-0-0-0.99][24238-3-3-0.80][24290-2-0-0.99][24345-0-0-0.99][24364-1-2-0.89][24427-3-0-0.99][24477-2-2-0.99][24495-2-2--0.03][24893-2-2-0.99]
[25012-1-2-0.37][25121-2-4-0.99][25165-3-3-0.81][25183-0-0-0.99][25297-3-3-0.94][25398-0-0-0.99][25574-2-2-0.99][25644-1-1-0.99][25718-1-1-0.66][25774-2-2-0.99]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-2-0.50][26321-1-2-0.85][26732-1-1-0.54][26784-3-3-0.99][26827-3-0-0.63][26833-0-3-0.99][26838-2-2-0.43][26860-1-2-0.51]
[26948-0-0-0.96][27049-3-0-0.99][27098-1-1-0.83][27526-0-0-0.91][27639-3-0-0.93][27698-3-3-0.99][27772-0-3-0.99][27890-1-1-0.93][28040-0-2-0.75][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3-0.56][29777-0-0-0.99][29877-2-2-0.68][30035-1-1-0.98][30098-0-0-0.99][30326-1-1-0.99][30572-2-2-0.99][30716-0-4-0.99]
[30806-2-2-0.38][30906-1-1-0.99][31007-0-0-0.93][31181-3-0-0.99][31238-0-3-0.99][31347-0-0-0.99][31422-2-2-0.99][31429-3-3-0.80][31431-0-0-0.42][31432-1-1-0.92]
[31477-0-0-0.99][31524-1-2-0.38][31597-1-2-0.99][31619-1-3-0.89][31701-0-0-0.99][31755-0-0-0.98][31854-3-3-0.94][32074-1-3-0.48][32078-3-3-0.99][32111-1-1-0.98]
[32127-1-2-0.99][32140-3-3-0.99][32263-2-0-0.73][32365-0-0-0.99][32411-2-0-0.99][32429-3-0-0.68][32473-3-0-0.99][32574-3-0-0.99][32584-0-4-0.48][32622-0-3-0.76]
[32858-3-0-0.71][32969-3-0-0.99][33016-2-2-0.98][33031-1-3-0.99][33035-2-2-0.99][33133-2-2-0.99][33173-2-2-0.98][33175-3-1-0.97][33306-3-2-0.99][33309-2-3-0.86]
[33474-0-3-0.88][33478-2-1-0.14][33618-1-1-0.99][33712-0-0-0.98][33782-2-1-0.99][33914-3-3-0.99][34076-3-2-0.81][34112-2-2-0.98][34138-2-2-0.99][34239-1-1-0.86]
[34364-2-1-0.99][34617-1-2-0.98][34751-3-3-0.99][34783-2-2-0.97][35015-3-2-0.99][35018-1-2-0.99][35288-2-2-0.64][0-4-2-0.99][1-4-4-0.99][2-4-4-0.67]
[3-4-4-0.89][4-4-2-0.12][5-4-3-0.85][6-4-4-0.99][7-4-4-0.99][8-4-2-0.91][9-4-2-0.78][10-4-4-0.95][11-4-2-0.99][12-4-1-0.71]
[14-4-3-0.22][15-4-0-0.94][16-4-4-0.93][17-4-1-0.59][18-4-1-0.77][19-4-0-0.99][20-4-2-0.22][21-4-2-0.94][22-4-4-0.99][23-4-2-0.43]
[24-4-4-0.99][25-4-3-0.60][26-4-1-0.21][27-4-2-0.98][28-4-1-0.79][29-4-1-0.63][30-4-0-0.27][31-4-2-0.95][32-4-4-0.99][33-4-2-0.99]
[34-4-2-0.82][35-4-3-0.99][37-4-2-0.79][39-4-0-0.99][40-4-0--0.02][41-4-2-0.96][42-4-2-0.99][43-4-2-0.99][45-4-2-0.83][46-4-4-0.99]
[47-4-4-0.99][48-4-2-0.87][51-4-4-0.81][52-4-4-0.60][53-4-4-0.29][54-4-0--0.06][55-4-2-0.94][56-4-4-0.85][57-4-3-0.24][58-4-2-0.98]
[59-4-0-0.79][60-4-1-0.21][61-4-4-0.99][62-4-2-0.99][63-4-2-0.99][64-4-2-0.70][65-4-4-0.99][66-4-4-0.99][67-4-2-0.60][68-4-1-0.99]
[69-4-0-0.91][70-4-2-0.99][72-4-1-0.98][73-4-2-0.84][74-4-2-0.99][75-4-0-0.94][77-4-4-0.99][78-4-2-0.88][79-4-2-0.98][80-4-4-0.99]
[81-4-2-0.99][82-4-1-0.97][83-4-1-0.83][84-4-2-0.99][85-4-4-0.99][86-4-4-0.14][87-4-4-0.99][88-4-1-0.83][89-4-2-0.97][90-4-0--0.12]
[91-4-2-0.78][92-4-2-0.84][93-4-4-0.48][94-4-4-0.99][95-4-2-0.54][96-4-1-0.99][97-4-2-0.21][98-4-2-0.56][99-4-4-0.78][100-4-1-0.99]
[101-4-4-0.99][102-4-2-0.90][103-4-3-0.53][104-4-4-0.99][105-4-4-0.99][106-4-4-0.99][107-4-0-0.75][108-4-2-0.25][109-4-1-0.74][110-4-4-0.91]
[111-4-0-0.99][112-4-4-0.99][113-4-2-0.91][114-4-2-0.94][115-4-4-0.93][116-4-2-0.71][117-4-4-0.99][119-4-2-0.99][121-4-1-0.97][122-4-4-0.97]
[124-4-3-0.86][125-4-1-0.99][126-4-4-0.99][127-4-2-0.99][128-4-0-0.38][129-4-2-0.97][130-4-2-0.62][131-4-2-0.99][132-4-3-0.28][133-4-4-0.99]
[135-4-3-0.63][136-4-0-0.93][137-4-3--0.11][138-4-2-0.72][139-4-4-0.83][140-4-1-0.92][141-4-3-0.97][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.99][148-4-0-0.99][149-4-3-0.44][150-4-2-0.88][151-4-4-0.99][152-4-2-0.62][153-4-2-0.99][154-4-2-0.96][155-4-4-0.92][156-4-4-0.31]
[157-4-2-0.97][158-4-2-0.96][160-4-1-0.39][161-4-2-0.32][162-4-2-0.50][164-4-2-0.87][165-4-4-0.93][167-4-0-0.71][168-4-4-0.99][170-4-0-0.86]
[171-4-1-0.99][172-4-4-0.94][173-4-0-0.82][174-4-0-0.97][175-4-4-0.56][177-4-0-0.95][178-4-4-0.99][179-4-2-0.27][180-4-4-0.99][181-4-3-0.68]
[182-4-2-0.95][183-4-4-0.98][184-4-1-0.43][186-4-0-0.29][187-4-1-0.99][188-4-2-0.86][189-4-1-0.71][190-4-2-0.51][191-4-2-0.98][192-4-1-0.91]
[193-4-1-0.99][194-4-2-0.61][195-4-1-0.78][196-4-1-0.15][197-4-1-0.99][198-4-4-0.99][199-4-2-0.98]
---------------------------
I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.011 | Acc: 57.750% | Wgt Acc: 68.077%
	I - Batch: 100 | Loss: 1.025 | Acc: 55.312% | Wgt Acc: 65.534%
	I - Batch: 150 | Loss: 1.030 | Acc: 55.083% | Wgt Acc: 64.926%
	I - Batch: 200 | Loss: 1.023 | Acc: 55.531% | Wgt Acc: 65.439%
I - num batch: 222
I - Train -- Loss: 1.024 | Acc: 55.822% | Wgt Acc: 65.709% | LR: 1.000000e-03 | Dur: 142.19s
I - Confusion Matrix: [row->prediction - col->label]
[[528.  24.  37. 120. 172.]
 [ 38. 397. 110.  31. 181.]
 [ 34. 113. 511.  46. 311.]
 [ 87.  30.  50. 336. 128.]
 [ 10.  14.  26.   5. 208.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.356 | Acc: 45.168% | Wgt Acc: 51.021% | Dur: 14.99s
I - Confusion Matrix: [row->prediction - col->label]
[[41.  2.  1.  6. 13.]
 [ 2. 29.  7.  5. 29.]
 [10. 36. 55. 21. 68.]
 [31.  4.  6. 50. 16.]
 [ 4.  7.  6.  4. 54.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.012 | Acc: 56.625% | Wgt Acc: 64.904%
	I - Batch: 100 | Loss: 1.007 | Acc: 56.625% | Wgt Acc: 65.900%
	I - Batch: 150 | Loss: 0.996 | Acc: 57.125% | Wgt Acc: 66.273%
	I - Batch: 200 | Loss: 0.991 | Acc: 57.438% | Wgt Acc: 66.597%
I - num batch: 222
I - Train -- Loss: 0.996 | Acc: 57.034% | Wgt Acc: 66.593% | LR: 1.000000e-03 | Dur: 145.33s
I - Confusion Matrix: [row->prediction - col->label]
[[534.  25.  30. 136. 148.]
 [ 22. 401.  94.  25. 163.]
 [ 38.  97. 532.  41. 346.]
 [ 93.  39.  45. 326. 113.]
 [ 10.  16.  33.  10. 230.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.318 | Acc: 49.112% | Wgt Acc: 57.663% | Dur: 14.45s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  4. 22. 22.]
 [ 6. 53. 21. 10. 50.]
 [ 2. 15. 43. 11. 42.]
 [13.  3.  2. 42. 18.]
 [ 4.  3.  5.  1. 48.]]

I - Local maximum validation set accuracy:  49.11

I - Validation set results: 
[14-1-1-0.87][50-3-4-0.62][124-2-2--0.07][127-0-0-0.99][443-2-2-0.99][567-0-0-0.86][573-1-1-0.99][615-0-3-0.97][695-1-2-0.21][722-3-0-0.97]
[826-0-0-0.99][878-0-0-0.99][1103-0-0-0.67][1212-3-3-0.23][1368-0-0-0.99][2181-2-2-0.08][2476-2-2-0.98][2721-2-2-0.83][2818-1-3-0.11][2886-2-4-0.89]
[3231-2-1-0.99][3333-2-1-0.97][3482-2-2-0.99][3536-3-3-0.91][3625-1-1-0.99][3909-0-0-0.43][4035-0-0-0.34][4140-0-0-0.99][4214-1-3-0.99][4346-1-0-0.66]
[4581-2-1-0.99][4708-3-2-0.99][4838-3-0-0.24][4845-1-2-0.86][4868-0-0-0.99][4939-0-1-0.76][4984-2-2-0.99][5078-1-1-0.04][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.72][5843-1-1-0.99][5949-3-3-0.95][5987-2-4-0.99][6014-3-1-0.99][6033-3-0--0.03][6313-0-0-0.96][6421-3-3-0.95][6500-1-1-0.34][6583-3-2-0.68]
[6683-3-3-0.39][6825-2-1-0.99][6998-3-2-0.31][7049-3-2-0.37][7517-1-1-0.94][7521-1-1-0.99][7528-1-3-0.96][7949-1-2-0.59][8135-1-0-0.99][8185-3-0-0.98]
[8269-3-1-0.11][8273-3-3-0.01][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.84][8903-1-1-0.93][9001-2-1-0.99][9036-2-2-0.99][9281-3-2-0.53][9300-2-2-0.99]
[9571-0-3-0.54][9617-1-1-0.99][9644-2-1-0.99][9705-2-1-0.81][9801-0-3-0.96][9803-3-1-0.54][9865-3-3-0.99][9896-2-2-0.72][10314-1-4-0.73][10337-3-3-0.99]
[10403-0-4--0.13][10653-2-1-0.83][10704-2-1-0.99][10719-1-1-0.99][10727-1-4-0.96][10836-0-0-0.99][10969-2-2-0.62][11042-0-0-0.87][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.16][11502-3-3-0.98][11512-3-2-0.20][11608-1-1-0.99][11610-0-0-0.99][11692-0-3-0.92][11905-0-0-0.99][11993-1-1-0.87][12002-2-2-0.32]
[12052-0-0-0.99][12201-0-3-0.94][12235-2-1-0.97][12320-1-0-0.88][12377-2-4-0.56][12398-2-1-0.80][12503-1-2-0.51][12617-0-1-0.92][12685-3-1--0.05][12738-2-1-0.64]
[12742-2-2-0.99][12823-0-0-0.93][13110-1-1-0.99][13240-3-3-0.46][13253-1-1-0.91][13273-0-0-0.99][13634-1-1-0.17][13763-2-3-0.65][13905-3-1-0.02][14060-2-1-0.99]
[14065-3-0-0.25][14147-3-0-0.85][14595-2-2-0.92][14687-2-2-0.99][14788-2-2-0.99][14869-1-1-0.98][14872-3-0--0.03][14877-1-1-0.99][14927-0-3-0.69][15066-0-0-0.99]
[15175-1-1-0.99][15178-2-3-0.26][15375-3-0-0.80][15389-3-3-0.99][15568-2-1-0.99][15675-3-3-0.97][15869-1-2-0.95][16207-3-0-0.81][16236-0-2-0.59][16302-3-3-0.58]
[16331-2-2-0.99][16381-0-0-0.26][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-2-0.32][16801-0-0-0.99][16828-0-0-0.99][17137-3-0--0.15][17245-1-1-0.91]
[17278-3-0-0.22][17282-0-0-0.61][17311-2-2-0.98][17336-2-1-0.48][17608-3-3-0.99][17627-0-0-0.41][17877-3-1-0.99][17924-1-2-0.92][17984-3-0-0.99][18211-0-1-0.94]
[18276-3-0-0.58][18287-1-1-0.95][18394-0-0-0.96][18428-0-0-0.99][18442-0-3-0.72][18478-3-0--0.10][18607-0-0-0.99][18616-0-0-0.55][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.97][18824-2-4-0.98][18890-3-3-0.79][18930-3-2-0.09][18938-3-3-0.79][19817-1-1-0.99][19839-0-4-0.98][19930-3-3-0.34][19944-0-4-0.47][20036-2-2-0.99]
[20101-3-3-0.03][20474-1-1-0.99][20547-3-0-0.58][20929-2-2-0.99][21245-1-2-0.99][21257-3-3--0.16][21293-1-1-0.99][21316-1-1-0.99][21384-1-1-0.88][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.97][21714-0-1-0.01][21943-3-2-0.99][21947-0-0-0.55][21948-0-0-0.99][21965-2-2-0.86][21998-1-1-0.58][22025-0-3-0.62][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.98][22757-0-0-0.99][22811-3-3-0.26][22976-3-1-0.99][22985-3-3-0.99][23014-0-0-0.99][23112-1-1-0.99][23144-3-3-0.93][23168-2-0-0.96]
[23219-0-0-0.22][23363-3-3-0.74][23470-0-0-0.47][23486-2-2-0.87][23497-0-3-0.99][23516-0-0-0.99][23690-1-1-0.96][23921-2-2-0.65][23936-1-2-0.98][24040-3-0--0.14]
[24111-1-4-0.99][24182-0-0-0.96][24238-3-3-0.99][24290-2-0-0.97][24345-0-0-0.99][24364-1-2-0.97][24427-3-3-0.86][24477-2-2-0.99][24495-2-1-0.79][24893-2-2-0.94]
[25012-1-1-0.03][25121-2-2-0.99][25165-3-3-0.45][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.91][25574-2-2-0.99][25644-1-1-0.99][25718-1-1-0.99][25774-2-2-0.99]
[26032-3-3-0.03][26051-3-3-0.99][26120-0-2-0.59][26321-1-1-0.94][26732-1-1-0.96][26784-3-3-0.99][26827-3-3-0.38][26833-0-0-0.98][26838-2-1-0.73][26860-1-2-0.53]
[26948-0-0-0.77][27049-3-0-0.99][27098-1-1-0.98][27526-0-0-0.93][27639-3-3-0.95][27698-3-3-0.95][27772-0-0-0.99][27890-1-1-0.87][28040-0-0-0.65][28503-2-2-0.99]
[28577-1-1-0.97][28959-0-0-0.99][29198-3-1-0.77][29777-0-0-0.99][29877-2-1-0.75][30035-1-1-0.98][30098-0-3-0.58][30326-1-1-0.99][30572-2-2-0.99][30716-0-4-0.97]
[30806-2-2-0.33][30906-1-1-0.99][31007-0-0-0.67][31181-3-3-0.38][31238-0-3-0.24][31347-0-0-0.99][31422-2-2-0.98][31429-3-1-0.67][31431-0-3-0.56][31432-1-1-0.99]
[31477-0-0-0.99][31524-1-2-0.81][31597-1-2-0.99][31619-1-0-0.10][31701-0-0--0.12][31755-0-0-0.24][31854-3-3-0.37][32074-1-1-0.46][32078-3-3-0.90][32111-1-1-0.99]
[32127-1-2-0.93][32140-3-3-0.97][32263-2-0-0.84][32365-0-0-0.99][32411-2-0-0.99][32429-3-0-0.40][32473-3-0-0.99][32574-3-0-0.99][32584-0-3--0.02][32622-0-1-0.49]
[32858-3-3--0.28][32969-3-0-0.99][33016-2-2-0.99][33031-1-1-0.70][33035-2-2-0.99][33133-2-2-0.92][33173-2-1-0.51][33175-3-1-0.98][33306-3-2-0.77][33309-2-2-0.63]
[33474-0-1-0.63][33478-2-1-0.42][33618-1-1-0.99][33712-0-0-0.99][33782-2-2-0.99][33914-3-3-0.99][34076-3-2-0.58][34112-2-2-0.99][34138-2-1-0.71][34239-1-1-0.44]
[34364-2-2-0.99][34617-1-2-0.36][34751-3-3-0.99][34783-2-4-0.66][35015-3-2-0.93][35018-1-1-0.97][35288-2-2-0.47][0-4-4-0.77][1-4-3-0.27][2-4-1--0.00]
[3-4-0-0.04][4-4-0-0.65][5-4-1-0.99][6-4-4-0.16][7-4-4-0.34][8-4-2-0.97][9-4-2-0.94][10-4-3-0.35][11-4-2-0.99][12-4-2-0.94]
[14-4-3-0.38][15-4-3-0.99][16-4-0-0.92][17-4-1-0.09][18-4-4-0.85][19-4-3-0.90][20-4-1-0.91][21-4-2-0.72][22-4-4-0.99][23-4-2-0.53]
[24-4-4-0.98][25-4-3-0.46][26-4-1-0.45][27-4-2-0.97][28-4-1-0.95][29-4-1-0.95][30-4-0-0.90][31-4-2-0.99][32-4-1-0.99][33-4-2-0.95]
[34-4-2-0.28][35-4-1--0.17][37-4-2-0.77][39-4-0-0.99][40-4-4-0.29][41-4-1-0.87][42-4-3-0.98][43-4-2-0.53][45-4-3-0.78][46-4-4-0.99]
[47-4-4-0.99][48-4-2-0.53][51-4-4-0.99][52-4-1-0.97][53-4-1-0.69][54-4-3-0.35][55-4-2-0.97][56-4-1-0.99][57-4-3-0.97][58-4-2-0.99]
[59-4-0-0.81][60-4-1-0.99][61-4-4-0.76][62-4-2-0.99][63-4-2-0.99][64-4-0-0.53][65-4-4-0.98][66-4-4-0.92][67-4-4-0.88][68-4-2-0.78]
[69-4-0-0.22][70-4-4-0.62][72-4-4-0.85][73-4-1-0.74][74-4-2-0.96][75-4-2-0.44][77-4-4-0.99][78-4-2-0.98][79-4-2-0.99][80-4-4-0.99]
[81-4-4-0.70][82-4-1-0.96][83-4-1-0.93][84-4-2-0.99][85-4-4-0.99][86-4-4-0.94][87-4-4-0.97][88-4-1-0.47][89-4-2-0.99][90-4-0-0.15]
[91-4-1-0.77][92-4-3-0.42][93-4-0-0.99][94-4-4-0.99][95-4-1--0.20][96-4-1-0.83][97-4-4-0.95][98-4-1-0.78][99-4-4-0.38][100-4-4-0.90]
[101-4-4-0.99][102-4-1-0.68][103-4-3-0.49][104-4-4-0.99][105-4-4-0.95][106-4-4-0.23][107-4-1-0.33][108-4-2-0.73][109-4-1-0.87][110-4-4-0.92]
[111-4-0-0.99][112-4-0-0.98][113-4-3-0.13][114-4-2-0.07][115-4-0-0.80][116-4-1-0.32][117-4-1-0.84][119-4-2-0.77][121-4-4-0.91][122-4-3-0.94]
[124-4-1-0.96][125-4-4-0.99][126-4-4-0.34][127-4-1-0.12][128-4-1-0.49][129-4-1-0.91][130-4-1-0.44][131-4-2-0.98][132-4-1--0.02][133-4-0-0.99]
[135-4-2-0.83][136-4-1-0.98][137-4-1-0.55][138-4-1-0.85][139-4-2--0.32][140-4-1-0.85][141-4-3-0.98][142-4-4-0.99][143-4-4-0.90][144-4-4-0.99]
[145-4-1-0.99][148-4-0-0.99][149-4-2-0.93][150-4-2-0.99][151-4-1-0.68][152-4-1-0.48][153-4-2-0.99][154-4-2-0.96][155-4-1-0.58][156-4-3-0.34]
[157-4-0-0.73][158-4-2-0.77][160-4-4-0.83][161-4-1-0.88][162-4-1-0.40][164-4-2-0.69][165-4-1-0.65][167-4-0-0.99][168-4-4-0.99][170-4-3-0.56]
[171-4-1-0.96][172-4-4-0.98][173-4-4-0.49][174-4-0-0.91][175-4-4-0.45][177-4-0-0.82][178-4-2-0.76][179-4-1-0.96][180-4-4-0.99][181-4-3-0.89]
[182-4-2-0.97][183-4-4-0.63][184-4-2-0.58][186-4-0-0.15][187-4-1-0.92][188-4-2-0.58][189-4-4-0.99][190-4-0-0.36][191-4-4-0.88][192-4-1-0.55]
[193-4-2-0.99][194-4-2-0.97][195-4-0-0.58][196-4-1-0.97][197-4-1-0.90][198-4-4-0.97][199-4-4-0.97]
---------------------------
I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 0.979 | Acc: 60.500% | Wgt Acc: 69.973%
	I - Batch: 100 | Loss: 0.983 | Acc: 59.250% | Wgt Acc: 68.424%
	I - Batch: 150 | Loss: 0.977 | Acc: 58.917% | Wgt Acc: 68.683%
	I - Batch: 200 | Loss: 0.988 | Acc: 58.094% | Wgt Acc: 67.864%
I - num batch: 222
I - Train -- Loss: 0.989 | Acc: 57.880% | Wgt Acc: 67.861% | LR: 1.000000e-03 | Dur: 143.77s
I - Confusion Matrix: [row->prediction - col->label]
[[545.  27.  38. 140. 196.]
 [ 22. 436.  96.  36. 150.]
 [ 42.  84. 535.  41. 315.]
 [ 81.  23.  45. 314. 116.]
 [  7.   8.  20.   7. 223.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.272 | Acc: 51.085% | Wgt Acc: 56.868% | Dur: 15.48s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  6.  3. 15. 22.]
 [ 1. 26.  5.  0.  5.]
 [ 1. 30. 48.  9. 64.]
 [20. 10. 13. 59. 24.]
 [ 5.  6.  6.  3. 65.]]

I - Local maximum validation set accuracy:  51.08

I - Validation set results: 
[14-1-2-0.92][50-3-3-0.94][124-2-3-0.48][127-0-0-0.99][443-2-2-0.99][567-0-0-0.93][573-1-1-0.99][615-0-3-0.99][695-1-2-0.37][722-3-0-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-0-0.69][1212-3-3-0.55][1368-0-0-0.99][2181-2-3-0.99][2476-2-2-0.48][2721-2-2-0.91][2818-1-3-0.97][2886-2-2-0.98]
[3231-2-2-0.99][3333-2-2-0.95][3482-2-2-0.99][3536-3-3-0.99][3625-1-1-0.99][3909-0-0-0.99][4035-0-3-0.99][4140-0-0-0.99][4214-1-3-0.99][4346-1-3-0.99]
[4581-2-2-0.99][4708-3-2-0.99][4838-3-2-0.61][4845-1-2-0.99][4868-0-0-0.99][4939-0-4-0.73][4984-2-2-0.99][5078-1-2-0.50][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.99][5843-1-2-0.96][5949-3-3-0.99][5987-2-4-0.72][6014-3-3-0.99][6033-3-2-0.53][6313-0-3-0.99][6421-3-3-0.99][6500-1-3-0.16][6583-3-2-0.23]
[6683-3-3-0.99][6825-2-1-0.70][6998-3-3-0.96][7049-3-3-0.99][7517-1-1-0.97][7521-1-1-0.46][7528-1-1-0.41][7949-1-4-0.98][8135-1-0-0.99][8185-3-0-0.99]
[8269-3-4-0.69][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.83][8672-0-0-0.99][8903-1-2-0.45][9001-2-2-0.33][9036-2-2-0.99][9281-3-2-0.82][9300-2-2-0.99]
[9571-0-3-0.99][9617-1-3-0.31][9644-2-2-0.99][9705-2-0-0.49][9801-0-3-0.51][9803-3-3-0.99][9865-3-3-0.99][9896-2-2-0.71][10314-1-2-0.58][10337-3-3-0.99]
[10403-0-2-0.73][10653-2-2-0.81][10704-2-2-0.99][10719-1-1-0.99][10727-1-4-0.78][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.99][11502-3-3-0.98][11512-3-3-0.98][11608-1-2-0.99][11610-0-0-0.99][11692-0-0-0.99][11905-0-0-0.99][11993-1-2-0.25][12002-2-3-0.55]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-2-0.70][12320-1-0-0.55][12377-2-4-0.72][12398-2-3-0.99][12503-1-2-0.98][12617-0-3-0.99][12685-3-3-0.99][12738-2-0-0.86]
[12742-2-2-0.99][12823-0-3-0.99][13110-1-1-0.94][13240-3-3-0.96][13253-1-1-0.43][13273-0-0-0.99][13634-1-4-0.42][13763-2-3-0.99][13905-3-3-0.84][14060-2-2-0.79]
[14065-3-3-0.99][14147-3-3-0.99][14595-2-2-0.99][14687-2-2-0.99][14788-2-2-0.99][14869-1-1-0.93][14872-3-0-0.36][14877-1-1-0.96][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-1-0.64][15178-2-3-0.31][15375-3-3-0.99][15389-3-3-0.99][15568-2-1-0.70][15675-3-3-0.99][15869-1-2-0.99][16207-3-0-0.78][16236-0-0-0.73][16302-3-3-0.99]
[16331-2-2-0.99][16381-0-0-0.63][16488-1-1-0.41][16495-0-0-0.99][16650-0-0-0.99][16719-1-2-0.98][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.99][17245-1-3--0.24]
[17278-3-0-0.20][17282-0-0-0.60][17311-2-2-0.99][17336-2-2-0.93][17608-3-3-0.99][17627-0-0-0.76][17877-3-4-0.69][17924-1-1--0.02][17984-3-0-0.99][18211-0-3-0.99]
[18276-3-0-0.97][18287-1-2-0.24][18394-0-0-0.99][18428-0-0-0.09][18442-0-3-0.99][18478-3-3-0.55][18607-0-0-0.99][18616-0-0-0.46][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.93][18824-2-4-0.99][18890-3-3-0.97][18930-3-4-0.91][18938-3-3-0.76][19817-1-2-0.93][19839-0-0-0.99][19930-3-3-0.99][19944-0-4-0.46][20036-2-2-0.99]
[20101-3-3-0.99][20474-1-1-0.99][20547-3-0-0.83][20929-2-2-0.99][21245-1-2-0.33][21257-3-2-0.32][21293-1-2-0.69][21316-1-1-0.99][21384-1-2-0.99][21448-1-2-0.92]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-0-0.97][21943-3-2-0.99][21947-0-4-0.26][21948-0-0-0.99][21965-2-2-0.67][21998-1-1-0.92][22025-0-3-0.99][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-2-0.61][22985-3-3-0.99][23014-0-3-0.99][23112-1-1-0.39][23144-3-3-0.99][23168-2-3-0.93]
[23219-0-0-0.99][23363-3-3-0.99][23470-0-0-0.93][23486-2-2-0.85][23497-0-3-0.99][23516-0-0-0.99][23690-1-3-0.12][23921-2-2-0.34][23936-1-2-0.98][24040-3-0-0.98]
[24111-1-4-0.99][24182-0-3-0.99][24238-3-3-0.99][24290-2-0-0.98][24345-0-0-0.84][24364-1-2-0.81][24427-3-3-0.99][24477-2-2-0.99][24495-2-3-0.11][24893-2-2-0.98]
[25012-1-2-0.74][25121-2-4-0.99][25165-3-3-0.92][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.95][25644-1-2-0.89][25718-1-4-0.72][25774-2-2-0.79]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.95][26321-1-1-0.64][26732-1-0-0.05][26784-3-3-0.99][26827-3-3-0.69][26833-0-3-0.99][26838-2-4-0.05][26860-1-2-0.41]
[26948-0-0-0.67][27049-3-0-0.99][27098-1-0-0.71][27526-0-0-0.79][27639-3-3-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.08][28040-0-0-0.97][28503-2-2-0.99]
[28577-1-2-0.99][28959-0-0-0.99][29198-3-3-0.98][29777-0-0-0.99][29877-2-1-0.59][30035-1-2-0.61][30098-0-0-0.99][30326-1-1-0.99][30572-2-2-0.99][30716-0-4-0.99]
[30806-2-3-0.97][30906-1-4-0.99][31007-0-0-0.99][31181-3-3-0.98][31238-0-3-0.96][31347-0-0-0.99][31422-2-2-0.39][31429-3-3-0.10][31431-0-0-0.70][31432-1-1-0.99]
[31477-0-3-0.99][31524-1-3-0.98][31597-1-2-0.99][31619-1-0-0.59][31701-0-0-0.96][31755-0-0-0.99][31854-3-3-0.99][32074-1-3-0.97][32078-3-3-0.99][32111-1-2-0.32]
[32127-1-2-0.99][32140-3-3-0.99][32263-2-4-0.55][32365-0-0-0.99][32411-2-3-0.97][32429-3-3-0.99][32473-3-3-0.99][32574-3-0-0.99][32584-0-0-0.75][32622-0-1-0.90]
[32858-3-0-0.99][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.99][33133-2-2-0.81][33173-2-1-0.31][33175-3-2-0.99][33306-3-3-0.99][33309-2-3-0.99]
[33474-0-3-0.75][33478-2-1-0.02][33618-1-1-0.99][33712-0-3-0.91][33782-2-2-0.99][33914-3-3-0.99][34076-3-3-0.99][34112-2-2-0.99][34138-2-3-0.81][34239-1-0-0.16]
[34364-2-2-0.99][34617-1-2-0.86][34751-3-3-0.99][34783-2-2-0.71][35015-3-3-0.99][35018-1-2-0.73][35288-2-2-0.48][0-4-2-0.99][1-4-4-0.99][2-4-4-0.92]
[3-4-4-0.98][4-4-2-0.90][5-4-3-0.99][6-4-4-0.76][7-4-4-0.94][8-4-3-0.68][9-4-2-0.82][10-4-4-0.54][11-4-2-0.99][12-4-2-0.37]
[14-4-3-0.99][15-4-0-0.98][16-4-4-0.99][17-4-0-0.33][18-4-4-0.99][19-4-3-0.99][20-4-2-0.93][21-4-2-0.99][22-4-4-0.99][23-4-2-0.31]
[24-4-4-0.99][25-4-3-0.95][26-4-3--0.20][27-4-2-0.18][28-4-4-0.96][29-4-1-0.21][30-4-0-0.51][31-4-2-0.99][32-4-2-0.98][33-4-2-0.95]
[34-4-0-0.66][35-4-3-0.98][37-4-2-0.22][39-4-0-0.99][40-4-0-0.97][41-4-4-0.58][42-4-3-0.99][43-4-2-0.86][45-4-2-0.92][46-4-4-0.99]
[47-4-2-0.99][48-4-4-0.91][51-4-4-0.99][52-4-0-0.17][53-4-2-0.94][54-4-3-0.86][55-4-3-0.80][56-4-2-0.62][57-4-3-0.99][58-4-2-0.99]
[59-4-4-0.90][60-4-2-0.31][61-4-4-0.99][62-4-3-0.59][63-4-2-0.78][64-4-2-0.98][65-4-4-0.93][66-4-4-0.99][67-4-1-0.46][68-4-3--0.21]
[69-4-2-0.73][70-4-4-0.93][72-4-0-0.97][73-4-4-0.97][74-4-2-0.95][75-4-2-0.85][77-4-4-0.99][78-4-2-0.56][79-4-2-0.99][80-4-4-0.99]
[81-4-2-0.99][82-4-4-0.84][83-4-2-0.40][84-4-4-0.94][85-4-2-0.99][86-4-2-0.35][87-4-4-0.99][88-4-4-0.61][89-4-2-0.92][90-4-4-0.69]
[91-4-1-0.20][92-4-3-0.98][93-4-0-0.99][94-4-2-0.99][95-4-3-0.24][96-4-4-0.97][97-4-4-0.58][98-4-2-0.76][99-4-4-0.28][100-4-1-0.85]
[101-4-4-0.83][102-4-4-0.53][103-4-3-0.99][104-4-4-0.99][105-4-4-0.99][106-4-4-0.99][107-4-0-0.98][108-4-2-0.41][109-4-4-0.85][110-4-4-0.98]
[111-4-3-0.95][112-4-4-0.93][113-4-4-0.30][114-4-3-0.02][115-4-4-0.99][116-4-2-0.82][117-4-4-0.98][119-4-2-0.99][121-4-2-0.99][122-4-4-0.93]
[124-4-2-0.76][125-4-2-0.99][126-4-4-0.99][127-4-2-0.61][128-4-0-0.97][129-4-4-0.06][130-4-2-0.57][131-4-0-0.62][132-4-0-0.96][133-4-4-0.99]
[135-4-2-0.81][136-4-0-0.01][137-4-2--0.17][138-4-0-0.88][139-4-4-0.99][140-4-2-0.98][141-4-3-0.94][142-4-4-0.99][143-4-4-0.99][144-4-4-0.94]
[145-4-2-0.99][148-4-0-0.99][149-4-2-0.84][150-4-2-0.99][151-4-4-0.99][152-4-4-0.99][153-4-4-0.72][154-4-2-0.98][155-4-4-0.99][156-4-3-0.62]
[157-4-2-0.69][158-4-3-0.94][160-4-2-0.28][161-4-2-0.77][162-4-2--0.20][164-4-2-0.88][165-4-4-0.98][167-4-4-0.67][168-4-4-0.86][170-4-3-0.99]
[171-4-3-0.17][172-4-4-0.99][173-4-4-0.90][174-4-0-0.97][175-4-4-0.95][177-4-0-0.98][178-4-4-0.52][179-4-4-0.75][180-4-4-0.99][181-4-2-0.60]
[182-4-3-0.99][183-4-4-0.99][184-4-2-0.99][186-4-0-0.58][187-4-1-0.68][188-4-2-0.68][189-4-0-0.99][190-4-2-0.05][191-4-2-0.94][192-4-0-0.78]
[193-4-2-0.98][194-4-0-0.42][195-4-2-0.70][196-4-2-0.77][197-4-2-0.76][198-4-4-0.99][199-4-2-0.73]
---------------------------
I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 0.948 | Acc: 60.500% | Wgt Acc: 70.468%
	I - Batch: 100 | Loss: 0.947 | Acc: 61.312% | Wgt Acc: 70.604%
	I - Batch: 150 | Loss: 0.953 | Acc: 60.750% | Wgt Acc: 70.500%
	I - Batch: 200 | Loss: 0.947 | Acc: 60.906% | Wgt Acc: 71.137%
I - num batch: 222
I - Train -- Loss: 0.946 | Acc: 61.009% | Wgt Acc: 71.086% | LR: 1.000000e-03 | Dur: 146.11s
I - Confusion Matrix: [row->prediction - col->label]
[[544.  22.  22. 111. 162.]
 [ 23. 437.  83.  22. 168.]
 [ 30.  77. 571.  37. 316.]
 [ 85.  27.  34. 359. 101.]
 [ 15.  15.  24.   9. 253.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.341 | Acc: 48.323% | Wgt Acc: 56.522% | Dur: 14.18s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5.  3. 21. 36.]
 [ 4. 42. 19.  7. 26.]
 [ 1. 14. 36.  3. 37.]
 [15. 12. 14. 53. 32.]
 [ 3.  5.  3.  2. 49.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 0.913 | Acc: 62.500% | Wgt Acc: 72.523%
	I - Batch: 100 | Loss: 0.926 | Acc: 62.500% | Wgt Acc: 72.184%
	I - Batch: 150 | Loss: 0.938 | Acc: 61.500% | Wgt Acc: 71.313%
	I - Batch: 200 | Loss: 0.935 | Acc: 61.531% | Wgt Acc: 71.393%
I - num batch: 222
I - Train -- Loss: 0.936 | Acc: 61.545% | Wgt Acc: 71.387% | LR: 1.000000e-03 | Dur: 135.23s
I - Confusion Matrix: [row->prediction - col->label]
[[535.  13.  27. 103. 176.]
 [ 34. 452.  71.  28. 141.]
 [ 29.  80. 562.  32. 313.]
 [ 85.  20.  46. 366. 102.]
 [ 14.  13.  28.   9. 268.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.252 | Acc: 52.663% | Wgt Acc: 59.301% | Dur: 15.32s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  6.  3. 16. 24.]
 [ 2. 37. 13.  2. 22.]
 [ 2. 19. 44.  7. 52.]
 [16.  9.  9. 58. 18.]
 [ 4.  7.  6.  3. 64.]]

I - Local maximum validation set accuracy:  52.66

I - Validation set results: 
[14-1-2-0.52][50-3-4-0.49][124-2-3-0.13][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.99][615-0-3-0.99][695-1-2-0.99][722-3-3-0.65]
[826-0-0-0.99][878-0-0-0.98][1103-0-0-0.94][1212-3-3-0.88][1368-0-0-0.99][2181-2-3-0.99][2476-2-2-0.99][2721-2-2-0.99][2818-1-3-0.99][2886-2-1-0.83]
[3231-2-2-0.99][3333-2-2-0.22][3482-2-2-0.99][3536-3-3-0.96][3625-1-1-0.99][3909-0-0-0.99][4035-0-3-0.99][4140-0-0-0.99][4214-1-1-0.22][4346-1-3-0.99]
[4581-2-2-0.99][4708-3-2-0.87][4838-3-3-0.50][4845-1-2-0.88][4868-0-0-0.99][4939-0-2-0.00][4984-2-2-0.97][5078-1-3-0.52][5396-0-0-0.99][5479-1-1-0.98]
[5717-0-0-0.99][5843-1-1-0.99][5949-3-3-0.99][5987-2-4-0.87][6014-3-3-0.99][6033-3-2-0.66][6313-0-0-0.99][6421-3-3-0.99][6500-1-3--0.08][6583-3-2-0.80]
[6683-3-3-0.43][6825-2-1-0.89][6998-3-3-0.66][7049-3-3-0.88][7517-1-1-0.64][7521-1-0-0.71][7528-1-3-0.62][7949-1-2-0.99][8135-1-0-0.80][8185-3-0-0.99]
[8269-3-1-0.25][8273-3-3-0.98][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.99][8903-1-2-0.20][9001-2-4-0.31][9036-2-2-0.99][9281-3-3--0.10][9300-2-2-0.99]
[9571-0-3-0.98][9617-1-4-0.27][9644-2-2-0.99][9705-2-4-0.96][9801-0-3-0.99][9803-3-3-0.99][9865-3-0-0.99][9896-2-2-0.83][10314-1-2-0.01][10337-3-3-0.99]
[10403-0-4-0.99][10653-2-1-0.55][10704-2-1-0.70][10719-1-1-0.83][10727-1-4-0.98][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.91][11499-0-0-0.82][11502-3-3-0.99][11512-3-0--0.20][11608-1-1-0.99][11610-0-0-0.50][11692-0-3-0.99][11905-0-0-0.99][11993-1-1-0.89][12002-2-2-0.76]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-2-0.95][12320-1-0-0.49][12377-2-4-0.63][12398-2-3-0.99][12503-1-1-0.94][12617-0-3-0.27][12685-3-3-0.64][12738-2-3-0.58]
[12742-2-2-0.99][12823-0-0-0.99][13110-1-3-0.95][13240-3-3-0.97][13253-1-1-0.93][13273-0-0-0.99][13634-1-4--0.35][13763-2-3-0.98][13905-3-3-0.99][14060-2-1-0.99]
[14065-3-3-0.93][14147-3-0-0.97][14595-2-2-0.99][14687-2-2-0.99][14788-2-2-0.99][14869-1-1-0.59][14872-3-2-0.25][14877-1-1-0.21][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-3--0.34][15178-2-2-0.64][15375-3-3-0.53][15389-3-3-0.99][15568-2-1-0.49][15675-3-3-0.99][15869-1-2-0.69][16207-3-0-0.99][16236-0-0-0.97][16302-3-2-0.86]
[16331-2-2-0.99][16381-0-0-0.99][16488-1-1-0.86][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.99][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.99][17245-1-1-0.46]
[17278-3-4-0.44][17282-0-0-0.80][17311-2-2-0.99][17336-2-1-0.34][17608-3-3-0.99][17627-0-0-0.72][17877-3-0-0.73][17924-1-3-0.65][17984-3-0-0.99][18211-0-3-0.99]
[18276-3-3-0.99][18287-1-1-0.62][18394-0-0-0.99][18428-0-0-0.83][18442-0-0-0.99][18478-3-0-0.95][18607-0-0-0.99][18616-0-0-0.99][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-1-0.98][18824-2-4-0.45][18890-3-3-0.98][18930-3-2-0.33][18938-3-3-0.60][19817-1-2-0.81][19839-0-4-0.96][19930-3-3-0.99][19944-0-4-0.84][20036-2-2-0.99]
[20101-3-1-0.80][20474-1-1-0.93][20547-3-3-0.96][20929-2-2-0.99][21245-1-2-0.63][21257-3-3-0.51][21293-1-1-0.96][21316-1-1-0.33][21384-1-4-0.99][21448-1-1-0.64]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-3-0.76][21943-3-3-0.16][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.90][21998-1-0-0.11][22025-0-3-0.99][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-2-0.17][22985-3-0-0.99][23014-0-0-0.99][23112-1-1-0.92][23144-3-3-0.99][23168-2-3-0.99]
[23219-0-0-0.97][23363-3-3-0.99][23470-0-0-0.19][23486-2-2-0.75][23497-0-3-0.99][23516-0-0-0.99][23690-1-4-0.94][23921-2-2-0.83][23936-1-2-0.85][24040-3-3-0.12]
[24111-1-4-0.99][24182-0-3-0.92][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.99][24364-1-2-0.72][24427-3-0-0.99][24477-2-2-0.96][24495-2-1-0.96][24893-2-2-0.99]
[25012-1-2--0.22][25121-2-2-0.97][25165-3-3-0.93][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.99][25644-1-1-0.99][25718-1-1-0.99][25774-2-3-0.80]
[26032-3-0-0.99][26051-3-3-0.99][26120-0-0-0.75][26321-1-1-0.83][26732-1-1-0.89][26784-3-3-0.99][26827-3-3-0.83][26833-0-3-0.99][26838-2-2-0.07][26860-1-2-0.83]
[26948-0-0-0.98][27049-3-0-0.99][27098-1-0-0.99][27526-0-0-0.99][27639-3-3-0.88][27698-3-3-0.99][27772-0-3-0.99][27890-1-1-0.77][28040-0-2-0.97][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3-0.77][29777-0-0-0.99][29877-2-2-0.39][30035-1-2-0.99][30098-0-0-0.99][30326-1-1-0.99][30572-2-2-0.99][30716-0-4-0.99]
[30806-2-2-0.75][30906-1-1-0.99][31007-0-0-0.97][31181-3-3-0.89][31238-0-3-0.92][31347-0-0-0.99][31422-2-1-0.02][31429-3-3-0.72][31431-0-0-0.89][31432-1-1-0.98]
[31477-0-0-0.99][31524-1-2-0.55][31597-1-2-0.85][31619-1-0-0.99][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.99][32074-1-2-0.39][32078-3-3-0.99][32111-1-1-0.60]
[32127-1-2-0.88][32140-3-3-0.99][32263-2-2-0.89][32365-0-0-0.99][32411-2-0-0.99][32429-3-3-0.99][32473-3-3-0.99][32574-3-3-0.99][32584-0-0-0.99][32622-0-1-0.22]
[32858-3-0-0.98][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.99][33133-2-2-0.99][33173-2-1-0.98][33175-3-4-0.79][33306-3-3-0.78][33309-2-3-0.99]
[33474-0-1-0.30][33478-2-0-0.57][33618-1-1-0.99][33712-0-0-0.85][33782-2-1-0.87][33914-3-3-0.79][34076-3-3-0.86][34112-2-2-0.99][34138-2-1-0.18][34239-1-1-0.38]
[34364-2-2-0.98][34617-1-2-0.20][34751-3-3-0.99][34783-2-4-0.80][35015-3-3-0.99][35018-1-1-0.48][35288-2-2-0.51][0-4-2-0.98][1-4-4-0.99][2-4-4-0.89]
[3-4-4-0.90][4-4-2-0.99][5-4-1-0.88][6-4-4-0.96][7-4-4-0.40][8-4-2-0.97][9-4-1-0.80][10-4-4-0.99][11-4-2-0.99][12-4-1-0.74]
[14-4-3-0.95][15-4-0-0.99][16-4-4-0.78][17-4-3-0.11][18-4-4-0.99][19-4-3-0.99][20-4-0-0.54][21-4-2-0.47][22-4-4-0.93][23-4-2--0.47]
[24-4-4-0.99][25-4-3-0.69][26-4-1--0.04][27-4-2-0.94][28-4-4-0.71][29-4-1-0.87][30-4-4--0.00][31-4-4-0.84][32-4-1-0.90][33-4-2-0.99]
[34-4-2-0.77][35-4-3-0.99][37-4-2-0.57][39-4-0-0.99][40-4-4-0.99][41-4-3-0.15][42-4-3-0.38][43-4-2-0.99][45-4-2-0.76][46-4-4-0.98]
[47-4-4-0.99][48-4-2-0.95][51-4-4-0.99][52-4-2-0.59][53-4-4--0.22][54-4-3-0.99][55-4-2-0.99][56-4-1-0.58][57-4-3-0.99][58-4-2-0.64]
[59-4-0-0.99][60-4-1-0.60][61-4-4-0.98][62-4-2-0.99][63-4-2-0.99][64-4-2-0.99][65-4-4-0.97][66-4-4-0.96][67-4-4-0.69][68-4-1-0.76]
[69-4-2-0.37][70-4-4--0.12][72-4-2-0.52][73-4-1--0.03][74-4-2-0.67][75-4-2-0.98][77-4-4-0.99][78-4-3-0.91][79-4-2-0.99][80-4-4-0.54]
[81-4-2-0.64][82-4-1-0.20][83-4-1-0.58][84-4-4-0.99][85-4-4-0.99][86-4-4-0.98][87-4-4-0.99][88-4-4-0.89][89-4-2-0.76][90-4-0-0.66]
[91-4-2-0.44][92-4-0-0.72][93-4-0-0.92][94-4-4-0.99][95-4-2-0.30][96-4-4-0.85][97-4-2-0.19][98-4-2-0.88][99-4-4-0.78][100-4-1-0.82]
[101-4-4-0.99][102-4-2-0.30][103-4-3-0.99][104-4-4-0.99][105-4-1-0.76][106-4-4-0.99][107-4-4-0.97][108-4-2-0.68][109-4-1-0.54][110-4-0-0.26]
[111-4-0-0.99][112-4-2-0.99][113-4-3-0.13][114-4-0-0.44][115-4-4-0.29][116-4-2-0.73][117-4-4-0.54][119-4-2-0.99][121-4-4-0.90][122-4-3-0.98]
[124-4-1-0.34][125-4-4-0.95][126-4-4-0.99][127-4-2-0.99][128-4-0-0.40][129-4-1-0.99][130-4-4-0.91][131-4-2-0.98][132-4-3-0.56][133-4-0-0.98]
[135-4-2-0.94][136-4-1-0.98][137-4-4-0.18][138-4-2-0.62][139-4-2-0.88][140-4-4-0.19][141-4-3-0.69][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-2-0.99][148-4-0-0.99][149-4-4-0.92][150-4-4-0.60][151-4-2-0.95][152-4-0-0.14][153-4-4-0.40][154-4-4-0.99][155-4-4-0.96][156-4-3-0.96]
[157-4-0-0.99][158-4-2-0.95][160-4-0-0.86][161-4-2-0.46][162-4-2--0.33][164-4-0-0.10][165-4-4-0.93][167-4-4-0.33][168-4-2-0.75][170-4-0-0.85]
[171-4-1-0.55][172-4-4-0.90][173-4-0-0.96][174-4-0-0.99][175-4-4-0.73][177-4-0-0.90][178-4-2-0.78][179-4-0-0.63][180-4-4-0.99][181-4-2-0.03]
[182-4-2-0.99][183-4-4-0.84][184-4-2-0.99][186-4-0--0.06][187-4-1-0.72][188-4-4-0.98][189-4-4-0.99][190-4-1-0.61][191-4-2-0.95][192-4-4-0.74]
[193-4-1-0.99][194-4-3-0.99][195-4-0--0.11][196-4-2-0.64][197-4-4-0.73][198-4-4-0.99][199-4-3-0.05]
---------------------------
I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 0.948 | Acc: 60.500% | Wgt Acc: 70.547%
	I - Batch: 100 | Loss: 0.933 | Acc: 61.500% | Wgt Acc: 72.323%
	I - Batch: 150 | Loss: 0.923 | Acc: 62.167% | Wgt Acc: 72.885%
	I - Batch: 200 | Loss: 0.928 | Acc: 62.125% | Wgt Acc: 72.616%
I - num batch: 222
I - Train -- Loss: 0.928 | Acc: 61.714% | Wgt Acc: 72.440% | LR: 1.000000e-03 | Dur: 142.70s
I - Confusion Matrix: [row->prediction - col->label]
[[569.  18.  19. 104. 170.]
 [ 30. 449.  78.  28. 174.]
 [ 19.  73. 571.  34. 283.]
 [ 66.  22.  39. 363. 136.]
 [ 13.  16.  27.   9. 237.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.254 | Acc: 51.282% | Wgt Acc: 55.288% | Dur: 15.19s
I - Confusion Matrix: [row->prediction - col->label]
[[74.  6.  8. 32. 38.]
 [ 3. 36.  7.  6. 20.]
 [ 3. 24. 41. 10. 33.]
 [ 5.  7.  9. 37. 17.]
 [ 3.  5. 10.  1. 72.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 0.895 | Acc: 67.125% | Wgt Acc: 75.763%
	I - Batch: 100 | Loss: 0.912 | Acc: 64.062% | Wgt Acc: 73.665%
	I - Batch: 150 | Loss: 0.924 | Acc: 63.250% | Wgt Acc: 72.984%
	I - Batch: 200 | Loss: 0.921 | Acc: 63.500% | Wgt Acc: 73.388%
I - num batch: 222
I - Train -- Loss: 0.918 | Acc: 63.716% | Wgt Acc: 73.717% | LR: 1.000000e-03 | Dur: 134.04s
I - Confusion Matrix: [row->prediction - col->label]
[[555.  15.  28. 111. 199.]
 [ 26. 457.  63.  16. 151.]
 [ 27.  65. 592.  29. 271.]
 [ 66.  24.  28. 373.  96.]
 [ 23.  17.  23.   9. 283.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.348 | Acc: 47.337% | Wgt Acc: 57.444% | Dur: 14.39s
I - Confusion Matrix: [row->prediction - col->label]
[[50.  3.  4. 13. 35.]
 [ 1. 39. 12.  3. 20.]
 [ 7. 23. 50.  7. 62.]
 [30.  9.  7. 62. 24.]
 [ 0.  4.  2.  1. 39.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 0.909 | Acc: 61.875% | Wgt Acc: 72.317%
	I - Batch: 100 | Loss: 0.902 | Acc: 63.062% | Wgt Acc: 72.954%
	I - Batch: 150 | Loss: 0.909 | Acc: 63.000% | Wgt Acc: 72.885%
	I - Batch: 200 | Loss: 0.904 | Acc: 63.438% | Wgt Acc: 73.215%
I - num batch: 222
I - Train -- Loss: 0.904 | Acc: 63.547% | Wgt Acc: 73.300% | LR: 1.000000e-03 | Dur: 135.94s
I - Confusion Matrix: [row->prediction - col->label]
[[548.  21.  28. 108. 175.]
 [ 22. 459.  60.  26. 128.]
 [ 24.  74. 599.  38. 283.]
 [ 87.  13.  21. 359. 125.]
 [ 16.  11.  26.   7. 289.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.237 | Acc: 49.704% | Wgt Acc: 55.968% | Dur: 18.89s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  3.  2. 15. 24.]
 [ 1. 45. 21.  7. 29.]
 [ 4. 16. 32.  4. 50.]
 [17.  6. 12. 55. 16.]
 [ 7.  8.  8.  5. 61.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 0.860 | Acc: 66.875% | Wgt Acc: 76.472%
	I - Batch: 100 | Loss: 0.844 | Acc: 67.250% | Wgt Acc: 77.874%
	I - Batch: 150 | Loss: 0.831 | Acc: 68.792% | Wgt Acc: 78.902%
	I - Batch: 200 | Loss: 0.823 | Acc: 69.750% | Wgt Acc: 79.560%
I - num batch: 222
I - Train -- Loss: 0.823 | Acc: 69.383% | Wgt Acc: 79.296% | LR: 5.000000e-04 | Dur: 132.57s
I - Confusion Matrix: [row->prediction - col->label]
[[577.  14.  20.  79. 172.]
 [ 16. 498.  44.  14. 125.]
 [ 23.  45. 632.  27. 239.]
 [ 65.   9.  16. 410. 120.]
 [ 16.  12.  22.   8. 344.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.176 | Acc: 55.819% | Wgt Acc: 60.777% | Dur: 14.17s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  3.  2. 13. 21.]
 [ 2. 41. 11.  3. 23.]
 [ 3. 16. 45.  6. 37.]
 [17. 11. 11. 62. 21.]
 [ 9.  7.  6.  2. 78.]]

I - Local maximum validation set accuracy:  55.82

I - Validation set results: 
[14-1-2-0.68][50-3-3-0.99][124-2-2-0.72][127-0-0-0.99][443-2-2-0.99][567-0-0-0.96][573-1-1-0.99][615-0-3-0.99][695-1-2-0.99][722-3-0-0.62]
[826-0-0-0.64][878-0-3-0.98][1103-0-0-0.51][1212-3-3-0.89][1368-0-0-0.99][2181-2-3-0.99][2476-2-2-0.99][2721-2-2-0.99][2818-1-3-0.39][2886-2-2-0.94]
[3231-2-2-0.99][3333-2-2-0.99][3482-2-2-0.99][3536-3-3-0.49][3625-1-1-0.99][3909-0-0-0.91][4035-0-3-0.99][4140-0-0-0.99][4214-1-3-0.99][4346-1-3-0.96]
[4581-2-2-0.98][4708-3-2-0.86][4838-3-3--0.12][4845-1-3-0.61][4868-0-0-0.99][4939-0-1-0.14][4984-2-2-0.99][5078-1-4-0.53][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.95][5843-1-1-0.99][5949-3-3-0.99][5987-2-4-0.99][6014-3-3-0.99][6033-3-3-0.98][6313-0-0-0.99][6421-3-3-0.99][6500-1-3-0.23][6583-3-2-0.99]
[6683-3-3-0.73][6825-2-1-0.99][6998-3-3-0.31][7049-3-3-0.99][7517-1-2-0.51][7521-1-1-0.42][7528-1-3-0.94][7949-1-2-0.97][8135-1-0-0.99][8185-3-0-0.99]
[8269-3-4-0.58][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.82][8672-0-0-0.93][8903-1-2-0.99][9001-2-1-0.99][9036-2-2-0.99][9281-3-1-0.90][9300-2-2-0.99]
[9571-0-3-0.93][9617-1-1-0.99][9644-2-2-0.99][9705-2-4-0.93][9801-0-3-0.96][9803-3-3-0.95][9865-3-3-0.99][9896-2-2-0.85][10314-1-2-0.60][10337-3-3-0.99]
[10403-0-4-0.83][10653-2-4-0.59][10704-2-1-0.81][10719-1-1-0.80][10727-1-4-0.99][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.71][11502-3-3-0.99][11512-3-3-0.90][11608-1-1-0.99][11610-0-0-0.94][11692-0-0-0.99][11905-0-0-0.99][11993-1-1-0.99][12002-2-2-0.99]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-1-0.66][12320-1-0-0.95][12377-2-4-0.97][12398-2-3-0.99][12503-1-4-0.70][12617-0-2-0.99][12685-3-3-0.93][12738-2-3-0.94]
[12742-2-2-0.99][12823-0-0-0.97][13110-1-1-0.96][13240-3-3-0.99][13253-1-1-0.97][13273-0-0-0.99][13634-1-3-0.98][13763-2-3-0.99][13905-3-3-0.90][14060-2-1-0.99]
[14065-3-3-0.96][14147-3-3-0.98][14595-2-2-0.98][14687-2-2-0.99][14788-2-2-0.99][14869-1-1-0.97][14872-3-0-0.89][14877-1-1-0.65][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-1-0.81][15178-2-4-0.93][15375-3-0-0.56][15389-3-3-0.99][15568-2-1-0.99][15675-3-3-0.99][15869-1-3-0.47][16207-3-0-0.76][16236-0-3-0.99][16302-3-2-0.72]
[16331-2-2-0.99][16381-0-3-0.94][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-2-0.41][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.86][17245-1-2-0.49]
[17278-3-3-0.26][17282-0-0-0.87][17311-2-2-0.99][17336-2-1-0.23][17608-3-3-0.99][17627-0-2--0.08][17877-3-1-0.92][17924-1-2-0.54][17984-3-0-0.99][18211-0-3-0.86]
[18276-3-3-0.99][18287-1-1-0.78][18394-0-0-0.99][18428-0-2--0.24][18442-0-3-0.99][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.85][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.98][18824-2-2-0.99][18890-3-3-0.99][18930-3-3-0.32][18938-3-3-0.99][19817-1-1-0.99][19839-0-4-0.97][19930-3-3-0.99][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-3-0.99][20474-1-1-0.99][20547-3-3-0.93][20929-2-2-0.99][21245-1-1-0.86][21257-3-3-0.91][21293-1-2-0.91][21316-1-1-0.99][21384-1-4-0.95][21448-1-1-0.91]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-3-0.99][21943-3-3-0.99][21947-0-0-0.61][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.37][22025-0-4-0.54][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-1-0.99][22985-3-3-0.99][23014-0-0-0.99][23112-1-1-0.99][23144-3-3-0.99][23168-2-3-0.87]
[23219-0-0-0.64][23363-3-3-0.99][23470-0-0-0.85][23486-2-2-0.74][23497-0-3-0.99][23516-0-0-0.99][23690-1-4-0.99][23921-2-2-0.92][23936-1-2-0.96][24040-3-4-0.99]
[24111-1-4-0.99][24182-0-0-0.92][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.50][24364-1-2-0.39][24427-3-0-0.96][24477-2-2-0.99][24495-2-1-0.71][24893-2-2-0.99]
[25012-1-3-0.54][25121-2-4-0.83][25165-3-3-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.66][25644-1-1-0.99][25718-1-4-0.99][25774-2-3-0.93]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.56][26321-1-1-0.92][26732-1-1-0.54][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-3-0.52][26860-1-2-0.99]
[26948-0-0-0.83][27049-3-0-0.99][27098-1-1-0.72][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-0-0.75][27890-1-1-0.99][28040-0-4-0.43][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3-0.99][29777-0-0-0.99][29877-2-2-0.97][30035-1-1-0.43][30098-0-3-0.99][30326-1-1-0.99][30572-2-2-0.99][30716-0-4-0.99]
[30806-2-2-0.99][30906-1-1-0.99][31007-0-4-0.69][31181-3-3-0.99][31238-0-3-0.99][31347-0-0-0.99][31422-2-2-0.99][31429-3-3--0.24][31431-0-0-0.77][31432-1-1-0.82]
[31477-0-0-0.99][31524-1-2-0.66][31597-1-2-0.86][31619-1-0-0.97][31701-0-0-0.59][31755-0-0-0.77][31854-3-3-0.99][32074-1-1-0.13][32078-3-3-0.99][32111-1-1-0.88]
[32127-1-1-0.95][32140-3-3-0.99][32263-2-0-0.76][32365-0-0-0.99][32411-2-3-0.99][32429-3-3-0.97][32473-3-3-0.99][32574-3-0-0.99][32584-0-4-0.77][32622-0-1-0.88]
[32858-3-0-0.99][32969-3-0-0.99][33016-2-2-0.98][33031-1-3-0.99][33035-2-2-0.99][33133-2-2-0.84][33173-2-2-0.80][33175-3-2--0.06][33306-3-3-0.99][33309-2-3-0.99]
[33474-0-3-0.18][33478-2-1-0.79][33618-1-1-0.99][33712-0-0-0.99][33782-2-1-0.99][33914-3-3-0.99][34076-3-2-0.94][34112-2-2-0.73][34138-2-3-0.96][34239-1-3-0.52]
[34364-2-2-0.99][34617-1-2-0.16][34751-3-3-0.99][34783-2-1-0.50][35015-3-2-0.99][35018-1-1-0.45][35288-2-2-0.39][0-4-4-0.42][1-4-4-0.99][2-4-4-0.36]
[3-4-4-0.99][4-4-2-0.97][5-4-1-0.29][6-4-0-0.69][7-4-4-0.59][8-4-2-0.40][9-4-0-0.39][10-4-4-0.99][11-4-2-0.99][12-4-1-0.61]
[14-4-3-0.90][15-4-3-0.99][16-4-4-0.99][17-4-4-0.46][18-4-4-0.95][19-4-3-0.99][20-4-0--0.14][21-4-2-0.27][22-4-4-0.78][23-4-4--0.04]
[24-4-4-0.99][25-4-3-0.89][26-4-3-0.41][27-4-2-0.99][28-4-4-0.99][29-4-1-0.99][30-4-0-0.74][31-4-4-0.91][32-4-1-0.99][33-4-2-0.99]
[34-4-4-0.60][35-4-0-0.41][37-4-2-0.99][39-4-0-0.99][40-4-0-0.05][41-4-2-0.76][42-4-1-0.21][43-4-2-0.87][45-4-2-0.99][46-4-4-0.99]
[47-4-2-0.99][48-4-4-0.67][51-4-4-0.99][52-4-0--0.35][53-4-4-0.16][54-4-3-0.99][55-4-2-0.99][56-4-1-0.63][57-4-0-0.99][58-4-2-0.28]
[59-4-0-0.99][60-4-4-0.92][61-4-4-0.90][62-4-3-0.96][63-4-2-0.99][64-4-2-0.67][65-4-4-0.98][66-4-4-0.99][67-4-2-0.97][68-4-3-0.94]
[69-4-3-0.56][70-4-4-0.91][72-4-1-0.95][73-4-1-0.71][74-4-2-0.99][75-4-0-0.96][77-4-4-0.99][78-4-2-0.99][79-4-2-0.96][80-4-4-0.81]
[81-4-4-0.94][82-4-1-0.99][83-4-4-0.01][84-4-4-0.99][85-4-4-0.99][86-4-4-0.97][87-4-4-0.99][88-4-1-0.98][89-4-2-0.64][90-4-3-0.03]
[91-4-1-0.32][92-4-3-0.10][93-4-0-0.98][94-4-4-0.99][95-4-3-0.55][96-4-0-0.40][97-4-4-0.38][98-4-2-0.70][99-4-4-0.32][100-4-2-0.85]
[101-4-4-0.99][102-4-2-0.28][103-4-3-0.99][104-4-4-0.99][105-4-1-0.62][106-4-1-0.99][107-4-1-0.99][108-4-2-0.98][109-4-4-0.74][110-4-4-0.96]
[111-4-0-0.99][112-4-4-0.65][113-4-3-0.53][114-4-2-0.45][115-4-4-0.61][116-4-4-0.27][117-4-4-0.99][119-4-2-0.95][121-4-1-0.99][122-4-4-0.99]
[124-4-1-0.35][125-4-4-0.99][126-4-4-0.97][127-4-1-0.11][128-4-0-0.03][129-4-1-0.99][130-4-1-0.08][131-4-2-0.98][132-4-2-0.72][133-4-0-0.99]
[135-4-2-0.58][136-4-1-0.99][137-4-4-0.05][138-4-2-0.60][139-4-4-0.99][140-4-4-0.90][141-4-3-0.24][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.99][148-4-0-0.99][149-4-4-0.99][150-4-4-0.99][151-4-4-0.81][152-4-4-0.98][153-4-2-0.97][154-4-4-0.99][155-4-4-0.99][156-4-3-0.73]
[157-4-0-0.84][158-4-3-0.41][160-4-2-0.74][161-4-2-0.99][162-4-4-0.03][164-4-3-0.97][165-4-4-0.99][167-4-0-0.97][168-4-4-0.95][170-4-2--0.16]
[171-4-2-0.99][172-4-4-0.69][173-4-4-0.96][174-4-0-0.99][175-4-4-0.96][177-4-4-0.82][178-4-4-0.95][179-4-4-0.10][180-4-4-0.99][181-4-3-0.99]
[182-4-3-0.99][183-4-4-0.99][184-4-2-0.99][186-4-0-0.42][187-4-4-0.55][188-4-4-0.99][189-4-4-0.99][190-4-1-0.85][191-4-4-0.89][192-4-4-0.98]
[193-4-1-0.95][194-4-3--0.00][195-4-4-0.92][196-4-4-0.77][197-4-1-0.27][198-4-4-0.99][199-4-2-0.99]
---------------------------
I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 0.820 | Acc: 68.500% | Wgt Acc: 79.505%
	I - Batch: 100 | Loss: 0.807 | Acc: 69.250% | Wgt Acc: 79.829%
	I - Batch: 150 | Loss: 0.811 | Acc: 69.333% | Wgt Acc: 79.634%
	I - Batch: 200 | Loss: 0.815 | Acc: 69.219% | Wgt Acc: 79.295%
I - num batch: 222
I - Train -- Loss: 0.813 | Acc: 69.383% | Wgt Acc: 79.719% | LR: 5.000000e-04 | Dur: 135.06s
I - Confusion Matrix: [row->prediction - col->label]
[[590.  10.  15.  71. 161.]
 [ 16. 502.  46.  17. 131.]
 [ 12.  38. 619.  23. 257.]
 [ 68.  16.  28. 420. 121.]
 [ 11.  12.  26.   7. 330.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.164 | Acc: 56.607% | Wgt Acc: 61.077% | Dur: 19.04s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  3.  2. 10. 11.]
 [ 3. 39. 11.  2. 15.]
 [ 2. 20. 43.  5. 46.]
 [21. 10. 12. 66. 26.]
 [ 5.  6.  7.  3. 82.]]

I - Local maximum validation set accuracy:  56.61

I - Validation set results: 
[14-1-2-0.75][50-3-3-0.83][124-2-3-0.88][127-0-0-0.99][443-2-2-0.99][567-0-0-0.72][573-1-1-0.55][615-0-3-0.99][695-1-2-0.99][722-3-3-0.81]
[826-0-0-0.99][878-0-3-0.95][1103-0-0-0.99][1212-3-3-0.99][1368-0-0-0.99][2181-2-3-0.99][2476-2-2-0.99][2721-2-2-0.99][2818-1-3-0.99][2886-2-2-0.93]
[3231-2-2-0.99][3333-2-1-0.27][3482-2-2-0.99][3536-3-3-0.99][3625-1-1-0.99][3909-0-0-0.99][4035-0-3-0.99][4140-0-0-0.99][4214-1-1-0.85][4346-1-3-0.95]
[4581-2-2-0.97][4708-3-2-0.95][4838-3-3-0.97][4845-1-2-0.46][4868-0-0-0.99][4939-0-1-0.75][4984-2-2-0.99][5078-1-3-0.69][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.95][5843-1-1-0.99][5949-3-3-0.99][5987-2-4-0.87][6014-3-3-0.98][6033-3-3-0.99][6313-0-0-0.99][6421-3-3-0.99][6500-1-4-0.88][6583-3-2-0.99]
[6683-3-3-0.99][6825-2-1-0.99][6998-3-3-0.94][7049-3-3-0.99][7517-1-1-0.96][7521-1-1-0.81][7528-1-2-0.74][7949-1-2-0.99][8135-1-0-0.98][8185-3-0-0.99]
[8269-3-1-0.97][8273-3-3-0.93][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.99][8903-1-2-0.99][9001-2-2-0.98][9036-2-2-0.99][9281-3-3-0.02][9300-2-2-0.99]
[9571-0-3-0.97][9617-1-3-0.52][9644-2-2-0.96][9705-2-4-0.92][9801-0-3-0.99][9803-3-3-0.99][9865-3-3-0.99][9896-2-2-0.65][10314-1-2-0.99][10337-3-3-0.99]
[10403-0-4-0.85][10653-2-1-0.93][10704-2-1-0.83][10719-1-1-0.99][10727-1-4-0.98][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.97][11499-0-0-0.94][11502-3-3-0.99][11512-3-3-0.73][11608-1-1-0.83][11610-0-0-0.92][11692-0-3-0.92][11905-0-0-0.99][11993-1-1-0.99][12002-2-3-0.69]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-1-0.37][12320-1-0-0.90][12377-2-4-0.74][12398-2-3-0.99][12503-1-1-0.99][12617-0-3-0.99][12685-3-3-0.91][12738-2-3-0.57]
[12742-2-2-0.99][12823-0-3-0.99][13110-1-1-0.99][13240-3-3-0.99][13253-1-2-0.89][13273-0-0-0.99][13634-1-3-0.33][13763-2-3-0.99][13905-3-3-0.99][14060-2-1-0.94]
[14065-3-3-0.81][14147-3-3-0.92][14595-2-2-0.99][14687-2-2-0.99][14788-2-2-0.99][14869-1-1-0.99][14872-3-0-0.67][14877-1-1-0.94][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-1-0.34][15178-2-4-0.86][15375-3-3-0.27][15389-3-3-0.99][15568-2-1-0.80][15675-3-3-0.99][15869-1-3--0.22][16207-3-0-0.43][16236-0-0-0.98][16302-3-2-0.94]
[16331-2-2-0.99][16381-0-3-0.99][16488-1-1-0.97][16495-0-0-0.99][16650-0-0-0.99][16719-1-2-0.55][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.97][17245-1-2-0.51]
[17278-3-0--0.01][17282-0-0-0.90][17311-2-2-0.99][17336-2-1-0.08][17608-3-3-0.99][17627-0-1--0.19][17877-3-4-0.99][17924-1-2-0.45][17984-3-0-0.99][18211-0-3-0.99]
[18276-3-3-0.99][18287-1-1-0.38][18394-0-0-0.99][18428-0-2-0.12][18442-0-3-0.99][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.93][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.99][18824-2-2-0.99][18890-3-3-0.99][18930-3-3-0.12][18938-3-3-0.99][19817-1-1-0.65][19839-0-4-0.86][19930-3-3-0.99][19944-0-2-0.99][20036-2-2-0.99]
[20101-3-3-0.99][20474-1-1-0.99][20547-3-3-0.94][20929-2-2-0.99][21245-1-2-0.26][21257-3-3-0.88][21293-1-2-0.95][21316-1-1-0.99][21384-1-4-0.97][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-3-0.99][21943-3-3-0.99][21947-0-0-0.77][21948-0-0-0.99][21965-2-2-0.97][21998-1-1-0.25][22025-0-3-0.98][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-2-0.24][22985-3-3-0.99][23014-0-0-0.99][23112-1-1-0.86][23144-3-3-0.99][23168-2-3-0.96]
[23219-0-0-0.89][23363-3-3-0.99][23470-0-0-0.17][23486-2-3-0.19][23497-0-3-0.99][23516-0-0-0.99][23690-1-1-0.64][23921-2-4-0.45][23936-1-2-0.99][24040-3-4-0.98]
[24111-1-4-0.99][24182-0-0-0.98][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.91][24364-1-2-0.16][24427-3-3-0.99][24477-2-2-0.99][24495-2-1-0.87][24893-2-2-0.95]
[25012-1-3-0.14][25121-2-2-0.60][25165-3-3-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.99][25644-1-4-0.09][25718-1-1-0.81][25774-2-2-0.99]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-0-0.81][26321-1-1-0.99][26732-1-1-0.99][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-3-0.67][26860-1-4-0.61]
[26948-0-0-0.99][27049-3-0-0.97][27098-1-1-0.87][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-3-0.92][27890-1-1-0.84][28040-0-0-0.99][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3-0.99][29777-0-0-0.99][29877-2-1-0.69][30035-1-2-0.95][30098-0-3-0.99][30326-1-1-0.99][30572-2-2-0.99][30716-0-4-0.99]
[30806-2-4-0.99][30906-1-1-0.99][31007-0-4-0.70][31181-3-3-0.99][31238-0-3-0.48][31347-0-3-0.99][31422-2-2-0.77][31429-3-3-0.97][31431-0-0-0.19][31432-1-1-0.97]
[31477-0-0-0.99][31524-1-2-0.23][31597-1-2-0.85][31619-1-0-0.99][31701-0-0-0.63][31755-0-0-0.98][31854-3-3-0.99][32074-1-3-0.42][32078-3-3-0.99][32111-1-1-0.83]
[32127-1-1-0.95][32140-3-3-0.99][32263-2-0-0.89][32365-0-0-0.99][32411-2-3-0.99][32429-3-3-0.77][32473-3-3-0.99][32574-3-3-0.99][32584-0-4-0.51][32622-0-1-0.56]
[32858-3-0-0.96][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.97][33133-2-2-0.95][33173-2-2-0.58][33175-3-4-0.95][33306-3-1-0.72][33309-2-3-0.99]
[33474-0-0-0.89][33478-2-1-0.56][33618-1-1-0.99][33712-0-0-0.99][33782-2-2-0.48][33914-3-3-0.99][34076-3-2-0.99][34112-2-2-0.99][34138-2-2-0.62][34239-1-3-0.81]
[34364-2-2-0.99][34617-1-2-0.09][34751-3-3-0.99][34783-2-4-0.99][35015-3-3-0.99][35018-1-2-0.60][35288-2-2-0.27][0-4-2-0.94][1-4-4-0.99][2-4-4-0.99]
[3-4-4-0.99][4-4-2-0.89][5-4-1-0.99][6-4-0-0.24][7-4-2-0.81][8-4-2-0.19][9-4-1-0.79][10-4-4-0.99][11-4-2-0.99][12-4-2-0.02]
[14-4-3-0.99][15-4-3-0.99][16-4-4-0.99][17-4-4-0.37][18-4-2-0.22][19-4-3-0.99][20-4-2-0.99][21-4-2-0.35][22-4-4-0.97][23-4-4-0.26]
[24-4-4-0.99][25-4-3-0.99][26-4-3-0.57][27-4-4-0.99][28-4-4-0.99][29-4-1-0.05][30-4-3-0.29][31-4-4-0.98][32-4-2--0.06][33-4-2-0.99]
[34-4-4-0.76][35-4-3-0.99][37-4-3-0.83][39-4-0-0.99][40-4-4--0.07][41-4-4-0.52][42-4-1-0.79][43-4-2-0.95][45-4-2-0.73][46-4-4-0.99]
[47-4-2-0.99][48-4-2-0.92][51-4-4-0.90][52-4-4-0.38][53-4-4-0.10][54-4-3-0.94][55-4-3-0.84][56-4-4-0.76][57-4-3-0.99][58-4-2-0.94]
[59-4-0-0.62][60-4-1-0.99][61-4-4-0.99][62-4-4-0.92][63-4-2-0.99][64-4-2-0.97][65-4-4-0.99][66-4-4-0.94][67-4-4-0.70][68-4-4-0.75]
[69-4-2-0.95][70-4-2-0.80][72-4-4-0.66][73-4-2-0.49][74-4-2-0.99][75-4-3-0.93][77-4-4-0.99][78-4-3-0.97][79-4-2-0.99][80-4-4-0.99]
[81-4-1-0.93][82-4-1-0.97][83-4-4-0.45][84-4-4-0.99][85-4-4-0.99][86-4-4-0.92][87-4-4-0.99][88-4-4-0.98][89-4-2-0.58][90-4-4-0.10]
[91-4-1-0.99][92-4-3-0.49][93-4-3-0.19][94-4-4-0.99][95-4-4-0.58][96-4-1-0.78][97-4-1-0.99][98-4-3-0.59][99-4-4-0.28][100-4-4-0.91]
[101-4-4-0.99][102-4-1--0.04][103-4-3-0.99][104-4-4-0.99][105-4-4-0.84][106-4-4-0.99][107-4-0-0.82][108-4-2-0.57][109-4-4-0.99][110-4-4-0.97]
[111-4-3-0.99][112-4-2-0.78][113-4-3-0.81][114-4-2-0.46][115-4-4-0.43][116-4-2-0.65][117-4-4-0.98][119-4-2-0.99][121-4-4-0.99][122-4-4-0.99]
[124-4-2-0.25][125-4-4-0.99][126-4-4-0.99][127-4-4-0.08][128-4-0-0.58][129-4-1-0.99][130-4-4-0.51][131-4-3-0.96][132-4-3-0.56][133-4-4-0.99]
[135-4-2-0.94][136-4-1-0.96][137-4-2--0.19][138-4-2-0.40][139-4-4-0.84][140-4-4-0.64][141-4-3-0.90][142-4-4-0.97][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.98][148-4-0-0.99][149-4-4-0.86][150-4-4-0.99][151-4-4-0.99][152-4-4-0.97][153-4-2-0.94][154-4-4-0.99][155-4-4-0.99][156-4-3-0.98]
[157-4-2-0.60][158-4-2-0.87][160-4-1-0.14][161-4-2-0.98][162-4-2--0.05][164-4-4-0.36][165-4-4-0.99][167-4-0-0.43][168-4-4-0.85][170-4-2--0.48]
[171-4-2-0.99][172-4-4-0.99][173-4-4-0.79][174-4-0-0.99][175-4-4-0.93][177-4-4-0.97][178-4-4-0.99][179-4-0-0.31][180-4-4-0.99][181-4-2-0.13]
[182-4-3-0.99][183-4-4-0.99][184-4-2-0.99][186-4-0-0.01][187-4-2-0.96][188-4-2-0.99][189-4-4-0.99][190-4-1-0.30][191-4-4-0.85][192-4-4-0.98]
[193-4-2-0.98][194-4-3-0.33][195-4-0-0.91][196-4-3-0.98][197-4-4-0.82][198-4-4-0.99][199-4-2-0.99]
---------------------------
I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 0.795 | Acc: 72.000% | Wgt Acc: 82.078%
	I - Batch: 100 | Loss: 0.798 | Acc: 71.750% | Wgt Acc: 81.717%
	I - Batch: 150 | Loss: 0.796 | Acc: 71.042% | Wgt Acc: 81.231%
	I - Batch: 200 | Loss: 0.807 | Acc: 70.531% | Wgt Acc: 80.591%
I - num batch: 222
I - Train -- Loss: 0.804 | Acc: 70.905% | Wgt Acc: 80.881% | LR: 5.000000e-04 | Dur: 133.05s
I - Confusion Matrix: [row->prediction - col->label]
[[597.  11.  14.  73. 164.]
 [ 10. 500.  29.  19. 135.]
 [ 22.  45. 649.  24. 237.]
 [ 52.  14.  21. 413. 108.]
 [ 16.   8.  21.   9. 356.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.185 | Acc: 56.410% | Wgt Acc: 59.682% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  2.  1. 12. 13.]
 [ 9. 49. 18.  8. 29.]
 [ 4. 12. 46.  9. 44.]
 [12.  4.  4. 48.  8.]
 [ 6. 11.  6.  9. 86.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 0.762 | Acc: 76.250% | Wgt Acc: 84.771%
	I - Batch: 100 | Loss: 0.757 | Acc: 75.500% | Wgt Acc: 84.719%
	I - Batch: 150 | Loss: 0.771 | Acc: 73.833% | Wgt Acc: 83.754%
	I - Batch: 200 | Loss: 0.778 | Acc: 73.625% | Wgt Acc: 83.348%
I - num batch: 222
I - Train -- Loss: 0.782 | Acc: 73.245% | Wgt Acc: 83.069% | LR: 5.000000e-04 | Dur: 133.09s
I - Confusion Matrix: [row->prediction - col->label]
[[606.  13.  10.  57. 153.]
 [ 19. 508.  33.  18. 158.]
 [  9.  35. 660.  18. 208.]
 [ 46.   9.  13. 438.  95.]
 [ 17.  13.  18.   7. 386.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.215 | Acc: 52.268% | Wgt Acc: 58.932% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  4.  8. 21. 25.]
 [ 4. 39. 10.  4. 23.]
 [ 4. 18. 44.  4. 50.]
 [12. 11. 10. 55. 19.]
 [ 4.  6.  3.  2. 63.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 0.770 | Acc: 72.375% | Wgt Acc: 83.132%
	I - Batch: 100 | Loss: 0.773 | Acc: 71.875% | Wgt Acc: 82.489%
	I - Batch: 150 | Loss: 0.785 | Acc: 71.750% | Wgt Acc: 82.160%
	I - Batch: 200 | Loss: 0.785 | Acc: 71.969% | Wgt Acc: 82.000%
I - num batch: 222
I - Train -- Loss: 0.784 | Acc: 71.948% | Wgt Acc: 82.039% | LR: 5.000000e-04 | Dur: 133.64s
I - Confusion Matrix: [row->prediction - col->label]
[[610.   8.  10.  73. 165.]
 [ 10. 504.  30.  15. 115.]
 [ 17.  38. 659.  28. 253.]
 [ 47.   9.  15. 417. 105.]
 [ 13.  19.  20.   5. 362.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.280 | Acc: 50.099% | Wgt Acc: 58.263% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  5.  3. 11. 19.]
 [ 1. 38.  8.  8. 31.]
 [ 7. 24. 51. 11. 62.]
 [17.  8.  9. 54. 16.]
 [ 4.  3.  4.  2. 52.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 0.776 | Acc: 70.250% | Wgt Acc: 82.273%
	I - Batch: 100 | Loss: 0.770 | Acc: 72.250% | Wgt Acc: 83.255%
	I - Batch: 150 | Loss: 0.780 | Acc: 72.083% | Wgt Acc: 82.266%
	I - Batch: 200 | Loss: 0.774 | Acc: 72.625% | Wgt Acc: 82.793%
I - num batch: 222
I - Train -- Loss: 0.775 | Acc: 72.681% | Wgt Acc: 82.863% | LR: 5.000000e-04 | Dur: 133.98s
I - Confusion Matrix: [row->prediction - col->label]
[[597.   8.  13.  76. 177.]
 [ 15. 524.  23.  15. 117.]
 [ 13.  27. 668.  19. 239.]
 [ 58.   7.  10. 422. 100.]
 [ 14.  12.  20.   6. 367.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.231 | Acc: 51.677% | Wgt Acc: 57.940% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  7.  7. 24. 37.]
 [ 3. 44. 12.  4. 24.]
 [ 5. 16. 38.  3. 34.]
 [14.  6. 11. 53. 21.]
 [ 3.  5.  7.  2. 64.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 0.745 | Acc: 74.875% | Wgt Acc: 85.790%
	I - Batch: 100 | Loss: 0.748 | Acc: 74.312% | Wgt Acc: 84.989%
	I - Batch: 150 | Loss: 0.750 | Acc: 74.542% | Wgt Acc: 84.737%
	I - Batch: 200 | Loss: 0.757 | Acc: 73.938% | Wgt Acc: 84.015%
I - num batch: 222
I - Train -- Loss: 0.758 | Acc: 73.781% | Wgt Acc: 83.783% | LR: 5.000000e-04 | Dur: 132.75s
I - Confusion Matrix: [row->prediction - col->label]
[[601.   8.  12.  61. 154.]
 [  9. 521.  18.  13. 118.]
 [ 14.  28. 673.  21. 257.]
 [ 55.  13.  12. 437.  86.]
 [ 18.   8.  19.   6. 385.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.131 | Acc: 56.213% | Wgt Acc: 57.490% | Dur: 14.01s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  9.  5. 17. 27.]
 [ 2. 34. 11.  3. 10.]
 [ 4. 20. 40.  7. 40.]
 [11.  4.  7. 53.  8.]
 [ 8. 11. 12.  6. 95.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 0.739 | Acc: 76.125% | Wgt Acc: 85.590%
	I - Batch: 100 | Loss: 0.743 | Acc: 75.125% | Wgt Acc: 84.386%
	I - Batch: 150 | Loss: 0.746 | Acc: 74.417% | Wgt Acc: 84.781%
	I - Batch: 200 | Loss: 0.750 | Acc: 73.812% | Wgt Acc: 84.204%
I - num batch: 222
I - Train -- Loss: 0.749 | Acc: 73.865% | Wgt Acc: 84.252% | LR: 5.000000e-04 | Dur: 135.08s
I - Confusion Matrix: [row->prediction - col->label]
[[610.   6.  15.  66. 182.]
 [  6. 524.  17.  10. 135.]
 [ 14.  26. 677.  21. 201.]
 [ 53.  10.   9. 437. 110.]
 [ 14.  12.  16.   4. 372.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.149 | Acc: 57.791% | Wgt Acc: 62.623% | Dur: 15.32s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  3.  2. 15. 21.]
 [ 2. 47. 14.  4. 22.]
 [ 3. 14. 43.  6. 42.]
 [17.  7.  9. 60. 13.]
 [ 5.  7.  7.  1. 82.]]

I - Local maximum validation set accuracy:  57.79

I - Validation set results: 
[14-1-2-0.29][50-3-1--0.18][124-2-2-0.38][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.71][615-0-3-0.99][695-1-2-0.99][722-3-3-0.38]
[826-0-0-0.99][878-0-0-0.97][1103-0-0-0.71][1212-3-0-0.20][1368-0-0-0.99][2181-2-3-0.11][2476-2-2-0.99][2721-2-2-0.99][2818-1-3-0.99][2886-2-1-0.86]
[3231-2-2-0.99][3333-2-1-0.21][3482-2-2-0.36][3536-3-3-0.99][3625-1-1-0.99][3909-0-0-0.99][4035-0-3-0.24][4140-0-0-0.99][4214-1-1-0.35][4346-1-3-0.72]
[4581-2-1-0.98][4708-3-2-0.41][4838-3-3-0.22][4845-1-2-0.09][4868-0-0-0.99][4939-0-1-0.96][4984-2-3-0.99][5078-1-4--0.39][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.24][5843-1-1-0.99][5949-3-3-0.79][5987-2-4-0.99][6014-3-1-0.65][6033-3-3-0.59][6313-0-3-0.93][6421-3-3-0.99][6500-1-1-0.13][6583-3-2-0.92]
[6683-3-3-0.57][6825-2-1-0.99][6998-3-2-0.59][7049-3-3-0.99][7517-1-2-0.76][7521-1-1-0.99][7528-1-3-0.33][7949-1-2-0.99][8135-1-0-0.99][8185-3-0-0.99]
[8269-3-1-0.16][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.93][8903-1-1-0.13][9001-2-2-0.99][9036-2-2-0.99][9281-3-3-0.35][9300-2-2-0.99]
[9571-0-3-0.62][9617-1-4-0.02][9644-2-1-0.99][9705-2-4-0.55][9801-0-3-0.99][9803-3-3-0.96][9865-3-3-0.99][9896-2-2-0.97][10314-1-1--0.09][10337-3-3-0.99]
[10403-0-4-0.45][10653-2-4-0.06][10704-2-1-0.55][10719-1-1-0.99][10727-1-4-0.92][10836-0-0-0.99][10969-2-3-0.87][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.88][11499-0-0-0.90][11502-3-3-0.99][11512-3-3-0.67][11608-1-1-0.84][11610-0-0-0.99][11692-0-3-0.87][11905-0-0-0.99][11993-1-1-0.99][12002-2-2-0.83]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-2-0.94][12320-1-4-0.67][12377-2-4-0.68][12398-2-3-0.99][12503-1-1--0.00][12617-0-2-0.98][12685-3-3-0.55][12738-2-0-0.54]
[12742-2-2-0.99][12823-0-0-0.99][13110-1-1-0.80][13240-3-3-0.96][13253-1-1-0.57][13273-0-0-0.99][13634-1-3-0.50][13763-2-2-0.26][13905-3-3-0.99][14060-2-1-0.99]
[14065-3-0-0.82][14147-3-3-0.97][14595-2-2-0.99][14687-2-2-0.99][14788-2-2-0.98][14869-1-1-0.99][14872-3-0-0.09][14877-1-1-0.99][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-1-0.68][15178-2-3-0.95][15375-3-3-0.87][15389-3-3-0.99][15568-2-1-0.97][15675-3-3-0.99][15869-1-0--0.23][16207-3-0-0.91][16236-0-0-0.72][16302-3-0--0.11]
[16331-2-2-0.99][16381-0-0-0.97][16488-1-1-0.10][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.84][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.30][17245-1-1-0.31]
[17278-3-0-0.01][17282-0-2-0.01][17311-2-2-0.99][17336-2-1-0.44][17608-3-3-0.99][17627-0-2-0.22][17877-3-1-0.99][17924-1-2-0.07][17984-3-0-0.99][18211-0-3-0.72]
[18276-3-3-0.81][18287-1-1-0.45][18394-0-0-0.99][18428-0-0-0.65][18442-0-3-0.99][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.99][18663-0-0-0.99][18718-0-0-0.95]
[18766-2-2-0.99][18824-2-2-0.95][18890-3-3-0.99][18930-3-3--0.04][18938-3-3-0.74][19817-1-2-0.70][19839-0-0-0.96][19930-3-3-0.23][19944-0-4-0.97][20036-2-2-0.99]
[20101-3-3-0.90][20474-1-1-0.99][20547-3-3-0.58][20929-2-2-0.99][21245-1-1-0.41][21257-3-3--0.22][21293-1-2-0.99][21316-1-1-0.99][21384-1-2-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-3-0.76][21943-3-3-0.38][21947-0-0-0.58][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.99][22025-0-3-0.22][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.99][22976-3-2-0.03][22985-3-3-0.99][23014-0-0-0.99][23112-1-1-0.75][23144-3-3-0.99][23168-2-3-0.67]
[23219-0-0-0.51][23363-3-3-0.99][23470-0-0-0.94][23486-2-4--0.04][23497-0-3-0.99][23516-0-0-0.99][23690-1-4-0.97][23921-2-1-0.55][23936-1-2-0.46][24040-3-4-0.78]
[24111-1-4-0.99][24182-0-0-0.57][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.99][24364-1-2-0.27][24427-3-0-0.47][24477-2-2-0.99][24495-2-1-0.53][24893-2-2-0.98]
[25012-1-3-0.75][25121-2-4-0.10][25165-3-3-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.70][25574-2-2-0.16][25644-1-1-0.99][25718-1-1--0.09][25774-2-2-0.49]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.99][26321-1-1-0.99][26732-1-1-0.62][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-2--0.25][26860-1-2-0.75]
[26948-0-0-0.96][27049-3-0-0.99][27098-1-1-0.59][27526-0-0-0.94][27639-3-3-0.65][27698-3-0-0.68][27772-0-0-0.99][27890-1-1-0.70][28040-0-4-0.24][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.96][29198-3-3-0.99][29777-0-0-0.99][29877-2-1-0.81][30035-1-1-0.91][30098-0-3-0.99][30326-1-1-0.99][30572-2-3-0.80][30716-0-4-0.99]
[30806-2-2-0.99][30906-1-1-0.88][31007-0-0-0.57][31181-3-3-0.99][31238-0-3-0.86][31347-0-0-0.99][31422-2-2-0.98][31429-3-3-0.85][31431-0-0--0.10][31432-1-1-0.99]
[31477-0-3-0.99][31524-1-3--0.27][31597-1-1-0.61][31619-1-0-0.61][31701-0-0-0.87][31755-0-0-0.72][31854-3-3-0.99][32074-1-1-0.15][32078-3-3-0.99][32111-1-1-0.92]
[32127-1-2-0.79][32140-3-3-0.99][32263-2-2-0.73][32365-0-0-0.99][32411-2-3-0.99][32429-3-3-0.94][32473-3-3-0.99][32574-3-3-0.99][32584-0-0--0.40][32622-0-0--0.27]
[32858-3-0-0.98][32969-3-0-0.99][33016-2-2-0.74][33031-1-3-0.99][33035-2-2-0.65][33133-2-2-0.92][33173-2-1-0.99][33175-3-2-0.74][33306-3-3-0.40][33309-2-3-0.99]
[33474-0-1-0.18][33478-2-1-0.98][33618-1-1-0.99][33712-0-0-0.31][33782-2-2-0.92][33914-3-3-0.24][34076-3-2-0.31][34112-2-2-0.98][34138-2-2--0.22][34239-1-1-0.51]
[34364-2-2-0.99][34617-1-2--0.03][34751-3-3-0.99][34783-2-4-0.89][35015-3-3-0.80][35018-1-1-0.87][35288-2-2-0.63][0-4-2-0.30][1-4-4-0.99][2-4-4-0.40]
[3-4-4-0.69][4-4-4-0.73][5-4-1-0.99][6-4-4-0.99][7-4-2-0.75][8-4-2--0.40][9-4-0-0.62][10-4-4-0.96][11-4-2-0.99][12-4-1-0.99]
[14-4-3-0.88][15-4-0-0.99][16-4-4-0.78][17-4-0--0.42][18-4-4-0.95][19-4-0-0.54][20-4-2-0.65][21-4-2-0.30][22-4-4-0.99][23-4-4--0.53]
[24-4-4-0.99][25-4-3-0.41][26-4-1-0.24][27-4-2-0.99][28-4-4-0.99][29-4-1-0.84][30-4-0-0.31][31-4-4-0.80][32-4-2-0.17][33-4-2-0.92]
[34-4-4-0.51][35-4-0-0.13][37-4-2-0.79][39-4-0-0.99][40-4-4-0.32][41-4-1--0.02][42-4-3-0.60][43-4-2-0.91][45-4-2-0.86][46-4-4-0.98]
[47-4-4-0.99][48-4-4-0.31][51-4-4-0.95][52-4-0-0.19][53-4-4-0.39][54-4-3-0.73][55-4-2-0.99][56-4-1-0.92][57-4-0-0.99][58-4-2-0.96]
[59-4-0-0.99][60-4-4--0.02][61-4-4-0.99][62-4-2-0.91][63-4-2-0.99][64-4-2-0.96][65-4-4-0.99][66-4-4-0.99][67-4-2-0.96][68-4-1-0.90]
[69-4-3-0.37][70-4-2-0.38][72-4-4-0.95][73-4-1-0.42][74-4-2-0.99][75-4-0-0.69][77-4-4-0.99][78-4-3-0.97][79-4-2-0.98][80-4-4-0.99]
[81-4-4-0.71][82-4-1-0.98][83-4-1-0.32][84-4-4-0.97][85-4-4-0.99][86-4-2-0.89][87-4-4-0.99][88-4-4-0.86][89-4-2-0.94][90-4-3-0.05]
[91-4-4-0.13][92-4-0-0.16][93-4-0-0.28][94-4-4-0.99][95-4-4-0.47][96-4-2-0.31][97-4-4-0.89][98-4-2-0.72][99-4-4-0.20][100-4-2-0.61]
[101-4-4-0.99][102-4-2--0.03][103-4-3-0.80][104-4-4-0.99][105-4-2-0.71][106-4-4-0.99][107-4-1-0.99][108-4-1--0.13][109-4-1-0.94][110-4-4-0.52]
[111-4-0-0.99][112-4-2-0.96][113-4-4-0.36][114-4-3-0.53][115-4-4-0.83][116-4-2-0.68][117-4-4-0.90][119-4-4-0.44][121-4-1-0.96][122-4-4-0.99]
[124-4-1-0.67][125-4-4-0.98][126-4-4-0.99][127-4-2-0.06][128-4-0-0.19][129-4-1-0.44][130-4-4-0.10][131-4-2-0.40][132-4-4-0.78][133-4-0-0.99]
[135-4-2-0.92][136-4-1-0.94][137-4-4--0.08][138-4-2--0.06][139-4-4-0.17][140-4-2-0.39][141-4-3-0.32][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.99][148-4-0-0.99][149-4-4-0.80][150-4-4-0.99][151-4-4-0.99][152-4-4-0.25][153-4-2-0.53][154-4-4-0.89][155-4-4-0.99][156-4-0--0.13]
[157-4-2-0.99][158-4-3-0.53][160-4-1-0.43][161-4-2-0.99][162-4-2-0.16][164-4-4-0.59][165-4-4-0.89][167-4-4-0.40][168-4-4-0.89][170-4-2--0.49]
[171-4-4-0.97][172-4-4-0.99][173-4-4-0.99][174-4-0-0.90][175-4-4-0.82][177-4-0-0.75][178-4-4-0.99][179-4-4-0.36][180-4-4-0.99][181-4-3-0.93]
[182-4-3-0.99][183-4-4-0.68][184-4-2-0.98][186-4-4-0.31][187-4-1-0.25][188-4-4-0.98][189-4-4-0.99][190-4-1-0.11][191-4-4-0.93][192-4-4-0.77]
[193-4-1-0.99][194-4-1-0.48][195-4-0-0.95][196-4-4--0.07][197-4-4-0.90][198-4-4-0.99][199-4-2-0.99]
---------------------------
I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 0.714 | Acc: 77.375% | Wgt Acc: 87.871%
	I - Batch: 100 | Loss: 0.726 | Acc: 76.750% | Wgt Acc: 86.639%
	I - Batch: 150 | Loss: 0.735 | Acc: 75.917% | Wgt Acc: 86.174%
	I - Batch: 200 | Loss: 0.738 | Acc: 75.594% | Wgt Acc: 85.765%
I - num batch: 222
I - Train -- Loss: 0.741 | Acc: 75.529% | Wgt Acc: 85.492% | LR: 5.000000e-04 | Dur: 131.66s
I - Confusion Matrix: [row->prediction - col->label]
[[622.  11.  12.  57. 147.]
 [ 15. 528.  17.  13. 124.]
 [ 11.  21. 676.  13. 189.]
 [ 38.   7.   9. 449. 136.]
 [ 11.  11.  20.   6. 404.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.132 | Acc: 59.369% | Wgt Acc: 59.209% | Dur: 16.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   6.   3.  20.  25.]
 [  3.  47.  18.   5.  20.]
 [  2.   6.  30.   0.  17.]
 [ 13.   8.  11.  54.  10.]
 [  8.  11.  13.   7. 108.]]

I - Local maximum validation set accuracy:  59.37

I - Validation set results: 
[14-1-1-0.80][50-3-1-0.18][124-2-3-0.51][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.89][615-0-0-0.93][695-1-2-0.81][722-3-3-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-0-0.46][1212-3-0-0.38][1368-0-0-0.99][2181-2-3-0.86][2476-2-2-0.77][2721-2-2-0.67][2818-1-3-0.99][2886-2-1-0.95]
[3231-2-2-0.72][3333-2-1-0.85][3482-2-2-0.17][3536-3-3-0.09][3625-1-1-0.99][3909-0-0-0.99][4035-0-3-0.76][4140-0-0-0.99][4214-1-1-0.85][4346-1-3-0.97]
[4581-2-1-0.99][4708-3-3-0.30][4838-3-3-0.04][4845-1-1-0.71][4868-0-0-0.99][4939-0-1-0.36][4984-2-3-0.80][5078-1-2-0.79][5396-0-0-0.99][5479-1-1-0.84]
[5717-0-0-0.62][5843-1-1-0.99][5949-3-4-0.32][5987-2-4-0.90][6014-3-3-0.99][6033-3-3-0.60][6313-0-0-0.99][6421-3-3-0.99][6500-1-1-0.59][6583-3-3-0.58]
[6683-3-3-0.69][6825-2-1-0.99][6998-3-0-0.01][7049-3-3-0.99][7517-1-1-0.74][7521-1-0-0.15][7528-1-3-0.76][7949-1-2-0.99][8135-1-0-0.98][8185-3-0-0.99]
[8269-3-4-0.42][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.94][8672-0-0-0.91][8903-1-2-0.76][9001-2-4-0.88][9036-2-2-0.87][9281-3-4-0.71][9300-2-2-0.99]
[9571-0-3-0.12][9617-1-1-0.84][9644-2-1-0.73][9705-2-4-0.88][9801-0-3-0.99][9803-3-3-0.67][9865-3-3-0.99][9896-2-2-0.77][10314-1-4-0.60][10337-3-3-0.99]
[10403-0-4-0.81][10653-2-1-0.12][10704-2-1-0.85][10719-1-1-0.99][10727-1-1-0.97][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-4-0.84][11499-0-0-0.98][11502-3-3-0.99][11512-3-3-0.72][11608-1-1-0.98][11610-0-3-0.93][11692-0-3-0.92][11905-0-0-0.99][11993-1-1-0.99][12002-2-2-0.41]
[12052-0-0-0.98][12201-0-3-0.99][12235-2-1-0.66][12320-1-4-0.87][12377-2-4-0.95][12398-2-3-0.40][12503-1-4-0.72][12617-0-2-0.99][12685-3-1-0.33][12738-2-0-0.95]
[12742-2-2-0.99][12823-0-3-0.96][13110-1-2-0.63][13240-3-3-0.98][13253-1-1-0.99][13273-0-0-0.99][13634-1-4-0.99][13763-2-3-0.80][13905-3-3-0.28][14060-2-1-0.99]
[14065-3-0-0.99][14147-3-0-0.99][14595-2-2-0.54][14687-2-2-0.98][14788-2-2-0.87][14869-1-1-0.99][14872-3-0-0.62][14877-1-1-0.50][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-1-0.57][15178-2-3-0.98][15375-3-0-0.89][15389-3-3-0.99][15568-2-1-0.99][15675-3-3-0.99][15869-1-0--0.90][16207-3-0-0.99][16236-0-2--0.31][16302-3-0-0.99]
[16331-2-2-0.99][16381-0-0-0.49][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-3-0.49][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.93][17245-1-3-0.45]
[17278-3-0-0.25][17282-0-0-0.91][17311-2-2-0.62][17336-2-1-0.85][17608-3-3-0.99][17627-0-0-0.72][17877-3-4-0.75][17924-1-3--0.06][17984-3-0-0.99][18211-0-3-0.76]
[18276-3-3-0.96][18287-1-1-0.89][18394-0-0-0.98][18428-0-0-0.99][18442-0-3-0.99][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.99][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-1-0.66][18824-2-2-0.87][18890-3-3-0.99][18930-3-4-0.53][18938-3-3-0.99][19817-1-1-0.99][19839-0-4-0.93][19930-3-3-0.99][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-3-0.99][20474-1-1-0.99][20547-3-0-0.12][20929-2-2-0.99][21245-1-2-0.31][21257-3-1-0.25][21293-1-1-0.87][21316-1-1-0.66][21384-1-4-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-0-0.33][21943-3-3-0.77][21947-0-0-0.94][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.99][22025-0-4-0.76][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.94][22757-0-0-0.99][22811-3-3-0.99][22976-3-1--0.11][22985-3-3-0.99][23014-0-0-0.99][23112-1-1-0.99][23144-3-3-0.99][23168-2-3-0.52]
[23219-0-0-0.99][23363-3-3-0.57][23470-0-0-0.82][23486-2-2-0.55][23497-0-3-0.99][23516-0-0-0.99][23690-1-4-0.23][23921-2-4-0.63][23936-1-0-0.21][24040-3-4-0.99]
[24111-1-4-0.99][24182-0-0-0.92][24238-3-3-0.99][24290-2-0-0.99][24345-0-4-0.70][24364-1-4-0.02][24427-3-0-0.99][24477-2-2-0.93][24495-2-1-0.88][24893-2-2-0.32]
[25012-1-3-0.14][25121-2-4-0.82][25165-3-3-0.99][25183-0-0-0.99][25297-3-3-0.99][25398-0-0-0.99][25574-2-4-0.65][25644-1-1-0.96][25718-1-1-0.73][25774-2-4-0.76]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.84][26321-1-1-0.99][26732-1-1-0.99][26784-3-3-0.99][26827-3-3-0.99][26833-0-3-0.99][26838-2-4-0.31][26860-1-4-0.46]
[26948-0-0-0.83][27049-3-0-0.99][27098-1-1-0.89][27526-0-0-0.99][27639-3-0-0.74][27698-3-0-0.87][27772-0-0-0.99][27890-1-1-0.83][28040-0-4-0.99][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3-0.72][29777-0-0-0.99][29877-2-1-0.99][30035-1-1-0.52][30098-0-3-0.94][30326-1-1-0.99][30572-2-2-0.82][30716-0-4-0.99]
[30806-2-4-0.66][30906-1-1-0.99][31007-0-0-0.92][31181-3-3-0.82][31238-0-0-0.52][31347-0-0-0.99][31422-2-2-0.94][31429-3-3-0.93][31431-0-0-0.99][31432-1-1-0.99]
[31477-0-0-0.99][31524-1-0-0.50][31597-1-1--0.09][31619-1-0-0.91][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.98][32074-1-1-0.38][32078-3-3-0.99][32111-1-1-0.99]
[32127-1-1-0.99][32140-3-3-0.87][32263-2-0-0.44][32365-0-0-0.96][32411-2-3-0.99][32429-3-3-0.68][32473-3-3-0.95][32574-3-3-0.99][32584-0-0-0.53][32622-0-1-0.66]
[32858-3-0-0.99][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-4-0.28][33133-2-2-0.56][33173-2-2-0.23][33175-3-1-0.95][33306-3-3-0.99][33309-2-3-0.88]
[33474-0-1-0.37][33478-2-1-0.74][33618-1-1-0.99][33712-0-0-0.90][33782-2-1-0.42][33914-3-4-0.48][34076-3-3--0.65][34112-2-2-0.99][34138-2-3-0.98][34239-1-4-0.73]
[34364-2-1-0.98][34617-1-4-0.91][34751-3-3-0.99][34783-2-4-0.98][35015-3-3-0.99][35018-1-1-0.78][35288-2-1-0.07][0-4-2-0.99][1-4-4-0.99][2-4-4-0.99]
[3-4-4-0.99][4-4-4-0.94][5-4-1-0.09][6-4-4-0.98][7-4-4-0.41][8-4-4-0.15][9-4-2-0.62][10-4-4-0.97][11-4-4-0.99][12-4-1-0.76]
[14-4-3-0.71][15-4-0-0.99][16-4-4-0.99][17-4-0-0.18][18-4-4-0.89][19-4-0-0.55][20-4-0--0.18][21-4-2-0.92][22-4-4-0.99][23-4-4-0.96]
[24-4-4-0.99][25-4-3-0.95][26-4-1-0.49][27-4-2-0.99][28-4-4-0.99][29-4-1-0.56][30-4-4--0.33][31-4-4-0.88][32-4-1-0.99][33-4-3-0.96]
[34-4-4-0.72][35-4-0-0.99][37-4-3-0.04][39-4-0-0.99][40-4-4-0.35][41-4-4-0.73][42-4-4-0.61][43-4-4-0.86][45-4-1-0.48][46-4-4-0.99]
[47-4-4-0.99][48-4-4-0.99][51-4-4-0.99][52-4-4-0.68][53-4-4-0.72][54-4-4-0.38][55-4-4-0.99][56-4-1-0.98][57-4-0-0.99][58-4-4-0.94]
[59-4-0-0.99][60-4-4-0.32][61-4-4-0.99][62-4-2-0.99][63-4-2-0.72][64-4-2-0.32][65-4-4-0.99][66-4-4-0.99][67-4-4-0.99][68-4-1-0.87]
[69-4-0--0.56][70-4-4-0.98][72-4-4-0.87][73-4-1-0.99][74-4-2-0.55][75-4-2-0.68][77-4-4-0.99][78-4-3-0.86][79-4-4-0.95][80-4-4-0.99]
[81-4-1-0.86][82-4-1-0.99][83-4-4-0.46][84-4-4-0.99][85-4-4-0.50][86-4-4-0.99][87-4-4-0.99][88-4-4-0.95][89-4-3-0.09][90-4-0--0.01]
[91-4-4-0.51][92-4-0-0.58][93-4-0-0.89][94-4-4-0.99][95-4-4-0.66][96-4-4-0.50][97-4-4-0.83][98-4-3--0.07][99-4-4-0.18][100-4-1-0.86]
[101-4-4-0.99][102-4-2-0.25][103-4-0-0.75][104-4-4-0.99][105-4-4-0.99][106-4-1-0.99][107-4-1-0.98][108-4-1-0.16][109-4-4-0.95][110-4-4-0.78]
[111-4-0-0.99][112-4-4-0.82][113-4-4-0.81][114-4-3-0.05][115-4-4-0.98][116-4-2-0.49][117-4-4-0.98][119-4-4-0.80][121-4-4-0.98][122-4-4-0.98]
[124-4-1-0.16][125-4-4-0.99][126-4-4-0.99][127-4-2-0.89][128-4-0-0.29][129-4-1-0.67][130-4-4-0.01][131-4-4-0.48][132-4-4-0.99][133-4-0-0.99]
[135-4-2-0.75][136-4-1-0.99][137-4-4-0.08][138-4-2-0.99][139-4-4-0.99][140-4-4-0.08][141-4-0-0.99][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.96][148-4-0-0.99][149-4-4-0.99][150-4-4-0.99][151-4-4-0.99][152-4-4-0.94][153-4-4-0.75][154-4-4-0.99][155-4-4-0.99][156-4-0-0.55]
[157-4-2--0.05][158-4-4-0.52][160-4-4-0.92][161-4-2-0.99][162-4-2--0.19][164-4-4-0.57][165-4-4-0.99][167-4-0-0.98][168-4-4-0.58][170-4-3--0.46]
[171-4-4-0.99][172-4-4-0.98][173-4-4-0.99][174-4-0-0.99][175-4-4-0.99][177-4-0-0.99][178-4-4-0.99][179-4-4-0.95][180-4-4-0.99][181-4-4-0.78]
[182-4-4-0.99][183-4-4-0.99][184-4-4-0.99][186-4-0--0.12][187-4-4-0.66][188-4-4-0.99][189-4-4-0.99][190-4-4-0.55][191-4-4-0.45][192-4-1-0.99]
[193-4-1-0.99][194-4-0-0.93][195-4-0-0.28][196-4-3-0.35][197-4-4-0.92][198-4-4-0.99][199-4-4-0.99]
---------------------------
I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 0.750 | Acc: 75.750% | Wgt Acc: 85.960%
	I - Batch: 100 | Loss: 0.746 | Acc: 75.562% | Wgt Acc: 85.515%
	I - Batch: 150 | Loss: 0.750 | Acc: 74.750% | Wgt Acc: 84.915%
	I - Batch: 200 | Loss: 0.750 | Acc: 74.656% | Wgt Acc: 84.798%
I - num batch: 222
I - Train -- Loss: 0.749 | Acc: 74.655% | Wgt Acc: 84.867% | LR: 5.000000e-04 | Dur: 131.99s
I - Confusion Matrix: [row->prediction - col->label]
[[613.   8.  13.  52. 156.]
 [ 12. 521.  24.   7. 114.]
 [ 15.  33. 675.  17. 228.]
 [ 47.   6.  10. 452. 115.]
 [ 10.  10.  12.  10. 387.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.093 | Acc: 62.327% | Wgt Acc: 59.186% | Dur: 13.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   2.  12.  10.]
 [  0.  42.   8.   4.  15.]
 [  2.   8.  36.   4.  19.]
 [ 17.   8.  10.  54.   9.]
 [ 12.  18.  19.  12. 127.]]

I - Local maximum validation set accuracy:  62.33

I - Validation set results: 
[14-1-1--0.13][50-3-4-0.97][124-2-3-0.99][127-0-0-0.99][443-2-2-0.99][567-0-4-0.89][573-1-1-0.80][615-0-0-0.99][695-1-2-0.99][722-3-0-0.54]
[826-0-0-0.92][878-0-3-0.67][1103-0-0-0.48][1212-3-3-0.99][1368-0-0-0.99][2181-2-2-0.12][2476-2-2-0.93][2721-2-2-0.99][2818-1-1-0.61][2886-2-4-0.97]
[3231-2-2-0.99][3333-2-3-0.28][3482-2-2-0.22][3536-3-2-0.48][3625-1-1-0.99][3909-0-0-0.85][4035-0-3-0.42][4140-0-0-0.97][4214-1-3-0.67][4346-1-3-0.97]
[4581-2-1-0.96][4708-3-3-0.96][4838-3-3-0.45][4845-1-1--0.01][4868-0-0-0.99][4939-0-3-0.39][4984-2-3-0.99][5078-1-4-0.98][5396-0-0-0.99][5479-1-1-0.86]
[5717-0-0-0.99][5843-1-4-0.95][5949-3-3-0.98][5987-2-4-0.95][6014-3-1-0.99][6033-3-3--0.11][6313-0-0-0.70][6421-3-3-0.99][6500-1-4-0.95][6583-3-2-0.88]
[6683-3-3-0.73][6825-2-1-0.99][6998-3-2--0.25][7049-3-3-0.99][7517-1-2-0.34][7521-1-1-0.99][7528-1-1-0.68][7949-1-2-0.35][8135-1-4-0.94][8185-3-0-0.99]
[8269-3-4-0.58][8273-3-3-0.21][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.88][8903-1-2-0.85][9001-2-2-0.99][9036-2-2-0.99][9281-3-4-0.65][9300-2-2-0.99]
[9571-0-3-0.44][9617-1-4-0.24][9644-2-2-0.97][9705-2-4-0.97][9801-0-3-0.99][9803-3-3-0.69][9865-3-3-0.99][9896-2-4-0.98][10314-1-2-0.90][10337-3-3-0.99]
[10403-0-4-0.74][10653-2-4-0.99][10704-2-1-0.25][10719-1-1-0.52][10727-1-4-0.99][10836-0-0-0.99][10969-2-3-0.99][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.23][11499-0-0-0.95][11502-3-3-0.98][11512-3-4-0.83][11608-1-1-0.11][11610-0-3-0.88][11692-0-3-0.70][11905-0-0-0.99][11993-1-1-0.99][12002-2-3-0.47]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-2-0.87][12320-1-4-0.99][12377-2-4-0.96][12398-2-3-0.80][12503-1-4-0.99][12617-0-2-0.96][12685-3-1-0.44][12738-2-0-0.52]
[12742-2-2-0.99][12823-0-3-0.64][13110-1-2-0.60][13240-3-3-0.99][13253-1-4-0.99][13273-0-0-0.99][13634-1-3-0.68][13763-2-3-0.11][13905-3-3-0.90][14060-2-4-0.16]
[14065-3-3--0.28][14147-3-3-0.40][14595-2-2-0.93][14687-2-2-0.90][14788-2-2-0.99][14869-1-1-0.80][14872-3-4-0.51][14877-1-1-0.99][14927-0-3-0.87][15066-0-0-0.99]
[15175-1-1-0.20][15178-2-4-0.88][15375-3-0-0.04][15389-3-3-0.98][15568-2-4-0.86][15675-3-3-0.99][15869-1-1--0.17][16207-3-0-0.99][16236-0-2-0.86][16302-3-4-0.06]
[16331-2-2-0.99][16381-0-0-0.68][16488-1-1-0.71][16495-0-0-0.99][16650-0-0-0.98][16719-1-4-0.98][16801-0-0-0.99][16828-0-0-0.97][17137-3-3-0.68][17245-1-1-0.07]
[17278-3-0-0.21][17282-0-0-0.20][17311-2-2-0.99][17336-2-1-0.99][17608-3-3-0.99][17627-0-0-0.49][17877-3-4-0.68][17924-1-1-0.47][17984-3-0-0.99][18211-0-3-0.96]
[18276-3-3-0.91][18287-1-4-0.14][18394-0-0-0.99][18428-0-0-0.99][18442-0-3-0.92][18478-3-3-0.99][18607-0-0-0.99][18616-0-4-0.44][18663-0-0-0.99][18718-0-0-0.18]
[18766-2-2-0.64][18824-2-2-0.95][18890-3-3-0.99][18930-3-4-0.83][18938-3-3-0.98][19817-1-1-0.46][19839-0-4-0.99][19930-3-0-0.38][19944-0-4-0.99][20036-2-2-0.99]
[20101-3-4-0.84][20474-1-1-0.99][20547-3-3-0.84][20929-2-2-0.99][21245-1-1-0.38][21257-3-1-0.48][21293-1-2-0.97][21316-1-1-0.99][21384-1-4-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-4-0.95][21714-0-3-0.58][21943-3-3-0.98][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.82][21998-1-1-0.93][22025-0-4-0.93][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.94][22757-0-0-0.99][22811-3-3-0.99][22976-3-4--0.58][22985-3-3-0.99][23014-0-0-0.95][23112-1-1-0.72][23144-3-3-0.99][23168-2-3-0.82]
[23219-0-4-0.29][23363-3-3-0.99][23470-0-0-0.36][23486-2-2--0.40][23497-0-3-0.99][23516-0-0-0.99][23690-1-4-0.31][23921-2-2-0.99][23936-1-0--0.02][24040-3-4-0.99]
[24111-1-4-0.99][24182-0-0-0.98][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.98][24364-1-3--0.27][24427-3-0-0.94][24477-2-2-0.99][24495-2-4-0.62][24893-2-2-0.94]
[25012-1-1--0.02][25121-2-4-0.45][25165-3-3-0.99][25183-0-4-0.91][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.99][25644-1-2-0.98][25718-1-4-0.94][25774-2-4-0.48]
[26032-3-3-0.95][26051-3-3-0.99][26120-0-4-0.63][26321-1-1-0.38][26732-1-1-0.46][26784-3-3-0.99][26827-3-3-0.82][26833-0-3-0.99][26838-2-2-0.30][26860-1-1-0.82]
[26948-0-0-0.91][27049-3-0--0.03][27098-1-1-0.52][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.95][28040-0-4-0.82][28503-2-2-0.99]
[28577-1-1-0.97][28959-0-0-0.99][29198-3-3-0.88][29777-0-0-0.99][29877-2-1-0.10][30035-1-3--0.03][30098-0-3-0.99][30326-1-1-0.99][30572-2-2-0.92][30716-0-4-0.99]
[30806-2-4-0.94][30906-1-1-0.81][31007-0-0-0.96][31181-3-3-0.11][31238-0-0-0.35][31347-0-0-0.99][31422-2-4-0.80][31429-3-1-0.86][31431-0-0-0.99][31432-1-1-0.99]
[31477-0-3-0.86][31524-1-4-0.21][31597-1-1-0.61][31619-1-0-0.49][31701-0-0-0.61][31755-0-0-0.97][31854-3-3-0.98][32074-1-3-0.57][32078-3-3-0.99][32111-1-1-0.22]
[32127-1-1-0.98][32140-3-3-0.99][32263-2-4-0.80][32365-0-0-0.98][32411-2-3-0.99][32429-3-3-0.58][32473-3-3-0.99][32574-3-3-0.99][32584-0-4-0.75][32622-0-0-0.39]
[32858-3-0-0.99][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.50][33133-2-1-0.87][33173-2-1-0.13][33175-3-4-0.99][33306-3-3-0.99][33309-2-2-0.89]
[33474-0-0-0.99][33478-2-1-0.84][33618-1-1-0.99][33712-0-0-0.60][33782-2-4-0.98][33914-3-3-0.99][34076-3-2--0.37][34112-2-2-0.98][34138-2-3-0.99][34239-1-3-0.22]
[34364-2-2-0.98][34617-1-4-0.17][34751-3-3-0.63][34783-2-4-0.99][35015-3-3-0.99][35018-1-4-0.34][35288-2-4-0.80][0-4-4-0.92][1-4-4-0.99][2-4-4-0.99]
[3-4-4-0.99][4-4-2-0.45][5-4-1-0.70][6-4-4-0.99][7-4-4-0.98][8-4-2-0.48][9-4-1-0.99][10-4-4-0.96][11-4-4-0.97][12-4-1-0.51]
[14-4-3-0.94][15-4-0-0.88][16-4-4-0.99][17-4-4-0.17][18-4-4-0.95][19-4-3-0.38][20-4-1-0.42][21-4-2-0.87][22-4-4-0.99][23-4-4-0.02]
[24-4-4-0.99][25-4-4-0.99][26-4-4-0.73][27-4-4-0.99][28-4-4-0.99][29-4-1-0.87][30-4-4-0.28][31-4-4-0.97][32-4-4-0.92][33-4-2-0.99]
[34-4-4-0.99][35-4-3-0.28][37-4-4-0.70][39-4-0-0.99][40-4-0-0.69][41-4-2--0.14][42-4-3-0.99][43-4-4-0.57][45-4-4-0.86][46-4-4-0.99]
[47-4-4-0.99][48-4-4-0.93][51-4-4-0.99][52-4-4-0.76][53-4-4-0.84][54-4-4-0.99][55-4-4-0.95][56-4-1-0.56][57-4-0-0.77][58-4-2-0.60]
[59-4-4-0.99][60-4-4-0.50][61-4-4-0.99][62-4-4-0.91][63-4-2-0.96][64-4-2-0.99][65-4-4-0.99][66-4-4-0.99][67-4-4-0.98][68-4-1-0.98]
[69-4-2-0.71][70-4-2-0.91][72-4-4-0.99][73-4-4-0.86][74-4-2-0.97][75-4-3-0.88][77-4-4-0.99][78-4-1--0.30][79-4-2-0.99][80-4-4-0.95]
[81-4-4-0.96][82-4-4-0.82][83-4-4-0.95][84-4-4-0.99][85-4-4-0.97][86-4-4-0.99][87-4-4-0.99][88-4-4-0.90][89-4-4-0.17][90-4-4-0.01]
[91-4-4--0.15][92-4-4--0.13][93-4-4-0.06][94-4-4-0.99][95-4-4-0.81][96-4-4-0.97][97-4-4-0.97][98-4-3-0.44][99-4-4-0.54][100-4-4-0.99]
[101-4-4-0.99][102-4-1-0.05][103-4-3-0.09][104-4-4-0.99][105-4-4-0.93][106-4-4-0.99][107-4-4-0.99][108-4-4-0.82][109-4-4-0.99][110-4-4-0.96]
[111-4-0-0.99][112-4-2-0.99][113-4-4-0.99][114-4-2--0.35][115-4-4-0.99][116-4-4-0.77][117-4-4-0.99][119-4-4-0.77][121-4-4-0.99][122-4-4-0.99]
[124-4-1-0.42][125-4-2-0.99][126-4-4-0.99][127-4-2-0.49][128-4-4--0.03][129-4-4-0.96][130-4-4-0.33][131-4-2--0.18][132-4-4-0.87][133-4-4-0.99]
[135-4-1-0.32][136-4-4-0.25][137-4-4-0.67][138-4-4-0.26][139-4-4-0.99][140-4-4-0.97][141-4-3-0.75][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.99][148-4-0-0.99][149-4-4-0.95][150-4-4-0.99][151-4-4-0.99][152-4-4-0.79][153-4-4-0.98][154-4-4-0.99][155-4-4-0.99][156-4-4-0.51]
[157-4-0-0.95][158-4-4-0.98][160-4-4-0.98][161-4-2-0.97][162-4-1--0.25][164-4-4-0.95][165-4-4-0.99][167-4-4-0.99][168-4-4-0.99][170-4-4-0.31]
[171-4-4-0.99][172-4-4-0.99][173-4-4-0.99][174-4-0-0.99][175-4-4-0.99][177-4-0-0.99][178-4-4-0.99][179-4-4-0.99][180-4-4-0.99][181-4-4-0.80]
[182-4-3-0.87][183-4-4-0.99][184-4-4-0.95][186-4-4--0.37][187-4-2-0.46][188-4-4-0.99][189-4-4-0.99][190-4-1--0.08][191-4-4-0.99][192-4-4-0.99]
[193-4-1-0.97][194-4-1-0.61][195-4-0-0.61][196-4-4-0.35][197-4-4-0.93][198-4-4-0.99][199-4-4-0.83]
---------------------------
I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.699 | Acc: 78.625% | Wgt Acc: 87.744%
	I - Batch: 100 | Loss: 0.687 | Acc: 79.938% | Wgt Acc: 88.800%
	I - Batch: 150 | Loss: 0.695 | Acc: 79.333% | Wgt Acc: 88.466%
	I - Batch: 200 | Loss: 0.690 | Acc: 79.750% | Wgt Acc: 88.824%
I - num batch: 222
I - Train -- Loss: 0.695 | Acc: 79.306% | Wgt Acc: 88.416% | LR: 2.500000e-04 | Dur: 133.82s
I - Confusion Matrix: [row->prediction - col->label]
[[632.   2.  11.  41. 123.]
 [  9. 545.  11.   7. 107.]
 [  9.  13. 689.   9. 184.]
 [ 32.   4.   7. 474. 113.]
 [ 15.  14.  16.   7. 473.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.115 | Acc: 59.369% | Wgt Acc: 63.049% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  5.  4. 17. 23.]
 [ 0. 44. 16.  3. 24.]
 [ 2. 16. 40.  2. 29.]
 [13.  7. 11. 60. 14.]
 [ 6.  6.  4.  4. 90.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 0.680 | Acc: 78.750% | Wgt Acc: 89.690%
	I - Batch: 100 | Loss: 0.674 | Acc: 79.750% | Wgt Acc: 90.306%
	I - Batch: 150 | Loss: 0.671 | Acc: 80.125% | Wgt Acc: 90.404%
	I - Batch: 200 | Loss: 0.671 | Acc: 80.062% | Wgt Acc: 90.295%
I - num batch: 222
I - Train -- Loss: 0.671 | Acc: 80.096% | Wgt Acc: 90.149% | LR: 2.500000e-04 | Dur: 135.03s
I - Confusion Matrix: [row->prediction - col->label]
[[642.   2.   8.  25. 159.]
 [  6. 554.   5.   2.  96.]
 [ 15.   8. 699.   6. 176.]
 [ 21.   1.   4. 497. 120.]
 [ 13.  13.  18.   8. 449.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.111 | Acc: 60.158% | Wgt Acc: 60.985% | Dur: 14.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   3.  12.   9.]
 [  2.  37.   9.   0.  12.]
 [  1.  17.  40.   4.  34.]
 [ 19.  10.  13.  63.  20.]
 [  6.  10.  10.   7. 105.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.642 | Acc: 80.875% | Wgt Acc: 90.924%
	I - Batch: 100 | Loss: 0.644 | Acc: 81.750% | Wgt Acc: 90.934%
	I - Batch: 150 | Loss: 0.655 | Acc: 81.208% | Wgt Acc: 90.646%
	I - Batch: 200 | Loss: 0.659 | Acc: 81.188% | Wgt Acc: 90.668%
I - num batch: 222
I - Train -- Loss: 0.660 | Acc: 81.308% | Wgt Acc: 90.608% | LR: 2.500000e-04 | Dur: 134.62s
I - Confusion Matrix: [row->prediction - col->label]
[[645.   7.   2.  26. 145.]
 [  4. 552.  10.   4.  81.]
 [  5.  10. 705.   9. 177.]
 [ 29.   2.   8. 495. 110.]
 [ 14.   7.   9.   4. 487.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.128 | Acc: 60.355% | Wgt Acc: 61.711% | Dur: 14.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   3.  10.  11.]
 [  1.  43.   8.   5.  18.]
 [  3.  11.  38.   2.  32.]
 [ 16.  11.  14.  62.  16.]
 [  8.  11.  12.   7. 103.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 0.644 | Acc: 81.250% | Wgt Acc: 91.217%
	I - Batch: 100 | Loss: 0.654 | Acc: 80.438% | Wgt Acc: 90.018%
	I - Batch: 150 | Loss: 0.658 | Acc: 80.125% | Wgt Acc: 90.037%
	I - Batch: 200 | Loss: 0.659 | Acc: 80.312% | Wgt Acc: 90.291%
I - num batch: 222
I - Train -- Loss: 0.658 | Acc: 80.434% | Wgt Acc: 90.387% | LR: 2.500000e-04 | Dur: 136.31s
I - Confusion Matrix: [row->prediction - col->label]
[[651.   4.   9.  27. 136.]
 [  4. 553.   6.   1. 118.]
 [  8.   9. 703.  12. 175.]
 [ 22.   4.   4. 491. 116.]
 [ 12.   8.  12.   7. 455.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.134 | Acc: 60.158% | Wgt Acc: 62.761% | Dur: 14.94s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  6.  8. 18. 25.]
 [ 2. 43.  9.  4. 16.]
 [ 3. 13. 40.  1. 27.]
 [ 9.  7.  8. 56. 16.]
 [ 4.  9. 10.  7. 96.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.666 | Acc: 81.000% | Wgt Acc: 91.087%
	I - Batch: 100 | Loss: 0.653 | Acc: 81.000% | Wgt Acc: 91.221%
	I - Batch: 150 | Loss: 0.653 | Acc: 80.875% | Wgt Acc: 91.117%
	I - Batch: 200 | Loss: 0.653 | Acc: 81.219% | Wgt Acc: 91.183%
I - num batch: 222
I - Train -- Loss: 0.657 | Acc: 80.942% | Wgt Acc: 90.954% | LR: 2.500000e-04 | Dur: 136.52s
I - Confusion Matrix: [row->prediction - col->label]
[[658.   3.   6.  28. 144.]
 [  4. 555.   4.   1. 102.]
 [ 10.   7. 705.   9. 168.]
 [ 14.   1.   7. 495. 128.]
 [ 11.  12.  12.   5. 458.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.102 | Acc: 59.763% | Wgt Acc: 60.766% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   2.  10.  22.]
 [  1.  42.  19.   4.  21.]
 [  1.  12.  32.   1.  18.]
 [ 17.   9.  15.  65.  15.]
 [  9.  11.   7.   6. 104.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.624 | Acc: 82.000% | Wgt Acc: 91.460%
	I - Batch: 100 | Loss: 0.621 | Acc: 82.312% | Wgt Acc: 91.998%
	I - Batch: 150 | Loss: 0.629 | Acc: 81.875% | Wgt Acc: 91.742%
	I - Batch: 200 | Loss: 0.630 | Acc: 81.844% | Wgt Acc: 91.686%
I - num batch: 222
I - Train -- Loss: 0.630 | Acc: 82.126% | Wgt Acc: 91.859% | LR: 1.250000e-04 | Dur: 133.82s
I - Confusion Matrix: [row->prediction - col->label]
[[654.   4.   4.  19. 153.]
 [  5. 563.   9.   1. 100.]
 [  3.   4. 705.   4. 151.]
 [ 20.   1.   2. 510. 115.]
 [ 15.   6.  14.   4. 481.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.097 | Acc: 61.538% | Wgt Acc: 62.426% | Dur: 16.62s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   6.   3.  17.  27.]
 [  2.  43.  12.   1.  18.]
 [  2.  10.  34.   3.  20.]
 [ 10.  13.  16.  60.   8.]
 [  6.   6.  10.   5. 107.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.609 | Acc: 84.000% | Wgt Acc: 93.055%
	I - Batch: 100 | Loss: 0.605 | Acc: 85.375% | Wgt Acc: 93.734%
	I - Batch: 150 | Loss: 0.608 | Acc: 84.667% | Wgt Acc: 93.234%
	I - Batch: 200 | Loss: 0.606 | Acc: 84.688% | Wgt Acc: 93.361%
I - num batch: 208
I - Train -- Loss: 0.606 | Acc: 84.826% | Wgt Acc: 93.319% | LR: 1.250000e-04 | Dur: 125.60s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   3.   4.  15.  99.]
 [  1. 560.   4.   1.  80.]
 [  5.   9. 717.   2. 146.]
 [ 15.   0.   2. 513.  86.]
 [ 13.   6.   7.   7. 370.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.141 | Acc: 57.988% | Wgt Acc: 61.931% | Dur: 14.23s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  7.  7. 21. 36.]
 [ 1. 44. 11.  3. 16.]
 [ 1. 10. 37.  2. 23.]
 [10. 11. 14. 56. 19.]
 [ 5.  6.  6.  4. 86.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.625 | Acc: 82.750% | Wgt Acc: 92.722%
	I - Batch: 100 | Loss: 0.622 | Acc: 82.062% | Wgt Acc: 92.404%
	I - Batch: 150 | Loss: 0.620 | Acc: 83.167% | Wgt Acc: 92.633%
	I - Batch: 200 | Loss: 0.623 | Acc: 82.375% | Wgt Acc: 92.316%
I - num batch: 222
I - Train -- Loss: 0.620 | Acc: 82.774% | Wgt Acc: 92.515% | LR: 1.250000e-04 | Dur: 134.64s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   2.   4.  16. 145.]
 [  4. 560.   4.   1.  92.]
 [  5.   5. 707.   1. 160.]
 [ 15.   1.   6. 515. 116.]
 [  6.  10.  13.   5. 487.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.064 | Acc: 64.497% | Wgt Acc: 65.264% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   2.  12.  17.]
 [  2.  46.  10.   2.  16.]
 [  2.  12.  40.   1.  23.]
 [ 12.   4.  11.  63.  11.]
 [  7.  12.  12.   8. 113.]]

I - Local maximum validation set accuracy:  64.50

I - Validation set results: 
[14-1-1-0.18][50-3-4--0.11][124-2-2-0.00][127-0-0-0.99][443-2-2-0.99][567-0-0-0.10][573-1-1-0.48][615-0-0-0.58][695-1-2-0.85][722-3-3-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-0-0.99][1212-3-3-0.89][1368-0-0-0.99][2181-2-2--0.36][2476-2-2-0.99][2721-2-2-0.99][2818-1-1-0.88][2886-2-1-0.48]
[3231-2-2-0.99][3333-2-3--0.26][3482-2-4-0.08][3536-3-3-0.53][3625-1-1-0.95][3909-0-0-0.91][4035-0-3-0.70][4140-0-0-0.94][4214-1-1-0.72][4346-1-0--0.31]
[4581-2-1-0.99][4708-3-3-0.15][4838-3-3-0.99][4845-1-2--0.17][4868-0-0-0.67][4939-0-1--0.10][4984-2-3-0.97][5078-1-4-0.78][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.99][5843-1-1-0.99][5949-3-3--0.24][5987-2-4-0.90][6014-3-3-0.73][6033-3-4--0.30][6313-0-0-0.98][6421-3-3-0.99][6500-1-1-0.00][6583-3-3-0.77]
[6683-3-3-0.49][6825-2-1-0.99][6998-3-3--0.13][7049-3-3-0.99][7517-1-1-0.66][7521-1-1-0.81][7528-1-2-0.11][7949-1-2-0.46][8135-1-0-0.98][8185-3-0-0.99]
[8269-3-4-0.50][8273-3-3-0.96][8543-3-0-0.99][8666-1-1-0.96][8672-0-0-0.62][8903-1-2-0.99][9001-2-2-0.99][9036-2-2-0.81][9281-3-1-0.73][9300-2-2-0.99]
[9571-0-3-0.73][9617-1-1-0.34][9644-2-2-0.71][9705-2-4-0.45][9801-0-3-0.96][9803-3-3-0.68][9865-3-3-0.99][9896-2-2-0.31][10314-1-2--0.11][10337-3-3-0.99]
[10403-0-4-0.35][10653-2-4-0.83][10704-2-1-0.63][10719-1-1-0.81][10727-1-4-0.19][10836-0-0-0.99][10969-2-3-0.61][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.88][11502-3-3-0.99][11512-3-3-0.87][11608-1-1-0.85][11610-0-0-0.99][11692-0-0-0.53][11905-0-0-0.99][11993-1-1-0.90][12002-2-2-0.56]
[12052-0-0-0.99][12201-0-3-0.97][12235-2-2-0.53][12320-1-4-0.98][12377-2-4-0.82][12398-2-3-0.96][12503-1-4-0.86][12617-0-2-0.63][12685-3-1-0.67][12738-2-3--0.33]
[12742-2-2-0.99][12823-0-0-0.57][13110-1-2-0.44][13240-3-3-0.99][13253-1-1-0.41][13273-0-0-0.99][13634-1-4-0.62][13763-2-2--0.39][13905-3-3--0.29][14060-2-1-0.99]
[14065-3-3--0.47][14147-3-0-0.80][14595-2-2-0.99][14687-2-2-0.99][14788-2-2-0.99][14869-1-1-0.99][14872-3-4--0.26][14877-1-1-0.97][14927-0-3-0.99][15066-0-0-0.99]
[15175-1-1--0.14][15178-2-3-0.36][15375-3-3-0.10][15389-3-3-0.99][15568-2-1-0.98][15675-3-3-0.99][15869-1-0-0.05][16207-3-0-0.99][16236-0-0-0.99][16302-3-0--0.45]
[16331-2-2-0.99][16381-0-3-0.43][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.91][16719-1-3-0.95][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.20][17245-1-4-0.33]
[17278-3-0-0.76][17282-0-0-0.62][17311-2-2-0.99][17336-2-1--0.09][17608-3-3-0.99][17627-0-0--0.21][17877-3-4-0.36][17924-1-3--0.25][17984-3-3-0.99][18211-0-3-0.35]
[18276-3-3-0.80][18287-1-1-0.06][18394-0-0-0.99][18428-0-0--0.30][18442-0-3-0.80][18478-3-3-0.98][18607-0-0-0.99][18616-0-0-0.96][18663-0-0-0.98][18718-0-0-0.83]
[18766-2-2-0.92][18824-2-2-0.93][18890-3-3-0.99][18930-3-4-0.04][18938-3-3-0.87][19817-1-1-0.88][19839-0-2-0.56][19930-3-3-0.99][19944-0-4-0.77][20036-2-2-0.99]
[20101-3-3-0.60][20474-1-1-0.99][20547-3-3-0.14][20929-2-2-0.27][21245-1-1-0.16][21257-3-3--0.14][21293-1-2-0.99][21316-1-1-0.99][21384-1-1-0.90][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-3--0.63][21943-3-3-0.86][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.99][21998-1-2-0.25][22025-0-4-0.36][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.99][22757-0-0-0.99][22811-3-3-0.96][22976-3-4-0.40][22985-3-3-0.98][23014-0-0-0.97][23112-1-1-0.89][23144-3-3-0.99][23168-2-3-0.16]
[23219-0-0-0.28][23363-3-3-0.99][23470-0-0-0.73][23486-2-4-0.27][23497-0-3-0.99][23516-0-0-0.99][23690-1-4-0.98][23921-2-2-0.92][23936-1-2--0.45][24040-3-0-0.36]
[24111-1-4-0.99][24182-0-0-0.82][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.99][24364-1-2--0.05][24427-3-0-0.91][24477-2-2-0.99][24495-2-1--0.44][24893-2-2-0.88]
[25012-1-1-0.08][25121-2-4-0.55][25165-3-3-0.98][25183-0-0-0.97][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.66][25644-1-1-0.99][25718-1-4-0.72][25774-2-4--0.41]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.99][26321-1-1-0.85][26732-1-1-0.93][26784-3-3-0.99][26827-3-3-0.64][26833-0-3-0.99][26838-2-3--0.12][26860-1-4-0.45]
[26948-0-0-0.95][27049-3-0-0.99][27098-1-1-0.14][27526-0-0-0.72][27639-3-3-0.88][27698-3-3-0.24][27772-0-0-0.84][27890-1-1-0.73][28040-0-4-0.66][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.91][29198-3-3-0.97][29777-0-0-0.99][29877-2-1-0.29][30035-1-1-0.98][30098-0-3-0.99][30326-1-1-0.99][30572-2-2-0.60][30716-0-4-0.99]
[30806-2-4-0.68][30906-1-1-0.99][31007-0-0-0.88][31181-3-3-0.31][31238-0-0-0.44][31347-0-0-0.89][31422-2-2-0.53][31429-3-3-0.25][31431-0-0-0.99][31432-1-1-0.90]
[31477-0-0-0.99][31524-1-2-0.07][31597-1-1-0.63][31619-1-0-0.98][31701-0-0-0.80][31755-0-0-0.99][31854-3-3-0.73][32074-1-1-0.35][32078-3-3-0.99][32111-1-1-0.96]
[32127-1-1-0.98][32140-3-3-0.99][32263-2-0-0.31][32365-0-0-0.99][32411-2-3-0.99][32429-3-3-0.91][32473-3-3-0.39][32574-3-3-0.99][32584-0-4-0.88][32622-0-1-0.53]
[32858-3-0-0.99][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.18][33133-2-2-0.99][33173-2-2-0.73][33175-3-4-0.96][33306-3-3-0.86][33309-2-3-0.87]
[33474-0-0-0.96][33478-2-1-0.18][33618-1-1-0.99][33712-0-0-0.36][33782-2-4-0.08][33914-3-3-0.85][34076-3-2--0.85][34112-2-2-0.99][34138-2-3-0.83][34239-1-3-0.14]
[34364-2-2-0.99][34617-1-4-0.20][34751-3-3-0.79][34783-2-4-0.79][35015-3-3-0.71][35018-1-4-0.73][35288-2-4-0.31][0-4-4-0.45][1-4-4-0.98][2-4-4-0.99]
[3-4-4-0.82][4-4-4--0.15][5-4-1--0.43][6-4-4-0.98][7-4-4-0.36][8-4-2-0.24][9-4-0-0.72][10-4-4-0.99][11-4-4-0.99][12-4-1-0.79]
[14-4-3-0.99][15-4-3-0.99][16-4-4-0.98][17-4-4-0.26][18-4-4-0.15][19-4-0--0.32][20-4-2-0.68][21-4-1-0.99][22-4-4-0.99][23-4-4-0.89]
[24-4-4-0.99][25-4-3-0.75][26-4-1-0.07][27-4-4-0.99][28-4-4-0.99][29-4-1-0.63][30-4-4-0.44][31-4-4-0.98][32-4-4-0.74][33-4-2-0.86]
[34-4-4-0.86][35-4-4-0.79][37-4-4-0.84][39-4-0-0.99][40-4-4-0.15][41-4-4--0.24][42-4-3-0.25][43-4-4-0.78][45-4-2--0.05][46-4-4-0.99]
[47-4-4-0.99][48-4-4-0.85][51-4-4-0.99][52-4-4-0.68][53-4-2-0.41][54-4-4-0.10][55-4-4-0.29][56-4-1-0.48][57-4-0-0.99][58-4-2--0.35]
[59-4-0-0.99][60-4-4-0.67][61-4-4-0.99][62-4-4-0.78][63-4-2-0.99][64-4-2-0.98][65-4-4-0.99][66-4-4-0.99][67-4-4-0.71][68-4-1-0.99]
[69-4-3--0.47][70-4-2-0.89][72-4-4-0.67][73-4-1-0.94][74-4-2-0.89][75-4-3--0.50][77-4-4-0.99][78-4-3-0.09][79-4-4-0.84][80-4-4-0.99]
[81-4-4-0.38][82-4-4-0.70][83-4-1-0.11][84-4-4-0.99][85-4-4-0.99][86-4-4-0.87][87-4-4-0.99][88-4-4-0.96][89-4-3--0.18][90-4-0--0.17]
[91-4-4-0.48][92-4-4-0.26][93-4-0-0.05][94-4-4-0.99][95-4-4-0.89][96-4-4-0.32][97-4-4-0.98][98-4-3--0.32][99-4-4-0.97][100-4-1-0.67]
[101-4-4-0.99][102-4-4-0.32][103-4-0--0.41][104-4-4-0.99][105-4-4-0.55][106-4-4-0.99][107-4-4-0.99][108-4-4-0.41][109-4-4-0.91][110-4-4-0.55]
[111-4-0-0.99][112-4-2-0.62][113-4-4-0.77][114-4-0--0.40][115-4-4-0.71][116-4-2-0.76][117-4-4-0.99][119-4-4-0.37][121-4-4-0.89][122-4-4-0.99]
[124-4-1-0.38][125-4-4-0.99][126-4-4-0.99][127-4-2-0.06][128-4-0-0.46][129-4-4-0.45][130-4-4--0.06][131-4-2-0.55][132-4-4-0.33][133-4-4-0.99]
[135-4-2-0.46][136-4-4-0.17][137-4-4-0.26][138-4-2-0.55][139-4-4-0.99][140-4-1-0.29][141-4-0-0.99][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.91][148-4-0-0.99][149-4-4-0.46][150-4-4-0.99][151-4-4-0.99][152-4-4-0.95][153-4-4-0.78][154-4-4-0.99][155-4-4-0.99][156-4-4--0.03]
[157-4-2-0.59][158-4-2-0.27][160-4-1-0.16][161-4-2-0.99][162-4-2--0.02][164-4-4-0.64][165-4-4-0.99][167-4-4-0.79][168-4-4-0.78][170-4-4--0.19]
[171-4-4-0.99][172-4-4-0.99][173-4-4-0.99][174-4-0-0.99][175-4-4-0.91][177-4-0-0.99][178-4-4-0.60][179-4-4-0.90][180-4-4-0.99][181-4-3-0.59]
[182-4-3-0.95][183-4-4-0.99][184-4-4-0.99][186-4-4-0.54][187-4-2-0.65][188-4-4-0.95][189-4-4-0.94][190-4-1--0.13][191-4-4-0.99][192-4-4-0.74]
[193-4-1-0.43][194-4-0-0.38][195-4-0-0.12][196-4-2-0.21][197-4-1-0.95][198-4-4-0.99][199-4-2-0.99]
---------------------------
I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.596 | Acc: 85.000% | Wgt Acc: 93.694%
	I - Batch: 100 | Loss: 0.610 | Acc: 84.938% | Wgt Acc: 93.612%
	I - Batch: 150 | Loss: 0.606 | Acc: 85.125% | Wgt Acc: 93.780%
	I - Batch: 200 | Loss: 0.607 | Acc: 84.656% | Wgt Acc: 93.718%
I - num batch: 222
I - Train -- Loss: 0.607 | Acc: 84.945% | Wgt Acc: 93.789% | LR: 1.250000e-04 | Dur: 133.43s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   2.   4.  10. 127.]
 [  2. 566.   3.   1.  90.]
 [  4.   4. 714.   4. 143.]
 [  6.   1.   1. 519. 100.]
 [ 11.   5.  12.   4. 540.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.092 | Acc: 61.736% | Wgt Acc: 62.450% | Dur: 14.09s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   3.  14.  13.]
 [  1.  42.   9.   2.  14.]
 [  6.  20.  45.   3.  33.]
 [ 15.   6.   8.  59.  12.]
 [  7.   7.  10.   8. 108.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.598 | Acc: 84.500% | Wgt Acc: 93.942%
	I - Batch: 100 | Loss: 0.596 | Acc: 85.375% | Wgt Acc: 94.226%
	I - Batch: 150 | Loss: 0.607 | Acc: 84.583% | Wgt Acc: 93.644%
	I - Batch: 200 | Loss: 0.611 | Acc: 83.938% | Wgt Acc: 93.092%
I - num batch: 222
I - Train -- Loss: 0.610 | Acc: 84.212% | Wgt Acc: 93.183% | LR: 1.250000e-04 | Dur: 132.70s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   2.   4.  15. 167.]
 [  3. 565.   4.   2.  76.]
 [  3.   3. 714.   3. 145.]
 [ 13.   0.   2. 513.  84.]
 [ 11.   8.  10.   5. 528.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.077 | Acc: 61.736% | Wgt Acc: 62.945% | Dur: 13.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   6.   3.  16.  23.]
 [  0.  39.  10.   0.  15.]
 [  3.  15.  35.   1.  19.]
 [  9.   9.  15.  64.  17.]
 [  7.   9.  12.   5. 106.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.597 | Acc: 86.250% | Wgt Acc: 94.120%
	I - Batch: 100 | Loss: 0.599 | Acc: 85.625% | Wgt Acc: 93.903%
	I - Batch: 150 | Loss: 0.603 | Acc: 84.833% | Wgt Acc: 93.692%
	I - Batch: 200 | Loss: 0.607 | Acc: 84.656% | Wgt Acc: 93.575%
I - num batch: 222
I - Train -- Loss: 0.608 | Acc: 84.635% | Wgt Acc: 93.500% | LR: 1.250000e-04 | Dur: 131.34s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   3.   7.  13. 143.]
 [  2. 561.   3.   1.  93.]
 [  1.   7. 713.   3. 146.]
 [ 10.   1.   3. 518.  82.]
 [ 10.   6.   8.   3. 536.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.056 | Acc: 64.497% | Wgt Acc: 62.403% | Dur: 13.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   4.   3.   9.  11.]
 [  0.  36.   5.   2.   7.]
 [  3.  19.  48.   3.  27.]
 [ 15.   5.   6.  61.   9.]
 [ 14.  14.  13.  11. 126.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.601 | Acc: 84.375% | Wgt Acc: 93.979%
	I - Batch: 100 | Loss: 0.591 | Acc: 85.750% | Wgt Acc: 94.301%
	I - Batch: 150 | Loss: 0.595 | Acc: 85.250% | Wgt Acc: 93.940%
	I - Batch: 200 | Loss: 0.596 | Acc: 85.031% | Wgt Acc: 93.771%
I - num batch: 222
I - Train -- Loss: 0.597 | Acc: 84.719% | Wgt Acc: 93.726% | LR: 1.250000e-04 | Dur: 134.06s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   2.   1.  11. 140.]
 [  2. 565.   3.   0.  75.]
 [  5.   3. 717.   4. 163.]
 [  8.   2.   2. 519.  90.]
 [ 10.   6.  11.   4. 532.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 61.538% | Wgt Acc: 63.326% | Dur: 15.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   3.  13.  14.]
 [  2.  45.  10.   3.  19.]
 [  1.  12.  40.   2.  30.]
 [ 15.   8.  10.  62.  14.]
 [  8.   9.  12.   6. 103.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.583 | Acc: 86.250% | Wgt Acc: 95.116%
	I - Batch: 100 | Loss: 0.587 | Acc: 85.812% | Wgt Acc: 94.686%
	I - Batch: 150 | Loss: 0.594 | Acc: 85.542% | Wgt Acc: 94.338%
	I - Batch: 200 | Loss: 0.594 | Acc: 85.531% | Wgt Acc: 94.312%
I - num batch: 222
I - Train -- Loss: 0.592 | Acc: 85.847% | Wgt Acc: 94.370% | LR: 1.250000e-04 | Dur: 134.54s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   3.   1.  11. 119.]
 [  4. 569.   4.   2.  87.]
 [  2.   1. 720.   3. 140.]
 [  8.   0.   2. 519.  94.]
 [  6.   5.   7.   3. 560.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.052 | Acc: 65.483% | Wgt Acc: 62.242% | Dur: 18.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   6.   3.  14.  16.]
 [  1.  37.  11.   1.  11.]
 [  2.  12.  34.   1.  11.]
 [  8.   7.  10.  59.   9.]
 [  8.  16.  17.  11. 133.]]

I - Local maximum validation set accuracy:  65.48

I - Validation set results: 
[14-1-2--0.07][50-3-4-0.85][124-2-4-0.62][127-0-0-0.99][443-2-2-0.99][567-0-0-0.94][573-1-1-0.87][615-0-0-0.50][695-1-2-0.99][722-3-3-0.97]
[826-0-0-0.99][878-0-0-0.99][1103-0-4-0.47][1212-3-0--0.01][1368-0-0-0.99][2181-2-2-0.32][2476-2-2-0.99][2721-2-2-0.98][2818-1-3-0.78][2886-2-4-0.54]
[3231-2-2-0.99][3333-2-3-0.99][3482-2-2-0.44][3536-3-3-0.99][3625-1-1-0.99][3909-0-0-0.87][4035-0-3-0.04][4140-0-0-0.94][4214-1-4-0.69][4346-1-4--0.08]
[4581-2-1-0.75][4708-3-3-0.49][4838-3-3-0.51][4845-1-3--0.30][4868-0-0-0.99][4939-0-1-0.51][4984-2-2-0.98][5078-1-4-0.88][5396-0-0-0.99][5479-1-1-0.98]
[5717-0-0-0.15][5843-1-1-0.99][5949-3-3--0.27][5987-2-4-0.97][6014-3-3-0.78][6033-3-3--0.06][6313-0-0-0.99][6421-3-3-0.99][6500-1-4-0.12][6583-3-3-0.63]
[6683-3-3-0.80][6825-2-1-0.99][6998-3-3--0.13][7049-3-3-0.97][7517-1-1-0.68][7521-1-0-0.04][7528-1-1--0.05][7949-1-2-0.24][8135-1-0-0.99][8185-3-0-0.99]
[8269-3-4-0.97][8273-3-3-0.57][8543-3-0-0.75][8666-1-1-0.99][8672-0-0-0.77][8903-1-2-0.94][9001-2-2-0.99][9036-2-2-0.99][9281-3-1--0.11][9300-2-2-0.99]
[9571-0-3--0.14][9617-1-4--0.00][9644-2-1-0.40][9705-2-4-0.78][9801-0-3-0.23][9803-3-3-0.61][9865-3-3-0.99][9896-2-2-0.98][10314-1-2-0.17][10337-3-3-0.99]
[10403-0-4-0.94][10653-2-4-0.95][10704-2-1-0.24][10719-1-1-0.86][10727-1-4-0.72][10836-0-0-0.99][10969-2-3-0.31][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.99][11502-3-3-0.83][11512-3-3-0.99][11608-1-1-0.30][11610-0-0-0.99][11692-0-0-0.75][11905-0-0-0.98][11993-1-1-0.88][12002-2-2-0.68]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-4-0.58][12320-1-4-0.98][12377-2-4-0.97][12398-2-3-0.97][12503-1-4-0.99][12617-0-2-0.75][12685-3-3--0.18][12738-2-4-0.11]
[12742-2-2-0.99][12823-0-0-0.96][13110-1-2-0.82][13240-3-3-0.99][13253-1-1-0.65][13273-0-0-0.99][13634-1-4-0.47][13763-2-1--0.64][13905-3-3-0.04][14060-2-1-0.99]
[14065-3-3-0.11][14147-3-3-0.63][14595-2-2-0.91][14687-2-2-0.99][14788-2-2-0.98][14869-1-1-0.99][14872-3-4-0.42][14877-1-1-0.99][14927-0-3-0.93][15066-0-0-0.99]
[15175-1-4--0.04][15178-2-4-0.39][15375-3-0-0.16][15389-3-3--0.13][15568-2-1-0.98][15675-3-3-0.99][15869-1-0-0.70][16207-3-0-0.99][16236-0-0-0.79][16302-3-2--0.26]
[16331-2-2-0.99][16381-0-0-0.53][16488-1-1-0.92][16495-0-0-0.99][16650-0-0-0.99][16719-1-3-0.75][16801-0-0-0.99][16828-0-0-0.99][17137-3-0-0.71][17245-1-4-0.95]
[17278-3-0-0.53][17282-0-0-0.97][17311-2-2-0.97][17336-2-1-0.98][17608-3-3-0.99][17627-0-0-0.60][17877-3-4-0.99][17924-1-3--0.21][17984-3-0-0.99][18211-0-3-0.73]
[18276-3-3-0.90][18287-1-1-0.39][18394-0-0-0.99][18428-0-0-0.92][18442-0-0-0.73][18478-3-3-0.97][18607-0-0-0.99][18616-0-0-0.97][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.51][18824-2-4-0.99][18890-3-3-0.99][18930-3-4-0.81][18938-3-3-0.74][19817-1-1-0.89][19839-0-2-0.51][19930-3-3-0.99][19944-0-4-0.98][20036-2-2-0.99]
[20101-3-3-0.86][20474-1-1-0.86][20547-3-3-0.33][20929-2-2-0.99][21245-1-2-0.08][21257-3-3-0.93][21293-1-2-0.99][21316-1-1-0.99][21384-1-4-0.99][21448-1-1-0.94]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-0--0.00][21943-3-4-0.42][21947-0-0-0.89][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.99][22025-0-4-0.74][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.90][22757-0-0-0.99][22811-3-3-0.99][22976-3-4--0.23][22985-3-3-0.98][23014-0-0-0.99][23112-1-1-0.88][23144-3-3-0.99][23168-2-3-0.37]
[23219-0-0-0.98][23363-3-3-0.99][23470-0-0-0.93][23486-2-0--0.25][23497-0-0-0.99][23516-0-0-0.99][23690-1-3-0.97][23921-2-4-0.40][23936-1-0--0.56][24040-3-4-0.79]
[24111-1-4-0.99][24182-0-0-0.97][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.99][24364-1-2--0.17][24427-3-3-0.91][24477-2-2-0.99][24495-2-1--0.08][24893-2-2-0.51]
[25012-1-3-0.26][25121-2-1-0.21][25165-3-3-0.99][25183-0-0-0.93][25297-3-3-0.99][25398-0-0-0.79][25574-2-2-0.99][25644-1-2-0.99][25718-1-2-0.28][25774-2-3--0.35]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-0-0.99][26321-1-1-0.77][26732-1-1-0.29][26784-3-3-0.99][26827-3-0-0.20][26833-0-3-0.67][26838-2-2-0.02][26860-1-4-0.94]
[26948-0-0-0.99][27049-3-0-0.99][27098-1-0-0.54][27526-0-0-0.54][27639-3-3-0.99][27698-3-3-0.99][27772-0-0-0.99][27890-1-1-0.53][28040-0-4-0.64][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-4-0.98][29777-0-0-0.99][29877-2-3-0.80][30035-1-1-0.98][30098-0-3-0.73][30326-1-1-0.99][30572-2-2-0.46][30716-0-4-0.99]
[30806-2-4-0.87][30906-1-1-0.99][31007-0-0-0.78][31181-3-3--0.10][31238-0-0-0.77][31347-0-0-0.99][31422-2-4-0.28][31429-3-3-0.54][31431-0-0-0.99][31432-1-1-0.78]
[31477-0-0-0.74][31524-1-2--0.03][31597-1-1-0.24][31619-1-0-0.99][31701-0-0-0.93][31755-0-0-0.99][31854-3-3-0.99][32074-1-1-0.32][32078-3-3-0.99][32111-1-1-0.65]
[32127-1-1-0.99][32140-3-3-0.99][32263-2-0-0.96][32365-0-0-0.99][32411-2-3-0.99][32429-3-0-0.76][32473-3-3-0.46][32574-3-0-0.95][32584-0-4-0.83][32622-0-4--0.03]
[32858-3-0-0.99][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-4-0.24][33133-2-2-0.88][33173-2-3--0.19][33175-3-4-0.99][33306-3-3-0.79][33309-2-3-0.96]
[33474-0-0-0.99][33478-2-1-0.69][33618-1-1-0.91][33712-0-0-0.62][33782-2-4-0.99][33914-3-3-0.99][34076-3-4--0.05][34112-2-2-0.99][34138-2-3-0.48][34239-1-4--0.30]
[34364-2-2-0.99][34617-1-4-0.30][34751-3-3-0.88][34783-2-4-0.99][35015-3-3-0.63][35018-1-1-0.80][35288-2-4-0.32][0-4-4-0.99][1-4-4-0.99][2-4-4-0.94]
[3-4-4-0.97][4-4-0-0.48][5-4-1-0.86][6-4-4-0.99][7-4-4-0.51][8-4-4-0.57][9-4-0-0.94][10-4-4-0.99][11-4-4-0.99][12-4-4-0.33]
[14-4-3-0.93][15-4-3-0.62][16-4-4-0.56][17-4-4-0.76][18-4-4-0.99][19-4-3-0.58][20-4-0-0.46][21-4-1-0.99][22-4-4-0.98][23-4-4-0.95]
[24-4-4-0.99][25-4-4-0.98][26-4-4--0.17][27-4-4-0.99][28-4-4-0.99][29-4-1-0.72][30-4-4-0.66][31-4-4-0.83][32-4-4-0.99][33-4-2-0.96]
[34-4-4-0.98][35-4-4-0.97][37-4-4-0.98][39-4-0-0.99][40-4-4-0.90][41-4-4--0.38][42-4-4-0.33][43-4-4-0.99][45-4-1-0.28][46-4-4-0.99]
[47-4-4-0.99][48-4-4-0.99][51-4-4-0.99][52-4-4-0.88][53-4-4-0.80][54-4-4-0.90][55-4-4-0.97][56-4-4-0.97][57-4-0-0.97][58-4-2-0.43]
[59-4-0-0.85][60-4-0-0.16][61-4-4-0.99][62-4-4-0.99][63-4-4-0.71][64-4-2-0.98][65-4-4-0.99][66-4-4-0.99][67-4-4-0.46][68-4-4-0.70]
[69-4-4--0.05][70-4-4-0.94][72-4-4-0.78][73-4-1-0.72][74-4-2-0.96][75-4-3--0.35][77-4-4-0.99][78-4-3-0.95][79-4-4-0.99][80-4-4-0.99]
[81-4-4-0.92][82-4-4-0.94][83-4-4-0.64][84-4-4-0.99][85-4-4-0.99][86-4-4-0.77][87-4-4-0.99][88-4-4-0.97][89-4-4-0.89][90-4-4-0.20]
[91-4-4-0.73][92-4-4-0.41][93-4-0-0.07][94-4-4-0.99][95-4-4-0.61][96-4-4-0.99][97-4-4-0.99][98-4-4-0.60][99-4-4-0.91][100-4-1-0.94]
[101-4-4-0.99][102-4-4-0.96][103-4-3--0.16][104-4-4-0.99][105-4-4-0.82][106-4-4-0.99][107-4-4-0.99][108-4-4-0.89][109-4-4-0.89][110-4-4-0.97]
[111-4-0-0.99][112-4-4-0.56][113-4-4-0.04][114-4-3-0.23][115-4-1-0.99][116-4-4-0.47][117-4-4-0.99][119-4-4-0.99][121-4-4-0.99][122-4-4-0.99]
[124-4-1-0.54][125-4-4-0.99][126-4-4-0.99][127-4-1--0.40][128-4-4-0.62][129-4-4-0.98][130-4-4--0.12][131-4-4-0.32][132-4-4-0.74][133-4-4-0.99]
[135-4-4-0.93][136-4-1-0.54][137-4-4-0.81][138-4-4--0.18][139-4-4-0.99][140-4-4-0.99][141-4-0-0.99][142-4-4-0.99][143-4-4-0.99][144-4-4-0.86]
[145-4-4-0.99][148-4-0-0.99][149-4-4-0.98][150-4-4-0.99][151-4-4-0.99][152-4-4-0.82][153-4-2-0.39][154-4-4-0.99][155-4-4-0.99][156-4-3-0.21]
[157-4-0-0.95][158-4-4-0.53][160-4-2--0.11][161-4-2-0.98][162-4-4-0.70][164-4-4-0.99][165-4-2-0.96][167-4-0-0.93][168-4-4-0.95][170-4-2--0.01]
[171-4-4-0.99][172-4-4-0.99][173-4-4-0.99][174-4-0-0.99][175-4-4-0.99][177-4-4-0.99][178-4-4-0.64][179-4-4-0.57][180-4-4-0.99][181-4-4-0.97]
[182-4-3-0.99][183-4-4-0.99][184-4-4-0.99][186-4-0-0.02][187-4-2-0.65][188-4-4-0.99][189-4-4-0.99][190-4-4-0.70][191-4-4-0.99][192-4-4-0.97]
[193-4-1-0.55][194-4-0-0.86][195-4-4-0.85][196-4-4-0.33][197-4-4-0.99][198-4-4-0.99][199-4-2-0.99]
---------------------------
I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.590 | Acc: 85.375% | Wgt Acc: 94.433%
	I - Batch: 100 | Loss: 0.593 | Acc: 85.188% | Wgt Acc: 94.345%
	I - Batch: 150 | Loss: 0.596 | Acc: 85.333% | Wgt Acc: 94.073%
	I - Batch: 200 | Loss: 0.600 | Acc: 84.906% | Wgt Acc: 93.826%
I - num batch: 222
I - Train -- Loss: 0.600 | Acc: 84.945% | Wgt Acc: 93.769% | LR: 1.250000e-04 | Dur: 136.98s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   3.   5.   9. 135.]
 [  2. 567.   3.   1.  73.]
 [  4.   2. 713.   3. 138.]
 [ 13.   0.   3. 521. 113.]
 [  7.   6.  10.   4. 541.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.067 | Acc: 65.286% | Wgt Acc: 63.810% | Dur: 15.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   6.   3.  17.  11.]
 [  0.  40.  10.   2.  11.]
 [  2.  14.  40.   1.  21.]
 [  7.   7.  10.  56.  13.]
 [  8.  11.  12.  10. 124.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.604 | Acc: 85.375% | Wgt Acc: 94.373%
	I - Batch: 100 | Loss: 0.591 | Acc: 85.750% | Wgt Acc: 94.647%
	I - Batch: 150 | Loss: 0.593 | Acc: 85.417% | Wgt Acc: 94.347%
	I - Batch: 200 | Loss: 0.597 | Acc: 85.344% | Wgt Acc: 93.909%
I - num batch: 222
I - Train -- Loss: 0.597 | Acc: 85.340% | Wgt Acc: 93.959% | LR: 1.250000e-04 | Dur: 134.97s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   3.   4.  10. 111.]
 [  4. 567.   3.   0.  96.]
 [  2.   1. 715.   3. 152.]
 [  6.   2.   2. 521.  89.]
 [ 13.   5.  10.   4. 552.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.101 | Acc: 61.933% | Wgt Acc: 63.960% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   8.   4.  14.  21.]
 [  2.  38.   9.   2.  18.]
 [  2.  16.  45.   3.  27.]
 [ 11.   9.   9.  61.  12.]
 [  5.   7.   8.   6. 102.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.586 | Acc: 86.000% | Wgt Acc: 94.347%
	I - Batch: 100 | Loss: 0.588 | Acc: 86.375% | Wgt Acc: 94.587%
	I - Batch: 150 | Loss: 0.591 | Acc: 85.875% | Wgt Acc: 94.221%
	I - Batch: 200 | Loss: 0.593 | Acc: 85.719% | Wgt Acc: 94.155%
I - num batch: 222
I - Train -- Loss: 0.593 | Acc: 85.763% | Wgt Acc: 94.229% | LR: 1.250000e-04 | Dur: 132.74s
I - Confusion Matrix: [row->prediction - col->label]
[[673.   2.   1.   8. 116.]
 [  3. 570.   5.   0.  73.]
 [  2.   0. 712.   0. 144.]
 [  9.   0.   2. 525. 105.]
 [ 10.   6.  14.   5. 562.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.086 | Acc: 61.538% | Wgt Acc: 63.303% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   7.   4.  16.  20.]
 [  3.  47.  10.   2.  20.]
 [  3.  13.  38.   1.  20.]
 [ 11.   4.  14.  60.  17.]
 [  7.   7.   9.   7. 103.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.587 | Acc: 86.250% | Wgt Acc: 94.887%
	I - Batch: 100 | Loss: 0.591 | Acc: 85.125% | Wgt Acc: 94.319%
	I - Batch: 150 | Loss: 0.591 | Acc: 85.167% | Wgt Acc: 94.114%
	I - Batch: 200 | Loss: 0.588 | Acc: 85.469% | Wgt Acc: 94.272%
I - num batch: 222
I - Train -- Loss: 0.589 | Acc: 85.396% | Wgt Acc: 94.255% | LR: 1.250000e-04 | Dur: 135.31s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   1.   3.   9. 148.]
 [  2. 568.   3.   0.  93.]
 [  4.   5. 714.   0. 120.]
 [  4.   0.   2. 522.  95.]
 [  6.   4.  12.   7. 544.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.029 | Acc: 67.061% | Wgt Acc: 66.209% | Dur: 16.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   3.   3.  10.  17.]
 [  1.  47.   7.   2.  13.]
 [  2.  12.  41.   0.  15.]
 [ 13.   3.   5.  62.  10.]
 [  7.  13.  19.  12. 125.]]

I - Local maximum validation set accuracy:  67.06

I - Validation set results: 
[14-1-1--0.11][50-3-4-0.80][124-2-4-0.38][127-0-0-0.99][443-2-2-0.99][567-0-0-0.41][573-1-1-0.87][615-0-0-0.65][695-1-2-0.99][722-3-3-0.70]
[826-0-0-0.90][878-0-0-0.98][1103-0-0-0.99][1212-3-4-0.73][1368-0-0-0.99][2181-2-2-0.66][2476-2-2-0.99][2721-2-2-0.99][2818-1-3-0.65][2886-2-1-0.90]
[3231-2-2-0.99][3333-2-2--0.17][3482-2-2-0.33][3536-3-3-0.89][3625-1-1-0.92][3909-0-0-0.99][4035-0-3-0.82][4140-0-0-0.99][4214-1-1-0.52][4346-1-4-0.79]
[4581-2-1-0.99][4708-3-3-0.79][4838-3-3-0.46][4845-1-1--0.00][4868-0-0-0.97][4939-0-1-0.33][4984-2-3-0.98][5078-1-4-0.77][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.99][5843-1-1-0.98][5949-3-3-0.35][5987-2-4-0.95][6014-3-3-0.96][6033-3-3-0.69][6313-0-0-0.99][6421-3-3-0.99][6500-1-4-0.90][6583-3-3-0.40]
[6683-3-4-0.12][6825-2-1-0.99][6998-3-3--0.44][7049-3-3-0.99][7517-1-1-0.47][7521-1-1-0.74][7528-1-1-0.04][7949-1-2-0.75][8135-1-0-0.99][8185-3-0-0.99]
[8269-3-1-0.67][8273-3-3-0.70][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.68][8903-1-2-0.99][9001-2-2-0.99][9036-2-2-0.82][9281-3-1-0.20][9300-2-2-0.99]
[9571-0-3-0.35][9617-1-1-0.99][9644-2-1-0.86][9705-2-4-0.96][9801-0-3-0.98][9803-3-3-0.77][9865-3-3-0.99][9896-2-2-0.44][10314-1-2-0.29][10337-3-3-0.99]
[10403-0-4-0.98][10653-2-4-0.99][10704-2-2-0.86][10719-1-1-0.99][10727-1-4-0.79][10836-0-0-0.99][10969-2-4-0.19][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.82][11502-3-3-0.98][11512-3-3-0.58][11608-1-1-0.92][11610-0-0-0.97][11692-0-3-0.97][11905-0-0-0.99][11993-1-1-0.99][12002-2-0-0.99]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-2-0.73][12320-1-0-0.93][12377-2-4-0.99][12398-2-3-0.96][12503-1-4--0.21][12617-0-2-0.69][12685-3-3--0.31][12738-2-3-0.60]
[12742-2-2-0.99][12823-0-0-0.61][13110-1-2-0.31][13240-3-3-0.96][13253-1-1-0.92][13273-0-0-0.99][13634-1-4-0.45][13763-2-2--0.64][13905-3-3--0.30][14060-2-4-0.12]
[14065-3-0--0.25][14147-3-3-0.95][14595-2-2-0.81][14687-2-2-0.99][14788-2-2-0.98][14869-1-1-0.99][14872-3-4-0.38][14877-1-1-0.99][14927-0-3-0.11][15066-0-0-0.99]
[15175-1-1-0.27][15178-2-4-0.48][15375-3-0-0.39][15389-3-3-0.82][15568-2-1-0.69][15675-3-3-0.99][15869-1-0-0.76][16207-3-0-0.98][16236-0-0-0.56][16302-3-4-0.06]
[16331-2-2-0.99][16381-0-3-0.72][16488-1-1-0.91][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.62][16801-0-0-0.99][16828-0-0-0.99][17137-3-3-0.29][17245-1-2-0.12]
[17278-3-4-0.76][17282-0-0-0.89][17311-2-2-0.98][17336-2-1-0.43][17608-3-3-0.99][17627-0-4--0.14][17877-3-0-0.38][17924-1-3-0.54][17984-3-0-0.96][18211-0-3-0.26]
[18276-3-3-0.90][18287-1-1-0.41][18394-0-0-0.99][18428-0-4-0.06][18442-0-3-0.97][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.97][18663-0-0-0.99][18718-0-0-0.88]
[18766-2-2-0.99][18824-2-4-0.99][18890-3-3-0.99][18930-3-4-0.55][18938-3-3-0.99][19817-1-1-0.51][19839-0-2-0.24][19930-3-0-0.59][19944-0-4-0.96][20036-2-2-0.99]
[20101-3-3-0.89][20474-1-1-0.99][20547-3-3-0.65][20929-2-2-0.99][21245-1-2-0.99][21257-3-3-0.44][21293-1-2-0.99][21316-1-1-0.30][21384-1-4-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.98][21714-0-3-0.59][21943-3-4-0.46][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.99][21998-1-1-0.99][22025-0-4-0.99][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.95][22757-0-0-0.99][22811-3-3-0.99][22976-3-4-0.61][22985-3-3-0.87][23014-0-0-0.99][23112-1-1-0.74][23144-3-3-0.99][23168-2-0-0.04]
[23219-0-0-0.98][23363-3-3-0.99][23470-0-0-0.73][23486-2-4-0.31][23497-0-3-0.99][23516-0-0-0.98][23690-1-4-0.62][23921-2-4-0.70][23936-1-2-0.23][24040-3-4-0.38]
[24111-1-4-0.99][24182-0-0-0.95][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.99][24364-1-2--0.39][24427-3-3-0.99][24477-2-2-0.99][24495-2-4-0.59][24893-2-2-0.82]
[25012-1-1-0.56][25121-2-2-0.42][25165-3-3-0.99][25183-0-0-0.92][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.76][25644-1-2-0.99][25718-1-4-0.69][25774-2-4-0.74]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-0-0.87][26321-1-1-0.78][26732-1-1-0.43][26784-3-3-0.99][26827-3-3-0.96][26833-0-3-0.99][26838-2-2-0.35][26860-1-2-0.88]
[26948-0-0-0.97][27049-3-0-0.98][27098-1-1-0.54][27526-0-0-0.99][27639-3-3-0.99][27698-3-3-0.40][27772-0-0-0.98][27890-1-1-0.93][28040-0-0--0.35][28503-2-2-0.99]
[28577-1-1-0.99][28959-0-0-0.99][29198-3-3-0.99][29777-0-0-0.99][29877-2-2-0.83][30035-1-1-0.50][30098-0-3-0.99][30326-1-1-0.99][30572-2-2-0.72][30716-0-4-0.99]
[30806-2-4-0.95][30906-1-1-0.98][31007-0-0-0.98][31181-3-3-0.38][31238-0-0-0.61][31347-0-0-0.99][31422-2-2-0.61][31429-3-3-0.54][31431-0-0--0.12][31432-1-1-0.95]
[31477-0-0-0.95][31524-1-1-0.70][31597-1-1-0.75][31619-1-4-0.99][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.90][32074-1-1-0.37][32078-3-3-0.99][32111-1-1-0.96]
[32127-1-1-0.99][32140-3-3-0.99][32263-2-4-0.74][32365-0-0-0.99][32411-2-3-0.99][32429-3-0-0.63][32473-3-3-0.48][32574-3-3-0.99][32584-0-4-0.94][32622-0-0-0.03]
[32858-3-3-0.84][32969-3-3-0.89][33016-2-2-0.99][33031-1-3-0.99][33035-2-4-0.02][33133-2-2-0.99][33173-2-2-0.06][33175-3-4-0.99][33306-3-3-0.99][33309-2-2-0.99]
[33474-0-0-0.99][33478-2-1-0.67][33618-1-1-0.73][33712-0-0-0.11][33782-2-4-0.99][33914-3-3-0.99][34076-3-4-0.12][34112-2-2-0.99][34138-2-3-0.28][34239-1-1-0.21]
[34364-2-2-0.99][34617-1-4-0.97][34751-3-3-0.92][34783-2-4-0.93][35015-3-3-0.99][35018-1-1-0.64][35288-2-4-0.63][0-4-4-0.97][1-4-4-0.99][2-4-4-0.94]
[3-4-4-0.82][4-4-4-0.75][5-4-2--0.24][6-4-0-0.33][7-4-0-0.99][8-4-4-0.69][9-4-0-0.66][10-4-4-0.99][11-4-4-0.99][12-4-1-0.79]
[14-4-4-0.94][15-4-3-0.99][16-4-4-0.66][17-4-4-0.75][18-4-4-0.85][19-4-3-0.51][20-4-2-0.57][21-4-4-0.68][22-4-4-0.98][23-4-4--0.23]
[24-4-4-0.99][25-4-4-0.99][26-4-4-0.14][27-4-4-0.99][28-4-4-0.99][29-4-1-0.72][30-4-0-0.05][31-4-4-0.96][32-4-4-0.98][33-4-3-0.98]
[34-4-4-0.99][35-4-4-0.98][37-4-4-0.99][39-4-0-0.99][40-4-4-0.38][41-4-4--0.34][42-4-2-0.15][43-4-1-0.87][45-4-4-0.31][46-4-4-0.99]
[47-4-4-0.99][48-4-4-0.99][51-4-4-0.99][52-4-4-0.80][53-4-2-0.57][54-4-3-0.61][55-4-4-0.99][56-4-4-0.96][57-4-3-0.81][58-4-2-0.91]
[59-4-0-0.99][60-4-4-0.38][61-4-4-0.90][62-4-4-0.99][63-4-2-0.99][64-4-2-0.69][65-4-4-0.99][66-4-4-0.90][67-4-1-0.12][68-4-1-0.27]
[69-4-4-0.48][70-4-4-0.95][72-4-1-0.99][73-4-1-0.88][74-4-2-0.99][75-4-0--0.16][77-4-4-0.99][78-4-3-0.97][79-4-4-0.99][80-4-4-0.99]
[81-4-4-0.96][82-4-1-0.90][83-4-4-0.72][84-4-4-0.99][85-4-4-0.99][86-4-4-0.94][87-4-4-0.99][88-4-4-0.95][89-4-4-0.83][90-4-4--0.13]
[91-4-4-0.40][92-4-0-0.26][93-4-0--0.19][94-4-4-0.99][95-4-4-0.91][96-4-4-0.98][97-4-4-0.99][98-4-4-0.59][99-4-4-0.96][100-4-1-0.96]
[101-4-4-0.99][102-4-4-0.91][103-4-3-0.62][104-4-4-0.99][105-4-2-0.75][106-4-4-0.99][107-4-4-0.99][108-4-4-0.78][109-4-4-0.93][110-4-4-0.93]
[111-4-0-0.99][112-4-2-0.75][113-4-4--0.09][114-4-4--0.23][115-4-1-0.93][116-4-4-0.48][117-4-4-0.99][119-4-4-0.98][121-4-4-0.98][122-4-4-0.99]
[124-4-1-0.63][125-4-4-0.98][126-4-4-0.99][127-4-4--0.20][128-4-0-0.29][129-4-4-0.99][130-4-4-0.44][131-4-4-0.03][132-4-4-0.55][133-4-4-0.99]
[135-4-4-0.95][136-4-1-0.96][137-4-4-0.98][138-4-4-0.22][139-4-4-0.99][140-4-4-0.97][141-4-4--0.13][142-4-4-0.99][143-4-4-0.99][144-4-4-0.79]
[145-4-4-0.94][148-4-0-0.99][149-4-4-0.97][150-4-4-0.99][151-4-4-0.99][152-4-4-0.92][153-4-2--0.05][154-4-4-0.99][155-4-4-0.99][156-4-4-0.31]
[157-4-0-0.79][158-4-4-0.81][160-4-1-0.62][161-4-2-0.99][162-4-4-0.59][164-4-4-0.99][165-4-4-0.94][167-4-0-0.96][168-4-4-0.96][170-4-4-0.33]
[171-4-4-0.99][172-4-4-0.99][173-4-4-0.99][174-4-0-0.78][175-4-4-0.99][177-4-0-0.99][178-4-4-0.91][179-4-4-0.98][180-4-4-0.99][181-4-3-0.95]
[182-4-3-0.99][183-4-4-0.99][184-4-4-0.99][186-4-3-0.29][187-4-4--0.41][188-4-4-0.99][189-4-4-0.99][190-4-4-0.61][191-4-4-0.98][192-4-4-0.92]
[193-4-2-0.51][194-4-2-0.17][195-4-0-0.83][196-4-4-0.41][197-4-4-0.60][198-4-4-0.99][199-4-2-0.99]
---------------------------
I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.577 | Acc: 88.000% | Wgt Acc: 95.361%
	I - Batch: 100 | Loss: 0.588 | Acc: 86.500% | Wgt Acc: 94.258%
	I - Batch: 150 | Loss: 0.583 | Acc: 86.583% | Wgt Acc: 94.540%
	I - Batch: 200 | Loss: 0.588 | Acc: 86.875% | Wgt Acc: 94.632%
I - num batch: 222
I - Train -- Loss: 0.589 | Acc: 86.637% | Wgt Acc: 94.595% | LR: 1.250000e-04 | Dur: 136.54s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   1.   4.   8. 128.]
 [  0. 570.   4.   0.  70.]
 [  5.   2. 714.   2. 121.]
 [  5.   0.   3. 522.  93.]
 [  8.   5.   9.   6. 588.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 62.722% | Wgt Acc: 63.072% | Dur: 14.95s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   5.   2.   8.  14.]
 [  0.  39.  10.   2.  11.]
 [  5.  14.  38.   3.  26.]
 [ 14.   9.  12.  66.  17.]
 [  6.  11.  13.   7. 112.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.581 | Acc: 86.625% | Wgt Acc: 95.074%
	I - Batch: 100 | Loss: 0.591 | Acc: 85.375% | Wgt Acc: 94.332%
	I - Batch: 150 | Loss: 0.588 | Acc: 85.708% | Wgt Acc: 94.525%
	I - Batch: 200 | Loss: 0.587 | Acc: 85.656% | Wgt Acc: 94.563%
I - num batch: 222
I - Train -- Loss: 0.586 | Acc: 86.157% | Wgt Acc: 94.739% | LR: 1.250000e-04 | Dur: 132.99s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   0.   2.   9. 143.]
 [  1. 571.   5.   0.  68.]
 [  2.   2. 722.   1. 132.]
 [  8.   0.   1. 521.  96.]
 [  5.   5.   4.   7. 561.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.029 | Acc: 67.456% | Wgt Acc: 63.845% | Dur: 15.23s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   3.  18.  10.]
 [  0.  44.   9.   3.  12.]
 [  2.  11.  40.   1.  13.]
 [  9.   4.   9.  55.   7.]
 [ 12.  15.  14.   9. 138.]]

I - Local maximum validation set accuracy:  67.46

I - Validation set results: 
[14-1-2--0.46][50-3-4-0.98][124-2-3-0.12][127-0-0-0.99][443-2-2-0.99][567-0-0-0.99][573-1-1-0.73][615-0-3-0.39][695-1-2-0.98][722-3-3-0.48]
[826-0-0-0.99][878-0-0-0.95][1103-0-4-0.40][1212-3-3-0.94][1368-0-0-0.99][2181-2-2--0.04][2476-2-2-0.99][2721-2-2-0.99][2818-1-1-0.13][2886-2-1-0.31]
[3231-2-2-0.99][3333-2-2-0.10][3482-2-2-0.28][3536-3-3-0.99][3625-1-1-0.99][3909-0-0-0.98][4035-0-0-0.70][4140-0-0-0.99][4214-1-1-0.27][4346-1-4-0.55]
[4581-2-1-0.99][4708-3-4-0.15][4838-3-3--0.03][4845-1-1-0.31][4868-0-0-0.99][4939-0-4-0.47][4984-2-2-0.99][5078-1-4-0.96][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.99][5843-1-1-0.99][5949-3-3--0.38][5987-2-4-0.99][6014-3-3-0.74][6033-3-3--0.31][6313-0-0-0.99][6421-3-3-0.99][6500-1-1-0.22][6583-3-3-0.99]
[6683-3-4-0.69][6825-2-1-0.99][6998-3-2--0.21][7049-3-3-0.98][7517-1-1-0.96][7521-1-1-0.31][7528-1-1-0.45][7949-1-2-0.12][8135-1-0-0.93][8185-3-0-0.99]
[8269-3-1--0.00][8273-3-3-0.73][8543-3-0-0.99][8666-1-1-0.95][8672-0-0-0.78][8903-1-1-0.14][9001-2-2-0.43][9036-2-2-0.88][9281-3-1-0.72][9300-2-2-0.99]
[9571-0-4-0.66][9617-1-1-0.74][9644-2-2-0.83][9705-2-4-0.89][9801-0-3-0.98][9803-3-3-0.25][9865-3-3-0.99][9896-2-2-0.97][10314-1-4-0.01][10337-3-3-0.99]
[10403-0-4-0.86][10653-2-4-0.81][10704-2-1-0.24][10719-1-1-0.86][10727-1-4-0.78][10836-0-0-0.99][10969-2-3-0.23][11042-0-0-0.99][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-4-0.14][11499-0-0-0.99][11502-3-3-0.99][11512-3-3-0.83][11608-1-1-0.94][11610-0-0-0.96][11692-0-0-0.92][11905-0-0-0.95][11993-1-1-0.92][12002-2-2-0.43]
[12052-0-0-0.83][12201-0-3-0.99][12235-2-2-0.99][12320-1-4-0.99][12377-2-4-0.94][12398-2-3-0.84][12503-1-2-0.41][12617-0-2-0.84][12685-3-1-0.73][12738-2-0-0.40]
[12742-2-2-0.99][12823-0-0-0.99][13110-1-2-0.75][13240-3-3-0.91][13253-1-4-0.64][13273-0-0-0.99][13634-1-4-0.48][13763-2-2-0.42][13905-3-3--0.53][14060-2-1-0.89]
[14065-3-0--0.06][14147-3-0-0.89][14595-2-2-0.99][14687-2-2-0.99][14788-2-2-0.99][14869-1-1-0.99][14872-3-4-0.40][14877-1-1-0.99][14927-0-3-0.52][15066-0-0-0.99]
[15175-1-4-0.22][15178-2-4-0.19][15375-3-0-0.83][15389-3-3-0.98][15568-2-1-0.98][15675-3-3-0.99][15869-1-0-0.35][16207-3-0-0.99][16236-0-0-0.85][16302-3-3--0.66]
[16331-2-2-0.99][16381-0-3-0.53][16488-1-1-0.27][16495-0-0-0.99][16650-0-0-0.99][16719-1-3-0.84][16801-0-0-0.99][16828-0-0-0.98][17137-3-0-0.32][17245-1-4-0.92]
[17278-3-0-0.61][17282-0-0-0.98][17311-2-2-0.87][17336-2-1-0.35][17608-3-3-0.99][17627-0-2-0.12][17877-3-0-0.59][17924-1-3--0.30][17984-3-0-0.99][18211-0-3-0.33]
[18276-3-3-0.67][18287-1-1-0.36][18394-0-0-0.99][18428-0-0-0.99][18442-0-3-0.98][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.89][18663-0-0-0.99][18718-0-0-0.98]
[18766-2-2-0.94][18824-2-4-0.98][18890-3-3-0.99][18930-3-4-0.95][18938-3-3-0.34][19817-1-2-0.55][19839-0-0-0.92][19930-3-0-0.54][19944-0-4-0.97][20036-2-2-0.99]
[20101-3-3-0.75][20474-1-1-0.99][20547-3-3-0.94][20929-2-2-0.99][21245-1-2-0.74][21257-3-3-0.97][21293-1-2-0.99][21316-1-1-0.90][21384-1-4-0.97][21448-1-1-0.98]
[21483-0-0-0.99][21487-2-2-0.81][21714-0-4-0.43][21943-3-4-0.54][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.95][21998-1-1-0.86][22025-0-4-0.99][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.86][22757-0-0-0.99][22811-3-3-0.79][22976-3-4--0.32][22985-3-3-0.99][23014-0-0-0.99][23112-1-1-0.99][23144-3-3-0.99][23168-2-3--0.02]
[23219-0-0-0.82][23363-3-3-0.99][23470-0-0-0.62][23486-2-4-0.33][23497-0-0-0.99][23516-0-0-0.99][23690-1-4-0.52][23921-2-4-0.55][23936-1-0-0.55][24040-3-0-0.31]
[24111-1-4-0.99][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-4-0.28][24364-1-2-0.15][24427-3-0-0.58][24477-2-2-0.99][24495-2-4-0.72][24893-2-2-0.48]
[25012-1-3-0.16][25121-2-1-0.27][25165-3-3-0.99][25183-0-0-0.67][25297-3-3-0.98][25398-0-0-0.99][25574-2-2-0.87][25644-1-1-0.98][25718-1-2-0.77][25774-2-3--0.01]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.86][26321-1-1--0.25][26732-1-1-0.70][26784-3-3-0.99][26827-3-3--0.06][26833-0-3-0.90][26838-2-2-0.20][26860-1-4-0.97]
[26948-0-0-0.99][27049-3-0-0.99][27098-1-4--0.29][27526-0-0-0.99][27639-3-3-0.99][27698-3-0-0.61][27772-0-0-0.99][27890-1-1-0.87][28040-0-4-0.74][28503-2-2-0.99]
[28577-1-2-0.98][28959-0-0-0.99][29198-3-3-0.99][29777-0-0-0.99][29877-2-2-0.13][30035-1-1-0.91][30098-0-3-0.98][30326-1-1-0.99][30572-2-3-0.58][30716-0-4-0.99]
[30806-2-4-0.90][30906-1-1-0.99][31007-0-0-0.88][31181-3-3-0.93][31238-0-0-0.85][31347-0-0-0.99][31422-2-2-0.90][31429-3-3-0.61][31431-0-0-0.93][31432-1-1-0.88]
[31477-0-0-0.99][31524-1-1-0.48][31597-1-1--0.18][31619-1-0-0.65][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.99][32074-1-1-0.38][32078-3-3-0.99][32111-1-1-0.62]
[32127-1-1-0.99][32140-3-3-0.99][32263-2-0-0.64][32365-0-0-0.99][32411-2-3-0.99][32429-3-3-0.96][32473-3-0-0.01][32574-3-3-0.99][32584-0-4-0.97][32622-0-0--0.12]
[32858-3-0-0.99][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.99][33035-2-2-0.55][33133-2-2-0.99][33173-2-2-0.20][33175-3-4-0.99][33306-3-3-0.97][33309-2-3-0.99]
[33474-0-0-0.96][33478-2-1-0.40][33618-1-1-0.96][33712-0-0--0.03][33782-2-4-0.71][33914-3-3-0.99][34076-3-4--0.20][34112-2-2-0.99][34138-2-3-0.81][34239-1-4-0.23]
[34364-2-2-0.99][34617-1-1-0.13][34751-3-3-0.26][34783-2-4-0.98][35015-3-3-0.74][35018-1-1-0.88][35288-2-4--0.09][0-4-4-0.98][1-4-4-0.99][2-4-4-0.99]
[3-4-4-0.99][4-4-4-0.86][5-4-1-0.06][6-4-4-0.99][7-4-4-0.49][8-4-4-0.57][9-4-1-0.56][10-4-4-0.99][11-4-4-0.99][12-4-1-0.40]
[14-4-3-0.83][15-4-3-0.99][16-4-4-0.72][17-4-4-0.58][18-4-4-0.99][19-4-0--0.02][20-4-2-0.38][21-4-4-0.76][22-4-4-0.99][23-4-4-0.59]
[24-4-4-0.99][25-4-4-0.96][26-4-4-0.19][27-4-4-0.99][28-4-4-0.99][29-4-1-0.58][30-4-4-0.44][31-4-4-0.99][32-4-4-0.99][33-4-2-0.99]
[34-4-4-0.99][35-4-4-0.96][37-4-4-0.99][39-4-0-0.95][40-4-4-0.71][41-4-4--0.31][42-4-4-0.45][43-4-4-0.96][45-4-4-0.77][46-4-4-0.99]
[47-4-4-0.99][48-4-4-0.99][51-4-4-0.97][52-4-4-0.76][53-4-4-0.67][54-4-4-0.63][55-4-4-0.98][56-4-1-0.89][57-4-0-0.73][58-4-2-0.36]
[59-4-0-0.99][60-4-4-0.02][61-4-4-0.99][62-4-4-0.86][63-4-2-0.96][64-4-2-0.99][65-4-4-0.99][66-4-4-0.99][67-4-4-0.51][68-4-1-0.29]
[69-4-4--0.33][70-4-4-0.93][72-4-4-0.99][73-4-1-0.96][74-4-2-0.99][75-4-4--0.08][77-4-4-0.99][78-4-4-0.52][79-4-4-0.99][80-4-4-0.97]
[81-4-1-0.60][82-4-4-0.98][83-4-1-0.37][84-4-4-0.99][85-4-4-0.99][86-4-4-0.59][87-4-4-0.99][88-4-4-0.95][89-4-0-0.19][90-4-4-0.25]
[91-4-4-0.86][92-4-4-0.27][93-4-3--0.27][94-4-4-0.99][95-4-4-0.54][96-4-4-0.44][97-4-4-0.99][98-4-3-0.32][99-4-4-0.90][100-4-4-0.95]
[101-4-4-0.99][102-4-4-0.92][103-4-0-0.26][104-4-4-0.99][105-4-4-0.79][106-4-4-0.99][107-4-4-0.99][108-4-4-0.92][109-4-4-0.96][110-4-4-0.95]
[111-4-0-0.99][112-4-4-0.85][113-4-2-0.06][114-4-3-0.45][115-4-4-0.85][116-4-4-0.55][117-4-4-0.97][119-4-4-0.99][121-4-4-0.98][122-4-4-0.99]
[124-4-4-0.59][125-4-4-0.99][126-4-4-0.99][127-4-1--0.04][128-4-4-0.11][129-4-4-0.99][130-4-4-0.43][131-4-4-0.23][132-4-4-0.63][133-4-4-0.99]
[135-4-4-0.91][136-4-4-0.37][137-4-4-0.98][138-4-4-0.44][139-4-4-0.99][140-4-4-0.73][141-4-0-0.64][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.90][148-4-0-0.99][149-4-4-0.99][150-4-4-0.99][151-4-4-0.99][152-4-4-0.97][153-4-2-0.30][154-4-4-0.99][155-4-4-0.99][156-4-4-0.33]
[157-4-2-0.71][158-4-4-0.64][160-4-4-0.39][161-4-2-0.99][162-4-4-0.11][164-4-4-0.94][165-4-4-0.95][167-4-4-0.98][168-4-4-0.99][170-4-4-0.37]
[171-4-4-0.99][172-4-4-0.99][173-4-4-0.99][174-4-0-0.99][175-4-4-0.97][177-4-4-0.99][178-4-4-0.52][179-4-4-0.94][180-4-4-0.99][181-4-4-0.82]
[182-4-3-0.99][183-4-4-0.99][184-4-2-0.93][186-4-3--0.05][187-4-2-0.10][188-4-4-0.97][189-4-4-0.97][190-4-4-0.13][191-4-4-0.96][192-4-4-0.99]
[193-4-1-0.38][194-4-4--0.07][195-4-4-0.85][196-4-4-0.83][197-4-1-0.99][198-4-4-0.99][199-4-2-0.98]
---------------------------
I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.568 | Acc: 86.625% | Wgt Acc: 94.825%
	I - Batch: 100 | Loss: 0.574 | Acc: 86.750% | Wgt Acc: 94.880%
	I - Batch: 150 | Loss: 0.576 | Acc: 86.750% | Wgt Acc: 95.003%
	I - Batch: 200 | Loss: 0.583 | Acc: 86.469% | Wgt Acc: 94.822%
I - num batch: 222
I - Train -- Loss: 0.582 | Acc: 86.580% | Wgt Acc: 94.943% | LR: 1.250000e-04 | Dur: 131.25s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   1.   5.   7. 138.]
 [  0. 571.   1.   1.  81.]
 [  3.   1. 721.   2. 119.]
 [  2.   0.   0. 523.  89.]
 [  9.   5.   7.   5. 573.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.067 | Acc: 63.708% | Wgt Acc: 63.914% | Dur: 13.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   5.   3.  15.  16.]
 [  1.  39.   8.   0.  15.]
 [  0.  12.  40.   1.  21.]
 [ 15.  12.  15.  63.  14.]
 [  5.  10.   9.   7. 114.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.563 | Acc: 88.250% | Wgt Acc: 95.981%
	I - Batch: 100 | Loss: 0.572 | Acc: 87.125% | Wgt Acc: 95.534%
	I - Batch: 150 | Loss: 0.573 | Acc: 86.750% | Wgt Acc: 95.440%
	I - Batch: 200 | Loss: 0.576 | Acc: 87.156% | Wgt Acc: 95.370%
I - num batch: 222
I - Train -- Loss: 0.575 | Acc: 87.116% | Wgt Acc: 95.340% | LR: 1.250000e-04 | Dur: 135.72s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   2.   2.   8. 127.]
 [  1. 571.   3.   0.  76.]
 [  2.   1. 723.   0. 117.]
 [  1.   1.   1. 524.  97.]
 [  4.   3.   5.   6. 583.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.071 | Acc: 65.680% | Wgt Acc: 63.707% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   2.  14.  13.]
 [  1.  45.  11.   1.  14.]
 [  3.  11.  40.   1.  21.]
 [  4.   4.   8.  53.   5.]
 [ 12.  15.  14.  17. 127.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.568 | Acc: 88.375% | Wgt Acc: 95.937%
	I - Batch: 100 | Loss: 0.570 | Acc: 88.062% | Wgt Acc: 95.478%
	I - Batch: 150 | Loss: 0.574 | Acc: 87.125% | Wgt Acc: 94.966%
	I - Batch: 200 | Loss: 0.578 | Acc: 86.812% | Wgt Acc: 94.854%
I - num batch: 222
I - Train -- Loss: 0.579 | Acc: 86.778% | Wgt Acc: 94.803% | LR: 1.250000e-04 | Dur: 132.24s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   3.   1.   6. 133.]
 [  3. 569.   5.   1.  74.]
 [  2.   1. 720.   1. 123.]
 [  5.   1.   1. 524.  83.]
 [  9.   4.   7.   6. 587.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.090 | Acc: 62.919% | Wgt Acc: 61.850% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   7.   2.  14.  19.]
 [  2.  38.  13.   3.  14.]
 [  1.  19.  40.   2.  20.]
 [ 10.   4.   7.  56.   9.]
 [  8.  10.  13.  11. 118.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.576 | Acc: 86.250% | Wgt Acc: 94.620%
	I - Batch: 100 | Loss: 0.579 | Acc: 86.375% | Wgt Acc: 94.533%
	I - Batch: 150 | Loss: 0.576 | Acc: 86.500% | Wgt Acc: 94.923%
	I - Batch: 200 | Loss: 0.574 | Acc: 86.969% | Wgt Acc: 95.010%
I - num batch: 222
I - Train -- Loss: 0.574 | Acc: 87.088% | Wgt Acc: 95.090% | LR: 1.250000e-04 | Dur: 132.92s
I - Confusion Matrix: [row->prediction - col->label]
[[679.   2.   4.   8. 134.]
 [  1. 573.   0.   0.  70.]
 [  4.   1. 721.   1. 133.]
 [  2.   1.   2. 525.  72.]
 [ 11.   1.   7.   4. 591.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.057 | Acc: 64.892% | Wgt Acc: 62.519% | Dur: 14.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   3.  19.  18.]
 [  1.  44.  15.   2.  12.]
 [  1.  11.  34.   0.  14.]
 [ 10.   7.   9.  57.   8.]
 [ 10.  13.  14.   8. 128.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.567 | Acc: 87.750% | Wgt Acc: 95.683%
	I - Batch: 100 | Loss: 0.572 | Acc: 87.875% | Wgt Acc: 95.490%
	I - Batch: 150 | Loss: 0.573 | Acc: 87.625% | Wgt Acc: 95.328%
	I - Batch: 200 | Loss: 0.575 | Acc: 87.750% | Wgt Acc: 95.250%
I - num batch: 222
I - Train -- Loss: 0.574 | Acc: 87.821% | Wgt Acc: 95.315% | LR: 1.250000e-04 | Dur: 133.48s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   1.   2.   9. 117.]
 [  4. 570.   3.   0.  65.]
 [  3.   2. 722.   1. 135.]
 [  3.   2.   2. 526.  67.]
 [  6.   3.   5.   2. 616.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.092 | Acc: 61.144% | Wgt Acc: 62.796% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   6.   2.  14.  23.]
 [  1.  39.  10.   1.  11.]
 [  1.  16.  37.   3.  22.]
 [ 16.   7.  18.  65.  21.]
 [  4.  10.   8.   3. 103.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.566 | Acc: 88.375% | Wgt Acc: 96.225%
	I - Batch: 100 | Loss: 0.573 | Acc: 88.312% | Wgt Acc: 95.824%
	I - Batch: 150 | Loss: 0.573 | Acc: 87.583% | Wgt Acc: 95.675%
	I - Batch: 200 | Loss: 0.574 | Acc: 87.188% | Wgt Acc: 95.361%
I - num batch: 222
I - Train -- Loss: 0.575 | Acc: 87.313% | Wgt Acc: 95.271% | LR: 1.250000e-04 | Dur: 132.96s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   1.   6. 136.]
 [  2. 573.   3.   0.  66.]
 [  2.   1. 719.   1. 125.]
 [  5.   1.   1. 528.  78.]
 [  6.   3.  10.   3. 595.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.094 | Acc: 61.144% | Wgt Acc: 63.707% | Dur: 13.91s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  6.  3. 10. 21.]
 [ 1. 44.  6.  2. 13.]
 [ 2. 17. 41.  4. 24.]
 [19.  5. 16. 66. 23.]
 [ 6.  6.  9.  4. 99.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.577 | Acc: 86.250% | Wgt Acc: 95.219%
	I - Batch: 100 | Loss: 0.572 | Acc: 87.188% | Wgt Acc: 95.431%
	I - Batch: 150 | Loss: 0.573 | Acc: 87.000% | Wgt Acc: 95.464%
	I - Batch: 200 | Loss: 0.577 | Acc: 86.969% | Wgt Acc: 95.397%
I - num batch: 222
I - Train -- Loss: 0.577 | Acc: 87.116% | Wgt Acc: 95.357% | LR: 1.250000e-04 | Dur: 134.15s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   1.   1.   7. 124.]
 [  0. 572.   2.   0.  72.]
 [  1.   1. 721.   0. 120.]
 [  2.   0.   3. 529. 101.]
 [  9.   4.   7.   2. 583.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.093 | Acc: 63.511% | Wgt Acc: 65.437% | Dur: 15.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.  10.   5.  17.  22.]
 [  1.  40.   6.   2.  11.]
 [  4.  18.  48.   3.  30.]
 [  8.   4.   9.  59.  12.]
 [  5.   6.   7.   5. 105.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.556 | Acc: 87.625% | Wgt Acc: 96.136%
	I - Batch: 100 | Loss: 0.559 | Acc: 88.438% | Wgt Acc: 96.069%
	I - Batch: 150 | Loss: 0.567 | Acc: 87.917% | Wgt Acc: 95.722%
	I - Batch: 200 | Loss: 0.568 | Acc: 88.094% | Wgt Acc: 95.801%
I - num batch: 222
I - Train -- Loss: 0.567 | Acc: 88.244% | Wgt Acc: 95.855% | LR: 1.250000e-04 | Dur: 133.84s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   3.   3. 134.]
 [  2. 573.   1.   1.  76.]
 [  0.   1. 725.   1. 108.]
 [  2.   0.   1. 528.  66.]
 [  5.   4.   4.   5. 616.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.061 | Acc: 65.286% | Wgt Acc: 66.048% | Dur: 13.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   7.   4.  12.  24.]
 [  1.  40.  11.   2.  11.]
 [  1.  16.  47.   1.  21.]
 [ 10.   4.   6.  63.  10.]
 [  9.  11.   7.   8. 114.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.581 | Acc: 87.000% | Wgt Acc: 95.320%
	I - Batch: 100 | Loss: 0.573 | Acc: 87.188% | Wgt Acc: 95.292%
	I - Batch: 150 | Loss: 0.568 | Acc: 87.375% | Wgt Acc: 95.503%
	I - Batch: 200 | Loss: 0.568 | Acc: 87.656% | Wgt Acc: 95.523%
I - num batch: 222
I - Train -- Loss: 0.567 | Acc: 87.623% | Wgt Acc: 95.518% | LR: 1.250000e-04 | Dur: 132.03s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   2.   4. 118.]
 [  0. 573.   1.   1.  72.]
 [  2.   0. 724.   2. 122.]
 [  3.   0.   2. 527.  88.]
 [  8.   5.   5.   4. 600.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.052 | Acc: 65.878% | Wgt Acc: 66.671% | Dur: 14.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   3.   2.   5.  11.]
 [  2.  49.  12.   4.  17.]
 [  2.  11.  42.   2.  25.]
 [ 17.   5.   9.  69.  11.]
 [  9.  10.  10.   6. 116.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.565 | Acc: 86.250% | Wgt Acc: 95.298%
	I - Batch: 100 | Loss: 0.572 | Acc: 86.188% | Wgt Acc: 95.004%
	I - Batch: 150 | Loss: 0.576 | Acc: 86.542% | Wgt Acc: 94.741%
	I - Batch: 200 | Loss: 0.569 | Acc: 87.125% | Wgt Acc: 95.138%
I - num batch: 222
I - Train -- Loss: 0.570 | Acc: 86.919% | Wgt Acc: 95.122% | LR: 1.250000e-04 | Dur: 131.71s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   2.   9. 142.]
 [  4. 574.   4.   1.  79.]
 [  1.   1. 722.   0. 108.]
 [  1.   0.   0. 523.  89.]
 [  9.   3.   6.   5. 582.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.041 | Acc: 65.878% | Wgt Acc: 64.479% | Dur: 14.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   5.   2.  11.  14.]
 [  1.  49.   9.   1.   7.]
 [  2.  12.  37.   1.  20.]
 [ 16.   4.  16.  64.  13.]
 [ 11.   8.  11.   9. 126.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.568 | Acc: 86.750% | Wgt Acc: 95.353%
	I - Batch: 100 | Loss: 0.567 | Acc: 87.562% | Wgt Acc: 95.499%
	I - Batch: 150 | Loss: 0.570 | Acc: 87.958% | Wgt Acc: 95.508%
	I - Batch: 200 | Loss: 0.570 | Acc: 87.562% | Wgt Acc: 95.549%
I - num batch: 222
I - Train -- Loss: 0.569 | Acc: 87.821% | Wgt Acc: 95.677% | LR: 1.250000e-04 | Dur: 133.82s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   5. 116.]
 [  0. 574.   4.   1.  80.]
 [  1.   0. 722.   0. 115.]
 [  1.   1.   1. 525.  86.]
 [  4.   3.   6.   7. 603.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.044 | Acc: 66.469% | Wgt Acc: 62.334% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   2.   9.  12.]
 [  2.  42.   9.   2.   8.]
 [  3.  12.  37.   0.  17.]
 [  8.   3.   9.  57.   4.]
 [ 13.  18.  18.  18. 139.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.573 | Acc: 87.750% | Wgt Acc: 95.250%
	I - Batch: 100 | Loss: 0.565 | Acc: 87.688% | Wgt Acc: 95.643%
	I - Batch: 150 | Loss: 0.563 | Acc: 87.542% | Wgt Acc: 95.587%
	I - Batch: 200 | Loss: 0.566 | Acc: 87.531% | Wgt Acc: 95.654%
I - num batch: 222
I - Train -- Loss: 0.566 | Acc: 87.511% | Wgt Acc: 95.602% | LR: 1.250000e-04 | Dur: 133.28s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   2.   6. 118.]
 [  0. 574.   2.   1.  74.]
 [  1.   0. 726.   1. 116.]
 [  3.   0.   0. 528. 100.]
 [  9.   4.   4.   2. 592.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.055 | Acc: 66.272% | Wgt Acc: 63.476% | Dur: 15.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   4.  15.  14.]
 [  1.  36.   5.   1.   4.]
 [  2.  18.  43.   0.  21.]
 [ 10.   4.  10.  58.   9.]
 [  8.  16.  13.  12. 132.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.562 | Acc: 87.250% | Wgt Acc: 95.633%
	I - Batch: 100 | Loss: 0.559 | Acc: 88.438% | Wgt Acc: 95.987%
	I - Batch: 150 | Loss: 0.560 | Acc: 88.875% | Wgt Acc: 96.071%
	I - Batch: 200 | Loss: 0.559 | Acc: 88.906% | Wgt Acc: 96.158%
I - num batch: 222
I - Train -- Loss: 0.559 | Acc: 88.807% | Wgt Acc: 96.160% | LR: 1.250000e-04 | Dur: 132.59s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   1.   3. 109.]
 [  3. 576.   1.   2.  72.]
 [  1.   0. 724.   0. 107.]
 [  1.   0.   1. 531.  81.]
 [  4.   2.   7.   2. 631.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 64.300% | Wgt Acc: 64.998% | Dur: 13.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   4.   2.   9.  14.]
 [  2.  46.   8.   2.  14.]
 [  3.  15.  43.   3.  28.]
 [ 18.   6.  11.  63.  11.]
 [  4.   7.  11.   9. 113.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.570 | Acc: 86.625% | Wgt Acc: 94.960%
	I - Batch: 100 | Loss: 0.570 | Acc: 87.000% | Wgt Acc: 95.269%
	I - Batch: 150 | Loss: 0.570 | Acc: 86.875% | Wgt Acc: 95.270%
	I - Batch: 200 | Loss: 0.565 | Acc: 87.156% | Wgt Acc: 95.553%
I - num batch: 222
I - Train -- Loss: 0.564 | Acc: 87.285% | Wgt Acc: 95.622% | LR: 1.250000e-04 | Dur: 133.00s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   3.   4. 119.]
 [  1. 575.   1.   1.  76.]
 [  2.   2. 724.   1. 129.]
 [  1.   0.   0. 527.  95.]
 [  4.   1.   6.   5. 581.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.062 | Acc: 64.497% | Wgt Acc: 64.698% | Dur: 14.75s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   5.   3.  17.  22.]
 [  1.  39.   5.   0.  11.]
 [  0.  18.  42.   2.  16.]
 [ 11.   4.  11.  60.  16.]
 [  5.  12.  14.   7. 115.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.541 | Acc: 90.625% | Wgt Acc: 96.471%
	I - Batch: 100 | Loss: 0.545 | Acc: 89.500% | Wgt Acc: 96.120%
	I - Batch: 150 | Loss: 0.545 | Acc: 89.583% | Wgt Acc: 96.179%
	I - Batch: 200 | Loss: 0.546 | Acc: 89.688% | Wgt Acc: 96.181%
I - num batch: 208
I - Train -- Loss: 0.546 | Acc: 89.724% | Wgt Acc: 96.211% | LR: 1.250000e-04 | Dur: 125.07s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   3.   9.  82.]
 [  1. 576.   1.   1.  60.]
 [  3.   1. 727.   1.  92.]
 [  2.   0.   0. 521.  70.]
 [  6.   1.   3.   6. 477.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.050 | Acc: 66.667% | Wgt Acc: 65.967% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   5.   2.  12.  16.]
 [  0.  42.   9.   1.  11.]
 [  2.  17.  48.   5.  25.]
 [ 10.   2.   7.  59.   5.]
 [ 10.  12.   9.   9. 123.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.557 | Acc: 87.625% | Wgt Acc: 95.679%
	I - Batch: 100 | Loss: 0.555 | Acc: 87.375% | Wgt Acc: 95.790%
	I - Batch: 150 | Loss: 0.561 | Acc: 87.250% | Wgt Acc: 95.515%
	I - Batch: 200 | Loss: 0.561 | Acc: 87.312% | Wgt Acc: 95.501%
I - num batch: 222
I - Train -- Loss: 0.561 | Acc: 87.511% | Wgt Acc: 95.541% | LR: 1.250000e-04 | Dur: 134.48s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   3.   5. 126.]
 [  1. 574.   1.   2.  61.]
 [  0.   1. 726.   0. 118.]
 [  4.   0.   1. 526. 101.]
 [  8.   3.   3.   5. 594.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.055 | Acc: 63.708% | Wgt Acc: 64.179% | Dur: 15.20s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   3.  11.  22.]
 [  1.  42.   9.   3.  11.]
 [  2.  12.  39.   1.  20.]
 [ 11.   9.  13.  64.  14.]
 [  9.  11.  11.   7. 113.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.549 | Acc: 89.625% | Wgt Acc: 96.251%
	I - Batch: 100 | Loss: 0.548 | Acc: 89.438% | Wgt Acc: 96.370%
	I - Batch: 150 | Loss: 0.555 | Acc: 89.167% | Wgt Acc: 96.019%
	I - Batch: 200 | Loss: 0.556 | Acc: 89.188% | Wgt Acc: 96.082%
I - num batch: 222
I - Train -- Loss: 0.557 | Acc: 89.005% | Wgt Acc: 95.973% | LR: 1.250000e-04 | Dur: 140.18s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   2.   5.  91.]
 [  0. 573.   3.   0.  72.]
 [  2.   1. 727.   2. 106.]
 [  4.   1.   0. 526.  85.]
 [  6.   3.   2.   5. 646.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.076 | Acc: 64.892% | Wgt Acc: 63.649% | Dur: 15.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   4.   4.  19.  21.]
 [  2.  40.   7.   3.  10.]
 [  1.  13.  41.   2.  19.]
 [  5.   7.  11.  54.   8.]
 [  8.  14.  12.   8. 122.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.543 | Acc: 90.250% | Wgt Acc: 97.125%
	I - Batch: 100 | Loss: 0.546 | Acc: 90.625% | Wgt Acc: 97.028%
	I - Batch: 150 | Loss: 0.548 | Acc: 89.542% | Wgt Acc: 96.438%
	I - Batch: 200 | Loss: 0.554 | Acc: 89.000% | Wgt Acc: 96.193%
I - num batch: 222
I - Train -- Loss: 0.554 | Acc: 89.118% | Wgt Acc: 96.226% | LR: 1.250000e-04 | Dur: 137.22s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   0.   1. 108.]
 [  0. 576.   3.   0.  52.]
 [  2.   0. 723.   0. 109.]
 [  3.   0.   1. 534.  88.]
 [  7.   2.   7.   3. 643.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.045 | Acc: 64.694% | Wgt Acc: 62.530% | Dur: 14.88s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   3.  12.  15.]
 [  1.  42.   8.   3.   8.]
 [  2.   7.  36.   0.  14.]
 [ 14.   4.  11.  61.  16.]
 [  9.  22.  17.  10. 127.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.555 | Acc: 89.000% | Wgt Acc: 96.102%
	I - Batch: 100 | Loss: 0.558 | Acc: 88.875% | Wgt Acc: 95.939%
	I - Batch: 150 | Loss: 0.553 | Acc: 89.042% | Wgt Acc: 96.101%
	I - Batch: 200 | Loss: 0.555 | Acc: 89.031% | Wgt Acc: 96.097%
I - num batch: 222
I - Train -- Loss: 0.557 | Acc: 89.005% | Wgt Acc: 96.065% | LR: 1.250000e-04 | Dur: 132.24s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   2.   5.  99.]
 [  1. 573.   1.   0.  61.]
 [  0.   0. 724.   0. 123.]
 [  2.   1.   1. 529.  74.]
 [  6.   4.   6.   4. 643.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 65.483% | Wgt Acc: 63.949% | Dur: 13.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   5.   3.  13.  16.]
 [  1.  39.   6.   2.  11.]
 [  0.  12.  42.   1.  19.]
 [ 14.   5.   6.  60.   9.]
 [  7.  17.  18.  10. 125.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.555 | Acc: 88.250% | Wgt Acc: 95.529%
	I - Batch: 100 | Loss: 0.551 | Acc: 88.750% | Wgt Acc: 95.808%
	I - Batch: 150 | Loss: 0.552 | Acc: 88.667% | Wgt Acc: 95.874%
	I - Batch: 200 | Loss: 0.553 | Acc: 88.812% | Wgt Acc: 95.929%
I - num batch: 222
I - Train -- Loss: 0.552 | Acc: 88.892% | Wgt Acc: 96.056% | LR: 1.250000e-04 | Dur: 131.74s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   4. 120.]
 [  0. 575.   3.   0.  57.]
 [  2.   2. 727.   1. 107.]
 [  2.   0.   1. 526.  78.]
 [  6.   1.   3.   7. 638.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.042 | Acc: 66.469% | Wgt Acc: 62.796% | Dur: 13.69s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   6.   2.  16.  15.]
 [  0.  38.   7.   1.   6.]
 [  1.  12.  40.   2.  16.]
 [  7.   4.  10.  52.   7.]
 [  9.  18.  16.  15. 136.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.550 | Acc: 90.250% | Wgt Acc: 96.778%
	I - Batch: 100 | Loss: 0.548 | Acc: 90.125% | Wgt Acc: 96.755%
	I - Batch: 150 | Loss: 0.545 | Acc: 90.417% | Wgt Acc: 96.819%
	I - Batch: 200 | Loss: 0.551 | Acc: 89.688% | Wgt Acc: 96.382%
I - num batch: 222
I - Train -- Loss: 0.550 | Acc: 89.653% | Wgt Acc: 96.381% | LR: 1.250000e-04 | Dur: 132.59s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   2.   4. 104.]
 [  1. 574.   0.   1.  60.]
 [  1.   0. 727.   2. 103.]
 [  2.   1.   2. 529.  72.]
 [  4.   3.   3.   2. 661.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.071 | Acc: 63.314% | Wgt Acc: 64.283% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   9.   2.  23.  18.]
 [  2.  45.   9.   3.  13.]
 [  2.  14.  41.   0.  29.]
 [  7.   4.  14.  54.  11.]
 [  5.   6.   9.   6. 109.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.551 | Acc: 87.875% | Wgt Acc: 95.962%
	I - Batch: 100 | Loss: 0.552 | Acc: 88.812% | Wgt Acc: 95.950%
	I - Batch: 150 | Loss: 0.548 | Acc: 89.250% | Wgt Acc: 96.195%
	I - Batch: 200 | Loss: 0.549 | Acc: 89.250% | Wgt Acc: 96.194%
I - num batch: 222
I - Train -- Loss: 0.549 | Acc: 89.399% | Wgt Acc: 96.214% | LR: 1.250000e-04 | Dur: 134.98s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   0.   2. 110.]
 [  4. 576.   2.   0.  44.]
 [  1.   0. 725.   0. 105.]
 [  2.   0.   0. 532.  85.]
 [  8.   2.   7.   4. 656.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.074 | Acc: 62.327% | Wgt Acc: 61.112% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   4.  16.  18.]
 [  1.  37.   4.   1.  11.]
 [  2.  20.  47.   6.  30.]
 [ 10.   4.   7.  50.   4.]
 [ 10.  13.  13.  13. 117.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.555 | Acc: 88.875% | Wgt Acc: 95.747%
	I - Batch: 100 | Loss: 0.554 | Acc: 88.625% | Wgt Acc: 95.928%
	I - Batch: 150 | Loss: 0.554 | Acc: 88.667% | Wgt Acc: 95.847%
	I - Batch: 200 | Loss: 0.550 | Acc: 89.281% | Wgt Acc: 96.231%
I - num batch: 222
I - Train -- Loss: 0.551 | Acc: 89.174% | Wgt Acc: 96.146% | LR: 1.250000e-04 | Dur: 133.21s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   1.   2.   3. 104.]
 [  1. 576.   2.   0.  69.]
 [  0.   0. 723.   1. 100.]
 [  3.   0.   0. 530.  79.]
 [  7.   1.   7.   4. 648.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.040 | Acc: 65.878% | Wgt Acc: 63.499% | Dur: 14.00s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   2.   8.  10.]
 [  0.  42.   6.   0.   7.]
 [  3.  15.  42.   2.  25.]
 [ 14.   5.  10.  61.   8.]
 [ 12.  13.  15.  15. 130.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.557 | Acc: 88.625% | Wgt Acc: 95.757%
	I - Batch: 100 | Loss: 0.547 | Acc: 89.188% | Wgt Acc: 96.255%
	I - Batch: 150 | Loss: 0.546 | Acc: 89.292% | Wgt Acc: 96.440%
	I - Batch: 200 | Loss: 0.546 | Acc: 89.594% | Wgt Acc: 96.527%
I - num batch: 222
I - Train -- Loss: 0.547 | Acc: 89.371% | Wgt Acc: 96.448% | LR: 1.250000e-04 | Dur: 134.18s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   1.   3. 107.]
 [  1. 576.   1.   0.  50.]
 [  0.   0. 729.   1.  99.]
 [  3.   0.   0. 530.  98.]
 [  4.   2.   3.   4. 646.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.030 | Acc: 66.272% | Wgt Acc: 64.352% | Dur: 16.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   1.  10.  15.]
 [  1.  44.   7.   1.   8.]
 [  2.  12.  38.   2.  16.]
 [ 17.   7.  13.  65.  12.]
 [  8.  12.  16.   8. 129.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.535 | Acc: 90.875% | Wgt Acc: 96.774%
	I - Batch: 100 | Loss: 0.543 | Acc: 90.000% | Wgt Acc: 96.507%
	I - Batch: 150 | Loss: 0.548 | Acc: 89.417% | Wgt Acc: 96.255%
	I - Batch: 200 | Loss: 0.549 | Acc: 89.188% | Wgt Acc: 96.137%
I - num batch: 222
I - Train -- Loss: 0.550 | Acc: 89.089% | Wgt Acc: 96.060% | LR: 1.250000e-04 | Dur: 133.19s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   2.   2.   6. 114.]
 [  1. 575.   2.   0.  58.]
 [  0.   0. 723.   0. 100.]
 [  2.   0.   0. 528.  81.]
 [  7.   1.   7.   4. 647.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.058 | Acc: 65.483% | Wgt Acc: 62.565% | Dur: 14.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   2.   2.  14.  14.]
 [  1.  39.   7.   1.   6.]
 [  1.  16.  45.   2.  23.]
 [  5.   3.   6.  54.   6.]
 [ 18.  18.  15.  15. 131.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.544 | Acc: 91.125% | Wgt Acc: 96.629%
	I - Batch: 100 | Loss: 0.543 | Acc: 90.688% | Wgt Acc: 96.612%
	I - Batch: 150 | Loss: 0.542 | Acc: 90.542% | Wgt Acc: 96.685%
	I - Batch: 200 | Loss: 0.541 | Acc: 90.312% | Wgt Acc: 96.741%
I - num batch: 222
I - Train -- Loss: 0.541 | Acc: 90.358% | Wgt Acc: 96.723% | LR: 1.250000e-04 | Dur: 134.04s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   2.   1. 115.]
 [  0. 576.   0.   0.  58.]
 [  2.   0. 728.   0.  83.]
 [  0.   1.   0. 532.  63.]
 [  7.   1.   4.   5. 681.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.038 | Acc: 65.680% | Wgt Acc: 60.881% | Dur: 14.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   4.  12.  16.]
 [  0.  46.   8.   3.   8.]
 [  1.   9.  33.   0.  12.]
 [ 15.   2.   8.  56.   3.]
 [ 15.  19.  22.  15. 141.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.547 | Acc: 89.375% | Wgt Acc: 96.303%
	I - Batch: 100 | Loss: 0.544 | Acc: 90.562% | Wgt Acc: 96.706%
	I - Batch: 150 | Loss: 0.547 | Acc: 89.625% | Wgt Acc: 96.394%
	I - Batch: 200 | Loss: 0.551 | Acc: 89.031% | Wgt Acc: 96.230%
I - num batch: 222
I - Train -- Loss: 0.552 | Acc: 89.146% | Wgt Acc: 96.163% | LR: 1.250000e-04 | Dur: 134.30s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   1.   4.   3. 114.]
 [  1. 574.   1.   0.  56.]
 [  0.   1. 723.   0. 103.]
 [  3.   0.   0. 530.  81.]
 [  4.   2.   6.   5. 646.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.057 | Acc: 63.511% | Wgt Acc: 61.089% | Dur: 14.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   5.   2.  15.  15.]
 [  0.  35.   5.   1.   6.]
 [  4.  18.  46.   5.  28.]
 [ 13.   2.   9.  53.   6.]
 [  8.  18.  13.  12. 125.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.547 | Acc: 88.875% | Wgt Acc: 96.146%
	I - Batch: 100 | Loss: 0.550 | Acc: 89.562% | Wgt Acc: 96.271%
	I - Batch: 150 | Loss: 0.544 | Acc: 90.000% | Wgt Acc: 96.472%
	I - Batch: 200 | Loss: 0.545 | Acc: 89.812% | Wgt Acc: 96.365%
I - num batch: 222
I - Train -- Loss: 0.546 | Acc: 89.540% | Wgt Acc: 96.309% | LR: 1.250000e-04 | Dur: 135.00s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   1.   0. 111.]
 [  1. 575.   2.   1.  59.]
 [  0.   0. 725.   0.  99.]
 [  3.   1.   0. 533.  72.]
 [  9.   2.   6.   4. 659.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.041 | Acc: 66.272% | Wgt Acc: 63.476% | Dur: 16.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   7.   1.  14.  16.]
 [  0.  42.   6.   1.   6.]
 [  3.  13.  40.   5.  18.]
 [ 10.   3.  12.  55.   8.]
 [  8.  13.  16.  11. 132.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.550 | Acc: 90.250% | Wgt Acc: 96.316%
	I - Batch: 100 | Loss: 0.549 | Acc: 90.125% | Wgt Acc: 96.442%
	I - Batch: 150 | Loss: 0.543 | Acc: 90.792% | Wgt Acc: 96.829%
	I - Batch: 200 | Loss: 0.544 | Acc: 90.406% | Wgt Acc: 96.743%
I - num batch: 222
I - Train -- Loss: 0.546 | Acc: 90.161% | Wgt Acc: 96.583% | LR: 1.250000e-04 | Dur: 133.88s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   2.   3.  97.]
 [  1. 574.   1.   0.  50.]
 [  2.   0. 726.   1.  99.]
 [  0.   0.   1. 532.  77.]
 [  5.   4.   4.   2. 677.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.052 | Acc: 64.892% | Wgt Acc: 62.969% | Dur: 14.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   2.  13.  18.]
 [  1.  41.  10.   0.   9.]
 [  2.  13.  38.   1.  15.]
 [ 14.   8.  11.  60.  12.]
 [  7.  12.  14.  12. 126.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.543 | Acc: 90.500% | Wgt Acc: 96.581%
	I - Batch: 100 | Loss: 0.541 | Acc: 90.562% | Wgt Acc: 96.737%
	I - Batch: 150 | Loss: 0.541 | Acc: 90.625% | Wgt Acc: 96.681%
	I - Batch: 200 | Loss: 0.542 | Acc: 90.281% | Wgt Acc: 96.588%
I - num batch: 222
I - Train -- Loss: 0.541 | Acc: 90.358% | Wgt Acc: 96.654% | LR: 1.250000e-04 | Dur: 134.94s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   6. 102.]
 [  1. 577.   0.   0.  55.]
 [  0.   0. 726.   1.  92.]
 [  1.   0.   1. 527.  68.]
 [  3.   1.   7.   4. 683.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.038 | Acc: 64.497% | Wgt Acc: 61.135% | Dur: 15.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   7.   4.  20.  18.]
 [  0.  34.   6.   1.   7.]
 [  2.  15.  39.   1.  18.]
 [  6.   7.   8.  52.   6.]
 [  9.  15.  18.  12. 131.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.545 | Acc: 89.375% | Wgt Acc: 96.172%
	I - Batch: 100 | Loss: 0.545 | Acc: 88.688% | Wgt Acc: 96.336%
	I - Batch: 150 | Loss: 0.542 | Acc: 89.042% | Wgt Acc: 96.460%
	I - Batch: 200 | Loss: 0.545 | Acc: 88.719% | Wgt Acc: 96.264%
I - num batch: 222
I - Train -- Loss: 0.547 | Acc: 88.638% | Wgt Acc: 96.128% | LR: 1.250000e-04 | Dur: 134.08s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   2.   4. 117.]
 [  1. 575.   0.   1.  60.]
 [  1.   0. 728.   0. 114.]
 [  0.   0.   1. 527.  85.]
 [  5.   3.   3.   6. 624.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.076 | Acc: 62.327% | Wgt Acc: 60.385% | Dur: 14.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   4.   2.   8.  15.]
 [  2.  38.   5.   2.   8.]
 [  1.   9.  31.   0.  12.]
 [ 15.   8.  14.  63.  23.]
 [  8.  19.  23.  13. 122.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.535 | Acc: 90.000% | Wgt Acc: 96.963%
	I - Batch: 100 | Loss: 0.541 | Acc: 89.875% | Wgt Acc: 96.659%
	I - Batch: 150 | Loss: 0.543 | Acc: 90.417% | Wgt Acc: 96.723%
	I - Batch: 200 | Loss: 0.544 | Acc: 90.188% | Wgt Acc: 96.647%
I - num batch: 222
I - Train -- Loss: 0.542 | Acc: 90.386% | Wgt Acc: 96.764% | LR: 1.250000e-04 | Dur: 135.51s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   2.   2. 103.]
 [  1. 578.   2.   0.  48.]
 [  2.   0. 726.   1. 109.]
 [  0.   0.   2. 533.  59.]
 [  6.   0.   2.   2. 681.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.025 | Acc: 67.061% | Wgt Acc: 65.829% | Dur: 14.86s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   1.   7.  14.]
 [  0.  44.   6.   1.   9.]
 [  3.  11.  40.   0.  20.]
 [ 16.   7.  14.  65.  10.]
 [  5.  12.  14.  13. 127.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.529 | Acc: 90.750% | Wgt Acc: 97.158%
	I - Batch: 100 | Loss: 0.536 | Acc: 90.438% | Wgt Acc: 96.787%
	I - Batch: 150 | Loss: 0.538 | Acc: 90.292% | Wgt Acc: 96.779%
	I - Batch: 200 | Loss: 0.540 | Acc: 90.312% | Wgt Acc: 96.619%
I - num batch: 222
I - Train -- Loss: 0.540 | Acc: 90.330% | Wgt Acc: 96.686% | LR: 1.250000e-04 | Dur: 131.39s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   1.   1.   3. 101.]
 [  0. 575.   3.   0.  59.]
 [  0.   0. 727.   0.  86.]
 [  2.   0.   1. 532.  73.]
 [  6.   2.   2.   3. 681.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.035 | Acc: 65.680% | Wgt Acc: 62.253% | Dur: 13.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   5.   4.  14.  17.]
 [  0.  41.   9.   2.   9.]
 [  3.  14.  35.   0.  15.]
 [  7.   3.  11.  55.   5.]
 [ 10.  15.  16.  15. 134.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.529 | Acc: 91.625% | Wgt Acc: 96.908%
	I - Batch: 100 | Loss: 0.533 | Acc: 90.312% | Wgt Acc: 96.754%
	I - Batch: 150 | Loss: 0.533 | Acc: 90.208% | Wgt Acc: 96.887%
	I - Batch: 200 | Loss: 0.533 | Acc: 90.438% | Wgt Acc: 96.898%
I - num batch: 222
I - Train -- Loss: 0.537 | Acc: 90.217% | Wgt Acc: 96.677% | LR: 1.250000e-04 | Dur: 131.64s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   3.   3. 114.]
 [  0. 575.   0.   1.  50.]
 [  1.   0. 729.   2.  83.]
 [  1.   1.   0. 530.  77.]
 [  5.   2.   2.   2. 676.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.047 | Acc: 64.497% | Wgt Acc: 60.870% | Dur: 15.06s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   2.  13.  13.]
 [  2.  41.   8.   2.   8.]
 [  0.  13.  36.   3.  20.]
 [ 11.   5.  14.  55.   6.]
 [ 13.  16.  15.  13. 133.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.538 | Acc: 89.625% | Wgt Acc: 96.715%
	I - Batch: 100 | Loss: 0.538 | Acc: 90.375% | Wgt Acc: 96.775%
	I - Batch: 150 | Loss: 0.540 | Acc: 90.583% | Wgt Acc: 96.727%
	I - Batch: 200 | Loss: 0.541 | Acc: 90.562% | Wgt Acc: 96.630%
I - num batch: 222
I - Train -- Loss: 0.541 | Acc: 90.414% | Wgt Acc: 96.586% | LR: 1.250000e-04 | Dur: 133.61s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   4.   6. 104.]
 [  0. 577.   0.   0.  50.]
 [  1.   0. 727.   2.  93.]
 [  4.   0.   1. 527.  65.]
 [  4.   1.   2.   3. 688.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.056 | Acc: 64.300% | Wgt Acc: 62.703% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   6.   4.  17.  20.]
 [  2.  43.   6.   3.   6.]
 [  3.  10.  41.   0.  21.]
 [ 11.   5.  13.  55.  10.]
 [  8.  14.  11.  11. 123.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.532 | Acc: 90.250% | Wgt Acc: 96.465%
	I - Batch: 100 | Loss: 0.534 | Acc: 89.688% | Wgt Acc: 96.491%
	I - Batch: 150 | Loss: 0.533 | Acc: 90.125% | Wgt Acc: 96.694%
	I - Batch: 200 | Loss: 0.535 | Acc: 90.125% | Wgt Acc: 96.683%
I - num batch: 222
I - Train -- Loss: 0.536 | Acc: 90.217% | Wgt Acc: 96.663% | LR: 1.250000e-04 | Dur: 133.13s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   0. 105.]
 [  1. 574.   3.   1.  58.]
 [  1.   0. 727.   0.  83.]
 [  2.   1.   0. 535.  77.]
 [  6.   3.   4.   2. 677.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.050 | Acc: 64.497% | Wgt Acc: 62.576% | Dur: 16.77s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   7.   2.  13.  19.]
 [  0.  35.   5.   1.   6.]
 [  1.  13.  40.   1.  16.]
 [ 10.   7.  15.  60.  14.]
 [ 10.  16.  13.  11. 125.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.532 | Acc: 90.250% | Wgt Acc: 96.709%
	I - Batch: 100 | Loss: 0.535 | Acc: 90.062% | Wgt Acc: 96.631%
	I - Batch: 150 | Loss: 0.541 | Acc: 89.417% | Wgt Acc: 96.290%
	I - Batch: 200 | Loss: 0.542 | Acc: 89.625% | Wgt Acc: 96.350%
I - num batch: 222
I - Train -- Loss: 0.542 | Acc: 89.738% | Wgt Acc: 96.437% | LR: 1.250000e-04 | Dur: 134.49s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   1.   4. 125.]
 [  1. 578.   3.   0.  41.]
 [  2.   0. 725.   3.  87.]
 [  3.   0.   1. 529.  84.]
 [  3.   0.   4.   2. 663.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 67.061% | Wgt Acc: 64.502% | Dur: 14.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   5.   1.  10.  10.]
 [  2.  47.   9.   2.  11.]
 [  1.   9.  38.   0.  18.]
 [ 13.   4.  13.  60.   8.]
 [ 10.  13.  14.  14. 133.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.530 | Acc: 90.875% | Wgt Acc: 97.050%
	I - Batch: 100 | Loss: 0.534 | Acc: 90.938% | Wgt Acc: 96.956%
	I - Batch: 150 | Loss: 0.537 | Acc: 90.792% | Wgt Acc: 96.989%
	I - Batch: 200 | Loss: 0.543 | Acc: 90.312% | Wgt Acc: 96.658%
I - num batch: 222
I - Train -- Loss: 0.544 | Acc: 90.273% | Wgt Acc: 96.690% | LR: 1.250000e-04 | Dur: 133.90s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   2.   4.  97.]
 [  0. 576.   0.   0.  54.]
 [  0.   0. 725.   2.  85.]
 [  0.   0.   1. 529.  86.]
 [  3.   2.   6.   3. 678.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.061 | Acc: 63.511% | Wgt Acc: 63.015% | Dur: 15.89s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   4.   2.  15.  18.]
 [  1.  44.  10.   1.  11.]
 [  1.  10.  38.   1.  17.]
 [ 15.  10.  15.  60.  17.]
 [  8.  10.  10.   9. 117.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.531 | Acc: 91.000% | Wgt Acc: 97.017%
	I - Batch: 100 | Loss: 0.541 | Acc: 90.188% | Wgt Acc: 96.713%
	I - Batch: 150 | Loss: 0.541 | Acc: 90.167% | Wgt Acc: 96.766%
	I - Batch: 200 | Loss: 0.542 | Acc: 90.031% | Wgt Acc: 96.760%
I - num batch: 222
I - Train -- Loss: 0.542 | Acc: 90.048% | Wgt Acc: 96.756% | LR: 1.250000e-04 | Dur: 140.96s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   1.   1. 107.]
 [  1. 577.   3.   0.  57.]
 [  0.   0. 728.   1.  96.]
 [  0.   0.   0. 534.  74.]
 [  7.   1.   2.   2. 666.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.078 | Acc: 64.497% | Wgt Acc: 62.680% | Dur: 14.32s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   4.   1.   8.   9.]
 [  1.  38.   6.   1.   7.]
 [  4.  15.  42.   0.  30.]
 [ 11.   8.  10.  63.   9.]
 [ 13.  13.  16.  14. 125.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.540 | Acc: 89.875% | Wgt Acc: 96.647%
	I - Batch: 100 | Loss: 0.549 | Acc: 89.062% | Wgt Acc: 96.246%
	I - Batch: 150 | Loss: 0.548 | Acc: 89.167% | Wgt Acc: 96.186%
	I - Batch: 200 | Loss: 0.545 | Acc: 89.438% | Wgt Acc: 96.211%
I - num batch: 222
I - Train -- Loss: 0.545 | Acc: 89.484% | Wgt Acc: 96.258% | LR: 1.250000e-04 | Dur: 135.84s
I - Confusion Matrix: [row->prediction - col->label]
[[684.   0.   2.   3. 108.]
 [  0. 576.   0.   0.  60.]
 [  0.   1. 726.   2. 101.]
 [  3.   1.   1. 530.  73.]
 [ 10.   0.   5.   3. 658.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.044 | Acc: 64.300% | Wgt Acc: 60.489% | Dur: 16.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   3.  18.  15.]
 [  1.  41.   9.   4.   8.]
 [  3.  11.  35.   0.  18.]
 [  7.   3.   7.  50.   6.]
 [ 10.  19.  21.  14. 133.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.538 | Acc: 90.500% | Wgt Acc: 96.834%
	I - Batch: 100 | Loss: 0.540 | Acc: 90.188% | Wgt Acc: 96.770%
	I - Batch: 150 | Loss: 0.539 | Acc: 90.042% | Wgt Acc: 96.697%
	I - Batch: 200 | Loss: 0.541 | Acc: 89.875% | Wgt Acc: 96.538%
I - num batch: 222
I - Train -- Loss: 0.538 | Acc: 90.189% | Wgt Acc: 96.674% | LR: 1.250000e-04 | Dur: 131.92s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   2. 106.]
 [  2. 574.   2.   0.  59.]
 [  0.   1. 727.   0.  95.]
 [  1.   0.   1. 532.  65.]
 [  3.   3.   3.   4. 675.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.042 | Acc: 64.497% | Wgt Acc: 62.357% | Dur: 13.84s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   6.   4.  15.  19.]
 [  1.  38.   8.   1.   9.]
 [  1.  15.  39.   2.  16.]
 [ 12.   4.  10.  58.  10.]
 [  8.  15.  14.  10. 126.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.522 | Acc: 92.125% | Wgt Acc: 97.582%
	I - Batch: 100 | Loss: 0.521 | Acc: 92.000% | Wgt Acc: 97.469%
	I - Batch: 150 | Loss: 0.525 | Acc: 91.667% | Wgt Acc: 97.250%
	I - Batch: 200 | Loss: 0.524 | Acc: 91.969% | Wgt Acc: 97.320%
I - num batch: 208
I - Train -- Loss: 0.525 | Acc: 91.767% | Wgt Acc: 97.197% | LR: 1.250000e-04 | Dur: 125.15s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   3.   1.  80.]
 [  2. 574.   0.   0.  42.]
 [  0.   0. 729.   1.  64.]
 [  0.   1.   0. 533.  65.]
 [  7.   3.   2.   3. 530.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.066 | Acc: 63.314% | Wgt Acc: 63.580% | Dur: 14.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   5.   3.  13.  23.]
 [  2.  42.   6.   3.  15.]
 [  0.  12.  37.   0.  16.]
 [ 12.   6.  16.  61.  13.]
 [  6.  13.  13.   9. 113.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.540 | Acc: 90.500% | Wgt Acc: 96.139%
	I - Batch: 100 | Loss: 0.537 | Acc: 91.562% | Wgt Acc: 96.920%
	I - Batch: 150 | Loss: 0.538 | Acc: 90.958% | Wgt Acc: 96.729%
	I - Batch: 200 | Loss: 0.536 | Acc: 90.844% | Wgt Acc: 96.752%
I - num batch: 222
I - Train -- Loss: 0.536 | Acc: 90.922% | Wgt Acc: 96.853% | LR: 1.250000e-04 | Dur: 134.50s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   0.   1.   2. 100.]
 [  1. 577.   1.   2.  48.]
 [  0.   0. 727.   0.  74.]
 [  2.   0.   1. 533.  76.]
 [  8.   1.   4.   1. 702.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.083 | Acc: 62.722% | Wgt Acc: 62.865% | Dur: 14.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   2.  16.  15.]
 [  2.  41.  11.   2.  10.]
 [  2.  15.  45.   1.  29.]
 [ 15.   6.   9.  56.  14.]
 [  5.  11.   8.  11. 112.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.529 | Acc: 91.875% | Wgt Acc: 96.794%
	I - Batch: 100 | Loss: 0.535 | Acc: 90.375% | Wgt Acc: 96.530%
	I - Batch: 150 | Loss: 0.533 | Acc: 91.000% | Wgt Acc: 96.826%
	I - Batch: 200 | Loss: 0.532 | Acc: 91.188% | Wgt Acc: 96.929%
I - num batch: 222
I - Train -- Loss: 0.532 | Acc: 91.458% | Wgt Acc: 97.018% | LR: 1.250000e-04 | Dur: 134.61s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   3.   3.  85.]
 [  0. 578.   1.   0.  49.]
 [  0.   0. 726.   0.  83.]
 [  0.   0.   0. 530.  63.]
 [  7.   0.   4.   5. 720.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.042 | Acc: 65.089% | Wgt Acc: 60.316% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   2.   2.   6.   8.]
 [  1.  36.   5.   0.   3.]
 [  1.  12.  36.   0.  17.]
 [ 15.   6.  11.  62.  12.]
 [ 15.  22.  21.  18. 140.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.524 | Acc: 92.250% | Wgt Acc: 97.146%
	I - Batch: 100 | Loss: 0.528 | Acc: 91.875% | Wgt Acc: 97.353%
	I - Batch: 150 | Loss: 0.531 | Acc: 91.542% | Wgt Acc: 97.189%
	I - Batch: 200 | Loss: 0.533 | Acc: 91.062% | Wgt Acc: 96.995%
I - num batch: 222
I - Train -- Loss: 0.534 | Acc: 90.894% | Wgt Acc: 96.925% | LR: 1.250000e-04 | Dur: 132.61s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   3.   2.  98.]
 [  0. 575.   0.   0.  39.]
 [  0.   0. 729.   0.  94.]
 [  1.   1.   1. 533.  71.]
 [  7.   2.   1.   3. 698.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.040 | Acc: 64.300% | Wgt Acc: 59.428% | Dur: 13.74s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   2.   3.  17.  15.]
 [  1.  41.   8.   2.   6.]
 [  1.   9.  32.   1.  13.]
 [  7.   3.  12.  49.   8.]
 [ 13.  23.  20.  17. 138.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.531 | Acc: 91.125% | Wgt Acc: 96.855%
	I - Batch: 100 | Loss: 0.532 | Acc: 91.188% | Wgt Acc: 96.852%
	I - Batch: 150 | Loss: 0.534 | Acc: 90.792% | Wgt Acc: 96.706%
	I - Batch: 200 | Loss: 0.534 | Acc: 90.969% | Wgt Acc: 96.829%
I - num batch: 222
I - Train -- Loss: 0.534 | Acc: 90.866% | Wgt Acc: 96.842% | LR: 1.250000e-04 | Dur: 132.72s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   2.   1.  99.]
 [  1. 578.   0.   0.  56.]
 [  0.   0. 723.   0.  85.]
 [  2.   0.   1. 534.  60.]
 [  6.   0.   8.   3. 700.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.053 | Acc: 64.694% | Wgt Acc: 65.275% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   6.   4.  16.  19.]
 [  1.  46.  10.   1.  12.]
 [  2.   9.  37.   1.  17.]
 [ 10.   4.  13.  61.  18.]
 [  5.  13.  11.   7. 114.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.538 | Acc: 90.875% | Wgt Acc: 96.877%
	I - Batch: 100 | Loss: 0.534 | Acc: 90.688% | Wgt Acc: 96.957%
	I - Batch: 150 | Loss: 0.535 | Acc: 90.625% | Wgt Acc: 96.908%
	I - Batch: 200 | Loss: 0.533 | Acc: 90.750% | Wgt Acc: 96.927%
I - num batch: 222
I - Train -- Loss: 0.532 | Acc: 90.922% | Wgt Acc: 96.968% | LR: 1.250000e-04 | Dur: 132.90s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   2.   0. 115.]
 [  0. 576.   2.   0.  48.]
 [  2.   0. 727.   1.  77.]
 [  0.   0.   0. 535.  62.]
 [  6.   2.   3.   2. 698.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.036 | Acc: 65.089% | Wgt Acc: 58.759% | Dur: 13.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   2.  14.   8.]
 [  0.  35.   3.   2.   5.]
 [  4.  14.  40.   1.  15.]
 [ 11.   3.   7.  49.   6.]
 [ 13.  23.  23.  20. 146.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.524 | Acc: 91.500% | Wgt Acc: 96.938%
	I - Batch: 100 | Loss: 0.529 | Acc: 90.750% | Wgt Acc: 96.868%
	I - Batch: 150 | Loss: 0.528 | Acc: 91.250% | Wgt Acc: 97.055%
	I - Batch: 200 | Loss: 0.529 | Acc: 91.312% | Wgt Acc: 97.070%
I - num batch: 222
I - Train -- Loss: 0.530 | Acc: 91.345% | Wgt Acc: 97.081% | LR: 1.250000e-04 | Dur: 132.00s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.  92.]
 [  1. 577.   0.   0.  46.]
 [  0.   0. 724.   1.  87.]
 [  0.   0.   4. 534.  62.]
 [  4.   1.   6.   1. 713.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.057 | Acc: 64.103% | Wgt Acc: 59.797% | Dur: 15.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   6.   5.  15.  25.]
 [  0.  30.   4.   1.   4.]
 [  1.  13.  34.   0.   7.]
 [  6.   7.  15.  55.   9.]
 [ 10.  22.  17.  15. 135.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.533 | Acc: 90.125% | Wgt Acc: 96.153%
	I - Batch: 100 | Loss: 0.532 | Acc: 90.750% | Wgt Acc: 96.683%
	I - Batch: 150 | Loss: 0.534 | Acc: 90.333% | Wgt Acc: 96.601%
	I - Batch: 200 | Loss: 0.533 | Acc: 90.562% | Wgt Acc: 96.639%
I - num batch: 222
I - Train -- Loss: 0.533 | Acc: 90.527% | Wgt Acc: 96.654% | LR: 1.250000e-04 | Dur: 136.72s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   4. 107.]
 [  1. 575.   2.   1.  50.]
 [  1.   0. 723.   1.  77.]
 [  1.   0.   1. 531.  75.]
 [  3.   3.   8.   1. 691.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.047 | Acc: 66.667% | Wgt Acc: 63.199% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   7.   5.  16.  12.]
 [  0.  40.   8.   1.   4.]
 [  2.   9.  34.   0.  12.]
 [  8.   5.  13.  57.  16.]
 [  7.  17.  15.  12. 136.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.530 | Acc: 92.500% | Wgt Acc: 97.416%
	I - Batch: 100 | Loss: 0.528 | Acc: 91.938% | Wgt Acc: 97.357%
	I - Batch: 150 | Loss: 0.530 | Acc: 91.333% | Wgt Acc: 97.168%
	I - Batch: 200 | Loss: 0.530 | Acc: 91.312% | Wgt Acc: 97.127%
I - num batch: 222
I - Train -- Loss: 0.529 | Acc: 91.288% | Wgt Acc: 97.118% | LR: 1.250000e-04 | Dur: 134.02s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   2.  78.]
 [  1. 577.   0.   0.  56.]
 [  0.   1. 726.   0.  88.]
 [  0.   0.   2. 533.  69.]
 [  3.   0.   5.   3. 709.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.034 | Acc: 65.089% | Wgt Acc: 60.178% | Dur: 14.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   3.  11.  12.]
 [  0.  36.   7.   1.   9.]
 [  2.  10.  33.   1.  11.]
 [ 10.   2.   7.  56.   8.]
 [ 11.  26.  25.  17. 140.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.525 | Acc: 92.125% | Wgt Acc: 97.262%
	I - Batch: 100 | Loss: 0.523 | Acc: 91.875% | Wgt Acc: 97.307%
	I - Batch: 150 | Loss: 0.522 | Acc: 92.125% | Wgt Acc: 97.416%
	I - Batch: 200 | Loss: 0.525 | Acc: 91.906% | Wgt Acc: 97.292%
I - num batch: 222
I - Train -- Loss: 0.525 | Acc: 92.021% | Wgt Acc: 97.320% | LR: 1.250000e-04 | Dur: 134.97s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   2.   1.  86.]
 [  0. 577.   1.   0.  36.]
 [  1.   0. 729.   2.  73.]
 [  0.   0.   0. 534.  70.]
 [  7.   1.   2.   1. 735.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.066 | Acc: 65.878% | Wgt Acc: 64.652% | Dur: 17.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 80.   9.   7.  26.  27.]
 [  0.  44.   7.   2.  14.]
 [  1.   8.  40.   1.  13.]
 [  3.   2.   7.  47.   3.]
 [  4.  15.  14.  10. 123.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.524 | Acc: 91.875% | Wgt Acc: 97.653%
	I - Batch: 100 | Loss: 0.526 | Acc: 92.312% | Wgt Acc: 97.677%
	I - Batch: 150 | Loss: 0.523 | Acc: 91.917% | Wgt Acc: 97.528%
	I - Batch: 200 | Loss: 0.527 | Acc: 91.844% | Wgt Acc: 97.387%
I - num batch: 222
I - Train -- Loss: 0.528 | Acc: 91.796% | Wgt Acc: 97.347% | LR: 1.250000e-04 | Dur: 134.21s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   1.   1. 110.]
 [  1. 576.   0.   0.  40.]
 [  0.   1. 728.   1.  69.]
 [  0.   0.   2. 536.  57.]
 [  4.   1.   3.   0. 724.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.050 | Acc: 64.892% | Wgt Acc: 62.115% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   5.   2.  17.  16.]
 [  1.  41.   7.   1.   6.]
 [  1.  11.  32.   0.  14.]
 [ 12.   7.  14.  59.  14.]
 [  7.  14.  20.   9. 130.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.531 | Acc: 92.375% | Wgt Acc: 97.144%
	I - Batch: 100 | Loss: 0.527 | Acc: 91.688% | Wgt Acc: 97.002%
	I - Batch: 150 | Loss: 0.523 | Acc: 92.250% | Wgt Acc: 97.356%
	I - Batch: 200 | Loss: 0.522 | Acc: 92.469% | Wgt Acc: 97.469%
I - num batch: 222
I - Train -- Loss: 0.523 | Acc: 92.388% | Wgt Acc: 97.385% | LR: 1.250000e-04 | Dur: 130.38s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   2.  93.]
 [  1. 576.   1.   2.  40.]
 [  0.   0. 729.   0.  73.]
 [  0.   0.   0. 532.  45.]
 [  5.   2.   3.   2. 749.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.054 | Acc: 64.892% | Wgt Acc: 61.884% | Dur: 14.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   6.   5.  23.  18.]
 [  1.  43.  10.   2.  11.]
 [  1.  10.  36.   1.  14.]
 [  6.   5.  11.  48.   7.]
 [  8.  14.  13.  12. 130.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.518 | Acc: 92.750% | Wgt Acc: 97.813%
	I - Batch: 100 | Loss: 0.525 | Acc: 92.125% | Wgt Acc: 97.539%
	I - Batch: 150 | Loss: 0.527 | Acc: 91.875% | Wgt Acc: 97.412%
	I - Batch: 200 | Loss: 0.529 | Acc: 92.062% | Wgt Acc: 97.541%
I - num batch: 222
I - Train -- Loss: 0.530 | Acc: 91.796% | Wgt Acc: 97.373% | LR: 1.250000e-04 | Dur: 134.74s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   1.   0.  88.]
 [  3. 577.   0.   0.  40.]
 [  0.   0. 731.   1.  94.]
 [  1.   0.   1. 535.  55.]
 [  3.   1.   1.   2. 723.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.052 | Acc: 64.892% | Wgt Acc: 61.608% | Dur: 14.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   5.   6.  18.  18.]
 [  2.  37.  10.   2.   5.]
 [  2.  14.  42.   4.  18.]
 [  7.   4.   6.  48.   8.]
 [  6.  18.  11.  14. 131.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.528 | Acc: 91.750% | Wgt Acc: 97.160%
	I - Batch: 100 | Loss: 0.531 | Acc: 91.625% | Wgt Acc: 97.137%
	I - Batch: 150 | Loss: 0.529 | Acc: 91.833% | Wgt Acc: 97.180%
	I - Batch: 200 | Loss: 0.528 | Acc: 91.594% | Wgt Acc: 97.148%
I - num batch: 222
I - Train -- Loss: 0.527 | Acc: 91.711% | Wgt Acc: 97.175% | LR: 1.250000e-04 | Dur: 134.28s
I - Confusion Matrix: [row->prediction - col->label]
[[688.   0.   1.   2. 101.]
 [  1. 577.   0.   1.  40.]
 [  2.   0. 730.   1.  79.]
 [  2.   0.   0. 532.  54.]
 [  4.   1.   3.   2. 726.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.061 | Acc: 65.483% | Wgt Acc: 62.473% | Dur: 14.19s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   6.   4.  14.  12.]
 [  1.  41.   8.   0.  11.]
 [  1.  11.  34.   2.  16.]
 [ 11.   3.  13.  58.   9.]
 [  8.  17.  16.  12. 132.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.529 | Acc: 91.375% | Wgt Acc: 97.073%
	I - Batch: 100 | Loss: 0.520 | Acc: 91.625% | Wgt Acc: 97.383%
	I - Batch: 150 | Loss: 0.520 | Acc: 92.042% | Wgt Acc: 97.515%
	I - Batch: 200 | Loss: 0.524 | Acc: 91.375% | Wgt Acc: 97.185%
I - num batch: 222
I - Train -- Loss: 0.525 | Acc: 91.260% | Wgt Acc: 97.167% | LR: 1.250000e-04 | Dur: 133.41s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.  90.]
 [  0. 578.   2.   0.  48.]
 [  1.   0. 728.   0.  82.]
 [  0.   0.   0. 533.  74.]
 [  4.   0.   4.   3. 706.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.071 | Acc: 64.103% | Wgt Acc: 63.026% | Dur: 14.64s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   8.   5.  12.  17.]
 [  1.  34.   5.   1.   3.]
 [  1.  16.  42.   3.  25.]
 [  6.   6.  12.  58.  15.]
 [  9.  14.  11.  12. 120.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.525 | Acc: 91.500% | Wgt Acc: 97.321%
	I - Batch: 100 | Loss: 0.520 | Acc: 92.438% | Wgt Acc: 97.561%
	I - Batch: 150 | Loss: 0.520 | Acc: 92.500% | Wgt Acc: 97.637%
	I - Batch: 200 | Loss: 0.523 | Acc: 92.406% | Wgt Acc: 97.559%
I - num batch: 222
I - Train -- Loss: 0.523 | Acc: 92.247% | Wgt Acc: 97.547% | LR: 1.250000e-04 | Dur: 133.47s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   2.  98.]
 [  1. 577.   1.   0.  45.]
 [  0.   0. 732.   1.  60.]
 [  0.   0.   0. 534.  60.]
 [  4.   1.   1.   1. 737.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.040 | Acc: 65.878% | Wgt Acc: 63.626% | Dur: 14.03s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   6.   2.  16.  18.]
 [  2.  45.   8.   1.  10.]
 [  0.   7.  35.   0.  13.]
 [  9.   3.  12.  56.  10.]
 [  8.  17.  18.  13. 129.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.517 | Acc: 92.500% | Wgt Acc: 97.674%
	I - Batch: 100 | Loss: 0.527 | Acc: 91.125% | Wgt Acc: 97.032%
	I - Batch: 150 | Loss: 0.531 | Acc: 91.042% | Wgt Acc: 96.820%
	I - Batch: 200 | Loss: 0.530 | Acc: 91.156% | Wgt Acc: 97.024%
I - num batch: 222
I - Train -- Loss: 0.529 | Acc: 91.147% | Wgt Acc: 97.025% | LR: 1.250000e-04 | Dur: 134.93s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   0.   2. 105.]
 [  1. 577.   0.   0.  45.]
 [  1.   1. 730.   0.  85.]
 [  2.   0.   2. 533.  59.]
 [  6.   0.   2.   3. 706.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.025 | Acc: 67.653% | Wgt Acc: 62.623% | Dur: 16.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   7.   5.  13.  15.]
 [  1.  41.   6.   3.   4.]
 [  0.   9.  37.   0.  11.]
 [ 12.   5.  11.  55.   5.]
 [ 10.  16.  16.  15. 145.]]

I - Local maximum validation set accuracy:  67.65

I - Validation set results: 
[14-1-2-0.18][50-3-4--0.48][124-2-3--0.20][127-0-0-0.99][443-2-2-0.99][567-0-0-0.97][573-1-1-0.95][615-0-0-0.96][695-1-2-0.99][722-3-3-0.99]
[826-0-0-0.99][878-0-3-0.94][1103-0-4-0.40][1212-3-4-0.99][1368-0-0-0.99][2181-2-0--0.83][2476-2-2-0.99][2721-2-2-0.48][2818-1-3-0.97][2886-2-1-0.42]
[3231-2-2-0.99][3333-2-2--0.21][3482-2-2-0.86][3536-3-3-0.91][3625-1-1-0.76][3909-0-0-0.90][4035-0-0-0.49][4140-0-0-0.99][4214-1-3-0.10][4346-1-0--0.21]
[4581-2-4-0.57][4708-3-3-0.70][4838-3-4-0.23][4845-1-1--0.18][4868-0-0-0.99][4939-0-4-0.12][4984-2-3-0.99][5078-1-4-0.57][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.99][5843-1-1-0.97][5949-3-3-0.60][5987-2-4-0.99][6014-3-3-0.96][6033-3-3-0.93][6313-0-0-0.99][6421-3-3-0.99][6500-1-1--0.35][6583-3-3-0.99]
[6683-3-3-0.73][6825-2-1-0.99][6998-3-0-0.25][7049-3-3-0.93][7517-1-1-0.63][7521-1-1-0.98][7528-1-1-0.31][7949-1-2-0.96][8135-1-0-0.73][8185-3-0-0.99]
[8269-3-1-0.09][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.53][8903-1-2-0.65][9001-2-2-0.26][9036-2-2-0.73][9281-3-1-0.40][9300-2-2-0.99]
[9571-0-4-0.80][9617-1-1-0.99][9644-2-2-0.44][9705-2-4-0.81][9801-0-3-0.97][9803-3-3-0.98][9865-3-3-0.99][9896-2-0-0.07][10314-1-0-0.07][10337-3-3-0.99]
[10403-0-4-0.98][10653-2-4-0.68][10704-2-2-0.81][10719-1-1-0.45][10727-1-4-0.95][10836-0-0-0.99][10969-2-4-0.69][11042-0-0-0.87][11088-1-1-0.99][11322-0-0-0.98]
[11398-2-4-0.07][11499-0-0-0.99][11502-3-3--0.04][11512-3-3-0.99][11608-1-2-0.31][11610-0-0-0.99][11692-0-0-0.94][11905-0-0-0.73][11993-1-1-0.41][12002-2-2-0.19]
[12052-0-0-0.99][12201-0-3-0.99][12235-2-4-0.55][12320-1-4-0.99][12377-2-4-0.89][12398-2-3--0.17][12503-1-4-0.02][12617-0-3-0.70][12685-3-4-0.44][12738-2-0-0.99]
[12742-2-2-0.99][12823-0-0-0.99][13110-1-1-0.99][13240-3-0-0.83][13253-1-1-0.67][13273-0-0-0.99][13634-1-4-0.88][13763-2-3--0.61][13905-3-3--0.13][14060-2-1-0.96]
[14065-3-0-0.97][14147-3-3-0.96][14595-2-2--0.07][14687-2-2-0.99][14788-2-2-0.90][14869-1-4-0.54][14872-3-4-0.86][14877-1-1-0.99][14927-0-3-0.93][15066-0-0-0.99]
[15175-1-4-0.53][15178-2-3-0.85][15375-3-0-0.33][15389-3-3-0.99][15568-2-1-0.75][15675-3-3-0.99][15869-1-0-0.17][16207-3-0-0.93][16236-0-0-0.89][16302-3-4-0.47]
[16331-2-2-0.99][16381-0-3-0.97][16488-1-1-0.67][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.44][16801-0-0-0.99][16828-0-0-0.95][17137-3-3-0.43][17245-1-2-0.32]
[17278-3-4-0.56][17282-0-0-0.88][17311-2-2-0.95][17336-2-2--0.12][17608-3-3-0.99][17627-0-0-0.91][17877-3-4-0.53][17924-1-3-0.50][17984-3-3-0.99][18211-0-0--0.25]
[18276-3-3-0.99][18287-1-4--0.06][18394-0-0-0.99][18428-0-0-0.99][18442-0-3-0.99][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.99][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.99][18824-2-4-0.84][18890-3-3-0.99][18930-3-4-0.48][18938-3-1-0.00][19817-1-2--0.49][19839-0-0-0.99][19930-3-3-0.35][19944-0-4-0.89][20036-2-2-0.99]
[20101-3-3-0.95][20474-1-1-0.78][20547-3-4-0.57][20929-2-2-0.99][21245-1-1-0.69][21257-3-3-0.92][21293-1-2-0.93][21316-1-1-0.99][21384-1-4-0.98][21448-1-1-0.82]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-4-0.69][21943-3-4-0.96][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.92][21998-1-1-0.42][22025-0-4-0.99][22228-3-3-0.99]
[22446-1-1-0.99][22494-3-3-0.94][22757-0-0-0.99][22811-3-3-0.99][22976-3-4-0.73][22985-3-3-0.91][23014-0-0-0.99][23112-1-1-0.99][23144-3-3-0.99][23168-2-3-0.23]
[23219-0-0-0.97][23363-3-3-0.92][23470-0-0-0.96][23486-2-0-0.25][23497-0-3-0.99][23516-0-0-0.99][23690-1-4-0.45][23921-2-2-0.61][23936-1-0-0.99][24040-3-0-0.94]
[24111-1-4-0.98][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.94][24364-1-4--0.31][24427-3-3-0.77][24477-2-2-0.67][24495-2-1-0.02][24893-2-2--0.62]
[25012-1-1-0.92][25121-2-2-0.05][25165-3-3-0.99][25183-0-0-0.94][25297-3-3-0.99][25398-0-0-0.99][25574-2-2-0.92][25644-1-1-0.69][25718-1-1-0.44][25774-2-4-0.63]
[26032-3-3-0.99][26051-3-3-0.99][26120-0-4-0.99][26321-1-1-0.53][26732-1-1-0.99][26784-3-3-0.99][26827-3-3-0.43][26833-0-3-0.99][26838-2-4-0.46][26860-1-4-0.32]
[26948-0-0-0.99][27049-3-0-0.99][27098-1-0-0.54][27526-0-0-0.98][27639-3-3-0.94][27698-3-0-0.89][27772-0-0-0.94][27890-1-1-0.87][28040-0-4-0.99][28503-2-2-0.99]
[28577-1-1-0.17][28959-0-0-0.99][29198-3-3-0.99][29777-0-0-0.99][29877-2-2-0.13][30035-1-1-0.96][30098-0-3-0.82][30326-1-1-0.99][30572-2-3-0.14][30716-0-4-0.99]
[30806-2-3-0.21][30906-1-1-0.99][31007-0-0-0.99][31181-3-3-0.66][31238-0-3-0.62][31347-0-0-0.99][31422-2-2-0.89][31429-3-3-0.54][31431-0-0-0.01][31432-1-1-0.99]
[31477-0-0-0.98][31524-1-2--0.19][31597-1-1-0.27][31619-1-0-0.98][31701-0-3-0.57][31755-0-0-0.99][31854-3-3-0.97][32074-1-1--0.04][32078-3-3-0.93][32111-1-1-0.75]
[32127-1-1-0.99][32140-3-3-0.99][32263-2-4-0.30][32365-0-0-0.99][32411-2-3-0.99][32429-3-3-0.99][32473-3-0-0.34][32574-3-3-0.99][32584-0-0-0.56][32622-0-1--0.63]
[32858-3-0-0.97][32969-3-0-0.99][33016-2-2-0.99][33031-1-3-0.98][33035-2-3--0.01][33133-2-2-0.40][33173-2-4--0.09][33175-3-4-0.99][33306-3-3-0.99][33309-2-3-0.99]
[33474-0-0-0.99][33478-2-4--0.28][33618-1-4-0.93][33712-0-0-0.06][33782-2-4-0.96][33914-3-3-0.99][34076-3-4-0.51][34112-2-2-0.99][34138-2-4--0.16][34239-1-3-0.39]
[34364-2-2-0.99][34617-1-4-0.31][34751-3-3-0.94][34783-2-1--0.32][35015-3-4-0.98][35018-1-1-0.33][35288-2-2--0.07][0-4-4-0.63][1-4-4-0.96][2-4-4-0.99]
[3-4-4-0.86][4-4-4-0.17][5-4-1--0.41][6-4-4-0.66][7-4-4-0.31][8-4-4-0.64][9-4-0-0.91][10-4-4-0.99][11-4-4-0.99][12-4-4-0.97]
[14-4-4-0.98][15-4-3-0.99][16-4-4-0.99][17-4-4-0.46][18-4-4-0.83][19-4-0-0.20][20-4-0-0.78][21-4-4-0.22][22-4-4-0.99][23-4-4-0.53]
[24-4-4-0.99][25-4-4-0.99][26-4-4-0.56][27-4-4-0.99][28-4-4-0.99][29-4-4--0.49][30-4-4-0.38][31-4-4-0.99][32-4-4-0.99][33-4-2-0.59]
[34-4-4-0.99][35-4-4-0.99][37-4-4-0.99][39-4-0-0.99][40-4-4-0.97][41-4-4-0.64][42-4-4-0.75][43-4-4-0.97][45-4-4-0.26][46-4-4-0.99]
[47-4-4-0.99][48-4-4-0.99][51-4-4-0.99][52-4-4-0.94][53-4-4-0.99][54-4-4-0.97][55-4-4-0.88][56-4-1-0.43][57-4-0-0.94][58-4-2-0.05]
[59-4-0-0.99][60-4-4-0.53][61-4-4-0.93][62-4-4-0.95][63-4-2-0.55][64-4-2-0.94][65-4-4-0.99][66-4-4-0.99][67-4-4-0.79][68-4-4--0.41]
[69-4-0--0.12][70-4-4-0.91][72-4-4-0.92][73-4-1-0.99][74-4-2-0.70][75-4-2--0.62][77-4-4-0.99][78-4-4-0.93][79-4-4-0.99][80-4-4-0.99]
[81-4-4-0.58][82-4-4-0.84][83-4-4-0.24][84-4-4-0.99][85-4-4-0.99][86-4-4-0.99][87-4-4-0.99][88-4-4-0.99][89-4-0-0.49][90-4-4-0.70]
[91-4-4-0.54][92-4-3--0.23][93-4-0--0.18][94-4-4-0.99][95-4-3--0.39][96-4-4-0.99][97-4-4-0.75][98-4-4-0.15][99-4-4-0.98][100-4-4-0.95]
[101-4-4-0.99][102-4-4-0.69][103-4-3-0.56][104-4-4-0.99][105-4-4-0.94][106-4-4-0.99][107-4-4-0.99][108-4-4-0.86][109-4-4-0.49][110-4-4-0.28]
[111-4-0-0.98][112-4-4-0.58][113-4-4-0.93][114-4-4-0.30][115-4-4-0.95][116-4-4-0.26][117-4-4-0.99][119-4-4-0.99][121-4-4-0.91][122-4-4-0.99]
[124-4-4-0.68][125-4-4-0.99][126-4-4-0.99][127-4-2--0.14][128-4-4-0.30][129-4-4-0.89][130-4-4-0.62][131-4-4-0.05][132-4-4-0.87][133-4-4-0.99]
[135-4-4-0.73][136-4-1-0.88][137-4-4-0.96][138-4-4-0.99][139-4-4-0.99][140-4-0-0.69][141-4-0-0.85][142-4-4-0.99][143-4-4-0.98][144-4-4-0.99]
[145-4-4-0.99][148-4-0-0.99][149-4-4-0.98][150-4-4-0.99][151-4-4-0.99][152-4-4-0.95][153-4-4-0.99][154-4-4-0.99][155-4-4-0.99][156-4-4-0.50]
[157-4-2--0.35][158-4-4-0.93][160-4-4-0.08][161-4-2-0.94][162-4-4-0.06][164-4-4-0.98][165-4-4-0.95][167-4-4-0.95][168-4-4-0.78][170-4-4-0.96]
[171-4-4-0.99][172-4-4-0.99][173-4-4-0.99][174-4-0-0.81][175-4-4-0.98][177-4-4-0.99][178-4-4-0.98][179-4-4-0.98][180-4-4-0.98][181-4-4-0.81]
[182-4-3-0.99][183-4-4-0.99][184-4-4-0.98][186-4-4-0.89][187-4-4--0.01][188-4-4-0.86][189-4-0-0.98][190-4-4-0.17][191-4-4-0.97][192-4-4-0.89]
[193-4-2-0.04][194-4-2--0.03][195-4-4-0.84][196-4-4-0.99][197-4-4-0.99][198-4-4-0.99][199-4-4-0.50]
---------------------------
I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.532 | Acc: 91.000% | Wgt Acc: 96.987%
	I - Batch: 100 | Loss: 0.526 | Acc: 92.125% | Wgt Acc: 97.488%
	I - Batch: 150 | Loss: 0.520 | Acc: 92.167% | Wgt Acc: 97.595%
	I - Batch: 200 | Loss: 0.521 | Acc: 92.125% | Wgt Acc: 97.559%
I - num batch: 222
I - Train -- Loss: 0.522 | Acc: 92.191% | Wgt Acc: 97.537% | LR: 1.250000e-04 | Dur: 133.18s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   2.   2.  98.]
 [  1. 578.   0.   0.  40.]
 [  0.   0. 729.   1.  72.]
 [  0.   0.   0. 535.  55.]
 [  3.   0.   3.   0. 735.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.066 | Acc: 62.919% | Wgt Acc: 59.520% | Dur: 14.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   9.   9.  18.  25.]
 [  1.  34.   4.   1.   3.]
 [  1.  12.  28.   0.   8.]
 [  6.   4.  13.  56.  15.]
 [  8.  19.  21.  11. 129.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.517 | Acc: 92.375% | Wgt Acc: 97.726%
	I - Batch: 100 | Loss: 0.520 | Acc: 92.375% | Wgt Acc: 97.583%
	I - Batch: 150 | Loss: 0.522 | Acc: 92.042% | Wgt Acc: 97.506%
	I - Batch: 200 | Loss: 0.521 | Acc: 91.875% | Wgt Acc: 97.507%
I - num batch: 222
I - Train -- Loss: 0.521 | Acc: 92.078% | Wgt Acc: 97.592% | LR: 1.250000e-04 | Dur: 134.56s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  94.]
 [  0. 578.   1.   0.  48.]
 [  1.   0. 731.   0.  71.]
 [  0.   0.   1. 536.  59.]
 [  3.   0.   1.   1. 728.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.035 | Acc: 65.878% | Wgt Acc: 62.623% | Dur: 17.07s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   7.   3.  20.  20.]
 [  1.  40.   5.   3.   5.]
 [  2.  12.  40.   2.  14.]
 [  8.   7.  10.  51.   8.]
 [  7.  12.  17.  10. 133.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.527 | Acc: 90.750% | Wgt Acc: 96.993%
	I - Batch: 100 | Loss: 0.527 | Acc: 91.250% | Wgt Acc: 97.067%
	I - Batch: 150 | Loss: 0.528 | Acc: 91.542% | Wgt Acc: 97.116%
	I - Batch: 200 | Loss: 0.525 | Acc: 91.594% | Wgt Acc: 97.220%
I - num batch: 222
I - Train -- Loss: 0.525 | Acc: 91.739% | Wgt Acc: 97.278% | LR: 1.250000e-04 | Dur: 134.58s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   1.   1.  81.]
 [  3. 577.   2.   0.  46.]
 [  0.   0. 730.   0.  83.]
 [  2.   0.   0. 536.  66.]
 [  5.   1.   1.   1. 724.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.011 | Acc: 67.258% | Wgt Acc: 62.150% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   3.   2.  14.  11.]
 [  1.  34.   4.   2.   2.]
 [  2.  19.  48.   3.  14.]
 [ 10.   1.   5.  52.   9.]
 [ 12.  21.  16.  15. 144.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.518 | Acc: 92.500% | Wgt Acc: 97.437%
	I - Batch: 100 | Loss: 0.518 | Acc: 92.938% | Wgt Acc: 97.626%
	I - Batch: 150 | Loss: 0.519 | Acc: 92.792% | Wgt Acc: 97.568%
	I - Batch: 200 | Loss: 0.520 | Acc: 92.406% | Wgt Acc: 97.424%
I - num batch: 222
I - Train -- Loss: 0.520 | Acc: 92.332% | Wgt Acc: 97.405% | LR: 1.250000e-04 | Dur: 133.20s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   2.   0.  87.]
 [  0. 577.   0.   0.  44.]
 [  1.   0. 731.   0.  73.]
 [  3.   0.   0. 534.  50.]
 [  6.   1.   1.   4. 746.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.056 | Acc: 63.116% | Wgt Acc: 59.728% | Dur: 16.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 72.   7.   5.  16.  18.]
 [  1.  29.   8.   1.   4.]
 [  1.  10.  34.   3.  18.]
 [  5.   5.   8.  56.  11.]
 [  9.  27.  20.  10. 129.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.515 | Acc: 92.125% | Wgt Acc: 97.595%
	I - Batch: 100 | Loss: 0.514 | Acc: 92.812% | Wgt Acc: 97.836%
	I - Batch: 150 | Loss: 0.519 | Acc: 92.667% | Wgt Acc: 97.738%
	I - Batch: 200 | Loss: 0.519 | Acc: 92.656% | Wgt Acc: 97.715%
I - num batch: 222
I - Train -- Loss: 0.520 | Acc: 92.416% | Wgt Acc: 97.624% | LR: 1.250000e-04 | Dur: 135.76s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   0.  92.]
 [  0. 577.   0.   1.  37.]
 [  0.   0. 731.   0.  70.]
 [  0.   0.   1. 535.  59.]
 [  4.   1.   1.   2. 742.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 64.892% | Wgt Acc: 60.270% | Dur: 13.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   4.  15.  15.]
 [  1.  35.   4.   1.   6.]
 [  1.  12.  36.   1.  14.]
 [  8.   7.  13.  54.   7.]
 [ 12.  20.  18.  15. 138.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.514 | Acc: 92.625% | Wgt Acc: 97.905%
	I - Batch: 100 | Loss: 0.521 | Acc: 91.688% | Wgt Acc: 97.483%
	I - Batch: 150 | Loss: 0.523 | Acc: 91.750% | Wgt Acc: 97.408%
	I - Batch: 200 | Loss: 0.527 | Acc: 91.688% | Wgt Acc: 97.235%
I - num batch: 222
I - Train -- Loss: 0.527 | Acc: 91.655% | Wgt Acc: 97.227% | LR: 1.250000e-04 | Dur: 133.77s
I - Confusion Matrix: [row->prediction - col->label]
[[687.   0.   2.   1. 103.]
 [  1. 577.   1.   0.  43.]
 [  0.   0. 729.   1.  71.]
 [  3.   0.   1. 536.  61.]
 [  6.   1.   1.   0. 722.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.026 | Acc: 66.667% | Wgt Acc: 62.023% | Dur: 15.73s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   3.   2.  14.  14.]
 [  1.  42.   9.   3.  10.]
 [  2.  11.  36.   3.  11.]
 [  3.   4.  11.  50.   4.]
 [ 13.  18.  17.  16. 141.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.519 | Acc: 92.750% | Wgt Acc: 97.757%
	I - Batch: 100 | Loss: 0.524 | Acc: 91.938% | Wgt Acc: 97.231%
	I - Batch: 150 | Loss: 0.522 | Acc: 92.167% | Wgt Acc: 97.315%
	I - Batch: 200 | Loss: 0.523 | Acc: 91.969% | Wgt Acc: 97.246%
I - num batch: 222
I - Train -- Loss: 0.522 | Acc: 92.134% | Wgt Acc: 97.294% | LR: 1.250000e-04 | Dur: 132.64s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   1.   0.  81.]
 [  0. 574.   2.   0.  44.]
 [  1.   0. 728.   1.  78.]
 [  0.   1.   0. 535.  56.]
 [  6.   3.   3.   2. 741.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.059 | Acc: 63.905% | Wgt Acc: 62.738% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   5.   3.  17.  16.]
 [  2.  42.   6.   2.  10.]
 [  3.  17.  39.   3.  17.]
 [  5.   4.  17.  53.  17.]
 [  8.  10.  10.  11. 120.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.518 | Acc: 92.250% | Wgt Acc: 97.325%
	I - Batch: 100 | Loss: 0.518 | Acc: 92.375% | Wgt Acc: 97.446%
	I - Batch: 150 | Loss: 0.518 | Acc: 92.125% | Wgt Acc: 97.437%
	I - Batch: 200 | Loss: 0.518 | Acc: 92.312% | Wgt Acc: 97.531%
I - num batch: 222
I - Train -- Loss: 0.519 | Acc: 92.247% | Wgt Acc: 97.495% | LR: 1.250000e-04 | Dur: 133.03s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   1.   2.  84.]
 [  1. 577.   1.   0.  44.]
 [  0.   0. 731.   1.  72.]
 [  1.   0.   0. 535.  61.]
 [  5.   1.   1.   0. 739.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.059 | Acc: 65.680% | Wgt Acc: 61.239% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   2.   2.  12.   9.]
 [  4.  45.  11.   3.   9.]
 [  1.  12.  38.   3.  17.]
 [ 13.   2.   9.  54.   6.]
 [ 13.  17.  15.  14. 139.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.519 | Acc: 93.250% | Wgt Acc: 97.773%
	I - Batch: 100 | Loss: 0.522 | Acc: 92.250% | Wgt Acc: 97.570%
	I - Batch: 150 | Loss: 0.519 | Acc: 92.500% | Wgt Acc: 97.737%
	I - Batch: 200 | Loss: 0.521 | Acc: 92.531% | Wgt Acc: 97.590%
I - num batch: 222
I - Train -- Loss: 0.522 | Acc: 92.557% | Wgt Acc: 97.583% | LR: 1.250000e-04 | Dur: 134.52s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   0.  83.]
 [  2. 577.   1.   0.  42.]
 [  0.   0. 731.   1.  70.]
 [  1.   0.   0. 536.  55.]
 [  5.   1.   2.   1. 750.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.051 | Acc: 64.694% | Wgt Acc: 59.059% | Dur: 16.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   5.   3.  15.  12.]
 [  0.  33.   3.   2.   5.]
 [  1.  10.  33.   3.  14.]
 [  9.   4.   9.  50.   7.]
 [  8.  26.  27.  16. 142.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.521 | Acc: 92.125% | Wgt Acc: 97.594%
	I - Batch: 100 | Loss: 0.522 | Acc: 92.188% | Wgt Acc: 97.548%
	I - Batch: 150 | Loss: 0.520 | Acc: 92.292% | Wgt Acc: 97.605%
	I - Batch: 200 | Loss: 0.522 | Acc: 91.812% | Wgt Acc: 97.384%
I - num batch: 222
I - Train -- Loss: 0.521 | Acc: 92.050% | Wgt Acc: 97.463% | LR: 1.250000e-04 | Dur: 134.68s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   1.  97.]
 [  1. 577.   0.   1.  45.]
 [  0.   0. 731.   0.  70.]
 [  0.   0.   1. 533.  57.]
 [  3.   1.   1.   3. 731.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.040 | Acc: 65.286% | Wgt Acc: 62.657% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   5.   3.  14.  17.]
 [  1.  40.   6.   1.   5.]
 [  1.   9.  30.   1.  12.]
 [  9.   4.  14.  60.  16.]
 [  6.  20.  22.  10. 130.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.503 | Acc: 94.000% | Wgt Acc: 98.241%
	I - Batch: 100 | Loss: 0.507 | Acc: 93.375% | Wgt Acc: 98.180%
	I - Batch: 150 | Loss: 0.510 | Acc: 93.208% | Wgt Acc: 98.041%
	I - Batch: 200 | Loss: 0.510 | Acc: 93.375% | Wgt Acc: 98.048%
I - num batch: 208
I - Train -- Loss: 0.511 | Acc: 93.239% | Wgt Acc: 97.987% | LR: 1.250000e-04 | Dur: 127.16s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   1.  66.]
 [  0. 578.   0.   0.  36.]
 [  1.   0. 731.   1.  60.]
 [  1.   0.   1. 536.  54.]
 [  2.   0.   1.   0. 565.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.055 | Acc: 63.905% | Wgt Acc: 60.754% | Dur: 14.40s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   1.  16.  17.]
 [  1.  44.   9.   3.   8.]
 [  0.   8.  35.   1.  20.]
 [ 15.   3.  13.  56.   5.]
 [ 13.  20.  17.  10. 130.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.518 | Acc: 92.125% | Wgt Acc: 97.278%
	I - Batch: 100 | Loss: 0.524 | Acc: 91.938% | Wgt Acc: 97.185%
	I - Batch: 150 | Loss: 0.523 | Acc: 91.833% | Wgt Acc: 97.261%
	I - Batch: 200 | Loss: 0.522 | Acc: 91.844% | Wgt Acc: 97.317%
I - num batch: 222
I - Train -- Loss: 0.520 | Acc: 91.937% | Wgt Acc: 97.382% | LR: 1.250000e-04 | Dur: 141.21s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   1.  95.]
 [  1. 576.   0.   0.  46.]
 [  0.   0. 730.   0.  63.]
 [  0.   0.   2. 535.  67.]
 [  5.   2.   2.   2. 729.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.029 | Acc: 66.272% | Wgt Acc: 60.731% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   4.   2.  14.  11.]
 [  0.  34.   5.   0.   2.]
 [  2.   6.  36.   1.  13.]
 [  7.   5.  12.  57.   9.]
 [ 15.  29.  20.  14. 145.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.504 | Acc: 93.375% | Wgt Acc: 97.846%
	I - Batch: 100 | Loss: 0.510 | Acc: 92.938% | Wgt Acc: 97.741%
	I - Batch: 150 | Loss: 0.511 | Acc: 93.208% | Wgt Acc: 97.842%
	I - Batch: 200 | Loss: 0.515 | Acc: 92.969% | Wgt Acc: 97.745%
I - num batch: 222
I - Train -- Loss: 0.517 | Acc: 92.895% | Wgt Acc: 97.667% | LR: 1.250000e-04 | Dur: 131.43s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   1.   1.  85.]
 [  1. 576.   0.   0.  35.]
 [  0.   1. 731.   0.  60.]
 [  0.   0.   2. 534.  58.]
 [  4.   1.   0.   3. 762.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.031 | Acc: 65.680% | Wgt Acc: 60.777% | Dur: 14.02s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   5.  13.   7.]
 [  1.  43.  10.   2.  10.]
 [  1.  14.  36.   1.  18.]
 [ 14.   3.   7.  53.   4.]
 [ 12.  16.  17.  17. 141.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.515 | Acc: 93.375% | Wgt Acc: 97.741%
	I - Batch: 100 | Loss: 0.524 | Acc: 92.125% | Wgt Acc: 97.390%
	I - Batch: 150 | Loss: 0.525 | Acc: 91.708% | Wgt Acc: 97.332%
	I - Batch: 200 | Loss: 0.526 | Acc: 91.438% | Wgt Acc: 97.241%
I - num batch: 222
I - Train -- Loss: 0.524 | Acc: 91.683% | Wgt Acc: 97.319% | LR: 1.250000e-04 | Dur: 133.02s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   1.   0.   0. 108.]
 [  2. 576.   1.   1.  33.]
 [  1.   1. 729.   0.  82.]
 [  0.   0.   1. 537.  57.]
 [  4.   0.   3.   0. 720.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.058 | Acc: 64.892% | Wgt Acc: 57.560% | Dur: 15.71s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   4.   3.  14.  14.]
 [  0.  34.  10.   2.   3.]
 [  2.   9.  24.   1.   6.]
 [ 11.   1.   6.  54.   6.]
 [  9.  30.  32.  15. 151.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.512 | Acc: 92.750% | Wgt Acc: 97.666%
	I - Batch: 100 | Loss: 0.514 | Acc: 92.375% | Wgt Acc: 97.512%
	I - Batch: 150 | Loss: 0.515 | Acc: 92.125% | Wgt Acc: 97.506%
	I - Batch: 200 | Loss: 0.515 | Acc: 92.125% | Wgt Acc: 97.488%
I - num batch: 222
I - Train -- Loss: 0.516 | Acc: 92.078% | Wgt Acc: 97.422% | LR: 1.250000e-04 | Dur: 133.89s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   2.   0.  95.]
 [  0. 577.   1.   0.  47.]
 [  2.   0. 728.   0.  75.]
 [  0.   0.   0. 535.  49.]
 [  3.   1.   3.   3. 734.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.056 | Acc: 64.694% | Wgt Acc: 58.102% | Dur: 14.25s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   2.   3.  19.  10.]
 [  2.  36.   5.   2.   8.]
 [  0.   8.  34.   2.  12.]
 [  3.   3.   9.  43.   4.]
 [ 14.  29.  24.  20. 146.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.521 | Acc: 91.875% | Wgt Acc: 97.324%
	I - Batch: 100 | Loss: 0.519 | Acc: 92.188% | Wgt Acc: 97.393%
	I - Batch: 150 | Loss: 0.520 | Acc: 92.000% | Wgt Acc: 97.328%
	I - Batch: 200 | Loss: 0.521 | Acc: 91.969% | Wgt Acc: 97.346%
I - num batch: 222
I - Train -- Loss: 0.520 | Acc: 92.134% | Wgt Acc: 97.380% | LR: 1.250000e-04 | Dur: 132.21s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   2. 106.]
 [  0. 578.   0.   0.  39.]
 [  0.   0. 727.   1.  69.]
 [  1.   0.   0. 534.  48.]
 [  5.   0.   6.   1. 738.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.070 | Acc: 61.933% | Wgt Acc: 57.098% | Dur: 13.79s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   4.   5.  13.  13.]
 [  1.  35.   6.   2.   8.]
 [  3.  14.  34.   0.  17.]
 [ 13.   3.  13.  53.   8.]
 [ 13.  22.  17.  18. 134.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 0.512 | Acc: 93.125% | Wgt Acc: 97.840%
	I - Batch: 100 | Loss: 0.516 | Acc: 92.438% | Wgt Acc: 97.496%
	I - Batch: 150 | Loss: 0.514 | Acc: 93.083% | Wgt Acc: 97.833%
	I - Batch: 200 | Loss: 0.516 | Acc: 93.062% | Wgt Acc: 97.831%
I - num batch: 222
I - Train -- Loss: 0.518 | Acc: 92.754% | Wgt Acc: 97.745% | LR: 1.250000e-04 | Dur: 137.50s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   0.  91.]
 [  1. 578.   1.   1.  32.]
 [  0.   0. 731.   0.  70.]
 [  0.   0.   0. 535.  54.]
 [  3.   0.   1.   2. 753.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.037 | Acc: 65.680% | Wgt Acc: 61.654% | Dur: 15.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   2.  14.  10.]
 [  1.  39.   8.   1.   8.]
 [  0.  14.  36.   1.  18.]
 [ 13.   2.  10.  57.   7.]
 [ 10.  18.  19.  13. 137.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 0.514 | Acc: 92.250% | Wgt Acc: 97.176%
	I - Batch: 100 | Loss: 0.515 | Acc: 92.562% | Wgt Acc: 97.501%
	I - Batch: 150 | Loss: 0.516 | Acc: 92.375% | Wgt Acc: 97.422%
	I - Batch: 200 | Loss: 0.517 | Acc: 92.438% | Wgt Acc: 97.532%
I - num batch: 222
I - Train -- Loss: 0.517 | Acc: 92.529% | Wgt Acc: 97.601% | LR: 1.250000e-04 | Dur: 133.62s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   1.  86.]
 [  1. 576.   0.   0.  34.]
 [  1.   0. 731.   0.  68.]
 [  0.   0.   2. 536.  64.]
 [  4.   2.   1.   1. 748.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.050 | Acc: 64.497% | Wgt Acc: 60.547% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 74.   6.   3.  18.  15.]
 [  2.  37.   8.   2.  10.]
 [  1.  16.  40.   6.  16.]
 [  3.   4.   3.  43.   6.]
 [  8.  15.  21.  17. 133.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 0.511 | Acc: 92.375% | Wgt Acc: 97.398%
	I - Batch: 100 | Loss: 0.513 | Acc: 92.688% | Wgt Acc: 97.674%
	I - Batch: 150 | Loss: 0.515 | Acc: 92.375% | Wgt Acc: 97.633%
	I - Batch: 200 | Loss: 0.519 | Acc: 92.281% | Wgt Acc: 97.518%
I - num batch: 222
I - Train -- Loss: 0.519 | Acc: 92.247% | Wgt Acc: 97.461% | LR: 1.250000e-04 | Dur: 132.08s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   1.  75.]
 [  0. 575.   1.   0.  55.]
 [  1.   1. 732.   1.  76.]
 [  0.   1.   0. 534.  54.]
 [  5.   1.   1.   2. 740.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.065 | Acc: 64.103% | Wgt Acc: 59.440% | Dur: 13.91s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   5.   4.  17.   9.]
 [  2.  37.   5.   1.   9.]
 [  1.  15.  40.   2.  21.]
 [  5.   2.   6.  45.   5.]
 [ 13.  19.  20.  21. 136.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 0.518 | Acc: 92.750% | Wgt Acc: 97.622%
	I - Batch: 100 | Loss: 0.512 | Acc: 93.062% | Wgt Acc: 97.823%
	I - Batch: 150 | Loss: 0.519 | Acc: 92.667% | Wgt Acc: 97.414%
	I - Batch: 200 | Loss: 0.516 | Acc: 93.000% | Wgt Acc: 97.619%
I - num batch: 222
I - Train -- Loss: 0.516 | Acc: 92.952% | Wgt Acc: 97.653% | LR: 1.250000e-04 | Dur: 137.51s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   1.   1.   0.  84.]
 [  1. 575.   0.   1.  38.]
 [  0.   0. 730.   0.  59.]
 [  0.   0.   1. 534.  54.]
 [  3.   2.   2.   3. 765.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.037 | Acc: 65.483% | Wgt Acc: 59.797% | Dur: 14.59s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   3.   3.  17.  16.]
 [  0.  39.   6.   3.   5.]
 [  1.  10.  31.   1.   8.]
 [ 13.   3.  12.  51.   7.]
 [  7.  23.  23.  14. 144.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 0.514 | Acc: 92.375% | Wgt Acc: 97.800%
	I - Batch: 100 | Loss: 0.514 | Acc: 93.500% | Wgt Acc: 98.028%
	I - Batch: 150 | Loss: 0.513 | Acc: 93.708% | Wgt Acc: 98.026%
	I - Batch: 200 | Loss: 0.513 | Acc: 93.656% | Wgt Acc: 97.958%
I - num batch: 222
I - Train -- Loss: 0.514 | Acc: 93.459% | Wgt Acc: 97.931% | LR: 1.250000e-04 | Dur: 133.44s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   1.   1.  78.]
 [  0. 574.   0.   0.  35.]
 [  0.   0. 732.   0.  55.]
 [  0.   0.   0. 535.  54.]
 [  1.   4.   1.   2. 778.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.030 | Acc: 64.497% | Wgt Acc: 58.206% | Dur: 14.33s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   7.   3.  13.  15.]
 [  0.  32.   4.   1.   3.]
 [  1.  12.  34.   1.  11.]
 [  8.   5.   9.  52.   6.]
 [ 15.  22.  25.  19. 145.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 0.504 | Acc: 94.250% | Wgt Acc: 98.321%
	I - Batch: 100 | Loss: 0.511 | Acc: 93.500% | Wgt Acc: 97.801%
	I - Batch: 150 | Loss: 0.511 | Acc: 93.458% | Wgt Acc: 97.815%
	I - Batch: 200 | Loss: 0.513 | Acc: 93.406% | Wgt Acc: 97.780%
I - num batch: 222
I - Train -- Loss: 0.512 | Acc: 93.544% | Wgt Acc: 97.865% | LR: 1.250000e-04 | Dur: 135.44s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   1.   2.  71.]
 [  0. 578.   0.   2.  37.]
 [  1.   0. 731.   0.  59.]
 [  0.   0.   1. 531.  49.]
 [  2.   0.   1.   3. 784.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.032 | Acc: 66.667% | Wgt Acc: 62.023% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   2.   4.  18.  12.]
 [  0.  40.   6.   1.   8.]
 [  1.  13.  46.   7.  15.]
 [  7.   3.   3.  42.   5.]
 [ 10.  20.  16.  18. 140.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 0.512 | Acc: 93.250% | Wgt Acc: 98.025%
	I - Batch: 100 | Loss: 0.512 | Acc: 93.438% | Wgt Acc: 98.143%
	I - Batch: 150 | Loss: 0.512 | Acc: 93.458% | Wgt Acc: 98.088%
	I - Batch: 200 | Loss: 0.515 | Acc: 93.000% | Wgt Acc: 97.809%
I - num batch: 222
I - Train -- Loss: 0.516 | Acc: 92.839% | Wgt Acc: 97.710% | LR: 1.250000e-04 | Dur: 134.92s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   0.  86.]
 [  1. 578.   0.   1.  33.]
 [  0.   0. 732.   1.  77.]
 [  0.   0.   1. 534.  46.]
 [  5.   0.   1.   2. 758.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 62.919% | Wgt Acc: 59.013% | Dur: 14.43s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   5.   3.  12.  15.]
 [  1.  40.   8.   1.   6.]
 [  0.   8.  26.   0.  11.]
 [ 16.   6.  19.  58.  16.]
 [  8.  19.  19.  15. 132.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 0.510 | Acc: 93.375% | Wgt Acc: 97.694%
	I - Batch: 100 | Loss: 0.512 | Acc: 92.750% | Wgt Acc: 97.573%
	I - Batch: 150 | Loss: 0.514 | Acc: 92.750% | Wgt Acc: 97.640%
	I - Batch: 200 | Loss: 0.512 | Acc: 92.906% | Wgt Acc: 97.731%
I - num batch: 222
I - Train -- Loss: 0.513 | Acc: 93.093% | Wgt Acc: 97.782% | LR: 1.250000e-04 | Dur: 134.52s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   1.   1.  87.]
 [  1. 578.   0.   0.  39.]
 [  0.   0. 729.   0.  56.]
 [  0.   0.   1. 535.  51.]
 [  3.   0.   3.   2. 767.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.025 | Acc: 65.089% | Wgt Acc: 60.235% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   4.   3.  15.  16.]
 [  0.  43.   8.   0.   7.]
 [  1.   6.  23.   0.   9.]
 [ 11.   5.  12.  55.   8.]
 [  7.  20.  29.  16. 140.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 0.518 | Acc: 93.375% | Wgt Acc: 97.861%
	I - Batch: 100 | Loss: 0.517 | Acc: 93.000% | Wgt Acc: 97.706%
	I - Batch: 150 | Loss: 0.515 | Acc: 93.042% | Wgt Acc: 97.761%
	I - Batch: 200 | Loss: 0.514 | Acc: 93.156% | Wgt Acc: 97.853%
I - num batch: 222
I - Train -- Loss: 0.515 | Acc: 93.065% | Wgt Acc: 97.859% | LR: 1.250000e-04 | Dur: 141.07s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  68.]
 [  0. 577.   0.   1.  29.]
 [  0.   0. 732.   0.  72.]
 [  1.   0.   1. 536.  68.]
 [  3.   1.   1.   1. 763.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.038 | Acc: 65.680% | Wgt Acc: 60.385% | Dur: 14.93s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   4.  12.  16.]
 [  1.  37.   9.   1.   9.]
 [  0.  11.  31.   1.   5.]
 [ 14.   3.  12.  57.   7.]
 [  8.  23.  19.  15. 143.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 0.516 | Acc: 92.625% | Wgt Acc: 97.523%
	I - Batch: 100 | Loss: 0.511 | Acc: 93.188% | Wgt Acc: 97.794%
	I - Batch: 150 | Loss: 0.512 | Acc: 93.125% | Wgt Acc: 97.858%
	I - Batch: 200 | Loss: 0.514 | Acc: 92.812% | Wgt Acc: 97.704%
I - num batch: 222
I - Train -- Loss: 0.515 | Acc: 93.008% | Wgt Acc: 97.757% | LR: 1.250000e-04 | Dur: 132.37s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   0.  75.]
 [  1. 577.   0.   0.  52.]
 [  0.   0. 732.   0.  68.]
 [  0.   0.   0. 535.  41.]
 [  5.   1.   2.   3. 764.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.051 | Acc: 65.483% | Wgt Acc: 61.746% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   5.   3.  18.  14.]
 [  2.  46.   9.   2.   8.]
 [  0.   9.  35.   2.  11.]
 [ 12.   3.   9.  51.  12.]
 [  9.  15.  19.  13. 135.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 0.512 | Acc: 93.750% | Wgt Acc: 98.308%
	I - Batch: 100 | Loss: 0.509 | Acc: 94.062% | Wgt Acc: 98.211%
	I - Batch: 150 | Loss: 0.513 | Acc: 93.708% | Wgt Acc: 98.047%
	I - Batch: 200 | Loss: 0.514 | Acc: 93.656% | Wgt Acc: 98.020%
I - num batch: 222
I - Train -- Loss: 0.516 | Acc: 93.290% | Wgt Acc: 97.885% | LR: 1.250000e-04 | Dur: 133.38s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  82.]
 [  0. 576.   0.   0.  23.]
 [  0.   0. 733.   0.  74.]
 [  0.   0.   0. 534.  49.]
 [  3.   2.   1.   4. 772.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.052 | Acc: 65.089% | Wgt Acc: 60.604% | Dur: 14.31s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   3.   4.  19.  15.]
 [  2.  41.  12.   2.   8.]
 [  1.   9.  37.   3.  13.]
 [  5.   2.   7.  45.   7.]
 [ 10.  23.  15.  17. 137.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 0.509 | Acc: 93.750% | Wgt Acc: 98.189%
	I - Batch: 100 | Loss: 0.510 | Acc: 93.750% | Wgt Acc: 98.239%
	I - Batch: 150 | Loss: 0.513 | Acc: 93.208% | Wgt Acc: 97.983%
	I - Batch: 200 | Loss: 0.513 | Acc: 93.250% | Wgt Acc: 97.879%
I - num batch: 222
I - Train -- Loss: 0.512 | Acc: 93.290% | Wgt Acc: 97.888% | LR: 1.250000e-04 | Dur: 132.77s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  83.]
 [  0. 578.   0.   0.  32.]
 [  1.   0. 732.   1.  74.]
 [  2.   0.   0. 534.  39.]
 [  1.   0.   2.   2. 772.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 65.286% | Wgt Acc: 57.444% | Dur: 15.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   0.   2.  13.   6.]
 [  0.  38.   7.   1.   4.]
 [  1.  13.  32.   0.   8.]
 [ 12.   4.   5.  51.   8.]
 [ 19.  23.  29.  21. 154.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 0.512 | Acc: 93.375% | Wgt Acc: 98.247%
	I - Batch: 100 | Loss: 0.518 | Acc: 92.625% | Wgt Acc: 97.752%
	I - Batch: 150 | Loss: 0.515 | Acc: 92.833% | Wgt Acc: 97.801%
	I - Batch: 200 | Loss: 0.513 | Acc: 92.875% | Wgt Acc: 97.836%
I - num batch: 222
I - Train -- Loss: 0.514 | Acc: 92.839% | Wgt Acc: 97.793% | LR: 1.250000e-04 | Dur: 136.18s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   1.  74.]
 [  1. 578.   0.   0.  40.]
 [  0.   0. 734.   0.  71.]
 [  0.   0.   0. 534.  60.]
 [  4.   0.   0.   3. 755.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.039 | Acc: 64.300% | Wgt Acc: 60.097% | Dur: 14.15s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   6.   4.  17.  14.]
 [  0.  35.   9.   2.   4.]
 [  1.  15.  33.   1.  18.]
 [  6.   2.   7.  54.   9.]
 [ 12.  20.  22.  12. 135.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 126
I - Training: 
	I - Batch: 50 | Loss: 0.508 | Acc: 93.875% | Wgt Acc: 98.066%
	I - Batch: 100 | Loss: 0.505 | Acc: 93.562% | Wgt Acc: 98.002%
	I - Batch: 150 | Loss: 0.506 | Acc: 93.417% | Wgt Acc: 97.911%
	I - Batch: 200 | Loss: 0.506 | Acc: 93.875% | Wgt Acc: 98.037%
I - num batch: 222
I - Train -- Loss: 0.506 | Acc: 93.769% | Wgt Acc: 98.042% | LR: 1.250000e-04 | Dur: 136.70s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.  63.]
 [  0. 576.   0.   0.  31.]
 [  0.   0. 734.   1.  54.]
 [  0.   0.   0. 534.  64.]
 [  3.   2.   0.   2. 788.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.013 | Acc: 67.258% | Wgt Acc: 62.992% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   3.   2.  12.  14.]
 [  0.  39.   6.   2.   4.]
 [  0.   8.  32.   0.  11.]
 [ 12.   7.  13.  59.  10.]
 [  6.  21.  22.  13. 141.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 127
I - Training: 
	I - Batch: 50 | Loss: 0.510 | Acc: 93.750% | Wgt Acc: 98.148%
	I - Batch: 100 | Loss: 0.511 | Acc: 93.562% | Wgt Acc: 97.979%
	I - Batch: 150 | Loss: 0.511 | Acc: 93.125% | Wgt Acc: 97.911%
	I - Batch: 200 | Loss: 0.510 | Acc: 93.344% | Wgt Acc: 98.001%
I - num batch: 222
I - Train -- Loss: 0.510 | Acc: 93.346% | Wgt Acc: 97.967% | LR: 1.250000e-04 | Dur: 138.87s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   0.  92.]
 [  0. 578.   0.   0.  31.]
 [  0.   0. 734.   0.  64.]
 [  0.   0.   0. 537.  41.]
 [  7.   0.   0.   1. 772.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.058 | Acc: 65.483% | Wgt Acc: 60.916% | Dur: 15.53s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   2.   1.  12.  12.]
 [  0.  42.   8.   1.  10.]
 [  1.   6.  31.   0.  11.]
 [  9.   6.  14.  53.   8.]
 [ 11.  22.  21.  20. 139.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 128
I - Training: 
	I - Batch: 50 | Loss: 0.504 | Acc: 94.250% | Wgt Acc: 98.084%
	I - Batch: 100 | Loss: 0.509 | Acc: 93.312% | Wgt Acc: 97.794%
	I - Batch: 150 | Loss: 0.511 | Acc: 92.917% | Wgt Acc: 97.729%
	I - Batch: 200 | Loss: 0.509 | Acc: 93.125% | Wgt Acc: 97.849%
I - num batch: 222
I - Train -- Loss: 0.509 | Acc: 93.206% | Wgt Acc: 97.871% | LR: 1.250000e-04 | Dur: 134.96s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  83.]
 [  1. 578.   0.   0.  34.]
 [  0.   0. 731.   0.  59.]
 [  0.   0.   0. 536.  55.]
 [  4.   0.   3.   2. 769.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 64.103% | Wgt Acc: 62.507% | Dur: 14.01s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   4.  16.  14.]
 [  0.  39.   5.   1.   9.]
 [  0.  13.  47.   3.  23.]
 [ 10.   7.   8.  51.  12.]
 [ 12.  16.  11.  15. 122.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 129
I - Training: 
	I - Batch: 50 | Loss: 0.512 | Acc: 92.500% | Wgt Acc: 97.809%
	I - Batch: 100 | Loss: 0.512 | Acc: 93.312% | Wgt Acc: 98.003%
	I - Batch: 150 | Loss: 0.509 | Acc: 93.833% | Wgt Acc: 98.127%
	I - Batch: 200 | Loss: 0.513 | Acc: 93.125% | Wgt Acc: 97.760%
I - num batch: 222
I - Train -- Loss: 0.513 | Acc: 93.206% | Wgt Acc: 97.780% | LR: 1.250000e-04 | Dur: 138.27s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   2.  82.]
 [  0. 577.   1.   0.  41.]
 [  1.   0. 732.   2.  51.]
 [  0.   0.   0. 534.  54.]
 [  5.   1.   1.   0. 772.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.034 | Acc: 65.878% | Wgt Acc: 60.973% | Dur: 15.77s
I - Confusion Matrix: [row->prediction - col->label]
[[ 74.   6.   4.  15.  16.]
 [  0.  37.   7.   0.   8.]
 [  1.  10.  29.   0.  11.]
 [  6.   2.  10.  53.   4.]
 [  7.  23.  25.  18. 141.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 130
I - Training: 
	I - Batch: 50 | Loss: 0.500 | Acc: 93.750% | Wgt Acc: 97.953%
	I - Batch: 100 | Loss: 0.499 | Acc: 94.125% | Wgt Acc: 98.087%
	I - Batch: 150 | Loss: 0.502 | Acc: 94.000% | Wgt Acc: 98.159%
	I - Batch: 200 | Loss: 0.506 | Acc: 93.844% | Wgt Acc: 98.005%
I - num batch: 222
I - Train -- Loss: 0.506 | Acc: 93.798% | Wgt Acc: 98.023% | LR: 1.250000e-04 | Dur: 133.41s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   2.  76.]
 [  0. 578.   0.   0.  30.]
 [  0.   0. 733.   0.  50.]
 [  0.   0.   0. 533.  54.]
 [  4.   0.   1.   3. 790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.035 | Acc: 66.667% | Wgt Acc: 62.104% | Dur: 15.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   5.   4.  10.  10.]
 [  0.  41.  11.   0.   8.]
 [  1.  10.  39.   2.  11.]
 [ 14.   4.   9.  54.  10.]
 [ 10.  18.  12.  20. 141.]]

I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 131
I - Training: 
	I - Batch: 50 | Loss: 0.510 | Acc: 94.875% | Wgt Acc: 98.311%
	I - Batch: 100 | Loss: 0.511 | Acc: 94.125% | Wgt Acc: 98.196%
	I - Batch: 150 | Loss: 0.508 | Acc: 94.083% | Wgt Acc: 98.215%
	I - Batch: 200 | Loss: 0.508 | Acc: 93.812% | Wgt Acc: 98.192%
I - num batch: 222
I - Train -- Loss: 0.510 | Acc: 93.741% | Wgt Acc: 98.185% | LR: 1.250000e-04 | Dur: 136.54s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  84.]
 [  0. 578.   0.   1.  28.]
 [  0.   0. 733.   0.  57.]
 [  0.   0.   0. 537.  49.]
 [  2.   0.   1.   0. 782.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.050 | Acc: 65.286% | Wgt Acc: 60.512% | Dur: 16.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   3.   2.  10.  10.]
 [  1.  44.  12.   2.  12.]
 [  0.  10.  37.   2.  12.]
 [ 19.   4.  11.  55.   6.]
 [ 13.  17.  13.  17. 140.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 132
I - Training: 
	I - Batch: 50 | Loss: 0.513 | Acc: 93.875% | Wgt Acc: 98.229%
	I - Batch: 100 | Loss: 0.508 | Acc: 94.000% | Wgt Acc: 98.308%
	I - Batch: 150 | Loss: 0.510 | Acc: 94.000% | Wgt Acc: 98.158%
	I - Batch: 200 | Loss: 0.511 | Acc: 93.625% | Wgt Acc: 98.040%
I - num batch: 222
I - Train -- Loss: 0.513 | Acc: 93.628% | Wgt Acc: 98.010% | LR: 1.250000e-04 | Dur: 133.99s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  82.]
 [  0. 578.   0.   0.  36.]
 [  1.   0. 732.   1.  53.]
 [  0.   0.   0. 535.  46.]
 [  3.   0.   2.   1. 783.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.055 | Acc: 64.497% | Wgt Acc: 58.113% | Dur: 15.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 56.   4.   3.  13.   9.]
 [  0.  32.   3.   1.   1.]
 [  0.  11.  36.   1.  12.]
 [ 16.   3.  10.  57.  12.]
 [ 16.  28.  23.  14. 146.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 133
I - Training: 
	I - Batch: 50 | Loss: 0.508 | Acc: 93.375% | Wgt Acc: 97.957%
	I - Batch: 100 | Loss: 0.509 | Acc: 93.500% | Wgt Acc: 97.993%
	I - Batch: 150 | Loss: 0.513 | Acc: 93.000% | Wgt Acc: 97.816%
	I - Batch: 200 | Loss: 0.514 | Acc: 93.031% | Wgt Acc: 97.766%
I - num batch: 222
I - Train -- Loss: 0.514 | Acc: 93.093% | Wgt Acc: 97.811% | LR: 1.250000e-04 | Dur: 134.38s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   0.  95.]
 [  1. 577.   0.   1.  39.]
 [  0.   0. 734.   0.  62.]
 [  0.   0.   0. 536.  38.]
 [  7.   1.   0.   1. 766.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 63.116% | Wgt Acc: 57.467% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   4.   3.  17.  12.]
 [  1.  41.   8.   2.   8.]
 [  2.   7.  28.   2.  10.]
 [ 11.   4.  16.  51.  10.]
 [ 14.  22.  20.  14. 140.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 134
I - Training: 
	I - Batch: 50 | Loss: 0.501 | Acc: 94.500% | Wgt Acc: 98.102%
	I - Batch: 100 | Loss: 0.505 | Acc: 93.562% | Wgt Acc: 97.741%
	I - Batch: 150 | Loss: 0.507 | Acc: 93.750% | Wgt Acc: 97.936%
	I - Batch: 200 | Loss: 0.507 | Acc: 93.562% | Wgt Acc: 97.935%
I - num batch: 208
I - Train -- Loss: 0.507 | Acc: 93.690% | Wgt Acc: 97.981% | LR: 1.250000e-04 | Dur: 124.57s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   2.  59.]
 [  1. 577.   0.   0.  39.]
 [  0.   0. 733.   1.  53.]
 [  0.   0.   0. 533.  46.]
 [  5.   1.   1.   2. 584.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.025 | Acc: 67.258% | Wgt Acc: 62.553% | Dur: 14.21s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   4.   2.  10.   9.]
 [  2.  37.   6.   2.   6.]
 [  1.  13.  41.   1.  16.]
 [ 10.   2.   8.  59.   6.]
 [ 14.  22.  18.  14. 143.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 135
I - Training: 
	I - Batch: 50 | Loss: 0.510 | Acc: 94.000% | Wgt Acc: 97.830%
	I - Batch: 100 | Loss: 0.508 | Acc: 93.562% | Wgt Acc: 97.866%
	I - Batch: 150 | Loss: 0.506 | Acc: 93.583% | Wgt Acc: 98.021%
	I - Batch: 200 | Loss: 0.507 | Acc: 93.562% | Wgt Acc: 97.941%
I - num batch: 222
I - Train -- Loss: 0.509 | Acc: 93.572% | Wgt Acc: 97.859% | LR: 1.250000e-04 | Dur: 133.73s
I - Confusion Matrix: [row->prediction - col->label]
[[689.   0.   0.   0.  67.]
 [  1. 577.   0.   0.  35.]
 [  0.   0. 731.   1.  61.]
 [  1.   0.   0. 536.  51.]
 [  6.   1.   3.   1. 786.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.068 | Acc: 63.314% | Wgt Acc: 62.657% | Dur: 14.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 69.   7.   4.  16.  21.]
 [  1.  42.  10.   1.  10.]
 [  0.  13.  36.   3.  17.]
 [ 11.   8.  10.  57.  15.]
 [  7.   8.  15.   9. 117.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 136
I - Training: 
	I - Batch: 50 | Loss: 0.514 | Acc: 93.875% | Wgt Acc: 98.165%
	I - Batch: 100 | Loss: 0.504 | Acc: 94.625% | Wgt Acc: 98.415%
	I - Batch: 150 | Loss: 0.504 | Acc: 94.542% | Wgt Acc: 98.305%
	I - Batch: 200 | Loss: 0.505 | Acc: 94.438% | Wgt Acc: 98.236%
I - num batch: 222
I - Train -- Loss: 0.506 | Acc: 94.220% | Wgt Acc: 98.199% | LR: 1.250000e-04 | Dur: 132.84s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   1.   0.   1.  68.]
 [  0. 576.   0.   0.  29.]
 [  0.   0. 733.   0.  57.]
 [  0.   1.   0. 536.  43.]
 [  3.   0.   1.   1. 803.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.037 | Acc: 63.511% | Wgt Acc: 61.654% | Dur: 13.90s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   2.   1.   8.  10.]
 [  1.  44.   6.   4.  11.]
 [  0.  11.  45.   4.  23.]
 [ 18.   3.  11.  55.  13.]
 [ 14.  18.  12.  15. 123.]]

I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 137
I - Training: 
	I - Batch: 50 | Loss: 0.501 | Acc: 94.875% | Wgt Acc: 98.637%
	I - Batch: 100 | Loss: 0.506 | Acc: 94.312% | Wgt Acc: 98.273%
	I - Batch: 150 | Loss: 0.506 | Acc: 94.333% | Wgt Acc: 98.299%
	I - Batch: 200 | Loss: 0.507 | Acc: 93.875% | Wgt Acc: 98.113%
I - num batch: 222
I - Train -- Loss: 0.510 | Acc: 93.657% | Wgt Acc: 98.013% | LR: 1.250000e-04 | Dur: 132.70s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.  80.]
 [  0. 577.   0.   0.  23.]
 [  0.   0. 733.   1.  64.]
 [  0.   0.   0. 534.  49.]
 [  3.   1.   1.   2. 784.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.026 | Acc: 66.075% | Wgt Acc: 60.650% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   1.   2.   9.   9.]
 [  2.  42.  11.   2.  10.]
 [  1.   8.  34.   1.   8.]
 [ 10.   5.   8.  52.   9.]
 [ 12.  22.  20.  22. 144.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 138
I - Training: 
	I - Batch: 50 | Loss: 0.517 | Acc: 93.125% | Wgt Acc: 97.867%
	I - Batch: 100 | Loss: 0.511 | Acc: 93.375% | Wgt Acc: 97.930%
	I - Batch: 150 | Loss: 0.510 | Acc: 93.458% | Wgt Acc: 97.933%
	I - Batch: 200 | Loss: 0.509 | Acc: 93.594% | Wgt Acc: 98.005%
I - num batch: 222
I - Train -- Loss: 0.509 | Acc: 93.572% | Wgt Acc: 97.992% | LR: 1.250000e-04 | Dur: 132.88s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  80.]
 [  0. 578.   0.   1.  31.]
 [  1.   0. 731.   0.  56.]
 [  0.   0.   0. 534.  52.]
 [  1.   0.   3.   3. 781.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 66.075% | Wgt Acc: 60.258% | Dur: 15.58s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   2.  11.  13.]
 [  0.  30.   8.   0.   4.]
 [  1.  12.  34.   1.   8.]
 [ 10.   6.  10.  59.   9.]
 [ 11.  27.  21.  15. 146.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 139
I - Training: 
	I - Batch: 50 | Loss: 0.502 | Acc: 93.750% | Wgt Acc: 98.194%
	I - Batch: 100 | Loss: 0.505 | Acc: 93.438% | Wgt Acc: 98.099%
	I - Batch: 150 | Loss: 0.504 | Acc: 93.833% | Wgt Acc: 98.127%
	I - Batch: 200 | Loss: 0.507 | Acc: 93.500% | Wgt Acc: 97.941%
I - num batch: 222
I - Train -- Loss: 0.506 | Acc: 93.600% | Wgt Acc: 98.003% | LR: 1.250000e-04 | Dur: 135.29s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  94.]
 [  1. 578.   0.   1.  27.]
 [  0.   0. 733.   0.  56.]
 [  0.   0.   0. 535.  41.]
 [  4.   0.   1.   2. 782.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.038 | Acc: 64.694% | Wgt Acc: 58.540% | Dur: 15.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   1.   2.  16.  11.]
 [  0.  40.  10.   0.   4.]
 [  2.  10.  32.   1.   8.]
 [ 12.   4.  13.  51.  12.]
 [ 14.  23.  18.  18. 145.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 140
I - Training: 
	I - Batch: 50 | Loss: 0.503 | Acc: 94.250% | Wgt Acc: 98.046%
	I - Batch: 100 | Loss: 0.505 | Acc: 93.812% | Wgt Acc: 97.878%
	I - Batch: 150 | Loss: 0.507 | Acc: 93.667% | Wgt Acc: 97.942%
	I - Batch: 200 | Loss: 0.509 | Acc: 93.219% | Wgt Acc: 97.777%
I - num batch: 222
I - Train -- Loss: 0.509 | Acc: 93.346% | Wgt Acc: 97.788% | LR: 1.250000e-04 | Dur: 133.44s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   1.   0.  75.]
 [  0. 577.   0.   0.  33.]
 [  0.   0. 732.   2.  61.]
 [  1.   0.   0. 533.  53.]
 [  5.   1.   1.   3. 778.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.052 | Acc: 64.103% | Wgt Acc: 58.990% | Dur: 14.81s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   3.  13.  14.]
 [  1.  36.   7.   2.   5.]
 [  2.  15.  33.   1.  15.]
 [ 12.   2.   9.  53.   7.]
 [  9.  20.  23.  17. 139.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 141
I - Training: 
	I - Batch: 50 | Loss: 0.498 | Acc: 94.500% | Wgt Acc: 98.518%
	I - Batch: 100 | Loss: 0.502 | Acc: 94.188% | Wgt Acc: 98.345%
	I - Batch: 150 | Loss: 0.506 | Acc: 93.542% | Wgt Acc: 98.064%
	I - Batch: 200 | Loss: 0.505 | Acc: 93.875% | Wgt Acc: 98.175%
I - num batch: 222
I - Train -- Loss: 0.505 | Acc: 93.741% | Wgt Acc: 98.151% | LR: 1.250000e-04 | Dur: 133.47s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   1.  75.]
 [  0. 578.   0.   0.  36.]
 [  0.   0. 733.   0.  49.]
 [  0.   0.   0. 535.  57.]
 [  1.   0.   1.   2. 783.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.036 | Acc: 63.708% | Wgt Acc: 55.922% | Dur: 15.51s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   5.   1.  10.  10.]
 [  0.  28.   2.   0.   2.]
 [  0.   8.  28.   0.   7.]
 [ 10.   7.  14.  55.  10.]
 [ 17.  30.  30.  21. 151.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 142
I - Training: 
	I - Batch: 50 | Loss: 0.510 | Acc: 93.000% | Wgt Acc: 97.455%
	I - Batch: 100 | Loss: 0.509 | Acc: 93.438% | Wgt Acc: 97.824%
	I - Batch: 150 | Loss: 0.508 | Acc: 93.917% | Wgt Acc: 97.955%
	I - Batch: 200 | Loss: 0.506 | Acc: 93.906% | Wgt Acc: 98.052%
I - num batch: 222
I - Train -- Loss: 0.506 | Acc: 93.910% | Wgt Acc: 98.084% | LR: 1.250000e-04 | Dur: 132.58s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  62.]
 [  0. 578.   0.   0.  37.]
 [  1.   0. 733.   0.  59.]
 [  0.   0.   0. 534.  49.]
 [  3.   0.   1.   4. 793.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.029 | Acc: 67.456% | Wgt Acc: 62.772% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 67.   4.   6.  12.  12.]
 [  1.  37.   6.   1.   5.]
 [  0.  12.  38.   2.  13.]
 [  7.   4.  10.  57.   7.]
 [ 13.  21.  15.  14. 143.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 143
I - Training: 
	I - Batch: 50 | Loss: 0.505 | Acc: 93.875% | Wgt Acc: 98.050%
	I - Batch: 100 | Loss: 0.503 | Acc: 94.312% | Wgt Acc: 98.252%
	I - Batch: 150 | Loss: 0.504 | Acc: 94.250% | Wgt Acc: 98.301%
	I - Batch: 200 | Loss: 0.504 | Acc: 94.438% | Wgt Acc: 98.354%
I - num batch: 222
I - Train -- Loss: 0.503 | Acc: 94.474% | Wgt Acc: 98.381% | LR: 1.250000e-04 | Dur: 133.94s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  73.]
 [  0. 578.   0.   1.  16.]
 [  0.   0. 734.   0.  51.]
 [  1.   0.   0. 536.  52.]
 [  1.   0.   0.   1. 808.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.032 | Acc: 64.694% | Wgt Acc: 58.967% | Dur: 15.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   3.   2.  12.  12.]
 [  1.  39.   6.   2.   6.]
 [  0.  10.  34.   0.  13.]
 [ 11.   2.  12.  52.   6.]
 [ 16.  24.  21.  20. 143.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 144
I - Training: 
	I - Batch: 50 | Loss: 0.503 | Acc: 94.375% | Wgt Acc: 98.343%
	I - Batch: 100 | Loss: 0.508 | Acc: 93.438% | Wgt Acc: 98.003%
	I - Batch: 150 | Loss: 0.505 | Acc: 94.083% | Wgt Acc: 98.257%
	I - Batch: 200 | Loss: 0.506 | Acc: 93.938% | Wgt Acc: 98.157%
I - num batch: 222
I - Train -- Loss: 0.506 | Acc: 94.108% | Wgt Acc: 98.141% | LR: 1.250000e-04 | Dur: 133.57s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  72.]
 [  1. 576.   0.   1.  33.]
 [  0.   0. 733.   0.  55.]
 [  0.   0.   0. 536.  40.]
 [  3.   2.   1.   1. 800.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.055 | Acc: 66.667% | Wgt Acc: 62.969% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   4.   5.  15.  17.]
 [  0.  42.   6.   3.   6.]
 [  0.  12.  44.   3.  14.]
 [  5.   2.   5.  46.   7.]
 [ 13.  18.  15.  19. 136.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 145
I - Training: 
	I - Batch: 50 | Loss: 0.498 | Acc: 95.625% | Wgt Acc: 98.574%
	I - Batch: 100 | Loss: 0.506 | Acc: 94.375% | Wgt Acc: 98.146%
	I - Batch: 150 | Loss: 0.505 | Acc: 94.625% | Wgt Acc: 98.322%
	I - Batch: 200 | Loss: 0.506 | Acc: 94.594% | Wgt Acc: 98.213%
I - num batch: 222
I - Train -- Loss: 0.506 | Acc: 94.474% | Wgt Acc: 98.182% | LR: 1.250000e-04 | Dur: 134.49s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   1.  66.]
 [  0. 578.   0.   0.  32.]
 [  1.   0. 732.   1.  56.]
 [  1.   0.   0. 534.  31.]
 [  3.   0.   2.   2. 815.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.026 | Acc: 66.667% | Wgt Acc: 60.731% | Dur: 18.13s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   5.   2.  11.  14.]
 [  0.  38.  10.   1.   4.]
 [  1.   8.  29.   0.   6.]
 [ 11.   3.  11.  59.   8.]
 [ 12.  24.  23.  15. 148.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 146
I - Training: 
	I - Batch: 50 | Loss: 0.508 | Acc: 93.750% | Wgt Acc: 98.187%
	I - Batch: 100 | Loss: 0.511 | Acc: 93.875% | Wgt Acc: 98.163%
	I - Batch: 150 | Loss: 0.509 | Acc: 94.125% | Wgt Acc: 98.288%
	I - Batch: 200 | Loss: 0.509 | Acc: 94.094% | Wgt Acc: 98.208%
I - num batch: 222
I - Train -- Loss: 0.510 | Acc: 94.023% | Wgt Acc: 98.177% | LR: 1.250000e-04 | Dur: 133.86s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  76.]
 [  0. 577.   0.   1.  24.]
 [  0.   0. 733.   0.  55.]
 [  0.   1.   0. 537.  50.]
 [  4.   0.   1.   0. 795.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.038 | Acc: 65.680% | Wgt Acc: 59.993% | Dur: 14.16s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   5.   5.   7.  15.]
 [  0.  33.   6.   1.   5.]
 [  2.  13.  34.   1.   8.]
 [ 12.   4.  11.  60.   7.]
 [ 13.  23.  19.  17. 145.]]

I - Loading file: dataset_cls4_background12_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 147
I - Training: 
	I - Batch: 50 | Loss: 0.500 | Acc: 94.375% | Wgt Acc: 98.180%
	I - Batch: 100 | Loss: 0.504 | Acc: 93.938% | Wgt Acc: 98.155%
	I - Batch: 150 | Loss: 0.503 | Acc: 94.000% | Wgt Acc: 98.192%
	I - Batch: 200 | Loss: 0.504 | Acc: 94.031% | Wgt Acc: 98.251%
I - num batch: 222
I - Train -- Loss: 0.504 | Acc: 93.939% | Wgt Acc: 98.211% | LR: 1.250000e-04 | Dur: 134.26s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  73.]
 [  0. 578.   0.   0.  32.]
 [  0.   0. 733.   1.  62.]
 [  1.   0.   0. 537.  43.]
 [  2.   0.   1.   0. 790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.080 | Acc: 65.089% | Wgt Acc: 62.934% | Dur: 15.61s
I - Confusion Matrix: [row->prediction - col->label]
[[ 76.   8.   4.  22.  24.]
 [  2.  45.  11.   2.   7.]
 [  1.   9.  38.   4.  15.]
 [  3.   3.   8.  45.   8.]
 [  6.  13.  14.  13. 126.]]

I - Loading file: dataset_cls4_background13_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 148
I - Training: 
	I - Batch: 50 | Loss: 0.512 | Acc: 93.125% | Wgt Acc: 97.855%
	I - Batch: 100 | Loss: 0.511 | Acc: 93.750% | Wgt Acc: 97.769%
	I - Batch: 150 | Loss: 0.509 | Acc: 93.375% | Wgt Acc: 97.779%
	I - Batch: 200 | Loss: 0.507 | Acc: 93.688% | Wgt Acc: 97.906%
I - num batch: 222
I - Train -- Loss: 0.508 | Acc: 93.572% | Wgt Acc: 97.906% | LR: 1.250000e-04 | Dur: 135.47s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  71.]
 [  0. 576.   0.   0.  37.]
 [  0.   0. 733.   1.  54.]
 [  0.   0.   0. 534.  54.]
 [  5.   2.   1.   3. 784.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.043 | Acc: 65.878% | Wgt Acc: 61.769% | Dur: 14.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   4.   4.  17.   9.]
 [  1.  38.   6.   2.   9.]
 [  1.  14.  40.   3.  16.]
 [  7.   5.  10.  51.   9.]
 [ 11.  17.  15.  13. 137.]]

I - Loading file: dataset_cls4_background14_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 149
I - Training: 
	I - Batch: 50 | Loss: 0.504 | Acc: 94.375% | Wgt Acc: 98.395%
	I - Batch: 100 | Loss: 0.505 | Acc: 93.938% | Wgt Acc: 98.058%
	I - Batch: 150 | Loss: 0.507 | Acc: 94.208% | Wgt Acc: 98.129%
	I - Batch: 200 | Loss: 0.505 | Acc: 94.438% | Wgt Acc: 98.238%
I - num batch: 222
I - Train -- Loss: 0.505 | Acc: 94.390% | Wgt Acc: 98.220% | LR: 1.250000e-04 | Dur: 135.85s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   0.  78.]
 [  0. 578.   0.   0.  33.]
 [  0.   0. 733.   1.  40.]
 [  0.   0.   0. 536.  39.]
 [  6.   0.   1.   1. 810.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.047 | Acc: 65.089% | Wgt Acc: 57.825% | Dur: 14.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   0.   1.  10.   8.]
 [  2.  40.  11.   1.   5.]
 [  1.  11.  30.   4.  11.]
 [ 10.   2.   8.  51.   5.]
 [ 17.  25.  25.  20. 151.]]

I - Loading file: dataset_cls4_background15_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 150
I - Training: 
	I - Batch: 50 | Loss: 0.503 | Acc: 93.375% | Wgt Acc: 97.905%
	I - Batch: 100 | Loss: 0.500 | Acc: 94.062% | Wgt Acc: 98.265%
	I - Batch: 150 | Loss: 0.504 | Acc: 94.000% | Wgt Acc: 98.195%
	I - Batch: 200 | Loss: 0.504 | Acc: 93.812% | Wgt Acc: 98.124%
I - num batch: 222
I - Train -- Loss: 0.504 | Acc: 93.910% | Wgt Acc: 98.145% | LR: 1.250000e-04 | Dur: 133.80s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  78.]
 [  0. 578.   0.   0.  31.]
 [  1.   0. 734.   0.  63.]
 [  1.   0.   0. 536.  37.]
 [  3.   0.   0.   2. 791.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.063 | Acc: 65.089% | Wgt Acc: 59.416% | Dur: 16.27s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   5.   2.  12.  15.]
 [  0.  40.   9.   1.   8.]
 [  0.   6.  27.   0.   8.]
 [ 12.   5.   9.  57.   5.]
 [ 14.  22.  28.  16. 144.]]

I - Loading file: dataset_cls4_background16_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 151
I - Training: 
	I - Batch: 50 | Loss: 0.503 | Acc: 94.875% | Wgt Acc: 98.340%
	I - Batch: 100 | Loss: 0.504 | Acc: 94.625% | Wgt Acc: 98.256%
	I - Batch: 150 | Loss: 0.505 | Acc: 94.458% | Wgt Acc: 98.271%
	I - Batch: 200 | Loss: 0.503 | Acc: 94.438% | Wgt Acc: 98.290%
I - num batch: 222
I - Train -- Loss: 0.505 | Acc: 94.277% | Wgt Acc: 98.240% | LR: 1.250000e-04 | Dur: 133.81s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.  68.]
 [  0. 577.   0.   0.  30.]
 [  0.   0. 734.   1.  61.]
 [  0.   0.   0. 535.  37.]
 [  3.   1.   0.   1. 804.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.070 | Acc: 63.905% | Wgt Acc: 55.368% | Dur: 14.66s
I - Confusion Matrix: [row->prediction - col->label]
[[ 61.   6.   5.  16.  11.]
 [  0.  31.   5.   0.   3.]
 [  0.   7.  23.   0.   2.]
 [ 18.   8.  14.  54.   9.]
 [  9.  26.  28.  16. 155.]]

I - Loading file: dataset_cls4_background17_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 152
I - Training: 
	I - Batch: 50 | Loss: 0.505 | Acc: 95.000% | Wgt Acc: 98.600%
	I - Batch: 100 | Loss: 0.508 | Acc: 94.375% | Wgt Acc: 98.381%
	I - Batch: 150 | Loss: 0.509 | Acc: 94.042% | Wgt Acc: 98.228%
	I - Batch: 200 | Loss: 0.507 | Acc: 94.188% | Wgt Acc: 98.223%
I - num batch: 222
I - Train -- Loss: 0.508 | Acc: 94.023% | Wgt Acc: 98.170% | LR: 1.250000e-04 | Dur: 142.75s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  71.]
 [  1. 578.   0.   0.  32.]
 [  0.   0. 734.   1.  53.]
 [  0.   0.   0. 534.  49.]
 [  2.   0.   0.   3. 795.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 64.892% | Wgt Acc: 57.571% | Dur: 15.60s
I - Confusion Matrix: [row->prediction - col->label]
[[ 62.   3.   4.   9.   8.]
 [  2.  41.  10.   1.  13.]
 [  1.   5.  24.   0.   5.]
 [  7.   3.   7.  51.   3.]
 [ 16.  26.  30.  25. 151.]]

I - Loading file: dataset_cls4_background18_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 153
I - Training: 
	I - Batch: 50 | Loss: 0.502 | Acc: 95.250% | Wgt Acc: 98.315%
	I - Batch: 100 | Loss: 0.497 | Acc: 95.500% | Wgt Acc: 98.449%
	I - Batch: 150 | Loss: 0.498 | Acc: 95.292% | Wgt Acc: 98.510%
	I - Batch: 200 | Loss: 0.500 | Acc: 94.844% | Wgt Acc: 98.369%
I - num batch: 222
I - Train -- Loss: 0.501 | Acc: 94.700% | Wgt Acc: 98.328% | LR: 1.250000e-04 | Dur: 143.02s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   1.  50.]
 [  0. 577.   0.   1.  26.]
 [  1.   0. 734.   0.  54.]
 [  1.   0.   0. 535.  50.]
 [  2.   1.   0.   1. 820.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 64.300% | Wgt Acc: 57.479% | Dur: 14.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 55.   6.   2.  16.  10.]
 [  2.  46.  10.   3.   9.]
 [  0.   6.  25.   1.   6.]
 [ 15.   3.   9.  52.   7.]
 [ 16.  17.  29.  14. 148.]]

I - Loading file: dataset_cls4_background19_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 154
I - Training: 
	I - Batch: 50 | Loss: 0.504 | Acc: 94.125% | Wgt Acc: 98.157%
	I - Batch: 100 | Loss: 0.500 | Acc: 94.688% | Wgt Acc: 98.265%
	I - Batch: 150 | Loss: 0.505 | Acc: 94.000% | Wgt Acc: 98.118%
	I - Batch: 200 | Loss: 0.504 | Acc: 94.281% | Wgt Acc: 98.256%
I - num batch: 222
I - Train -- Loss: 0.505 | Acc: 94.333% | Wgt Acc: 98.285% | LR: 1.250000e-04 | Dur: 142.91s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.  71.]
 [  0. 578.   0.   0.  27.]
 [  1.   0. 734.   1.  57.]
 [  1.   0.   0. 535.  40.]
 [  1.   0.   0.   1. 805.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.047 | Acc: 66.469% | Wgt Acc: 60.108% | Dur: 18.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 64.   3.   2.  10.  12.]
 [  2.  37.   6.   2.   8.]
 [  1.   8.  34.   3.   7.]
 [ 10.   3.   9.  53.   4.]
 [ 11.  27.  24.  18. 149.]]

I - Loading file: dataset_cls4_background20_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 155
I - Training: 
	I - Batch: 50 | Loss: 0.503 | Acc: 93.875% | Wgt Acc: 98.116%
	I - Batch: 100 | Loss: 0.509 | Acc: 93.688% | Wgt Acc: 98.017%
	I - Batch: 150 | Loss: 0.507 | Acc: 93.625% | Wgt Acc: 97.962%
	I - Batch: 200 | Loss: 0.505 | Acc: 93.750% | Wgt Acc: 98.074%
I - num batch: 222
I - Train -- Loss: 0.505 | Acc: 93.910% | Wgt Acc: 98.142% | LR: 1.250000e-04 | Dur: 137.95s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  70.]
 [  1. 578.   0.   0.  29.]
 [  1.   0. 733.   0.  59.]
 [  0.   0.   0. 535.  51.]
 [  1.   0.   1.   3. 791.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.021 | Acc: 66.864% | Wgt Acc: 60.777% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   3.   4.  20.  10.]
 [  0.  35.   5.   0.   5.]
 [  0.  11.  37.   1.  12.]
 [ 10.   5.  10.  51.   5.]
 [ 10.  24.  19.  14. 148.]]

I - Loading file: dataset_cls4_background21_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 156
I - Training: 
	I - Batch: 50 | Loss: 0.502 | Acc: 93.875% | Wgt Acc: 98.064%
	I - Batch: 100 | Loss: 0.506 | Acc: 93.750% | Wgt Acc: 98.116%
	I - Batch: 150 | Loss: 0.508 | Acc: 94.000% | Wgt Acc: 98.154%
	I - Batch: 200 | Loss: 0.507 | Acc: 93.906% | Wgt Acc: 98.154%
I - num batch: 222
I - Train -- Loss: 0.509 | Acc: 93.769% | Wgt Acc: 97.995% | LR: 1.250000e-04 | Dur: 142.46s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   0.  94.]
 [  0. 577.   0.   0.  33.]
 [  1.   0. 732.   0.  41.]
 [  0.   0.   0. 536.  42.]
 [  5.   1.   2.   2. 790.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.045 | Acc: 65.286% | Wgt Acc: 62.057% | Dur: 17.10s
I - Confusion Matrix: [row->prediction - col->label]
[[ 75.   7.   4.  18.  25.]
 [  0.  34.   5.   0.   8.]
 [  0.  12.  35.   0.   9.]
 [  5.   7.  14.  55.   6.]
 [  8.  18.  17.  13. 132.]]

I - Loading file: dataset_cls4_background22_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 157
I - Training: 
	I - Batch: 50 | Loss: 0.512 | Acc: 93.000% | Wgt Acc: 97.863%
	I - Batch: 100 | Loss: 0.505 | Acc: 94.375% | Wgt Acc: 98.170%
	I - Batch: 150 | Loss: 0.508 | Acc: 94.000% | Wgt Acc: 98.076%
	I - Batch: 200 | Loss: 0.508 | Acc: 93.875% | Wgt Acc: 98.026%
I - num batch: 222
I - Train -- Loss: 0.507 | Acc: 94.080% | Wgt Acc: 98.081% | LR: 1.250000e-04 | Dur: 144.61s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   0.  69.]
 [  1. 578.   0.   1.  32.]
 [  0.   0. 732.   1.  57.]
 [  2.   0.   0. 536.  41.]
 [  4.   0.   2.   0. 801.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.027 | Acc: 67.850% | Wgt Acc: 60.224% | Dur: 14.24s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   3.  16.   7.]
 [  0.  36.   4.   0.   2.]
 [  0.  11.  35.   1.   7.]
 [  7.   3.  13.  50.   7.]
 [ 15.  25.  20.  19. 157.]]

I - Local maximum validation set accuracy:  67.85

I - Validation set results: 
[14-1-2-0.68][50-3-4-0.84][124-2-2-0.66][127-0-0-0.99][443-2-4-0.99][567-0-0-0.60][573-1-1-0.31][615-0-0-0.65][695-1-2-0.99][722-3-3-0.99]
[826-0-0-0.99][878-0-0-0.99][1103-0-4-0.78][1212-3-4-0.99][1368-0-0-0.99][2181-2-3-0.18][2476-2-2-0.99][2721-2-2-0.78][2818-1-3-0.39][2886-2-1-0.16]
[3231-2-2-0.99][3333-2-3-0.49][3482-2-2-0.87][3536-3-4-0.59][3625-1-1-0.32][3909-0-0-0.54][4035-0-0-0.99][4140-0-0-0.91][4214-1-1--0.02][4346-1-0-0.57]
[4581-2-1-0.32][4708-3-4-0.49][4838-3-4-0.92][4845-1-1-0.02][4868-0-0-0.99][4939-0-4--0.69][4984-2-3-0.99][5078-1-2-0.37][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.86][5843-1-1-0.39][5949-3-3-0.99][5987-2-4-0.99][6014-3-3-0.47][6033-3-4-0.78][6313-0-0-0.99][6421-3-3-0.98][6500-1-4-0.99][6583-3-3-0.99]
[6683-3-3-0.46][6825-2-1-0.99][6998-3-4--0.02][7049-3-3-0.97][7517-1-1-0.79][7521-1-1-0.99][7528-1-2--0.51][7949-1-2-0.97][8135-1-4-0.18][8185-3-0-0.99]
[8269-3-4-0.55][8273-3-3-0.90][8543-3-0-0.99][8666-1-1-0.95][8672-0-0-0.99][8903-1-2-0.99][9001-2-4-0.99][9036-2-2-0.47][9281-3-3-0.29][9300-2-2-0.99]
[9571-0-4-0.59][9617-1-4-0.63][9644-2-2-0.88][9705-2-4--0.16][9801-0-4-0.44][9803-3-3-0.73][9865-3-3-0.99][9896-2-2-0.96][10314-1-2--0.07][10337-3-3-0.99]
[10403-0-4-0.99][10653-2-4-0.99][10704-2-2-0.50][10719-1-1-0.74][10727-1-4-0.65][10836-0-0-0.99][10969-2-4-0.73][11042-0-0--0.07][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-2-0.99][11499-0-0-0.99][11502-3-0-0.50][11512-3-3-0.52][11608-1-1-0.28][11610-0-0-0.98][11692-0-3-0.73][11905-0-3-0.92][11993-1-1-0.90][12002-2-3-0.48]
[12052-0-0-0.99][12201-0-0-0.50][12235-2-4-0.99][12320-1-4-0.99][12377-2-4-0.86][12398-2-3--0.23][12503-1-4-0.90][12617-0-4--0.22][12685-3-4-0.95][12738-2-3-0.21]
[12742-2-2-0.99][12823-0-0-0.99][13110-1-1-0.62][13240-3-4-0.34][13253-1-1-0.11][13273-0-0-0.99][13634-1-4-0.98][13763-2-1--0.28][13905-3-3-0.78][14060-2-4-0.90]
[14065-3-0-0.65][14147-3-3-0.25][14595-2-2-0.84][14687-2-2-0.72][14788-2-2-0.57][14869-1-4-0.76][14872-3-4-0.72][14877-1-1-0.95][14927-0-0-0.34][15066-0-0-0.99]
[15175-1-4-0.98][15178-2-3-0.94][15375-3-0-0.99][15389-3-3-0.97][15568-2-4-0.96][15675-3-3-0.99][15869-1-0-0.07][16207-3-0-0.92][16236-0-0-0.97][16302-3-4-0.59]
[16331-2-2-0.99][16381-0-3-0.58][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.92][16801-0-0-0.99][16828-0-0-0.99][17137-3-3-0.58][17245-1-4-0.51]
[17278-3-0-0.32][17282-0-0-0.80][17311-2-2-0.69][17336-2-2--0.01][17608-3-3-0.99][17627-0-4-0.73][17877-3-0-0.57][17924-1-3-0.12][17984-3-3-0.83][18211-0-0-0.66]
[18276-3-3-0.99][18287-1-4-0.13][18394-0-0-0.99][18428-0-0--0.17][18442-0-3-0.92][18478-3-3-0.99][18607-0-0-0.99][18616-0-0-0.90][18663-0-0-0.99][18718-0-0-0.99]
[18766-2-2-0.99][18824-2-4-0.98][18890-3-3-0.99][18930-3-4-0.98][18938-3-3--0.13][19817-1-1-0.72][19839-0-0-0.39][19930-3-3-0.99][19944-0-4-0.38][20036-2-2-0.99]
[20101-3-3-0.39][20474-1-1-0.48][20547-3-3-0.96][20929-2-2-0.99][21245-1-2-0.24][21257-3-3-0.99][21293-1-2-0.94][21316-1-1-0.95][21384-1-4-0.96][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.53][21714-0-4-0.83][21943-3-3-0.38][21947-0-0-0.60][21948-0-0-0.99][21965-2-2-0.58][21998-1-1-0.98][22025-0-4-0.99][22228-3-3-0.98]
[22446-1-1-0.99][22494-3-0-0.65][22757-0-0-0.99][22811-3-3-0.99][22976-3-4-0.82][22985-3-0-0.62][23014-0-0-0.99][23112-1-1-0.99][23144-3-3-0.99][23168-2-3-0.75]
[23219-0-0-0.19][23363-3-3-0.96][23470-0-0-0.49][23486-2-2--0.13][23497-0-3-0.95][23516-0-0-0.87][23690-1-4-0.43][23921-2-2-0.85][23936-1-2-0.99][24040-3-0-0.59]
[24111-1-4-0.90][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.51][24345-0-0-0.25][24364-1-4-0.07][24427-3-3-0.99][24477-2-4-0.79][24495-2-4-0.53][24893-2-3-0.28]
[25012-1-4--0.31][25121-2-4-0.97][25165-3-3-0.96][25183-0-0-0.72][25297-3-3-0.99][25398-0-0-0.85][25574-2-2-0.40][25644-1-1-0.96][25718-1-4-0.96][25774-2-4--0.18]
[26032-3-3-0.97][26051-3-3-0.98][26120-0-4-0.99][26321-1-1-0.42][26732-1-1-0.49][26784-3-3-0.99][26827-3-3-0.54][26833-0-0-0.97][26838-2-2-0.05][26860-1-4-0.13]
[26948-0-0-0.99][27049-3-0-0.99][27098-1-0-0.28][27526-0-0-0.77][27639-3-3-0.99][27698-3-0-0.97][27772-0-0-0.95][27890-1-1-0.89][28040-0-3--0.01][28503-2-2-0.99]
[28577-1-2-0.50][28959-0-0-0.99][29198-3-4-0.99][29777-0-0-0.99][29877-2-2-0.58][30035-1-1-0.89][30098-0-3-0.61][30326-1-1-0.99][30572-2-3-0.01][30716-0-4-0.99]
[30806-2-2-0.10][30906-1-1-0.99][31007-0-0-0.47][31181-3-2--0.43][31238-0-0-0.90][31347-0-0-0.99][31422-2-2-0.86][31429-3-3-0.21][31431-0-0-0.90][31432-1-1-0.82]
[31477-0-0-0.97][31524-1-4--0.09][31597-1-4-0.62][31619-1-4-0.96][31701-0-0-0.45][31755-0-0-0.99][31854-3-3-0.97][32074-1-1-0.43][32078-3-3-0.85][32111-1-1-0.95]
[32127-1-1-0.70][32140-3-3-0.99][32263-2-4-0.61][32365-0-0-0.99][32411-2-0-0.99][32429-3-3-0.67][32473-3-3-0.59][32574-3-0-0.67][32584-0-4-0.65][32622-0-4-0.75]
[32858-3-0-0.90][32969-3-0-0.99][33016-2-2-0.84][33031-1-3-0.98][33035-2-4-0.92][33133-2-2-0.99][33173-2-3--0.15][33175-3-4-0.99][33306-3-3-0.99][33309-2-3-0.93]
[33474-0-0-0.31][33478-2-0--0.94][33618-1-4-0.98][33712-0-4-0.54][33782-2-4-0.99][33914-3-4-0.70][34076-3-4-0.97][34112-2-2-0.99][34138-2-3-0.97][34239-1-1-0.32]
[34364-2-2-0.99][34617-1-4-0.96][34751-3-3-0.65][34783-2-4-0.95][35015-3-4-0.50][35018-1-4-0.22][35288-2-4-0.77][0-4-4-0.97][1-4-4-0.99][2-4-4-0.99]
[3-4-4-0.98][4-4-4-0.99][5-4-3-0.23][6-4-4-0.88][7-4-4-0.51][8-4-4-0.82][9-4-4-0.98][10-4-4-0.99][11-4-4-0.99][12-4-4-0.99]
[14-4-4-0.99][15-4-3-0.86][16-4-4-0.83][17-4-4-0.88][18-4-4-0.99][19-4-4-0.16][20-4-4-0.67][21-4-2-0.82][22-4-4-0.99][23-4-4-0.68]
[24-4-4-0.99][25-4-4-0.99][26-4-4-0.62][27-4-4-0.99][28-4-4-0.99][29-4-4-0.92][30-4-3-0.27][31-4-4-0.98][32-4-4-0.99][33-4-4-0.81]
[34-4-4-0.99][35-4-4-0.99][37-4-4-0.99][39-4-0-0.98][40-4-4-0.99][41-4-4-0.90][42-4-4-0.50][43-4-4-0.99][45-4-4-0.99][46-4-4-0.99]
[47-4-4-0.99][48-4-4-0.99][51-4-4-0.99][52-4-4-0.65][53-4-4-0.99][54-4-4-0.99][55-4-4-0.99][56-4-1-0.71][57-4-4-0.53][58-4-4-0.34]
[59-4-0-0.97][60-4-4-0.99][61-4-4-0.99][62-4-4-0.60][63-4-4-0.98][64-4-2-0.96][65-4-4-0.99][66-4-4-0.99][67-4-4-0.70][68-4-4-0.59]
[69-4-4-0.49][70-4-4-0.73][72-4-4-0.78][73-4-1-0.95][74-4-4-0.94][75-4-3-0.74][77-4-4-0.99][78-4-4-0.99][79-4-4-0.99][80-4-4-0.99]
[81-4-4-0.92][82-4-4-0.75][83-4-4-0.83][84-4-4-0.99][85-4-4-0.99][86-4-4-0.99][87-4-4-0.99][88-4-4-0.99][89-4-3--0.43][90-4-4-0.91]
[91-4-4-0.99][92-4-4--0.18][93-4-3-0.18][94-4-4-0.98][95-4-4-0.99][96-4-4-0.99][97-4-4-0.99][98-4-4-0.99][99-4-4-0.64][100-4-4-0.55]
[101-4-4-0.99][102-4-4-0.82][103-4-4-0.90][104-4-4-0.94][105-4-4-0.57][106-4-4-0.99][107-4-4-0.99][108-4-4-0.69][109-4-4-0.99][110-4-4-0.55]
[111-4-0-0.86][112-4-4-0.96][113-4-4-0.80][114-4-4-0.95][115-4-4-0.91][116-4-4-0.95][117-4-4-0.99][119-4-4-0.99][121-4-4-0.99][122-4-4-0.99]
[124-4-4-0.96][125-4-4-0.98][126-4-4-0.99][127-4-2-0.77][128-4-0--0.03][129-4-4-0.99][130-4-4-0.82][131-4-4-0.67][132-4-4-0.93][133-4-4-0.99]
[135-4-4-0.99][136-4-4-0.80][137-4-4-0.99][138-4-4-0.99][139-4-4-0.99][140-4-4-0.82][141-4-4-0.55][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.99][148-4-0-0.99][149-4-4-0.99][150-4-4-0.99][151-4-4-0.99][152-4-4-0.97][153-4-4-0.99][154-4-4-0.99][155-4-4-0.99][156-4-4-0.54]
[157-4-4--0.07][158-4-4-0.99][160-4-4--0.21][161-4-2-0.81][162-4-2-0.66][164-4-4-0.99][165-4-4-0.99][167-4-4-0.99][168-4-4-0.79][170-4-4-0.99]
[171-4-4-0.99][172-4-4-0.99][173-4-4-0.99][174-4-0-0.99][175-4-4-0.99][177-4-4-0.99][178-4-4-0.09][179-4-4-0.99][180-4-4-0.99][181-4-4-0.68]
[182-4-3-0.99][183-4-4-0.99][184-4-4-0.99][186-4-4-0.70][187-4-4-0.62][188-4-4-0.99][189-4-0--0.31][190-4-4-0.71][191-4-4-0.98][192-4-4-0.99]
[193-4-2--0.06][194-4-4-0.65][195-4-4-0.48][196-4-4-0.99][197-4-4-0.99][198-4-4-0.99][199-4-2-0.96]
---------------------------
I - Loading file: dataset_cls4_background23_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 158
I - Training: 
	I - Batch: 50 | Loss: 0.503 | Acc: 94.375% | Wgt Acc: 98.441%
	I - Batch: 100 | Loss: 0.505 | Acc: 94.125% | Wgt Acc: 98.391%
	I - Batch: 150 | Loss: 0.503 | Acc: 94.417% | Wgt Acc: 98.390%
	I - Batch: 200 | Loss: 0.502 | Acc: 94.469% | Wgt Acc: 98.407%
I - num batch: 222
I - Train -- Loss: 0.502 | Acc: 94.390% | Wgt Acc: 98.389% | LR: 1.250000e-04 | Dur: 133.52s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  81.]
 [  0. 578.   0.   0.  28.]
 [  0.   0. 734.   1.  44.]
 [  1.   0.   0. 537.  43.]
 [  1.   0.   0.   0. 804.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.024 | Acc: 65.286% | Wgt Acc: 56.441% | Dur: 15.05s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   2.   3.  13.   6.]
 [  0.  35.   7.   1.   5.]
 [  0.   9.  23.   0.   4.]
 [ 12.   1.  12.  54.   6.]
 [ 16.  31.  30.  18. 159.]]

I - Loading file: dataset_cls4_background24_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 159
I - Training: 
	I - Batch: 50 | Loss: 0.497 | Acc: 95.500% | Wgt Acc: 98.778%
	I - Batch: 100 | Loss: 0.500 | Acc: 95.000% | Wgt Acc: 98.642%
	I - Batch: 150 | Loss: 0.498 | Acc: 95.250% | Wgt Acc: 98.711%
	I - Batch: 200 | Loss: 0.499 | Acc: 95.000% | Wgt Acc: 98.582%
I - num batch: 222
I - Train -- Loss: 0.499 | Acc: 94.869% | Wgt Acc: 98.547% | LR: 1.250000e-04 | Dur: 133.45s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   0.  69.]
 [  1. 578.   0.   1.  31.]
 [  0.   0. 734.   0.  44.]
 [  0.   0.   0. 537.  36.]
 [  0.   0.   0.   0. 820.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.055 | Acc: 64.892% | Wgt Acc: 57.248% | Dur: 14.34s
I - Confusion Matrix: [row->prediction - col->label]
[[ 70.   5.   2.  14.  14.]
 [  0.  29.   5.   0.   5.]
 [  0.  12.  26.   0.   4.]
 [  8.   2.  15.  52.   5.]
 [ 10.  30.  27.  20. 152.]]

I - Loading file: dataset_cls4_background25_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 160
I - Training: 
	I - Batch: 50 | Loss: 0.501 | Acc: 94.250% | Wgt Acc: 98.424%
	I - Batch: 100 | Loss: 0.502 | Acc: 94.812% | Wgt Acc: 98.534%
	I - Batch: 150 | Loss: 0.504 | Acc: 94.333% | Wgt Acc: 98.245%
	I - Batch: 200 | Loss: 0.503 | Acc: 94.250% | Wgt Acc: 98.219%
I - num batch: 222
I - Train -- Loss: 0.503 | Acc: 94.333% | Wgt Acc: 98.202% | LR: 1.250000e-04 | Dur: 133.92s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  74.]
 [  0. 576.   0.   0.  29.]
 [  1.   1. 733.   1.  50.]
 [  0.   0.   0. 536.  39.]
 [  3.   1.   1.   1. 808.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.014 | Acc: 66.864% | Wgt Acc: 61.965% | Dur: 14.22s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   3.  12.   8.]
 [  1.  36.   4.   2.   6.]
 [  2.  16.  38.   2.  15.]
 [ 14.   5.  17.  57.   8.]
 [  6.  17.  13.  13. 143.]]

I - Loading file: dataset_cls4_background26_no_samples781.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [697. 578. 734. 538. 781.]

I - Epoch: 161
I - Training: 
	I - Batch: 50 | Loss: 0.495 | Acc: 95.125% | Wgt Acc: 98.241%
	I - Batch: 100 | Loss: 0.493 | Acc: 95.250% | Wgt Acc: 98.524%
	I - Batch: 150 | Loss: 0.493 | Acc: 95.125% | Wgt Acc: 98.538%
	I - Batch: 200 | Loss: 0.494 | Acc: 94.812% | Wgt Acc: 98.471%
I - num batch: 208
I - Train -- Loss: 0.494 | Acc: 94.832% | Wgt Acc: 98.481% | LR: 1.250000e-04 | Dur: 125.25s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  55.]
 [  0. 578.   0.   2.  33.]
 [  1.   0. 734.   1.  44.]
 [  1.   0.   0. 535.  34.]
 [  1.   0.   0.   0. 615.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.022 | Acc: 65.680% | Wgt Acc: 59.001% | Dur: 14.38s
I - Confusion Matrix: [row->prediction - col->label]
[[ 58.   4.   4.  14.   9.]
 [  0.  35.   4.   0.   3.]
 [  0.   9.  39.   1.  12.]
 [ 15.   5.   7.  52.   7.]
 [ 15.  25.  21.  19. 149.]]

I - Loading file: dataset_cls4_background00_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 162
I - Training: 
	I - Batch: 50 | Loss: 0.497 | Acc: 94.375% | Wgt Acc: 98.213%
	I - Batch: 100 | Loss: 0.498 | Acc: 94.188% | Wgt Acc: 98.290%
	I - Batch: 150 | Loss: 0.498 | Acc: 94.417% | Wgt Acc: 98.358%
	I - Batch: 200 | Loss: 0.497 | Acc: 94.594% | Wgt Acc: 98.400%
I - num batch: 222
I - Train -- Loss: 0.497 | Acc: 94.615% | Wgt Acc: 98.418% | LR: 1.250000e-04 | Dur: 135.20s
I - Confusion Matrix: [row->prediction - col->label]
[[696.   0.   0.   0.  65.]
 [  0. 577.   0.   0.  31.]
 [  0.   0. 734.   0.  44.]
 [  0.   0.   0. 536.  47.]
 [  1.   1.   0.   2. 813.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.055 | Acc: 64.892% | Wgt Acc: 59.416% | Dur: 14.04s
I - Confusion Matrix: [row->prediction - col->label]
[[ 66.   3.   2.  16.  14.]
 [  5.  47.  12.   2.  10.]
 [  0.   7.  27.   1.   9.]
 [  7.   3.  10.  47.   5.]
 [ 10.  18.  24.  20. 142.]]

I - Loading file: dataset_cls4_background01_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 163
I - Training: 
	I - Batch: 50 | Loss: 0.496 | Acc: 94.875% | Wgt Acc: 98.495%
	I - Batch: 100 | Loss: 0.497 | Acc: 94.812% | Wgt Acc: 98.417%
	I - Batch: 150 | Loss: 0.499 | Acc: 94.667% | Wgt Acc: 98.391%
	I - Batch: 200 | Loss: 0.498 | Acc: 95.062% | Wgt Acc: 98.533%
I - num batch: 222
I - Train -- Loss: 0.499 | Acc: 95.010% | Wgt Acc: 98.502% | LR: 1.250000e-04 | Dur: 131.64s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  62.]
 [  1. 578.   0.   0.  30.]
 [  0.   0. 733.   0.  45.]
 [  1.   0.   0. 537.  35.]
 [  1.   0.   1.   1. 828.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.023 | Acc: 68.047% | Wgt Acc: 62.588% | Dur: 13.88s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   5.   4.  19.  11.]
 [  1.  44.   8.   2.   9.]
 [  1.   7.  36.   2.   9.]
 [  7.   2.   9.  47.   4.]
 [  8.  20.  18.  16. 147.]]

I - Local maximum validation set accuracy:  68.05

I - Validation set results: 
[14-1-2-0.37][50-3-4--0.42][124-2-3--0.49][127-0-0-0.99][443-2-4-0.59][567-0-0-0.97][573-1-1-0.21][615-0-0-0.99][695-1-2-0.99][722-3-3-0.95]
[826-0-0-0.99][878-0-0-0.99][1103-0-4-0.71][1212-3-4-0.62][1368-0-0-0.99][2181-2-3-0.01][2476-2-2-0.98][2721-2-2-0.63][2818-1-1-0.78][2886-2-1-0.60]
[3231-2-2-0.99][3333-2-1-0.16][3482-2-2-0.76][3536-3-2--0.06][3625-1-1-0.72][3909-0-0-0.89][4035-0-0-0.99][4140-0-0-0.99][4214-1-1-0.03][4346-1-0-0.81]
[4581-2-1-0.99][4708-3-4--0.22][4838-3-4-0.02][4845-1-1-0.36][4868-0-0-0.99][4939-0-0--0.31][4984-2-3-0.89][5078-1-4--0.31][5396-0-0-0.99][5479-1-1-0.99]
[5717-0-0-0.97][5843-1-1-0.62][5949-3-0--0.42][5987-2-4-0.90][6014-3-3-0.99][6033-3-3--0.50][6313-0-0-0.99][6421-3-3-0.16][6500-1-4-0.61][6583-3-3-0.99]
[6683-3-3-0.31][6825-2-1-0.99][6998-3-0--0.30][7049-3-3-0.38][7517-1-1-0.93][7521-1-1-0.38][7528-1-4--0.44][7949-1-4--0.07][8135-1-0-0.22][8185-3-0-0.99]
[8269-3-1-0.99][8273-3-3-0.99][8543-3-0-0.99][8666-1-1-0.99][8672-0-0-0.99][8903-1-1-0.99][9001-2-4-0.97][9036-2-2-0.66][9281-3-1-0.34][9300-2-2-0.99]
[9571-0-4-0.08][9617-1-4-0.58][9644-2-2-0.24][9705-2-4-0.51][9801-0-3-0.58][9803-3-0-0.37][9865-3-3-0.95][9896-2-2-0.97][10314-1-4-0.28][10337-3-3-0.73]
[10403-0-4-0.99][10653-2-4-0.75][10704-2-2-0.08][10719-1-1-0.99][10727-1-4-0.85][10836-0-0-0.99][10969-2-4-0.19][11042-0-0-0.11][11088-1-1-0.99][11322-0-0-0.99]
[11398-2-4-0.35][11499-0-0-0.99][11502-3-0--0.38][11512-3-3-0.78][11608-1-2-0.33][11610-0-3-0.27][11692-0-0-0.35][11905-0-0-0.97][11993-1-1-0.84][12002-2-2--0.02]
[12052-0-0-0.99][12201-0-0-0.56][12235-2-2-0.73][12320-1-4-0.85][12377-2-4-0.72][12398-2-3-0.81][12503-1-4-0.52][12617-0-2-0.70][12685-3-4-0.89][12738-2-3-0.63]
[12742-2-2-0.99][12823-0-0-0.99][13110-1-1-0.83][13240-3-0-0.33][13253-1-4-0.05][13273-0-0-0.99][13634-1-4-0.68][13763-2-1--0.54][13905-3-3-0.12][14060-2-4-0.46]
[14065-3-0-0.95][14147-3-0-0.62][14595-2-2-0.93][14687-2-2-0.48][14788-2-2-0.31][14869-1-1-0.85][14872-3-4-0.74][14877-1-1-0.99][14927-0-3-0.53][15066-0-0-0.99]
[15175-1-4-0.68][15178-2-3-0.42][15375-3-0-0.16][15389-3-3-0.96][15568-2-4-0.61][15675-3-3-0.99][15869-1-0-0.99][16207-3-0-0.97][16236-0-0-0.99][16302-3-4-0.32]
[16331-2-2-0.99][16381-0-0-0.67][16488-1-1-0.99][16495-0-0-0.99][16650-0-0-0.99][16719-1-4-0.74][16801-0-0-0.99][16828-0-0-0.99][17137-3-3-0.66][17245-1-4-0.50]
[17278-3-0-0.19][17282-0-0-0.97][17311-2-2-0.87][17336-2-2--0.38][17608-3-3-0.99][17627-0-0--0.18][17877-3-4-0.99][17924-1-3--0.14][17984-3-3-0.99][18211-0-0-0.05]
[18276-3-3-0.54][18287-1-1-0.74][18394-0-0-0.99][18428-0-0-0.21][18442-0-0-0.74][18478-3-3-0.94][18607-0-0-0.99][18616-0-0-0.99][18663-0-0-0.93][18718-0-0-0.99]
[18766-2-2-0.99][18824-2-4-0.78][18890-3-3--0.13][18930-3-4-0.95][18938-3-3--0.20][19817-1-1-0.80][19839-0-0-0.52][19930-3-3-0.15][19944-0-4-0.57][20036-2-2-0.99]
[20101-3-3-0.47][20474-1-1-0.97][20547-3-4-0.60][20929-2-2-0.99][21245-1-1-0.20][21257-3-3-0.98][21293-1-2-0.76][21316-1-1-0.99][21384-1-1-0.99][21448-1-1-0.99]
[21483-0-0-0.99][21487-2-2-0.99][21714-0-3--0.42][21943-3-4-0.99][21947-0-0-0.99][21948-0-0-0.99][21965-2-2-0.58][21998-1-1-0.99][22025-0-4-0.99][22228-3-3-0.95]
[22446-1-1-0.99][22494-3-3-0.00][22757-0-0-0.99][22811-3-3-0.99][22976-3-4-0.66][22985-3-0-0.72][23014-0-0-0.99][23112-1-1-0.99][23144-3-3-0.99][23168-2-0--0.12]
[23219-0-0-0.58][23363-3-3-0.99][23470-0-0-0.83][23486-2-4-0.71][23497-0-3-0.99][23516-0-0-0.99][23690-1-2-0.99][23921-2-1-0.09][23936-1-0-0.95][24040-3-0-0.24]
[24111-1-4-0.97][24182-0-0-0.99][24238-3-3-0.99][24290-2-0-0.99][24345-0-0-0.42][24364-1-1-0.05][24427-3-3-0.89][24477-2-2-0.02][24495-2-4-0.75][24893-2-1-0.43]
[25012-1-4--0.21][25121-2-4-0.53][25165-3-3-0.99][25183-0-0-0.84][25297-3-3-0.88][25398-0-0-0.99][25574-2-2-0.97][25644-1-2-0.97][25718-1-4-0.99][25774-2-0--0.29]
[26032-3-3-0.99][26051-3-3-0.90][26120-0-4-0.90][26321-1-1-0.75][26732-1-1-0.62][26784-3-3-0.99][26827-3-3-0.21][26833-0-0-0.53][26838-2-4--0.07][26860-1-4-0.11]
[26948-0-0-0.99][27049-3-0-0.99][27098-1-1-0.63][27526-0-0-0.83][27639-3-3-0.99][27698-3-0-0.70][27772-0-0-0.99][27890-1-1-0.82][28040-0-3-0.08][28503-2-2-0.99]
[28577-1-1-0.23][28959-0-0-0.99][29198-3-4-0.99][29777-0-0-0.99][29877-2-2-0.34][30035-1-1-0.99][30098-0-0-0.95][30326-1-1-0.99][30572-2-2-0.97][30716-0-4-0.99]
[30806-2-2-0.66][30906-1-1-0.99][31007-0-0-0.92][31181-3-2--0.58][31238-0-0-0.99][31347-0-0-0.99][31422-2-2-0.87][31429-3-3-0.36][31431-0-0-0.49][31432-1-1-0.99]
[31477-0-3-0.45][31524-1-2--0.06][31597-1-4-0.53][31619-1-0-0.27][31701-0-0-0.99][31755-0-0-0.99][31854-3-3-0.88][32074-1-1-0.40][32078-3-3-0.78][32111-1-1-0.98]
[32127-1-1-0.99][32140-3-3-0.99][32263-2-3--0.49][32365-0-0-0.99][32411-2-0-0.99][32429-3-3-0.99][32473-3-4--0.42][32574-3-3-0.77][32584-0-4-0.21][32622-0-1--0.12]
[32858-3-0-0.98][32969-3-0-0.99][33016-2-2-0.91][33031-1-3-0.99][33035-2-2-0.04][33133-2-2-0.99][33173-2-4-0.35][33175-3-4-0.99][33306-3-3-0.99][33309-2-3-0.51]
[33474-0-0-0.25][33478-2-1-0.57][33618-1-1-0.44][33712-0-0-0.68][33782-2-4-0.94][33914-3-3-0.99][34076-3-4-0.72][34112-2-2-0.99][34138-2-3--0.16][34239-1-1-0.47]
[34364-2-2-0.69][34617-1-4-0.60][34751-3-3-0.72][34783-2-4-0.32][35015-3-0--0.67][35018-1-1-0.80][35288-2-2-0.16][0-4-4-0.79][1-4-4-0.97][2-4-4-0.99]
[3-4-4-0.99][4-4-4--0.21][5-4-1-0.45][6-4-4-0.99][7-4-4-0.40][8-4-4-0.66][9-4-4-0.91][10-4-4-0.99][11-4-4-0.99][12-4-4-0.92]
[14-4-4-0.99][15-4-3-0.99][16-4-4-0.52][17-4-4-0.98][18-4-4-0.99][19-4-0-0.14][20-4-4-0.01][21-4-2-0.75][22-4-4-0.99][23-4-4-0.04]
[24-4-4-0.99][25-4-4-0.96][26-4-4-0.28][27-4-4-0.99][28-4-4-0.99][29-4-1-0.58][30-4-4-0.74][31-4-4-0.99][32-4-4-0.99][33-4-2--0.11]
[34-4-4-0.94][35-4-4-0.99][37-4-4-0.98][39-4-0-0.99][40-4-4-0.25][41-4-4-0.26][42-4-4-0.23][43-4-4-0.98][45-4-4-0.93][46-4-4-0.95]
[47-4-4-0.99][48-4-4-0.91][51-4-4-0.99][52-4-4-0.57][53-4-4-0.79][54-4-4-0.99][55-4-4-0.72][56-4-1-0.05][57-4-0-0.99][58-4-4-0.84]
[59-4-0-0.99][60-4-4-0.95][61-4-4-0.99][62-4-4-0.92][63-4-4-0.98][64-4-4-0.28][65-4-4-0.99][66-4-4-0.99][67-4-1-0.49][68-4-4-0.80]
[69-4-4--0.09][70-4-4-0.93][72-4-4-0.94][73-4-1-0.97][74-4-4-0.74][75-4-3--0.26][77-4-4-0.99][78-4-4-0.23][79-4-4-0.99][80-4-4-0.99]
[81-4-4-0.99][82-4-1-0.08][83-4-4-0.70][84-4-4-0.99][85-4-4-0.93][86-4-4-0.77][87-4-4-0.99][88-4-4-0.99][89-4-4-0.60][90-4-4-0.37]
[91-4-4-0.98][92-4-2--0.24][93-4-3-0.39][94-4-4-0.99][95-4-4-0.97][96-4-4-0.95][97-4-4-0.89][98-4-4-0.53][99-4-4-0.97][100-4-4-0.58]
[101-4-4-0.99][102-4-4-0.85][103-4-4-0.21][104-4-4-0.97][105-4-4-0.99][106-4-4-0.99][107-4-4-0.96][108-4-4-0.86][109-4-4-0.99][110-4-4-0.64]
[111-4-0-0.99][112-4-4-0.82][113-4-4--0.39][114-4-4-0.78][115-4-4-0.95][116-4-4-0.80][117-4-4-0.99][119-4-4-0.99][121-4-1-0.01][122-4-4-0.99]
[124-4-4-0.77][125-4-4-0.99][126-4-4-0.99][127-4-2--0.23][128-4-4-0.56][129-4-4-0.98][130-4-4-0.90][131-4-4--0.32][132-4-2-0.17][133-4-4-0.99]
[135-4-4-0.55][136-4-1-0.53][137-4-4-0.96][138-4-4-0.99][139-4-4-0.99][140-4-4-0.92][141-4-0--0.01][142-4-4-0.99][143-4-4-0.99][144-4-4-0.99]
[145-4-4-0.99][148-4-0-0.99][149-4-4-0.99][150-4-4-0.99][151-4-4-0.99][152-4-4-0.90][153-4-4-0.99][154-4-4-0.99][155-4-4-0.99][156-4-4-0.25]
[157-4-0-0.99][158-4-4-0.96][160-4-4--0.22][161-4-2--0.23][162-4-2--0.04][164-4-4-0.57][165-4-4-0.98][167-4-0-0.99][168-4-4-0.89][170-4-4-0.98]
[171-4-4-0.66][172-4-4-0.99][173-4-4-0.99][174-4-0-0.45][175-4-4-0.85][177-4-4-0.88][178-4-4--0.13][179-4-4-0.91][180-4-4-0.99][181-4-4-0.88]
[182-4-3-0.99][183-4-4-0.99][184-4-4-0.93][186-4-4-0.88][187-4-2--0.02][188-4-4-0.97][189-4-4-0.06][190-4-4--0.22][191-4-4-0.98][192-4-4-0.97]
[193-4-1-0.44][194-4-0-0.25][195-4-4-0.69][196-4-4-0.81][197-4-4-0.99][198-4-4-0.99][199-4-2-0.70]
---------------------------
I - Loading file: dataset_cls4_background02_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 164
I - Training: 
	I - Batch: 50 | Loss: 0.500 | Acc: 94.000% | Wgt Acc: 97.999%
	I - Batch: 100 | Loss: 0.500 | Acc: 94.438% | Wgt Acc: 98.240%
	I - Batch: 150 | Loss: 0.501 | Acc: 94.625% | Wgt Acc: 98.321%
	I - Batch: 200 | Loss: 0.500 | Acc: 94.812% | Wgt Acc: 98.399%
I - num batch: 222
I - Train -- Loss: 0.501 | Acc: 94.643% | Wgt Acc: 98.374% | LR: 1.250000e-04 | Dur: 134.91s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  67.]
 [  0. 577.   0.   0.  29.]
 [  0.   0. 733.   1.  56.]
 [  0.   0.   0. 537.  32.]
 [  3.   1.   1.   0. 816.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.039 | Acc: 66.469% | Wgt Acc: 61.158% | Dur: 15.92s
I - Confusion Matrix: [row->prediction - col->label]
[[ 59.   3.   2.  11.   9.]
 [  0.  40.   5.   2.   4.]
 [  6.  16.  42.   3.  15.]
 [  7.   2.   6.  52.   8.]
 [ 16.  17.  20.  18. 144.]]

I - Loading file: dataset_cls4_background03_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 165
I - Training: 
	I - Batch: 50 | Loss: 0.500 | Acc: 94.500% | Wgt Acc: 98.352%
	I - Batch: 100 | Loss: 0.499 | Acc: 94.438% | Wgt Acc: 98.429%
	I - Batch: 150 | Loss: 0.496 | Acc: 94.625% | Wgt Acc: 98.511%
	I - Batch: 200 | Loss: 0.497 | Acc: 94.750% | Wgt Acc: 98.486%
I - num batch: 222
I - Train -- Loss: 0.498 | Acc: 94.841% | Wgt Acc: 98.456% | LR: 1.250000e-04 | Dur: 136.20s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  63.]
 [  0. 578.   0.   0.  30.]
 [  1.   0. 733.   0.  58.]
 [  0.   0.   0. 537.  27.]
 [  2.   0.   1.   1. 822.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.008 | Acc: 67.456% | Wgt Acc: 61.169% | Dur: 14.55s
I - Confusion Matrix: [row->prediction - col->label]
[[ 71.   7.   3.  16.  14.]
 [  1.  38.   6.   3.   3.]
 [  1.   7.  33.   2.   8.]
 [  4.   4.  12.  50.   5.]
 [ 11.  22.  21.  15. 150.]]

I - Loading file: dataset_cls4_background04_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 166
I - Training: 
	I - Batch: 50 | Loss: 0.498 | Acc: 93.750% | Wgt Acc: 98.343%
	I - Batch: 100 | Loss: 0.505 | Acc: 93.312% | Wgt Acc: 97.983%
	I - Batch: 150 | Loss: 0.504 | Acc: 93.750% | Wgt Acc: 97.918%
	I - Batch: 200 | Loss: 0.503 | Acc: 94.094% | Wgt Acc: 98.079%
I - num batch: 222
I - Train -- Loss: 0.503 | Acc: 94.080% | Wgt Acc: 98.105% | LR: 1.250000e-04 | Dur: 134.40s
I - Confusion Matrix: [row->prediction - col->label]
[[691.   0.   0.   1.  79.]
 [  1. 578.   0.   0.  30.]
 [  0.   0. 733.   1.  46.]
 [  1.   0.   0. 535.  45.]
 [  4.   0.   1.   1. 800.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.031 | Acc: 65.878% | Wgt Acc: 60.397% | Dur: 16.26s
I - Confusion Matrix: [row->prediction - col->label]
[[ 60.   0.   3.   9.   7.]
 [  0.  37.   4.   2.   4.]
 [  1.  17.  38.   3.  19.]
 [ 13.   2.   9.  55.   6.]
 [ 14.  22.  21.  17. 144.]]

I - Loading file: dataset_cls4_background05_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 167
I - Training: 
	I - Batch: 50 | Loss: 0.508 | Acc: 94.375% | Wgt Acc: 98.077%
	I - Batch: 100 | Loss: 0.501 | Acc: 95.125% | Wgt Acc: 98.416%
	I - Batch: 150 | Loss: 0.501 | Acc: 94.667% | Wgt Acc: 98.256%
	I - Batch: 200 | Loss: 0.501 | Acc: 94.719% | Wgt Acc: 98.283%
I - num batch: 222
I - Train -- Loss: 0.500 | Acc: 94.728% | Wgt Acc: 98.314% | LR: 1.250000e-04 | Dur: 131.66s
I - Confusion Matrix: [row->prediction - col->label]
[[692.   0.   0.   0.  63.]
 [  1. 577.   0.   0.  27.]
 [  0.   0. 732.   0.  58.]
 [  0.   0.   0. 537.  30.]
 [  4.   1.   2.   1. 822.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.036 | Acc: 66.075% | Wgt Acc: 59.290% | Dur: 14.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   1.   4.  13.   8.]
 [  0.  38.   8.   1.   6.]
 [  1.  18.  33.   3.   9.]
 [  7.   4.   9.  49.   7.]
 [ 15.  17.  21.  20. 150.]]

I - Loading file: dataset_cls4_background06_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 168
I - Training: 
	I - Batch: 50 | Loss: 0.503 | Acc: 94.875% | Wgt Acc: 98.435%
	I - Batch: 100 | Loss: 0.496 | Acc: 95.062% | Wgt Acc: 98.532%
	I - Batch: 150 | Loss: 0.496 | Acc: 94.917% | Wgt Acc: 98.420%
	I - Batch: 200 | Loss: 0.498 | Acc: 94.594% | Wgt Acc: 98.378%
I - num batch: 222
I - Train -- Loss: 0.499 | Acc: 94.587% | Wgt Acc: 98.332% | LR: 1.250000e-04 | Dur: 134.64s
I - Confusion Matrix: [row->prediction - col->label]
[[693.   0.   0.   0.  72.]
 [  1. 578.   0.   1.  28.]
 [  0.   0. 732.   0.  47.]
 [  2.   0.   0. 537.  38.]
 [  1.   0.   2.   0. 815.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.076 | Acc: 63.511% | Wgt Acc: 54.446% | Dur: 16.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.   4.   6.  23.  13.]
 [  1.  33.   2.   1.   4.]
 [  0.  12.  25.   3.   5.]
 [  1.   1.   4.  36.   3.]
 [ 13.  28.  38.  23. 155.]]

I - Loading file: dataset_cls4_background07_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 169
I - Training: 
	I - Batch: 50 | Loss: 0.497 | Acc: 96.125% | Wgt Acc: 98.836%
	I - Batch: 100 | Loss: 0.498 | Acc: 95.875% | Wgt Acc: 98.762%
	I - Batch: 150 | Loss: 0.499 | Acc: 95.417% | Wgt Acc: 98.544%
	I - Batch: 200 | Loss: 0.500 | Acc: 95.281% | Wgt Acc: 98.525%
I - num batch: 222
I - Train -- Loss: 0.500 | Acc: 95.179% | Wgt Acc: 98.515% | LR: 1.250000e-04 | Dur: 132.95s
I - Confusion Matrix: [row->prediction - col->label]
[[695.   0.   0.   0.  51.]
 [  0. 578.   0.   0.  33.]
 [  1.   0. 733.   0.  39.]
 [  0.   0.   0. 535.  42.]
 [  1.   0.   1.   3. 835.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.049 | Acc: 64.694% | Wgt Acc: 57.860% | Dur: 15.35s
I - Confusion Matrix: [row->prediction - col->label]
[[ 57.   3.   3.  14.   9.]
 [  1.  37.   8.   3.   2.]
 [  1.   9.  36.   1.  12.]
 [ 12.   6.   7.  50.   9.]
 [ 17.  23.  21.  18. 148.]]

I - Loading file: dataset_cls4_background08_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 170
I - Training: 
	I - Batch: 50 | Loss: 0.496 | Acc: 96.125% | Wgt Acc: 98.814%
	I - Batch: 100 | Loss: 0.499 | Acc: 95.438% | Wgt Acc: 98.559%
	I - Batch: 150 | Loss: 0.496 | Acc: 95.458% | Wgt Acc: 98.632%
	I - Batch: 200 | Loss: 0.498 | Acc: 95.406% | Wgt Acc: 98.564%
I - num batch: 222
I - Train -- Loss: 0.500 | Acc: 95.038% | Wgt Acc: 98.427% | LR: 1.250000e-04 | Dur: 134.32s
I - Confusion Matrix: [row->prediction - col->label]
[[690.   0.   0.   0.  57.]
 [  0. 578.   0.   1.  32.]
 [  0.   0. 734.   0.  37.]
 [  1.   0.   0. 537.  42.]
 [  6.   0.   0.   0. 832.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.039 | Acc: 66.075% | Wgt Acc: 61.458% | Dur: 14.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 63.   0.   3.  14.   9.]
 [  3.  46.  10.   2.   9.]
 [  1.  16.  37.   6.  16.]
 [ 10.   4.   7.  49.   6.]
 [ 11.  12.  18.  15. 140.]]

I - Loading file: dataset_cls4_background09_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 171
I - Training: 
	I - Batch: 50 | Loss: 0.502 | Acc: 93.125% | Wgt Acc: 97.923%
	I - Batch: 100 | Loss: 0.500 | Acc: 93.750% | Wgt Acc: 98.132%
	I - Batch: 150 | Loss: 0.500 | Acc: 94.458% | Wgt Acc: 98.288%
	I - Batch: 200 | Loss: 0.500 | Acc: 94.844% | Wgt Acc: 98.374%
I - num batch: 222
I - Train -- Loss: 0.500 | Acc: 94.784% | Wgt Acc: 98.381% | LR: 1.250000e-04 | Dur: 135.40s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   0.  65.]
 [  0. 577.   0.   1.  32.]
 [  0.   0. 733.   1.  50.]
 [  0.   0.   0. 536.  31.]
 [  3.   1.   1.   0. 822.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.046 | Acc: 66.667% | Wgt Acc: 60.593% | Dur: 13.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 68.   7.   6.  15.  14.]
 [  0.  42.   5.   1.   7.]
 [  0.   9.  29.   0.   6.]
 [  8.   4.   9.  51.   5.]
 [ 12.  16.  26.  19. 148.]]

I - Loading file: dataset_cls4_background10_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 172
I - Training: 
	I - Batch: 50 | Loss: 0.489 | Acc: 95.750% | Wgt Acc: 98.737%
	I - Batch: 100 | Loss: 0.498 | Acc: 94.938% | Wgt Acc: 98.382%
	I - Batch: 150 | Loss: 0.496 | Acc: 95.333% | Wgt Acc: 98.518%
	I - Batch: 200 | Loss: 0.497 | Acc: 95.188% | Wgt Acc: 98.470%
I - num batch: 222
I - Train -- Loss: 0.498 | Acc: 95.123% | Wgt Acc: 98.441% | LR: 1.250000e-04 | Dur: 136.32s
I - Confusion Matrix: [row->prediction - col->label]
[[694.   0.   0.   1.  63.]
 [  1. 578.   0.   1.  34.]
 [  0.   0. 733.   1.  39.]
 [  0.   0.   0. 534.  29.]
 [  2.   0.   1.   1. 835.]]

I - Validation: 
I - num batch: 32
I - Val -- Loss: 1.044 | Acc: 64.694% | Wgt Acc: 56.326% | Dur: 14.87s
I - Confusion Matrix: [row->prediction - col->label]
[[ 65.   4.   4.  16.   8.]
 [  0.  30.   2.   3.   3.]
 [  0.   8.  28.   0.   6.]
 [  8.   2.  10.  50.   8.]
 [ 15.  34.  31.  17. 155.]]

I - Loading file: dataset_cls4_background11_no_samples1000.pkl in /data_ssd/processed/kinetics400/processed_4class_fixed_50frames_256x256/train/new_background
I - New label distribution: [ 697.  578.  734.  538. 1000.]

I - Epoch: 173
I - Training: 
	I - Batch: 50 | Loss: 0.494 | Acc: 95.375% | Wgt Acc: 98.753%
	I - Batch: 100 | Loss: 0.496 | Acc: 95.000% | Wgt Acc: 98.588%
	I - Batch: 150 | Loss: 0.501 | Acc: 94.833% | Wgt Acc: 98.419%
