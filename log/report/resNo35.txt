Mon Oct 17 15:56:14 2022
I - CONFIGURATION: {'batchSize': 8, 'bias': True, 'classWeights': [0.2, 0.25, 0.2, 0.25, 0.08], 'classWeightsFlag': True, 'dataConfig': {'bulkPickles': True, 'dataCount': 4, 'doubleClasses': [1, 2], 'fixedDataset': True, 'loadData2memory': True, 'multiplyData': False, 'tossFirstLastFrames': True}, 'dataPath': '/data/processed/Kinetics/', 'dropoutRate': 0.5, 'epochNo': 250, 'foldRatio': 2, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 10, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat'], 'lastLayerInitUniform': True, 'learningRate': 0.001, 'logBatchAt': 50, 'maxValidationAcc': 66.05504587155963, 'maxValidationTrainNo': 31, 'modelVersion': 5, 'multiStageModelList': [6, 7], 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 20, 25], 'trainNo': 35, 'validationAccThr': 65, 'weightDecay': 0.001}
I - CONFIGURATION: {'background': [6717, 104557, 117656, 118800, 12379, 126138, 133287, 135007, 141242, 144859, 46195, 46587, 77996, 98407], 'pull ups': [1466, 4735, 9363, 100435, 102041, 10225, 102947, 103716, 104734, 105033, 10560, 106340, 109059, 109641, 109703, 111345, 117580, 119571, 119672, 122762, 123022, 123478, 124666, 12635, 129261, 12966, 129753, 130508, 131478, 132213, 133243, 135288, 135611, 135763, 136798, 138779, 13934, 141056, 141652, 142917, 146622, 147919, 148588, 149022, 149145, 15832, 158879, 159023, 159709, 164471, 174922, 175015, 175601, 175837, 177131, 179636, 181907, 185449, 186289, 187166, 188352, 191254, 201928, 202460, 202742, 203196, 210375, 213343, 213832, 216082, 218783, 218869, 219024, 27502, 30141, 32450, 34307, 35192, 35469, 37937, 42237, 43359, 43561, 53750, 54715, 60242, 61148, 65757, 67801, 68225, 70288, 71340, 71574, 72992, 73680, 74104, 74587, 74618, 75408, 77194, 81119, 83857, 86305, 86583, 86944, 87697, 90088, 91254, 91916], 'push up': [790, 1376, 1603, 2377, 2750, 4599, 5166, 6351, 7888, 8059, 102124, 103237, 105800, 106743, 107365, 111006, 114150, 116746, 117373, 119751, 123552, 124724, 127391, 12777, 128686, 131204, 134202, 138067, 142848, 145566, 150321, 155706, 156714, 15810, 15892, 162251, 162602, 162736, 16319, 16663, 16730, 167610, 167928, 168786, 170519, 170933, 17129, 172521, 173206, 174806, 183725, 186930, 187541, 190408, 191107, 197324, 199276, 203358, 204694, 207133, 208126, 209276, 209796, 210367, 210667, 213350, 218691, 219325, 23397, 29694, 37645, 38840, 46952, 47445, 48601, 48658, 50008, 52236, 52467, 52900, 53520, 55638, 55682, 59738, 61515, 62146, 62281, 72963, 74435, 74462, 75827, 78477, 78856, 79602, 79984, 83353, 85540, 91035, 92263, 97051, 99142], 'situp': [1055, 2266, 4304, 6078, 7337, 100065, 102891, 104650, 107273, 107851, 108111, 10812, 108505, 109397, 110563, 111111, 111478, 112311, 113868, 114249, 114806, 116566, 116875, 117511, 11801, 118772, 119784, 120384, 123275, 123658, 124222, 126160, 126270, 127277, 128880, 128907, 129493, 129720, 131406, 132060, 133096, 134974, 136812, 137005, 137612, 137882, 139213, 141774, 14206, 143300, 143548, 143934, 14494, 145544, 145953, 147146, 148867, 149066, 149252, 149654, 150259, 150302, 153122, 153227, 153691, 156335, 159646, 160557, 16466, 166424, 169419, 170487, 170628, 171290, 172016, 174857, 177150, 177829, 179891, 180278, 180585, 181684, 181706, 182300, 183368, 183863, 184207, 184593, 184957, 186845, 187706, 187731, 188119, 188206, 189995, 190008, 190573, 190974, 191164, 191208, 191236, 19150, 192699, 193865, 193967, 19414, 195064, 195797, 196874, 19720, 197631, 199326, 199590, 200068, 202952, 204138, 207569, 207605, 209000, 20909, 209637, 209970, 212019, 212142, 213373, 214038, 215579, 216500, 216585, 217089, 23537, 24779, 25129, 25863, 26253, 27849, 28232, 29356, 31966, 32607, 33814, 33943, 33980, 34065, 35811, 36921, 37090, 38130, 39060, 40342, 41741, 42035, 43028, 43224, 44043, 45388, 45595, 46880, 47767, 49078, 51658, 52742, 53045, 53413, 53513, 54037, 56415, 57137, 58072, 58816, 59113, 62391, 64925, 66736, 68754, 71858, 72809, 74758, 74854, 75001, 77120, 77245, 78401, 78882, 78966, 80218, 82439, 84326, 86384, 91813, 92396, 94219, 95689, 98098, 99540], 'squat': [215, 909, 3104, 3412, 3874, 4090, 4780, 5263, 5335, 5871, 6372, 6376, 9404, 101769, 103303, 103599, 103888, 10452, 105075, 105187, 105705, 106330, 107185, 109752, 109807, 110159, 110534, 112017, 112018, 112173, 112319, 112506, 112842, 113334, 114681, 115030, 115093, 115386, 118011, 118149, 118191, 118592, 119202, 119505, 12063, 120751, 120752, 12135, 121653, 122418, 123235, 123237, 124365, 124379, 124381, 126146, 126727, 127111, 128631, 129484, 130633, 131213, 131499, 131502, 132036, 132243, 133907, 133947, 13397, 134955, 137236, 140543, 140610, 141399, 142777, 143184, 143512, 143925, 144349, 144352, 14614, 146153, 14615, 146977, 147684, 147886, 147904, 148783, 149752, 151859, 152117, 153603, 15417, 154652, 155334, 156285, 156287, 156588, 15807, 158190, 158219, 158642, 158969, 159204, 159443, 159832, 162160, 162750, 16390, 165228, 166328, 166567, 168765, 169224, 169473, 169907, 170431, 170738, 171418, 172115, 172146, 173139, 173316, 173967, 174116, 174855, 175040, 175699, 175768, 175771, 179253, 181702, 182061, 182062, 182916, 183802, 184090, 185433, 186723, 186794, 186886, 188017, 188391, 188392, 189690, 190146, 190188, 191780, 192239, 196272, 196437, 199877, 199881, 20076, 20078, 201326, 203580, 203768, 203799, 204217, 20495, 204978, 207543, 207582, 207586, 207854, 208375, 208385, 208803, 209226, 210596, 211423, 212103, 212420, 212471, 212472, 212870, 213655, 213946, 215180, 215592, 21631, 217382, 217548, 218504, 218729, 219686, 23241, 23477, 23479, 23978, 24358, 24519, 26198, 28238, 28403, 28628, 30376, 31045, 31410, 32637, 32652, 33136, 33339, 34215, 34314, 35111, 36104, 36106, 37331, 38749, 38864, 39181, 39506, 39903, 40063, 40087, 40877, 41372, 41448, 43573, 43792, 43795, 45193, 45888, 47014, 47275, 47663, 47708, 48670, 49026, 49355, 50029, 50865, 51112, 51116, 51544, 51686, 52267, 52930, 53042, 53203, 54936, 54938, 55552, 56691, 57924, 60772, 61689, 61813, 62036, 62510, 62637, 63445, 63656, 63976, 66228, 67972, 69578, 71206, 71931, 72878, 72964, 72966, 75573, 77471, 78072, 78438, 78623, 78865, 79453, 79697, 80281, 80282, 81787, 82866, 83151, 83559, 84713, 85369, 85420, 85988, 87453, 88421, 88446, 89332, 90414, 91106, 91785, 91990, 93075, 93153, 93503, 93652, 93839, 94764, 94929, 95719, 95877, 97294, 97596, 99981]}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_cls0_pull_ups00_no_samples806.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls1_push_up00_no_samples390.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls2_situp00_no_samples562.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Loading file: dataset_cls3_squat00_no_samples840.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/train
I - Train set length:  2547
I - Label distribution: [697. 578. 734. 538.]
I - ========== TEST  SET ==========
I - Loading file: dataset_test00_no_samples327.pkl in /data/processed/Kinetics/processed_4class_fixed_50frames_256x256/test
I - Test set length:  327
I - Label distribution: [88. 78. 75. 86.]
I - Batch size:  8  tensor shape:  torch.Size([10, 12, 128, 128])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(3) data number in dataset:  tensor([158706, 184007,  22303, 151560,  84363,  77285,  48180,  38607])
I - Initializing model TCNv5
I - Number of Model Parameters: 563744
I - Model output shape:  torch.Size([8, 4])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv5                                    [8, 4]                    --
├─FusedConv2dBNReLU: 1-1                 [8, 64, 128, 128]         838
│    └─OutputShiftSqueeze: 2-1           --                        --
│    └─One: 2-2                          [1]                       --
│    └─OutputScale: 2-3                  --                        --
│    └─Empty: 2-4                        [64, 12, 1, 1]            --
│    └─Empty: 2-5                        [64, 12, 1, 1]            --
│    └─Empty: 2-6                        [64]                      --
│    └─Empty: 2-7                        [64]                      --
│    └─BatchNorm2d: 2-8                  [8, 64, 128, 128]         --
│    └─Scaler: 2-9                       [8, 64, 128, 128]         --
│    └─ReLU: 2-10                        [8, 64, 128, 128]         --
│    └─Empty: 2-11                       [8, 64, 128, 128]         --
│    └─Clamp: 2-12                       [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-2                 [8, 64, 128, 128]         36,934
│    └─OutputShiftSqueeze: 2-13          --                        --
│    └─One: 2-14                         [1]                       --
│    └─OutputScale: 2-15                 --                        --
│    └─Empty: 2-16                       [64, 64, 3, 3]            --
│    └─Empty: 2-17                       [64, 64, 3, 3]            --
│    └─Empty: 2-18                       [64]                      --
│    └─Empty: 2-19                       [64]                      --
│    └─BatchNorm2d: 2-20                 [8, 64, 128, 128]         --
│    └─Scaler: 2-21                      [8, 64, 128, 128]         --
│    └─ReLU: 2-22                        [8, 64, 128, 128]         --
│    └─Empty: 2-23                       [8, 64, 128, 128]         --
│    └─Clamp: 2-24                       [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-3                 [8, 64, 128, 128]         4,166
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [64, 64, 1, 1]            --
│    └─Empty: 2-29                       [64, 64, 1, 1]            --
│    └─Empty: 2-30                       [64]                      --
│    └─Empty: 2-31                       [64]                      --
│    └─BatchNorm2d: 2-32                 [8, 64, 128, 128]         --
│    └─Scaler: 2-33                      [8, 64, 128, 128]         --
│    └─ReLU: 2-34                        [8, 64, 128, 128]         --
│    └─Empty: 2-35                       [8, 64, 128, 128]         --
│    └─Clamp: 2-36                       [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-4                 [8, 64, 128, 128]         36,934
│    └─OutputShiftSqueeze: 2-37          --                        --
│    └─One: 2-38                         [1]                       --
│    └─OutputScale: 2-39                 --                        --
│    └─Empty: 2-40                       [64, 64, 3, 3]            --
│    └─Empty: 2-41                       [64, 64, 3, 3]            --
│    └─Empty: 2-42                       [64]                      --
│    └─Empty: 2-43                       [64]                      --
│    └─BatchNorm2d: 2-44                 [8, 64, 128, 128]         --
│    └─Scaler: 2-45                      [8, 64, 128, 128]         --
│    └─ReLU: 2-46                        [8, 64, 128, 128]         --
│    └─Empty: 2-47                       [8, 64, 128, 128]         --
│    └─Clamp: 2-48                       [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-5          [8, 64, 64, 64]           36,934
│    └─MaxPool2d: 2-49                   [8, 64, 64, 64]           --
│    └─Empty: 2-50                       [8, 64, 64, 64]           --
│    └─Empty: 2-51                       [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-52          --                        --
│    └─One: 2-53                         [1]                       --
│    └─OutputScale: 2-54                 --                        --
│    └─Empty: 2-55                       [64, 64, 3, 3]            --
│    └─Empty: 2-56                       [64, 64, 3, 3]            --
│    └─Empty: 2-57                       [64]                      --
│    └─Empty: 2-58                       [64]                      --
│    └─BatchNorm2d: 2-59                 [8, 64, 64, 64]           --
│    └─Scaler: 2-60                      [8, 64, 64, 64]           --
│    └─ReLU: 2-61                        [8, 64, 64, 64]           --
│    └─Empty: 2-62                       [8, 64, 64, 64]           --
│    └─Clamp: 2-63                       [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-6                 [8, 64, 64, 64]           36,934
│    └─OutputShiftSqueeze: 2-64          --                        --
│    └─One: 2-65                         [1]                       --
│    └─OutputScale: 2-66                 --                        --
│    └─Empty: 2-67                       [64, 64, 3, 3]            --
│    └─Empty: 2-68                       [64, 64, 3, 3]            --
│    └─Empty: 2-69                       [64]                      --
│    └─Empty: 2-70                       [64]                      --
│    └─BatchNorm2d: 2-71                 [8, 64, 64, 64]           --
│    └─Scaler: 2-72                      [8, 64, 64, 64]           --
│    └─ReLU: 2-73                        [8, 64, 64, 64]           --
│    └─Empty: 2-74                       [8, 64, 64, 64]           --
│    └─Clamp: 2-75                       [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-7          [8, 64, 32, 32]           36,934
│    └─MaxPool2d: 2-76                   [8, 64, 32, 32]           --
│    └─Empty: 2-77                       [8, 64, 32, 32]           --
│    └─Empty: 2-78                       [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-79          --                        --
│    └─One: 2-80                         [1]                       --
│    └─OutputScale: 2-81                 --                        --
│    └─Empty: 2-82                       [64, 64, 3, 3]            --
│    └─Empty: 2-83                       [64, 64, 3, 3]            --
│    └─Empty: 2-84                       [64]                      --
│    └─Empty: 2-85                       [64]                      --
│    └─BatchNorm2d: 2-86                 [8, 64, 32, 32]           --
│    └─Scaler: 2-87                      [8, 64, 32, 32]           --
│    └─ReLU: 2-88                        [8, 64, 32, 32]           --
│    └─Empty: 2-89                       [8, 64, 32, 32]           --
│    └─Clamp: 2-90                       [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-8                 [8, 64, 32, 32]           36,934
│    └─OutputShiftSqueeze: 2-91          --                        --
│    └─One: 2-92                         [1]                       --
│    └─OutputScale: 2-93                 --                        --
│    └─Empty: 2-94                       [64, 64, 3, 3]            --
│    └─Empty: 2-95                       [64, 64, 3, 3]            --
│    └─Empty: 2-96                       [64]                      --
│    └─Empty: 2-97                       [64]                      --
│    └─BatchNorm2d: 2-98                 [8, 64, 32, 32]           --
│    └─Scaler: 2-99                      [8, 64, 32, 32]           --
│    └─ReLU: 2-100                       [8, 64, 32, 32]           --
│    └─Empty: 2-101                      [8, 64, 32, 32]           --
│    └─Clamp: 2-102                      [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-9          [8, 64, 16, 16]           36,934
│    └─MaxPool2d: 2-103                  [8, 64, 16, 16]           --
│    └─Empty: 2-104                      [8, 64, 16, 16]           --
│    └─Empty: 2-105                      [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-106         --                        --
│    └─One: 2-107                        [1]                       --
│    └─OutputScale: 2-108                --                        --
│    └─Empty: 2-109                      [64, 64, 3, 3]            --
│    └─Empty: 2-110                      [64, 64, 3, 3]            --
│    └─Empty: 2-111                      [64]                      --
│    └─Empty: 2-112                      [64]                      --
│    └─BatchNorm2d: 2-113                [8, 64, 16, 16]           --
│    └─Scaler: 2-114                     [8, 64, 16, 16]           --
│    └─ReLU: 2-115                       [8, 64, 16, 16]           --
│    └─Empty: 2-116                      [8, 64, 16, 16]           --
│    └─Clamp: 2-117                      [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-10                [8, 64, 16, 16]           4,166
│    └─OutputShiftSqueeze: 2-118         --                        --
│    └─One: 2-119                        [1]                       --
│    └─OutputScale: 2-120                --                        --
│    └─Empty: 2-121                      [64, 64, 1, 1]            --
│    └─Empty: 2-122                      [64, 64, 1, 1]            --
│    └─Empty: 2-123                      [64]                      --
│    └─Empty: 2-124                      [64]                      --
│    └─BatchNorm2d: 2-125                [8, 64, 16, 16]           --
│    └─Scaler: 2-126                     [8, 64, 16, 16]           --
│    └─ReLU: 2-127                       [8, 64, 16, 16]           --
│    └─Empty: 2-128                      [8, 64, 16, 16]           --
│    └─Clamp: 2-129                      [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-11         [8, 64, 16, 16]           36,934
│    └─MaxPool2d: 2-130                  [8, 64, 16, 16]           --
│    └─Empty: 2-131                      [8, 64, 16, 16]           --
│    └─Empty: 2-132                      [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-133         --                        --
│    └─One: 2-134                        [1]                       --
│    └─OutputScale: 2-135                --                        --
│    └─Empty: 2-136                      [64, 64, 3, 3]            --
│    └─Empty: 2-137                      [64, 64, 3, 3]            --
│    └─Empty: 2-138                      [64]                      --
│    └─Empty: 2-139                      [64]                      --
│    └─BatchNorm2d: 2-140                [8, 64, 16, 16]           --
│    └─Scaler: 2-141                     [8, 64, 16, 16]           --
│    └─ReLU: 2-142                       [8, 64, 16, 16]           --
│    └─Empty: 2-143                      [8, 64, 16, 16]           --
│    └─Clamp: 2-144                      [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-12         [8, 64, 8, 8]             36,934
│    └─MaxPool2d: 2-145                  [8, 64, 8, 8]             --
│    └─Empty: 2-146                      [8, 64, 8, 8]             --
│    └─Empty: 2-147                      [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-148         --                        --
│    └─One: 2-149                        [1]                       --
│    └─OutputScale: 2-150                --                        --
│    └─Empty: 2-151                      [64, 64, 3, 3]            --
│    └─Empty: 2-152                      [64, 64, 3, 3]            --
│    └─Empty: 2-153                      [64]                      --
│    └─Empty: 2-154                      [64]                      --
│    └─BatchNorm2d: 2-155                [8, 64, 8, 8]             --
│    └─Scaler: 2-156                     [8, 64, 8, 8]             --
│    └─ReLU: 2-157                       [8, 64, 8, 8]             --
│    └─Empty: 2-158                      [8, 64, 8, 8]             --
│    └─Clamp: 2-159                      [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-13                [8, 64, 8, 8]             4,166
│    └─OutputShiftSqueeze: 2-160         --                        --
│    └─One: 2-161                        [1]                       --
│    └─OutputScale: 2-162                --                        --
│    └─Empty: 2-163                      [64, 64, 1, 1]            --
│    └─Empty: 2-164                      [64, 64, 1, 1]            --
│    └─Empty: 2-165                      [64]                      --
│    └─Empty: 2-166                      [64]                      --
│    └─BatchNorm2d: 2-167                [8, 64, 8, 8]             --
│    └─Scaler: 2-168                     [8, 64, 8, 8]             --
│    └─ReLU: 2-169                       [8, 64, 8, 8]             --
│    └─Empty: 2-170                      [8, 64, 8, 8]             --
│    └─Clamp: 2-171                      [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-14         [8, 64, 8, 8]             36,934
│    └─MaxPool2d: 2-172                  [8, 64, 8, 8]             --
│    └─Empty: 2-173                      [8, 64, 8, 8]             --
│    └─Empty: 2-174                      [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-175         --                        --
│    └─One: 2-176                        [1]                       --
│    └─OutputScale: 2-177                --                        --
│    └─Empty: 2-178                      [64, 64, 3, 3]            --
│    └─Empty: 2-179                      [64, 64, 3, 3]            --
│    └─Empty: 2-180                      [64]                      --
│    └─Empty: 2-181                      [64]                      --
│    └─BatchNorm2d: 2-182                [8, 64, 8, 8]             --
│    └─Scaler: 2-183                     [8, 64, 8, 8]             --
│    └─ReLU: 2-184                       [8, 64, 8, 8]             --
│    └─Empty: 2-185                      [8, 64, 8, 8]             --
│    └─Clamp: 2-186                      [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-15         [8, 64, 4, 4]             4,166
│    └─MaxPool2d: 2-187                  [8, 64, 4, 4]             --
│    └─Empty: 2-188                      [8, 64, 4, 4]             --
│    └─Empty: 2-189                      [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-190         --                        --
│    └─One: 2-191                        [1]                       --
│    └─OutputScale: 2-192                --                        --
│    └─Empty: 2-193                      [64, 64, 1, 1]            --
│    └─Empty: 2-194                      [64, 64, 1, 1]            --
│    └─Empty: 2-195                      [64]                      --
│    └─Empty: 2-196                      [64]                      --
│    └─BatchNorm2d: 2-197                [8, 64, 4, 4]             --
│    └─Scaler: 2-198                     [8, 64, 4, 4]             --
│    └─ReLU: 2-199                       [8, 64, 4, 4]             --
│    └─Empty: 2-200                      [8, 64, 4, 4]             --
│    └─Clamp: 2-201                      [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-16                [8, 64, 4, 4]             4,166
│    └─OutputShiftSqueeze: 2-202         --                        --
│    └─One: 2-203                        [1]                       --
│    └─OutputScale: 2-204                --                        --
│    └─Empty: 2-205                      [64, 64, 1, 1]            --
│    └─Empty: 2-206                      [64, 64, 1, 1]            --
│    └─Empty: 2-207                      [64]                      --
│    └─Empty: 2-208                      [64]                      --
│    └─BatchNorm2d: 2-209                [8, 64, 4, 4]             --
│    └─Scaler: 2-210                     [8, 64, 4, 4]             --
│    └─ReLU: 2-211                       [8, 64, 4, 4]             --
│    └─Empty: 2-212                      [8, 64, 4, 4]             --
│    └─Clamp: 2-213                      [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-17         [8, 64, 4, 4]             36,934
│    └─MaxPool2d: 2-214                  [8, 64, 4, 4]             --
│    └─Empty: 2-215                      [8, 64, 4, 4]             --
│    └─Empty: 2-216                      [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-217         --                        --
│    └─One: 2-218                        [1]                       --
│    └─OutputScale: 2-219                --                        --
│    └─Empty: 2-220                      [64, 64, 3, 3]            --
│    └─Empty: 2-221                      [64, 64, 3, 3]            --
│    └─Empty: 2-222                      [64]                      --
│    └─Empty: 2-223                      [64]                      --
│    └─BatchNorm2d: 2-224                [8, 64, 4, 4]             --
│    └─Scaler: 2-225                     [8, 64, 4, 4]             --
│    └─ReLU: 2-226                       [8, 64, 4, 4]             --
│    └─Empty: 2-227                      [8, 64, 4, 4]             --
│    └─Clamp: 2-228                      [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-18                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-229         --                        --
│    └─One: 2-230                        [1]                       --
│    └─OutputScale: 2-231                --                        --
│    └─Empty: 2-232                      [64, 12, 1, 1]            --
│    └─Empty: 2-233                      [64, 12, 1, 1]            --
│    └─Empty: 2-234                      [64]                      --
│    └─Empty: 2-235                      [64]                      --
│    └─BatchNorm2d: 2-236                [8, 64, 128, 128]         --
│    └─Scaler: 2-237                     [8, 64, 128, 128]         --
│    └─ReLU: 2-238                       [8, 64, 128, 128]         --
│    └─Empty: 2-239                      [8, 64, 128, 128]         --
│    └─Clamp: 2-240                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-19                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-241         --                        --
│    └─One: 2-242                        [1]                       --
│    └─OutputScale: 2-243                --                        --
│    └─Empty: 2-244                      [64, 64, 3, 3]            --
│    └─Empty: 2-245                      [64, 64, 3, 3]            --
│    └─Empty: 2-246                      [64]                      --
│    └─Empty: 2-247                      [64]                      --
│    └─BatchNorm2d: 2-248                [8, 64, 128, 128]         --
│    └─Scaler: 2-249                     [8, 64, 128, 128]         --
│    └─ReLU: 2-250                       [8, 64, 128, 128]         --
│    └─Empty: 2-251                      [8, 64, 128, 128]         --
│    └─Clamp: 2-252                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-20                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-253         --                        --
│    └─One: 2-254                        [1]                       --
│    └─OutputScale: 2-255                --                        --
│    └─Empty: 2-256                      [64, 64, 1, 1]            --
│    └─Empty: 2-257                      [64, 64, 1, 1]            --
│    └─Empty: 2-258                      [64]                      --
│    └─Empty: 2-259                      [64]                      --
│    └─BatchNorm2d: 2-260                [8, 64, 128, 128]         --
│    └─Scaler: 2-261                     [8, 64, 128, 128]         --
│    └─ReLU: 2-262                       [8, 64, 128, 128]         --
│    └─Empty: 2-263                      [8, 64, 128, 128]         --
│    └─Clamp: 2-264                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-21                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-265         --                        --
│    └─One: 2-266                        [1]                       --
│    └─OutputScale: 2-267                --                        --
│    └─Empty: 2-268                      [64, 64, 3, 3]            --
│    └─Empty: 2-269                      [64, 64, 3, 3]            --
│    └─Empty: 2-270                      [64]                      --
│    └─Empty: 2-271                      [64]                      --
│    └─BatchNorm2d: 2-272                [8, 64, 128, 128]         --
│    └─Scaler: 2-273                     [8, 64, 128, 128]         --
│    └─ReLU: 2-274                       [8, 64, 128, 128]         --
│    └─Empty: 2-275                      [8, 64, 128, 128]         --
│    └─Clamp: 2-276                      [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-22         [8, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-277                  [8, 64, 64, 64]           --
│    └─Empty: 2-278                      [8, 64, 64, 64]           --
│    └─Empty: 2-279                      [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-280         --                        --
│    └─One: 2-281                        [1]                       --
│    └─OutputScale: 2-282                --                        --
│    └─Empty: 2-283                      [64, 64, 3, 3]            --
│    └─Empty: 2-284                      [64, 64, 3, 3]            --
│    └─Empty: 2-285                      [64]                      --
│    └─Empty: 2-286                      [64]                      --
│    └─BatchNorm2d: 2-287                [8, 64, 64, 64]           --
│    └─Scaler: 2-288                     [8, 64, 64, 64]           --
│    └─ReLU: 2-289                       [8, 64, 64, 64]           --
│    └─Empty: 2-290                      [8, 64, 64, 64]           --
│    └─Clamp: 2-291                      [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-23                [8, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-292         --                        --
│    └─One: 2-293                        [1]                       --
│    └─OutputScale: 2-294                --                        --
│    └─Empty: 2-295                      [64, 64, 3, 3]            --
│    └─Empty: 2-296                      [64, 64, 3, 3]            --
│    └─Empty: 2-297                      [64]                      --
│    └─Empty: 2-298                      [64]                      --
│    └─BatchNorm2d: 2-299                [8, 64, 64, 64]           --
│    └─Scaler: 2-300                     [8, 64, 64, 64]           --
│    └─ReLU: 2-301                       [8, 64, 64, 64]           --
│    └─Empty: 2-302                      [8, 64, 64, 64]           --
│    └─Clamp: 2-303                      [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-24         [8, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-304                  [8, 64, 32, 32]           --
│    └─Empty: 2-305                      [8, 64, 32, 32]           --
│    └─Empty: 2-306                      [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-307         --                        --
│    └─One: 2-308                        [1]                       --
│    └─OutputScale: 2-309                --                        --
│    └─Empty: 2-310                      [64, 64, 3, 3]            --
│    └─Empty: 2-311                      [64, 64, 3, 3]            --
│    └─Empty: 2-312                      [64]                      --
│    └─Empty: 2-313                      [64]                      --
│    └─BatchNorm2d: 2-314                [8, 64, 32, 32]           --
│    └─Scaler: 2-315                     [8, 64, 32, 32]           --
│    └─ReLU: 2-316                       [8, 64, 32, 32]           --
│    └─Empty: 2-317                      [8, 64, 32, 32]           --
│    └─Clamp: 2-318                      [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-25                [8, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-319         --                        --
│    └─One: 2-320                        [1]                       --
│    └─OutputScale: 2-321                --                        --
│    └─Empty: 2-322                      [64, 64, 3, 3]            --
│    └─Empty: 2-323                      [64, 64, 3, 3]            --
│    └─Empty: 2-324                      [64]                      --
│    └─Empty: 2-325                      [64]                      --
│    └─BatchNorm2d: 2-326                [8, 64, 32, 32]           --
│    └─Scaler: 2-327                     [8, 64, 32, 32]           --
│    └─ReLU: 2-328                       [8, 64, 32, 32]           --
│    └─Empty: 2-329                      [8, 64, 32, 32]           --
│    └─Clamp: 2-330                      [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-26         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-331                  [8, 64, 16, 16]           --
│    └─Empty: 2-332                      [8, 64, 16, 16]           --
│    └─Empty: 2-333                      [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-334         --                        --
│    └─One: 2-335                        [1]                       --
│    └─OutputScale: 2-336                --                        --
│    └─Empty: 2-337                      [64, 64, 3, 3]            --
│    └─Empty: 2-338                      [64, 64, 3, 3]            --
│    └─Empty: 2-339                      [64]                      --
│    └─Empty: 2-340                      [64]                      --
│    └─BatchNorm2d: 2-341                [8, 64, 16, 16]           --
│    └─Scaler: 2-342                     [8, 64, 16, 16]           --
│    └─ReLU: 2-343                       [8, 64, 16, 16]           --
│    └─Empty: 2-344                      [8, 64, 16, 16]           --
│    └─Clamp: 2-345                      [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-27                [8, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-346         --                        --
│    └─One: 2-347                        [1]                       --
│    └─OutputScale: 2-348                --                        --
│    └─Empty: 2-349                      [64, 64, 1, 1]            --
│    └─Empty: 2-350                      [64, 64, 1, 1]            --
│    └─Empty: 2-351                      [64]                      --
│    └─Empty: 2-352                      [64]                      --
│    └─BatchNorm2d: 2-353                [8, 64, 16, 16]           --
│    └─Scaler: 2-354                     [8, 64, 16, 16]           --
│    └─ReLU: 2-355                       [8, 64, 16, 16]           --
│    └─Empty: 2-356                      [8, 64, 16, 16]           --
│    └─Clamp: 2-357                      [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-28         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-358                  [8, 64, 16, 16]           --
│    └─Empty: 2-359                      [8, 64, 16, 16]           --
│    └─Empty: 2-360                      [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-361         --                        --
│    └─One: 2-362                        [1]                       --
│    └─OutputScale: 2-363                --                        --
│    └─Empty: 2-364                      [64, 64, 3, 3]            --
│    └─Empty: 2-365                      [64, 64, 3, 3]            --
│    └─Empty: 2-366                      [64]                      --
│    └─Empty: 2-367                      [64]                      --
│    └─BatchNorm2d: 2-368                [8, 64, 16, 16]           --
│    └─Scaler: 2-369                     [8, 64, 16, 16]           --
│    └─ReLU: 2-370                       [8, 64, 16, 16]           --
│    └─Empty: 2-371                      [8, 64, 16, 16]           --
│    └─Clamp: 2-372                      [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-29         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-373                  [8, 64, 8, 8]             --
│    └─Empty: 2-374                      [8, 64, 8, 8]             --
│    └─Empty: 2-375                      [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-376         --                        --
│    └─One: 2-377                        [1]                       --
│    └─OutputScale: 2-378                --                        --
│    └─Empty: 2-379                      [64, 64, 3, 3]            --
│    └─Empty: 2-380                      [64, 64, 3, 3]            --
│    └─Empty: 2-381                      [64]                      --
│    └─Empty: 2-382                      [64]                      --
│    └─BatchNorm2d: 2-383                [8, 64, 8, 8]             --
│    └─Scaler: 2-384                     [8, 64, 8, 8]             --
│    └─ReLU: 2-385                       [8, 64, 8, 8]             --
│    └─Empty: 2-386                      [8, 64, 8, 8]             --
│    └─Clamp: 2-387                      [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-30                [8, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-388         --                        --
│    └─One: 2-389                        [1]                       --
│    └─OutputScale: 2-390                --                        --
│    └─Empty: 2-391                      [64, 64, 1, 1]            --
│    └─Empty: 2-392                      [64, 64, 1, 1]            --
│    └─Empty: 2-393                      [64]                      --
│    └─Empty: 2-394                      [64]                      --
│    └─BatchNorm2d: 2-395                [8, 64, 8, 8]             --
│    └─Scaler: 2-396                     [8, 64, 8, 8]             --
│    └─ReLU: 2-397                       [8, 64, 8, 8]             --
│    └─Empty: 2-398                      [8, 64, 8, 8]             --
│    └─Clamp: 2-399                      [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-31         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-400                  [8, 64, 8, 8]             --
│    └─Empty: 2-401                      [8, 64, 8, 8]             --
│    └─Empty: 2-402                      [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-403         --                        --
│    └─One: 2-404                        [1]                       --
│    └─OutputScale: 2-405                --                        --
│    └─Empty: 2-406                      [64, 64, 3, 3]            --
│    └─Empty: 2-407                      [64, 64, 3, 3]            --
│    └─Empty: 2-408                      [64]                      --
│    └─Empty: 2-409                      [64]                      --
│    └─BatchNorm2d: 2-410                [8, 64, 8, 8]             --
│    └─Scaler: 2-411                     [8, 64, 8, 8]             --
│    └─ReLU: 2-412                       [8, 64, 8, 8]             --
│    └─Empty: 2-413                      [8, 64, 8, 8]             --
│    └─Clamp: 2-414                      [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-32         [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-415                  [8, 64, 4, 4]             --
│    └─Empty: 2-416                      [8, 64, 4, 4]             --
│    └─Empty: 2-417                      [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-418         --                        --
│    └─One: 2-419                        [1]                       --
│    └─OutputScale: 2-420                --                        --
│    └─Empty: 2-421                      [64, 64, 1, 1]            --
│    └─Empty: 2-422                      [64, 64, 1, 1]            --
│    └─Empty: 2-423                      [64]                      --
│    └─Empty: 2-424                      [64]                      --
│    └─BatchNorm2d: 2-425                [8, 64, 4, 4]             --
│    └─Scaler: 2-426                     [8, 64, 4, 4]             --
│    └─ReLU: 2-427                       [8, 64, 4, 4]             --
│    └─Empty: 2-428                      [8, 64, 4, 4]             --
│    └─Clamp: 2-429                      [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-33                [8, 64, 4, 4]             (recursive)
│    └─OutputShiftSqueeze: 2-430         --                        --
│    └─One: 2-431                        [1]                       --
│    └─OutputScale: 2-432                --                        --
│    └─Empty: 2-433                      [64, 64, 1, 1]            --
│    └─Empty: 2-434                      [64, 64, 1, 1]            --
│    └─Empty: 2-435                      [64]                      --
│    └─Empty: 2-436                      [64]                      --
│    └─BatchNorm2d: 2-437                [8, 64, 4, 4]             --
│    └─Scaler: 2-438                     [8, 64, 4, 4]             --
│    └─ReLU: 2-439                       [8, 64, 4, 4]             --
│    └─Empty: 2-440                      [8, 64, 4, 4]             --
│    └─Clamp: 2-441                      [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-34         [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-442                  [8, 64, 4, 4]             --
│    └─Empty: 2-443                      [8, 64, 4, 4]             --
│    └─Empty: 2-444                      [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-445         --                        --
│    └─One: 2-446                        [1]                       --
│    └─OutputScale: 2-447                --                        --
│    └─Empty: 2-448                      [64, 64, 3, 3]            --
│    └─Empty: 2-449                      [64, 64, 3, 3]            --
│    └─Empty: 2-450                      [64]                      --
│    └─Empty: 2-451                      [64]                      --
│    └─BatchNorm2d: 2-452                [8, 64, 4, 4]             --
│    └─Scaler: 2-453                     [8, 64, 4, 4]             --
│    └─ReLU: 2-454                       [8, 64, 4, 4]             --
│    └─Empty: 2-455                      [8, 64, 4, 4]             --
│    └─Clamp: 2-456                      [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-35                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-457         --                        --
│    └─One: 2-458                        [1]                       --
│    └─OutputScale: 2-459                --                        --
│    └─Empty: 2-460                      [64, 12, 1, 1]            --
│    └─Empty: 2-461                      [64, 12, 1, 1]            --
│    └─Empty: 2-462                      [64]                      --
│    └─Empty: 2-463                      [64]                      --
│    └─BatchNorm2d: 2-464                [8, 64, 128, 128]         --
│    └─Scaler: 2-465                     [8, 64, 128, 128]         --
│    └─ReLU: 2-466                       [8, 64, 128, 128]         --
│    └─Empty: 2-467                      [8, 64, 128, 128]         --
│    └─Clamp: 2-468                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-36                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-469         --                        --
│    └─One: 2-470                        [1]                       --
│    └─OutputScale: 2-471                --                        --
│    └─Empty: 2-472                      [64, 64, 3, 3]            --
│    └─Empty: 2-473                      [64, 64, 3, 3]            --
│    └─Empty: 2-474                      [64]                      --
│    └─Empty: 2-475                      [64]                      --
│    └─BatchNorm2d: 2-476                [8, 64, 128, 128]         --
│    └─Scaler: 2-477                     [8, 64, 128, 128]         --
│    └─ReLU: 2-478                       [8, 64, 128, 128]         --
│    └─Empty: 2-479                      [8, 64, 128, 128]         --
│    └─Clamp: 2-480                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-37                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-481         --                        --
│    └─One: 2-482                        [1]                       --
│    └─OutputScale: 2-483                --                        --
│    └─Empty: 2-484                      [64, 64, 1, 1]            --
│    └─Empty: 2-485                      [64, 64, 1, 1]            --
│    └─Empty: 2-486                      [64]                      --
│    └─Empty: 2-487                      [64]                      --
│    └─BatchNorm2d: 2-488                [8, 64, 128, 128]         --
│    └─Scaler: 2-489                     [8, 64, 128, 128]         --
│    └─ReLU: 2-490                       [8, 64, 128, 128]         --
│    └─Empty: 2-491                      [8, 64, 128, 128]         --
│    └─Clamp: 2-492                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-38                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-493         --                        --
│    └─One: 2-494                        [1]                       --
│    └─OutputScale: 2-495                --                        --
│    └─Empty: 2-496                      [64, 64, 3, 3]            --
│    └─Empty: 2-497                      [64, 64, 3, 3]            --
│    └─Empty: 2-498                      [64]                      --
│    └─Empty: 2-499                      [64]                      --
│    └─BatchNorm2d: 2-500                [8, 64, 128, 128]         --
│    └─Scaler: 2-501                     [8, 64, 128, 128]         --
│    └─ReLU: 2-502                       [8, 64, 128, 128]         --
│    └─Empty: 2-503                      [8, 64, 128, 128]         --
│    └─Clamp: 2-504                      [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-39         [8, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-505                  [8, 64, 64, 64]           --
│    └─Empty: 2-506                      [8, 64, 64, 64]           --
│    └─Empty: 2-507                      [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-508         --                        --
│    └─One: 2-509                        [1]                       --
│    └─OutputScale: 2-510                --                        --
│    └─Empty: 2-511                      [64, 64, 3, 3]            --
│    └─Empty: 2-512                      [64, 64, 3, 3]            --
│    └─Empty: 2-513                      [64]                      --
│    └─Empty: 2-514                      [64]                      --
│    └─BatchNorm2d: 2-515                [8, 64, 64, 64]           --
│    └─Scaler: 2-516                     [8, 64, 64, 64]           --
│    └─ReLU: 2-517                       [8, 64, 64, 64]           --
│    └─Empty: 2-518                      [8, 64, 64, 64]           --
│    └─Clamp: 2-519                      [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-40                [8, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-520         --                        --
│    └─One: 2-521                        [1]                       --
│    └─OutputScale: 2-522                --                        --
│    └─Empty: 2-523                      [64, 64, 3, 3]            --
│    └─Empty: 2-524                      [64, 64, 3, 3]            --
│    └─Empty: 2-525                      [64]                      --
│    └─Empty: 2-526                      [64]                      --
│    └─BatchNorm2d: 2-527                [8, 64, 64, 64]           --
│    └─Scaler: 2-528                     [8, 64, 64, 64]           --
│    └─ReLU: 2-529                       [8, 64, 64, 64]           --
│    └─Empty: 2-530                      [8, 64, 64, 64]           --
│    └─Clamp: 2-531                      [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-41         [8, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-532                  [8, 64, 32, 32]           --
│    └─Empty: 2-533                      [8, 64, 32, 32]           --
│    └─Empty: 2-534                      [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-535         --                        --
│    └─One: 2-536                        [1]                       --
│    └─OutputScale: 2-537                --                        --
│    └─Empty: 2-538                      [64, 64, 3, 3]            --
│    └─Empty: 2-539                      [64, 64, 3, 3]            --
│    └─Empty: 2-540                      [64]                      --
│    └─Empty: 2-541                      [64]                      --
│    └─BatchNorm2d: 2-542                [8, 64, 32, 32]           --
│    └─Scaler: 2-543                     [8, 64, 32, 32]           --
│    └─ReLU: 2-544                       [8, 64, 32, 32]           --
│    └─Empty: 2-545                      [8, 64, 32, 32]           --
│    └─Clamp: 2-546                      [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-42                [8, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-547         --                        --
│    └─One: 2-548                        [1]                       --
│    └─OutputScale: 2-549                --                        --
│    └─Empty: 2-550                      [64, 64, 3, 3]            --
│    └─Empty: 2-551                      [64, 64, 3, 3]            --
│    └─Empty: 2-552                      [64]                      --
│    └─Empty: 2-553                      [64]                      --
│    └─BatchNorm2d: 2-554                [8, 64, 32, 32]           --
│    └─Scaler: 2-555                     [8, 64, 32, 32]           --
│    └─ReLU: 2-556                       [8, 64, 32, 32]           --
│    └─Empty: 2-557                      [8, 64, 32, 32]           --
│    └─Clamp: 2-558                      [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-43         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-559                  [8, 64, 16, 16]           --
│    └─Empty: 2-560                      [8, 64, 16, 16]           --
│    └─Empty: 2-561                      [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-562         --                        --
│    └─One: 2-563                        [1]                       --
│    └─OutputScale: 2-564                --                        --
│    └─Empty: 2-565                      [64, 64, 3, 3]            --
│    └─Empty: 2-566                      [64, 64, 3, 3]            --
│    └─Empty: 2-567                      [64]                      --
│    └─Empty: 2-568                      [64]                      --
│    └─BatchNorm2d: 2-569                [8, 64, 16, 16]           --
│    └─Scaler: 2-570                     [8, 64, 16, 16]           --
│    └─ReLU: 2-571                       [8, 64, 16, 16]           --
│    └─Empty: 2-572                      [8, 64, 16, 16]           --
│    └─Clamp: 2-573                      [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-44                [8, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-574         --                        --
│    └─One: 2-575                        [1]                       --
│    └─OutputScale: 2-576                --                        --
│    └─Empty: 2-577                      [64, 64, 1, 1]            --
│    └─Empty: 2-578                      [64, 64, 1, 1]            --
│    └─Empty: 2-579                      [64]                      --
│    └─Empty: 2-580                      [64]                      --
│    └─BatchNorm2d: 2-581                [8, 64, 16, 16]           --
│    └─Scaler: 2-582                     [8, 64, 16, 16]           --
│    └─ReLU: 2-583                       [8, 64, 16, 16]           --
│    └─Empty: 2-584                      [8, 64, 16, 16]           --
│    └─Clamp: 2-585                      [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-45         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-586                  [8, 64, 16, 16]           --
│    └─Empty: 2-587                      [8, 64, 16, 16]           --
│    └─Empty: 2-588                      [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-589         --                        --
│    └─One: 2-590                        [1]                       --
│    └─OutputScale: 2-591                --                        --
│    └─Empty: 2-592                      [64, 64, 3, 3]            --
│    └─Empty: 2-593                      [64, 64, 3, 3]            --
│    └─Empty: 2-594                      [64]                      --
│    └─Empty: 2-595                      [64]                      --
│    └─BatchNorm2d: 2-596                [8, 64, 16, 16]           --
│    └─Scaler: 2-597                     [8, 64, 16, 16]           --
│    └─ReLU: 2-598                       [8, 64, 16, 16]           --
│    └─Empty: 2-599                      [8, 64, 16, 16]           --
│    └─Clamp: 2-600                      [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-46         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-601                  [8, 64, 8, 8]             --
│    └─Empty: 2-602                      [8, 64, 8, 8]             --
│    └─Empty: 2-603                      [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-604         --                        --
│    └─One: 2-605                        [1]                       --
│    └─OutputScale: 2-606                --                        --
│    └─Empty: 2-607                      [64, 64, 3, 3]            --
│    └─Empty: 2-608                      [64, 64, 3, 3]            --
│    └─Empty: 2-609                      [64]                      --
│    └─Empty: 2-610                      [64]                      --
│    └─BatchNorm2d: 2-611                [8, 64, 8, 8]             --
│    └─Scaler: 2-612                     [8, 64, 8, 8]             --
│    └─ReLU: 2-613                       [8, 64, 8, 8]             --
│    └─Empty: 2-614                      [8, 64, 8, 8]             --
│    └─Clamp: 2-615                      [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-47                [8, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-616         --                        --
│    └─One: 2-617                        [1]                       --
│    └─OutputScale: 2-618                --                        --
│    └─Empty: 2-619                      [64, 64, 1, 1]            --
│    └─Empty: 2-620                      [64, 64, 1, 1]            --
│    └─Empty: 2-621                      [64]                      --
│    └─Empty: 2-622                      [64]                      --
│    └─BatchNorm2d: 2-623                [8, 64, 8, 8]             --
│    └─Scaler: 2-624                     [8, 64, 8, 8]             --
│    └─ReLU: 2-625                       [8, 64, 8, 8]             --
│    └─Empty: 2-626                      [8, 64, 8, 8]             --
│    └─Clamp: 2-627                      [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-48         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-628                  [8, 64, 8, 8]             --
│    └─Empty: 2-629                      [8, 64, 8, 8]             --
│    └─Empty: 2-630                      [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-631         --                        --
│    └─One: 2-632                        [1]                       --
│    └─OutputScale: 2-633                --                        --
│    └─Empty: 2-634                      [64, 64, 3, 3]            --
│    └─Empty: 2-635                      [64, 64, 3, 3]            --
│    └─Empty: 2-636                      [64]                      --
│    └─Empty: 2-637                      [64]                      --
│    └─BatchNorm2d: 2-638                [8, 64, 8, 8]             --
│    └─Scaler: 2-639                     [8, 64, 8, 8]             --
│    └─ReLU: 2-640                       [8, 64, 8, 8]             --
│    └─Empty: 2-641                      [8, 64, 8, 8]             --
│    └─Clamp: 2-642                      [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-49         [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-643                  [8, 64, 4, 4]             --
│    └─Empty: 2-644                      [8, 64, 4, 4]             --
│    └─Empty: 2-645                      [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-646         --                        --
│    └─One: 2-647                        [1]                       --
│    └─OutputScale: 2-648                --                        --
│    └─Empty: 2-649                      [64, 64, 1, 1]            --
│    └─Empty: 2-650                      [64, 64, 1, 1]            --
│    └─Empty: 2-651                      [64]                      --
│    └─Empty: 2-652                      [64]                      --
│    └─BatchNorm2d: 2-653                [8, 64, 4, 4]             --
│    └─Scaler: 2-654                     [8, 64, 4, 4]             --
│    └─ReLU: 2-655                       [8, 64, 4, 4]             --
│    └─Empty: 2-656                      [8, 64, 4, 4]             --
│    └─Clamp: 2-657                      [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-50                [8, 64, 4, 4]             (recursive)
│    └─OutputShiftSqueeze: 2-658         --                        --
│    └─One: 2-659                        [1]                       --
│    └─OutputScale: 2-660                --                        --
│    └─Empty: 2-661                      [64, 64, 1, 1]            --
│    └─Empty: 2-662                      [64, 64, 1, 1]            --
│    └─Empty: 2-663                      [64]                      --
│    └─Empty: 2-664                      [64]                      --
│    └─BatchNorm2d: 2-665                [8, 64, 4, 4]             --
│    └─Scaler: 2-666                     [8, 64, 4, 4]             --
│    └─ReLU: 2-667                       [8, 64, 4, 4]             --
│    └─Empty: 2-668                      [8, 64, 4, 4]             --
│    └─Clamp: 2-669                      [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-51         [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-670                  [8, 64, 4, 4]             --
│    └─Empty: 2-671                      [8, 64, 4, 4]             --
│    └─Empty: 2-672                      [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-673         --                        --
│    └─One: 2-674                        [1]                       --
│    └─OutputScale: 2-675                --                        --
│    └─Empty: 2-676                      [64, 64, 3, 3]            --
│    └─Empty: 2-677                      [64, 64, 3, 3]            --
│    └─Empty: 2-678                      [64]                      --
│    └─Empty: 2-679                      [64]                      --
│    └─BatchNorm2d: 2-680                [8, 64, 4, 4]             --
│    └─Scaler: 2-681                     [8, 64, 4, 4]             --
│    └─ReLU: 2-682                       [8, 64, 4, 4]             --
│    └─Empty: 2-683                      [8, 64, 4, 4]             --
│    └─Clamp: 2-684                      [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-52                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-685         --                        --
│    └─One: 2-686                        [1]                       --
│    └─OutputScale: 2-687                --                        --
│    └─Empty: 2-688                      [64, 12, 1, 1]            --
│    └─Empty: 2-689                      [64, 12, 1, 1]            --
│    └─Empty: 2-690                      [64]                      --
│    └─Empty: 2-691                      [64]                      --
│    └─BatchNorm2d: 2-692                [8, 64, 128, 128]         --
│    └─Scaler: 2-693                     [8, 64, 128, 128]         --
│    └─ReLU: 2-694                       [8, 64, 128, 128]         --
│    └─Empty: 2-695                      [8, 64, 128, 128]         --
│    └─Clamp: 2-696                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-53                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-697         --                        --
│    └─One: 2-698                        [1]                       --
│    └─OutputScale: 2-699                --                        --
│    └─Empty: 2-700                      [64, 64, 3, 3]            --
│    └─Empty: 2-701                      [64, 64, 3, 3]            --
│    └─Empty: 2-702                      [64]                      --
│    └─Empty: 2-703                      [64]                      --
│    └─BatchNorm2d: 2-704                [8, 64, 128, 128]         --
│    └─Scaler: 2-705                     [8, 64, 128, 128]         --
│    └─ReLU: 2-706                       [8, 64, 128, 128]         --
│    └─Empty: 2-707                      [8, 64, 128, 128]         --
│    └─Clamp: 2-708                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-54                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-709         --                        --
│    └─One: 2-710                        [1]                       --
│    └─OutputScale: 2-711                --                        --
│    └─Empty: 2-712                      [64, 64, 1, 1]            --
│    └─Empty: 2-713                      [64, 64, 1, 1]            --
│    └─Empty: 2-714                      [64]                      --
│    └─Empty: 2-715                      [64]                      --
│    └─BatchNorm2d: 2-716                [8, 64, 128, 128]         --
│    └─Scaler: 2-717                     [8, 64, 128, 128]         --
│    └─ReLU: 2-718                       [8, 64, 128, 128]         --
│    └─Empty: 2-719                      [8, 64, 128, 128]         --
│    └─Clamp: 2-720                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-55                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-721         --                        --
│    └─One: 2-722                        [1]                       --
│    └─OutputScale: 2-723                --                        --
│    └─Empty: 2-724                      [64, 64, 3, 3]            --
│    └─Empty: 2-725                      [64, 64, 3, 3]            --
│    └─Empty: 2-726                      [64]                      --
│    └─Empty: 2-727                      [64]                      --
│    └─BatchNorm2d: 2-728                [8, 64, 128, 128]         --
│    └─Scaler: 2-729                     [8, 64, 128, 128]         --
│    └─ReLU: 2-730                       [8, 64, 128, 128]         --
│    └─Empty: 2-731                      [8, 64, 128, 128]         --
│    └─Clamp: 2-732                      [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-56         [8, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-733                  [8, 64, 64, 64]           --
│    └─Empty: 2-734                      [8, 64, 64, 64]           --
│    └─Empty: 2-735                      [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-736         --                        --
│    └─One: 2-737                        [1]                       --
│    └─OutputScale: 2-738                --                        --
│    └─Empty: 2-739                      [64, 64, 3, 3]            --
│    └─Empty: 2-740                      [64, 64, 3, 3]            --
│    └─Empty: 2-741                      [64]                      --
│    └─Empty: 2-742                      [64]                      --
│    └─BatchNorm2d: 2-743                [8, 64, 64, 64]           --
│    └─Scaler: 2-744                     [8, 64, 64, 64]           --
│    └─ReLU: 2-745                       [8, 64, 64, 64]           --
│    └─Empty: 2-746                      [8, 64, 64, 64]           --
│    └─Clamp: 2-747                      [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-57                [8, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-748         --                        --
│    └─One: 2-749                        [1]                       --
│    └─OutputScale: 2-750                --                        --
│    └─Empty: 2-751                      [64, 64, 3, 3]            --
│    └─Empty: 2-752                      [64, 64, 3, 3]            --
│    └─Empty: 2-753                      [64]                      --
│    └─Empty: 2-754                      [64]                      --
│    └─BatchNorm2d: 2-755                [8, 64, 64, 64]           --
│    └─Scaler: 2-756                     [8, 64, 64, 64]           --
│    └─ReLU: 2-757                       [8, 64, 64, 64]           --
│    └─Empty: 2-758                      [8, 64, 64, 64]           --
│    └─Clamp: 2-759                      [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-58         [8, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-760                  [8, 64, 32, 32]           --
│    └─Empty: 2-761                      [8, 64, 32, 32]           --
│    └─Empty: 2-762                      [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-763         --                        --
│    └─One: 2-764                        [1]                       --
│    └─OutputScale: 2-765                --                        --
│    └─Empty: 2-766                      [64, 64, 3, 3]            --
│    └─Empty: 2-767                      [64, 64, 3, 3]            --
│    └─Empty: 2-768                      [64]                      --
│    └─Empty: 2-769                      [64]                      --
│    └─BatchNorm2d: 2-770                [8, 64, 32, 32]           --
│    └─Scaler: 2-771                     [8, 64, 32, 32]           --
│    └─ReLU: 2-772                       [8, 64, 32, 32]           --
│    └─Empty: 2-773                      [8, 64, 32, 32]           --
│    └─Clamp: 2-774                      [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-59                [8, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-775         --                        --
│    └─One: 2-776                        [1]                       --
│    └─OutputScale: 2-777                --                        --
│    └─Empty: 2-778                      [64, 64, 3, 3]            --
│    └─Empty: 2-779                      [64, 64, 3, 3]            --
│    └─Empty: 2-780                      [64]                      --
│    └─Empty: 2-781                      [64]                      --
│    └─BatchNorm2d: 2-782                [8, 64, 32, 32]           --
│    └─Scaler: 2-783                     [8, 64, 32, 32]           --
│    └─ReLU: 2-784                       [8, 64, 32, 32]           --
│    └─Empty: 2-785                      [8, 64, 32, 32]           --
│    └─Clamp: 2-786                      [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-60         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-787                  [8, 64, 16, 16]           --
│    └─Empty: 2-788                      [8, 64, 16, 16]           --
│    └─Empty: 2-789                      [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-790         --                        --
│    └─One: 2-791                        [1]                       --
│    └─OutputScale: 2-792                --                        --
│    └─Empty: 2-793                      [64, 64, 3, 3]            --
│    └─Empty: 2-794                      [64, 64, 3, 3]            --
│    └─Empty: 2-795                      [64]                      --
│    └─Empty: 2-796                      [64]                      --
│    └─BatchNorm2d: 2-797                [8, 64, 16, 16]           --
│    └─Scaler: 2-798                     [8, 64, 16, 16]           --
│    └─ReLU: 2-799                       [8, 64, 16, 16]           --
│    └─Empty: 2-800                      [8, 64, 16, 16]           --
│    └─Clamp: 2-801                      [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-61                [8, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-802         --                        --
│    └─One: 2-803                        [1]                       --
│    └─OutputScale: 2-804                --                        --
│    └─Empty: 2-805                      [64, 64, 1, 1]            --
│    └─Empty: 2-806                      [64, 64, 1, 1]            --
│    └─Empty: 2-807                      [64]                      --
│    └─Empty: 2-808                      [64]                      --
│    └─BatchNorm2d: 2-809                [8, 64, 16, 16]           --
│    └─Scaler: 2-810                     [8, 64, 16, 16]           --
│    └─ReLU: 2-811                       [8, 64, 16, 16]           --
│    └─Empty: 2-812                      [8, 64, 16, 16]           --
│    └─Clamp: 2-813                      [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-62         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-814                  [8, 64, 16, 16]           --
│    └─Empty: 2-815                      [8, 64, 16, 16]           --
│    └─Empty: 2-816                      [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-817         --                        --
│    └─One: 2-818                        [1]                       --
│    └─OutputScale: 2-819                --                        --
│    └─Empty: 2-820                      [64, 64, 3, 3]            --
│    └─Empty: 2-821                      [64, 64, 3, 3]            --
│    └─Empty: 2-822                      [64]                      --
│    └─Empty: 2-823                      [64]                      --
│    └─BatchNorm2d: 2-824                [8, 64, 16, 16]           --
│    └─Scaler: 2-825                     [8, 64, 16, 16]           --
│    └─ReLU: 2-826                       [8, 64, 16, 16]           --
│    └─Empty: 2-827                      [8, 64, 16, 16]           --
│    └─Clamp: 2-828                      [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-63         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-829                  [8, 64, 8, 8]             --
│    └─Empty: 2-830                      [8, 64, 8, 8]             --
│    └─Empty: 2-831                      [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-832         --                        --
│    └─One: 2-833                        [1]                       --
│    └─OutputScale: 2-834                --                        --
│    └─Empty: 2-835                      [64, 64, 3, 3]            --
│    └─Empty: 2-836                      [64, 64, 3, 3]            --
│    └─Empty: 2-837                      [64]                      --
│    └─Empty: 2-838                      [64]                      --
│    └─BatchNorm2d: 2-839                [8, 64, 8, 8]             --
│    └─Scaler: 2-840                     [8, 64, 8, 8]             --
│    └─ReLU: 2-841                       [8, 64, 8, 8]             --
│    └─Empty: 2-842                      [8, 64, 8, 8]             --
│    └─Clamp: 2-843                      [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-64                [8, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-844         --                        --
│    └─One: 2-845                        [1]                       --
│    └─OutputScale: 2-846                --                        --
│    └─Empty: 2-847                      [64, 64, 1, 1]            --
│    └─Empty: 2-848                      [64, 64, 1, 1]            --
│    └─Empty: 2-849                      [64]                      --
│    └─Empty: 2-850                      [64]                      --
│    └─BatchNorm2d: 2-851                [8, 64, 8, 8]             --
│    └─Scaler: 2-852                     [8, 64, 8, 8]             --
│    └─ReLU: 2-853                       [8, 64, 8, 8]             --
│    └─Empty: 2-854                      [8, 64, 8, 8]             --
│    └─Clamp: 2-855                      [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-65         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-856                  [8, 64, 8, 8]             --
│    └─Empty: 2-857                      [8, 64, 8, 8]             --
│    └─Empty: 2-858                      [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-859         --                        --
│    └─One: 2-860                        [1]                       --
│    └─OutputScale: 2-861                --                        --
│    └─Empty: 2-862                      [64, 64, 3, 3]            --
│    └─Empty: 2-863                      [64, 64, 3, 3]            --
│    └─Empty: 2-864                      [64]                      --
│    └─Empty: 2-865                      [64]                      --
│    └─BatchNorm2d: 2-866                [8, 64, 8, 8]             --
│    └─Scaler: 2-867                     [8, 64, 8, 8]             --
│    └─ReLU: 2-868                       [8, 64, 8, 8]             --
│    └─Empty: 2-869                      [8, 64, 8, 8]             --
│    └─Clamp: 2-870                      [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-66         [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-871                  [8, 64, 4, 4]             --
│    └─Empty: 2-872                      [8, 64, 4, 4]             --
│    └─Empty: 2-873                      [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-874         --                        --
│    └─One: 2-875                        [1]                       --
│    └─OutputScale: 2-876                --                        --
│    └─Empty: 2-877                      [64, 64, 1, 1]            --
│    └─Empty: 2-878                      [64, 64, 1, 1]            --
│    └─Empty: 2-879                      [64]                      --
│    └─Empty: 2-880                      [64]                      --
│    └─BatchNorm2d: 2-881                [8, 64, 4, 4]             --
│    └─Scaler: 2-882                     [8, 64, 4, 4]             --
│    └─ReLU: 2-883                       [8, 64, 4, 4]             --
│    └─Empty: 2-884                      [8, 64, 4, 4]             --
│    └─Clamp: 2-885                      [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-67                [8, 64, 4, 4]             (recursive)
│    └─OutputShiftSqueeze: 2-886         --                        --
│    └─One: 2-887                        [1]                       --
│    └─OutputScale: 2-888                --                        --
│    └─Empty: 2-889                      [64, 64, 1, 1]            --
│    └─Empty: 2-890                      [64, 64, 1, 1]            --
│    └─Empty: 2-891                      [64]                      --
│    └─Empty: 2-892                      [64]                      --
│    └─BatchNorm2d: 2-893                [8, 64, 4, 4]             --
│    └─Scaler: 2-894                     [8, 64, 4, 4]             --
│    └─ReLU: 2-895                       [8, 64, 4, 4]             --
│    └─Empty: 2-896                      [8, 64, 4, 4]             --
│    └─Clamp: 2-897                      [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-68         [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-898                  [8, 64, 4, 4]             --
│    └─Empty: 2-899                      [8, 64, 4, 4]             --
│    └─Empty: 2-900                      [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-901         --                        --
│    └─One: 2-902                        [1]                       --
│    └─OutputScale: 2-903                --                        --
│    └─Empty: 2-904                      [64, 64, 3, 3]            --
│    └─Empty: 2-905                      [64, 64, 3, 3]            --
│    └─Empty: 2-906                      [64]                      --
│    └─Empty: 2-907                      [64]                      --
│    └─BatchNorm2d: 2-908                [8, 64, 4, 4]             --
│    └─Scaler: 2-909                     [8, 64, 4, 4]             --
│    └─ReLU: 2-910                       [8, 64, 4, 4]             --
│    └─Empty: 2-911                      [8, 64, 4, 4]             --
│    └─Clamp: 2-912                      [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-69                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-913         --                        --
│    └─One: 2-914                        [1]                       --
│    └─OutputScale: 2-915                --                        --
│    └─Empty: 2-916                      [64, 12, 1, 1]            --
│    └─Empty: 2-917                      [64, 12, 1, 1]            --
│    └─Empty: 2-918                      [64]                      --
│    └─Empty: 2-919                      [64]                      --
│    └─BatchNorm2d: 2-920                [8, 64, 128, 128]         --
│    └─Scaler: 2-921                     [8, 64, 128, 128]         --
│    └─ReLU: 2-922                       [8, 64, 128, 128]         --
│    └─Empty: 2-923                      [8, 64, 128, 128]         --
│    └─Clamp: 2-924                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-70                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-925         --                        --
│    └─One: 2-926                        [1]                       --
│    └─OutputScale: 2-927                --                        --
│    └─Empty: 2-928                      [64, 64, 3, 3]            --
│    └─Empty: 2-929                      [64, 64, 3, 3]            --
│    └─Empty: 2-930                      [64]                      --
│    └─Empty: 2-931                      [64]                      --
│    └─BatchNorm2d: 2-932                [8, 64, 128, 128]         --
│    └─Scaler: 2-933                     [8, 64, 128, 128]         --
│    └─ReLU: 2-934                       [8, 64, 128, 128]         --
│    └─Empty: 2-935                      [8, 64, 128, 128]         --
│    └─Clamp: 2-936                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-71                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-937         --                        --
│    └─One: 2-938                        [1]                       --
│    └─OutputScale: 2-939                --                        --
│    └─Empty: 2-940                      [64, 64, 1, 1]            --
│    └─Empty: 2-941                      [64, 64, 1, 1]            --
│    └─Empty: 2-942                      [64]                      --
│    └─Empty: 2-943                      [64]                      --
│    └─BatchNorm2d: 2-944                [8, 64, 128, 128]         --
│    └─Scaler: 2-945                     [8, 64, 128, 128]         --
│    └─ReLU: 2-946                       [8, 64, 128, 128]         --
│    └─Empty: 2-947                      [8, 64, 128, 128]         --
│    └─Clamp: 2-948                      [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-72                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-949         --                        --
│    └─One: 2-950                        [1]                       --
│    └─OutputScale: 2-951                --                        --
│    └─Empty: 2-952                      [64, 64, 3, 3]            --
│    └─Empty: 2-953                      [64, 64, 3, 3]            --
│    └─Empty: 2-954                      [64]                      --
│    └─Empty: 2-955                      [64]                      --
│    └─BatchNorm2d: 2-956                [8, 64, 128, 128]         --
│    └─Scaler: 2-957                     [8, 64, 128, 128]         --
│    └─ReLU: 2-958                       [8, 64, 128, 128]         --
│    └─Empty: 2-959                      [8, 64, 128, 128]         --
│    └─Clamp: 2-960                      [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-73         [8, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-961                  [8, 64, 64, 64]           --
│    └─Empty: 2-962                      [8, 64, 64, 64]           --
│    └─Empty: 2-963                      [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-964         --                        --
│    └─One: 2-965                        [1]                       --
│    └─OutputScale: 2-966                --                        --
│    └─Empty: 2-967                      [64, 64, 3, 3]            --
│    └─Empty: 2-968                      [64, 64, 3, 3]            --
│    └─Empty: 2-969                      [64]                      --
│    └─Empty: 2-970                      [64]                      --
│    └─BatchNorm2d: 2-971                [8, 64, 64, 64]           --
│    └─Scaler: 2-972                     [8, 64, 64, 64]           --
│    └─ReLU: 2-973                       [8, 64, 64, 64]           --
│    └─Empty: 2-974                      [8, 64, 64, 64]           --
│    └─Clamp: 2-975                      [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-74                [8, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-976         --                        --
│    └─One: 2-977                        [1]                       --
│    └─OutputScale: 2-978                --                        --
│    └─Empty: 2-979                      [64, 64, 3, 3]            --
│    └─Empty: 2-980                      [64, 64, 3, 3]            --
│    └─Empty: 2-981                      [64]                      --
│    └─Empty: 2-982                      [64]                      --
│    └─BatchNorm2d: 2-983                [8, 64, 64, 64]           --
│    └─Scaler: 2-984                     [8, 64, 64, 64]           --
│    └─ReLU: 2-985                       [8, 64, 64, 64]           --
│    └─Empty: 2-986                      [8, 64, 64, 64]           --
│    └─Clamp: 2-987                      [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-75         [8, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-988                  [8, 64, 32, 32]           --
│    └─Empty: 2-989                      [8, 64, 32, 32]           --
│    └─Empty: 2-990                      [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-991         --                        --
│    └─One: 2-992                        [1]                       --
│    └─OutputScale: 2-993                --                        --
│    └─Empty: 2-994                      [64, 64, 3, 3]            --
│    └─Empty: 2-995                      [64, 64, 3, 3]            --
│    └─Empty: 2-996                      [64]                      --
│    └─Empty: 2-997                      [64]                      --
│    └─BatchNorm2d: 2-998                [8, 64, 32, 32]           --
│    └─Scaler: 2-999                     [8, 64, 32, 32]           --
│    └─ReLU: 2-1000                      [8, 64, 32, 32]           --
│    └─Empty: 2-1001                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1002                     [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-76                [8, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1003        --                        --
│    └─One: 2-1004                       [1]                       --
│    └─OutputScale: 2-1005               --                        --
│    └─Empty: 2-1006                     [64, 64, 3, 3]            --
│    └─Empty: 2-1007                     [64, 64, 3, 3]            --
│    └─Empty: 2-1008                     [64]                      --
│    └─Empty: 2-1009                     [64]                      --
│    └─BatchNorm2d: 2-1010               [8, 64, 32, 32]           --
│    └─Scaler: 2-1011                    [8, 64, 32, 32]           --
│    └─ReLU: 2-1012                      [8, 64, 32, 32]           --
│    └─Empty: 2-1013                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1014                     [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-77         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1015                 [8, 64, 16, 16]           --
│    └─Empty: 2-1016                     [8, 64, 16, 16]           --
│    └─Empty: 2-1017                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1018        --                        --
│    └─One: 2-1019                       [1]                       --
│    └─OutputScale: 2-1020               --                        --
│    └─Empty: 2-1021                     [64, 64, 3, 3]            --
│    └─Empty: 2-1022                     [64, 64, 3, 3]            --
│    └─Empty: 2-1023                     [64]                      --
│    └─Empty: 2-1024                     [64]                      --
│    └─BatchNorm2d: 2-1025               [8, 64, 16, 16]           --
│    └─Scaler: 2-1026                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1027                      [8, 64, 16, 16]           --
│    └─Empty: 2-1028                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1029                     [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-78                [8, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1030        --                        --
│    └─One: 2-1031                       [1]                       --
│    └─OutputScale: 2-1032               --                        --
│    └─Empty: 2-1033                     [64, 64, 1, 1]            --
│    └─Empty: 2-1034                     [64, 64, 1, 1]            --
│    └─Empty: 2-1035                     [64]                      --
│    └─Empty: 2-1036                     [64]                      --
│    └─BatchNorm2d: 2-1037               [8, 64, 16, 16]           --
│    └─Scaler: 2-1038                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1039                      [8, 64, 16, 16]           --
│    └─Empty: 2-1040                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1041                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-79         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1042                 [8, 64, 16, 16]           --
│    └─Empty: 2-1043                     [8, 64, 16, 16]           --
│    └─Empty: 2-1044                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1045        --                        --
│    └─One: 2-1046                       [1]                       --
│    └─OutputScale: 2-1047               --                        --
│    └─Empty: 2-1048                     [64, 64, 3, 3]            --
│    └─Empty: 2-1049                     [64, 64, 3, 3]            --
│    └─Empty: 2-1050                     [64]                      --
│    └─Empty: 2-1051                     [64]                      --
│    └─BatchNorm2d: 2-1052               [8, 64, 16, 16]           --
│    └─Scaler: 2-1053                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1054                      [8, 64, 16, 16]           --
│    └─Empty: 2-1055                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1056                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-80         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1057                 [8, 64, 8, 8]             --
│    └─Empty: 2-1058                     [8, 64, 8, 8]             --
│    └─Empty: 2-1059                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1060        --                        --
│    └─One: 2-1061                       [1]                       --
│    └─OutputScale: 2-1062               --                        --
│    └─Empty: 2-1063                     [64, 64, 3, 3]            --
│    └─Empty: 2-1064                     [64, 64, 3, 3]            --
│    └─Empty: 2-1065                     [64]                      --
│    └─Empty: 2-1066                     [64]                      --
│    └─BatchNorm2d: 2-1067               [8, 64, 8, 8]             --
│    └─Scaler: 2-1068                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1069                      [8, 64, 8, 8]             --
│    └─Empty: 2-1070                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1071                     [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-81                [8, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-1072        --                        --
│    └─One: 2-1073                       [1]                       --
│    └─OutputScale: 2-1074               --                        --
│    └─Empty: 2-1075                     [64, 64, 1, 1]            --
│    └─Empty: 2-1076                     [64, 64, 1, 1]            --
│    └─Empty: 2-1077                     [64]                      --
│    └─Empty: 2-1078                     [64]                      --
│    └─BatchNorm2d: 2-1079               [8, 64, 8, 8]             --
│    └─Scaler: 2-1080                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1081                      [8, 64, 8, 8]             --
│    └─Empty: 2-1082                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1083                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-82         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1084                 [8, 64, 8, 8]             --
│    └─Empty: 2-1085                     [8, 64, 8, 8]             --
│    └─Empty: 2-1086                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1087        --                        --
│    └─One: 2-1088                       [1]                       --
│    └─OutputScale: 2-1089               --                        --
│    └─Empty: 2-1090                     [64, 64, 3, 3]            --
│    └─Empty: 2-1091                     [64, 64, 3, 3]            --
│    └─Empty: 2-1092                     [64]                      --
│    └─Empty: 2-1093                     [64]                      --
│    └─BatchNorm2d: 2-1094               [8, 64, 8, 8]             --
│    └─Scaler: 2-1095                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1096                      [8, 64, 8, 8]             --
│    └─Empty: 2-1097                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1098                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-83         [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-1099                 [8, 64, 4, 4]             --
│    └─Empty: 2-1100                     [8, 64, 4, 4]             --
│    └─Empty: 2-1101                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-1102        --                        --
│    └─One: 2-1103                       [1]                       --
│    └─OutputScale: 2-1104               --                        --
│    └─Empty: 2-1105                     [64, 64, 1, 1]            --
│    └─Empty: 2-1106                     [64, 64, 1, 1]            --
│    └─Empty: 2-1107                     [64]                      --
│    └─Empty: 2-1108                     [64]                      --
│    └─BatchNorm2d: 2-1109               [8, 64, 4, 4]             --
│    └─Scaler: 2-1110                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1111                      [8, 64, 4, 4]             --
│    └─Empty: 2-1112                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1113                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-84                [8, 64, 4, 4]             (recursive)
│    └─OutputShiftSqueeze: 2-1114        --                        --
│    └─One: 2-1115                       [1]                       --
│    └─OutputScale: 2-1116               --                        --
│    └─Empty: 2-1117                     [64, 64, 1, 1]            --
│    └─Empty: 2-1118                     [64, 64, 1, 1]            --
│    └─Empty: 2-1119                     [64]                      --
│    └─Empty: 2-1120                     [64]                      --
│    └─BatchNorm2d: 2-1121               [8, 64, 4, 4]             --
│    └─Scaler: 2-1122                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1123                      [8, 64, 4, 4]             --
│    └─Empty: 2-1124                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1125                     [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-85         [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-1126                 [8, 64, 4, 4]             --
│    └─Empty: 2-1127                     [8, 64, 4, 4]             --
│    └─Empty: 2-1128                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-1129        --                        --
│    └─One: 2-1130                       [1]                       --
│    └─OutputScale: 2-1131               --                        --
│    └─Empty: 2-1132                     [64, 64, 3, 3]            --
│    └─Empty: 2-1133                     [64, 64, 3, 3]            --
│    └─Empty: 2-1134                     [64]                      --
│    └─Empty: 2-1135                     [64]                      --
│    └─BatchNorm2d: 2-1136               [8, 64, 4, 4]             --
│    └─Scaler: 2-1137                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1138                      [8, 64, 4, 4]             --
│    └─Empty: 2-1139                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1140                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-86                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1141        --                        --
│    └─One: 2-1142                       [1]                       --
│    └─OutputScale: 2-1143               --                        --
│    └─Empty: 2-1144                     [64, 12, 1, 1]            --
│    └─Empty: 2-1145                     [64, 12, 1, 1]            --
│    └─Empty: 2-1146                     [64]                      --
│    └─Empty: 2-1147                     [64]                      --
│    └─BatchNorm2d: 2-1148               [8, 64, 128, 128]         --
│    └─Scaler: 2-1149                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1150                      [8, 64, 128, 128]         --
│    └─Empty: 2-1151                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1152                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-87                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1153        --                        --
│    └─One: 2-1154                       [1]                       --
│    └─OutputScale: 2-1155               --                        --
│    └─Empty: 2-1156                     [64, 64, 3, 3]            --
│    └─Empty: 2-1157                     [64, 64, 3, 3]            --
│    └─Empty: 2-1158                     [64]                      --
│    └─Empty: 2-1159                     [64]                      --
│    └─BatchNorm2d: 2-1160               [8, 64, 128, 128]         --
│    └─Scaler: 2-1161                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1162                      [8, 64, 128, 128]         --
│    └─Empty: 2-1163                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1164                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-88                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1165        --                        --
│    └─One: 2-1166                       [1]                       --
│    └─OutputScale: 2-1167               --                        --
│    └─Empty: 2-1168                     [64, 64, 1, 1]            --
│    └─Empty: 2-1169                     [64, 64, 1, 1]            --
│    └─Empty: 2-1170                     [64]                      --
│    └─Empty: 2-1171                     [64]                      --
│    └─BatchNorm2d: 2-1172               [8, 64, 128, 128]         --
│    └─Scaler: 2-1173                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1174                      [8, 64, 128, 128]         --
│    └─Empty: 2-1175                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1176                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-89                [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1177        --                        --
│    └─One: 2-1178                       [1]                       --
│    └─OutputScale: 2-1179               --                        --
│    └─Empty: 2-1180                     [64, 64, 3, 3]            --
│    └─Empty: 2-1181                     [64, 64, 3, 3]            --
│    └─Empty: 2-1182                     [64]                      --
│    └─Empty: 2-1183                     [64]                      --
│    └─BatchNorm2d: 2-1184               [8, 64, 128, 128]         --
│    └─Scaler: 2-1185                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1186                      [8, 64, 128, 128]         --
│    └─Empty: 2-1187                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1188                     [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-90         [8, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-1189                 [8, 64, 64, 64]           --
│    └─Empty: 2-1190                     [8, 64, 64, 64]           --
│    └─Empty: 2-1191                     [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-1192        --                        --
│    └─One: 2-1193                       [1]                       --
│    └─OutputScale: 2-1194               --                        --
│    └─Empty: 2-1195                     [64, 64, 3, 3]            --
│    └─Empty: 2-1196                     [64, 64, 3, 3]            --
│    └─Empty: 2-1197                     [64]                      --
│    └─Empty: 2-1198                     [64]                      --
│    └─BatchNorm2d: 2-1199               [8, 64, 64, 64]           --
│    └─Scaler: 2-1200                    [8, 64, 64, 64]           --
│    └─ReLU: 2-1201                      [8, 64, 64, 64]           --
│    └─Empty: 2-1202                     [8, 64, 64, 64]           --
│    └─Clamp: 2-1203                     [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-91                [8, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-1204        --                        --
│    └─One: 2-1205                       [1]                       --
│    └─OutputScale: 2-1206               --                        --
│    └─Empty: 2-1207                     [64, 64, 3, 3]            --
│    └─Empty: 2-1208                     [64, 64, 3, 3]            --
│    └─Empty: 2-1209                     [64]                      --
│    └─Empty: 2-1210                     [64]                      --
│    └─BatchNorm2d: 2-1211               [8, 64, 64, 64]           --
│    └─Scaler: 2-1212                    [8, 64, 64, 64]           --
│    └─ReLU: 2-1213                      [8, 64, 64, 64]           --
│    └─Empty: 2-1214                     [8, 64, 64, 64]           --
│    └─Clamp: 2-1215                     [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-92         [8, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1216                 [8, 64, 32, 32]           --
│    └─Empty: 2-1217                     [8, 64, 32, 32]           --
│    └─Empty: 2-1218                     [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1219        --                        --
│    └─One: 2-1220                       [1]                       --
│    └─OutputScale: 2-1221               --                        --
│    └─Empty: 2-1222                     [64, 64, 3, 3]            --
│    └─Empty: 2-1223                     [64, 64, 3, 3]            --
│    └─Empty: 2-1224                     [64]                      --
│    └─Empty: 2-1225                     [64]                      --
│    └─BatchNorm2d: 2-1226               [8, 64, 32, 32]           --
│    └─Scaler: 2-1227                    [8, 64, 32, 32]           --
│    └─ReLU: 2-1228                      [8, 64, 32, 32]           --
│    └─Empty: 2-1229                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1230                     [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-93                [8, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1231        --                        --
│    └─One: 2-1232                       [1]                       --
│    └─OutputScale: 2-1233               --                        --
│    └─Empty: 2-1234                     [64, 64, 3, 3]            --
│    └─Empty: 2-1235                     [64, 64, 3, 3]            --
│    └─Empty: 2-1236                     [64]                      --
│    └─Empty: 2-1237                     [64]                      --
│    └─BatchNorm2d: 2-1238               [8, 64, 32, 32]           --
│    └─Scaler: 2-1239                    [8, 64, 32, 32]           --
│    └─ReLU: 2-1240                      [8, 64, 32, 32]           --
│    └─Empty: 2-1241                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1242                     [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-94         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1243                 [8, 64, 16, 16]           --
│    └─Empty: 2-1244                     [8, 64, 16, 16]           --
│    └─Empty: 2-1245                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1246        --                        --
│    └─One: 2-1247                       [1]                       --
│    └─OutputScale: 2-1248               --                        --
│    └─Empty: 2-1249                     [64, 64, 3, 3]            --
│    └─Empty: 2-1250                     [64, 64, 3, 3]            --
│    └─Empty: 2-1251                     [64]                      --
│    └─Empty: 2-1252                     [64]                      --
│    └─BatchNorm2d: 2-1253               [8, 64, 16, 16]           --
│    └─Scaler: 2-1254                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1255                      [8, 64, 16, 16]           --
│    └─Empty: 2-1256                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1257                     [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-95                [8, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1258        --                        --
│    └─One: 2-1259                       [1]                       --
│    └─OutputScale: 2-1260               --                        --
│    └─Empty: 2-1261                     [64, 64, 1, 1]            --
│    └─Empty: 2-1262                     [64, 64, 1, 1]            --
│    └─Empty: 2-1263                     [64]                      --
│    └─Empty: 2-1264                     [64]                      --
│    └─BatchNorm2d: 2-1265               [8, 64, 16, 16]           --
│    └─Scaler: 2-1266                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1267                      [8, 64, 16, 16]           --
│    └─Empty: 2-1268                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1269                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-96         [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1270                 [8, 64, 16, 16]           --
│    └─Empty: 2-1271                     [8, 64, 16, 16]           --
│    └─Empty: 2-1272                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1273        --                        --
│    └─One: 2-1274                       [1]                       --
│    └─OutputScale: 2-1275               --                        --
│    └─Empty: 2-1276                     [64, 64, 3, 3]            --
│    └─Empty: 2-1277                     [64, 64, 3, 3]            --
│    └─Empty: 2-1278                     [64]                      --
│    └─Empty: 2-1279                     [64]                      --
│    └─BatchNorm2d: 2-1280               [8, 64, 16, 16]           --
│    └─Scaler: 2-1281                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1282                      [8, 64, 16, 16]           --
│    └─Empty: 2-1283                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1284                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-97         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1285                 [8, 64, 8, 8]             --
│    └─Empty: 2-1286                     [8, 64, 8, 8]             --
│    └─Empty: 2-1287                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1288        --                        --
│    └─One: 2-1289                       [1]                       --
│    └─OutputScale: 2-1290               --                        --
│    └─Empty: 2-1291                     [64, 64, 3, 3]            --
│    └─Empty: 2-1292                     [64, 64, 3, 3]            --
│    └─Empty: 2-1293                     [64]                      --
│    └─Empty: 2-1294                     [64]                      --
│    └─BatchNorm2d: 2-1295               [8, 64, 8, 8]             --
│    └─Scaler: 2-1296                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1297                      [8, 64, 8, 8]             --
│    └─Empty: 2-1298                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1299                     [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-98                [8, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-1300        --                        --
│    └─One: 2-1301                       [1]                       --
│    └─OutputScale: 2-1302               --                        --
│    └─Empty: 2-1303                     [64, 64, 1, 1]            --
│    └─Empty: 2-1304                     [64, 64, 1, 1]            --
│    └─Empty: 2-1305                     [64]                      --
│    └─Empty: 2-1306                     [64]                      --
│    └─BatchNorm2d: 2-1307               [8, 64, 8, 8]             --
│    └─Scaler: 2-1308                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1309                      [8, 64, 8, 8]             --
│    └─Empty: 2-1310                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1311                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-99         [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1312                 [8, 64, 8, 8]             --
│    └─Empty: 2-1313                     [8, 64, 8, 8]             --
│    └─Empty: 2-1314                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1315        --                        --
│    └─One: 2-1316                       [1]                       --
│    └─OutputScale: 2-1317               --                        --
│    └─Empty: 2-1318                     [64, 64, 3, 3]            --
│    └─Empty: 2-1319                     [64, 64, 3, 3]            --
│    └─Empty: 2-1320                     [64]                      --
│    └─Empty: 2-1321                     [64]                      --
│    └─BatchNorm2d: 2-1322               [8, 64, 8, 8]             --
│    └─Scaler: 2-1323                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1324                      [8, 64, 8, 8]             --
│    └─Empty: 2-1325                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1326                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-100        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-1327                 [8, 64, 4, 4]             --
│    └─Empty: 2-1328                     [8, 64, 4, 4]             --
│    └─Empty: 2-1329                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-1330        --                        --
│    └─One: 2-1331                       [1]                       --
│    └─OutputScale: 2-1332               --                        --
│    └─Empty: 2-1333                     [64, 64, 1, 1]            --
│    └─Empty: 2-1334                     [64, 64, 1, 1]            --
│    └─Empty: 2-1335                     [64]                      --
│    └─Empty: 2-1336                     [64]                      --
│    └─BatchNorm2d: 2-1337               [8, 64, 4, 4]             --
│    └─Scaler: 2-1338                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1339                      [8, 64, 4, 4]             --
│    └─Empty: 2-1340                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1341                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-101               [8, 64, 4, 4]             (recursive)
│    └─OutputShiftSqueeze: 2-1342        --                        --
│    └─One: 2-1343                       [1]                       --
│    └─OutputScale: 2-1344               --                        --
│    └─Empty: 2-1345                     [64, 64, 1, 1]            --
│    └─Empty: 2-1346                     [64, 64, 1, 1]            --
│    └─Empty: 2-1347                     [64]                      --
│    └─Empty: 2-1348                     [64]                      --
│    └─BatchNorm2d: 2-1349               [8, 64, 4, 4]             --
│    └─Scaler: 2-1350                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1351                      [8, 64, 4, 4]             --
│    └─Empty: 2-1352                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1353                     [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-102        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-1354                 [8, 64, 4, 4]             --
│    └─Empty: 2-1355                     [8, 64, 4, 4]             --
│    └─Empty: 2-1356                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-1357        --                        --
│    └─One: 2-1358                       [1]                       --
│    └─OutputScale: 2-1359               --                        --
│    └─Empty: 2-1360                     [64, 64, 3, 3]            --
│    └─Empty: 2-1361                     [64, 64, 3, 3]            --
│    └─Empty: 2-1362                     [64]                      --
│    └─Empty: 2-1363                     [64]                      --
│    └─BatchNorm2d: 2-1364               [8, 64, 4, 4]             --
│    └─Scaler: 2-1365                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1366                      [8, 64, 4, 4]             --
│    └─Empty: 2-1367                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1368                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-103               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1369        --                        --
│    └─One: 2-1370                       [1]                       --
│    └─OutputScale: 2-1371               --                        --
│    └─Empty: 2-1372                     [64, 12, 1, 1]            --
│    └─Empty: 2-1373                     [64, 12, 1, 1]            --
│    └─Empty: 2-1374                     [64]                      --
│    └─Empty: 2-1375                     [64]                      --
│    └─BatchNorm2d: 2-1376               [8, 64, 128, 128]         --
│    └─Scaler: 2-1377                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1378                      [8, 64, 128, 128]         --
│    └─Empty: 2-1379                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1380                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-104               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1381        --                        --
│    └─One: 2-1382                       [1]                       --
│    └─OutputScale: 2-1383               --                        --
│    └─Empty: 2-1384                     [64, 64, 3, 3]            --
│    └─Empty: 2-1385                     [64, 64, 3, 3]            --
│    └─Empty: 2-1386                     [64]                      --
│    └─Empty: 2-1387                     [64]                      --
│    └─BatchNorm2d: 2-1388               [8, 64, 128, 128]         --
│    └─Scaler: 2-1389                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1390                      [8, 64, 128, 128]         --
│    └─Empty: 2-1391                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1392                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-105               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1393        --                        --
│    └─One: 2-1394                       [1]                       --
│    └─OutputScale: 2-1395               --                        --
│    └─Empty: 2-1396                     [64, 64, 1, 1]            --
│    └─Empty: 2-1397                     [64, 64, 1, 1]            --
│    └─Empty: 2-1398                     [64]                      --
│    └─Empty: 2-1399                     [64]                      --
│    └─BatchNorm2d: 2-1400               [8, 64, 128, 128]         --
│    └─Scaler: 2-1401                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1402                      [8, 64, 128, 128]         --
│    └─Empty: 2-1403                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1404                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-106               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1405        --                        --
│    └─One: 2-1406                       [1]                       --
│    └─OutputScale: 2-1407               --                        --
│    └─Empty: 2-1408                     [64, 64, 3, 3]            --
│    └─Empty: 2-1409                     [64, 64, 3, 3]            --
│    └─Empty: 2-1410                     [64]                      --
│    └─Empty: 2-1411                     [64]                      --
│    └─BatchNorm2d: 2-1412               [8, 64, 128, 128]         --
│    └─Scaler: 2-1413                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1414                      [8, 64, 128, 128]         --
│    └─Empty: 2-1415                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1416                     [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-107        [8, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-1417                 [8, 64, 64, 64]           --
│    └─Empty: 2-1418                     [8, 64, 64, 64]           --
│    └─Empty: 2-1419                     [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-1420        --                        --
│    └─One: 2-1421                       [1]                       --
│    └─OutputScale: 2-1422               --                        --
│    └─Empty: 2-1423                     [64, 64, 3, 3]            --
│    └─Empty: 2-1424                     [64, 64, 3, 3]            --
│    └─Empty: 2-1425                     [64]                      --
│    └─Empty: 2-1426                     [64]                      --
│    └─BatchNorm2d: 2-1427               [8, 64, 64, 64]           --
│    └─Scaler: 2-1428                    [8, 64, 64, 64]           --
│    └─ReLU: 2-1429                      [8, 64, 64, 64]           --
│    └─Empty: 2-1430                     [8, 64, 64, 64]           --
│    └─Clamp: 2-1431                     [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-108               [8, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-1432        --                        --
│    └─One: 2-1433                       [1]                       --
│    └─OutputScale: 2-1434               --                        --
│    └─Empty: 2-1435                     [64, 64, 3, 3]            --
│    └─Empty: 2-1436                     [64, 64, 3, 3]            --
│    └─Empty: 2-1437                     [64]                      --
│    └─Empty: 2-1438                     [64]                      --
│    └─BatchNorm2d: 2-1439               [8, 64, 64, 64]           --
│    └─Scaler: 2-1440                    [8, 64, 64, 64]           --
│    └─ReLU: 2-1441                      [8, 64, 64, 64]           --
│    └─Empty: 2-1442                     [8, 64, 64, 64]           --
│    └─Clamp: 2-1443                     [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-109        [8, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1444                 [8, 64, 32, 32]           --
│    └─Empty: 2-1445                     [8, 64, 32, 32]           --
│    └─Empty: 2-1446                     [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1447        --                        --
│    └─One: 2-1448                       [1]                       --
│    └─OutputScale: 2-1449               --                        --
│    └─Empty: 2-1450                     [64, 64, 3, 3]            --
│    └─Empty: 2-1451                     [64, 64, 3, 3]            --
│    └─Empty: 2-1452                     [64]                      --
│    └─Empty: 2-1453                     [64]                      --
│    └─BatchNorm2d: 2-1454               [8, 64, 32, 32]           --
│    └─Scaler: 2-1455                    [8, 64, 32, 32]           --
│    └─ReLU: 2-1456                      [8, 64, 32, 32]           --
│    └─Empty: 2-1457                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1458                     [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-110               [8, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1459        --                        --
│    └─One: 2-1460                       [1]                       --
│    └─OutputScale: 2-1461               --                        --
│    └─Empty: 2-1462                     [64, 64, 3, 3]            --
│    └─Empty: 2-1463                     [64, 64, 3, 3]            --
│    └─Empty: 2-1464                     [64]                      --
│    └─Empty: 2-1465                     [64]                      --
│    └─BatchNorm2d: 2-1466               [8, 64, 32, 32]           --
│    └─Scaler: 2-1467                    [8, 64, 32, 32]           --
│    └─ReLU: 2-1468                      [8, 64, 32, 32]           --
│    └─Empty: 2-1469                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1470                     [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-111        [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1471                 [8, 64, 16, 16]           --
│    └─Empty: 2-1472                     [8, 64, 16, 16]           --
│    └─Empty: 2-1473                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1474        --                        --
│    └─One: 2-1475                       [1]                       --
│    └─OutputScale: 2-1476               --                        --
│    └─Empty: 2-1477                     [64, 64, 3, 3]            --
│    └─Empty: 2-1478                     [64, 64, 3, 3]            --
│    └─Empty: 2-1479                     [64]                      --
│    └─Empty: 2-1480                     [64]                      --
│    └─BatchNorm2d: 2-1481               [8, 64, 16, 16]           --
│    └─Scaler: 2-1482                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1483                      [8, 64, 16, 16]           --
│    └─Empty: 2-1484                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1485                     [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-112               [8, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1486        --                        --
│    └─One: 2-1487                       [1]                       --
│    └─OutputScale: 2-1488               --                        --
│    └─Empty: 2-1489                     [64, 64, 1, 1]            --
│    └─Empty: 2-1490                     [64, 64, 1, 1]            --
│    └─Empty: 2-1491                     [64]                      --
│    └─Empty: 2-1492                     [64]                      --
│    └─BatchNorm2d: 2-1493               [8, 64, 16, 16]           --
│    └─Scaler: 2-1494                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1495                      [8, 64, 16, 16]           --
│    └─Empty: 2-1496                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1497                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-113        [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1498                 [8, 64, 16, 16]           --
│    └─Empty: 2-1499                     [8, 64, 16, 16]           --
│    └─Empty: 2-1500                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1501        --                        --
│    └─One: 2-1502                       [1]                       --
│    └─OutputScale: 2-1503               --                        --
│    └─Empty: 2-1504                     [64, 64, 3, 3]            --
│    └─Empty: 2-1505                     [64, 64, 3, 3]            --
│    └─Empty: 2-1506                     [64]                      --
│    └─Empty: 2-1507                     [64]                      --
│    └─BatchNorm2d: 2-1508               [8, 64, 16, 16]           --
│    └─Scaler: 2-1509                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1510                      [8, 64, 16, 16]           --
│    └─Empty: 2-1511                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1512                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-114        [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1513                 [8, 64, 8, 8]             --
│    └─Empty: 2-1514                     [8, 64, 8, 8]             --
│    └─Empty: 2-1515                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1516        --                        --
│    └─One: 2-1517                       [1]                       --
│    └─OutputScale: 2-1518               --                        --
│    └─Empty: 2-1519                     [64, 64, 3, 3]            --
│    └─Empty: 2-1520                     [64, 64, 3, 3]            --
│    └─Empty: 2-1521                     [64]                      --
│    └─Empty: 2-1522                     [64]                      --
│    └─BatchNorm2d: 2-1523               [8, 64, 8, 8]             --
│    └─Scaler: 2-1524                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1525                      [8, 64, 8, 8]             --
│    └─Empty: 2-1526                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1527                     [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-115               [8, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-1528        --                        --
│    └─One: 2-1529                       [1]                       --
│    └─OutputScale: 2-1530               --                        --
│    └─Empty: 2-1531                     [64, 64, 1, 1]            --
│    └─Empty: 2-1532                     [64, 64, 1, 1]            --
│    └─Empty: 2-1533                     [64]                      --
│    └─Empty: 2-1534                     [64]                      --
│    └─BatchNorm2d: 2-1535               [8, 64, 8, 8]             --
│    └─Scaler: 2-1536                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1537                      [8, 64, 8, 8]             --
│    └─Empty: 2-1538                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1539                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-116        [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1540                 [8, 64, 8, 8]             --
│    └─Empty: 2-1541                     [8, 64, 8, 8]             --
│    └─Empty: 2-1542                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1543        --                        --
│    └─One: 2-1544                       [1]                       --
│    └─OutputScale: 2-1545               --                        --
│    └─Empty: 2-1546                     [64, 64, 3, 3]            --
│    └─Empty: 2-1547                     [64, 64, 3, 3]            --
│    └─Empty: 2-1548                     [64]                      --
│    └─Empty: 2-1549                     [64]                      --
│    └─BatchNorm2d: 2-1550               [8, 64, 8, 8]             --
│    └─Scaler: 2-1551                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1552                      [8, 64, 8, 8]             --
│    └─Empty: 2-1553                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1554                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-117        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-1555                 [8, 64, 4, 4]             --
│    └─Empty: 2-1556                     [8, 64, 4, 4]             --
│    └─Empty: 2-1557                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-1558        --                        --
│    └─One: 2-1559                       [1]                       --
│    └─OutputScale: 2-1560               --                        --
│    └─Empty: 2-1561                     [64, 64, 1, 1]            --
│    └─Empty: 2-1562                     [64, 64, 1, 1]            --
│    └─Empty: 2-1563                     [64]                      --
│    └─Empty: 2-1564                     [64]                      --
│    └─BatchNorm2d: 2-1565               [8, 64, 4, 4]             --
│    └─Scaler: 2-1566                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1567                      [8, 64, 4, 4]             --
│    └─Empty: 2-1568                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1569                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-118               [8, 64, 4, 4]             (recursive)
│    └─OutputShiftSqueeze: 2-1570        --                        --
│    └─One: 2-1571                       [1]                       --
│    └─OutputScale: 2-1572               --                        --
│    └─Empty: 2-1573                     [64, 64, 1, 1]            --
│    └─Empty: 2-1574                     [64, 64, 1, 1]            --
│    └─Empty: 2-1575                     [64]                      --
│    └─Empty: 2-1576                     [64]                      --
│    └─BatchNorm2d: 2-1577               [8, 64, 4, 4]             --
│    └─Scaler: 2-1578                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1579                      [8, 64, 4, 4]             --
│    └─Empty: 2-1580                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1581                     [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-119        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-1582                 [8, 64, 4, 4]             --
│    └─Empty: 2-1583                     [8, 64, 4, 4]             --
│    └─Empty: 2-1584                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-1585        --                        --
│    └─One: 2-1586                       [1]                       --
│    └─OutputScale: 2-1587               --                        --
│    └─Empty: 2-1588                     [64, 64, 3, 3]            --
│    └─Empty: 2-1589                     [64, 64, 3, 3]            --
│    └─Empty: 2-1590                     [64]                      --
│    └─Empty: 2-1591                     [64]                      --
│    └─BatchNorm2d: 2-1592               [8, 64, 4, 4]             --
│    └─Scaler: 2-1593                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1594                      [8, 64, 4, 4]             --
│    └─Empty: 2-1595                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1596                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-120               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1597        --                        --
│    └─One: 2-1598                       [1]                       --
│    └─OutputScale: 2-1599               --                        --
│    └─Empty: 2-1600                     [64, 12, 1, 1]            --
│    └─Empty: 2-1601                     [64, 12, 1, 1]            --
│    └─Empty: 2-1602                     [64]                      --
│    └─Empty: 2-1603                     [64]                      --
│    └─BatchNorm2d: 2-1604               [8, 64, 128, 128]         --
│    └─Scaler: 2-1605                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1606                      [8, 64, 128, 128]         --
│    └─Empty: 2-1607                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1608                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-121               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1609        --                        --
│    └─One: 2-1610                       [1]                       --
│    └─OutputScale: 2-1611               --                        --
│    └─Empty: 2-1612                     [64, 64, 3, 3]            --
│    └─Empty: 2-1613                     [64, 64, 3, 3]            --
│    └─Empty: 2-1614                     [64]                      --
│    └─Empty: 2-1615                     [64]                      --
│    └─BatchNorm2d: 2-1616               [8, 64, 128, 128]         --
│    └─Scaler: 2-1617                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1618                      [8, 64, 128, 128]         --
│    └─Empty: 2-1619                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1620                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-122               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1621        --                        --
│    └─One: 2-1622                       [1]                       --
│    └─OutputScale: 2-1623               --                        --
│    └─Empty: 2-1624                     [64, 64, 1, 1]            --
│    └─Empty: 2-1625                     [64, 64, 1, 1]            --
│    └─Empty: 2-1626                     [64]                      --
│    └─Empty: 2-1627                     [64]                      --
│    └─BatchNorm2d: 2-1628               [8, 64, 128, 128]         --
│    └─Scaler: 2-1629                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1630                      [8, 64, 128, 128]         --
│    └─Empty: 2-1631                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1632                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-123               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1633        --                        --
│    └─One: 2-1634                       [1]                       --
│    └─OutputScale: 2-1635               --                        --
│    └─Empty: 2-1636                     [64, 64, 3, 3]            --
│    └─Empty: 2-1637                     [64, 64, 3, 3]            --
│    └─Empty: 2-1638                     [64]                      --
│    └─Empty: 2-1639                     [64]                      --
│    └─BatchNorm2d: 2-1640               [8, 64, 128, 128]         --
│    └─Scaler: 2-1641                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1642                      [8, 64, 128, 128]         --
│    └─Empty: 2-1643                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1644                     [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-124        [8, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-1645                 [8, 64, 64, 64]           --
│    └─Empty: 2-1646                     [8, 64, 64, 64]           --
│    └─Empty: 2-1647                     [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-1648        --                        --
│    └─One: 2-1649                       [1]                       --
│    └─OutputScale: 2-1650               --                        --
│    └─Empty: 2-1651                     [64, 64, 3, 3]            --
│    └─Empty: 2-1652                     [64, 64, 3, 3]            --
│    └─Empty: 2-1653                     [64]                      --
│    └─Empty: 2-1654                     [64]                      --
│    └─BatchNorm2d: 2-1655               [8, 64, 64, 64]           --
│    └─Scaler: 2-1656                    [8, 64, 64, 64]           --
│    └─ReLU: 2-1657                      [8, 64, 64, 64]           --
│    └─Empty: 2-1658                     [8, 64, 64, 64]           --
│    └─Clamp: 2-1659                     [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-125               [8, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-1660        --                        --
│    └─One: 2-1661                       [1]                       --
│    └─OutputScale: 2-1662               --                        --
│    └─Empty: 2-1663                     [64, 64, 3, 3]            --
│    └─Empty: 2-1664                     [64, 64, 3, 3]            --
│    └─Empty: 2-1665                     [64]                      --
│    └─Empty: 2-1666                     [64]                      --
│    └─BatchNorm2d: 2-1667               [8, 64, 64, 64]           --
│    └─Scaler: 2-1668                    [8, 64, 64, 64]           --
│    └─ReLU: 2-1669                      [8, 64, 64, 64]           --
│    └─Empty: 2-1670                     [8, 64, 64, 64]           --
│    └─Clamp: 2-1671                     [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-126        [8, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1672                 [8, 64, 32, 32]           --
│    └─Empty: 2-1673                     [8, 64, 32, 32]           --
│    └─Empty: 2-1674                     [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1675        --                        --
│    └─One: 2-1676                       [1]                       --
│    └─OutputScale: 2-1677               --                        --
│    └─Empty: 2-1678                     [64, 64, 3, 3]            --
│    └─Empty: 2-1679                     [64, 64, 3, 3]            --
│    └─Empty: 2-1680                     [64]                      --
│    └─Empty: 2-1681                     [64]                      --
│    └─BatchNorm2d: 2-1682               [8, 64, 32, 32]           --
│    └─Scaler: 2-1683                    [8, 64, 32, 32]           --
│    └─ReLU: 2-1684                      [8, 64, 32, 32]           --
│    └─Empty: 2-1685                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1686                     [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-127               [8, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1687        --                        --
│    └─One: 2-1688                       [1]                       --
│    └─OutputScale: 2-1689               --                        --
│    └─Empty: 2-1690                     [64, 64, 3, 3]            --
│    └─Empty: 2-1691                     [64, 64, 3, 3]            --
│    └─Empty: 2-1692                     [64]                      --
│    └─Empty: 2-1693                     [64]                      --
│    └─BatchNorm2d: 2-1694               [8, 64, 32, 32]           --
│    └─Scaler: 2-1695                    [8, 64, 32, 32]           --
│    └─ReLU: 2-1696                      [8, 64, 32, 32]           --
│    └─Empty: 2-1697                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1698                     [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-128        [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1699                 [8, 64, 16, 16]           --
│    └─Empty: 2-1700                     [8, 64, 16, 16]           --
│    └─Empty: 2-1701                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1702        --                        --
│    └─One: 2-1703                       [1]                       --
│    └─OutputScale: 2-1704               --                        --
│    └─Empty: 2-1705                     [64, 64, 3, 3]            --
│    └─Empty: 2-1706                     [64, 64, 3, 3]            --
│    └─Empty: 2-1707                     [64]                      --
│    └─Empty: 2-1708                     [64]                      --
│    └─BatchNorm2d: 2-1709               [8, 64, 16, 16]           --
│    └─Scaler: 2-1710                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1711                      [8, 64, 16, 16]           --
│    └─Empty: 2-1712                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1713                     [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-129               [8, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1714        --                        --
│    └─One: 2-1715                       [1]                       --
│    └─OutputScale: 2-1716               --                        --
│    └─Empty: 2-1717                     [64, 64, 1, 1]            --
│    └─Empty: 2-1718                     [64, 64, 1, 1]            --
│    └─Empty: 2-1719                     [64]                      --
│    └─Empty: 2-1720                     [64]                      --
│    └─BatchNorm2d: 2-1721               [8, 64, 16, 16]           --
│    └─Scaler: 2-1722                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1723                      [8, 64, 16, 16]           --
│    └─Empty: 2-1724                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1725                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-130        [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1726                 [8, 64, 16, 16]           --
│    └─Empty: 2-1727                     [8, 64, 16, 16]           --
│    └─Empty: 2-1728                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1729        --                        --
│    └─One: 2-1730                       [1]                       --
│    └─OutputScale: 2-1731               --                        --
│    └─Empty: 2-1732                     [64, 64, 3, 3]            --
│    └─Empty: 2-1733                     [64, 64, 3, 3]            --
│    └─Empty: 2-1734                     [64]                      --
│    └─Empty: 2-1735                     [64]                      --
│    └─BatchNorm2d: 2-1736               [8, 64, 16, 16]           --
│    └─Scaler: 2-1737                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1738                      [8, 64, 16, 16]           --
│    └─Empty: 2-1739                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1740                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-131        [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1741                 [8, 64, 8, 8]             --
│    └─Empty: 2-1742                     [8, 64, 8, 8]             --
│    └─Empty: 2-1743                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1744        --                        --
│    └─One: 2-1745                       [1]                       --
│    └─OutputScale: 2-1746               --                        --
│    └─Empty: 2-1747                     [64, 64, 3, 3]            --
│    └─Empty: 2-1748                     [64, 64, 3, 3]            --
│    └─Empty: 2-1749                     [64]                      --
│    └─Empty: 2-1750                     [64]                      --
│    └─BatchNorm2d: 2-1751               [8, 64, 8, 8]             --
│    └─Scaler: 2-1752                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1753                      [8, 64, 8, 8]             --
│    └─Empty: 2-1754                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1755                     [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-132               [8, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-1756        --                        --
│    └─One: 2-1757                       [1]                       --
│    └─OutputScale: 2-1758               --                        --
│    └─Empty: 2-1759                     [64, 64, 1, 1]            --
│    └─Empty: 2-1760                     [64, 64, 1, 1]            --
│    └─Empty: 2-1761                     [64]                      --
│    └─Empty: 2-1762                     [64]                      --
│    └─BatchNorm2d: 2-1763               [8, 64, 8, 8]             --
│    └─Scaler: 2-1764                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1765                      [8, 64, 8, 8]             --
│    └─Empty: 2-1766                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1767                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-133        [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1768                 [8, 64, 8, 8]             --
│    └─Empty: 2-1769                     [8, 64, 8, 8]             --
│    └─Empty: 2-1770                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1771        --                        --
│    └─One: 2-1772                       [1]                       --
│    └─OutputScale: 2-1773               --                        --
│    └─Empty: 2-1774                     [64, 64, 3, 3]            --
│    └─Empty: 2-1775                     [64, 64, 3, 3]            --
│    └─Empty: 2-1776                     [64]                      --
│    └─Empty: 2-1777                     [64]                      --
│    └─BatchNorm2d: 2-1778               [8, 64, 8, 8]             --
│    └─Scaler: 2-1779                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1780                      [8, 64, 8, 8]             --
│    └─Empty: 2-1781                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1782                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-134        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-1783                 [8, 64, 4, 4]             --
│    └─Empty: 2-1784                     [8, 64, 4, 4]             --
│    └─Empty: 2-1785                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-1786        --                        --
│    └─One: 2-1787                       [1]                       --
│    └─OutputScale: 2-1788               --                        --
│    └─Empty: 2-1789                     [64, 64, 1, 1]            --
│    └─Empty: 2-1790                     [64, 64, 1, 1]            --
│    └─Empty: 2-1791                     [64]                      --
│    └─Empty: 2-1792                     [64]                      --
│    └─BatchNorm2d: 2-1793               [8, 64, 4, 4]             --
│    └─Scaler: 2-1794                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1795                      [8, 64, 4, 4]             --
│    └─Empty: 2-1796                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1797                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-135               [8, 64, 4, 4]             (recursive)
│    └─OutputShiftSqueeze: 2-1798        --                        --
│    └─One: 2-1799                       [1]                       --
│    └─OutputScale: 2-1800               --                        --
│    └─Empty: 2-1801                     [64, 64, 1, 1]            --
│    └─Empty: 2-1802                     [64, 64, 1, 1]            --
│    └─Empty: 2-1803                     [64]                      --
│    └─Empty: 2-1804                     [64]                      --
│    └─BatchNorm2d: 2-1805               [8, 64, 4, 4]             --
│    └─Scaler: 2-1806                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1807                      [8, 64, 4, 4]             --
│    └─Empty: 2-1808                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1809                     [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-136        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-1810                 [8, 64, 4, 4]             --
│    └─Empty: 2-1811                     [8, 64, 4, 4]             --
│    └─Empty: 2-1812                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-1813        --                        --
│    └─One: 2-1814                       [1]                       --
│    └─OutputScale: 2-1815               --                        --
│    └─Empty: 2-1816                     [64, 64, 3, 3]            --
│    └─Empty: 2-1817                     [64, 64, 3, 3]            --
│    └─Empty: 2-1818                     [64]                      --
│    └─Empty: 2-1819                     [64]                      --
│    └─BatchNorm2d: 2-1820               [8, 64, 4, 4]             --
│    └─Scaler: 2-1821                    [8, 64, 4, 4]             --
│    └─ReLU: 2-1822                      [8, 64, 4, 4]             --
│    └─Empty: 2-1823                     [8, 64, 4, 4]             --
│    └─Clamp: 2-1824                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-137               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1825        --                        --
│    └─One: 2-1826                       [1]                       --
│    └─OutputScale: 2-1827               --                        --
│    └─Empty: 2-1828                     [64, 12, 1, 1]            --
│    └─Empty: 2-1829                     [64, 12, 1, 1]            --
│    └─Empty: 2-1830                     [64]                      --
│    └─Empty: 2-1831                     [64]                      --
│    └─BatchNorm2d: 2-1832               [8, 64, 128, 128]         --
│    └─Scaler: 2-1833                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1834                      [8, 64, 128, 128]         --
│    └─Empty: 2-1835                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1836                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-138               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1837        --                        --
│    └─One: 2-1838                       [1]                       --
│    └─OutputScale: 2-1839               --                        --
│    └─Empty: 2-1840                     [64, 64, 3, 3]            --
│    └─Empty: 2-1841                     [64, 64, 3, 3]            --
│    └─Empty: 2-1842                     [64]                      --
│    └─Empty: 2-1843                     [64]                      --
│    └─BatchNorm2d: 2-1844               [8, 64, 128, 128]         --
│    └─Scaler: 2-1845                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1846                      [8, 64, 128, 128]         --
│    └─Empty: 2-1847                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1848                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-139               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1849        --                        --
│    └─One: 2-1850                       [1]                       --
│    └─OutputScale: 2-1851               --                        --
│    └─Empty: 2-1852                     [64, 64, 1, 1]            --
│    └─Empty: 2-1853                     [64, 64, 1, 1]            --
│    └─Empty: 2-1854                     [64]                      --
│    └─Empty: 2-1855                     [64]                      --
│    └─BatchNorm2d: 2-1856               [8, 64, 128, 128]         --
│    └─Scaler: 2-1857                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1858                      [8, 64, 128, 128]         --
│    └─Empty: 2-1859                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1860                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-140               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-1861        --                        --
│    └─One: 2-1862                       [1]                       --
│    └─OutputScale: 2-1863               --                        --
│    └─Empty: 2-1864                     [64, 64, 3, 3]            --
│    └─Empty: 2-1865                     [64, 64, 3, 3]            --
│    └─Empty: 2-1866                     [64]                      --
│    └─Empty: 2-1867                     [64]                      --
│    └─BatchNorm2d: 2-1868               [8, 64, 128, 128]         --
│    └─Scaler: 2-1869                    [8, 64, 128, 128]         --
│    └─ReLU: 2-1870                      [8, 64, 128, 128]         --
│    └─Empty: 2-1871                     [8, 64, 128, 128]         --
│    └─Clamp: 2-1872                     [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-141        [8, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-1873                 [8, 64, 64, 64]           --
│    └─Empty: 2-1874                     [8, 64, 64, 64]           --
│    └─Empty: 2-1875                     [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-1876        --                        --
│    └─One: 2-1877                       [1]                       --
│    └─OutputScale: 2-1878               --                        --
│    └─Empty: 2-1879                     [64, 64, 3, 3]            --
│    └─Empty: 2-1880                     [64, 64, 3, 3]            --
│    └─Empty: 2-1881                     [64]                      --
│    └─Empty: 2-1882                     [64]                      --
│    └─BatchNorm2d: 2-1883               [8, 64, 64, 64]           --
│    └─Scaler: 2-1884                    [8, 64, 64, 64]           --
│    └─ReLU: 2-1885                      [8, 64, 64, 64]           --
│    └─Empty: 2-1886                     [8, 64, 64, 64]           --
│    └─Clamp: 2-1887                     [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-142               [8, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-1888        --                        --
│    └─One: 2-1889                       [1]                       --
│    └─OutputScale: 2-1890               --                        --
│    └─Empty: 2-1891                     [64, 64, 3, 3]            --
│    └─Empty: 2-1892                     [64, 64, 3, 3]            --
│    └─Empty: 2-1893                     [64]                      --
│    └─Empty: 2-1894                     [64]                      --
│    └─BatchNorm2d: 2-1895               [8, 64, 64, 64]           --
│    └─Scaler: 2-1896                    [8, 64, 64, 64]           --
│    └─ReLU: 2-1897                      [8, 64, 64, 64]           --
│    └─Empty: 2-1898                     [8, 64, 64, 64]           --
│    └─Clamp: 2-1899                     [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-143        [8, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-1900                 [8, 64, 32, 32]           --
│    └─Empty: 2-1901                     [8, 64, 32, 32]           --
│    └─Empty: 2-1902                     [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-1903        --                        --
│    └─One: 2-1904                       [1]                       --
│    └─OutputScale: 2-1905               --                        --
│    └─Empty: 2-1906                     [64, 64, 3, 3]            --
│    └─Empty: 2-1907                     [64, 64, 3, 3]            --
│    └─Empty: 2-1908                     [64]                      --
│    └─Empty: 2-1909                     [64]                      --
│    └─BatchNorm2d: 2-1910               [8, 64, 32, 32]           --
│    └─Scaler: 2-1911                    [8, 64, 32, 32]           --
│    └─ReLU: 2-1912                      [8, 64, 32, 32]           --
│    └─Empty: 2-1913                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1914                     [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-144               [8, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-1915        --                        --
│    └─One: 2-1916                       [1]                       --
│    └─OutputScale: 2-1917               --                        --
│    └─Empty: 2-1918                     [64, 64, 3, 3]            --
│    └─Empty: 2-1919                     [64, 64, 3, 3]            --
│    └─Empty: 2-1920                     [64]                      --
│    └─Empty: 2-1921                     [64]                      --
│    └─BatchNorm2d: 2-1922               [8, 64, 32, 32]           --
│    └─Scaler: 2-1923                    [8, 64, 32, 32]           --
│    └─ReLU: 2-1924                      [8, 64, 32, 32]           --
│    └─Empty: 2-1925                     [8, 64, 32, 32]           --
│    └─Clamp: 2-1926                     [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-145        [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1927                 [8, 64, 16, 16]           --
│    └─Empty: 2-1928                     [8, 64, 16, 16]           --
│    └─Empty: 2-1929                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1930        --                        --
│    └─One: 2-1931                       [1]                       --
│    └─OutputScale: 2-1932               --                        --
│    └─Empty: 2-1933                     [64, 64, 3, 3]            --
│    └─Empty: 2-1934                     [64, 64, 3, 3]            --
│    └─Empty: 2-1935                     [64]                      --
│    └─Empty: 2-1936                     [64]                      --
│    └─BatchNorm2d: 2-1937               [8, 64, 16, 16]           --
│    └─Scaler: 2-1938                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1939                      [8, 64, 16, 16]           --
│    └─Empty: 2-1940                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1941                     [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-146               [8, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-1942        --                        --
│    └─One: 2-1943                       [1]                       --
│    └─OutputScale: 2-1944               --                        --
│    └─Empty: 2-1945                     [64, 64, 1, 1]            --
│    └─Empty: 2-1946                     [64, 64, 1, 1]            --
│    └─Empty: 2-1947                     [64]                      --
│    └─Empty: 2-1948                     [64]                      --
│    └─BatchNorm2d: 2-1949               [8, 64, 16, 16]           --
│    └─Scaler: 2-1950                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1951                      [8, 64, 16, 16]           --
│    └─Empty: 2-1952                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1953                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-147        [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-1954                 [8, 64, 16, 16]           --
│    └─Empty: 2-1955                     [8, 64, 16, 16]           --
│    └─Empty: 2-1956                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-1957        --                        --
│    └─One: 2-1958                       [1]                       --
│    └─OutputScale: 2-1959               --                        --
│    └─Empty: 2-1960                     [64, 64, 3, 3]            --
│    └─Empty: 2-1961                     [64, 64, 3, 3]            --
│    └─Empty: 2-1962                     [64]                      --
│    └─Empty: 2-1963                     [64]                      --
│    └─BatchNorm2d: 2-1964               [8, 64, 16, 16]           --
│    └─Scaler: 2-1965                    [8, 64, 16, 16]           --
│    └─ReLU: 2-1966                      [8, 64, 16, 16]           --
│    └─Empty: 2-1967                     [8, 64, 16, 16]           --
│    └─Clamp: 2-1968                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-148        [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1969                 [8, 64, 8, 8]             --
│    └─Empty: 2-1970                     [8, 64, 8, 8]             --
│    └─Empty: 2-1971                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1972        --                        --
│    └─One: 2-1973                       [1]                       --
│    └─OutputScale: 2-1974               --                        --
│    └─Empty: 2-1975                     [64, 64, 3, 3]            --
│    └─Empty: 2-1976                     [64, 64, 3, 3]            --
│    └─Empty: 2-1977                     [64]                      --
│    └─Empty: 2-1978                     [64]                      --
│    └─BatchNorm2d: 2-1979               [8, 64, 8, 8]             --
│    └─Scaler: 2-1980                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1981                      [8, 64, 8, 8]             --
│    └─Empty: 2-1982                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1983                     [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-149               [8, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-1984        --                        --
│    └─One: 2-1985                       [1]                       --
│    └─OutputScale: 2-1986               --                        --
│    └─Empty: 2-1987                     [64, 64, 1, 1]            --
│    └─Empty: 2-1988                     [64, 64, 1, 1]            --
│    └─Empty: 2-1989                     [64]                      --
│    └─Empty: 2-1990                     [64]                      --
│    └─BatchNorm2d: 2-1991               [8, 64, 8, 8]             --
│    └─Scaler: 2-1992                    [8, 64, 8, 8]             --
│    └─ReLU: 2-1993                      [8, 64, 8, 8]             --
│    └─Empty: 2-1994                     [8, 64, 8, 8]             --
│    └─Clamp: 2-1995                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-150        [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-1996                 [8, 64, 8, 8]             --
│    └─Empty: 2-1997                     [8, 64, 8, 8]             --
│    └─Empty: 2-1998                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-1999        --                        --
│    └─One: 2-2000                       [1]                       --
│    └─OutputScale: 2-2001               --                        --
│    └─Empty: 2-2002                     [64, 64, 3, 3]            --
│    └─Empty: 2-2003                     [64, 64, 3, 3]            --
│    └─Empty: 2-2004                     [64]                      --
│    └─Empty: 2-2005                     [64]                      --
│    └─BatchNorm2d: 2-2006               [8, 64, 8, 8]             --
│    └─Scaler: 2-2007                    [8, 64, 8, 8]             --
│    └─ReLU: 2-2008                      [8, 64, 8, 8]             --
│    └─Empty: 2-2009                     [8, 64, 8, 8]             --
│    └─Clamp: 2-2010                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-151        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-2011                 [8, 64, 4, 4]             --
│    └─Empty: 2-2012                     [8, 64, 4, 4]             --
│    └─Empty: 2-2013                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-2014        --                        --
│    └─One: 2-2015                       [1]                       --
│    └─OutputScale: 2-2016               --                        --
│    └─Empty: 2-2017                     [64, 64, 1, 1]            --
│    └─Empty: 2-2018                     [64, 64, 1, 1]            --
│    └─Empty: 2-2019                     [64]                      --
│    └─Empty: 2-2020                     [64]                      --
│    └─BatchNorm2d: 2-2021               [8, 64, 4, 4]             --
│    └─Scaler: 2-2022                    [8, 64, 4, 4]             --
│    └─ReLU: 2-2023                      [8, 64, 4, 4]             --
│    └─Empty: 2-2024                     [8, 64, 4, 4]             --
│    └─Clamp: 2-2025                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-152               [8, 64, 4, 4]             (recursive)
│    └─OutputShiftSqueeze: 2-2026        --                        --
│    └─One: 2-2027                       [1]                       --
│    └─OutputScale: 2-2028               --                        --
│    └─Empty: 2-2029                     [64, 64, 1, 1]            --
│    └─Empty: 2-2030                     [64, 64, 1, 1]            --
│    └─Empty: 2-2031                     [64]                      --
│    └─Empty: 2-2032                     [64]                      --
│    └─BatchNorm2d: 2-2033               [8, 64, 4, 4]             --
│    └─Scaler: 2-2034                    [8, 64, 4, 4]             --
│    └─ReLU: 2-2035                      [8, 64, 4, 4]             --
│    └─Empty: 2-2036                     [8, 64, 4, 4]             --
│    └─Clamp: 2-2037                     [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-153        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-2038                 [8, 64, 4, 4]             --
│    └─Empty: 2-2039                     [8, 64, 4, 4]             --
│    └─Empty: 2-2040                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-2041        --                        --
│    └─One: 2-2042                       [1]                       --
│    └─OutputScale: 2-2043               --                        --
│    └─Empty: 2-2044                     [64, 64, 3, 3]            --
│    └─Empty: 2-2045                     [64, 64, 3, 3]            --
│    └─Empty: 2-2046                     [64]                      --
│    └─Empty: 2-2047                     [64]                      --
│    └─BatchNorm2d: 2-2048               [8, 64, 4, 4]             --
│    └─Scaler: 2-2049                    [8, 64, 4, 4]             --
│    └─ReLU: 2-2050                      [8, 64, 4, 4]             --
│    └─Empty: 2-2051                     [8, 64, 4, 4]             --
│    └─Clamp: 2-2052                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-154               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-2053        --                        --
│    └─One: 2-2054                       [1]                       --
│    └─OutputScale: 2-2055               --                        --
│    └─Empty: 2-2056                     [64, 12, 1, 1]            --
│    └─Empty: 2-2057                     [64, 12, 1, 1]            --
│    └─Empty: 2-2058                     [64]                      --
│    └─Empty: 2-2059                     [64]                      --
│    └─BatchNorm2d: 2-2060               [8, 64, 128, 128]         --
│    └─Scaler: 2-2061                    [8, 64, 128, 128]         --
│    └─ReLU: 2-2062                      [8, 64, 128, 128]         --
│    └─Empty: 2-2063                     [8, 64, 128, 128]         --
│    └─Clamp: 2-2064                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-155               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-2065        --                        --
│    └─One: 2-2066                       [1]                       --
│    └─OutputScale: 2-2067               --                        --
│    └─Empty: 2-2068                     [64, 64, 3, 3]            --
│    └─Empty: 2-2069                     [64, 64, 3, 3]            --
│    └─Empty: 2-2070                     [64]                      --
│    └─Empty: 2-2071                     [64]                      --
│    └─BatchNorm2d: 2-2072               [8, 64, 128, 128]         --
│    └─Scaler: 2-2073                    [8, 64, 128, 128]         --
│    └─ReLU: 2-2074                      [8, 64, 128, 128]         --
│    └─Empty: 2-2075                     [8, 64, 128, 128]         --
│    └─Clamp: 2-2076                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-156               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-2077        --                        --
│    └─One: 2-2078                       [1]                       --
│    └─OutputScale: 2-2079               --                        --
│    └─Empty: 2-2080                     [64, 64, 1, 1]            --
│    └─Empty: 2-2081                     [64, 64, 1, 1]            --
│    └─Empty: 2-2082                     [64]                      --
│    └─Empty: 2-2083                     [64]                      --
│    └─BatchNorm2d: 2-2084               [8, 64, 128, 128]         --
│    └─Scaler: 2-2085                    [8, 64, 128, 128]         --
│    └─ReLU: 2-2086                      [8, 64, 128, 128]         --
│    └─Empty: 2-2087                     [8, 64, 128, 128]         --
│    └─Clamp: 2-2088                     [8, 64, 128, 128]         --
├─FusedConv2dBNReLU: 1-157               [8, 64, 128, 128]         (recursive)
│    └─OutputShiftSqueeze: 2-2089        --                        --
│    └─One: 2-2090                       [1]                       --
│    └─OutputScale: 2-2091               --                        --
│    └─Empty: 2-2092                     [64, 64, 3, 3]            --
│    └─Empty: 2-2093                     [64, 64, 3, 3]            --
│    └─Empty: 2-2094                     [64]                      --
│    └─Empty: 2-2095                     [64]                      --
│    └─BatchNorm2d: 2-2096               [8, 64, 128, 128]         --
│    └─Scaler: 2-2097                    [8, 64, 128, 128]         --
│    └─ReLU: 2-2098                      [8, 64, 128, 128]         --
│    └─Empty: 2-2099                     [8, 64, 128, 128]         --
│    └─Clamp: 2-2100                     [8, 64, 128, 128]         --
├─FusedMaxPoolConv2dBNReLU: 1-158        [8, 64, 64, 64]           (recursive)
│    └─MaxPool2d: 2-2101                 [8, 64, 64, 64]           --
│    └─Empty: 2-2102                     [8, 64, 64, 64]           --
│    └─Empty: 2-2103                     [8, 64, 64, 64]           --
│    └─OutputShiftSqueeze: 2-2104        --                        --
│    └─One: 2-2105                       [1]                       --
│    └─OutputScale: 2-2106               --                        --
│    └─Empty: 2-2107                     [64, 64, 3, 3]            --
│    └─Empty: 2-2108                     [64, 64, 3, 3]            --
│    └─Empty: 2-2109                     [64]                      --
│    └─Empty: 2-2110                     [64]                      --
│    └─BatchNorm2d: 2-2111               [8, 64, 64, 64]           --
│    └─Scaler: 2-2112                    [8, 64, 64, 64]           --
│    └─ReLU: 2-2113                      [8, 64, 64, 64]           --
│    └─Empty: 2-2114                     [8, 64, 64, 64]           --
│    └─Clamp: 2-2115                     [8, 64, 64, 64]           --
├─FusedConv2dBNReLU: 1-159               [8, 64, 64, 64]           (recursive)
│    └─OutputShiftSqueeze: 2-2116        --                        --
│    └─One: 2-2117                       [1]                       --
│    └─OutputScale: 2-2118               --                        --
│    └─Empty: 2-2119                     [64, 64, 3, 3]            --
│    └─Empty: 2-2120                     [64, 64, 3, 3]            --
│    └─Empty: 2-2121                     [64]                      --
│    └─Empty: 2-2122                     [64]                      --
│    └─BatchNorm2d: 2-2123               [8, 64, 64, 64]           --
│    └─Scaler: 2-2124                    [8, 64, 64, 64]           --
│    └─ReLU: 2-2125                      [8, 64, 64, 64]           --
│    └─Empty: 2-2126                     [8, 64, 64, 64]           --
│    └─Clamp: 2-2127                     [8, 64, 64, 64]           --
├─FusedMaxPoolConv2dBNReLU: 1-160        [8, 64, 32, 32]           (recursive)
│    └─MaxPool2d: 2-2128                 [8, 64, 32, 32]           --
│    └─Empty: 2-2129                     [8, 64, 32, 32]           --
│    └─Empty: 2-2130                     [8, 64, 32, 32]           --
│    └─OutputShiftSqueeze: 2-2131        --                        --
│    └─One: 2-2132                       [1]                       --
│    └─OutputScale: 2-2133               --                        --
│    └─Empty: 2-2134                     [64, 64, 3, 3]            --
│    └─Empty: 2-2135                     [64, 64, 3, 3]            --
│    └─Empty: 2-2136                     [64]                      --
│    └─Empty: 2-2137                     [64]                      --
│    └─BatchNorm2d: 2-2138               [8, 64, 32, 32]           --
│    └─Scaler: 2-2139                    [8, 64, 32, 32]           --
│    └─ReLU: 2-2140                      [8, 64, 32, 32]           --
│    └─Empty: 2-2141                     [8, 64, 32, 32]           --
│    └─Clamp: 2-2142                     [8, 64, 32, 32]           --
├─FusedConv2dBNReLU: 1-161               [8, 64, 32, 32]           (recursive)
│    └─OutputShiftSqueeze: 2-2143        --                        --
│    └─One: 2-2144                       [1]                       --
│    └─OutputScale: 2-2145               --                        --
│    └─Empty: 2-2146                     [64, 64, 3, 3]            --
│    └─Empty: 2-2147                     [64, 64, 3, 3]            --
│    └─Empty: 2-2148                     [64]                      --
│    └─Empty: 2-2149                     [64]                      --
│    └─BatchNorm2d: 2-2150               [8, 64, 32, 32]           --
│    └─Scaler: 2-2151                    [8, 64, 32, 32]           --
│    └─ReLU: 2-2152                      [8, 64, 32, 32]           --
│    └─Empty: 2-2153                     [8, 64, 32, 32]           --
│    └─Clamp: 2-2154                     [8, 64, 32, 32]           --
├─FusedMaxPoolConv2dBNReLU: 1-162        [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2155                 [8, 64, 16, 16]           --
│    └─Empty: 2-2156                     [8, 64, 16, 16]           --
│    └─Empty: 2-2157                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2158        --                        --
│    └─One: 2-2159                       [1]                       --
│    └─OutputScale: 2-2160               --                        --
│    └─Empty: 2-2161                     [64, 64, 3, 3]            --
│    └─Empty: 2-2162                     [64, 64, 3, 3]            --
│    └─Empty: 2-2163                     [64]                      --
│    └─Empty: 2-2164                     [64]                      --
│    └─BatchNorm2d: 2-2165               [8, 64, 16, 16]           --
│    └─Scaler: 2-2166                    [8, 64, 16, 16]           --
│    └─ReLU: 2-2167                      [8, 64, 16, 16]           --
│    └─Empty: 2-2168                     [8, 64, 16, 16]           --
│    └─Clamp: 2-2169                     [8, 64, 16, 16]           --
├─FusedConv2dBNReLU: 1-163               [8, 64, 16, 16]           (recursive)
│    └─OutputShiftSqueeze: 2-2170        --                        --
│    └─One: 2-2171                       [1]                       --
│    └─OutputScale: 2-2172               --                        --
│    └─Empty: 2-2173                     [64, 64, 1, 1]            --
│    └─Empty: 2-2174                     [64, 64, 1, 1]            --
│    └─Empty: 2-2175                     [64]                      --
│    └─Empty: 2-2176                     [64]                      --
│    └─BatchNorm2d: 2-2177               [8, 64, 16, 16]           --
│    └─Scaler: 2-2178                    [8, 64, 16, 16]           --
│    └─ReLU: 2-2179                      [8, 64, 16, 16]           --
│    └─Empty: 2-2180                     [8, 64, 16, 16]           --
│    └─Clamp: 2-2181                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-164        [8, 64, 16, 16]           (recursive)
│    └─MaxPool2d: 2-2182                 [8, 64, 16, 16]           --
│    └─Empty: 2-2183                     [8, 64, 16, 16]           --
│    └─Empty: 2-2184                     [8, 64, 16, 16]           --
│    └─OutputShiftSqueeze: 2-2185        --                        --
│    └─One: 2-2186                       [1]                       --
│    └─OutputScale: 2-2187               --                        --
│    └─Empty: 2-2188                     [64, 64, 3, 3]            --
│    └─Empty: 2-2189                     [64, 64, 3, 3]            --
│    └─Empty: 2-2190                     [64]                      --
│    └─Empty: 2-2191                     [64]                      --
│    └─BatchNorm2d: 2-2192               [8, 64, 16, 16]           --
│    └─Scaler: 2-2193                    [8, 64, 16, 16]           --
│    └─ReLU: 2-2194                      [8, 64, 16, 16]           --
│    └─Empty: 2-2195                     [8, 64, 16, 16]           --
│    └─Clamp: 2-2196                     [8, 64, 16, 16]           --
├─FusedMaxPoolConv2dBNReLU: 1-165        [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2197                 [8, 64, 8, 8]             --
│    └─Empty: 2-2198                     [8, 64, 8, 8]             --
│    └─Empty: 2-2199                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2200        --                        --
│    └─One: 2-2201                       [1]                       --
│    └─OutputScale: 2-2202               --                        --
│    └─Empty: 2-2203                     [64, 64, 3, 3]            --
│    └─Empty: 2-2204                     [64, 64, 3, 3]            --
│    └─Empty: 2-2205                     [64]                      --
│    └─Empty: 2-2206                     [64]                      --
│    └─BatchNorm2d: 2-2207               [8, 64, 8, 8]             --
│    └─Scaler: 2-2208                    [8, 64, 8, 8]             --
│    └─ReLU: 2-2209                      [8, 64, 8, 8]             --
│    └─Empty: 2-2210                     [8, 64, 8, 8]             --
│    └─Clamp: 2-2211                     [8, 64, 8, 8]             --
├─FusedConv2dBNReLU: 1-166               [8, 64, 8, 8]             (recursive)
│    └─OutputShiftSqueeze: 2-2212        --                        --
│    └─One: 2-2213                       [1]                       --
│    └─OutputScale: 2-2214               --                        --
│    └─Empty: 2-2215                     [64, 64, 1, 1]            --
│    └─Empty: 2-2216                     [64, 64, 1, 1]            --
│    └─Empty: 2-2217                     [64]                      --
│    └─Empty: 2-2218                     [64]                      --
│    └─BatchNorm2d: 2-2219               [8, 64, 8, 8]             --
│    └─Scaler: 2-2220                    [8, 64, 8, 8]             --
│    └─ReLU: 2-2221                      [8, 64, 8, 8]             --
│    └─Empty: 2-2222                     [8, 64, 8, 8]             --
│    └─Clamp: 2-2223                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-167        [8, 64, 8, 8]             (recursive)
│    └─MaxPool2d: 2-2224                 [8, 64, 8, 8]             --
│    └─Empty: 2-2225                     [8, 64, 8, 8]             --
│    └─Empty: 2-2226                     [8, 64, 8, 8]             --
│    └─OutputShiftSqueeze: 2-2227        --                        --
│    └─One: 2-2228                       [1]                       --
│    └─OutputScale: 2-2229               --                        --
│    └─Empty: 2-2230                     [64, 64, 3, 3]            --
│    └─Empty: 2-2231                     [64, 64, 3, 3]            --
│    └─Empty: 2-2232                     [64]                      --
│    └─Empty: 2-2233                     [64]                      --
│    └─BatchNorm2d: 2-2234               [8, 64, 8, 8]             --
│    └─Scaler: 2-2235                    [8, 64, 8, 8]             --
│    └─ReLU: 2-2236                      [8, 64, 8, 8]             --
│    └─Empty: 2-2237                     [8, 64, 8, 8]             --
│    └─Clamp: 2-2238                     [8, 64, 8, 8]             --
├─FusedMaxPoolConv2dBNReLU: 1-168        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-2239                 [8, 64, 4, 4]             --
│    └─Empty: 2-2240                     [8, 64, 4, 4]             --
│    └─Empty: 2-2241                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-2242        --                        --
│    └─One: 2-2243                       [1]                       --
│    └─OutputScale: 2-2244               --                        --
│    └─Empty: 2-2245                     [64, 64, 1, 1]            --
│    └─Empty: 2-2246                     [64, 64, 1, 1]            --
│    └─Empty: 2-2247                     [64]                      --
│    └─Empty: 2-2248                     [64]                      --
│    └─BatchNorm2d: 2-2249               [8, 64, 4, 4]             --
│    └─Scaler: 2-2250                    [8, 64, 4, 4]             --
│    └─ReLU: 2-2251                      [8, 64, 4, 4]             --
│    └─Empty: 2-2252                     [8, 64, 4, 4]             --
│    └─Clamp: 2-2253                     [8, 64, 4, 4]             --
├─FusedConv2dBNReLU: 1-169               [8, 64, 4, 4]             (recursive)
│    └─OutputShiftSqueeze: 2-2254        --                        --
│    └─One: 2-2255                       [1]                       --
│    └─OutputScale: 2-2256               --                        --
│    └─Empty: 2-2257                     [64, 64, 1, 1]            --
│    └─Empty: 2-2258                     [64, 64, 1, 1]            --
│    └─Empty: 2-2259                     [64]                      --
│    └─Empty: 2-2260                     [64]                      --
│    └─BatchNorm2d: 2-2261               [8, 64, 4, 4]             --
│    └─Scaler: 2-2262                    [8, 64, 4, 4]             --
│    └─ReLU: 2-2263                      [8, 64, 4, 4]             --
│    └─Empty: 2-2264                     [8, 64, 4, 4]             --
│    └─Clamp: 2-2265                     [8, 64, 4, 4]             --
├─FusedMaxPoolConv2dBNReLU: 1-170        [8, 64, 4, 4]             (recursive)
│    └─MaxPool2d: 2-2266                 [8, 64, 4, 4]             --
│    └─Empty: 2-2267                     [8, 64, 4, 4]             --
│    └─Empty: 2-2268                     [8, 64, 4, 4]             --
│    └─OutputShiftSqueeze: 2-2269        --                        --
│    └─One: 2-2270                       [1]                       --
│    └─OutputScale: 2-2271               --                        --
│    └─Empty: 2-2272                     [64, 64, 3, 3]            --
│    └─Empty: 2-2273                     [64, 64, 3, 3]            --
│    └─Empty: 2-2274                     [64]                      --
│    └─Empty: 2-2275                     [64]                      --
│    └─BatchNorm2d: 2-2276               [8, 64, 4, 4]             --
│    └─Scaler: 2-2277                    [8, 64, 4, 4]             --
│    └─ReLU: 2-2278                      [8, 64, 4, 4]             --
│    └─Empty: 2-2279                     [8, 64, 4, 4]             --
│    └─Clamp: 2-2280                     [8, 64, 4, 4]             --
├─Conv1d: 1-171                          [8, 64, 10]               65,606
│    └─OutputShiftSqueeze: 2-2281        --                        --
│    └─One: 2-2282                       [1]                       --
│    └─OutputScale: 2-2283               --                        --
│    └─Empty: 2-2284                     [64, 1024, 1]             --
│    └─Empty: 2-2285                     [64, 1024, 1]             --
│    └─Empty: 2-2286                     [64]                      --
│    └─Empty: 2-2287                     [64]                      --
│    └─Scaler: 2-2288                    [8, 64, 10]               --
│    └─Empty: 2-2289                     [8, 64, 10]               --
│    └─Empty: 2-2290                     [8, 64, 10]               --
│    └─Clamp: 2-2291                     [8, 64, 10]               --
├─FusedConv1dBNReLU: 1-172               [8, 64, 10]               12,358
│    └─OutputShiftSqueeze: 2-2292        --                        --
│    └─One: 2-2293                       [1]                       --
│    └─OutputScale: 2-2294               --                        --
│    └─Empty: 2-2295                     [64, 64, 3]               --
│    └─Empty: 2-2296                     [64, 64, 3]               --
│    └─Empty: 2-2297                     [64]                      --
│    └─Empty: 2-2298                     [64]                      --
│    └─BatchNorm1d: 2-2299               [8, 64, 10]               --
│    └─Scaler: 2-2300                    [8, 64, 10]               --
│    └─ReLU: 2-2301                      [8, 64, 10]               --
│    └─Empty: 2-2302                     [8, 64, 10]               --
│    └─Clamp: 2-2303                     [8, 64, 10]               --
├─Conv1d: 1-173                          [8, 64, 10]               4,166
│    └─OutputShiftSqueeze: 2-2304        --                        --
│    └─One: 2-2305                       [1]                       --
│    └─OutputScale: 2-2306               --                        --
│    └─Empty: 2-2307                     [64, 64, 1]               --
│    └─Empty: 2-2308                     [64, 64, 1]               --
│    └─Empty: 2-2309                     [64]                      --
│    └─Empty: 2-2310                     [64]                      --
│    └─Scaler: 2-2311                    [8, 64, 10]               --
│    └─Empty: 2-2312                     [8, 64, 10]               --
│    └─Empty: 2-2313                     [8, 64, 10]               --
│    └─Clamp: 2-2314                     [8, 64, 10]               --
├─FusedConv1dBNReLU: 1-174               [8, 64, 10]               12,358
│    └─OutputShiftSqueeze: 2-2315        --                        --
│    └─One: 2-2316                       [1]                       --
│    └─OutputScale: 2-2317               --                        --
│    └─Empty: 2-2318                     [64, 64, 3]               --
│    └─Empty: 2-2319                     [64, 64, 3]               --
│    └─Empty: 2-2320                     [64]                      --
│    └─Empty: 2-2321                     [64]                      --
│    └─BatchNorm1d: 2-2322               [8, 64, 10]               --
│    └─Scaler: 2-2323                    [8, 64, 10]               --
│    └─ReLU: 2-2324                      [8, 64, 10]               --
│    └─Empty: 2-2325                     [8, 64, 10]               --
│    └─Clamp: 2-2326                     [8, 64, 10]               --
├─Conv1d: 1-175                          [8, 64, 10]               4,166
│    └─OutputShiftSqueeze: 2-2327        --                        --
│    └─One: 2-2328                       [1]                       --
│    └─OutputScale: 2-2329               --                        --
│    └─Empty: 2-2330                     [64, 64, 1]               --
│    └─Empty: 2-2331                     [64, 64, 1]               --
│    └─Empty: 2-2332                     [64]                      --
│    └─Empty: 2-2333                     [64]                      --
│    └─Scaler: 2-2334                    [8, 64, 10]               --
│    └─Empty: 2-2335                     [8, 64, 10]               --
│    └─Empty: 2-2336                     [8, 64, 10]               --
│    └─Clamp: 2-2337                     [8, 64, 10]               --
├─FusedConv1dBNReLU: 1-176               [8, 64, 6]                12,358
│    └─OutputShiftSqueeze: 2-2338        --                        --
│    └─One: 2-2339                       [1]                       --
│    └─OutputScale: 2-2340               --                        --
│    └─Empty: 2-2341                     [64, 64, 3]               --
│    └─Empty: 2-2342                     [64, 64, 3]               --
│    └─Empty: 2-2343                     [64]                      --
│    └─Empty: 2-2344                     [64]                      --
│    └─BatchNorm1d: 2-2345               [8, 64, 6]                --
│    └─Scaler: 2-2346                    [8, 64, 6]                --
│    └─ReLU: 2-2347                      [8, 64, 6]                --
│    └─Empty: 2-2348                     [8, 64, 6]                --
│    └─Clamp: 2-2349                     [8, 64, 6]                --
├─Conv1d: 1-177                          [8, 64, 6]                4,166
│    └─OutputShiftSqueeze: 2-2350        --                        --
│    └─One: 2-2351                       [1]                       --
│    └─OutputScale: 2-2352               --                        --
│    └─Empty: 2-2353                     [64, 64, 1]               --
│    └─Empty: 2-2354                     [64, 64, 1]               --
│    └─Empty: 2-2355                     [64]                      --
│    └─Empty: 2-2356                     [64]                      --
│    └─Scaler: 2-2357                    [8, 64, 6]                --
│    └─Empty: 2-2358                     [8, 64, 6]                --
│    └─Empty: 2-2359                     [8, 64, 6]                --
│    └─Clamp: 2-2360                     [8, 64, 6]                --
├─FusedConv1dBNReLU: 1-178               [8, 64, 2]                12,358
│    └─OutputShiftSqueeze: 2-2361        --                        --
│    └─One: 2-2362                       [1]                       --
│    └─OutputScale: 2-2363               --                        --
│    └─Empty: 2-2364                     [64, 64, 3]               --
│    └─Empty: 2-2365                     [64, 64, 3]               --
│    └─Empty: 2-2366                     [64]                      --
│    └─Empty: 2-2367                     [64]                      --
│    └─BatchNorm1d: 2-2368               [8, 64, 2]                --
│    └─Scaler: 2-2369                    [8, 64, 2]                --
│    └─ReLU: 2-2370                      [8, 64, 2]                --
│    └─Empty: 2-2371                     [8, 64, 2]                --
│    └─Clamp: 2-2372                     [8, 64, 2]                --
├─Conv1d: 1-179                          [8, 64, 2]                4,166
│    └─OutputShiftSqueeze: 2-2373        --                        --
│    └─One: 2-2374                       [1]                       --
│    └─OutputScale: 2-2375               --                        --
│    └─Empty: 2-2376                     [64, 64, 1]               --
│    └─Empty: 2-2377                     [64, 64, 1]               --
│    └─Empty: 2-2378                     [64]                      --
│    └─Empty: 2-2379                     [64]                      --
│    └─Scaler: 2-2380                    [8, 64, 2]                --
│    └─Empty: 2-2381                     [8, 64, 2]                --
│    └─Empty: 2-2382                     [8, 64, 2]                --
│    └─Clamp: 2-2383                     [8, 64, 2]                --
├─FusedLinearReLU: 1-180                 [8, 32]                   4,134
│    └─OutputShiftSqueeze: 2-2384        --                        --
│    └─One: 2-2385                       [1]                       --
│    └─OutputScale: 2-2386               --                        --
│    └─Empty: 2-2387                     [32, 128]                 --
│    └─Empty: 2-2388                     [32, 128]                 --
│    └─Empty: 2-2389                     [32]                      --
│    └─Empty: 2-2390                     [32]                      --
│    └─Scaler: 2-2391                    [8, 32]                   --
│    └─ReLU: 2-2392                      [8, 32]                   --
│    └─Empty: 2-2393                     [8, 32]                   --
│    └─Clamp: 2-2394                     [8, 32]                   --
├─Linear: 1-181                          [8, 4]                    134
│    └─OutputShiftSqueeze: 2-2395        --                        --
│    └─One: 2-2396                       [1]                       --
│    └─OutputScale: 2-2397               --                        --
│    └─Empty: 2-2398                     [4, 32]                   --
│    └─Empty: 2-2399                     [4, 32]                   --
│    └─Empty: 2-2400                     [8, 4]                    --
│    └─Empty: 2-2401                     [8, 4]                    --
│    └─Clamp: 2-2402                     [8, 4]                    --
==========================================================================================
Total params: 563,912
Trainable params: 563,744
Non-trainable params: 168
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 62.91
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 62.91
==========================================================================================
I - Epoch: 0
I - Training: 
	I - Batch: 50 | Loss: 1.387 | Acc: 24.000% | Wgt Acc: 26.124%
	I - Batch: 100 | Loss: 1.387 | Acc: 23.250% | Wgt Acc: 25.436%
	I - Batch: 150 | Loss: 1.386 | Acc: 23.667% | Wgt Acc: 25.478%
	I - Batch: 200 | Loss: 1.385 | Acc: 24.125% | Wgt Acc: 26.298%
	I - Batch: 250 | Loss: 1.385 | Acc: 24.150% | Wgt Acc: 26.078%
	I - Batch: 300 | Loss: 1.386 | Acc: 24.708% | Wgt Acc: 25.866%
I - num batch: 319
I - Train -- Loss: 1.386 | Acc: 24.931% | Wgt Acc: 25.867% | LR: 1.000000e-03 | Dur: 120.37s
I - Confusion Matrix: [row->prediction - col->label]
[[ 19.  24.  23.  19.]
 [314. 275. 354. 225.]
 [244. 191. 232. 185.]
 [120.  88. 125. 109.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.392 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 9.28s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Local maximum validation set accuracy:  22.94

I - Validation set results: 
[14-1-2-0.87][50-3-2-0.93][124-2-2-0.90][127-0-2-0.94][443-2-2-0.95][567-0-2-0.99][573-1-2-0.99][615-0-2-0.93][695-1-2-0.92][722-3-2-0.90]
[826-0-2-0.98][878-0-2-0.97][1103-0-2-0.83][1212-3-2-0.91][1368-0-2-0.97][2181-2-2-0.99][2476-2-2-0.94][2721-2-2-0.85][2818-1-2-0.90][2886-2-2-0.83]
[3231-2-2-0.99][3333-2-2-0.99][3482-2-2-0.99][3536-3-2-0.99][3625-1-2-0.99][3909-0-2-0.72][4035-0-2-0.96][4140-0-2-0.99][4214-1-2-0.99][4346-1-2-0.99]
[4581-2-2-0.99][4708-3-2-0.97][4838-3-2-0.99][4845-1-2-0.98][4868-0-2-0.99][4939-0-2-0.99][4984-2-2-0.99][5078-1-2-0.99][5396-0-2-0.94][5479-1-2-0.68]
[5717-0-2-0.99][5843-1-2-0.89][5949-3-2-0.95][5987-2-2-0.99][6014-3-2-0.89][6033-3-2-0.96][6313-0-2-0.99][6421-3-2-0.86][6500-1-2-0.83][6583-3-2-0.80]
[6683-3-2-0.93][6825-2-2-0.97][6998-3-2-0.99][7049-3-2-0.92][7517-1-2-0.77][7521-1-2-0.95][7528-1-2-0.99][7949-1-2-0.99][8135-1-2-0.99][8185-3-2-0.97]
[8269-3-2-0.97][8273-3-2-0.99][8543-3-2-0.98][8666-1-2-0.80][8672-0-2-0.90][8903-1-2-0.97][9001-2-2-0.99][9036-2-2-0.87][9281-3-2-0.96][9300-2-2-0.81]
[9571-0-2-0.82][9617-1-2-0.99][9644-2-2-0.95][9705-2-2-0.95][9801-0-2-0.92][9803-3-2-0.94][9865-3-2-0.99][9896-2-2-0.96][10314-1-2-0.87][10337-3-2-0.98]
[10403-0-2-0.99][10653-2-2-0.99][10704-2-2-0.95][10719-1-2-0.84][10727-1-2-0.90][10836-0-2-0.96][10969-2-2-0.84][11042-0-2-0.99][11088-1-2-0.99][11322-0-2-0.77]
[11398-2-2-0.99][11499-0-2-0.89][11502-3-2-0.87][11512-3-2-0.81][11608-1-2-0.98][11610-0-2-0.99][11692-0-2-0.99][11905-0-2-0.98][11993-1-2-0.85][12002-2-2-0.99]
[12052-0-2-0.99][12201-0-2-0.86][12235-2-2-0.99][12320-1-2-0.99][12377-2-2-0.99][12398-2-2-0.90][12503-1-2-0.99][12617-0-2-0.99][12685-3-2-0.99][12738-2-2-0.78]
[12742-2-2-0.99][12823-0-2-0.88][13110-1-2-0.99][13240-3-2-0.69][13253-1-2-0.79][13273-0-2-0.91][13634-1-2-0.92][13763-2-2-0.99][13905-3-2-0.99][14060-2-2-0.99]
[14065-3-2-0.99][14147-3-2-0.88][14595-2-2-0.85][14687-2-2-0.98][14788-2-2-0.99][14869-1-2-0.95][14872-3-2-0.99][14877-1-2-0.99][14927-0-2-0.94][15066-0-2-0.97]
[15175-1-2-0.99][15178-2-2-0.88][15375-3-2-0.84][15389-3-2-0.98][15568-2-2-0.99][15675-3-2-0.89][15869-1-2-0.77][16207-3-2-0.82][16236-0-2-0.89][16302-3-2-0.88]
[16331-2-2-0.99][16381-0-2-0.99][16488-1-2-0.80][16495-0-2-0.99][16650-0-2-0.99][16719-1-2-0.99][16801-0-2-0.90][16828-0-2-0.99][17137-3-2-0.93][17245-1-2-0.93]
[17278-3-2-0.96][17282-0-2-0.76][17311-2-2-0.99][17336-2-2-0.93][17608-3-2-0.90][17627-0-2-0.92][17877-3-2-0.76][17924-1-2-0.99][17984-3-2-0.97][18211-0-2-0.99]
[18276-3-2-0.94][18287-1-2-0.93][18394-0-2-0.92][18428-0-2-0.99][18442-0-2-0.84][18478-3-2-0.95][18607-0-2-0.99][18616-0-2-0.99][18663-0-2-0.70][18718-0-2-0.97]
[18766-2-2-0.99][18824-2-2-0.99][18890-3-2-0.99][18930-3-2-0.91][18938-3-2-0.95][19817-1-2-0.94][19839-0-2-0.99][19930-3-2-0.92][19944-0-2-0.89][20036-2-2-0.78]
[20101-3-2-0.95][20474-1-2-0.98][20547-3-2-0.98][20929-2-2-0.82][21245-1-2-0.79][21257-3-2-0.97][21293-1-2-0.86][21316-1-2-0.98][21384-1-2-0.98][21448-1-2-0.99]
[21483-0-2-0.95][21487-2-2-0.99][21714-0-2-0.80][21943-3-2-0.99][21947-0-2-0.97][21948-0-2-0.91][21965-2-2-0.99][21998-1-2-0.99][22025-0-2-0.99][22228-3-2-0.99]
[22446-1-2-0.91][22494-3-2-0.96][22757-0-2-0.89][22811-3-2-0.99][22976-3-2-0.99][22985-3-2-0.95][23014-0-2-0.98][23112-1-2-0.77][23144-3-2-0.93][23168-2-2-0.99]
[23219-0-2-0.95][23363-3-2-0.99][23470-0-2-0.81][23486-2-2-0.99][23497-0-2-0.83][23516-0-2-0.94][23690-1-2-0.99][23921-2-2-0.92][23936-1-2-0.95][24040-3-2-0.93]
[24111-1-2-0.95][24182-0-2-0.94][24238-3-2-0.97][24290-2-2-0.99][24345-0-2-0.93][24364-1-2-0.97][24427-3-2-0.83][24477-2-2-0.88][24495-2-2-0.99][24893-2-2-0.74]
[25012-1-2-0.90][25121-2-2-0.90][25165-3-2-0.99][25183-0-2-0.93][25297-3-2-0.83][25398-0-2-0.99][25574-2-2-0.99][25644-1-2-0.99][25718-1-2-0.96][25774-2-2-0.96]
[26032-3-2-0.99][26051-3-2-0.97][26120-0-2-0.99][26321-1-2-0.95][26732-1-2-0.75][26784-3-2-0.99][26827-3-2-0.84][26833-0-2-0.91][26838-2-2-0.83][26860-1-2-0.99]
[26948-0-2-0.99][27049-3-2-0.99][27098-1-2-0.88][27526-0-2-0.87][27639-3-2-0.99][27698-3-2-0.92][27772-0-2-0.84][27890-1-2-0.99][28040-0-2-0.99][28503-2-2-0.99]
[28577-1-2-0.87][28959-0-2-0.89][29198-3-2-0.99][29777-0-2-0.94][29877-2-2-0.90][30035-1-2-0.76][30098-0-2-0.92][30326-1-2-0.99][30572-2-2-0.92][30716-0-2-0.90]
[30806-2-2-0.72][30906-1-2-0.99][31007-0-2-0.99][31181-3-2-0.90][31238-0-2-0.82][31347-0-2-0.99][31422-2-2-0.91][31429-3-2-0.99][31431-0-2-0.98][31432-1-2-0.91]
[31477-0-2-0.99][31524-1-2-0.75][31597-1-2-0.73][31619-1-2-0.99][31701-0-2-0.99][31755-0-2-0.97][31854-3-2-0.93][32074-1-2-0.92][32078-3-2-0.90][32111-1-2-0.99]
[32127-1-2-0.99][32140-3-2-0.97][32263-2-2-0.91][32365-0-2-0.82][32411-2-2-0.83][32429-3-2-0.99][32473-3-2-0.95][32574-3-2-0.88][32584-0-2-0.99][32622-0-2-0.99]
[32858-3-2-0.95][32969-3-2-0.99][33016-2-2-0.99][33031-1-2-0.99][33035-2-2-0.94][33133-2-2-0.95][33173-2-2-0.88][33175-3-2-0.98][33306-3-2-0.73][33309-2-2-0.99]
[33474-0-2-0.99][33478-2-2-0.92][33618-1-2-0.95][33712-0-2-0.95][33782-2-2-0.99][33914-3-2-0.88][34076-3-2-0.92][34112-2-2-0.83][34138-2-2-0.92][34239-1-2-0.97]
[34364-2-2-0.96][34617-1-2-0.99][34751-3-2-0.95][34783-2-2-0.99][35015-3-2-0.91][35018-1-2-0.92][35288-2-2-0.85]
---------------------------
I - Epoch: 1
I - Training: 
	I - Batch: 50 | Loss: 1.389 | Acc: 25.750% | Wgt Acc: 23.120%
	I - Batch: 100 | Loss: 1.387 | Acc: 27.625% | Wgt Acc: 25.049%
	I - Batch: 150 | Loss: 1.387 | Acc: 27.500% | Wgt Acc: 25.838%
	I - Batch: 200 | Loss: 1.385 | Acc: 27.438% | Wgt Acc: 27.020%
	I - Batch: 250 | Loss: 1.387 | Acc: 26.700% | Wgt Acc: 26.843%
	I - Batch: 300 | Loss: 1.387 | Acc: 27.208% | Wgt Acc: 26.834%
I - num batch: 319
I - Train -- Loss: 1.387 | Acc: 27.248% | Wgt Acc: 26.760% | LR: 1.000000e-03 | Dur: 117.68s
I - Confusion Matrix: [row->prediction - col->label]
[[ 39.  33.  43.  35.]
 [156. 143. 172. 115.]
 [377. 326. 406. 282.]
 [125.  76. 113. 106.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.395 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 9.30s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 2
I - Training: 
	I - Batch: 50 | Loss: 1.387 | Acc: 29.750% | Wgt Acc: 26.930%
	I - Batch: 100 | Loss: 1.388 | Acc: 28.250% | Wgt Acc: 26.360%
	I - Batch: 150 | Loss: 1.387 | Acc: 28.167% | Wgt Acc: 25.973%
	I - Batch: 200 | Loss: 1.387 | Acc: 27.688% | Wgt Acc: 25.769%
	I - Batch: 250 | Loss: 1.386 | Acc: 28.400% | Wgt Acc: 26.664%
	I - Batch: 300 | Loss: 1.384 | Acc: 28.625% | Wgt Acc: 26.986%
I - num batch: 319
I - Train -- Loss: 1.385 | Acc: 28.504% | Wgt Acc: 26.805% | LR: 1.000000e-03 | Dur: 117.63s
I - Confusion Matrix: [row->prediction - col->label]
[[ 73.  53.  73.  58.]
 [ 19.   5.   5.   8.]
 [455. 414. 527. 351.]
 [150. 106. 129. 121.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.401 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 9.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 3
I - Training: 
	I - Batch: 50 | Loss: 1.392 | Acc: 27.500% | Wgt Acc: 24.705%
	I - Batch: 100 | Loss: 1.390 | Acc: 25.500% | Wgt Acc: 23.296%
	I - Batch: 150 | Loss: 1.390 | Acc: 24.833% | Wgt Acc: 24.175%
	I - Batch: 200 | Loss: 1.388 | Acc: 25.375% | Wgt Acc: 24.409%
	I - Batch: 250 | Loss: 1.387 | Acc: 25.400% | Wgt Acc: 24.327%
	I - Batch: 300 | Loss: 1.387 | Acc: 26.375% | Wgt Acc: 24.977%
I - num batch: 319
I - Train -- Loss: 1.388 | Acc: 26.423% | Wgt Acc: 24.938% | LR: 1.000000e-03 | Dur: 117.23s
I - Confusion Matrix: [row->prediction - col->label]
[[152. 143. 167. 128.]
 [147. 116. 148. 116.]
 [374. 307. 394. 283.]
 [ 24.  12.  25.  11.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.387 | Acc: 26.300% | Wgt Acc: 23.913% | Dur: 9.17s
I - Confusion Matrix: [row->prediction - col->label]
[[40. 28. 23. 41.]
 [ 0.  0.  0.  0.]
 [41. 38. 38. 37.]
 [ 7. 12. 14.  8.]]

I - Local maximum validation set accuracy:  26.30

I - Validation set results: 
[14-1-2-0.10][50-3-0-0.30][124-2-2-0.93][127-0-0-1.09][443-2-3-0.50][567-0-0-0.23][573-1-2-0.91][615-0-0-0.11][695-1-0-1.09][722-3-3-0.76]
[826-0-2-0.22][878-0-2-0.93][1103-0-0-1.09][1212-3-0-0.16][1368-0-0-1.01][2181-2-2-0.89][2476-2-2-0.93][2721-2-0-1.09][2818-1-0-1.05][2886-2-3-0.33]
[3231-2-3-0.15][3333-2-2-0.45][3482-2-2-0.93][3536-3-2-0.89][3625-1-3-0.39][3909-0-2-0.91][4035-0-2-0.93][4140-0-2-0.93][4214-1-0-1.09][4346-1-2-0.30]
[4581-2-0-1.08][4708-3-2-0.93][4838-3-2-0.73][4845-1-2-0.91][4868-0-2-0.93][4939-0-2-0.89][4984-2-0-1.06][5078-1-2-0.91][5396-0-2-0.38][5479-1-2-0.43]
[5717-0-0-1.03][5843-1-0-1.06][5949-3-2-0.54][5987-2-2-0.93][6014-3-0-0.05][6033-3-2-0.93][6313-0-2-0.92][6421-3-0-1.09][6500-1-2-0.23][6583-3-0-1.09]
[6683-3-0-0.66][6825-2-2-0.19][6998-3-2-0.93][7049-3-2-0.75][7517-1-3-0.77][7521-1-0-0.08][7528-1-2-0.93][7949-1-2-0.86][8135-1-2-0.93][8185-3-3-0.48]
[8269-3-2-0.14][8273-3-0-1.09][8543-3-0-0.92][8666-1-2-0.89][8672-0-0-0.13][8903-1-0-1.09][9001-2-2-0.90][9036-2-2-0.93][9281-3-2-0.93][9300-2-2-0.26]
[9571-0-0-0.04][9617-1-2-0.84][9644-2-2-0.93][9705-2-0-1.09][9801-0-0-0.99][9803-3-2-0.87][9865-3-2-0.90][9896-2-0-1.09][10314-1-0-1.09][10337-3-2-0.93]
[10403-0-2-0.50][10653-2-0-0.90][10704-2-2-0.68][10719-1-2-0.50][10727-1-0-1.09][10836-0-2-0.46][10969-2-2-0.87][11042-0-2-0.93][11088-1-2-0.93][11322-0-0-1.09]
[11398-2-0-1.09][11499-0-2-0.92][11502-3-0-0.97][11512-3-0-0.80][11608-1-2-0.93][11610-0-0-1.09][11692-0-0-1.09][11905-0-0-1.09][11993-1-3-0.51][12002-2-2-0.93]
[12052-0-3-0.82][12201-0-3-0.67][12235-2-2-0.46][12320-1-0-0.94][12377-2-0-1.05][12398-2-2-0.86][12503-1-0-1.09][12617-0-2-0.93][12685-3-2-0.93][12738-2-0-1.09]
[12742-2-2-0.93][12823-0-0-1.09][13110-1-2-0.93][13240-3-0-1.09][13253-1-0-1.09][13273-0-0-1.09][13634-1-0-1.09][13763-2-2-0.91][13905-3-0-1.09][14060-2-2-0.93]
[14065-3-2-0.54][14147-3-0-1.09][14595-2-0-1.09][14687-2-3-0.60][14788-2-2-0.89][14869-1-2-0.33][14872-3-2-0.93][14877-1-2-0.89][14927-0-0-1.09][15066-0-0-1.09]
[15175-1-3-0.32][15178-2-0-0.96][15375-3-0-1.06][15389-3-0-1.09][15568-2-0-1.09][15675-3-0-0.07][15869-1-2-0.93][16207-3-2-0.32][16236-0-2-0.63][16302-3-0-1.09]
[16331-2-2-0.93][16381-0-2-0.89][16488-1-0-1.09][16495-0-2-0.87][16650-0-0-0.97][16719-1-3-0.46][16801-0-2-0.13][16828-0-0-1.09][17137-3-0-1.09][17245-1-0-0.07]
[17278-3-3-0.20][17282-0-0-0.11][17311-2-3-0.18][17336-2-2-0.87][17608-3-3-0.76][17627-0-0-1.09][17877-3-0-1.09][17924-1-3-0.84][17984-3-0-0.77][18211-0-2-0.86]
[18276-3-3-0.65][18287-1-0-1.13][18394-0-0-0.12][18428-0-0-1.09][18442-0-2-0.72][18478-3-0-1.07][18607-0-0-0.77][18616-0-2-0.93][18663-0-2-0.63][18718-0-0-1.09]
[18766-2-3-0.32][18824-2-2-0.93][18890-3-2-0.89][18930-3-0-1.09][18938-3-2-0.92][19817-1-0-1.09][19839-0-0-1.09][19930-3-3-0.32][19944-0-3-0.16][20036-2-2-0.87]
[20101-3-2-0.93][20474-1-2-0.93][20547-3-2-0.11][20929-2-0-1.09][21245-1-0-0.15][21257-3-2-0.93][21293-1-2-0.15][21316-1-0-1.09][21384-1-2-0.93][21448-1-2-0.93]
[21483-0-0-1.09][21487-2-2-0.30][21714-0-3-0.14][21943-3-2-0.93][21947-0-2-0.77][21948-0-3-0.75][21965-2-2-0.93][21998-1-2-0.93][22025-0-2-0.87][22228-3-2-0.88]
[22446-1-0-1.09][22494-3-0-0.15][22757-0-3-0.26][22811-3-2-0.88][22976-3-2-0.93][22985-3-2-0.84][23014-0-2-0.93][23112-1-2-0.91][23144-3-0-0.22][23168-2-2-0.93]
[23219-0-2-0.86][23363-3-2-0.93][23470-0-0-1.05][23486-2-3-0.27][23497-0-0-0.26][23516-0-0-1.08][23690-1-0-0.94][23921-2-2-0.22][23936-1-2-0.90][24040-3-0-1.09]
[24111-1-3-0.22][24182-0-0-1.09][24238-3-3-0.32][24290-2-2-0.93][24345-0-2-0.93][24364-1-3-0.38][24427-3-2-0.14][24477-2-3-0.69][24495-2-2-0.93][24893-2-0-0.07]
[25012-1-3-0.63][25121-2-2-0.88][25165-3-2-0.93][25183-0-2-0.93][25297-3-0-1.07][25398-0-2-0.92][25574-2-2-0.93][25644-1-0-1.07][25718-1-2-0.70][25774-2-2-0.93]
[26032-3-0-1.09][26051-3-0-0.90][26120-0-2-0.92][26321-1-3-0.52][26732-1-0-1.08][26784-3-0-1.09][26827-3-0-0.80][26833-0-0-1.07][26838-2-3-0.63][26860-1-0-1.09]
[26948-0-2-0.93][27049-3-0-0.06][27098-1-0-1.09][27526-0-3-0.28][27639-3-2-0.93][27698-3-3-0.35][27772-0-0-1.05][27890-1-2-0.85][28040-0-0-1.09][28503-2-2-0.93]
[28577-1-0-0.09][28959-0-2-0.12][29198-3-2-0.93][29777-0-0-1.09][29877-2-3-0.54][30035-1-2-0.20][30098-0-2-0.58][30326-1-3-0.73][30572-2-2-0.74][30716-0-2-0.30]
[30806-2-0-0.29][30906-1-0-0.04][31007-0-2-0.93][31181-3-2-0.91][31238-0-2-0.88][31347-0-0-1.09][31422-2-0-1.09][31429-3-0-0.08][31431-0-2-0.12][31432-1-2-0.92]
[31477-0-2-0.83][31524-1-0-1.05][31597-1-2-0.72][31619-1-3-0.37][31701-0-2-0.36][31755-0-0-1.08][31854-3-0-1.09][32074-1-2-0.90][32078-3-2-0.13][32111-1-2-0.93]
[32127-1-2-0.93][32140-3-0-1.09][32263-2-3-0.36][32365-0-0-1.09][32411-2-0-1.09][32429-3-2-0.86][32473-3-0-1.09][32574-3-2-0.36][32584-0-2-0.93][32622-0-0-1.09]
[32858-3-0-0.13][32969-3-2-0.55][33016-2-2-0.29][33031-1-2-0.93][33035-2-2-0.93][33133-2-3-0.23][33173-2-0-0.91][33175-3-2-0.69][33306-3-0-0.08][33309-2-0-1.00]
[33474-0-2-0.92][33478-2-0-1.09][33618-1-0-1.09][33712-0-0-0.89][33782-2-2-0.61][33914-3-0-1.09][34076-3-0-1.06][34112-2-3-0.40][34138-2-0-0.86][34239-1-2-0.93]
[34364-2-3-0.43][34617-1-2-0.93][34751-3-0-1.09][34783-2-0-1.11][35015-3-0-0.81][35018-1-2-0.88][35288-2-0-1.09]
---------------------------
I - Epoch: 4
I - Training: 
	I - Batch: 50 | Loss: 1.384 | Acc: 28.250% | Wgt Acc: 26.138%
	I - Batch: 100 | Loss: 1.380 | Acc: 30.000% | Wgt Acc: 27.416%
	I - Batch: 150 | Loss: 1.379 | Acc: 29.750% | Wgt Acc: 27.553%
	I - Batch: 200 | Loss: 1.376 | Acc: 30.062% | Wgt Acc: 28.431%
	I - Batch: 250 | Loss: 1.379 | Acc: 29.750% | Wgt Acc: 28.629%
	I - Batch: 300 | Loss: 1.377 | Acc: 29.542% | Wgt Acc: 28.368%
I - num batch: 319
I - Train -- Loss: 1.376 | Acc: 29.878% | Wgt Acc: 28.592% | LR: 1.000000e-03 | Dur: 117.33s
I - Confusion Matrix: [row->prediction - col->label]
[[271. 157. 225. 205.]
 [ 93. 127. 140.  93.]
 [246. 251. 302. 179.]
 [ 87.  43.  67.  61.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.415 | Acc: 28.440% | Wgt Acc: 25.272% | Dur: 9.23s
I - Confusion Matrix: [row->prediction - col->label]
[[25. 15.  7. 14.]
 [ 0.  0.  0.  0.]
 [63. 63. 68. 72.]
 [ 0.  0.  0.  0.]]

I - Local maximum validation set accuracy:  28.44

I - Validation set results: 
[14-1-2-2.87][50-3-2-2.87][124-2-2-2.87][127-0-2-2.37][443-2-2-2.85][567-0-2-2.60][573-1-2-2.85][615-0-0-1.80][695-1-0-2.07][722-3-2-2.82]
[826-0-0-2.27][878-0-2-2.87][1103-0-2-2.87][1212-3-2-2.87][1368-0-0-3.27][2181-2-0-3.31][2476-2-2-2.87][2721-2-2-2.84][2818-1-0-3.28][2886-2-2-2.87]
[3231-2-0-3.30][3333-2-2-2.87][3482-2-2-2.87][3536-3-2-2.87][3625-1-2-2.85][3909-0-2-2.87][4035-0-2-2.86][4140-0-2-2.84][4214-1-0-3.34][4346-1-0-3.12]
[4581-2-2-2.87][4708-3-2-2.86][4838-3-2-2.87][4845-1-2-2.87][4868-0-2-1.96][4939-0-2-2.87][4984-2-2-2.75][5078-1-2-2.87][5396-0-2-2.87][5479-1-2-2.87]
[5717-0-2-2.87][5843-1-2-2.85][5949-3-2-2.87][5987-2-2-2.87][6014-3-2-2.87][6033-3-0-3.32][6313-0-0-3.17][6421-3-2-2.78][6500-1-2-2.87][6583-3-2-2.87]
[6683-3-2-2.87][6825-2-2-2.87][6998-3-2-2.87][7049-3-2-2.87][7517-1-2-2.87][7521-1-2-2.87][7528-1-0-3.18][7949-1-2-2.87][8135-1-2-2.87][8185-3-2-2.71]
[8269-3-0-2.95][8273-3-2-2.87][8543-3-0-3.11][8666-1-2-2.87][8672-0-2-2.77][8903-1-0-3.07][9001-2-0-3.30][9036-2-2-2.87][9281-3-0-3.28][9300-2-2-2.87]
[9571-0-2-2.87][9617-1-2-2.87][9644-2-2-2.87][9705-2-2-2.87][9801-0-2-2.87][9803-3-2-2.87][9865-3-2-2.57][9896-2-2-2.87][10314-1-2-2.87][10337-3-2-2.87]
[10403-0-0-3.12][10653-2-2-2.85][10704-2-2-2.87][10719-1-0-2.49][10727-1-2-2.87][10836-0-0-3.15][10969-2-2-2.87][11042-0-2-2.87][11088-1-2-2.87][11322-0-2-2.87]
[11398-2-2-2.81][11499-0-2-2.87][11502-3-2-2.83][11512-3-2-2.87][11608-1-2-2.87][11610-0-2-2.87][11692-0-0-3.29][11905-0-2-2.68][11993-1-2-2.87][12002-2-2-2.85]
[12052-0-0-1.57][12201-0-2-2.87][12235-2-2-2.87][12320-1-0-3.35][12377-2-2-2.52][12398-2-0-1.86][12503-1-0-3.34][12617-0-2-2.87][12685-3-2-2.67][12738-2-2-2.87]
[12742-2-2-2.87][12823-0-2-2.87][13110-1-2-2.87][13240-3-2-2.87][13253-1-2-2.87][13273-0-2-2.81][13634-1-2-2.87][13763-2-2-2.87][13905-3-0-2.71][14060-2-2-2.87]
[14065-3-0-3.24][14147-3-2-2.87][14595-2-2-2.06][14687-2-2-2.86][14788-2-2-2.87][14869-1-0-3.32][14872-3-0-3.33][14877-1-2-2.87][14927-0-2-2.45][15066-0-0-3.31]
[15175-1-0-3.34][15178-2-2-2.87][15375-3-2-2.87][15389-3-0-3.10][15568-2-2-2.23][15675-3-2-2.87][15869-1-2-2.87][16207-3-2-2.87][16236-0-2-2.62][16302-3-2-2.80]
[16331-2-2-2.87][16381-0-2-2.87][16488-1-2-2.86][16495-0-0-2.71][16650-0-2-2.77][16719-1-2-2.87][16801-0-0-2.85][16828-0-0-3.32][17137-3-2-2.87][17245-1-0-3.31]
[17278-3-0-3.32][17282-0-2-2.87][17311-2-2-2.87][17336-2-2-2.87][17608-3-2-2.57][17627-0-2-2.39][17877-3-2-2.83][17924-1-2-2.87][17984-3-0-3.00][18211-0-2-2.87]
[18276-3-2-2.86][18287-1-2-2.87][18394-0-2-2.87][18428-0-2-2.87][18442-0-2-2.82][18478-3-0-3.22][18607-0-2-2.87][18616-0-2-2.87][18663-0-2-2.87][18718-0-0-3.33]
[18766-2-2-2.87][18824-2-2-2.87][18890-3-2-2.81][18930-3-2-2.63][18938-3-2-2.87][19817-1-2-2.19][19839-0-0-3.06][19930-3-2-2.79][19944-0-2-2.87][20036-2-2-2.87]
[20101-3-2-2.87][20474-1-2-2.86][20547-3-2-2.86][20929-2-0-3.21][21245-1-2-2.87][21257-3-2-2.87][21293-1-2-2.87][21316-1-2-2.86][21384-1-2-2.82][21448-1-2-2.87]
[21483-0-2-2.87][21487-2-0-3.34][21714-0-0-1.96][21943-3-2-2.87][21947-0-2-2.87][21948-0-2-2.87][21965-2-2-2.55][21998-1-2-2.87][22025-0-2-2.87][22228-3-2-2.87]
[22446-1-2-2.87][22494-3-2-2.87][22757-0-2-2.87][22811-3-2-2.87][22976-3-0-3.32][22985-3-2-2.87][23014-0-0-3.29][23112-1-2-2.87][23144-3-2-2.87][23168-2-2-2.87]
[23219-0-2-2.86][23363-3-2-2.87][23470-0-2-2.87][23486-2-2-2.64][23497-0-2-2.87][23516-0-2-2.87][23690-1-2-2.86][23921-2-2-2.87][23936-1-2-2.87][24040-3-2-2.87]
[24111-1-2-2.87][24182-0-2-2.86][24238-3-2-2.84][24290-2-2-2.87][24345-0-0-2.55][24364-1-0-2.53][24427-3-2-2.87][24477-2-2-2.87][24495-2-2-2.87][24893-2-2-2.87]
[25012-1-0-3.12][25121-2-2-2.87][25165-3-2-2.87][25183-0-2-2.87][25297-3-2-2.85][25398-0-2-2.08][25574-2-2-2.87][25644-1-2-2.87][25718-1-2-2.87][25774-2-2-2.87]
[26032-3-2-2.75][26051-3-2-2.87][26120-0-0-3.34][26321-1-2-2.86][26732-1-2-2.87][26784-3-2-2.86][26827-3-2-2.87][26833-0-2-2.85][26838-2-2-2.87][26860-1-2-2.87]
[26948-0-2-2.87][27049-3-0-3.00][27098-1-2-2.87][27526-0-2-2.87][27639-3-2-2.12][27698-3-2-2.87][27772-0-2-2.87][27890-1-2-2.87][28040-0-0-3.34][28503-2-2-2.87]
[28577-1-2-2.87][28959-0-0-3.12][29198-3-2-2.87][29777-0-0-3.29][29877-2-2-2.87][30035-1-2-2.87][30098-0-2-2.86][30326-1-2-2.87][30572-2-2-2.87][30716-0-2-2.86]
[30806-2-2-2.87][30906-1-2-2.87][31007-0-2-2.87][31181-3-2-2.78][31238-0-2-2.87][31347-0-0-3.28][31422-2-2-2.87][31429-3-2-2.80][31431-0-2-2.87][31432-1-2-2.62]
[31477-0-0-3.34][31524-1-2-2.87][31597-1-2-2.87][31619-1-2-2.76][31701-0-2-2.73][31755-0-2-2.87][31854-3-2-2.84][32074-1-2-2.87][32078-3-2-2.87][32111-1-2-2.87]
[32127-1-2-2.78][32140-3-2-2.76][32263-2-2-2.71][32365-0-2-2.84][32411-2-2-2.72][32429-3-0-2.38][32473-3-2-2.87][32574-3-2-2.87][32584-0-2-2.87][32622-0-0-3.25]
[32858-3-2-2.87][32969-3-2-2.77][33016-2-0-3.34][33031-1-2-2.85][33035-2-2-2.87][33133-2-2-2.87][33173-2-2-2.87][33175-3-2-2.87][33306-3-2-2.87][33309-2-2-2.87]
[33474-0-2-2.87][33478-2-2-2.02][33618-1-0-3.02][33712-0-0-2.15][33782-2-2-2.87][33914-3-2-2.87][34076-3-2-2.87][34112-2-2-2.87][34138-2-2-2.87][34239-1-2-2.87]
[34364-2-2-2.83][34617-1-2-2.87][34751-3-2-2.87][34783-2-2-2.82][35015-3-2-2.87][35018-1-2-2.87][35288-2-2-2.87]
---------------------------
I - Epoch: 5
I - Training: 
	I - Batch: 50 | Loss: 1.364 | Acc: 28.750% | Wgt Acc: 28.136%
	I - Batch: 100 | Loss: 1.368 | Acc: 30.375% | Wgt Acc: 30.225%
	I - Batch: 150 | Loss: 1.368 | Acc: 30.667% | Wgt Acc: 30.641%
	I - Batch: 200 | Loss: 1.373 | Acc: 30.750% | Wgt Acc: 29.966%
	I - Batch: 250 | Loss: 1.378 | Acc: 30.650% | Wgt Acc: 29.416%
	I - Batch: 300 | Loss: 1.380 | Acc: 28.917% | Wgt Acc: 27.968%
I - num batch: 319
I - Train -- Loss: 1.380 | Acc: 28.897% | Wgt Acc: 27.849% | LR: 1.000000e-03 | Dur: 117.15s
I - Confusion Matrix: [row->prediction - col->label]
[[213. 110. 169. 130.]
 [161. 190. 221. 161.]
 [296. 258. 319. 233.]
 [ 27.  20.  25.  14.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.385 | Acc: 26.911% | Wgt Acc: 23.913% | Dur: 9.17s
I - Confusion Matrix: [row->prediction - col->label]
[[88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 6
I - Training: 
	I - Batch: 50 | Loss: 1.379 | Acc: 30.750% | Wgt Acc: 30.666%
	I - Batch: 100 | Loss: 1.376 | Acc: 29.125% | Wgt Acc: 29.680%
	I - Batch: 150 | Loss: 1.376 | Acc: 29.750% | Wgt Acc: 30.162%
	I - Batch: 200 | Loss: 1.375 | Acc: 30.062% | Wgt Acc: 29.898%
	I - Batch: 250 | Loss: 1.378 | Acc: 28.500% | Wgt Acc: 28.686%
	I - Batch: 300 | Loss: 1.379 | Acc: 27.750% | Wgt Acc: 28.090%
I - num batch: 319
I - Train -- Loss: 1.381 | Acc: 27.326% | Wgt Acc: 27.787% | LR: 1.000000e-03 | Dur: 117.21s
I - Confusion Matrix: [row->prediction - col->label]
[[134.  76. 119. 119.]
 [297. 309. 352. 249.]
 [212. 160. 205. 122.]
 [ 54.  33.  58.  48.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.383 | Acc: 23.853% | Wgt Acc: 26.495% | Dur: 9.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

I - Epoch: 7
I - Training: 
	I - Batch: 50 | Loss: 1.387 | Acc: 23.750% | Wgt Acc: 25.380%
	I - Batch: 100 | Loss: 1.388 | Acc: 25.875% | Wgt Acc: 25.807%
	I - Batch: 150 | Loss: 1.389 | Acc: 24.833% | Wgt Acc: 25.253%
	I - Batch: 200 | Loss: 1.389 | Acc: 24.625% | Wgt Acc: 25.081%
	I - Batch: 250 | Loss: 1.389 | Acc: 25.350% | Wgt Acc: 25.208%
	I - Batch: 300 | Loss: 1.388 | Acc: 26.458% | Wgt Acc: 25.845%
I - num batch: 319
I - Train -- Loss: 1.389 | Acc: 25.913% | Wgt Acc: 25.327% | LR: 1.000000e-03 | Dur: 117.26s
I - Confusion Matrix: [row->prediction - col->label]
[[116.  99. 102.  66.]
 [122. 106. 139.  93.]
 [297. 235. 321. 262.]
 [162. 138. 172. 117.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.391 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 9.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 8
I - Training: 
	I - Batch: 50 | Loss: 1.385 | Acc: 26.500% | Wgt Acc: 24.064%
	I - Batch: 100 | Loss: 1.389 | Acc: 27.000% | Wgt Acc: 24.414%
	I - Batch: 150 | Loss: 1.390 | Acc: 26.083% | Wgt Acc: 23.895%
	I - Batch: 200 | Loss: 1.389 | Acc: 26.188% | Wgt Acc: 23.917%
	I - Batch: 250 | Loss: 1.389 | Acc: 26.700% | Wgt Acc: 24.338%
	I - Batch: 300 | Loss: 1.389 | Acc: 26.042% | Wgt Acc: 24.519%
I - num batch: 319
I - Train -- Loss: 1.389 | Acc: 25.795% | Wgt Acc: 24.222% | LR: 1.000000e-03 | Dur: 116.77s
I - Confusion Matrix: [row->prediction - col->label]
[[154. 151. 207. 120.]
 [116.  93. 115.  90.]
 [402. 320. 393. 311.]
 [ 25.  14.  19.  17.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.390 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 9.14s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 9
I - Training: 
	I - Batch: 50 | Loss: 1.388 | Acc: 24.000% | Wgt Acc: 26.102%
	I - Batch: 100 | Loss: 1.386 | Acc: 25.875% | Wgt Acc: 26.444%
	I - Batch: 150 | Loss: 1.390 | Acc: 24.667% | Wgt Acc: 24.588%
	I - Batch: 200 | Loss: 1.390 | Acc: 25.188% | Wgt Acc: 24.606%
	I - Batch: 250 | Loss: 1.389 | Acc: 24.800% | Wgt Acc: 24.172%
	I - Batch: 300 | Loss: 1.389 | Acc: 25.208% | Wgt Acc: 24.223%
I - num batch: 319
I - Train -- Loss: 1.389 | Acc: 25.481% | Wgt Acc: 24.390% | LR: 1.000000e-03 | Dur: 116.72s
I - Confusion Matrix: [row->prediction - col->label]
[[113. 124. 144.  99.]
 [144. 105. 139. 108.]
 [380. 293. 375. 275.]
 [ 60.  56.  76.  56.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.391 | Acc: 22.936% | Wgt Acc: 20.380% | Dur: 9.18s
I - Confusion Matrix: [row->prediction - col->label]
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [88. 78. 75. 86.]
 [ 0.  0.  0.  0.]]

I - Epoch: 10
I - Training: 
	I - Batch: 50 | Loss: 1.390 | Acc: 21.750% | Wgt Acc: 20.819%
	I - Batch: 100 | Loss: 1.388 | Acc: 24.625% | Wgt Acc: 22.813%
	I - Batch: 150 | Loss: 1.388 | Acc: 24.500% | Wgt Acc: 22.828%
	I - Batch: 200 | Loss: 1.386 | Acc: 24.938% | Wgt Acc: 23.570%
	I - Batch: 250 | Loss: 1.387 | Acc: 24.350% | Wgt Acc: 23.490%
	I - Batch: 300 | Loss: 1.384 | Acc: 25.375% | Wgt Acc: 24.699%
I - num batch: 319
I - Train -- Loss: 1.383 | Acc: 25.559% | Wgt Acc: 24.956% | LR: 5.000000e-04 | Dur: 117.20s
I - Confusion Matrix: [row->prediction - col->label]
[[147.  93. 159. 120.]
 [128. 133. 166.  93.]
 [294. 243. 287. 241.]
 [128. 109. 122.  84.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.390 | Acc: 27.829% | Wgt Acc: 26.766% | Dur: 9.21s
I - Confusion Matrix: [row->prediction - col->label]
[[55. 43. 39. 60.]
 [28. 30. 30. 22.]
 [ 5.  5.  6.  4.]
 [ 0.  0.  0.  0.]]

I - Epoch: 11
I - Training: 
	I - Batch: 50 | Loss: 1.378 | Acc: 32.250% | Wgt Acc: 30.960%
	I - Batch: 100 | Loss: 1.365 | Acc: 32.125% | Wgt Acc: 31.183%
	I - Batch: 150 | Loss: 1.362 | Acc: 31.750% | Wgt Acc: 30.994%
	I - Batch: 200 | Loss: 1.360 | Acc: 31.625% | Wgt Acc: 31.234%
	I - Batch: 250 | Loss: 1.357 | Acc: 31.950% | Wgt Acc: 31.571%
	I - Batch: 300 | Loss: 1.354 | Acc: 31.417% | Wgt Acc: 31.510%
I - num batch: 319
I - Train -- Loss: 1.355 | Acc: 31.370% | Wgt Acc: 31.369% | LR: 5.000000e-04 | Dur: 116.96s
I - Confusion Matrix: [row->prediction - col->label]
[[418. 217. 340. 335.]
 [179. 311. 326. 145.]
 [ 33.  28.  31.  19.]
 [ 67.  22.  37.  39.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.410 | Acc: 25.994% | Wgt Acc: 23.370% | Dur: 9.09s
I - Confusion Matrix: [row->prediction - col->label]
[[80. 71. 62. 75.]
 [ 8.  4. 12. 10.]
 [ 0.  3.  1.  1.]
 [ 0.  0.  0.  0.]]

I - Epoch: 12
I - Training: 
	I - Batch: 50 | Loss: 1.375 | Acc: 32.750% | Wgt Acc: 31.656%
	I - Batch: 100 | Loss: 1.353 | Acc: 34.500% | Wgt Acc: 34.097%
	I - Batch: 150 | Loss: 1.345 | Acc: 34.083% | Wgt Acc: 33.703%
	I - Batch: 200 | Loss: 1.342 | Acc: 33.750% | Wgt Acc: 33.324%
	I - Batch: 250 | Loss: 1.344 | Acc: 34.150% | Wgt Acc: 33.683%
	I - Batch: 300 | Loss: 1.346 | Acc: 33.250% | Wgt Acc: 32.711%
I - num batch: 319
I - Train -- Loss: 1.345 | Acc: 33.255% | Wgt Acc: 32.714% | LR: 5.000000e-04 | Dur: 117.13s
I - Confusion Matrix: [row->prediction - col->label]
[[517. 247. 365. 407.]
 [164. 310. 348. 129.]
 [ 14.  21.  20.   2.]
 [  2.   0.   1.   0.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.408 | Acc: 25.994% | Wgt Acc: 27.310% | Dur: 9.18s
I - Confusion Matrix: [row->prediction - col->label]
[[23. 14. 13. 29.]
 [57. 62. 62. 51.]
 [ 8.  2.  0.  6.]
 [ 0.  0.  0.  0.]]

I - Epoch: 13
I - Training: 
	I - Batch: 50 | Loss: 1.349 | Acc: 30.750% | Wgt Acc: 30.342%
	I - Batch: 100 | Loss: 1.343 | Acc: 32.625% | Wgt Acc: 32.485%
	I - Batch: 150 | Loss: 1.346 | Acc: 32.417% | Wgt Acc: 32.329%
	I - Batch: 200 | Loss: 1.349 | Acc: 33.062% | Wgt Acc: 32.681%
	I - Batch: 250 | Loss: 1.348 | Acc: 33.000% | Wgt Acc: 32.320%
	I - Batch: 300 | Loss: 1.350 | Acc: 32.750% | Wgt Acc: 31.884%
I - num batch: 319
I - Train -- Loss: 1.352 | Acc: 32.587% | Wgt Acc: 31.679% | LR: 5.000000e-04 | Dur: 118.41s
I - Confusion Matrix: [row->prediction - col->label]
[[451. 232. 314. 333.]
 [122. 239. 294. 123.]
 [ 89.  92. 118.  60.]
 [ 35.  15.   8.  22.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.352 | Acc: 31.804% | Wgt Acc: 28.601% | Dur: 9.23s
I - Confusion Matrix: [row->prediction - col->label]
[[53. 21. 28. 52.]
 [ 1.  5.  1.  9.]
 [34. 52. 46. 25.]
 [ 0.  0.  0.  0.]]

I - Local maximum validation set accuracy:  31.80

I - Validation set results: 
[14-1-1-0.36][50-3-2-2.83][124-2-2-2.84][127-0-0-1.31][443-2-2-2.83][567-0-0-0.66][573-1-2-2.83][615-0-0-1.31][695-1-0-1.26][722-3-0-1.11]
[826-0-0-1.31][878-0-1-2.58][1103-0-2-2.84][1212-3-0-1.31][1368-0-0-1.31][2181-2-0-1.19][2476-2-2-2.81][2721-2-0-1.26][2818-1-0-1.27][2886-2-2-2.84]
[3231-2-0-1.31][3333-2-0-1.26][3482-2-2-2.83][3536-3-0-0.74][3625-1-2-2.83][3909-0-2-2.84][4035-0-2-2.84][4140-0-2-0.56][4214-1-2-2.83][4346-1-0-1.15]
[4581-2-0-1.31][4708-3-0-1.08][4838-3-0-1.28][4845-1-0-0.13][4868-0-2-2.84][4939-0-2-2.84][4984-2-0-1.31][5078-1-2-2.83][5396-0-0-1.31][5479-1-2-2.84]
[5717-0-2-2.49][5843-1-2-2.74][5949-3-0-1.31][5987-2-2-2.85][6014-3-2-2.33][6033-3-1-2.48][6313-0-0-1.31][6421-3-0-1.31][6500-1-2-2.83][6583-3-2-2.83]
[6683-3-0-1.26][6825-2-0-1.31][6998-3-1-0.27][7049-3-0-0.57][7517-1-2-2.83][7521-1-0-1.02][7528-1-2-2.83][7949-1-2-2.85][8135-1-2-2.76][8185-3-0-1.30]
[8269-3-2-2.79][8273-3-0-1.31][8543-3-0-1.31][8666-1-2-2.83][8672-0-0-0.83][8903-1-0-1.31][9001-2-2-2.83][9036-2-2-2.83][9281-3-2-2.80][9300-2-2-2.84]
[9571-0-0-0.14][9617-1-2-2.83][9644-2-2-2.76][9705-2-0-1.08][9801-0-0-1.31][9803-3-1-2.61][9865-3-0-1.31][9896-2-0-1.31][10314-1-2-2.83][10337-3-0-1.31]
[10403-0-0-0.02][10653-2-2-0.69][10704-2-1-0.24][10719-1-2-2.83][10727-1-2-2.83][10836-0-0-1.30][10969-2-2-2.84][11042-0-2-2.83][11088-1-2-2.84][11322-0-2-2.84]
[11398-2-2-2.38][11499-0-2-2.83][11502-3-0-1.10][11512-3-2-2.83][11608-1-2-2.83][11610-0-0-1.31][11692-0-0-1.31][11905-0-0-1.31][11993-1-2-2.83][12002-2-0-0.85]
[12052-0-0-1.23][12201-0-0-1.23][12235-2-0-1.26][12320-1-0-1.31][12377-2-2-2.83][12398-2-2-2.85][12503-1-0-1.31][12617-0-2-2.81][12685-3-0-1.26][12738-2-0-1.30]
[12742-2-2-2.83][12823-0-0-0.96][13110-1-2-2.83][13240-3-2-2.86][13253-1-2-2.86][13273-0-2-2.83][13634-1-0-1.31][13763-2-0-1.17][13905-3-0-1.31][14060-2-2-2.83]
[14065-3-0-1.31][14147-3-0-1.30][14595-2-0-1.31][14687-2-2-0.55][14788-2-2-1.10][14869-1-0-1.31][14872-3-0-0.61][14877-1-1-1.90][14927-0-0-1.31][15066-0-0-0.48]
[15175-1-2-2.84][15178-2-2-2.83][15375-3-0-1.31][15389-3-0-1.31][15568-2-2-2.83][15675-3-2-2.84][15869-1-2-2.83][16207-3-2-2.82][16236-0-2-1.07][16302-3-1-2.19]
[16331-2-2-2.83][16381-0-0-1.12][16488-1-2-2.83][16495-0-0-1.31][16650-0-2-0.26][16719-1-2-2.83][16801-0-0-0.88][16828-0-0-1.31][17137-3-0-1.29][17245-1-2-2.83]
[17278-3-0-1.32][17282-0-2-2.83][17311-2-0-1.04][17336-2-2-2.83][17608-3-0-1.31][17627-0-0-1.31][17877-3-2-2.83][17924-1-0-1.31][17984-3-0-1.31][18211-0-2-2.83]
[18276-3-0-1.31][18287-1-2-2.83][18394-0-0-0.55][18428-0-2-2.80][18442-0-0-1.28][18478-3-0-1.31][18607-0-0-0.85][18616-0-2-0.49][18663-0-2-2.83][18718-0-0-1.31]
[18766-2-2-2.78][18824-2-2-2.83][18890-3-2-2.83][18930-3-2-2.83][18938-3-0-1.31][19817-1-0-1.28][19839-0-0-1.31][19930-3-0-0.95][19944-0-0-1.31][20036-2-2-2.84]
[20101-3-1-0.62][20474-1-0-1.26][20547-3-0-1.31][20929-2-2-2.83][21245-1-2-2.83][21257-3-2-2.80][21293-1-2-2.84][21316-1-0-1.15][21384-1-1-2.45][21448-1-2-2.78]
[21483-0-0-1.32][21487-2-0-1.31][21714-0-2-2.83][21943-3-2-2.83][21947-0-0-1.24][21948-0-2-2.76][21965-2-2-2.83][21998-1-0-1.31][22025-0-2-2.85][22228-3-2-2.54]
[22446-1-0-1.08][22494-3-0-1.31][22757-0-0-1.19][22811-3-0-1.31][22976-3-0-1.30][22985-3-0-1.31][23014-0-0-1.29][23112-1-2-2.83][23144-3-0-1.31][23168-2-0-1.31]
[23219-0-2-2.75][23363-3-2-2.83][23470-0-2-2.84][23486-2-0-1.31][23497-0-0-1.30][23516-0-0-1.30][23690-1-1-2.64][23921-2-2-2.83][23936-1-2-2.83][24040-3-0-1.31]
[24111-1-2-2.83][24182-0-0-1.31][24238-3-0-1.31][24290-2-2-2.84][24345-0-2-2.83][24364-1-0-1.25][24427-3-2-2.76][24477-2-2-2.12][24495-2-2-2.83][24893-2-2-2.84]
[25012-1-2-1.87][25121-2-2-2.83][25165-3-2-1.64][25183-0-2-2.83][25297-3-0-0.89][25398-0-0-1.31][25574-2-0-0.01][25644-1-0-0.48][25718-1-2-2.20][25774-2-2-2.82]
[26032-3-0-1.26][26051-3-0-1.29][26120-0-2-2.84][26321-1-2-2.67][26732-1-2-2.83][26784-3-0-1.31][26827-3-2-2.85][26833-0-0-1.31][26838-2-0-1.05][26860-1-0-1.31]
[26948-0-0-1.31][27049-3-0-1.07][27098-1-2-2.83][27526-0-0-0.22][27639-3-1-0.46][27698-3-0-1.31][27772-0-0-1.31][27890-1-2-2.83][28040-0-2-2.83][28503-2-2-2.78]
[28577-1-2-2.83][28959-0-0-1.05][29198-3-2-2.83][29777-0-0-1.31][29877-2-2-2.64][30035-1-2-2.84][30098-0-2-2.83][30326-1-2-2.83][30572-2-2-2.71][30716-0-2-2.83]
[30806-2-2-0.64][30906-1-2-2.79][31007-0-2-2.84][31181-3-2-2.83][31238-0-0-1.31][31347-0-0-1.31][31422-2-0-1.23][31429-3-0-1.31][31431-0-0-1.05][31432-1-2-2.81]
[31477-0-0-1.31][31524-1-2-2.84][31597-1-2-2.83][31619-1-0-0.36][31701-0-0-1.31][31755-0-0-1.31][31854-3-2-2.84][32074-1-2-2.83][32078-3-1-1.98][32111-1-2-2.83]
[32127-1-2-2.73][32140-3-0-1.32][32263-2-0-0.02][32365-0-2-2.83][32411-2-0-1.31][32429-3-0-1.31][32473-3-0-1.31][32574-3-2-2.85][32584-0-2-2.84][32622-0-2-2.83]
[32858-3-0-0.88][32969-3-2-2.84][33016-2-0-1.31][33031-1-2-2.71][33035-2-2-2.83][33133-2-0-1.15][33173-2-2-2.83][33175-3-2-2.83][33306-3-1-1.72][33309-2-0-1.31]
[33474-0-0-1.31][33478-2-2-2.79][33618-1-0-1.31][33712-0-0-1.17][33782-2-2-0.66][33914-3-0-1.31][34076-3-0-1.31][34112-2-2-1.30][34138-2-0-1.31][34239-1-2-2.83]
[34364-2-0-0.92][34617-1-1-2.60][34751-3-0-1.31][34783-2-2-2.83][35015-3-1-0.61][35018-1-2-2.83][35288-2-2-2.83]
---------------------------
I - Epoch: 14
I - Training: 
	I - Batch: 50 | Loss: 1.327 | Acc: 33.000% | Wgt Acc: 30.381%
	I - Batch: 100 | Loss: 1.321 | Acc: 34.125% | Wgt Acc: 32.940%
	I - Batch: 150 | Loss: 1.328 | Acc: 33.167% | Wgt Acc: 32.685%
	I - Batch: 200 | Loss: 1.318 | Acc: 32.812% | Wgt Acc: 33.389%
	I - Batch: 250 | Loss: 1.316 | Acc: 32.900% | Wgt Acc: 33.439%
	I - Batch: 300 | Loss: 1.312 | Acc: 34.292% | Wgt Acc: 34.241%
I - num batch: 319
I - Train -- Loss: 1.311 | Acc: 34.315% | Wgt Acc: 34.068% | LR: 5.000000e-04 | Dur: 117.46s
I - Confusion Matrix: [row->prediction - col->label]
[[368. 150. 215. 283.]
 [120. 237. 291.  81.]
 [ 63. 141. 151.  56.]
 [146.  50.  77. 118.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.297 | Acc: 35.780% | Wgt Acc: 32.405% | Dur: 9.24s
I - Confusion Matrix: [row->prediction - col->label]
[[75. 34. 35. 72.]
 [ 4.  7.  6.  1.]
 [ 9. 36. 33. 11.]
 [ 0.  1.  1.  2.]]

I - Local maximum validation set accuracy:  35.78

I - Validation set results: 
[14-1-0-2.10][50-3-0-3.67][124-2-1-1.24][127-0-0-3.84][443-2-2-0.19][567-0-0-3.81][573-1-1-2.92][615-0-0-3.81][695-1-0-3.83][722-3-0-3.82]
[826-0-0-3.84][878-0-0-3.81][1103-0-0-2.68][1212-3-0-3.81][1368-0-0-3.85][2181-2-0-2.46][2476-2-0-3.79][2721-2-2-2.28][2818-1-0-3.83][2886-2-2-2.99]
[3231-2-0-3.84][3333-2-0-1.67][3482-2-0-1.90][3536-3-0-3.82][3625-1-1-2.93][3909-0-2-2.98][4035-0-1-0.23][4140-0-0-3.77][4214-1-0-3.85][4346-1-0-3.82]
[4581-2-0-3.73][4708-3-2-1.46][4838-3-0-3.84][4845-1-2-0.42][4868-0-0-0.35][4939-0-2-2.96][4984-2-0-3.84][5078-1-0-3.82][5396-0-0-3.82][5479-1-2-3.00]
[5717-0-0-2.85][5843-1-2-2.96][5949-3-0-3.77][5987-2-1-2.91][6014-3-2-2.96][6033-3-0-3.81][6313-0-0-3.84][6421-3-0-3.84][6500-1-2-1.09][6583-3-0-3.84]
[6683-3-0-2.95][6825-2-0-3.80][6998-3-0-3.52][7049-3-0-3.44][7517-1-2-2.96][7521-1-1-2.95][7528-1-0-3.10][7949-1-0-3.08][8135-1-2-0.79][8185-3-0-3.84]
[8269-3-0-2.45][8273-3-0-3.83][8543-3-0-3.83][8666-1-2-2.97][8672-0-0-3.82][8903-1-0-3.84][9001-2-2-2.96][9036-2-2-3.00][9281-3-0-3.84][9300-2-1-2.91]
[9571-0-0-3.83][9617-1-0-3.42][9644-2-0-3.40][9705-2-2-1.08][9801-0-0-3.76][9803-3-3-0.34][9865-3-0-3.81][9896-2-1-2.91][10314-1-2-2.99][10337-3-0-3.41]
[10403-0-0-3.82][10653-2-1-2.06][10704-2-2-2.95][10719-1-0-3.50][10727-1-2-3.00][10836-0-0-3.83][10969-2-0-2.87][11042-0-0-3.40][11088-1-2-2.98][11322-0-0-3.50]
[11398-2-0-3.67][11499-0-2-1.06][11502-3-0-3.82][11512-3-2-2.98][11608-1-2-3.00][11610-0-0-3.85][11692-0-0-3.85][11905-0-0-3.84][11993-1-2-2.99][12002-2-0-3.82]
[12052-0-0-3.38][12201-0-0-3.84][12235-2-0-1.84][12320-1-0-3.84][12377-2-0-3.78][12398-2-0-2.83][12503-1-0-3.85][12617-0-2-2.99][12685-3-3-0.67][12738-2-2-1.26]
[12742-2-2-2.96][12823-0-0-3.83][13110-1-0-3.19][13240-3-0-3.78][13253-1-2-3.01][13273-0-0-3.80][13634-1-2-1.88][13763-2-0-3.35][13905-3-0-3.84][14060-2-2-2.96]
[14065-3-0-3.84][14147-3-0-3.49][14595-2-0-3.26][14687-2-2-0.75][14788-2-0-3.74][14869-1-0-3.82][14872-3-0-3.52][14877-1-2-2.97][14927-0-0-3.83][15066-0-0-3.82]
[15175-1-0-3.82][15178-2-0-3.06][15375-3-0-3.84][15389-3-0-3.84][15568-2-0-3.83][15675-3-2-0.60][15869-1-2-0.30][16207-3-0-3.82][16236-0-0-3.63][16302-3-0-3.12]
[16331-2-2-2.99][16381-0-0-3.75][16488-1-2-2.98][16495-0-0-3.84][16650-0-0-3.67][16719-1-2-2.98][16801-0-0-3.83][16828-0-1-2.79][17137-3-0-3.32][17245-1-0-2.69]
[17278-3-0-3.82][17282-0-2-2.82][17311-2-0-3.86][17336-2-2-2.97][17608-3-0-3.82][17627-0-0-3.76][17877-3-2-2.96][17924-1-0-3.86][17984-3-0-3.82][18211-0-0-3.71]
[18276-3-0-3.81][18287-1-1-2.89][18394-0-0-3.83][18428-0-0-3.83][18442-0-0-3.83][18478-3-0-3.84][18607-0-0-3.81][18616-0-0-3.82][18663-0-0-3.78][18718-0-0-3.84]
[18766-2-2-0.45][18824-2-1-1.70][18890-3-0-2.49][18930-3-0-2.49][18938-3-0-3.84][19817-1-0-3.50][19839-0-0-3.74][19930-3-0-1.46][19944-0-0-3.66][20036-2-2-2.96]
[20101-3-0-3.41][20474-1-0-2.96][20547-3-0-2.21][20929-2-2-2.96][21245-1-2-3.00][21257-3-0-3.33][21293-1-2-3.01][21316-1-2-1.32][21384-1-0-3.84][21448-1-2-1.53]
[21483-0-0-3.84][21487-2-0-3.72][21714-0-0-3.83][21943-3-0-3.49][21947-0-0-0.94][21948-0-0-3.84][21965-2-0-2.53][21998-1-2-2.07][22025-0-0-3.24][22228-3-0-3.79]
[22446-1-0-2.93][22494-3-0-3.83][22757-0-0-3.70][22811-3-0-3.70][22976-3-2-0.75][22985-3-0-3.81][23014-0-0-3.83][23112-1-2-2.96][23144-3-0-3.83][23168-2-0-3.44]
[23219-0-0-3.83][23363-3-0-2.53][23470-0-2-2.97][23486-2-0-3.81][23497-0-0-3.82][23516-0-0-3.84][23690-1-0-3.84][23921-2-2-2.97][23936-1-0-2.19][24040-3-2-2.99]
[24111-1-2-2.99][24182-0-0-3.80][24238-3-0-3.67][24290-2-2-2.96][24345-0-2-0.36][24364-1-0-3.82][24427-3-0-3.65][24477-2-2-2.96][24495-2-2-2.96][24893-2-2-2.96]
[25012-1-2-0.94][25121-2-2-3.00][25165-3-0-3.77][25183-0-1-2.26][25297-3-0-3.81][25398-0-0-3.72][25574-2-0-3.84][25644-1-2-2.96][25718-1-3-0.79][25774-2-2-2.98]
[26032-3-0-3.85][26051-3-0-3.82][26120-0-0-3.85][26321-1-1-1.57][26732-1-2-2.96][26784-3-0-3.83][26827-3-0-3.81][26833-0-0-3.84][26838-2-0-3.86][26860-1-0-3.68]
[26948-0-0-3.85][27049-3-0-3.84][27098-1-1-2.93][27526-0-0-3.84][27639-3-0-3.82][27698-3-0-3.82][27772-0-0-3.86][27890-1-0-2.58][28040-0-0-3.83][28503-2-2-2.97]
[28577-1-0-1.39][28959-0-0-3.83][29198-3-2-2.99][29777-0-0-3.83][29877-2-0-1.52][30035-1-2-2.96][30098-0-0-3.70][30326-1-2-3.00][30572-2-2-3.01][30716-0-2-0.68]
[30806-2-2-1.26][30906-1-0-3.48][31007-0-0-2.36][31181-3-0-3.84][31238-0-0-3.83][31347-0-0-3.84][31422-2-0-3.26][31429-3-0-3.69][31431-0-1-2.94][31432-1-0-3.50]
[31477-0-0-3.83][31524-1-0-1.38][31597-1-2-3.01][31619-1-0-3.84][31701-0-0-3.83][31755-0-0-3.84][31854-3-1-2.94][32074-1-2-2.64][32078-3-0-3.83][32111-1-2-2.96]
[32127-1-2-2.96][32140-3-0-3.78][32263-2-0-3.81][32365-0-2-2.68][32411-2-0-3.83][32429-3-0-3.83][32473-3-0-1.86][32574-3-0-3.78][32584-0-0-0.51][32622-0-0-3.76]
[32858-3-0-3.32][32969-3-0-3.82][33016-2-0-3.84][33031-1-2-0.30][33035-2-2-3.01][33133-2-2-2.85][33173-2-2-2.97][33175-3-2-1.69][33306-3-2-2.61][33309-2-0-3.84]
[33474-0-0-3.60][33478-2-0-3.12][33618-1-0-3.83][33712-0-0-3.82][33782-2-2-2.99][33914-3-0-2.80][34076-3-2-2.98][34112-2-0-2.55][34138-2-2-2.35][34239-1-1-1.31]
[34364-2-3-0.46][34617-1-0-1.90][34751-3-0-3.85][34783-2-2-2.96][35015-3-0-3.81][35018-1-2-3.00][35288-2-2-2.56]
---------------------------
I - Epoch: 15
I - Training: 
	I - Batch: 50 | Loss: 1.300 | Acc: 33.500% | Wgt Acc: 34.741%
	I - Batch: 100 | Loss: 1.308 | Acc: 33.750% | Wgt Acc: 34.762%
	I - Batch: 150 | Loss: 1.283 | Acc: 36.167% | Wgt Acc: 36.693%
	I - Batch: 200 | Loss: 1.277 | Acc: 36.438% | Wgt Acc: 36.910%
	I - Batch: 250 | Loss: 1.271 | Acc: 36.500% | Wgt Acc: 36.654%
	I - Batch: 300 | Loss: 1.259 | Acc: 37.417% | Wgt Acc: 37.180%
I - num batch: 319
I - Train -- Loss: 1.251 | Acc: 38.005% | Wgt Acc: 37.535% | LR: 5.000000e-04 | Dur: 117.50s
I - Confusion Matrix: [row->prediction - col->label]
[[446. 111. 173. 335.]
 [139. 322. 376. 108.]
 [ 57. 123. 151.  46.]
 [ 55.  22.  34.  49.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.311 | Acc: 32.416% | Wgt Acc: 28.804% | Dur: 9.22s
I - Confusion Matrix: [row->prediction - col->label]
[[56. 19. 24. 58.]
 [ 1.  0.  1.  3.]
 [31. 59. 50. 25.]
 [ 0.  0.  0.  0.]]

I - Epoch: 16
I - Training: 
	I - Batch: 50 | Loss: 1.183 | Acc: 44.250% | Wgt Acc: 39.865%
	I - Batch: 100 | Loss: 1.176 | Acc: 43.250% | Wgt Acc: 39.172%
	I - Batch: 150 | Loss: 1.156 | Acc: 43.833% | Wgt Acc: 41.028%
	I - Batch: 200 | Loss: 1.153 | Acc: 43.625% | Wgt Acc: 40.473%
	I - Batch: 250 | Loss: 1.158 | Acc: 45.300% | Wgt Acc: 42.074%
	I - Batch: 300 | Loss: 1.153 | Acc: 45.667% | Wgt Acc: 42.323%
I - num batch: 319
I - Train -- Loss: 1.146 | Acc: 45.819% | Wgt Acc: 42.436% | LR: 5.000000e-04 | Dur: 117.50s
I - Confusion Matrix: [row->prediction - col->label]
[[522.  73.  95. 338.]
 [ 17.  66.  95.  15.]
 [109. 415. 516. 122.]
 [ 49.  24.  28.  63.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.143 | Acc: 40.367% | Wgt Acc: 36.141% | Dur: 9.47s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6. 12. 63.]
 [ 1.  1.  0.  1.]
 [20. 69. 63. 19.]
 [ 2.  2.  0.  3.]]

I - Local maximum validation set accuracy:  40.37

I - Validation set results: 
[14-1-2-3.53][50-3-2-3.39][124-2-2-2.95][127-0-0-4.90][443-2-2-3.53][567-0-0-4.94][573-1-2-3.53][615-0-0-4.94][695-1-2-3.53][722-3-0-4.93]
[826-0-0-4.91][878-0-0-4.91][1103-0-2-3.53][1212-3-0-4.91][1368-0-0-4.57][2181-2-0-2.57][2476-2-2-3.53][2721-2-2-3.53][2818-1-2-3.53][2886-2-2-3.53]
[3231-2-2-3.53][3333-2-2-3.53][3482-2-2-3.53][3536-3-3-1.20][3625-1-2-3.53][3909-0-0-3.73][4035-0-0-4.92][4140-0-0-4.77][4214-1-0-4.91][4346-1-3-2.08]
[4581-2-2-3.53][4708-3-3-0.64][4838-3-2-3.53][4845-1-2-3.53][4868-0-2-1.27][4939-0-2-3.53][4984-2-2-1.31][5078-1-2-3.53][5396-0-0-4.90][5479-1-2-3.53]
[5717-0-0-4.28][5843-1-2-3.53][5949-3-0-4.94][5987-2-2-3.53][6014-3-0-4.97][6033-3-0-4.27][6313-0-0-4.97][6421-3-0-4.92][6500-1-2-1.17][6583-3-0-2.77]
[6683-3-2-2.00][6825-2-0-4.95][6998-3-0-3.63][7049-3-0-3.13][7517-1-2-3.53][7521-1-2-3.53][7528-1-0-2.10][7949-1-2-3.53][8135-1-0-2.41][8185-3-0-4.94]
[8269-3-2-3.53][8273-3-0-4.95][8543-3-0-4.93][8666-1-0-2.41][8672-0-0-4.92][8903-1-2-3.53][9001-2-2-3.53][9036-2-2-3.53][9281-3-2-3.53][9300-2-2-2.09]
[9571-0-0-2.52][9617-1-2-3.53][9644-2-2-3.53][9705-2-0-4.23][9801-0-0-4.92][9803-3-0-4.94][9865-3-0-4.90][9896-2-2-3.53][10314-1-2-3.53][10337-3-0-4.92]
[10403-0-2-3.53][10653-2-2-3.53][10704-2-2-3.43][10719-1-2-3.53][10727-1-2-3.53][10836-0-0-4.90][10969-2-0-4.07][11042-0-0-3.34][11088-1-2-3.53][11322-0-0-4.92]
[11398-2-2-3.53][11499-0-0-4.48][11502-3-0-4.92][11512-3-3-0.44][11608-1-2-3.53][11610-0-0-3.46][11692-0-0-4.91][11905-0-0-4.90][11993-1-2-3.53][12002-2-0-3.86]
[12052-0-0-4.90][12201-0-0-4.90][12235-2-2-3.53][12320-1-2-3.53][12377-2-2-3.53][12398-2-2-3.47][12503-1-2-3.53][12617-0-2-3.20][12685-3-0-4.55][12738-2-0-4.94]
[12742-2-2-3.53][12823-0-0-4.91][13110-1-2-3.53][13240-3-0-4.93][13253-1-2-3.53][13273-0-0-4.90][13634-1-2-3.51][13763-2-2-3.53][13905-3-0-2.99][14060-2-2-3.53]
[14065-3-0-4.95][14147-3-0-4.91][14595-2-2-3.53][14687-2-2-3.53][14788-2-2-3.52][14869-1-2-3.49][14872-3-2-3.53][14877-1-2-3.53][14927-0-0-4.33][15066-0-0-4.90]
[15175-1-2-3.53][15178-2-0-3.53][15375-3-0-4.90][15389-3-0-4.94][15568-2-2-3.53][15675-3-0-3.57][15869-1-2-3.52][16207-3-1-0.96][16236-0-2-2.37][16302-3-0-4.92]
[16331-2-2-3.53][16381-0-0-3.63][16488-1-2-3.53][16495-0-0-3.93][16650-0-0-4.90][16719-1-2-3.53][16801-0-0-4.90][16828-0-0-4.22][17137-3-0-4.82][17245-1-2-3.53]
[17278-3-2-0.79][17282-0-0-2.18][17311-2-2-3.53][17336-2-2-3.11][17608-3-0-4.91][17627-0-2-3.53][17877-3-2-3.53][17924-1-2-3.06][17984-3-0-4.92][18211-0-0-4.51]
[18276-3-0-4.90][18287-1-2-3.52][18394-0-0-4.92][18428-0-2-3.53][18442-0-0-4.90][18478-3-0-4.91][18607-0-2-0.94][18616-0-0-4.14][18663-0-0-4.90][18718-0-0-3.78]
[18766-2-2-3.53][18824-2-2-3.53][18890-3-2-3.53][18930-3-2-2.22][18938-3-0-4.97][19817-1-2-3.53][19839-0-2-2.56][19930-3-0-4.87][19944-0-2-3.53][20036-2-2-2.28]
[20101-3-2-3.53][20474-1-2-3.53][20547-3-2-3.53][20929-2-2-3.53][21245-1-2-3.53][21257-3-0-3.26][21293-1-2-3.53][21316-1-2-3.53][21384-1-2-3.53][21448-1-2-3.53]
[21483-0-2-0.49][21487-2-2-3.53][21714-0-1-1.40][21943-3-2-3.53][21947-0-0-4.84][21948-0-0-4.93][21965-2-2-3.53][21998-1-1-1.71][22025-0-3-0.52][22228-3-0-4.90]
[22446-1-2-3.53][22494-3-0-4.92][22757-0-0-4.91][22811-3-0-4.91][22976-3-2-1.10][22985-3-0-4.94][23014-0-0-4.90][23112-1-2-3.53][23144-3-0-4.91][23168-2-0-3.71]
[23219-0-0-4.93][23363-3-0-4.95][23470-0-0-3.39][23486-2-0-4.19][23497-0-0-4.91][23516-0-0-4.90][23690-1-2-3.53][23921-2-2-3.53][23936-1-0-3.18][24040-3-2-0.28]
[24111-1-2-3.53][24182-0-0-4.92][24238-3-0-4.95][24290-2-0-4.36][24345-0-0-3.87][24364-1-2-2.50][24427-3-0-4.94][24477-2-2-3.53][24495-2-2-3.49][24893-2-2-3.53]
[25012-1-2-3.53][25121-2-2-3.53][25165-3-0-4.29][25183-0-0-4.96][25297-3-0-4.46][25398-0-0-4.92][25574-2-2-3.53][25644-1-2-3.53][25718-1-2-1.43][25774-2-2-3.53]
[26032-3-0-4.91][26051-3-0-4.92][26120-0-2-3.53][26321-1-2-3.53][26732-1-2-3.53][26784-3-0-4.95][26827-3-0-4.92][26833-0-0-4.95][26838-2-0-2.17][26860-1-2-3.53]
[26948-0-2-0.43][27049-3-0-4.92][27098-1-2-3.53][27526-0-0-4.90][27639-3-0-4.97][27698-3-0-4.90][27772-0-0-4.91][27890-1-2-3.53][28040-0-2-0.48][28503-2-2-3.53]
[28577-1-2-3.53][28959-0-0-4.91][29198-3-2-3.45][29777-0-0-4.89][29877-2-2-3.53][30035-1-2-3.53][30098-0-0-4.92][30326-1-2-3.53][30572-2-2-3.28][30716-0-2-3.48]
[30806-2-2-1.89][30906-1-2-3.53][31007-0-0-4.41][31181-3-0-4.84][31238-0-0-4.90][31347-0-0-4.91][31422-2-2-3.53][31429-3-0-4.89][31431-0-0-2.67][31432-1-2-3.53]
[31477-0-0-4.90][31524-1-2-3.41][31597-1-2-3.53][31619-1-2-3.52][31701-0-0-4.90][31755-0-0-4.90][31854-3-0-4.83][32074-1-2-2.34][32078-3-0-4.08][32111-1-2-3.53]
[32127-1-2-3.53][32140-3-0-4.90][32263-2-2-3.50][32365-0-2-3.50][32411-2-0-4.92][32429-3-0-4.93][32473-3-0-4.97][32574-3-0-4.93][32584-0-2-1.66][32622-0-2-3.53]
[32858-3-0-4.90][32969-3-0-4.91][33016-2-2-3.53][33031-1-0-3.12][33035-2-2-3.53][33133-2-2-3.53][33173-2-2-2.62][33175-3-2-3.53][33306-3-2-3.53][33309-2-2-3.53]
[33474-0-2-1.07][33478-2-2-0.87][33618-1-3-0.74][33712-0-3-1.54][33782-2-2-3.53][33914-3-2-1.13][34076-3-0-4.95][34112-2-2-3.53][34138-2-2-3.48][34239-1-2-3.49]
[34364-2-2-3.53][34617-1-2-3.53][34751-3-0-4.97][34783-2-2-3.53][35015-3-0-4.97][35018-1-2-3.53][35288-2-2-3.53]
---------------------------
I - Epoch: 17
I - Training: 
	I - Batch: 50 | Loss: 1.084 | Acc: 42.250% | Wgt Acc: 42.610%
	I - Batch: 100 | Loss: 1.110 | Acc: 44.250% | Wgt Acc: 44.535%
	I - Batch: 150 | Loss: 1.109 | Acc: 44.083% | Wgt Acc: 44.471%
	I - Batch: 200 | Loss: 1.107 | Acc: 43.250% | Wgt Acc: 43.670%
	I - Batch: 250 | Loss: 1.094 | Acc: 44.200% | Wgt Acc: 44.321%
	I - Batch: 300 | Loss: 1.082 | Acc: 45.833% | Wgt Acc: 45.360%
I - num batch: 319
I - Train -- Loss: 1.079 | Acc: 46.290% | Wgt Acc: 45.665% | LR: 5.000000e-04 | Dur: 119.04s
I - Confusion Matrix: [row->prediction - col->label]
[[484.  28.  61. 298.]
 [ 61. 309. 372.  59.]
 [ 56. 197. 249.  44.]
 [ 96.  44.  52. 137.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.210 | Acc: 42.813% | Wgt Acc: 39.810% | Dur: 9.38s
I - Confusion Matrix: [row->prediction - col->label]
[[48.  6.  4. 23.]
 [ 0.  0.  1.  2.]
 [18. 67. 66. 35.]
 [22.  5.  4. 26.]]

I - Local maximum validation set accuracy:  42.81

I - Validation set results: 
[14-1-2-3.70][50-3-2-3.69][124-2-2-3.59][127-0-3-3.76][443-2-2-3.70][567-0-3-3.49][573-1-2-3.70][615-0-3-3.28][695-1-0-0.25][722-3-3-3.98]
[826-0-0-4.66][878-0-0-4.44][1103-0-0-3.97][1212-3-2-3.06][1368-0-0-4.74][2181-2-3-3.81][2476-2-2-3.70][2721-2-2-1.08][2818-1-0-4.06][2886-2-2-3.70]
[3231-2-2-3.70][3333-2-2-3.70][3482-2-2-3.67][3536-3-3-2.03][3625-1-2-3.70][3909-0-3-0.93][4035-0-0-4.67][4140-0-0-4.74][4214-1-2-3.63][4346-1-2-3.66]
[4581-2-2-3.57][4708-3-2-3.70][4838-3-1-0.16][4845-1-2-3.70][4868-0-0-4.75][4939-0-2-3.70][4984-2-2-3.70][5078-1-2-3.70][5396-0-0-4.74][5479-1-2-3.70]
[5717-0-0-4.41][5843-1-2-3.70][5949-3-0-4.71][5987-2-2-3.70][6014-3-2-3.23][6033-3-0-4.68][6313-0-3-2.25][6421-3-3-1.80][6500-1-2-3.70][6583-3-2-3.70]
[6683-3-3-1.84][6825-2-0-4.74][6998-3-3-3.41][7049-3-2-3.42][7517-1-2-3.70][7521-1-2-2.48][7528-1-2-3.62][7949-1-2-0.14][8135-1-3-3.67][8185-3-3-3.95]
[8269-3-2-3.66][8273-3-3-4.02][8543-3-0-4.81][8666-1-0-1.47][8672-0-0-4.78][8903-1-2-3.70][9001-2-2-3.70][9036-2-2-3.70][9281-3-2-3.55][9300-2-2-3.70]
[9571-0-0-4.50][9617-1-3-1.38][9644-2-2-3.70][9705-2-3-1.46][9801-0-2-3.09][9803-3-0-0.77][9865-3-0-4.77][9896-2-2-3.42][10314-1-2-3.56][10337-3-3-3.92]
[10403-0-0-1.83][10653-2-2-3.00][10704-2-2-3.70][10719-1-2-2.89][10727-1-2-3.11][10836-0-0-4.61][10969-2-2-3.54][11042-0-0-4.05][11088-1-2-3.70][11322-0-0-4.64]
[11398-2-2-3.70][11499-0-2-2.84][11502-3-3-1.03][11512-3-2-3.70][11608-1-2-3.70][11610-0-3-1.70][11692-0-3-3.55][11905-0-0-4.69][11993-1-2-3.70][12002-2-3-3.91]
[12052-0-3-2.43][12201-0-0-4.78][12235-2-2-3.70][12320-1-2-3.03][12377-2-2-3.48][12398-2-2-3.64][12503-1-2-3.70][12617-0-2-3.76][12685-3-2-3.70][12738-2-3-2.76]
[12742-2-2-3.70][12823-0-3-3.79][13110-1-2-3.70][13240-3-0-4.07][13253-1-2-3.42][13273-0-0-4.74][13634-1-2-3.70][13763-2-2-3.70][13905-3-2-3.72][14060-2-2-3.70]
[14065-3-3-3.45][14147-3-3-3.74][14595-2-2-3.58][14687-2-2-3.70][14788-2-2-3.63][14869-1-2-2.79][14872-3-0-4.47][14877-1-2-3.70][14927-0-2-1.49][15066-0-0-4.79]
[15175-1-2-3.64][15178-2-2-3.24][15375-3-3-4.09][15389-3-3-3.66][15568-2-2-2.95][15675-3-2-3.58][15869-1-2-3.70][16207-3-2-3.05][16236-0-2-3.60][16302-3-2-3.66]
[16331-2-2-3.70][16381-0-0-4.24][16488-1-0-0.51][16495-0-3-3.59][16650-0-0-4.74][16719-1-2-3.70][16801-0-0-4.75][16828-0-3-3.69][17137-3-0-4.77][17245-1-2-3.19]
[17278-3-2-0.41][17282-0-0-4.74][17311-2-2-3.70][17336-2-2-3.70][17608-3-3-4.14][17627-0-2-3.70][17877-3-1-0.51][17924-1-2-3.06][17984-3-0-4.74][18211-0-3-3.66]
[18276-3-0-4.78][18287-1-2-3.69][18394-0-0-4.21][18428-0-0-2.64][18442-0-0-4.54][18478-3-0-4.58][18607-0-3-1.25][18616-0-3-0.83][18663-0-0-4.68][18718-0-0-4.24]
[18766-2-2-3.70][18824-2-2-3.49][18890-3-2-3.70][18930-3-0-1.91][18938-3-2-1.97][19817-1-2-0.44][19839-0-2-2.89][19930-3-3-0.92][19944-0-2-3.70][20036-2-2-3.74]
[20101-3-2-3.37][20474-1-2-3.70][20547-3-2-3.70][20929-2-2-3.70][21245-1-2-3.70][21257-3-3-1.39][21293-1-2-3.70][21316-1-2-3.68][21384-1-2-3.70][21448-1-2-3.70]
[21483-0-0-1.95][21487-2-2-3.70][21714-0-3-3.27][21943-3-2-3.65][21947-0-2-2.47][21948-0-0-4.81][21965-2-2-3.70][21998-1-3-0.65][22025-0-2-3.71][22228-3-0-4.42]
[22446-1-2-3.70][22494-3-0-4.37][22757-0-0-4.79][22811-3-3-3.86][22976-3-2-3.70][22985-3-0-4.26][23014-0-0-4.28][23112-1-2-3.70][23144-3-0-4.42][23168-2-2-3.54]
[23219-0-0-4.28][23363-3-3-1.39][23470-0-2-1.49][23486-2-2-2.67][23497-0-0-4.44][23516-0-0-4.56][23690-1-2-3.70][23921-2-2-3.70][23936-1-2-3.70][24040-3-2-3.29]
[24111-1-2-3.27][24182-0-3-4.00][24238-3-3-2.63][24290-2-0-4.06][24345-0-3-1.66][24364-1-2-2.12][24427-3-3-3.36][24477-2-2-3.70][24495-2-2-2.70][24893-2-2-3.70]
[25012-1-2-3.70][25121-2-2-3.03][25165-3-2-3.44][25183-0-0-4.53][25297-3-2-3.49][25398-0-0-4.75][25574-2-2-3.70][25644-1-2-3.70][25718-1-2-3.70][25774-2-2-3.70]
[26032-3-2-1.53][26051-3-3-3.97][26120-0-0-4.74][26321-1-2-3.70][26732-1-2-3.70][26784-3-3-3.30][26827-3-3-2.16][26833-0-3-3.80][26838-2-2-3.70][26860-1-2-3.70]
[26948-0-0-3.58][27049-3-0-4.30][27098-1-2-3.71][27526-0-0-4.80][27639-3-3-2.43][27698-3-0-4.21][27772-0-0-4.67][27890-1-2-2.92][28040-0-2-3.70][28503-2-2-3.70]
[28577-1-2-3.70][28959-0-0-4.74][29198-3-2-3.70][29777-0-0-4.74][29877-2-2-3.44][30035-1-2-3.70][30098-0-3-2.26][30326-1-2-3.70][30572-2-2-3.70][30716-0-2-3.68]
[30806-2-2-3.70][30906-1-2-3.70][31007-0-0-4.50][31181-3-2-3.59][31238-0-2-3.66][31347-0-3-4.09][31422-2-1-1.01][31429-3-2-3.43][31431-0-2-3.56][31432-1-2-3.70]
[31477-0-0-4.36][31524-1-3-3.09][31597-1-2-3.61][31619-1-0-1.56][31701-0-0-4.74][31755-0-0-4.74][31854-3-2-2.50][32074-1-2-3.70][32078-3-2-1.82][32111-1-2-0.87]
[32127-1-2-3.70][32140-3-3-1.19][32263-2-2-2.10][32365-0-2-0.44][32411-2-0-4.34][32429-3-0-4.78][32473-3-0-4.62][32574-3-0-4.59][32584-0-3-0.45][32622-0-2-0.83]
[32858-3-0-4.74][32969-3-0-4.46][33016-2-2-3.70][33031-1-2-3.52][33035-2-2-3.70][33133-2-2-3.70][33173-2-2-3.70][33175-3-2-3.70][33306-3-2-3.49][33309-2-2-3.70]
[33474-0-3-0.87][33478-2-0-1.00][33618-1-0-0.59][33712-0-0-4.10][33782-2-2-3.70][33914-3-2-3.70][34076-3-2-3.69][34112-2-2-3.70][34138-2-2-3.56][34239-1-3-0.65]
[34364-2-2-3.70][34617-1-2-3.70][34751-3-2-3.58][34783-2-2-3.66][35015-3-3-0.99][35018-1-2-3.70][35288-2-2-3.70]
---------------------------
I - Epoch: 18
I - Training: 
	I - Batch: 50 | Loss: 1.039 | Acc: 49.750% | Wgt Acc: 46.348%
	I - Batch: 100 | Loss: 1.068 | Acc: 48.875% | Wgt Acc: 45.703%
	I - Batch: 150 | Loss: 1.073 | Acc: 50.667% | Wgt Acc: 47.487%
	I - Batch: 200 | Loss: 1.065 | Acc: 50.562% | Wgt Acc: 47.484%
	I - Batch: 250 | Loss: 1.057 | Acc: 50.350% | Wgt Acc: 47.325%
	I - Batch: 300 | Loss: 1.055 | Acc: 49.292% | Wgt Acc: 46.993%
I - num batch: 319
I - Train -- Loss: 1.054 | Acc: 49.274% | Wgt Acc: 46.957% | LR: 5.000000e-04 | Dur: 118.43s
I - Confusion Matrix: [row->prediction - col->label]
[[441.  25.  25. 210.]
 [ 19.  76. 120.  20.]
 [ 77. 425. 526.  96.]
 [160.  52.  63. 212.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.111 | Acc: 45.260% | Wgt Acc: 42.323% | Dur: 9.38s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  4.  6. 34.]
 [ 0.  0.  0.  0.]
 [15. 68. 62. 21.]
 [18.  6.  7. 31.]]

I - Local maximum validation set accuracy:  45.26

I - Validation set results: 
[14-1-2-3.19][50-3-2-1.77][124-2-2-1.80][127-0-0-4.79][443-2-2-2.72][567-0-0-3.88][573-1-2-2.57][615-0-0-4.65][695-1-2-1.27][722-3-0-4.92]
[826-0-0-4.92][878-0-0-4.90][1103-0-2-0.50][1212-3-3-0.95][1368-0-0-4.92][2181-2-3-4.12][2476-2-2-3.39][2721-2-2-0.58][2818-1-0-2.08][2886-2-2-2.97]
[3231-2-2-3.06][3333-2-2-1.50][3482-2-2-2.56][3536-3-3-3.15][3625-1-2-3.39][3909-0-2-1.30][4035-0-0-4.92][4140-0-0-4.82][4214-1-2-1.34][4346-1-3-2.97]
[4581-2-2-2.63][4708-3-2-2.68][4838-3-0-4.92][4845-1-2-2.20][4868-0-0-4.90][4939-0-2-0.62][4984-2-3-2.35][5078-1-2-0.08][5396-0-0-4.92][5479-1-2-2.18]
[5717-0-0-3.36][5843-1-2-3.16][5949-3-3-4.18][5987-2-2-2.89][6014-3-2-2.29][6033-3-0-4.89][6313-0-0-4.41][6421-3-3-3.25][6500-1-2-2.39][6583-3-3-0.61]
[6683-3-3-4.12][6825-2-0-3.30][6998-3-0-4.92][7049-3-3-1.12][7517-1-2-3.62][7521-1-3-1.81][7528-1-2-3.08][7949-1-2-3.22][8135-1-3-0.38][8185-3-0-4.92]
[8269-3-2-3.32][8273-3-3-4.12][8543-3-0-4.91][8666-1-2-2.28][8672-0-0-4.92][8903-1-2-0.27][9001-2-0-3.94][9036-2-2-3.49][9281-3-2-3.16][9300-2-2-2.20]
[9571-0-3-3.12][9617-1-2-2.19][9644-2-2-3.27][9705-2-3-1.55][9801-0-3-4.12][9803-3-2-0.23][9865-3-0-4.92][9896-2-2-2.16][10314-1-2-3.24][10337-3-3-4.12]
[10403-0-3-0.44][10653-2-2-3.02][10704-2-2-2.31][10719-1-2-2.49][10727-1-2-3.10][10836-0-0-4.92][10969-2-3-0.51][11042-0-0-4.85][11088-1-2-3.56][11322-0-0-4.86]
[11398-2-2-3.28][11499-0-2-0.66][11502-3-0-4.64][11512-3-2-2.48][11608-1-2-2.95][11610-0-3-1.23][11692-0-0-4.81][11905-0-0-4.92][11993-1-2-2.34][12002-2-2-1.99]
[12052-0-2-0.97][12201-0-0-4.92][12235-2-2-3.42][12320-1-2-0.88][12377-2-2-2.69][12398-2-3-0.06][12503-1-2-3.25][12617-0-2-3.29][12685-3-3-1.22][12738-2-3-1.43]
[12742-2-2-3.54][12823-0-0-4.67][13110-1-2-3.51][13240-3-0-4.25][13253-1-2-3.14][13273-0-0-4.92][13634-1-2-2.71][13763-2-2-3.13][13905-3-0-3.24][14060-2-2-3.26]
[14065-3-0-4.78][14147-3-3-3.83][14595-2-2-3.04][14687-2-2-3.59][14788-2-2-2.19][14869-1-2-2.42][14872-3-2-0.22][14877-1-2-3.56][14927-0-3-4.16][15066-0-0-4.91]
[15175-1-2-2.26][15178-2-2-1.21][15375-3-2-0.25][15389-3-0-4.50][15568-2-2-2.82][15675-3-2-3.05][15869-1-2-0.42][16207-3-3-1.08][16236-0-2-2.10][16302-3-0-4.36]
[16331-2-2-3.27][16381-0-3-3.48][16488-1-2-2.20][16495-0-3-2.93][16650-0-0-4.92][16719-1-2-3.78][16801-0-0-4.92][16828-0-0-4.92][17137-3-0-4.73][17245-1-2-0.17]
[17278-3-3-4.04][17282-0-2-0.16][17311-2-2-1.80][17336-2-2-2.72][17608-3-0-4.92][17627-0-2-0.31][17877-3-3-0.48][17924-1-0-4.86][17984-3-0-4.92][18211-0-3-3.39]
[18276-3-0-4.86][18287-1-2-2.50][18394-0-0-4.92][18428-0-0-4.92][18442-0-0-4.92][18478-3-0-4.92][18607-0-3-0.85][18616-0-3-1.00][18663-0-0-4.55][18718-0-0-4.75]
[18766-2-2-2.85][18824-2-2-3.15][18890-3-2-2.38][18930-3-2-2.96][18938-3-3-2.37][19817-1-2-1.92][19839-0-0-4.40][19930-3-0-4.77][19944-0-3-1.26][20036-2-2-3.24]
[20101-3-2-1.31][20474-1-2-0.27][20547-3-2-3.12][20929-2-2-3.63][21245-1-2-1.97][21257-3-2-2.35][21293-1-2-3.15][21316-1-2-2.32][21384-1-2-3.09][21448-1-2-3.51]
[21483-0-0-4.89][21487-2-2-3.00][21714-0-3-0.19][21943-3-2-0.86][21947-0-0-4.92][21948-0-0-4.92][21965-2-2-3.33][21998-1-2-2.53][22025-0-2-2.35][22228-3-3-3.68]
[22446-1-2-2.40][22494-3-0-4.92][22757-0-0-4.82][22811-3-3-4.22][22976-3-2-3.13][22985-3-0-4.92][23014-0-0-4.92][23112-1-2-3.47][23144-3-0-4.92][23168-2-0-3.70]
[23219-0-0-3.53][23363-3-3-1.37][23470-0-2-2.72][23486-2-2-2.75][23497-0-0-4.67][23516-0-0-4.65][23690-1-2-2.46][23921-2-2-2.86][23936-1-2-2.87][24040-3-0-4.23]
[24111-1-2-2.95][24182-0-3-4.14][24238-3-3-4.17][24290-2-0-3.57][24345-0-0-4.31][24364-1-2-2.64][24427-3-3-1.30][24477-2-2-0.71][24495-2-2-2.44][24893-2-2-2.20]
[25012-1-2-3.55][25121-2-2-3.43][25165-3-2-0.99][25183-0-3-0.74][25297-3-3-3.65][25398-0-0-4.92][25574-2-2-3.16][25644-1-2-3.47][25718-1-2-0.66][25774-2-2-3.24]
[26032-3-0-4.72][26051-3-3-4.15][26120-0-0-4.92][26321-1-2-1.93][26732-1-2-3.49][26784-3-3-4.14][26827-3-3-1.79][26833-0-3-4.18][26838-2-2-2.33][26860-1-3-3.70]
[26948-0-0-4.89][27049-3-0-4.92][27098-1-0-2.00][27526-0-0-4.92][27639-3-3-4.12][27698-3-0-4.69][27772-0-0-4.88][27890-1-2-2.72][28040-0-2-2.37][28503-2-2-2.33]
[28577-1-2-3.42][28959-0-0-4.92][29198-3-2-3.32][29777-0-0-4.92][29877-2-2-3.13][30035-1-2-3.09][30098-0-3-3.03][30326-1-2-3.54][30572-2-2-3.56][30716-0-2-3.21]
[30806-2-2-1.58][30906-1-2-2.96][31007-0-0-0.74][31181-3-3-4.08][31238-0-0-4.92][31347-0-0-4.92][31422-2-2-2.21][31429-3-0-3.26][31431-0-0-4.65][31432-1-2-1.12]
[31477-0-0-4.92][31524-1-2-1.14][31597-1-2-2.80][31619-1-0-2.06][31701-0-0-4.92][31755-0-0-4.92][31854-3-3-1.43][32074-1-2-3.39][32078-3-3-4.11][32111-1-2-1.91]
[32127-1-2-2.98][32140-3-0-4.11][32263-2-2-2.37][32365-0-2-2.10][32411-2-0-4.52][32429-3-0-4.87][32473-3-0-4.41][32574-3-0-4.80][32584-0-3-0.74][32622-0-2-2.66]
[32858-3-0-4.92][32969-3-0-4.87][33016-2-2-2.60][33031-1-3-3.64][33035-2-2-2.77][33133-2-2-2.00][33173-2-2-3.15][33175-3-2-2.50][33306-3-2-2.00][33309-2-2-2.74]
[33474-0-3-3.49][33478-2-0-2.46][33618-1-3-1.78][33712-0-0-4.47][33782-2-2-3.29][33914-3-3-4.13][34076-3-3-4.12][34112-2-2-3.47][34138-2-3-3.74][34239-1-2-1.61]
[34364-2-2-2.94][34617-1-2-3.30][34751-3-3-4.13][34783-2-2-1.19][35015-3-0-4.14][35018-1-2-3.33][35288-2-2-3.13]
---------------------------
I - Epoch: 19
I - Training: 
	I - Batch: 50 | Loss: 1.035 | Acc: 49.250% | Wgt Acc: 45.552%
	I - Batch: 100 | Loss: 1.037 | Acc: 50.875% | Wgt Acc: 47.579%
	I - Batch: 150 | Loss: 1.052 | Acc: 49.000% | Wgt Acc: 46.566%
	I - Batch: 200 | Loss: 1.048 | Acc: 49.188% | Wgt Acc: 47.585%
	I - Batch: 250 | Loss: 1.028 | Acc: 49.450% | Wgt Acc: 48.183%
	I - Batch: 300 | Loss: 1.036 | Acc: 48.917% | Wgt Acc: 47.864%
I - num batch: 319
I - Train -- Loss: 1.032 | Acc: 48.685% | Wgt Acc: 47.726% | LR: 5.000000e-04 | Dur: 118.46s
I - Confusion Matrix: [row->prediction - col->label]
[[451.  16.  36. 223.]
 [ 27. 241. 275.  42.]
 [ 68. 274. 354.  79.]
 [151.  47.  69. 194.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.194 | Acc: 38.532% | Wgt Acc: 37.364% | Dur: 9.41s
I - Confusion Matrix: [row->prediction - col->label]
[[ 9.  1.  0.  1.]
 [ 0.  0.  0.  0.]
 [25. 75. 71. 39.]
 [54.  2.  4. 46.]]

I - Epoch: 20
I - Training: 
	I - Batch: 50 | Loss: 0.992 | Acc: 49.250% | Wgt Acc: 47.109%
	I - Batch: 100 | Loss: 1.040 | Acc: 48.375% | Wgt Acc: 46.082%
	I - Batch: 150 | Loss: 1.019 | Acc: 49.833% | Wgt Acc: 47.597%
	I - Batch: 200 | Loss: 1.027 | Acc: 48.875% | Wgt Acc: 46.701%
	I - Batch: 250 | Loss: 1.019 | Acc: 48.950% | Wgt Acc: 47.193%
	I - Batch: 300 | Loss: 1.022 | Acc: 49.375% | Wgt Acc: 47.464%
I - num batch: 319
I - Train -- Loss: 1.019 | Acc: 49.470% | Wgt Acc: 47.443% | LR: 2.500000e-04 | Dur: 118.40s
I - Confusion Matrix: [row->prediction - col->label]
[[375.  18.  15. 159.]
 [  6.  67.  93.   8.]
 [104. 447. 562. 115.]
 [212.  46.  64. 256.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.074 | Acc: 44.343% | Wgt Acc: 41.576% | Dur: 9.36s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  5.  8. 45.]
 [ 0.  1.  0.  0.]
 [12. 65. 56. 10.]
 [19.  7. 11. 31.]]

I - Epoch: 21
I - Training: 
	I - Batch: 50 | Loss: 1.010 | Acc: 52.500% | Wgt Acc: 51.655%
	I - Batch: 100 | Loss: 0.987 | Acc: 53.750% | Wgt Acc: 52.961%
	I - Batch: 150 | Loss: 0.995 | Acc: 52.750% | Wgt Acc: 51.716%
	I - Batch: 200 | Loss: 0.985 | Acc: 53.000% | Wgt Acc: 52.158%
	I - Batch: 250 | Loss: 0.989 | Acc: 52.550% | Wgt Acc: 51.723%
	I - Batch: 300 | Loss: 0.993 | Acc: 52.083% | Wgt Acc: 50.921%
I - num batch: 319
I - Train -- Loss: 0.986 | Acc: 52.218% | Wgt Acc: 51.194% | LR: 2.500000e-04 | Dur: 118.45s
I - Confusion Matrix: [row->prediction - col->label]
[[459.  19.  23. 193.]
 [ 11. 224. 254.  11.]
 [ 74. 285. 404.  91.]
 [153.  50.  53. 243.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.034 | Acc: 45.260% | Wgt Acc: 46.739% | Dur: 9.39s
I - Confusion Matrix: [row->prediction - col->label]
[[28.  1.  2.  9.]
 [ 2. 34. 34.  3.]
 [ 6. 34. 24. 12.]
 [52.  9. 15. 62.]]

I - Epoch: 22
I - Training: 
	I - Batch: 50 | Loss: 0.918 | Acc: 52.500% | Wgt Acc: 51.519%
	I - Batch: 100 | Loss: 0.937 | Acc: 54.250% | Wgt Acc: 53.781%
	I - Batch: 150 | Loss: 0.953 | Acc: 53.583% | Wgt Acc: 53.263%
	I - Batch: 200 | Loss: 0.948 | Acc: 52.750% | Wgt Acc: 52.730%
	I - Batch: 250 | Loss: 0.959 | Acc: 52.050% | Wgt Acc: 52.119%
	I - Batch: 300 | Loss: 0.960 | Acc: 51.667% | Wgt Acc: 51.417%
I - num batch: 319
I - Train -- Loss: 0.957 | Acc: 51.786% | Wgt Acc: 51.469% | LR: 2.500000e-04 | Dur: 118.48s
I - Confusion Matrix: [row->prediction - col->label]
[[451.  20.  19. 154.]
 [  8. 259. 326.  20.]
 [ 66. 264. 326.  81.]
 [172.  35.  63. 283.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.042 | Acc: 48.624% | Wgt Acc: 45.924% | Dur: 9.35s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  6.  7. 39.]
 [ 0.  6.  1.  0.]
 [10. 61. 59. 13.]
 [18.  5.  8. 34.]]

I - Local maximum validation set accuracy:  48.62

I - Validation set results: 
[14-1-2-3.71][50-3-3-4.72][124-2-3-0.37][127-0-0-5.22][443-2-2-3.81][567-0-0-5.38][573-1-2-3.55][615-0-0-5.27][695-1-2-3.41][722-3-3-4.62]
[826-0-0-5.21][878-0-0-5.29][1103-0-3-2.63][1212-3-0-5.21][1368-0-0-5.17][2181-2-3-4.57][2476-2-2-3.88][2721-2-2-2.83][2818-1-0-5.21][2886-2-2-3.67]
[3231-2-2-3.81][3333-2-2-4.01][3482-2-2-1.37][3536-3-3-4.56][3625-1-2-3.88][3909-0-3-4.45][4035-0-0-5.44][4140-0-0-5.09][4214-1-2-4.02][4346-1-2-2.40]
[4581-2-2-3.86][4708-3-3-1.91][4838-3-0-5.24][4845-1-2-3.70][4868-0-3-3.50][4939-0-3-2.80][4984-2-3-2.55][5078-1-2-3.58][5396-0-0-5.20][5479-1-2-3.78]
[5717-0-0-5.18][5843-1-2-4.11][5949-3-3-4.59][5987-2-2-3.35][6014-3-2-0.50][6033-3-0-5.02][6313-0-3-4.50][6421-3-3-4.28][6500-1-2-4.09][6583-3-3-3.46]
[6683-3-3-2.96][6825-2-0-5.13][6998-3-3-1.25][7049-3-0-5.48][7517-1-2-3.88][7521-1-3-2.11][7528-1-3-1.36][7949-1-2-3.77][8135-1-0-5.36][8185-3-0-5.20]
[8269-3-2-0.12][8273-3-0-5.44][8543-3-0-5.24][8666-1-2-1.60][8672-0-0-5.17][8903-1-2-3.82][9001-2-2-3.96][9036-2-2-3.91][9281-3-2-1.57][9300-2-2-3.88]
[9571-0-3-4.63][9617-1-0-0.98][9644-2-2-3.89][9705-2-3-3.59][9801-0-3-4.63][9803-3-0-3.54][9865-3-0-5.30][9896-2-2-3.78][10314-1-2-3.69][10337-3-0-5.50]
[10403-0-0-3.07][10653-2-2-2.62][10704-2-2-3.87][10719-1-1-3.78][10727-1-0-0.60][10836-0-0-5.35][10969-2-0-4.78][11042-0-0-5.39][11088-1-2-3.88][11322-0-0-5.35]
[11398-2-1-3.78][11499-0-3-4.30][11502-3-0-5.52][11512-3-2-2.26][11608-1-1-3.78][11610-0-3-1.70][11692-0-0-5.52][11905-0-0-5.23][11993-1-2-3.56][12002-2-2-4.07]
[12052-0-0-5.11][12201-0-0-5.21][12235-2-2-3.88][12320-1-3-1.29][12377-2-2-3.15][12398-2-2-3.86][12503-1-2-3.86][12617-0-2-4.05][12685-3-3-1.67][12738-2-3-3.65]
[12742-2-2-3.79][12823-0-0-5.48][13110-1-2-3.75][13240-3-0-5.31][13253-1-2-3.87][13273-0-0-5.22][13634-1-2-2.17][13763-2-2-3.70][13905-3-2-3.88][14060-2-2-4.08]
[14065-3-3-4.58][14147-3-3-3.10][14595-2-2-3.37][14687-2-2-3.92][14788-2-2-4.15][14869-1-2-3.88][14872-3-0-4.05][14877-1-2-4.00][14927-0-3-4.55][15066-0-0-5.26]
[15175-1-2-3.73][15178-2-0-0.54][15375-3-0-5.51][15389-3-0-5.46][15568-2-2-4.09][15675-3-3-4.48][15869-1-3-1.55][16207-3-2-1.05][16236-0-2-0.20][16302-3-3-4.61]
[16331-2-2-3.90][16381-0-3-3.06][16488-1-2-0.52][16495-0-0-5.39][16650-0-0-5.21][16719-1-2-4.10][16801-0-0-5.22][16828-0-0-5.53][17137-3-0-5.24][17245-1-2-1.54]
[17278-3-2-0.37][17282-0-0-5.34][17311-2-2-3.65][17336-2-2-3.88][17608-3-0-5.51][17627-0-2-2.43][17877-3-0-5.50][17924-1-2-3.75][17984-3-0-5.23][18211-0-0-4.89]
[18276-3-0-5.23][18287-1-2-1.84][18394-0-0-4.36][18428-0-0-5.18][18442-0-0-5.39][18478-3-0-5.27][18607-0-0-5.19][18616-0-2-2.69][18663-0-0-5.40][18718-0-0-5.21]
[18766-2-2-3.53][18824-2-2-3.74][18890-3-3-0.32][18930-3-0-4.94][18938-3-3-4.05][19817-1-2-3.73][19839-0-2-1.07][19930-3-0-5.30][19944-0-2-2.22][20036-2-2-3.88]
[20101-3-3-4.58][20474-1-1-3.67][20547-3-3-1.14][20929-2-2-3.88][21245-1-1-3.70][21257-3-2-2.97][21293-1-2-3.88][21316-1-2-3.88][21384-1-1-3.55][21448-1-1-3.67]
[21483-0-0-5.13][21487-2-2-3.82][21714-0-0-5.31][21943-3-2-1.17][21947-0-0-1.69][21948-0-0-5.25][21965-2-2-3.88][21998-1-2-3.87][22025-0-2-1.63][22228-3-0-5.43]
[22446-1-2-3.88][22494-3-0-5.53][22757-0-0-5.43][22811-3-0-4.97][22976-3-2-3.47][22985-3-0-5.25][23014-0-0-5.30][23112-1-2-3.81][23144-3-0-4.80][23168-2-0-5.36]
[23219-0-0-1.76][23363-3-3-4.60][23470-0-2-0.75][23486-2-2-2.82][23497-0-0-5.50][23516-0-0-5.32][23690-1-0-2.84][23921-2-2-3.81][23936-1-2-4.11][24040-3-3-1.68]
[24111-1-2-2.12][24182-0-0-5.48][24238-3-0-5.46][24290-2-0-4.64][24345-0-0-5.21][24364-1-2-3.85][24427-3-0-5.28][24477-2-2-3.87][24495-2-2-3.30][24893-2-2-3.90]
[25012-1-2-3.88][25121-2-2-3.80][25165-3-3-2.41][25183-0-0-5.23][25297-3-3-4.64][25398-0-0-5.39][25574-2-2-2.87][25644-1-2-3.88][25718-1-2-3.12][25774-2-2-4.18]
[26032-3-3-4.53][26051-3-3-4.63][26120-0-2-2.13][26321-1-2-4.17][26732-1-2-3.82][26784-3-3-4.52][26827-3-0-5.33][26833-0-3-4.62][26838-2-2-1.00][26860-1-2-2.65]
[26948-0-0-4.93][27049-3-0-5.26][27098-1-2-2.35][27526-0-0-5.20][27639-3-3-4.56][27698-3-3-4.62][27772-0-0-5.36][27890-1-2-3.92][28040-0-3-4.60][28503-2-2-3.88]
[28577-1-2-3.85][28959-0-0-5.23][29198-3-3-0.92][29777-0-0-5.20][29877-2-2-3.70][30035-1-2-3.80][30098-0-3-4.53][30326-1-2-4.09][30572-2-2-4.14][30716-0-3-2.46]
[30806-2-3-1.38][30906-1-2-3.88][31007-0-0-5.36][31181-3-3-2.91][31238-0-0-5.51][31347-0-0-5.40][31422-2-2-3.23][31429-3-2-0.55][31431-0-3-1.27][31432-1-2-0.82]
[31477-0-0-5.53][31524-1-2-4.15][31597-1-2-3.80][31619-1-2-0.51][31701-0-0-5.16][31755-0-0-5.13][31854-3-0-5.38][32074-1-2-0.17][32078-3-3-4.58][32111-1-2-2.96]
[32127-1-2-3.93][32140-3-3-4.46][32263-2-3-2.10][32365-0-0-5.06][32411-2-0-5.32][32429-3-0-5.47][32473-3-0-5.53][32574-3-0-5.55][32584-0-0-5.22][32622-0-2-1.79]
[32858-3-0-5.20][32969-3-0-5.42][33016-2-2-3.88][33031-1-3-4.51][33035-2-2-3.88][33133-2-2-3.88][33173-2-3-3.78][33175-3-2-3.88][33306-3-2-3.16][33309-2-2-2.93]
[33474-0-3-4.61][33478-2-0-4.95][33618-1-0-5.16][33712-0-3-4.00][33782-2-2-3.80][33914-3-0-5.42][34076-3-3-1.67][34112-2-2-3.93][34138-2-2-1.42][34239-1-2-2.37]
[34364-2-2-3.81][34617-1-2-3.76][34751-3-3-1.85][34783-2-2-3.82][35015-3-3-1.96][35018-1-2-3.85][35288-2-2-2.20]
---------------------------
I - Epoch: 23
I - Training: 
	I - Batch: 50 | Loss: 1.028 | Acc: 54.250% | Wgt Acc: 51.724%
	I - Batch: 100 | Loss: 0.985 | Acc: 52.875% | Wgt Acc: 50.845%
	I - Batch: 150 | Loss: 0.978 | Acc: 52.750% | Wgt Acc: 50.995%
	I - Batch: 200 | Loss: 0.980 | Acc: 52.188% | Wgt Acc: 50.964%
	I - Batch: 250 | Loss: 0.977 | Acc: 51.950% | Wgt Acc: 51.069%
	I - Batch: 300 | Loss: 0.973 | Acc: 52.417% | Wgt Acc: 51.792%
I - num batch: 319
I - Train -- Loss: 0.967 | Acc: 52.375% | Wgt Acc: 51.822% | LR: 2.500000e-04 | Dur: 117.17s
I - Confusion Matrix: [row->prediction - col->label]
[[420.  20.  17. 148.]
 [ 12. 220. 252.  11.]
 [ 54. 297. 392.  77.]
 [211.  41.  73. 302.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.037 | Acc: 49.235% | Wgt Acc: 48.845% | Dur: 8.88s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  3.  3. 26.]
 [ 2. 36. 39.  3.]
 [ 9. 32. 27. 18.]
 [18.  7.  6. 39.]]

I - Local maximum validation set accuracy:  49.24

I - Validation set results: 
[14-1-1-3.62][50-3-3-4.05][124-2-1-3.49][127-0-0-5.34][443-2-1-3.77][567-0-0-5.06][573-1-2-2.76][615-0-0-5.32][695-1-2-1.94][722-3-3-4.41]
[826-0-0-4.76][878-0-0-5.19][1103-0-0-2.39][1212-3-3-3.03][1368-0-0-4.88][2181-2-3-4.34][2476-2-2-1.17][2721-2-2-3.46][2818-1-0-2.07][2886-2-2-3.35]
[3231-2-1-3.74][3333-2-2-3.96][3482-2-1-3.23][3536-3-3-1.83][3625-1-1-3.80][3909-0-3-0.57][4035-0-0-5.20][4140-0-0-4.47][4214-1-3-2.98][4346-1-3-2.69]
[4581-2-1-3.91][4708-3-3-1.59][4838-3-0-3.39][4845-1-2-2.01][4868-0-3-4.50][4939-0-1-2.99][4984-2-2-1.59][5078-1-2-1.74][5396-0-0-5.01][5479-1-1-3.25]
[5717-0-0-3.41][5843-1-2-3.39][5949-3-3-3.96][5987-2-1-3.43][6014-3-2-1.43][6033-3-3-2.04][6313-0-0-5.21][6421-3-3-3.46][6500-1-1-3.80][6583-3-2-2.63]
[6683-3-3-2.63][6825-2-0-3.50][6998-3-3-1.12][7049-3-3-1.11][7517-1-1-3.91][7521-1-3-2.72][7528-1-2-3.40][7949-1-2-3.87][8135-1-3-3.73][8185-3-0-5.21]
[8269-3-2-3.59][8273-3-3-4.48][8543-3-0-5.32][8666-1-2-3.06][8672-0-0-5.01][8903-1-2-3.00][9001-2-1-3.73][9036-2-1-3.91][9281-3-2-2.26][9300-2-1-3.91]
[9571-0-3-2.67][9617-1-2-1.99][9644-2-2-3.96][9705-2-3-1.89][9801-0-3-4.00][9803-3-0-5.00][9865-3-0-5.17][9896-2-1-3.39][10314-1-1-3.58][10337-3-3-2.60]
[10403-0-0-1.56][10653-2-2-0.60][10704-2-2-2.21][10719-1-1-3.60][10727-1-2-2.47][10836-0-0-5.48][10969-2-2-1.42][11042-0-0-4.63][11088-1-1-3.91][11322-0-0-5.44]
[11398-2-1-3.91][11499-0-0-0.62][11502-3-3-3.34][11512-3-0-1.90][11608-1-1-3.54][11610-0-3-4.47][11692-0-0-5.16][11905-0-0-5.24][11993-1-1-3.91][12002-2-3-2.35]
[12052-0-0-3.15][12201-0-0-5.10][12235-2-1-3.91][12320-1-2-1.11][12377-2-2-3.74][12398-2-3-1.75][12503-1-1-3.64][12617-0-2-3.45][12685-3-2-1.65][12738-2-2-2.22]
[12742-2-1-3.81][12823-0-0-5.34][13110-1-2-3.47][13240-3-0-4.86][13253-1-1-3.58][13273-0-0-5.17][13634-1-1-3.63][13763-2-3-2.11][13905-3-3-1.81][14060-2-1-3.91]
[14065-3-3-3.83][14147-3-2-0.61][14595-2-1-3.49][14687-2-1-3.94][14788-2-1-3.91][14869-1-2-1.87][14872-3-0-2.67][14877-1-2-3.27][14927-0-3-3.46][15066-0-0-5.36]
[15175-1-2-2.45][15178-2-2-1.63][15375-3-2-0.43][15389-3-3-4.51][15568-2-2-1.03][15675-3-2-2.62][15869-1-2-1.29][16207-3-2-1.56][16236-0-2-1.37][16302-3-3-3.82]
[16331-2-1-3.88][16381-0-3-3.07][16488-1-2-2.73][16495-0-0-5.03][16650-0-0-4.95][16719-1-2-4.02][16801-0-0-5.17][16828-0-0-5.06][17137-3-0-4.03][17245-1-1-3.36]
[17278-3-0-5.07][17282-0-0-3.04][17311-2-1-3.67][17336-2-1-3.75][17608-3-0-4.63][17627-0-2-1.98][17877-3-3-1.47][17924-1-2-1.76][17984-3-0-4.96][18211-0-3-3.47]
[18276-3-0-5.29][18287-1-2-3.25][18394-0-0-4.26][18428-0-0-4.98][18442-0-3-4.47][18478-3-3-3.98][18607-0-3-3.70][18616-0-3-0.48][18663-0-3-4.00][18718-0-0-5.02]
[18766-2-1-3.67][18824-2-1-3.84][18890-3-2-1.30][18930-3-2-1.61][18938-3-3-2.77][19817-1-1-3.49][19839-0-2-0.53][19930-3-0-5.40][19944-0-1-3.91][20036-2-1-3.90]
[20101-3-3-1.14][20474-1-1-3.60][20547-3-2-3.21][20929-2-1-3.91][21245-1-1-3.82][21257-3-1-3.89][21293-1-1-3.72][21316-1-1-3.67][21384-1-1-3.48][21448-1-1-3.77]
[21483-0-0-4.35][21487-2-1-3.69][21714-0-0-5.22][21943-3-2-1.28][21947-0-2-1.70][21948-0-0-5.21][21965-2-1-3.91][21998-1-2-2.59][22025-0-2-1.66][22228-3-3-2.87]
[22446-1-2-3.83][22494-3-0-5.49][22757-0-0-4.69][22811-3-3-4.48][22976-3-1-3.90][22985-3-0-5.46][23014-0-0-5.47][23112-1-1-3.91][23144-3-0-5.15][23168-2-3-3.65]
[23219-0-3-1.04][23363-3-3-3.16][23470-0-0-1.80][23486-2-2-2.57][23497-0-0-5.03][23516-0-0-5.12][23690-1-3-1.52][23921-2-1-3.76][23936-1-2-3.44][24040-3-3-1.36]
[24111-1-2-2.81][24182-0-0-5.35][24238-3-3-4.38][24290-2-0-3.97][24345-0-2-2.10][24364-1-2-1.89][24427-3-0-4.91][24477-2-1-3.91][24495-2-2-1.97][24893-2-2-3.55]
[25012-1-2-3.65][25121-2-1-3.89][25165-3-2-1.96][25183-0-0-5.40][25297-3-3-3.62][25398-0-0-4.91][25574-2-2-3.53][25644-1-1-3.91][25718-1-1-3.52][25774-2-1-3.91]
[26032-3-3-4.00][26051-3-3-4.47][26120-0-0-0.43][26321-1-3-0.85][26732-1-1-3.63][26784-3-3-4.10][26827-3-0-5.30][26833-0-3-4.30][26838-2-2-3.88][26860-1-1-3.82]
[26948-0-0-3.39][27049-3-0-5.20][27098-1-1-3.20][27526-0-0-5.01][27639-3-3-4.50][27698-3-0-3.57][27772-0-0-5.47][27890-1-2-2.35][28040-0-0-4.98][28503-2-1-3.74]
[28577-1-1-3.89][28959-0-0-5.08][29198-3-2-1.90][29777-0-0-5.01][29877-2-1-3.91][30035-1-1-3.81][30098-0-3-4.07][30326-1-1-3.91][30572-2-2-3.95][30716-0-2-3.63]
[30806-2-1-3.45][30906-1-2-3.89][31007-0-3-1.12][31181-3-3-3.74][31238-0-0-5.31][31347-0-0-5.09][31422-2-2-3.92][31429-3-2-0.97][31431-0-3-3.50][31432-1-1-3.84]
[31477-0-0-5.47][31524-1-2-0.85][31597-1-1-3.52][31619-1-2-0.71][31701-0-0-5.01][31755-0-0-4.19][31854-3-0-2.11][32074-1-0-3.84][32078-3-3-2.62][32111-1-2-2.64]
[32127-1-1-3.70][32140-3-3-3.14][32263-2-2-1.68][32365-0-0-2.53][32411-2-0-5.19][32429-3-0-5.40][32473-3-0-4.84][32574-3-3-4.37][32584-0-0-2.90][32622-0-2-1.60]
[32858-3-0-5.08][32969-3-0-5.27][33016-2-2-3.78][33031-1-3-2.28][33035-2-1-3.91][33133-2-1-3.68][33173-2-2-3.34][33175-3-1-3.90][33306-3-2-2.46][33309-2-2-2.71]
[33474-0-3-3.90][33478-2-2-2.12][33618-1-0-4.46][33712-0-0-3.75][33782-2-1-3.86][33914-3-3-1.74][34076-3-3-2.34][34112-2-1-3.88][34138-2-2-1.81][34239-1-1-3.43]
[34364-2-1-3.62][34617-1-1-3.89][34751-3-3-3.30][34783-2-1-3.66][35015-3-2-0.56][35018-1-1-3.86][35288-2-2-3.64]
---------------------------
I - Epoch: 24
I - Training: 
	I - Batch: 50 | Loss: 0.968 | Acc: 49.500% | Wgt Acc: 49.165%
	I - Batch: 100 | Loss: 0.964 | Acc: 50.250% | Wgt Acc: 49.958%
	I - Batch: 150 | Loss: 0.956 | Acc: 53.000% | Wgt Acc: 51.989%
	I - Batch: 200 | Loss: 0.945 | Acc: 54.188% | Wgt Acc: 53.070%
	I - Batch: 250 | Loss: 0.939 | Acc: 55.150% | Wgt Acc: 53.923%
	I - Batch: 300 | Loss: 0.930 | Acc: 55.292% | Wgt Acc: 53.753%
I - num batch: 319
I - Train -- Loss: 0.931 | Acc: 54.967% | Wgt Acc: 53.379% | LR: 2.500000e-04 | Dur: 114.26s
I - Confusion Matrix: [row->prediction - col->label]
[[488.  21.  24. 155.]
 [  6. 132. 165.   9.]
 [ 49. 380. 478.  72.]
 [154.  45.  67. 302.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.025 | Acc: 50.153% | Wgt Acc: 48.370% | Dur: 8.80s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  4.  6. 27.]
 [ 1.  3.  4.  1.]
 [ 4. 58. 46.  5.]
 [21. 13. 19. 53.]]

I - Local maximum validation set accuracy:  50.15

I - Validation set results: 
[14-1-2-3.90][50-3-3-4.47][124-2-2-1.48][127-0-0-5.49][443-2-2-3.84][567-0-3-4.56][573-1-2-4.13][615-0-0-5.36][695-1-3-0.58][722-3-3-4.50]
[826-0-0-5.45][878-0-0-5.39][1103-0-0-1.10][1212-3-3-4.50][1368-0-0-4.55][2181-2-3-4.47][2476-2-2-1.67][2721-2-2-3.39][2818-1-0-1.87][2886-2-2-3.80]
[3231-2-2-3.81][3333-2-2-4.06][3482-2-3-0.59][3536-3-3-4.42][3625-1-2-3.78][3909-0-3-2.38][4035-0-0-4.65][4140-0-0-4.31][4214-1-2-2.75][4346-1-3-2.85]
[4581-2-2-3.16][4708-3-3-2.51][4838-3-0-5.49][4845-1-3-1.18][4868-0-3-4.51][4939-0-2-3.58][4984-2-3-3.20][5078-1-3-1.63][5396-0-0-5.06][5479-1-2-3.55]
[5717-0-3-1.70][5843-1-2-2.70][5949-3-3-4.64][5987-2-1-3.08][6014-3-0-4.00][6033-3-0-3.85][6313-0-3-4.52][6421-3-3-4.32][6500-1-2-1.94][6583-3-3-4.45]
[6683-3-3-2.78][6825-2-0-5.33][6998-3-3-1.10][7049-3-0-5.63][7517-1-2-3.94][7521-1-3-3.93][7528-1-3-2.57][7949-1-2-2.43][8135-1-0-5.41][8185-3-0-5.22]
[8269-3-2-0.83][8273-3-3-4.66][8543-3-3-4.51][8666-1-2-1.46][8672-0-0-5.03][8903-1-2-3.72][9001-2-2-2.59][9036-2-2-4.21][9281-3-3-1.71][9300-2-2-3.94]
[9571-0-0-4.23][9617-1-3-0.73][9644-2-2-3.83][9705-2-3-2.19][9801-0-3-4.51][9803-3-3-3.53][9865-3-0-5.17][9896-2-1-3.07][10314-1-2-3.94][10337-3-3-4.50]
[10403-0-0-2.19][10653-2-3-0.70][10704-2-3-3.72][10719-1-1-3.64][10727-1-2-2.20][10836-0-0-5.26][10969-2-3-4.37][11042-0-0-5.02][11088-1-2-3.94][11322-0-0-5.24]
[11398-2-2-2.60][11499-0-0-3.02][11502-3-0-5.00][11512-3-0-4.84][11608-1-2-3.94][11610-0-3-3.16][11692-0-0-5.49][11905-0-0-5.37][11993-1-2-1.64][12002-2-3-3.01]
[12052-0-0-5.55][12201-0-0-5.18][12235-2-2-3.94][12320-1-2-2.75][12377-2-2-3.85][12398-2-3-1.59][12503-1-2-3.91][12617-0-3-3.04][12685-3-3-2.70][12738-2-3-4.25]
[12742-2-2-3.96][12823-0-0-5.59][13110-1-2-4.27][13240-3-0-5.26][13253-1-2-3.96][13273-0-0-5.21][13634-1-2-2.04][13763-2-3-2.34][13905-3-3-1.97][14060-2-2-4.22]
[14065-3-3-4.42][14147-3-0-4.97][14595-2-2-3.59][14687-2-2-4.05][14788-2-2-4.19][14869-1-2-3.65][14872-3-2-0.18][14877-1-2-4.19][14927-0-3-4.49][15066-0-0-5.31]
[15175-1-2-3.40][15178-2-0-4.72][15375-3-3-4.24][15389-3-3-4.64][15568-2-2-2.21][15675-3-3-2.34][15869-1-0-1.68][16207-3-0-1.66][16236-0-3-1.77][16302-3-3-4.48]
[16331-2-2-3.94][16381-0-3-3.83][16488-1-1-3.27][16495-0-0-3.31][16650-0-0-5.20][16719-1-2-4.02][16801-0-0-5.18][16828-0-0-5.49][17137-3-3-3.68][17245-1-2-2.08]
[17278-3-3-2.51][17282-0-0-5.15][17311-2-2-3.89][17336-2-2-3.94][17608-3-0-5.20][17627-0-3-4.55][17877-3-3-1.21][17924-1-2-2.86][17984-3-0-5.33][18211-0-3-4.45]
[18276-3-0-5.23][18287-1-2-3.27][18394-0-0-5.52][18428-0-0-5.14][18442-0-0-5.56][18478-3-0-5.21][18607-0-3-4.51][18616-0-3-3.61][18663-0-0-5.54][18718-0-0-5.40]
[18766-2-2-1.49][18824-2-2-3.94][18890-3-3-1.13][18930-3-3-1.10][18938-3-3-3.52][19817-1-2-3.20][19839-0-1-0.58][19930-3-0-5.30][19944-0-2-3.95][20036-2-2-3.94]
[20101-3-3-3.43][20474-1-2-3.23][20547-3-3-1.47][20929-2-2-2.76][21245-1-2-3.79][21257-3-3-2.58][21293-1-2-3.86][21316-1-3-3.06][21384-1-2-3.86][21448-1-2-3.95]
[21483-0-0-4.65][21487-2-2-4.02][21714-0-0-5.40][21943-3-3-1.03][21947-0-0-4.53][21948-0-0-5.21][21965-2-2-2.90][21998-1-2-1.28][22025-0-3-2.84][22228-3-3-4.22]
[22446-1-2-3.31][22494-3-0-5.60][22757-0-0-5.50][22811-3-3-4.49][22976-3-2-2.54][22985-3-3-4.62][23014-0-0-5.28][23112-1-2-4.20][23144-3-3-4.49][23168-2-0-5.10]
[23219-0-0-4.07][23363-3-3-4.44][23470-0-0-0.99][23486-2-3-2.45][23497-0-3-4.60][23516-0-0-5.24][23690-1-3-4.13][23921-2-2-3.86][23936-1-3-2.84][24040-3-2-1.46]
[24111-1-2-3.03][24182-0-0-5.19][24238-3-3-4.66][24290-2-0-3.70][24345-0-0-5.19][24364-1-2-2.21][24427-3-0-5.37][24477-2-2-3.96][24495-2-3-0.28][24893-2-2-3.69]
[25012-1-2-4.08][25121-2-2-3.49][25165-3-3-3.60][25183-0-0-5.29][25297-3-3-4.46][25398-0-0-5.29][25574-2-2-3.71][25644-1-2-3.89][25718-1-2-0.40][25774-2-2-4.14]
[26032-3-3-4.40][26051-3-3-4.50][26120-0-0-0.37][26321-1-2-3.49][26732-1-2-3.94][26784-3-3-4.44][26827-3-0-5.53][26833-0-3-4.48][26838-2-2-1.47][26860-1-2-3.70]
[26948-0-0-3.86][27049-3-0-5.14][27098-1-1-2.98][27526-0-0-5.05][27639-3-3-4.44][27698-3-3-4.59][27772-0-0-5.50][27890-1-2-2.48][28040-0-0-5.36][28503-2-2-3.94]
[28577-1-2-3.94][28959-0-0-5.12][29198-3-3-1.23][29777-0-0-5.14][29877-2-2-1.82][30035-1-2-3.88][30098-0-0-5.60][30326-1-2-4.12][30572-2-2-3.66][30716-0-3-1.99]
[30806-2-3-1.85][30906-1-2-1.86][31007-0-2-1.00][31181-3-0-5.07][31238-0-3-4.45][31347-0-0-5.23][31422-2-2-3.96][31429-3-3-1.08][31431-0-0-3.48][31432-1-2-0.74]
[31477-0-0-5.34][31524-1-2-3.63][31597-1-2-2.48][31619-1-0-0.24][31701-0-0-4.84][31755-0-0-4.83][31854-3-0-5.47][32074-1-3-1.28][32078-3-3-4.49][32111-1-2-1.24]
[32127-1-2-3.94][32140-3-3-4.42][32263-2-3-3.06][32365-0-0-2.34][32411-2-0-5.35][32429-3-0-5.12][32473-3-0-5.32][32574-3-0-5.65][32584-0-0-5.37][32622-0-2-1.00]
[32858-3-3-4.53][32969-3-0-5.36][33016-2-2-3.96][33031-1-3-4.51][33035-2-2-4.18][33133-2-2-4.03][33173-2-3-1.62][33175-3-1-2.97][33306-3-2-1.38][33309-2-2-2.42]
[33474-0-3-4.13][33478-2-3-1.51][33618-1-3-3.24][33712-0-0-5.15][33782-2-2-3.94][33914-3-0-4.75][34076-3-3-2.30][34112-2-3-1.78][34138-2-0-2.67][34239-1-2-1.75]
[34364-2-1-3.70][34617-1-2-3.81][34751-3-3-4.45][34783-2-1-2.91][35015-3-3-3.28][35018-1-2-4.02][35288-2-3-2.30]
---------------------------
I - Epoch: 25
I - Training: 
	I - Batch: 50 | Loss: 0.882 | Acc: 58.500% | Wgt Acc: 56.512%
	I - Batch: 100 | Loss: 0.894 | Acc: 57.875% | Wgt Acc: 55.659%
	I - Batch: 150 | Loss: 0.907 | Acc: 57.667% | Wgt Acc: 55.685%
	I - Batch: 200 | Loss: 0.902 | Acc: 58.188% | Wgt Acc: 56.079%
	I - Batch: 250 | Loss: 0.907 | Acc: 58.000% | Wgt Acc: 55.774%
	I - Batch: 300 | Loss: 0.902 | Acc: 58.125% | Wgt Acc: 56.063%
I - num batch: 319
I - Train -- Loss: 0.900 | Acc: 58.382% | Wgt Acc: 56.334% | LR: 1.250000e-04 | Dur: 114.28s
I - Confusion Matrix: [row->prediction - col->label]
[[466.  13.  18. 134.]
 [  6.  77.  61.   3.]
 [ 55. 445. 601.  58.]
 [170.  43.  54. 343.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 0.987 | Acc: 55.046% | Wgt Acc: 54.144% | Dur: 8.87s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  4.  7. 20.]
 [ 5. 25. 18.  2.]
 [ 6. 42. 43. 12.]
 [17.  7.  7. 52.]]

I - Local maximum validation set accuracy:  55.05

I - Validation set results: 
[14-1-1-3.75][50-3-3-2.53][124-2-2-3.10][127-0-0-5.27][443-2-1-3.70][567-0-3-4.70][573-1-2-3.67][615-0-0-5.29][695-1-2-1.85][722-3-3-4.55]
[826-0-0-4.91][878-0-0-5.39][1103-0-0-0.77][1212-3-3-3.44][1368-0-0-4.92][2181-2-0-5.57][2476-2-2-3.95][2721-2-2-3.59][2818-1-2-1.04][2886-2-2-3.85]
[3231-2-1-3.83][3333-2-2-4.06][3482-2-2-1.83][3536-3-3-3.45][3625-1-1-3.85][3909-0-3-2.03][4035-0-0-5.46][4140-0-0-3.47][4214-1-3-2.38][4346-1-3-2.82]
[4581-2-2-3.50][4708-3-3-1.45][4838-3-0-1.20][4845-1-2-2.83][4868-0-0-4.92][4939-0-0-0.94][4984-2-3-2.91][5078-1-2-1.30][5396-0-0-5.23][5479-1-2-3.43]
[5717-0-0-3.62][5843-1-2-3.97][5949-3-3-4.53][5987-2-2-3.83][6014-3-3-1.76][6033-3-3-1.59][6313-0-0-5.46][6421-3-3-3.99][6500-1-2-3.98][6583-3-3-3.19]
[6683-3-3-3.66][6825-2-0-2.94][6998-3-3-0.85][7049-3-3-3.65][7517-1-2-3.96][7521-1-3-0.89][7528-1-2-1.95][7949-1-2-3.68][8135-1-0-3.51][8185-3-0-5.24]
[8269-3-1-3.79][8273-3-0-5.63][8543-3-0-5.18][8666-1-2-3.92][8672-0-3-4.76][8903-1-1-3.79][9001-2-1-3.76][9036-2-2-3.95][9281-3-2-0.97][9300-2-1-3.84]
[9571-0-3-2.67][9617-1-2-2.33][9644-2-2-4.01][9705-2-3-0.36][9801-0-3-4.60][9803-3-3-2.84][9865-3-0-5.31][9896-2-2-3.96][10314-1-1-3.72][10337-3-3-4.63]
[10403-0-0-1.28][10653-2-2-2.29][10704-2-2-2.60][10719-1-1-3.72][10727-1-2-2.91][10836-0-0-5.36][10969-2-3-2.66][11042-0-0-3.50][11088-1-2-3.89][11322-0-0-5.30]
[11398-2-2-3.47][11499-0-3-1.83][11502-3-3-4.55][11512-3-3-0.75][11608-1-2-3.96][11610-0-3-3.51][11692-0-0-5.68][11905-0-0-5.33][11993-1-2-3.96][12002-2-3-3.94]
[12052-0-0-1.54][12201-0-0-5.29][12235-2-2-3.88][12320-1-1-3.45][12377-2-1-3.74][12398-2-1-2.85][12503-1-2-3.96][12617-0-2-2.06][12685-3-2-2.25][12738-2-2-2.30]
[12742-2-2-3.56][12823-0-0-5.59][13110-1-2-4.15][13240-3-0-5.22][13253-1-2-3.96][13273-0-0-5.28][13634-1-2-2.82][13763-2-2-2.92][13905-3-0-2.45][14060-2-2-3.96]
[14065-3-0-5.05][14147-3-3-2.56][14595-2-1-3.64][14687-2-2-4.01][14788-2-2-3.96][14869-1-1-3.67][14872-3-3-0.42][14877-1-2-3.87][14927-0-3-3.64][15066-0-0-5.38]
[15175-1-2-3.10][15178-2-3-3.00][15375-3-0-5.64][15389-3-3-4.55][15568-2-2-2.30][15675-3-2-1.92][15869-1-2-1.91][16207-3-3-0.21][16236-0-2-1.73][16302-3-3-4.41]
[16331-2-2-3.96][16381-0-3-3.22][16488-1-2-3.18][16495-0-0-2.75][16650-0-0-5.27][16719-1-2-4.15][16801-0-0-5.59][16828-0-0-5.68][17137-3-3-3.07][17245-1-2-3.83]
[17278-3-3-4.08][17282-0-0-3.51][17311-2-1-3.70][17336-2-2-3.96][17608-3-0-5.13][17627-0-0-2.36][17877-3-3-4.06][17924-1-2-3.01][17984-3-0-5.26][18211-0-3-4.42]
[18276-3-3-4.44][18287-1-2-1.06][18394-0-0-4.74][18428-0-1-3.29][18442-0-0-5.71][18478-3-0-5.03][18607-0-0-2.91][18616-0-2-1.66][18663-0-3-4.63][18718-0-0-5.33]
[18766-2-2-3.93][18824-2-1-3.85][18890-3-2-1.33][18930-3-2-1.28][18938-3-3-2.60][19817-1-1-3.70][19839-0-1-2.68][19930-3-3-3.99][19944-0-1-3.70][20036-2-2-3.97]
[20101-3-3-2.47][20474-1-2-3.43][20547-3-2-2.42][20929-2-2-3.96][21245-1-1-3.86][21257-3-2-3.55][21293-1-1-3.73][21316-1-1-3.90][21384-1-1-3.67][21448-1-2-3.96]
[21483-0-0-3.42][21487-2-2-3.96][21714-0-0-5.11][21943-3-2-1.45][21947-0-0-3.25][21948-0-0-5.29][21965-2-2-3.55][21998-1-2-3.32][22025-0-3-2.29][22228-3-3-3.42]
[22446-1-2-3.96][22494-3-0-5.40][22757-0-0-5.44][22811-3-3-4.75][22976-3-2-0.37][22985-3-3-4.63][23014-0-0-5.64][23112-1-2-4.14][23144-3-3-4.54][23168-2-3-3.73]
[23219-0-0-4.04][23363-3-3-3.94][23470-0-2-0.78][23486-2-3-1.77][23497-0-0-5.53][23516-0-0-5.71][23690-1-0-0.42][23921-2-1-3.75][23936-1-2-4.32][24040-3-3-0.83]
[24111-1-1-3.29][24182-0-0-5.31][24238-3-0-5.68][24290-2-0-1.98][24345-0-1-3.37][24364-1-2-3.92][24427-3-3-4.54][24477-2-2-3.96][24495-2-1-3.17][24893-2-2-2.10]
[25012-1-1-3.79][25121-2-2-3.96][25165-3-3-2.80][25183-0-0-5.10][25297-3-3-4.39][25398-0-3-4.65][25574-2-1-3.92][25644-1-2-3.94][25718-1-1-3.77][25774-2-2-4.26]
[26032-3-3-4.50][26051-3-3-4.54][26120-0-2-0.94][26321-1-3-1.65][26732-1-1-3.49][26784-3-3-4.51][26827-3-3-3.11][26833-0-3-4.55][26838-2-1-3.13][26860-1-1-3.03]
[26948-0-0-2.12][27049-3-0-5.25][27098-1-1-3.81][27526-0-0-4.70][27639-3-3-4.48][27698-3-3-4.52][27772-0-0-1.69][27890-1-1-3.52][28040-0-0-5.22][28503-2-1-3.91]
[28577-1-2-3.96][28959-0-0-5.27][29198-3-2-1.85][29777-0-0-5.29][29877-2-2-3.20][30035-1-2-3.70][30098-0-3-4.67][30326-1-2-3.94][30572-2-2-4.05][30716-0-1-3.85]
[30806-2-0-0.44][30906-1-1-3.92][31007-0-0-4.84][31181-3-3-3.54][31238-0-3-4.56][31347-0-0-5.35][31422-2-2-3.95][31429-3-2-2.15][31431-0-0-3.18][31432-1-1-3.76]
[31477-0-0-5.70][31524-1-0-1.97][31597-1-2-3.94][31619-1-1-2.12][31701-0-0-3.83][31755-0-0-5.09][31854-3-3-4.19][32074-1-3-4.54][32078-3-3-3.83][32111-1-0-1.17]
[32127-1-1-3.90][32140-3-3-3.97][32263-2-0-1.48][32365-0-0-1.71][32411-2-0-5.62][32429-3-0-5.28][32473-3-0-5.44][32574-3-0-5.69][32584-0-3-3.09][32622-0-2-1.61]
[32858-3-0-5.39][32969-3-0-5.32][33016-2-2-3.96][33031-1-3-2.76][33035-2-2-3.96][33133-2-2-3.96][33173-2-2-2.71][33175-3-1-3.86][33306-3-2-2.57][33309-2-2-1.10]
[33474-0-0-2.61][33478-2-0-3.42][33618-1-3-2.14][33712-0-0-1.69][33782-2-1-3.77][33914-3-3-4.55][34076-3-3-1.30][34112-2-2-3.96][34138-2-1-3.16][34239-1-1-3.85]
[34364-2-1-3.83][34617-1-2-3.94][34751-3-3-3.49][34783-2-1-3.87][35015-3-3-2.51][35018-1-1-3.82][35288-2-2-3.22]
---------------------------
I - Epoch: 26
I - Training: 
	I - Batch: 50 | Loss: 0.833 | Acc: 61.000% | Wgt Acc: 59.921%
	I - Batch: 100 | Loss: 0.848 | Acc: 59.250% | Wgt Acc: 58.958%
	I - Batch: 150 | Loss: 0.869 | Acc: 58.167% | Wgt Acc: 58.052%
	I - Batch: 200 | Loss: 0.859 | Acc: 59.312% | Wgt Acc: 59.369%
	I - Batch: 250 | Loss: 0.860 | Acc: 59.550% | Wgt Acc: 59.624%
	I - Batch: 300 | Loss: 0.866 | Acc: 59.542% | Wgt Acc: 59.642%
I - num batch: 319
I - Train -- Loss: 0.866 | Acc: 59.285% | Wgt Acc: 59.404% | LR: 1.250000e-04 | Dur: 114.56s
I - Confusion Matrix: [row->prediction - col->label]
[[485.  14.  22. 115.]
 [ 15. 312. 305.   8.]
 [ 29. 210. 350.  52.]
 [168.  42.  57. 363.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.017 | Acc: 52.294% | Wgt Acc: 52.649% | Dur: 8.86s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  2.  3. 24.]
 [ 5. 47. 39.  5.]
 [ 6. 23. 23. 13.]
 [20.  6. 10. 44.]]

I - Epoch: 27
I - Training: 
	I - Batch: 50 | Loss: 0.871 | Acc: 54.750% | Wgt Acc: 54.287%
	I - Batch: 100 | Loss: 0.838 | Acc: 58.500% | Wgt Acc: 58.437%
	I - Batch: 150 | Loss: 0.841 | Acc: 58.000% | Wgt Acc: 58.394%
	I - Batch: 200 | Loss: 0.855 | Acc: 58.250% | Wgt Acc: 58.584%
	I - Batch: 250 | Loss: 0.854 | Acc: 58.350% | Wgt Acc: 58.518%
	I - Batch: 300 | Loss: 0.861 | Acc: 57.625% | Wgt Acc: 57.850%
I - num batch: 319
I - Train -- Loss: 0.863 | Acc: 57.676% | Wgt Acc: 57.900% | LR: 1.250000e-04 | Dur: 114.54s
I - Confusion Matrix: [row->prediction - col->label]
[[481.   9.  17. 110.]
 [ 15. 308. 332.  11.]
 [ 25. 226. 319.  56.]
 [176.  35.  66. 361.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.021 | Acc: 49.235% | Wgt Acc: 49.389% | Dur: 8.89s
I - Confusion Matrix: [row->prediction - col->label]
[[49.  6.  3. 20.]
 [ 2. 25. 23.  1.]
 [ 1. 34. 29.  7.]
 [36. 13. 20. 58.]]

I - Epoch: 28
I - Training: 
	I - Batch: 50 | Loss: 0.838 | Acc: 60.500% | Wgt Acc: 60.462%
	I - Batch: 100 | Loss: 0.853 | Acc: 58.750% | Wgt Acc: 58.655%
	I - Batch: 150 | Loss: 0.863 | Acc: 57.667% | Wgt Acc: 57.714%
	I - Batch: 200 | Loss: 0.865 | Acc: 58.375% | Wgt Acc: 58.342%
	I - Batch: 250 | Loss: 0.859 | Acc: 58.550% | Wgt Acc: 58.567%
	I - Batch: 300 | Loss: 0.852 | Acc: 59.083% | Wgt Acc: 59.243%
I - num batch: 319
I - Train -- Loss: 0.850 | Acc: 59.168% | Wgt Acc: 59.280% | LR: 1.250000e-04 | Dur: 114.65s
I - Confusion Matrix: [row->prediction - col->label]
[[492.  12.  15. 131.]
 [ 17. 337. 316.   7.]
 [ 29. 203. 342.  64.]
 [159.  26.  61. 336.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 0.981 | Acc: 55.352% | Wgt Acc: 54.823% | Dur: 8.90s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  5.  6. 23.]
 [ 4. 40. 28.  3.]
 [ 5. 28. 34. 17.]
 [15.  5.  7. 43.]]

I - Local maximum validation set accuracy:  55.35

I - Validation set results: 
[14-1-1-4.04][50-3-3-4.34][124-2-2-2.60][127-0-0-5.61][443-2-1-3.93][567-0-3-4.53][573-1-2-3.37][615-0-3-4.61][695-1-0-0.66][722-3-3-4.51]
[826-0-0-5.40][878-0-0-5.44][1103-0-0-2.18][1212-3-3-4.52][1368-0-0-5.34][2181-2-3-2.86][2476-2-2-2.05][2721-2-2-3.87][2818-1-2-1.61][2886-2-2-3.15]
[3231-2-1-4.02][3333-2-1-4.07][3482-2-2-1.76][3536-3-2-2.39][3625-1-1-4.04][3909-0-1-2.20][4035-0-0-5.41][4140-0-0-5.11][4214-1-3-4.13][4346-1-2-2.21]
[4581-2-2-3.66][4708-3-3-1.83][4838-3-0-4.89][4845-1-2-3.63][4868-0-0-5.34][4939-0-0-3.21][4984-2-3-1.94][5078-1-2-0.87][5396-0-0-5.34][5479-1-2-3.77]
[5717-0-0-2.26][5843-1-1-4.04][5949-3-3-4.53][5987-2-1-3.49][6014-3-2-1.65][6033-3-3-2.63][6313-0-0-5.68][6421-3-3-4.56][6500-1-2-3.63][6583-3-3-4.10]
[6683-3-3-2.67][6825-2-0-5.68][6998-3-2-1.26][7049-3-3-2.16][7517-1-1-4.04][7521-1-2-2.11][7528-1-2-2.11][7949-1-2-3.81][8135-1-0-4.94][8185-3-0-5.34]
[8269-3-1-4.04][8273-3-0-5.59][8543-3-0-5.38][8666-1-1-2.79][8672-0-0-5.45][8903-1-2-3.85][9001-2-1-4.00][9036-2-1-3.98][9281-3-2-2.91][9300-2-2-3.90]
[9571-0-3-2.51][9617-1-1-3.47][9644-2-2-3.78][9705-2-2-1.05][9801-0-3-4.53][9803-3-0-4.11][9865-3-0-5.44][9896-2-1-3.67][10314-1-1-3.69][10337-3-3-4.49]
[10403-0-1-1.78][10653-2-2-1.42][10704-2-2-3.99][10719-1-1-3.75][10727-1-1-3.24][10836-0-0-5.42][10969-2-3-3.37][11042-0-0-5.06][11088-1-1-3.88][11322-0-0-5.37]
[11398-2-1-3.89][11499-0-0-3.71][11502-3-3-4.24][11512-3-0-4.76][11608-1-1-4.04][11610-0-0-5.42][11692-0-0-4.92][11905-0-0-5.43][11993-1-1-3.98][12002-2-3-2.83]
[12052-0-0-5.27][12201-0-0-5.34][12235-2-1-4.04][12320-1-1-1.93][12377-2-1-3.95][12398-2-1-1.97][12503-1-1-3.78][12617-0-2-2.29][12685-3-3-2.65][12738-2-0-2.31]
[12742-2-2-3.82][12823-0-0-5.56][13110-1-2-4.09][13240-3-0-5.34][13253-1-1-3.93][13273-0-0-5.34][13634-1-2-3.89][13763-2-2-2.98][13905-3-0-2.40][14060-2-1-4.07]
[14065-3-3-4.58][14147-3-3-4.49][14595-2-1-3.64][14687-2-2-4.09][14788-2-2-3.99][14869-1-1-4.00][14872-3-0-2.69][14877-1-2-3.76][14927-0-3-4.33][15066-0-0-5.46]
[15175-1-2-2.83][15178-2-3-2.04][15375-3-3-2.00][15389-3-3-4.56][15568-2-2-3.66][15675-3-3-2.14][15869-1-3-2.23][16207-3-2-1.95][16236-0-3-1.50][16302-3-3-4.34]
[16331-2-1-3.99][16381-0-3-4.19][16488-1-1-3.96][16495-0-0-5.37][16650-0-0-5.34][16719-1-1-4.11][16801-0-0-5.34][16828-0-0-5.39][17137-3-0-5.74][17245-1-2-2.66]
[17278-3-0-5.21][17282-0-0-5.35][17311-2-2-2.96][17336-2-1-4.04][17608-3-3-4.61][17627-0-2-2.15][17877-3-3-4.05][17924-1-1-3.40][17984-3-0-5.34][18211-0-3-4.33]
[18276-3-0-4.89][18287-1-1-4.04][18394-0-0-5.39][18428-0-0-5.34][18442-0-0-5.29][18478-3-3-4.51][18607-0-0-5.05][18616-0-2-1.58][18663-0-3-4.51][18718-0-0-5.34]
[18766-2-1-3.43][18824-2-2-3.89][18890-3-2-2.63][18930-3-1-2.65][18938-3-3-4.61][19817-1-1-3.95][19839-0-0-1.86][19930-3-3-4.34][19944-0-1-4.03][20036-2-2-3.97]
[20101-3-2-1.23][20474-1-2-3.54][20547-3-2-1.13][20929-2-1-4.04][21245-1-1-4.04][21257-3-2-2.82][21293-1-1-3.98][21316-1-1-4.04][21384-1-1-3.77][21448-1-1-4.04]
[21483-0-0-5.14][21487-2-2-3.61][21714-0-0-5.13][21943-3-2-2.07][21947-0-0-5.16][21948-0-0-5.34][21965-2-1-3.88][21998-1-2-2.49][22025-0-2-3.35][22228-3-3-3.69]
[22446-1-1-4.07][22494-3-0-5.70][22757-0-0-5.40][22811-3-3-4.62][22976-3-2-2.58][22985-3-3-4.54][23014-0-0-5.48][23112-1-1-4.02][23144-3-3-4.46][23168-2-3-4.35]
[23219-0-0-5.55][23363-3-3-4.49][23470-0-0-1.99][23486-2-2-2.33][23497-0-0-5.59][23516-0-0-5.44][23690-1-1-3.96][23921-2-1-4.04][23936-1-2-2.33][24040-3-2-1.04]
[24111-1-1-3.32][24182-0-3-4.63][24238-3-0-5.49][24290-2-0-3.55][24345-0-0-4.16][24364-1-2-3.87][24427-3-3-4.49][24477-2-1-4.04][24495-2-1-3.33][24893-2-2-2.93]
[25012-1-2-3.77][25121-2-2-3.85][25165-3-2-2.22][25183-0-0-5.34][25297-3-3-4.14][25398-0-0-5.34][25574-2-2-3.54][25644-1-1-4.04][25718-1-1-3.75][25774-2-2-3.17]
[26032-3-3-4.58][26051-3-3-4.59][26120-0-0-2.72][26321-1-2-4.07][26732-1-1-3.73][26784-3-3-4.61][26827-3-3-2.40][26833-0-3-4.61][26838-2-2-3.99][26860-1-1-3.54]
[26948-0-0-4.46][27049-3-0-5.34][27098-1-0-4.50][27526-0-0-5.40][27639-3-3-4.44][27698-3-3-4.54][27772-0-0-5.37][27890-1-2-2.65][28040-0-3-4.55][28503-2-1-4.05]
[28577-1-1-4.05][28959-0-0-5.49][29198-3-2-2.24][29777-0-0-5.34][29877-2-2-2.46][30035-1-2-3.81][30098-0-3-4.55][30326-1-1-4.05][30572-2-2-4.08][30716-0-1-3.77]
[30806-2-0-1.01][30906-1-1-3.69][31007-0-0-4.93][31181-3-3-2.48][31238-0-3-4.64][31347-0-0-5.36][31422-2-1-4.04][31429-3-2-2.26][31431-0-0-5.12][31432-1-2-3.03]
[31477-0-0-5.42][31524-1-2-3.94][31597-1-1-4.04][31619-1-0-2.47][31701-0-0-5.34][31755-0-0-5.42][31854-3-0-5.60][32074-1-3-2.60][32078-3-0-4.42][32111-1-0-2.40]
[32127-1-1-4.02][32140-3-3-3.28][32263-2-3-2.65][32365-0-0-2.43][32411-2-0-5.60][32429-3-0-5.34][32473-3-0-5.67][32574-3-0-5.12][32584-0-3-4.30][32622-0-2-1.38]
[32858-3-0-5.36][32969-3-3-4.54][33016-2-1-4.06][33031-1-3-1.92][33035-2-2-4.14][33133-2-1-4.04][33173-2-2-3.05][33175-3-1-3.72][33306-3-2-2.97][33309-2-2-2.09]
[33474-0-0-5.24][33478-2-0-1.23][33618-1-3-1.39][33712-0-0-5.14][33782-2-1-3.94][33914-3-3-4.53][34076-3-2-1.57][34112-2-1-4.04][34138-2-2-1.87][34239-1-2-3.19]
[34364-2-2-3.08][34617-1-2-3.47][34751-3-3-3.94][34783-2-1-3.92][35015-3-3-2.20][35018-1-1-3.98][35288-2-2-2.89]
---------------------------
I - Epoch: 29
I - Training: 
	I - Batch: 50 | Loss: 0.838 | Acc: 61.000% | Wgt Acc: 60.656%
	I - Batch: 100 | Loss: 0.828 | Acc: 60.750% | Wgt Acc: 60.960%
	I - Batch: 150 | Loss: 0.820 | Acc: 61.333% | Wgt Acc: 61.641%
	I - Batch: 200 | Loss: 0.819 | Acc: 61.125% | Wgt Acc: 61.458%
	I - Batch: 250 | Loss: 0.822 | Acc: 61.350% | Wgt Acc: 61.856%
	I - Batch: 300 | Loss: 0.825 | Acc: 61.542% | Wgt Acc: 62.102%
I - num batch: 319
I - Train -- Loss: 0.828 | Acc: 61.013% | Wgt Acc: 61.545% | LR: 1.250000e-04 | Dur: 116.56s
I - Confusion Matrix: [row->prediction - col->label]
[[513.  12.  10. 116.]
 [ 14. 376. 367.  15.]
 [ 27. 156. 300.  42.]
 [143.  34.  57. 365.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 0.972 | Acc: 54.128% | Wgt Acc: 55.027% | Dur: 9.28s
I - Confusion Matrix: [row->prediction - col->label]
[[47.  7.  3.  9.]
 [ 1. 34. 29.  0.]
 [ 1. 24. 28.  9.]
 [39. 13. 15. 68.]]

I - Epoch: 30
I - Training: 
	I - Batch: 50 | Loss: 0.817 | Acc: 60.750% | Wgt Acc: 60.956%
	I - Batch: 100 | Loss: 0.800 | Acc: 61.750% | Wgt Acc: 62.220%
	I - Batch: 150 | Loss: 0.806 | Acc: 62.167% | Wgt Acc: 62.224%
	I - Batch: 200 | Loss: 0.810 | Acc: 62.750% | Wgt Acc: 63.055%
	I - Batch: 250 | Loss: 0.808 | Acc: 63.100% | Wgt Acc: 63.569%
	I - Batch: 300 | Loss: 0.808 | Acc: 62.875% | Wgt Acc: 63.377%
I - num batch: 319
I - Train -- Loss: 0.813 | Acc: 62.348% | Wgt Acc: 62.774% | LR: 1.250000e-04 | Dur: 121.21s
I - Confusion Matrix: [row->prediction - col->label]
[[531.  15.  12. 115.]
 [ 19. 369. 349.  10.]
 [ 19. 168. 313.  38.]
 [128.  26.  60. 375.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 0.994 | Acc: 54.434% | Wgt Acc: 53.397% | Dur: 9.28s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  8.  3. 27.]
 [ 1. 24. 23.  3.]
 [ 2. 35. 35.  6.]
 [16. 11. 14. 50.]]

I - Epoch: 31
I - Training: 
	I - Batch: 50 | Loss: 0.825 | Acc: 58.000% | Wgt Acc: 58.077%
	I - Batch: 100 | Loss: 0.813 | Acc: 60.500% | Wgt Acc: 60.991%
	I - Batch: 150 | Loss: 0.804 | Acc: 62.333% | Wgt Acc: 62.491%
	I - Batch: 200 | Loss: 0.805 | Acc: 62.188% | Wgt Acc: 62.342%
	I - Batch: 250 | Loss: 0.814 | Acc: 61.700% | Wgt Acc: 61.856%
	I - Batch: 300 | Loss: 0.805 | Acc: 62.667% | Wgt Acc: 62.852%
I - num batch: 319
I - Train -- Loss: 0.809 | Acc: 62.348% | Wgt Acc: 62.571% | LR: 1.250000e-04 | Dur: 117.99s
I - Confusion Matrix: [row->prediction - col->label]
[[517.  10.  14. 120.]
 [ 10. 352. 316.   9.]
 [ 24. 193. 350.  40.]
 [146.  23.  54. 369.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.076 | Acc: 49.847% | Wgt Acc: 49.253% | Dur: 9.29s
I - Confusion Matrix: [row->prediction - col->label]
[[71. 10. 10. 43.]
 [ 3. 42. 35.  2.]
 [ 4. 20. 19. 10.]
 [10.  6. 11. 31.]]

I - Epoch: 32
I - Training: 
	I - Batch: 50 | Loss: 0.813 | Acc: 61.750% | Wgt Acc: 61.910%
	I - Batch: 100 | Loss: 0.779 | Acc: 63.375% | Wgt Acc: 64.104%
	I - Batch: 150 | Loss: 0.775 | Acc: 62.833% | Wgt Acc: 63.458%
	I - Batch: 200 | Loss: 0.774 | Acc: 63.000% | Wgt Acc: 63.568%
	I - Batch: 250 | Loss: 0.789 | Acc: 61.650% | Wgt Acc: 62.394%
	I - Batch: 300 | Loss: 0.799 | Acc: 61.708% | Wgt Acc: 62.353%
I - num batch: 319
I - Train -- Loss: 0.802 | Acc: 61.563% | Wgt Acc: 62.226% | LR: 1.250000e-04 | Dur: 118.03s
I - Confusion Matrix: [row->prediction - col->label]
[[542.  16.  13. 133.]
 [ 21. 412. 404.  14.]
 [ 14. 128. 264.  41.]
 [120.  22.  53. 350.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.044 | Acc: 52.599% | Wgt Acc: 52.921% | Dur: 9.33s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  5.  6. 19.]
 [ 6. 49. 39.  8.]
 [ 8. 21. 26. 17.]
 [19.  3.  4. 42.]]

I - Epoch: 33
I - Training: 
	I - Batch: 50 | Loss: 0.781 | Acc: 67.750% | Wgt Acc: 68.025%
	I - Batch: 100 | Loss: 0.779 | Acc: 65.250% | Wgt Acc: 65.469%
	I - Batch: 150 | Loss: 0.773 | Acc: 64.833% | Wgt Acc: 64.989%
	I - Batch: 200 | Loss: 0.763 | Acc: 65.500% | Wgt Acc: 65.492%
	I - Batch: 250 | Loss: 0.769 | Acc: 65.300% | Wgt Acc: 65.316%
	I - Batch: 300 | Loss: 0.771 | Acc: 65.125% | Wgt Acc: 65.332%
I - num batch: 319
I - Train -- Loss: 0.770 | Acc: 64.978% | Wgt Acc: 65.154% | LR: 1.250000e-04 | Dur: 117.91s
I - Confusion Matrix: [row->prediction - col->label]
[[533.  11.   8. 109.]
 [ 15. 360. 292.  10.]
 [ 18. 183. 377.  34.]
 [131.  24.  57. 385.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.014 | Acc: 54.434% | Wgt Acc: 54.484% | Dur: 9.31s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  0.  6. 21.]
 [ 3. 46. 36.  5.]
 [ 7. 25. 29. 16.]
 [19.  7.  4. 44.]]

I - Epoch: 34
I - Training: 
	I - Batch: 50 | Loss: 0.742 | Acc: 61.000% | Wgt Acc: 61.547%
	I - Batch: 100 | Loss: 0.770 | Acc: 61.750% | Wgt Acc: 62.077%
	I - Batch: 150 | Loss: 0.782 | Acc: 63.417% | Wgt Acc: 63.792%
	I - Batch: 200 | Loss: 0.778 | Acc: 64.312% | Wgt Acc: 64.806%
	I - Batch: 250 | Loss: 0.793 | Acc: 63.550% | Wgt Acc: 64.122%
	I - Batch: 300 | Loss: 0.787 | Acc: 64.458% | Wgt Acc: 65.001%
I - num batch: 319
I - Train -- Loss: 0.783 | Acc: 64.704% | Wgt Acc: 65.260% | LR: 1.250000e-04 | Dur: 117.98s
I - Confusion Matrix: [row->prediction - col->label]
[[529.  18.  13. 108.]
 [ 21. 401. 342.  12.]
 [ 18. 133. 334.  34.]
 [129.  26.  45. 384.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 0.959 | Acc: 57.187% | Wgt Acc: 57.677% | Dur: 9.34s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  3.  5. 19.]
 [ 5. 47. 30.  5.]
 [ 3. 20. 28.  8.]
 [22.  8. 12. 54.]]

I - Local maximum validation set accuracy:  57.19

I - Validation set results: 
[14-1-2-4.09][50-3-3-4.42][124-2-2-3.21][127-0-0-5.53][443-2-1-4.15][567-0-3-4.66][573-1-2-4.02][615-0-3-4.72][695-1-3-2.11][722-3-3-4.82]
[826-0-0-5.63][878-0-0-5.57][1103-0-1-0.54][1212-3-3-4.42][1368-0-0-5.53][2181-2-1-1.93][2476-2-2-3.46][2721-2-1-4.18][2818-1-3-0.87][2886-2-1-3.76]
[3231-2-1-4.19][3333-2-1-4.19][3482-2-2-1.84][3536-3-3-1.01][3625-1-1-4.19][3909-0-0-1.96][4035-0-0-5.68][4140-0-0-5.57][4214-1-1-4.15][4346-1-3-4.65]
[4581-2-2-3.83][4708-3-3-2.47][4838-3-0-4.57][4845-1-2-3.46][4868-0-0-4.24][4939-0-3-1.99][4984-2-3-2.99][5078-1-2-0.61][5396-0-0-5.53][5479-1-2-3.90]
[5717-0-0-3.98][5843-1-1-4.23][5949-3-3-4.74][5987-2-1-3.77][6014-3-2-1.88][6033-3-3-3.39][6313-0-0-4.88][6421-3-3-4.86][6500-1-2-3.24][6583-3-3-4.44]
[6683-3-3-4.65][6825-2-0-5.21][6998-3-1-3.14][7049-3-3-4.75][7517-1-1-4.19][7521-1-1-1.62][7528-1-3-2.34][7949-1-2-3.20][8135-1-0-4.55][8185-3-0-5.66]
[8269-3-1-2.45][8273-3-0-5.55][8543-3-3-4.76][8666-1-1-2.97][8672-0-0-5.62][8903-1-1-4.11][9001-2-1-4.10][9036-2-1-4.19][9281-3-3-4.13][9300-2-2-4.10]
[9571-0-3-4.62][9617-1-1-4.12][9644-2-2-4.15][9705-2-3-2.42][9801-0-3-4.79][9803-3-0-5.51][9865-3-0-5.68][9896-2-1-3.41][10314-1-1-3.75][10337-3-3-4.83]
[10403-0-0-0.72][10653-2-1-3.67][10704-2-1-4.20][10719-1-1-3.99][10727-1-1-2.11][10836-0-0-5.72][10969-2-3-4.34][11042-0-0-5.46][11088-1-1-4.19][11322-0-0-5.57]
[11398-2-1-3.20][11499-0-0-1.42][11502-3-0-4.76][11512-3-1-1.91][11608-1-1-3.85][11610-0-0-5.34][11692-0-3-4.82][11905-0-0-5.73][11993-1-1-4.20][12002-2-0-1.87]
[12052-0-0-3.34][12201-0-0-5.57][12235-2-2-4.01][12320-1-0-1.76][12377-2-1-4.18][12398-2-1-3.83][12503-1-1-4.20][12617-0-1-1.92][12685-3-3-3.37][12738-2-3-2.85]
[12742-2-2-4.07][12823-0-3-4.84][13110-1-2-4.00][13240-3-0-5.53][13253-1-1-4.19][13273-0-0-5.53][13634-1-2-4.06][13763-2-2-3.46][13905-3-0-3.27][14060-2-1-4.18]
[14065-3-3-4.66][14147-3-3-4.72][14595-2-1-3.94][14687-2-2-4.09][14788-2-2-4.04][14869-1-1-3.97][14872-3-3-0.87][14877-1-2-3.23][14927-0-3-4.86][15066-0-0-5.12]
[15175-1-1-3.79][15178-2-3-2.53][15375-3-3-3.14][15389-3-3-4.87][15568-2-1-3.93][15675-3-1-2.00][15869-1-1-2.55][16207-3-3-0.68][16236-0-3-3.79][16302-3-0-5.13]
[16331-2-1-4.21][16381-0-3-4.48][16488-1-1-4.18][16495-0-0-4.90][16650-0-0-5.55][16719-1-1-4.20][16801-0-0-5.53][16828-0-0-5.58][17137-3-0-4.91][17245-1-1-3.13]
[17278-3-0-5.53][17282-0-0-5.10][17311-2-2-3.88][17336-2-1-4.19][17608-3-3-4.84][17627-0-0-5.51][17877-3-0-5.46][17924-1-1-3.87][17984-3-0-5.51][18211-0-3-4.64]
[18276-3-3-4.76][18287-1-1-3.56][18394-0-0-5.53][18428-0-0-5.53][18442-0-3-4.84][18478-3-3-4.84][18607-0-0-5.07][18616-0-2-0.89][18663-0-3-4.04][18718-0-0-5.60]
[18766-2-2-2.00][18824-2-2-4.08][18890-3-2-3.25][18930-3-0-2.55][18938-3-3-4.83][19817-1-2-3.98][19839-0-0-2.58][19930-3-3-4.77][19944-0-1-3.94][20036-2-2-4.02]
[20101-3-3-2.47][20474-1-2-4.01][20547-3-2-2.20][20929-2-1-4.19][21245-1-1-4.09][21257-3-3-2.96][21293-1-1-4.17][21316-1-1-4.19][21384-1-1-3.99][21448-1-1-4.10]
[21483-0-0-5.53][21487-2-2-4.09][21714-0-0-3.41][21943-3-2-2.75][21947-0-0-2.94][21948-0-0-5.56][21965-2-2-3.93][21998-1-1-3.82][22025-0-2-3.21][22228-3-3-4.70]
[22446-1-1-4.19][22494-3-3-4.74][22757-0-0-5.64][22811-3-3-4.87][22976-3-3-1.16][22985-3-3-4.84][23014-0-3-4.79][23112-1-2-4.26][23144-3-3-4.85][23168-2-0-5.52]
[23219-0-3-1.23][23363-3-3-4.84][23470-0-0-2.34][23486-2-3-2.77][23497-0-3-4.83][23516-0-0-5.65][23690-1-2-4.17][23921-2-1-4.09][23936-1-3-3.11][24040-3-3-0.91]
[24111-1-1-3.31][24182-0-3-4.78][24238-3-3-4.84][24290-2-0-2.43][24345-0-1-3.25][24364-1-2-4.11][24427-3-3-4.82][24477-2-2-4.17][24495-2-1-3.66][24893-2-2-3.47]
[25012-1-1-3.82][25121-2-2-3.92][25165-3-3-4.79][25183-0-0-5.53][25297-3-3-4.71][25398-0-0-5.34][25574-2-2-3.36][25644-1-1-4.19][25718-1-1-2.68][25774-2-2-3.91]
[26032-3-3-4.85][26051-3-3-4.87][26120-0-0-5.27][26321-1-2-4.16][26732-1-1-4.21][26784-3-3-4.87][26827-3-3-4.84][26833-0-3-4.86][26838-2-2-4.08][26860-1-1-4.01]
[26948-0-0-3.52][27049-3-0-5.09][27098-1-1-3.26][27526-0-0-5.25][27639-3-3-4.73][27698-3-3-4.86][27772-0-0-5.70][27890-1-1-3.61][28040-0-3-2.49][28503-2-1-4.14]
[28577-1-1-4.21][28959-0-0-5.53][29198-3-2-1.71][29777-0-0-5.53][29877-2-3-2.13][30035-1-1-4.17][30098-0-0-5.63][30326-1-1-4.19][30572-2-2-4.09][30716-0-2-2.57]
[30806-2-3-1.81][30906-1-1-3.79][31007-0-0-5.56][31181-3-3-4.70][31238-0-3-4.85][31347-0-0-5.26][31422-2-1-4.12][31429-3-2-1.53][31431-0-0-5.58][31432-1-1-3.87]
[31477-0-3-4.84][31524-1-2-2.87][31597-1-1-4.21][31619-1-3-4.68][31701-0-0-5.53][31755-0-0-5.64][31854-3-0-5.62][32074-1-3-4.49][32078-3-3-4.83][32111-1-0-2.38]
[32127-1-1-4.19][32140-3-3-4.73][32263-2-3-4.39][32365-0-0-4.44][32411-2-3-4.80][32429-3-0-5.56][32473-3-3-4.78][32574-3-3-4.79][32584-0-0-2.40][32622-0-1-1.53]
[32858-3-3-4.63][32969-3-0-5.09][33016-2-1-4.22][33031-1-3-1.37][33035-2-2-4.29][33133-2-1-4.22][33173-2-2-4.00][33175-3-1-3.83][33306-3-2-3.36][33309-2-3-2.09]
[33474-0-0-2.35][33478-2-3-4.45][33618-1-2-3.50][33712-0-3-4.59][33782-2-1-4.21][33914-3-0-2.29][34076-3-2-2.03][34112-2-1-3.69][34138-2-0-2.08][34239-1-2-2.52]
[34364-2-2-4.11][34617-1-2-4.06][34751-3-3-4.46][34783-2-1-3.90][35015-3-3-4.14][35018-1-1-4.00][35288-2-2-4.14]
---------------------------
I - Epoch: 35
I - Training: 
	I - Batch: 50 | Loss: 0.709 | Acc: 69.500% | Wgt Acc: 69.938%
	I - Batch: 100 | Loss: 0.705 | Acc: 69.250% | Wgt Acc: 69.558%
	I - Batch: 150 | Loss: 0.715 | Acc: 67.750% | Wgt Acc: 68.058%
	I - Batch: 200 | Loss: 0.744 | Acc: 65.125% | Wgt Acc: 65.424%
	I - Batch: 250 | Loss: 0.757 | Acc: 64.500% | Wgt Acc: 64.700%
	I - Batch: 300 | Loss: 0.772 | Acc: 64.292% | Wgt Acc: 64.539%
I - num batch: 319
I - Train -- Loss: 0.771 | Acc: 64.547% | Wgt Acc: 64.800% | LR: 1.250000e-04 | Dur: 118.00s
I - Confusion Matrix: [row->prediction - col->label]
[[526.  12.  10. 108.]
 [ 21. 371. 310.  11.]
 [ 14. 176. 369.  41.]
 [136.  19.  45. 378.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.050 | Acc: 51.376% | Wgt Acc: 51.359% | Dur: 9.32s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  7.  7. 29.]
 [ 5. 41. 34.  7.]
 [ 2. 19. 26.  7.]
 [23. 11.  8. 43.]]

I - Epoch: 36
I - Training: 
	I - Batch: 50 | Loss: 0.800 | Acc: 65.000% | Wgt Acc: 65.508%
	I - Batch: 100 | Loss: 0.764 | Acc: 65.875% | Wgt Acc: 66.554%
	I - Batch: 150 | Loss: 0.771 | Acc: 65.167% | Wgt Acc: 65.503%
	I - Batch: 200 | Loss: 0.767 | Acc: 65.375% | Wgt Acc: 65.662%
	I - Batch: 250 | Loss: 0.770 | Acc: 65.800% | Wgt Acc: 65.987%
	I - Batch: 300 | Loss: 0.765 | Acc: 65.708% | Wgt Acc: 65.928%
I - num batch: 319
I - Train -- Loss: 0.769 | Acc: 65.567% | Wgt Acc: 65.800% | LR: 1.250000e-04 | Dur: 118.27s
I - Confusion Matrix: [row->prediction - col->label]
[[536.  13.   7. 104.]
 [ 14. 371. 299.  16.]
 [ 15. 173. 376.  31.]
 [132.  21.  52. 387.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.018 | Acc: 55.963% | Wgt Acc: 55.774% | Dur: 9.30s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  8.  7. 33.]
 [ 3. 46. 33.  6.]
 [ 2. 18. 26.  4.]
 [15.  6.  9. 43.]]

I - Epoch: 37
I - Training: 
	I - Batch: 50 | Loss: 0.752 | Acc: 69.500% | Wgt Acc: 69.735%
	I - Batch: 100 | Loss: 0.747 | Acc: 66.125% | Wgt Acc: 66.545%
	I - Batch: 150 | Loss: 0.744 | Acc: 67.083% | Wgt Acc: 67.455%
	I - Batch: 200 | Loss: 0.764 | Acc: 66.062% | Wgt Acc: 66.343%
	I - Batch: 250 | Loss: 0.753 | Acc: 66.650% | Wgt Acc: 66.749%
	I - Batch: 300 | Loss: 0.749 | Acc: 67.042% | Wgt Acc: 67.170%
I - num batch: 319
I - Train -- Loss: 0.746 | Acc: 67.138% | Wgt Acc: 67.312% | LR: 1.250000e-04 | Dur: 118.36s
I - Confusion Matrix: [row->prediction - col->label]
[[563.  14.  11.  99.]
 [ 17. 382. 296.  12.]
 [ 12. 165. 378.  40.]
 [105.  17.  49. 387.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 0.995 | Acc: 55.046% | Wgt Acc: 56.182% | Dur: 9.35s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  6.  6. 22.]
 [12. 58. 46.  6.]
 [ 3. 13. 20.  9.]
 [20.  1.  3. 49.]]

I - Epoch: 38
I - Training: 
	I - Batch: 50 | Loss: 0.715 | Acc: 66.500% | Wgt Acc: 67.363%
	I - Batch: 100 | Loss: 0.728 | Acc: 68.375% | Wgt Acc: 68.986%
	I - Batch: 150 | Loss: 0.741 | Acc: 68.250% | Wgt Acc: 68.755%
	I - Batch: 200 | Loss: 0.733 | Acc: 68.375% | Wgt Acc: 68.825%
	I - Batch: 250 | Loss: 0.724 | Acc: 69.050% | Wgt Acc: 69.566%
	I - Batch: 300 | Loss: 0.723 | Acc: 68.708% | Wgt Acc: 69.106%
I - num batch: 319
I - Train -- Loss: 0.725 | Acc: 68.394% | Wgt Acc: 68.781% | LR: 1.250000e-04 | Dur: 125.01s
I - Confusion Matrix: [row->prediction - col->label]
[[559.  12.   4. 103.]
 [ 19. 406. 313.  12.]
 [  9. 146. 376.  22.]
 [110.  14.  41. 401.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.014 | Acc: 57.492% | Wgt Acc: 56.861% | Dur: 9.26s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  6.  4. 17.]
 [ 0. 23. 17.  1.]
 [ 3. 38. 40.  6.]
 [22. 11. 14. 62.]]

I - Local maximum validation set accuracy:  57.49

I - Validation set results: 
[14-1-2-3.34][50-3-3-3.61][124-2-2-2.38][127-0-0-5.67][443-2-2-2.98][567-0-3-4.74][573-1-3-2.55][615-0-3-4.94][695-1-3-3.58][722-3-3-4.82]
[826-0-0-5.72][878-0-0-5.71][1103-0-0-2.60][1212-3-3-4.41][1368-0-0-5.62][2181-2-3-3.63][2476-2-2-1.98][2721-2-2-3.98][2818-1-3-2.34][2886-2-2-3.37]
[3231-2-1-4.29][3333-2-1-4.30][3482-2-3-2.36][3536-3-1-1.17][3625-1-1-4.10][3909-0-0-4.26][4035-0-0-5.78][4140-0-0-5.69][4214-1-1-4.01][4346-1-0-5.63]
[4581-2-2-3.63][4708-3-3-3.06][4838-3-0-3.62][4845-1-2-3.87][4868-0-0-5.78][4939-0-2-3.04][4984-2-2-3.85][5078-1-3-2.21][5396-0-0-5.60][5479-1-1-3.61]
[5717-0-0-5.36][5843-1-1-4.23][5949-3-3-4.90][5987-2-1-2.95][6014-3-3-4.52][6033-3-3-4.60][6313-0-0-5.73][6421-3-3-4.93][6500-1-2-3.48][6583-3-3-4.92]
[6683-3-3-4.14][6825-2-3-4.87][6998-3-3-3.65][7049-3-3-4.77][7517-1-1-4.20][7521-1-0-3.45][7528-1-3-3.34][7949-1-2-3.74][8135-1-3-4.76][8185-3-0-5.72]
[8269-3-3-1.50][8273-3-0-5.68][8543-3-0-5.07][8666-1-1-4.16][8672-0-3-4.90][8903-1-2-4.06][9001-2-1-3.57][9036-2-2-3.98][9281-3-2-3.58][9300-2-2-4.08]
[9571-0-3-4.96][9617-1-3-2.62][9644-2-2-4.06][9705-2-3-3.50][9801-0-0-4.72][9803-3-0-5.68][9865-3-0-5.18][9896-2-2-3.41][10314-1-2-2.48][10337-3-3-4.82]
[10403-0-0-3.55][10653-2-2-2.12][10704-2-1-4.15][10719-1-2-3.99][10727-1-2-1.53][10836-0-0-5.68][10969-2-3-4.70][11042-0-0-5.37][11088-1-2-3.87][11322-0-0-5.68]
[11398-2-1-3.32][11499-0-0-4.20][11502-3-3-4.88][11512-3-3-4.73][11608-1-1-3.89][11610-0-0-5.83][11692-0-0-5.43][11905-0-0-5.72][11993-1-2-4.11][12002-2-0-5.13]
[12052-0-0-5.55][12201-0-0-5.68][12235-2-2-2.76][12320-1-2-1.03][12377-2-1-2.63][12398-2-1-4.27][12503-1-1-4.09][12617-0-0-3.69][12685-3-3-4.47][12738-2-3-4.94]
[12742-2-2-4.15][12823-0-3-4.84][13110-1-2-4.05][13240-3-0-5.68][13253-1-1-4.28][13273-0-0-5.64][13634-1-2-2.86][13763-2-2-2.86][13905-3-0-3.24][14060-2-1-4.21]
[14065-3-3-4.89][14147-3-3-3.14][14595-2-1-3.67][14687-2-2-4.01][14788-2-2-2.70][14869-1-2-3.92][14872-3-3-2.82][14877-1-2-3.77][14927-0-3-4.97][15066-0-0-5.73]
[15175-1-2-3.37][15178-2-3-3.74][15375-3-0-5.46][15389-3-3-4.95][15568-2-1-3.57][15675-3-3-3.84][15869-1-3-2.70][16207-3-0-1.78][16236-0-3-3.10][16302-3-3-4.49]
[16331-2-2-3.97][16381-0-3-1.77][16488-1-2-2.28][16495-0-0-5.63][16650-0-0-5.71][16719-1-1-4.31][16801-0-0-5.62][16828-0-0-5.72][17137-3-3-4.93][17245-1-2-1.23]
[17278-3-0-5.46][17282-0-0-5.65][17311-2-2-2.52][17336-2-1-4.23][17608-3-3-4.92][17627-0-3-4.74][17877-3-2-2.97][17924-1-2-3.81][17984-3-0-5.57][18211-0-3-4.96]
[18276-3-0-5.43][18287-1-1-4.29][18394-0-0-5.68][18428-0-0-5.47][18442-0-3-4.89][18478-3-3-4.95][18607-0-0-5.72][18616-0-0-1.44][18663-0-3-4.89][18718-0-0-5.68]
[18766-2-2-2.27][18824-2-2-3.48][18890-3-3-2.40][18930-3-0-2.17][18938-3-3-4.88][19817-1-2-3.76][19839-0-0-3.48][19930-3-3-4.64][19944-0-2-3.89][20036-2-2-4.00]
[20101-3-3-3.40][20474-1-2-3.77][20547-3-2-2.82][20929-2-1-4.30][21245-1-1-4.19][21257-3-2-4.11][21293-1-1-4.18][21316-1-1-4.25][21384-1-2-2.40][21448-1-2-3.98]
[21483-0-0-5.64][21487-2-2-4.07][21714-0-0-3.87][21943-3-3-2.78][21947-0-0-2.49][21948-0-0-5.70][21965-2-1-4.24][21998-1-2-2.49][22025-0-2-3.81][22228-3-3-4.65]
[22446-1-1-4.14][22494-3-3-4.86][22757-0-0-5.68][22811-3-3-4.98][22976-3-2-3.38][22985-3-3-4.94][23014-0-0-5.36][23112-1-2-3.97][23144-3-3-4.56][23168-2-3-1.61]
[23219-0-3-4.65][23363-3-3-4.98][23470-0-0-4.96][23486-2-3-3.23][23497-0-3-4.90][23516-0-0-5.75][23690-1-2-4.00][23921-2-2-3.27][23936-1-3-4.16][24040-3-3-4.62]
[24111-1-2-1.61][24182-0-0-5.63][24238-3-0-5.87][24290-2-0-2.94][24345-0-0-5.44][24364-1-2-4.10][24427-3-3-4.82][24477-2-2-4.06][24495-2-2-2.23][24893-2-2-3.89]
[25012-1-2-3.64][25121-2-2-3.66][25165-3-3-4.52][25183-0-0-5.62][25297-3-3-4.68][25398-0-0-5.19][25574-2-2-3.92][25644-1-1-4.12][25718-1-1-3.91][25774-2-2-4.17]
[26032-3-3-4.97][26051-3-3-4.97][26120-0-3-3.37][26321-1-2-3.88][26732-1-1-1.90][26784-3-3-4.58][26827-3-3-4.91][26833-0-3-4.98][26838-2-0-2.19][26860-1-1-1.59]
[26948-0-0-3.19][27049-3-3-4.70][27098-1-3-4.43][27526-0-0-5.66][27639-3-3-3.60][27698-3-3-4.98][27772-0-3-4.49][27890-1-2-3.16][28040-0-0-5.36][28503-2-2-4.09]
[28577-1-1-4.29][28959-0-0-5.78][29198-3-3-3.27][29777-0-0-5.65][29877-2-2-3.03][30035-1-2-3.85][30098-0-0-5.23][30326-1-1-3.95][30572-2-2-4.08][30716-0-0-1.37]
[30806-2-3-4.13][30906-1-2-3.92][31007-0-3-4.64][31181-3-3-4.34][31238-0-3-4.92][31347-0-0-5.81][31422-2-1-4.09][31429-3-3-2.31][31431-0-3-4.56][31432-1-1-2.63]
[31477-0-3-4.93][31524-1-0-2.85][31597-1-2-3.42][31619-1-0-5.22][31701-0-0-5.60][31755-0-0-5.68][31854-3-0-5.77][32074-1-3-4.86][32078-3-3-4.97][32111-1-0-3.88]
[32127-1-1-4.16][32140-3-3-4.51][32263-2-3-4.11][32365-0-0-5.58][32411-2-0-5.75][32429-3-0-5.71][32473-3-3-4.88][32574-3-3-4.91][32584-0-3-2.89][32622-0-0-1.39]
[32858-3-3-4.89][32969-3-3-4.95][33016-2-2-4.10][33031-1-0-5.09][33035-2-2-4.06][33133-2-2-4.10][33173-2-2-3.94][33175-3-2-3.71][33306-3-3-3.49][33309-2-3-2.99]
[33474-0-0-4.02][33478-2-3-4.90][33618-1-2-2.62][33712-0-0-5.40][33782-2-2-3.92][33914-3-3-4.93][34076-3-3-3.56][34112-2-1-4.27][34138-2-3-2.47][34239-1-2-3.79]
[34364-2-2-2.28][34617-1-2-3.32][34751-3-3-4.55][34783-2-1-3.52][35015-3-3-4.06][35018-1-2-3.47][35288-2-2-2.63]
---------------------------
I - Epoch: 39
I - Training: 
	I - Batch: 50 | Loss: 0.717 | Acc: 67.750% | Wgt Acc: 67.942%
	I - Batch: 100 | Loss: 0.719 | Acc: 67.250% | Wgt Acc: 67.382%
	I - Batch: 150 | Loss: 0.717 | Acc: 68.167% | Wgt Acc: 68.411%
	I - Batch: 200 | Loss: 0.718 | Acc: 68.875% | Wgt Acc: 69.138%
	I - Batch: 250 | Loss: 0.726 | Acc: 68.550% | Wgt Acc: 68.713%
	I - Batch: 300 | Loss: 0.718 | Acc: 69.083% | Wgt Acc: 69.339%
I - num batch: 319
I - Train -- Loss: 0.719 | Acc: 69.219% | Wgt Acc: 69.480% | LR: 1.250000e-04 | Dur: 117.70s
I - Confusion Matrix: [row->prediction - col->label]
[[559.  12.  14.  89.]
 [ 19. 392. 278.   9.]
 [ 14. 161. 402.  30.]
 [105.  13.  40. 410.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.050 | Acc: 55.963% | Wgt Acc: 55.639% | Dur: 9.24s
I - Confusion Matrix: [row->prediction - col->label]
[[63.  4.  8. 27.]
 [ 3. 35. 20.  3.]
 [ 1. 26. 33.  4.]
 [21. 13. 14. 52.]]

I - Epoch: 40
I - Training: 
	I - Batch: 50 | Loss: 0.668 | Acc: 73.750% | Wgt Acc: 73.989%
	I - Batch: 100 | Loss: 0.684 | Acc: 71.000% | Wgt Acc: 71.320%
	I - Batch: 150 | Loss: 0.702 | Acc: 69.750% | Wgt Acc: 70.084%
	I - Batch: 200 | Loss: 0.712 | Acc: 69.312% | Wgt Acc: 69.582%
	I - Batch: 250 | Loss: 0.718 | Acc: 68.750% | Wgt Acc: 69.097%
	I - Batch: 300 | Loss: 0.715 | Acc: 68.708% | Wgt Acc: 68.984%
I - num batch: 319
I - Train -- Loss: 0.716 | Acc: 68.433% | Wgt Acc: 68.701% | LR: 1.250000e-04 | Dur: 117.62s
I - Confusion Matrix: [row->prediction - col->label]
[[556.  14.   6.  92.]
 [ 16. 389. 283.   7.]
 [ 16. 162. 393.  34.]
 [109.  13.  52. 405.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.029 | Acc: 54.434% | Wgt Acc: 54.688% | Dur: 9.26s
I - Confusion Matrix: [row->prediction - col->label]
[[42.  4.  3. 10.]
 [ 9. 45. 25.  5.]
 [ 4. 26. 43. 23.]
 [33.  3.  4. 48.]]

I - Epoch: 41
I - Training: 
	I - Batch: 50 | Loss: 0.727 | Acc: 68.500% | Wgt Acc: 68.335%
	I - Batch: 100 | Loss: 0.720 | Acc: 69.250% | Wgt Acc: 69.482%
	I - Batch: 150 | Loss: 0.710 | Acc: 69.500% | Wgt Acc: 69.818%
	I - Batch: 200 | Loss: 0.709 | Acc: 69.188% | Wgt Acc: 69.396%
	I - Batch: 250 | Loss: 0.710 | Acc: 68.600% | Wgt Acc: 68.827%
	I - Batch: 300 | Loss: 0.709 | Acc: 68.250% | Wgt Acc: 68.473%
I - num batch: 319
I - Train -- Loss: 0.704 | Acc: 68.355% | Wgt Acc: 68.604% | LR: 1.250000e-04 | Dur: 117.50s
I - Confusion Matrix: [row->prediction - col->label]
[[555.  11.  10. 100.]
 [ 16. 399. 292.  11.]
 [ 11. 155. 395.  35.]
 [115.  13.  37. 392.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.061 | Acc: 52.905% | Wgt Acc: 53.125% | Dur: 9.23s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5.  7. 22.]
 [ 5. 33. 31.  2.]
 [ 3. 22. 22.  5.]
 [19. 18. 15. 57.]]

I - Epoch: 42
I - Training: 
	I - Batch: 50 | Loss: 0.706 | Acc: 69.500% | Wgt Acc: 69.882%
	I - Batch: 100 | Loss: 0.695 | Acc: 69.625% | Wgt Acc: 70.079%
	I - Batch: 150 | Loss: 0.697 | Acc: 69.250% | Wgt Acc: 69.889%
	I - Batch: 200 | Loss: 0.689 | Acc: 70.688% | Wgt Acc: 71.312%
	I - Batch: 250 | Loss: 0.686 | Acc: 70.500% | Wgt Acc: 71.099%
	I - Batch: 300 | Loss: 0.687 | Acc: 70.458% | Wgt Acc: 71.050%
I - num batch: 319
I - Train -- Loss: 0.693 | Acc: 69.965% | Wgt Acc: 70.506% | LR: 1.250000e-04 | Dur: 119.93s
I - Confusion Matrix: [row->prediction - col->label]
[[552.  14.   9.  74.]
 [ 16. 419. 293.  12.]
 [ 11. 135. 388.  29.]
 [118.  10.  44. 423.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.009 | Acc: 60.245% | Wgt Acc: 60.734% | Dur: 9.23s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  5. 11. 20.]
 [ 4. 47. 23.  4.]
 [ 2. 16. 30.  3.]
 [21. 10. 11. 59.]]

I - Local maximum validation set accuracy:  60.24

I - Validation set results: 
[14-1-1-4.20][50-3-3-3.14][124-2-1-3.54][127-0-0-5.68][443-2-2-3.36][567-0-3-4.85][573-1-1-3.60][615-0-3-5.08][695-1-3-4.81][722-3-3-4.98]
[826-0-0-5.40][878-0-0-5.80][1103-0-1-3.07][1212-3-0-3.49][1368-0-0-5.42][2181-2-3-1.78][2476-2-0-3.11][2721-2-2-3.85][2818-1-0-3.77][2886-2-1-3.70]
[3231-2-1-4.30][3333-2-1-4.22][3482-2-0-2.44][3536-3-0-1.43][3625-1-1-4.16][3909-0-0-3.82][4035-0-0-5.64][4140-0-0-5.52][4214-1-3-5.08][4346-1-3-4.91]
[4581-2-2-3.50][4708-3-3-3.71][4838-3-0-3.55][4845-1-2-3.82][4868-0-0-5.43][4939-0-3-2.44][4984-2-3-3.24][5078-1-3-2.68][5396-0-0-5.42][5479-1-1-4.18]
[5717-0-1-3.70][5843-1-1-4.02][5949-3-3-4.89][5987-2-1-4.22][6014-3-3-3.54][6033-3-1-2.12][6313-0-0-4.88][6421-3-3-5.10][6500-1-3-2.78][6583-3-3-4.80]
[6683-3-3-4.64][6825-2-0-3.12][6998-3-3-1.29][7049-3-3-4.91][7517-1-1-4.39][7521-1-1-4.22][7528-1-3-4.00][7949-1-2-3.92][8135-1-0-4.56][8185-3-0-4.61]
[8269-3-1-3.86][8273-3-0-5.82][8543-3-3-4.98][8666-1-1-3.87][8672-0-3-5.06][8903-1-1-4.38][9001-2-1-4.07][9036-2-2-4.02][9281-3-0-1.76][9300-2-2-4.12]
[9571-0-3-5.03][9617-1-1-3.99][9644-2-2-4.09][9705-2-0-4.05][9801-0-3-5.05][9803-3-0-4.09][9865-3-3-4.90][9896-2-2-3.53][10314-1-1-4.08][10337-3-3-5.05]
[10403-0-0-3.51][10653-2-0-2.11][10704-2-1-4.22][10719-1-1-4.16][10727-1-1-3.80][10836-0-0-5.73][10969-2-0-1.41][11042-0-0-5.26][11088-1-1-3.96][11322-0-0-5.69]
[11398-2-1-3.94][11499-0-0-3.99][11502-3-3-5.05][11512-3-3-3.68][11608-1-1-4.10][11610-0-0-5.69][11692-0-0-5.54][11905-0-0-5.79][11993-1-1-4.34][12002-2-1-2.68]
[12052-0-0-4.11][12201-0-0-5.55][12235-2-2-3.36][12320-1-1-2.91][12377-2-1-3.05][12398-2-1-3.68][12503-1-2-3.91][12617-0-0-3.14][12685-3-3-3.25][12738-2-3-3.38]
[12742-2-2-4.09][12823-0-3-4.71][13110-1-2-3.92][13240-3-0-5.64][13253-1-1-4.18][13273-0-0-5.60][13634-1-2-3.81][13763-2-3-3.95][13905-3-3-4.59][14060-2-1-4.18]
[14065-3-3-4.84][14147-3-3-4.17][14595-2-1-4.17][14687-2-2-4.00][14788-2-2-3.58][14869-1-1-4.18][14872-3-0-3.91][14877-1-1-4.33][14927-0-3-5.03][15066-0-0-5.64]
[15175-1-3-1.66][15178-2-3-3.52][15375-3-3-2.66][15389-3-3-5.06][15568-2-1-3.75][15675-3-3-2.85][15869-1-2-3.99][16207-3-0-3.02][16236-0-3-3.70][16302-3-0-4.80]
[16331-2-2-4.03][16381-0-3-3.51][16488-1-1-4.17][16495-0-0-4.50][16650-0-0-5.13][16719-1-1-4.25][16801-0-0-5.53][16828-0-0-5.57][17137-3-3-4.98][17245-1-0-2.16]
[17278-3-0-4.54][17282-0-0-4.79][17311-2-2-2.93][17336-2-1-4.31][17608-3-3-5.09][17627-0-0-4.40][17877-3-0-5.71][17924-1-1-4.29][17984-3-0-5.42][18211-0-3-3.80]
[18276-3-3-4.68][18287-1-1-3.96][18394-0-0-5.67][18428-0-0-5.46][18442-0-3-5.11][18478-3-3-4.98][18607-0-0-5.59][18616-0-0-2.09][18663-0-0-4.02][18718-0-0-5.63]
[18766-2-3-2.99][18824-2-1-3.94][18890-3-3-3.09][18930-3-0-5.28][18938-3-3-4.54][19817-1-2-3.18][19839-0-0-4.77][19930-3-3-4.86][19944-0-1-4.10][20036-2-2-4.07]
[20101-3-3-4.64][20474-1-2-3.85][20547-3-3-1.91][20929-2-1-4.37][21245-1-1-4.15][21257-3-2-4.07][21293-1-1-4.18][21316-1-3-3.75][21384-1-1-4.14][21448-1-1-4.17]
[21483-0-0-5.39][21487-2-2-4.26][21714-0-0-4.09][21943-3-2-2.50][21947-0-0-2.64][21948-0-0-5.67][21965-2-2-3.66][21998-1-1-3.06][22025-0-2-3.57][22228-3-3-4.59]
[22446-1-1-4.26][22494-3-0-5.66][22757-0-0-5.79][22811-3-3-5.11][22976-3-1-4.15][22985-3-3-5.04][23014-0-0-5.87][23112-1-2-3.87][23144-3-3-5.04][23168-2-0-4.27]
[23219-0-0-5.27][23363-3-3-5.11][23470-0-3-4.85][23486-2-3-4.39][23497-0-3-5.05][23516-0-3-4.94][23690-1-0-2.73][23921-2-2-3.47][23936-1-2-3.10][24040-3-3-3.72]
[24111-1-1-4.10][24182-0-3-4.96][24238-3-0-5.90][24290-2-0-3.03][24345-0-0-5.71][24364-1-2-4.04][24427-3-3-4.55][24477-2-1-3.29][24495-2-0-1.79][24893-2-2-2.63]
[25012-1-2-2.26][25121-2-2-3.62][25165-3-3-4.12][25183-0-0-4.59][25297-3-3-4.48][25398-0-0-5.59][25574-2-2-3.45][25644-1-1-4.21][25718-1-1-3.93][25774-2-2-3.82]
[26032-3-3-5.14][26051-3-3-5.19][26120-0-0-4.48][26321-1-1-3.24][26732-1-1-4.10][26784-3-3-5.13][26827-3-3-4.76][26833-0-3-5.11][26838-2-2-3.71][26860-1-1-3.09]
[26948-0-0-4.00][27049-3-3-4.43][27098-1-1-3.63][27526-0-0-5.22][27639-3-3-4.91][27698-3-3-4.84][27772-0-3-3.82][27890-1-1-4.16][28040-0-0-5.71][28503-2-2-4.22]
[28577-1-1-4.36][28959-0-0-5.62][29198-3-3-1.72][29777-0-0-5.40][29877-2-3-3.19][30035-1-2-3.89][30098-0-0-5.77][30326-1-1-4.11][30572-2-2-3.92][30716-0-1-3.99]
[30806-2-0-1.55][30906-1-1-4.03][31007-0-0-5.08][31181-3-3-3.70][31238-0-3-5.02][31347-0-0-5.82][31422-2-1-4.09][31429-3-2-3.65][31431-0-3-4.59][31432-1-1-4.14]
[31477-0-0-5.87][31524-1-1-4.38][31597-1-2-4.03][31619-1-0-4.72][31701-0-0-5.63][31755-0-0-5.49][31854-3-3-4.59][32074-1-3-3.01][32078-3-3-3.52][32111-1-1-3.89]
[32127-1-1-4.28][32140-3-3-5.02][32263-2-3-4.70][32365-0-0-4.42][32411-2-3-4.95][32429-3-0-5.16][32473-3-3-4.93][32574-3-3-5.01][32584-0-2-3.72][32622-0-0-2.48]
[32858-3-0-5.45][32969-3-0-5.28][33016-2-1-4.37][33031-1-3-3.33][33035-2-2-4.23][33133-2-1-3.53][33173-2-2-2.95][33175-3-1-3.51][33306-3-3-4.79][33309-2-3-3.14]
[33474-0-0-1.96][33478-2-0-1.57][33618-1-1-3.98][33712-0-3-4.16][33782-2-2-3.21][33914-3-3-5.08][34076-3-3-5.01][34112-2-1-2.85][34138-2-2-1.74][34239-1-2-3.69]
[34364-2-2-3.58][34617-1-2-3.82][34751-3-3-5.08][34783-2-1-3.96][35015-3-3-3.74][35018-1-1-3.89][35288-2-2-3.36]
---------------------------
I - Epoch: 43
I - Training: 
	I - Batch: 50 | Loss: 0.684 | Acc: 71.000% | Wgt Acc: 71.676%
	I - Batch: 100 | Loss: 0.677 | Acc: 70.000% | Wgt Acc: 70.552%
	I - Batch: 150 | Loss: 0.678 | Acc: 69.750% | Wgt Acc: 70.285%
	I - Batch: 200 | Loss: 0.676 | Acc: 70.375% | Wgt Acc: 70.731%
	I - Batch: 250 | Loss: 0.683 | Acc: 70.700% | Wgt Acc: 71.026%
	I - Batch: 300 | Loss: 0.684 | Acc: 70.458% | Wgt Acc: 70.841%
I - num batch: 319
I - Train -- Loss: 0.684 | Acc: 70.357% | Wgt Acc: 70.692% | LR: 1.250000e-04 | Dur: 119.64s
I - Confusion Matrix: [row->prediction - col->label]
[[564.   6.  13.  96.]
 [ 13. 428. 274.  13.]
 [ 12. 136. 405.  34.]
 [108.   8.  42. 395.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.011 | Acc: 56.881% | Wgt Acc: 56.997% | Dur: 9.15s
I - Confusion Matrix: [row->prediction - col->label]
[[52.  2.  5. 15.]
 [ 9. 44. 26.  4.]
 [ 7. 28. 39. 16.]
 [20.  4.  5. 51.]]

I - Epoch: 44
I - Training: 
	I - Batch: 50 | Loss: 0.621 | Acc: 76.250% | Wgt Acc: 77.130%
	I - Batch: 100 | Loss: 0.640 | Acc: 73.375% | Wgt Acc: 74.290%
	I - Batch: 150 | Loss: 0.660 | Acc: 71.667% | Wgt Acc: 72.359%
	I - Batch: 200 | Loss: 0.667 | Acc: 71.250% | Wgt Acc: 71.919%
	I - Batch: 250 | Loss: 0.665 | Acc: 71.800% | Wgt Acc: 72.366%
	I - Batch: 300 | Loss: 0.656 | Acc: 72.500% | Wgt Acc: 72.969%
I - num batch: 319
I - Train -- Loss: 0.658 | Acc: 72.320% | Wgt Acc: 72.753% | LR: 1.250000e-04 | Dur: 116.74s
I - Confusion Matrix: [row->prediction - col->label]
[[566.   8.  11.  83.]
 [ 13. 436. 271.  10.]
 [ 13. 127. 420.  25.]
 [105.   7.  32. 420.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.054 | Acc: 58.104% | Wgt Acc: 57.880% | Dur: 9.38s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  5.  5. 15.]
 [ 5. 36. 21.  6.]
 [ 5. 30. 38.  9.]
 [18.  7. 11. 56.]]

I - Epoch: 45
I - Training: 
	I - Batch: 50 | Loss: 0.645 | Acc: 70.500% | Wgt Acc: 70.714%
	I - Batch: 100 | Loss: 0.620 | Acc: 73.500% | Wgt Acc: 73.709%
	I - Batch: 150 | Loss: 0.651 | Acc: 72.333% | Wgt Acc: 72.534%
	I - Batch: 200 | Loss: 0.660 | Acc: 71.375% | Wgt Acc: 71.737%
	I - Batch: 250 | Loss: 0.663 | Acc: 70.950% | Wgt Acc: 71.398%
	I - Batch: 300 | Loss: 0.655 | Acc: 71.708% | Wgt Acc: 72.201%
I - num batch: 319
I - Train -- Loss: 0.658 | Acc: 71.771% | Wgt Acc: 72.275% | LR: 1.250000e-04 | Dur: 118.37s
I - Confusion Matrix: [row->prediction - col->label]
[[557.   9.   3.  67.]
 [ 19. 423. 281.  14.]
 [ 12. 139. 413.  22.]
 [109.   7.  37. 435.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.054 | Acc: 55.657% | Wgt Acc: 55.299% | Dur: 9.18s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  8.  5. 21.]
 [10. 37. 21.  3.]
 [ 2. 27. 39. 13.]
 [19.  6. 10. 49.]]

I - Epoch: 46
I - Training: 
	I - Batch: 50 | Loss: 0.632 | Acc: 75.000% | Wgt Acc: 75.741%
	I - Batch: 100 | Loss: 0.619 | Acc: 74.625% | Wgt Acc: 75.126%
	I - Batch: 150 | Loss: 0.622 | Acc: 74.083% | Wgt Acc: 74.476%
	I - Batch: 200 | Loss: 0.626 | Acc: 74.625% | Wgt Acc: 74.856%
	I - Batch: 250 | Loss: 0.636 | Acc: 74.350% | Wgt Acc: 74.530%
	I - Batch: 300 | Loss: 0.636 | Acc: 74.375% | Wgt Acc: 74.519%
I - num batch: 319
I - Train -- Loss: 0.635 | Acc: 74.087% | Wgt Acc: 74.266% | LR: 1.250000e-04 | Dur: 117.54s
I - Confusion Matrix: [row->prediction - col->label]
[[574.  11.  10.  76.]
 [ 15. 418. 234.   8.]
 [ 11. 137. 466.  25.]
 [ 97.  12.  24. 429.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 0.957 | Acc: 62.385% | Wgt Acc: 62.908% | Dur: 9.29s
I - Confusion Matrix: [row->prediction - col->label]
[[69.  3. 10. 23.]
 [ 2. 53. 33.  4.]
 [ 3. 15. 25.  2.]
 [14.  7.  7. 57.]]

I - Local maximum validation set accuracy:  62.39

I - Validation set results: 
[14-1-2-3.36][50-3-3-5.06][124-2-1-2.89][127-0-0-5.68][443-2-1-3.56][567-0-0-5.88][573-1-1-4.27][615-0-0-5.70][695-1-3-3.14][722-3-3-5.21]
[826-0-0-5.82][878-0-0-5.86][1103-0-0-2.69][1212-3-3-4.99][1368-0-0-5.60][2181-2-2-2.62][2476-2-0-2.91][2721-2-2-4.03][2818-1-0-5.02][2886-2-2-3.81]
[3231-2-1-4.41][3333-2-1-3.43][3482-2-1-3.43][3536-3-3-2.96][3625-1-1-4.27][3909-0-0-5.27][4035-0-0-5.79][4140-0-0-5.57][4214-1-1-4.27][4346-1-3-4.78]
[4581-2-2-4.09][4708-3-3-3.61][4838-3-0-5.70][4845-1-2-3.75][4868-0-0-5.71][4939-0-2-3.97][4984-2-3-2.22][5078-1-1-2.46][5396-0-0-5.75][5479-1-1-2.81]
[5717-0-0-5.71][5843-1-1-4.42][5949-3-3-5.07][5987-2-1-3.67][6014-3-0-4.07][6033-3-3-4.74][6313-0-0-5.80][6421-3-3-5.22][6500-1-2-4.47][6583-3-3-5.22]
[6683-3-3-4.46][6825-2-0-5.06][6998-3-1-2.10][7049-3-3-5.14][7517-1-1-4.29][7521-1-1-3.13][7528-1-3-3.40][7949-1-2-2.02][8135-1-0-4.82][8185-3-0-5.75]
[8269-3-2-3.86][8273-3-0-5.88][8543-3-3-5.12][8666-1-1-3.11][8672-0-0-5.76][8903-1-1-3.79][9001-2-1-4.30][9036-2-1-4.36][9281-3-3-1.47][9300-2-2-4.21]
[9571-0-3-4.46][9617-1-1-4.09][9644-2-1-4.32][9705-2-0-4.18][9801-0-3-5.18][9803-3-0-4.93][9865-3-0-5.30][9896-2-1-4.26][10314-1-1-4.17][10337-3-3-5.17]
[10403-0-1-3.26][10653-2-2-2.18][10704-2-1-4.27][10719-1-1-4.30][10727-1-2-1.71][10836-0-0-5.78][10969-2-0-5.20][11042-0-0-5.47][11088-1-1-4.29][11322-0-0-5.81]
[11398-2-0-0.78][11499-0-0-2.66][11502-3-3-5.10][11512-3-0-5.50][11608-1-1-4.27][11610-0-0-5.30][11692-0-3-5.14][11905-0-0-5.86][11993-1-1-2.95][12002-2-0-3.38]
[12052-0-0-5.41][12201-0-0-5.83][12235-2-2-4.22][12320-1-1-3.22][12377-2-1-4.33][12398-2-1-3.98][12503-1-1-4.27][12617-0-3-3.70][12685-3-3-3.18][12738-2-3-4.93]
[12742-2-2-4.19][12823-0-3-4.92][13110-1-2-4.17][13240-3-0-5.75][13253-1-1-4.40][13273-0-0-5.75][13634-1-3-3.02][13763-2-3-4.09][13905-3-3-2.96][14060-2-1-4.35]
[14065-3-3-5.14][14147-3-3-4.29][14595-2-1-4.21][14687-2-2-4.18][14788-2-2-4.45][14869-1-2-3.68][14872-3-0-4.44][14877-1-1-4.37][14927-0-3-5.16][15066-0-0-5.91]
[15175-1-1-3.63][15178-2-3-2.54][15375-3-3-3.29][15389-3-3-5.18][15568-2-1-4.01][15675-3-3-3.30][15869-1-1-3.54][16207-3-0-3.06][16236-0-0-3.11][16302-3-3-4.97]
[16331-2-2-4.11][16381-0-3-4.93][16488-1-1-3.88][16495-0-0-4.29][16650-0-0-5.72][16719-1-1-4.40][16801-0-0-5.76][16828-0-0-5.77][17137-3-3-4.47][17245-1-2-3.55]
[17278-3-3-4.00][17282-0-0-5.66][17311-2-2-1.67][17336-2-1-4.38][17608-3-3-5.20][17627-0-0-3.18][17877-3-0-5.11][17924-1-1-4.12][17984-3-0-5.70][18211-0-3-3.97]
[18276-3-0-5.77][18287-1-1-4.28][18394-0-0-5.72][18428-0-0-3.56][18442-0-3-5.22][18478-3-0-4.95][18607-0-0-4.71][18616-0-0-2.20][18663-0-3-4.12][18718-0-0-5.82]
[18766-2-1-4.07][18824-2-2-4.26][18890-3-3-2.86][18930-3-0-4.07][18938-3-3-5.22][19817-1-2-3.70][19839-0-0-5.76][19930-3-0-4.84][19944-0-0-5.72][20036-2-2-4.37]
[20101-3-3-5.18][20474-1-2-4.25][20547-3-3-2.39][20929-2-1-4.47][21245-1-1-4.30][21257-3-3-2.78][21293-1-1-4.29][21316-1-1-4.39][21384-1-1-4.27][21448-1-1-4.40]
[21483-0-0-5.63][21487-2-2-4.28][21714-0-0-5.26][21943-3-2-3.31][21947-0-0-5.42][21948-0-0-5.80][21965-2-2-4.05][21998-1-1-4.32][22025-0-2-3.89][22228-3-3-5.24]
[22446-1-1-4.13][22494-3-0-5.23][22757-0-0-5.77][22811-3-3-5.22][22976-3-1-4.24][22985-3-3-5.19][23014-0-0-6.00][23112-1-2-4.46][23144-3-3-4.99][23168-2-0-5.56]
[23219-0-0-5.13][23363-3-3-5.22][23470-0-0-3.60][23486-2-2-3.76][23497-0-3-5.12][23516-0-0-5.89][23690-1-1-3.31][23921-2-1-4.28][23936-1-3-3.67][24040-3-3-3.96]
[24111-1-1-3.17][24182-0-0-5.86][24238-3-3-5.12][24290-2-0-5.09][24345-0-0-5.42][24364-1-2-4.24][24427-3-0-4.95][24477-2-1-4.00][24495-2-1-3.43][24893-2-3-2.46]
[25012-1-1-4.41][25121-2-2-3.81][25165-3-3-4.01][25183-0-0-5.72][25297-3-3-5.15][25398-0-0-5.79][25574-2-1-4.27][25644-1-1-4.39][25718-1-1-2.96][25774-2-2-3.39]
[26032-3-3-5.25][26051-3-3-5.21][26120-0-0-3.96][26321-1-2-4.22][26732-1-1-4.29][26784-3-3-5.22][26827-3-3-5.23][26833-0-3-5.22][26838-2-2-3.87][26860-1-1-4.03]
[26948-0-0-5.61][27049-3-0-5.81][27098-1-1-4.27][27526-0-0-5.31][27639-3-3-4.72][27698-3-3-5.18][27772-0-0-5.82][27890-1-1-4.33][28040-0-3-4.83][28503-2-2-3.95]
[28577-1-1-4.33][28959-0-0-5.68][29198-3-0-3.97][29777-0-0-5.63][29877-2-1-3.92][30035-1-1-4.29][30098-0-0-5.97][30326-1-1-4.27][30572-2-1-4.46][30716-0-2-2.97]
[30806-2-2-2.73][30906-1-1-4.31][31007-0-0-5.72][31181-3-3-4.82][31238-0-0-5.46][31347-0-0-5.83][31422-2-1-3.98][31429-3-1-2.06][31431-0-0-5.56][31432-1-1-4.03]
[31477-0-0-5.62][31524-1-2-2.67][31597-1-2-4.22][31619-1-0-5.64][31701-0-0-5.74][31755-0-0-5.74][31854-3-0-5.51][32074-1-3-4.83][32078-3-3-5.17][32111-1-1-3.48]
[32127-1-1-4.27][32140-3-3-5.23][32263-2-3-4.29][32365-0-0-5.45][32411-2-0-4.91][32429-3-0-5.64][32473-3-3-5.11][32574-3-3-5.19][32584-0-0-3.86][32622-0-1-2.14]
[32858-3-0-5.76][32969-3-3-5.22][33016-2-1-4.47][33031-1-3-4.78][33035-2-2-4.50][33133-2-2-4.41][33173-2-1-3.61][33175-3-1-3.64][33306-3-3-2.45][33309-2-1-1.62]
[33474-0-0-5.76][33478-2-0-5.64][33618-1-1-2.93][33712-0-3-5.16][33782-2-1-3.83][33914-3-3-5.19][34076-3-3-5.21][34112-2-1-4.36][34138-2-3-3.95][34239-1-1-2.04]
[34364-2-1-4.28][34617-1-1-4.27][34751-3-3-5.22][34783-2-1-3.87][35015-3-3-5.00][35018-1-1-4.26][35288-2-2-3.94]
---------------------------
I - Epoch: 47
I - Training: 
	I - Batch: 50 | Loss: 0.618 | Acc: 75.000% | Wgt Acc: 75.242%
	I - Batch: 100 | Loss: 0.643 | Acc: 73.250% | Wgt Acc: 73.407%
	I - Batch: 150 | Loss: 0.643 | Acc: 73.500% | Wgt Acc: 73.691%
	I - Batch: 200 | Loss: 0.645 | Acc: 73.062% | Wgt Acc: 73.258%
	I - Batch: 250 | Loss: 0.647 | Acc: 72.800% | Wgt Acc: 73.073%
	I - Batch: 300 | Loss: 0.641 | Acc: 73.208% | Wgt Acc: 73.429%
I - num batch: 319
I - Train -- Loss: 0.635 | Acc: 73.459% | Wgt Acc: 73.717% | LR: 1.250000e-04 | Dur: 118.23s
I - Confusion Matrix: [row->prediction - col->label]
[[568.   6.   9.  81.]
 [ 14. 436. 238.   8.]
 [ 13. 128. 454.  36.]
 [102.   8.  33. 413.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.120 | Acc: 56.881% | Wgt Acc: 57.133% | Dur: 9.24s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  8.  7. 25.]
 [ 1. 39. 16.  1.]
 [ 1. 16. 25.  2.]
 [22. 15. 27. 58.]]

I - Epoch: 48
I - Training: 
	I - Batch: 50 | Loss: 0.643 | Acc: 73.750% | Wgt Acc: 74.076%
	I - Batch: 100 | Loss: 0.631 | Acc: 74.875% | Wgt Acc: 75.119%
	I - Batch: 150 | Loss: 0.624 | Acc: 75.167% | Wgt Acc: 75.374%
	I - Batch: 200 | Loss: 0.635 | Acc: 74.750% | Wgt Acc: 75.084%
	I - Batch: 250 | Loss: 0.631 | Acc: 74.850% | Wgt Acc: 75.070%
	I - Batch: 300 | Loss: 0.633 | Acc: 74.083% | Wgt Acc: 74.343%
I - num batch: 319
I - Train -- Loss: 0.629 | Acc: 74.519% | Wgt Acc: 74.770% | LR: 1.250000e-04 | Dur: 119.46s
I - Confusion Matrix: [row->prediction - col->label]
[[585.  10.   3.  86.]
 [ 17. 441. 238.   6.]
 [ 11. 121. 453.  27.]
 [ 84.   6.  40. 419.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.016 | Acc: 61.162% | Wgt Acc: 60.938% | Dur: 9.20s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  6.  5. 19.]
 [ 2. 35. 18.  4.]
 [ 2. 25. 38.  1.]
 [19. 12. 14. 62.]]

I - Epoch: 49
I - Training: 
	I - Batch: 50 | Loss: 0.621 | Acc: 75.000% | Wgt Acc: 74.930%
	I - Batch: 100 | Loss: 0.637 | Acc: 73.250% | Wgt Acc: 73.510%
	I - Batch: 150 | Loss: 0.640 | Acc: 73.417% | Wgt Acc: 73.707%
	I - Batch: 200 | Loss: 0.634 | Acc: 74.250% | Wgt Acc: 74.560%
	I - Batch: 250 | Loss: 0.625 | Acc: 74.750% | Wgt Acc: 75.059%
	I - Batch: 300 | Loss: 0.626 | Acc: 74.917% | Wgt Acc: 75.181%
I - num batch: 319
I - Train -- Loss: 0.624 | Acc: 75.108% | Wgt Acc: 75.380% | LR: 1.250000e-04 | Dur: 117.28s
I - Confusion Matrix: [row->prediction - col->label]
[[572.   8.  15.  84.]
 [ 17. 452. 218.   8.]
 [  7. 109. 472.  29.]
 [101.   9.  29. 417.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.005 | Acc: 58.410% | Wgt Acc: 58.220% | Dur: 9.25s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  5.  5. 21.]
 [ 7. 41. 21.  3.]
 [ 4. 28. 42. 10.]
 [21.  4.  7. 52.]]

I - Epoch: 50
I - Training: 
	I - Batch: 50 | Loss: 0.577 | Acc: 78.250% | Wgt Acc: 78.443%
	I - Batch: 100 | Loss: 0.583 | Acc: 77.625% | Wgt Acc: 77.934%
	I - Batch: 150 | Loss: 0.612 | Acc: 76.167% | Wgt Acc: 76.472%
	I - Batch: 200 | Loss: 0.609 | Acc: 76.500% | Wgt Acc: 76.819%
	I - Batch: 250 | Loss: 0.611 | Acc: 76.250% | Wgt Acc: 76.518%
	I - Batch: 300 | Loss: 0.606 | Acc: 76.333% | Wgt Acc: 76.558%
I - num batch: 319
I - Train -- Loss: 0.602 | Acc: 76.443% | Wgt Acc: 76.690% | LR: 1.250000e-04 | Dur: 117.21s
I - Confusion Matrix: [row->prediction - col->label]
[[596.   9.  13.  70.]
 [ 13. 447. 219.   8.]
 [ 16. 112. 470.  26.]
 [ 72.  10.  32. 434.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.108 | Acc: 56.881% | Wgt Acc: 56.114% | Dur: 9.17s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  5.  6. 25.]
 [ 4. 28. 12.  1.]
 [ 3. 28. 45.  6.]
 [22. 17. 12. 54.]]

I - Epoch: 51
I - Training: 
	I - Batch: 50 | Loss: 0.601 | Acc: 75.750% | Wgt Acc: 75.763%
	I - Batch: 100 | Loss: 0.617 | Acc: 75.875% | Wgt Acc: 75.585%
	I - Batch: 150 | Loss: 0.607 | Acc: 76.667% | Wgt Acc: 76.631%
	I - Batch: 200 | Loss: 0.606 | Acc: 76.750% | Wgt Acc: 76.700%
	I - Batch: 250 | Loss: 0.595 | Acc: 77.350% | Wgt Acc: 77.401%
	I - Batch: 300 | Loss: 0.596 | Acc: 77.208% | Wgt Acc: 77.177%
I - num batch: 319
I - Train -- Loss: 0.595 | Acc: 77.385% | Wgt Acc: 77.380% | LR: 1.250000e-04 | Dur: 117.03s
I - Confusion Matrix: [row->prediction - col->label]
[[587.   7.   8.  71.]
 [ 11. 427. 172.   5.]
 [  9. 138. 521.  26.]
 [ 90.   6.  33. 436.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.028 | Acc: 61.468% | Wgt Acc: 61.617% | Dur: 9.19s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  5.  9. 14.]
 [ 7. 48. 22.  7.]
 [ 3. 20. 40. 10.]
 [20.  5.  4. 55.]]

I - Epoch: 52
I - Training: 
	I - Batch: 50 | Loss: 0.554 | Acc: 78.250% | Wgt Acc: 78.419%
	I - Batch: 100 | Loss: 0.550 | Acc: 79.250% | Wgt Acc: 79.402%
	I - Batch: 150 | Loss: 0.546 | Acc: 79.000% | Wgt Acc: 79.103%
	I - Batch: 200 | Loss: 0.565 | Acc: 78.125% | Wgt Acc: 78.149%
	I - Batch: 250 | Loss: 0.566 | Acc: 77.900% | Wgt Acc: 78.022%
	I - Batch: 300 | Loss: 0.576 | Acc: 77.542% | Wgt Acc: 77.699%
I - num batch: 319
I - Train -- Loss: 0.570 | Acc: 78.053% | Wgt Acc: 78.194% | LR: 1.250000e-04 | Dur: 117.22s
I - Confusion Matrix: [row->prediction - col->label]
[[586.   8.  10.  61.]
 [ 10. 440. 185.   7.]
 [ 16. 123. 515.  23.]
 [ 85.   7.  24. 447.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.054 | Acc: 59.939% | Wgt Acc: 59.783% | Dur: 9.24s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  9.  7. 19.]
 [ 2. 34. 17.  2.]
 [ 4. 21. 33.  3.]
 [15. 14. 18. 62.]]

I - Epoch: 53
I - Training: 
	I - Batch: 50 | Loss: 0.522 | Acc: 80.250% | Wgt Acc: 80.442%
	I - Batch: 100 | Loss: 0.555 | Acc: 79.500% | Wgt Acc: 79.571%
	I - Batch: 150 | Loss: 0.555 | Acc: 78.500% | Wgt Acc: 78.838%
	I - Batch: 200 | Loss: 0.547 | Acc: 78.750% | Wgt Acc: 79.103%
	I - Batch: 250 | Loss: 0.559 | Acc: 77.950% | Wgt Acc: 78.281%
	I - Batch: 300 | Loss: 0.559 | Acc: 78.417% | Wgt Acc: 78.665%
I - num batch: 319
I - Train -- Loss: 0.565 | Acc: 78.210% | Wgt Acc: 78.477% | LR: 1.250000e-04 | Dur: 119.15s
I - Confusion Matrix: [row->prediction - col->label]
[[588.   8.   8.  66.]
 [ 13. 463. 192.  11.]
 [  9. 102. 501.  21.]
 [ 87.   5.  33. 440.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.142 | Acc: 53.823% | Wgt Acc: 52.310% | Dur: 9.32s
I - Confusion Matrix: [row->prediction - col->label]
[[71.  9. 11. 44.]
 [ 4. 31. 16.  4.]
 [ 2. 35. 39.  3.]
 [11.  3.  9. 35.]]

I - Epoch: 54
I - Training: 
	I - Batch: 50 | Loss: 0.544 | Acc: 80.500% | Wgt Acc: 80.661%
	I - Batch: 100 | Loss: 0.509 | Acc: 82.500% | Wgt Acc: 82.528%
	I - Batch: 150 | Loss: 0.502 | Acc: 82.167% | Wgt Acc: 82.283%
	I - Batch: 200 | Loss: 0.504 | Acc: 81.812% | Wgt Acc: 82.028%
	I - Batch: 250 | Loss: 0.514 | Acc: 81.650% | Wgt Acc: 81.798%
	I - Batch: 300 | Loss: 0.528 | Acc: 80.750% | Wgt Acc: 80.922%
I - num batch: 319
I - Train -- Loss: 0.532 | Acc: 80.526% | Wgt Acc: 80.697% | LR: 1.250000e-04 | Dur: 119.91s
I - Confusion Matrix: [row->prediction - col->label]
[[599.   7.  10.  52.]
 [  8. 469. 160.  11.]
 [ 10.  99. 534.  26.]
 [ 80.   3.  30. 449.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.138 | Acc: 57.187% | Wgt Acc: 56.726% | Dur: 9.19s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  7.  6. 16.]
 [ 2. 23. 11.  2.]
 [ 3. 37. 42.  4.]
 [25. 11. 16. 64.]]

I - Epoch: 55
I - Training: 
	I - Batch: 50 | Loss: 0.530 | Acc: 79.750% | Wgt Acc: 80.125%
	I - Batch: 100 | Loss: 0.528 | Acc: 80.000% | Wgt Acc: 80.282%
	I - Batch: 150 | Loss: 0.498 | Acc: 81.917% | Wgt Acc: 82.081%
	I - Batch: 200 | Loss: 0.487 | Acc: 82.562% | Wgt Acc: 82.641%
	I - Batch: 250 | Loss: 0.500 | Acc: 82.100% | Wgt Acc: 82.163%
	I - Batch: 300 | Loss: 0.510 | Acc: 81.208% | Wgt Acc: 81.254%
I - num batch: 319
I - Train -- Loss: 0.510 | Acc: 81.311% | Wgt Acc: 81.378% | LR: 1.250000e-04 | Dur: 116.93s
I - Confusion Matrix: [row->prediction - col->label]
[[595.   6.   9.  61.]
 [ 15. 463. 137.   5.]
 [ 11. 105. 561.  20.]
 [ 76.   4.  27. 452.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.150 | Acc: 54.740% | Wgt Acc: 53.804% | Dur: 9.16s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  8.  6. 26.]
 [ 7. 31. 18.  7.]
 [ 7. 36. 41.  8.]
 [12.  3. 10. 45.]]

I - Epoch: 56
I - Training: 
	I - Batch: 50 | Loss: 0.553 | Acc: 81.000% | Wgt Acc: 81.035%
	I - Batch: 100 | Loss: 0.535 | Acc: 81.375% | Wgt Acc: 81.248%
	I - Batch: 150 | Loss: 0.520 | Acc: 81.417% | Wgt Acc: 81.394%
	I - Batch: 200 | Loss: 0.518 | Acc: 81.188% | Wgt Acc: 81.179%
	I - Batch: 250 | Loss: 0.512 | Acc: 81.450% | Wgt Acc: 81.528%
	I - Batch: 300 | Loss: 0.512 | Acc: 81.333% | Wgt Acc: 81.389%
I - num batch: 319
I - Train -- Loss: 0.507 | Acc: 81.743% | Wgt Acc: 81.785% | LR: 1.250000e-04 | Dur: 120.29s
I - Confusion Matrix: [row->prediction - col->label]
[[599.  11.   3.  52.]
 [  6. 458. 137.   9.]
 [ 15. 105. 566.  18.]
 [ 77.   4.  28. 459.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.049 | Acc: 60.856% | Wgt Acc: 59.715% | Dur: 9.27s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  9.  9. 22.]
 [ 3. 28.  9.  3.]
 [ 2. 33. 46.  6.]
 [13.  8. 11. 55.]]

I - Epoch: 57
I - Training: 
	I - Batch: 50 | Loss: 0.508 | Acc: 83.250% | Wgt Acc: 83.521%
	I - Batch: 100 | Loss: 0.503 | Acc: 82.000% | Wgt Acc: 82.133%
	I - Batch: 150 | Loss: 0.485 | Acc: 83.833% | Wgt Acc: 83.827%
	I - Batch: 200 | Loss: 0.488 | Acc: 83.938% | Wgt Acc: 84.008%
	I - Batch: 250 | Loss: 0.493 | Acc: 83.450% | Wgt Acc: 83.511%
	I - Batch: 300 | Loss: 0.496 | Acc: 82.958% | Wgt Acc: 83.088%
I - num batch: 319
I - Train -- Loss: 0.500 | Acc: 82.921% | Wgt Acc: 83.015% | LR: 1.250000e-04 | Dur: 119.82s
I - Confusion Matrix: [row->prediction - col->label]
[[599.   8.   8.  62.]
 [ 12. 484. 131.   4.]
 [ 10.  80. 577.  20.]
 [ 76.   6.  18. 452.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.079 | Acc: 55.963% | Wgt Acc: 55.299% | Dur: 9.39s
I - Confusion Matrix: [row->prediction - col->label]
[[51.  7.  6. 21.]
 [ 8. 39. 13.  6.]
 [ 4. 30. 50. 16.]
 [25.  2.  6. 43.]]

I - Epoch: 58
I - Training: 
	I - Batch: 50 | Loss: 0.470 | Acc: 84.250% | Wgt Acc: 84.320%
	I - Batch: 100 | Loss: 0.475 | Acc: 84.625% | Wgt Acc: 84.522%
	I - Batch: 150 | Loss: 0.477 | Acc: 84.333% | Wgt Acc: 84.314%
	I - Batch: 200 | Loss: 0.472 | Acc: 84.000% | Wgt Acc: 84.041%
	I - Batch: 250 | Loss: 0.473 | Acc: 83.750% | Wgt Acc: 83.807%
	I - Batch: 300 | Loss: 0.466 | Acc: 84.292% | Wgt Acc: 84.327%
I - num batch: 319
I - Train -- Loss: 0.467 | Acc: 84.060% | Wgt Acc: 84.094% | LR: 1.250000e-04 | Dur: 118.30s
I - Confusion Matrix: [row->prediction - col->label]
[[618.   8.   6.  56.]
 [  4. 491. 119.   8.]
 [ 10.  78. 581.  23.]
 [ 65.   1.  28. 451.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.089 | Acc: 52.905% | Wgt Acc: 52.582% | Dur: 9.37s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  8. 12. 22.]
 [ 6. 31. 19.  4.]
 [ 2. 35. 37.  9.]
 [26.  4.  7. 51.]]

I - Epoch: 59
I - Training: 
	I - Batch: 50 | Loss: 0.455 | Acc: 86.250% | Wgt Acc: 86.174%
	I - Batch: 100 | Loss: 0.439 | Acc: 85.375% | Wgt Acc: 85.351%
	I - Batch: 150 | Loss: 0.421 | Acc: 86.500% | Wgt Acc: 86.559%
	I - Batch: 200 | Loss: 0.426 | Acc: 85.750% | Wgt Acc: 85.923%
	I - Batch: 250 | Loss: 0.430 | Acc: 85.650% | Wgt Acc: 85.827%
	I - Batch: 300 | Loss: 0.428 | Acc: 85.458% | Wgt Acc: 85.568%
I - num batch: 319
I - Train -- Loss: 0.433 | Acc: 85.316% | Wgt Acc: 85.368% | LR: 1.250000e-04 | Dur: 118.26s
I - Confusion Matrix: [row->prediction - col->label]
[[612.   5.   5.  49.]
 [  6. 494. 107.   1.]
 [  9.  73. 603.  24.]
 [ 70.   6.  19. 464.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.217 | Acc: 55.657% | Wgt Acc: 54.416% | Dur: 9.36s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  3.  6. 19.]
 [ 6. 25. 12.  2.]
 [10. 41. 51. 17.]
 [14.  9.  6. 48.]]

I - Epoch: 60
I - Training: 
	I - Batch: 50 | Loss: 0.483 | Acc: 84.250% | Wgt Acc: 83.905%
	I - Batch: 100 | Loss: 0.458 | Acc: 85.250% | Wgt Acc: 85.032%
	I - Batch: 150 | Loss: 0.462 | Acc: 84.500% | Wgt Acc: 84.367%
	I - Batch: 200 | Loss: 0.453 | Acc: 84.688% | Wgt Acc: 84.599%
	I - Batch: 250 | Loss: 0.449 | Acc: 85.150% | Wgt Acc: 85.087%
	I - Batch: 300 | Loss: 0.440 | Acc: 85.500% | Wgt Acc: 85.422%
I - num batch: 319
I - Train -- Loss: 0.439 | Acc: 85.669% | Wgt Acc: 85.589% | LR: 1.250000e-04 | Dur: 118.51s
I - Confusion Matrix: [row->prediction - col->label]
[[621.   7.   3.  50.]
 [  9. 488.  95.   4.]
 [  9.  78. 614.  25.]
 [ 58.   5.  22. 459.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.051 | Acc: 62.997% | Wgt Acc: 62.432% | Dur: 10.07s
I - Confusion Matrix: [row->prediction - col->label]
[[68.  7.  9. 28.]
 [ 2. 45. 13.  4.]
 [ 7. 24. 43.  4.]
 [11.  2. 10. 50.]]

I - Local maximum validation set accuracy:  63.00

I - Validation set results: 
[14-1-1-5.28][50-3-3-4.60][124-2-2-4.77][127-0-0-5.90][443-2-2-4.69][567-0-0-6.09][573-1-1-5.33][615-0-3-5.00][695-1-2-3.54][722-3-3-5.52]
[826-0-0-5.07][878-0-0-6.06][1103-0-0-4.44][1212-3-3-5.18][1368-0-0-5.82][2181-2-3-3.93][2476-2-3-4.53][2721-2-2-4.84][2818-1-0-5.88][2886-2-2-4.27]
[3231-2-2-4.82][3333-2-1-5.30][3482-2-3-3.86][3536-3-3-4.98][3625-1-1-5.41][3909-0-0-3.61][4035-0-0-5.97][4140-0-0-5.13][4214-1-3-5.57][4346-1-0-5.95]
[4581-2-2-4.65][4708-3-3-4.94][4838-3-0-5.42][4845-1-2-4.24][4868-0-0-5.47][4939-0-2-3.81][4984-2-2-4.80][5078-1-0-1.75][5396-0-0-6.02][5479-1-2-4.38]
[5717-0-0-5.08][5843-1-1-5.13][5949-3-3-5.31][5987-2-2-3.97][6014-3-3-4.29][6033-3-3-5.01][6313-0-0-6.05][6421-3-3-5.57][6500-1-1-4.96][6583-3-3-5.57]
[6683-3-3-5.21][6825-2-0-6.08][6998-3-2-4.17][7049-3-3-5.37][7517-1-1-5.14][7521-1-1-3.60][7528-1-2-4.60][7949-1-2-4.75][8135-1-0-4.02][8185-3-0-5.96]
[8269-3-1-4.91][8273-3-3-5.57][8543-3-0-5.99][8666-1-1-5.43][8672-0-0-6.01][8903-1-2-4.78][9001-2-1-5.44][9036-2-2-4.76][9281-3-3-2.98][9300-2-2-4.75]
[9571-0-3-5.52][9617-1-1-5.37][9644-2-2-4.77][9705-2-0-3.52][9801-0-3-4.85][9803-3-0-6.03][9865-3-0-5.94][9896-2-2-4.57][10314-1-1-4.69][10337-3-3-5.54]
[10403-0-1-4.86][10653-2-1-5.34][10704-2-1-5.43][10719-1-1-5.25][10727-1-2-3.25][10836-0-0-5.96][10969-2-0-3.90][11042-0-0-5.72][11088-1-1-5.05][11322-0-0-5.81]
[11398-2-2-4.70][11499-0-2-2.52][11502-3-3-5.32][11512-3-2-3.56][11608-1-1-5.19][11610-0-0-6.01][11692-0-0-4.97][11905-0-0-6.01][11993-1-1-5.15][12002-2-0-6.01]
[12052-0-0-5.62][12201-0-0-6.18][12235-2-2-4.61][12320-1-0-3.18][12377-2-1-5.15][12398-2-1-5.33][12503-1-1-4.73][12617-0-1-4.48][12685-3-1-2.28][12738-2-0-5.63]
[12742-2-2-4.80][12823-0-0-5.14][13110-1-1-4.69][13240-3-0-5.93][13253-1-1-4.92][13273-0-0-5.98][13634-1-1-3.62][13763-2-3-3.94][13905-3-0-3.94][14060-2-1-5.32]
[14065-3-0-4.89][14147-3-3-5.58][14595-2-2-4.72][14687-2-2-4.51][14788-2-2-4.84][14869-1-1-4.83][14872-3-0-4.95][14877-1-1-5.30][14927-0-0-3.34][15066-0-0-6.04]
[15175-1-2-3.89][15178-2-3-4.72][15375-3-3-3.42][15389-3-3-5.38][15568-2-3-2.61][15675-3-1-5.27][15869-1-3-5.01][16207-3-0-3.94][16236-0-3-3.07][16302-3-0-5.96]
[16331-2-2-4.69][16381-0-0-4.59][16488-1-1-5.41][16495-0-0-5.81][16650-0-0-6.02][16719-1-1-4.89][16801-0-0-5.97][16828-0-0-6.02][17137-3-3-3.50][17245-1-2-4.70]
[17278-3-0-5.70][17282-0-3-5.44][17311-2-2-4.30][17336-2-2-4.75][17608-3-3-5.37][17627-0-0-3.25][17877-3-3-2.43][17924-1-2-4.83][17984-3-0-5.85][18211-0-3-4.24]
[18276-3-0-5.83][18287-1-1-3.13][18394-0-0-5.89][18428-0-0-3.71][18442-0-3-5.12][18478-3-0-6.13][18607-0-3-5.15][18616-0-0-3.37][18663-0-0-4.83][18718-0-0-5.93]
[18766-2-2-4.24][18824-2-2-4.77][18890-3-3-5.16][18930-3-0-2.86][18938-3-3-5.48][19817-1-2-4.47][19839-0-0-5.49][19930-3-0-5.96][19944-0-2-4.65][20036-2-1-4.99]
[20101-3-3-4.05][20474-1-2-4.73][20547-3-0-2.03][20929-2-1-4.77][21245-1-2-4.78][21257-3-3-4.17][21293-1-1-5.32][21316-1-1-5.43][21384-1-1-5.43][21448-1-1-5.15]
[21483-0-0-5.90][21487-2-2-4.75][21714-0-0-3.63][21943-3-1-3.58][21947-0-0-2.89][21948-0-0-6.05][21965-2-2-4.69][21998-1-1-5.06][22025-0-2-3.02][22228-3-3-5.08]
[22446-1-1-5.39][22494-3-0-6.13][22757-0-0-6.02][22811-3-3-5.57][22976-3-2-4.43][22985-3-0-5.15][23014-0-0-6.09][23112-1-2-5.04][23144-3-0-4.40][23168-2-0-6.03]
[23219-0-0-5.97][23363-3-0-4.84][23470-0-0-3.92][23486-2-2-4.72][23497-0-3-5.28][23516-0-0-6.11][23690-1-2-4.10][23921-2-2-4.72][23936-1-2-3.87][24040-3-3-3.34]
[24111-1-2-4.20][24182-0-0-6.02][24238-3-3-5.47][24290-2-0-5.00][24345-0-0-3.29][24364-1-2-4.82][24427-3-3-5.38][24477-2-2-4.73][24495-2-1-5.31][24893-2-1-4.64]
[25012-1-1-4.66][25121-2-2-4.51][25165-3-0-5.73][25183-0-0-6.03][25297-3-3-5.57][25398-0-0-5.88][25574-2-2-3.95][25644-1-1-5.12][25718-1-1-5.27][25774-2-2-4.76]
[26032-3-3-5.57][26051-3-3-5.57][26120-0-0-5.82][26321-1-1-5.05][26732-1-1-5.43][26784-3-3-5.58][26827-3-3-5.28][26833-0-3-5.57][26838-2-2-4.75][26860-1-2-4.76]
[26948-0-0-5.23][27049-3-0-6.02][27098-1-1-4.87][27526-0-0-5.53][27639-3-3-5.03][27698-3-3-4.95][27772-0-0-6.09][27890-1-1-5.41][28040-0-2-4.04][28503-2-2-4.74]
[28577-1-1-4.84][28959-0-0-6.03][29198-3-3-5.45][29777-0-0-5.81][29877-2-3-4.04][30035-1-1-5.37][30098-0-0-5.99][30326-1-1-5.39][30572-2-2-4.77][30716-0-2-4.75]
[30806-2-2-4.75][30906-1-1-4.43][31007-0-2-3.07][31181-3-3-5.57][31238-0-0-5.80][31347-0-0-6.15][31422-2-2-4.56][31429-3-3-3.95][31431-0-0-4.62][31432-1-1-5.42]
[31477-0-0-6.12][31524-1-2-4.42][31597-1-2-4.83][31619-1-0-6.09][31701-0-0-5.94][31755-0-0-5.91][31854-3-0-6.11][32074-1-1-5.36][32078-3-3-5.57][32111-1-1-3.39]
[32127-1-1-5.29][32140-3-3-5.03][32263-2-3-4.13][32365-0-0-5.42][32411-2-0-6.11][32429-3-0-5.84][32473-3-0-5.41][32574-3-3-5.14][32584-0-0-2.88][32622-0-0-3.33]
[32858-3-0-6.08][32969-3-3-5.30][33016-2-1-5.02][33031-1-0-5.53][33035-2-2-4.73][33133-2-2-4.76][33173-2-2-3.99][33175-3-2-2.62][33306-3-3-4.05][33309-2-3-3.86]
[33474-0-0-4.35][33478-2-0-5.39][33618-1-1-4.41][33712-0-3-5.51][33782-2-2-4.77][33914-3-3-5.06][34076-3-3-5.57][34112-2-2-4.87][34138-2-3-4.00][34239-1-2-4.23]
[34364-2-2-4.53][34617-1-2-4.80][34751-3-3-5.56][34783-2-1-4.95][35015-3-3-5.54][35018-1-2-4.78][35288-2-2-3.49]
---------------------------
I - Epoch: 61
I - Training: 
	I - Batch: 50 | Loss: 0.479 | Acc: 82.000% | Wgt Acc: 82.070%
	I - Batch: 100 | Loss: 0.454 | Acc: 83.125% | Wgt Acc: 83.084%
	I - Batch: 150 | Loss: 0.431 | Acc: 84.583% | Wgt Acc: 84.500%
	I - Batch: 200 | Loss: 0.434 | Acc: 84.812% | Wgt Acc: 84.803%
	I - Batch: 250 | Loss: 0.430 | Acc: 85.200% | Wgt Acc: 85.212%
	I - Batch: 300 | Loss: 0.432 | Acc: 84.917% | Wgt Acc: 84.943%
I - num batch: 319
I - Train -- Loss: 0.435 | Acc: 84.884% | Wgt Acc: 84.864% | LR: 1.250000e-04 | Dur: 121.03s
I - Confusion Matrix: [row->prediction - col->label]
[[604.   7.   4.  57.]
 [ 11. 493.  91.   3.]
 [ 12.  76. 613.  26.]
 [ 70.   2.  26. 452.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.182 | Acc: 55.657% | Wgt Acc: 55.095% | Dur: 10.08s
I - Confusion Matrix: [row->prediction - col->label]
[[62. 12.  8. 29.]
 [ 4. 38. 26.  7.]
 [ 3. 24. 37.  5.]
 [19.  4.  4. 45.]]

I - Epoch: 62
I - Training: 
	I - Batch: 50 | Loss: 0.343 | Acc: 91.000% | Wgt Acc: 90.904%
	I - Batch: 100 | Loss: 0.380 | Acc: 88.500% | Wgt Acc: 88.369%
	I - Batch: 150 | Loss: 0.390 | Acc: 87.917% | Wgt Acc: 87.847%
	I - Batch: 200 | Loss: 0.399 | Acc: 87.312% | Wgt Acc: 87.178%
	I - Batch: 250 | Loss: 0.390 | Acc: 87.300% | Wgt Acc: 87.212%
	I - Batch: 300 | Loss: 0.401 | Acc: 86.917% | Wgt Acc: 86.778%
I - num batch: 319
I - Train -- Loss: 0.404 | Acc: 86.887% | Wgt Acc: 86.757% | LR: 1.250000e-04 | Dur: 118.20s
I - Confusion Matrix: [row->prediction - col->label]
[[629.   5.   6.  44.]
 [  7. 493.  73.   4.]
 [ 10.  74. 629.  28.]
 [ 51.   6.  26. 462.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.235 | Acc: 54.740% | Wgt Acc: 54.144% | Dur: 9.12s
I - Confusion Matrix: [row->prediction - col->label]
[[42.  3.  4. 12.]
 [10. 30.  9.  4.]
 [ 8. 38. 56. 19.]
 [28.  7.  6. 51.]]

I - Epoch: 63
I - Training: 
	I - Batch: 50 | Loss: 0.371 | Acc: 86.500% | Wgt Acc: 86.412%
	I - Batch: 100 | Loss: 0.356 | Acc: 88.625% | Wgt Acc: 88.510%
	I - Batch: 150 | Loss: 0.354 | Acc: 89.000% | Wgt Acc: 88.983%
	I - Batch: 200 | Loss: 0.367 | Acc: 88.562% | Wgt Acc: 88.549%
	I - Batch: 250 | Loss: 0.388 | Acc: 87.650% | Wgt Acc: 87.693%
	I - Batch: 300 | Loss: 0.384 | Acc: 88.000% | Wgt Acc: 87.994%
I - num batch: 319
I - Train -- Loss: 0.383 | Acc: 87.790% | Wgt Acc: 87.783% | LR: 1.250000e-04 | Dur: 118.40s
I - Confusion Matrix: [row->prediction - col->label]
[[625.   9.   1.  34.]
 [ 15. 496.  84.   3.]
 [  4.  72. 632.  18.]
 [ 53.   1.  17. 483.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.257 | Acc: 57.492% | Wgt Acc: 56.997% | Dur: 9.20s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  6.  8. 19.]
 [ 1. 26.  5.  3.]
 [ 1. 32. 47.  3.]
 [32. 14. 15. 61.]]

I - Epoch: 64
I - Training: 
	I - Batch: 50 | Loss: 0.413 | Acc: 88.500% | Wgt Acc: 88.036%
	I - Batch: 100 | Loss: 0.429 | Acc: 87.000% | Wgt Acc: 86.893%
	I - Batch: 150 | Loss: 0.418 | Acc: 87.000% | Wgt Acc: 86.879%
	I - Batch: 200 | Loss: 0.402 | Acc: 87.438% | Wgt Acc: 87.331%
	I - Batch: 250 | Loss: 0.388 | Acc: 87.950% | Wgt Acc: 87.850%
	I - Batch: 300 | Loss: 0.388 | Acc: 87.875% | Wgt Acc: 87.826%
I - num batch: 319
I - Train -- Loss: 0.383 | Acc: 87.947% | Wgt Acc: 87.872% | LR: 1.250000e-04 | Dur: 121.71s
I - Confusion Matrix: [row->prediction - col->label]
[[629.   6.   7.  46.]
 [ 11. 508.  73.   2.]
 [  7.  58. 638.  25.]
 [ 50.   6.  16. 465.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.259 | Acc: 57.492% | Wgt Acc: 56.318% | Dur: 9.28s
I - Confusion Matrix: [row->prediction - col->label]
[[74. 10. 12. 36.]
 [ 3. 32. 15.  2.]
 [ 2. 27. 37.  3.]
 [ 9.  9. 11. 45.]]

I - Epoch: 65
I - Training: 
	I - Batch: 50 | Loss: 0.330 | Acc: 90.000% | Wgt Acc: 89.989%
	I - Batch: 100 | Loss: 0.328 | Acc: 89.625% | Wgt Acc: 89.662%
	I - Batch: 150 | Loss: 0.327 | Acc: 90.167% | Wgt Acc: 90.154%
	I - Batch: 200 | Loss: 0.345 | Acc: 89.250% | Wgt Acc: 89.273%
	I - Batch: 250 | Loss: 0.354 | Acc: 89.000% | Wgt Acc: 88.945%
	I - Batch: 300 | Loss: 0.351 | Acc: 89.250% | Wgt Acc: 89.196%
I - num batch: 319
I - Train -- Loss: 0.353 | Acc: 89.203% | Wgt Acc: 89.172% | LR: 1.250000e-04 | Dur: 119.08s
I - Confusion Matrix: [row->prediction - col->label]
[[627.   4.   4.  49.]
 [  7. 521.  60.   4.]
 [  7.  51. 653.  14.]
 [ 56.   2.  17. 471.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.253 | Acc: 58.104% | Wgt Acc: 58.084% | Dur: 9.31s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  8.  8. 19.]
 [ 9. 42. 26. 10.]
 [ 3. 21. 36.  4.]
 [17.  7.  5. 53.]]

I - Epoch: 66
I - Training: 
	I - Batch: 50 | Loss: 0.352 | Acc: 89.250% | Wgt Acc: 89.058%
	I - Batch: 100 | Loss: 0.369 | Acc: 88.000% | Wgt Acc: 87.779%
	I - Batch: 150 | Loss: 0.374 | Acc: 87.500% | Wgt Acc: 87.385%
	I - Batch: 200 | Loss: 0.385 | Acc: 86.938% | Wgt Acc: 86.843%
	I - Batch: 250 | Loss: 0.375 | Acc: 87.650% | Wgt Acc: 87.556%
	I - Batch: 300 | Loss: 0.372 | Acc: 87.750% | Wgt Acc: 87.668%
I - num batch: 319
I - Train -- Loss: 0.379 | Acc: 87.554% | Wgt Acc: 87.447% | LR: 1.250000e-04 | Dur: 121.01s
I - Confusion Matrix: [row->prediction - col->label]
[[627.   5.   5.  54.]
 [ 10. 508.  74.   4.]
 [  9.  61. 638.  23.]
 [ 51.   4.  17. 457.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.287 | Acc: 54.128% | Wgt Acc: 52.649% | Dur: 9.35s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  9.  8. 34.]
 [ 4. 17.  8.  0.]
 [ 3. 42. 45.  2.]
 [16. 10. 14. 50.]]

I - Epoch: 67
I - Training: 
	I - Batch: 50 | Loss: 0.330 | Acc: 89.500% | Wgt Acc: 89.647%
	I - Batch: 100 | Loss: 0.351 | Acc: 89.000% | Wgt Acc: 89.014%
	I - Batch: 150 | Loss: 0.338 | Acc: 89.167% | Wgt Acc: 89.139%
	I - Batch: 200 | Loss: 0.342 | Acc: 89.312% | Wgt Acc: 89.283%
	I - Batch: 250 | Loss: 0.333 | Acc: 89.950% | Wgt Acc: 89.908%
	I - Batch: 300 | Loss: 0.325 | Acc: 90.333% | Wgt Acc: 90.242%
I - num batch: 319
I - Train -- Loss: 0.328 | Acc: 90.185% | Wgt Acc: 90.092% | LR: 1.250000e-04 | Dur: 120.86s
I - Confusion Matrix: [row->prediction - col->label]
[[641.   4.   2.  41.]
 [  3. 518.  56.   2.]
 [  6.  54. 660.  17.]
 [ 47.   2.  16. 478.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.202 | Acc: 58.410% | Wgt Acc: 58.424% | Dur: 9.52s
I - Confusion Matrix: [row->prediction - col->label]
[[57.  7. 14. 22.]
 [ 4. 42. 19.  6.]
 [ 1. 25. 38.  4.]
 [26.  4.  4. 54.]]

I - Epoch: 68
I - Training: 
	I - Batch: 50 | Loss: 0.330 | Acc: 89.250% | Wgt Acc: 89.524%
	I - Batch: 100 | Loss: 0.320 | Acc: 90.250% | Wgt Acc: 90.546%
	I - Batch: 150 | Loss: 0.319 | Acc: 90.667% | Wgt Acc: 90.837%
	I - Batch: 200 | Loss: 0.323 | Acc: 90.688% | Wgt Acc: 90.814%
	I - Batch: 250 | Loss: 0.317 | Acc: 90.850% | Wgt Acc: 91.004%
	I - Batch: 300 | Loss: 0.331 | Acc: 90.042% | Wgt Acc: 90.145%
I - num batch: 319
I - Train -- Loss: 0.330 | Acc: 90.027% | Wgt Acc: 90.145% | LR: 1.250000e-04 | Dur: 119.33s
I - Confusion Matrix: [row->prediction - col->label]
[[631.   6.   5.  33.]
 [ 12. 533.  67.   1.]
 [  5.  36. 644.  19.]
 [ 49.   3.  18. 485.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.343 | Acc: 55.046% | Wgt Acc: 54.755% | Dur: 9.45s
I - Confusion Matrix: [row->prediction - col->label]
[[49.  9.  2. 14.]
 [ 1. 16.  7.  1.]
 [ 1. 39. 45.  1.]
 [37. 14. 21. 70.]]

I - Epoch: 69
I - Training: 
	I - Batch: 50 | Loss: 0.323 | Acc: 89.500% | Wgt Acc: 89.597%
	I - Batch: 100 | Loss: 0.321 | Acc: 89.500% | Wgt Acc: 89.545%
	I - Batch: 150 | Loss: 0.317 | Acc: 90.083% | Wgt Acc: 90.015%
	I - Batch: 200 | Loss: 0.313 | Acc: 90.062% | Wgt Acc: 90.025%
	I - Batch: 250 | Loss: 0.306 | Acc: 90.300% | Wgt Acc: 90.308%
	I - Batch: 300 | Loss: 0.310 | Acc: 90.167% | Wgt Acc: 90.158%
I - num batch: 319
I - Train -- Loss: 0.313 | Acc: 89.949% | Wgt Acc: 89.924% | LR: 1.250000e-04 | Dur: 122.18s
I - Confusion Matrix: [row->prediction - col->label]
[[630.  10.   3.  42.]
 [ 13. 525.  51.   3.]
 [  7.  42. 660.  17.]
 [ 47.   1.  20. 476.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.074 | Acc: 63.303% | Wgt Acc: 62.840% | Dur: 10.62s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  4.  7. 23.]
 [ 5. 40. 10.  2.]
 [ 5. 30. 48.  4.]
 [16.  4. 10. 57.]]

I - Local maximum validation set accuracy:  63.30

I - Validation set results: 
[14-1-2-4.66][50-3-3-3.15][124-2-1-4.41][127-0-0-6.02][443-2-2-5.11][567-0-0-5.93][573-1-1-4.91][615-0-3-4.34][695-1-3-3.93][722-3-3-5.46]
[826-0-0-6.12][878-0-0-6.10][1103-0-0-5.27][1212-3-3-4.81][1368-0-0-5.94][2181-2-3-5.05][2476-2-2-4.21][2721-2-2-5.18][2818-1-2-3.34][2886-2-2-4.68]
[3231-2-2-4.85][3333-2-2-4.32][3482-2-2-4.62][3536-3-3-4.40][3625-1-1-5.87][3909-0-0-5.60][4035-0-0-6.07][4140-0-3-4.85][4214-1-2-4.52][4346-1-3-5.15]
[4581-2-2-3.95][4708-3-3-4.85][4838-3-1-2.51][4845-1-2-5.24][4868-0-0-6.09][4939-0-2-4.46][4984-2-2-5.19][5078-1-2-2.54][5396-0-0-6.09][5479-1-1-5.86]
[5717-0-0-4.21][5843-1-2-4.79][5949-3-3-5.68][5987-2-2-4.79][6014-3-3-4.41][6033-3-2-4.31][6313-0-0-6.06][6421-3-3-5.47][6500-1-2-4.95][6583-3-3-5.62]
[6683-3-3-4.67][6825-2-0-4.64][6998-3-2-4.97][7049-3-3-5.66][7517-1-1-5.69][7521-1-1-4.11][7528-1-2-4.27][7949-1-2-5.08][8135-1-0-4.07][8185-3-0-6.04]
[8269-3-2-4.29][8273-3-3-5.76][8543-3-3-5.66][8666-1-1-5.87][8672-0-0-6.08][8903-1-2-5.27][9001-2-1-5.87][9036-2-2-5.06][9281-3-3-3.56][9300-2-2-5.25]
[9571-0-0-4.24][9617-1-2-2.28][9644-2-2-5.22][9705-2-0-3.89][9801-0-0-4.88][9803-3-0-5.95][9865-3-3-5.68][9896-2-2-4.78][10314-1-1-5.87][10337-3-3-5.76]
[10403-0-0-3.84][10653-2-1-5.40][10704-2-1-5.87][10719-1-1-5.87][10727-1-1-4.27][10836-0-0-6.16][10969-2-0-5.46][11042-0-0-5.67][11088-1-2-4.41][11322-0-0-5.98]
[11398-2-2-5.10][11499-0-0-2.57][11502-3-3-4.54][11512-3-0-5.87][11608-1-1-5.87][11610-0-0-5.22][11692-0-3-5.21][11905-0-0-6.22][11993-1-2-4.81][12002-2-0-3.51]
[12052-0-0-4.23][12201-0-0-4.88][12235-2-2-4.40][12320-1-0-3.24][12377-2-1-5.14][12398-2-2-4.71][12503-1-1-5.87][12617-0-1-5.82][12685-3-3-5.76][12738-2-3-5.67]
[12742-2-2-5.19][12823-0-3-5.21][13110-1-1-5.43][13240-3-0-6.06][13253-1-1-5.87][13273-0-0-6.03][13634-1-3-3.09][13763-2-3-4.79][13905-3-0-3.18][14060-2-1-5.87]
[14065-3-3-5.41][14147-3-3-5.76][14595-2-2-5.28][14687-2-2-4.94][14788-2-2-4.53][14869-1-2-4.65][14872-3-3-5.54][14877-1-1-4.98][14927-0-2-3.43][15066-0-3-5.35]
[15175-1-1-3.08][15178-2-3-4.27][15375-3-0-6.02][15389-3-3-5.48][15568-2-1-5.87][15675-3-3-3.26][15869-1-2-4.85][16207-3-0-5.50][16236-0-1-4.35][16302-3-0-5.98]
[16331-2-2-5.15][16381-0-0-3.92][16488-1-1-5.87][16495-0-0-6.02][16650-0-0-6.15][16719-1-2-5.15][16801-0-0-6.12][16828-0-0-6.07][17137-3-3-5.58][17245-1-2-5.13]
[17278-3-0-4.16][17282-0-0-4.90][17311-2-2-3.94][17336-2-2-5.15][17608-3-3-5.76][17627-0-1-5.60][17877-3-0-5.53][17924-1-2-4.51][17984-3-0-6.04][18211-0-3-5.65]
[18276-3-3-5.70][18287-1-2-3.57][18394-0-0-5.99][18428-0-0-5.98][18442-0-3-5.75][18478-3-3-5.32][18607-0-3-5.21][18616-0-0-3.34][18663-0-0-5.20][18718-0-0-6.00]
[18766-2-1-5.87][18824-2-2-5.23][18890-3-3-5.51][18930-3-0-3.03][18938-3-3-5.76][19817-1-2-4.63][19839-0-0-5.93][19930-3-0-5.37][19944-0-2-5.14][20036-2-2-4.80]
[20101-3-3-5.31][20474-1-2-3.82][20547-3-0-4.12][20929-2-2-5.17][21245-1-1-5.11][21257-3-1-5.87][21293-1-1-5.87][21316-1-1-5.87][21384-1-1-5.84][21448-1-1-4.82]
[21483-0-0-5.93][21487-2-2-5.17][21714-0-2-2.72][21943-3-3-5.28][21947-0-0-3.97][21948-0-0-6.19][21965-2-2-4.90][21998-1-2-5.05][22025-0-1-4.28][22228-3-3-5.55]
[22446-1-2-5.21][22494-3-3-5.22][22757-0-0-5.98][22811-3-3-5.73][22976-3-3-4.07][22985-3-3-5.41][23014-0-0-5.34][23112-1-2-5.30][23144-3-3-5.76][23168-2-0-5.26]
[23219-0-0-3.56][23363-3-3-5.48][23470-0-0-4.94][23486-2-3-4.88][23497-0-3-5.70][23516-0-0-6.08][23690-1-2-5.11][23921-2-1-5.87][23936-1-3-5.03][24040-3-0-4.89]
[24111-1-1-4.54][24182-0-3-5.34][24238-3-0-5.91][24290-2-0-5.37][24345-0-0-5.52][24364-1-2-5.24][24427-3-0-5.39][24477-2-2-4.87][24495-2-3-3.93][24893-2-2-4.90]
[25012-1-2-5.19][25121-2-2-4.86][25165-3-0-4.57][25183-0-0-6.04][25297-3-3-5.76][25398-0-0-5.94][25574-2-2-4.29][25644-1-1-5.69][25718-1-1-5.85][25774-2-2-3.82]
[26032-3-3-5.76][26051-3-3-5.76][26120-0-0-2.81][26321-1-1-5.87][26732-1-1-5.83][26784-3-3-5.65][26827-3-3-5.76][26833-0-3-5.76][26838-2-2-5.24][26860-1-2-4.22]
[26948-0-0-4.63][27049-3-0-6.08][27098-1-1-4.52][27526-0-0-5.98][27639-3-0-5.46][27698-3-3-5.76][27772-0-0-6.07][27890-1-1-5.82][28040-0-2-3.97][28503-2-2-5.09]
[28577-1-1-5.69][28959-0-0-6.14][29198-3-3-3.98][29777-0-0-5.93][29877-2-2-3.78][30035-1-1-5.86][30098-0-0-6.15][30326-1-1-5.87][30572-2-2-5.26][30716-0-1-3.56]
[30806-2-2-2.49][30906-1-1-5.80][31007-0-3-5.48][31181-3-3-5.74][31238-0-3-5.56][31347-0-3-5.49][31422-2-2-3.37][31429-3-3-5.57][31431-0-0-5.54][31432-1-1-5.70]
[31477-0-3-5.39][31524-1-1-4.91][31597-1-2-5.19][31619-1-0-6.01][31701-0-0-5.96][31755-0-0-5.11][31854-3-0-5.84][32074-1-1-4.87][32078-3-3-5.76][32111-1-1-5.87]
[32127-1-1-5.87][32140-3-3-5.28][32263-2-3-3.57][32365-0-0-5.45][32411-2-3-5.57][32429-3-0-5.84][32473-3-3-5.70][32574-3-3-5.70][32584-0-0-5.57][32622-0-0-4.94]
[32858-3-0-5.20][32969-3-3-5.57][33016-2-2-4.91][33031-1-0-3.27][33035-2-2-5.15][33133-2-2-5.19][33173-2-2-4.44][33175-3-2-4.16][33306-3-3-5.59][33309-2-3-3.93]
[33474-0-0-5.83][33478-2-0-6.04][33618-1-1-5.87][33712-0-3-5.47][33782-2-2-4.99][33914-3-3-5.75][34076-3-3-5.73][34112-2-2-5.35][34138-2-3-4.91][34239-1-2-4.82]
[34364-2-2-4.53][34617-1-1-4.99][34751-3-3-5.69][34783-2-1-5.32][35015-3-3-5.25][35018-1-1-5.72][35288-2-2-5.17]
---------------------------
I - Epoch: 70
I - Training: 
	I - Batch: 50 | Loss: 0.309 | Acc: 91.000% | Wgt Acc: 90.966%
	I - Batch: 100 | Loss: 0.328 | Acc: 90.000% | Wgt Acc: 90.025%
	I - Batch: 150 | Loss: 0.331 | Acc: 89.167% | Wgt Acc: 89.197%
	I - Batch: 200 | Loss: 0.326 | Acc: 89.562% | Wgt Acc: 89.587%
	I - Batch: 250 | Loss: 0.323 | Acc: 89.850% | Wgt Acc: 89.872%
	I - Batch: 300 | Loss: 0.322 | Acc: 89.792% | Wgt Acc: 89.781%
I - num batch: 319
I - Train -- Loss: 0.318 | Acc: 89.910% | Wgt Acc: 89.915% | LR: 1.250000e-04 | Dur: 120.49s
I - Confusion Matrix: [row->prediction - col->label]
[[639.   9.   4.  35.]
 [  8. 525.  59.   2.]
 [  7.  41. 647.  22.]
 [ 43.   3.  24. 479.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.204 | Acc: 55.657% | Wgt Acc: 55.774% | Dur: 9.42s
I - Confusion Matrix: [row->prediction - col->label]
[[40.  9.  5.  9.]
 [ 2. 24. 11.  2.]
 [ 3. 37. 49.  6.]
 [43.  8. 10. 69.]]

I - Epoch: 71
I - Training: 
	I - Batch: 50 | Loss: 0.254 | Acc: 92.750% | Wgt Acc: 92.918%
	I - Batch: 100 | Loss: 0.260 | Acc: 92.625% | Wgt Acc: 92.708%
	I - Batch: 150 | Loss: 0.256 | Acc: 92.667% | Wgt Acc: 92.703%
	I - Batch: 200 | Loss: 0.267 | Acc: 92.000% | Wgt Acc: 92.092%
	I - Batch: 250 | Loss: 0.282 | Acc: 91.450% | Wgt Acc: 91.512%
	I - Batch: 300 | Loss: 0.299 | Acc: 90.583% | Wgt Acc: 90.586%
I - num batch: 319
I - Train -- Loss: 0.299 | Acc: 90.577% | Wgt Acc: 90.587% | LR: 1.250000e-04 | Dur: 118.50s
I - Confusion Matrix: [row->prediction - col->label]
[[642.   3.   4.  32.]
 [  9. 532.  55.   2.]
 [  5.  41. 653.  24.]
 [ 41.   2.  22. 480.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.242 | Acc: 55.963% | Wgt Acc: 55.435% | Dur: 9.29s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  9.  9. 31.]
 [ 3. 46. 30.  8.]
 [ 4. 17. 32.  9.]
 [14.  6.  4. 38.]]

I - Epoch: 72
I - Training: 
	I - Batch: 50 | Loss: 0.306 | Acc: 91.000% | Wgt Acc: 90.950%
	I - Batch: 100 | Loss: 0.264 | Acc: 92.750% | Wgt Acc: 92.545%
	I - Batch: 150 | Loss: 0.271 | Acc: 92.500% | Wgt Acc: 92.401%
	I - Batch: 200 | Loss: 0.274 | Acc: 92.125% | Wgt Acc: 92.063%
	I - Batch: 250 | Loss: 0.279 | Acc: 91.900% | Wgt Acc: 91.804%
	I - Batch: 300 | Loss: 0.277 | Acc: 91.833% | Wgt Acc: 91.749%
I - num batch: 319
I - Train -- Loss: 0.282 | Acc: 91.598% | Wgt Acc: 91.525% | LR: 1.250000e-04 | Dur: 120.83s
I - Confusion Matrix: [row->prediction - col->label]
[[646.   6.   1.  35.]
 [  6. 528.  41.   0.]
 [  4.  42. 673.  17.]
 [ 41.   2.  19. 486.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.241 | Acc: 57.492% | Wgt Acc: 57.609% | Dur: 9.54s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  5.  8. 18.]
 [ 4. 41. 23.  5.]
 [ 2. 22. 38.  8.]
 [28. 10.  6. 55.]]

I - Epoch: 73
I - Training: 
	I - Batch: 50 | Loss: 0.258 | Acc: 93.500% | Wgt Acc: 93.557%
	I - Batch: 100 | Loss: 0.244 | Acc: 93.500% | Wgt Acc: 93.516%
	I - Batch: 150 | Loss: 0.240 | Acc: 93.500% | Wgt Acc: 93.576%
	I - Batch: 200 | Loss: 0.241 | Acc: 93.562% | Wgt Acc: 93.578%
	I - Batch: 250 | Loss: 0.250 | Acc: 93.100% | Wgt Acc: 93.138%
	I - Batch: 300 | Loss: 0.267 | Acc: 92.292% | Wgt Acc: 92.281%
I - num batch: 319
I - Train -- Loss: 0.271 | Acc: 91.951% | Wgt Acc: 91.923% | LR: 1.250000e-04 | Dur: 120.68s
I - Confusion Matrix: [row->prediction - col->label]
[[654.   9.   4.  19.]
 [  9. 529.  46.   4.]
 [  4.  39. 665.  21.]
 [ 30.   1.  19. 494.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.288 | Acc: 58.716% | Wgt Acc: 58.832% | Dur: 10.31s
I - Confusion Matrix: [row->prediction - col->label]
[[56. 10.  6. 18.]
 [ 1. 34. 13.  0.]
 [ 1. 23. 38.  4.]
 [30. 11. 18. 64.]]

I - Epoch: 74
I - Training: 
	I - Batch: 50 | Loss: 0.236 | Acc: 93.500% | Wgt Acc: 93.481%
	I - Batch: 100 | Loss: 0.221 | Acc: 93.750% | Wgt Acc: 93.766%
	I - Batch: 150 | Loss: 0.220 | Acc: 93.917% | Wgt Acc: 93.886%
	I - Batch: 200 | Loss: 0.228 | Acc: 93.562% | Wgt Acc: 93.527%
	I - Batch: 250 | Loss: 0.235 | Acc: 93.250% | Wgt Acc: 93.253%
	I - Batch: 300 | Loss: 0.252 | Acc: 92.625% | Wgt Acc: 92.605%
I - num batch: 319
I - Train -- Loss: 0.254 | Acc: 92.540% | Wgt Acc: 92.542% | LR: 1.250000e-04 | Dur: 122.49s
I - Confusion Matrix: [row->prediction - col->label]
[[647.   3.   4.  34.]
 [  9. 544.  36.   3.]
 [  4.  30. 677.  12.]
 [ 37.   1.  17. 489.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.277 | Acc: 56.269% | Wgt Acc: 55.299% | Dur: 9.44s
I - Confusion Matrix: [row->prediction - col->label]
[[61. 10.  7. 25.]
 [ 4. 24.  9.  2.]
 [ 4. 37. 45.  5.]
 [19.  7. 14. 54.]]

I - Epoch: 75
I - Training: 
	I - Batch: 50 | Loss: 0.223 | Acc: 93.750% | Wgt Acc: 93.599%
	I - Batch: 100 | Loss: 0.240 | Acc: 93.000% | Wgt Acc: 92.847%
	I - Batch: 150 | Loss: 0.251 | Acc: 92.583% | Wgt Acc: 92.550%
	I - Batch: 200 | Loss: 0.261 | Acc: 92.438% | Wgt Acc: 92.411%
	I - Batch: 250 | Loss: 0.262 | Acc: 92.250% | Wgt Acc: 92.194%
	I - Batch: 300 | Loss: 0.260 | Acc: 92.125% | Wgt Acc: 92.077%
I - num batch: 319
I - Train -- Loss: 0.260 | Acc: 92.030% | Wgt Acc: 91.985% | LR: 1.250000e-04 | Dur: 118.74s
I - Confusion Matrix: [row->prediction - col->label]
[[641.   4.   3.  40.]
 [  6. 539.  33.   1.]
 [  6.  34. 681.  14.]
 [ 44.   1.  17. 483.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.176 | Acc: 59.939% | Wgt Acc: 59.986% | Dur: 9.36s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  8.  8. 16.]
 [ 5. 38. 16.  3.]
 [ 3. 27. 43.  6.]
 [26.  5.  8. 61.]]

I - Epoch: 76
I - Training: 
	I - Batch: 50 | Loss: 0.245 | Acc: 93.250% | Wgt Acc: 93.292%
	I - Batch: 100 | Loss: 0.244 | Acc: 93.250% | Wgt Acc: 93.229%
	I - Batch: 150 | Loss: 0.233 | Acc: 93.417% | Wgt Acc: 93.436%
	I - Batch: 200 | Loss: 0.247 | Acc: 92.562% | Wgt Acc: 92.524%
	I - Batch: 250 | Loss: 0.253 | Acc: 92.200% | Wgt Acc: 92.150%
	I - Batch: 300 | Loss: 0.260 | Acc: 92.042% | Wgt Acc: 92.006%
I - num batch: 319
I - Train -- Loss: 0.255 | Acc: 92.226% | Wgt Acc: 92.224% | LR: 1.250000e-04 | Dur: 137.27s
I - Confusion Matrix: [row->prediction - col->label]
[[647.   3.   1.  31.]
 [  7. 542.  41.   4.]
 [  5.  32. 673.  16.]
 [ 38.   1.  19. 487.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.223 | Acc: 57.492% | Wgt Acc: 56.726% | Dur: 16.49s
I - Confusion Matrix: [row->prediction - col->label]
[[72. 10. 11. 39.]
 [ 5. 41. 23.  2.]
 [ 3. 23. 33.  3.]
 [ 8.  4.  8. 42.]]

I - Epoch: 77
I - Training: 
	I - Batch: 50 | Loss: 0.230 | Acc: 91.750% | Wgt Acc: 91.676%
	I - Batch: 100 | Loss: 0.246 | Acc: 91.125% | Wgt Acc: 91.114%
	I - Batch: 150 | Loss: 0.234 | Acc: 92.500% | Wgt Acc: 92.452%
	I - Batch: 200 | Loss: 0.247 | Acc: 92.188% | Wgt Acc: 92.117%
	I - Batch: 250 | Loss: 0.241 | Acc: 92.450% | Wgt Acc: 92.431%
	I - Batch: 300 | Loss: 0.244 | Acc: 92.583% | Wgt Acc: 92.528%
I - num batch: 319
I - Train -- Loss: 0.242 | Acc: 92.658% | Wgt Acc: 92.604% | LR: 1.250000e-04 | Dur: 185.38s
I - Confusion Matrix: [row->prediction - col->label]
[[653.   4.   3.  37.]
 [  7. 542.  36.   4.]
 [  6.  32. 679.  11.]
 [ 31.   0.  16. 486.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.246 | Acc: 57.492% | Wgt Acc: 56.454% | Dur: 16.70s
I - Confusion Matrix: [row->prediction - col->label]
[[66.  8.  7. 32.]
 [ 1. 41. 16.  7.]
 [ 2. 22. 43.  9.]
 [19.  7.  9. 38.]]

I - Epoch: 78
I - Training: 
	I - Batch: 50 | Loss: 0.225 | Acc: 93.750% | Wgt Acc: 93.636%
	I - Batch: 100 | Loss: 0.236 | Acc: 93.375% | Wgt Acc: 93.262%
	I - Batch: 150 | Loss: 0.237 | Acc: 93.000% | Wgt Acc: 92.969%
	I - Batch: 200 | Loss: 0.243 | Acc: 92.750% | Wgt Acc: 92.684%
	I - Batch: 250 | Loss: 0.239 | Acc: 93.100% | Wgt Acc: 93.016%
	I - Batch: 300 | Loss: 0.242 | Acc: 92.875% | Wgt Acc: 92.804%
I - num batch: 319
I - Train -- Loss: 0.242 | Acc: 92.894% | Wgt Acc: 92.834% | LR: 1.250000e-04 | Dur: 139.33s
I - Confusion Matrix: [row->prediction - col->label]
[[654.   4.   1.  29.]
 [  6. 544.  35.   2.]
 [  4.  27. 682.  21.]
 [ 33.   3.  16. 486.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.253 | Acc: 59.939% | Wgt Acc: 59.103% | Dur: 9.24s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  9. 10. 21.]
 [ 2. 35. 10.  1.]
 [ 9. 27. 50. 13.]
 [17.  7.  5. 51.]]

I - Epoch: 79
I - Training: 
	I - Batch: 50 | Loss: 0.278 | Acc: 91.000% | Wgt Acc: 90.715%
	I - Batch: 100 | Loss: 0.260 | Acc: 92.250% | Wgt Acc: 92.033%
	I - Batch: 150 | Loss: 0.258 | Acc: 92.333% | Wgt Acc: 92.190%
	I - Batch: 200 | Loss: 0.241 | Acc: 92.875% | Wgt Acc: 92.768%
	I - Batch: 250 | Loss: 0.235 | Acc: 92.950% | Wgt Acc: 92.836%
	I - Batch: 300 | Loss: 0.239 | Acc: 92.625% | Wgt Acc: 92.534%
I - num batch: 319
I - Train -- Loss: 0.235 | Acc: 92.854% | Wgt Acc: 92.755% | LR: 1.250000e-04 | Dur: 118.40s
I - Confusion Matrix: [row->prediction - col->label]
[[655.   6.   2.  29.]
 [  6. 538.  33.   1.]
 [  3.  30. 685.  21.]
 [ 33.   4.  14. 487.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.315 | Acc: 58.104% | Wgt Acc: 56.861% | Dur: 9.50s
I - Confusion Matrix: [row->prediction - col->label]
[[72. 10. 12. 37.]
 [ 4. 34. 16.  3.]
 [ 1. 25. 41.  3.]
 [11.  9.  6. 43.]]

I - Epoch: 80
I - Training: 
	I - Batch: 50 | Loss: 0.216 | Acc: 94.500% | Wgt Acc: 94.535%
	I - Batch: 100 | Loss: 0.231 | Acc: 93.625% | Wgt Acc: 93.595%
	I - Batch: 150 | Loss: 0.229 | Acc: 93.583% | Wgt Acc: 93.635%
	I - Batch: 200 | Loss: 0.225 | Acc: 93.562% | Wgt Acc: 93.625%
	I - Batch: 250 | Loss: 0.216 | Acc: 93.850% | Wgt Acc: 93.858%
	I - Batch: 300 | Loss: 0.215 | Acc: 93.667% | Wgt Acc: 93.683%
I - num batch: 319
I - Train -- Loss: 0.218 | Acc: 93.522% | Wgt Acc: 93.551% | LR: 1.250000e-04 | Dur: 119.13s
I - Confusion Matrix: [row->prediction - col->label]
[[650.   1.   2.  26.]
 [  7. 548.  29.   0.]
 [  7.  29. 685.  13.]
 [ 33.   0.  18. 499.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.311 | Acc: 58.410% | Wgt Acc: 57.065% | Dur: 9.69s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  8.  6. 23.]
 [ 0. 22.  8.  1.]
 [ 3. 41. 51.  8.]
 [21.  7. 10. 54.]]

I - Epoch: 81
I - Training: 
	I - Batch: 50 | Loss: 0.207 | Acc: 94.000% | Wgt Acc: 94.011%
	I - Batch: 100 | Loss: 0.212 | Acc: 94.250% | Wgt Acc: 94.231%
	I - Batch: 150 | Loss: 0.228 | Acc: 93.917% | Wgt Acc: 93.811%
	I - Batch: 200 | Loss: 0.248 | Acc: 93.188% | Wgt Acc: 93.037%
	I - Batch: 250 | Loss: 0.240 | Acc: 93.500% | Wgt Acc: 93.391%
	I - Batch: 300 | Loss: 0.237 | Acc: 93.333% | Wgt Acc: 93.258%
I - num batch: 319
I - Train -- Loss: 0.234 | Acc: 93.443% | Wgt Acc: 93.356% | LR: 1.250000e-04 | Dur: 119.55s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   4.   2.  25.]
 [  4. 542.  33.   6.]
 [  3.  30. 684.  16.]
 [ 27.   2.  15. 491.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.300 | Acc: 57.798% | Wgt Acc: 57.269% | Dur: 9.41s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  6.  8. 22.]
 [ 3. 36. 14.  4.]
 [ 6. 31. 46.  9.]
 [23.  5.  7. 51.]]

I - Epoch: 82
I - Training: 
	I - Batch: 50 | Loss: 0.175 | Acc: 96.750% | Wgt Acc: 96.732%
	I - Batch: 100 | Loss: 0.185 | Acc: 95.625% | Wgt Acc: 95.546%
	I - Batch: 150 | Loss: 0.189 | Acc: 95.750% | Wgt Acc: 95.711%
	I - Batch: 200 | Loss: 0.183 | Acc: 95.688% | Wgt Acc: 95.667%
	I - Batch: 250 | Loss: 0.184 | Acc: 95.450% | Wgt Acc: 95.449%
	I - Batch: 300 | Loss: 0.185 | Acc: 95.375% | Wgt Acc: 95.399%
I - num batch: 319
I - Train -- Loss: 0.184 | Acc: 95.406% | Wgt Acc: 95.426% | LR: 1.250000e-04 | Dur: 117.60s
I - Confusion Matrix: [row->prediction - col->label]
[[656.   4.   0.  17.]
 [  8. 554.  20.   0.]
 [  4.  20. 707.   8.]
 [ 29.   0.   7. 513.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.332 | Acc: 56.269% | Wgt Acc: 55.435% | Dur: 9.38s
I - Confusion Matrix: [row->prediction - col->label]
[[60. 15. 12. 32.]
 [ 4. 32. 12.  1.]
 [ 3. 30. 44.  5.]
 [21.  1.  7. 48.]]

I - Epoch: 83
I - Training: 
	I - Batch: 50 | Loss: 0.173 | Acc: 96.500% | Wgt Acc: 96.534%
	I - Batch: 100 | Loss: 0.164 | Acc: 96.750% | Wgt Acc: 96.744%
	I - Batch: 150 | Loss: 0.204 | Acc: 94.833% | Wgt Acc: 94.829%
	I - Batch: 200 | Loss: 0.210 | Acc: 94.438% | Wgt Acc: 94.452%
	I - Batch: 250 | Loss: 0.214 | Acc: 94.300% | Wgt Acc: 94.254%
	I - Batch: 300 | Loss: 0.213 | Acc: 94.250% | Wgt Acc: 94.237%
I - num batch: 319
I - Train -- Loss: 0.213 | Acc: 94.111% | Wgt Acc: 94.117% | LR: 1.250000e-04 | Dur: 121.33s
I - Confusion Matrix: [row->prediction - col->label]
[[652.   4.   4.  27.]
 [  7. 551.  26.   0.]
 [  6.  21. 694.  11.]
 [ 32.   2.  10. 500.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.403 | Acc: 55.352% | Wgt Acc: 53.872% | Dur: 9.51s
I - Confusion Matrix: [row->prediction - col->label]
[[73. 21. 11. 37.]
 [ 5. 27. 19.  2.]
 [ 2. 23. 39.  5.]
 [ 8.  7.  6. 42.]]

I - Epoch: 84
I - Training: 
	I - Batch: 50 | Loss: 0.224 | Acc: 93.500% | Wgt Acc: 93.284%
	I - Batch: 100 | Loss: 0.221 | Acc: 93.875% | Wgt Acc: 93.720%
	I - Batch: 150 | Loss: 0.208 | Acc: 94.500% | Wgt Acc: 94.442%
	I - Batch: 200 | Loss: 0.198 | Acc: 94.938% | Wgt Acc: 94.873%
	I - Batch: 250 | Loss: 0.192 | Acc: 95.000% | Wgt Acc: 94.928%
	I - Batch: 300 | Loss: 0.195 | Acc: 94.750% | Wgt Acc: 94.697%
I - num batch: 319
I - Train -- Loss: 0.201 | Acc: 94.660% | Wgt Acc: 94.613% | LR: 1.250000e-04 | Dur: 118.76s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   3.   2.  28.]
 [  4. 557.  19.   1.]
 [  5.  16. 700.  15.]
 [ 28.   2.  13. 494.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.388 | Acc: 57.492% | Wgt Acc: 56.726% | Dur: 9.27s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  7.  7. 22.]
 [ 3. 27. 10.  1.]
 [ 6. 38. 47.  7.]
 [21.  6. 11. 56.]]

I - Epoch: 85
I - Training: 
	I - Batch: 50 | Loss: 0.188 | Acc: 94.000% | Wgt Acc: 94.001%
	I - Batch: 100 | Loss: 0.199 | Acc: 93.625% | Wgt Acc: 93.607%
	I - Batch: 150 | Loss: 0.201 | Acc: 93.750% | Wgt Acc: 93.758%
	I - Batch: 200 | Loss: 0.202 | Acc: 93.812% | Wgt Acc: 93.805%
	I - Batch: 250 | Loss: 0.215 | Acc: 93.500% | Wgt Acc: 93.480%
	I - Batch: 300 | Loss: 0.214 | Acc: 93.458% | Wgt Acc: 93.485%
I - num batch: 319
I - Train -- Loss: 0.209 | Acc: 93.640% | Wgt Acc: 93.675% | LR: 1.250000e-04 | Dur: 120.21s
I - Confusion Matrix: [row->prediction - col->label]
[[652.   4.   4.  29.]
 [  4. 548.  33.   0.]
 [  8.  26. 684.   8.]
 [ 33.   0.  13. 501.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.446 | Acc: 57.187% | Wgt Acc: 55.774% | Dur: 9.39s
I - Confusion Matrix: [row->prediction - col->label]
[[66. 13. 10. 32.]
 [ 0. 25.  5.  0.]
 [ 2. 31. 48.  6.]
 [20.  9. 12. 48.]]

I - Epoch: 86
I - Training: 
	I - Batch: 50 | Loss: 0.171 | Acc: 96.250% | Wgt Acc: 96.180%
	I - Batch: 100 | Loss: 0.174 | Acc: 95.875% | Wgt Acc: 95.808%
	I - Batch: 150 | Loss: 0.177 | Acc: 95.917% | Wgt Acc: 95.883%
	I - Batch: 200 | Loss: 0.186 | Acc: 95.125% | Wgt Acc: 95.097%
	I - Batch: 250 | Loss: 0.190 | Acc: 95.150% | Wgt Acc: 95.138%
	I - Batch: 300 | Loss: 0.187 | Acc: 95.167% | Wgt Acc: 95.156%
I - num batch: 319
I - Train -- Loss: 0.191 | Acc: 94.974% | Wgt Acc: 94.975% | LR: 1.250000e-04 | Dur: 119.25s
I - Confusion Matrix: [row->prediction - col->label]
[[663.   4.   4.  22.]
 [  8. 556.  23.   2.]
 [  4.  15. 696.  10.]
 [ 22.   3.  11. 504.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.428 | Acc: 53.823% | Wgt Acc: 53.057% | Dur: 9.20s
I - Confusion Matrix: [row->prediction - col->label]
[[61.  7.  8. 25.]
 [ 2. 26. 18.  3.]
 [ 6. 38. 38.  7.]
 [19.  7. 11. 51.]]

I - Epoch: 87
I - Training: 
	I - Batch: 50 | Loss: 0.192 | Acc: 94.000% | Wgt Acc: 93.891%
	I - Batch: 100 | Loss: 0.194 | Acc: 94.500% | Wgt Acc: 94.474%
	I - Batch: 150 | Loss: 0.186 | Acc: 94.500% | Wgt Acc: 94.521%
	I - Batch: 200 | Loss: 0.191 | Acc: 94.562% | Wgt Acc: 94.557%
	I - Batch: 250 | Loss: 0.187 | Acc: 94.750% | Wgt Acc: 94.733%
	I - Batch: 300 | Loss: 0.186 | Acc: 94.667% | Wgt Acc: 94.637%
I - num batch: 319
I - Train -- Loss: 0.184 | Acc: 94.700% | Wgt Acc: 94.666% | LR: 1.250000e-04 | Dur: 118.05s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   5.   1.  21.]
 [  7. 552.  21.   2.]
 [  2.  20. 699.  14.]
 [ 28.   1.  13. 501.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.488 | Acc: 55.657% | Wgt Acc: 53.804% | Dur: 9.28s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  5.  8. 25.]
 [ 3. 16.  6.  3.]
 [ 5. 53. 54. 10.]
 [16.  4.  7. 48.]]

I - Epoch: 88
I - Training: 
	I - Batch: 50 | Loss: 0.171 | Acc: 95.500% | Wgt Acc: 95.485%
	I - Batch: 100 | Loss: 0.167 | Acc: 95.250% | Wgt Acc: 95.313%
	I - Batch: 150 | Loss: 0.175 | Acc: 94.833% | Wgt Acc: 94.893%
	I - Batch: 200 | Loss: 0.174 | Acc: 95.125% | Wgt Acc: 95.128%
	I - Batch: 250 | Loss: 0.169 | Acc: 95.300% | Wgt Acc: 95.304%
	I - Batch: 300 | Loss: 0.176 | Acc: 95.042% | Wgt Acc: 95.068%
I - num batch: 319
I - Train -- Loss: 0.178 | Acc: 94.974% | Wgt Acc: 95.002% | LR: 1.250000e-04 | Dur: 118.73s
I - Confusion Matrix: [row->prediction - col->label]
[[653.   2.   5.  26.]
 [ 10. 558.  16.   1.]
 [  2.  17. 703.   6.]
 [ 32.   1.  10. 505.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.274 | Acc: 59.021% | Wgt Acc: 58.628% | Dur: 9.19s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  5.  6. 18.]
 [ 2. 31. 13.  4.]
 [ 5. 35. 46.  4.]
 [25.  7. 10. 60.]]

I - Epoch: 89
I - Training: 
	I - Batch: 50 | Loss: 0.147 | Acc: 96.000% | Wgt Acc: 95.930%
	I - Batch: 100 | Loss: 0.169 | Acc: 95.375% | Wgt Acc: 95.241%
	I - Batch: 150 | Loss: 0.177 | Acc: 95.000% | Wgt Acc: 94.915%
	I - Batch: 200 | Loss: 0.184 | Acc: 94.938% | Wgt Acc: 94.913%
	I - Batch: 250 | Loss: 0.182 | Acc: 95.150% | Wgt Acc: 95.120%
	I - Batch: 300 | Loss: 0.178 | Acc: 95.333% | Wgt Acc: 95.313%
I - num batch: 319
I - Train -- Loss: 0.177 | Acc: 95.446% | Wgt Acc: 95.418% | LR: 1.250000e-04 | Dur: 144.24s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   4.   0.  19.]
 [  5. 555.  18.   0.]
 [  4.  18. 705.  12.]
 [ 24.   1.  11. 507.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.216 | Acc: 63.609% | Wgt Acc: 62.636% | Dur: 15.94s
I - Confusion Matrix: [row->prediction - col->label]
[[67. 10. 10. 21.]
 [ 1. 34.  5.  2.]
 [ 2. 28. 51.  7.]
 [18.  6.  9. 56.]]

I - Local maximum validation set accuracy:  63.61

I - Validation set results: 
[14-1-2-5.28][50-3-2-5.00][124-2-2-4.47][127-0-0-6.33][443-2-2-5.29][567-0-0-6.38][573-1-1-6.51][615-0-3-5.84][695-1-3-3.99][722-3-3-6.18]
[826-0-0-6.04][878-0-0-6.38][1103-0-0-6.25][1212-3-3-5.54][1368-0-0-6.30][2181-2-3-5.20][2476-2-2-5.79][2721-2-2-5.93][2818-1-0-6.30][2886-2-2-5.48]
[3231-2-2-5.78][3333-2-2-4.51][3482-2-0-4.71][3536-3-3-5.69][3625-1-1-6.48][3909-0-0-5.66][4035-0-0-5.23][4140-0-3-5.01][4214-1-3-5.96][4346-1-3-6.18]
[4581-2-2-3.91][4708-3-2-5.63][4838-3-0-5.22][4845-1-2-5.55][4868-0-0-6.30][4939-0-3-5.18][4984-2-2-5.74][5078-1-0-2.72][5396-0-0-6.46][5479-1-1-5.58]
[5717-0-0-6.08][5843-1-2-5.20][5949-3-3-6.00][5987-2-2-5.72][6014-3-1-5.71][6033-3-2-5.43][6313-0-3-5.42][6421-3-3-6.11][6500-1-2-5.75][6583-3-3-5.52]
[6683-3-3-5.99][6825-2-0-5.99][6998-3-1-4.05][7049-3-3-5.95][7517-1-1-6.31][7521-1-0-5.15][7528-1-3-6.18][7949-1-2-5.31][8135-1-0-6.30][8185-3-0-6.31]
[8269-3-3-4.36][8273-3-3-6.18][8543-3-3-6.00][8666-1-1-6.53][8672-0-3-6.16][8903-1-2-5.93][9001-2-0-6.07][9036-2-2-5.76][9281-3-0-3.68][9300-2-2-5.75]
[9571-0-3-5.62][9617-1-1-5.62][9644-2-2-5.82][9705-2-0-5.15][9801-0-3-6.08][9803-3-0-5.59][9865-3-3-6.07][9896-2-2-4.99][10314-1-1-6.34][10337-3-3-6.15]
[10403-0-0-4.74][10653-2-2-3.91][10704-2-1-6.49][10719-1-1-4.88][10727-1-2-4.41][10836-0-0-6.35][10969-2-0-5.45][11042-0-0-6.35][11088-1-1-6.49][11322-0-0-6.31]
[11398-2-2-5.76][11499-0-0-4.87][11502-3-3-5.99][11512-3-2-5.09][11608-1-2-4.65][11610-0-0-5.55][11692-0-0-5.50][11905-0-0-6.43][11993-1-1-5.61][12002-2-2-3.99]
[12052-0-0-6.30][12201-0-0-5.99][12235-2-2-3.52][12320-1-0-4.51][12377-2-2-4.94][12398-2-1-4.00][12503-1-1-6.48][12617-0-2-3.63][12685-3-3-3.81][12738-2-0-5.24]
[12742-2-2-5.93][12823-0-3-4.99][13110-1-2-5.84][13240-3-0-6.36][13253-1-2-5.77][13273-0-0-6.30][13634-1-3-3.58][13763-2-2-5.77][13905-3-0-4.96][14060-2-1-6.49]
[14065-3-3-5.82][14147-3-3-5.23][14595-2-2-5.80][14687-2-2-5.80][14788-2-2-5.72][14869-1-1-5.51][14872-3-3-4.85][14877-1-1-5.17][14927-0-3-5.76][15066-0-0-6.43]
[15175-1-1-4.06][15178-2-2-4.53][15375-3-0-2.67][15389-3-3-5.94][15568-2-1-6.38][15675-3-3-2.69][15869-1-3-3.95][16207-3-0-4.23][16236-0-0-4.26][16302-3-0-5.90]
[16331-2-2-5.12][16381-0-3-4.93][16488-1-1-6.49][16495-0-0-6.30][16650-0-3-4.93][16719-1-2-5.93][16801-0-0-6.34][16828-0-0-6.30][17137-3-3-6.10][17245-1-2-4.34]
[17278-3-0-4.05][17282-0-0-4.40][17311-2-3-4.39][17336-2-2-5.70][17608-3-3-6.18][17627-0-0-5.56][17877-3-0-5.19][17924-1-2-4.83][17984-3-0-6.54][18211-0-3-5.12]
[18276-3-0-5.50][18287-1-1-5.13][18394-0-0-6.30][18428-0-0-6.30][18442-0-3-6.16][18478-3-3-4.88][18607-0-3-5.34][18616-0-1-6.03][18663-0-0-4.90][18718-0-0-6.30]
[18766-2-2-5.66][18824-2-2-5.13][18890-3-3-6.01][18930-3-0-3.80][18938-3-3-5.80][19817-1-2-5.47][19839-0-0-6.27][19930-3-3-4.53][19944-0-0-6.30][20036-2-2-5.79]
[20101-3-3-5.42][20474-1-2-4.66][20547-3-3-3.97][20929-2-2-5.79][21245-1-1-6.34][21257-3-3-5.08][21293-1-1-6.48][21316-1-1-6.49][21384-1-1-5.99][21448-1-1-5.92]
[21483-0-0-6.30][21487-2-2-5.80][21714-0-0-4.35][21943-3-3-6.10][21947-0-0-6.30][21948-0-0-6.30][21965-2-2-4.97][21998-1-1-6.46][22025-0-0-4.62][22228-3-3-5.67]
[22446-1-2-5.93][22494-3-0-5.68][22757-0-0-6.30][22811-3-3-5.99][22976-3-2-5.07][22985-3-3-6.18][23014-0-0-6.43][23112-1-2-5.93][23144-3-3-6.18][23168-2-3-5.06]
[23219-0-0-5.35][23363-3-3-5.64][23470-0-0-4.90][23486-2-3-6.13][23497-0-3-6.16][23516-0-0-6.29][23690-1-1-5.84][23921-2-2-4.56][23936-1-0-3.82][24040-3-0-2.57]
[24111-1-0-3.25][24182-0-0-5.34][24238-3-3-6.03][24290-2-0-5.79][24345-0-0-6.07][24364-1-2-5.90][24427-3-3-5.80][24477-2-2-5.80][24495-2-3-2.99][24893-2-2-3.01]
[25012-1-2-5.82][25121-2-2-5.30][25165-3-3-3.62][25183-0-0-5.98][25297-3-3-5.97][25398-0-0-6.33][25574-2-1-5.47][25644-1-1-5.65][25718-1-1-4.78][25774-2-2-5.24]
[26032-3-3-5.78][26051-3-3-6.18][26120-0-0-4.11][26321-1-1-6.49][26732-1-1-5.97][26784-3-3-6.18][26827-3-3-6.18][26833-0-3-6.18][26838-2-2-5.82][26860-1-1-3.40]
[26948-0-0-5.09][27049-3-0-6.39][27098-1-0-4.46][27526-0-0-6.33][27639-3-3-6.16][27698-3-3-6.18][27772-0-0-6.37][27890-1-1-5.69][28040-0-0-4.81][28503-2-2-5.86]
[28577-1-2-5.44][28959-0-0-5.46][29198-3-2-4.50][29777-0-0-6.36][29877-2-2-5.69][30035-1-1-6.49][30098-0-0-6.31][30326-1-1-6.49][30572-2-2-5.91][30716-0-2-4.93]
[30806-2-2-4.71][30906-1-1-4.23][31007-0-0-3.15][31181-3-3-6.18][31238-0-3-6.02][31347-0-0-5.58][31422-2-0-4.81][31429-3-0-4.14][31431-0-0-6.29][31432-1-1-6.50]
[31477-0-0-6.51][31524-1-2-5.24][31597-1-2-4.99][31619-1-0-5.99][31701-0-0-6.35][31755-0-0-6.30][31854-3-3-5.41][32074-1-2-4.77][32078-3-3-6.18][32111-1-2-4.67]
[32127-1-2-5.74][32140-3-3-5.91][32263-2-3-5.16][32365-0-0-6.29][32411-2-3-6.01][32429-3-0-4.97][32473-3-0-5.31][32574-3-3-6.18][32584-0-0-4.00][32622-0-0-3.36]
[32858-3-0-4.98][32969-3-3-5.72][33016-2-2-5.79][33031-1-0-3.48][33035-2-2-5.90][33133-2-2-5.81][33173-2-0-1.97][33175-3-2-3.55][33306-3-3-6.17][33309-2-3-4.10]
[33474-0-0-5.11][33478-2-0-6.32][33618-1-1-6.23][33712-0-3-6.18][33782-2-2-5.81][33914-3-3-6.15][34076-3-3-6.16][34112-2-2-5.93][34138-2-3-5.44][34239-1-2-5.37]
[34364-2-2-5.90][34617-1-2-4.44][34751-3-3-3.97][34783-2-2-5.35][35015-3-3-6.14][35018-1-2-5.65][35288-2-2-5.69]
---------------------------
I - Epoch: 90
I - Training: 
	I - Batch: 50 | Loss: 0.155 | Acc: 95.250% | Wgt Acc: 95.195%
	I - Batch: 100 | Loss: 0.150 | Acc: 95.250% | Wgt Acc: 95.230%
	I - Batch: 150 | Loss: 0.159 | Acc: 95.083% | Wgt Acc: 95.028%
	I - Batch: 200 | Loss: 0.152 | Acc: 95.438% | Wgt Acc: 95.382%
	I - Batch: 250 | Loss: 0.150 | Acc: 95.550% | Wgt Acc: 95.517%
	I - Batch: 300 | Loss: 0.153 | Acc: 95.625% | Wgt Acc: 95.597%
I - num batch: 319
I - Train -- Loss: 0.156 | Acc: 95.563% | Wgt Acc: 95.533% | LR: 1.250000e-04 | Dur: 185.18s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   2.   4.  24.]
 [  1. 561.  17.   2.]
 [  1.  14. 706.  10.]
 [ 30.   1.   7. 502.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.442 | Acc: 57.798% | Wgt Acc: 56.182% | Dur: 16.96s
I - Confusion Matrix: [row->prediction - col->label]
[[70.  9. 10. 31.]
 [ 2. 27. 10.  6.]
 [ 6. 37. 48.  5.]
 [10.  5.  7. 44.]]

I - Epoch: 91
I - Training: 
	I - Batch: 50 | Loss: 0.161 | Acc: 95.750% | Wgt Acc: 95.677%
	I - Batch: 100 | Loss: 0.163 | Acc: 95.875% | Wgt Acc: 95.831%
	I - Batch: 150 | Loss: 0.162 | Acc: 95.750% | Wgt Acc: 95.719%
	I - Batch: 200 | Loss: 0.168 | Acc: 95.312% | Wgt Acc: 95.249%
	I - Batch: 250 | Loss: 0.166 | Acc: 95.350% | Wgt Acc: 95.301%
	I - Batch: 300 | Loss: 0.172 | Acc: 95.125% | Wgt Acc: 95.068%
I - num batch: 319
I - Train -- Loss: 0.172 | Acc: 95.053% | Wgt Acc: 95.002% | LR: 1.250000e-04 | Dur: 184.63s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   2.   0.  23.]
 [  6. 553.  16.   4.]
 [  5.  23. 702.   9.]
 [ 22.   0.  16. 502.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.397 | Acc: 57.492% | Wgt Acc: 56.861% | Dur: 16.84s
I - Confusion Matrix: [row->prediction - col->label]
[[64. 11. 10. 22.]
 [ 4. 25. 13.  1.]
 [ 1. 33. 39.  3.]
 [19.  9. 13. 60.]]

I - Epoch: 92
I - Training: 
	I - Batch: 50 | Loss: 0.175 | Acc: 95.000% | Wgt Acc: 94.898%
	I - Batch: 100 | Loss: 0.177 | Acc: 94.750% | Wgt Acc: 94.762%
	I - Batch: 150 | Loss: 0.176 | Acc: 94.833% | Wgt Acc: 94.854%
	I - Batch: 200 | Loss: 0.173 | Acc: 94.938% | Wgt Acc: 94.937%
	I - Batch: 250 | Loss: 0.171 | Acc: 95.050% | Wgt Acc: 94.999%
	I - Batch: 300 | Loss: 0.160 | Acc: 95.458% | Wgt Acc: 95.445%
I - num batch: 319
I - Train -- Loss: 0.157 | Acc: 95.642% | Wgt Acc: 95.621% | LR: 1.250000e-04 | Dur: 185.68s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   4.   1.  20.]
 [  5. 558.  20.   0.]
 [  1.  15. 702.  11.]
 [ 22.   1.  11. 507.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.399 | Acc: 61.774% | Wgt Acc: 61.481% | Dur: 16.33s
I - Confusion Matrix: [row->prediction - col->label]
[[53.  8.  7.  9.]
 [ 0. 29.  8.  1.]
 [ 7. 32. 52.  8.]
 [28.  9.  8. 68.]]

I - Epoch: 93
I - Training: 
	I - Batch: 50 | Loss: 0.135 | Acc: 96.750% | Wgt Acc: 96.635%
	I - Batch: 100 | Loss: 0.157 | Acc: 95.500% | Wgt Acc: 95.360%
	I - Batch: 150 | Loss: 0.148 | Acc: 95.750% | Wgt Acc: 95.670%
	I - Batch: 200 | Loss: 0.152 | Acc: 95.688% | Wgt Acc: 95.631%
	I - Batch: 250 | Loss: 0.166 | Acc: 95.250% | Wgt Acc: 95.175%
	I - Batch: 300 | Loss: 0.165 | Acc: 95.375% | Wgt Acc: 95.325%
I - num batch: 319
I - Train -- Loss: 0.166 | Acc: 95.289% | Wgt Acc: 95.258% | LR: 1.250000e-04 | Dur: 184.86s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   3.   3.  23.]
 [  9. 558.  14.   1.]
 [  3.  16. 707.  12.]
 [ 25.   1.  10. 502.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.470 | Acc: 55.963% | Wgt Acc: 55.299% | Dur: 15.67s
I - Confusion Matrix: [row->prediction - col->label]
[[54.  5.  5. 14.]
 [ 4. 23. 12.  2.]
 [ 7. 40. 47. 11.]
 [23. 10. 11. 59.]]

I - Epoch: 94
I - Training: 
	I - Batch: 50 | Loss: 0.158 | Acc: 95.750% | Wgt Acc: 95.747%
	I - Batch: 100 | Loss: 0.152 | Acc: 95.500% | Wgt Acc: 95.405%
	I - Batch: 150 | Loss: 0.139 | Acc: 96.000% | Wgt Acc: 95.861%
	I - Batch: 200 | Loss: 0.161 | Acc: 95.312% | Wgt Acc: 95.252%
	I - Batch: 250 | Loss: 0.163 | Acc: 95.300% | Wgt Acc: 95.282%
	I - Batch: 300 | Loss: 0.168 | Acc: 95.125% | Wgt Acc: 95.091%
I - num batch: 319
I - Train -- Loss: 0.168 | Acc: 95.014% | Wgt Acc: 94.949% | LR: 1.250000e-04 | Dur: 185.62s
I - Confusion Matrix: [row->prediction - col->label]
[[669.   4.   1.  25.]
 [  7. 555.  23.   1.]
 [  3.  17. 698.  14.]
 [ 18.   2.  12. 498.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.365 | Acc: 59.633% | Wgt Acc: 59.239% | Dur: 16.30s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  9. 10. 20.]
 [ 0. 31. 15.  1.]
 [ 1. 27. 39.  4.]
 [23. 11. 11. 61.]]

I - Epoch: 95
I - Training: 
	I - Batch: 50 | Loss: 0.190 | Acc: 93.000% | Wgt Acc: 93.061%
	I - Batch: 100 | Loss: 0.171 | Acc: 93.750% | Wgt Acc: 93.746%
	I - Batch: 150 | Loss: 0.159 | Acc: 94.583% | Wgt Acc: 94.558%
	I - Batch: 200 | Loss: 0.153 | Acc: 95.125% | Wgt Acc: 95.089%
	I - Batch: 250 | Loss: 0.149 | Acc: 95.400% | Wgt Acc: 95.403%
	I - Batch: 300 | Loss: 0.146 | Acc: 95.583% | Wgt Acc: 95.577%
I - num batch: 319
I - Train -- Loss: 0.141 | Acc: 95.799% | Wgt Acc: 95.798% | LR: 1.250000e-04 | Dur: 187.49s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   1.   1.  20.]
 [  4. 562.  19.   0.]
 [  5.  14. 706.  11.]
 [ 23.   1.   8. 507.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.465 | Acc: 55.046% | Wgt Acc: 54.144% | Dur: 16.37s
I - Confusion Matrix: [row->prediction - col->label]
[[55. 12. 10. 19.]
 [ 7. 27. 12.  5.]
 [ 5. 37. 48. 12.]
 [21.  2.  5. 50.]]

I - Epoch: 96
I - Training: 
	I - Batch: 50 | Loss: 0.141 | Acc: 95.500% | Wgt Acc: 95.467%
	I - Batch: 100 | Loss: 0.133 | Acc: 96.000% | Wgt Acc: 96.033%
	I - Batch: 150 | Loss: 0.143 | Acc: 95.417% | Wgt Acc: 95.460%
	I - Batch: 200 | Loss: 0.139 | Acc: 95.750% | Wgt Acc: 95.804%
	I - Batch: 250 | Loss: 0.136 | Acc: 96.000% | Wgt Acc: 96.023%
	I - Batch: 300 | Loss: 0.136 | Acc: 96.167% | Wgt Acc: 96.191%
I - num batch: 319
I - Train -- Loss: 0.136 | Acc: 96.231% | Wgt Acc: 96.231% | LR: 1.250000e-04 | Dur: 184.74s
I - Confusion Matrix: [row->prediction - col->label]
[[664.   4.   2.  20.]
 [  4. 563.  13.   1.]
 [  3.  10. 713.   6.]
 [ 26.   1.   6. 511.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.417 | Acc: 59.939% | Wgt Acc: 58.764% | Dur: 16.05s
I - Confusion Matrix: [row->prediction - col->label]
[[73.  8. 10. 32.]
 [ 4. 41. 16.  6.]
 [ 1. 25. 42.  8.]
 [10.  4.  7. 40.]]

I - Epoch: 97
I - Training: 
	I - Batch: 50 | Loss: 0.132 | Acc: 96.500% | Wgt Acc: 96.413%
	I - Batch: 100 | Loss: 0.147 | Acc: 96.125% | Wgt Acc: 96.134%
	I - Batch: 150 | Loss: 0.133 | Acc: 96.500% | Wgt Acc: 96.499%
	I - Batch: 200 | Loss: 0.127 | Acc: 96.562% | Wgt Acc: 96.555%
	I - Batch: 250 | Loss: 0.130 | Acc: 96.550% | Wgt Acc: 96.557%
	I - Batch: 300 | Loss: 0.129 | Acc: 96.625% | Wgt Acc: 96.623%
I - num batch: 319
I - Train -- Loss: 0.130 | Acc: 96.702% | Wgt Acc: 96.691% | LR: 1.250000e-04 | Dur: 185.20s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   5.   0.  17.]
 [  5. 564.  10.   0.]
 [  2.   8. 713.   7.]
 [ 18.   1.  11. 514.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.435 | Acc: 56.881% | Wgt Acc: 56.522% | Dur: 16.97s
I - Confusion Matrix: [row->prediction - col->label]
[[60. 11. 14. 27.]
 [ 3. 34. 13.  2.]
 [ 3. 24. 38.  3.]
 [22.  9. 10. 54.]]

I - Epoch: 98
I - Training: 
	I - Batch: 50 | Loss: 0.148 | Acc: 95.500% | Wgt Acc: 95.532%
	I - Batch: 100 | Loss: 0.126 | Acc: 96.500% | Wgt Acc: 96.490%
	I - Batch: 150 | Loss: 0.119 | Acc: 97.167% | Wgt Acc: 97.118%
	I - Batch: 200 | Loss: 0.116 | Acc: 97.375% | Wgt Acc: 97.329%
	I - Batch: 250 | Loss: 0.121 | Acc: 97.100% | Wgt Acc: 97.077%
	I - Batch: 300 | Loss: 0.124 | Acc: 97.042% | Wgt Acc: 97.012%
I - num batch: 319
I - Train -- Loss: 0.126 | Acc: 96.898% | Wgt Acc: 96.860% | LR: 1.250000e-04 | Dur: 185.61s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   5.   1.  11.]
 [  4. 561.  12.   0.]
 [  1.  10. 715.  11.]
 [ 16.   2.   6. 516.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.519 | Acc: 56.269% | Wgt Acc: 56.318% | Dur: 16.57s
I - Confusion Matrix: [row->prediction - col->label]
[[42.  5.  6. 11.]
 [ 7. 23.  8.  1.]
 [ 3. 36. 49.  4.]
 [36. 14. 12. 70.]]

I - Epoch: 99
I - Training: 
	I - Batch: 50 | Loss: 0.120 | Acc: 96.250% | Wgt Acc: 96.402%
	I - Batch: 100 | Loss: 0.131 | Acc: 96.000% | Wgt Acc: 96.094%
	I - Batch: 150 | Loss: 0.131 | Acc: 96.417% | Wgt Acc: 96.475%
	I - Batch: 200 | Loss: 0.133 | Acc: 96.125% | Wgt Acc: 96.145%
	I - Batch: 250 | Loss: 0.141 | Acc: 95.900% | Wgt Acc: 95.931%
	I - Batch: 300 | Loss: 0.135 | Acc: 96.125% | Wgt Acc: 96.158%
I - num batch: 319
I - Train -- Loss: 0.135 | Acc: 96.074% | Wgt Acc: 96.116% | LR: 1.250000e-04 | Dur: 184.98s
I - Confusion Matrix: [row->prediction - col->label]
[[660.   7.   1.  17.]
 [ 11. 561.  13.   0.]
 [  2.  10. 710.   5.]
 [ 24.   0.  10. 516.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.361 | Acc: 59.327% | Wgt Acc: 59.035% | Dur: 17.19s
I - Confusion Matrix: [row->prediction - col->label]
[[62.  6. 10. 24.]
 [ 5. 39. 18.  3.]
 [ 6. 29. 39.  5.]
 [15.  4.  8. 54.]]

I - Epoch: 100
I - Training: 
	I - Batch: 50 | Loss: 0.117 | Acc: 97.000% | Wgt Acc: 96.983%
	I - Batch: 100 | Loss: 0.112 | Acc: 97.625% | Wgt Acc: 97.627%
	I - Batch: 150 | Loss: 0.107 | Acc: 97.833% | Wgt Acc: 97.856%
	I - Batch: 200 | Loss: 0.130 | Acc: 96.625% | Wgt Acc: 96.620%
	I - Batch: 250 | Loss: 0.139 | Acc: 96.250% | Wgt Acc: 96.263%
	I - Batch: 300 | Loss: 0.142 | Acc: 96.042% | Wgt Acc: 96.049%
I - num batch: 319
I - Train -- Loss: 0.142 | Acc: 96.074% | Wgt Acc: 96.081% | LR: 1.250000e-04 | Dur: 185.79s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   2.   1.  15.]
 [  3. 561.  16.   1.]
 [  2.  13. 707.  10.]
 [ 25.   2.  10. 512.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.400 | Acc: 58.716% | Wgt Acc: 58.288% | Dur: 16.02s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  5.  8. 13.]
 [ 2. 25.  9.  0.]
 [ 4. 38. 47.  8.]
 [27. 10. 11. 65.]]

I - Epoch: 101
I - Training: 
	I - Batch: 50 | Loss: 0.131 | Acc: 96.000% | Wgt Acc: 95.955%
	I - Batch: 100 | Loss: 0.127 | Acc: 96.500% | Wgt Acc: 96.492%
	I - Batch: 150 | Loss: 0.116 | Acc: 96.833% | Wgt Acc: 96.874%
	I - Batch: 200 | Loss: 0.115 | Acc: 97.000% | Wgt Acc: 97.009%
	I - Batch: 250 | Loss: 0.120 | Acc: 96.750% | Wgt Acc: 96.740%
	I - Batch: 300 | Loss: 0.122 | Acc: 96.667% | Wgt Acc: 96.664%
I - num batch: 319
I - Train -- Loss: 0.120 | Acc: 96.781% | Wgt Acc: 96.780% | LR: 1.250000e-04 | Dur: 186.96s
I - Confusion Matrix: [row->prediction - col->label]
[[665.   1.   0.  20.]
 [  4. 568.   7.   0.]
 [  3.   8. 720.   6.]
 [ 25.   1.   7. 512.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.521 | Acc: 56.269% | Wgt Acc: 54.552% | Dur: 16.56s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  9.  7. 29.]
 [ 6. 31.  9.  7.]
 [ 4. 37. 53. 14.]
 [14.  1.  6. 36.]]

I - Epoch: 102
I - Training: 
	I - Batch: 50 | Loss: 0.085 | Acc: 98.250% | Wgt Acc: 98.308%
	I - Batch: 100 | Loss: 0.088 | Acc: 98.250% | Wgt Acc: 98.252%
	I - Batch: 150 | Loss: 0.106 | Acc: 97.500% | Wgt Acc: 97.465%
	I - Batch: 200 | Loss: 0.111 | Acc: 97.312% | Wgt Acc: 97.271%
	I - Batch: 250 | Loss: 0.118 | Acc: 97.150% | Wgt Acc: 97.141%
	I - Batch: 300 | Loss: 0.132 | Acc: 96.542% | Wgt Acc: 96.508%
I - num batch: 319
I - Train -- Loss: 0.131 | Acc: 96.545% | Wgt Acc: 96.506% | LR: 1.250000e-04 | Dur: 185.97s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   3.   0.  17.]
 [  2. 562.  13.   3.]
 [  3.  11. 716.   7.]
 [ 22.   2.   5. 511.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.572 | Acc: 57.798% | Wgt Acc: 56.454% | Dur: 16.26s
I - Confusion Matrix: [row->prediction - col->label]
[[71. 12. 11. 31.]
 [ 2. 24.  9.  0.]
 [ 2. 28. 43.  4.]
 [13. 14. 12. 51.]]

I - Epoch: 103
I - Training: 
	I - Batch: 50 | Loss: 0.130 | Acc: 96.250% | Wgt Acc: 96.013%
	I - Batch: 100 | Loss: 0.135 | Acc: 96.500% | Wgt Acc: 96.303%
	I - Batch: 150 | Loss: 0.134 | Acc: 96.250% | Wgt Acc: 96.152%
	I - Batch: 200 | Loss: 0.138 | Acc: 96.062% | Wgt Acc: 96.007%
	I - Batch: 250 | Loss: 0.136 | Acc: 96.100% | Wgt Acc: 96.046%
	I - Batch: 300 | Loss: 0.136 | Acc: 96.042% | Wgt Acc: 95.972%
I - num batch: 319
I - Train -- Loss: 0.134 | Acc: 96.152% | Wgt Acc: 96.099% | LR: 1.250000e-04 | Dur: 186.70s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   4.   1.  15.]
 [  7. 555.  15.   1.]
 [  2.  17. 710.  10.]
 [ 16.   2.   8. 512.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.398 | Acc: 61.468% | Wgt Acc: 60.734% | Dur: 16.94s
I - Confusion Matrix: [row->prediction - col->label]
[[68. 10.  7. 16.]
 [ 1. 28. 14.  1.]
 [ 1. 30. 43.  7.]
 [18. 10. 11. 62.]]

I - Epoch: 104
I - Training: 
	I - Batch: 50 | Loss: 0.141 | Acc: 96.000% | Wgt Acc: 95.883%
	I - Batch: 100 | Loss: 0.130 | Acc: 96.625% | Wgt Acc: 96.632%
	I - Batch: 150 | Loss: 0.126 | Acc: 96.750% | Wgt Acc: 96.770%
	I - Batch: 200 | Loss: 0.121 | Acc: 96.812% | Wgt Acc: 96.821%
	I - Batch: 250 | Loss: 0.111 | Acc: 97.150% | Wgt Acc: 97.158%
	I - Batch: 300 | Loss: 0.111 | Acc: 97.125% | Wgt Acc: 97.126%
I - num batch: 319
I - Train -- Loss: 0.112 | Acc: 97.095% | Wgt Acc: 97.098% | LR: 1.250000e-04 | Dur: 154.94s
I - Confusion Matrix: [row->prediction - col->label]
[[670.   2.   2.  17.]
 [  8. 567.  10.   0.]
 [  3.   9. 719.   4.]
 [ 16.   0.   3. 517.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.481 | Acc: 55.657% | Wgt Acc: 54.620% | Dur: 9.68s
I - Confusion Matrix: [row->prediction - col->label]
[[65. 13. 11. 31.]
 [ 3. 28. 11.  0.]
 [ 1. 33. 41.  7.]
 [19.  4. 12. 48.]]

I - Epoch: 105
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 98.000% | Wgt Acc: 97.965%
	I - Batch: 100 | Loss: 0.100 | Acc: 97.500% | Wgt Acc: 97.506%
	I - Batch: 150 | Loss: 0.105 | Acc: 97.250% | Wgt Acc: 97.253%
	I - Batch: 200 | Loss: 0.107 | Acc: 97.125% | Wgt Acc: 97.128%
	I - Batch: 250 | Loss: 0.110 | Acc: 97.150% | Wgt Acc: 97.143%
	I - Batch: 300 | Loss: 0.115 | Acc: 97.000% | Wgt Acc: 97.006%
I - num batch: 319
I - Train -- Loss: 0.116 | Acc: 96.898% | Wgt Acc: 96.913% | LR: 1.250000e-04 | Dur: 120.34s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   5.   4.  12.]
 [  4. 565.  11.   0.]
 [  1.   7. 710.   8.]
 [ 17.   1.   9. 518.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.490 | Acc: 58.104% | Wgt Acc: 57.269% | Dur: 9.26s
I - Confusion Matrix: [row->prediction - col->label]
[[58.  6.  9. 20.]
 [ 0. 27. 10.  1.]
 [ 5. 37. 49.  9.]
 [25.  8.  7. 56.]]

I - Epoch: 106
I - Training: 
	I - Batch: 50 | Loss: 0.121 | Acc: 97.000% | Wgt Acc: 96.898%
	I - Batch: 100 | Loss: 0.124 | Acc: 96.625% | Wgt Acc: 96.582%
	I - Batch: 150 | Loss: 0.114 | Acc: 97.000% | Wgt Acc: 96.996%
	I - Batch: 200 | Loss: 0.111 | Acc: 97.312% | Wgt Acc: 97.313%
	I - Batch: 250 | Loss: 0.113 | Acc: 97.000% | Wgt Acc: 97.016%
	I - Batch: 300 | Loss: 0.110 | Acc: 97.167% | Wgt Acc: 97.173%
I - num batch: 319
I - Train -- Loss: 0.113 | Acc: 97.016% | Wgt Acc: 97.036% | LR: 1.250000e-04 | Dur: 120.10s
I - Confusion Matrix: [row->prediction - col->label]
[[671.   2.   1.  13.]
 [  5. 569.   9.   1.]
 [  2.   7. 715.   8.]
 [ 19.   0.   9. 516.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.534 | Acc: 58.716% | Wgt Acc: 57.880% | Dur: 9.41s
I - Confusion Matrix: [row->prediction - col->label]
[[73. 13. 11. 29.]
 [ 2. 34. 22.  4.]
 [ 1. 24. 35.  3.]
 [12.  7.  7. 50.]]

I - Epoch: 107
I - Training: 
	I - Batch: 50 | Loss: 0.106 | Acc: 97.250% | Wgt Acc: 97.320%
	I - Batch: 100 | Loss: 0.094 | Acc: 97.750% | Wgt Acc: 97.844%
	I - Batch: 150 | Loss: 0.115 | Acc: 97.000% | Wgt Acc: 97.098%
	I - Batch: 200 | Loss: 0.114 | Acc: 97.062% | Wgt Acc: 97.132%
	I - Batch: 250 | Loss: 0.115 | Acc: 96.800% | Wgt Acc: 96.855%
	I - Batch: 300 | Loss: 0.125 | Acc: 96.292% | Wgt Acc: 96.364%
I - num batch: 319
I - Train -- Loss: 0.128 | Acc: 96.231% | Wgt Acc: 96.311% | LR: 1.250000e-04 | Dur: 119.69s
I - Confusion Matrix: [row->prediction - col->label]
[[667.   2.   3.  11.]
 [  6. 569.  17.   1.]
 [  6.   6. 701.  12.]
 [ 18.   1.  13. 514.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.462 | Acc: 55.963% | Wgt Acc: 55.910% | Dur: 9.35s
I - Confusion Matrix: [row->prediction - col->label]
[[46.  8.  5. 12.]
 [ 1. 25. 14.  2.]
 [ 1. 38. 46.  6.]
 [40.  7. 10. 66.]]

I - Epoch: 108
I - Training: 
	I - Batch: 50 | Loss: 0.171 | Acc: 94.000% | Wgt Acc: 94.104%
	I - Batch: 100 | Loss: 0.135 | Acc: 95.375% | Wgt Acc: 95.405%
	I - Batch: 150 | Loss: 0.137 | Acc: 95.583% | Wgt Acc: 95.634%
	I - Batch: 200 | Loss: 0.131 | Acc: 95.875% | Wgt Acc: 95.902%
	I - Batch: 250 | Loss: 0.130 | Acc: 96.000% | Wgt Acc: 96.013%
	I - Batch: 300 | Loss: 0.131 | Acc: 95.958% | Wgt Acc: 95.964%
I - num batch: 319
I - Train -- Loss: 0.130 | Acc: 95.995% | Wgt Acc: 95.993% | LR: 1.250000e-04 | Dur: 118.51s
I - Confusion Matrix: [row->prediction - col->label]
[[666.   3.   2.  18.]
 [  5. 563.  14.   0.]
 [  3.  11. 708.  12.]
 [ 23.   1.  10. 508.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.595 | Acc: 55.352% | Wgt Acc: 54.620% | Dur: 9.37s
I - Confusion Matrix: [row->prediction - col->label]
[[59. 12. 12. 28.]
 [ 4. 30. 15.  5.]
 [ 5. 27. 42.  3.]
 [20.  9.  6. 50.]]

I - Epoch: 109
I - Training: 
	I - Batch: 50 | Loss: 0.113 | Acc: 96.500% | Wgt Acc: 96.530%
	I - Batch: 100 | Loss: 0.107 | Acc: 97.000% | Wgt Acc: 96.967%
	I - Batch: 150 | Loss: 0.094 | Acc: 97.667% | Wgt Acc: 97.638%
	I - Batch: 200 | Loss: 0.088 | Acc: 98.000% | Wgt Acc: 97.987%
	I - Batch: 250 | Loss: 0.089 | Acc: 98.000% | Wgt Acc: 97.974%
	I - Batch: 300 | Loss: 0.092 | Acc: 97.958% | Wgt Acc: 97.933%
I - num batch: 319
I - Train -- Loss: 0.090 | Acc: 97.998% | Wgt Acc: 97.974% | LR: 1.250000e-04 | Dur: 120.75s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   5.   1.  12.]
 [  3. 566.   7.   0.]
 [  1.   7. 722.   1.]
 [ 10.   0.   4. 525.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.516 | Acc: 59.633% | Wgt Acc: 58.696% | Dur: 9.37s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  5. 10. 30.]
 [ 1. 32. 10.  2.]
 [ 5. 33. 46.  2.]
 [17.  8.  9. 52.]]

I - Epoch: 110
I - Training: 
	I - Batch: 50 | Loss: 0.116 | Acc: 97.500% | Wgt Acc: 97.531%
	I - Batch: 100 | Loss: 0.099 | Acc: 97.750% | Wgt Acc: 97.746%
	I - Batch: 150 | Loss: 0.091 | Acc: 98.167% | Wgt Acc: 98.183%
	I - Batch: 200 | Loss: 0.091 | Acc: 98.000% | Wgt Acc: 98.014%
	I - Batch: 250 | Loss: 0.093 | Acc: 97.950% | Wgt Acc: 97.937%
	I - Batch: 300 | Loss: 0.098 | Acc: 97.792% | Wgt Acc: 97.783%
I - num batch: 319
I - Train -- Loss: 0.095 | Acc: 97.919% | Wgt Acc: 97.912% | LR: 1.250000e-04 | Dur: 120.68s
I - Confusion Matrix: [row->prediction - col->label]
[[682.   0.   1.   9.]
 [  5. 570.   5.   1.]
 [  1.   8. 720.   6.]
 [  9.   0.   8. 522.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.451 | Acc: 58.410% | Wgt Acc: 57.473% | Dur: 9.22s
I - Confusion Matrix: [row->prediction - col->label]
[[60.  7.  6. 27.]
 [ 3. 32.  9.  0.]
 [ 4. 32. 49.  9.]
 [21.  7. 11. 50.]]

I - Epoch: 111
I - Training: 
	I - Batch: 50 | Loss: 0.105 | Acc: 97.250% | Wgt Acc: 97.258%
	I - Batch: 100 | Loss: 0.098 | Acc: 97.500% | Wgt Acc: 97.494%
	I - Batch: 150 | Loss: 0.104 | Acc: 97.083% | Wgt Acc: 97.094%
	I - Batch: 200 | Loss: 0.107 | Acc: 96.875% | Wgt Acc: 96.882%
	I - Batch: 250 | Loss: 0.107 | Acc: 96.950% | Wgt Acc: 96.973%
	I - Batch: 300 | Loss: 0.112 | Acc: 96.833% | Wgt Acc: 96.855%
I - num batch: 319
I - Train -- Loss: 0.110 | Acc: 96.859% | Wgt Acc: 96.895% | LR: 1.250000e-04 | Dur: 117.59s
I - Confusion Matrix: [row->prediction - col->label]
[[668.   1.   3.  17.]
 [  6. 570.   8.   0.]
 [  0.   7. 714.   6.]
 [ 23.   0.   9. 515.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.504 | Acc: 59.327% | Wgt Acc: 58.220% | Dur: 9.26s
I - Confusion Matrix: [row->prediction - col->label]
[[67. 12. 12. 33.]
 [ 2. 35. 11.  5.]
 [ 4. 28. 46.  2.]
 [15.  3.  6. 46.]]

I - Epoch: 112
I - Training: 
	I - Batch: 50 | Loss: 0.106 | Acc: 97.500% | Wgt Acc: 97.456%
	I - Batch: 100 | Loss: 0.112 | Acc: 96.625% | Wgt Acc: 96.595%
	I - Batch: 150 | Loss: 0.123 | Acc: 96.083% | Wgt Acc: 96.096%
	I - Batch: 200 | Loss: 0.114 | Acc: 96.625% | Wgt Acc: 96.608%
	I - Batch: 250 | Loss: 0.114 | Acc: 96.650% | Wgt Acc: 96.622%
	I - Batch: 300 | Loss: 0.112 | Acc: 96.708% | Wgt Acc: 96.678%
I - num batch: 319
I - Train -- Loss: 0.110 | Acc: 96.781% | Wgt Acc: 96.762% | LR: 1.250000e-04 | Dur: 117.80s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   3.   0.  15.]
 [  7. 564.  14.   0.]
 [  1.   9. 713.   9.]
 [ 15.   2.   7. 514.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.614 | Acc: 54.740% | Wgt Acc: 53.804% | Dur: 9.23s
I - Confusion Matrix: [row->prediction - col->label]
[[71. 14. 17. 42.]
 [ 2. 39. 20.  2.]
 [ 4. 19. 32.  5.]
 [11.  6.  6. 37.]]

I - Epoch: 113
I - Training: 
	I - Batch: 50 | Loss: 0.112 | Acc: 96.500% | Wgt Acc: 96.435%
	I - Batch: 100 | Loss: 0.113 | Acc: 96.500% | Wgt Acc: 96.526%
	I - Batch: 150 | Loss: 0.114 | Acc: 96.917% | Wgt Acc: 96.902%
	I - Batch: 200 | Loss: 0.108 | Acc: 97.125% | Wgt Acc: 97.125%
	I - Batch: 250 | Loss: 0.112 | Acc: 97.000% | Wgt Acc: 97.013%
	I - Batch: 300 | Loss: 0.109 | Acc: 97.125% | Wgt Acc: 97.136%
I - num batch: 319
I - Train -- Loss: 0.110 | Acc: 97.055% | Wgt Acc: 97.063% | LR: 1.250000e-04 | Dur: 118.68s
I - Confusion Matrix: [row->prediction - col->label]
[[672.   3.   3.  14.]
 [  3. 565.   8.   0.]
 [  4.  10. 716.   5.]
 [ 18.   0.   7. 519.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.667 | Acc: 55.963% | Wgt Acc: 55.027% | Dur: 9.29s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  7.  8. 17.]
 [ 8. 24.  9.  5.]
 [ 6. 41. 50. 10.]
 [19.  6.  8. 54.]]

I - Epoch: 114
I - Training: 
	I - Batch: 50 | Loss: 0.160 | Acc: 94.250% | Wgt Acc: 94.283%
	I - Batch: 100 | Loss: 0.161 | Acc: 94.375% | Wgt Acc: 94.348%
	I - Batch: 150 | Loss: 0.135 | Acc: 95.583% | Wgt Acc: 95.597%
	I - Batch: 200 | Loss: 0.129 | Acc: 95.938% | Wgt Acc: 95.905%
	I - Batch: 250 | Loss: 0.124 | Acc: 96.250% | Wgt Acc: 96.208%
	I - Batch: 300 | Loss: 0.120 | Acc: 96.292% | Wgt Acc: 96.234%
I - num batch: 319
I - Train -- Loss: 0.120 | Acc: 96.309% | Wgt Acc: 96.249% | LR: 1.250000e-04 | Dur: 118.42s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   1.   1.  15.]
 [  4. 558.  14.   1.]
 [  3.  17. 711.  12.]
 [ 16.   2.   8. 510.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.522 | Acc: 55.657% | Wgt Acc: 54.755% | Dur: 9.15s
I - Confusion Matrix: [row->prediction - col->label]
[[67. 10. 12. 36.]
 [ 7. 43. 24.  6.]
 [ 1. 23. 37.  9.]
 [13.  2.  2. 35.]]

I - Epoch: 115
I - Training: 
	I - Batch: 50 | Loss: 0.084 | Acc: 98.000% | Wgt Acc: 98.078%
	I - Batch: 100 | Loss: 0.101 | Acc: 97.250% | Wgt Acc: 97.235%
	I - Batch: 150 | Loss: 0.101 | Acc: 97.417% | Wgt Acc: 97.391%
	I - Batch: 200 | Loss: 0.093 | Acc: 97.688% | Wgt Acc: 97.649%
	I - Batch: 250 | Loss: 0.095 | Acc: 97.550% | Wgt Acc: 97.526%
	I - Batch: 300 | Loss: 0.100 | Acc: 97.375% | Wgt Acc: 97.372%
I - num batch: 319
I - Train -- Loss: 0.102 | Acc: 97.212% | Wgt Acc: 97.205% | LR: 1.250000e-04 | Dur: 117.82s
I - Confusion Matrix: [row->prediction - col->label]
[[678.   3.   2.   9.]
 [  8. 568.   9.   2.]
 [  0.   6. 714.  11.]
 [ 11.   1.   9. 516.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.596 | Acc: 55.657% | Wgt Acc: 54.280% | Dur: 9.19s
I - Confusion Matrix: [row->prediction - col->label]
[[68. 11.  9. 28.]
 [ 4. 23. 13.  5.]
 [ 2. 35. 43.  5.]
 [14.  9. 10. 48.]]

I - Epoch: 116
I - Training: 
	I - Batch: 50 | Loss: 0.103 | Acc: 96.750% | Wgt Acc: 96.731%
	I - Batch: 100 | Loss: 0.101 | Acc: 97.250% | Wgt Acc: 97.210%
	I - Batch: 150 | Loss: 0.092 | Acc: 97.667% | Wgt Acc: 97.616%
	I - Batch: 200 | Loss: 0.084 | Acc: 98.000% | Wgt Acc: 97.955%
	I - Batch: 250 | Loss: 0.078 | Acc: 98.250% | Wgt Acc: 98.220%
	I - Batch: 300 | Loss: 0.089 | Acc: 97.750% | Wgt Acc: 97.738%
I - num batch: 319
I - Train -- Loss: 0.089 | Acc: 97.762% | Wgt Acc: 97.753% | LR: 1.250000e-04 | Dur: 117.18s
I - Confusion Matrix: [row->prediction - col->label]
[[674.   1.   1.  16.]
 [  4. 572.   2.   0.]
 [  1.   5. 726.   4.]
 [ 18.   0.   5. 518.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.355 | Acc: 62.080% | Wgt Acc: 60.598% | Dur: 9.25s
I - Confusion Matrix: [row->prediction - col->label]
[[64.  6.  5. 25.]
 [ 7. 32.  5.  1.]
 [ 4. 36. 59. 12.]
 [13.  4.  6. 48.]]

I - Epoch: 117
I - Training: 
	I - Batch: 50 | Loss: 0.092 | Acc: 97.750% | Wgt Acc: 97.580%
	I - Batch: 100 | Loss: 0.118 | Acc: 96.875% | Wgt Acc: 96.761%
	I - Batch: 150 | Loss: 0.118 | Acc: 97.083% | Wgt Acc: 96.964%
	I - Batch: 200 | Loss: 0.127 | Acc: 96.875% | Wgt Acc: 96.791%
	I - Batch: 250 | Loss: 0.116 | Acc: 97.200% | Wgt Acc: 97.130%
	I - Batch: 300 | Loss: 0.116 | Acc: 97.042% | Wgt Acc: 96.968%
I - num batch: 319
I - Train -- Loss: 0.114 | Acc: 97.095% | Wgt Acc: 97.028% | LR: 1.250000e-04 | Dur: 118.32s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   5.   2.  14.]
 [  5. 563.  10.   3.]
 [  2.   8. 720.   8.]
 [ 13.   2.   2. 513.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.431 | Acc: 61.774% | Wgt Acc: 60.666% | Dur: 10.04s
I - Confusion Matrix: [row->prediction - col->label]
[[75. 15. 13. 32.]
 [ 0. 32. 13.  0.]
 [ 2. 26. 42.  1.]
 [11.  5.  7. 53.]]

I - Epoch: 118
I - Training: 
	I - Batch: 50 | Loss: 0.094 | Acc: 97.250% | Wgt Acc: 97.288%
	I - Batch: 100 | Loss: 0.090 | Acc: 97.500% | Wgt Acc: 97.488%
	I - Batch: 150 | Loss: 0.086 | Acc: 97.667% | Wgt Acc: 97.674%
	I - Batch: 200 | Loss: 0.079 | Acc: 98.000% | Wgt Acc: 97.988%
	I - Batch: 250 | Loss: 0.084 | Acc: 97.950% | Wgt Acc: 97.937%
	I - Batch: 300 | Loss: 0.083 | Acc: 97.958% | Wgt Acc: 97.945%
I - num batch: 319
I - Train -- Loss: 0.082 | Acc: 97.958% | Wgt Acc: 97.939% | LR: 1.250000e-04 | Dur: 117.91s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   2.   0.  14.]
 [  1. 569.   9.   1.]
 [  1.   7. 724.   1.]
 [ 15.   0.   1. 522.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.463 | Acc: 59.633% | Wgt Acc: 59.239% | Dur: 9.15s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  6.  6. 15.]
 [ 3. 32. 14.  2.]
 [ 4. 34. 47.  9.]
 [25.  6.  8. 60.]]

I - Epoch: 119
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 98.500% | Wgt Acc: 98.423%
	I - Batch: 100 | Loss: 0.070 | Acc: 98.500% | Wgt Acc: 98.483%
	I - Batch: 150 | Loss: 0.090 | Acc: 97.833% | Wgt Acc: 97.774%
	I - Batch: 200 | Loss: 0.096 | Acc: 97.500% | Wgt Acc: 97.473%
	I - Batch: 250 | Loss: 0.097 | Acc: 97.400% | Wgt Acc: 97.381%
	I - Batch: 300 | Loss: 0.095 | Acc: 97.292% | Wgt Acc: 97.286%
I - num batch: 319
I - Train -- Loss: 0.095 | Acc: 97.369% | Wgt Acc: 97.364% | LR: 1.250000e-04 | Dur: 118.85s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   2.   1.  11.]
 [  3. 566.  10.   2.]
 [  1.  10. 719.   5.]
 [ 18.   0.   4. 520.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.740 | Acc: 55.046% | Wgt Acc: 53.397% | Dur: 9.25s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  3.  7. 19.]
 [ 6. 21.  3.  5.]
 [11. 49. 59. 17.]
 [16.  5.  6. 45.]]

I - Epoch: 120
I - Training: 
	I - Batch: 50 | Loss: 0.136 | Acc: 96.000% | Wgt Acc: 96.018%
	I - Batch: 100 | Loss: 0.123 | Acc: 96.125% | Wgt Acc: 96.067%
	I - Batch: 150 | Loss: 0.113 | Acc: 96.417% | Wgt Acc: 96.359%
	I - Batch: 200 | Loss: 0.101 | Acc: 96.812% | Wgt Acc: 96.774%
	I - Batch: 250 | Loss: 0.106 | Acc: 96.650% | Wgt Acc: 96.610%
	I - Batch: 300 | Loss: 0.123 | Acc: 95.958% | Wgt Acc: 95.895%
I - num batch: 319
I - Train -- Loss: 0.120 | Acc: 96.074% | Wgt Acc: 96.019% | LR: 1.250000e-04 | Dur: 119.55s
I - Confusion Matrix: [row->prediction - col->label]
[[675.   4.   1.  18.]
 [  4. 556.  15.   4.]
 [  1.  17. 706.   6.]
 [ 17.   1.  12. 510.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.542 | Acc: 58.716% | Wgt Acc: 57.677% | Dur: 9.35s
I - Confusion Matrix: [row->prediction - col->label]
[[59.  6.  7. 19.]
 [ 8. 35. 13.  3.]
 [ 6. 35. 52. 18.]
 [15.  2.  3. 46.]]

I - Epoch: 121
I - Training: 
	I - Batch: 50 | Loss: 0.091 | Acc: 98.000% | Wgt Acc: 97.982%
	I - Batch: 100 | Loss: 0.093 | Acc: 97.750% | Wgt Acc: 97.752%
	I - Batch: 150 | Loss: 0.093 | Acc: 97.833% | Wgt Acc: 97.811%
	I - Batch: 200 | Loss: 0.101 | Acc: 97.438% | Wgt Acc: 97.386%
	I - Batch: 250 | Loss: 0.095 | Acc: 97.750% | Wgt Acc: 97.723%
	I - Batch: 300 | Loss: 0.099 | Acc: 97.583% | Wgt Acc: 97.569%
I - num batch: 319
I - Train -- Loss: 0.100 | Acc: 97.527% | Wgt Acc: 97.523% | LR: 1.250000e-04 | Dur: 118.68s
I - Confusion Matrix: [row->prediction - col->label]
[[677.   2.   1.  14.]
 [  4. 571.   6.   2.]
 [  6.   4. 719.   5.]
 [ 10.   1.   8. 517.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.904 | Acc: 52.905% | Wgt Acc: 50.679% | Dur: 9.42s
I - Confusion Matrix: [row->prediction - col->label]
[[77. 24. 14. 45.]
 [ 1. 17.  7.  1.]
 [ 2. 27. 42.  3.]
 [ 8. 10. 12. 37.]]

I - Epoch: 122
I - Training: 
	I - Batch: 50 | Loss: 0.117 | Acc: 95.500% | Wgt Acc: 95.308%
	I - Batch: 100 | Loss: 0.110 | Acc: 96.000% | Wgt Acc: 95.880%
	I - Batch: 150 | Loss: 0.103 | Acc: 96.417% | Wgt Acc: 96.343%
	I - Batch: 200 | Loss: 0.100 | Acc: 96.875% | Wgt Acc: 96.840%
	I - Batch: 250 | Loss: 0.099 | Acc: 97.100% | Wgt Acc: 97.034%
	I - Batch: 300 | Loss: 0.099 | Acc: 97.167% | Wgt Acc: 97.117%
I - num batch: 319
I - Train -- Loss: 0.097 | Acc: 97.212% | Wgt Acc: 97.178% | LR: 1.250000e-04 | Dur: 117.85s
I - Confusion Matrix: [row->prediction - col->label]
[[680.   4.   3.  16.]
 [  1. 568.   7.   1.]
 [  4.   5. 715.   8.]
 [ 12.   1.   9. 513.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.545 | Acc: 55.963% | Wgt Acc: 54.552% | Dur: 9.21s
I - Confusion Matrix: [row->prediction - col->label]
[[68. 10.  7. 32.]
 [ 4. 28. 16.  3.]
 [ 2. 35. 44.  8.]
 [14.  5.  8. 43.]]

I - Epoch: 123
I - Training: 
	I - Batch: 50 | Loss: 0.070 | Acc: 98.000% | Wgt Acc: 97.982%
	I - Batch: 100 | Loss: 0.084 | Acc: 97.250% | Wgt Acc: 97.287%
	I - Batch: 150 | Loss: 0.082 | Acc: 97.333% | Wgt Acc: 97.343%
	I - Batch: 200 | Loss: 0.080 | Acc: 97.500% | Wgt Acc: 97.496%
	I - Batch: 250 | Loss: 0.077 | Acc: 97.750% | Wgt Acc: 97.760%
	I - Batch: 300 | Loss: 0.076 | Acc: 97.792% | Wgt Acc: 97.813%
I - num batch: 319
I - Train -- Loss: 0.077 | Acc: 97.841% | Wgt Acc: 97.859% | LR: 1.250000e-04 | Dur: 117.97s
I - Confusion Matrix: [row->prediction - col->label]
[[681.   1.   1.  11.]
 [  3. 572.   8.   0.]
 [  1.   5. 717.   5.]
 [ 12.   0.   8. 522.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.628 | Acc: 55.963% | Wgt Acc: 54.620% | Dur: 9.38s
I - Confusion Matrix: [row->prediction - col->label]
[[67.  7. 12. 32.]
 [ 2. 29. 13.  1.]
 [ 3. 32. 44. 10.]
 [16. 10.  6. 43.]]

I - Epoch: 124
I - Training: 
	I - Batch: 50 | Loss: 0.076 | Acc: 98.750% | Wgt Acc: 98.701%
	I - Batch: 100 | Loss: 0.088 | Acc: 98.250% | Wgt Acc: 98.226%
	I - Batch: 150 | Loss: 0.079 | Acc: 98.500% | Wgt Acc: 98.499%
	I - Batch: 200 | Loss: 0.070 | Acc: 98.812% | Wgt Acc: 98.816%
	I - Batch: 250 | Loss: 0.066 | Acc: 98.850% | Wgt Acc: 98.874%
	I - Batch: 300 | Loss: 0.069 | Acc: 98.750% | Wgt Acc: 98.761%
I - num batch: 319
I - Train -- Loss: 0.068 | Acc: 98.783% | Wgt Acc: 98.797% | LR: 1.250000e-04 | Dur: 118.54s
I - Confusion Matrix: [row->prediction - col->label]
[[685.   0.   0.   7.]
 [  3. 575.   2.   0.]
 [  0.   3. 727.   2.]
 [  9.   0.   5. 529.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.438 | Acc: 59.633% | Wgt Acc: 58.560% | Dur: 10.12s
I - Confusion Matrix: [row->prediction - col->label]
[[65. 10.  6. 23.]
 [ 2. 27. 11.  2.]
 [ 2. 33. 48.  6.]
 [19.  8. 10. 55.]]

I - Epoch: 125
I - Training: 
	I - Batch: 50 | Loss: 0.083 | Acc: 97.750% | Wgt Acc: 97.678%
	I - Batch: 100 | Loss: 0.091 | Acc: 97.750% | Wgt Acc: 97.726%
	I - Batch: 150 | Loss: 0.080 | Acc: 98.250% | Wgt Acc: 98.220%
	I - Batch: 200 | Loss: 0.084 | Acc: 98.000% | Wgt Acc: 97.976%
	I - Batch: 250 | Loss: 0.078 | Acc: 98.200% | Wgt Acc: 98.177%
	I - Batch: 300 | Loss: 0.078 | Acc: 98.250% | Wgt Acc: 98.218%
I - num batch: 319
I - Train -- Loss: 0.077 | Acc: 98.233% | Wgt Acc: 98.213% | LR: 1.250000e-04 | Dur: 118.49s
I - Confusion Matrix: [row->prediction - col->label]
[[686.   2.   0.  11.]
 [  1. 569.   8.   1.]
 [  2.   6. 722.   1.]
 [  8.   1.   4. 525.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.510 | Acc: 57.798% | Wgt Acc: 56.929% | Dur: 9.16s
I - Confusion Matrix: [row->prediction - col->label]
[[65.  9.  7. 22.]
 [ 4. 29. 20.  4.]
 [ 2. 33. 42.  7.]
 [17.  7.  6. 53.]]

I - Epoch: 126
I - Training: 
	I - Batch: 50 | Loss: 0.076 | Acc: 98.250% | Wgt Acc: 98.237%
	I - Batch: 100 | Loss: 0.067 | Acc: 98.875% | Wgt Acc: 98.897%
	I - Batch: 150 | Loss: 0.071 | Acc: 98.667% | Wgt Acc: 98.682%
	I - Batch: 200 | Loss: 0.072 | Acc: 98.562% | Wgt Acc: 98.557%
	I - Batch: 250 | Loss: 0.073 | Acc: 98.450% | Wgt Acc: 98.443%
	I - Batch: 300 | Loss: 0.074 | Acc: 98.375% | Wgt Acc: 98.375%
I - num batch: 319
I - Train -- Loss: 0.075 | Acc: 98.312% | Wgt Acc: 98.319% | LR: 1.250000e-04 | Dur: 116.51s
I - Confusion Matrix: [row->prediction - col->label]
[[683.   3.   0.   6.]
 [  4. 570.   6.   0.]
 [  2.   5. 723.   4.]
 [  8.   0.   5. 528.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.594 | Acc: 57.492% | Wgt Acc: 56.793% | Dur: 9.13s
I - Confusion Matrix: [row->prediction - col->label]
[[55.  7.  8. 21.]
 [ 9. 31. 11.  6.]
 [ 2. 33. 49.  6.]
 [22.  7.  7. 53.]]

I - Epoch: 127
I - Training: 
	I - Batch: 50 | Loss: 0.040 | Acc: 99.500% | Wgt Acc: 99.546%
	I - Batch: 100 | Loss: 0.043 | Acc: 99.250% | Wgt Acc: 99.292%
	I - Batch: 150 | Loss: 0.058 | Acc: 98.750% | Wgt Acc: 98.760%
	I - Batch: 200 | Loss: 0.067 | Acc: 98.562% | Wgt Acc: 98.577%
	I - Batch: 250 | Loss: 0.069 | Acc: 98.350% | Wgt Acc: 98.366%
	I - Batch: 300 | Loss: 0.068 | Acc: 98.417% | Wgt Acc: 98.432%
I - num batch: 319
I - Train -- Loss: 0.071 | Acc: 98.194% | Wgt Acc: 98.222% | LR: 1.250000e-04 | Dur: 116.35s
I - Confusion Matrix: [row->prediction - col->label]
[[676.   3.   1.   9.]
 [  5. 572.   4.   1.]
 [  1.   2. 726.   1.]
 [ 15.   1.   3. 527.]]

I - Validation: 
I - num batch: 41
I - Val -- Loss: 1.578 | Acc: 58.716% | Wgt Acc: 58.084% | Dur: 9.17s
I - Confusion Matrix: [row->prediction - col->label]
[[56.  6.  7. 19.]
 [ 2. 27. 12.  1.]
 [ 4. 38. 49.  6.]
 [26.  7.  7. 60.]]

I - Epoch: 128
I - Training: 
	I - Batch: 50 | Loss: 0.079 | Acc: 98.000% | Wgt Acc: 98.017%
	I - Batch: 100 | Loss: 0.070 | Acc: 98.250% | Wgt Acc: 98.246%
	I - Batch: 150 | Loss: 0.071 | Acc: 98.417% | Wgt Acc: 98.423%
	I - Batch: 200 | Loss: 0.072 | Acc: 98.312% | Wgt Acc: 98.284%
	I - Batch: 250 | Loss: 0.080 | Acc: 97.900% | Wgt Acc: 97.872%
