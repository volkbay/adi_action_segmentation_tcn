Tue Sep 13 16:19:32 2022
I - CONFIGURATION: {'batchSize': 16, 'bias': True, 'classWeights': [0.23, 0.23, 0.23, 0.23, 0.08], 'classWeightsFlag': True, 'dataConfig': {'augmentData': True, 'bulkPickles': True, 'dataCount': 4, 'loadData2memory': True}, 'dataPath': '/data/processed/Kinetics/', 'epochNo': 60, 'foldRatio': 4, 'fps': 5, 'frameNoDataset': 50, 'frameNoModel': 16, 'imgSize': [256, 256], 'labels': ['pull ups', 'push up', 'situp', 'squat', 'background'], 'learningRate': 0.001, 'maxValidationAcc': 0, 'maxValidationTrainNo': 0, 'modelVersion': 1, 'schedulerFlag': True, 'schedulerGamma': 0.5, 'schedulerMilestones': [10, 25, 40], 'trainNo': 3, 'validationAccThr': 55}
I - Running on device: cuda:0
I - Configuring device: MAX78000, simulate=False.
I - ========== TRAIN  SET ==========
I - Loading file: dataset_000.pkl in /data/processed/Kinetics/processed_4class_5fps_50frames_256x256/train
I - Loading file: dataset_001.pkl in /data/processed/Kinetics/processed_4class_5fps_50frames_256x256/train
I - Loading file: dataset_002.pkl in /data/processed/Kinetics/processed_4class_5fps_50frames_256x256/train
I - Loading file: dataset_003.pkl in /data/processed/Kinetics/processed_4class_5fps_50frames_256x256/train
I - Train set length:  11216
I - ========== TEST  SET ==========
I - Loading file: dataset_000.pkl in /data/processed/Kinetics/processed_4class_5fps_50frames_256x256/test
I - Loading file: dataset_005.pkl in /data/processed/Kinetics/processed_4class_5fps_50frames_256x256/test
I - Test set length:  3086
I - Batch size:  16  tensor shape:  torch.Size([16, 48, 64, 64])  data min-max:  tensor(-1.) tensor(0.9922)
I - Label min-max:  tensor(0) tensor(4) data number in dataset:  tensor([ 4616,  9042,  3139,  9601, 10687,  7680,  1556,  3757,  3187,  2636,
         5856,  3244,  3701,  4707,  6639,  7613])
I - Number of Model Parameters: 730048
I - Model output shape:  torch.Size([16, 5])
I - Model summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TCNv1                                    [16, 5]                   --
├─FusedConv2dBNReLU: 1-1                 [16, 64, 64, 64]          3,142
│    └─OutputShiftSqueeze: 2-1           --                        --
│    └─One: 2-2                          [1]                       --
│    └─OutputScale: 2-3                  --                        --
│    └─Empty: 2-4                        [64, 48, 1, 1]            --
│    └─Empty: 2-5                        [64, 48, 1, 1]            --
│    └─Empty: 2-6                        [64]                      --
│    └─Empty: 2-7                        [64]                      --
│    └─BatchNorm2d: 2-8                  [16, 64, 64, 64]          --
│    └─Scaler: 2-9                       [16, 64, 64, 64]          --
│    └─ReLU: 2-10                        [16, 64, 64, 64]          --
│    └─Empty: 2-11                       [16, 64, 64, 64]          --
│    └─Clamp: 2-12                       [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-2                 [16, 64, 64, 64]          36,934
│    └─OutputShiftSqueeze: 2-13          --                        --
│    └─One: 2-14                         [1]                       --
│    └─OutputScale: 2-15                 --                        --
│    └─Empty: 2-16                       [64, 64, 3, 3]            --
│    └─Empty: 2-17                       [64, 64, 3, 3]            --
│    └─Empty: 2-18                       [64]                      --
│    └─Empty: 2-19                       [64]                      --
│    └─BatchNorm2d: 2-20                 [16, 64, 64, 64]          --
│    └─Scaler: 2-21                      [16, 64, 64, 64]          --
│    └─ReLU: 2-22                        [16, 64, 64, 64]          --
│    └─Empty: 2-23                       [16, 64, 64, 64]          --
│    └─Clamp: 2-24                       [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-3                 [16, 64, 64, 64]          4,166
│    └─OutputShiftSqueeze: 2-25          --                        --
│    └─One: 2-26                         [1]                       --
│    └─OutputScale: 2-27                 --                        --
│    └─Empty: 2-28                       [64, 64, 1, 1]            --
│    └─Empty: 2-29                       [64, 64, 1, 1]            --
│    └─Empty: 2-30                       [64]                      --
│    └─Empty: 2-31                       [64]                      --
│    └─BatchNorm2d: 2-32                 [16, 64, 64, 64]          --
│    └─Scaler: 2-33                      [16, 64, 64, 64]          --
│    └─ReLU: 2-34                        [16, 64, 64, 64]          --
│    └─Empty: 2-35                       [16, 64, 64, 64]          --
│    └─Clamp: 2-36                       [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-4                 [16, 64, 64, 64]          36,934
│    └─OutputShiftSqueeze: 2-37          --                        --
│    └─One: 2-38                         [1]                       --
│    └─OutputScale: 2-39                 --                        --
│    └─Empty: 2-40                       [64, 64, 3, 3]            --
│    └─Empty: 2-41                       [64, 64, 3, 3]            --
│    └─Empty: 2-42                       [64]                      --
│    └─Empty: 2-43                       [64]                      --
│    └─BatchNorm2d: 2-44                 [16, 64, 64, 64]          --
│    └─Scaler: 2-45                      [16, 64, 64, 64]          --
│    └─ReLU: 2-46                        [16, 64, 64, 64]          --
│    └─Empty: 2-47                       [16, 64, 64, 64]          --
│    └─Clamp: 2-48                       [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-5          [16, 64, 32, 32]          36,934
│    └─MaxPool2d: 2-49                   [16, 64, 32, 32]          --
│    └─Empty: 2-50                       [16, 64, 32, 32]          --
│    └─Empty: 2-51                       [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-52          --                        --
│    └─One: 2-53                         [1]                       --
│    └─OutputScale: 2-54                 --                        --
│    └─Empty: 2-55                       [64, 64, 3, 3]            --
│    └─Empty: 2-56                       [64, 64, 3, 3]            --
│    └─Empty: 2-57                       [64]                      --
│    └─Empty: 2-58                       [64]                      --
│    └─BatchNorm2d: 2-59                 [16, 64, 32, 32]          --
│    └─Scaler: 2-60                      [16, 64, 32, 32]          --
│    └─ReLU: 2-61                        [16, 64, 32, 32]          --
│    └─Empty: 2-62                       [16, 64, 32, 32]          --
│    └─Clamp: 2-63                       [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-6                 [16, 64, 32, 32]          36,934
│    └─OutputShiftSqueeze: 2-64          --                        --
│    └─One: 2-65                         [1]                       --
│    └─OutputScale: 2-66                 --                        --
│    └─Empty: 2-67                       [64, 64, 3, 3]            --
│    └─Empty: 2-68                       [64, 64, 3, 3]            --
│    └─Empty: 2-69                       [64]                      --
│    └─Empty: 2-70                       [64]                      --
│    └─BatchNorm2d: 2-71                 [16, 64, 32, 32]          --
│    └─Scaler: 2-72                      [16, 64, 32, 32]          --
│    └─ReLU: 2-73                        [16, 64, 32, 32]          --
│    └─Empty: 2-74                       [16, 64, 32, 32]          --
│    └─Clamp: 2-75                       [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-7          [16, 64, 16, 16]          36,934
│    └─MaxPool2d: 2-76                   [16, 64, 16, 16]          --
│    └─Empty: 2-77                       [16, 64, 16, 16]          --
│    └─Empty: 2-78                       [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-79          --                        --
│    └─One: 2-80                         [1]                       --
│    └─OutputScale: 2-81                 --                        --
│    └─Empty: 2-82                       [64, 64, 3, 3]            --
│    └─Empty: 2-83                       [64, 64, 3, 3]            --
│    └─Empty: 2-84                       [64]                      --
│    └─Empty: 2-85                       [64]                      --
│    └─BatchNorm2d: 2-86                 [16, 64, 16, 16]          --
│    └─Scaler: 2-87                      [16, 64, 16, 16]          --
│    └─ReLU: 2-88                        [16, 64, 16, 16]          --
│    └─Empty: 2-89                       [16, 64, 16, 16]          --
│    └─Clamp: 2-90                       [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-8                 [16, 64, 16, 16]          36,934
│    └─OutputShiftSqueeze: 2-91          --                        --
│    └─One: 2-92                         [1]                       --
│    └─OutputScale: 2-93                 --                        --
│    └─Empty: 2-94                       [64, 64, 3, 3]            --
│    └─Empty: 2-95                       [64, 64, 3, 3]            --
│    └─Empty: 2-96                       [64]                      --
│    └─Empty: 2-97                       [64]                      --
│    └─BatchNorm2d: 2-98                 [16, 64, 16, 16]          --
│    └─Scaler: 2-99                      [16, 64, 16, 16]          --
│    └─ReLU: 2-100                       [16, 64, 16, 16]          --
│    └─Empty: 2-101                      [16, 64, 16, 16]          --
│    └─Clamp: 2-102                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-9          [16, 64, 8, 8]            36,934
│    └─MaxPool2d: 2-103                  [16, 64, 8, 8]            --
│    └─Empty: 2-104                      [16, 64, 8, 8]            --
│    └─Empty: 2-105                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-106         --                        --
│    └─One: 2-107                        [1]                       --
│    └─OutputScale: 2-108                --                        --
│    └─Empty: 2-109                      [64, 64, 3, 3]            --
│    └─Empty: 2-110                      [64, 64, 3, 3]            --
│    └─Empty: 2-111                      [64]                      --
│    └─Empty: 2-112                      [64]                      --
│    └─BatchNorm2d: 2-113                [16, 64, 8, 8]            --
│    └─Scaler: 2-114                     [16, 64, 8, 8]            --
│    └─ReLU: 2-115                       [16, 64, 8, 8]            --
│    └─Empty: 2-116                      [16, 64, 8, 8]            --
│    └─Clamp: 2-117                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-10                [16, 64, 8, 8]            4,166
│    └─OutputShiftSqueeze: 2-118         --                        --
│    └─One: 2-119                        [1]                       --
│    └─OutputScale: 2-120                --                        --
│    └─Empty: 2-121                      [64, 64, 1, 1]            --
│    └─Empty: 2-122                      [64, 64, 1, 1]            --
│    └─Empty: 2-123                      [64]                      --
│    └─Empty: 2-124                      [64]                      --
│    └─BatchNorm2d: 2-125                [16, 64, 8, 8]            --
│    └─Scaler: 2-126                     [16, 64, 8, 8]            --
│    └─ReLU: 2-127                       [16, 64, 8, 8]            --
│    └─Empty: 2-128                      [16, 64, 8, 8]            --
│    └─Clamp: 2-129                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-11         [16, 64, 8, 8]            36,934
│    └─MaxPool2d: 2-130                  [16, 64, 8, 8]            --
│    └─Empty: 2-131                      [16, 64, 8, 8]            --
│    └─Empty: 2-132                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-133         --                        --
│    └─One: 2-134                        [1]                       --
│    └─OutputScale: 2-135                --                        --
│    └─Empty: 2-136                      [64, 64, 3, 3]            --
│    └─Empty: 2-137                      [64, 64, 3, 3]            --
│    └─Empty: 2-138                      [64]                      --
│    └─Empty: 2-139                      [64]                      --
│    └─BatchNorm2d: 2-140                [16, 64, 8, 8]            --
│    └─Scaler: 2-141                     [16, 64, 8, 8]            --
│    └─ReLU: 2-142                       [16, 64, 8, 8]            --
│    └─Empty: 2-143                      [16, 64, 8, 8]            --
│    └─Clamp: 2-144                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-12         [16, 64, 4, 4]            36,934
│    └─MaxPool2d: 2-145                  [16, 64, 4, 4]            --
│    └─Empty: 2-146                      [16, 64, 4, 4]            --
│    └─Empty: 2-147                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-148         --                        --
│    └─One: 2-149                        [1]                       --
│    └─OutputScale: 2-150                --                        --
│    └─Empty: 2-151                      [64, 64, 3, 3]            --
│    └─Empty: 2-152                      [64, 64, 3, 3]            --
│    └─Empty: 2-153                      [64]                      --
│    └─Empty: 2-154                      [64]                      --
│    └─BatchNorm2d: 2-155                [16, 64, 4, 4]            --
│    └─Scaler: 2-156                     [16, 64, 4, 4]            --
│    └─ReLU: 2-157                       [16, 64, 4, 4]            --
│    └─Empty: 2-158                      [16, 64, 4, 4]            --
│    └─Clamp: 2-159                      [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-13                [16, 64, 4, 4]            4,166
│    └─OutputShiftSqueeze: 2-160         --                        --
│    └─One: 2-161                        [1]                       --
│    └─OutputScale: 2-162                --                        --
│    └─Empty: 2-163                      [64, 64, 1, 1]            --
│    └─Empty: 2-164                      [64, 64, 1, 1]            --
│    └─Empty: 2-165                      [64]                      --
│    └─Empty: 2-166                      [64]                      --
│    └─BatchNorm2d: 2-167                [16, 64, 4, 4]            --
│    └─Scaler: 2-168                     [16, 64, 4, 4]            --
│    └─ReLU: 2-169                       [16, 64, 4, 4]            --
│    └─Empty: 2-170                      [16, 64, 4, 4]            --
│    └─Clamp: 2-171                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-14         [16, 64, 4, 4]            36,934
│    └─MaxPool2d: 2-172                  [16, 64, 4, 4]            --
│    └─Empty: 2-173                      [16, 64, 4, 4]            --
│    └─Empty: 2-174                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-175         --                        --
│    └─One: 2-176                        [1]                       --
│    └─OutputScale: 2-177                --                        --
│    └─Empty: 2-178                      [64, 64, 3, 3]            --
│    └─Empty: 2-179                      [64, 64, 3, 3]            --
│    └─Empty: 2-180                      [64]                      --
│    └─Empty: 2-181                      [64]                      --
│    └─BatchNorm2d: 2-182                [16, 64, 4, 4]            --
│    └─Scaler: 2-183                     [16, 64, 4, 4]            --
│    └─ReLU: 2-184                       [16, 64, 4, 4]            --
│    └─Empty: 2-185                      [16, 64, 4, 4]            --
│    └─Clamp: 2-186                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-15         [16, 64, 2, 2]            4,166
│    └─MaxPool2d: 2-187                  [16, 64, 2, 2]            --
│    └─Empty: 2-188                      [16, 64, 2, 2]            --
│    └─Empty: 2-189                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-190         --                        --
│    └─One: 2-191                        [1]                       --
│    └─OutputScale: 2-192                --                        --
│    └─Empty: 2-193                      [64, 64, 1, 1]            --
│    └─Empty: 2-194                      [64, 64, 1, 1]            --
│    └─Empty: 2-195                      [64]                      --
│    └─Empty: 2-196                      [64]                      --
│    └─BatchNorm2d: 2-197                [16, 64, 2, 2]            --
│    └─Scaler: 2-198                     [16, 64, 2, 2]            --
│    └─ReLU: 2-199                       [16, 64, 2, 2]            --
│    └─Empty: 2-200                      [16, 64, 2, 2]            --
│    └─Clamp: 2-201                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-16                [16, 64, 2, 2]            4,166
│    └─OutputShiftSqueeze: 2-202         --                        --
│    └─One: 2-203                        [1]                       --
│    └─OutputScale: 2-204                --                        --
│    └─Empty: 2-205                      [64, 64, 1, 1]            --
│    └─Empty: 2-206                      [64, 64, 1, 1]            --
│    └─Empty: 2-207                      [64]                      --
│    └─Empty: 2-208                      [64]                      --
│    └─BatchNorm2d: 2-209                [16, 64, 2, 2]            --
│    └─Scaler: 2-210                     [16, 64, 2, 2]            --
│    └─ReLU: 2-211                       [16, 64, 2, 2]            --
│    └─Empty: 2-212                      [16, 64, 2, 2]            --
│    └─Clamp: 2-213                      [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-17         [16, 64, 2, 2]            36,934
│    └─MaxPool2d: 2-214                  [16, 64, 2, 2]            --
│    └─Empty: 2-215                      [16, 64, 2, 2]            --
│    └─Empty: 2-216                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-217         --                        --
│    └─One: 2-218                        [1]                       --
│    └─OutputScale: 2-219                --                        --
│    └─Empty: 2-220                      [64, 64, 3, 3]            --
│    └─Empty: 2-221                      [64, 64, 3, 3]            --
│    └─Empty: 2-222                      [64]                      --
│    └─Empty: 2-223                      [64]                      --
│    └─BatchNorm2d: 2-224                [16, 64, 2, 2]            --
│    └─Scaler: 2-225                     [16, 64, 2, 2]            --
│    └─ReLU: 2-226                       [16, 64, 2, 2]            --
│    └─Empty: 2-227                      [16, 64, 2, 2]            --
│    └─Clamp: 2-228                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-18                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-229         --                        --
│    └─One: 2-230                        [1]                       --
│    └─OutputScale: 2-231                --                        --
│    └─Empty: 2-232                      [64, 48, 1, 1]            --
│    └─Empty: 2-233                      [64, 48, 1, 1]            --
│    └─Empty: 2-234                      [64]                      --
│    └─Empty: 2-235                      [64]                      --
│    └─BatchNorm2d: 2-236                [16, 64, 64, 64]          --
│    └─Scaler: 2-237                     [16, 64, 64, 64]          --
│    └─ReLU: 2-238                       [16, 64, 64, 64]          --
│    └─Empty: 2-239                      [16, 64, 64, 64]          --
│    └─Clamp: 2-240                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-19                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-241         --                        --
│    └─One: 2-242                        [1]                       --
│    └─OutputScale: 2-243                --                        --
│    └─Empty: 2-244                      [64, 64, 3, 3]            --
│    └─Empty: 2-245                      [64, 64, 3, 3]            --
│    └─Empty: 2-246                      [64]                      --
│    └─Empty: 2-247                      [64]                      --
│    └─BatchNorm2d: 2-248                [16, 64, 64, 64]          --
│    └─Scaler: 2-249                     [16, 64, 64, 64]          --
│    └─ReLU: 2-250                       [16, 64, 64, 64]          --
│    └─Empty: 2-251                      [16, 64, 64, 64]          --
│    └─Clamp: 2-252                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-20                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-253         --                        --
│    └─One: 2-254                        [1]                       --
│    └─OutputScale: 2-255                --                        --
│    └─Empty: 2-256                      [64, 64, 1, 1]            --
│    └─Empty: 2-257                      [64, 64, 1, 1]            --
│    └─Empty: 2-258                      [64]                      --
│    └─Empty: 2-259                      [64]                      --
│    └─BatchNorm2d: 2-260                [16, 64, 64, 64]          --
│    └─Scaler: 2-261                     [16, 64, 64, 64]          --
│    └─ReLU: 2-262                       [16, 64, 64, 64]          --
│    └─Empty: 2-263                      [16, 64, 64, 64]          --
│    └─Clamp: 2-264                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-21                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-265         --                        --
│    └─One: 2-266                        [1]                       --
│    └─OutputScale: 2-267                --                        --
│    └─Empty: 2-268                      [64, 64, 3, 3]            --
│    └─Empty: 2-269                      [64, 64, 3, 3]            --
│    └─Empty: 2-270                      [64]                      --
│    └─Empty: 2-271                      [64]                      --
│    └─BatchNorm2d: 2-272                [16, 64, 64, 64]          --
│    └─Scaler: 2-273                     [16, 64, 64, 64]          --
│    └─ReLU: 2-274                       [16, 64, 64, 64]          --
│    └─Empty: 2-275                      [16, 64, 64, 64]          --
│    └─Clamp: 2-276                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-22         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-277                  [16, 64, 32, 32]          --
│    └─Empty: 2-278                      [16, 64, 32, 32]          --
│    └─Empty: 2-279                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-280         --                        --
│    └─One: 2-281                        [1]                       --
│    └─OutputScale: 2-282                --                        --
│    └─Empty: 2-283                      [64, 64, 3, 3]            --
│    └─Empty: 2-284                      [64, 64, 3, 3]            --
│    └─Empty: 2-285                      [64]                      --
│    └─Empty: 2-286                      [64]                      --
│    └─BatchNorm2d: 2-287                [16, 64, 32, 32]          --
│    └─Scaler: 2-288                     [16, 64, 32, 32]          --
│    └─ReLU: 2-289                       [16, 64, 32, 32]          --
│    └─Empty: 2-290                      [16, 64, 32, 32]          --
│    └─Clamp: 2-291                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-23                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-292         --                        --
│    └─One: 2-293                        [1]                       --
│    └─OutputScale: 2-294                --                        --
│    └─Empty: 2-295                      [64, 64, 3, 3]            --
│    └─Empty: 2-296                      [64, 64, 3, 3]            --
│    └─Empty: 2-297                      [64]                      --
│    └─Empty: 2-298                      [64]                      --
│    └─BatchNorm2d: 2-299                [16, 64, 32, 32]          --
│    └─Scaler: 2-300                     [16, 64, 32, 32]          --
│    └─ReLU: 2-301                       [16, 64, 32, 32]          --
│    └─Empty: 2-302                      [16, 64, 32, 32]          --
│    └─Clamp: 2-303                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-24         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-304                  [16, 64, 16, 16]          --
│    └─Empty: 2-305                      [16, 64, 16, 16]          --
│    └─Empty: 2-306                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-307         --                        --
│    └─One: 2-308                        [1]                       --
│    └─OutputScale: 2-309                --                        --
│    └─Empty: 2-310                      [64, 64, 3, 3]            --
│    └─Empty: 2-311                      [64, 64, 3, 3]            --
│    └─Empty: 2-312                      [64]                      --
│    └─Empty: 2-313                      [64]                      --
│    └─BatchNorm2d: 2-314                [16, 64, 16, 16]          --
│    └─Scaler: 2-315                     [16, 64, 16, 16]          --
│    └─ReLU: 2-316                       [16, 64, 16, 16]          --
│    └─Empty: 2-317                      [16, 64, 16, 16]          --
│    └─Clamp: 2-318                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-25                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-319         --                        --
│    └─One: 2-320                        [1]                       --
│    └─OutputScale: 2-321                --                        --
│    └─Empty: 2-322                      [64, 64, 3, 3]            --
│    └─Empty: 2-323                      [64, 64, 3, 3]            --
│    └─Empty: 2-324                      [64]                      --
│    └─Empty: 2-325                      [64]                      --
│    └─BatchNorm2d: 2-326                [16, 64, 16, 16]          --
│    └─Scaler: 2-327                     [16, 64, 16, 16]          --
│    └─ReLU: 2-328                       [16, 64, 16, 16]          --
│    └─Empty: 2-329                      [16, 64, 16, 16]          --
│    └─Clamp: 2-330                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-26         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-331                  [16, 64, 8, 8]            --
│    └─Empty: 2-332                      [16, 64, 8, 8]            --
│    └─Empty: 2-333                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-334         --                        --
│    └─One: 2-335                        [1]                       --
│    └─OutputScale: 2-336                --                        --
│    └─Empty: 2-337                      [64, 64, 3, 3]            --
│    └─Empty: 2-338                      [64, 64, 3, 3]            --
│    └─Empty: 2-339                      [64]                      --
│    └─Empty: 2-340                      [64]                      --
│    └─BatchNorm2d: 2-341                [16, 64, 8, 8]            --
│    └─Scaler: 2-342                     [16, 64, 8, 8]            --
│    └─ReLU: 2-343                       [16, 64, 8, 8]            --
│    └─Empty: 2-344                      [16, 64, 8, 8]            --
│    └─Clamp: 2-345                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-27                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-346         --                        --
│    └─One: 2-347                        [1]                       --
│    └─OutputScale: 2-348                --                        --
│    └─Empty: 2-349                      [64, 64, 1, 1]            --
│    └─Empty: 2-350                      [64, 64, 1, 1]            --
│    └─Empty: 2-351                      [64]                      --
│    └─Empty: 2-352                      [64]                      --
│    └─BatchNorm2d: 2-353                [16, 64, 8, 8]            --
│    └─Scaler: 2-354                     [16, 64, 8, 8]            --
│    └─ReLU: 2-355                       [16, 64, 8, 8]            --
│    └─Empty: 2-356                      [16, 64, 8, 8]            --
│    └─Clamp: 2-357                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-28         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-358                  [16, 64, 8, 8]            --
│    └─Empty: 2-359                      [16, 64, 8, 8]            --
│    └─Empty: 2-360                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-361         --                        --
│    └─One: 2-362                        [1]                       --
│    └─OutputScale: 2-363                --                        --
│    └─Empty: 2-364                      [64, 64, 3, 3]            --
│    └─Empty: 2-365                      [64, 64, 3, 3]            --
│    └─Empty: 2-366                      [64]                      --
│    └─Empty: 2-367                      [64]                      --
│    └─BatchNorm2d: 2-368                [16, 64, 8, 8]            --
│    └─Scaler: 2-369                     [16, 64, 8, 8]            --
│    └─ReLU: 2-370                       [16, 64, 8, 8]            --
│    └─Empty: 2-371                      [16, 64, 8, 8]            --
│    └─Clamp: 2-372                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-29         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-373                  [16, 64, 4, 4]            --
│    └─Empty: 2-374                      [16, 64, 4, 4]            --
│    └─Empty: 2-375                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-376         --                        --
│    └─One: 2-377                        [1]                       --
│    └─OutputScale: 2-378                --                        --
│    └─Empty: 2-379                      [64, 64, 3, 3]            --
│    └─Empty: 2-380                      [64, 64, 3, 3]            --
│    └─Empty: 2-381                      [64]                      --
│    └─Empty: 2-382                      [64]                      --
│    └─BatchNorm2d: 2-383                [16, 64, 4, 4]            --
│    └─Scaler: 2-384                     [16, 64, 4, 4]            --
│    └─ReLU: 2-385                       [16, 64, 4, 4]            --
│    └─Empty: 2-386                      [16, 64, 4, 4]            --
│    └─Clamp: 2-387                      [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-30                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-388         --                        --
│    └─One: 2-389                        [1]                       --
│    └─OutputScale: 2-390                --                        --
│    └─Empty: 2-391                      [64, 64, 1, 1]            --
│    └─Empty: 2-392                      [64, 64, 1, 1]            --
│    └─Empty: 2-393                      [64]                      --
│    └─Empty: 2-394                      [64]                      --
│    └─BatchNorm2d: 2-395                [16, 64, 4, 4]            --
│    └─Scaler: 2-396                     [16, 64, 4, 4]            --
│    └─ReLU: 2-397                       [16, 64, 4, 4]            --
│    └─Empty: 2-398                      [16, 64, 4, 4]            --
│    └─Clamp: 2-399                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-31         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-400                  [16, 64, 4, 4]            --
│    └─Empty: 2-401                      [16, 64, 4, 4]            --
│    └─Empty: 2-402                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-403         --                        --
│    └─One: 2-404                        [1]                       --
│    └─OutputScale: 2-405                --                        --
│    └─Empty: 2-406                      [64, 64, 3, 3]            --
│    └─Empty: 2-407                      [64, 64, 3, 3]            --
│    └─Empty: 2-408                      [64]                      --
│    └─Empty: 2-409                      [64]                      --
│    └─BatchNorm2d: 2-410                [16, 64, 4, 4]            --
│    └─Scaler: 2-411                     [16, 64, 4, 4]            --
│    └─ReLU: 2-412                       [16, 64, 4, 4]            --
│    └─Empty: 2-413                      [16, 64, 4, 4]            --
│    └─Clamp: 2-414                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-32         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-415                  [16, 64, 2, 2]            --
│    └─Empty: 2-416                      [16, 64, 2, 2]            --
│    └─Empty: 2-417                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-418         --                        --
│    └─One: 2-419                        [1]                       --
│    └─OutputScale: 2-420                --                        --
│    └─Empty: 2-421                      [64, 64, 1, 1]            --
│    └─Empty: 2-422                      [64, 64, 1, 1]            --
│    └─Empty: 2-423                      [64]                      --
│    └─Empty: 2-424                      [64]                      --
│    └─BatchNorm2d: 2-425                [16, 64, 2, 2]            --
│    └─Scaler: 2-426                     [16, 64, 2, 2]            --
│    └─ReLU: 2-427                       [16, 64, 2, 2]            --
│    └─Empty: 2-428                      [16, 64, 2, 2]            --
│    └─Clamp: 2-429                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-33                [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-430         --                        --
│    └─One: 2-431                        [1]                       --
│    └─OutputScale: 2-432                --                        --
│    └─Empty: 2-433                      [64, 64, 1, 1]            --
│    └─Empty: 2-434                      [64, 64, 1, 1]            --
│    └─Empty: 2-435                      [64]                      --
│    └─Empty: 2-436                      [64]                      --
│    └─BatchNorm2d: 2-437                [16, 64, 2, 2]            --
│    └─Scaler: 2-438                     [16, 64, 2, 2]            --
│    └─ReLU: 2-439                       [16, 64, 2, 2]            --
│    └─Empty: 2-440                      [16, 64, 2, 2]            --
│    └─Clamp: 2-441                      [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-34         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-442                  [16, 64, 2, 2]            --
│    └─Empty: 2-443                      [16, 64, 2, 2]            --
│    └─Empty: 2-444                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-445         --                        --
│    └─One: 2-446                        [1]                       --
│    └─OutputScale: 2-447                --                        --
│    └─Empty: 2-448                      [64, 64, 3, 3]            --
│    └─Empty: 2-449                      [64, 64, 3, 3]            --
│    └─Empty: 2-450                      [64]                      --
│    └─Empty: 2-451                      [64]                      --
│    └─BatchNorm2d: 2-452                [16, 64, 2, 2]            --
│    └─Scaler: 2-453                     [16, 64, 2, 2]            --
│    └─ReLU: 2-454                       [16, 64, 2, 2]            --
│    └─Empty: 2-455                      [16, 64, 2, 2]            --
│    └─Clamp: 2-456                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-35                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-457         --                        --
│    └─One: 2-458                        [1]                       --
│    └─OutputScale: 2-459                --                        --
│    └─Empty: 2-460                      [64, 48, 1, 1]            --
│    └─Empty: 2-461                      [64, 48, 1, 1]            --
│    └─Empty: 2-462                      [64]                      --
│    └─Empty: 2-463                      [64]                      --
│    └─BatchNorm2d: 2-464                [16, 64, 64, 64]          --
│    └─Scaler: 2-465                     [16, 64, 64, 64]          --
│    └─ReLU: 2-466                       [16, 64, 64, 64]          --
│    └─Empty: 2-467                      [16, 64, 64, 64]          --
│    └─Clamp: 2-468                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-36                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-469         --                        --
│    └─One: 2-470                        [1]                       --
│    └─OutputScale: 2-471                --                        --
│    └─Empty: 2-472                      [64, 64, 3, 3]            --
│    └─Empty: 2-473                      [64, 64, 3, 3]            --
│    └─Empty: 2-474                      [64]                      --
│    └─Empty: 2-475                      [64]                      --
│    └─BatchNorm2d: 2-476                [16, 64, 64, 64]          --
│    └─Scaler: 2-477                     [16, 64, 64, 64]          --
│    └─ReLU: 2-478                       [16, 64, 64, 64]          --
│    └─Empty: 2-479                      [16, 64, 64, 64]          --
│    └─Clamp: 2-480                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-37                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-481         --                        --
│    └─One: 2-482                        [1]                       --
│    └─OutputScale: 2-483                --                        --
│    └─Empty: 2-484                      [64, 64, 1, 1]            --
│    └─Empty: 2-485                      [64, 64, 1, 1]            --
│    └─Empty: 2-486                      [64]                      --
│    └─Empty: 2-487                      [64]                      --
│    └─BatchNorm2d: 2-488                [16, 64, 64, 64]          --
│    └─Scaler: 2-489                     [16, 64, 64, 64]          --
│    └─ReLU: 2-490                       [16, 64, 64, 64]          --
│    └─Empty: 2-491                      [16, 64, 64, 64]          --
│    └─Clamp: 2-492                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-38                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-493         --                        --
│    └─One: 2-494                        [1]                       --
│    └─OutputScale: 2-495                --                        --
│    └─Empty: 2-496                      [64, 64, 3, 3]            --
│    └─Empty: 2-497                      [64, 64, 3, 3]            --
│    └─Empty: 2-498                      [64]                      --
│    └─Empty: 2-499                      [64]                      --
│    └─BatchNorm2d: 2-500                [16, 64, 64, 64]          --
│    └─Scaler: 2-501                     [16, 64, 64, 64]          --
│    └─ReLU: 2-502                       [16, 64, 64, 64]          --
│    └─Empty: 2-503                      [16, 64, 64, 64]          --
│    └─Clamp: 2-504                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-39         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-505                  [16, 64, 32, 32]          --
│    └─Empty: 2-506                      [16, 64, 32, 32]          --
│    └─Empty: 2-507                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-508         --                        --
│    └─One: 2-509                        [1]                       --
│    └─OutputScale: 2-510                --                        --
│    └─Empty: 2-511                      [64, 64, 3, 3]            --
│    └─Empty: 2-512                      [64, 64, 3, 3]            --
│    └─Empty: 2-513                      [64]                      --
│    └─Empty: 2-514                      [64]                      --
│    └─BatchNorm2d: 2-515                [16, 64, 32, 32]          --
│    └─Scaler: 2-516                     [16, 64, 32, 32]          --
│    └─ReLU: 2-517                       [16, 64, 32, 32]          --
│    └─Empty: 2-518                      [16, 64, 32, 32]          --
│    └─Clamp: 2-519                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-40                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-520         --                        --
│    └─One: 2-521                        [1]                       --
│    └─OutputScale: 2-522                --                        --
│    └─Empty: 2-523                      [64, 64, 3, 3]            --
│    └─Empty: 2-524                      [64, 64, 3, 3]            --
│    └─Empty: 2-525                      [64]                      --
│    └─Empty: 2-526                      [64]                      --
│    └─BatchNorm2d: 2-527                [16, 64, 32, 32]          --
│    └─Scaler: 2-528                     [16, 64, 32, 32]          --
│    └─ReLU: 2-529                       [16, 64, 32, 32]          --
│    └─Empty: 2-530                      [16, 64, 32, 32]          --
│    └─Clamp: 2-531                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-41         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-532                  [16, 64, 16, 16]          --
│    └─Empty: 2-533                      [16, 64, 16, 16]          --
│    └─Empty: 2-534                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-535         --                        --
│    └─One: 2-536                        [1]                       --
│    └─OutputScale: 2-537                --                        --
│    └─Empty: 2-538                      [64, 64, 3, 3]            --
│    └─Empty: 2-539                      [64, 64, 3, 3]            --
│    └─Empty: 2-540                      [64]                      --
│    └─Empty: 2-541                      [64]                      --
│    └─BatchNorm2d: 2-542                [16, 64, 16, 16]          --
│    └─Scaler: 2-543                     [16, 64, 16, 16]          --
│    └─ReLU: 2-544                       [16, 64, 16, 16]          --
│    └─Empty: 2-545                      [16, 64, 16, 16]          --
│    └─Clamp: 2-546                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-42                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-547         --                        --
│    └─One: 2-548                        [1]                       --
│    └─OutputScale: 2-549                --                        --
│    └─Empty: 2-550                      [64, 64, 3, 3]            --
│    └─Empty: 2-551                      [64, 64, 3, 3]            --
│    └─Empty: 2-552                      [64]                      --
│    └─Empty: 2-553                      [64]                      --
│    └─BatchNorm2d: 2-554                [16, 64, 16, 16]          --
│    └─Scaler: 2-555                     [16, 64, 16, 16]          --
│    └─ReLU: 2-556                       [16, 64, 16, 16]          --
│    └─Empty: 2-557                      [16, 64, 16, 16]          --
│    └─Clamp: 2-558                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-43         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-559                  [16, 64, 8, 8]            --
│    └─Empty: 2-560                      [16, 64, 8, 8]            --
│    └─Empty: 2-561                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-562         --                        --
│    └─One: 2-563                        [1]                       --
│    └─OutputScale: 2-564                --                        --
│    └─Empty: 2-565                      [64, 64, 3, 3]            --
│    └─Empty: 2-566                      [64, 64, 3, 3]            --
│    └─Empty: 2-567                      [64]                      --
│    └─Empty: 2-568                      [64]                      --
│    └─BatchNorm2d: 2-569                [16, 64, 8, 8]            --
│    └─Scaler: 2-570                     [16, 64, 8, 8]            --
│    └─ReLU: 2-571                       [16, 64, 8, 8]            --
│    └─Empty: 2-572                      [16, 64, 8, 8]            --
│    └─Clamp: 2-573                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-44                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-574         --                        --
│    └─One: 2-575                        [1]                       --
│    └─OutputScale: 2-576                --                        --
│    └─Empty: 2-577                      [64, 64, 1, 1]            --
│    └─Empty: 2-578                      [64, 64, 1, 1]            --
│    └─Empty: 2-579                      [64]                      --
│    └─Empty: 2-580                      [64]                      --
│    └─BatchNorm2d: 2-581                [16, 64, 8, 8]            --
│    └─Scaler: 2-582                     [16, 64, 8, 8]            --
│    └─ReLU: 2-583                       [16, 64, 8, 8]            --
│    └─Empty: 2-584                      [16, 64, 8, 8]            --
│    └─Clamp: 2-585                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-45         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-586                  [16, 64, 8, 8]            --
│    └─Empty: 2-587                      [16, 64, 8, 8]            --
│    └─Empty: 2-588                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-589         --                        --
│    └─One: 2-590                        [1]                       --
│    └─OutputScale: 2-591                --                        --
│    └─Empty: 2-592                      [64, 64, 3, 3]            --
│    └─Empty: 2-593                      [64, 64, 3, 3]            --
│    └─Empty: 2-594                      [64]                      --
│    └─Empty: 2-595                      [64]                      --
│    └─BatchNorm2d: 2-596                [16, 64, 8, 8]            --
│    └─Scaler: 2-597                     [16, 64, 8, 8]            --
│    └─ReLU: 2-598                       [16, 64, 8, 8]            --
│    └─Empty: 2-599                      [16, 64, 8, 8]            --
│    └─Clamp: 2-600                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-46         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-601                  [16, 64, 4, 4]            --
│    └─Empty: 2-602                      [16, 64, 4, 4]            --
│    └─Empty: 2-603                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-604         --                        --
│    └─One: 2-605                        [1]                       --
│    └─OutputScale: 2-606                --                        --
│    └─Empty: 2-607                      [64, 64, 3, 3]            --
│    └─Empty: 2-608                      [64, 64, 3, 3]            --
│    └─Empty: 2-609                      [64]                      --
│    └─Empty: 2-610                      [64]                      --
│    └─BatchNorm2d: 2-611                [16, 64, 4, 4]            --
│    └─Scaler: 2-612                     [16, 64, 4, 4]            --
│    └─ReLU: 2-613                       [16, 64, 4, 4]            --
│    └─Empty: 2-614                      [16, 64, 4, 4]            --
│    └─Clamp: 2-615                      [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-47                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-616         --                        --
│    └─One: 2-617                        [1]                       --
│    └─OutputScale: 2-618                --                        --
│    └─Empty: 2-619                      [64, 64, 1, 1]            --
│    └─Empty: 2-620                      [64, 64, 1, 1]            --
│    └─Empty: 2-621                      [64]                      --
│    └─Empty: 2-622                      [64]                      --
│    └─BatchNorm2d: 2-623                [16, 64, 4, 4]            --
│    └─Scaler: 2-624                     [16, 64, 4, 4]            --
│    └─ReLU: 2-625                       [16, 64, 4, 4]            --
│    └─Empty: 2-626                      [16, 64, 4, 4]            --
│    └─Clamp: 2-627                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-48         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-628                  [16, 64, 4, 4]            --
│    └─Empty: 2-629                      [16, 64, 4, 4]            --
│    └─Empty: 2-630                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-631         --                        --
│    └─One: 2-632                        [1]                       --
│    └─OutputScale: 2-633                --                        --
│    └─Empty: 2-634                      [64, 64, 3, 3]            --
│    └─Empty: 2-635                      [64, 64, 3, 3]            --
│    └─Empty: 2-636                      [64]                      --
│    └─Empty: 2-637                      [64]                      --
│    └─BatchNorm2d: 2-638                [16, 64, 4, 4]            --
│    └─Scaler: 2-639                     [16, 64, 4, 4]            --
│    └─ReLU: 2-640                       [16, 64, 4, 4]            --
│    └─Empty: 2-641                      [16, 64, 4, 4]            --
│    └─Clamp: 2-642                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-49         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-643                  [16, 64, 2, 2]            --
│    └─Empty: 2-644                      [16, 64, 2, 2]            --
│    └─Empty: 2-645                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-646         --                        --
│    └─One: 2-647                        [1]                       --
│    └─OutputScale: 2-648                --                        --
│    └─Empty: 2-649                      [64, 64, 1, 1]            --
│    └─Empty: 2-650                      [64, 64, 1, 1]            --
│    └─Empty: 2-651                      [64]                      --
│    └─Empty: 2-652                      [64]                      --
│    └─BatchNorm2d: 2-653                [16, 64, 2, 2]            --
│    └─Scaler: 2-654                     [16, 64, 2, 2]            --
│    └─ReLU: 2-655                       [16, 64, 2, 2]            --
│    └─Empty: 2-656                      [16, 64, 2, 2]            --
│    └─Clamp: 2-657                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-50                [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-658         --                        --
│    └─One: 2-659                        [1]                       --
│    └─OutputScale: 2-660                --                        --
│    └─Empty: 2-661                      [64, 64, 1, 1]            --
│    └─Empty: 2-662                      [64, 64, 1, 1]            --
│    └─Empty: 2-663                      [64]                      --
│    └─Empty: 2-664                      [64]                      --
│    └─BatchNorm2d: 2-665                [16, 64, 2, 2]            --
│    └─Scaler: 2-666                     [16, 64, 2, 2]            --
│    └─ReLU: 2-667                       [16, 64, 2, 2]            --
│    └─Empty: 2-668                      [16, 64, 2, 2]            --
│    └─Clamp: 2-669                      [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-51         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-670                  [16, 64, 2, 2]            --
│    └─Empty: 2-671                      [16, 64, 2, 2]            --
│    └─Empty: 2-672                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-673         --                        --
│    └─One: 2-674                        [1]                       --
│    └─OutputScale: 2-675                --                        --
│    └─Empty: 2-676                      [64, 64, 3, 3]            --
│    └─Empty: 2-677                      [64, 64, 3, 3]            --
│    └─Empty: 2-678                      [64]                      --
│    └─Empty: 2-679                      [64]                      --
│    └─BatchNorm2d: 2-680                [16, 64, 2, 2]            --
│    └─Scaler: 2-681                     [16, 64, 2, 2]            --
│    └─ReLU: 2-682                       [16, 64, 2, 2]            --
│    └─Empty: 2-683                      [16, 64, 2, 2]            --
│    └─Clamp: 2-684                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-52                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-685         --                        --
│    └─One: 2-686                        [1]                       --
│    └─OutputScale: 2-687                --                        --
│    └─Empty: 2-688                      [64, 48, 1, 1]            --
│    └─Empty: 2-689                      [64, 48, 1, 1]            --
│    └─Empty: 2-690                      [64]                      --
│    └─Empty: 2-691                      [64]                      --
│    └─BatchNorm2d: 2-692                [16, 64, 64, 64]          --
│    └─Scaler: 2-693                     [16, 64, 64, 64]          --
│    └─ReLU: 2-694                       [16, 64, 64, 64]          --
│    └─Empty: 2-695                      [16, 64, 64, 64]          --
│    └─Clamp: 2-696                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-53                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-697         --                        --
│    └─One: 2-698                        [1]                       --
│    └─OutputScale: 2-699                --                        --
│    └─Empty: 2-700                      [64, 64, 3, 3]            --
│    └─Empty: 2-701                      [64, 64, 3, 3]            --
│    └─Empty: 2-702                      [64]                      --
│    └─Empty: 2-703                      [64]                      --
│    └─BatchNorm2d: 2-704                [16, 64, 64, 64]          --
│    └─Scaler: 2-705                     [16, 64, 64, 64]          --
│    └─ReLU: 2-706                       [16, 64, 64, 64]          --
│    └─Empty: 2-707                      [16, 64, 64, 64]          --
│    └─Clamp: 2-708                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-54                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-709         --                        --
│    └─One: 2-710                        [1]                       --
│    └─OutputScale: 2-711                --                        --
│    └─Empty: 2-712                      [64, 64, 1, 1]            --
│    └─Empty: 2-713                      [64, 64, 1, 1]            --
│    └─Empty: 2-714                      [64]                      --
│    └─Empty: 2-715                      [64]                      --
│    └─BatchNorm2d: 2-716                [16, 64, 64, 64]          --
│    └─Scaler: 2-717                     [16, 64, 64, 64]          --
│    └─ReLU: 2-718                       [16, 64, 64, 64]          --
│    └─Empty: 2-719                      [16, 64, 64, 64]          --
│    └─Clamp: 2-720                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-55                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-721         --                        --
│    └─One: 2-722                        [1]                       --
│    └─OutputScale: 2-723                --                        --
│    └─Empty: 2-724                      [64, 64, 3, 3]            --
│    └─Empty: 2-725                      [64, 64, 3, 3]            --
│    └─Empty: 2-726                      [64]                      --
│    └─Empty: 2-727                      [64]                      --
│    └─BatchNorm2d: 2-728                [16, 64, 64, 64]          --
│    └─Scaler: 2-729                     [16, 64, 64, 64]          --
│    └─ReLU: 2-730                       [16, 64, 64, 64]          --
│    └─Empty: 2-731                      [16, 64, 64, 64]          --
│    └─Clamp: 2-732                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-56         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-733                  [16, 64, 32, 32]          --
│    └─Empty: 2-734                      [16, 64, 32, 32]          --
│    └─Empty: 2-735                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-736         --                        --
│    └─One: 2-737                        [1]                       --
│    └─OutputScale: 2-738                --                        --
│    └─Empty: 2-739                      [64, 64, 3, 3]            --
│    └─Empty: 2-740                      [64, 64, 3, 3]            --
│    └─Empty: 2-741                      [64]                      --
│    └─Empty: 2-742                      [64]                      --
│    └─BatchNorm2d: 2-743                [16, 64, 32, 32]          --
│    └─Scaler: 2-744                     [16, 64, 32, 32]          --
│    └─ReLU: 2-745                       [16, 64, 32, 32]          --
│    └─Empty: 2-746                      [16, 64, 32, 32]          --
│    └─Clamp: 2-747                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-57                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-748         --                        --
│    └─One: 2-749                        [1]                       --
│    └─OutputScale: 2-750                --                        --
│    └─Empty: 2-751                      [64, 64, 3, 3]            --
│    └─Empty: 2-752                      [64, 64, 3, 3]            --
│    └─Empty: 2-753                      [64]                      --
│    └─Empty: 2-754                      [64]                      --
│    └─BatchNorm2d: 2-755                [16, 64, 32, 32]          --
│    └─Scaler: 2-756                     [16, 64, 32, 32]          --
│    └─ReLU: 2-757                       [16, 64, 32, 32]          --
│    └─Empty: 2-758                      [16, 64, 32, 32]          --
│    └─Clamp: 2-759                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-58         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-760                  [16, 64, 16, 16]          --
│    └─Empty: 2-761                      [16, 64, 16, 16]          --
│    └─Empty: 2-762                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-763         --                        --
│    └─One: 2-764                        [1]                       --
│    └─OutputScale: 2-765                --                        --
│    └─Empty: 2-766                      [64, 64, 3, 3]            --
│    └─Empty: 2-767                      [64, 64, 3, 3]            --
│    └─Empty: 2-768                      [64]                      --
│    └─Empty: 2-769                      [64]                      --
│    └─BatchNorm2d: 2-770                [16, 64, 16, 16]          --
│    └─Scaler: 2-771                     [16, 64, 16, 16]          --
│    └─ReLU: 2-772                       [16, 64, 16, 16]          --
│    └─Empty: 2-773                      [16, 64, 16, 16]          --
│    └─Clamp: 2-774                      [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-59                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-775         --                        --
│    └─One: 2-776                        [1]                       --
│    └─OutputScale: 2-777                --                        --
│    └─Empty: 2-778                      [64, 64, 3, 3]            --
│    └─Empty: 2-779                      [64, 64, 3, 3]            --
│    └─Empty: 2-780                      [64]                      --
│    └─Empty: 2-781                      [64]                      --
│    └─BatchNorm2d: 2-782                [16, 64, 16, 16]          --
│    └─Scaler: 2-783                     [16, 64, 16, 16]          --
│    └─ReLU: 2-784                       [16, 64, 16, 16]          --
│    └─Empty: 2-785                      [16, 64, 16, 16]          --
│    └─Clamp: 2-786                      [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-60         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-787                  [16, 64, 8, 8]            --
│    └─Empty: 2-788                      [16, 64, 8, 8]            --
│    └─Empty: 2-789                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-790         --                        --
│    └─One: 2-791                        [1]                       --
│    └─OutputScale: 2-792                --                        --
│    └─Empty: 2-793                      [64, 64, 3, 3]            --
│    └─Empty: 2-794                      [64, 64, 3, 3]            --
│    └─Empty: 2-795                      [64]                      --
│    └─Empty: 2-796                      [64]                      --
│    └─BatchNorm2d: 2-797                [16, 64, 8, 8]            --
│    └─Scaler: 2-798                     [16, 64, 8, 8]            --
│    └─ReLU: 2-799                       [16, 64, 8, 8]            --
│    └─Empty: 2-800                      [16, 64, 8, 8]            --
│    └─Clamp: 2-801                      [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-61                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-802         --                        --
│    └─One: 2-803                        [1]                       --
│    └─OutputScale: 2-804                --                        --
│    └─Empty: 2-805                      [64, 64, 1, 1]            --
│    └─Empty: 2-806                      [64, 64, 1, 1]            --
│    └─Empty: 2-807                      [64]                      --
│    └─Empty: 2-808                      [64]                      --
│    └─BatchNorm2d: 2-809                [16, 64, 8, 8]            --
│    └─Scaler: 2-810                     [16, 64, 8, 8]            --
│    └─ReLU: 2-811                       [16, 64, 8, 8]            --
│    └─Empty: 2-812                      [16, 64, 8, 8]            --
│    └─Clamp: 2-813                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-62         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-814                  [16, 64, 8, 8]            --
│    └─Empty: 2-815                      [16, 64, 8, 8]            --
│    └─Empty: 2-816                      [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-817         --                        --
│    └─One: 2-818                        [1]                       --
│    └─OutputScale: 2-819                --                        --
│    └─Empty: 2-820                      [64, 64, 3, 3]            --
│    └─Empty: 2-821                      [64, 64, 3, 3]            --
│    └─Empty: 2-822                      [64]                      --
│    └─Empty: 2-823                      [64]                      --
│    └─BatchNorm2d: 2-824                [16, 64, 8, 8]            --
│    └─Scaler: 2-825                     [16, 64, 8, 8]            --
│    └─ReLU: 2-826                       [16, 64, 8, 8]            --
│    └─Empty: 2-827                      [16, 64, 8, 8]            --
│    └─Clamp: 2-828                      [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-63         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-829                  [16, 64, 4, 4]            --
│    └─Empty: 2-830                      [16, 64, 4, 4]            --
│    └─Empty: 2-831                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-832         --                        --
│    └─One: 2-833                        [1]                       --
│    └─OutputScale: 2-834                --                        --
│    └─Empty: 2-835                      [64, 64, 3, 3]            --
│    └─Empty: 2-836                      [64, 64, 3, 3]            --
│    └─Empty: 2-837                      [64]                      --
│    └─Empty: 2-838                      [64]                      --
│    └─BatchNorm2d: 2-839                [16, 64, 4, 4]            --
│    └─Scaler: 2-840                     [16, 64, 4, 4]            --
│    └─ReLU: 2-841                       [16, 64, 4, 4]            --
│    └─Empty: 2-842                      [16, 64, 4, 4]            --
│    └─Clamp: 2-843                      [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-64                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-844         --                        --
│    └─One: 2-845                        [1]                       --
│    └─OutputScale: 2-846                --                        --
│    └─Empty: 2-847                      [64, 64, 1, 1]            --
│    └─Empty: 2-848                      [64, 64, 1, 1]            --
│    └─Empty: 2-849                      [64]                      --
│    └─Empty: 2-850                      [64]                      --
│    └─BatchNorm2d: 2-851                [16, 64, 4, 4]            --
│    └─Scaler: 2-852                     [16, 64, 4, 4]            --
│    └─ReLU: 2-853                       [16, 64, 4, 4]            --
│    └─Empty: 2-854                      [16, 64, 4, 4]            --
│    └─Clamp: 2-855                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-65         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-856                  [16, 64, 4, 4]            --
│    └─Empty: 2-857                      [16, 64, 4, 4]            --
│    └─Empty: 2-858                      [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-859         --                        --
│    └─One: 2-860                        [1]                       --
│    └─OutputScale: 2-861                --                        --
│    └─Empty: 2-862                      [64, 64, 3, 3]            --
│    └─Empty: 2-863                      [64, 64, 3, 3]            --
│    └─Empty: 2-864                      [64]                      --
│    └─Empty: 2-865                      [64]                      --
│    └─BatchNorm2d: 2-866                [16, 64, 4, 4]            --
│    └─Scaler: 2-867                     [16, 64, 4, 4]            --
│    └─ReLU: 2-868                       [16, 64, 4, 4]            --
│    └─Empty: 2-869                      [16, 64, 4, 4]            --
│    └─Clamp: 2-870                      [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-66         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-871                  [16, 64, 2, 2]            --
│    └─Empty: 2-872                      [16, 64, 2, 2]            --
│    └─Empty: 2-873                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-874         --                        --
│    └─One: 2-875                        [1]                       --
│    └─OutputScale: 2-876                --                        --
│    └─Empty: 2-877                      [64, 64, 1, 1]            --
│    └─Empty: 2-878                      [64, 64, 1, 1]            --
│    └─Empty: 2-879                      [64]                      --
│    └─Empty: 2-880                      [64]                      --
│    └─BatchNorm2d: 2-881                [16, 64, 2, 2]            --
│    └─Scaler: 2-882                     [16, 64, 2, 2]            --
│    └─ReLU: 2-883                       [16, 64, 2, 2]            --
│    └─Empty: 2-884                      [16, 64, 2, 2]            --
│    └─Clamp: 2-885                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-67                [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-886         --                        --
│    └─One: 2-887                        [1]                       --
│    └─OutputScale: 2-888                --                        --
│    └─Empty: 2-889                      [64, 64, 1, 1]            --
│    └─Empty: 2-890                      [64, 64, 1, 1]            --
│    └─Empty: 2-891                      [64]                      --
│    └─Empty: 2-892                      [64]                      --
│    └─BatchNorm2d: 2-893                [16, 64, 2, 2]            --
│    └─Scaler: 2-894                     [16, 64, 2, 2]            --
│    └─ReLU: 2-895                       [16, 64, 2, 2]            --
│    └─Empty: 2-896                      [16, 64, 2, 2]            --
│    └─Clamp: 2-897                      [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-68         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-898                  [16, 64, 2, 2]            --
│    └─Empty: 2-899                      [16, 64, 2, 2]            --
│    └─Empty: 2-900                      [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-901         --                        --
│    └─One: 2-902                        [1]                       --
│    └─OutputScale: 2-903                --                        --
│    └─Empty: 2-904                      [64, 64, 3, 3]            --
│    └─Empty: 2-905                      [64, 64, 3, 3]            --
│    └─Empty: 2-906                      [64]                      --
│    └─Empty: 2-907                      [64]                      --
│    └─BatchNorm2d: 2-908                [16, 64, 2, 2]            --
│    └─Scaler: 2-909                     [16, 64, 2, 2]            --
│    └─ReLU: 2-910                       [16, 64, 2, 2]            --
│    └─Empty: 2-911                      [16, 64, 2, 2]            --
│    └─Clamp: 2-912                      [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-69                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-913         --                        --
│    └─One: 2-914                        [1]                       --
│    └─OutputScale: 2-915                --                        --
│    └─Empty: 2-916                      [64, 48, 1, 1]            --
│    └─Empty: 2-917                      [64, 48, 1, 1]            --
│    └─Empty: 2-918                      [64]                      --
│    └─Empty: 2-919                      [64]                      --
│    └─BatchNorm2d: 2-920                [16, 64, 64, 64]          --
│    └─Scaler: 2-921                     [16, 64, 64, 64]          --
│    └─ReLU: 2-922                       [16, 64, 64, 64]          --
│    └─Empty: 2-923                      [16, 64, 64, 64]          --
│    └─Clamp: 2-924                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-70                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-925         --                        --
│    └─One: 2-926                        [1]                       --
│    └─OutputScale: 2-927                --                        --
│    └─Empty: 2-928                      [64, 64, 3, 3]            --
│    └─Empty: 2-929                      [64, 64, 3, 3]            --
│    └─Empty: 2-930                      [64]                      --
│    └─Empty: 2-931                      [64]                      --
│    └─BatchNorm2d: 2-932                [16, 64, 64, 64]          --
│    └─Scaler: 2-933                     [16, 64, 64, 64]          --
│    └─ReLU: 2-934                       [16, 64, 64, 64]          --
│    └─Empty: 2-935                      [16, 64, 64, 64]          --
│    └─Clamp: 2-936                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-71                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-937         --                        --
│    └─One: 2-938                        [1]                       --
│    └─OutputScale: 2-939                --                        --
│    └─Empty: 2-940                      [64, 64, 1, 1]            --
│    └─Empty: 2-941                      [64, 64, 1, 1]            --
│    └─Empty: 2-942                      [64]                      --
│    └─Empty: 2-943                      [64]                      --
│    └─BatchNorm2d: 2-944                [16, 64, 64, 64]          --
│    └─Scaler: 2-945                     [16, 64, 64, 64]          --
│    └─ReLU: 2-946                       [16, 64, 64, 64]          --
│    └─Empty: 2-947                      [16, 64, 64, 64]          --
│    └─Clamp: 2-948                      [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-72                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-949         --                        --
│    └─One: 2-950                        [1]                       --
│    └─OutputScale: 2-951                --                        --
│    └─Empty: 2-952                      [64, 64, 3, 3]            --
│    └─Empty: 2-953                      [64, 64, 3, 3]            --
│    └─Empty: 2-954                      [64]                      --
│    └─Empty: 2-955                      [64]                      --
│    └─BatchNorm2d: 2-956                [16, 64, 64, 64]          --
│    └─Scaler: 2-957                     [16, 64, 64, 64]          --
│    └─ReLU: 2-958                       [16, 64, 64, 64]          --
│    └─Empty: 2-959                      [16, 64, 64, 64]          --
│    └─Clamp: 2-960                      [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-73         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-961                  [16, 64, 32, 32]          --
│    └─Empty: 2-962                      [16, 64, 32, 32]          --
│    └─Empty: 2-963                      [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-964         --                        --
│    └─One: 2-965                        [1]                       --
│    └─OutputScale: 2-966                --                        --
│    └─Empty: 2-967                      [64, 64, 3, 3]            --
│    └─Empty: 2-968                      [64, 64, 3, 3]            --
│    └─Empty: 2-969                      [64]                      --
│    └─Empty: 2-970                      [64]                      --
│    └─BatchNorm2d: 2-971                [16, 64, 32, 32]          --
│    └─Scaler: 2-972                     [16, 64, 32, 32]          --
│    └─ReLU: 2-973                       [16, 64, 32, 32]          --
│    └─Empty: 2-974                      [16, 64, 32, 32]          --
│    └─Clamp: 2-975                      [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-74                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-976         --                        --
│    └─One: 2-977                        [1]                       --
│    └─OutputScale: 2-978                --                        --
│    └─Empty: 2-979                      [64, 64, 3, 3]            --
│    └─Empty: 2-980                      [64, 64, 3, 3]            --
│    └─Empty: 2-981                      [64]                      --
│    └─Empty: 2-982                      [64]                      --
│    └─BatchNorm2d: 2-983                [16, 64, 32, 32]          --
│    └─Scaler: 2-984                     [16, 64, 32, 32]          --
│    └─ReLU: 2-985                       [16, 64, 32, 32]          --
│    └─Empty: 2-986                      [16, 64, 32, 32]          --
│    └─Clamp: 2-987                      [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-75         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-988                  [16, 64, 16, 16]          --
│    └─Empty: 2-989                      [16, 64, 16, 16]          --
│    └─Empty: 2-990                      [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-991         --                        --
│    └─One: 2-992                        [1]                       --
│    └─OutputScale: 2-993                --                        --
│    └─Empty: 2-994                      [64, 64, 3, 3]            --
│    └─Empty: 2-995                      [64, 64, 3, 3]            --
│    └─Empty: 2-996                      [64]                      --
│    └─Empty: 2-997                      [64]                      --
│    └─BatchNorm2d: 2-998                [16, 64, 16, 16]          --
│    └─Scaler: 2-999                     [16, 64, 16, 16]          --
│    └─ReLU: 2-1000                      [16, 64, 16, 16]          --
│    └─Empty: 2-1001                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1002                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-76                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1003        --                        --
│    └─One: 2-1004                       [1]                       --
│    └─OutputScale: 2-1005               --                        --
│    └─Empty: 2-1006                     [64, 64, 3, 3]            --
│    └─Empty: 2-1007                     [64, 64, 3, 3]            --
│    └─Empty: 2-1008                     [64]                      --
│    └─Empty: 2-1009                     [64]                      --
│    └─BatchNorm2d: 2-1010               [16, 64, 16, 16]          --
│    └─Scaler: 2-1011                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1012                      [16, 64, 16, 16]          --
│    └─Empty: 2-1013                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1014                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-77         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1015                 [16, 64, 8, 8]            --
│    └─Empty: 2-1016                     [16, 64, 8, 8]            --
│    └─Empty: 2-1017                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1018        --                        --
│    └─One: 2-1019                       [1]                       --
│    └─OutputScale: 2-1020               --                        --
│    └─Empty: 2-1021                     [64, 64, 3, 3]            --
│    └─Empty: 2-1022                     [64, 64, 3, 3]            --
│    └─Empty: 2-1023                     [64]                      --
│    └─Empty: 2-1024                     [64]                      --
│    └─BatchNorm2d: 2-1025               [16, 64, 8, 8]            --
│    └─Scaler: 2-1026                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1027                      [16, 64, 8, 8]            --
│    └─Empty: 2-1028                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1029                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-78                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1030        --                        --
│    └─One: 2-1031                       [1]                       --
│    └─OutputScale: 2-1032               --                        --
│    └─Empty: 2-1033                     [64, 64, 1, 1]            --
│    └─Empty: 2-1034                     [64, 64, 1, 1]            --
│    └─Empty: 2-1035                     [64]                      --
│    └─Empty: 2-1036                     [64]                      --
│    └─BatchNorm2d: 2-1037               [16, 64, 8, 8]            --
│    └─Scaler: 2-1038                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1039                      [16, 64, 8, 8]            --
│    └─Empty: 2-1040                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1041                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-79         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1042                 [16, 64, 8, 8]            --
│    └─Empty: 2-1043                     [16, 64, 8, 8]            --
│    └─Empty: 2-1044                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1045        --                        --
│    └─One: 2-1046                       [1]                       --
│    └─OutputScale: 2-1047               --                        --
│    └─Empty: 2-1048                     [64, 64, 3, 3]            --
│    └─Empty: 2-1049                     [64, 64, 3, 3]            --
│    └─Empty: 2-1050                     [64]                      --
│    └─Empty: 2-1051                     [64]                      --
│    └─BatchNorm2d: 2-1052               [16, 64, 8, 8]            --
│    └─Scaler: 2-1053                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1054                      [16, 64, 8, 8]            --
│    └─Empty: 2-1055                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1056                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-80         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1057                 [16, 64, 4, 4]            --
│    └─Empty: 2-1058                     [16, 64, 4, 4]            --
│    └─Empty: 2-1059                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1060        --                        --
│    └─One: 2-1061                       [1]                       --
│    └─OutputScale: 2-1062               --                        --
│    └─Empty: 2-1063                     [64, 64, 3, 3]            --
│    └─Empty: 2-1064                     [64, 64, 3, 3]            --
│    └─Empty: 2-1065                     [64]                      --
│    └─Empty: 2-1066                     [64]                      --
│    └─BatchNorm2d: 2-1067               [16, 64, 4, 4]            --
│    └─Scaler: 2-1068                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1069                      [16, 64, 4, 4]            --
│    └─Empty: 2-1070                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1071                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-81                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1072        --                        --
│    └─One: 2-1073                       [1]                       --
│    └─OutputScale: 2-1074               --                        --
│    └─Empty: 2-1075                     [64, 64, 1, 1]            --
│    └─Empty: 2-1076                     [64, 64, 1, 1]            --
│    └─Empty: 2-1077                     [64]                      --
│    └─Empty: 2-1078                     [64]                      --
│    └─BatchNorm2d: 2-1079               [16, 64, 4, 4]            --
│    └─Scaler: 2-1080                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1081                      [16, 64, 4, 4]            --
│    └─Empty: 2-1082                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1083                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-82         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1084                 [16, 64, 4, 4]            --
│    └─Empty: 2-1085                     [16, 64, 4, 4]            --
│    └─Empty: 2-1086                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1087        --                        --
│    └─One: 2-1088                       [1]                       --
│    └─OutputScale: 2-1089               --                        --
│    └─Empty: 2-1090                     [64, 64, 3, 3]            --
│    └─Empty: 2-1091                     [64, 64, 3, 3]            --
│    └─Empty: 2-1092                     [64]                      --
│    └─Empty: 2-1093                     [64]                      --
│    └─BatchNorm2d: 2-1094               [16, 64, 4, 4]            --
│    └─Scaler: 2-1095                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1096                      [16, 64, 4, 4]            --
│    └─Empty: 2-1097                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1098                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-83         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1099                 [16, 64, 2, 2]            --
│    └─Empty: 2-1100                     [16, 64, 2, 2]            --
│    └─Empty: 2-1101                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1102        --                        --
│    └─One: 2-1103                       [1]                       --
│    └─OutputScale: 2-1104               --                        --
│    └─Empty: 2-1105                     [64, 64, 1, 1]            --
│    └─Empty: 2-1106                     [64, 64, 1, 1]            --
│    └─Empty: 2-1107                     [64]                      --
│    └─Empty: 2-1108                     [64]                      --
│    └─BatchNorm2d: 2-1109               [16, 64, 2, 2]            --
│    └─Scaler: 2-1110                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1111                      [16, 64, 2, 2]            --
│    └─Empty: 2-1112                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1113                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-84                [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-1114        --                        --
│    └─One: 2-1115                       [1]                       --
│    └─OutputScale: 2-1116               --                        --
│    └─Empty: 2-1117                     [64, 64, 1, 1]            --
│    └─Empty: 2-1118                     [64, 64, 1, 1]            --
│    └─Empty: 2-1119                     [64]                      --
│    └─Empty: 2-1120                     [64]                      --
│    └─BatchNorm2d: 2-1121               [16, 64, 2, 2]            --
│    └─Scaler: 2-1122                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1123                      [16, 64, 2, 2]            --
│    └─Empty: 2-1124                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1125                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-85         [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1126                 [16, 64, 2, 2]            --
│    └─Empty: 2-1127                     [16, 64, 2, 2]            --
│    └─Empty: 2-1128                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1129        --                        --
│    └─One: 2-1130                       [1]                       --
│    └─OutputScale: 2-1131               --                        --
│    └─Empty: 2-1132                     [64, 64, 3, 3]            --
│    └─Empty: 2-1133                     [64, 64, 3, 3]            --
│    └─Empty: 2-1134                     [64]                      --
│    └─Empty: 2-1135                     [64]                      --
│    └─BatchNorm2d: 2-1136               [16, 64, 2, 2]            --
│    └─Scaler: 2-1137                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1138                      [16, 64, 2, 2]            --
│    └─Empty: 2-1139                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1140                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-86                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1141        --                        --
│    └─One: 2-1142                       [1]                       --
│    └─OutputScale: 2-1143               --                        --
│    └─Empty: 2-1144                     [64, 48, 1, 1]            --
│    └─Empty: 2-1145                     [64, 48, 1, 1]            --
│    └─Empty: 2-1146                     [64]                      --
│    └─Empty: 2-1147                     [64]                      --
│    └─BatchNorm2d: 2-1148               [16, 64, 64, 64]          --
│    └─Scaler: 2-1149                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1150                      [16, 64, 64, 64]          --
│    └─Empty: 2-1151                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1152                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-87                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1153        --                        --
│    └─One: 2-1154                       [1]                       --
│    └─OutputScale: 2-1155               --                        --
│    └─Empty: 2-1156                     [64, 64, 3, 3]            --
│    └─Empty: 2-1157                     [64, 64, 3, 3]            --
│    └─Empty: 2-1158                     [64]                      --
│    └─Empty: 2-1159                     [64]                      --
│    └─BatchNorm2d: 2-1160               [16, 64, 64, 64]          --
│    └─Scaler: 2-1161                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1162                      [16, 64, 64, 64]          --
│    └─Empty: 2-1163                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1164                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-88                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1165        --                        --
│    └─One: 2-1166                       [1]                       --
│    └─OutputScale: 2-1167               --                        --
│    └─Empty: 2-1168                     [64, 64, 1, 1]            --
│    └─Empty: 2-1169                     [64, 64, 1, 1]            --
│    └─Empty: 2-1170                     [64]                      --
│    └─Empty: 2-1171                     [64]                      --
│    └─BatchNorm2d: 2-1172               [16, 64, 64, 64]          --
│    └─Scaler: 2-1173                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1174                      [16, 64, 64, 64]          --
│    └─Empty: 2-1175                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1176                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-89                [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1177        --                        --
│    └─One: 2-1178                       [1]                       --
│    └─OutputScale: 2-1179               --                        --
│    └─Empty: 2-1180                     [64, 64, 3, 3]            --
│    └─Empty: 2-1181                     [64, 64, 3, 3]            --
│    └─Empty: 2-1182                     [64]                      --
│    └─Empty: 2-1183                     [64]                      --
│    └─BatchNorm2d: 2-1184               [16, 64, 64, 64]          --
│    └─Scaler: 2-1185                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1186                      [16, 64, 64, 64]          --
│    └─Empty: 2-1187                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1188                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-90         [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1189                 [16, 64, 32, 32]          --
│    └─Empty: 2-1190                     [16, 64, 32, 32]          --
│    └─Empty: 2-1191                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1192        --                        --
│    └─One: 2-1193                       [1]                       --
│    └─OutputScale: 2-1194               --                        --
│    └─Empty: 2-1195                     [64, 64, 3, 3]            --
│    └─Empty: 2-1196                     [64, 64, 3, 3]            --
│    └─Empty: 2-1197                     [64]                      --
│    └─Empty: 2-1198                     [64]                      --
│    └─BatchNorm2d: 2-1199               [16, 64, 32, 32]          --
│    └─Scaler: 2-1200                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1201                      [16, 64, 32, 32]          --
│    └─Empty: 2-1202                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1203                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-91                [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1204        --                        --
│    └─One: 2-1205                       [1]                       --
│    └─OutputScale: 2-1206               --                        --
│    └─Empty: 2-1207                     [64, 64, 3, 3]            --
│    └─Empty: 2-1208                     [64, 64, 3, 3]            --
│    └─Empty: 2-1209                     [64]                      --
│    └─Empty: 2-1210                     [64]                      --
│    └─BatchNorm2d: 2-1211               [16, 64, 32, 32]          --
│    └─Scaler: 2-1212                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1213                      [16, 64, 32, 32]          --
│    └─Empty: 2-1214                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1215                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-92         [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1216                 [16, 64, 16, 16]          --
│    └─Empty: 2-1217                     [16, 64, 16, 16]          --
│    └─Empty: 2-1218                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1219        --                        --
│    └─One: 2-1220                       [1]                       --
│    └─OutputScale: 2-1221               --                        --
│    └─Empty: 2-1222                     [64, 64, 3, 3]            --
│    └─Empty: 2-1223                     [64, 64, 3, 3]            --
│    └─Empty: 2-1224                     [64]                      --
│    └─Empty: 2-1225                     [64]                      --
│    └─BatchNorm2d: 2-1226               [16, 64, 16, 16]          --
│    └─Scaler: 2-1227                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1228                      [16, 64, 16, 16]          --
│    └─Empty: 2-1229                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1230                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-93                [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1231        --                        --
│    └─One: 2-1232                       [1]                       --
│    └─OutputScale: 2-1233               --                        --
│    └─Empty: 2-1234                     [64, 64, 3, 3]            --
│    └─Empty: 2-1235                     [64, 64, 3, 3]            --
│    └─Empty: 2-1236                     [64]                      --
│    └─Empty: 2-1237                     [64]                      --
│    └─BatchNorm2d: 2-1238               [16, 64, 16, 16]          --
│    └─Scaler: 2-1239                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1240                      [16, 64, 16, 16]          --
│    └─Empty: 2-1241                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1242                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-94         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1243                 [16, 64, 8, 8]            --
│    └─Empty: 2-1244                     [16, 64, 8, 8]            --
│    └─Empty: 2-1245                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1246        --                        --
│    └─One: 2-1247                       [1]                       --
│    └─OutputScale: 2-1248               --                        --
│    └─Empty: 2-1249                     [64, 64, 3, 3]            --
│    └─Empty: 2-1250                     [64, 64, 3, 3]            --
│    └─Empty: 2-1251                     [64]                      --
│    └─Empty: 2-1252                     [64]                      --
│    └─BatchNorm2d: 2-1253               [16, 64, 8, 8]            --
│    └─Scaler: 2-1254                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1255                      [16, 64, 8, 8]            --
│    └─Empty: 2-1256                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1257                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-95                [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1258        --                        --
│    └─One: 2-1259                       [1]                       --
│    └─OutputScale: 2-1260               --                        --
│    └─Empty: 2-1261                     [64, 64, 1, 1]            --
│    └─Empty: 2-1262                     [64, 64, 1, 1]            --
│    └─Empty: 2-1263                     [64]                      --
│    └─Empty: 2-1264                     [64]                      --
│    └─BatchNorm2d: 2-1265               [16, 64, 8, 8]            --
│    └─Scaler: 2-1266                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1267                      [16, 64, 8, 8]            --
│    └─Empty: 2-1268                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1269                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-96         [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1270                 [16, 64, 8, 8]            --
│    └─Empty: 2-1271                     [16, 64, 8, 8]            --
│    └─Empty: 2-1272                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1273        --                        --
│    └─One: 2-1274                       [1]                       --
│    └─OutputScale: 2-1275               --                        --
│    └─Empty: 2-1276                     [64, 64, 3, 3]            --
│    └─Empty: 2-1277                     [64, 64, 3, 3]            --
│    └─Empty: 2-1278                     [64]                      --
│    └─Empty: 2-1279                     [64]                      --
│    └─BatchNorm2d: 2-1280               [16, 64, 8, 8]            --
│    └─Scaler: 2-1281                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1282                      [16, 64, 8, 8]            --
│    └─Empty: 2-1283                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1284                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-97         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1285                 [16, 64, 4, 4]            --
│    └─Empty: 2-1286                     [16, 64, 4, 4]            --
│    └─Empty: 2-1287                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1288        --                        --
│    └─One: 2-1289                       [1]                       --
│    └─OutputScale: 2-1290               --                        --
│    └─Empty: 2-1291                     [64, 64, 3, 3]            --
│    └─Empty: 2-1292                     [64, 64, 3, 3]            --
│    └─Empty: 2-1293                     [64]                      --
│    └─Empty: 2-1294                     [64]                      --
│    └─BatchNorm2d: 2-1295               [16, 64, 4, 4]            --
│    └─Scaler: 2-1296                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1297                      [16, 64, 4, 4]            --
│    └─Empty: 2-1298                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1299                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-98                [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1300        --                        --
│    └─One: 2-1301                       [1]                       --
│    └─OutputScale: 2-1302               --                        --
│    └─Empty: 2-1303                     [64, 64, 1, 1]            --
│    └─Empty: 2-1304                     [64, 64, 1, 1]            --
│    └─Empty: 2-1305                     [64]                      --
│    └─Empty: 2-1306                     [64]                      --
│    └─BatchNorm2d: 2-1307               [16, 64, 4, 4]            --
│    └─Scaler: 2-1308                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1309                      [16, 64, 4, 4]            --
│    └─Empty: 2-1310                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1311                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-99         [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1312                 [16, 64, 4, 4]            --
│    └─Empty: 2-1313                     [16, 64, 4, 4]            --
│    └─Empty: 2-1314                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1315        --                        --
│    └─One: 2-1316                       [1]                       --
│    └─OutputScale: 2-1317               --                        --
│    └─Empty: 2-1318                     [64, 64, 3, 3]            --
│    └─Empty: 2-1319                     [64, 64, 3, 3]            --
│    └─Empty: 2-1320                     [64]                      --
│    └─Empty: 2-1321                     [64]                      --
│    └─BatchNorm2d: 2-1322               [16, 64, 4, 4]            --
│    └─Scaler: 2-1323                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1324                      [16, 64, 4, 4]            --
│    └─Empty: 2-1325                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1326                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-100        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1327                 [16, 64, 2, 2]            --
│    └─Empty: 2-1328                     [16, 64, 2, 2]            --
│    └─Empty: 2-1329                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1330        --                        --
│    └─One: 2-1331                       [1]                       --
│    └─OutputScale: 2-1332               --                        --
│    └─Empty: 2-1333                     [64, 64, 1, 1]            --
│    └─Empty: 2-1334                     [64, 64, 1, 1]            --
│    └─Empty: 2-1335                     [64]                      --
│    └─Empty: 2-1336                     [64]                      --
│    └─BatchNorm2d: 2-1337               [16, 64, 2, 2]            --
│    └─Scaler: 2-1338                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1339                      [16, 64, 2, 2]            --
│    └─Empty: 2-1340                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1341                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-101               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-1342        --                        --
│    └─One: 2-1343                       [1]                       --
│    └─OutputScale: 2-1344               --                        --
│    └─Empty: 2-1345                     [64, 64, 1, 1]            --
│    └─Empty: 2-1346                     [64, 64, 1, 1]            --
│    └─Empty: 2-1347                     [64]                      --
│    └─Empty: 2-1348                     [64]                      --
│    └─BatchNorm2d: 2-1349               [16, 64, 2, 2]            --
│    └─Scaler: 2-1350                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1351                      [16, 64, 2, 2]            --
│    └─Empty: 2-1352                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1353                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-102        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1354                 [16, 64, 2, 2]            --
│    └─Empty: 2-1355                     [16, 64, 2, 2]            --
│    └─Empty: 2-1356                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1357        --                        --
│    └─One: 2-1358                       [1]                       --
│    └─OutputScale: 2-1359               --                        --
│    └─Empty: 2-1360                     [64, 64, 3, 3]            --
│    └─Empty: 2-1361                     [64, 64, 3, 3]            --
│    └─Empty: 2-1362                     [64]                      --
│    └─Empty: 2-1363                     [64]                      --
│    └─BatchNorm2d: 2-1364               [16, 64, 2, 2]            --
│    └─Scaler: 2-1365                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1366                      [16, 64, 2, 2]            --
│    └─Empty: 2-1367                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1368                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-103               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1369        --                        --
│    └─One: 2-1370                       [1]                       --
│    └─OutputScale: 2-1371               --                        --
│    └─Empty: 2-1372                     [64, 48, 1, 1]            --
│    └─Empty: 2-1373                     [64, 48, 1, 1]            --
│    └─Empty: 2-1374                     [64]                      --
│    └─Empty: 2-1375                     [64]                      --
│    └─BatchNorm2d: 2-1376               [16, 64, 64, 64]          --
│    └─Scaler: 2-1377                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1378                      [16, 64, 64, 64]          --
│    └─Empty: 2-1379                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1380                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-104               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1381        --                        --
│    └─One: 2-1382                       [1]                       --
│    └─OutputScale: 2-1383               --                        --
│    └─Empty: 2-1384                     [64, 64, 3, 3]            --
│    └─Empty: 2-1385                     [64, 64, 3, 3]            --
│    └─Empty: 2-1386                     [64]                      --
│    └─Empty: 2-1387                     [64]                      --
│    └─BatchNorm2d: 2-1388               [16, 64, 64, 64]          --
│    └─Scaler: 2-1389                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1390                      [16, 64, 64, 64]          --
│    └─Empty: 2-1391                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1392                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-105               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1393        --                        --
│    └─One: 2-1394                       [1]                       --
│    └─OutputScale: 2-1395               --                        --
│    └─Empty: 2-1396                     [64, 64, 1, 1]            --
│    └─Empty: 2-1397                     [64, 64, 1, 1]            --
│    └─Empty: 2-1398                     [64]                      --
│    └─Empty: 2-1399                     [64]                      --
│    └─BatchNorm2d: 2-1400               [16, 64, 64, 64]          --
│    └─Scaler: 2-1401                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1402                      [16, 64, 64, 64]          --
│    └─Empty: 2-1403                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1404                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-106               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1405        --                        --
│    └─One: 2-1406                       [1]                       --
│    └─OutputScale: 2-1407               --                        --
│    └─Empty: 2-1408                     [64, 64, 3, 3]            --
│    └─Empty: 2-1409                     [64, 64, 3, 3]            --
│    └─Empty: 2-1410                     [64]                      --
│    └─Empty: 2-1411                     [64]                      --
│    └─BatchNorm2d: 2-1412               [16, 64, 64, 64]          --
│    └─Scaler: 2-1413                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1414                      [16, 64, 64, 64]          --
│    └─Empty: 2-1415                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1416                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-107        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1417                 [16, 64, 32, 32]          --
│    └─Empty: 2-1418                     [16, 64, 32, 32]          --
│    └─Empty: 2-1419                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1420        --                        --
│    └─One: 2-1421                       [1]                       --
│    └─OutputScale: 2-1422               --                        --
│    └─Empty: 2-1423                     [64, 64, 3, 3]            --
│    └─Empty: 2-1424                     [64, 64, 3, 3]            --
│    └─Empty: 2-1425                     [64]                      --
│    └─Empty: 2-1426                     [64]                      --
│    └─BatchNorm2d: 2-1427               [16, 64, 32, 32]          --
│    └─Scaler: 2-1428                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1429                      [16, 64, 32, 32]          --
│    └─Empty: 2-1430                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1431                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-108               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1432        --                        --
│    └─One: 2-1433                       [1]                       --
│    └─OutputScale: 2-1434               --                        --
│    └─Empty: 2-1435                     [64, 64, 3, 3]            --
│    └─Empty: 2-1436                     [64, 64, 3, 3]            --
│    └─Empty: 2-1437                     [64]                      --
│    └─Empty: 2-1438                     [64]                      --
│    └─BatchNorm2d: 2-1439               [16, 64, 32, 32]          --
│    └─Scaler: 2-1440                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1441                      [16, 64, 32, 32]          --
│    └─Empty: 2-1442                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1443                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-109        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1444                 [16, 64, 16, 16]          --
│    └─Empty: 2-1445                     [16, 64, 16, 16]          --
│    └─Empty: 2-1446                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1447        --                        --
│    └─One: 2-1448                       [1]                       --
│    └─OutputScale: 2-1449               --                        --
│    └─Empty: 2-1450                     [64, 64, 3, 3]            --
│    └─Empty: 2-1451                     [64, 64, 3, 3]            --
│    └─Empty: 2-1452                     [64]                      --
│    └─Empty: 2-1453                     [64]                      --
│    └─BatchNorm2d: 2-1454               [16, 64, 16, 16]          --
│    └─Scaler: 2-1455                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1456                      [16, 64, 16, 16]          --
│    └─Empty: 2-1457                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1458                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-110               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1459        --                        --
│    └─One: 2-1460                       [1]                       --
│    └─OutputScale: 2-1461               --                        --
│    └─Empty: 2-1462                     [64, 64, 3, 3]            --
│    └─Empty: 2-1463                     [64, 64, 3, 3]            --
│    └─Empty: 2-1464                     [64]                      --
│    └─Empty: 2-1465                     [64]                      --
│    └─BatchNorm2d: 2-1466               [16, 64, 16, 16]          --
│    └─Scaler: 2-1467                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1468                      [16, 64, 16, 16]          --
│    └─Empty: 2-1469                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1470                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-111        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1471                 [16, 64, 8, 8]            --
│    └─Empty: 2-1472                     [16, 64, 8, 8]            --
│    └─Empty: 2-1473                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1474        --                        --
│    └─One: 2-1475                       [1]                       --
│    └─OutputScale: 2-1476               --                        --
│    └─Empty: 2-1477                     [64, 64, 3, 3]            --
│    └─Empty: 2-1478                     [64, 64, 3, 3]            --
│    └─Empty: 2-1479                     [64]                      --
│    └─Empty: 2-1480                     [64]                      --
│    └─BatchNorm2d: 2-1481               [16, 64, 8, 8]            --
│    └─Scaler: 2-1482                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1483                      [16, 64, 8, 8]            --
│    └─Empty: 2-1484                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1485                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-112               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1486        --                        --
│    └─One: 2-1487                       [1]                       --
│    └─OutputScale: 2-1488               --                        --
│    └─Empty: 2-1489                     [64, 64, 1, 1]            --
│    └─Empty: 2-1490                     [64, 64, 1, 1]            --
│    └─Empty: 2-1491                     [64]                      --
│    └─Empty: 2-1492                     [64]                      --
│    └─BatchNorm2d: 2-1493               [16, 64, 8, 8]            --
│    └─Scaler: 2-1494                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1495                      [16, 64, 8, 8]            --
│    └─Empty: 2-1496                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1497                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-113        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1498                 [16, 64, 8, 8]            --
│    └─Empty: 2-1499                     [16, 64, 8, 8]            --
│    └─Empty: 2-1500                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1501        --                        --
│    └─One: 2-1502                       [1]                       --
│    └─OutputScale: 2-1503               --                        --
│    └─Empty: 2-1504                     [64, 64, 3, 3]            --
│    └─Empty: 2-1505                     [64, 64, 3, 3]            --
│    └─Empty: 2-1506                     [64]                      --
│    └─Empty: 2-1507                     [64]                      --
│    └─BatchNorm2d: 2-1508               [16, 64, 8, 8]            --
│    └─Scaler: 2-1509                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1510                      [16, 64, 8, 8]            --
│    └─Empty: 2-1511                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1512                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-114        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1513                 [16, 64, 4, 4]            --
│    └─Empty: 2-1514                     [16, 64, 4, 4]            --
│    └─Empty: 2-1515                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1516        --                        --
│    └─One: 2-1517                       [1]                       --
│    └─OutputScale: 2-1518               --                        --
│    └─Empty: 2-1519                     [64, 64, 3, 3]            --
│    └─Empty: 2-1520                     [64, 64, 3, 3]            --
│    └─Empty: 2-1521                     [64]                      --
│    └─Empty: 2-1522                     [64]                      --
│    └─BatchNorm2d: 2-1523               [16, 64, 4, 4]            --
│    └─Scaler: 2-1524                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1525                      [16, 64, 4, 4]            --
│    └─Empty: 2-1526                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1527                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-115               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1528        --                        --
│    └─One: 2-1529                       [1]                       --
│    └─OutputScale: 2-1530               --                        --
│    └─Empty: 2-1531                     [64, 64, 1, 1]            --
│    └─Empty: 2-1532                     [64, 64, 1, 1]            --
│    └─Empty: 2-1533                     [64]                      --
│    └─Empty: 2-1534                     [64]                      --
│    └─BatchNorm2d: 2-1535               [16, 64, 4, 4]            --
│    └─Scaler: 2-1536                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1537                      [16, 64, 4, 4]            --
│    └─Empty: 2-1538                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1539                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-116        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1540                 [16, 64, 4, 4]            --
│    └─Empty: 2-1541                     [16, 64, 4, 4]            --
│    └─Empty: 2-1542                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1543        --                        --
│    └─One: 2-1544                       [1]                       --
│    └─OutputScale: 2-1545               --                        --
│    └─Empty: 2-1546                     [64, 64, 3, 3]            --
│    └─Empty: 2-1547                     [64, 64, 3, 3]            --
│    └─Empty: 2-1548                     [64]                      --
│    └─Empty: 2-1549                     [64]                      --
│    └─BatchNorm2d: 2-1550               [16, 64, 4, 4]            --
│    └─Scaler: 2-1551                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1552                      [16, 64, 4, 4]            --
│    └─Empty: 2-1553                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1554                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-117        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1555                 [16, 64, 2, 2]            --
│    └─Empty: 2-1556                     [16, 64, 2, 2]            --
│    └─Empty: 2-1557                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1558        --                        --
│    └─One: 2-1559                       [1]                       --
│    └─OutputScale: 2-1560               --                        --
│    └─Empty: 2-1561                     [64, 64, 1, 1]            --
│    └─Empty: 2-1562                     [64, 64, 1, 1]            --
│    └─Empty: 2-1563                     [64]                      --
│    └─Empty: 2-1564                     [64]                      --
│    └─BatchNorm2d: 2-1565               [16, 64, 2, 2]            --
│    └─Scaler: 2-1566                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1567                      [16, 64, 2, 2]            --
│    └─Empty: 2-1568                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1569                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-118               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-1570        --                        --
│    └─One: 2-1571                       [1]                       --
│    └─OutputScale: 2-1572               --                        --
│    └─Empty: 2-1573                     [64, 64, 1, 1]            --
│    └─Empty: 2-1574                     [64, 64, 1, 1]            --
│    └─Empty: 2-1575                     [64]                      --
│    └─Empty: 2-1576                     [64]                      --
│    └─BatchNorm2d: 2-1577               [16, 64, 2, 2]            --
│    └─Scaler: 2-1578                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1579                      [16, 64, 2, 2]            --
│    └─Empty: 2-1580                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1581                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-119        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1582                 [16, 64, 2, 2]            --
│    └─Empty: 2-1583                     [16, 64, 2, 2]            --
│    └─Empty: 2-1584                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1585        --                        --
│    └─One: 2-1586                       [1]                       --
│    └─OutputScale: 2-1587               --                        --
│    └─Empty: 2-1588                     [64, 64, 3, 3]            --
│    └─Empty: 2-1589                     [64, 64, 3, 3]            --
│    └─Empty: 2-1590                     [64]                      --
│    └─Empty: 2-1591                     [64]                      --
│    └─BatchNorm2d: 2-1592               [16, 64, 2, 2]            --
│    └─Scaler: 2-1593                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1594                      [16, 64, 2, 2]            --
│    └─Empty: 2-1595                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1596                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-120               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1597        --                        --
│    └─One: 2-1598                       [1]                       --
│    └─OutputScale: 2-1599               --                        --
│    └─Empty: 2-1600                     [64, 48, 1, 1]            --
│    └─Empty: 2-1601                     [64, 48, 1, 1]            --
│    └─Empty: 2-1602                     [64]                      --
│    └─Empty: 2-1603                     [64]                      --
│    └─BatchNorm2d: 2-1604               [16, 64, 64, 64]          --
│    └─Scaler: 2-1605                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1606                      [16, 64, 64, 64]          --
│    └─Empty: 2-1607                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1608                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-121               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1609        --                        --
│    └─One: 2-1610                       [1]                       --
│    └─OutputScale: 2-1611               --                        --
│    └─Empty: 2-1612                     [64, 64, 3, 3]            --
│    └─Empty: 2-1613                     [64, 64, 3, 3]            --
│    └─Empty: 2-1614                     [64]                      --
│    └─Empty: 2-1615                     [64]                      --
│    └─BatchNorm2d: 2-1616               [16, 64, 64, 64]          --
│    └─Scaler: 2-1617                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1618                      [16, 64, 64, 64]          --
│    └─Empty: 2-1619                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1620                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-122               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1621        --                        --
│    └─One: 2-1622                       [1]                       --
│    └─OutputScale: 2-1623               --                        --
│    └─Empty: 2-1624                     [64, 64, 1, 1]            --
│    └─Empty: 2-1625                     [64, 64, 1, 1]            --
│    └─Empty: 2-1626                     [64]                      --
│    └─Empty: 2-1627                     [64]                      --
│    └─BatchNorm2d: 2-1628               [16, 64, 64, 64]          --
│    └─Scaler: 2-1629                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1630                      [16, 64, 64, 64]          --
│    └─Empty: 2-1631                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1632                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-123               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1633        --                        --
│    └─One: 2-1634                       [1]                       --
│    └─OutputScale: 2-1635               --                        --
│    └─Empty: 2-1636                     [64, 64, 3, 3]            --
│    └─Empty: 2-1637                     [64, 64, 3, 3]            --
│    └─Empty: 2-1638                     [64]                      --
│    └─Empty: 2-1639                     [64]                      --
│    └─BatchNorm2d: 2-1640               [16, 64, 64, 64]          --
│    └─Scaler: 2-1641                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1642                      [16, 64, 64, 64]          --
│    └─Empty: 2-1643                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1644                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-124        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1645                 [16, 64, 32, 32]          --
│    └─Empty: 2-1646                     [16, 64, 32, 32]          --
│    └─Empty: 2-1647                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1648        --                        --
│    └─One: 2-1649                       [1]                       --
│    └─OutputScale: 2-1650               --                        --
│    └─Empty: 2-1651                     [64, 64, 3, 3]            --
│    └─Empty: 2-1652                     [64, 64, 3, 3]            --
│    └─Empty: 2-1653                     [64]                      --
│    └─Empty: 2-1654                     [64]                      --
│    └─BatchNorm2d: 2-1655               [16, 64, 32, 32]          --
│    └─Scaler: 2-1656                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1657                      [16, 64, 32, 32]          --
│    └─Empty: 2-1658                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1659                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-125               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1660        --                        --
│    └─One: 2-1661                       [1]                       --
│    └─OutputScale: 2-1662               --                        --
│    └─Empty: 2-1663                     [64, 64, 3, 3]            --
│    └─Empty: 2-1664                     [64, 64, 3, 3]            --
│    └─Empty: 2-1665                     [64]                      --
│    └─Empty: 2-1666                     [64]                      --
│    └─BatchNorm2d: 2-1667               [16, 64, 32, 32]          --
│    └─Scaler: 2-1668                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1669                      [16, 64, 32, 32]          --
│    └─Empty: 2-1670                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1671                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-126        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1672                 [16, 64, 16, 16]          --
│    └─Empty: 2-1673                     [16, 64, 16, 16]          --
│    └─Empty: 2-1674                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1675        --                        --
│    └─One: 2-1676                       [1]                       --
│    └─OutputScale: 2-1677               --                        --
│    └─Empty: 2-1678                     [64, 64, 3, 3]            --
│    └─Empty: 2-1679                     [64, 64, 3, 3]            --
│    └─Empty: 2-1680                     [64]                      --
│    └─Empty: 2-1681                     [64]                      --
│    └─BatchNorm2d: 2-1682               [16, 64, 16, 16]          --
│    └─Scaler: 2-1683                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1684                      [16, 64, 16, 16]          --
│    └─Empty: 2-1685                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1686                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-127               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1687        --                        --
│    └─One: 2-1688                       [1]                       --
│    └─OutputScale: 2-1689               --                        --
│    └─Empty: 2-1690                     [64, 64, 3, 3]            --
│    └─Empty: 2-1691                     [64, 64, 3, 3]            --
│    └─Empty: 2-1692                     [64]                      --
│    └─Empty: 2-1693                     [64]                      --
│    └─BatchNorm2d: 2-1694               [16, 64, 16, 16]          --
│    └─Scaler: 2-1695                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1696                      [16, 64, 16, 16]          --
│    └─Empty: 2-1697                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1698                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-128        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1699                 [16, 64, 8, 8]            --
│    └─Empty: 2-1700                     [16, 64, 8, 8]            --
│    └─Empty: 2-1701                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1702        --                        --
│    └─One: 2-1703                       [1]                       --
│    └─OutputScale: 2-1704               --                        --
│    └─Empty: 2-1705                     [64, 64, 3, 3]            --
│    └─Empty: 2-1706                     [64, 64, 3, 3]            --
│    └─Empty: 2-1707                     [64]                      --
│    └─Empty: 2-1708                     [64]                      --
│    └─BatchNorm2d: 2-1709               [16, 64, 8, 8]            --
│    └─Scaler: 2-1710                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1711                      [16, 64, 8, 8]            --
│    └─Empty: 2-1712                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1713                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-129               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1714        --                        --
│    └─One: 2-1715                       [1]                       --
│    └─OutputScale: 2-1716               --                        --
│    └─Empty: 2-1717                     [64, 64, 1, 1]            --
│    └─Empty: 2-1718                     [64, 64, 1, 1]            --
│    └─Empty: 2-1719                     [64]                      --
│    └─Empty: 2-1720                     [64]                      --
│    └─BatchNorm2d: 2-1721               [16, 64, 8, 8]            --
│    └─Scaler: 2-1722                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1723                      [16, 64, 8, 8]            --
│    └─Empty: 2-1724                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1725                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-130        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1726                 [16, 64, 8, 8]            --
│    └─Empty: 2-1727                     [16, 64, 8, 8]            --
│    └─Empty: 2-1728                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1729        --                        --
│    └─One: 2-1730                       [1]                       --
│    └─OutputScale: 2-1731               --                        --
│    └─Empty: 2-1732                     [64, 64, 3, 3]            --
│    └─Empty: 2-1733                     [64, 64, 3, 3]            --
│    └─Empty: 2-1734                     [64]                      --
│    └─Empty: 2-1735                     [64]                      --
│    └─BatchNorm2d: 2-1736               [16, 64, 8, 8]            --
│    └─Scaler: 2-1737                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1738                      [16, 64, 8, 8]            --
│    └─Empty: 2-1739                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1740                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-131        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1741                 [16, 64, 4, 4]            --
│    └─Empty: 2-1742                     [16, 64, 4, 4]            --
│    └─Empty: 2-1743                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1744        --                        --
│    └─One: 2-1745                       [1]                       --
│    └─OutputScale: 2-1746               --                        --
│    └─Empty: 2-1747                     [64, 64, 3, 3]            --
│    └─Empty: 2-1748                     [64, 64, 3, 3]            --
│    └─Empty: 2-1749                     [64]                      --
│    └─Empty: 2-1750                     [64]                      --
│    └─BatchNorm2d: 2-1751               [16, 64, 4, 4]            --
│    └─Scaler: 2-1752                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1753                      [16, 64, 4, 4]            --
│    └─Empty: 2-1754                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1755                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-132               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1756        --                        --
│    └─One: 2-1757                       [1]                       --
│    └─OutputScale: 2-1758               --                        --
│    └─Empty: 2-1759                     [64, 64, 1, 1]            --
│    └─Empty: 2-1760                     [64, 64, 1, 1]            --
│    └─Empty: 2-1761                     [64]                      --
│    └─Empty: 2-1762                     [64]                      --
│    └─BatchNorm2d: 2-1763               [16, 64, 4, 4]            --
│    └─Scaler: 2-1764                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1765                      [16, 64, 4, 4]            --
│    └─Empty: 2-1766                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1767                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-133        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1768                 [16, 64, 4, 4]            --
│    └─Empty: 2-1769                     [16, 64, 4, 4]            --
│    └─Empty: 2-1770                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1771        --                        --
│    └─One: 2-1772                       [1]                       --
│    └─OutputScale: 2-1773               --                        --
│    └─Empty: 2-1774                     [64, 64, 3, 3]            --
│    └─Empty: 2-1775                     [64, 64, 3, 3]            --
│    └─Empty: 2-1776                     [64]                      --
│    └─Empty: 2-1777                     [64]                      --
│    └─BatchNorm2d: 2-1778               [16, 64, 4, 4]            --
│    └─Scaler: 2-1779                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1780                      [16, 64, 4, 4]            --
│    └─Empty: 2-1781                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1782                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-134        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1783                 [16, 64, 2, 2]            --
│    └─Empty: 2-1784                     [16, 64, 2, 2]            --
│    └─Empty: 2-1785                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1786        --                        --
│    └─One: 2-1787                       [1]                       --
│    └─OutputScale: 2-1788               --                        --
│    └─Empty: 2-1789                     [64, 64, 1, 1]            --
│    └─Empty: 2-1790                     [64, 64, 1, 1]            --
│    └─Empty: 2-1791                     [64]                      --
│    └─Empty: 2-1792                     [64]                      --
│    └─BatchNorm2d: 2-1793               [16, 64, 2, 2]            --
│    └─Scaler: 2-1794                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1795                      [16, 64, 2, 2]            --
│    └─Empty: 2-1796                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1797                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-135               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-1798        --                        --
│    └─One: 2-1799                       [1]                       --
│    └─OutputScale: 2-1800               --                        --
│    └─Empty: 2-1801                     [64, 64, 1, 1]            --
│    └─Empty: 2-1802                     [64, 64, 1, 1]            --
│    └─Empty: 2-1803                     [64]                      --
│    └─Empty: 2-1804                     [64]                      --
│    └─BatchNorm2d: 2-1805               [16, 64, 2, 2]            --
│    └─Scaler: 2-1806                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1807                      [16, 64, 2, 2]            --
│    └─Empty: 2-1808                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1809                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-136        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-1810                 [16, 64, 2, 2]            --
│    └─Empty: 2-1811                     [16, 64, 2, 2]            --
│    └─Empty: 2-1812                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-1813        --                        --
│    └─One: 2-1814                       [1]                       --
│    └─OutputScale: 2-1815               --                        --
│    └─Empty: 2-1816                     [64, 64, 3, 3]            --
│    └─Empty: 2-1817                     [64, 64, 3, 3]            --
│    └─Empty: 2-1818                     [64]                      --
│    └─Empty: 2-1819                     [64]                      --
│    └─BatchNorm2d: 2-1820               [16, 64, 2, 2]            --
│    └─Scaler: 2-1821                    [16, 64, 2, 2]            --
│    └─ReLU: 2-1822                      [16, 64, 2, 2]            --
│    └─Empty: 2-1823                     [16, 64, 2, 2]            --
│    └─Clamp: 2-1824                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-137               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1825        --                        --
│    └─One: 2-1826                       [1]                       --
│    └─OutputScale: 2-1827               --                        --
│    └─Empty: 2-1828                     [64, 48, 1, 1]            --
│    └─Empty: 2-1829                     [64, 48, 1, 1]            --
│    └─Empty: 2-1830                     [64]                      --
│    └─Empty: 2-1831                     [64]                      --
│    └─BatchNorm2d: 2-1832               [16, 64, 64, 64]          --
│    └─Scaler: 2-1833                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1834                      [16, 64, 64, 64]          --
│    └─Empty: 2-1835                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1836                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-138               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1837        --                        --
│    └─One: 2-1838                       [1]                       --
│    └─OutputScale: 2-1839               --                        --
│    └─Empty: 2-1840                     [64, 64, 3, 3]            --
│    └─Empty: 2-1841                     [64, 64, 3, 3]            --
│    └─Empty: 2-1842                     [64]                      --
│    └─Empty: 2-1843                     [64]                      --
│    └─BatchNorm2d: 2-1844               [16, 64, 64, 64]          --
│    └─Scaler: 2-1845                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1846                      [16, 64, 64, 64]          --
│    └─Empty: 2-1847                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1848                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-139               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1849        --                        --
│    └─One: 2-1850                       [1]                       --
│    └─OutputScale: 2-1851               --                        --
│    └─Empty: 2-1852                     [64, 64, 1, 1]            --
│    └─Empty: 2-1853                     [64, 64, 1, 1]            --
│    └─Empty: 2-1854                     [64]                      --
│    └─Empty: 2-1855                     [64]                      --
│    └─BatchNorm2d: 2-1856               [16, 64, 64, 64]          --
│    └─Scaler: 2-1857                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1858                      [16, 64, 64, 64]          --
│    └─Empty: 2-1859                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1860                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-140               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-1861        --                        --
│    └─One: 2-1862                       [1]                       --
│    └─OutputScale: 2-1863               --                        --
│    └─Empty: 2-1864                     [64, 64, 3, 3]            --
│    └─Empty: 2-1865                     [64, 64, 3, 3]            --
│    └─Empty: 2-1866                     [64]                      --
│    └─Empty: 2-1867                     [64]                      --
│    └─BatchNorm2d: 2-1868               [16, 64, 64, 64]          --
│    └─Scaler: 2-1869                    [16, 64, 64, 64]          --
│    └─ReLU: 2-1870                      [16, 64, 64, 64]          --
│    └─Empty: 2-1871                     [16, 64, 64, 64]          --
│    └─Clamp: 2-1872                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-141        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-1873                 [16, 64, 32, 32]          --
│    └─Empty: 2-1874                     [16, 64, 32, 32]          --
│    └─Empty: 2-1875                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-1876        --                        --
│    └─One: 2-1877                       [1]                       --
│    └─OutputScale: 2-1878               --                        --
│    └─Empty: 2-1879                     [64, 64, 3, 3]            --
│    └─Empty: 2-1880                     [64, 64, 3, 3]            --
│    └─Empty: 2-1881                     [64]                      --
│    └─Empty: 2-1882                     [64]                      --
│    └─BatchNorm2d: 2-1883               [16, 64, 32, 32]          --
│    └─Scaler: 2-1884                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1885                      [16, 64, 32, 32]          --
│    └─Empty: 2-1886                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1887                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-142               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-1888        --                        --
│    └─One: 2-1889                       [1]                       --
│    └─OutputScale: 2-1890               --                        --
│    └─Empty: 2-1891                     [64, 64, 3, 3]            --
│    └─Empty: 2-1892                     [64, 64, 3, 3]            --
│    └─Empty: 2-1893                     [64]                      --
│    └─Empty: 2-1894                     [64]                      --
│    └─BatchNorm2d: 2-1895               [16, 64, 32, 32]          --
│    └─Scaler: 2-1896                    [16, 64, 32, 32]          --
│    └─ReLU: 2-1897                      [16, 64, 32, 32]          --
│    └─Empty: 2-1898                     [16, 64, 32, 32]          --
│    └─Clamp: 2-1899                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-143        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-1900                 [16, 64, 16, 16]          --
│    └─Empty: 2-1901                     [16, 64, 16, 16]          --
│    └─Empty: 2-1902                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-1903        --                        --
│    └─One: 2-1904                       [1]                       --
│    └─OutputScale: 2-1905               --                        --
│    └─Empty: 2-1906                     [64, 64, 3, 3]            --
│    └─Empty: 2-1907                     [64, 64, 3, 3]            --
│    └─Empty: 2-1908                     [64]                      --
│    └─Empty: 2-1909                     [64]                      --
│    └─BatchNorm2d: 2-1910               [16, 64, 16, 16]          --
│    └─Scaler: 2-1911                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1912                      [16, 64, 16, 16]          --
│    └─Empty: 2-1913                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1914                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-144               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-1915        --                        --
│    └─One: 2-1916                       [1]                       --
│    └─OutputScale: 2-1917               --                        --
│    └─Empty: 2-1918                     [64, 64, 3, 3]            --
│    └─Empty: 2-1919                     [64, 64, 3, 3]            --
│    └─Empty: 2-1920                     [64]                      --
│    └─Empty: 2-1921                     [64]                      --
│    └─BatchNorm2d: 2-1922               [16, 64, 16, 16]          --
│    └─Scaler: 2-1923                    [16, 64, 16, 16]          --
│    └─ReLU: 2-1924                      [16, 64, 16, 16]          --
│    └─Empty: 2-1925                     [16, 64, 16, 16]          --
│    └─Clamp: 2-1926                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-145        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1927                 [16, 64, 8, 8]            --
│    └─Empty: 2-1928                     [16, 64, 8, 8]            --
│    └─Empty: 2-1929                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1930        --                        --
│    └─One: 2-1931                       [1]                       --
│    └─OutputScale: 2-1932               --                        --
│    └─Empty: 2-1933                     [64, 64, 3, 3]            --
│    └─Empty: 2-1934                     [64, 64, 3, 3]            --
│    └─Empty: 2-1935                     [64]                      --
│    └─Empty: 2-1936                     [64]                      --
│    └─BatchNorm2d: 2-1937               [16, 64, 8, 8]            --
│    └─Scaler: 2-1938                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1939                      [16, 64, 8, 8]            --
│    └─Empty: 2-1940                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1941                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-146               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-1942        --                        --
│    └─One: 2-1943                       [1]                       --
│    └─OutputScale: 2-1944               --                        --
│    └─Empty: 2-1945                     [64, 64, 1, 1]            --
│    └─Empty: 2-1946                     [64, 64, 1, 1]            --
│    └─Empty: 2-1947                     [64]                      --
│    └─Empty: 2-1948                     [64]                      --
│    └─BatchNorm2d: 2-1949               [16, 64, 8, 8]            --
│    └─Scaler: 2-1950                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1951                      [16, 64, 8, 8]            --
│    └─Empty: 2-1952                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1953                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-147        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-1954                 [16, 64, 8, 8]            --
│    └─Empty: 2-1955                     [16, 64, 8, 8]            --
│    └─Empty: 2-1956                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-1957        --                        --
│    └─One: 2-1958                       [1]                       --
│    └─OutputScale: 2-1959               --                        --
│    └─Empty: 2-1960                     [64, 64, 3, 3]            --
│    └─Empty: 2-1961                     [64, 64, 3, 3]            --
│    └─Empty: 2-1962                     [64]                      --
│    └─Empty: 2-1963                     [64]                      --
│    └─BatchNorm2d: 2-1964               [16, 64, 8, 8]            --
│    └─Scaler: 2-1965                    [16, 64, 8, 8]            --
│    └─ReLU: 2-1966                      [16, 64, 8, 8]            --
│    └─Empty: 2-1967                     [16, 64, 8, 8]            --
│    └─Clamp: 2-1968                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-148        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1969                 [16, 64, 4, 4]            --
│    └─Empty: 2-1970                     [16, 64, 4, 4]            --
│    └─Empty: 2-1971                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1972        --                        --
│    └─One: 2-1973                       [1]                       --
│    └─OutputScale: 2-1974               --                        --
│    └─Empty: 2-1975                     [64, 64, 3, 3]            --
│    └─Empty: 2-1976                     [64, 64, 3, 3]            --
│    └─Empty: 2-1977                     [64]                      --
│    └─Empty: 2-1978                     [64]                      --
│    └─BatchNorm2d: 2-1979               [16, 64, 4, 4]            --
│    └─Scaler: 2-1980                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1981                      [16, 64, 4, 4]            --
│    └─Empty: 2-1982                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1983                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-149               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-1984        --                        --
│    └─One: 2-1985                       [1]                       --
│    └─OutputScale: 2-1986               --                        --
│    └─Empty: 2-1987                     [64, 64, 1, 1]            --
│    └─Empty: 2-1988                     [64, 64, 1, 1]            --
│    └─Empty: 2-1989                     [64]                      --
│    └─Empty: 2-1990                     [64]                      --
│    └─BatchNorm2d: 2-1991               [16, 64, 4, 4]            --
│    └─Scaler: 2-1992                    [16, 64, 4, 4]            --
│    └─ReLU: 2-1993                      [16, 64, 4, 4]            --
│    └─Empty: 2-1994                     [16, 64, 4, 4]            --
│    └─Clamp: 2-1995                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-150        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-1996                 [16, 64, 4, 4]            --
│    └─Empty: 2-1997                     [16, 64, 4, 4]            --
│    └─Empty: 2-1998                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-1999        --                        --
│    └─One: 2-2000                       [1]                       --
│    └─OutputScale: 2-2001               --                        --
│    └─Empty: 2-2002                     [64, 64, 3, 3]            --
│    └─Empty: 2-2003                     [64, 64, 3, 3]            --
│    └─Empty: 2-2004                     [64]                      --
│    └─Empty: 2-2005                     [64]                      --
│    └─BatchNorm2d: 2-2006               [16, 64, 4, 4]            --
│    └─Scaler: 2-2007                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2008                      [16, 64, 4, 4]            --
│    └─Empty: 2-2009                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2010                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-151        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2011                 [16, 64, 2, 2]            --
│    └─Empty: 2-2012                     [16, 64, 2, 2]            --
│    └─Empty: 2-2013                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2014        --                        --
│    └─One: 2-2015                       [1]                       --
│    └─OutputScale: 2-2016               --                        --
│    └─Empty: 2-2017                     [64, 64, 1, 1]            --
│    └─Empty: 2-2018                     [64, 64, 1, 1]            --
│    └─Empty: 2-2019                     [64]                      --
│    └─Empty: 2-2020                     [64]                      --
│    └─BatchNorm2d: 2-2021               [16, 64, 2, 2]            --
│    └─Scaler: 2-2022                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2023                      [16, 64, 2, 2]            --
│    └─Empty: 2-2024                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2025                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-152               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2026        --                        --
│    └─One: 2-2027                       [1]                       --
│    └─OutputScale: 2-2028               --                        --
│    └─Empty: 2-2029                     [64, 64, 1, 1]            --
│    └─Empty: 2-2030                     [64, 64, 1, 1]            --
│    └─Empty: 2-2031                     [64]                      --
│    └─Empty: 2-2032                     [64]                      --
│    └─BatchNorm2d: 2-2033               [16, 64, 2, 2]            --
│    └─Scaler: 2-2034                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2035                      [16, 64, 2, 2]            --
│    └─Empty: 2-2036                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2037                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-153        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2038                 [16, 64, 2, 2]            --
│    └─Empty: 2-2039                     [16, 64, 2, 2]            --
│    └─Empty: 2-2040                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2041        --                        --
│    └─One: 2-2042                       [1]                       --
│    └─OutputScale: 2-2043               --                        --
│    └─Empty: 2-2044                     [64, 64, 3, 3]            --
│    └─Empty: 2-2045                     [64, 64, 3, 3]            --
│    └─Empty: 2-2046                     [64]                      --
│    └─Empty: 2-2047                     [64]                      --
│    └─BatchNorm2d: 2-2048               [16, 64, 2, 2]            --
│    └─Scaler: 2-2049                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2050                      [16, 64, 2, 2]            --
│    └─Empty: 2-2051                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2052                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-154               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2053        --                        --
│    └─One: 2-2054                       [1]                       --
│    └─OutputScale: 2-2055               --                        --
│    └─Empty: 2-2056                     [64, 48, 1, 1]            --
│    └─Empty: 2-2057                     [64, 48, 1, 1]            --
│    └─Empty: 2-2058                     [64]                      --
│    └─Empty: 2-2059                     [64]                      --
│    └─BatchNorm2d: 2-2060               [16, 64, 64, 64]          --
│    └─Scaler: 2-2061                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2062                      [16, 64, 64, 64]          --
│    └─Empty: 2-2063                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2064                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-155               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2065        --                        --
│    └─One: 2-2066                       [1]                       --
│    └─OutputScale: 2-2067               --                        --
│    └─Empty: 2-2068                     [64, 64, 3, 3]            --
│    └─Empty: 2-2069                     [64, 64, 3, 3]            --
│    └─Empty: 2-2070                     [64]                      --
│    └─Empty: 2-2071                     [64]                      --
│    └─BatchNorm2d: 2-2072               [16, 64, 64, 64]          --
│    └─Scaler: 2-2073                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2074                      [16, 64, 64, 64]          --
│    └─Empty: 2-2075                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2076                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-156               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2077        --                        --
│    └─One: 2-2078                       [1]                       --
│    └─OutputScale: 2-2079               --                        --
│    └─Empty: 2-2080                     [64, 64, 1, 1]            --
│    └─Empty: 2-2081                     [64, 64, 1, 1]            --
│    └─Empty: 2-2082                     [64]                      --
│    └─Empty: 2-2083                     [64]                      --
│    └─BatchNorm2d: 2-2084               [16, 64, 64, 64]          --
│    └─Scaler: 2-2085                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2086                      [16, 64, 64, 64]          --
│    └─Empty: 2-2087                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2088                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-157               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2089        --                        --
│    └─One: 2-2090                       [1]                       --
│    └─OutputScale: 2-2091               --                        --
│    └─Empty: 2-2092                     [64, 64, 3, 3]            --
│    └─Empty: 2-2093                     [64, 64, 3, 3]            --
│    └─Empty: 2-2094                     [64]                      --
│    └─Empty: 2-2095                     [64]                      --
│    └─BatchNorm2d: 2-2096               [16, 64, 64, 64]          --
│    └─Scaler: 2-2097                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2098                      [16, 64, 64, 64]          --
│    └─Empty: 2-2099                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2100                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-158        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2101                 [16, 64, 32, 32]          --
│    └─Empty: 2-2102                     [16, 64, 32, 32]          --
│    └─Empty: 2-2103                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2104        --                        --
│    └─One: 2-2105                       [1]                       --
│    └─OutputScale: 2-2106               --                        --
│    └─Empty: 2-2107                     [64, 64, 3, 3]            --
│    └─Empty: 2-2108                     [64, 64, 3, 3]            --
│    └─Empty: 2-2109                     [64]                      --
│    └─Empty: 2-2110                     [64]                      --
│    └─BatchNorm2d: 2-2111               [16, 64, 32, 32]          --
│    └─Scaler: 2-2112                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2113                      [16, 64, 32, 32]          --
│    └─Empty: 2-2114                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2115                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-159               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2116        --                        --
│    └─One: 2-2117                       [1]                       --
│    └─OutputScale: 2-2118               --                        --
│    └─Empty: 2-2119                     [64, 64, 3, 3]            --
│    └─Empty: 2-2120                     [64, 64, 3, 3]            --
│    └─Empty: 2-2121                     [64]                      --
│    └─Empty: 2-2122                     [64]                      --
│    └─BatchNorm2d: 2-2123               [16, 64, 32, 32]          --
│    └─Scaler: 2-2124                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2125                      [16, 64, 32, 32]          --
│    └─Empty: 2-2126                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2127                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-160        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2128                 [16, 64, 16, 16]          --
│    └─Empty: 2-2129                     [16, 64, 16, 16]          --
│    └─Empty: 2-2130                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2131        --                        --
│    └─One: 2-2132                       [1]                       --
│    └─OutputScale: 2-2133               --                        --
│    └─Empty: 2-2134                     [64, 64, 3, 3]            --
│    └─Empty: 2-2135                     [64, 64, 3, 3]            --
│    └─Empty: 2-2136                     [64]                      --
│    └─Empty: 2-2137                     [64]                      --
│    └─BatchNorm2d: 2-2138               [16, 64, 16, 16]          --
│    └─Scaler: 2-2139                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2140                      [16, 64, 16, 16]          --
│    └─Empty: 2-2141                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2142                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-161               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2143        --                        --
│    └─One: 2-2144                       [1]                       --
│    └─OutputScale: 2-2145               --                        --
│    └─Empty: 2-2146                     [64, 64, 3, 3]            --
│    └─Empty: 2-2147                     [64, 64, 3, 3]            --
│    └─Empty: 2-2148                     [64]                      --
│    └─Empty: 2-2149                     [64]                      --
│    └─BatchNorm2d: 2-2150               [16, 64, 16, 16]          --
│    └─Scaler: 2-2151                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2152                      [16, 64, 16, 16]          --
│    └─Empty: 2-2153                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2154                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-162        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2155                 [16, 64, 8, 8]            --
│    └─Empty: 2-2156                     [16, 64, 8, 8]            --
│    └─Empty: 2-2157                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2158        --                        --
│    └─One: 2-2159                       [1]                       --
│    └─OutputScale: 2-2160               --                        --
│    └─Empty: 2-2161                     [64, 64, 3, 3]            --
│    └─Empty: 2-2162                     [64, 64, 3, 3]            --
│    └─Empty: 2-2163                     [64]                      --
│    └─Empty: 2-2164                     [64]                      --
│    └─BatchNorm2d: 2-2165               [16, 64, 8, 8]            --
│    └─Scaler: 2-2166                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2167                      [16, 64, 8, 8]            --
│    └─Empty: 2-2168                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2169                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-163               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2170        --                        --
│    └─One: 2-2171                       [1]                       --
│    └─OutputScale: 2-2172               --                        --
│    └─Empty: 2-2173                     [64, 64, 1, 1]            --
│    └─Empty: 2-2174                     [64, 64, 1, 1]            --
│    └─Empty: 2-2175                     [64]                      --
│    └─Empty: 2-2176                     [64]                      --
│    └─BatchNorm2d: 2-2177               [16, 64, 8, 8]            --
│    └─Scaler: 2-2178                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2179                      [16, 64, 8, 8]            --
│    └─Empty: 2-2180                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2181                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-164        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2182                 [16, 64, 8, 8]            --
│    └─Empty: 2-2183                     [16, 64, 8, 8]            --
│    └─Empty: 2-2184                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2185        --                        --
│    └─One: 2-2186                       [1]                       --
│    └─OutputScale: 2-2187               --                        --
│    └─Empty: 2-2188                     [64, 64, 3, 3]            --
│    └─Empty: 2-2189                     [64, 64, 3, 3]            --
│    └─Empty: 2-2190                     [64]                      --
│    └─Empty: 2-2191                     [64]                      --
│    └─BatchNorm2d: 2-2192               [16, 64, 8, 8]            --
│    └─Scaler: 2-2193                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2194                      [16, 64, 8, 8]            --
│    └─Empty: 2-2195                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2196                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-165        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2197                 [16, 64, 4, 4]            --
│    └─Empty: 2-2198                     [16, 64, 4, 4]            --
│    └─Empty: 2-2199                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2200        --                        --
│    └─One: 2-2201                       [1]                       --
│    └─OutputScale: 2-2202               --                        --
│    └─Empty: 2-2203                     [64, 64, 3, 3]            --
│    └─Empty: 2-2204                     [64, 64, 3, 3]            --
│    └─Empty: 2-2205                     [64]                      --
│    └─Empty: 2-2206                     [64]                      --
│    └─BatchNorm2d: 2-2207               [16, 64, 4, 4]            --
│    └─Scaler: 2-2208                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2209                      [16, 64, 4, 4]            --
│    └─Empty: 2-2210                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2211                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-166               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-2212        --                        --
│    └─One: 2-2213                       [1]                       --
│    └─OutputScale: 2-2214               --                        --
│    └─Empty: 2-2215                     [64, 64, 1, 1]            --
│    └─Empty: 2-2216                     [64, 64, 1, 1]            --
│    └─Empty: 2-2217                     [64]                      --
│    └─Empty: 2-2218                     [64]                      --
│    └─BatchNorm2d: 2-2219               [16, 64, 4, 4]            --
│    └─Scaler: 2-2220                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2221                      [16, 64, 4, 4]            --
│    └─Empty: 2-2222                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2223                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-167        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2224                 [16, 64, 4, 4]            --
│    └─Empty: 2-2225                     [16, 64, 4, 4]            --
│    └─Empty: 2-2226                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2227        --                        --
│    └─One: 2-2228                       [1]                       --
│    └─OutputScale: 2-2229               --                        --
│    └─Empty: 2-2230                     [64, 64, 3, 3]            --
│    └─Empty: 2-2231                     [64, 64, 3, 3]            --
│    └─Empty: 2-2232                     [64]                      --
│    └─Empty: 2-2233                     [64]                      --
│    └─BatchNorm2d: 2-2234               [16, 64, 4, 4]            --
│    └─Scaler: 2-2235                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2236                      [16, 64, 4, 4]            --
│    └─Empty: 2-2237                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2238                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-168        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2239                 [16, 64, 2, 2]            --
│    └─Empty: 2-2240                     [16, 64, 2, 2]            --
│    └─Empty: 2-2241                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2242        --                        --
│    └─One: 2-2243                       [1]                       --
│    └─OutputScale: 2-2244               --                        --
│    └─Empty: 2-2245                     [64, 64, 1, 1]            --
│    └─Empty: 2-2246                     [64, 64, 1, 1]            --
│    └─Empty: 2-2247                     [64]                      --
│    └─Empty: 2-2248                     [64]                      --
│    └─BatchNorm2d: 2-2249               [16, 64, 2, 2]            --
│    └─Scaler: 2-2250                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2251                      [16, 64, 2, 2]            --
│    └─Empty: 2-2252                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2253                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-169               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2254        --                        --
│    └─One: 2-2255                       [1]                       --
│    └─OutputScale: 2-2256               --                        --
│    └─Empty: 2-2257                     [64, 64, 1, 1]            --
│    └─Empty: 2-2258                     [64, 64, 1, 1]            --
│    └─Empty: 2-2259                     [64]                      --
│    └─Empty: 2-2260                     [64]                      --
│    └─BatchNorm2d: 2-2261               [16, 64, 2, 2]            --
│    └─Scaler: 2-2262                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2263                      [16, 64, 2, 2]            --
│    └─Empty: 2-2264                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2265                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-170        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2266                 [16, 64, 2, 2]            --
│    └─Empty: 2-2267                     [16, 64, 2, 2]            --
│    └─Empty: 2-2268                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2269        --                        --
│    └─One: 2-2270                       [1]                       --
│    └─OutputScale: 2-2271               --                        --
│    └─Empty: 2-2272                     [64, 64, 3, 3]            --
│    └─Empty: 2-2273                     [64, 64, 3, 3]            --
│    └─Empty: 2-2274                     [64]                      --
│    └─Empty: 2-2275                     [64]                      --
│    └─BatchNorm2d: 2-2276               [16, 64, 2, 2]            --
│    └─Scaler: 2-2277                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2278                      [16, 64, 2, 2]            --
│    └─Empty: 2-2279                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2280                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-171               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2281        --                        --
│    └─One: 2-2282                       [1]                       --
│    └─OutputScale: 2-2283               --                        --
│    └─Empty: 2-2284                     [64, 48, 1, 1]            --
│    └─Empty: 2-2285                     [64, 48, 1, 1]            --
│    └─Empty: 2-2286                     [64]                      --
│    └─Empty: 2-2287                     [64]                      --
│    └─BatchNorm2d: 2-2288               [16, 64, 64, 64]          --
│    └─Scaler: 2-2289                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2290                      [16, 64, 64, 64]          --
│    └─Empty: 2-2291                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2292                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-172               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2293        --                        --
│    └─One: 2-2294                       [1]                       --
│    └─OutputScale: 2-2295               --                        --
│    └─Empty: 2-2296                     [64, 64, 3, 3]            --
│    └─Empty: 2-2297                     [64, 64, 3, 3]            --
│    └─Empty: 2-2298                     [64]                      --
│    └─Empty: 2-2299                     [64]                      --
│    └─BatchNorm2d: 2-2300               [16, 64, 64, 64]          --
│    └─Scaler: 2-2301                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2302                      [16, 64, 64, 64]          --
│    └─Empty: 2-2303                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2304                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-173               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2305        --                        --
│    └─One: 2-2306                       [1]                       --
│    └─OutputScale: 2-2307               --                        --
│    └─Empty: 2-2308                     [64, 64, 1, 1]            --
│    └─Empty: 2-2309                     [64, 64, 1, 1]            --
│    └─Empty: 2-2310                     [64]                      --
│    └─Empty: 2-2311                     [64]                      --
│    └─BatchNorm2d: 2-2312               [16, 64, 64, 64]          --
│    └─Scaler: 2-2313                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2314                      [16, 64, 64, 64]          --
│    └─Empty: 2-2315                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2316                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-174               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2317        --                        --
│    └─One: 2-2318                       [1]                       --
│    └─OutputScale: 2-2319               --                        --
│    └─Empty: 2-2320                     [64, 64, 3, 3]            --
│    └─Empty: 2-2321                     [64, 64, 3, 3]            --
│    └─Empty: 2-2322                     [64]                      --
│    └─Empty: 2-2323                     [64]                      --
│    └─BatchNorm2d: 2-2324               [16, 64, 64, 64]          --
│    └─Scaler: 2-2325                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2326                      [16, 64, 64, 64]          --
│    └─Empty: 2-2327                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2328                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-175        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2329                 [16, 64, 32, 32]          --
│    └─Empty: 2-2330                     [16, 64, 32, 32]          --
│    └─Empty: 2-2331                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2332        --                        --
│    └─One: 2-2333                       [1]                       --
│    └─OutputScale: 2-2334               --                        --
│    └─Empty: 2-2335                     [64, 64, 3, 3]            --
│    └─Empty: 2-2336                     [64, 64, 3, 3]            --
│    └─Empty: 2-2337                     [64]                      --
│    └─Empty: 2-2338                     [64]                      --
│    └─BatchNorm2d: 2-2339               [16, 64, 32, 32]          --
│    └─Scaler: 2-2340                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2341                      [16, 64, 32, 32]          --
│    └─Empty: 2-2342                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2343                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-176               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2344        --                        --
│    └─One: 2-2345                       [1]                       --
│    └─OutputScale: 2-2346               --                        --
│    └─Empty: 2-2347                     [64, 64, 3, 3]            --
│    └─Empty: 2-2348                     [64, 64, 3, 3]            --
│    └─Empty: 2-2349                     [64]                      --
│    └─Empty: 2-2350                     [64]                      --
│    └─BatchNorm2d: 2-2351               [16, 64, 32, 32]          --
│    └─Scaler: 2-2352                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2353                      [16, 64, 32, 32]          --
│    └─Empty: 2-2354                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2355                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-177        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2356                 [16, 64, 16, 16]          --
│    └─Empty: 2-2357                     [16, 64, 16, 16]          --
│    └─Empty: 2-2358                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2359        --                        --
│    └─One: 2-2360                       [1]                       --
│    └─OutputScale: 2-2361               --                        --
│    └─Empty: 2-2362                     [64, 64, 3, 3]            --
│    └─Empty: 2-2363                     [64, 64, 3, 3]            --
│    └─Empty: 2-2364                     [64]                      --
│    └─Empty: 2-2365                     [64]                      --
│    └─BatchNorm2d: 2-2366               [16, 64, 16, 16]          --
│    └─Scaler: 2-2367                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2368                      [16, 64, 16, 16]          --
│    └─Empty: 2-2369                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2370                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-178               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2371        --                        --
│    └─One: 2-2372                       [1]                       --
│    └─OutputScale: 2-2373               --                        --
│    └─Empty: 2-2374                     [64, 64, 3, 3]            --
│    └─Empty: 2-2375                     [64, 64, 3, 3]            --
│    └─Empty: 2-2376                     [64]                      --
│    └─Empty: 2-2377                     [64]                      --
│    └─BatchNorm2d: 2-2378               [16, 64, 16, 16]          --
│    └─Scaler: 2-2379                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2380                      [16, 64, 16, 16]          --
│    └─Empty: 2-2381                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2382                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-179        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2383                 [16, 64, 8, 8]            --
│    └─Empty: 2-2384                     [16, 64, 8, 8]            --
│    └─Empty: 2-2385                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2386        --                        --
│    └─One: 2-2387                       [1]                       --
│    └─OutputScale: 2-2388               --                        --
│    └─Empty: 2-2389                     [64, 64, 3, 3]            --
│    └─Empty: 2-2390                     [64, 64, 3, 3]            --
│    └─Empty: 2-2391                     [64]                      --
│    └─Empty: 2-2392                     [64]                      --
│    └─BatchNorm2d: 2-2393               [16, 64, 8, 8]            --
│    └─Scaler: 2-2394                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2395                      [16, 64, 8, 8]            --
│    └─Empty: 2-2396                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2397                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-180               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2398        --                        --
│    └─One: 2-2399                       [1]                       --
│    └─OutputScale: 2-2400               --                        --
│    └─Empty: 2-2401                     [64, 64, 1, 1]            --
│    └─Empty: 2-2402                     [64, 64, 1, 1]            --
│    └─Empty: 2-2403                     [64]                      --
│    └─Empty: 2-2404                     [64]                      --
│    └─BatchNorm2d: 2-2405               [16, 64, 8, 8]            --
│    └─Scaler: 2-2406                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2407                      [16, 64, 8, 8]            --
│    └─Empty: 2-2408                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2409                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-181        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2410                 [16, 64, 8, 8]            --
│    └─Empty: 2-2411                     [16, 64, 8, 8]            --
│    └─Empty: 2-2412                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2413        --                        --
│    └─One: 2-2414                       [1]                       --
│    └─OutputScale: 2-2415               --                        --
│    └─Empty: 2-2416                     [64, 64, 3, 3]            --
│    └─Empty: 2-2417                     [64, 64, 3, 3]            --
│    └─Empty: 2-2418                     [64]                      --
│    └─Empty: 2-2419                     [64]                      --
│    └─BatchNorm2d: 2-2420               [16, 64, 8, 8]            --
│    └─Scaler: 2-2421                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2422                      [16, 64, 8, 8]            --
│    └─Empty: 2-2423                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2424                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-182        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2425                 [16, 64, 4, 4]            --
│    └─Empty: 2-2426                     [16, 64, 4, 4]            --
│    └─Empty: 2-2427                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2428        --                        --
│    └─One: 2-2429                       [1]                       --
│    └─OutputScale: 2-2430               --                        --
│    └─Empty: 2-2431                     [64, 64, 3, 3]            --
│    └─Empty: 2-2432                     [64, 64, 3, 3]            --
│    └─Empty: 2-2433                     [64]                      --
│    └─Empty: 2-2434                     [64]                      --
│    └─BatchNorm2d: 2-2435               [16, 64, 4, 4]            --
│    └─Scaler: 2-2436                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2437                      [16, 64, 4, 4]            --
│    └─Empty: 2-2438                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2439                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-183               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-2440        --                        --
│    └─One: 2-2441                       [1]                       --
│    └─OutputScale: 2-2442               --                        --
│    └─Empty: 2-2443                     [64, 64, 1, 1]            --
│    └─Empty: 2-2444                     [64, 64, 1, 1]            --
│    └─Empty: 2-2445                     [64]                      --
│    └─Empty: 2-2446                     [64]                      --
│    └─BatchNorm2d: 2-2447               [16, 64, 4, 4]            --
│    └─Scaler: 2-2448                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2449                      [16, 64, 4, 4]            --
│    └─Empty: 2-2450                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2451                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-184        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2452                 [16, 64, 4, 4]            --
│    └─Empty: 2-2453                     [16, 64, 4, 4]            --
│    └─Empty: 2-2454                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2455        --                        --
│    └─One: 2-2456                       [1]                       --
│    └─OutputScale: 2-2457               --                        --
│    └─Empty: 2-2458                     [64, 64, 3, 3]            --
│    └─Empty: 2-2459                     [64, 64, 3, 3]            --
│    └─Empty: 2-2460                     [64]                      --
│    └─Empty: 2-2461                     [64]                      --
│    └─BatchNorm2d: 2-2462               [16, 64, 4, 4]            --
│    └─Scaler: 2-2463                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2464                      [16, 64, 4, 4]            --
│    └─Empty: 2-2465                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2466                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-185        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2467                 [16, 64, 2, 2]            --
│    └─Empty: 2-2468                     [16, 64, 2, 2]            --
│    └─Empty: 2-2469                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2470        --                        --
│    └─One: 2-2471                       [1]                       --
│    └─OutputScale: 2-2472               --                        --
│    └─Empty: 2-2473                     [64, 64, 1, 1]            --
│    └─Empty: 2-2474                     [64, 64, 1, 1]            --
│    └─Empty: 2-2475                     [64]                      --
│    └─Empty: 2-2476                     [64]                      --
│    └─BatchNorm2d: 2-2477               [16, 64, 2, 2]            --
│    └─Scaler: 2-2478                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2479                      [16, 64, 2, 2]            --
│    └─Empty: 2-2480                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2481                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-186               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2482        --                        --
│    └─One: 2-2483                       [1]                       --
│    └─OutputScale: 2-2484               --                        --
│    └─Empty: 2-2485                     [64, 64, 1, 1]            --
│    └─Empty: 2-2486                     [64, 64, 1, 1]            --
│    └─Empty: 2-2487                     [64]                      --
│    └─Empty: 2-2488                     [64]                      --
│    └─BatchNorm2d: 2-2489               [16, 64, 2, 2]            --
│    └─Scaler: 2-2490                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2491                      [16, 64, 2, 2]            --
│    └─Empty: 2-2492                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2493                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-187        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2494                 [16, 64, 2, 2]            --
│    └─Empty: 2-2495                     [16, 64, 2, 2]            --
│    └─Empty: 2-2496                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2497        --                        --
│    └─One: 2-2498                       [1]                       --
│    └─OutputScale: 2-2499               --                        --
│    └─Empty: 2-2500                     [64, 64, 3, 3]            --
│    └─Empty: 2-2501                     [64, 64, 3, 3]            --
│    └─Empty: 2-2502                     [64]                      --
│    └─Empty: 2-2503                     [64]                      --
│    └─BatchNorm2d: 2-2504               [16, 64, 2, 2]            --
│    └─Scaler: 2-2505                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2506                      [16, 64, 2, 2]            --
│    └─Empty: 2-2507                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2508                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-188               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2509        --                        --
│    └─One: 2-2510                       [1]                       --
│    └─OutputScale: 2-2511               --                        --
│    └─Empty: 2-2512                     [64, 48, 1, 1]            --
│    └─Empty: 2-2513                     [64, 48, 1, 1]            --
│    └─Empty: 2-2514                     [64]                      --
│    └─Empty: 2-2515                     [64]                      --
│    └─BatchNorm2d: 2-2516               [16, 64, 64, 64]          --
│    └─Scaler: 2-2517                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2518                      [16, 64, 64, 64]          --
│    └─Empty: 2-2519                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2520                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-189               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2521        --                        --
│    └─One: 2-2522                       [1]                       --
│    └─OutputScale: 2-2523               --                        --
│    └─Empty: 2-2524                     [64, 64, 3, 3]            --
│    └─Empty: 2-2525                     [64, 64, 3, 3]            --
│    └─Empty: 2-2526                     [64]                      --
│    └─Empty: 2-2527                     [64]                      --
│    └─BatchNorm2d: 2-2528               [16, 64, 64, 64]          --
│    └─Scaler: 2-2529                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2530                      [16, 64, 64, 64]          --
│    └─Empty: 2-2531                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2532                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-190               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2533        --                        --
│    └─One: 2-2534                       [1]                       --
│    └─OutputScale: 2-2535               --                        --
│    └─Empty: 2-2536                     [64, 64, 1, 1]            --
│    └─Empty: 2-2537                     [64, 64, 1, 1]            --
│    └─Empty: 2-2538                     [64]                      --
│    └─Empty: 2-2539                     [64]                      --
│    └─BatchNorm2d: 2-2540               [16, 64, 64, 64]          --
│    └─Scaler: 2-2541                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2542                      [16, 64, 64, 64]          --
│    └─Empty: 2-2543                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2544                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-191               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2545        --                        --
│    └─One: 2-2546                       [1]                       --
│    └─OutputScale: 2-2547               --                        --
│    └─Empty: 2-2548                     [64, 64, 3, 3]            --
│    └─Empty: 2-2549                     [64, 64, 3, 3]            --
│    └─Empty: 2-2550                     [64]                      --
│    └─Empty: 2-2551                     [64]                      --
│    └─BatchNorm2d: 2-2552               [16, 64, 64, 64]          --
│    └─Scaler: 2-2553                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2554                      [16, 64, 64, 64]          --
│    └─Empty: 2-2555                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2556                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-192        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2557                 [16, 64, 32, 32]          --
│    └─Empty: 2-2558                     [16, 64, 32, 32]          --
│    └─Empty: 2-2559                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2560        --                        --
│    └─One: 2-2561                       [1]                       --
│    └─OutputScale: 2-2562               --                        --
│    └─Empty: 2-2563                     [64, 64, 3, 3]            --
│    └─Empty: 2-2564                     [64, 64, 3, 3]            --
│    └─Empty: 2-2565                     [64]                      --
│    └─Empty: 2-2566                     [64]                      --
│    └─BatchNorm2d: 2-2567               [16, 64, 32, 32]          --
│    └─Scaler: 2-2568                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2569                      [16, 64, 32, 32]          --
│    └─Empty: 2-2570                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2571                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-193               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2572        --                        --
│    └─One: 2-2573                       [1]                       --
│    └─OutputScale: 2-2574               --                        --
│    └─Empty: 2-2575                     [64, 64, 3, 3]            --
│    └─Empty: 2-2576                     [64, 64, 3, 3]            --
│    └─Empty: 2-2577                     [64]                      --
│    └─Empty: 2-2578                     [64]                      --
│    └─BatchNorm2d: 2-2579               [16, 64, 32, 32]          --
│    └─Scaler: 2-2580                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2581                      [16, 64, 32, 32]          --
│    └─Empty: 2-2582                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2583                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-194        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2584                 [16, 64, 16, 16]          --
│    └─Empty: 2-2585                     [16, 64, 16, 16]          --
│    └─Empty: 2-2586                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2587        --                        --
│    └─One: 2-2588                       [1]                       --
│    └─OutputScale: 2-2589               --                        --
│    └─Empty: 2-2590                     [64, 64, 3, 3]            --
│    └─Empty: 2-2591                     [64, 64, 3, 3]            --
│    └─Empty: 2-2592                     [64]                      --
│    └─Empty: 2-2593                     [64]                      --
│    └─BatchNorm2d: 2-2594               [16, 64, 16, 16]          --
│    └─Scaler: 2-2595                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2596                      [16, 64, 16, 16]          --
│    └─Empty: 2-2597                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2598                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-195               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2599        --                        --
│    └─One: 2-2600                       [1]                       --
│    └─OutputScale: 2-2601               --                        --
│    └─Empty: 2-2602                     [64, 64, 3, 3]            --
│    └─Empty: 2-2603                     [64, 64, 3, 3]            --
│    └─Empty: 2-2604                     [64]                      --
│    └─Empty: 2-2605                     [64]                      --
│    └─BatchNorm2d: 2-2606               [16, 64, 16, 16]          --
│    └─Scaler: 2-2607                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2608                      [16, 64, 16, 16]          --
│    └─Empty: 2-2609                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2610                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-196        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2611                 [16, 64, 8, 8]            --
│    └─Empty: 2-2612                     [16, 64, 8, 8]            --
│    └─Empty: 2-2613                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2614        --                        --
│    └─One: 2-2615                       [1]                       --
│    └─OutputScale: 2-2616               --                        --
│    └─Empty: 2-2617                     [64, 64, 3, 3]            --
│    └─Empty: 2-2618                     [64, 64, 3, 3]            --
│    └─Empty: 2-2619                     [64]                      --
│    └─Empty: 2-2620                     [64]                      --
│    └─BatchNorm2d: 2-2621               [16, 64, 8, 8]            --
│    └─Scaler: 2-2622                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2623                      [16, 64, 8, 8]            --
│    └─Empty: 2-2624                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2625                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-197               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2626        --                        --
│    └─One: 2-2627                       [1]                       --
│    └─OutputScale: 2-2628               --                        --
│    └─Empty: 2-2629                     [64, 64, 1, 1]            --
│    └─Empty: 2-2630                     [64, 64, 1, 1]            --
│    └─Empty: 2-2631                     [64]                      --
│    └─Empty: 2-2632                     [64]                      --
│    └─BatchNorm2d: 2-2633               [16, 64, 8, 8]            --
│    └─Scaler: 2-2634                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2635                      [16, 64, 8, 8]            --
│    └─Empty: 2-2636                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2637                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-198        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2638                 [16, 64, 8, 8]            --
│    └─Empty: 2-2639                     [16, 64, 8, 8]            --
│    └─Empty: 2-2640                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2641        --                        --
│    └─One: 2-2642                       [1]                       --
│    └─OutputScale: 2-2643               --                        --
│    └─Empty: 2-2644                     [64, 64, 3, 3]            --
│    └─Empty: 2-2645                     [64, 64, 3, 3]            --
│    └─Empty: 2-2646                     [64]                      --
│    └─Empty: 2-2647                     [64]                      --
│    └─BatchNorm2d: 2-2648               [16, 64, 8, 8]            --
│    └─Scaler: 2-2649                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2650                      [16, 64, 8, 8]            --
│    └─Empty: 2-2651                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2652                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-199        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2653                 [16, 64, 4, 4]            --
│    └─Empty: 2-2654                     [16, 64, 4, 4]            --
│    └─Empty: 2-2655                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2656        --                        --
│    └─One: 2-2657                       [1]                       --
│    └─OutputScale: 2-2658               --                        --
│    └─Empty: 2-2659                     [64, 64, 3, 3]            --
│    └─Empty: 2-2660                     [64, 64, 3, 3]            --
│    └─Empty: 2-2661                     [64]                      --
│    └─Empty: 2-2662                     [64]                      --
│    └─BatchNorm2d: 2-2663               [16, 64, 4, 4]            --
│    └─Scaler: 2-2664                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2665                      [16, 64, 4, 4]            --
│    └─Empty: 2-2666                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2667                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-200               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-2668        --                        --
│    └─One: 2-2669                       [1]                       --
│    └─OutputScale: 2-2670               --                        --
│    └─Empty: 2-2671                     [64, 64, 1, 1]            --
│    └─Empty: 2-2672                     [64, 64, 1, 1]            --
│    └─Empty: 2-2673                     [64]                      --
│    └─Empty: 2-2674                     [64]                      --
│    └─BatchNorm2d: 2-2675               [16, 64, 4, 4]            --
│    └─Scaler: 2-2676                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2677                      [16, 64, 4, 4]            --
│    └─Empty: 2-2678                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2679                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-201        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2680                 [16, 64, 4, 4]            --
│    └─Empty: 2-2681                     [16, 64, 4, 4]            --
│    └─Empty: 2-2682                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2683        --                        --
│    └─One: 2-2684                       [1]                       --
│    └─OutputScale: 2-2685               --                        --
│    └─Empty: 2-2686                     [64, 64, 3, 3]            --
│    └─Empty: 2-2687                     [64, 64, 3, 3]            --
│    └─Empty: 2-2688                     [64]                      --
│    └─Empty: 2-2689                     [64]                      --
│    └─BatchNorm2d: 2-2690               [16, 64, 4, 4]            --
│    └─Scaler: 2-2691                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2692                      [16, 64, 4, 4]            --
│    └─Empty: 2-2693                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2694                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-202        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2695                 [16, 64, 2, 2]            --
│    └─Empty: 2-2696                     [16, 64, 2, 2]            --
│    └─Empty: 2-2697                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2698        --                        --
│    └─One: 2-2699                       [1]                       --
│    └─OutputScale: 2-2700               --                        --
│    └─Empty: 2-2701                     [64, 64, 1, 1]            --
│    └─Empty: 2-2702                     [64, 64, 1, 1]            --
│    └─Empty: 2-2703                     [64]                      --
│    └─Empty: 2-2704                     [64]                      --
│    └─BatchNorm2d: 2-2705               [16, 64, 2, 2]            --
│    └─Scaler: 2-2706                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2707                      [16, 64, 2, 2]            --
│    └─Empty: 2-2708                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2709                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-203               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2710        --                        --
│    └─One: 2-2711                       [1]                       --
│    └─OutputScale: 2-2712               --                        --
│    └─Empty: 2-2713                     [64, 64, 1, 1]            --
│    └─Empty: 2-2714                     [64, 64, 1, 1]            --
│    └─Empty: 2-2715                     [64]                      --
│    └─Empty: 2-2716                     [64]                      --
│    └─BatchNorm2d: 2-2717               [16, 64, 2, 2]            --
│    └─Scaler: 2-2718                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2719                      [16, 64, 2, 2]            --
│    └─Empty: 2-2720                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2721                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-204        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2722                 [16, 64, 2, 2]            --
│    └─Empty: 2-2723                     [16, 64, 2, 2]            --
│    └─Empty: 2-2724                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2725        --                        --
│    └─One: 2-2726                       [1]                       --
│    └─OutputScale: 2-2727               --                        --
│    └─Empty: 2-2728                     [64, 64, 3, 3]            --
│    └─Empty: 2-2729                     [64, 64, 3, 3]            --
│    └─Empty: 2-2730                     [64]                      --
│    └─Empty: 2-2731                     [64]                      --
│    └─BatchNorm2d: 2-2732               [16, 64, 2, 2]            --
│    └─Scaler: 2-2733                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2734                      [16, 64, 2, 2]            --
│    └─Empty: 2-2735                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2736                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-205               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2737        --                        --
│    └─One: 2-2738                       [1]                       --
│    └─OutputScale: 2-2739               --                        --
│    └─Empty: 2-2740                     [64, 48, 1, 1]            --
│    └─Empty: 2-2741                     [64, 48, 1, 1]            --
│    └─Empty: 2-2742                     [64]                      --
│    └─Empty: 2-2743                     [64]                      --
│    └─BatchNorm2d: 2-2744               [16, 64, 64, 64]          --
│    └─Scaler: 2-2745                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2746                      [16, 64, 64, 64]          --
│    └─Empty: 2-2747                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2748                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-206               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2749        --                        --
│    └─One: 2-2750                       [1]                       --
│    └─OutputScale: 2-2751               --                        --
│    └─Empty: 2-2752                     [64, 64, 3, 3]            --
│    └─Empty: 2-2753                     [64, 64, 3, 3]            --
│    └─Empty: 2-2754                     [64]                      --
│    └─Empty: 2-2755                     [64]                      --
│    └─BatchNorm2d: 2-2756               [16, 64, 64, 64]          --
│    └─Scaler: 2-2757                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2758                      [16, 64, 64, 64]          --
│    └─Empty: 2-2759                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2760                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-207               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2761        --                        --
│    └─One: 2-2762                       [1]                       --
│    └─OutputScale: 2-2763               --                        --
│    └─Empty: 2-2764                     [64, 64, 1, 1]            --
│    └─Empty: 2-2765                     [64, 64, 1, 1]            --
│    └─Empty: 2-2766                     [64]                      --
│    └─Empty: 2-2767                     [64]                      --
│    └─BatchNorm2d: 2-2768               [16, 64, 64, 64]          --
│    └─Scaler: 2-2769                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2770                      [16, 64, 64, 64]          --
│    └─Empty: 2-2771                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2772                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-208               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2773        --                        --
│    └─One: 2-2774                       [1]                       --
│    └─OutputScale: 2-2775               --                        --
│    └─Empty: 2-2776                     [64, 64, 3, 3]            --
│    └─Empty: 2-2777                     [64, 64, 3, 3]            --
│    └─Empty: 2-2778                     [64]                      --
│    └─Empty: 2-2779                     [64]                      --
│    └─BatchNorm2d: 2-2780               [16, 64, 64, 64]          --
│    └─Scaler: 2-2781                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2782                      [16, 64, 64, 64]          --
│    └─Empty: 2-2783                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2784                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-209        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-2785                 [16, 64, 32, 32]          --
│    └─Empty: 2-2786                     [16, 64, 32, 32]          --
│    └─Empty: 2-2787                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-2788        --                        --
│    └─One: 2-2789                       [1]                       --
│    └─OutputScale: 2-2790               --                        --
│    └─Empty: 2-2791                     [64, 64, 3, 3]            --
│    └─Empty: 2-2792                     [64, 64, 3, 3]            --
│    └─Empty: 2-2793                     [64]                      --
│    └─Empty: 2-2794                     [64]                      --
│    └─BatchNorm2d: 2-2795               [16, 64, 32, 32]          --
│    └─Scaler: 2-2796                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2797                      [16, 64, 32, 32]          --
│    └─Empty: 2-2798                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2799                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-210               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-2800        --                        --
│    └─One: 2-2801                       [1]                       --
│    └─OutputScale: 2-2802               --                        --
│    └─Empty: 2-2803                     [64, 64, 3, 3]            --
│    └─Empty: 2-2804                     [64, 64, 3, 3]            --
│    └─Empty: 2-2805                     [64]                      --
│    └─Empty: 2-2806                     [64]                      --
│    └─BatchNorm2d: 2-2807               [16, 64, 32, 32]          --
│    └─Scaler: 2-2808                    [16, 64, 32, 32]          --
│    └─ReLU: 2-2809                      [16, 64, 32, 32]          --
│    └─Empty: 2-2810                     [16, 64, 32, 32]          --
│    └─Clamp: 2-2811                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-211        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-2812                 [16, 64, 16, 16]          --
│    └─Empty: 2-2813                     [16, 64, 16, 16]          --
│    └─Empty: 2-2814                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-2815        --                        --
│    └─One: 2-2816                       [1]                       --
│    └─OutputScale: 2-2817               --                        --
│    └─Empty: 2-2818                     [64, 64, 3, 3]            --
│    └─Empty: 2-2819                     [64, 64, 3, 3]            --
│    └─Empty: 2-2820                     [64]                      --
│    └─Empty: 2-2821                     [64]                      --
│    └─BatchNorm2d: 2-2822               [16, 64, 16, 16]          --
│    └─Scaler: 2-2823                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2824                      [16, 64, 16, 16]          --
│    └─Empty: 2-2825                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2826                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-212               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-2827        --                        --
│    └─One: 2-2828                       [1]                       --
│    └─OutputScale: 2-2829               --                        --
│    └─Empty: 2-2830                     [64, 64, 3, 3]            --
│    └─Empty: 2-2831                     [64, 64, 3, 3]            --
│    └─Empty: 2-2832                     [64]                      --
│    └─Empty: 2-2833                     [64]                      --
│    └─BatchNorm2d: 2-2834               [16, 64, 16, 16]          --
│    └─Scaler: 2-2835                    [16, 64, 16, 16]          --
│    └─ReLU: 2-2836                      [16, 64, 16, 16]          --
│    └─Empty: 2-2837                     [16, 64, 16, 16]          --
│    └─Clamp: 2-2838                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-213        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2839                 [16, 64, 8, 8]            --
│    └─Empty: 2-2840                     [16, 64, 8, 8]            --
│    └─Empty: 2-2841                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2842        --                        --
│    └─One: 2-2843                       [1]                       --
│    └─OutputScale: 2-2844               --                        --
│    └─Empty: 2-2845                     [64, 64, 3, 3]            --
│    └─Empty: 2-2846                     [64, 64, 3, 3]            --
│    └─Empty: 2-2847                     [64]                      --
│    └─Empty: 2-2848                     [64]                      --
│    └─BatchNorm2d: 2-2849               [16, 64, 8, 8]            --
│    └─Scaler: 2-2850                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2851                      [16, 64, 8, 8]            --
│    └─Empty: 2-2852                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2853                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-214               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-2854        --                        --
│    └─One: 2-2855                       [1]                       --
│    └─OutputScale: 2-2856               --                        --
│    └─Empty: 2-2857                     [64, 64, 1, 1]            --
│    └─Empty: 2-2858                     [64, 64, 1, 1]            --
│    └─Empty: 2-2859                     [64]                      --
│    └─Empty: 2-2860                     [64]                      --
│    └─BatchNorm2d: 2-2861               [16, 64, 8, 8]            --
│    └─Scaler: 2-2862                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2863                      [16, 64, 8, 8]            --
│    └─Empty: 2-2864                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2865                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-215        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-2866                 [16, 64, 8, 8]            --
│    └─Empty: 2-2867                     [16, 64, 8, 8]            --
│    └─Empty: 2-2868                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-2869        --                        --
│    └─One: 2-2870                       [1]                       --
│    └─OutputScale: 2-2871               --                        --
│    └─Empty: 2-2872                     [64, 64, 3, 3]            --
│    └─Empty: 2-2873                     [64, 64, 3, 3]            --
│    └─Empty: 2-2874                     [64]                      --
│    └─Empty: 2-2875                     [64]                      --
│    └─BatchNorm2d: 2-2876               [16, 64, 8, 8]            --
│    └─Scaler: 2-2877                    [16, 64, 8, 8]            --
│    └─ReLU: 2-2878                      [16, 64, 8, 8]            --
│    └─Empty: 2-2879                     [16, 64, 8, 8]            --
│    └─Clamp: 2-2880                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-216        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2881                 [16, 64, 4, 4]            --
│    └─Empty: 2-2882                     [16, 64, 4, 4]            --
│    └─Empty: 2-2883                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2884        --                        --
│    └─One: 2-2885                       [1]                       --
│    └─OutputScale: 2-2886               --                        --
│    └─Empty: 2-2887                     [64, 64, 3, 3]            --
│    └─Empty: 2-2888                     [64, 64, 3, 3]            --
│    └─Empty: 2-2889                     [64]                      --
│    └─Empty: 2-2890                     [64]                      --
│    └─BatchNorm2d: 2-2891               [16, 64, 4, 4]            --
│    └─Scaler: 2-2892                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2893                      [16, 64, 4, 4]            --
│    └─Empty: 2-2894                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2895                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-217               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-2896        --                        --
│    └─One: 2-2897                       [1]                       --
│    └─OutputScale: 2-2898               --                        --
│    └─Empty: 2-2899                     [64, 64, 1, 1]            --
│    └─Empty: 2-2900                     [64, 64, 1, 1]            --
│    └─Empty: 2-2901                     [64]                      --
│    └─Empty: 2-2902                     [64]                      --
│    └─BatchNorm2d: 2-2903               [16, 64, 4, 4]            --
│    └─Scaler: 2-2904                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2905                      [16, 64, 4, 4]            --
│    └─Empty: 2-2906                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2907                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-218        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-2908                 [16, 64, 4, 4]            --
│    └─Empty: 2-2909                     [16, 64, 4, 4]            --
│    └─Empty: 2-2910                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-2911        --                        --
│    └─One: 2-2912                       [1]                       --
│    └─OutputScale: 2-2913               --                        --
│    └─Empty: 2-2914                     [64, 64, 3, 3]            --
│    └─Empty: 2-2915                     [64, 64, 3, 3]            --
│    └─Empty: 2-2916                     [64]                      --
│    └─Empty: 2-2917                     [64]                      --
│    └─BatchNorm2d: 2-2918               [16, 64, 4, 4]            --
│    └─Scaler: 2-2919                    [16, 64, 4, 4]            --
│    └─ReLU: 2-2920                      [16, 64, 4, 4]            --
│    └─Empty: 2-2921                     [16, 64, 4, 4]            --
│    └─Clamp: 2-2922                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-219        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2923                 [16, 64, 2, 2]            --
│    └─Empty: 2-2924                     [16, 64, 2, 2]            --
│    └─Empty: 2-2925                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2926        --                        --
│    └─One: 2-2927                       [1]                       --
│    └─OutputScale: 2-2928               --                        --
│    └─Empty: 2-2929                     [64, 64, 1, 1]            --
│    └─Empty: 2-2930                     [64, 64, 1, 1]            --
│    └─Empty: 2-2931                     [64]                      --
│    └─Empty: 2-2932                     [64]                      --
│    └─BatchNorm2d: 2-2933               [16, 64, 2, 2]            --
│    └─Scaler: 2-2934                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2935                      [16, 64, 2, 2]            --
│    └─Empty: 2-2936                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2937                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-220               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-2938        --                        --
│    └─One: 2-2939                       [1]                       --
│    └─OutputScale: 2-2940               --                        --
│    └─Empty: 2-2941                     [64, 64, 1, 1]            --
│    └─Empty: 2-2942                     [64, 64, 1, 1]            --
│    └─Empty: 2-2943                     [64]                      --
│    └─Empty: 2-2944                     [64]                      --
│    └─BatchNorm2d: 2-2945               [16, 64, 2, 2]            --
│    └─Scaler: 2-2946                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2947                      [16, 64, 2, 2]            --
│    └─Empty: 2-2948                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2949                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-221        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-2950                 [16, 64, 2, 2]            --
│    └─Empty: 2-2951                     [16, 64, 2, 2]            --
│    └─Empty: 2-2952                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-2953        --                        --
│    └─One: 2-2954                       [1]                       --
│    └─OutputScale: 2-2955               --                        --
│    └─Empty: 2-2956                     [64, 64, 3, 3]            --
│    └─Empty: 2-2957                     [64, 64, 3, 3]            --
│    └─Empty: 2-2958                     [64]                      --
│    └─Empty: 2-2959                     [64]                      --
│    └─BatchNorm2d: 2-2960               [16, 64, 2, 2]            --
│    └─Scaler: 2-2961                    [16, 64, 2, 2]            --
│    └─ReLU: 2-2962                      [16, 64, 2, 2]            --
│    └─Empty: 2-2963                     [16, 64, 2, 2]            --
│    └─Clamp: 2-2964                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-222               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2965        --                        --
│    └─One: 2-2966                       [1]                       --
│    └─OutputScale: 2-2967               --                        --
│    └─Empty: 2-2968                     [64, 48, 1, 1]            --
│    └─Empty: 2-2969                     [64, 48, 1, 1]            --
│    └─Empty: 2-2970                     [64]                      --
│    └─Empty: 2-2971                     [64]                      --
│    └─BatchNorm2d: 2-2972               [16, 64, 64, 64]          --
│    └─Scaler: 2-2973                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2974                      [16, 64, 64, 64]          --
│    └─Empty: 2-2975                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2976                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-223               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2977        --                        --
│    └─One: 2-2978                       [1]                       --
│    └─OutputScale: 2-2979               --                        --
│    └─Empty: 2-2980                     [64, 64, 3, 3]            --
│    └─Empty: 2-2981                     [64, 64, 3, 3]            --
│    └─Empty: 2-2982                     [64]                      --
│    └─Empty: 2-2983                     [64]                      --
│    └─BatchNorm2d: 2-2984               [16, 64, 64, 64]          --
│    └─Scaler: 2-2985                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2986                      [16, 64, 64, 64]          --
│    └─Empty: 2-2987                     [16, 64, 64, 64]          --
│    └─Clamp: 2-2988                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-224               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-2989        --                        --
│    └─One: 2-2990                       [1]                       --
│    └─OutputScale: 2-2991               --                        --
│    └─Empty: 2-2992                     [64, 64, 1, 1]            --
│    └─Empty: 2-2993                     [64, 64, 1, 1]            --
│    └─Empty: 2-2994                     [64]                      --
│    └─Empty: 2-2995                     [64]                      --
│    └─BatchNorm2d: 2-2996               [16, 64, 64, 64]          --
│    └─Scaler: 2-2997                    [16, 64, 64, 64]          --
│    └─ReLU: 2-2998                      [16, 64, 64, 64]          --
│    └─Empty: 2-2999                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3000                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-225               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3001        --                        --
│    └─One: 2-3002                       [1]                       --
│    └─OutputScale: 2-3003               --                        --
│    └─Empty: 2-3004                     [64, 64, 3, 3]            --
│    └─Empty: 2-3005                     [64, 64, 3, 3]            --
│    └─Empty: 2-3006                     [64]                      --
│    └─Empty: 2-3007                     [64]                      --
│    └─BatchNorm2d: 2-3008               [16, 64, 64, 64]          --
│    └─Scaler: 2-3009                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3010                      [16, 64, 64, 64]          --
│    └─Empty: 2-3011                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3012                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-226        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-3013                 [16, 64, 32, 32]          --
│    └─Empty: 2-3014                     [16, 64, 32, 32]          --
│    └─Empty: 2-3015                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-3016        --                        --
│    └─One: 2-3017                       [1]                       --
│    └─OutputScale: 2-3018               --                        --
│    └─Empty: 2-3019                     [64, 64, 3, 3]            --
│    └─Empty: 2-3020                     [64, 64, 3, 3]            --
│    └─Empty: 2-3021                     [64]                      --
│    └─Empty: 2-3022                     [64]                      --
│    └─BatchNorm2d: 2-3023               [16, 64, 32, 32]          --
│    └─Scaler: 2-3024                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3025                      [16, 64, 32, 32]          --
│    └─Empty: 2-3026                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3027                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-227               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-3028        --                        --
│    └─One: 2-3029                       [1]                       --
│    └─OutputScale: 2-3030               --                        --
│    └─Empty: 2-3031                     [64, 64, 3, 3]            --
│    └─Empty: 2-3032                     [64, 64, 3, 3]            --
│    └─Empty: 2-3033                     [64]                      --
│    └─Empty: 2-3034                     [64]                      --
│    └─BatchNorm2d: 2-3035               [16, 64, 32, 32]          --
│    └─Scaler: 2-3036                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3037                      [16, 64, 32, 32]          --
│    └─Empty: 2-3038                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3039                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-228        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-3040                 [16, 64, 16, 16]          --
│    └─Empty: 2-3041                     [16, 64, 16, 16]          --
│    └─Empty: 2-3042                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-3043        --                        --
│    └─One: 2-3044                       [1]                       --
│    └─OutputScale: 2-3045               --                        --
│    └─Empty: 2-3046                     [64, 64, 3, 3]            --
│    └─Empty: 2-3047                     [64, 64, 3, 3]            --
│    └─Empty: 2-3048                     [64]                      --
│    └─Empty: 2-3049                     [64]                      --
│    └─BatchNorm2d: 2-3050               [16, 64, 16, 16]          --
│    └─Scaler: 2-3051                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3052                      [16, 64, 16, 16]          --
│    └─Empty: 2-3053                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3054                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-229               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-3055        --                        --
│    └─One: 2-3056                       [1]                       --
│    └─OutputScale: 2-3057               --                        --
│    └─Empty: 2-3058                     [64, 64, 3, 3]            --
│    └─Empty: 2-3059                     [64, 64, 3, 3]            --
│    └─Empty: 2-3060                     [64]                      --
│    └─Empty: 2-3061                     [64]                      --
│    └─BatchNorm2d: 2-3062               [16, 64, 16, 16]          --
│    └─Scaler: 2-3063                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3064                      [16, 64, 16, 16]          --
│    └─Empty: 2-3065                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3066                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-230        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3067                 [16, 64, 8, 8]            --
│    └─Empty: 2-3068                     [16, 64, 8, 8]            --
│    └─Empty: 2-3069                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3070        --                        --
│    └─One: 2-3071                       [1]                       --
│    └─OutputScale: 2-3072               --                        --
│    └─Empty: 2-3073                     [64, 64, 3, 3]            --
│    └─Empty: 2-3074                     [64, 64, 3, 3]            --
│    └─Empty: 2-3075                     [64]                      --
│    └─Empty: 2-3076                     [64]                      --
│    └─BatchNorm2d: 2-3077               [16, 64, 8, 8]            --
│    └─Scaler: 2-3078                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3079                      [16, 64, 8, 8]            --
│    └─Empty: 2-3080                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3081                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-231               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-3082        --                        --
│    └─One: 2-3083                       [1]                       --
│    └─OutputScale: 2-3084               --                        --
│    └─Empty: 2-3085                     [64, 64, 1, 1]            --
│    └─Empty: 2-3086                     [64, 64, 1, 1]            --
│    └─Empty: 2-3087                     [64]                      --
│    └─Empty: 2-3088                     [64]                      --
│    └─BatchNorm2d: 2-3089               [16, 64, 8, 8]            --
│    └─Scaler: 2-3090                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3091                      [16, 64, 8, 8]            --
│    └─Empty: 2-3092                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3093                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-232        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3094                 [16, 64, 8, 8]            --
│    └─Empty: 2-3095                     [16, 64, 8, 8]            --
│    └─Empty: 2-3096                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3097        --                        --
│    └─One: 2-3098                       [1]                       --
│    └─OutputScale: 2-3099               --                        --
│    └─Empty: 2-3100                     [64, 64, 3, 3]            --
│    └─Empty: 2-3101                     [64, 64, 3, 3]            --
│    └─Empty: 2-3102                     [64]                      --
│    └─Empty: 2-3103                     [64]                      --
│    └─BatchNorm2d: 2-3104               [16, 64, 8, 8]            --
│    └─Scaler: 2-3105                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3106                      [16, 64, 8, 8]            --
│    └─Empty: 2-3107                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3108                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-233        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3109                 [16, 64, 4, 4]            --
│    └─Empty: 2-3110                     [16, 64, 4, 4]            --
│    └─Empty: 2-3111                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3112        --                        --
│    └─One: 2-3113                       [1]                       --
│    └─OutputScale: 2-3114               --                        --
│    └─Empty: 2-3115                     [64, 64, 3, 3]            --
│    └─Empty: 2-3116                     [64, 64, 3, 3]            --
│    └─Empty: 2-3117                     [64]                      --
│    └─Empty: 2-3118                     [64]                      --
│    └─BatchNorm2d: 2-3119               [16, 64, 4, 4]            --
│    └─Scaler: 2-3120                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3121                      [16, 64, 4, 4]            --
│    └─Empty: 2-3122                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3123                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-234               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-3124        --                        --
│    └─One: 2-3125                       [1]                       --
│    └─OutputScale: 2-3126               --                        --
│    └─Empty: 2-3127                     [64, 64, 1, 1]            --
│    └─Empty: 2-3128                     [64, 64, 1, 1]            --
│    └─Empty: 2-3129                     [64]                      --
│    └─Empty: 2-3130                     [64]                      --
│    └─BatchNorm2d: 2-3131               [16, 64, 4, 4]            --
│    └─Scaler: 2-3132                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3133                      [16, 64, 4, 4]            --
│    └─Empty: 2-3134                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3135                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-235        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3136                 [16, 64, 4, 4]            --
│    └─Empty: 2-3137                     [16, 64, 4, 4]            --
│    └─Empty: 2-3138                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3139        --                        --
│    └─One: 2-3140                       [1]                       --
│    └─OutputScale: 2-3141               --                        --
│    └─Empty: 2-3142                     [64, 64, 3, 3]            --
│    └─Empty: 2-3143                     [64, 64, 3, 3]            --
│    └─Empty: 2-3144                     [64]                      --
│    └─Empty: 2-3145                     [64]                      --
│    └─BatchNorm2d: 2-3146               [16, 64, 4, 4]            --
│    └─Scaler: 2-3147                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3148                      [16, 64, 4, 4]            --
│    └─Empty: 2-3149                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3150                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-236        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3151                 [16, 64, 2, 2]            --
│    └─Empty: 2-3152                     [16, 64, 2, 2]            --
│    └─Empty: 2-3153                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3154        --                        --
│    └─One: 2-3155                       [1]                       --
│    └─OutputScale: 2-3156               --                        --
│    └─Empty: 2-3157                     [64, 64, 1, 1]            --
│    └─Empty: 2-3158                     [64, 64, 1, 1]            --
│    └─Empty: 2-3159                     [64]                      --
│    └─Empty: 2-3160                     [64]                      --
│    └─BatchNorm2d: 2-3161               [16, 64, 2, 2]            --
│    └─Scaler: 2-3162                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3163                      [16, 64, 2, 2]            --
│    └─Empty: 2-3164                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3165                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-237               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-3166        --                        --
│    └─One: 2-3167                       [1]                       --
│    └─OutputScale: 2-3168               --                        --
│    └─Empty: 2-3169                     [64, 64, 1, 1]            --
│    └─Empty: 2-3170                     [64, 64, 1, 1]            --
│    └─Empty: 2-3171                     [64]                      --
│    └─Empty: 2-3172                     [64]                      --
│    └─BatchNorm2d: 2-3173               [16, 64, 2, 2]            --
│    └─Scaler: 2-3174                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3175                      [16, 64, 2, 2]            --
│    └─Empty: 2-3176                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3177                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-238        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3178                 [16, 64, 2, 2]            --
│    └─Empty: 2-3179                     [16, 64, 2, 2]            --
│    └─Empty: 2-3180                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3181        --                        --
│    └─One: 2-3182                       [1]                       --
│    └─OutputScale: 2-3183               --                        --
│    └─Empty: 2-3184                     [64, 64, 3, 3]            --
│    └─Empty: 2-3185                     [64, 64, 3, 3]            --
│    └─Empty: 2-3186                     [64]                      --
│    └─Empty: 2-3187                     [64]                      --
│    └─BatchNorm2d: 2-3188               [16, 64, 2, 2]            --
│    └─Scaler: 2-3189                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3190                      [16, 64, 2, 2]            --
│    └─Empty: 2-3191                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3192                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-239               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3193        --                        --
│    └─One: 2-3194                       [1]                       --
│    └─OutputScale: 2-3195               --                        --
│    └─Empty: 2-3196                     [64, 48, 1, 1]            --
│    └─Empty: 2-3197                     [64, 48, 1, 1]            --
│    └─Empty: 2-3198                     [64]                      --
│    └─Empty: 2-3199                     [64]                      --
│    └─BatchNorm2d: 2-3200               [16, 64, 64, 64]          --
│    └─Scaler: 2-3201                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3202                      [16, 64, 64, 64]          --
│    └─Empty: 2-3203                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3204                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-240               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3205        --                        --
│    └─One: 2-3206                       [1]                       --
│    └─OutputScale: 2-3207               --                        --
│    └─Empty: 2-3208                     [64, 64, 3, 3]            --
│    └─Empty: 2-3209                     [64, 64, 3, 3]            --
│    └─Empty: 2-3210                     [64]                      --
│    └─Empty: 2-3211                     [64]                      --
│    └─BatchNorm2d: 2-3212               [16, 64, 64, 64]          --
│    └─Scaler: 2-3213                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3214                      [16, 64, 64, 64]          --
│    └─Empty: 2-3215                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3216                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-241               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3217        --                        --
│    └─One: 2-3218                       [1]                       --
│    └─OutputScale: 2-3219               --                        --
│    └─Empty: 2-3220                     [64, 64, 1, 1]            --
│    └─Empty: 2-3221                     [64, 64, 1, 1]            --
│    └─Empty: 2-3222                     [64]                      --
│    └─Empty: 2-3223                     [64]                      --
│    └─BatchNorm2d: 2-3224               [16, 64, 64, 64]          --
│    └─Scaler: 2-3225                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3226                      [16, 64, 64, 64]          --
│    └─Empty: 2-3227                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3228                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-242               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3229        --                        --
│    └─One: 2-3230                       [1]                       --
│    └─OutputScale: 2-3231               --                        --
│    └─Empty: 2-3232                     [64, 64, 3, 3]            --
│    └─Empty: 2-3233                     [64, 64, 3, 3]            --
│    └─Empty: 2-3234                     [64]                      --
│    └─Empty: 2-3235                     [64]                      --
│    └─BatchNorm2d: 2-3236               [16, 64, 64, 64]          --
│    └─Scaler: 2-3237                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3238                      [16, 64, 64, 64]          --
│    └─Empty: 2-3239                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3240                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-243        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-3241                 [16, 64, 32, 32]          --
│    └─Empty: 2-3242                     [16, 64, 32, 32]          --
│    └─Empty: 2-3243                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-3244        --                        --
│    └─One: 2-3245                       [1]                       --
│    └─OutputScale: 2-3246               --                        --
│    └─Empty: 2-3247                     [64, 64, 3, 3]            --
│    └─Empty: 2-3248                     [64, 64, 3, 3]            --
│    └─Empty: 2-3249                     [64]                      --
│    └─Empty: 2-3250                     [64]                      --
│    └─BatchNorm2d: 2-3251               [16, 64, 32, 32]          --
│    └─Scaler: 2-3252                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3253                      [16, 64, 32, 32]          --
│    └─Empty: 2-3254                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3255                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-244               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-3256        --                        --
│    └─One: 2-3257                       [1]                       --
│    └─OutputScale: 2-3258               --                        --
│    └─Empty: 2-3259                     [64, 64, 3, 3]            --
│    └─Empty: 2-3260                     [64, 64, 3, 3]            --
│    └─Empty: 2-3261                     [64]                      --
│    └─Empty: 2-3262                     [64]                      --
│    └─BatchNorm2d: 2-3263               [16, 64, 32, 32]          --
│    └─Scaler: 2-3264                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3265                      [16, 64, 32, 32]          --
│    └─Empty: 2-3266                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3267                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-245        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-3268                 [16, 64, 16, 16]          --
│    └─Empty: 2-3269                     [16, 64, 16, 16]          --
│    └─Empty: 2-3270                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-3271        --                        --
│    └─One: 2-3272                       [1]                       --
│    └─OutputScale: 2-3273               --                        --
│    └─Empty: 2-3274                     [64, 64, 3, 3]            --
│    └─Empty: 2-3275                     [64, 64, 3, 3]            --
│    └─Empty: 2-3276                     [64]                      --
│    └─Empty: 2-3277                     [64]                      --
│    └─BatchNorm2d: 2-3278               [16, 64, 16, 16]          --
│    └─Scaler: 2-3279                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3280                      [16, 64, 16, 16]          --
│    └─Empty: 2-3281                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3282                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-246               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-3283        --                        --
│    └─One: 2-3284                       [1]                       --
│    └─OutputScale: 2-3285               --                        --
│    └─Empty: 2-3286                     [64, 64, 3, 3]            --
│    └─Empty: 2-3287                     [64, 64, 3, 3]            --
│    └─Empty: 2-3288                     [64]                      --
│    └─Empty: 2-3289                     [64]                      --
│    └─BatchNorm2d: 2-3290               [16, 64, 16, 16]          --
│    └─Scaler: 2-3291                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3292                      [16, 64, 16, 16]          --
│    └─Empty: 2-3293                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3294                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-247        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3295                 [16, 64, 8, 8]            --
│    └─Empty: 2-3296                     [16, 64, 8, 8]            --
│    └─Empty: 2-3297                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3298        --                        --
│    └─One: 2-3299                       [1]                       --
│    └─OutputScale: 2-3300               --                        --
│    └─Empty: 2-3301                     [64, 64, 3, 3]            --
│    └─Empty: 2-3302                     [64, 64, 3, 3]            --
│    └─Empty: 2-3303                     [64]                      --
│    └─Empty: 2-3304                     [64]                      --
│    └─BatchNorm2d: 2-3305               [16, 64, 8, 8]            --
│    └─Scaler: 2-3306                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3307                      [16, 64, 8, 8]            --
│    └─Empty: 2-3308                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3309                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-248               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-3310        --                        --
│    └─One: 2-3311                       [1]                       --
│    └─OutputScale: 2-3312               --                        --
│    └─Empty: 2-3313                     [64, 64, 1, 1]            --
│    └─Empty: 2-3314                     [64, 64, 1, 1]            --
│    └─Empty: 2-3315                     [64]                      --
│    └─Empty: 2-3316                     [64]                      --
│    └─BatchNorm2d: 2-3317               [16, 64, 8, 8]            --
│    └─Scaler: 2-3318                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3319                      [16, 64, 8, 8]            --
│    └─Empty: 2-3320                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3321                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-249        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3322                 [16, 64, 8, 8]            --
│    └─Empty: 2-3323                     [16, 64, 8, 8]            --
│    └─Empty: 2-3324                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3325        --                        --
│    └─One: 2-3326                       [1]                       --
│    └─OutputScale: 2-3327               --                        --
│    └─Empty: 2-3328                     [64, 64, 3, 3]            --
│    └─Empty: 2-3329                     [64, 64, 3, 3]            --
│    └─Empty: 2-3330                     [64]                      --
│    └─Empty: 2-3331                     [64]                      --
│    └─BatchNorm2d: 2-3332               [16, 64, 8, 8]            --
│    └─Scaler: 2-3333                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3334                      [16, 64, 8, 8]            --
│    └─Empty: 2-3335                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3336                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-250        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3337                 [16, 64, 4, 4]            --
│    └─Empty: 2-3338                     [16, 64, 4, 4]            --
│    └─Empty: 2-3339                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3340        --                        --
│    └─One: 2-3341                       [1]                       --
│    └─OutputScale: 2-3342               --                        --
│    └─Empty: 2-3343                     [64, 64, 3, 3]            --
│    └─Empty: 2-3344                     [64, 64, 3, 3]            --
│    └─Empty: 2-3345                     [64]                      --
│    └─Empty: 2-3346                     [64]                      --
│    └─BatchNorm2d: 2-3347               [16, 64, 4, 4]            --
│    └─Scaler: 2-3348                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3349                      [16, 64, 4, 4]            --
│    └─Empty: 2-3350                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3351                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-251               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-3352        --                        --
│    └─One: 2-3353                       [1]                       --
│    └─OutputScale: 2-3354               --                        --
│    └─Empty: 2-3355                     [64, 64, 1, 1]            --
│    └─Empty: 2-3356                     [64, 64, 1, 1]            --
│    └─Empty: 2-3357                     [64]                      --
│    └─Empty: 2-3358                     [64]                      --
│    └─BatchNorm2d: 2-3359               [16, 64, 4, 4]            --
│    └─Scaler: 2-3360                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3361                      [16, 64, 4, 4]            --
│    └─Empty: 2-3362                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3363                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-252        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3364                 [16, 64, 4, 4]            --
│    └─Empty: 2-3365                     [16, 64, 4, 4]            --
│    └─Empty: 2-3366                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3367        --                        --
│    └─One: 2-3368                       [1]                       --
│    └─OutputScale: 2-3369               --                        --
│    └─Empty: 2-3370                     [64, 64, 3, 3]            --
│    └─Empty: 2-3371                     [64, 64, 3, 3]            --
│    └─Empty: 2-3372                     [64]                      --
│    └─Empty: 2-3373                     [64]                      --
│    └─BatchNorm2d: 2-3374               [16, 64, 4, 4]            --
│    └─Scaler: 2-3375                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3376                      [16, 64, 4, 4]            --
│    └─Empty: 2-3377                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3378                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-253        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3379                 [16, 64, 2, 2]            --
│    └─Empty: 2-3380                     [16, 64, 2, 2]            --
│    └─Empty: 2-3381                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3382        --                        --
│    └─One: 2-3383                       [1]                       --
│    └─OutputScale: 2-3384               --                        --
│    └─Empty: 2-3385                     [64, 64, 1, 1]            --
│    └─Empty: 2-3386                     [64, 64, 1, 1]            --
│    └─Empty: 2-3387                     [64]                      --
│    └─Empty: 2-3388                     [64]                      --
│    └─BatchNorm2d: 2-3389               [16, 64, 2, 2]            --
│    └─Scaler: 2-3390                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3391                      [16, 64, 2, 2]            --
│    └─Empty: 2-3392                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3393                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-254               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-3394        --                        --
│    └─One: 2-3395                       [1]                       --
│    └─OutputScale: 2-3396               --                        --
│    └─Empty: 2-3397                     [64, 64, 1, 1]            --
│    └─Empty: 2-3398                     [64, 64, 1, 1]            --
│    └─Empty: 2-3399                     [64]                      --
│    └─Empty: 2-3400                     [64]                      --
│    └─BatchNorm2d: 2-3401               [16, 64, 2, 2]            --
│    └─Scaler: 2-3402                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3403                      [16, 64, 2, 2]            --
│    └─Empty: 2-3404                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3405                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-255        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3406                 [16, 64, 2, 2]            --
│    └─Empty: 2-3407                     [16, 64, 2, 2]            --
│    └─Empty: 2-3408                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3409        --                        --
│    └─One: 2-3410                       [1]                       --
│    └─OutputScale: 2-3411               --                        --
│    └─Empty: 2-3412                     [64, 64, 3, 3]            --
│    └─Empty: 2-3413                     [64, 64, 3, 3]            --
│    └─Empty: 2-3414                     [64]                      --
│    └─Empty: 2-3415                     [64]                      --
│    └─BatchNorm2d: 2-3416               [16, 64, 2, 2]            --
│    └─Scaler: 2-3417                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3418                      [16, 64, 2, 2]            --
│    └─Empty: 2-3419                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3420                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-256               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3421        --                        --
│    └─One: 2-3422                       [1]                       --
│    └─OutputScale: 2-3423               --                        --
│    └─Empty: 2-3424                     [64, 48, 1, 1]            --
│    └─Empty: 2-3425                     [64, 48, 1, 1]            --
│    └─Empty: 2-3426                     [64]                      --
│    └─Empty: 2-3427                     [64]                      --
│    └─BatchNorm2d: 2-3428               [16, 64, 64, 64]          --
│    └─Scaler: 2-3429                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3430                      [16, 64, 64, 64]          --
│    └─Empty: 2-3431                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3432                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-257               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3433        --                        --
│    └─One: 2-3434                       [1]                       --
│    └─OutputScale: 2-3435               --                        --
│    └─Empty: 2-3436                     [64, 64, 3, 3]            --
│    └─Empty: 2-3437                     [64, 64, 3, 3]            --
│    └─Empty: 2-3438                     [64]                      --
│    └─Empty: 2-3439                     [64]                      --
│    └─BatchNorm2d: 2-3440               [16, 64, 64, 64]          --
│    └─Scaler: 2-3441                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3442                      [16, 64, 64, 64]          --
│    └─Empty: 2-3443                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3444                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-258               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3445        --                        --
│    └─One: 2-3446                       [1]                       --
│    └─OutputScale: 2-3447               --                        --
│    └─Empty: 2-3448                     [64, 64, 1, 1]            --
│    └─Empty: 2-3449                     [64, 64, 1, 1]            --
│    └─Empty: 2-3450                     [64]                      --
│    └─Empty: 2-3451                     [64]                      --
│    └─BatchNorm2d: 2-3452               [16, 64, 64, 64]          --
│    └─Scaler: 2-3453                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3454                      [16, 64, 64, 64]          --
│    └─Empty: 2-3455                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3456                     [16, 64, 64, 64]          --
├─FusedConv2dBNReLU: 1-259               [16, 64, 64, 64]          (recursive)
│    └─OutputShiftSqueeze: 2-3457        --                        --
│    └─One: 2-3458                       [1]                       --
│    └─OutputScale: 2-3459               --                        --
│    └─Empty: 2-3460                     [64, 64, 3, 3]            --
│    └─Empty: 2-3461                     [64, 64, 3, 3]            --
│    └─Empty: 2-3462                     [64]                      --
│    └─Empty: 2-3463                     [64]                      --
│    └─BatchNorm2d: 2-3464               [16, 64, 64, 64]          --
│    └─Scaler: 2-3465                    [16, 64, 64, 64]          --
│    └─ReLU: 2-3466                      [16, 64, 64, 64]          --
│    └─Empty: 2-3467                     [16, 64, 64, 64]          --
│    └─Clamp: 2-3468                     [16, 64, 64, 64]          --
├─FusedMaxPoolConv2dBNReLU: 1-260        [16, 64, 32, 32]          (recursive)
│    └─MaxPool2d: 2-3469                 [16, 64, 32, 32]          --
│    └─Empty: 2-3470                     [16, 64, 32, 32]          --
│    └─Empty: 2-3471                     [16, 64, 32, 32]          --
│    └─OutputShiftSqueeze: 2-3472        --                        --
│    └─One: 2-3473                       [1]                       --
│    └─OutputScale: 2-3474               --                        --
│    └─Empty: 2-3475                     [64, 64, 3, 3]            --
│    └─Empty: 2-3476                     [64, 64, 3, 3]            --
│    └─Empty: 2-3477                     [64]                      --
│    └─Empty: 2-3478                     [64]                      --
│    └─BatchNorm2d: 2-3479               [16, 64, 32, 32]          --
│    └─Scaler: 2-3480                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3481                      [16, 64, 32, 32]          --
│    └─Empty: 2-3482                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3483                     [16, 64, 32, 32]          --
├─FusedConv2dBNReLU: 1-261               [16, 64, 32, 32]          (recursive)
│    └─OutputShiftSqueeze: 2-3484        --                        --
│    └─One: 2-3485                       [1]                       --
│    └─OutputScale: 2-3486               --                        --
│    └─Empty: 2-3487                     [64, 64, 3, 3]            --
│    └─Empty: 2-3488                     [64, 64, 3, 3]            --
│    └─Empty: 2-3489                     [64]                      --
│    └─Empty: 2-3490                     [64]                      --
│    └─BatchNorm2d: 2-3491               [16, 64, 32, 32]          --
│    └─Scaler: 2-3492                    [16, 64, 32, 32]          --
│    └─ReLU: 2-3493                      [16, 64, 32, 32]          --
│    └─Empty: 2-3494                     [16, 64, 32, 32]          --
│    └─Clamp: 2-3495                     [16, 64, 32, 32]          --
├─FusedMaxPoolConv2dBNReLU: 1-262        [16, 64, 16, 16]          (recursive)
│    └─MaxPool2d: 2-3496                 [16, 64, 16, 16]          --
│    └─Empty: 2-3497                     [16, 64, 16, 16]          --
│    └─Empty: 2-3498                     [16, 64, 16, 16]          --
│    └─OutputShiftSqueeze: 2-3499        --                        --
│    └─One: 2-3500                       [1]                       --
│    └─OutputScale: 2-3501               --                        --
│    └─Empty: 2-3502                     [64, 64, 3, 3]            --
│    └─Empty: 2-3503                     [64, 64, 3, 3]            --
│    └─Empty: 2-3504                     [64]                      --
│    └─Empty: 2-3505                     [64]                      --
│    └─BatchNorm2d: 2-3506               [16, 64, 16, 16]          --
│    └─Scaler: 2-3507                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3508                      [16, 64, 16, 16]          --
│    └─Empty: 2-3509                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3510                     [16, 64, 16, 16]          --
├─FusedConv2dBNReLU: 1-263               [16, 64, 16, 16]          (recursive)
│    └─OutputShiftSqueeze: 2-3511        --                        --
│    └─One: 2-3512                       [1]                       --
│    └─OutputScale: 2-3513               --                        --
│    └─Empty: 2-3514                     [64, 64, 3, 3]            --
│    └─Empty: 2-3515                     [64, 64, 3, 3]            --
│    └─Empty: 2-3516                     [64]                      --
│    └─Empty: 2-3517                     [64]                      --
│    └─BatchNorm2d: 2-3518               [16, 64, 16, 16]          --
│    └─Scaler: 2-3519                    [16, 64, 16, 16]          --
│    └─ReLU: 2-3520                      [16, 64, 16, 16]          --
│    └─Empty: 2-3521                     [16, 64, 16, 16]          --
│    └─Clamp: 2-3522                     [16, 64, 16, 16]          --
├─FusedMaxPoolConv2dBNReLU: 1-264        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3523                 [16, 64, 8, 8]            --
│    └─Empty: 2-3524                     [16, 64, 8, 8]            --
│    └─Empty: 2-3525                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3526        --                        --
│    └─One: 2-3527                       [1]                       --
│    └─OutputScale: 2-3528               --                        --
│    └─Empty: 2-3529                     [64, 64, 3, 3]            --
│    └─Empty: 2-3530                     [64, 64, 3, 3]            --
│    └─Empty: 2-3531                     [64]                      --
│    └─Empty: 2-3532                     [64]                      --
│    └─BatchNorm2d: 2-3533               [16, 64, 8, 8]            --
│    └─Scaler: 2-3534                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3535                      [16, 64, 8, 8]            --
│    └─Empty: 2-3536                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3537                     [16, 64, 8, 8]            --
├─FusedConv2dBNReLU: 1-265               [16, 64, 8, 8]            (recursive)
│    └─OutputShiftSqueeze: 2-3538        --                        --
│    └─One: 2-3539                       [1]                       --
│    └─OutputScale: 2-3540               --                        --
│    └─Empty: 2-3541                     [64, 64, 1, 1]            --
│    └─Empty: 2-3542                     [64, 64, 1, 1]            --
│    └─Empty: 2-3543                     [64]                      --
│    └─Empty: 2-3544                     [64]                      --
│    └─BatchNorm2d: 2-3545               [16, 64, 8, 8]            --
│    └─Scaler: 2-3546                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3547                      [16, 64, 8, 8]            --
│    └─Empty: 2-3548                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3549                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-266        [16, 64, 8, 8]            (recursive)
│    └─MaxPool2d: 2-3550                 [16, 64, 8, 8]            --
│    └─Empty: 2-3551                     [16, 64, 8, 8]            --
│    └─Empty: 2-3552                     [16, 64, 8, 8]            --
│    └─OutputShiftSqueeze: 2-3553        --                        --
│    └─One: 2-3554                       [1]                       --
│    └─OutputScale: 2-3555               --                        --
│    └─Empty: 2-3556                     [64, 64, 3, 3]            --
│    └─Empty: 2-3557                     [64, 64, 3, 3]            --
│    └─Empty: 2-3558                     [64]                      --
│    └─Empty: 2-3559                     [64]                      --
│    └─BatchNorm2d: 2-3560               [16, 64, 8, 8]            --
│    └─Scaler: 2-3561                    [16, 64, 8, 8]            --
│    └─ReLU: 2-3562                      [16, 64, 8, 8]            --
│    └─Empty: 2-3563                     [16, 64, 8, 8]            --
│    └─Clamp: 2-3564                     [16, 64, 8, 8]            --
├─FusedMaxPoolConv2dBNReLU: 1-267        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3565                 [16, 64, 4, 4]            --
│    └─Empty: 2-3566                     [16, 64, 4, 4]            --
│    └─Empty: 2-3567                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3568        --                        --
│    └─One: 2-3569                       [1]                       --
│    └─OutputScale: 2-3570               --                        --
│    └─Empty: 2-3571                     [64, 64, 3, 3]            --
│    └─Empty: 2-3572                     [64, 64, 3, 3]            --
│    └─Empty: 2-3573                     [64]                      --
│    └─Empty: 2-3574                     [64]                      --
│    └─BatchNorm2d: 2-3575               [16, 64, 4, 4]            --
│    └─Scaler: 2-3576                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3577                      [16, 64, 4, 4]            --
│    └─Empty: 2-3578                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3579                     [16, 64, 4, 4]            --
├─FusedConv2dBNReLU: 1-268               [16, 64, 4, 4]            (recursive)
│    └─OutputShiftSqueeze: 2-3580        --                        --
│    └─One: 2-3581                       [1]                       --
│    └─OutputScale: 2-3582               --                        --
│    └─Empty: 2-3583                     [64, 64, 1, 1]            --
│    └─Empty: 2-3584                     [64, 64, 1, 1]            --
│    └─Empty: 2-3585                     [64]                      --
│    └─Empty: 2-3586                     [64]                      --
│    └─BatchNorm2d: 2-3587               [16, 64, 4, 4]            --
│    └─Scaler: 2-3588                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3589                      [16, 64, 4, 4]            --
│    └─Empty: 2-3590                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3591                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-269        [16, 64, 4, 4]            (recursive)
│    └─MaxPool2d: 2-3592                 [16, 64, 4, 4]            --
│    └─Empty: 2-3593                     [16, 64, 4, 4]            --
│    └─Empty: 2-3594                     [16, 64, 4, 4]            --
│    └─OutputShiftSqueeze: 2-3595        --                        --
│    └─One: 2-3596                       [1]                       --
│    └─OutputScale: 2-3597               --                        --
│    └─Empty: 2-3598                     [64, 64, 3, 3]            --
│    └─Empty: 2-3599                     [64, 64, 3, 3]            --
│    └─Empty: 2-3600                     [64]                      --
│    └─Empty: 2-3601                     [64]                      --
│    └─BatchNorm2d: 2-3602               [16, 64, 4, 4]            --
│    └─Scaler: 2-3603                    [16, 64, 4, 4]            --
│    └─ReLU: 2-3604                      [16, 64, 4, 4]            --
│    └─Empty: 2-3605                     [16, 64, 4, 4]            --
│    └─Clamp: 2-3606                     [16, 64, 4, 4]            --
├─FusedMaxPoolConv2dBNReLU: 1-270        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3607                 [16, 64, 2, 2]            --
│    └─Empty: 2-3608                     [16, 64, 2, 2]            --
│    └─Empty: 2-3609                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3610        --                        --
│    └─One: 2-3611                       [1]                       --
│    └─OutputScale: 2-3612               --                        --
│    └─Empty: 2-3613                     [64, 64, 1, 1]            --
│    └─Empty: 2-3614                     [64, 64, 1, 1]            --
│    └─Empty: 2-3615                     [64]                      --
│    └─Empty: 2-3616                     [64]                      --
│    └─BatchNorm2d: 2-3617               [16, 64, 2, 2]            --
│    └─Scaler: 2-3618                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3619                      [16, 64, 2, 2]            --
│    └─Empty: 2-3620                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3621                     [16, 64, 2, 2]            --
├─FusedConv2dBNReLU: 1-271               [16, 64, 2, 2]            (recursive)
│    └─OutputShiftSqueeze: 2-3622        --                        --
│    └─One: 2-3623                       [1]                       --
│    └─OutputScale: 2-3624               --                        --
│    └─Empty: 2-3625                     [64, 64, 1, 1]            --
│    └─Empty: 2-3626                     [64, 64, 1, 1]            --
│    └─Empty: 2-3627                     [64]                      --
│    └─Empty: 2-3628                     [64]                      --
│    └─BatchNorm2d: 2-3629               [16, 64, 2, 2]            --
│    └─Scaler: 2-3630                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3631                      [16, 64, 2, 2]            --
│    └─Empty: 2-3632                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3633                     [16, 64, 2, 2]            --
├─FusedMaxPoolConv2dBNReLU: 1-272        [16, 64, 2, 2]            (recursive)
│    └─MaxPool2d: 2-3634                 [16, 64, 2, 2]            --
│    └─Empty: 2-3635                     [16, 64, 2, 2]            --
│    └─Empty: 2-3636                     [16, 64, 2, 2]            --
│    └─OutputShiftSqueeze: 2-3637        --                        --
│    └─One: 2-3638                       [1]                       --
│    └─OutputScale: 2-3639               --                        --
│    └─Empty: 2-3640                     [64, 64, 3, 3]            --
│    └─Empty: 2-3641                     [64, 64, 3, 3]            --
│    └─Empty: 2-3642                     [64]                      --
│    └─Empty: 2-3643                     [64]                      --
│    └─BatchNorm2d: 2-3644               [16, 64, 2, 2]            --
│    └─Scaler: 2-3645                    [16, 64, 2, 2]            --
│    └─ReLU: 2-3646                      [16, 64, 2, 2]            --
│    └─Empty: 2-3647                     [16, 64, 2, 2]            --
│    └─Clamp: 2-3648                     [16, 64, 2, 2]            --
├─FusedConv1dBNReLU: 1-273               [16, 64, 12]              81,990
│    └─OutputShiftSqueeze: 2-3649        --                        --
│    └─One: 2-3650                       [1]                       --
│    └─OutputScale: 2-3651               --                        --
│    └─Empty: 2-3652                     [64, 256, 5]              --
│    └─Empty: 2-3653                     [64, 256, 5]              --
│    └─Empty: 2-3654                     [64]                      --
│    └─Empty: 2-3655                     [64]                      --
│    └─BatchNorm1d: 2-3656               [16, 64, 12]              --
│    └─Scaler: 2-3657                    [16, 64, 12]              --
│    └─ReLU: 2-3658                      [16, 64, 12]              --
│    └─Empty: 2-3659                     [16, 64, 12]              --
│    └─Clamp: 2-3660                     [16, 64, 12]              --
├─FusedConv1dBNReLU: 1-274               [16, 64, 8]               20,550
│    └─OutputShiftSqueeze: 2-3661        --                        --
│    └─One: 2-3662                       [1]                       --
│    └─OutputScale: 2-3663               --                        --
│    └─Empty: 2-3664                     [64, 64, 5]               --
│    └─Empty: 2-3665                     [64, 64, 5]               --
│    └─Empty: 2-3666                     [64]                      --
│    └─Empty: 2-3667                     [64]                      --
│    └─BatchNorm1d: 2-3668               [16, 64, 8]               --
│    └─Scaler: 2-3669                    [16, 64, 8]               --
│    └─ReLU: 2-3670                      [16, 64, 8]               --
│    └─Empty: 2-3671                     [16, 64, 8]               --
│    └─Clamp: 2-3672                     [16, 64, 8]               --
├─FusedConv1dBNReLU: 1-275               [16, 64, 4]               20,550
│    └─OutputShiftSqueeze: 2-3673        --                        --
│    └─One: 2-3674                       [1]                       --
│    └─OutputScale: 2-3675               --                        --
│    └─Empty: 2-3676                     [64, 64, 5]               --
│    └─Empty: 2-3677                     [64, 64, 5]               --
│    └─Empty: 2-3678                     [64]                      --
│    └─Empty: 2-3679                     [64]                      --
│    └─BatchNorm1d: 2-3680               [16, 64, 4]               --
│    └─Scaler: 2-3681                    [16, 64, 4]               --
│    └─ReLU: 2-3682                      [16, 64, 4]               --
│    └─Empty: 2-3683                     [16, 64, 4]               --
│    └─Clamp: 2-3684                     [16, 64, 4]               --
├─FusedConv1dBNReLU: 1-276               [16, 64, 2]               12,358
│    └─OutputShiftSqueeze: 2-3685        --                        --
│    └─One: 2-3686                       [1]                       --
│    └─OutputScale: 2-3687               --                        --
│    └─Empty: 2-3688                     [64, 64, 3]               --
│    └─Empty: 2-3689                     [64, 64, 3]               --
│    └─Empty: 2-3690                     [64]                      --
│    └─Empty: 2-3691                     [64]                      --
│    └─BatchNorm1d: 2-3692               [16, 64, 2]               --
│    └─Scaler: 2-3693                    [16, 64, 2]               --
│    └─ReLU: 2-3694                      [16, 64, 2]               --
│    └─Empty: 2-3695                     [16, 64, 2]               --
│    └─Clamp: 2-3696                     [16, 64, 2]               --
├─FusedConv1dBNReLU: 1-277               [16, 64, 12]              49,222
│    └─OutputShiftSqueeze: 2-3697        --                        --
│    └─One: 2-3698                       [1]                       --
│    └─OutputScale: 2-3699               --                        --
│    └─Empty: 2-3700                     [64, 256, 3]              --
│    └─Empty: 2-3701                     [64, 256, 3]              --
│    └─Empty: 2-3702                     [64]                      --
│    └─Empty: 2-3703                     [64]                      --
│    └─BatchNorm1d: 2-3704               [16, 64, 12]              --
│    └─Scaler: 2-3705                    [16, 64, 12]              --
│    └─ReLU: 2-3706                      [16, 64, 12]              --
│    └─Empty: 2-3707                     [16, 64, 12]              --
│    └─Clamp: 2-3708                     [16, 64, 12]              --
├─FusedConv1dBNReLU: 1-278               [16, 64, 8]               12,358
│    └─OutputShiftSqueeze: 2-3709        --                        --
│    └─One: 2-3710                       [1]                       --
│    └─OutputScale: 2-3711               --                        --
│    └─Empty: 2-3712                     [64, 64, 3]               --
│    └─Empty: 2-3713                     [64, 64, 3]               --
│    └─Empty: 2-3714                     [64]                      --
│    └─Empty: 2-3715                     [64]                      --
│    └─BatchNorm1d: 2-3716               [16, 64, 8]               --
│    └─Scaler: 2-3717                    [16, 64, 8]               --
│    └─ReLU: 2-3718                      [16, 64, 8]               --
│    └─Empty: 2-3719                     [16, 64, 8]               --
│    └─Clamp: 2-3720                     [16, 64, 8]               --
├─FusedConv1dBNReLU: 1-279               [16, 64, 4]               12,358
│    └─OutputShiftSqueeze: 2-3721        --                        --
│    └─One: 2-3722                       [1]                       --
│    └─OutputScale: 2-3723               --                        --
│    └─Empty: 2-3724                     [64, 64, 3]               --
│    └─Empty: 2-3725                     [64, 64, 3]               --
│    └─Empty: 2-3726                     [64]                      --
│    └─Empty: 2-3727                     [64]                      --
│    └─BatchNorm1d: 2-3728               [16, 64, 4]               --
│    └─Scaler: 2-3729                    [16, 64, 4]               --
│    └─ReLU: 2-3730                      [16, 64, 4]               --
│    └─Empty: 2-3731                     [16, 64, 4]               --
│    └─Clamp: 2-3732                     [16, 64, 4]               --
├─FusedConv1dBNReLU: 1-280               [16, 64, 2]               12,358
│    └─OutputShiftSqueeze: 2-3733        --                        --
│    └─One: 2-3734                       [1]                       --
│    └─OutputScale: 2-3735               --                        --
│    └─Empty: 2-3736                     [64, 64, 3]               --
│    └─Empty: 2-3737                     [64, 64, 3]               --
│    └─Empty: 2-3738                     [64]                      --
│    └─Empty: 2-3739                     [64]                      --
│    └─BatchNorm1d: 2-3740               [16, 64, 2]               --
│    └─Scaler: 2-3741                    [16, 64, 2]               --
│    └─ReLU: 2-3742                      [16, 64, 2]               --
│    └─Empty: 2-3743                     [16, 64, 2]               --
│    └─Clamp: 2-3744                     [16, 64, 2]               --
├─FusedConv1dBNReLU: 1-281               [16, 64, 14]              49,222
│    └─OutputShiftSqueeze: 2-3745        --                        --
│    └─One: 2-3746                       [1]                       --
│    └─OutputScale: 2-3747               --                        --
│    └─Empty: 2-3748                     [64, 256, 3]              --
│    └─Empty: 2-3749                     [64, 256, 3]              --
│    └─Empty: 2-3750                     [64]                      --
│    └─Empty: 2-3751                     [64]                      --
│    └─BatchNorm1d: 2-3752               [16, 64, 14]              --
│    └─Scaler: 2-3753                    [16, 64, 14]              --
│    └─ReLU: 2-3754                      [16, 64, 14]              --
│    └─Empty: 2-3755                     [16, 64, 14]              --
│    └─Clamp: 2-3756                     [16, 64, 14]              --
├─FusedConv1dBNReLU: 1-282               [16, 64, 10]              12,358
│    └─OutputShiftSqueeze: 2-3757        --                        --
│    └─One: 2-3758                       [1]                       --
│    └─OutputScale: 2-3759               --                        --
│    └─Empty: 2-3760                     [64, 64, 3]               --
│    └─Empty: 2-3761                     [64, 64, 3]               --
│    └─Empty: 2-3762                     [64]                      --
│    └─Empty: 2-3763                     [64]                      --
│    └─BatchNorm1d: 2-3764               [16, 64, 10]              --
│    └─Scaler: 2-3765                    [16, 64, 10]              --
│    └─ReLU: 2-3766                      [16, 64, 10]              --
│    └─Empty: 2-3767                     [16, 64, 10]              --
│    └─Clamp: 2-3768                     [16, 64, 10]              --
├─FusedConv1dBNReLU: 1-283               [16, 64, 2]               12,358
│    └─OutputShiftSqueeze: 2-3769        --                        --
│    └─One: 2-3770                       [1]                       --
│    └─OutputScale: 2-3771               --                        --
│    └─Empty: 2-3772                     [64, 64, 3]               --
│    └─Empty: 2-3773                     [64, 64, 3]               --
│    └─Empty: 2-3774                     [64]                      --
│    └─Empty: 2-3775                     [64]                      --
│    └─BatchNorm1d: 2-3776               [16, 64, 2]               --
│    └─Scaler: 2-3777                    [16, 64, 2]               --
│    └─ReLU: 2-3778                      [16, 64, 2]               --
│    └─Empty: 2-3779                     [16, 64, 2]               --
│    └─Clamp: 2-3780                     [16, 64, 2]               --
├─FusedLinearReLU: 1-284                 [16, 32]                  4,134
│    └─OutputShiftSqueeze: 2-3781        --                        --
│    └─One: 2-3782                       [1]                       --
│    └─OutputScale: 2-3783               --                        --
│    └─Empty: 2-3784                     [32, 128]                 --
│    └─Empty: 2-3785                     [32, 128]                 --
│    └─Empty: 2-3786                     [32]                      --
│    └─Empty: 2-3787                     [32]                      --
│    └─Scaler: 2-3788                    [16, 32]                  --
│    └─ReLU: 2-3789                      [16, 32]                  --
│    └─Empty: 2-3790                     [16, 32]                  --
│    └─Clamp: 2-3791                     [16, 32]                  --
├─Linear: 1-285                          [16, 5]                   166
│    └─OutputShiftSqueeze: 2-3792        --                        --
│    └─One: 2-3793                       [1]                       --
│    └─OutputScale: 2-3794               --                        --
│    └─Empty: 2-3795                     [5, 32]                   --
│    └─Empty: 2-3796                     [5, 32]                   --
│    └─Empty: 2-3797                     [16, 5]                   --
│    └─Empty: 2-3798                     [16, 5]                   --
│    └─Clamp: 2-3799                     [16, 5]                   --
==========================================================================================
Total params: 730,228
Trainable params: 730,048
Non-trainable params: 180
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 201.33
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 201.33
==========================================================================================
I - Epoch: 0
	I - Batch: 10 | Loss: 1.611 | Acc: 37.500%
	I - Batch: 20 | Loss: 1.593 | Acc: 42.812%
	I - Batch: 30 | Loss: 1.567 | Acc: 47.708%
	I - Batch: 40 | Loss: 1.564 | Acc: 49.219%
	I - Batch: 50 | Loss: 1.561 | Acc: 49.750%
	I - Batch: 60 | Loss: 1.545 | Acc: 51.354%
	I - Batch: 70 | Loss: 1.541 | Acc: 52.054%
	I - Batch: 80 | Loss: 1.530 | Acc: 52.422%
	I - Batch: 90 | Loss: 1.519 | Acc: 53.056%
	I - Batch: 100 | Loss: 1.511 | Acc: 53.062%
	I - Batch: 110 | Loss: 1.507 | Acc: 53.125%
	I - Batch: 120 | Loss: 1.509 | Acc: 52.396%
	I - Batch: 130 | Loss: 1.503 | Acc: 52.452%
	I - Batch: 140 | Loss: 1.497 | Acc: 52.411%
	I - Batch: 150 | Loss: 1.498 | Acc: 51.792%
	I - Batch: 160 | Loss: 1.495 | Acc: 51.523%
	I - Batch: 170 | Loss: 1.495 | Acc: 51.581%
	I - Batch: 180 | Loss: 1.499 | Acc: 51.042%
	I - Batch: 190 | Loss: 1.499 | Acc: 51.020%
	I - Batch: 200 | Loss: 1.493 | Acc: 51.344%
	I - Batch: 210 | Loss: 1.491 | Acc: 51.577%
	I - Batch: 220 | Loss: 1.489 | Acc: 51.534%
	I - Batch: 230 | Loss: 1.489 | Acc: 51.413%
	I - Batch: 240 | Loss: 1.486 | Acc: 51.641%
	I - Batch: 250 | Loss: 1.485 | Acc: 51.650%
	I - Batch: 260 | Loss: 1.480 | Acc: 52.115%
	I - Batch: 270 | Loss: 1.478 | Acc: 52.269%
	I - Batch: 280 | Loss: 1.473 | Acc: 52.522%
	I - Batch: 290 | Loss: 1.470 | Acc: 52.737%
	I - Batch: 300 | Loss: 1.464 | Acc: 53.000%
	I - Batch: 310 | Loss: 1.460 | Acc: 53.044%
	I - Batch: 320 | Loss: 1.458 | Acc: 53.184%
	I - Batch: 330 | Loss: 1.456 | Acc: 53.258%
	I - Batch: 340 | Loss: 1.456 | Acc: 53.346%
	I - Batch: 350 | Loss: 1.452 | Acc: 53.411%
	I - Batch: 360 | Loss: 1.450 | Acc: 53.385%
	I - Batch: 370 | Loss: 1.447 | Acc: 53.311%
	I - Batch: 380 | Loss: 1.444 | Acc: 53.470%
	I - Batch: 390 | Loss: 1.444 | Acc: 53.462%
	I - Batch: 400 | Loss: 1.441 | Acc: 53.484%
	I - Batch: 410 | Loss: 1.439 | Acc: 53.506%
	I - Batch: 420 | Loss: 1.437 | Acc: 53.631%
	I - Batch: 430 | Loss: 1.435 | Acc: 53.779%
	I - Batch: 440 | Loss: 1.429 | Acc: 53.991%
	I - Batch: 450 | Loss: 1.426 | Acc: 54.069%
	I - Batch: 460 | Loss: 1.426 | Acc: 54.198%
	I - Batch: 470 | Loss: 1.424 | Acc: 54.295%
	I - Batch: 480 | Loss: 1.421 | Acc: 54.375%
	I - Batch: 490 | Loss: 1.417 | Acc: 54.515%
	I - Batch: 500 | Loss: 1.414 | Acc: 54.650%
	I - Batch: 510 | Loss: 1.413 | Acc: 54.681%
	I - Batch: 520 | Loss: 1.410 | Acc: 54.820%
	I - Batch: 530 | Loss: 1.409 | Acc: 54.835%
	I - Batch: 540 | Loss: 1.408 | Acc: 54.919%
	I - Batch: 550 | Loss: 1.407 | Acc: 55.023%
	I - Batch: 560 | Loss: 1.404 | Acc: 55.067%
	I - Batch: 570 | Loss: 1.402 | Acc: 55.197%
	I - Batch: 580 | Loss: 1.403 | Acc: 55.151%
	I - Batch: 590 | Loss: 1.400 | Acc: 55.244%
	I - Batch: 600 | Loss: 1.399 | Acc: 55.208%
	I - Batch: 610 | Loss: 1.400 | Acc: 55.164%
	I - Batch: 620 | Loss: 1.398 | Acc: 55.292%
	I - Batch: 630 | Loss: 1.397 | Acc: 55.337%
	I - Batch: 640 | Loss: 1.395 | Acc: 55.400%
	I - Batch: 650 | Loss: 1.394 | Acc: 55.500%
	I - Batch: 660 | Loss: 1.392 | Acc: 55.606%
	I - Batch: 670 | Loss: 1.391 | Acc: 55.653%
	I - Batch: 680 | Loss: 1.391 | Acc: 55.607%
	I - Batch: 690 | Loss: 1.389 | Acc: 55.697%
	I - Batch: 700 | Loss: 1.389 | Acc: 55.652%
I - num batch: 701
I - Train -- Loss: 1.389 | Acc: 55.653% | LR: 1.000000e-03 | Dur: 558.52s
I - Confusion Matrix: [row->prediction - col->label]
[[ 737.   56.  118.  590.  763.]
 [   3.    0.    0.    4.    6.]
 [   4.    3.    0.    4.   14.]
 [ 256.   55.   72.  293.  373.]
 [ 546.  621.  873.  613. 5212.]]

I - Val -- Loss: 1.335 | Acc: 44.848%
I - Confusion Matrix: [row->prediction - col->label]
[[408.  32.  34. 390. 181.]
 [  0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.]
 [ 30.  41.  31.  62. 120.]
 [ 97. 297. 327. 122. 914.]]

I - Local maximum validation set accuracy:  44.85
I - Epoch: 1
	I - Batch: 10 | Loss: 1.248 | Acc: 60.625%
	I - Batch: 20 | Loss: 1.313 | Acc: 58.125%
	I - Batch: 30 | Loss: 1.322 | Acc: 57.708%
	I - Batch: 40 | Loss: 1.296 | Acc: 59.219%
	I - Batch: 50 | Loss: 1.293 | Acc: 60.000%
	I - Batch: 60 | Loss: 1.280 | Acc: 60.938%
	I - Batch: 70 | Loss: 1.289 | Acc: 60.714%
	I - Batch: 80 | Loss: 1.290 | Acc: 60.938%
	I - Batch: 90 | Loss: 1.309 | Acc: 60.000%
	I - Batch: 100 | Loss: 1.310 | Acc: 59.438%
	I - Batch: 110 | Loss: 1.314 | Acc: 59.034%
	I - Batch: 120 | Loss: 1.309 | Acc: 59.010%
	I - Batch: 130 | Loss: 1.311 | Acc: 58.750%
	I - Batch: 140 | Loss: 1.306 | Acc: 58.884%
	I - Batch: 150 | Loss: 1.304 | Acc: 58.958%
	I - Batch: 160 | Loss: 1.309 | Acc: 58.867%
	I - Batch: 170 | Loss: 1.314 | Acc: 59.191%
	I - Batch: 180 | Loss: 1.308 | Acc: 59.444%
	I - Batch: 190 | Loss: 1.308 | Acc: 59.408%
	I - Batch: 200 | Loss: 1.305 | Acc: 59.656%
	I - Batch: 210 | Loss: 1.307 | Acc: 59.345%
	I - Batch: 220 | Loss: 1.306 | Acc: 59.290%
	I - Batch: 230 | Loss: 1.307 | Acc: 59.239%
	I - Batch: 240 | Loss: 1.302 | Acc: 59.531%
	I - Batch: 250 | Loss: 1.299 | Acc: 59.675%
	I - Batch: 260 | Loss: 1.297 | Acc: 59.663%
	I - Batch: 270 | Loss: 1.292 | Acc: 59.769%
	I - Batch: 280 | Loss: 1.293 | Acc: 59.576%
	I - Batch: 290 | Loss: 1.291 | Acc: 59.612%
	I - Batch: 300 | Loss: 1.287 | Acc: 59.729%
	I - Batch: 310 | Loss: 1.286 | Acc: 59.738%
	I - Batch: 320 | Loss: 1.284 | Acc: 59.688%
	I - Batch: 330 | Loss: 1.283 | Acc: 59.867%
	I - Batch: 340 | Loss: 1.286 | Acc: 59.669%
	I - Batch: 350 | Loss: 1.288 | Acc: 59.643%
	I - Batch: 360 | Loss: 1.289 | Acc: 59.479%
	I - Batch: 370 | Loss: 1.290 | Acc: 59.409%
	I - Batch: 380 | Loss: 1.291 | Acc: 59.260%
	I - Batch: 390 | Loss: 1.287 | Acc: 59.231%
	I - Batch: 400 | Loss: 1.287 | Acc: 59.328%
	I - Batch: 410 | Loss: 1.286 | Acc: 59.329%
	I - Batch: 420 | Loss: 1.286 | Acc: 59.330%
	I - Batch: 430 | Loss: 1.286 | Acc: 59.259%
	I - Batch: 440 | Loss: 1.286 | Acc: 59.176%
	I - Batch: 450 | Loss: 1.284 | Acc: 59.208%
	I - Batch: 460 | Loss: 1.284 | Acc: 59.035%
	I - Batch: 470 | Loss: 1.283 | Acc: 58.816%
	I - Batch: 480 | Loss: 1.280 | Acc: 58.919%
	I - Batch: 490 | Loss: 1.278 | Acc: 58.980%
	I - Batch: 500 | Loss: 1.279 | Acc: 58.812%
	I - Batch: 510 | Loss: 1.277 | Acc: 58.811%
	I - Batch: 520 | Loss: 1.277 | Acc: 58.726%
	I - Batch: 530 | Loss: 1.276 | Acc: 58.656%
	I - Batch: 540 | Loss: 1.277 | Acc: 58.519%
	I - Batch: 550 | Loss: 1.279 | Acc: 58.432%
	I - Batch: 560 | Loss: 1.281 | Acc: 58.359%
	I - Batch: 570 | Loss: 1.281 | Acc: 58.333%
	I - Batch: 580 | Loss: 1.281 | Acc: 58.233%
	I - Batch: 590 | Loss: 1.280 | Acc: 58.263%
	I - Batch: 600 | Loss: 1.279 | Acc: 58.240%
	I - Batch: 610 | Loss: 1.280 | Acc: 58.074%
	I - Batch: 620 | Loss: 1.281 | Acc: 58.004%
	I - Batch: 630 | Loss: 1.280 | Acc: 57.946%
	I - Batch: 640 | Loss: 1.279 | Acc: 57.900%
	I - Batch: 650 | Loss: 1.278 | Acc: 57.865%
	I - Batch: 660 | Loss: 1.277 | Acc: 57.831%
	I - Batch: 670 | Loss: 1.276 | Acc: 57.780%
	I - Batch: 680 | Loss: 1.277 | Acc: 57.684%
	I - Batch: 690 | Loss: 1.276 | Acc: 57.681%
	I - Batch: 700 | Loss: 1.275 | Acc: 57.750%
I - num batch: 701
I - Train -- Loss: 1.275 | Acc: 57.739% | LR: 1.000000e-03 | Dur: 577.98s
I - Confusion Matrix: [row->prediction - col->label]
[[ 869.   31.   50.  575.  551.]
 [   0.    0.    0.    0.    0.]
 [  16.  137.  176.   67.  327.]
 [ 275.   85.  122.  486.  545.]
 [ 386.  482.  715.  376. 4945.]]

I - Val -- Loss: 1.320 | Acc: 52.041%
I - Confusion Matrix: [row->prediction - col->label]
[[ 332.    6.   15.  212.   61.]
 [   0.    0.    0.    0.    0.]
 [   7.   39.   46.   15.   46.]
 [  49.   30.   24.  191.   71.]
 [ 147.  295.  307.  156. 1037.]]

I - Local maximum validation set accuracy:  52.04
I - Epoch: 2
	I - Batch: 10 | Loss: 1.271 | Acc: 59.375%
	I - Batch: 20 | Loss: 1.288 | Acc: 59.375%
	I - Batch: 30 | Loss: 1.248 | Acc: 59.167%
	I - Batch: 40 | Loss: 1.239 | Acc: 60.000%
	I - Batch: 50 | Loss: 1.232 | Acc: 60.125%
	I - Batch: 60 | Loss: 1.216 | Acc: 60.521%
	I - Batch: 70 | Loss: 1.233 | Acc: 59.196%
	I - Batch: 80 | Loss: 1.224 | Acc: 59.688%
	I - Batch: 90 | Loss: 1.226 | Acc: 59.444%
	I - Batch: 100 | Loss: 1.231 | Acc: 59.312%
	I - Batch: 110 | Loss: 1.244 | Acc: 58.977%
	I - Batch: 120 | Loss: 1.243 | Acc: 59.062%
	I - Batch: 130 | Loss: 1.242 | Acc: 59.135%
	I - Batch: 140 | Loss: 1.233 | Acc: 59.330%
	I - Batch: 150 | Loss: 1.234 | Acc: 59.125%
	I - Batch: 160 | Loss: 1.231 | Acc: 58.906%
	I - Batch: 170 | Loss: 1.225 | Acc: 58.934%
	I - Batch: 180 | Loss: 1.226 | Acc: 58.750%
	I - Batch: 190 | Loss: 1.232 | Acc: 58.520%
	I - Batch: 200 | Loss: 1.234 | Acc: 58.250%
	I - Batch: 210 | Loss: 1.233 | Acc: 58.155%
	I - Batch: 220 | Loss: 1.237 | Acc: 58.182%
	I - Batch: 230 | Loss: 1.235 | Acc: 58.071%
	I - Batch: 240 | Loss: 1.238 | Acc: 58.047%
	I - Batch: 250 | Loss: 1.243 | Acc: 57.650%
	I - Batch: 260 | Loss: 1.245 | Acc: 57.332%
	I - Batch: 270 | Loss: 1.239 | Acc: 57.384%
	I - Batch: 280 | Loss: 1.240 | Acc: 57.143%
	I - Batch: 290 | Loss: 1.237 | Acc: 57.112%
	I - Batch: 300 | Loss: 1.237 | Acc: 57.188%
	I - Batch: 310 | Loss: 1.235 | Acc: 57.419%
	I - Batch: 320 | Loss: 1.233 | Acc: 57.559%
	I - Batch: 330 | Loss: 1.231 | Acc: 57.595%
	I - Batch: 340 | Loss: 1.229 | Acc: 57.684%
	I - Batch: 350 | Loss: 1.227 | Acc: 57.768%
	I - Batch: 360 | Loss: 1.225 | Acc: 57.847%
	I - Batch: 370 | Loss: 1.224 | Acc: 57.855%
	I - Batch: 380 | Loss: 1.223 | Acc: 57.845%
	I - Batch: 390 | Loss: 1.223 | Acc: 57.756%
	I - Batch: 400 | Loss: 1.223 | Acc: 57.828%
	I - Batch: 410 | Loss: 1.224 | Acc: 57.591%
	I - Batch: 420 | Loss: 1.223 | Acc: 57.798%
	I - Batch: 430 | Loss: 1.224 | Acc: 57.791%
	I - Batch: 440 | Loss: 1.224 | Acc: 57.827%
	I - Batch: 450 | Loss: 1.221 | Acc: 58.014%
	I - Batch: 460 | Loss: 1.221 | Acc: 58.030%
	I - Batch: 470 | Loss: 1.220 | Acc: 57.939%
	I - Batch: 480 | Loss: 1.219 | Acc: 57.904%
	I - Batch: 490 | Loss: 1.221 | Acc: 57.781%
	I - Batch: 500 | Loss: 1.220 | Acc: 57.788%
	I - Batch: 510 | Loss: 1.220 | Acc: 57.855%
	I - Batch: 520 | Loss: 1.221 | Acc: 57.752%
	I - Batch: 530 | Loss: 1.221 | Acc: 57.724%
	I - Batch: 540 | Loss: 1.221 | Acc: 57.627%
	I - Batch: 550 | Loss: 1.222 | Acc: 57.523%
	I - Batch: 560 | Loss: 1.223 | Acc: 57.366%
	I - Batch: 570 | Loss: 1.221 | Acc: 57.325%
	I - Batch: 580 | Loss: 1.219 | Acc: 57.468%
	I - Batch: 590 | Loss: 1.218 | Acc: 57.511%
	I - Batch: 600 | Loss: 1.219 | Acc: 57.583%
	I - Batch: 610 | Loss: 1.218 | Acc: 57.654%
	I - Batch: 620 | Loss: 1.216 | Acc: 57.873%
	I - Batch: 630 | Loss: 1.215 | Acc: 58.046%
	I - Batch: 640 | Loss: 1.214 | Acc: 58.115%
	I - Batch: 650 | Loss: 1.213 | Acc: 58.163%
	I - Batch: 660 | Loss: 1.213 | Acc: 58.220%
	I - Batch: 670 | Loss: 1.213 | Acc: 58.340%
	I - Batch: 680 | Loss: 1.211 | Acc: 58.520%
	I - Batch: 690 | Loss: 1.211 | Acc: 58.460%
	I - Batch: 700 | Loss: 1.210 | Acc: 58.518%
I - num batch: 701
I - Train -- Loss: 1.211 | Acc: 58.470% | LR: 1.000000e-03 | Dur: 573.76s
I - Confusion Matrix: [row->prediction - col->label]
[[ 911.   21.   36.  488.  523.]
 [   0.    0.    0.    0.    0.]
 [  30.  292.  362.   82.  561.]
 [ 306.  112.  166.  616.  615.]
 [ 299.  310.  499.  318. 4669.]]

I - Val -- Loss: 1.254 | Acc: 52.981%
I - Confusion Matrix: [row->prediction - col->label]
[[ 356.    9.   11.  222.   52.]
 [   0.    0.    0.    0.    0.]
 [   6.  150.  130.   25.  102.]
 [  24.    6.   16.  117.   29.]
 [ 149.  205.  235.  210. 1032.]]

I - Local maximum validation set accuracy:  52.98
I - Epoch: 3
	I - Batch: 10 | Loss: 1.092 | Acc: 62.500%
	I - Batch: 20 | Loss: 1.116 | Acc: 60.625%
	I - Batch: 30 | Loss: 1.106 | Acc: 62.500%
	I - Batch: 40 | Loss: 1.109 | Acc: 63.750%
	I - Batch: 50 | Loss: 1.125 | Acc: 63.375%
	I - Batch: 60 | Loss: 1.129 | Acc: 62.917%
	I - Batch: 70 | Loss: 1.136 | Acc: 62.679%
	I - Batch: 80 | Loss: 1.130 | Acc: 62.266%
	I - Batch: 90 | Loss: 1.136 | Acc: 61.806%
	I - Batch: 100 | Loss: 1.142 | Acc: 61.375%
	I - Batch: 110 | Loss: 1.150 | Acc: 60.795%
	I - Batch: 120 | Loss: 1.156 | Acc: 60.365%
	I - Batch: 130 | Loss: 1.162 | Acc: 60.192%
	I - Batch: 140 | Loss: 1.162 | Acc: 60.045%
	I - Batch: 150 | Loss: 1.161 | Acc: 60.083%
	I - Batch: 160 | Loss: 1.160 | Acc: 59.727%
	I - Batch: 170 | Loss: 1.162 | Acc: 59.743%
	I - Batch: 180 | Loss: 1.158 | Acc: 60.069%
	I - Batch: 190 | Loss: 1.161 | Acc: 59.934%
	I - Batch: 200 | Loss: 1.161 | Acc: 59.812%
	I - Batch: 210 | Loss: 1.158 | Acc: 59.821%
	I - Batch: 220 | Loss: 1.161 | Acc: 59.091%
	I - Batch: 230 | Loss: 1.161 | Acc: 59.103%
	I - Batch: 240 | Loss: 1.163 | Acc: 59.036%
	I - Batch: 250 | Loss: 1.161 | Acc: 59.250%
	I - Batch: 260 | Loss: 1.162 | Acc: 59.447%
	I - Batch: 270 | Loss: 1.164 | Acc: 59.606%
	I - Batch: 280 | Loss: 1.168 | Acc: 59.531%
	I - Batch: 290 | Loss: 1.167 | Acc: 59.526%
	I - Batch: 300 | Loss: 1.164 | Acc: 59.625%
	I - Batch: 310 | Loss: 1.162 | Acc: 59.899%
	I - Batch: 320 | Loss: 1.161 | Acc: 60.039%
	I - Batch: 330 | Loss: 1.162 | Acc: 60.152%
	I - Batch: 340 | Loss: 1.165 | Acc: 60.129%
	I - Batch: 350 | Loss: 1.163 | Acc: 60.196%
	I - Batch: 360 | Loss: 1.165 | Acc: 59.965%
	I - Batch: 370 | Loss: 1.166 | Acc: 59.916%
	I - Batch: 380 | Loss: 1.168 | Acc: 59.819%
	I - Batch: 390 | Loss: 1.166 | Acc: 59.728%
	I - Batch: 400 | Loss: 1.165 | Acc: 59.766%
	I - Batch: 410 | Loss: 1.163 | Acc: 59.863%
	I - Batch: 420 | Loss: 1.161 | Acc: 60.045%
	I - Batch: 430 | Loss: 1.162 | Acc: 60.160%
	I - Batch: 440 | Loss: 1.164 | Acc: 60.199%
	I - Batch: 450 | Loss: 1.165 | Acc: 60.181%
	I - Batch: 460 | Loss: 1.162 | Acc: 60.326%
	I - Batch: 470 | Loss: 1.163 | Acc: 60.266%
	I - Batch: 480 | Loss: 1.159 | Acc: 60.391%
	I - Batch: 490 | Loss: 1.157 | Acc: 60.472%
	I - Batch: 500 | Loss: 1.157 | Acc: 60.487%
	I - Batch: 510 | Loss: 1.156 | Acc: 60.625%
	I - Batch: 520 | Loss: 1.155 | Acc: 60.769%
	I - Batch: 530 | Loss: 1.155 | Acc: 60.755%
	I - Batch: 540 | Loss: 1.155 | Acc: 60.822%
	I - Batch: 550 | Loss: 1.154 | Acc: 60.955%
	I - Batch: 560 | Loss: 1.154 | Acc: 61.004%
	I - Batch: 570 | Loss: 1.154 | Acc: 61.020%
	I - Batch: 580 | Loss: 1.157 | Acc: 61.002%
	I - Batch: 590 | Loss: 1.159 | Acc: 60.932%
	I - Batch: 600 | Loss: 1.158 | Acc: 60.979%
	I - Batch: 610 | Loss: 1.157 | Acc: 61.035%
	I - Batch: 620 | Loss: 1.156 | Acc: 61.079%
	I - Batch: 630 | Loss: 1.157 | Acc: 61.062%
	I - Batch: 640 | Loss: 1.157 | Acc: 61.055%
	I - Batch: 650 | Loss: 1.156 | Acc: 61.144%
	I - Batch: 660 | Loss: 1.157 | Acc: 61.117%
	I - Batch: 670 | Loss: 1.156 | Acc: 61.194%
	I - Batch: 680 | Loss: 1.157 | Acc: 61.213%
	I - Batch: 690 | Loss: 1.156 | Acc: 61.304%
	I - Batch: 700 | Loss: 1.154 | Acc: 61.286%
I - num batch: 701
I - Train -- Loss: 1.155 | Acc: 61.279% | LR: 1.000000e-03 | Dur: 550.11s
I - Confusion Matrix: [row->prediction - col->label]
[[ 913.   16.   36.  365.  418.]
 [   0.    1.    4.    0.    2.]
 [  30.  360.  460.   86.  568.]
 [ 308.  109.  154.  767.  648.]
 [ 295.  249.  409.  286. 4732.]]

I - Val -- Loss: 1.206 | Acc: 51.782%
I - Confusion Matrix: [row->prediction - col->label]
[[385.  10.  20. 271.  99.]
 [  0.   0.   0.   0.   0.]
 [ 12. 213. 208.  41. 172.]
 [ 45.  37.  42. 173. 112.]
 [ 93. 110. 122.  89. 832.]]

I - Epoch: 4
	I - Batch: 10 | Loss: 1.180 | Acc: 60.000%
	I - Batch: 20 | Loss: 1.115 | Acc: 62.188%
	I - Batch: 30 | Loss: 1.119 | Acc: 63.333%
	I - Batch: 40 | Loss: 1.111 | Acc: 63.594%
	I - Batch: 50 | Loss: 1.088 | Acc: 64.750%
	I - Batch: 60 | Loss: 1.085 | Acc: 64.167%
	I - Batch: 70 | Loss: 1.082 | Acc: 63.929%
	I - Batch: 80 | Loss: 1.079 | Acc: 63.438%
	I - Batch: 90 | Loss: 1.074 | Acc: 63.611%
	I - Batch: 100 | Loss: 1.075 | Acc: 63.312%
	I - Batch: 110 | Loss: 1.068 | Acc: 63.466%
	I - Batch: 120 | Loss: 1.067 | Acc: 63.333%
	I - Batch: 130 | Loss: 1.063 | Acc: 63.317%
	I - Batch: 140 | Loss: 1.064 | Acc: 63.214%
	I - Batch: 150 | Loss: 1.074 | Acc: 62.917%
	I - Batch: 160 | Loss: 1.084 | Acc: 62.891%
	I - Batch: 170 | Loss: 1.089 | Acc: 62.794%
	I - Batch: 180 | Loss: 1.096 | Acc: 62.604%
	I - Batch: 190 | Loss: 1.095 | Acc: 62.500%
	I - Batch: 200 | Loss: 1.097 | Acc: 62.188%
	I - Batch: 210 | Loss: 1.096 | Acc: 62.143%
	I - Batch: 220 | Loss: 1.097 | Acc: 62.074%
	I - Batch: 230 | Loss: 1.099 | Acc: 61.984%
	I - Batch: 240 | Loss: 1.100 | Acc: 61.849%
	I - Batch: 250 | Loss: 1.108 | Acc: 61.650%
	I - Batch: 260 | Loss: 1.106 | Acc: 61.779%
	I - Batch: 270 | Loss: 1.104 | Acc: 61.806%
	I - Batch: 280 | Loss: 1.106 | Acc: 61.808%
	I - Batch: 290 | Loss: 1.105 | Acc: 61.875%
	I - Batch: 300 | Loss: 1.104 | Acc: 61.854%
	I - Batch: 310 | Loss: 1.104 | Acc: 61.855%
	I - Batch: 320 | Loss: 1.102 | Acc: 62.070%
	I - Batch: 330 | Loss: 1.101 | Acc: 62.197%
	I - Batch: 340 | Loss: 1.102 | Acc: 62.132%
	I - Batch: 350 | Loss: 1.101 | Acc: 62.179%
	I - Batch: 360 | Loss: 1.101 | Acc: 62.222%
	I - Batch: 370 | Loss: 1.101 | Acc: 62.213%
	I - Batch: 380 | Loss: 1.105 | Acc: 62.007%
	I - Batch: 390 | Loss: 1.106 | Acc: 61.907%
	I - Batch: 400 | Loss: 1.110 | Acc: 61.750%
	I - Batch: 410 | Loss: 1.110 | Acc: 61.829%
	I - Batch: 420 | Loss: 1.111 | Acc: 61.830%
	I - Batch: 430 | Loss: 1.112 | Acc: 61.933%
	I - Batch: 440 | Loss: 1.116 | Acc: 61.733%
	I - Batch: 450 | Loss: 1.117 | Acc: 61.736%
	I - Batch: 460 | Loss: 1.117 | Acc: 61.630%
	I - Batch: 470 | Loss: 1.114 | Acc: 61.715%
	I - Batch: 480 | Loss: 1.115 | Acc: 61.797%
	I - Batch: 490 | Loss: 1.115 | Acc: 61.862%
	I - Batch: 500 | Loss: 1.113 | Acc: 61.925%
	I - Batch: 510 | Loss: 1.113 | Acc: 61.936%
	I - Batch: 520 | Loss: 1.114 | Acc: 61.983%
	I - Batch: 530 | Loss: 1.117 | Acc: 61.875%
	I - Batch: 540 | Loss: 1.113 | Acc: 62.049%
	I - Batch: 550 | Loss: 1.112 | Acc: 62.159%
	I - Batch: 560 | Loss: 1.111 | Acc: 62.254%
	I - Batch: 570 | Loss: 1.112 | Acc: 62.248%
	I - Batch: 580 | Loss: 1.111 | Acc: 62.231%
	I - Batch: 590 | Loss: 1.109 | Acc: 62.341%
	I - Batch: 600 | Loss: 1.108 | Acc: 62.354%
	I - Batch: 610 | Loss: 1.107 | Acc: 62.398%
	I - Batch: 620 | Loss: 1.108 | Acc: 62.379%
	I - Batch: 630 | Loss: 1.109 | Acc: 62.361%
	I - Batch: 640 | Loss: 1.107 | Acc: 62.334%
	I - Batch: 650 | Loss: 1.106 | Acc: 62.385%
	I - Batch: 660 | Loss: 1.106 | Acc: 62.453%
	I - Batch: 670 | Loss: 1.105 | Acc: 62.463%
	I - Batch: 680 | Loss: 1.103 | Acc: 62.583%
	I - Batch: 690 | Loss: 1.104 | Acc: 62.572%
	I - Batch: 700 | Loss: 1.107 | Acc: 62.509%
I - num batch: 701
I - Train -- Loss: 1.107 | Acc: 62.500% | LR: 1.000000e-03 | Dur: 549.36s
I - Confusion Matrix: [row->prediction - col->label]
[[ 914.   11.   19.  301.  418.]
 [   0.    0.    0.    0.    0.]
 [  23.  381.  495.   84.  559.]
 [ 332.  103.  147.  861.  651.]
 [ 277.  240.  402.  258. 4740.]]

I - Val -- Loss: 1.234 | Acc: 52.690%
I - Confusion Matrix: [row->prediction - col->label]
[[441.  22.  34. 275. 208.]
 [  0.   0.   0.   0.   0.]
 [  2. 126. 162.  12.  74.]
 [ 50.  76.  68. 237. 147.]
 [ 42. 146. 128.  50. 786.]]

I - Epoch: 5
	I - Batch: 10 | Loss: 1.016 | Acc: 66.875%
	I - Batch: 20 | Loss: 1.054 | Acc: 65.312%
	I - Batch: 30 | Loss: 1.033 | Acc: 65.208%
	I - Batch: 40 | Loss: 1.051 | Acc: 65.469%
	I - Batch: 50 | Loss: 1.055 | Acc: 64.875%
	I - Batch: 60 | Loss: 1.045 | Acc: 65.312%
	I - Batch: 70 | Loss: 1.050 | Acc: 64.643%
	I - Batch: 80 | Loss: 1.055 | Acc: 64.609%
	I - Batch: 90 | Loss: 1.057 | Acc: 64.306%
	I - Batch: 100 | Loss: 1.058 | Acc: 64.625%
	I - Batch: 110 | Loss: 1.049 | Acc: 65.057%
	I - Batch: 120 | Loss: 1.047 | Acc: 65.104%
	I - Batch: 130 | Loss: 1.041 | Acc: 65.481%
	I - Batch: 140 | Loss: 1.036 | Acc: 65.670%
	I - Batch: 150 | Loss: 1.039 | Acc: 65.792%
	I - Batch: 160 | Loss: 1.044 | Acc: 65.352%
	I - Batch: 170 | Loss: 1.042 | Acc: 65.294%
	I - Batch: 180 | Loss: 1.043 | Acc: 65.069%
	I - Batch: 190 | Loss: 1.041 | Acc: 65.164%
	I - Batch: 200 | Loss: 1.048 | Acc: 64.562%
	I - Batch: 210 | Loss: 1.061 | Acc: 64.256%
	I - Batch: 220 | Loss: 1.066 | Acc: 63.722%
	I - Batch: 230 | Loss: 1.071 | Acc: 63.342%
	I - Batch: 240 | Loss: 1.072 | Acc: 63.151%
	I - Batch: 250 | Loss: 1.072 | Acc: 63.125%
	I - Batch: 260 | Loss: 1.073 | Acc: 63.005%
	I - Batch: 270 | Loss: 1.070 | Acc: 63.171%
	I - Batch: 280 | Loss: 1.072 | Acc: 63.170%
	I - Batch: 290 | Loss: 1.072 | Acc: 63.147%
	I - Batch: 300 | Loss: 1.069 | Acc: 63.333%
	I - Batch: 310 | Loss: 1.066 | Acc: 63.367%
	I - Batch: 320 | Loss: 1.066 | Acc: 63.359%
	I - Batch: 330 | Loss: 1.068 | Acc: 63.277%
	I - Batch: 340 | Loss: 1.069 | Acc: 63.217%
	I - Batch: 350 | Loss: 1.070 | Acc: 63.143%
	I - Batch: 360 | Loss: 1.073 | Acc: 63.056%
	I - Batch: 370 | Loss: 1.074 | Acc: 63.074%
	I - Batch: 380 | Loss: 1.074 | Acc: 63.158%
	I - Batch: 390 | Loss: 1.073 | Acc: 63.301%
	I - Batch: 400 | Loss: 1.074 | Acc: 63.219%
	I - Batch: 410 | Loss: 1.075 | Acc: 63.125%
	I - Batch: 420 | Loss: 1.073 | Acc: 63.155%
	I - Batch: 430 | Loss: 1.071 | Acc: 63.198%
	I - Batch: 440 | Loss: 1.072 | Acc: 63.153%
	I - Batch: 450 | Loss: 1.071 | Acc: 63.222%
	I - Batch: 460 | Loss: 1.069 | Acc: 63.315%
	I - Batch: 470 | Loss: 1.068 | Acc: 63.351%
	I - Batch: 480 | Loss: 1.067 | Acc: 63.320%
	I - Batch: 490 | Loss: 1.068 | Acc: 63.202%
	I - Batch: 500 | Loss: 1.068 | Acc: 63.150%
	I - Batch: 510 | Loss: 1.070 | Acc: 62.929%
	I - Batch: 520 | Loss: 1.068 | Acc: 62.933%
	I - Batch: 530 | Loss: 1.065 | Acc: 63.125%
	I - Batch: 540 | Loss: 1.065 | Acc: 63.056%
	I - Batch: 550 | Loss: 1.063 | Acc: 63.114%
	I - Batch: 560 | Loss: 1.063 | Acc: 63.080%
	I - Batch: 570 | Loss: 1.063 | Acc: 63.059%
	I - Batch: 580 | Loss: 1.064 | Acc: 63.017%
	I - Batch: 590 | Loss: 1.062 | Acc: 63.136%
	I - Batch: 600 | Loss: 1.062 | Acc: 63.188%
	I - Batch: 610 | Loss: 1.060 | Acc: 63.186%
	I - Batch: 620 | Loss: 1.060 | Acc: 63.115%
	I - Batch: 630 | Loss: 1.058 | Acc: 63.175%
	I - Batch: 640 | Loss: 1.058 | Acc: 63.291%
	I - Batch: 650 | Loss: 1.059 | Acc: 63.231%
	I - Batch: 660 | Loss: 1.058 | Acc: 63.295%
	I - Batch: 670 | Loss: 1.057 | Acc: 63.330%
	I - Batch: 680 | Loss: 1.058 | Acc: 63.346%
	I - Batch: 690 | Loss: 1.058 | Acc: 63.342%
	I - Batch: 700 | Loss: 1.057 | Acc: 63.357%
I - num batch: 701
I - Train -- Loss: 1.057 | Acc: 63.374% | LR: 1.000000e-03 | Dur: 556.08s
I - Confusion Matrix: [row->prediction - col->label]
[[ 956.   10.   17.  260.  422.]
 [   0.    1.    0.    0.    0.]
 [  24.  440.  498.   83.  539.]
 [ 310.   90.  146.  943.  697.]
 [ 256.  194.  402.  218. 4710.]]

I - Val -- Loss: 1.186 | Acc: 55.412%
I - Confusion Matrix: [row->prediction - col->label]
[[400.   9.  31. 248. 104.]
 [  0.   0.   0.   0.   0.]
 [ 13. 207. 201.  32. 137.]
 [ 22.  35.  33. 196.  61.]
 [100. 119. 127.  98. 913.]]

I - Local maximum validation set accuracy:  55.41
I - Global maximum validation set accuracy:  55.41
I - Epoch: 6
	I - Batch: 10 | Loss: 1.062 | Acc: 61.875%
	I - Batch: 20 | Loss: 0.954 | Acc: 66.875%
	I - Batch: 30 | Loss: 0.934 | Acc: 68.125%
	I - Batch: 40 | Loss: 0.996 | Acc: 65.469%
	I - Batch: 50 | Loss: 1.002 | Acc: 65.125%
	I - Batch: 60 | Loss: 1.010 | Acc: 65.312%
	I - Batch: 70 | Loss: 1.019 | Acc: 64.554%
	I - Batch: 80 | Loss: 1.028 | Acc: 64.062%
	I - Batch: 90 | Loss: 1.031 | Acc: 64.583%
	I - Batch: 100 | Loss: 1.031 | Acc: 64.625%
	I - Batch: 110 | Loss: 1.041 | Acc: 64.432%
	I - Batch: 120 | Loss: 1.033 | Acc: 64.479%
	I - Batch: 130 | Loss: 1.031 | Acc: 64.327%
	I - Batch: 140 | Loss: 1.031 | Acc: 64.241%
	I - Batch: 150 | Loss: 1.027 | Acc: 64.458%
	I - Batch: 160 | Loss: 1.021 | Acc: 64.844%
	I - Batch: 170 | Loss: 1.019 | Acc: 64.963%
	I - Batch: 180 | Loss: 1.007 | Acc: 65.625%
	I - Batch: 190 | Loss: 1.004 | Acc: 65.658%
	I - Batch: 200 | Loss: 1.007 | Acc: 65.469%
	I - Batch: 210 | Loss: 1.013 | Acc: 65.089%
	I - Batch: 220 | Loss: 1.018 | Acc: 64.773%
	I - Batch: 230 | Loss: 1.027 | Acc: 64.511%
	I - Batch: 240 | Loss: 1.028 | Acc: 64.297%
	I - Batch: 250 | Loss: 1.022 | Acc: 64.575%
	I - Batch: 260 | Loss: 1.020 | Acc: 64.712%
	I - Batch: 270 | Loss: 1.022 | Acc: 64.769%
	I - Batch: 280 | Loss: 1.017 | Acc: 65.112%
	I - Batch: 290 | Loss: 1.013 | Acc: 65.151%
	I - Batch: 300 | Loss: 1.010 | Acc: 65.208%
	I - Batch: 310 | Loss: 1.011 | Acc: 65.141%
	I - Batch: 320 | Loss: 1.008 | Acc: 65.156%
	I - Batch: 330 | Loss: 1.013 | Acc: 64.905%
	I - Batch: 340 | Loss: 1.012 | Acc: 65.074%
	I - Batch: 350 | Loss: 1.013 | Acc: 65.196%
	I - Batch: 360 | Loss: 1.011 | Acc: 65.191%
	I - Batch: 370 | Loss: 1.012 | Acc: 65.152%
	I - Batch: 380 | Loss: 1.010 | Acc: 65.247%
	I - Batch: 390 | Loss: 1.009 | Acc: 65.272%
	I - Batch: 400 | Loss: 1.008 | Acc: 65.234%
	I - Batch: 410 | Loss: 1.004 | Acc: 65.442%
	I - Batch: 420 | Loss: 1.008 | Acc: 65.283%
	I - Batch: 430 | Loss: 1.009 | Acc: 65.203%
	I - Batch: 440 | Loss: 1.012 | Acc: 65.142%
	I - Batch: 450 | Loss: 1.013 | Acc: 65.167%
	I - Batch: 460 | Loss: 1.011 | Acc: 65.217%
	I - Batch: 470 | Loss: 1.011 | Acc: 65.226%
	I - Batch: 480 | Loss: 1.009 | Acc: 65.286%
	I - Batch: 490 | Loss: 1.009 | Acc: 65.344%
	I - Batch: 500 | Loss: 1.007 | Acc: 65.400%
	I - Batch: 510 | Loss: 1.008 | Acc: 65.355%
	I - Batch: 520 | Loss: 1.009 | Acc: 65.300%
	I - Batch: 530 | Loss: 1.011 | Acc: 65.283%
	I - Batch: 540 | Loss: 1.011 | Acc: 65.359%
	I - Batch: 550 | Loss: 1.014 | Acc: 65.284%
	I - Batch: 560 | Loss: 1.014 | Acc: 65.346%
	I - Batch: 570 | Loss: 1.014 | Acc: 65.274%
	I - Batch: 580 | Loss: 1.017 | Acc: 65.226%
	I - Batch: 590 | Loss: 1.019 | Acc: 65.085%
	I - Batch: 600 | Loss: 1.018 | Acc: 65.135%
	I - Batch: 610 | Loss: 1.018 | Acc: 65.072%
	I - Batch: 620 | Loss: 1.016 | Acc: 65.101%
	I - Batch: 630 | Loss: 1.017 | Acc: 65.040%
	I - Batch: 640 | Loss: 1.018 | Acc: 65.039%
	I - Batch: 650 | Loss: 1.016 | Acc: 65.106%
	I - Batch: 660 | Loss: 1.015 | Acc: 65.142%
	I - Batch: 670 | Loss: 1.016 | Acc: 65.131%
	I - Batch: 680 | Loss: 1.016 | Acc: 65.193%
	I - Batch: 690 | Loss: 1.016 | Acc: 65.217%
	I - Batch: 700 | Loss: 1.015 | Acc: 65.205%
I - num batch: 701
I - Train -- Loss: 1.015 | Acc: 65.201% | LR: 1.000000e-03 | Dur: 559.39s
I - Confusion Matrix: [row->prediction - col->label]
[[ 999.    5.   14.  256.  431.]
 [   0.    7.   11.    0.    3.]
 [  20.  447.  543.   72.  547.]
 [ 278.   83.  127.  948.  571.]
 [ 249.  193.  368.  228. 4816.]]

I - Val -- Loss: 1.207 | Acc: 55.476%
I - Confusion Matrix: [row->prediction - col->label]
[[429.  14.  35. 268. 128.]
 [  0.   0.   0.   0.   0.]
 [ 10. 161. 157.  19.  87.]
 [ 22.  47.  36. 208.  82.]
 [ 74. 148. 164.  79. 918.]]

I - Local maximum validation set accuracy:  55.48
I - Global maximum validation set accuracy:  55.48
I - Epoch: 7
	I - Batch: 10 | Loss: 0.867 | Acc: 71.250%
	I - Batch: 20 | Loss: 0.926 | Acc: 69.375%
	I - Batch: 30 | Loss: 0.922 | Acc: 69.792%
	I - Batch: 40 | Loss: 0.930 | Acc: 70.156%
	I - Batch: 50 | Loss: 0.942 | Acc: 69.500%
	I - Batch: 60 | Loss: 0.944 | Acc: 68.854%
	I - Batch: 70 | Loss: 0.958 | Acc: 68.036%
	I - Batch: 80 | Loss: 0.957 | Acc: 68.359%
	I - Batch: 90 | Loss: 0.954 | Acc: 68.403%
	I - Batch: 100 | Loss: 0.939 | Acc: 69.000%
	I - Batch: 110 | Loss: 0.933 | Acc: 68.636%
	I - Batch: 120 | Loss: 0.926 | Acc: 69.062%
	I - Batch: 130 | Loss: 0.923 | Acc: 69.471%
	I - Batch: 140 | Loss: 0.929 | Acc: 69.062%
	I - Batch: 150 | Loss: 0.931 | Acc: 69.000%
	I - Batch: 160 | Loss: 0.935 | Acc: 69.023%
	I - Batch: 170 | Loss: 0.926 | Acc: 69.301%
	I - Batch: 180 | Loss: 0.924 | Acc: 69.062%
	I - Batch: 190 | Loss: 0.927 | Acc: 69.178%
	I - Batch: 200 | Loss: 0.925 | Acc: 69.250%
	I - Batch: 210 | Loss: 0.932 | Acc: 68.839%
	I - Batch: 220 | Loss: 0.933 | Acc: 68.892%
	I - Batch: 230 | Loss: 0.936 | Acc: 68.777%
	I - Batch: 240 | Loss: 0.935 | Acc: 68.776%
	I - Batch: 250 | Loss: 0.938 | Acc: 68.550%
	I - Batch: 260 | Loss: 0.934 | Acc: 68.702%
	I - Batch: 270 | Loss: 0.939 | Acc: 68.495%
	I - Batch: 280 | Loss: 0.940 | Acc: 68.348%
	I - Batch: 290 | Loss: 0.944 | Acc: 68.254%
	I - Batch: 300 | Loss: 0.944 | Acc: 68.188%
	I - Batch: 310 | Loss: 0.949 | Acc: 68.125%
	I - Batch: 320 | Loss: 0.950 | Acc: 68.223%
	I - Batch: 330 | Loss: 0.948 | Acc: 68.277%
	I - Batch: 340 | Loss: 0.946 | Acc: 68.309%
	I - Batch: 350 | Loss: 0.945 | Acc: 68.357%
	I - Batch: 360 | Loss: 0.949 | Acc: 68.090%
	I - Batch: 370 | Loss: 0.947 | Acc: 68.243%
	I - Batch: 380 | Loss: 0.947 | Acc: 68.289%
	I - Batch: 390 | Loss: 0.944 | Acc: 68.526%
	I - Batch: 400 | Loss: 0.945 | Acc: 68.594%
	I - Batch: 410 | Loss: 0.947 | Acc: 68.567%
	I - Batch: 420 | Loss: 0.947 | Acc: 68.557%
	I - Batch: 430 | Loss: 0.952 | Acc: 68.488%
	I - Batch: 440 | Loss: 0.951 | Acc: 68.423%
	I - Batch: 450 | Loss: 0.955 | Acc: 68.139%
	I - Batch: 460 | Loss: 0.955 | Acc: 68.043%
	I - Batch: 470 | Loss: 0.956 | Acc: 68.019%
	I - Batch: 480 | Loss: 0.958 | Acc: 67.878%
	I - Batch: 490 | Loss: 0.956 | Acc: 67.972%
	I - Batch: 500 | Loss: 0.955 | Acc: 67.925%
	I - Batch: 510 | Loss: 0.956 | Acc: 67.892%
	I - Batch: 520 | Loss: 0.957 | Acc: 67.788%
	I - Batch: 530 | Loss: 0.953 | Acc: 67.995%
	I - Batch: 540 | Loss: 0.952 | Acc: 68.032%
	I - Batch: 550 | Loss: 0.951 | Acc: 68.034%
	I - Batch: 560 | Loss: 0.952 | Acc: 67.969%
	I - Batch: 570 | Loss: 0.951 | Acc: 67.982%
	I - Batch: 580 | Loss: 0.951 | Acc: 67.909%
	I - Batch: 590 | Loss: 0.954 | Acc: 67.839%
	I - Batch: 600 | Loss: 0.954 | Acc: 67.771%
	I - Batch: 610 | Loss: 0.955 | Acc: 67.695%
	I - Batch: 620 | Loss: 0.954 | Acc: 67.661%
	I - Batch: 630 | Loss: 0.953 | Acc: 67.698%
	I - Batch: 640 | Loss: 0.953 | Acc: 67.695%
	I - Batch: 650 | Loss: 0.951 | Acc: 67.750%
	I - Batch: 660 | Loss: 0.951 | Acc: 67.784%
	I - Batch: 670 | Loss: 0.948 | Acc: 67.854%
	I - Batch: 680 | Loss: 0.948 | Acc: 67.895%
	I - Batch: 690 | Loss: 0.948 | Acc: 67.899%
	I - Batch: 700 | Loss: 0.948 | Acc: 67.884%
I - num batch: 701
I - Train -- Loss: 0.948 | Acc: 67.894% | LR: 1.000000e-03 | Dur: 577.28s
I - Confusion Matrix: [row->prediction - col->label]
[[1035.    7.   13.  174.  401.]
 [   0.    1.    1.    0.    0.]
 [  16.  510.  584.   63.  536.]
 [ 234.   53.  103. 1044.  480.]
 [ 261.  164.  362.  223. 4951.]]

I - Val -- Loss: 1.199 | Acc: 56.060%
I - Confusion Matrix: [row->prediction - col->label]
[[ 323.    3.   14.  127.   52.]
 [   0.    0.    0.    0.    0.]
 [  14.  209.  188.   41.  128.]
 [  57.   13.   13.  219.   35.]
 [ 141.  145.  177.  187. 1000.]]

I - Local maximum validation set accuracy:  56.06
I - Global maximum validation set accuracy:  56.06
I - Epoch: 8
	I - Batch: 10 | Loss: 0.835 | Acc: 73.125%
	I - Batch: 20 | Loss: 0.885 | Acc: 70.938%
	I - Batch: 30 | Loss: 0.869 | Acc: 72.500%
	I - Batch: 40 | Loss: 0.917 | Acc: 71.406%
	I - Batch: 50 | Loss: 0.903 | Acc: 72.000%
	I - Batch: 60 | Loss: 0.892 | Acc: 72.188%
	I - Batch: 70 | Loss: 0.909 | Acc: 71.339%
	I - Batch: 80 | Loss: 0.932 | Acc: 70.234%
	I - Batch: 90 | Loss: 0.922 | Acc: 70.625%
	I - Batch: 100 | Loss: 0.919 | Acc: 70.188%
	I - Batch: 110 | Loss: 0.919 | Acc: 70.000%
	I - Batch: 120 | Loss: 0.929 | Acc: 69.323%
	I - Batch: 130 | Loss: 0.930 | Acc: 69.183%
	I - Batch: 140 | Loss: 0.922 | Acc: 69.464%
	I - Batch: 150 | Loss: 0.909 | Acc: 69.792%
	I - Batch: 160 | Loss: 0.901 | Acc: 69.961%
	I - Batch: 170 | Loss: 0.898 | Acc: 69.963%
	I - Batch: 180 | Loss: 0.903 | Acc: 69.757%
	I - Batch: 190 | Loss: 0.897 | Acc: 69.836%
	I - Batch: 200 | Loss: 0.903 | Acc: 69.750%
	I - Batch: 210 | Loss: 0.908 | Acc: 69.494%
	I - Batch: 220 | Loss: 0.907 | Acc: 69.773%
	I - Batch: 230 | Loss: 0.906 | Acc: 69.783%
	I - Batch: 240 | Loss: 0.909 | Acc: 69.453%
	I - Batch: 250 | Loss: 0.906 | Acc: 69.525%
	I - Batch: 260 | Loss: 0.907 | Acc: 69.495%
	I - Batch: 270 | Loss: 0.907 | Acc: 69.444%
	I - Batch: 280 | Loss: 0.903 | Acc: 69.330%
	I - Batch: 290 | Loss: 0.904 | Acc: 69.138%
	I - Batch: 300 | Loss: 0.905 | Acc: 69.021%
	I - Batch: 310 | Loss: 0.907 | Acc: 68.992%
	I - Batch: 320 | Loss: 0.903 | Acc: 69.180%
	I - Batch: 330 | Loss: 0.902 | Acc: 69.091%
	I - Batch: 340 | Loss: 0.904 | Acc: 68.879%
	I - Batch: 350 | Loss: 0.900 | Acc: 68.946%
	I - Batch: 360 | Loss: 0.899 | Acc: 69.028%
	I - Batch: 370 | Loss: 0.900 | Acc: 68.986%
	I - Batch: 380 | Loss: 0.901 | Acc: 68.997%
	I - Batch: 390 | Loss: 0.900 | Acc: 69.038%
	I - Batch: 400 | Loss: 0.900 | Acc: 68.922%
	I - Batch: 410 | Loss: 0.901 | Acc: 68.948%
	I - Batch: 420 | Loss: 0.904 | Acc: 68.750%
	I - Batch: 430 | Loss: 0.905 | Acc: 68.576%
	I - Batch: 440 | Loss: 0.909 | Acc: 68.523%
	I - Batch: 450 | Loss: 0.908 | Acc: 68.708%
	I - Batch: 460 | Loss: 0.906 | Acc: 68.791%
	I - Batch: 470 | Loss: 0.905 | Acc: 68.777%
	I - Batch: 480 | Loss: 0.907 | Acc: 68.724%
	I - Batch: 490 | Loss: 0.904 | Acc: 68.814%
	I - Batch: 500 | Loss: 0.905 | Acc: 68.725%
	I - Batch: 510 | Loss: 0.906 | Acc: 68.664%
	I - Batch: 520 | Loss: 0.905 | Acc: 68.642%
	I - Batch: 530 | Loss: 0.904 | Acc: 68.597%
	I - Batch: 540 | Loss: 0.904 | Acc: 68.588%
	I - Batch: 550 | Loss: 0.904 | Acc: 68.580%
	I - Batch: 560 | Loss: 0.904 | Acc: 68.638%
	I - Batch: 570 | Loss: 0.904 | Acc: 68.596%
	I - Batch: 580 | Loss: 0.903 | Acc: 68.675%
	I - Batch: 590 | Loss: 0.901 | Acc: 68.739%
	I - Batch: 600 | Loss: 0.903 | Acc: 68.635%
	I - Batch: 610 | Loss: 0.902 | Acc: 68.637%
	I - Batch: 620 | Loss: 0.903 | Acc: 68.639%
	I - Batch: 630 | Loss: 0.903 | Acc: 68.581%
	I - Batch: 640 | Loss: 0.904 | Acc: 68.613%
	I - Batch: 650 | Loss: 0.903 | Acc: 68.702%
	I - Batch: 660 | Loss: 0.903 | Acc: 68.759%
	I - Batch: 670 | Loss: 0.901 | Acc: 68.843%
	I - Batch: 680 | Loss: 0.901 | Acc: 68.814%
	I - Batch: 690 | Loss: 0.901 | Acc: 68.813%
	I - Batch: 700 | Loss: 0.899 | Acc: 68.938%
I - num batch: 701
I - Train -- Loss: 0.899 | Acc: 68.919% | LR: 1.000000e-03 | Dur: 579.17s
I - Confusion Matrix: [row->prediction - col->label]
[[1059.    7.    8.  148.  362.]
 [   0.   29.   20.    2.    7.]
 [  14.  492.  627.   68.  566.]
 [ 202.   54.   77. 1092.  510.]
 [ 271.  153.  331.  194. 4923.]]

I - Val -- Loss: 1.165 | Acc: 58.944%
I - Confusion Matrix: [row->prediction - col->label]
[[ 378.    7.   18.  183.   58.]
 [   2.   63.   47.    6.   15.]
 [   8.  112.  104.   17.   46.]
 [  42.   30.   32.  245.   67.]
 [ 105.  158.  191.  123. 1029.]]

I - Local maximum validation set accuracy:  58.94
I - Global maximum validation set accuracy:  58.94
I - Epoch: 9
	I - Batch: 10 | Loss: 0.863 | Acc: 70.000%
	I - Batch: 20 | Loss: 0.855 | Acc: 69.688%
	I - Batch: 30 | Loss: 0.865 | Acc: 68.750%
	I - Batch: 40 | Loss: 0.870 | Acc: 68.281%
	I - Batch: 50 | Loss: 0.893 | Acc: 68.000%
	I - Batch: 60 | Loss: 0.887 | Acc: 68.854%
	I - Batch: 70 | Loss: 0.893 | Acc: 68.839%
	I - Batch: 80 | Loss: 0.895 | Acc: 68.906%
	I - Batch: 90 | Loss: 0.889 | Acc: 69.236%
	I - Batch: 100 | Loss: 0.880 | Acc: 69.812%
	I - Batch: 110 | Loss: 0.883 | Acc: 70.170%
	I - Batch: 120 | Loss: 0.876 | Acc: 70.625%
	I - Batch: 130 | Loss: 0.871 | Acc: 70.817%
	I - Batch: 140 | Loss: 0.871 | Acc: 70.759%
	I - Batch: 150 | Loss: 0.873 | Acc: 70.208%
	I - Batch: 160 | Loss: 0.878 | Acc: 69.922%
	I - Batch: 170 | Loss: 0.876 | Acc: 69.816%
	I - Batch: 180 | Loss: 0.878 | Acc: 69.757%
	I - Batch: 190 | Loss: 0.875 | Acc: 69.638%
	I - Batch: 200 | Loss: 0.873 | Acc: 69.906%
	I - Batch: 210 | Loss: 0.872 | Acc: 70.000%
	I - Batch: 220 | Loss: 0.872 | Acc: 69.915%
	I - Batch: 230 | Loss: 0.873 | Acc: 70.027%
	I - Batch: 240 | Loss: 0.874 | Acc: 69.948%
	I - Batch: 250 | Loss: 0.874 | Acc: 69.850%
	I - Batch: 260 | Loss: 0.875 | Acc: 69.591%
	I - Batch: 270 | Loss: 0.879 | Acc: 69.537%
	I - Batch: 280 | Loss: 0.883 | Acc: 69.397%
	I - Batch: 290 | Loss: 0.885 | Acc: 69.547%
	I - Batch: 300 | Loss: 0.890 | Acc: 69.438%
	I - Batch: 310 | Loss: 0.890 | Acc: 69.496%
	I - Batch: 320 | Loss: 0.887 | Acc: 69.609%
	I - Batch: 330 | Loss: 0.885 | Acc: 69.640%
	I - Batch: 340 | Loss: 0.884 | Acc: 69.614%
	I - Batch: 350 | Loss: 0.881 | Acc: 69.786%
	I - Batch: 360 | Loss: 0.882 | Acc: 69.740%
	I - Batch: 370 | Loss: 0.878 | Acc: 69.949%
	I - Batch: 380 | Loss: 0.875 | Acc: 69.984%
	I - Batch: 390 | Loss: 0.872 | Acc: 70.096%
	I - Batch: 400 | Loss: 0.869 | Acc: 70.234%
	I - Batch: 410 | Loss: 0.869 | Acc: 70.183%
	I - Batch: 420 | Loss: 0.870 | Acc: 70.104%
	I - Batch: 430 | Loss: 0.868 | Acc: 70.145%
	I - Batch: 440 | Loss: 0.866 | Acc: 70.099%
	I - Batch: 450 | Loss: 0.866 | Acc: 70.014%
	I - Batch: 460 | Loss: 0.863 | Acc: 70.149%
	I - Batch: 470 | Loss: 0.861 | Acc: 70.226%
	I - Batch: 480 | Loss: 0.863 | Acc: 70.234%
	I - Batch: 490 | Loss: 0.862 | Acc: 70.242%
	I - Batch: 500 | Loss: 0.862 | Acc: 70.287%
	I - Batch: 510 | Loss: 0.862 | Acc: 70.368%
	I - Batch: 520 | Loss: 0.862 | Acc: 70.325%
	I - Batch: 530 | Loss: 0.861 | Acc: 70.283%
	I - Batch: 540 | Loss: 0.861 | Acc: 70.255%
	I - Batch: 550 | Loss: 0.862 | Acc: 70.273%
	I - Batch: 560 | Loss: 0.862 | Acc: 70.301%
	I - Batch: 570 | Loss: 0.861 | Acc: 70.241%
	I - Batch: 580 | Loss: 0.861 | Acc: 70.291%
	I - Batch: 590 | Loss: 0.862 | Acc: 70.318%
	I - Batch: 600 | Loss: 0.861 | Acc: 70.333%
	I - Batch: 610 | Loss: 0.860 | Acc: 70.379%
	I - Batch: 620 | Loss: 0.857 | Acc: 70.444%
	I - Batch: 630 | Loss: 0.856 | Acc: 70.506%
	I - Batch: 640 | Loss: 0.855 | Acc: 70.498%
	I - Batch: 650 | Loss: 0.852 | Acc: 70.644%
	I - Batch: 660 | Loss: 0.852 | Acc: 70.653%
	I - Batch: 670 | Loss: 0.853 | Acc: 70.606%
	I - Batch: 680 | Loss: 0.853 | Acc: 70.616%
	I - Batch: 690 | Loss: 0.853 | Acc: 70.571%
	I - Batch: 700 | Loss: 0.853 | Acc: 70.616%
I - num batch: 701
I - Train -- Loss: 0.853 | Acc: 70.622% | LR: 1.000000e-03 | Dur: 561.57s
I - Confusion Matrix: [row->prediction - col->label]
[[1103.    4.    4.  148.  373.]
 [   0.  116.  106.    5.   26.]
 [  10.  443.  545.   51.  477.]
 [ 190.   44.   79. 1125.  460.]
 [ 243.  128.  329.  175. 5032.]]

I - Val -- Loss: 1.254 | Acc: 56.837%
I - Confusion Matrix: [row->prediction - col->label]
[[351.   4.  18. 140.  69.]
 [  0.   5.   1.   3.   2.]
 [ 25. 206. 205.  60. 136.]
 [ 46.  11.  15. 216.  31.]
 [113. 144. 153. 155. 977.]]

I - Epoch: 10
	I - Batch: 10 | Loss: 0.639 | Acc: 77.500%
	I - Batch: 20 | Loss: 0.672 | Acc: 76.875%
	I - Batch: 30 | Loss: 0.715 | Acc: 75.625%
	I - Batch: 40 | Loss: 0.729 | Acc: 75.469%
	I - Batch: 50 | Loss: 0.733 | Acc: 75.500%
	I - Batch: 60 | Loss: 0.744 | Acc: 75.521%
	I - Batch: 70 | Loss: 0.735 | Acc: 75.804%
	I - Batch: 80 | Loss: 0.731 | Acc: 75.781%
	I - Batch: 90 | Loss: 0.748 | Acc: 75.347%
	I - Batch: 100 | Loss: 0.753 | Acc: 75.188%
	I - Batch: 110 | Loss: 0.747 | Acc: 75.227%
	I - Batch: 120 | Loss: 0.744 | Acc: 75.365%
	I - Batch: 130 | Loss: 0.744 | Acc: 75.337%
	I - Batch: 140 | Loss: 0.737 | Acc: 75.312%
	I - Batch: 150 | Loss: 0.737 | Acc: 74.958%
	I - Batch: 160 | Loss: 0.736 | Acc: 75.078%
	I - Batch: 170 | Loss: 0.726 | Acc: 75.625%
	I - Batch: 180 | Loss: 0.727 | Acc: 75.729%
	I - Batch: 190 | Loss: 0.727 | Acc: 75.526%
	I - Batch: 200 | Loss: 0.727 | Acc: 75.500%
	I - Batch: 210 | Loss: 0.723 | Acc: 75.506%
	I - Batch: 220 | Loss: 0.726 | Acc: 75.398%
	I - Batch: 230 | Loss: 0.727 | Acc: 75.435%
	I - Batch: 240 | Loss: 0.727 | Acc: 75.312%
	I - Batch: 250 | Loss: 0.736 | Acc: 75.000%
	I - Batch: 260 | Loss: 0.736 | Acc: 74.952%
	I - Batch: 270 | Loss: 0.735 | Acc: 74.977%
	I - Batch: 280 | Loss: 0.732 | Acc: 75.000%
	I - Batch: 290 | Loss: 0.728 | Acc: 75.172%
	I - Batch: 300 | Loss: 0.726 | Acc: 75.229%
	I - Batch: 310 | Loss: 0.726 | Acc: 75.101%
	I - Batch: 320 | Loss: 0.729 | Acc: 74.980%
	I - Batch: 330 | Loss: 0.727 | Acc: 74.867%
	I - Batch: 340 | Loss: 0.724 | Acc: 74.982%
	I - Batch: 350 | Loss: 0.723 | Acc: 75.071%
	I - Batch: 360 | Loss: 0.724 | Acc: 75.035%
	I - Batch: 370 | Loss: 0.720 | Acc: 75.169%
	I - Batch: 380 | Loss: 0.723 | Acc: 75.115%
	I - Batch: 390 | Loss: 0.726 | Acc: 74.952%
	I - Batch: 400 | Loss: 0.725 | Acc: 75.016%
	I - Batch: 410 | Loss: 0.722 | Acc: 75.030%
	I - Batch: 420 | Loss: 0.725 | Acc: 75.000%
	I - Batch: 430 | Loss: 0.726 | Acc: 74.884%
	I - Batch: 440 | Loss: 0.728 | Acc: 74.730%
	I - Batch: 450 | Loss: 0.726 | Acc: 74.861%
	I - Batch: 460 | Loss: 0.728 | Acc: 74.810%
	I - Batch: 470 | Loss: 0.729 | Acc: 74.840%
	I - Batch: 480 | Loss: 0.727 | Acc: 74.922%
	I - Batch: 490 | Loss: 0.726 | Acc: 75.013%
	I - Batch: 500 | Loss: 0.727 | Acc: 74.950%
	I - Batch: 510 | Loss: 0.726 | Acc: 75.000%
	I - Batch: 520 | Loss: 0.725 | Acc: 75.012%
	I - Batch: 530 | Loss: 0.725 | Acc: 74.965%
	I - Batch: 540 | Loss: 0.723 | Acc: 75.035%
	I - Batch: 550 | Loss: 0.722 | Acc: 75.080%
	I - Batch: 560 | Loss: 0.720 | Acc: 75.167%
	I - Batch: 570 | Loss: 0.719 | Acc: 75.230%
	I - Batch: 580 | Loss: 0.717 | Acc: 75.269%
	I - Batch: 590 | Loss: 0.718 | Acc: 75.297%
	I - Batch: 600 | Loss: 0.719 | Acc: 75.271%
	I - Batch: 610 | Loss: 0.719 | Acc: 75.277%
	I - Batch: 620 | Loss: 0.720 | Acc: 75.292%
	I - Batch: 630 | Loss: 0.718 | Acc: 75.377%
	I - Batch: 640 | Loss: 0.719 | Acc: 75.361%
	I - Batch: 650 | Loss: 0.720 | Acc: 75.356%
	I - Batch: 660 | Loss: 0.721 | Acc: 75.350%
	I - Batch: 670 | Loss: 0.722 | Acc: 75.299%
	I - Batch: 680 | Loss: 0.721 | Acc: 75.303%
	I - Batch: 690 | Loss: 0.720 | Acc: 75.371%
	I - Batch: 700 | Loss: 0.718 | Acc: 75.482%
I - num batch: 701
I - Train -- Loss: 0.717 | Acc: 75.508% | LR: 5.000000e-04 | Dur: 556.21s
I - Confusion Matrix: [row->prediction - col->label]
[[1244.    3.    5.   92.  365.]
 [   2.  148.  126.    3.    8.]
 [  10.  452.  588.   31.  420.]
 [ 117.   33.   51. 1246.  332.]
 [ 173.   99.  293.  132. 5243.]]

I - Val -- Loss: 1.195 | Acc: 58.425%
I - Confusion Matrix: [row->prediction - col->label]
[[319.   6.  18.  87.  70.]
 [  2.  37.  26.   6.   7.]
 [  9. 193. 183.  36. 133.]
 [107.  31.  35. 355.  96.]
 [ 98. 103. 130.  90. 909.]]

I - Epoch: 11
	I - Batch: 10 | Loss: 0.609 | Acc: 77.500%
	I - Batch: 20 | Loss: 0.615 | Acc: 77.500%
	I - Batch: 30 | Loss: 0.655 | Acc: 75.833%
	I - Batch: 40 | Loss: 0.668 | Acc: 76.094%
	I - Batch: 50 | Loss: 0.646 | Acc: 76.750%
	I - Batch: 60 | Loss: 0.653 | Acc: 76.667%
	I - Batch: 70 | Loss: 0.644 | Acc: 77.411%
	I - Batch: 80 | Loss: 0.639 | Acc: 77.500%
	I - Batch: 90 | Loss: 0.633 | Acc: 78.125%
	I - Batch: 100 | Loss: 0.643 | Acc: 77.562%
	I - Batch: 110 | Loss: 0.634 | Acc: 77.955%
	I - Batch: 120 | Loss: 0.633 | Acc: 77.865%
	I - Batch: 130 | Loss: 0.636 | Acc: 78.029%
	I - Batch: 140 | Loss: 0.640 | Acc: 77.902%
	I - Batch: 150 | Loss: 0.639 | Acc: 77.917%
	I - Batch: 160 | Loss: 0.647 | Acc: 77.852%
	I - Batch: 170 | Loss: 0.644 | Acc: 78.015%
	I - Batch: 180 | Loss: 0.636 | Acc: 78.090%
	I - Batch: 190 | Loss: 0.632 | Acc: 78.125%
	I - Batch: 200 | Loss: 0.627 | Acc: 78.562%
	I - Batch: 210 | Loss: 0.627 | Acc: 78.690%
	I - Batch: 220 | Loss: 0.631 | Acc: 78.608%
	I - Batch: 230 | Loss: 0.630 | Acc: 78.533%
	I - Batch: 240 | Loss: 0.630 | Acc: 78.464%
	I - Batch: 250 | Loss: 0.631 | Acc: 78.400%
	I - Batch: 260 | Loss: 0.631 | Acc: 78.245%
	I - Batch: 270 | Loss: 0.632 | Acc: 78.102%
	I - Batch: 280 | Loss: 0.629 | Acc: 78.214%
	I - Batch: 290 | Loss: 0.626 | Acc: 78.341%
	I - Batch: 300 | Loss: 0.625 | Acc: 78.458%
	I - Batch: 310 | Loss: 0.627 | Acc: 78.468%
	I - Batch: 320 | Loss: 0.626 | Acc: 78.574%
	I - Batch: 330 | Loss: 0.627 | Acc: 78.371%
	I - Batch: 340 | Loss: 0.631 | Acc: 78.235%
	I - Batch: 350 | Loss: 0.631 | Acc: 78.196%
	I - Batch: 360 | Loss: 0.632 | Acc: 78.142%
	I - Batch: 370 | Loss: 0.634 | Acc: 78.007%
	I - Batch: 380 | Loss: 0.634 | Acc: 77.993%
	I - Batch: 390 | Loss: 0.634 | Acc: 78.013%
	I - Batch: 400 | Loss: 0.634 | Acc: 77.938%
	I - Batch: 410 | Loss: 0.634 | Acc: 77.896%
	I - Batch: 420 | Loss: 0.632 | Acc: 77.917%
	I - Batch: 430 | Loss: 0.632 | Acc: 77.849%
	I - Batch: 440 | Loss: 0.631 | Acc: 77.855%
	I - Batch: 450 | Loss: 0.630 | Acc: 77.764%
	I - Batch: 460 | Loss: 0.632 | Acc: 77.758%
	I - Batch: 470 | Loss: 0.632 | Acc: 77.633%
	I - Batch: 480 | Loss: 0.634 | Acc: 77.630%
	I - Batch: 490 | Loss: 0.635 | Acc: 77.526%
	I - Batch: 500 | Loss: 0.637 | Acc: 77.475%
	I - Batch: 510 | Loss: 0.640 | Acc: 77.341%
	I - Batch: 520 | Loss: 0.640 | Acc: 77.368%
	I - Batch: 530 | Loss: 0.640 | Acc: 77.476%
	I - Batch: 540 | Loss: 0.640 | Acc: 77.523%
	I - Batch: 550 | Loss: 0.639 | Acc: 77.534%
	I - Batch: 560 | Loss: 0.638 | Acc: 77.578%
	I - Batch: 570 | Loss: 0.639 | Acc: 77.610%
	I - Batch: 580 | Loss: 0.644 | Acc: 77.446%
	I - Batch: 590 | Loss: 0.644 | Acc: 77.458%
	I - Batch: 600 | Loss: 0.644 | Acc: 77.448%
	I - Batch: 610 | Loss: 0.645 | Acc: 77.469%
	I - Batch: 620 | Loss: 0.645 | Acc: 77.480%
	I - Batch: 630 | Loss: 0.648 | Acc: 77.351%
	I - Batch: 640 | Loss: 0.649 | Acc: 77.324%
	I - Batch: 650 | Loss: 0.648 | Acc: 77.423%
	I - Batch: 660 | Loss: 0.648 | Acc: 77.472%
	I - Batch: 670 | Loss: 0.648 | Acc: 77.500%
	I - Batch: 680 | Loss: 0.648 | Acc: 77.472%
	I - Batch: 690 | Loss: 0.646 | Acc: 77.518%
	I - Batch: 700 | Loss: 0.647 | Acc: 77.518%
I - num batch: 701
I - Train -- Loss: 0.647 | Acc: 77.505% | LR: 5.000000e-04 | Dur: 556.16s
I - Confusion Matrix: [row->prediction - col->label]
[[1276.    3.    3.   67.  319.]
 [   0.  195.  157.    1.   15.]
 [   8.  445.  635.   33.  431.]
 [  97.   25.   31. 1287.  303.]
 [ 165.   67.  237.  116. 5300.]]

I - Val -- Loss: 1.372 | Acc: 56.092%
I - Confusion Matrix: [row->prediction - col->label]
[[333.   3.  21. 114.  58.]
 [  3.  58.  45.  23.  15.]
 [ 12. 162. 143.  43. 118.]
 [ 55.  13.  16. 205.  32.]
 [132. 134. 167. 189. 992.]]

I - Epoch: 12
	I - Batch: 10 | Loss: 0.463 | Acc: 82.500%
	I - Batch: 20 | Loss: 0.556 | Acc: 79.062%
	I - Batch: 30 | Loss: 0.560 | Acc: 79.375%
	I - Batch: 40 | Loss: 0.565 | Acc: 79.375%
	I - Batch: 50 | Loss: 0.547 | Acc: 80.500%
	I - Batch: 60 | Loss: 0.586 | Acc: 79.167%
	I - Batch: 70 | Loss: 0.586 | Acc: 79.554%
	I - Batch: 80 | Loss: 0.590 | Acc: 79.766%
	I - Batch: 90 | Loss: 0.589 | Acc: 79.306%
	I - Batch: 100 | Loss: 0.595 | Acc: 79.688%
	I - Batch: 110 | Loss: 0.594 | Acc: 79.659%
	I - Batch: 120 | Loss: 0.604 | Acc: 79.219%
	I - Batch: 130 | Loss: 0.600 | Acc: 79.135%
	I - Batch: 140 | Loss: 0.592 | Acc: 79.420%
	I - Batch: 150 | Loss: 0.592 | Acc: 79.500%
	I - Batch: 160 | Loss: 0.593 | Acc: 79.609%
	I - Batch: 170 | Loss: 0.598 | Acc: 79.228%
	I - Batch: 180 | Loss: 0.593 | Acc: 79.375%
	I - Batch: 190 | Loss: 0.598 | Acc: 79.375%
	I - Batch: 200 | Loss: 0.594 | Acc: 79.469%
	I - Batch: 210 | Loss: 0.595 | Acc: 79.435%
	I - Batch: 220 | Loss: 0.600 | Acc: 79.403%
	I - Batch: 230 | Loss: 0.599 | Acc: 79.402%
	I - Batch: 240 | Loss: 0.600 | Acc: 79.479%
	I - Batch: 250 | Loss: 0.597 | Acc: 79.575%
	I - Batch: 260 | Loss: 0.595 | Acc: 79.639%
	I - Batch: 270 | Loss: 0.593 | Acc: 79.838%
	I - Batch: 280 | Loss: 0.591 | Acc: 79.866%
	I - Batch: 290 | Loss: 0.592 | Acc: 79.849%
	I - Batch: 300 | Loss: 0.593 | Acc: 79.750%
	I - Batch: 310 | Loss: 0.591 | Acc: 79.839%
	I - Batch: 320 | Loss: 0.590 | Acc: 79.863%
	I - Batch: 330 | Loss: 0.592 | Acc: 79.754%
	I - Batch: 340 | Loss: 0.594 | Acc: 79.798%
	I - Batch: 350 | Loss: 0.594 | Acc: 79.786%
	I - Batch: 360 | Loss: 0.593 | Acc: 79.757%
	I - Batch: 370 | Loss: 0.592 | Acc: 79.645%
	I - Batch: 380 | Loss: 0.597 | Acc: 79.391%
	I - Batch: 390 | Loss: 0.596 | Acc: 79.439%
	I - Batch: 400 | Loss: 0.594 | Acc: 79.406%
	I - Batch: 410 | Loss: 0.596 | Acc: 79.360%
	I - Batch: 420 | Loss: 0.597 | Acc: 79.226%
	I - Batch: 430 | Loss: 0.598 | Acc: 79.201%
	I - Batch: 440 | Loss: 0.596 | Acc: 79.304%
	I - Batch: 450 | Loss: 0.595 | Acc: 79.403%
	I - Batch: 460 | Loss: 0.599 | Acc: 79.266%
	I - Batch: 470 | Loss: 0.600 | Acc: 79.176%
	I - Batch: 480 | Loss: 0.601 | Acc: 79.206%
	I - Batch: 490 | Loss: 0.602 | Acc: 79.158%
	I - Batch: 500 | Loss: 0.603 | Acc: 79.062%
	I - Batch: 510 | Loss: 0.603 | Acc: 78.958%
	I - Batch: 520 | Loss: 0.604 | Acc: 78.846%
	I - Batch: 530 | Loss: 0.605 | Acc: 78.750%
	I - Batch: 540 | Loss: 0.606 | Acc: 78.750%
	I - Batch: 550 | Loss: 0.603 | Acc: 78.886%
	I - Batch: 560 | Loss: 0.602 | Acc: 78.962%
	I - Batch: 570 | Loss: 0.602 | Acc: 78.969%
	I - Batch: 580 | Loss: 0.601 | Acc: 79.009%
	I - Batch: 590 | Loss: 0.599 | Acc: 79.089%
	I - Batch: 600 | Loss: 0.596 | Acc: 79.188%
	I - Batch: 610 | Loss: 0.596 | Acc: 79.139%
	I - Batch: 620 | Loss: 0.598 | Acc: 79.083%
	I - Batch: 630 | Loss: 0.598 | Acc: 79.087%
	I - Batch: 640 | Loss: 0.599 | Acc: 79.082%
	I - Batch: 650 | Loss: 0.599 | Acc: 79.135%
	I - Batch: 660 | Loss: 0.599 | Acc: 79.091%
	I - Batch: 670 | Loss: 0.598 | Acc: 79.132%
	I - Batch: 680 | Loss: 0.598 | Acc: 79.164%
	I - Batch: 690 | Loss: 0.600 | Acc: 79.112%
	I - Batch: 700 | Loss: 0.601 | Acc: 79.098%
I - num batch: 701
I - Train -- Loss: 0.600 | Acc: 79.101% | LR: 5.000000e-04 | Dur: 550.59s
I - Confusion Matrix: [row->prediction - col->label]
[[1310.    3.    0.   54.  306.]
 [   0.  257.  172.    7.   24.]
 [   9.  382.  636.   17.  422.]
 [  74.   22.   30. 1326.  273.]
 [ 153.   71.  225.  100. 5343.]]

I - Val -- Loss: 1.274 | Acc: 57.615%
I - Confusion Matrix: [row->prediction - col->label]
[[301.   4.  14.  73.  60.]
 [  5.  78.  51.  21.  31.]
 [ 16. 163. 164.  51. 136.]
 [129.  15.  32. 322.  75.]
 [ 84. 110. 131. 107. 913.]]

I - Epoch: 13
	I - Batch: 10 | Loss: 0.576 | Acc: 73.750%
	I - Batch: 20 | Loss: 0.634 | Acc: 73.750%
	I - Batch: 30 | Loss: 0.598 | Acc: 75.625%
	I - Batch: 40 | Loss: 0.600 | Acc: 76.094%
	I - Batch: 50 | Loss: 0.581 | Acc: 77.000%
	I - Batch: 60 | Loss: 0.573 | Acc: 78.125%
	I - Batch: 70 | Loss: 0.569 | Acc: 78.750%
	I - Batch: 80 | Loss: 0.568 | Acc: 78.984%
	I - Batch: 90 | Loss: 0.567 | Acc: 79.653%
	I - Batch: 100 | Loss: 0.570 | Acc: 79.312%
	I - Batch: 110 | Loss: 0.562 | Acc: 79.716%
	I - Batch: 120 | Loss: 0.550 | Acc: 80.417%
	I - Batch: 130 | Loss: 0.550 | Acc: 80.673%
	I - Batch: 140 | Loss: 0.545 | Acc: 81.116%
	I - Batch: 150 | Loss: 0.544 | Acc: 81.250%
	I - Batch: 160 | Loss: 0.544 | Acc: 81.211%
	I - Batch: 170 | Loss: 0.548 | Acc: 81.066%
	I - Batch: 180 | Loss: 0.552 | Acc: 80.764%
	I - Batch: 190 | Loss: 0.557 | Acc: 80.526%
	I - Batch: 200 | Loss: 0.555 | Acc: 80.531%
	I - Batch: 210 | Loss: 0.556 | Acc: 80.357%
	I - Batch: 220 | Loss: 0.558 | Acc: 80.170%
	I - Batch: 230 | Loss: 0.558 | Acc: 80.163%
	I - Batch: 240 | Loss: 0.559 | Acc: 80.182%
	I - Batch: 250 | Loss: 0.562 | Acc: 80.175%
	I - Batch: 260 | Loss: 0.562 | Acc: 80.312%
	I - Batch: 270 | Loss: 0.560 | Acc: 80.463%
	I - Batch: 280 | Loss: 0.559 | Acc: 80.513%
	I - Batch: 290 | Loss: 0.557 | Acc: 80.517%
	I - Batch: 300 | Loss: 0.560 | Acc: 80.521%
	I - Batch: 310 | Loss: 0.559 | Acc: 80.484%
	I - Batch: 320 | Loss: 0.560 | Acc: 80.352%
	I - Batch: 330 | Loss: 0.563 | Acc: 80.322%
	I - Batch: 340 | Loss: 0.564 | Acc: 80.423%
	I - Batch: 350 | Loss: 0.568 | Acc: 80.411%
	I - Batch: 360 | Loss: 0.566 | Acc: 80.434%
	I - Batch: 370 | Loss: 0.564 | Acc: 80.557%
	I - Batch: 380 | Loss: 0.566 | Acc: 80.543%
	I - Batch: 390 | Loss: 0.568 | Acc: 80.497%
	I - Batch: 400 | Loss: 0.566 | Acc: 80.625%
	I - Batch: 410 | Loss: 0.567 | Acc: 80.640%
	I - Batch: 420 | Loss: 0.567 | Acc: 80.536%
	I - Batch: 430 | Loss: 0.566 | Acc: 80.494%
	I - Batch: 440 | Loss: 0.566 | Acc: 80.483%
	I - Batch: 450 | Loss: 0.564 | Acc: 80.542%
	I - Batch: 460 | Loss: 0.564 | Acc: 80.516%
	I - Batch: 470 | Loss: 0.564 | Acc: 80.532%
	I - Batch: 480 | Loss: 0.565 | Acc: 80.495%
	I - Batch: 490 | Loss: 0.566 | Acc: 80.472%
	I - Batch: 500 | Loss: 0.567 | Acc: 80.425%
	I - Batch: 510 | Loss: 0.567 | Acc: 80.392%
	I - Batch: 520 | Loss: 0.568 | Acc: 80.385%
	I - Batch: 530 | Loss: 0.571 | Acc: 80.283%
	I - Batch: 540 | Loss: 0.573 | Acc: 80.301%
	I - Batch: 550 | Loss: 0.572 | Acc: 80.330%
	I - Batch: 560 | Loss: 0.572 | Acc: 80.446%
	I - Batch: 570 | Loss: 0.570 | Acc: 80.592%
	I - Batch: 580 | Loss: 0.570 | Acc: 80.560%
	I - Batch: 590 | Loss: 0.572 | Acc: 80.466%
	I - Batch: 600 | Loss: 0.573 | Acc: 80.469%
	I - Batch: 610 | Loss: 0.572 | Acc: 80.533%
	I - Batch: 620 | Loss: 0.572 | Acc: 80.494%
	I - Batch: 630 | Loss: 0.573 | Acc: 80.456%
	I - Batch: 640 | Loss: 0.573 | Acc: 80.439%
	I - Batch: 650 | Loss: 0.573 | Acc: 80.471%
	I - Batch: 660 | Loss: 0.572 | Acc: 80.530%
	I - Batch: 670 | Loss: 0.574 | Acc: 80.448%
	I - Batch: 680 | Loss: 0.573 | Acc: 80.441%
	I - Batch: 690 | Loss: 0.572 | Acc: 80.444%
	I - Batch: 700 | Loss: 0.572 | Acc: 80.500%
I - num batch: 701
I - Train -- Loss: 0.571 | Acc: 80.492% | LR: 5.000000e-04 | Dur: 575.67s
I - Confusion Matrix: [row->prediction - col->label]
[[1324.    4.    1.   46.  265.]
 [   0.  310.  185.   12.   28.]
 [   9.  345.  629.   13.  416.]
 [  65.   21.   28. 1334.  228.]
 [ 148.   55.  220.   99. 5431.]]

I - Val -- Loss: 1.298 | Acc: 57.161%
I - Confusion Matrix: [row->prediction - col->label]
[[369.  12.  25. 135. 127.]
 [  2.  68.  42.   7.  11.]
 [  6. 157. 159.  23. 115.]
 [ 90.  28.  26. 274.  68.]
 [ 68. 105. 140. 135. 894.]]

I - Epoch: 14
	I - Batch: 10 | Loss: 0.460 | Acc: 85.000%
	I - Batch: 20 | Loss: 0.449 | Acc: 84.062%
	I - Batch: 30 | Loss: 0.449 | Acc: 83.542%
	I - Batch: 40 | Loss: 0.451 | Acc: 83.750%
	I - Batch: 50 | Loss: 0.453 | Acc: 84.375%
	I - Batch: 60 | Loss: 0.462 | Acc: 84.271%
	I - Batch: 70 | Loss: 0.473 | Acc: 83.750%
	I - Batch: 80 | Loss: 0.476 | Acc: 83.516%
	I - Batch: 90 | Loss: 0.490 | Acc: 83.056%
	I - Batch: 100 | Loss: 0.489 | Acc: 83.062%
	I - Batch: 110 | Loss: 0.503 | Acc: 83.125%
	I - Batch: 120 | Loss: 0.505 | Acc: 83.125%
	I - Batch: 130 | Loss: 0.504 | Acc: 83.317%
	I - Batch: 140 | Loss: 0.503 | Acc: 83.438%
	I - Batch: 150 | Loss: 0.505 | Acc: 83.417%
	I - Batch: 160 | Loss: 0.507 | Acc: 83.477%
	I - Batch: 170 | Loss: 0.503 | Acc: 83.493%
	I - Batch: 180 | Loss: 0.506 | Acc: 83.264%
	I - Batch: 190 | Loss: 0.511 | Acc: 83.059%
	I - Batch: 200 | Loss: 0.514 | Acc: 82.969%
	I - Batch: 210 | Loss: 0.517 | Acc: 82.619%
	I - Batch: 220 | Loss: 0.516 | Acc: 82.585%
	I - Batch: 230 | Loss: 0.513 | Acc: 82.717%
	I - Batch: 240 | Loss: 0.513 | Acc: 82.682%
	I - Batch: 250 | Loss: 0.518 | Acc: 82.450%
	I - Batch: 260 | Loss: 0.516 | Acc: 82.524%
	I - Batch: 270 | Loss: 0.515 | Acc: 82.593%
	I - Batch: 280 | Loss: 0.519 | Acc: 82.545%
	I - Batch: 290 | Loss: 0.520 | Acc: 82.457%
	I - Batch: 300 | Loss: 0.519 | Acc: 82.479%
	I - Batch: 310 | Loss: 0.518 | Acc: 82.581%
	I - Batch: 320 | Loss: 0.521 | Acc: 82.344%
	I - Batch: 330 | Loss: 0.516 | Acc: 82.500%
	I - Batch: 340 | Loss: 0.515 | Acc: 82.574%
	I - Batch: 350 | Loss: 0.519 | Acc: 82.500%
	I - Batch: 360 | Loss: 0.518 | Acc: 82.587%
	I - Batch: 370 | Loss: 0.519 | Acc: 82.466%
	I - Batch: 380 | Loss: 0.521 | Acc: 82.401%
	I - Batch: 390 | Loss: 0.522 | Acc: 82.356%
	I - Batch: 400 | Loss: 0.522 | Acc: 82.328%
	I - Batch: 410 | Loss: 0.522 | Acc: 82.226%
	I - Batch: 420 | Loss: 0.520 | Acc: 82.262%
	I - Batch: 430 | Loss: 0.520 | Acc: 82.282%
	I - Batch: 440 | Loss: 0.522 | Acc: 82.244%
	I - Batch: 450 | Loss: 0.520 | Acc: 82.264%
	I - Batch: 460 | Loss: 0.519 | Acc: 82.269%
	I - Batch: 470 | Loss: 0.518 | Acc: 82.327%
	I - Batch: 480 | Loss: 0.515 | Acc: 82.487%
	I - Batch: 490 | Loss: 0.514 | Acc: 82.577%
	I - Batch: 500 | Loss: 0.514 | Acc: 82.562%
	I - Batch: 510 | Loss: 0.514 | Acc: 82.475%
	I - Batch: 520 | Loss: 0.515 | Acc: 82.476%
	I - Batch: 530 | Loss: 0.514 | Acc: 82.465%
	I - Batch: 540 | Loss: 0.511 | Acc: 82.593%
	I - Batch: 550 | Loss: 0.510 | Acc: 82.580%
	I - Batch: 560 | Loss: 0.512 | Acc: 82.578%
	I - Batch: 570 | Loss: 0.514 | Acc: 82.478%
	I - Batch: 580 | Loss: 0.514 | Acc: 82.532%
	I - Batch: 590 | Loss: 0.513 | Acc: 82.553%
	I - Batch: 600 | Loss: 0.512 | Acc: 82.562%
	I - Batch: 610 | Loss: 0.514 | Acc: 82.480%
	I - Batch: 620 | Loss: 0.514 | Acc: 82.389%
	I - Batch: 630 | Loss: 0.515 | Acc: 82.331%
	I - Batch: 640 | Loss: 0.515 | Acc: 82.344%
	I - Batch: 650 | Loss: 0.515 | Acc: 82.365%
	I - Batch: 660 | Loss: 0.515 | Acc: 82.396%
	I - Batch: 670 | Loss: 0.515 | Acc: 82.360%
	I - Batch: 680 | Loss: 0.515 | Acc: 82.408%
	I - Batch: 690 | Loss: 0.514 | Acc: 82.446%
	I - Batch: 700 | Loss: 0.515 | Acc: 82.393%
I - num batch: 701
I - Train -- Loss: 0.515 | Acc: 82.391% | LR: 5.000000e-04 | Dur: 576.44s
I - Confusion Matrix: [row->prediction - col->label]
[[1360.    0.    2.   34.  248.]
 [   2.  346.  172.    5.   23.]
 [   5.  327.  675.   13.  390.]
 [  59.   19.   19. 1357.  204.]
 [ 120.   43.  195.   95. 5503.]]

I - Val -- Loss: 1.411 | Acc: 56.773%
I - Confusion Matrix: [row->prediction - col->label]
[[342.   5.  17. 111.  75.]
 [  2.  99.  62.  10.  26.]
 [  2. 126. 121.  25.  97.]
 [ 80.  18.  32. 242.  69.]
 [109. 122. 160. 186. 948.]]

I - Epoch: 15
	I - Batch: 10 | Loss: 0.418 | Acc: 85.625%
	I - Batch: 20 | Loss: 0.435 | Acc: 83.750%
	I - Batch: 30 | Loss: 0.395 | Acc: 85.833%
	I - Batch: 40 | Loss: 0.437 | Acc: 85.000%
	I - Batch: 50 | Loss: 0.457 | Acc: 84.375%
	I - Batch: 60 | Loss: 0.469 | Acc: 84.375%
	I - Batch: 70 | Loss: 0.467 | Acc: 84.375%
	I - Batch: 80 | Loss: 0.489 | Acc: 83.594%
	I - Batch: 90 | Loss: 0.484 | Acc: 83.681%
	I - Batch: 100 | Loss: 0.486 | Acc: 83.750%
	I - Batch: 110 | Loss: 0.482 | Acc: 83.920%
	I - Batch: 120 | Loss: 0.482 | Acc: 83.958%
	I - Batch: 130 | Loss: 0.478 | Acc: 84.183%
	I - Batch: 140 | Loss: 0.476 | Acc: 84.286%
	I - Batch: 150 | Loss: 0.468 | Acc: 84.625%
	I - Batch: 160 | Loss: 0.462 | Acc: 84.922%
	I - Batch: 170 | Loss: 0.462 | Acc: 85.037%
	I - Batch: 180 | Loss: 0.465 | Acc: 84.792%
	I - Batch: 190 | Loss: 0.465 | Acc: 84.868%
	I - Batch: 200 | Loss: 0.470 | Acc: 84.688%
	I - Batch: 210 | Loss: 0.465 | Acc: 84.762%
	I - Batch: 220 | Loss: 0.470 | Acc: 84.432%
	I - Batch: 230 | Loss: 0.471 | Acc: 84.457%
	I - Batch: 240 | Loss: 0.466 | Acc: 84.609%
	I - Batch: 250 | Loss: 0.469 | Acc: 84.600%
	I - Batch: 260 | Loss: 0.471 | Acc: 84.399%
	I - Batch: 270 | Loss: 0.471 | Acc: 84.167%
	I - Batch: 280 | Loss: 0.469 | Acc: 84.286%
	I - Batch: 290 | Loss: 0.469 | Acc: 84.246%
	I - Batch: 300 | Loss: 0.467 | Acc: 84.417%
	I - Batch: 310 | Loss: 0.472 | Acc: 84.294%
	I - Batch: 320 | Loss: 0.472 | Acc: 84.336%
	I - Batch: 330 | Loss: 0.475 | Acc: 84.280%
	I - Batch: 340 | Loss: 0.476 | Acc: 84.283%
	I - Batch: 350 | Loss: 0.477 | Acc: 84.250%
	I - Batch: 360 | Loss: 0.475 | Acc: 84.306%
	I - Batch: 370 | Loss: 0.474 | Acc: 84.341%
	I - Batch: 380 | Loss: 0.475 | Acc: 84.408%
	I - Batch: 390 | Loss: 0.475 | Acc: 84.279%
	I - Batch: 400 | Loss: 0.475 | Acc: 84.266%
	I - Batch: 410 | Loss: 0.474 | Acc: 84.268%
	I - Batch: 420 | Loss: 0.479 | Acc: 84.122%
	I - Batch: 430 | Loss: 0.480 | Acc: 83.997%
	I - Batch: 440 | Loss: 0.482 | Acc: 83.864%
	I - Batch: 450 | Loss: 0.485 | Acc: 83.694%
	I - Batch: 460 | Loss: 0.486 | Acc: 83.641%
	I - Batch: 470 | Loss: 0.483 | Acc: 83.803%
	I - Batch: 480 | Loss: 0.484 | Acc: 83.802%
	I - Batch: 490 | Loss: 0.482 | Acc: 83.890%
	I - Batch: 500 | Loss: 0.482 | Acc: 83.825%
	I - Batch: 510 | Loss: 0.480 | Acc: 83.848%
	I - Batch: 520 | Loss: 0.482 | Acc: 83.822%
	I - Batch: 530 | Loss: 0.482 | Acc: 83.785%
	I - Batch: 540 | Loss: 0.483 | Acc: 83.785%
	I - Batch: 550 | Loss: 0.482 | Acc: 83.841%
	I - Batch: 560 | Loss: 0.482 | Acc: 83.817%
	I - Batch: 570 | Loss: 0.480 | Acc: 83.925%
	I - Batch: 580 | Loss: 0.480 | Acc: 84.009%
	I - Batch: 590 | Loss: 0.479 | Acc: 84.036%
	I - Batch: 600 | Loss: 0.480 | Acc: 84.000%
	I - Batch: 610 | Loss: 0.479 | Acc: 84.027%
	I - Batch: 620 | Loss: 0.479 | Acc: 84.022%
	I - Batch: 630 | Loss: 0.479 | Acc: 84.018%
	I - Batch: 640 | Loss: 0.478 | Acc: 84.102%
	I - Batch: 650 | Loss: 0.479 | Acc: 84.010%
	I - Batch: 660 | Loss: 0.478 | Acc: 83.996%
	I - Batch: 670 | Loss: 0.477 | Acc: 83.974%
	I - Batch: 680 | Loss: 0.476 | Acc: 83.989%
	I - Batch: 690 | Loss: 0.477 | Acc: 83.904%
	I - Batch: 700 | Loss: 0.477 | Acc: 83.973%
I - num batch: 701
I - Train -- Loss: 0.477 | Acc: 83.987% | LR: 5.000000e-04 | Dur: 573.02s
I - Confusion Matrix: [row->prediction - col->label]
[[1382.    0.    0.   41.  226.]
 [   0.  390.  163.   15.   22.]
 [   2.  294.  723.    9.  359.]
 [  53.   18.   10. 1363.  199.]
 [ 109.   33.  167.   76. 5562.]]

I - Val -- Loss: 1.413 | Acc: 57.518%
I - Confusion Matrix: [row->prediction - col->label]
[[297.   5.   8.  78.  48.]
 [  4. 106.  49.  14.  26.]
 [ 12. 138. 147.  32. 140.]
 [ 86.  10.  27. 286.  62.]
 [136. 111. 161. 164. 939.]]

I - Epoch: 16
	I - Batch: 10 | Loss: 0.491 | Acc: 85.625%
	I - Batch: 20 | Loss: 0.453 | Acc: 84.062%
	I - Batch: 30 | Loss: 0.446 | Acc: 84.167%
	I - Batch: 40 | Loss: 0.449 | Acc: 84.531%
	I - Batch: 50 | Loss: 0.427 | Acc: 85.875%
	I - Batch: 60 | Loss: 0.429 | Acc: 85.729%
	I - Batch: 70 | Loss: 0.421 | Acc: 85.536%
	I - Batch: 80 | Loss: 0.426 | Acc: 85.156%
	I - Batch: 90 | Loss: 0.418 | Acc: 85.694%
	I - Batch: 100 | Loss: 0.410 | Acc: 85.938%
	I - Batch: 110 | Loss: 0.410 | Acc: 85.852%
	I - Batch: 120 | Loss: 0.417 | Acc: 85.677%
	I - Batch: 130 | Loss: 0.418 | Acc: 85.769%
	I - Batch: 140 | Loss: 0.415 | Acc: 85.938%
	I - Batch: 150 | Loss: 0.421 | Acc: 85.583%
	I - Batch: 160 | Loss: 0.416 | Acc: 85.781%
	I - Batch: 170 | Loss: 0.414 | Acc: 85.772%
	I - Batch: 180 | Loss: 0.414 | Acc: 85.660%
	I - Batch: 190 | Loss: 0.416 | Acc: 85.592%
	I - Batch: 200 | Loss: 0.416 | Acc: 85.594%
	I - Batch: 210 | Loss: 0.413 | Acc: 85.685%
	I - Batch: 220 | Loss: 0.410 | Acc: 85.824%
	I - Batch: 230 | Loss: 0.408 | Acc: 85.842%
	I - Batch: 240 | Loss: 0.407 | Acc: 85.859%
	I - Batch: 250 | Loss: 0.410 | Acc: 85.725%
	I - Batch: 260 | Loss: 0.408 | Acc: 85.889%
	I - Batch: 270 | Loss: 0.407 | Acc: 85.856%
	I - Batch: 280 | Loss: 0.406 | Acc: 85.714%
	I - Batch: 290 | Loss: 0.406 | Acc: 85.711%
	I - Batch: 300 | Loss: 0.408 | Acc: 85.792%
	I - Batch: 310 | Loss: 0.408 | Acc: 85.746%
	I - Batch: 320 | Loss: 0.411 | Acc: 85.566%
	I - Batch: 330 | Loss: 0.411 | Acc: 85.644%
	I - Batch: 340 | Loss: 0.411 | Acc: 85.699%
	I - Batch: 350 | Loss: 0.415 | Acc: 85.536%
	I - Batch: 360 | Loss: 0.418 | Acc: 85.521%
	I - Batch: 370 | Loss: 0.420 | Acc: 85.389%
	I - Batch: 380 | Loss: 0.420 | Acc: 85.280%
	I - Batch: 390 | Loss: 0.423 | Acc: 85.080%
	I - Batch: 400 | Loss: 0.426 | Acc: 84.953%
	I - Batch: 410 | Loss: 0.425 | Acc: 84.985%
	I - Batch: 420 | Loss: 0.426 | Acc: 84.851%
	I - Batch: 430 | Loss: 0.424 | Acc: 84.956%
	I - Batch: 440 | Loss: 0.424 | Acc: 85.000%
	I - Batch: 450 | Loss: 0.422 | Acc: 85.125%
	I - Batch: 460 | Loss: 0.424 | Acc: 85.136%
	I - Batch: 470 | Loss: 0.424 | Acc: 85.266%
	I - Batch: 480 | Loss: 0.425 | Acc: 85.286%
	I - Batch: 490 | Loss: 0.424 | Acc: 85.383%
	I - Batch: 500 | Loss: 0.427 | Acc: 85.350%
	I - Batch: 510 | Loss: 0.426 | Acc: 85.380%
	I - Batch: 520 | Loss: 0.426 | Acc: 85.373%
	I - Batch: 530 | Loss: 0.426 | Acc: 85.436%
	I - Batch: 540 | Loss: 0.424 | Acc: 85.498%
	I - Batch: 550 | Loss: 0.427 | Acc: 85.500%
	I - Batch: 560 | Loss: 0.429 | Acc: 85.458%
	I - Batch: 570 | Loss: 0.431 | Acc: 85.450%
	I - Batch: 580 | Loss: 0.433 | Acc: 85.420%
	I - Batch: 590 | Loss: 0.432 | Acc: 85.519%
	I - Batch: 600 | Loss: 0.431 | Acc: 85.490%
	I - Batch: 610 | Loss: 0.431 | Acc: 85.471%
	I - Batch: 620 | Loss: 0.431 | Acc: 85.514%
	I - Batch: 630 | Loss: 0.434 | Acc: 85.496%
	I - Batch: 640 | Loss: 0.434 | Acc: 85.430%
	I - Batch: 650 | Loss: 0.436 | Acc: 85.394%
	I - Batch: 660 | Loss: 0.436 | Acc: 85.379%
	I - Batch: 670 | Loss: 0.436 | Acc: 85.354%
	I - Batch: 680 | Loss: 0.439 | Acc: 85.248%
	I - Batch: 690 | Loss: 0.439 | Acc: 85.199%
	I - Batch: 700 | Loss: 0.440 | Acc: 85.152%
I - num batch: 701
I - Train -- Loss: 0.440 | Acc: 85.155% | LR: 5.000000e-04 | Dur: 560.91s
I - Confusion Matrix: [row->prediction - col->label]
[[1400.    1.    1.   31.  206.]
 [   4.  421.  159.    5.   31.]
 [   2.  263.  735.    3.  358.]
 [  46.   18.    6. 1402.  180.]
 [  94.   32.  162.   63. 5593.]]

I - Val -- Loss: 1.575 | Acc: 55.250%
I - Confusion Matrix: [row->prediction - col->label]
[[368.   9.  28. 148.  84.]
 [  1.  70.  41.   9.  15.]
 [  8. 157. 148.  28. 135.]
 [ 47.  23.  39. 195.  57.]
 [111. 111. 136. 194. 924.]]

I - Epoch: 17
	I - Batch: 10 | Loss: 0.554 | Acc: 80.000%
	I - Batch: 20 | Loss: 0.526 | Acc: 83.438%
	I - Batch: 30 | Loss: 0.492 | Acc: 83.750%
	I - Batch: 40 | Loss: 0.455 | Acc: 85.156%
	I - Batch: 50 | Loss: 0.450 | Acc: 85.375%
	I - Batch: 60 | Loss: 0.437 | Acc: 85.521%
	I - Batch: 70 | Loss: 0.425 | Acc: 85.804%
	I - Batch: 80 | Loss: 0.430 | Acc: 85.703%
	I - Batch: 90 | Loss: 0.420 | Acc: 86.042%
	I - Batch: 100 | Loss: 0.417 | Acc: 86.438%
	I - Batch: 110 | Loss: 0.410 | Acc: 86.420%
	I - Batch: 120 | Loss: 0.408 | Acc: 86.354%
	I - Batch: 130 | Loss: 0.406 | Acc: 86.635%
	I - Batch: 140 | Loss: 0.405 | Acc: 86.875%
	I - Batch: 150 | Loss: 0.412 | Acc: 86.500%
	I - Batch: 160 | Loss: 0.410 | Acc: 86.719%
	I - Batch: 170 | Loss: 0.411 | Acc: 86.507%
	I - Batch: 180 | Loss: 0.409 | Acc: 86.667%
	I - Batch: 190 | Loss: 0.409 | Acc: 86.546%
	I - Batch: 200 | Loss: 0.407 | Acc: 86.531%
	I - Batch: 210 | Loss: 0.408 | Acc: 86.339%
	I - Batch: 220 | Loss: 0.405 | Acc: 86.449%
	I - Batch: 230 | Loss: 0.404 | Acc: 86.440%
	I - Batch: 240 | Loss: 0.403 | Acc: 86.615%
	I - Batch: 250 | Loss: 0.406 | Acc: 86.475%
	I - Batch: 260 | Loss: 0.403 | Acc: 86.659%
	I - Batch: 270 | Loss: 0.399 | Acc: 86.898%
	I - Batch: 280 | Loss: 0.396 | Acc: 87.031%
	I - Batch: 290 | Loss: 0.393 | Acc: 87.091%
	I - Batch: 300 | Loss: 0.392 | Acc: 87.125%
	I - Batch: 310 | Loss: 0.392 | Acc: 87.198%
	I - Batch: 320 | Loss: 0.392 | Acc: 87.168%
	I - Batch: 330 | Loss: 0.391 | Acc: 87.159%
	I - Batch: 340 | Loss: 0.394 | Acc: 87.169%
	I - Batch: 350 | Loss: 0.391 | Acc: 87.339%
	I - Batch: 360 | Loss: 0.393 | Acc: 87.222%
	I - Batch: 370 | Loss: 0.394 | Acc: 87.162%
	I - Batch: 380 | Loss: 0.394 | Acc: 87.188%
	I - Batch: 390 | Loss: 0.398 | Acc: 87.051%
	I - Batch: 400 | Loss: 0.399 | Acc: 87.031%
	I - Batch: 410 | Loss: 0.399 | Acc: 87.073%
	I - Batch: 420 | Loss: 0.398 | Acc: 87.098%
	I - Batch: 430 | Loss: 0.397 | Acc: 87.108%
	I - Batch: 440 | Loss: 0.398 | Acc: 87.116%
	I - Batch: 450 | Loss: 0.395 | Acc: 87.125%
	I - Batch: 460 | Loss: 0.395 | Acc: 87.160%
	I - Batch: 470 | Loss: 0.396 | Acc: 87.061%
	I - Batch: 480 | Loss: 0.395 | Acc: 87.031%
	I - Batch: 490 | Loss: 0.396 | Acc: 87.054%
	I - Batch: 500 | Loss: 0.397 | Acc: 86.975%
	I - Batch: 510 | Loss: 0.399 | Acc: 86.924%
	I - Batch: 520 | Loss: 0.399 | Acc: 86.899%
	I - Batch: 530 | Loss: 0.398 | Acc: 86.887%
	I - Batch: 540 | Loss: 0.399 | Acc: 86.863%
	I - Batch: 550 | Loss: 0.400 | Acc: 86.875%
	I - Batch: 560 | Loss: 0.401 | Acc: 86.830%
	I - Batch: 570 | Loss: 0.404 | Acc: 86.787%
	I - Batch: 580 | Loss: 0.403 | Acc: 86.810%
	I - Batch: 590 | Loss: 0.405 | Acc: 86.684%
	I - Batch: 600 | Loss: 0.406 | Acc: 86.667%
	I - Batch: 610 | Loss: 0.405 | Acc: 86.701%
	I - Batch: 620 | Loss: 0.406 | Acc: 86.704%
	I - Batch: 630 | Loss: 0.408 | Acc: 86.647%
	I - Batch: 640 | Loss: 0.408 | Acc: 86.602%
	I - Batch: 650 | Loss: 0.410 | Acc: 86.558%
	I - Batch: 660 | Loss: 0.410 | Acc: 86.572%
	I - Batch: 670 | Loss: 0.410 | Acc: 86.604%
	I - Batch: 680 | Loss: 0.410 | Acc: 86.654%
	I - Batch: 690 | Loss: 0.409 | Acc: 86.712%
	I - Batch: 700 | Loss: 0.409 | Acc: 86.670%
I - num batch: 701
I - Train -- Loss: 0.409 | Acc: 86.680% | LR: 5.000000e-04 | Dur: 549.00s
I - Confusion Matrix: [row->prediction - col->label]
[[1409.    0.    1.   23.  204.]
 [   1.  480.  142.   10.   33.]
 [   0.  219.  770.    8.  312.]
 [  40.   16.    9. 1403.  159.]
 [  96.   20.  141.   60. 5660.]]

I - Val -- Loss: 1.539 | Acc: 55.476%
I - Confusion Matrix: [row->prediction - col->label]
[[364.   9.  24. 150. 112.]
 [  6.  73.  29.  13.  23.]
 [  6. 140. 134.  21. 104.]
 [ 67.  23.  34. 239.  74.]
 [ 92. 125. 171. 151. 902.]]

I - Epoch: 18
	I - Batch: 10 | Loss: 0.381 | Acc: 86.875%
	I - Batch: 20 | Loss: 0.397 | Acc: 87.500%
	I - Batch: 30 | Loss: 0.373 | Acc: 88.333%
	I - Batch: 40 | Loss: 0.348 | Acc: 89.062%
	I - Batch: 50 | Loss: 0.346 | Acc: 89.250%
	I - Batch: 60 | Loss: 0.336 | Acc: 89.688%
	I - Batch: 70 | Loss: 0.322 | Acc: 90.357%
	I - Batch: 80 | Loss: 0.327 | Acc: 89.766%
	I - Batch: 90 | Loss: 0.327 | Acc: 89.792%
	I - Batch: 100 | Loss: 0.337 | Acc: 89.062%
	I - Batch: 110 | Loss: 0.351 | Acc: 88.693%
	I - Batch: 120 | Loss: 0.350 | Acc: 88.698%
	I - Batch: 130 | Loss: 0.350 | Acc: 88.894%
	I - Batch: 140 | Loss: 0.350 | Acc: 88.884%
	I - Batch: 150 | Loss: 0.352 | Acc: 88.750%
	I - Batch: 160 | Loss: 0.353 | Acc: 88.555%
	I - Batch: 170 | Loss: 0.356 | Acc: 88.456%
	I - Batch: 180 | Loss: 0.358 | Acc: 88.542%
	I - Batch: 190 | Loss: 0.359 | Acc: 88.651%
	I - Batch: 200 | Loss: 0.362 | Acc: 88.500%
	I - Batch: 210 | Loss: 0.365 | Acc: 88.363%
	I - Batch: 220 | Loss: 0.364 | Acc: 88.324%
	I - Batch: 230 | Loss: 0.362 | Acc: 88.397%
	I - Batch: 240 | Loss: 0.365 | Acc: 88.255%
	I - Batch: 250 | Loss: 0.362 | Acc: 88.325%
	I - Batch: 260 | Loss: 0.364 | Acc: 88.173%
	I - Batch: 270 | Loss: 0.368 | Acc: 88.125%
	I - Batch: 280 | Loss: 0.367 | Acc: 88.259%
	I - Batch: 290 | Loss: 0.367 | Acc: 88.190%
	I - Batch: 300 | Loss: 0.368 | Acc: 88.104%
	I - Batch: 310 | Loss: 0.369 | Acc: 87.923%
	I - Batch: 320 | Loss: 0.372 | Acc: 87.930%
	I - Batch: 330 | Loss: 0.370 | Acc: 88.030%
	I - Batch: 340 | Loss: 0.370 | Acc: 87.941%
	I - Batch: 350 | Loss: 0.370 | Acc: 87.893%
	I - Batch: 360 | Loss: 0.371 | Acc: 87.760%
	I - Batch: 370 | Loss: 0.375 | Acc: 87.517%
	I - Batch: 380 | Loss: 0.375 | Acc: 87.484%
	I - Batch: 390 | Loss: 0.372 | Acc: 87.596%
	I - Batch: 400 | Loss: 0.372 | Acc: 87.656%
	I - Batch: 410 | Loss: 0.373 | Acc: 87.637%
	I - Batch: 420 | Loss: 0.371 | Acc: 87.679%
	I - Batch: 430 | Loss: 0.369 | Acc: 87.762%
	I - Batch: 440 | Loss: 0.368 | Acc: 87.784%
	I - Batch: 450 | Loss: 0.370 | Acc: 87.792%
	I - Batch: 460 | Loss: 0.368 | Acc: 87.826%
	I - Batch: 470 | Loss: 0.371 | Acc: 87.739%
	I - Batch: 480 | Loss: 0.372 | Acc: 87.682%
	I - Batch: 490 | Loss: 0.372 | Acc: 87.666%
	I - Batch: 500 | Loss: 0.371 | Acc: 87.662%
	I - Batch: 510 | Loss: 0.369 | Acc: 87.721%
	I - Batch: 520 | Loss: 0.370 | Acc: 87.644%
	I - Batch: 530 | Loss: 0.370 | Acc: 87.677%
	I - Batch: 540 | Loss: 0.370 | Acc: 87.674%
	I - Batch: 550 | Loss: 0.371 | Acc: 87.682%
	I - Batch: 560 | Loss: 0.370 | Acc: 87.779%
	I - Batch: 570 | Loss: 0.370 | Acc: 87.785%
	I - Batch: 580 | Loss: 0.372 | Acc: 87.748%
	I - Batch: 590 | Loss: 0.371 | Acc: 87.797%
	I - Batch: 600 | Loss: 0.369 | Acc: 87.844%
	I - Batch: 610 | Loss: 0.368 | Acc: 87.900%
	I - Batch: 620 | Loss: 0.367 | Acc: 87.893%
	I - Batch: 630 | Loss: 0.368 | Acc: 87.827%
	I - Batch: 640 | Loss: 0.367 | Acc: 87.812%
	I - Batch: 650 | Loss: 0.369 | Acc: 87.779%
	I - Batch: 660 | Loss: 0.369 | Acc: 87.718%
	I - Batch: 670 | Loss: 0.371 | Acc: 87.631%
	I - Batch: 680 | Loss: 0.372 | Acc: 87.564%
	I - Batch: 690 | Loss: 0.373 | Acc: 87.591%
	I - Batch: 700 | Loss: 0.372 | Acc: 87.571%
I - num batch: 701
I - Train -- Loss: 0.372 | Acc: 87.580% | LR: 5.000000e-04 | Dur: 549.78s
I - Confusion Matrix: [row->prediction - col->label]
[[1442.    1.    0.   20.  192.]
 [   0.  519.  130.   10.   20.]
 [   0.  191.  792.    4.  323.]
 [  24.   12.    6. 1411.  174.]
 [  80.   12.  135.   59. 5659.]]

I - Val -- Loss: 1.603 | Acc: 55.606%
I - Confusion Matrix: [row->prediction - col->label]
[[334.   8.  19. 103.  87.]
 [  4.  71.  32.  12.  18.]
 [  5. 149. 135.  28. 132.]
 [ 86.  25.  33. 280.  82.]
 [106. 117. 173. 151. 896.]]

I - Epoch: 19
	I - Batch: 10 | Loss: 0.301 | Acc: 93.750%
	I - Batch: 20 | Loss: 0.287 | Acc: 92.812%
	I - Batch: 30 | Loss: 0.265 | Acc: 94.167%
	I - Batch: 40 | Loss: 0.258 | Acc: 94.219%
	I - Batch: 50 | Loss: 0.268 | Acc: 93.250%
	I - Batch: 60 | Loss: 0.290 | Acc: 91.771%
	I - Batch: 70 | Loss: 0.281 | Acc: 91.786%
	I - Batch: 80 | Loss: 0.284 | Acc: 91.719%
	I - Batch: 90 | Loss: 0.286 | Acc: 91.528%
	I - Batch: 100 | Loss: 0.292 | Acc: 91.438%
	I - Batch: 110 | Loss: 0.295 | Acc: 91.364%
	I - Batch: 120 | Loss: 0.304 | Acc: 90.938%
	I - Batch: 130 | Loss: 0.303 | Acc: 90.769%
	I - Batch: 140 | Loss: 0.309 | Acc: 90.402%
	I - Batch: 150 | Loss: 0.312 | Acc: 90.208%
	I - Batch: 160 | Loss: 0.310 | Acc: 90.430%
	I - Batch: 170 | Loss: 0.315 | Acc: 90.294%
	I - Batch: 180 | Loss: 0.321 | Acc: 90.208%
	I - Batch: 190 | Loss: 0.320 | Acc: 90.132%
	I - Batch: 200 | Loss: 0.321 | Acc: 90.188%
	I - Batch: 210 | Loss: 0.327 | Acc: 90.119%
	I - Batch: 220 | Loss: 0.326 | Acc: 90.028%
	I - Batch: 230 | Loss: 0.326 | Acc: 90.000%
	I - Batch: 240 | Loss: 0.323 | Acc: 90.104%
	I - Batch: 250 | Loss: 0.326 | Acc: 89.900%
	I - Batch: 260 | Loss: 0.328 | Acc: 89.760%
	I - Batch: 270 | Loss: 0.327 | Acc: 89.769%
	I - Batch: 280 | Loss: 0.327 | Acc: 89.732%
	I - Batch: 290 | Loss: 0.325 | Acc: 89.871%
	I - Batch: 300 | Loss: 0.325 | Acc: 89.917%
	I - Batch: 310 | Loss: 0.326 | Acc: 89.859%
	I - Batch: 320 | Loss: 0.326 | Acc: 89.922%
	I - Batch: 330 | Loss: 0.324 | Acc: 89.981%
	I - Batch: 340 | Loss: 0.321 | Acc: 90.037%
	I - Batch: 350 | Loss: 0.322 | Acc: 90.036%
	I - Batch: 360 | Loss: 0.321 | Acc: 90.069%
	I - Batch: 370 | Loss: 0.322 | Acc: 90.084%
	I - Batch: 380 | Loss: 0.322 | Acc: 90.082%
	I - Batch: 390 | Loss: 0.321 | Acc: 90.016%
	I - Batch: 400 | Loss: 0.321 | Acc: 90.016%
	I - Batch: 410 | Loss: 0.322 | Acc: 89.985%
	I - Batch: 420 | Loss: 0.322 | Acc: 89.985%
	I - Batch: 430 | Loss: 0.324 | Acc: 89.884%
	I - Batch: 440 | Loss: 0.325 | Acc: 89.787%
	I - Batch: 450 | Loss: 0.326 | Acc: 89.778%
	I - Batch: 460 | Loss: 0.327 | Acc: 89.755%
	I - Batch: 470 | Loss: 0.325 | Acc: 89.814%
	I - Batch: 480 | Loss: 0.324 | Acc: 89.831%
	I - Batch: 490 | Loss: 0.324 | Acc: 89.821%
	I - Batch: 500 | Loss: 0.323 | Acc: 89.900%
	I - Batch: 510 | Loss: 0.324 | Acc: 89.865%
	I - Batch: 520 | Loss: 0.322 | Acc: 89.904%
	I - Batch: 530 | Loss: 0.324 | Acc: 89.776%
	I - Batch: 540 | Loss: 0.327 | Acc: 89.699%
	I - Batch: 550 | Loss: 0.326 | Acc: 89.705%
	I - Batch: 560 | Loss: 0.328 | Acc: 89.665%
	I - Batch: 570 | Loss: 0.328 | Acc: 89.616%
	I - Batch: 580 | Loss: 0.328 | Acc: 89.526%
	I - Batch: 590 | Loss: 0.328 | Acc: 89.534%
	I - Batch: 600 | Loss: 0.329 | Acc: 89.531%
	I - Batch: 610 | Loss: 0.332 | Acc: 89.395%
	I - Batch: 620 | Loss: 0.335 | Acc: 89.284%
	I - Batch: 630 | Loss: 0.337 | Acc: 89.157%
	I - Batch: 640 | Loss: 0.337 | Acc: 89.131%
	I - Batch: 650 | Loss: 0.339 | Acc: 89.067%
	I - Batch: 660 | Loss: 0.340 | Acc: 89.015%
	I - Batch: 670 | Loss: 0.341 | Acc: 88.946%
	I - Batch: 680 | Loss: 0.341 | Acc: 88.971%
	I - Batch: 690 | Loss: 0.340 | Acc: 88.986%
	I - Batch: 700 | Loss: 0.339 | Acc: 89.000%
I - num batch: 701
I - Train -- Loss: 0.339 | Acc: 89.007% | LR: 5.000000e-04 | Dur: 559.37s
I - Confusion Matrix: [row->prediction - col->label]
[[1441.    0.    0.   21.  152.]
 [   2.  543.  102.    7.   12.]
 [   1.  159.  834.    2.  332.]
 [  31.   11.    4. 1431.  138.]
 [  71.   22.  123.   43. 5734.]]

I - Val -- Loss: 1.504 | Acc: 56.740%
I - Confusion Matrix: [row->prediction - col->label]
[[262.   3.  11.  58.  50.]
 [  5. 109.  48.  16.  29.]
 [ 14. 123. 159.  32. 150.]
 [162.  32.  51. 382. 147.]
 [ 92. 103. 123.  86. 839.]]

I - Epoch: 20
	I - Batch: 10 | Loss: 0.339 | Acc: 90.625%
	I - Batch: 20 | Loss: 0.282 | Acc: 92.812%
	I - Batch: 30 | Loss: 0.284 | Acc: 93.125%
	I - Batch: 40 | Loss: 0.257 | Acc: 94.219%
	I - Batch: 50 | Loss: 0.256 | Acc: 93.375%
	I - Batch: 60 | Loss: 0.271 | Acc: 93.021%
	I - Batch: 70 | Loss: 0.282 | Acc: 92.500%
	I - Batch: 80 | Loss: 0.275 | Acc: 92.422%
	I - Batch: 90 | Loss: 0.289 | Acc: 91.667%
	I - Batch: 100 | Loss: 0.293 | Acc: 91.500%
	I - Batch: 110 | Loss: 0.286 | Acc: 91.818%
	I - Batch: 120 | Loss: 0.293 | Acc: 91.719%
	I - Batch: 130 | Loss: 0.296 | Acc: 91.538%
	I - Batch: 140 | Loss: 0.300 | Acc: 91.071%
	I - Batch: 150 | Loss: 0.307 | Acc: 90.708%
	I - Batch: 160 | Loss: 0.309 | Acc: 90.703%
	I - Batch: 170 | Loss: 0.310 | Acc: 90.625%
	I - Batch: 180 | Loss: 0.313 | Acc: 90.625%
	I - Batch: 190 | Loss: 0.317 | Acc: 90.230%
	I - Batch: 200 | Loss: 0.317 | Acc: 90.094%
	I - Batch: 210 | Loss: 0.316 | Acc: 89.940%
	I - Batch: 220 | Loss: 0.319 | Acc: 89.801%
	I - Batch: 230 | Loss: 0.318 | Acc: 89.783%
	I - Batch: 240 | Loss: 0.320 | Acc: 89.635%
	I - Batch: 250 | Loss: 0.321 | Acc: 89.650%
	I - Batch: 260 | Loss: 0.318 | Acc: 89.591%
	I - Batch: 270 | Loss: 0.315 | Acc: 89.560%
	I - Batch: 280 | Loss: 0.317 | Acc: 89.420%
	I - Batch: 290 | Loss: 0.316 | Acc: 89.483%
	I - Batch: 300 | Loss: 0.317 | Acc: 89.521%
	I - Batch: 310 | Loss: 0.314 | Acc: 89.597%
	I - Batch: 320 | Loss: 0.312 | Acc: 89.609%
	I - Batch: 330 | Loss: 0.313 | Acc: 89.527%
	I - Batch: 340 | Loss: 0.312 | Acc: 89.632%
	I - Batch: 350 | Loss: 0.313 | Acc: 89.607%
	I - Batch: 360 | Loss: 0.315 | Acc: 89.531%
	I - Batch: 370 | Loss: 0.319 | Acc: 89.510%
	I - Batch: 380 | Loss: 0.323 | Acc: 89.276%
	I - Batch: 390 | Loss: 0.325 | Acc: 89.215%
	I - Batch: 400 | Loss: 0.325 | Acc: 89.172%
	I - Batch: 410 | Loss: 0.325 | Acc: 89.131%
	I - Batch: 420 | Loss: 0.325 | Acc: 89.122%
	I - Batch: 430 | Loss: 0.325 | Acc: 89.099%
	I - Batch: 440 | Loss: 0.327 | Acc: 89.048%
	I - Batch: 450 | Loss: 0.326 | Acc: 89.097%
	I - Batch: 460 | Loss: 0.323 | Acc: 89.212%
	I - Batch: 470 | Loss: 0.323 | Acc: 89.202%
	I - Batch: 480 | Loss: 0.321 | Acc: 89.336%
	I - Batch: 490 | Loss: 0.319 | Acc: 89.349%
	I - Batch: 500 | Loss: 0.317 | Acc: 89.412%
	I - Batch: 510 | Loss: 0.316 | Acc: 89.412%
	I - Batch: 520 | Loss: 0.314 | Acc: 89.531%
	I - Batch: 530 | Loss: 0.318 | Acc: 89.469%
	I - Batch: 540 | Loss: 0.318 | Acc: 89.421%
	I - Batch: 550 | Loss: 0.319 | Acc: 89.330%
	I - Batch: 560 | Loss: 0.319 | Acc: 89.330%
	I - Batch: 570 | Loss: 0.321 | Acc: 89.309%
	I - Batch: 580 | Loss: 0.321 | Acc: 89.321%
	I - Batch: 590 | Loss: 0.322 | Acc: 89.311%
	I - Batch: 600 | Loss: 0.323 | Acc: 89.281%
	I - Batch: 610 | Loss: 0.322 | Acc: 89.283%
	I - Batch: 620 | Loss: 0.322 | Acc: 89.274%
	I - Batch: 630 | Loss: 0.322 | Acc: 89.236%
	I - Batch: 640 | Loss: 0.321 | Acc: 89.297%
	I - Batch: 650 | Loss: 0.321 | Acc: 89.288%
	I - Batch: 660 | Loss: 0.321 | Acc: 89.280%
	I - Batch: 670 | Loss: 0.322 | Acc: 89.272%
	I - Batch: 680 | Loss: 0.323 | Acc: 89.246%
	I - Batch: 690 | Loss: 0.323 | Acc: 89.248%
	I - Batch: 700 | Loss: 0.324 | Acc: 89.241%
I - num batch: 701
I - Train -- Loss: 0.324 | Acc: 89.248% | LR: 5.000000e-04 | Dur: 575.23s
I - Confusion Matrix: [row->prediction - col->label]
[[1448.    0.    3.   18.  154.]
 [   5.  561.  105.    9.   27.]
 [   1.  144.  832.    3.  297.]
 [  19.   13.    5. 1424.  145.]
 [  73.   17.  118.   50. 5745.]]

I - Val -- Loss: 1.673 | Acc: 57.550%
I - Confusion Matrix: [row->prediction - col->label]
[[337.  11.  25. 109.  96.]
 [  3.  98.  34.   8.  22.]
 [  2.  84. 105.   7.  75.]
 [124.  50.  65. 356. 142.]
 [ 69. 127. 163.  94. 880.]]

I - Epoch: 21
	I - Batch: 10 | Loss: 0.348 | Acc: 91.250%
	I - Batch: 20 | Loss: 0.356 | Acc: 88.125%
	I - Batch: 30 | Loss: 0.328 | Acc: 89.583%
	I - Batch: 40 | Loss: 0.331 | Acc: 89.531%
	I - Batch: 50 | Loss: 0.330 | Acc: 89.875%
	I - Batch: 60 | Loss: 0.322 | Acc: 90.417%
	I - Batch: 70 | Loss: 0.307 | Acc: 90.893%
	I - Batch: 80 | Loss: 0.304 | Acc: 91.016%
	I - Batch: 90 | Loss: 0.294 | Acc: 91.250%
	I - Batch: 100 | Loss: 0.293 | Acc: 91.125%
	I - Batch: 110 | Loss: 0.296 | Acc: 90.852%
	I - Batch: 120 | Loss: 0.292 | Acc: 90.938%
	I - Batch: 130 | Loss: 0.284 | Acc: 91.058%
	I - Batch: 140 | Loss: 0.290 | Acc: 90.759%
	I - Batch: 150 | Loss: 0.291 | Acc: 90.542%
	I - Batch: 160 | Loss: 0.291 | Acc: 90.664%
	I - Batch: 170 | Loss: 0.295 | Acc: 90.551%
	I - Batch: 180 | Loss: 0.294 | Acc: 90.486%
	I - Batch: 190 | Loss: 0.298 | Acc: 90.362%
	I - Batch: 200 | Loss: 0.300 | Acc: 90.281%
	I - Batch: 210 | Loss: 0.305 | Acc: 90.179%
	I - Batch: 220 | Loss: 0.303 | Acc: 90.199%
	I - Batch: 230 | Loss: 0.301 | Acc: 90.190%
	I - Batch: 240 | Loss: 0.300 | Acc: 90.234%
	I - Batch: 250 | Loss: 0.299 | Acc: 90.300%
	I - Batch: 260 | Loss: 0.297 | Acc: 90.385%
	I - Batch: 270 | Loss: 0.295 | Acc: 90.509%
	I - Batch: 280 | Loss: 0.294 | Acc: 90.580%
	I - Batch: 290 | Loss: 0.292 | Acc: 90.711%
	I - Batch: 300 | Loss: 0.291 | Acc: 90.812%
	I - Batch: 310 | Loss: 0.290 | Acc: 90.806%
	I - Batch: 320 | Loss: 0.288 | Acc: 90.820%
	I - Batch: 330 | Loss: 0.288 | Acc: 90.871%
	I - Batch: 340 | Loss: 0.287 | Acc: 90.901%
	I - Batch: 350 | Loss: 0.285 | Acc: 90.893%
	I - Batch: 360 | Loss: 0.286 | Acc: 90.903%
	I - Batch: 370 | Loss: 0.288 | Acc: 90.811%
	I - Batch: 380 | Loss: 0.288 | Acc: 90.773%
	I - Batch: 390 | Loss: 0.288 | Acc: 90.817%
	I - Batch: 400 | Loss: 0.287 | Acc: 90.844%
	I - Batch: 410 | Loss: 0.285 | Acc: 90.930%
	I - Batch: 420 | Loss: 0.284 | Acc: 90.997%
	I - Batch: 430 | Loss: 0.285 | Acc: 90.974%
	I - Batch: 440 | Loss: 0.285 | Acc: 90.966%
	I - Batch: 450 | Loss: 0.286 | Acc: 90.958%
	I - Batch: 460 | Loss: 0.285 | Acc: 90.965%
	I - Batch: 470 | Loss: 0.286 | Acc: 91.011%
	I - Batch: 480 | Loss: 0.285 | Acc: 91.042%
	I - Batch: 490 | Loss: 0.286 | Acc: 90.995%
	I - Batch: 500 | Loss: 0.287 | Acc: 90.963%
	I - Batch: 510 | Loss: 0.286 | Acc: 90.956%
	I - Batch: 520 | Loss: 0.287 | Acc: 90.901%
	I - Batch: 530 | Loss: 0.289 | Acc: 90.802%
	I - Batch: 540 | Loss: 0.289 | Acc: 90.810%
	I - Batch: 550 | Loss: 0.288 | Acc: 90.795%
	I - Batch: 560 | Loss: 0.292 | Acc: 90.681%
	I - Batch: 570 | Loss: 0.292 | Acc: 90.691%
	I - Batch: 580 | Loss: 0.291 | Acc: 90.711%
	I - Batch: 590 | Loss: 0.292 | Acc: 90.657%
	I - Batch: 600 | Loss: 0.292 | Acc: 90.656%
	I - Batch: 610 | Loss: 0.293 | Acc: 90.615%
	I - Batch: 620 | Loss: 0.292 | Acc: 90.605%
	I - Batch: 630 | Loss: 0.292 | Acc: 90.565%
	I - Batch: 640 | Loss: 0.290 | Acc: 90.615%
	I - Batch: 650 | Loss: 0.291 | Acc: 90.548%
	I - Batch: 660 | Loss: 0.292 | Acc: 90.530%
	I - Batch: 670 | Loss: 0.293 | Acc: 90.494%
	I - Batch: 680 | Loss: 0.295 | Acc: 90.414%
	I - Batch: 690 | Loss: 0.295 | Acc: 90.417%
	I - Batch: 700 | Loss: 0.295 | Acc: 90.411%
I - num batch: 701
I - Train -- Loss: 0.295 | Acc: 90.424% | LR: 5.000000e-04 | Dur: 582.94s
I - Confusion Matrix: [row->prediction - col->label]
[[1454.    0.    0.   12.  150.]
 [   1.  575.   75.    9.   23.]
 [   2.  137.  882.    2.  267.]
 [  27.    8.    3. 1436.  133.]
 [  62.   15.  103.   45. 5795.]]

I - Val -- Loss: 1.759 | Acc: 56.513%
I - Confusion Matrix: [row->prediction - col->label]
[[352.   9.  24. 166.  73.]
 [  3.  75.  35.  11.  20.]
 [ 10. 121. 148.  17. 116.]
 [ 70.  27.  34. 233.  70.]
 [100. 138. 151. 147. 936.]]

I - Epoch: 22
	I - Batch: 10 | Loss: 0.332 | Acc: 88.125%
	I - Batch: 20 | Loss: 0.274 | Acc: 90.625%
	I - Batch: 30 | Loss: 0.302 | Acc: 90.208%
	I - Batch: 40 | Loss: 0.296 | Acc: 91.094%
	I - Batch: 50 | Loss: 0.289 | Acc: 91.125%
	I - Batch: 60 | Loss: 0.283 | Acc: 91.250%
	I - Batch: 70 | Loss: 0.282 | Acc: 91.518%
	I - Batch: 80 | Loss: 0.294 | Acc: 91.016%
	I - Batch: 90 | Loss: 0.281 | Acc: 91.458%
	I - Batch: 100 | Loss: 0.290 | Acc: 91.125%
	I - Batch: 110 | Loss: 0.295 | Acc: 90.795%
	I - Batch: 120 | Loss: 0.295 | Acc: 90.625%
	I - Batch: 130 | Loss: 0.291 | Acc: 90.769%
	I - Batch: 140 | Loss: 0.285 | Acc: 90.938%
	I - Batch: 150 | Loss: 0.284 | Acc: 90.833%
	I - Batch: 160 | Loss: 0.288 | Acc: 90.625%
	I - Batch: 170 | Loss: 0.294 | Acc: 90.515%
	I - Batch: 180 | Loss: 0.289 | Acc: 90.764%
	I - Batch: 190 | Loss: 0.289 | Acc: 90.822%
	I - Batch: 200 | Loss: 0.287 | Acc: 90.938%
	I - Batch: 210 | Loss: 0.286 | Acc: 90.833%
	I - Batch: 220 | Loss: 0.286 | Acc: 90.682%
	I - Batch: 230 | Loss: 0.283 | Acc: 90.842%
	I - Batch: 240 | Loss: 0.282 | Acc: 90.911%
	I - Batch: 250 | Loss: 0.278 | Acc: 91.125%
	I - Batch: 260 | Loss: 0.278 | Acc: 91.178%
	I - Batch: 270 | Loss: 0.278 | Acc: 91.250%
	I - Batch: 280 | Loss: 0.277 | Acc: 91.272%
	I - Batch: 290 | Loss: 0.274 | Acc: 91.401%
	I - Batch: 300 | Loss: 0.270 | Acc: 91.583%
	I - Batch: 310 | Loss: 0.269 | Acc: 91.613%
	I - Batch: 320 | Loss: 0.271 | Acc: 91.621%
	I - Batch: 330 | Loss: 0.270 | Acc: 91.629%
	I - Batch: 340 | Loss: 0.268 | Acc: 91.673%
	I - Batch: 350 | Loss: 0.267 | Acc: 91.732%
	I - Batch: 360 | Loss: 0.266 | Acc: 91.771%
	I - Batch: 370 | Loss: 0.264 | Acc: 91.892%
	I - Batch: 380 | Loss: 0.264 | Acc: 91.842%
	I - Batch: 390 | Loss: 0.264 | Acc: 91.875%
	I - Batch: 400 | Loss: 0.262 | Acc: 91.906%
	I - Batch: 410 | Loss: 0.263 | Acc: 91.814%
	I - Batch: 420 | Loss: 0.261 | Acc: 91.875%
	I - Batch: 430 | Loss: 0.263 | Acc: 91.817%
	I - Batch: 440 | Loss: 0.265 | Acc: 91.733%
	I - Batch: 450 | Loss: 0.266 | Acc: 91.653%
	I - Batch: 460 | Loss: 0.266 | Acc: 91.576%
	I - Batch: 470 | Loss: 0.269 | Acc: 91.543%
	I - Batch: 480 | Loss: 0.270 | Acc: 91.458%
	I - Batch: 490 | Loss: 0.270 | Acc: 91.403%
	I - Batch: 500 | Loss: 0.269 | Acc: 91.438%
	I - Batch: 510 | Loss: 0.269 | Acc: 91.360%
	I - Batch: 520 | Loss: 0.271 | Acc: 91.250%
	I - Batch: 530 | Loss: 0.272 | Acc: 91.191%
	I - Batch: 540 | Loss: 0.271 | Acc: 91.227%
	I - Batch: 550 | Loss: 0.270 | Acc: 91.227%
	I - Batch: 560 | Loss: 0.270 | Acc: 91.205%
	I - Batch: 570 | Loss: 0.270 | Acc: 91.228%
	I - Batch: 580 | Loss: 0.271 | Acc: 91.207%
	I - Batch: 590 | Loss: 0.271 | Acc: 91.112%
	I - Batch: 600 | Loss: 0.271 | Acc: 91.125%
	I - Batch: 610 | Loss: 0.270 | Acc: 91.158%
	I - Batch: 620 | Loss: 0.271 | Acc: 91.190%
	I - Batch: 630 | Loss: 0.270 | Acc: 91.250%
	I - Batch: 640 | Loss: 0.271 | Acc: 91.260%
	I - Batch: 650 | Loss: 0.271 | Acc: 91.279%
	I - Batch: 660 | Loss: 0.271 | Acc: 91.288%
	I - Batch: 670 | Loss: 0.272 | Acc: 91.213%
	I - Batch: 680 | Loss: 0.271 | Acc: 91.204%
	I - Batch: 690 | Loss: 0.273 | Acc: 91.150%
	I - Batch: 700 | Loss: 0.273 | Acc: 91.143%
I - num batch: 701
I - Train -- Loss: 0.272 | Acc: 91.155% | LR: 5.000000e-04 | Dur: 565.62s
I - Confusion Matrix: [row->prediction - col->label]
[[1462.    1.    0.   14.  136.]
 [   0.  590.   83.    7.   25.]
 [   1.  124.  883.    1.  237.]
 [  11.    9.    4. 1438.  119.]
 [  72.   11.   93.   44. 5851.]]

I - Val -- Loss: 1.724 | Acc: 56.935%
I - Confusion Matrix: [row->prediction - col->label]
[[270.   3.  11.  65.  32.]
 [  4. 110.  44.  16.  32.]
 [ 13.  99. 123.  26.  88.]
 [116.  37.  47. 313. 122.]
 [132. 121. 167. 154. 941.]]

I - Epoch: 23
	I - Batch: 10 | Loss: 0.379 | Acc: 87.500%
	I - Batch: 20 | Loss: 0.327 | Acc: 89.375%
	I - Batch: 30 | Loss: 0.283 | Acc: 90.833%
	I - Batch: 40 | Loss: 0.286 | Acc: 90.781%
	I - Batch: 50 | Loss: 0.283 | Acc: 90.625%
	I - Batch: 60 | Loss: 0.270 | Acc: 91.042%
	I - Batch: 70 | Loss: 0.255 | Acc: 92.054%
	I - Batch: 80 | Loss: 0.243 | Acc: 92.656%
	I - Batch: 90 | Loss: 0.237 | Acc: 92.917%
	I - Batch: 100 | Loss: 0.242 | Acc: 92.375%
	I - Batch: 110 | Loss: 0.251 | Acc: 91.932%
	I - Batch: 120 | Loss: 0.245 | Acc: 92.031%
	I - Batch: 130 | Loss: 0.253 | Acc: 91.827%
	I - Batch: 140 | Loss: 0.249 | Acc: 92.009%
	I - Batch: 150 | Loss: 0.249 | Acc: 91.875%
	I - Batch: 160 | Loss: 0.247 | Acc: 92.031%
	I - Batch: 170 | Loss: 0.245 | Acc: 92.059%
	I - Batch: 180 | Loss: 0.247 | Acc: 92.049%
	I - Batch: 190 | Loss: 0.241 | Acc: 92.270%
	I - Batch: 200 | Loss: 0.240 | Acc: 92.281%
	I - Batch: 210 | Loss: 0.236 | Acc: 92.470%
	I - Batch: 220 | Loss: 0.236 | Acc: 92.614%
	I - Batch: 230 | Loss: 0.236 | Acc: 92.609%
	I - Batch: 240 | Loss: 0.233 | Acc: 92.656%
	I - Batch: 250 | Loss: 0.234 | Acc: 92.575%
	I - Batch: 260 | Loss: 0.233 | Acc: 92.620%
	I - Batch: 270 | Loss: 0.236 | Acc: 92.477%
	I - Batch: 280 | Loss: 0.236 | Acc: 92.522%
	I - Batch: 290 | Loss: 0.236 | Acc: 92.522%
	I - Batch: 300 | Loss: 0.236 | Acc: 92.458%
	I - Batch: 310 | Loss: 0.236 | Acc: 92.500%
	I - Batch: 320 | Loss: 0.237 | Acc: 92.422%
	I - Batch: 330 | Loss: 0.238 | Acc: 92.443%
	I - Batch: 340 | Loss: 0.237 | Acc: 92.463%
	I - Batch: 350 | Loss: 0.236 | Acc: 92.411%
	I - Batch: 360 | Loss: 0.235 | Acc: 92.483%
	I - Batch: 370 | Loss: 0.235 | Acc: 92.449%
	I - Batch: 380 | Loss: 0.234 | Acc: 92.533%
	I - Batch: 390 | Loss: 0.234 | Acc: 92.500%
	I - Batch: 400 | Loss: 0.233 | Acc: 92.547%
	I - Batch: 410 | Loss: 0.234 | Acc: 92.515%
	I - Batch: 420 | Loss: 0.234 | Acc: 92.545%
	I - Batch: 430 | Loss: 0.234 | Acc: 92.544%
	I - Batch: 440 | Loss: 0.234 | Acc: 92.557%
	I - Batch: 450 | Loss: 0.234 | Acc: 92.528%
	I - Batch: 460 | Loss: 0.234 | Acc: 92.514%
	I - Batch: 470 | Loss: 0.232 | Acc: 92.540%
	I - Batch: 480 | Loss: 0.232 | Acc: 92.513%
	I - Batch: 490 | Loss: 0.233 | Acc: 92.500%
	I - Batch: 500 | Loss: 0.235 | Acc: 92.463%
	I - Batch: 510 | Loss: 0.236 | Acc: 92.365%
	I - Batch: 520 | Loss: 0.238 | Acc: 92.332%
	I - Batch: 530 | Loss: 0.239 | Acc: 92.323%
	I - Batch: 540 | Loss: 0.240 | Acc: 92.280%
	I - Batch: 550 | Loss: 0.240 | Acc: 92.216%
	I - Batch: 560 | Loss: 0.240 | Acc: 92.221%
	I - Batch: 570 | Loss: 0.240 | Acc: 92.270%
	I - Batch: 580 | Loss: 0.241 | Acc: 92.231%
	I - Batch: 590 | Loss: 0.242 | Acc: 92.203%
	I - Batch: 600 | Loss: 0.241 | Acc: 92.229%
	I - Batch: 610 | Loss: 0.242 | Acc: 92.213%
	I - Batch: 620 | Loss: 0.243 | Acc: 92.177%
	I - Batch: 630 | Loss: 0.243 | Acc: 92.183%
	I - Batch: 640 | Loss: 0.243 | Acc: 92.207%
	I - Batch: 650 | Loss: 0.244 | Acc: 92.154%
	I - Batch: 660 | Loss: 0.246 | Acc: 92.083%
	I - Batch: 670 | Loss: 0.246 | Acc: 92.024%
	I - Batch: 680 | Loss: 0.247 | Acc: 91.921%
	I - Batch: 690 | Loss: 0.247 | Acc: 91.938%
	I - Batch: 700 | Loss: 0.247 | Acc: 91.929%
I - num batch: 701
I - Train -- Loss: 0.248 | Acc: 91.913% | LR: 5.000000e-04 | Dur: 559.53s
I - Confusion Matrix: [row->prediction - col->label]
[[1465.    1.    0.   12.  123.]
 [   2.  605.   66.    6.   23.]
 [   0.  105.  898.    1.  235.]
 [  20.    8.    0. 1449.   95.]
 [  59.   16.   99.   36. 5892.]]

I - Val -- Loss: 1.945 | Acc: 55.703%
I - Confusion Matrix: [row->prediction - col->label]
[[420.  14.  39. 227. 152.]
 [  2.  84.  28.   6.  20.]
 [  6. 124. 132.  18.  95.]
 [ 37.  16.  20. 173.  38.]
 [ 70. 132. 173. 150. 910.]]

I - Epoch: 24
	I - Batch: 10 | Loss: 0.242 | Acc: 91.875%
	I - Batch: 20 | Loss: 0.251 | Acc: 91.875%
	I - Batch: 30 | Loss: 0.278 | Acc: 91.250%
	I - Batch: 40 | Loss: 0.259 | Acc: 91.562%
	I - Batch: 50 | Loss: 0.254 | Acc: 91.500%
	I - Batch: 60 | Loss: 0.280 | Acc: 91.250%
	I - Batch: 70 | Loss: 0.266 | Acc: 91.607%
	I - Batch: 80 | Loss: 0.267 | Acc: 91.719%
	I - Batch: 90 | Loss: 0.261 | Acc: 91.944%
	I - Batch: 100 | Loss: 0.259 | Acc: 91.625%
	I - Batch: 110 | Loss: 0.257 | Acc: 91.648%
	I - Batch: 120 | Loss: 0.247 | Acc: 92.083%
	I - Batch: 130 | Loss: 0.247 | Acc: 92.260%
	I - Batch: 140 | Loss: 0.241 | Acc: 92.455%
	I - Batch: 150 | Loss: 0.235 | Acc: 92.750%
	I - Batch: 160 | Loss: 0.238 | Acc: 92.734%
	I - Batch: 170 | Loss: 0.239 | Acc: 92.647%
	I - Batch: 180 | Loss: 0.234 | Acc: 92.778%
	I - Batch: 190 | Loss: 0.238 | Acc: 92.796%
	I - Batch: 200 | Loss: 0.235 | Acc: 92.750%
	I - Batch: 210 | Loss: 0.233 | Acc: 92.798%
	I - Batch: 220 | Loss: 0.232 | Acc: 92.898%
	I - Batch: 230 | Loss: 0.236 | Acc: 92.880%
	I - Batch: 240 | Loss: 0.238 | Acc: 92.734%
	I - Batch: 250 | Loss: 0.240 | Acc: 92.650%
	I - Batch: 260 | Loss: 0.241 | Acc: 92.524%
	I - Batch: 270 | Loss: 0.237 | Acc: 92.639%
	I - Batch: 280 | Loss: 0.235 | Acc: 92.790%
	I - Batch: 290 | Loss: 0.235 | Acc: 92.672%
	I - Batch: 300 | Loss: 0.235 | Acc: 92.562%
	I - Batch: 310 | Loss: 0.234 | Acc: 92.601%
	I - Batch: 320 | Loss: 0.232 | Acc: 92.676%
	I - Batch: 330 | Loss: 0.233 | Acc: 92.633%
	I - Batch: 340 | Loss: 0.234 | Acc: 92.518%
	I - Batch: 350 | Loss: 0.233 | Acc: 92.589%
	I - Batch: 360 | Loss: 0.236 | Acc: 92.552%
	I - Batch: 370 | Loss: 0.234 | Acc: 92.601%
	I - Batch: 380 | Loss: 0.236 | Acc: 92.549%
	I - Batch: 390 | Loss: 0.235 | Acc: 92.548%
	I - Batch: 400 | Loss: 0.238 | Acc: 92.484%
	I - Batch: 410 | Loss: 0.236 | Acc: 92.546%
	I - Batch: 420 | Loss: 0.235 | Acc: 92.530%
	I - Batch: 430 | Loss: 0.236 | Acc: 92.515%
	I - Batch: 440 | Loss: 0.236 | Acc: 92.528%
	I - Batch: 450 | Loss: 0.236 | Acc: 92.528%
	I - Batch: 460 | Loss: 0.235 | Acc: 92.554%
	I - Batch: 470 | Loss: 0.235 | Acc: 92.540%
	I - Batch: 480 | Loss: 0.233 | Acc: 92.578%
	I - Batch: 490 | Loss: 0.234 | Acc: 92.577%
	I - Batch: 500 | Loss: 0.234 | Acc: 92.612%
	I - Batch: 510 | Loss: 0.233 | Acc: 92.623%
	I - Batch: 520 | Loss: 0.235 | Acc: 92.560%
	I - Batch: 530 | Loss: 0.234 | Acc: 92.571%
	I - Batch: 540 | Loss: 0.234 | Acc: 92.593%
	I - Batch: 550 | Loss: 0.233 | Acc: 92.625%
	I - Batch: 560 | Loss: 0.231 | Acc: 92.690%
	I - Batch: 570 | Loss: 0.232 | Acc: 92.686%
	I - Batch: 580 | Loss: 0.231 | Acc: 92.694%
	I - Batch: 590 | Loss: 0.230 | Acc: 92.733%
	I - Batch: 600 | Loss: 0.229 | Acc: 92.740%
	I - Batch: 610 | Loss: 0.230 | Acc: 92.725%
	I - Batch: 620 | Loss: 0.230 | Acc: 92.702%
	I - Batch: 630 | Loss: 0.230 | Acc: 92.649%
	I - Batch: 640 | Loss: 0.232 | Acc: 92.588%
	I - Batch: 650 | Loss: 0.232 | Acc: 92.596%
	I - Batch: 660 | Loss: 0.231 | Acc: 92.614%
	I - Batch: 670 | Loss: 0.232 | Acc: 92.565%
	I - Batch: 680 | Loss: 0.233 | Acc: 92.564%
	I - Batch: 690 | Loss: 0.233 | Acc: 92.500%
	I - Batch: 700 | Loss: 0.234 | Acc: 92.491%
I - num batch: 701
I - Train -- Loss: 0.235 | Acc: 92.475% | LR: 5.000000e-04 | Dur: 550.27s
I - Confusion Matrix: [row->prediction - col->label]
[[1475.    1.    0.    9.  149.]
 [   0.  640.   58.    3.   27.]
 [   0.   83.  925.    1.  208.]
 [  17.    4.    3. 1448.  100.]
 [  54.    7.   77.   43. 5884.]]

I - Val -- Loss: 2.104 | Acc: 54.245%
I - Confusion Matrix: [row->prediction - col->label]
[[285.   3.  17.  99.  40.]
 [  5.  89.  33.  12.  24.]
 [  9. 113. 129.  29. 117.]
 [ 55.  18.  19. 184.  47.]
 [181. 147. 194. 250. 987.]]

I - Epoch: 25
	I - Batch: 10 | Loss: 0.294 | Acc: 91.875%
	I - Batch: 20 | Loss: 0.290 | Acc: 92.812%
	I - Batch: 30 | Loss: 0.273 | Acc: 92.708%
	I - Batch: 40 | Loss: 0.272 | Acc: 91.875%
	I - Batch: 50 | Loss: 0.254 | Acc: 92.250%
	I - Batch: 60 | Loss: 0.247 | Acc: 92.292%
	I - Batch: 70 | Loss: 0.237 | Acc: 92.768%
	I - Batch: 80 | Loss: 0.224 | Acc: 93.203%
	I - Batch: 90 | Loss: 0.212 | Acc: 93.611%
	I - Batch: 100 | Loss: 0.214 | Acc: 93.812%
	I - Batch: 110 | Loss: 0.213 | Acc: 93.636%
	I - Batch: 120 | Loss: 0.211 | Acc: 93.854%
	I - Batch: 130 | Loss: 0.209 | Acc: 93.750%
	I - Batch: 140 | Loss: 0.204 | Acc: 93.884%
	I - Batch: 150 | Loss: 0.207 | Acc: 93.875%
	I - Batch: 160 | Loss: 0.207 | Acc: 93.672%
	I - Batch: 170 | Loss: 0.199 | Acc: 93.897%
	I - Batch: 180 | Loss: 0.197 | Acc: 93.924%
	I - Batch: 190 | Loss: 0.197 | Acc: 93.914%
	I - Batch: 200 | Loss: 0.197 | Acc: 93.875%
	I - Batch: 210 | Loss: 0.197 | Acc: 93.958%
	I - Batch: 220 | Loss: 0.194 | Acc: 94.091%
	I - Batch: 230 | Loss: 0.192 | Acc: 94.076%
	I - Batch: 240 | Loss: 0.189 | Acc: 94.141%
	I - Batch: 250 | Loss: 0.186 | Acc: 94.250%
	I - Batch: 260 | Loss: 0.185 | Acc: 94.351%
	I - Batch: 270 | Loss: 0.185 | Acc: 94.329%
	I - Batch: 280 | Loss: 0.183 | Acc: 94.353%
	I - Batch: 290 | Loss: 0.180 | Acc: 94.440%
	I - Batch: 300 | Loss: 0.179 | Acc: 94.500%
	I - Batch: 310 | Loss: 0.178 | Acc: 94.556%
	I - Batch: 320 | Loss: 0.177 | Acc: 94.590%
	I - Batch: 330 | Loss: 0.175 | Acc: 94.678%
	I - Batch: 340 | Loss: 0.174 | Acc: 94.669%
	I - Batch: 350 | Loss: 0.175 | Acc: 94.679%
	I - Batch: 360 | Loss: 0.174 | Acc: 94.740%
	I - Batch: 370 | Loss: 0.174 | Acc: 94.764%
	I - Batch: 380 | Loss: 0.173 | Acc: 94.770%
	I - Batch: 390 | Loss: 0.172 | Acc: 94.808%
	I - Batch: 400 | Loss: 0.171 | Acc: 94.828%
	I - Batch: 410 | Loss: 0.170 | Acc: 94.893%
	I - Batch: 420 | Loss: 0.170 | Acc: 94.896%
	I - Batch: 430 | Loss: 0.170 | Acc: 94.913%
	I - Batch: 440 | Loss: 0.170 | Acc: 94.915%
	I - Batch: 450 | Loss: 0.170 | Acc: 94.917%
	I - Batch: 460 | Loss: 0.169 | Acc: 94.932%
	I - Batch: 470 | Loss: 0.168 | Acc: 94.960%
	I - Batch: 480 | Loss: 0.168 | Acc: 95.000%
	I - Batch: 490 | Loss: 0.167 | Acc: 95.038%
	I - Batch: 500 | Loss: 0.167 | Acc: 94.975%
	I - Batch: 510 | Loss: 0.167 | Acc: 94.963%
	I - Batch: 520 | Loss: 0.168 | Acc: 94.964%
	I - Batch: 530 | Loss: 0.167 | Acc: 95.000%
	I - Batch: 540 | Loss: 0.167 | Acc: 95.000%
	I - Batch: 550 | Loss: 0.167 | Acc: 95.023%
	I - Batch: 560 | Loss: 0.166 | Acc: 95.078%
	I - Batch: 570 | Loss: 0.164 | Acc: 95.132%
	I - Batch: 580 | Loss: 0.165 | Acc: 95.119%
	I - Batch: 590 | Loss: 0.164 | Acc: 95.138%
	I - Batch: 600 | Loss: 0.167 | Acc: 95.094%
	I - Batch: 610 | Loss: 0.167 | Acc: 95.102%
	I - Batch: 620 | Loss: 0.167 | Acc: 95.111%
	I - Batch: 630 | Loss: 0.166 | Acc: 95.159%
	I - Batch: 640 | Loss: 0.167 | Acc: 95.146%
	I - Batch: 650 | Loss: 0.167 | Acc: 95.183%
	I - Batch: 660 | Loss: 0.167 | Acc: 95.189%
	I - Batch: 670 | Loss: 0.166 | Acc: 95.233%
	I - Batch: 680 | Loss: 0.165 | Acc: 95.257%
	I - Batch: 690 | Loss: 0.164 | Acc: 95.290%
	I - Batch: 700 | Loss: 0.164 | Acc: 95.312%
I - num batch: 701
I - Train -- Loss: 0.165 | Acc: 95.310% | LR: 2.500000e-04 | Dur: 554.25s
I - Confusion Matrix: [row->prediction - col->label]
[[1504.    0.    0.    3.   72.]
 [   0.  659.   36.    1.   22.]
 [   0.   63.  973.    0.  131.]
 [   6.    7.    3. 1477.   66.]
 [  36.    6.   51.   23. 6077.]]

I - Val -- Loss: 1.914 | Acc: 57.745%
I - Confusion Matrix: [row->prediction - col->label]
[[ 319.    7.   17.   84.   52.]
 [   3.   80.   22.    5.   16.]
 [   4.  113.  136.   27.   84.]
 [  65.   21.   23.  247.   63.]
 [ 144.  149.  194.  211. 1000.]]

I - Epoch: 26
	I - Batch: 10 | Loss: 0.104 | Acc: 97.500%
	I - Batch: 20 | Loss: 0.119 | Acc: 97.500%
	I - Batch: 30 | Loss: 0.138 | Acc: 96.667%
	I - Batch: 40 | Loss: 0.131 | Acc: 96.719%
	I - Batch: 50 | Loss: 0.130 | Acc: 96.750%
	I - Batch: 60 | Loss: 0.128 | Acc: 96.771%
	I - Batch: 70 | Loss: 0.122 | Acc: 96.875%
	I - Batch: 80 | Loss: 0.132 | Acc: 96.875%
	I - Batch: 90 | Loss: 0.127 | Acc: 97.083%
	I - Batch: 100 | Loss: 0.121 | Acc: 97.250%
	I - Batch: 110 | Loss: 0.120 | Acc: 97.273%
	I - Batch: 120 | Loss: 0.117 | Acc: 97.344%
	I - Batch: 130 | Loss: 0.119 | Acc: 97.308%
	I - Batch: 140 | Loss: 0.116 | Acc: 97.455%
	I - Batch: 150 | Loss: 0.114 | Acc: 97.458%
	I - Batch: 160 | Loss: 0.112 | Acc: 97.539%
	I - Batch: 170 | Loss: 0.111 | Acc: 97.463%
	I - Batch: 180 | Loss: 0.115 | Acc: 97.361%
	I - Batch: 190 | Loss: 0.117 | Acc: 97.336%
	I - Batch: 200 | Loss: 0.120 | Acc: 97.250%
	I - Batch: 210 | Loss: 0.121 | Acc: 97.143%
	I - Batch: 220 | Loss: 0.122 | Acc: 97.188%
	I - Batch: 230 | Loss: 0.121 | Acc: 97.228%
	I - Batch: 240 | Loss: 0.123 | Acc: 97.214%
	I - Batch: 250 | Loss: 0.124 | Acc: 97.225%
	I - Batch: 260 | Loss: 0.124 | Acc: 97.163%
	I - Batch: 270 | Loss: 0.124 | Acc: 97.153%
	I - Batch: 280 | Loss: 0.125 | Acc: 97.009%
	I - Batch: 290 | Loss: 0.123 | Acc: 97.069%
	I - Batch: 300 | Loss: 0.122 | Acc: 97.083%
	I - Batch: 310 | Loss: 0.123 | Acc: 96.996%
	I - Batch: 320 | Loss: 0.124 | Acc: 96.953%
	I - Batch: 330 | Loss: 0.125 | Acc: 96.951%
	I - Batch: 340 | Loss: 0.123 | Acc: 97.004%
	I - Batch: 350 | Loss: 0.121 | Acc: 97.071%
	I - Batch: 360 | Loss: 0.122 | Acc: 97.049%
	I - Batch: 370 | Loss: 0.123 | Acc: 97.061%
	I - Batch: 380 | Loss: 0.123 | Acc: 97.072%
	I - Batch: 390 | Loss: 0.123 | Acc: 97.099%
	I - Batch: 400 | Loss: 0.123 | Acc: 97.109%
	I - Batch: 410 | Loss: 0.125 | Acc: 96.982%
	I - Batch: 420 | Loss: 0.124 | Acc: 97.024%
	I - Batch: 430 | Loss: 0.125 | Acc: 96.991%
	I - Batch: 440 | Loss: 0.125 | Acc: 96.946%
	I - Batch: 450 | Loss: 0.127 | Acc: 96.903%
	I - Batch: 460 | Loss: 0.127 | Acc: 96.929%
	I - Batch: 470 | Loss: 0.128 | Acc: 96.902%
	I - Batch: 480 | Loss: 0.127 | Acc: 96.927%
	I - Batch: 490 | Loss: 0.127 | Acc: 96.875%
	I - Batch: 500 | Loss: 0.126 | Acc: 96.888%
	I - Batch: 510 | Loss: 0.126 | Acc: 96.863%
	I - Batch: 520 | Loss: 0.127 | Acc: 96.827%
	I - Batch: 530 | Loss: 0.128 | Acc: 96.816%
	I - Batch: 540 | Loss: 0.127 | Acc: 96.840%
	I - Batch: 550 | Loss: 0.126 | Acc: 96.852%
	I - Batch: 560 | Loss: 0.127 | Acc: 96.786%
	I - Batch: 570 | Loss: 0.126 | Acc: 96.820%
	I - Batch: 580 | Loss: 0.126 | Acc: 96.810%
	I - Batch: 590 | Loss: 0.126 | Acc: 96.790%
	I - Batch: 600 | Loss: 0.126 | Acc: 96.792%
	I - Batch: 610 | Loss: 0.125 | Acc: 96.814%
	I - Batch: 620 | Loss: 0.125 | Acc: 96.845%
	I - Batch: 630 | Loss: 0.126 | Acc: 96.796%
	I - Batch: 640 | Loss: 0.126 | Acc: 96.777%
	I - Batch: 650 | Loss: 0.126 | Acc: 96.788%
	I - Batch: 660 | Loss: 0.125 | Acc: 96.809%
	I - Batch: 670 | Loss: 0.125 | Acc: 96.791%
	I - Batch: 680 | Loss: 0.125 | Acc: 96.765%
	I - Batch: 690 | Loss: 0.125 | Acc: 96.775%
	I - Batch: 700 | Loss: 0.126 | Acc: 96.759%
I - num batch: 701
I - Train -- Loss: 0.126 | Acc: 96.737% | LR: 2.500000e-04 | Dur: 572.05s
I - Confusion Matrix: [row->prediction - col->label]
[[1521.    0.    0.    5.   42.]
 [   1.  674.   25.    5.    8.]
 [   0.   49. 1002.    1.   87.]
 [   3.    4.    0. 1473.   51.]
 [  21.    8.   36.   20. 6180.]]

I - Val -- Loss: 1.870 | Acc: 57.291%
I - Confusion Matrix: [row->prediction - col->label]
[[326.   4.  21.  96.  76.]
 [  3. 104.  34.  13.  25.]
 [  3. 109. 132.  20.  85.]
 [ 79.  24.  37. 257.  80.]
 [124. 129. 168. 188. 949.]]

I - Epoch: 27
	I - Batch: 10 | Loss: 0.163 | Acc: 94.375%
	I - Batch: 20 | Loss: 0.115 | Acc: 97.188%
	I - Batch: 30 | Loss: 0.104 | Acc: 97.083%
	I - Batch: 40 | Loss: 0.106 | Acc: 96.875%
	I - Batch: 50 | Loss: 0.098 | Acc: 97.000%
	I - Batch: 60 | Loss: 0.108 | Acc: 96.771%
	I - Batch: 70 | Loss: 0.108 | Acc: 96.964%
	I - Batch: 80 | Loss: 0.103 | Acc: 97.188%
	I - Batch: 90 | Loss: 0.105 | Acc: 97.292%
	I - Batch: 100 | Loss: 0.100 | Acc: 97.562%
	I - Batch: 110 | Loss: 0.098 | Acc: 97.557%
	I - Batch: 120 | Loss: 0.099 | Acc: 97.552%
	I - Batch: 130 | Loss: 0.101 | Acc: 97.548%
	I - Batch: 140 | Loss: 0.101 | Acc: 97.500%
	I - Batch: 150 | Loss: 0.099 | Acc: 97.583%
	I - Batch: 160 | Loss: 0.097 | Acc: 97.617%
	I - Batch: 170 | Loss: 0.103 | Acc: 97.537%
	I - Batch: 180 | Loss: 0.106 | Acc: 97.465%
	I - Batch: 190 | Loss: 0.106 | Acc: 97.467%
	I - Batch: 200 | Loss: 0.106 | Acc: 97.469%
	I - Batch: 210 | Loss: 0.104 | Acc: 97.560%
	I - Batch: 220 | Loss: 0.103 | Acc: 97.642%
	I - Batch: 230 | Loss: 0.104 | Acc: 97.609%
	I - Batch: 240 | Loss: 0.106 | Acc: 97.500%
	I - Batch: 250 | Loss: 0.105 | Acc: 97.525%
	I - Batch: 260 | Loss: 0.104 | Acc: 97.548%
	I - Batch: 270 | Loss: 0.103 | Acc: 97.569%
	I - Batch: 280 | Loss: 0.104 | Acc: 97.567%
	I - Batch: 290 | Loss: 0.103 | Acc: 97.586%
	I - Batch: 300 | Loss: 0.102 | Acc: 97.625%
	I - Batch: 310 | Loss: 0.102 | Acc: 97.540%
	I - Batch: 320 | Loss: 0.104 | Acc: 97.520%
	I - Batch: 330 | Loss: 0.106 | Acc: 97.424%
	I - Batch: 340 | Loss: 0.107 | Acc: 97.408%
	I - Batch: 350 | Loss: 0.109 | Acc: 97.357%
	I - Batch: 360 | Loss: 0.110 | Acc: 97.361%
	I - Batch: 370 | Loss: 0.111 | Acc: 97.348%
	I - Batch: 380 | Loss: 0.111 | Acc: 97.319%
	I - Batch: 390 | Loss: 0.111 | Acc: 97.308%
	I - Batch: 400 | Loss: 0.110 | Acc: 97.344%
	I - Batch: 410 | Loss: 0.109 | Acc: 97.393%
	I - Batch: 420 | Loss: 0.109 | Acc: 97.426%
	I - Batch: 430 | Loss: 0.108 | Acc: 97.456%
	I - Batch: 440 | Loss: 0.107 | Acc: 97.472%
	I - Batch: 450 | Loss: 0.107 | Acc: 97.458%
	I - Batch: 460 | Loss: 0.106 | Acc: 97.514%
	I - Batch: 470 | Loss: 0.107 | Acc: 97.513%
	I - Batch: 480 | Loss: 0.106 | Acc: 97.526%
	I - Batch: 490 | Loss: 0.107 | Acc: 97.513%
	I - Batch: 500 | Loss: 0.107 | Acc: 97.463%
	I - Batch: 510 | Loss: 0.107 | Acc: 97.475%
	I - Batch: 520 | Loss: 0.107 | Acc: 97.440%
	I - Batch: 530 | Loss: 0.107 | Acc: 97.453%
	I - Batch: 540 | Loss: 0.107 | Acc: 97.442%
	I - Batch: 550 | Loss: 0.107 | Acc: 97.409%
	I - Batch: 560 | Loss: 0.107 | Acc: 97.400%
	I - Batch: 570 | Loss: 0.107 | Acc: 97.379%
	I - Batch: 580 | Loss: 0.108 | Acc: 97.317%
	I - Batch: 590 | Loss: 0.108 | Acc: 97.320%
	I - Batch: 600 | Loss: 0.109 | Acc: 97.292%
	I - Batch: 610 | Loss: 0.109 | Acc: 97.275%
	I - Batch: 620 | Loss: 0.111 | Acc: 97.228%
	I - Batch: 630 | Loss: 0.110 | Acc: 97.232%
	I - Batch: 640 | Loss: 0.110 | Acc: 97.178%
	I - Batch: 650 | Loss: 0.111 | Acc: 97.183%
	I - Batch: 660 | Loss: 0.110 | Acc: 97.197%
	I - Batch: 670 | Loss: 0.111 | Acc: 97.164%
	I - Batch: 680 | Loss: 0.112 | Acc: 97.114%
	I - Batch: 690 | Loss: 0.112 | Acc: 97.101%
	I - Batch: 700 | Loss: 0.113 | Acc: 97.080%
I - num batch: 701
I - Train -- Loss: 0.113 | Acc: 97.076% | LR: 2.500000e-04 | Dur: 582.59s
I - Confusion Matrix: [row->prediction - col->label]
[[1520.    0.    0.    5.   42.]
 [   0.  685.   18.    1.   11.]
 [   0.   43. 1011.    0.   97.]
 [   4.    1.    1. 1487.   33.]
 [  22.    6.   33.   11. 6185.]]

I - Val -- Loss: 1.940 | Acc: 57.809%
I - Confusion Matrix: [row->prediction - col->label]
[[310.   5.  15. 103.  55.]
 [  6. 101.  37.  11.  26.]
 [  6. 100. 131.  18. 102.]
 [ 79.  23.  32. 274.  64.]
 [134. 141. 177. 168. 968.]]

I - Epoch: 28
	I - Batch: 10 | Loss: 0.063 | Acc: 99.375%
	I - Batch: 20 | Loss: 0.078 | Acc: 98.438%
	I - Batch: 30 | Loss: 0.088 | Acc: 97.917%
	I - Batch: 40 | Loss: 0.086 | Acc: 98.125%
	I - Batch: 50 | Loss: 0.082 | Acc: 98.250%
	I - Batch: 60 | Loss: 0.091 | Acc: 97.708%
	I - Batch: 70 | Loss: 0.092 | Acc: 97.679%
	I - Batch: 80 | Loss: 0.087 | Acc: 97.812%
	I - Batch: 90 | Loss: 0.088 | Acc: 97.917%
	I - Batch: 100 | Loss: 0.090 | Acc: 97.750%
	I - Batch: 110 | Loss: 0.093 | Acc: 97.670%
	I - Batch: 120 | Loss: 0.091 | Acc: 97.760%
	I - Batch: 130 | Loss: 0.095 | Acc: 97.740%
	I - Batch: 140 | Loss: 0.098 | Acc: 97.723%
	I - Batch: 150 | Loss: 0.095 | Acc: 97.875%
	I - Batch: 160 | Loss: 0.094 | Acc: 97.852%
	I - Batch: 170 | Loss: 0.093 | Acc: 97.831%
	I - Batch: 180 | Loss: 0.095 | Acc: 97.708%
	I - Batch: 190 | Loss: 0.095 | Acc: 97.730%
	I - Batch: 200 | Loss: 0.093 | Acc: 97.844%
	I - Batch: 210 | Loss: 0.093 | Acc: 97.768%
	I - Batch: 220 | Loss: 0.094 | Acc: 97.642%
	I - Batch: 230 | Loss: 0.095 | Acc: 97.582%
	I - Batch: 240 | Loss: 0.098 | Acc: 97.474%
	I - Batch: 250 | Loss: 0.099 | Acc: 97.450%
	I - Batch: 260 | Loss: 0.098 | Acc: 97.500%
	I - Batch: 270 | Loss: 0.097 | Acc: 97.523%
	I - Batch: 280 | Loss: 0.098 | Acc: 97.545%
	I - Batch: 290 | Loss: 0.097 | Acc: 97.586%
	I - Batch: 300 | Loss: 0.097 | Acc: 97.583%
	I - Batch: 310 | Loss: 0.097 | Acc: 97.601%
	I - Batch: 320 | Loss: 0.099 | Acc: 97.520%
	I - Batch: 330 | Loss: 0.099 | Acc: 97.500%
	I - Batch: 340 | Loss: 0.100 | Acc: 97.445%
	I - Batch: 350 | Loss: 0.099 | Acc: 97.446%
	I - Batch: 360 | Loss: 0.100 | Acc: 97.413%
	I - Batch: 370 | Loss: 0.099 | Acc: 97.449%
	I - Batch: 380 | Loss: 0.099 | Acc: 97.451%
	I - Batch: 390 | Loss: 0.098 | Acc: 97.500%
	I - Batch: 400 | Loss: 0.098 | Acc: 97.531%
	I - Batch: 410 | Loss: 0.097 | Acc: 97.530%
	I - Batch: 420 | Loss: 0.099 | Acc: 97.440%
	I - Batch: 430 | Loss: 0.099 | Acc: 97.398%
	I - Batch: 440 | Loss: 0.100 | Acc: 97.401%
	I - Batch: 450 | Loss: 0.099 | Acc: 97.431%
	I - Batch: 460 | Loss: 0.099 | Acc: 97.459%
	I - Batch: 470 | Loss: 0.099 | Acc: 97.473%
	I - Batch: 480 | Loss: 0.098 | Acc: 97.487%
	I - Batch: 490 | Loss: 0.099 | Acc: 97.462%
	I - Batch: 500 | Loss: 0.099 | Acc: 97.412%
	I - Batch: 510 | Loss: 0.100 | Acc: 97.414%
	I - Batch: 520 | Loss: 0.099 | Acc: 97.428%
	I - Batch: 530 | Loss: 0.100 | Acc: 97.394%
	I - Batch: 540 | Loss: 0.100 | Acc: 97.396%
	I - Batch: 550 | Loss: 0.100 | Acc: 97.420%
	I - Batch: 560 | Loss: 0.102 | Acc: 97.366%
	I - Batch: 570 | Loss: 0.101 | Acc: 97.401%
	I - Batch: 580 | Loss: 0.100 | Acc: 97.435%
	I - Batch: 590 | Loss: 0.101 | Acc: 97.373%
	I - Batch: 600 | Loss: 0.102 | Acc: 97.365%
	I - Batch: 610 | Loss: 0.102 | Acc: 97.398%
	I - Batch: 620 | Loss: 0.102 | Acc: 97.409%
	I - Batch: 630 | Loss: 0.102 | Acc: 97.421%
	I - Batch: 640 | Loss: 0.103 | Acc: 97.402%
	I - Batch: 650 | Loss: 0.103 | Acc: 97.394%
	I - Batch: 660 | Loss: 0.102 | Acc: 97.415%
	I - Batch: 670 | Loss: 0.102 | Acc: 97.435%
	I - Batch: 680 | Loss: 0.101 | Acc: 97.454%
	I - Batch: 690 | Loss: 0.101 | Acc: 97.455%
	I - Batch: 700 | Loss: 0.101 | Acc: 97.473%
I - num batch: 701
I - Train -- Loss: 0.101 | Acc: 97.468% | LR: 2.500000e-04 | Dur: 568.71s
I - Confusion Matrix: [row->prediction - col->label]
[[1531.    0.    0.    2.   40.]
 [   0.  697.   21.    0.    9.]
 [   0.   34. 1018.    0.   79.]
 [   3.    2.    0. 1489.   43.]
 [  12.    2.   24.   13. 6197.]]

I - Val -- Loss: 1.968 | Acc: 56.610%
I - Confusion Matrix: [row->prediction - col->label]
[[329.  10.  22. 113.  75.]
 [  4.  69.  26.   9.  19.]
 [  5. 148. 141.  24. 101.]
 [ 70.  20.  21. 242.  54.]
 [127. 123. 182. 186. 966.]]

I - Epoch: 29
	I - Batch: 10 | Loss: 0.088 | Acc: 98.750%
	I - Batch: 20 | Loss: 0.082 | Acc: 98.438%
	I - Batch: 30 | Loss: 0.087 | Acc: 98.333%
	I - Batch: 40 | Loss: 0.088 | Acc: 98.281%
	I - Batch: 50 | Loss: 0.087 | Acc: 98.125%
	I - Batch: 60 | Loss: 0.084 | Acc: 98.125%
	I - Batch: 70 | Loss: 0.081 | Acc: 98.214%
	I - Batch: 80 | Loss: 0.079 | Acc: 98.203%
	I - Batch: 90 | Loss: 0.079 | Acc: 98.264%
	I - Batch: 100 | Loss: 0.076 | Acc: 98.375%
	I - Batch: 110 | Loss: 0.076 | Acc: 98.409%
	I - Batch: 120 | Loss: 0.078 | Acc: 98.333%
	I - Batch: 130 | Loss: 0.079 | Acc: 98.173%
	I - Batch: 140 | Loss: 0.078 | Acc: 98.214%
	I - Batch: 150 | Loss: 0.085 | Acc: 98.042%
	I - Batch: 160 | Loss: 0.082 | Acc: 98.164%
	I - Batch: 170 | Loss: 0.082 | Acc: 98.162%
	I - Batch: 180 | Loss: 0.083 | Acc: 98.090%
	I - Batch: 190 | Loss: 0.084 | Acc: 98.092%
	I - Batch: 200 | Loss: 0.083 | Acc: 98.156%
	I - Batch: 210 | Loss: 0.088 | Acc: 97.976%
	I - Batch: 220 | Loss: 0.086 | Acc: 98.068%
	I - Batch: 230 | Loss: 0.088 | Acc: 97.908%
	I - Batch: 240 | Loss: 0.093 | Acc: 97.708%
	I - Batch: 250 | Loss: 0.094 | Acc: 97.700%
	I - Batch: 260 | Loss: 0.092 | Acc: 97.764%
	I - Batch: 270 | Loss: 0.091 | Acc: 97.778%
	I - Batch: 280 | Loss: 0.092 | Acc: 97.746%
	I - Batch: 290 | Loss: 0.092 | Acc: 97.716%
	I - Batch: 300 | Loss: 0.092 | Acc: 97.729%
	I - Batch: 310 | Loss: 0.092 | Acc: 97.722%
	I - Batch: 320 | Loss: 0.094 | Acc: 97.676%
	I - Batch: 330 | Loss: 0.092 | Acc: 97.727%
	I - Batch: 340 | Loss: 0.092 | Acc: 97.739%
	I - Batch: 350 | Loss: 0.093 | Acc: 97.714%
	I - Batch: 360 | Loss: 0.094 | Acc: 97.708%
	I - Batch: 370 | Loss: 0.094 | Acc: 97.720%
	I - Batch: 380 | Loss: 0.093 | Acc: 97.747%
	I - Batch: 390 | Loss: 0.094 | Acc: 97.724%
	I - Batch: 400 | Loss: 0.095 | Acc: 97.688%
	I - Batch: 410 | Loss: 0.094 | Acc: 97.668%
	I - Batch: 420 | Loss: 0.095 | Acc: 97.679%
	I - Batch: 430 | Loss: 0.094 | Acc: 97.660%
	I - Batch: 440 | Loss: 0.093 | Acc: 97.670%
	I - Batch: 450 | Loss: 0.093 | Acc: 97.667%
	I - Batch: 460 | Loss: 0.093 | Acc: 97.636%
	I - Batch: 470 | Loss: 0.092 | Acc: 97.646%
	I - Batch: 480 | Loss: 0.092 | Acc: 97.630%
	I - Batch: 490 | Loss: 0.092 | Acc: 97.640%
	I - Batch: 500 | Loss: 0.092 | Acc: 97.650%
	I - Batch: 510 | Loss: 0.092 | Acc: 97.672%
	I - Batch: 520 | Loss: 0.092 | Acc: 97.680%
	I - Batch: 530 | Loss: 0.093 | Acc: 97.665%
	I - Batch: 540 | Loss: 0.094 | Acc: 97.639%
	I - Batch: 550 | Loss: 0.094 | Acc: 97.625%
	I - Batch: 560 | Loss: 0.095 | Acc: 97.612%
	I - Batch: 570 | Loss: 0.095 | Acc: 97.632%
	I - Batch: 580 | Loss: 0.095 | Acc: 97.619%
	I - Batch: 590 | Loss: 0.094 | Acc: 97.648%
	I - Batch: 600 | Loss: 0.095 | Acc: 97.604%
	I - Batch: 610 | Loss: 0.095 | Acc: 97.572%
	I - Batch: 620 | Loss: 0.095 | Acc: 97.601%
	I - Batch: 630 | Loss: 0.094 | Acc: 97.639%
	I - Batch: 640 | Loss: 0.093 | Acc: 97.656%
	I - Batch: 650 | Loss: 0.094 | Acc: 97.625%
	I - Batch: 660 | Loss: 0.094 | Acc: 97.633%
	I - Batch: 670 | Loss: 0.093 | Acc: 97.659%
	I - Batch: 680 | Loss: 0.093 | Acc: 97.684%
	I - Batch: 690 | Loss: 0.092 | Acc: 97.708%
	I - Batch: 700 | Loss: 0.092 | Acc: 97.732%
I - num batch: 701
I - Train -- Loss: 0.092 | Acc: 97.735% | LR: 2.500000e-04 | Dur: 555.68s
I - Confusion Matrix: [row->prediction - col->label]
[[1525.    0.    0.    2.   37.]
 [   0.  694.   19.    0.    3.]
 [   0.   37. 1019.    1.   55.]
 [   3.    1.    0. 1490.   39.]
 [  18.    3.   25.   11. 6234.]]

I - Val -- Loss: 2.046 | Acc: 57.615%
I - Confusion Matrix: [row->prediction - col->label]
[[334.   7.  19. 105.  66.]
 [  5.  86.  33.   8.  18.]
 [ 10. 112. 125.  29.  88.]
 [ 65.  25.  30. 243.  53.]
 [121. 140. 185. 189. 990.]]

I - Epoch: 30
	I - Batch: 10 | Loss: 0.101 | Acc: 97.500%
	I - Batch: 20 | Loss: 0.093 | Acc: 97.500%
	I - Batch: 30 | Loss: 0.093 | Acc: 97.708%
	I - Batch: 40 | Loss: 0.096 | Acc: 97.812%
	I - Batch: 50 | Loss: 0.113 | Acc: 97.750%
	I - Batch: 60 | Loss: 0.105 | Acc: 97.917%
	I - Batch: 70 | Loss: 0.102 | Acc: 97.857%
	I - Batch: 80 | Loss: 0.097 | Acc: 97.891%
	I - Batch: 90 | Loss: 0.099 | Acc: 97.708%
	I - Batch: 100 | Loss: 0.095 | Acc: 97.750%
	I - Batch: 110 | Loss: 0.098 | Acc: 97.670%
	I - Batch: 120 | Loss: 0.097 | Acc: 97.708%
	I - Batch: 130 | Loss: 0.101 | Acc: 97.452%
	I - Batch: 140 | Loss: 0.097 | Acc: 97.545%
	I - Batch: 150 | Loss: 0.094 | Acc: 97.667%
	I - Batch: 160 | Loss: 0.093 | Acc: 97.734%
	I - Batch: 170 | Loss: 0.090 | Acc: 97.868%
	I - Batch: 180 | Loss: 0.093 | Acc: 97.812%
	I - Batch: 190 | Loss: 0.090 | Acc: 97.895%
	I - Batch: 200 | Loss: 0.090 | Acc: 97.938%
	I - Batch: 210 | Loss: 0.089 | Acc: 97.976%
	I - Batch: 220 | Loss: 0.088 | Acc: 98.011%
	I - Batch: 230 | Loss: 0.086 | Acc: 98.098%
	I - Batch: 240 | Loss: 0.085 | Acc: 98.125%
	I - Batch: 250 | Loss: 0.085 | Acc: 98.125%
	I - Batch: 260 | Loss: 0.085 | Acc: 98.029%
	I - Batch: 270 | Loss: 0.086 | Acc: 98.032%
	I - Batch: 280 | Loss: 0.089 | Acc: 97.902%
	I - Batch: 290 | Loss: 0.088 | Acc: 97.931%
	I - Batch: 300 | Loss: 0.087 | Acc: 97.958%
	I - Batch: 310 | Loss: 0.087 | Acc: 97.964%
	I - Batch: 320 | Loss: 0.087 | Acc: 97.949%
	I - Batch: 330 | Loss: 0.086 | Acc: 97.992%
	I - Batch: 340 | Loss: 0.085 | Acc: 98.033%
	I - Batch: 350 | Loss: 0.085 | Acc: 97.964%
	I - Batch: 360 | Loss: 0.084 | Acc: 97.986%
	I - Batch: 370 | Loss: 0.083 | Acc: 98.041%
	I - Batch: 380 | Loss: 0.083 | Acc: 98.043%
	I - Batch: 390 | Loss: 0.083 | Acc: 98.061%
	I - Batch: 400 | Loss: 0.082 | Acc: 98.109%
	I - Batch: 410 | Loss: 0.083 | Acc: 98.079%
	I - Batch: 420 | Loss: 0.082 | Acc: 98.110%
	I - Batch: 430 | Loss: 0.083 | Acc: 98.067%
	I - Batch: 440 | Loss: 0.082 | Acc: 98.097%
	I - Batch: 450 | Loss: 0.082 | Acc: 98.083%
	I - Batch: 460 | Loss: 0.083 | Acc: 98.057%
	I - Batch: 470 | Loss: 0.083 | Acc: 98.045%
	I - Batch: 480 | Loss: 0.083 | Acc: 98.034%
	I - Batch: 490 | Loss: 0.084 | Acc: 98.048%
	I - Batch: 500 | Loss: 0.084 | Acc: 98.050%
	I - Batch: 510 | Loss: 0.084 | Acc: 98.039%
	I - Batch: 520 | Loss: 0.083 | Acc: 98.053%
	I - Batch: 530 | Loss: 0.083 | Acc: 98.066%
	I - Batch: 540 | Loss: 0.082 | Acc: 98.090%
	I - Batch: 550 | Loss: 0.081 | Acc: 98.091%
	I - Batch: 560 | Loss: 0.081 | Acc: 98.103%
	I - Batch: 570 | Loss: 0.082 | Acc: 98.092%
	I - Batch: 580 | Loss: 0.081 | Acc: 98.093%
	I - Batch: 590 | Loss: 0.081 | Acc: 98.072%
	I - Batch: 600 | Loss: 0.081 | Acc: 98.094%
	I - Batch: 610 | Loss: 0.081 | Acc: 98.074%
	I - Batch: 620 | Loss: 0.081 | Acc: 98.095%
	I - Batch: 630 | Loss: 0.081 | Acc: 98.075%
	I - Batch: 640 | Loss: 0.081 | Acc: 98.086%
	I - Batch: 650 | Loss: 0.081 | Acc: 98.106%
	I - Batch: 660 | Loss: 0.081 | Acc: 98.116%
	I - Batch: 670 | Loss: 0.080 | Acc: 98.134%
	I - Batch: 680 | Loss: 0.080 | Acc: 98.153%
	I - Batch: 690 | Loss: 0.079 | Acc: 98.161%
	I - Batch: 700 | Loss: 0.079 | Acc: 98.161%
I - num batch: 701
I - Train -- Loss: 0.079 | Acc: 98.154% | LR: 2.500000e-04 | Dur: 554.82s
I - Confusion Matrix: [row->prediction - col->label]
[[1527.    1.    0.    0.   38.]
 [   0.  707.    9.    1.    7.]
 [   0.   22. 1035.    0.   58.]
 [   1.    1.    0. 1496.   21.]
 [  18.    4.   19.    7. 6244.]]

I - Val -- Loss: 2.040 | Acc: 58.069%
I - Confusion Matrix: [row->prediction - col->label]
[[ 310.    7.   21.   97.   59.]
 [   5.   72.   22.   11.   12.]
 [   6.  107.  125.   20.   73.]
 [  87.   22.   31.  280.   66.]
 [ 127.  162.  193.  166. 1005.]]

I - Epoch: 31
	I - Batch: 10 | Loss: 0.064 | Acc: 99.375%
	I - Batch: 20 | Loss: 0.075 | Acc: 98.750%
	I - Batch: 30 | Loss: 0.068 | Acc: 98.958%
	I - Batch: 40 | Loss: 0.066 | Acc: 98.906%
	I - Batch: 50 | Loss: 0.077 | Acc: 98.875%
	I - Batch: 60 | Loss: 0.075 | Acc: 98.958%
	I - Batch: 70 | Loss: 0.076 | Acc: 98.929%
	I - Batch: 80 | Loss: 0.075 | Acc: 98.828%
	I - Batch: 90 | Loss: 0.071 | Acc: 98.958%
	I - Batch: 100 | Loss: 0.073 | Acc: 98.688%
	I - Batch: 110 | Loss: 0.077 | Acc: 98.693%
	I - Batch: 120 | Loss: 0.075 | Acc: 98.698%
	I - Batch: 130 | Loss: 0.075 | Acc: 98.654%
	I - Batch: 140 | Loss: 0.073 | Acc: 98.705%
	I - Batch: 150 | Loss: 0.070 | Acc: 98.750%
	I - Batch: 160 | Loss: 0.068 | Acc: 98.828%
	I - Batch: 170 | Loss: 0.068 | Acc: 98.824%
	I - Batch: 180 | Loss: 0.069 | Acc: 98.819%
	I - Batch: 190 | Loss: 0.069 | Acc: 98.816%
	I - Batch: 200 | Loss: 0.069 | Acc: 98.844%
	I - Batch: 210 | Loss: 0.069 | Acc: 98.839%
	I - Batch: 220 | Loss: 0.069 | Acc: 98.807%
	I - Batch: 230 | Loss: 0.069 | Acc: 98.777%
	I - Batch: 240 | Loss: 0.068 | Acc: 98.828%
	I - Batch: 250 | Loss: 0.069 | Acc: 98.800%
	I - Batch: 260 | Loss: 0.070 | Acc: 98.678%
	I - Batch: 270 | Loss: 0.070 | Acc: 98.704%
	I - Batch: 280 | Loss: 0.071 | Acc: 98.616%
	I - Batch: 290 | Loss: 0.070 | Acc: 98.642%
	I - Batch: 300 | Loss: 0.072 | Acc: 98.562%
	I - Batch: 310 | Loss: 0.071 | Acc: 98.569%
	I - Batch: 320 | Loss: 0.071 | Acc: 98.574%
	I - Batch: 330 | Loss: 0.072 | Acc: 98.542%
	I - Batch: 340 | Loss: 0.072 | Acc: 98.511%
	I - Batch: 350 | Loss: 0.072 | Acc: 98.500%
	I - Batch: 360 | Loss: 0.072 | Acc: 98.524%
	I - Batch: 370 | Loss: 0.072 | Acc: 98.497%
	I - Batch: 380 | Loss: 0.072 | Acc: 98.503%
	I - Batch: 390 | Loss: 0.071 | Acc: 98.526%
	I - Batch: 400 | Loss: 0.072 | Acc: 98.484%
	I - Batch: 410 | Loss: 0.074 | Acc: 98.415%
	I - Batch: 420 | Loss: 0.073 | Acc: 98.423%
	I - Batch: 430 | Loss: 0.073 | Acc: 98.459%
	I - Batch: 440 | Loss: 0.073 | Acc: 98.480%
	I - Batch: 450 | Loss: 0.074 | Acc: 98.444%
	I - Batch: 460 | Loss: 0.075 | Acc: 98.397%
	I - Batch: 470 | Loss: 0.074 | Acc: 98.418%
	I - Batch: 480 | Loss: 0.075 | Acc: 98.385%
	I - Batch: 490 | Loss: 0.075 | Acc: 98.380%
	I - Batch: 500 | Loss: 0.075 | Acc: 98.312%
	I - Batch: 510 | Loss: 0.075 | Acc: 98.346%
	I - Batch: 520 | Loss: 0.076 | Acc: 98.329%
	I - Batch: 530 | Loss: 0.076 | Acc: 98.349%
	I - Batch: 540 | Loss: 0.075 | Acc: 98.380%
	I - Batch: 550 | Loss: 0.076 | Acc: 98.341%
	I - Batch: 560 | Loss: 0.075 | Acc: 98.359%
	I - Batch: 570 | Loss: 0.075 | Acc: 98.355%
	I - Batch: 580 | Loss: 0.075 | Acc: 98.373%
	I - Batch: 590 | Loss: 0.075 | Acc: 98.369%
	I - Batch: 600 | Loss: 0.075 | Acc: 98.375%
	I - Batch: 610 | Loss: 0.075 | Acc: 98.391%
	I - Batch: 620 | Loss: 0.074 | Acc: 98.397%
	I - Batch: 630 | Loss: 0.074 | Acc: 98.403%
	I - Batch: 640 | Loss: 0.074 | Acc: 98.398%
	I - Batch: 650 | Loss: 0.074 | Acc: 98.413%
	I - Batch: 660 | Loss: 0.075 | Acc: 98.371%
	I - Batch: 670 | Loss: 0.075 | Acc: 98.358%
	I - Batch: 680 | Loss: 0.076 | Acc: 98.364%
	I - Batch: 690 | Loss: 0.076 | Acc: 98.333%
	I - Batch: 700 | Loss: 0.076 | Acc: 98.312%
I - num batch: 701
I - Train -- Loss: 0.076 | Acc: 98.315% | LR: 2.500000e-04 | Dur: 559.48s
I - Confusion Matrix: [row->prediction - col->label]
[[1532.    0.    0.    1.   32.]
 [   1.  717.    7.    2.    6.]
 [   0.   13. 1035.    0.   50.]
 [   2.    1.    0. 1490.   27.]
 [  11.    4.   21.   11. 6253.]]

I - Val -- Loss: 2.195 | Acc: 56.546%
I - Confusion Matrix: [row->prediction - col->label]
[[ 342.    9.   26.  129.   66.]
 [   4.   79.   27.    4.   21.]
 [   2.   99.  121.   24.   72.]
 [  44.   26.   29.  198.   51.]
 [ 143.  157.  189.  219. 1005.]]

I - Epoch: 32
	I - Batch: 10 | Loss: 0.118 | Acc: 95.625%
	I - Batch: 20 | Loss: 0.084 | Acc: 97.500%
	I - Batch: 30 | Loss: 0.074 | Acc: 97.708%
	I - Batch: 40 | Loss: 0.079 | Acc: 97.969%
	I - Batch: 50 | Loss: 0.072 | Acc: 98.250%
	I - Batch: 60 | Loss: 0.075 | Acc: 97.812%
	I - Batch: 70 | Loss: 0.071 | Acc: 98.036%
	I - Batch: 80 | Loss: 0.071 | Acc: 97.969%
	I - Batch: 90 | Loss: 0.071 | Acc: 97.847%
	I - Batch: 100 | Loss: 0.076 | Acc: 97.750%
	I - Batch: 110 | Loss: 0.075 | Acc: 97.727%
	I - Batch: 120 | Loss: 0.076 | Acc: 97.760%
	I - Batch: 130 | Loss: 0.076 | Acc: 97.788%
	I - Batch: 140 | Loss: 0.080 | Acc: 97.679%
	I - Batch: 150 | Loss: 0.077 | Acc: 97.750%
	I - Batch: 160 | Loss: 0.078 | Acc: 97.695%
	I - Batch: 170 | Loss: 0.077 | Acc: 97.757%
	I - Batch: 180 | Loss: 0.078 | Acc: 97.708%
	I - Batch: 190 | Loss: 0.079 | Acc: 97.664%
	I - Batch: 200 | Loss: 0.078 | Acc: 97.719%
	I - Batch: 210 | Loss: 0.077 | Acc: 97.708%
	I - Batch: 220 | Loss: 0.076 | Acc: 97.812%
	I - Batch: 230 | Loss: 0.075 | Acc: 97.853%
	I - Batch: 240 | Loss: 0.074 | Acc: 97.943%
	I - Batch: 250 | Loss: 0.074 | Acc: 97.925%
	I - Batch: 260 | Loss: 0.073 | Acc: 97.933%
	I - Batch: 270 | Loss: 0.073 | Acc: 97.940%
	I - Batch: 280 | Loss: 0.072 | Acc: 97.991%
	I - Batch: 290 | Loss: 0.071 | Acc: 98.017%
	I - Batch: 300 | Loss: 0.071 | Acc: 98.021%
	I - Batch: 310 | Loss: 0.073 | Acc: 98.004%
	I - Batch: 320 | Loss: 0.076 | Acc: 97.871%
	I - Batch: 330 | Loss: 0.076 | Acc: 97.860%
	I - Batch: 340 | Loss: 0.078 | Acc: 97.849%
	I - Batch: 350 | Loss: 0.078 | Acc: 97.839%
	I - Batch: 360 | Loss: 0.080 | Acc: 97.812%
	I - Batch: 370 | Loss: 0.080 | Acc: 97.821%
	I - Batch: 380 | Loss: 0.080 | Acc: 97.812%
	I - Batch: 390 | Loss: 0.080 | Acc: 97.869%
	I - Batch: 400 | Loss: 0.079 | Acc: 97.891%
	I - Batch: 410 | Loss: 0.079 | Acc: 97.912%
	I - Batch: 420 | Loss: 0.078 | Acc: 97.946%
	I - Batch: 430 | Loss: 0.078 | Acc: 97.965%
	I - Batch: 440 | Loss: 0.077 | Acc: 97.997%
	I - Batch: 450 | Loss: 0.077 | Acc: 98.014%
	I - Batch: 460 | Loss: 0.076 | Acc: 98.043%
	I - Batch: 470 | Loss: 0.075 | Acc: 98.072%
	I - Batch: 480 | Loss: 0.075 | Acc: 98.099%
	I - Batch: 490 | Loss: 0.075 | Acc: 98.087%
	I - Batch: 500 | Loss: 0.074 | Acc: 98.075%
	I - Batch: 510 | Loss: 0.074 | Acc: 98.088%
	I - Batch: 520 | Loss: 0.073 | Acc: 98.113%
	I - Batch: 530 | Loss: 0.073 | Acc: 98.137%
	I - Batch: 540 | Loss: 0.073 | Acc: 98.137%
	I - Batch: 550 | Loss: 0.073 | Acc: 98.159%
	I - Batch: 560 | Loss: 0.072 | Acc: 98.181%
	I - Batch: 570 | Loss: 0.072 | Acc: 98.191%
	I - Batch: 580 | Loss: 0.072 | Acc: 98.168%
	I - Batch: 590 | Loss: 0.072 | Acc: 98.178%
	I - Batch: 600 | Loss: 0.072 | Acc: 98.188%
	I - Batch: 610 | Loss: 0.072 | Acc: 98.176%
	I - Batch: 620 | Loss: 0.072 | Acc: 98.185%
	I - Batch: 630 | Loss: 0.072 | Acc: 98.185%
	I - Batch: 640 | Loss: 0.072 | Acc: 98.193%
	I - Batch: 650 | Loss: 0.071 | Acc: 98.212%
	I - Batch: 660 | Loss: 0.072 | Acc: 98.210%
	I - Batch: 670 | Loss: 0.072 | Acc: 98.209%
	I - Batch: 680 | Loss: 0.072 | Acc: 98.217%
	I - Batch: 690 | Loss: 0.073 | Acc: 98.216%
	I - Batch: 700 | Loss: 0.073 | Acc: 98.205%
I - num batch: 701
I - Train -- Loss: 0.073 | Acc: 98.208% | LR: 2.500000e-04 | Dur: 553.09s
I - Confusion Matrix: [row->prediction - col->label]
[[1529.    0.    0.    2.   28.]
 [   1.  708.   10.    0.   12.]
 [   0.   20. 1030.    0.   48.]
 [   1.    1.    0. 1492.   24.]
 [  15.    6.   23.   10. 6256.]]

I - Val -- Loss: 2.117 | Acc: 57.518%
I - Confusion Matrix: [row->prediction - col->label]
[[308.   8.  22. 112.  64.]
 [  5. 100.  37.  13.  19.]
 [  9. 100. 133.  30.  82.]
 [ 86.  22.  30. 253.  69.]
 [127. 140. 170. 166. 981.]]

I - Epoch: 33
	I - Batch: 10 | Loss: 0.068 | Acc: 98.750%
	I - Batch: 20 | Loss: 0.072 | Acc: 98.438%
	I - Batch: 30 | Loss: 0.066 | Acc: 98.542%
	I - Batch: 40 | Loss: 0.058 | Acc: 98.906%
	I - Batch: 50 | Loss: 0.063 | Acc: 98.500%
	I - Batch: 60 | Loss: 0.066 | Acc: 98.333%
	I - Batch: 70 | Loss: 0.063 | Acc: 98.482%
	I - Batch: 80 | Loss: 0.067 | Acc: 98.281%
	I - Batch: 90 | Loss: 0.064 | Acc: 98.403%
	I - Batch: 100 | Loss: 0.062 | Acc: 98.500%
	I - Batch: 110 | Loss: 0.070 | Acc: 98.295%
	I - Batch: 120 | Loss: 0.074 | Acc: 98.177%
	I - Batch: 130 | Loss: 0.075 | Acc: 98.221%
	I - Batch: 140 | Loss: 0.075 | Acc: 98.214%
	I - Batch: 150 | Loss: 0.072 | Acc: 98.292%
	I - Batch: 160 | Loss: 0.070 | Acc: 98.398%
	I - Batch: 170 | Loss: 0.068 | Acc: 98.493%
	I - Batch: 180 | Loss: 0.069 | Acc: 98.403%
	I - Batch: 190 | Loss: 0.070 | Acc: 98.421%
	I - Batch: 200 | Loss: 0.071 | Acc: 98.375%
	I - Batch: 210 | Loss: 0.069 | Acc: 98.452%
	I - Batch: 220 | Loss: 0.069 | Acc: 98.466%
	I - Batch: 230 | Loss: 0.070 | Acc: 98.424%
	I - Batch: 240 | Loss: 0.070 | Acc: 98.464%
	I - Batch: 250 | Loss: 0.071 | Acc: 98.425%
	I - Batch: 260 | Loss: 0.071 | Acc: 98.438%
	I - Batch: 270 | Loss: 0.071 | Acc: 98.472%
	I - Batch: 280 | Loss: 0.072 | Acc: 98.482%
	I - Batch: 290 | Loss: 0.071 | Acc: 98.534%
	I - Batch: 300 | Loss: 0.070 | Acc: 98.562%
	I - Batch: 310 | Loss: 0.069 | Acc: 98.589%
	I - Batch: 320 | Loss: 0.068 | Acc: 98.613%
	I - Batch: 330 | Loss: 0.067 | Acc: 98.636%
	I - Batch: 340 | Loss: 0.067 | Acc: 98.676%
	I - Batch: 350 | Loss: 0.067 | Acc: 98.679%
	I - Batch: 360 | Loss: 0.069 | Acc: 98.646%
	I - Batch: 370 | Loss: 0.068 | Acc: 98.682%
	I - Batch: 380 | Loss: 0.067 | Acc: 98.717%
	I - Batch: 390 | Loss: 0.067 | Acc: 98.718%
	I - Batch: 400 | Loss: 0.067 | Acc: 98.688%
	I - Batch: 410 | Loss: 0.066 | Acc: 98.720%
	I - Batch: 420 | Loss: 0.066 | Acc: 98.720%
	I - Batch: 430 | Loss: 0.066 | Acc: 98.721%
	I - Batch: 440 | Loss: 0.066 | Acc: 98.722%
	I - Batch: 450 | Loss: 0.067 | Acc: 98.694%
	I - Batch: 460 | Loss: 0.068 | Acc: 98.682%
	I - Batch: 470 | Loss: 0.068 | Acc: 98.697%
	I - Batch: 480 | Loss: 0.068 | Acc: 98.698%
	I - Batch: 490 | Loss: 0.068 | Acc: 98.699%
	I - Batch: 500 | Loss: 0.068 | Acc: 98.700%
	I - Batch: 510 | Loss: 0.067 | Acc: 98.713%
	I - Batch: 520 | Loss: 0.067 | Acc: 98.738%
	I - Batch: 530 | Loss: 0.067 | Acc: 98.691%
	I - Batch: 540 | Loss: 0.068 | Acc: 98.681%
	I - Batch: 550 | Loss: 0.068 | Acc: 98.648%
	I - Batch: 560 | Loss: 0.068 | Acc: 98.650%
	I - Batch: 570 | Loss: 0.068 | Acc: 98.629%
	I - Batch: 580 | Loss: 0.068 | Acc: 98.653%
	I - Batch: 590 | Loss: 0.070 | Acc: 98.581%
	I - Batch: 600 | Loss: 0.070 | Acc: 98.583%
	I - Batch: 610 | Loss: 0.070 | Acc: 98.586%
	I - Batch: 620 | Loss: 0.070 | Acc: 98.609%
	I - Batch: 630 | Loss: 0.070 | Acc: 98.611%
	I - Batch: 640 | Loss: 0.069 | Acc: 98.623%
	I - Batch: 650 | Loss: 0.069 | Acc: 98.635%
	I - Batch: 660 | Loss: 0.069 | Acc: 98.646%
	I - Batch: 670 | Loss: 0.069 | Acc: 98.638%
	I - Batch: 680 | Loss: 0.069 | Acc: 98.631%
	I - Batch: 690 | Loss: 0.069 | Acc: 98.623%
	I - Batch: 700 | Loss: 0.068 | Acc: 98.625%
I - num batch: 701
I - Train -- Loss: 0.068 | Acc: 98.627% | LR: 2.500000e-04 | Dur: 578.58s
I - Confusion Matrix: [row->prediction - col->label]
[[1533.    0.    0.    0.   26.]
 [   0.  712.   12.    0.    2.]
 [   0.   17. 1040.    0.   33.]
 [   2.    3.    0. 1493.   23.]
 [  11.    3.   11.   11. 6284.]]

I - Val -- Loss: 2.195 | Acc: 56.578%
I - Confusion Matrix: [row->prediction - col->label]
[[292.   7.  15.  89.  35.]
 [  5. 105.  45.  16.  36.]
 [ 10.  91. 123.  25.  90.]
 [ 85.  16.  24. 228.  56.]
 [143. 151. 185. 216. 998.]]

I - Epoch: 34
	I - Batch: 10 | Loss: 0.089 | Acc: 98.750%
	I - Batch: 20 | Loss: 0.096 | Acc: 98.750%
	I - Batch: 30 | Loss: 0.073 | Acc: 99.167%
	I - Batch: 40 | Loss: 0.066 | Acc: 99.219%
	I - Batch: 50 | Loss: 0.078 | Acc: 98.625%
	I - Batch: 60 | Loss: 0.075 | Acc: 98.646%
	I - Batch: 70 | Loss: 0.073 | Acc: 98.661%
	I - Batch: 80 | Loss: 0.070 | Acc: 98.750%
	I - Batch: 90 | Loss: 0.075 | Acc: 98.403%
	I - Batch: 100 | Loss: 0.072 | Acc: 98.500%
	I - Batch: 110 | Loss: 0.069 | Acc: 98.580%
	I - Batch: 120 | Loss: 0.067 | Acc: 98.594%
	I - Batch: 130 | Loss: 0.066 | Acc: 98.606%
	I - Batch: 140 | Loss: 0.066 | Acc: 98.661%
	I - Batch: 150 | Loss: 0.066 | Acc: 98.667%
	I - Batch: 160 | Loss: 0.066 | Acc: 98.633%
	I - Batch: 170 | Loss: 0.066 | Acc: 98.566%
	I - Batch: 180 | Loss: 0.065 | Acc: 98.611%
	I - Batch: 190 | Loss: 0.065 | Acc: 98.618%
	I - Batch: 200 | Loss: 0.064 | Acc: 98.656%
	I - Batch: 210 | Loss: 0.064 | Acc: 98.631%
	I - Batch: 220 | Loss: 0.064 | Acc: 98.636%
	I - Batch: 230 | Loss: 0.063 | Acc: 98.696%
	I - Batch: 240 | Loss: 0.065 | Acc: 98.594%
	I - Batch: 250 | Loss: 0.066 | Acc: 98.500%
	I - Batch: 260 | Loss: 0.066 | Acc: 98.438%
	I - Batch: 270 | Loss: 0.065 | Acc: 98.472%
	I - Batch: 280 | Loss: 0.066 | Acc: 98.393%
	I - Batch: 290 | Loss: 0.065 | Acc: 98.427%
	I - Batch: 300 | Loss: 0.064 | Acc: 98.417%
	I - Batch: 310 | Loss: 0.066 | Acc: 98.387%
	I - Batch: 320 | Loss: 0.065 | Acc: 98.418%
	I - Batch: 330 | Loss: 0.065 | Acc: 98.428%
	I - Batch: 340 | Loss: 0.064 | Acc: 98.456%
	I - Batch: 350 | Loss: 0.064 | Acc: 98.446%
	I - Batch: 360 | Loss: 0.064 | Acc: 98.438%
	I - Batch: 370 | Loss: 0.064 | Acc: 98.446%
	I - Batch: 380 | Loss: 0.064 | Acc: 98.454%
	I - Batch: 390 | Loss: 0.064 | Acc: 98.462%
	I - Batch: 400 | Loss: 0.063 | Acc: 98.469%
	I - Batch: 410 | Loss: 0.063 | Acc: 98.491%
	I - Batch: 420 | Loss: 0.063 | Acc: 98.482%
	I - Batch: 430 | Loss: 0.063 | Acc: 98.488%
	I - Batch: 440 | Loss: 0.063 | Acc: 98.466%
	I - Batch: 450 | Loss: 0.063 | Acc: 98.472%
	I - Batch: 460 | Loss: 0.064 | Acc: 98.465%
	I - Batch: 470 | Loss: 0.064 | Acc: 98.471%
	I - Batch: 480 | Loss: 0.064 | Acc: 98.438%
	I - Batch: 490 | Loss: 0.063 | Acc: 98.444%
	I - Batch: 500 | Loss: 0.063 | Acc: 98.450%
	I - Batch: 510 | Loss: 0.064 | Acc: 98.456%
	I - Batch: 520 | Loss: 0.064 | Acc: 98.425%
	I - Batch: 530 | Loss: 0.064 | Acc: 98.432%
	I - Batch: 540 | Loss: 0.065 | Acc: 98.426%
	I - Batch: 550 | Loss: 0.064 | Acc: 98.432%
	I - Batch: 560 | Loss: 0.064 | Acc: 98.460%
	I - Batch: 570 | Loss: 0.066 | Acc: 98.410%
	I - Batch: 580 | Loss: 0.066 | Acc: 98.405%
	I - Batch: 590 | Loss: 0.068 | Acc: 98.305%
	I - Batch: 600 | Loss: 0.068 | Acc: 98.312%
	I - Batch: 610 | Loss: 0.069 | Acc: 98.279%
	I - Batch: 620 | Loss: 0.069 | Acc: 98.296%
	I - Batch: 630 | Loss: 0.069 | Acc: 98.294%
	I - Batch: 640 | Loss: 0.069 | Acc: 98.281%
	I - Batch: 650 | Loss: 0.069 | Acc: 98.269%
	I - Batch: 660 | Loss: 0.069 | Acc: 98.295%
	I - Batch: 670 | Loss: 0.068 | Acc: 98.321%
	I - Batch: 680 | Loss: 0.068 | Acc: 98.327%
	I - Batch: 690 | Loss: 0.068 | Acc: 98.351%
	I - Batch: 700 | Loss: 0.069 | Acc: 98.339%
I - num batch: 701
I - Train -- Loss: 0.069 | Acc: 98.342% | LR: 2.500000e-04 | Dur: 586.26s
I - Confusion Matrix: [row->prediction - col->label]
[[1530.    0.    0.    1.   28.]
 [   0.  717.    9.    0.    2.]
 [   0.   13. 1036.    0.   47.]
 [   4.    3.    0. 1488.   32.]
 [  12.    2.   18.   15. 6259.]]

I - Val -- Loss: 2.268 | Acc: 56.805%
I - Confusion Matrix: [row->prediction - col->label]
[[353.   8.  23. 133.  79.]
 [  4.  59.  19.   7.  13.]
 [  7. 101. 127.  26.  77.]
 [ 54.  28.  28. 220.  52.]
 [117. 174. 195. 188. 994.]]

I - Epoch: 35
	I - Batch: 10 | Loss: 0.062 | Acc: 98.750%
	I - Batch: 20 | Loss: 0.063 | Acc: 98.750%
	I - Batch: 30 | Loss: 0.071 | Acc: 98.542%
	I - Batch: 40 | Loss: 0.065 | Acc: 98.594%
	I - Batch: 50 | Loss: 0.072 | Acc: 98.500%
	I - Batch: 60 | Loss: 0.068 | Acc: 98.542%
	I - Batch: 70 | Loss: 0.065 | Acc: 98.571%
	I - Batch: 80 | Loss: 0.063 | Acc: 98.672%
	I - Batch: 90 | Loss: 0.060 | Acc: 98.750%
	I - Batch: 100 | Loss: 0.059 | Acc: 98.625%
	I - Batch: 110 | Loss: 0.058 | Acc: 98.750%
	I - Batch: 120 | Loss: 0.063 | Acc: 98.542%
	I - Batch: 130 | Loss: 0.062 | Acc: 98.510%
	I - Batch: 140 | Loss: 0.061 | Acc: 98.571%
	I - Batch: 150 | Loss: 0.061 | Acc: 98.583%
	I - Batch: 160 | Loss: 0.059 | Acc: 98.633%
	I - Batch: 170 | Loss: 0.059 | Acc: 98.676%
	I - Batch: 180 | Loss: 0.058 | Acc: 98.715%
	I - Batch: 190 | Loss: 0.057 | Acc: 98.750%
	I - Batch: 200 | Loss: 0.056 | Acc: 98.781%
	I - Batch: 210 | Loss: 0.058 | Acc: 98.780%
	I - Batch: 220 | Loss: 0.059 | Acc: 98.722%
	I - Batch: 230 | Loss: 0.060 | Acc: 98.668%
	I - Batch: 240 | Loss: 0.061 | Acc: 98.646%
	I - Batch: 250 | Loss: 0.060 | Acc: 98.675%
	I - Batch: 260 | Loss: 0.061 | Acc: 98.606%
	I - Batch: 270 | Loss: 0.060 | Acc: 98.657%
	I - Batch: 280 | Loss: 0.061 | Acc: 98.661%
	I - Batch: 290 | Loss: 0.060 | Acc: 98.707%
	I - Batch: 300 | Loss: 0.060 | Acc: 98.708%
	I - Batch: 310 | Loss: 0.061 | Acc: 98.669%
	I - Batch: 320 | Loss: 0.060 | Acc: 98.691%
	I - Batch: 330 | Loss: 0.059 | Acc: 98.731%
	I - Batch: 340 | Loss: 0.059 | Acc: 98.750%
	I - Batch: 350 | Loss: 0.061 | Acc: 98.696%
	I - Batch: 360 | Loss: 0.061 | Acc: 98.681%
	I - Batch: 370 | Loss: 0.060 | Acc: 98.716%
	I - Batch: 380 | Loss: 0.063 | Acc: 98.651%
	I - Batch: 390 | Loss: 0.063 | Acc: 98.654%
	I - Batch: 400 | Loss: 0.064 | Acc: 98.594%
	I - Batch: 410 | Loss: 0.064 | Acc: 98.598%
	I - Batch: 420 | Loss: 0.063 | Acc: 98.586%
	I - Batch: 430 | Loss: 0.063 | Acc: 98.605%
	I - Batch: 440 | Loss: 0.063 | Acc: 98.608%
	I - Batch: 450 | Loss: 0.063 | Acc: 98.611%
	I - Batch: 460 | Loss: 0.063 | Acc: 98.614%
	I - Batch: 470 | Loss: 0.063 | Acc: 98.590%
	I - Batch: 480 | Loss: 0.063 | Acc: 98.568%
	I - Batch: 490 | Loss: 0.062 | Acc: 98.584%
	I - Batch: 500 | Loss: 0.062 | Acc: 98.612%
	I - Batch: 510 | Loss: 0.062 | Acc: 98.615%
	I - Batch: 520 | Loss: 0.062 | Acc: 98.582%
	I - Batch: 530 | Loss: 0.064 | Acc: 98.561%
	I - Batch: 540 | Loss: 0.065 | Acc: 98.519%
	I - Batch: 550 | Loss: 0.066 | Acc: 98.523%
	I - Batch: 560 | Loss: 0.066 | Acc: 98.527%
	I - Batch: 570 | Loss: 0.067 | Acc: 98.465%
	I - Batch: 580 | Loss: 0.067 | Acc: 98.438%
	I - Batch: 590 | Loss: 0.067 | Acc: 98.453%
	I - Batch: 600 | Loss: 0.067 | Acc: 98.448%
	I - Batch: 610 | Loss: 0.067 | Acc: 98.463%
	I - Batch: 620 | Loss: 0.067 | Acc: 98.468%
	I - Batch: 630 | Loss: 0.066 | Acc: 98.472%
	I - Batch: 640 | Loss: 0.067 | Acc: 98.447%
	I - Batch: 650 | Loss: 0.067 | Acc: 98.433%
	I - Batch: 660 | Loss: 0.066 | Acc: 98.456%
	I - Batch: 670 | Loss: 0.066 | Acc: 98.461%
	I - Batch: 680 | Loss: 0.066 | Acc: 98.465%
	I - Batch: 690 | Loss: 0.066 | Acc: 98.460%
	I - Batch: 700 | Loss: 0.066 | Acc: 98.455%
I - num batch: 701
I - Train -- Loss: 0.066 | Acc: 98.458% | LR: 2.500000e-04 | Dur: 558.80s
I - Confusion Matrix: [row->prediction - col->label]
[[1535.    0.    0.    1.   27.]
 [   0.  713.    8.    1.    8.]
 [   0.   16. 1035.    0.   51.]
 [   2.    1.    0. 1497.   19.]
 [   9.    5.   20.    5. 6263.]]

I - Val -- Loss: 2.145 | Acc: 57.064%
I - Confusion Matrix: [row->prediction - col->label]
[[313.   7.  16.  90.  62.]
 [  4.  90.  34.   8.  26.]
 [  6. 108. 136.  24. 110.]
 [ 81.  24.  36. 259.  54.]
 [131. 141. 170. 193. 963.]]

I - Epoch: 36
	I - Batch: 10 | Loss: 0.077 | Acc: 98.125%
	I - Batch: 20 | Loss: 0.072 | Acc: 98.438%
	I - Batch: 30 | Loss: 0.079 | Acc: 98.125%
	I - Batch: 40 | Loss: 0.086 | Acc: 97.969%
	I - Batch: 50 | Loss: 0.076 | Acc: 98.250%
	I - Batch: 60 | Loss: 0.072 | Acc: 98.333%
	I - Batch: 70 | Loss: 0.072 | Acc: 98.304%
	I - Batch: 80 | Loss: 0.072 | Acc: 98.359%
	I - Batch: 90 | Loss: 0.069 | Acc: 98.542%
	I - Batch: 100 | Loss: 0.068 | Acc: 98.500%
	I - Batch: 110 | Loss: 0.072 | Acc: 98.295%
	I - Batch: 120 | Loss: 0.070 | Acc: 98.438%
	I - Batch: 130 | Loss: 0.078 | Acc: 98.125%
	I - Batch: 140 | Loss: 0.075 | Acc: 98.214%
	I - Batch: 150 | Loss: 0.075 | Acc: 98.167%
	I - Batch: 160 | Loss: 0.075 | Acc: 98.164%
	I - Batch: 170 | Loss: 0.074 | Acc: 98.199%
	I - Batch: 180 | Loss: 0.073 | Acc: 98.229%
	I - Batch: 190 | Loss: 0.072 | Acc: 98.289%
	I - Batch: 200 | Loss: 0.070 | Acc: 98.344%
	I - Batch: 210 | Loss: 0.068 | Acc: 98.393%
	I - Batch: 220 | Loss: 0.068 | Acc: 98.381%
	I - Batch: 230 | Loss: 0.067 | Acc: 98.451%
	I - Batch: 240 | Loss: 0.066 | Acc: 98.464%
	I - Batch: 250 | Loss: 0.065 | Acc: 98.525%
	I - Batch: 260 | Loss: 0.067 | Acc: 98.486%
	I - Batch: 270 | Loss: 0.071 | Acc: 98.472%
	I - Batch: 280 | Loss: 0.071 | Acc: 98.482%
	I - Batch: 290 | Loss: 0.071 | Acc: 98.448%
	I - Batch: 300 | Loss: 0.071 | Acc: 98.479%
	I - Batch: 310 | Loss: 0.072 | Acc: 98.387%
	I - Batch: 320 | Loss: 0.073 | Acc: 98.320%
	I - Batch: 330 | Loss: 0.073 | Acc: 98.295%
	I - Batch: 340 | Loss: 0.072 | Acc: 98.327%
	I - Batch: 350 | Loss: 0.072 | Acc: 98.357%
	I - Batch: 360 | Loss: 0.072 | Acc: 98.316%
	I - Batch: 370 | Loss: 0.073 | Acc: 98.311%
	I - Batch: 380 | Loss: 0.074 | Acc: 98.306%
	I - Batch: 390 | Loss: 0.073 | Acc: 98.349%
	I - Batch: 400 | Loss: 0.072 | Acc: 98.375%
	I - Batch: 410 | Loss: 0.071 | Acc: 98.415%
	I - Batch: 420 | Loss: 0.071 | Acc: 98.423%
	I - Batch: 430 | Loss: 0.070 | Acc: 98.459%
	I - Batch: 440 | Loss: 0.070 | Acc: 98.466%
	I - Batch: 450 | Loss: 0.073 | Acc: 98.375%
	I - Batch: 460 | Loss: 0.073 | Acc: 98.383%
	I - Batch: 470 | Loss: 0.072 | Acc: 98.404%
	I - Batch: 480 | Loss: 0.072 | Acc: 98.411%
	I - Batch: 490 | Loss: 0.071 | Acc: 98.406%
	I - Batch: 500 | Loss: 0.071 | Acc: 98.400%
	I - Batch: 510 | Loss: 0.070 | Acc: 98.407%
	I - Batch: 520 | Loss: 0.070 | Acc: 98.401%
	I - Batch: 530 | Loss: 0.070 | Acc: 98.396%
	I - Batch: 540 | Loss: 0.070 | Acc: 98.368%
	I - Batch: 550 | Loss: 0.070 | Acc: 98.398%
	I - Batch: 560 | Loss: 0.070 | Acc: 98.404%
	I - Batch: 570 | Loss: 0.070 | Acc: 98.399%
	I - Batch: 580 | Loss: 0.071 | Acc: 98.394%
	I - Batch: 590 | Loss: 0.071 | Acc: 98.400%
	I - Batch: 600 | Loss: 0.070 | Acc: 98.406%
	I - Batch: 610 | Loss: 0.070 | Acc: 98.412%
	I - Batch: 620 | Loss: 0.070 | Acc: 98.407%
	I - Batch: 630 | Loss: 0.070 | Acc: 98.393%
	I - Batch: 640 | Loss: 0.070 | Acc: 98.389%
	I - Batch: 650 | Loss: 0.070 | Acc: 98.404%
	I - Batch: 660 | Loss: 0.070 | Acc: 98.400%
	I - Batch: 670 | Loss: 0.070 | Acc: 98.396%
	I - Batch: 680 | Loss: 0.070 | Acc: 98.382%
	I - Batch: 690 | Loss: 0.070 | Acc: 98.379%
	I - Batch: 700 | Loss: 0.071 | Acc: 98.348%
I - num batch: 701
I - Train -- Loss: 0.071 | Acc: 98.351% | LR: 2.500000e-04 | Dur: 555.33s
I - Confusion Matrix: [row->prediction - col->label]
[[1528.    0.    0.    2.   37.]
 [   0.  713.    9.    1.    9.]
 [   0.   13. 1042.    0.   29.]
 [   1.    4.    0. 1487.   32.]
 [  17.    5.   12.   14. 6261.]]

I - Val -- Loss: 2.266 | Acc: 57.356%
I - Confusion Matrix: [row->prediction - col->label]
[[ 319.    3.   20.  105.   46.]
 [   3.   76.   25.    9.   18.]
 [   7.  113.  131.   27.   87.]
 [  61.   17.   21.  227.   47.]
 [ 145.  161.  195.  206. 1017.]]

I - Epoch: 37
	I - Batch: 10 | Loss: 0.099 | Acc: 98.125%
	I - Batch: 20 | Loss: 0.072 | Acc: 98.438%
	I - Batch: 30 | Loss: 0.062 | Acc: 98.542%
	I - Batch: 40 | Loss: 0.066 | Acc: 98.281%
	I - Batch: 50 | Loss: 0.058 | Acc: 98.625%
	I - Batch: 60 | Loss: 0.055 | Acc: 98.646%
	I - Batch: 70 | Loss: 0.055 | Acc: 98.750%
	I - Batch: 80 | Loss: 0.053 | Acc: 98.828%
	I - Batch: 90 | Loss: 0.053 | Acc: 98.819%
	I - Batch: 100 | Loss: 0.054 | Acc: 98.812%
	I - Batch: 110 | Loss: 0.054 | Acc: 98.693%
	I - Batch: 120 | Loss: 0.053 | Acc: 98.750%
	I - Batch: 130 | Loss: 0.051 | Acc: 98.846%
	I - Batch: 140 | Loss: 0.050 | Acc: 98.929%
	I - Batch: 150 | Loss: 0.051 | Acc: 98.875%
	I - Batch: 160 | Loss: 0.050 | Acc: 98.906%
	I - Batch: 170 | Loss: 0.050 | Acc: 98.971%
	I - Batch: 180 | Loss: 0.049 | Acc: 98.993%
	I - Batch: 190 | Loss: 0.051 | Acc: 99.013%
	I - Batch: 200 | Loss: 0.054 | Acc: 98.938%
	I - Batch: 210 | Loss: 0.053 | Acc: 98.988%
	I - Batch: 220 | Loss: 0.052 | Acc: 99.006%
	I - Batch: 230 | Loss: 0.052 | Acc: 99.022%
	I - Batch: 240 | Loss: 0.051 | Acc: 99.062%
	I - Batch: 250 | Loss: 0.050 | Acc: 99.100%
	I - Batch: 260 | Loss: 0.051 | Acc: 99.062%
	I - Batch: 270 | Loss: 0.052 | Acc: 99.074%
	I - Batch: 280 | Loss: 0.052 | Acc: 99.062%
	I - Batch: 290 | Loss: 0.051 | Acc: 99.052%
	I - Batch: 300 | Loss: 0.052 | Acc: 99.021%
	I - Batch: 310 | Loss: 0.051 | Acc: 99.032%
	I - Batch: 320 | Loss: 0.051 | Acc: 99.023%
	I - Batch: 330 | Loss: 0.052 | Acc: 98.977%
	I - Batch: 340 | Loss: 0.052 | Acc: 98.971%
	I - Batch: 350 | Loss: 0.052 | Acc: 98.946%
	I - Batch: 360 | Loss: 0.053 | Acc: 98.924%
	I - Batch: 370 | Loss: 0.053 | Acc: 98.919%
	I - Batch: 380 | Loss: 0.054 | Acc: 98.882%
	I - Batch: 390 | Loss: 0.056 | Acc: 98.798%
	I - Batch: 400 | Loss: 0.055 | Acc: 98.828%
	I - Batch: 410 | Loss: 0.056 | Acc: 98.811%
	I - Batch: 420 | Loss: 0.056 | Acc: 98.780%
	I - Batch: 430 | Loss: 0.056 | Acc: 98.794%
	I - Batch: 440 | Loss: 0.056 | Acc: 98.778%
	I - Batch: 450 | Loss: 0.056 | Acc: 98.778%
	I - Batch: 460 | Loss: 0.055 | Acc: 98.804%
	I - Batch: 470 | Loss: 0.055 | Acc: 98.763%
	I - Batch: 480 | Loss: 0.055 | Acc: 98.737%
	I - Batch: 490 | Loss: 0.055 | Acc: 98.750%
	I - Batch: 500 | Loss: 0.055 | Acc: 98.763%
	I - Batch: 510 | Loss: 0.055 | Acc: 98.775%
	I - Batch: 520 | Loss: 0.054 | Acc: 98.786%
	I - Batch: 530 | Loss: 0.055 | Acc: 98.774%
	I - Batch: 540 | Loss: 0.056 | Acc: 98.727%
	I - Batch: 550 | Loss: 0.057 | Acc: 98.727%
	I - Batch: 560 | Loss: 0.057 | Acc: 98.739%
	I - Batch: 570 | Loss: 0.056 | Acc: 98.739%
	I - Batch: 580 | Loss: 0.057 | Acc: 98.750%
	I - Batch: 590 | Loss: 0.057 | Acc: 98.739%
	I - Batch: 600 | Loss: 0.058 | Acc: 98.708%
	I - Batch: 610 | Loss: 0.058 | Acc: 98.719%
	I - Batch: 620 | Loss: 0.057 | Acc: 98.730%
	I - Batch: 630 | Loss: 0.057 | Acc: 98.730%
	I - Batch: 640 | Loss: 0.058 | Acc: 98.721%
	I - Batch: 650 | Loss: 0.057 | Acc: 98.721%
	I - Batch: 660 | Loss: 0.057 | Acc: 98.712%
	I - Batch: 670 | Loss: 0.057 | Acc: 98.722%
	I - Batch: 680 | Loss: 0.058 | Acc: 98.667%
	I - Batch: 690 | Loss: 0.059 | Acc: 98.668%
	I - Batch: 700 | Loss: 0.059 | Acc: 98.652%
I - num batch: 701
I - Train -- Loss: 0.059 | Acc: 98.654% | LR: 2.500000e-04 | Dur: 565.37s
I - Confusion Matrix: [row->prediction - col->label]
[[1534.    0.    0.    1.   35.]
 [   0.  716.    8.    0.    3.]
 [   1.   15. 1043.    0.   36.]
 [   0.    2.    0. 1497.   19.]
 [  11.    2.   12.    6. 6275.]]

I - Val -- Loss: 2.246 | Acc: 57.809%
I - Confusion Matrix: [row->prediction - col->label]
[[ 333.    7.   16.  117.   61.]
 [   2.   59.   20.    6.   19.]
 [   7.  104.  137.   14.   66.]
 [  68.   22.   33.  234.   48.]
 [ 125.  178.  186.  203. 1021.]]

I - Epoch: 38
	I - Batch: 10 | Loss: 0.070 | Acc: 98.125%
	I - Batch: 20 | Loss: 0.062 | Acc: 98.438%
	I - Batch: 30 | Loss: 0.072 | Acc: 98.333%
	I - Batch: 40 | Loss: 0.104 | Acc: 97.188%
	I - Batch: 50 | Loss: 0.096 | Acc: 97.375%
	I - Batch: 60 | Loss: 0.092 | Acc: 97.604%
	I - Batch: 70 | Loss: 0.086 | Acc: 97.679%
	I - Batch: 80 | Loss: 0.081 | Acc: 97.891%
	I - Batch: 90 | Loss: 0.078 | Acc: 97.986%
	I - Batch: 100 | Loss: 0.074 | Acc: 98.125%
	I - Batch: 110 | Loss: 0.074 | Acc: 98.125%
	I - Batch: 120 | Loss: 0.073 | Acc: 98.125%
	I - Batch: 130 | Loss: 0.070 | Acc: 98.269%
	I - Batch: 140 | Loss: 0.067 | Acc: 98.304%
	I - Batch: 150 | Loss: 0.069 | Acc: 98.250%
	I - Batch: 160 | Loss: 0.068 | Acc: 98.281%
	I - Batch: 170 | Loss: 0.066 | Acc: 98.382%
	I - Batch: 180 | Loss: 0.069 | Acc: 98.299%
	I - Batch: 190 | Loss: 0.067 | Acc: 98.322%
	I - Batch: 200 | Loss: 0.066 | Acc: 98.344%
	I - Batch: 210 | Loss: 0.065 | Acc: 98.363%
	I - Batch: 220 | Loss: 0.066 | Acc: 98.381%
	I - Batch: 230 | Loss: 0.066 | Acc: 98.288%
	I - Batch: 240 | Loss: 0.065 | Acc: 98.359%
	I - Batch: 250 | Loss: 0.065 | Acc: 98.375%
	I - Batch: 260 | Loss: 0.064 | Acc: 98.413%
	I - Batch: 270 | Loss: 0.063 | Acc: 98.472%
	I - Batch: 280 | Loss: 0.065 | Acc: 98.393%
	I - Batch: 290 | Loss: 0.065 | Acc: 98.362%
	I - Batch: 300 | Loss: 0.065 | Acc: 98.375%
	I - Batch: 310 | Loss: 0.065 | Acc: 98.367%
	I - Batch: 320 | Loss: 0.066 | Acc: 98.320%
	I - Batch: 330 | Loss: 0.066 | Acc: 98.333%
	I - Batch: 340 | Loss: 0.066 | Acc: 98.364%
	I - Batch: 350 | Loss: 0.066 | Acc: 98.357%
	I - Batch: 360 | Loss: 0.066 | Acc: 98.385%
	I - Batch: 370 | Loss: 0.066 | Acc: 98.345%
	I - Batch: 380 | Loss: 0.065 | Acc: 98.388%
	I - Batch: 390 | Loss: 0.066 | Acc: 98.381%
	I - Batch: 400 | Loss: 0.066 | Acc: 98.375%
	I - Batch: 410 | Loss: 0.065 | Acc: 98.399%
	I - Batch: 420 | Loss: 0.065 | Acc: 98.408%
	I - Batch: 430 | Loss: 0.065 | Acc: 98.416%
	I - Batch: 440 | Loss: 0.064 | Acc: 98.409%
	I - Batch: 450 | Loss: 0.063 | Acc: 98.444%
	I - Batch: 460 | Loss: 0.064 | Acc: 98.465%
	I - Batch: 470 | Loss: 0.063 | Acc: 98.444%
	I - Batch: 480 | Loss: 0.064 | Acc: 98.438%
	I - Batch: 490 | Loss: 0.063 | Acc: 98.444%
	I - Batch: 500 | Loss: 0.063 | Acc: 98.463%
	I - Batch: 510 | Loss: 0.064 | Acc: 98.444%
	I - Batch: 520 | Loss: 0.064 | Acc: 98.450%
	I - Batch: 530 | Loss: 0.064 | Acc: 98.443%
	I - Batch: 540 | Loss: 0.064 | Acc: 98.449%
	I - Batch: 550 | Loss: 0.064 | Acc: 98.443%
	I - Batch: 560 | Loss: 0.064 | Acc: 98.426%
	I - Batch: 570 | Loss: 0.065 | Acc: 98.421%
	I - Batch: 580 | Loss: 0.064 | Acc: 98.438%
	I - Batch: 590 | Loss: 0.064 | Acc: 98.432%
	I - Batch: 600 | Loss: 0.064 | Acc: 98.438%
	I - Batch: 610 | Loss: 0.064 | Acc: 98.422%
	I - Batch: 620 | Loss: 0.064 | Acc: 98.427%
	I - Batch: 630 | Loss: 0.064 | Acc: 98.442%
	I - Batch: 640 | Loss: 0.063 | Acc: 98.447%
	I - Batch: 650 | Loss: 0.063 | Acc: 98.471%
	I - Batch: 660 | Loss: 0.064 | Acc: 98.428%
	I - Batch: 670 | Loss: 0.063 | Acc: 98.451%
	I - Batch: 680 | Loss: 0.064 | Acc: 98.428%
	I - Batch: 690 | Loss: 0.063 | Acc: 98.433%
	I - Batch: 700 | Loss: 0.064 | Acc: 98.411%
I - num batch: 701
I - Train -- Loss: 0.064 | Acc: 98.413% | LR: 2.500000e-04 | Dur: 556.49s
I - Confusion Matrix: [row->prediction - col->label]
[[1526.    0.    0.    1.   30.]
 [   0.  717.    9.    2.    4.]
 [   0.   14. 1039.    0.   48.]
 [   2.    2.    0. 1490.   20.]
 [  18.    2.   15.   11. 6266.]]

I - Val -- Loss: 2.447 | Acc: 56.902%
I - Confusion Matrix: [row->prediction - col->label]
[[ 316.    7.   13.  113.   45.]
 [   2.   50.   15.    6.   15.]
 [   5.  100.  110.   17.   45.]
 [  54.   22.   29.  213.   43.]
 [ 158.  191.  225.  225. 1067.]]

I - Epoch: 39
	I - Batch: 10 | Loss: 0.087 | Acc: 96.250%
	I - Batch: 20 | Loss: 0.058 | Acc: 97.812%
	I - Batch: 30 | Loss: 0.052 | Acc: 98.333%
	I - Batch: 40 | Loss: 0.049 | Acc: 98.281%
	I - Batch: 50 | Loss: 0.045 | Acc: 98.625%
	I - Batch: 60 | Loss: 0.045 | Acc: 98.750%
	I - Batch: 70 | Loss: 0.045 | Acc: 98.839%
	I - Batch: 80 | Loss: 0.044 | Acc: 98.906%
	I - Batch: 90 | Loss: 0.046 | Acc: 98.819%
	I - Batch: 100 | Loss: 0.048 | Acc: 98.875%
	I - Batch: 110 | Loss: 0.050 | Acc: 98.750%
	I - Batch: 120 | Loss: 0.053 | Acc: 98.594%
	I - Batch: 130 | Loss: 0.053 | Acc: 98.606%
	I - Batch: 140 | Loss: 0.058 | Acc: 98.571%
	I - Batch: 150 | Loss: 0.057 | Acc: 98.583%
	I - Batch: 160 | Loss: 0.058 | Acc: 98.594%
	I - Batch: 170 | Loss: 0.058 | Acc: 98.603%
	I - Batch: 180 | Loss: 0.060 | Acc: 98.542%
	I - Batch: 190 | Loss: 0.062 | Acc: 98.487%
	I - Batch: 200 | Loss: 0.062 | Acc: 98.500%
	I - Batch: 210 | Loss: 0.060 | Acc: 98.542%
	I - Batch: 220 | Loss: 0.060 | Acc: 98.551%
	I - Batch: 230 | Loss: 0.060 | Acc: 98.533%
	I - Batch: 240 | Loss: 0.059 | Acc: 98.542%
	I - Batch: 250 | Loss: 0.059 | Acc: 98.525%
	I - Batch: 260 | Loss: 0.059 | Acc: 98.486%
	I - Batch: 270 | Loss: 0.060 | Acc: 98.495%
	I - Batch: 280 | Loss: 0.059 | Acc: 98.527%
	I - Batch: 290 | Loss: 0.058 | Acc: 98.578%
	I - Batch: 300 | Loss: 0.058 | Acc: 98.542%
	I - Batch: 310 | Loss: 0.058 | Acc: 98.548%
	I - Batch: 320 | Loss: 0.059 | Acc: 98.555%
	I - Batch: 330 | Loss: 0.058 | Acc: 98.561%
	I - Batch: 340 | Loss: 0.058 | Acc: 98.585%
	I - Batch: 350 | Loss: 0.059 | Acc: 98.589%
	I - Batch: 360 | Loss: 0.059 | Acc: 98.576%
	I - Batch: 370 | Loss: 0.058 | Acc: 98.598%
	I - Batch: 380 | Loss: 0.058 | Acc: 98.618%
	I - Batch: 390 | Loss: 0.060 | Acc: 98.558%
	I - Batch: 400 | Loss: 0.060 | Acc: 98.562%
	I - Batch: 410 | Loss: 0.059 | Acc: 98.582%
	I - Batch: 420 | Loss: 0.059 | Acc: 98.586%
	I - Batch: 430 | Loss: 0.058 | Acc: 98.590%
	I - Batch: 440 | Loss: 0.058 | Acc: 98.608%
	I - Batch: 450 | Loss: 0.057 | Acc: 98.611%
	I - Batch: 460 | Loss: 0.058 | Acc: 98.601%
	I - Batch: 470 | Loss: 0.058 | Acc: 98.564%
	I - Batch: 480 | Loss: 0.059 | Acc: 98.568%
	I - Batch: 490 | Loss: 0.058 | Acc: 98.584%
	I - Batch: 500 | Loss: 0.058 | Acc: 98.612%
	I - Batch: 510 | Loss: 0.057 | Acc: 98.627%
	I - Batch: 520 | Loss: 0.058 | Acc: 98.618%
	I - Batch: 530 | Loss: 0.057 | Acc: 98.632%
	I - Batch: 540 | Loss: 0.057 | Acc: 98.623%
	I - Batch: 550 | Loss: 0.058 | Acc: 98.614%
	I - Batch: 560 | Loss: 0.058 | Acc: 98.605%
	I - Batch: 570 | Loss: 0.060 | Acc: 98.564%
	I - Batch: 580 | Loss: 0.059 | Acc: 98.578%
	I - Batch: 590 | Loss: 0.059 | Acc: 98.570%
	I - Batch: 600 | Loss: 0.059 | Acc: 98.583%
	I - Batch: 610 | Loss: 0.059 | Acc: 98.596%
	I - Batch: 620 | Loss: 0.059 | Acc: 98.619%
	I - Batch: 630 | Loss: 0.058 | Acc: 98.621%
	I - Batch: 640 | Loss: 0.058 | Acc: 98.604%
	I - Batch: 650 | Loss: 0.058 | Acc: 98.596%
	I - Batch: 660 | Loss: 0.058 | Acc: 98.608%
	I - Batch: 670 | Loss: 0.058 | Acc: 98.601%
	I - Batch: 680 | Loss: 0.058 | Acc: 98.612%
	I - Batch: 690 | Loss: 0.058 | Acc: 98.623%
	I - Batch: 700 | Loss: 0.058 | Acc: 98.616%
I - num batch: 701
I - Train -- Loss: 0.058 | Acc: 98.618% | LR: 2.500000e-04 | Dur: 572.14s
I - Confusion Matrix: [row->prediction - col->label]
[[1532.    0.    0.    1.   22.]
 [   0.  715.    7.    1.    4.]
 [   0.   17. 1038.    0.   36.]
 [   2.    1.    0. 1495.   25.]
 [  12.    2.   18.    7. 6281.]]

I - Val -- Loss: 2.207 | Acc: 56.546%
I - Confusion Matrix: [row->prediction - col->label]
[[353.   9.  22. 126.  99.]
 [  3.  91.  35.  15.  26.]
 [ 11. 106. 125.  30.  93.]
 [ 72.  23.  33. 248.  69.]
 [ 96. 141. 177. 155. 928.]]

I - Epoch: 40
	I - Batch: 10 | Loss: 0.069 | Acc: 98.750%
	I - Batch: 20 | Loss: 0.045 | Acc: 99.375%
	I - Batch: 30 | Loss: 0.045 | Acc: 99.167%
	I - Batch: 40 | Loss: 0.042 | Acc: 99.062%
	I - Batch: 50 | Loss: 0.043 | Acc: 99.125%
	I - Batch: 60 | Loss: 0.042 | Acc: 99.167%
	I - Batch: 70 | Loss: 0.040 | Acc: 99.286%
	I - Batch: 80 | Loss: 0.040 | Acc: 99.297%
	I - Batch: 90 | Loss: 0.043 | Acc: 99.167%
	I - Batch: 100 | Loss: 0.041 | Acc: 99.188%
	I - Batch: 110 | Loss: 0.053 | Acc: 98.920%
	I - Batch: 120 | Loss: 0.051 | Acc: 98.958%
	I - Batch: 130 | Loss: 0.050 | Acc: 98.942%
	I - Batch: 140 | Loss: 0.049 | Acc: 98.973%
	I - Batch: 150 | Loss: 0.049 | Acc: 98.958%
	I - Batch: 160 | Loss: 0.048 | Acc: 98.984%
	I - Batch: 170 | Loss: 0.047 | Acc: 99.044%
	I - Batch: 180 | Loss: 0.046 | Acc: 99.028%
	I - Batch: 190 | Loss: 0.045 | Acc: 99.079%
	I - Batch: 200 | Loss: 0.045 | Acc: 99.062%
	I - Batch: 210 | Loss: 0.044 | Acc: 99.077%
	I - Batch: 220 | Loss: 0.044 | Acc: 99.091%
	I - Batch: 230 | Loss: 0.043 | Acc: 99.130%
	I - Batch: 240 | Loss: 0.042 | Acc: 99.167%
	I - Batch: 250 | Loss: 0.041 | Acc: 99.200%
	I - Batch: 260 | Loss: 0.041 | Acc: 99.183%
	I - Batch: 270 | Loss: 0.042 | Acc: 99.167%
	I - Batch: 280 | Loss: 0.041 | Acc: 99.196%
	I - Batch: 290 | Loss: 0.042 | Acc: 99.181%
	I - Batch: 300 | Loss: 0.042 | Acc: 99.146%
	I - Batch: 310 | Loss: 0.042 | Acc: 99.173%
	I - Batch: 320 | Loss: 0.042 | Acc: 99.141%
	I - Batch: 330 | Loss: 0.041 | Acc: 99.167%
	I - Batch: 340 | Loss: 0.041 | Acc: 99.173%
	I - Batch: 350 | Loss: 0.042 | Acc: 99.161%
	I - Batch: 360 | Loss: 0.042 | Acc: 99.184%
	I - Batch: 370 | Loss: 0.041 | Acc: 99.189%
	I - Batch: 380 | Loss: 0.041 | Acc: 99.194%
	I - Batch: 390 | Loss: 0.042 | Acc: 99.167%
	I - Batch: 400 | Loss: 0.042 | Acc: 99.188%
	I - Batch: 410 | Loss: 0.042 | Acc: 99.177%
	I - Batch: 420 | Loss: 0.042 | Acc: 99.182%
	I - Batch: 430 | Loss: 0.041 | Acc: 99.201%
	I - Batch: 440 | Loss: 0.041 | Acc: 99.205%
	I - Batch: 450 | Loss: 0.041 | Acc: 99.208%
	I - Batch: 460 | Loss: 0.041 | Acc: 99.212%
	I - Batch: 470 | Loss: 0.041 | Acc: 99.229%
	I - Batch: 480 | Loss: 0.040 | Acc: 99.232%
	I - Batch: 490 | Loss: 0.040 | Acc: 99.247%
	I - Batch: 500 | Loss: 0.040 | Acc: 99.263%
	I - Batch: 510 | Loss: 0.040 | Acc: 99.265%
	I - Batch: 520 | Loss: 0.039 | Acc: 99.279%
	I - Batch: 530 | Loss: 0.039 | Acc: 99.269%
	I - Batch: 540 | Loss: 0.039 | Acc: 99.282%
	I - Batch: 550 | Loss: 0.039 | Acc: 99.295%
	I - Batch: 560 | Loss: 0.039 | Acc: 99.308%
	I - Batch: 570 | Loss: 0.039 | Acc: 99.298%
	I - Batch: 580 | Loss: 0.039 | Acc: 99.300%
	I - Batch: 590 | Loss: 0.039 | Acc: 99.301%
	I - Batch: 600 | Loss: 0.039 | Acc: 99.271%
	I - Batch: 610 | Loss: 0.040 | Acc: 99.262%
	I - Batch: 620 | Loss: 0.039 | Acc: 99.274%
	I - Batch: 630 | Loss: 0.039 | Acc: 99.286%
	I - Batch: 640 | Loss: 0.039 | Acc: 99.297%
	I - Batch: 650 | Loss: 0.039 | Acc: 99.288%
	I - Batch: 660 | Loss: 0.039 | Acc: 99.280%
	I - Batch: 670 | Loss: 0.039 | Acc: 99.282%
	I - Batch: 680 | Loss: 0.039 | Acc: 99.283%
	I - Batch: 690 | Loss: 0.039 | Acc: 99.284%
	I - Batch: 700 | Loss: 0.039 | Acc: 99.277%
I - num batch: 701
I - Train -- Loss: 0.039 | Acc: 99.278% | LR: 1.250000e-04 | Dur: 578.37s
I - Confusion Matrix: [row->prediction - col->label]
[[1539.    0.    0.    0.   14.]
 [   0.  722.    3.    0.    1.]
 [   0.    8. 1050.    0.   18.]
 [   1.    4.    0. 1500.   11.]
 [   6.    1.   10.    4. 6324.]]

I - Val -- Loss: 2.270 | Acc: 57.583%
I - Confusion Matrix: [row->prediction - col->label]
[[329.   4.  23. 118.  67.]
 [  2.  84.  27.  10.  24.]
 [ 11.  97. 128.  21.  70.]
 [ 70.  21.  28. 239.  57.]
 [123. 164. 186. 186. 997.]]

I - Epoch: 41
	I - Batch: 10 | Loss: 0.021 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.046 | Acc: 99.375%
	I - Batch: 30 | Loss: 0.046 | Acc: 99.375%
	I - Batch: 40 | Loss: 0.042 | Acc: 99.531%
	I - Batch: 50 | Loss: 0.040 | Acc: 99.500%
	I - Batch: 60 | Loss: 0.037 | Acc: 99.583%
	I - Batch: 70 | Loss: 0.035 | Acc: 99.643%
	I - Batch: 80 | Loss: 0.034 | Acc: 99.609%
	I - Batch: 90 | Loss: 0.038 | Acc: 99.514%
	I - Batch: 100 | Loss: 0.042 | Acc: 99.375%
	I - Batch: 110 | Loss: 0.040 | Acc: 99.432%
	I - Batch: 120 | Loss: 0.045 | Acc: 99.375%
	I - Batch: 130 | Loss: 0.045 | Acc: 99.279%
	I - Batch: 140 | Loss: 0.043 | Acc: 99.330%
	I - Batch: 150 | Loss: 0.042 | Acc: 99.375%
	I - Batch: 160 | Loss: 0.042 | Acc: 99.336%
	I - Batch: 170 | Loss: 0.041 | Acc: 99.338%
	I - Batch: 180 | Loss: 0.041 | Acc: 99.271%
	I - Batch: 190 | Loss: 0.044 | Acc: 99.178%
	I - Batch: 200 | Loss: 0.044 | Acc: 99.188%
	I - Batch: 210 | Loss: 0.044 | Acc: 99.196%
	I - Batch: 220 | Loss: 0.043 | Acc: 99.205%
	I - Batch: 230 | Loss: 0.043 | Acc: 99.212%
	I - Batch: 240 | Loss: 0.044 | Acc: 99.141%
	I - Batch: 250 | Loss: 0.043 | Acc: 99.125%
	I - Batch: 260 | Loss: 0.043 | Acc: 99.087%
	I - Batch: 270 | Loss: 0.043 | Acc: 99.097%
	I - Batch: 280 | Loss: 0.042 | Acc: 99.129%
	I - Batch: 290 | Loss: 0.045 | Acc: 99.095%
	I - Batch: 300 | Loss: 0.044 | Acc: 99.125%
	I - Batch: 310 | Loss: 0.044 | Acc: 99.153%
	I - Batch: 320 | Loss: 0.043 | Acc: 99.180%
	I - Batch: 330 | Loss: 0.042 | Acc: 99.205%
	I - Batch: 340 | Loss: 0.042 | Acc: 99.228%
	I - Batch: 350 | Loss: 0.041 | Acc: 99.250%
	I - Batch: 360 | Loss: 0.041 | Acc: 99.253%
	I - Batch: 370 | Loss: 0.041 | Acc: 99.274%
	I - Batch: 380 | Loss: 0.041 | Acc: 99.260%
	I - Batch: 390 | Loss: 0.040 | Acc: 99.279%
	I - Batch: 400 | Loss: 0.040 | Acc: 99.297%
	I - Batch: 410 | Loss: 0.039 | Acc: 99.314%
	I - Batch: 420 | Loss: 0.039 | Acc: 99.315%
	I - Batch: 430 | Loss: 0.039 | Acc: 99.317%
	I - Batch: 440 | Loss: 0.039 | Acc: 99.332%
	I - Batch: 450 | Loss: 0.041 | Acc: 99.292%
	I - Batch: 460 | Loss: 0.041 | Acc: 99.293%
	I - Batch: 470 | Loss: 0.040 | Acc: 99.309%
	I - Batch: 480 | Loss: 0.040 | Acc: 99.310%
	I - Batch: 490 | Loss: 0.041 | Acc: 99.298%
	I - Batch: 500 | Loss: 0.041 | Acc: 99.300%
	I - Batch: 510 | Loss: 0.042 | Acc: 99.277%
	I - Batch: 520 | Loss: 0.043 | Acc: 99.267%
	I - Batch: 530 | Loss: 0.043 | Acc: 99.269%
	I - Batch: 540 | Loss: 0.042 | Acc: 99.271%
	I - Batch: 550 | Loss: 0.042 | Acc: 99.273%
	I - Batch: 560 | Loss: 0.042 | Acc: 99.275%
	I - Batch: 570 | Loss: 0.042 | Acc: 99.287%
	I - Batch: 580 | Loss: 0.042 | Acc: 99.289%
	I - Batch: 590 | Loss: 0.041 | Acc: 99.290%
	I - Batch: 600 | Loss: 0.041 | Acc: 99.302%
	I - Batch: 610 | Loss: 0.042 | Acc: 99.293%
	I - Batch: 620 | Loss: 0.042 | Acc: 99.284%
	I - Batch: 630 | Loss: 0.042 | Acc: 99.286%
	I - Batch: 640 | Loss: 0.042 | Acc: 99.287%
	I - Batch: 650 | Loss: 0.042 | Acc: 99.288%
	I - Batch: 660 | Loss: 0.042 | Acc: 99.299%
	I - Batch: 670 | Loss: 0.041 | Acc: 99.310%
	I - Batch: 680 | Loss: 0.041 | Acc: 99.320%
	I - Batch: 690 | Loss: 0.041 | Acc: 99.321%
	I - Batch: 700 | Loss: 0.041 | Acc: 99.312%
I - num batch: 701
I - Train -- Loss: 0.041 | Acc: 99.313% | LR: 1.250000e-04 | Dur: 569.67s
I - Confusion Matrix: [row->prediction - col->label]
[[1542.    0.    0.    0.   15.]
 [   0.  723.    3.    0.    2.]
 [   0.   10. 1048.    0.   19.]
 [   1.    0.    0. 1503.    9.]
 [   3.    2.   12.    1. 6323.]]

I - Val -- Loss: 2.270 | Acc: 57.647%
I - Confusion Matrix: [row->prediction - col->label]
[[311.   7.  18.  89.  54.]
 [  2.  79.  29.   6.  18.]
 [  8.  99. 111.  17.  59.]
 [ 86.  29.  39. 282.  88.]
 [128. 156. 195. 180. 996.]]

I - Epoch: 42
	I - Batch: 10 | Loss: 0.022 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.029 | Acc: 99.375%
	I - Batch: 30 | Loss: 0.047 | Acc: 99.167%
	I - Batch: 40 | Loss: 0.042 | Acc: 99.375%
	I - Batch: 50 | Loss: 0.039 | Acc: 99.375%
	I - Batch: 60 | Loss: 0.037 | Acc: 99.479%
	I - Batch: 70 | Loss: 0.046 | Acc: 99.375%
	I - Batch: 80 | Loss: 0.043 | Acc: 99.453%
	I - Batch: 90 | Loss: 0.041 | Acc: 99.514%
	I - Batch: 100 | Loss: 0.039 | Acc: 99.562%
	I - Batch: 110 | Loss: 0.037 | Acc: 99.602%
	I - Batch: 120 | Loss: 0.036 | Acc: 99.635%
	I - Batch: 130 | Loss: 0.039 | Acc: 99.519%
	I - Batch: 140 | Loss: 0.040 | Acc: 99.509%
	I - Batch: 150 | Loss: 0.039 | Acc: 99.542%
	I - Batch: 160 | Loss: 0.038 | Acc: 99.570%
	I - Batch: 170 | Loss: 0.037 | Acc: 99.596%
	I - Batch: 180 | Loss: 0.037 | Acc: 99.618%
	I - Batch: 190 | Loss: 0.036 | Acc: 99.605%
	I - Batch: 200 | Loss: 0.038 | Acc: 99.562%
	I - Batch: 210 | Loss: 0.037 | Acc: 99.583%
	I - Batch: 220 | Loss: 0.037 | Acc: 99.602%
	I - Batch: 230 | Loss: 0.037 | Acc: 99.592%
	I - Batch: 240 | Loss: 0.037 | Acc: 99.557%
	I - Batch: 250 | Loss: 0.037 | Acc: 99.550%
	I - Batch: 260 | Loss: 0.036 | Acc: 99.567%
	I - Batch: 270 | Loss: 0.036 | Acc: 99.583%
	I - Batch: 280 | Loss: 0.036 | Acc: 99.576%
	I - Batch: 290 | Loss: 0.035 | Acc: 99.569%
	I - Batch: 300 | Loss: 0.035 | Acc: 99.542%
	I - Batch: 310 | Loss: 0.037 | Acc: 99.456%
	I - Batch: 320 | Loss: 0.037 | Acc: 99.473%
	I - Batch: 330 | Loss: 0.037 | Acc: 99.470%
	I - Batch: 340 | Loss: 0.037 | Acc: 99.485%
	I - Batch: 350 | Loss: 0.037 | Acc: 99.500%
	I - Batch: 360 | Loss: 0.036 | Acc: 99.497%
	I - Batch: 370 | Loss: 0.036 | Acc: 99.510%
	I - Batch: 380 | Loss: 0.037 | Acc: 99.490%
	I - Batch: 390 | Loss: 0.037 | Acc: 99.487%
	I - Batch: 400 | Loss: 0.037 | Acc: 99.500%
	I - Batch: 410 | Loss: 0.038 | Acc: 99.421%
	I - Batch: 420 | Loss: 0.038 | Acc: 99.420%
	I - Batch: 430 | Loss: 0.038 | Acc: 99.404%
	I - Batch: 440 | Loss: 0.038 | Acc: 99.403%
	I - Batch: 450 | Loss: 0.038 | Acc: 99.403%
	I - Batch: 460 | Loss: 0.038 | Acc: 99.389%
	I - Batch: 470 | Loss: 0.038 | Acc: 99.388%
	I - Batch: 480 | Loss: 0.038 | Acc: 99.388%
	I - Batch: 490 | Loss: 0.039 | Acc: 99.388%
	I - Batch: 500 | Loss: 0.039 | Acc: 99.388%
	I - Batch: 510 | Loss: 0.038 | Acc: 99.400%
	I - Batch: 520 | Loss: 0.038 | Acc: 99.411%
	I - Batch: 530 | Loss: 0.038 | Acc: 99.422%
	I - Batch: 540 | Loss: 0.037 | Acc: 99.421%
	I - Batch: 550 | Loss: 0.037 | Acc: 99.432%
	I - Batch: 560 | Loss: 0.037 | Acc: 99.431%
	I - Batch: 570 | Loss: 0.037 | Acc: 99.441%
	I - Batch: 580 | Loss: 0.037 | Acc: 99.440%
	I - Batch: 590 | Loss: 0.036 | Acc: 99.449%
	I - Batch: 600 | Loss: 0.036 | Acc: 99.458%
	I - Batch: 610 | Loss: 0.036 | Acc: 99.467%
	I - Batch: 620 | Loss: 0.036 | Acc: 99.466%
	I - Batch: 630 | Loss: 0.036 | Acc: 99.464%
	I - Batch: 640 | Loss: 0.036 | Acc: 99.453%
	I - Batch: 650 | Loss: 0.037 | Acc: 99.423%
	I - Batch: 660 | Loss: 0.036 | Acc: 99.422%
	I - Batch: 670 | Loss: 0.036 | Acc: 99.431%
	I - Batch: 680 | Loss: 0.036 | Acc: 99.439%
	I - Batch: 690 | Loss: 0.036 | Acc: 99.438%
	I - Batch: 700 | Loss: 0.036 | Acc: 99.446%
I - num batch: 701
I - Train -- Loss: 0.036 | Acc: 99.447% | LR: 1.250000e-04 | Dur: 556.67s
I - Confusion Matrix: [row->prediction - col->label]
[[1542.    0.    0.    0.    8.]
 [   0.  722.    2.    0.    0.]
 [   0.   10. 1050.    0.   12.]
 [   0.    1.    0. 1502.   10.]
 [   4.    2.   11.    2. 6338.]]

I - Val -- Loss: 2.363 | Acc: 57.226%
I - Confusion Matrix: [row->prediction - col->label]
[[ 333.    6.   24.  113.   64.]
 [   3.   80.   27.    9.   20.]
 [   6.   91.  112.   17.   67.]
 [  65.   22.   22.  226.   49.]
 [ 128.  171.  207.  209. 1015.]]

I - Epoch: 43
	I - Batch: 10 | Loss: 0.019 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.020 | Acc: 100.000%
	I - Batch: 30 | Loss: 0.021 | Acc: 100.000%
	I - Batch: 40 | Loss: 0.021 | Acc: 100.000%
	I - Batch: 50 | Loss: 0.021 | Acc: 100.000%
	I - Batch: 60 | Loss: 0.021 | Acc: 100.000%
	I - Batch: 70 | Loss: 0.024 | Acc: 99.821%
	I - Batch: 80 | Loss: 0.024 | Acc: 99.844%
	I - Batch: 90 | Loss: 0.026 | Acc: 99.722%
	I - Batch: 100 | Loss: 0.025 | Acc: 99.750%
	I - Batch: 110 | Loss: 0.025 | Acc: 99.773%
	I - Batch: 120 | Loss: 0.024 | Acc: 99.792%
	I - Batch: 130 | Loss: 0.024 | Acc: 99.760%
	I - Batch: 140 | Loss: 0.026 | Acc: 99.643%
	I - Batch: 150 | Loss: 0.029 | Acc: 99.542%
	I - Batch: 160 | Loss: 0.031 | Acc: 99.531%
	I - Batch: 170 | Loss: 0.034 | Acc: 99.412%
	I - Batch: 180 | Loss: 0.035 | Acc: 99.306%
	I - Batch: 190 | Loss: 0.037 | Acc: 99.243%
	I - Batch: 200 | Loss: 0.038 | Acc: 99.219%
	I - Batch: 210 | Loss: 0.037 | Acc: 99.256%
	I - Batch: 220 | Loss: 0.037 | Acc: 99.290%
	I - Batch: 230 | Loss: 0.037 | Acc: 99.293%
	I - Batch: 240 | Loss: 0.037 | Acc: 99.271%
	I - Batch: 250 | Loss: 0.036 | Acc: 99.300%
	I - Batch: 260 | Loss: 0.036 | Acc: 99.327%
	I - Batch: 270 | Loss: 0.035 | Acc: 99.329%
	I - Batch: 280 | Loss: 0.036 | Acc: 99.330%
	I - Batch: 290 | Loss: 0.035 | Acc: 99.353%
	I - Batch: 300 | Loss: 0.035 | Acc: 99.375%
	I - Batch: 310 | Loss: 0.035 | Acc: 99.395%
	I - Batch: 320 | Loss: 0.034 | Acc: 99.414%
	I - Batch: 330 | Loss: 0.034 | Acc: 99.375%
	I - Batch: 340 | Loss: 0.034 | Acc: 99.357%
	I - Batch: 350 | Loss: 0.034 | Acc: 99.357%
	I - Batch: 360 | Loss: 0.034 | Acc: 99.375%
	I - Batch: 370 | Loss: 0.034 | Acc: 99.358%
	I - Batch: 380 | Loss: 0.034 | Acc: 99.359%
	I - Batch: 390 | Loss: 0.033 | Acc: 99.375%
	I - Batch: 400 | Loss: 0.033 | Acc: 99.391%
	I - Batch: 410 | Loss: 0.033 | Acc: 99.390%
	I - Batch: 420 | Loss: 0.033 | Acc: 99.390%
	I - Batch: 430 | Loss: 0.033 | Acc: 99.390%
	I - Batch: 440 | Loss: 0.033 | Acc: 99.403%
	I - Batch: 450 | Loss: 0.033 | Acc: 99.403%
	I - Batch: 460 | Loss: 0.033 | Acc: 99.402%
	I - Batch: 470 | Loss: 0.033 | Acc: 99.415%
	I - Batch: 480 | Loss: 0.032 | Acc: 99.427%
	I - Batch: 490 | Loss: 0.032 | Acc: 99.439%
	I - Batch: 500 | Loss: 0.033 | Acc: 99.425%
	I - Batch: 510 | Loss: 0.033 | Acc: 99.424%
	I - Batch: 520 | Loss: 0.034 | Acc: 99.423%
	I - Batch: 530 | Loss: 0.033 | Acc: 99.434%
	I - Batch: 540 | Loss: 0.033 | Acc: 99.444%
	I - Batch: 550 | Loss: 0.033 | Acc: 99.455%
	I - Batch: 560 | Loss: 0.033 | Acc: 99.464%
	I - Batch: 570 | Loss: 0.033 | Acc: 99.441%
	I - Batch: 580 | Loss: 0.033 | Acc: 99.450%
	I - Batch: 590 | Loss: 0.033 | Acc: 99.439%
	I - Batch: 600 | Loss: 0.033 | Acc: 99.448%
	I - Batch: 610 | Loss: 0.033 | Acc: 99.457%
	I - Batch: 620 | Loss: 0.033 | Acc: 99.466%
	I - Batch: 630 | Loss: 0.033 | Acc: 99.464%
	I - Batch: 640 | Loss: 0.033 | Acc: 99.463%
	I - Batch: 650 | Loss: 0.032 | Acc: 99.471%
	I - Batch: 660 | Loss: 0.032 | Acc: 99.479%
	I - Batch: 670 | Loss: 0.032 | Acc: 99.478%
	I - Batch: 680 | Loss: 0.032 | Acc: 99.485%
	I - Batch: 690 | Loss: 0.032 | Acc: 99.484%
	I - Batch: 700 | Loss: 0.033 | Acc: 99.482%
I - num batch: 701
I - Train -- Loss: 0.033 | Acc: 99.483% | LR: 1.250000e-04 | Dur: 546.79s
I - Confusion Matrix: [row->prediction - col->label]
[[1543.    0.    0.    0.    4.]
 [   0.  725.    1.    0.    2.]
 [   0.    8. 1054.    0.   21.]
 [   0.    1.    0. 1502.    7.]
 [   3.    1.    8.    2. 6334.]]

I - Val -- Loss: 2.316 | Acc: 57.194%
I - Confusion Matrix: [row->prediction - col->label]
[[ 311.    6.   19.  103.   57.]
 [   3.   84.   31.   14.   22.]
 [   8.   92.  112.   15.   63.]
 [  72.   22.   26.  243.   58.]
 [ 141.  166.  204.  199. 1015.]]

I - Epoch: 44
	I - Batch: 10 | Loss: 0.022 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.021 | Acc: 100.000%
	I - Batch: 30 | Loss: 0.021 | Acc: 100.000%
	I - Batch: 40 | Loss: 0.021 | Acc: 99.844%
	I - Batch: 50 | Loss: 0.021 | Acc: 99.875%
	I - Batch: 60 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 70 | Loss: 0.024 | Acc: 99.643%
	I - Batch: 80 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 90 | Loss: 0.024 | Acc: 99.653%
	I - Batch: 100 | Loss: 0.028 | Acc: 99.562%
	I - Batch: 110 | Loss: 0.030 | Acc: 99.432%
	I - Batch: 120 | Loss: 0.029 | Acc: 99.479%
	I - Batch: 130 | Loss: 0.031 | Acc: 99.471%
	I - Batch: 140 | Loss: 0.032 | Acc: 99.375%
	I - Batch: 150 | Loss: 0.033 | Acc: 99.333%
	I - Batch: 160 | Loss: 0.032 | Acc: 99.336%
	I - Batch: 170 | Loss: 0.031 | Acc: 99.375%
	I - Batch: 180 | Loss: 0.031 | Acc: 99.375%
	I - Batch: 190 | Loss: 0.031 | Acc: 99.408%
	I - Batch: 200 | Loss: 0.030 | Acc: 99.438%
	I - Batch: 210 | Loss: 0.030 | Acc: 99.435%
	I - Batch: 220 | Loss: 0.031 | Acc: 99.432%
	I - Batch: 230 | Loss: 0.030 | Acc: 99.457%
	I - Batch: 240 | Loss: 0.030 | Acc: 99.453%
	I - Batch: 250 | Loss: 0.030 | Acc: 99.425%
	I - Batch: 260 | Loss: 0.030 | Acc: 99.447%
	I - Batch: 270 | Loss: 0.030 | Acc: 99.444%
	I - Batch: 280 | Loss: 0.030 | Acc: 99.442%
	I - Batch: 290 | Loss: 0.031 | Acc: 99.418%
	I - Batch: 300 | Loss: 0.033 | Acc: 99.417%
	I - Batch: 310 | Loss: 0.034 | Acc: 99.395%
	I - Batch: 320 | Loss: 0.033 | Acc: 99.414%
	I - Batch: 330 | Loss: 0.033 | Acc: 99.432%
	I - Batch: 340 | Loss: 0.033 | Acc: 99.449%
	I - Batch: 350 | Loss: 0.033 | Acc: 99.464%
	I - Batch: 360 | Loss: 0.032 | Acc: 99.479%
	I - Batch: 370 | Loss: 0.032 | Acc: 99.476%
	I - Batch: 380 | Loss: 0.033 | Acc: 99.457%
	I - Batch: 390 | Loss: 0.032 | Acc: 99.471%
	I - Batch: 400 | Loss: 0.032 | Acc: 99.469%
	I - Batch: 410 | Loss: 0.032 | Acc: 99.466%
	I - Batch: 420 | Loss: 0.032 | Acc: 99.479%
	I - Batch: 430 | Loss: 0.032 | Acc: 99.462%
	I - Batch: 440 | Loss: 0.032 | Acc: 99.446%
	I - Batch: 450 | Loss: 0.032 | Acc: 99.431%
	I - Batch: 460 | Loss: 0.032 | Acc: 99.443%
	I - Batch: 470 | Loss: 0.033 | Acc: 99.441%
	I - Batch: 480 | Loss: 0.033 | Acc: 99.440%
	I - Batch: 490 | Loss: 0.032 | Acc: 99.452%
	I - Batch: 500 | Loss: 0.032 | Acc: 99.463%
	I - Batch: 510 | Loss: 0.032 | Acc: 99.473%
	I - Batch: 520 | Loss: 0.032 | Acc: 99.459%
	I - Batch: 530 | Loss: 0.032 | Acc: 99.458%
	I - Batch: 540 | Loss: 0.032 | Acc: 99.468%
	I - Batch: 550 | Loss: 0.032 | Acc: 99.477%
	I - Batch: 560 | Loss: 0.032 | Acc: 99.475%
	I - Batch: 570 | Loss: 0.032 | Acc: 99.463%
	I - Batch: 580 | Loss: 0.032 | Acc: 99.461%
	I - Batch: 590 | Loss: 0.032 | Acc: 99.470%
	I - Batch: 600 | Loss: 0.031 | Acc: 99.479%
	I - Batch: 610 | Loss: 0.032 | Acc: 99.467%
	I - Batch: 620 | Loss: 0.031 | Acc: 99.476%
	I - Batch: 630 | Loss: 0.032 | Acc: 99.454%
	I - Batch: 640 | Loss: 0.032 | Acc: 99.463%
	I - Batch: 650 | Loss: 0.032 | Acc: 99.471%
	I - Batch: 660 | Loss: 0.032 | Acc: 99.479%
	I - Batch: 670 | Loss: 0.032 | Acc: 99.478%
	I - Batch: 680 | Loss: 0.032 | Acc: 99.476%
	I - Batch: 690 | Loss: 0.032 | Acc: 99.466%
	I - Batch: 700 | Loss: 0.033 | Acc: 99.446%
I - num batch: 701
I - Train -- Loss: 0.033 | Acc: 99.447% | LR: 1.250000e-04 | Dur: 561.51s
I - Confusion Matrix: [row->prediction - col->label]
[[1541.    1.    0.    0.    9.]
 [   0.  724.    1.    0.    1.]
 [   0.    8. 1056.    0.   16.]
 [   1.    0.    0. 1499.    8.]
 [   4.    2.    6.    5. 6334.]]

I - Val -- Loss: 2.446 | Acc: 56.999%
I - Confusion Matrix: [row->prediction - col->label]
[[ 319.    6.   19.  106.   57.]
 [   2.   56.   15.    4.   10.]
 [   5.  100.  113.   15.   58.]
 [  66.   26.   31.  237.   56.]
 [ 143.  182.  214.  212. 1034.]]

I - Epoch: 45
	I - Batch: 10 | Loss: 0.053 | Acc: 99.375%
	I - Batch: 20 | Loss: 0.041 | Acc: 99.375%
	I - Batch: 30 | Loss: 0.045 | Acc: 99.375%
	I - Batch: 40 | Loss: 0.040 | Acc: 99.375%
	I - Batch: 50 | Loss: 0.037 | Acc: 99.500%
	I - Batch: 60 | Loss: 0.035 | Acc: 99.479%
	I - Batch: 70 | Loss: 0.033 | Acc: 99.554%
	I - Batch: 80 | Loss: 0.032 | Acc: 99.609%
	I - Batch: 90 | Loss: 0.032 | Acc: 99.653%
	I - Batch: 100 | Loss: 0.031 | Acc: 99.688%
	I - Batch: 110 | Loss: 0.030 | Acc: 99.716%
	I - Batch: 120 | Loss: 0.030 | Acc: 99.740%
	I - Batch: 130 | Loss: 0.029 | Acc: 99.712%
	I - Batch: 140 | Loss: 0.033 | Acc: 99.554%
	I - Batch: 150 | Loss: 0.032 | Acc: 99.583%
	I - Batch: 160 | Loss: 0.032 | Acc: 99.570%
	I - Batch: 170 | Loss: 0.031 | Acc: 99.559%
	I - Batch: 180 | Loss: 0.032 | Acc: 99.514%
	I - Batch: 190 | Loss: 0.032 | Acc: 99.539%
	I - Batch: 200 | Loss: 0.031 | Acc: 99.531%
	I - Batch: 210 | Loss: 0.031 | Acc: 99.554%
	I - Batch: 220 | Loss: 0.031 | Acc: 99.574%
	I - Batch: 230 | Loss: 0.030 | Acc: 99.592%
	I - Batch: 240 | Loss: 0.031 | Acc: 99.583%
	I - Batch: 250 | Loss: 0.031 | Acc: 99.600%
	I - Batch: 260 | Loss: 0.031 | Acc: 99.591%
	I - Batch: 270 | Loss: 0.030 | Acc: 99.606%
	I - Batch: 280 | Loss: 0.030 | Acc: 99.598%
	I - Batch: 290 | Loss: 0.030 | Acc: 99.591%
	I - Batch: 300 | Loss: 0.030 | Acc: 99.604%
	I - Batch: 310 | Loss: 0.029 | Acc: 99.617%
	I - Batch: 320 | Loss: 0.029 | Acc: 99.629%
	I - Batch: 330 | Loss: 0.029 | Acc: 99.640%
	I - Batch: 340 | Loss: 0.029 | Acc: 99.651%
	I - Batch: 350 | Loss: 0.028 | Acc: 99.661%
	I - Batch: 360 | Loss: 0.028 | Acc: 99.670%
	I - Batch: 370 | Loss: 0.028 | Acc: 99.679%
	I - Batch: 380 | Loss: 0.029 | Acc: 99.655%
	I - Batch: 390 | Loss: 0.031 | Acc: 99.615%
	I - Batch: 400 | Loss: 0.031 | Acc: 99.625%
	I - Batch: 410 | Loss: 0.031 | Acc: 99.619%
	I - Batch: 420 | Loss: 0.030 | Acc: 99.628%
	I - Batch: 430 | Loss: 0.030 | Acc: 99.637%
	I - Batch: 440 | Loss: 0.030 | Acc: 99.616%
	I - Batch: 450 | Loss: 0.030 | Acc: 99.597%
	I - Batch: 460 | Loss: 0.030 | Acc: 99.606%
	I - Batch: 470 | Loss: 0.030 | Acc: 99.574%
	I - Batch: 480 | Loss: 0.030 | Acc: 99.583%
	I - Batch: 490 | Loss: 0.030 | Acc: 99.592%
	I - Batch: 500 | Loss: 0.030 | Acc: 99.588%
	I - Batch: 510 | Loss: 0.030 | Acc: 99.596%
	I - Batch: 520 | Loss: 0.030 | Acc: 99.591%
	I - Batch: 530 | Loss: 0.031 | Acc: 99.528%
	I - Batch: 540 | Loss: 0.031 | Acc: 99.537%
	I - Batch: 550 | Loss: 0.031 | Acc: 99.545%
	I - Batch: 560 | Loss: 0.031 | Acc: 99.542%
	I - Batch: 570 | Loss: 0.031 | Acc: 99.550%
	I - Batch: 580 | Loss: 0.031 | Acc: 99.558%
	I - Batch: 590 | Loss: 0.031 | Acc: 99.544%
	I - Batch: 600 | Loss: 0.031 | Acc: 99.552%
	I - Batch: 610 | Loss: 0.031 | Acc: 99.529%
	I - Batch: 620 | Loss: 0.031 | Acc: 99.536%
	I - Batch: 630 | Loss: 0.031 | Acc: 99.534%
	I - Batch: 640 | Loss: 0.031 | Acc: 99.531%
	I - Batch: 650 | Loss: 0.033 | Acc: 99.481%
	I - Batch: 660 | Loss: 0.033 | Acc: 99.479%
	I - Batch: 670 | Loss: 0.033 | Acc: 99.450%
	I - Batch: 680 | Loss: 0.033 | Acc: 99.458%
	I - Batch: 690 | Loss: 0.034 | Acc: 99.438%
	I - Batch: 700 | Loss: 0.034 | Acc: 99.438%
I - num batch: 701
I - Train -- Loss: 0.034 | Acc: 99.438% | LR: 1.250000e-04 | Dur: 564.85s
I - Confusion Matrix: [row->prediction - col->label]
[[1542.    0.    0.    1.   11.]
 [   0.  726.    1.    0.    3.]
 [   0.    7. 1056.    0.   11.]
 [   0.    1.    0. 1499.   13.]
 [   4.    1.    6.    4. 6330.]]

I - Val -- Loss: 2.334 | Acc: 57.323%
I - Confusion Matrix: [row->prediction - col->label]
[[ 291.    4.   18.   81.   41.]
 [   4.  103.   33.   16.   30.]
 [   6.   87.  114.   19.   62.]
 [  81.   20.   29.  244.   65.]
 [ 153.  156.  198.  214. 1017.]]

I - Epoch: 46
	I - Batch: 10 | Loss: 0.042 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.032 | Acc: 100.000%
	I - Batch: 30 | Loss: 0.028 | Acc: 99.792%
	I - Batch: 40 | Loss: 0.029 | Acc: 99.531%
	I - Batch: 50 | Loss: 0.027 | Acc: 99.625%
	I - Batch: 60 | Loss: 0.027 | Acc: 99.583%
	I - Batch: 70 | Loss: 0.035 | Acc: 99.375%
	I - Batch: 80 | Loss: 0.033 | Acc: 99.453%
	I - Batch: 90 | Loss: 0.035 | Acc: 99.306%
	I - Batch: 100 | Loss: 0.034 | Acc: 99.375%
	I - Batch: 110 | Loss: 0.032 | Acc: 99.432%
	I - Batch: 120 | Loss: 0.031 | Acc: 99.427%
	I - Batch: 130 | Loss: 0.031 | Acc: 99.423%
	I - Batch: 140 | Loss: 0.031 | Acc: 99.420%
	I - Batch: 150 | Loss: 0.030 | Acc: 99.458%
	I - Batch: 160 | Loss: 0.029 | Acc: 99.492%
	I - Batch: 170 | Loss: 0.029 | Acc: 99.522%
	I - Batch: 180 | Loss: 0.029 | Acc: 99.479%
	I - Batch: 190 | Loss: 0.029 | Acc: 99.507%
	I - Batch: 200 | Loss: 0.029 | Acc: 99.500%
	I - Batch: 210 | Loss: 0.031 | Acc: 99.464%
	I - Batch: 220 | Loss: 0.031 | Acc: 99.489%
	I - Batch: 230 | Loss: 0.031 | Acc: 99.457%
	I - Batch: 240 | Loss: 0.031 | Acc: 99.427%
	I - Batch: 250 | Loss: 0.031 | Acc: 99.450%
	I - Batch: 260 | Loss: 0.032 | Acc: 99.423%
	I - Batch: 270 | Loss: 0.031 | Acc: 99.444%
	I - Batch: 280 | Loss: 0.031 | Acc: 99.442%
	I - Batch: 290 | Loss: 0.031 | Acc: 99.440%
	I - Batch: 300 | Loss: 0.031 | Acc: 99.438%
	I - Batch: 310 | Loss: 0.031 | Acc: 99.435%
	I - Batch: 320 | Loss: 0.031 | Acc: 99.434%
	I - Batch: 330 | Loss: 0.030 | Acc: 99.451%
	I - Batch: 340 | Loss: 0.030 | Acc: 99.449%
	I - Batch: 350 | Loss: 0.030 | Acc: 99.464%
	I - Batch: 360 | Loss: 0.031 | Acc: 99.462%
	I - Batch: 370 | Loss: 0.031 | Acc: 99.426%
	I - Batch: 380 | Loss: 0.031 | Acc: 99.441%
	I - Batch: 390 | Loss: 0.031 | Acc: 99.423%
	I - Batch: 400 | Loss: 0.031 | Acc: 99.438%
	I - Batch: 410 | Loss: 0.031 | Acc: 99.451%
	I - Batch: 420 | Loss: 0.031 | Acc: 99.464%
	I - Batch: 430 | Loss: 0.031 | Acc: 99.462%
	I - Batch: 440 | Loss: 0.031 | Acc: 99.432%
	I - Batch: 450 | Loss: 0.031 | Acc: 99.431%
	I - Batch: 460 | Loss: 0.031 | Acc: 99.429%
	I - Batch: 470 | Loss: 0.030 | Acc: 99.441%
	I - Batch: 480 | Loss: 0.030 | Acc: 99.453%
	I - Batch: 490 | Loss: 0.030 | Acc: 99.464%
	I - Batch: 500 | Loss: 0.030 | Acc: 99.475%
	I - Batch: 510 | Loss: 0.030 | Acc: 99.485%
	I - Batch: 520 | Loss: 0.030 | Acc: 99.483%
	I - Batch: 530 | Loss: 0.030 | Acc: 99.469%
	I - Batch: 540 | Loss: 0.030 | Acc: 99.468%
	I - Batch: 550 | Loss: 0.030 | Acc: 99.455%
	I - Batch: 560 | Loss: 0.030 | Acc: 99.453%
	I - Batch: 570 | Loss: 0.030 | Acc: 99.452%
	I - Batch: 580 | Loss: 0.030 | Acc: 99.450%
	I - Batch: 590 | Loss: 0.030 | Acc: 99.460%
	I - Batch: 600 | Loss: 0.030 | Acc: 99.458%
	I - Batch: 610 | Loss: 0.030 | Acc: 99.457%
	I - Batch: 620 | Loss: 0.030 | Acc: 99.456%
	I - Batch: 630 | Loss: 0.030 | Acc: 99.464%
	I - Batch: 640 | Loss: 0.030 | Acc: 99.453%
	I - Batch: 650 | Loss: 0.030 | Acc: 99.442%
	I - Batch: 660 | Loss: 0.030 | Acc: 99.451%
	I - Batch: 670 | Loss: 0.030 | Acc: 99.440%
	I - Batch: 680 | Loss: 0.030 | Acc: 99.449%
	I - Batch: 690 | Loss: 0.030 | Acc: 99.438%
	I - Batch: 700 | Loss: 0.030 | Acc: 99.438%
I - num batch: 701
I - Train -- Loss: 0.030 | Acc: 99.438% | LR: 1.250000e-04 | Dur: 581.41s
I - Confusion Matrix: [row->prediction - col->label]
[[1541.    0.    0.    1.    9.]
 [   0.  727.    3.    1.    0.]
 [   0.    6. 1053.    0.   18.]
 [   0.    1.    0. 1501.   10.]
 [   5.    1.    7.    1. 6331.]]

I - Val -- Loss: 2.336 | Acc: 57.097%
I - Confusion Matrix: [row->prediction - col->label]
[[ 302.    3.   20.   92.   45.]
 [   3.   82.   29.   11.   26.]
 [   5.  111.  121.   18.   67.]
 [  75.   26.   35.  250.   70.]
 [ 150.  148.  187.  203. 1007.]]

I - Epoch: 47
	I - Batch: 10 | Loss: 0.019 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.018 | Acc: 100.000%
	I - Batch: 30 | Loss: 0.033 | Acc: 99.583%
	I - Batch: 40 | Loss: 0.029 | Acc: 99.688%
	I - Batch: 50 | Loss: 0.030 | Acc: 99.625%
	I - Batch: 60 | Loss: 0.028 | Acc: 99.688%
	I - Batch: 70 | Loss: 0.026 | Acc: 99.732%
	I - Batch: 80 | Loss: 0.025 | Acc: 99.766%
	I - Batch: 90 | Loss: 0.029 | Acc: 99.653%
	I - Batch: 100 | Loss: 0.037 | Acc: 99.438%
	I - Batch: 110 | Loss: 0.037 | Acc: 99.318%
	I - Batch: 120 | Loss: 0.036 | Acc: 99.323%
	I - Batch: 130 | Loss: 0.034 | Acc: 99.375%
	I - Batch: 140 | Loss: 0.033 | Acc: 99.420%
	I - Batch: 150 | Loss: 0.032 | Acc: 99.458%
	I - Batch: 160 | Loss: 0.032 | Acc: 99.492%
	I - Batch: 170 | Loss: 0.032 | Acc: 99.449%
	I - Batch: 180 | Loss: 0.031 | Acc: 99.479%
	I - Batch: 190 | Loss: 0.031 | Acc: 99.474%
	I - Batch: 200 | Loss: 0.034 | Acc: 99.312%
	I - Batch: 210 | Loss: 0.033 | Acc: 99.345%
	I - Batch: 220 | Loss: 0.033 | Acc: 99.375%
	I - Batch: 230 | Loss: 0.032 | Acc: 99.375%
	I - Batch: 240 | Loss: 0.032 | Acc: 99.349%
	I - Batch: 250 | Loss: 0.033 | Acc: 99.350%
	I - Batch: 260 | Loss: 0.032 | Acc: 99.375%
	I - Batch: 270 | Loss: 0.033 | Acc: 99.352%
	I - Batch: 280 | Loss: 0.033 | Acc: 99.375%
	I - Batch: 290 | Loss: 0.032 | Acc: 99.397%
	I - Batch: 300 | Loss: 0.032 | Acc: 99.417%
	I - Batch: 310 | Loss: 0.031 | Acc: 99.435%
	I - Batch: 320 | Loss: 0.031 | Acc: 99.453%
	I - Batch: 330 | Loss: 0.031 | Acc: 99.470%
	I - Batch: 340 | Loss: 0.030 | Acc: 99.467%
	I - Batch: 350 | Loss: 0.030 | Acc: 99.482%
	I - Batch: 360 | Loss: 0.030 | Acc: 99.497%
	I - Batch: 370 | Loss: 0.029 | Acc: 99.510%
	I - Batch: 380 | Loss: 0.029 | Acc: 99.507%
	I - Batch: 390 | Loss: 0.029 | Acc: 99.487%
	I - Batch: 400 | Loss: 0.030 | Acc: 99.484%
	I - Batch: 410 | Loss: 0.030 | Acc: 99.497%
	I - Batch: 420 | Loss: 0.029 | Acc: 99.509%
	I - Batch: 430 | Loss: 0.030 | Acc: 99.506%
	I - Batch: 440 | Loss: 0.030 | Acc: 99.503%
	I - Batch: 450 | Loss: 0.031 | Acc: 99.472%
	I - Batch: 460 | Loss: 0.031 | Acc: 99.470%
	I - Batch: 470 | Loss: 0.032 | Acc: 99.428%
	I - Batch: 480 | Loss: 0.032 | Acc: 99.427%
	I - Batch: 490 | Loss: 0.031 | Acc: 99.439%
	I - Batch: 500 | Loss: 0.031 | Acc: 99.425%
	I - Batch: 510 | Loss: 0.031 | Acc: 99.424%
	I - Batch: 520 | Loss: 0.031 | Acc: 99.435%
	I - Batch: 530 | Loss: 0.031 | Acc: 99.434%
	I - Batch: 540 | Loss: 0.031 | Acc: 99.444%
	I - Batch: 550 | Loss: 0.030 | Acc: 99.455%
	I - Batch: 560 | Loss: 0.033 | Acc: 99.408%
	I - Batch: 570 | Loss: 0.032 | Acc: 99.408%
	I - Batch: 580 | Loss: 0.032 | Acc: 99.418%
	I - Batch: 590 | Loss: 0.032 | Acc: 99.417%
	I - Batch: 600 | Loss: 0.032 | Acc: 99.396%
	I - Batch: 610 | Loss: 0.032 | Acc: 99.406%
	I - Batch: 620 | Loss: 0.032 | Acc: 99.415%
	I - Batch: 630 | Loss: 0.032 | Acc: 99.425%
	I - Batch: 640 | Loss: 0.031 | Acc: 99.434%
	I - Batch: 650 | Loss: 0.031 | Acc: 99.442%
	I - Batch: 660 | Loss: 0.031 | Acc: 99.451%
	I - Batch: 670 | Loss: 0.031 | Acc: 99.440%
	I - Batch: 680 | Loss: 0.031 | Acc: 99.439%
	I - Batch: 690 | Loss: 0.031 | Acc: 99.447%
	I - Batch: 700 | Loss: 0.031 | Acc: 99.455%
I - num batch: 701
I - Train -- Loss: 0.031 | Acc: 99.456% | LR: 1.250000e-04 | Dur: 575.15s
I - Confusion Matrix: [row->prediction - col->label]
[[1539.    0.    0.    0.   12.]
 [   1.  729.    0.    1.    3.]
 [   0.    2. 1058.    0.   10.]
 [   0.    1.    0. 1498.   12.]
 [   6.    3.    5.    5. 6331.]]

I - Val -- Loss: 2.360 | Acc: 57.129%
I - Confusion Matrix: [row->prediction - col->label]
[[ 323.    7.   20.   99.   61.]
 [   3.   71.   19.    8.   19.]
 [   6.  113.  130.   26.   74.]
 [  69.   21.   31.  238.   60.]
 [ 134.  158.  192.  203. 1001.]]

I - Epoch: 48
	I - Batch: 10 | Loss: 0.021 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.039 | Acc: 99.688%
	I - Batch: 30 | Loss: 0.039 | Acc: 99.583%
	I - Batch: 40 | Loss: 0.033 | Acc: 99.688%
	I - Batch: 50 | Loss: 0.030 | Acc: 99.750%
	I - Batch: 60 | Loss: 0.029 | Acc: 99.792%
	I - Batch: 70 | Loss: 0.041 | Acc: 99.286%
	I - Batch: 80 | Loss: 0.045 | Acc: 99.297%
	I - Batch: 90 | Loss: 0.043 | Acc: 99.236%
	I - Batch: 100 | Loss: 0.045 | Acc: 99.125%
	I - Batch: 110 | Loss: 0.043 | Acc: 99.148%
	I - Batch: 120 | Loss: 0.042 | Acc: 99.219%
	I - Batch: 130 | Loss: 0.040 | Acc: 99.279%
	I - Batch: 140 | Loss: 0.039 | Acc: 99.330%
	I - Batch: 150 | Loss: 0.039 | Acc: 99.292%
	I - Batch: 160 | Loss: 0.039 | Acc: 99.297%
	I - Batch: 170 | Loss: 0.040 | Acc: 99.154%
	I - Batch: 180 | Loss: 0.039 | Acc: 99.201%
	I - Batch: 190 | Loss: 0.037 | Acc: 99.243%
	I - Batch: 200 | Loss: 0.037 | Acc: 99.281%
	I - Batch: 210 | Loss: 0.039 | Acc: 99.226%
	I - Batch: 220 | Loss: 0.040 | Acc: 99.176%
	I - Batch: 230 | Loss: 0.039 | Acc: 99.212%
	I - Batch: 240 | Loss: 0.039 | Acc: 99.219%
	I - Batch: 250 | Loss: 0.038 | Acc: 99.250%
	I - Batch: 260 | Loss: 0.038 | Acc: 99.231%
	I - Batch: 270 | Loss: 0.038 | Acc: 99.236%
	I - Batch: 280 | Loss: 0.037 | Acc: 99.263%
	I - Batch: 290 | Loss: 0.037 | Acc: 99.267%
	I - Batch: 300 | Loss: 0.036 | Acc: 99.292%
	I - Batch: 310 | Loss: 0.036 | Acc: 99.315%
	I - Batch: 320 | Loss: 0.036 | Acc: 99.297%
	I - Batch: 330 | Loss: 0.036 | Acc: 99.318%
	I - Batch: 340 | Loss: 0.035 | Acc: 99.320%
	I - Batch: 350 | Loss: 0.036 | Acc: 99.286%
	I - Batch: 360 | Loss: 0.036 | Acc: 99.271%
	I - Batch: 370 | Loss: 0.036 | Acc: 99.274%
	I - Batch: 380 | Loss: 0.035 | Acc: 99.276%
	I - Batch: 390 | Loss: 0.035 | Acc: 99.295%
	I - Batch: 400 | Loss: 0.034 | Acc: 99.312%
	I - Batch: 410 | Loss: 0.034 | Acc: 99.314%
	I - Batch: 420 | Loss: 0.034 | Acc: 99.315%
	I - Batch: 430 | Loss: 0.034 | Acc: 99.331%
	I - Batch: 440 | Loss: 0.033 | Acc: 99.332%
	I - Batch: 450 | Loss: 0.033 | Acc: 99.347%
	I - Batch: 460 | Loss: 0.033 | Acc: 99.348%
	I - Batch: 470 | Loss: 0.033 | Acc: 99.348%
	I - Batch: 480 | Loss: 0.034 | Acc: 99.336%
	I - Batch: 490 | Loss: 0.033 | Acc: 99.349%
	I - Batch: 500 | Loss: 0.033 | Acc: 99.350%
	I - Batch: 510 | Loss: 0.033 | Acc: 99.363%
	I - Batch: 520 | Loss: 0.033 | Acc: 99.375%
	I - Batch: 530 | Loss: 0.032 | Acc: 99.387%
	I - Batch: 540 | Loss: 0.033 | Acc: 99.363%
	I - Batch: 550 | Loss: 0.033 | Acc: 99.375%
	I - Batch: 560 | Loss: 0.033 | Acc: 99.386%
	I - Batch: 570 | Loss: 0.033 | Acc: 99.375%
	I - Batch: 580 | Loss: 0.033 | Acc: 99.386%
	I - Batch: 590 | Loss: 0.032 | Acc: 99.396%
	I - Batch: 600 | Loss: 0.032 | Acc: 99.396%
	I - Batch: 610 | Loss: 0.032 | Acc: 99.406%
	I - Batch: 620 | Loss: 0.032 | Acc: 99.415%
	I - Batch: 630 | Loss: 0.032 | Acc: 99.425%
	I - Batch: 640 | Loss: 0.032 | Acc: 99.434%
	I - Batch: 650 | Loss: 0.033 | Acc: 99.423%
	I - Batch: 660 | Loss: 0.034 | Acc: 99.394%
	I - Batch: 670 | Loss: 0.034 | Acc: 99.384%
	I - Batch: 680 | Loss: 0.033 | Acc: 99.393%
	I - Batch: 690 | Loss: 0.033 | Acc: 99.402%
	I - Batch: 700 | Loss: 0.034 | Acc: 99.384%
I - num batch: 701
I - Train -- Loss: 0.034 | Acc: 99.385% | LR: 1.250000e-04 | Dur: 553.42s
I - Confusion Matrix: [row->prediction - col->label]
[[1540.    0.    0.    1.   14.]
 [   0.  725.    2.    0.    0.]
 [   0.    8. 1052.    0.   13.]
 [   1.    0.    0. 1495.    6.]
 [   5.    2.    9.    8. 6335.]]

I - Val -- Loss: 2.411 | Acc: 57.518%
I - Confusion Matrix: [row->prediction - col->label]
[[ 340.    6.   23.  113.   66.]
 [   1.   57.   15.    4.   12.]
 [   2.   85.  101.   10.   40.]
 [  72.   30.   32.  255.   75.]
 [ 120.  192.  221.  192. 1022.]]

I - Epoch: 49
	I - Batch: 10 | Loss: 0.016 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.017 | Acc: 100.000%
	I - Batch: 30 | Loss: 0.021 | Acc: 100.000%
	I - Batch: 40 | Loss: 0.028 | Acc: 99.844%
	I - Batch: 50 | Loss: 0.026 | Acc: 99.750%
	I - Batch: 60 | Loss: 0.025 | Acc: 99.792%
	I - Batch: 70 | Loss: 0.024 | Acc: 99.821%
	I - Batch: 80 | Loss: 0.024 | Acc: 99.844%
	I - Batch: 90 | Loss: 0.023 | Acc: 99.861%
	I - Batch: 100 | Loss: 0.023 | Acc: 99.875%
	I - Batch: 110 | Loss: 0.022 | Acc: 99.886%
	I - Batch: 120 | Loss: 0.022 | Acc: 99.896%
	I - Batch: 130 | Loss: 0.022 | Acc: 99.904%
	I - Batch: 140 | Loss: 0.022 | Acc: 99.911%
	I - Batch: 150 | Loss: 0.022 | Acc: 99.875%
	I - Batch: 160 | Loss: 0.022 | Acc: 99.883%
	I - Batch: 170 | Loss: 0.023 | Acc: 99.779%
	I - Batch: 180 | Loss: 0.023 | Acc: 99.757%
	I - Batch: 190 | Loss: 0.025 | Acc: 99.737%
	I - Batch: 200 | Loss: 0.024 | Acc: 99.750%
	I - Batch: 210 | Loss: 0.024 | Acc: 99.732%
	I - Batch: 220 | Loss: 0.024 | Acc: 99.744%
	I - Batch: 230 | Loss: 0.024 | Acc: 99.755%
	I - Batch: 240 | Loss: 0.024 | Acc: 99.766%
	I - Batch: 250 | Loss: 0.024 | Acc: 99.775%
	I - Batch: 260 | Loss: 0.024 | Acc: 99.784%
	I - Batch: 270 | Loss: 0.025 | Acc: 99.769%
	I - Batch: 280 | Loss: 0.025 | Acc: 99.754%
	I - Batch: 290 | Loss: 0.025 | Acc: 99.763%
	I - Batch: 300 | Loss: 0.025 | Acc: 99.771%
	I - Batch: 310 | Loss: 0.024 | Acc: 99.778%
	I - Batch: 320 | Loss: 0.024 | Acc: 99.785%
	I - Batch: 330 | Loss: 0.024 | Acc: 99.792%
	I - Batch: 340 | Loss: 0.024 | Acc: 99.798%
	I - Batch: 350 | Loss: 0.024 | Acc: 99.786%
	I - Batch: 360 | Loss: 0.024 | Acc: 99.792%
	I - Batch: 370 | Loss: 0.024 | Acc: 99.797%
	I - Batch: 380 | Loss: 0.024 | Acc: 99.803%
	I - Batch: 390 | Loss: 0.024 | Acc: 99.808%
	I - Batch: 400 | Loss: 0.024 | Acc: 99.797%
	I - Batch: 410 | Loss: 0.024 | Acc: 99.802%
	I - Batch: 420 | Loss: 0.024 | Acc: 99.807%
	I - Batch: 430 | Loss: 0.024 | Acc: 99.797%
	I - Batch: 440 | Loss: 0.024 | Acc: 99.801%
	I - Batch: 450 | Loss: 0.024 | Acc: 99.792%
	I - Batch: 460 | Loss: 0.024 | Acc: 99.783%
	I - Batch: 470 | Loss: 0.024 | Acc: 99.787%
	I - Batch: 480 | Loss: 0.024 | Acc: 99.779%
	I - Batch: 490 | Loss: 0.024 | Acc: 99.770%
	I - Batch: 500 | Loss: 0.024 | Acc: 99.775%
	I - Batch: 510 | Loss: 0.024 | Acc: 99.779%
	I - Batch: 520 | Loss: 0.024 | Acc: 99.784%
	I - Batch: 530 | Loss: 0.024 | Acc: 99.776%
	I - Batch: 540 | Loss: 0.025 | Acc: 99.769%
	I - Batch: 550 | Loss: 0.025 | Acc: 99.773%
	I - Batch: 560 | Loss: 0.025 | Acc: 99.777%
	I - Batch: 570 | Loss: 0.025 | Acc: 99.770%
	I - Batch: 580 | Loss: 0.026 | Acc: 99.763%
	I - Batch: 590 | Loss: 0.026 | Acc: 99.767%
	I - Batch: 600 | Loss: 0.026 | Acc: 99.760%
	I - Batch: 610 | Loss: 0.026 | Acc: 99.764%
	I - Batch: 620 | Loss: 0.026 | Acc: 99.768%
	I - Batch: 630 | Loss: 0.025 | Acc: 99.772%
	I - Batch: 640 | Loss: 0.025 | Acc: 99.766%
	I - Batch: 650 | Loss: 0.025 | Acc: 99.769%
	I - Batch: 660 | Loss: 0.025 | Acc: 99.773%
	I - Batch: 670 | Loss: 0.025 | Acc: 99.776%
	I - Batch: 680 | Loss: 0.026 | Acc: 99.752%
	I - Batch: 690 | Loss: 0.026 | Acc: 99.755%
	I - Batch: 700 | Loss: 0.026 | Acc: 99.741%
I - num batch: 701
I - Train -- Loss: 0.026 | Acc: 99.741% | LR: 1.250000e-04 | Dur: 544.27s
I - Confusion Matrix: [row->prediction - col->label]
[[1544.    0.    0.    0.    4.]
 [   0.  730.    2.    0.    0.]
 [   0.    3. 1058.    0.    6.]
 [   0.    2.    0. 1504.    7.]
 [   2.    0.    3.    0. 6351.]]

I - Val -- Loss: 2.346 | Acc: 57.129%
I - Confusion Matrix: [row->prediction - col->label]
[[ 291.    5.   18.   72.   45.]
 [   2.   78.   28.    9.   19.]
 [   7.  101.  115.   12.   61.]
 [  75.   22.   29.  257.   68.]
 [ 160.  164.  202.  224. 1022.]]

I - Epoch: 50
	I - Batch: 10 | Loss: 0.028 | Acc: 99.375%
	I - Batch: 20 | Loss: 0.026 | Acc: 99.688%
	I - Batch: 30 | Loss: 0.023 | Acc: 99.792%
	I - Batch: 40 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 50 | Loss: 0.023 | Acc: 99.625%
	I - Batch: 60 | Loss: 0.022 | Acc: 99.688%
	I - Batch: 70 | Loss: 0.022 | Acc: 99.732%
	I - Batch: 80 | Loss: 0.021 | Acc: 99.766%
	I - Batch: 90 | Loss: 0.021 | Acc: 99.792%
	I - Batch: 100 | Loss: 0.020 | Acc: 99.812%
	I - Batch: 110 | Loss: 0.020 | Acc: 99.830%
	I - Batch: 120 | Loss: 0.020 | Acc: 99.844%
	I - Batch: 130 | Loss: 0.020 | Acc: 99.856%
	I - Batch: 140 | Loss: 0.020 | Acc: 99.777%
	I - Batch: 150 | Loss: 0.023 | Acc: 99.750%
	I - Batch: 160 | Loss: 0.023 | Acc: 99.766%
	I - Batch: 170 | Loss: 0.022 | Acc: 99.779%
	I - Batch: 180 | Loss: 0.023 | Acc: 99.792%
	I - Batch: 190 | Loss: 0.022 | Acc: 99.803%
	I - Batch: 200 | Loss: 0.022 | Acc: 99.812%
	I - Batch: 210 | Loss: 0.022 | Acc: 99.821%
	I - Batch: 220 | Loss: 0.023 | Acc: 99.801%
	I - Batch: 230 | Loss: 0.024 | Acc: 99.755%
	I - Batch: 240 | Loss: 0.023 | Acc: 99.766%
	I - Batch: 250 | Loss: 0.025 | Acc: 99.725%
	I - Batch: 260 | Loss: 0.025 | Acc: 99.736%
	I - Batch: 270 | Loss: 0.025 | Acc: 99.722%
	I - Batch: 280 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 290 | Loss: 0.026 | Acc: 99.634%
	I - Batch: 300 | Loss: 0.026 | Acc: 99.646%
	I - Batch: 310 | Loss: 0.026 | Acc: 99.637%
	I - Batch: 320 | Loss: 0.026 | Acc: 99.648%
	I - Batch: 330 | Loss: 0.026 | Acc: 99.659%
	I - Batch: 340 | Loss: 0.026 | Acc: 99.669%
	I - Batch: 350 | Loss: 0.025 | Acc: 99.679%
	I - Batch: 360 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 370 | Loss: 0.025 | Acc: 99.696%
	I - Batch: 380 | Loss: 0.025 | Acc: 99.704%
	I - Batch: 390 | Loss: 0.025 | Acc: 99.712%
	I - Batch: 400 | Loss: 0.024 | Acc: 99.719%
	I - Batch: 410 | Loss: 0.024 | Acc: 99.726%
	I - Batch: 420 | Loss: 0.024 | Acc: 99.732%
	I - Batch: 430 | Loss: 0.024 | Acc: 99.738%
	I - Batch: 440 | Loss: 0.024 | Acc: 99.744%
	I - Batch: 450 | Loss: 0.023 | Acc: 99.750%
	I - Batch: 460 | Loss: 0.023 | Acc: 99.755%
	I - Batch: 470 | Loss: 0.024 | Acc: 99.747%
	I - Batch: 480 | Loss: 0.023 | Acc: 99.753%
	I - Batch: 490 | Loss: 0.024 | Acc: 99.707%
	I - Batch: 500 | Loss: 0.025 | Acc: 99.700%
	I - Batch: 510 | Loss: 0.025 | Acc: 99.706%
	I - Batch: 520 | Loss: 0.025 | Acc: 99.700%
	I - Batch: 530 | Loss: 0.025 | Acc: 99.682%
	I - Batch: 540 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 550 | Loss: 0.025 | Acc: 99.693%
	I - Batch: 560 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 570 | Loss: 0.025 | Acc: 99.693%
	I - Batch: 580 | Loss: 0.026 | Acc: 99.677%
	I - Batch: 590 | Loss: 0.026 | Acc: 99.672%
	I - Batch: 600 | Loss: 0.026 | Acc: 99.677%
	I - Batch: 610 | Loss: 0.025 | Acc: 99.682%
	I - Batch: 620 | Loss: 0.026 | Acc: 99.657%
	I - Batch: 630 | Loss: 0.026 | Acc: 99.663%
	I - Batch: 640 | Loss: 0.026 | Acc: 99.668%
	I - Batch: 650 | Loss: 0.026 | Acc: 99.673%
	I - Batch: 660 | Loss: 0.026 | Acc: 99.678%
	I - Batch: 670 | Loss: 0.025 | Acc: 99.683%
	I - Batch: 680 | Loss: 0.025 | Acc: 99.678%
	I - Batch: 690 | Loss: 0.026 | Acc: 99.656%
	I - Batch: 700 | Loss: 0.026 | Acc: 99.661%
I - num batch: 701
I - Train -- Loss: 0.026 | Acc: 99.661% | LR: 1.250000e-04 | Dur: 550.47s
I - Confusion Matrix: [row->prediction - col->label]
[[1544.    0.    0.    0.    7.]
 [   0.  728.    1.    0.    1.]
 [   0.    5. 1055.    0.    9.]
 [   0.    1.    0. 1504.    4.]
 [   2.    1.    7.    0. 6347.]]

I - Val -- Loss: 2.478 | Acc: 56.189%
I - Confusion Matrix: [row->prediction - col->label]
[[ 297.    4.   17.   96.   45.]
 [   2.   76.   27.   12.   18.]
 [   6.  102.  109.   20.   65.]
 [  67.   21.   27.  216.   51.]
 [ 163.  167.  212.  230. 1036.]]

I - Epoch: 51
	I - Batch: 10 | Loss: 0.016 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.023 | Acc: 99.375%
	I - Batch: 30 | Loss: 0.023 | Acc: 99.375%
	I - Batch: 40 | Loss: 0.027 | Acc: 99.375%
	I - Batch: 50 | Loss: 0.025 | Acc: 99.500%
	I - Batch: 60 | Loss: 0.024 | Acc: 99.583%
	I - Batch: 70 | Loss: 0.023 | Acc: 99.554%
	I - Batch: 80 | Loss: 0.024 | Acc: 99.453%
	I - Batch: 90 | Loss: 0.024 | Acc: 99.514%
	I - Batch: 100 | Loss: 0.023 | Acc: 99.562%
	I - Batch: 110 | Loss: 0.022 | Acc: 99.602%
	I - Batch: 120 | Loss: 0.021 | Acc: 99.635%
	I - Batch: 130 | Loss: 0.021 | Acc: 99.615%
	I - Batch: 140 | Loss: 0.021 | Acc: 99.643%
	I - Batch: 150 | Loss: 0.021 | Acc: 99.667%
	I - Batch: 160 | Loss: 0.021 | Acc: 99.688%
	I - Batch: 170 | Loss: 0.021 | Acc: 99.706%
	I - Batch: 180 | Loss: 0.021 | Acc: 99.722%
	I - Batch: 190 | Loss: 0.021 | Acc: 99.671%
	I - Batch: 200 | Loss: 0.021 | Acc: 99.688%
	I - Batch: 210 | Loss: 0.021 | Acc: 99.702%
	I - Batch: 220 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 230 | Loss: 0.024 | Acc: 99.701%
	I - Batch: 240 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 250 | Loss: 0.023 | Acc: 99.700%
	I - Batch: 260 | Loss: 0.023 | Acc: 99.712%
	I - Batch: 270 | Loss: 0.024 | Acc: 99.699%
	I - Batch: 280 | Loss: 0.024 | Acc: 99.710%
	I - Batch: 290 | Loss: 0.024 | Acc: 99.698%
	I - Batch: 300 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 310 | Loss: 0.025 | Acc: 99.677%
	I - Batch: 320 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 330 | Loss: 0.024 | Acc: 99.678%
	I - Batch: 340 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 350 | Loss: 0.024 | Acc: 99.696%
	I - Batch: 360 | Loss: 0.024 | Acc: 99.705%
	I - Batch: 370 | Loss: 0.024 | Acc: 99.696%
	I - Batch: 380 | Loss: 0.024 | Acc: 99.704%
	I - Batch: 390 | Loss: 0.024 | Acc: 99.712%
	I - Batch: 400 | Loss: 0.024 | Acc: 99.719%
	I - Batch: 410 | Loss: 0.024 | Acc: 99.710%
	I - Batch: 420 | Loss: 0.024 | Acc: 99.702%
	I - Batch: 430 | Loss: 0.024 | Acc: 99.709%
	I - Batch: 440 | Loss: 0.024 | Acc: 99.702%
	I - Batch: 450 | Loss: 0.024 | Acc: 99.708%
	I - Batch: 460 | Loss: 0.024 | Acc: 99.701%
	I - Batch: 470 | Loss: 0.024 | Acc: 99.707%
	I - Batch: 480 | Loss: 0.024 | Acc: 99.714%
	I - Batch: 490 | Loss: 0.025 | Acc: 99.694%
	I - Batch: 500 | Loss: 0.024 | Acc: 99.700%
	I - Batch: 510 | Loss: 0.024 | Acc: 99.694%
	I - Batch: 520 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 530 | Loss: 0.025 | Acc: 99.682%
	I - Batch: 540 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 550 | Loss: 0.025 | Acc: 99.670%
	I - Batch: 560 | Loss: 0.026 | Acc: 99.654%
	I - Batch: 570 | Loss: 0.026 | Acc: 99.649%
	I - Batch: 580 | Loss: 0.026 | Acc: 99.655%
	I - Batch: 590 | Loss: 0.025 | Acc: 99.661%
	I - Batch: 600 | Loss: 0.025 | Acc: 99.667%
	I - Batch: 610 | Loss: 0.025 | Acc: 99.672%
	I - Batch: 620 | Loss: 0.025 | Acc: 99.677%
	I - Batch: 630 | Loss: 0.025 | Acc: 99.673%
	I - Batch: 640 | Loss: 0.025 | Acc: 99.678%
	I - Batch: 650 | Loss: 0.025 | Acc: 99.683%
	I - Batch: 660 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 670 | Loss: 0.025 | Acc: 99.692%
	I - Batch: 680 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 690 | Loss: 0.025 | Acc: 99.692%
	I - Batch: 700 | Loss: 0.025 | Acc: 99.688%
I - num batch: 701
I - Train -- Loss: 0.025 | Acc: 99.688% | LR: 1.250000e-04 | Dur: 551.85s
I - Confusion Matrix: [row->prediction - col->label]
[[1544.    0.    0.    0.    3.]
 [   0.  728.    2.    0.    3.]
 [   0.    3. 1057.    0.   11.]
 [   0.    3.    0. 1502.    1.]
 [   2.    1.    4.    2. 6350.]]

I - Val -- Loss: 2.390 | Acc: 57.809%
I - Confusion Matrix: [row->prediction - col->label]
[[ 323.    6.   18.  100.   64.]
 [   2.   86.   32.   19.   22.]
 [   5.   86.  107.   15.   53.]
 [  77.   18.   25.  249.   57.]
 [ 128.  174.  210.  191. 1019.]]

I - Epoch: 52
	I - Batch: 10 | Loss: 0.019 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 30 | Loss: 0.026 | Acc: 99.375%
	I - Batch: 40 | Loss: 0.027 | Acc: 99.375%
	I - Batch: 50 | Loss: 0.025 | Acc: 99.500%
	I - Batch: 60 | Loss: 0.032 | Acc: 99.375%
	I - Batch: 70 | Loss: 0.031 | Acc: 99.464%
	I - Batch: 80 | Loss: 0.030 | Acc: 99.453%
	I - Batch: 90 | Loss: 0.029 | Acc: 99.514%
	I - Batch: 100 | Loss: 0.028 | Acc: 99.562%
	I - Batch: 110 | Loss: 0.027 | Acc: 99.602%
	I - Batch: 120 | Loss: 0.027 | Acc: 99.583%
	I - Batch: 130 | Loss: 0.026 | Acc: 99.615%
	I - Batch: 140 | Loss: 0.030 | Acc: 99.554%
	I - Batch: 150 | Loss: 0.030 | Acc: 99.583%
	I - Batch: 160 | Loss: 0.029 | Acc: 99.609%
	I - Batch: 170 | Loss: 0.028 | Acc: 99.632%
	I - Batch: 180 | Loss: 0.028 | Acc: 99.653%
	I - Batch: 190 | Loss: 0.027 | Acc: 99.671%
	I - Batch: 200 | Loss: 0.027 | Acc: 99.656%
	I - Batch: 210 | Loss: 0.027 | Acc: 99.673%
	I - Batch: 220 | Loss: 0.026 | Acc: 99.688%
	I - Batch: 230 | Loss: 0.026 | Acc: 99.701%
	I - Batch: 240 | Loss: 0.026 | Acc: 99.714%
	I - Batch: 250 | Loss: 0.026 | Acc: 99.675%
	I - Batch: 260 | Loss: 0.026 | Acc: 99.688%
	I - Batch: 270 | Loss: 0.026 | Acc: 99.699%
	I - Batch: 280 | Loss: 0.026 | Acc: 99.665%
	I - Batch: 290 | Loss: 0.026 | Acc: 99.677%
	I - Batch: 300 | Loss: 0.026 | Acc: 99.688%
	I - Batch: 310 | Loss: 0.026 | Acc: 99.677%
	I - Batch: 320 | Loss: 0.027 | Acc: 99.648%
	I - Batch: 330 | Loss: 0.026 | Acc: 99.659%
	I - Batch: 340 | Loss: 0.026 | Acc: 99.632%
	I - Batch: 350 | Loss: 0.026 | Acc: 99.625%
	I - Batch: 360 | Loss: 0.026 | Acc: 99.635%
	I - Batch: 370 | Loss: 0.026 | Acc: 99.645%
	I - Batch: 380 | Loss: 0.025 | Acc: 99.655%
	I - Batch: 390 | Loss: 0.025 | Acc: 99.663%
	I - Batch: 400 | Loss: 0.025 | Acc: 99.672%
	I - Batch: 410 | Loss: 0.025 | Acc: 99.680%
	I - Batch: 420 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 430 | Loss: 0.024 | Acc: 99.695%
	I - Batch: 440 | Loss: 0.024 | Acc: 99.702%
	I - Batch: 450 | Loss: 0.024 | Acc: 99.694%
	I - Batch: 460 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 470 | Loss: 0.024 | Acc: 99.654%
	I - Batch: 480 | Loss: 0.025 | Acc: 99.648%
	I - Batch: 490 | Loss: 0.025 | Acc: 99.643%
	I - Batch: 500 | Loss: 0.025 | Acc: 99.650%
	I - Batch: 510 | Loss: 0.024 | Acc: 99.657%
	I - Batch: 520 | Loss: 0.024 | Acc: 99.663%
	I - Batch: 530 | Loss: 0.024 | Acc: 99.670%
	I - Batch: 540 | Loss: 0.024 | Acc: 99.653%
	I - Batch: 550 | Loss: 0.025 | Acc: 99.648%
	I - Batch: 560 | Loss: 0.025 | Acc: 99.654%
	I - Batch: 570 | Loss: 0.024 | Acc: 99.660%
	I - Batch: 580 | Loss: 0.025 | Acc: 99.644%
	I - Batch: 590 | Loss: 0.025 | Acc: 99.650%
	I - Batch: 600 | Loss: 0.025 | Acc: 99.646%
	I - Batch: 610 | Loss: 0.025 | Acc: 99.652%
	I - Batch: 620 | Loss: 0.025 | Acc: 99.637%
	I - Batch: 630 | Loss: 0.024 | Acc: 99.643%
	I - Batch: 640 | Loss: 0.024 | Acc: 99.648%
	I - Batch: 650 | Loss: 0.024 | Acc: 99.654%
	I - Batch: 660 | Loss: 0.024 | Acc: 99.659%
	I - Batch: 670 | Loss: 0.024 | Acc: 99.664%
	I - Batch: 680 | Loss: 0.024 | Acc: 99.669%
	I - Batch: 690 | Loss: 0.024 | Acc: 99.674%
	I - Batch: 700 | Loss: 0.024 | Acc: 99.679%
I - num batch: 701
I - Train -- Loss: 0.024 | Acc: 99.679% | LR: 1.250000e-04 | Dur: 580.94s
I - Confusion Matrix: [row->prediction - col->label]
[[1540.    0.    0.    1.    5.]
 [   0.  732.    1.    0.    2.]
 [   0.    2. 1058.    0.   10.]
 [   1.    1.    0. 1502.    3.]
 [   5.    0.    4.    1. 6348.]]

I - Val -- Loss: 2.345 | Acc: 57.129%
I - Confusion Matrix: [row->prediction - col->label]
[[303.   5.  17.  87.  54.]
 [  2.  77.  27.  12.  22.]
 [  4.  95. 109.  16.  60.]
 [ 97.  27.  39. 288.  93.]
 [129. 166. 200. 171. 986.]]

I - Epoch: 53
	I - Batch: 10 | Loss: 0.026 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.027 | Acc: 99.688%
	I - Batch: 30 | Loss: 0.023 | Acc: 99.792%
	I - Batch: 40 | Loss: 0.022 | Acc: 99.844%
	I - Batch: 50 | Loss: 0.022 | Acc: 99.750%
	I - Batch: 60 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 70 | Loss: 0.023 | Acc: 99.554%
	I - Batch: 80 | Loss: 0.022 | Acc: 99.609%
	I - Batch: 90 | Loss: 0.021 | Acc: 99.653%
	I - Batch: 100 | Loss: 0.029 | Acc: 99.562%
	I - Batch: 110 | Loss: 0.029 | Acc: 99.545%
	I - Batch: 120 | Loss: 0.028 | Acc: 99.531%
	I - Batch: 130 | Loss: 0.027 | Acc: 99.567%
	I - Batch: 140 | Loss: 0.027 | Acc: 99.554%
	I - Batch: 150 | Loss: 0.027 | Acc: 99.542%
	I - Batch: 160 | Loss: 0.026 | Acc: 99.531%
	I - Batch: 170 | Loss: 0.030 | Acc: 99.449%
	I - Batch: 180 | Loss: 0.029 | Acc: 99.479%
	I - Batch: 190 | Loss: 0.029 | Acc: 99.474%
	I - Batch: 200 | Loss: 0.030 | Acc: 99.469%
	I - Batch: 210 | Loss: 0.030 | Acc: 99.494%
	I - Batch: 220 | Loss: 0.029 | Acc: 99.517%
	I - Batch: 230 | Loss: 0.029 | Acc: 99.511%
	I - Batch: 240 | Loss: 0.031 | Acc: 99.453%
	I - Batch: 250 | Loss: 0.031 | Acc: 99.450%
	I - Batch: 260 | Loss: 0.031 | Acc: 99.447%
	I - Batch: 270 | Loss: 0.030 | Acc: 99.468%
	I - Batch: 280 | Loss: 0.030 | Acc: 99.487%
	I - Batch: 290 | Loss: 0.029 | Acc: 99.504%
	I - Batch: 300 | Loss: 0.029 | Acc: 99.500%
	I - Batch: 310 | Loss: 0.029 | Acc: 99.516%
	I - Batch: 320 | Loss: 0.028 | Acc: 99.531%
	I - Batch: 330 | Loss: 0.028 | Acc: 99.527%
	I - Batch: 340 | Loss: 0.028 | Acc: 99.522%
	I - Batch: 350 | Loss: 0.028 | Acc: 99.518%
	I - Batch: 360 | Loss: 0.028 | Acc: 99.531%
	I - Batch: 370 | Loss: 0.027 | Acc: 99.544%
	I - Batch: 380 | Loss: 0.027 | Acc: 99.556%
	I - Batch: 390 | Loss: 0.028 | Acc: 99.535%
	I - Batch: 400 | Loss: 0.028 | Acc: 99.547%
	I - Batch: 410 | Loss: 0.028 | Acc: 99.527%
	I - Batch: 420 | Loss: 0.028 | Acc: 99.524%
	I - Batch: 430 | Loss: 0.028 | Acc: 99.520%
	I - Batch: 440 | Loss: 0.028 | Acc: 99.531%
	I - Batch: 450 | Loss: 0.028 | Acc: 99.542%
	I - Batch: 460 | Loss: 0.027 | Acc: 99.552%
	I - Batch: 470 | Loss: 0.027 | Acc: 99.561%
	I - Batch: 480 | Loss: 0.027 | Acc: 99.570%
	I - Batch: 490 | Loss: 0.027 | Acc: 99.579%
	I - Batch: 500 | Loss: 0.027 | Acc: 99.562%
	I - Batch: 510 | Loss: 0.027 | Acc: 99.571%
	I - Batch: 520 | Loss: 0.027 | Acc: 99.567%
	I - Batch: 530 | Loss: 0.026 | Acc: 99.575%
	I - Batch: 540 | Loss: 0.026 | Acc: 99.560%
	I - Batch: 550 | Loss: 0.026 | Acc: 99.568%
	I - Batch: 560 | Loss: 0.026 | Acc: 99.565%
	I - Batch: 570 | Loss: 0.026 | Acc: 99.572%
	I - Batch: 580 | Loss: 0.026 | Acc: 99.580%
	I - Batch: 590 | Loss: 0.026 | Acc: 99.576%
	I - Batch: 600 | Loss: 0.026 | Acc: 99.583%
	I - Batch: 610 | Loss: 0.025 | Acc: 99.590%
	I - Batch: 620 | Loss: 0.025 | Acc: 99.597%
	I - Batch: 630 | Loss: 0.025 | Acc: 99.603%
	I - Batch: 640 | Loss: 0.025 | Acc: 99.600%
	I - Batch: 650 | Loss: 0.025 | Acc: 99.596%
	I - Batch: 660 | Loss: 0.025 | Acc: 99.593%
	I - Batch: 670 | Loss: 0.025 | Acc: 99.590%
	I - Batch: 680 | Loss: 0.025 | Acc: 99.586%
	I - Batch: 690 | Loss: 0.025 | Acc: 99.592%
	I - Batch: 700 | Loss: 0.025 | Acc: 99.598%
I - num batch: 701
I - Train -- Loss: 0.025 | Acc: 99.599% | LR: 1.250000e-04 | Dur: 576.08s
I - Confusion Matrix: [row->prediction - col->label]
[[1543.    0.    0.    0.   11.]
 [   0.  732.    1.    0.    0.]
 [   0.    2. 1055.    0.   10.]
 [   0.    0.    0. 1500.    6.]
 [   3.    1.    7.    4. 6341.]]

I - Val -- Loss: 2.449 | Acc: 57.226%
I - Confusion Matrix: [row->prediction - col->label]
[[ 325.    6.   18.  118.   60.]
 [   4.   72.   23.    9.   14.]
 [   5.   87.  115.   21.   59.]
 [  66.   22.   26.  222.   50.]
 [ 135.  183.  210.  204. 1032.]]

I - Epoch: 54
	I - Batch: 10 | Loss: 0.014 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.015 | Acc: 100.000%
	I - Batch: 30 | Loss: 0.015 | Acc: 100.000%
	I - Batch: 40 | Loss: 0.031 | Acc: 99.688%
	I - Batch: 50 | Loss: 0.028 | Acc: 99.625%
	I - Batch: 60 | Loss: 0.027 | Acc: 99.688%
	I - Batch: 70 | Loss: 0.026 | Acc: 99.732%
	I - Batch: 80 | Loss: 0.025 | Acc: 99.766%
	I - Batch: 90 | Loss: 0.026 | Acc: 99.583%
	I - Batch: 100 | Loss: 0.025 | Acc: 99.625%
	I - Batch: 110 | Loss: 0.024 | Acc: 99.659%
	I - Batch: 120 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 130 | Loss: 0.023 | Acc: 99.712%
	I - Batch: 140 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 150 | Loss: 0.023 | Acc: 99.708%
	I - Batch: 160 | Loss: 0.023 | Acc: 99.727%
	I - Batch: 170 | Loss: 0.023 | Acc: 99.743%
	I - Batch: 180 | Loss: 0.023 | Acc: 99.757%
	I - Batch: 190 | Loss: 0.023 | Acc: 99.737%
	I - Batch: 200 | Loss: 0.022 | Acc: 99.750%
	I - Batch: 210 | Loss: 0.022 | Acc: 99.762%
	I - Batch: 220 | Loss: 0.022 | Acc: 99.773%
	I - Batch: 230 | Loss: 0.022 | Acc: 99.783%
	I - Batch: 240 | Loss: 0.021 | Acc: 99.792%
	I - Batch: 250 | Loss: 0.022 | Acc: 99.775%
	I - Batch: 260 | Loss: 0.021 | Acc: 99.784%
	I - Batch: 270 | Loss: 0.021 | Acc: 99.792%
	I - Batch: 280 | Loss: 0.021 | Acc: 99.777%
	I - Batch: 290 | Loss: 0.022 | Acc: 99.720%
	I - Batch: 300 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 310 | Loss: 0.022 | Acc: 99.698%
	I - Batch: 320 | Loss: 0.022 | Acc: 99.688%
	I - Batch: 330 | Loss: 0.023 | Acc: 99.697%
	I - Batch: 340 | Loss: 0.022 | Acc: 99.706%
	I - Batch: 350 | Loss: 0.022 | Acc: 99.696%
	I - Batch: 360 | Loss: 0.022 | Acc: 99.705%
	I - Batch: 370 | Loss: 0.022 | Acc: 99.713%
	I - Batch: 380 | Loss: 0.022 | Acc: 99.720%
	I - Batch: 390 | Loss: 0.022 | Acc: 99.728%
	I - Batch: 400 | Loss: 0.023 | Acc: 99.703%
	I - Batch: 410 | Loss: 0.024 | Acc: 99.680%
	I - Batch: 420 | Loss: 0.024 | Acc: 99.673%
	I - Batch: 430 | Loss: 0.024 | Acc: 99.680%
	I - Batch: 440 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 450 | Loss: 0.024 | Acc: 99.681%
	I - Batch: 460 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 470 | Loss: 0.024 | Acc: 99.694%
	I - Batch: 480 | Loss: 0.024 | Acc: 99.701%
	I - Batch: 490 | Loss: 0.024 | Acc: 99.694%
	I - Batch: 500 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 510 | Loss: 0.024 | Acc: 99.694%
	I - Batch: 520 | Loss: 0.024 | Acc: 99.675%
	I - Batch: 530 | Loss: 0.023 | Acc: 99.682%
	I - Batch: 540 | Loss: 0.024 | Acc: 99.664%
	I - Batch: 550 | Loss: 0.024 | Acc: 99.670%
	I - Batch: 560 | Loss: 0.023 | Acc: 99.676%
	I - Batch: 570 | Loss: 0.024 | Acc: 99.638%
	I - Batch: 580 | Loss: 0.024 | Acc: 99.644%
	I - Batch: 590 | Loss: 0.023 | Acc: 99.650%
	I - Batch: 600 | Loss: 0.024 | Acc: 99.646%
	I - Batch: 610 | Loss: 0.023 | Acc: 99.652%
	I - Batch: 620 | Loss: 0.023 | Acc: 99.657%
	I - Batch: 630 | Loss: 0.024 | Acc: 99.653%
	I - Batch: 640 | Loss: 0.024 | Acc: 99.639%
	I - Batch: 650 | Loss: 0.024 | Acc: 99.644%
	I - Batch: 660 | Loss: 0.024 | Acc: 99.650%
	I - Batch: 670 | Loss: 0.023 | Acc: 99.655%
	I - Batch: 680 | Loss: 0.023 | Acc: 99.660%
	I - Batch: 690 | Loss: 0.024 | Acc: 99.647%
	I - Batch: 700 | Loss: 0.023 | Acc: 99.652%
I - num batch: 701
I - Train -- Loss: 0.023 | Acc: 99.652% | LR: 1.250000e-04 | Dur: 571.96s
I - Confusion Matrix: [row->prediction - col->label]
[[1544.    0.    0.    0.    7.]
 [   0.  731.    0.    0.    0.]
 [   0.    3. 1058.    0.   12.]
 [   1.    0.    0. 1503.    8.]
 [   1.    1.    5.    1. 6341.]]

I - Val -- Loss: 2.484 | Acc: 57.064%
I - Confusion Matrix: [row->prediction - col->label]
[[ 302.    7.   20.   94.   44.]
 [   2.   73.   24.    9.   16.]
 [   4.   81.  103.   10.   43.]
 [  65.   24.   28.  223.   52.]
 [ 162.  185.  217.  238. 1060.]]

I - Epoch: 55
	I - Batch: 10 | Loss: 0.013 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.018 | Acc: 99.688%
	I - Batch: 30 | Loss: 0.019 | Acc: 99.583%
	I - Batch: 40 | Loss: 0.022 | Acc: 99.531%
	I - Batch: 50 | Loss: 0.036 | Acc: 99.375%
	I - Batch: 60 | Loss: 0.034 | Acc: 99.375%
	I - Batch: 70 | Loss: 0.033 | Acc: 99.375%
	I - Batch: 80 | Loss: 0.031 | Acc: 99.453%
	I - Batch: 90 | Loss: 0.030 | Acc: 99.514%
	I - Batch: 100 | Loss: 0.028 | Acc: 99.562%
	I - Batch: 110 | Loss: 0.027 | Acc: 99.602%
	I - Batch: 120 | Loss: 0.026 | Acc: 99.635%
	I - Batch: 130 | Loss: 0.025 | Acc: 99.663%
	I - Batch: 140 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 150 | Loss: 0.024 | Acc: 99.708%
	I - Batch: 160 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 170 | Loss: 0.023 | Acc: 99.706%
	I - Batch: 180 | Loss: 0.023 | Acc: 99.722%
	I - Batch: 190 | Loss: 0.023 | Acc: 99.737%
	I - Batch: 200 | Loss: 0.022 | Acc: 99.750%
	I - Batch: 210 | Loss: 0.023 | Acc: 99.732%
	I - Batch: 220 | Loss: 0.023 | Acc: 99.716%
	I - Batch: 230 | Loss: 0.024 | Acc: 99.674%
	I - Batch: 240 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 250 | Loss: 0.023 | Acc: 99.700%
	I - Batch: 260 | Loss: 0.023 | Acc: 99.712%
	I - Batch: 270 | Loss: 0.023 | Acc: 99.699%
	I - Batch: 280 | Loss: 0.023 | Acc: 99.710%
	I - Batch: 290 | Loss: 0.024 | Acc: 99.698%
	I - Batch: 300 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 310 | Loss: 0.024 | Acc: 99.698%
	I - Batch: 320 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 330 | Loss: 0.024 | Acc: 99.697%
	I - Batch: 340 | Loss: 0.024 | Acc: 99.706%
	I - Batch: 350 | Loss: 0.023 | Acc: 99.714%
	I - Batch: 360 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 370 | Loss: 0.024 | Acc: 99.679%
	I - Batch: 380 | Loss: 0.024 | Acc: 99.671%
	I - Batch: 390 | Loss: 0.024 | Acc: 99.679%
	I - Batch: 400 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 410 | Loss: 0.023 | Acc: 99.695%
	I - Batch: 420 | Loss: 0.023 | Acc: 99.702%
	I - Batch: 430 | Loss: 0.023 | Acc: 99.695%
	I - Batch: 440 | Loss: 0.023 | Acc: 99.702%
	I - Batch: 450 | Loss: 0.023 | Acc: 99.708%
	I - Batch: 460 | Loss: 0.023 | Acc: 99.715%
	I - Batch: 470 | Loss: 0.023 | Acc: 99.721%
	I - Batch: 480 | Loss: 0.023 | Acc: 99.714%
	I - Batch: 490 | Loss: 0.023 | Acc: 99.719%
	I - Batch: 500 | Loss: 0.023 | Acc: 99.713%
	I - Batch: 510 | Loss: 0.023 | Acc: 99.718%
	I - Batch: 520 | Loss: 0.023 | Acc: 99.724%
	I - Batch: 530 | Loss: 0.024 | Acc: 99.705%
	I - Batch: 540 | Loss: 0.023 | Acc: 99.711%
	I - Batch: 550 | Loss: 0.024 | Acc: 99.705%
	I - Batch: 560 | Loss: 0.024 | Acc: 99.710%
	I - Batch: 570 | Loss: 0.024 | Acc: 99.704%
	I - Batch: 580 | Loss: 0.024 | Acc: 99.709%
	I - Batch: 590 | Loss: 0.024 | Acc: 99.682%
	I - Batch: 600 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 610 | Loss: 0.024 | Acc: 99.693%
	I - Batch: 620 | Loss: 0.024 | Acc: 99.698%
	I - Batch: 630 | Loss: 0.024 | Acc: 99.683%
	I - Batch: 640 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 650 | Loss: 0.024 | Acc: 99.692%
	I - Batch: 660 | Loss: 0.024 | Acc: 99.697%
	I - Batch: 670 | Loss: 0.024 | Acc: 99.692%
	I - Batch: 680 | Loss: 0.024 | Acc: 99.678%
	I - Batch: 690 | Loss: 0.024 | Acc: 99.683%
	I - Batch: 700 | Loss: 0.024 | Acc: 99.679%
I - num batch: 701
I - Train -- Loss: 0.024 | Acc: 99.679% | LR: 1.250000e-04 | Dur: 557.76s
I - Confusion Matrix: [row->prediction - col->label]
[[1544.    0.    0.    0.    5.]
 [   0.  728.    4.    0.    1.]
 [   0.    3. 1055.    0.    6.]
 [   0.    1.    0. 1502.    5.]
 [   2.    3.    4.    2. 6351.]]

I - Val -- Loss: 2.561 | Acc: 55.703%
I - Confusion Matrix: [row->prediction - col->label]
[[ 297.    8.   16.  101.   52.]
 [   4.   72.   26.   20.   16.]
 [   8.  112.  117.   20.   70.]
 [  61.   12.   22.  190.   34.]
 [ 165.  166.  211.  243. 1043.]]

I - Epoch: 56
	I - Batch: 10 | Loss: 0.028 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.022 | Acc: 100.000%
	I - Batch: 30 | Loss: 0.022 | Acc: 99.792%
	I - Batch: 40 | Loss: 0.031 | Acc: 99.688%
	I - Batch: 50 | Loss: 0.030 | Acc: 99.625%
	I - Batch: 60 | Loss: 0.029 | Acc: 99.688%
	I - Batch: 70 | Loss: 0.026 | Acc: 99.732%
	I - Batch: 80 | Loss: 0.025 | Acc: 99.766%
	I - Batch: 90 | Loss: 0.024 | Acc: 99.792%
	I - Batch: 100 | Loss: 0.023 | Acc: 99.812%
	I - Batch: 110 | Loss: 0.023 | Acc: 99.830%
	I - Batch: 120 | Loss: 0.022 | Acc: 99.844%
	I - Batch: 130 | Loss: 0.021 | Acc: 99.856%
	I - Batch: 140 | Loss: 0.021 | Acc: 99.866%
	I - Batch: 150 | Loss: 0.021 | Acc: 99.875%
	I - Batch: 160 | Loss: 0.020 | Acc: 99.883%
	I - Batch: 170 | Loss: 0.020 | Acc: 99.890%
	I - Batch: 180 | Loss: 0.021 | Acc: 99.861%
	I - Batch: 190 | Loss: 0.023 | Acc: 99.803%
	I - Batch: 200 | Loss: 0.023 | Acc: 99.812%
	I - Batch: 210 | Loss: 0.022 | Acc: 99.821%
	I - Batch: 220 | Loss: 0.023 | Acc: 99.801%
	I - Batch: 230 | Loss: 0.024 | Acc: 99.783%
	I - Batch: 240 | Loss: 0.024 | Acc: 99.792%
	I - Batch: 250 | Loss: 0.023 | Acc: 99.800%
	I - Batch: 260 | Loss: 0.023 | Acc: 99.808%
	I - Batch: 270 | Loss: 0.023 | Acc: 99.815%
	I - Batch: 280 | Loss: 0.023 | Acc: 99.799%
	I - Batch: 290 | Loss: 0.023 | Acc: 99.784%
	I - Batch: 300 | Loss: 0.023 | Acc: 99.792%
	I - Batch: 310 | Loss: 0.022 | Acc: 99.798%
	I - Batch: 320 | Loss: 0.022 | Acc: 99.805%
	I - Batch: 330 | Loss: 0.023 | Acc: 99.792%
	I - Batch: 340 | Loss: 0.023 | Acc: 99.798%
	I - Batch: 350 | Loss: 0.023 | Acc: 99.804%
	I - Batch: 360 | Loss: 0.023 | Acc: 99.809%
	I - Batch: 370 | Loss: 0.023 | Acc: 99.780%
	I - Batch: 380 | Loss: 0.023 | Acc: 99.770%
	I - Batch: 390 | Loss: 0.023 | Acc: 99.776%
	I - Batch: 400 | Loss: 0.022 | Acc: 99.781%
	I - Batch: 410 | Loss: 0.022 | Acc: 99.787%
	I - Batch: 420 | Loss: 0.023 | Acc: 99.747%
	I - Batch: 430 | Loss: 0.023 | Acc: 99.753%
	I - Batch: 440 | Loss: 0.023 | Acc: 99.759%
	I - Batch: 450 | Loss: 0.023 | Acc: 99.764%
	I - Batch: 460 | Loss: 0.023 | Acc: 99.769%
	I - Batch: 470 | Loss: 0.022 | Acc: 99.774%
	I - Batch: 480 | Loss: 0.022 | Acc: 99.779%
	I - Batch: 490 | Loss: 0.022 | Acc: 99.783%
	I - Batch: 500 | Loss: 0.022 | Acc: 99.787%
	I - Batch: 510 | Loss: 0.022 | Acc: 99.792%
	I - Batch: 520 | Loss: 0.024 | Acc: 99.772%
	I - Batch: 530 | Loss: 0.023 | Acc: 99.776%
	I - Batch: 540 | Loss: 0.023 | Acc: 99.780%
	I - Batch: 550 | Loss: 0.023 | Acc: 99.784%
	I - Batch: 560 | Loss: 0.024 | Acc: 99.777%
	I - Batch: 570 | Loss: 0.023 | Acc: 99.781%
	I - Batch: 580 | Loss: 0.023 | Acc: 99.774%
	I - Batch: 590 | Loss: 0.023 | Acc: 99.778%
	I - Batch: 600 | Loss: 0.024 | Acc: 99.771%
	I - Batch: 610 | Loss: 0.024 | Acc: 99.764%
	I - Batch: 620 | Loss: 0.024 | Acc: 99.768%
	I - Batch: 630 | Loss: 0.024 | Acc: 99.762%
	I - Batch: 640 | Loss: 0.024 | Acc: 99.766%
	I - Batch: 650 | Loss: 0.024 | Acc: 99.769%
	I - Batch: 660 | Loss: 0.024 | Acc: 99.773%
	I - Batch: 670 | Loss: 0.023 | Acc: 99.776%
	I - Batch: 680 | Loss: 0.023 | Acc: 99.779%
	I - Batch: 690 | Loss: 0.023 | Acc: 99.783%
	I - Batch: 700 | Loss: 0.023 | Acc: 99.786%
I - num batch: 701
I - Train -- Loss: 0.023 | Acc: 99.786% | LR: 1.250000e-04 | Dur: 555.44s
I - Confusion Matrix: [row->prediction - col->label]
[[1544.    0.    0.    0.    5.]
 [   0.  731.    0.    0.    1.]
 [   0.    3. 1061.    0.    5.]
 [   0.    0.    0. 1503.    4.]
 [   2.    1.    2.    1. 6353.]]

I - Val -- Loss: 2.429 | Acc: 56.578%
I - Confusion Matrix: [row->prediction - col->label]
[[313.   6.  20. 106.  64.]
 [  5.  99.  36.  22.  29.]
 [  8.  95. 125.  18.  75.]
 [ 68.  18.  24. 214.  52.]
 [141. 152. 187. 214. 995.]]

I - Epoch: 57
	I - Batch: 10 | Loss: 0.027 | Acc: 98.750%
	I - Batch: 20 | Loss: 0.023 | Acc: 99.375%
	I - Batch: 30 | Loss: 0.019 | Acc: 99.583%
	I - Batch: 40 | Loss: 0.022 | Acc: 99.688%
	I - Batch: 50 | Loss: 0.020 | Acc: 99.750%
	I - Batch: 60 | Loss: 0.020 | Acc: 99.792%
	I - Batch: 70 | Loss: 0.024 | Acc: 99.464%
	I - Batch: 80 | Loss: 0.023 | Acc: 99.531%
	I - Batch: 90 | Loss: 0.022 | Acc: 99.583%
	I - Batch: 100 | Loss: 0.022 | Acc: 99.625%
	I - Batch: 110 | Loss: 0.022 | Acc: 99.659%
	I - Batch: 120 | Loss: 0.022 | Acc: 99.688%
	I - Batch: 130 | Loss: 0.021 | Acc: 99.712%
	I - Batch: 140 | Loss: 0.022 | Acc: 99.688%
	I - Batch: 150 | Loss: 0.022 | Acc: 99.708%
	I - Batch: 160 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 170 | Loss: 0.024 | Acc: 99.706%
	I - Batch: 180 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 190 | Loss: 0.025 | Acc: 99.704%
	I - Batch: 200 | Loss: 0.024 | Acc: 99.719%
	I - Batch: 210 | Loss: 0.025 | Acc: 99.702%
	I - Batch: 220 | Loss: 0.024 | Acc: 99.716%
	I - Batch: 230 | Loss: 0.024 | Acc: 99.728%
	I - Batch: 240 | Loss: 0.023 | Acc: 99.740%
	I - Batch: 250 | Loss: 0.023 | Acc: 99.750%
	I - Batch: 260 | Loss: 0.023 | Acc: 99.736%
	I - Batch: 270 | Loss: 0.023 | Acc: 99.745%
	I - Batch: 280 | Loss: 0.023 | Acc: 99.754%
	I - Batch: 290 | Loss: 0.023 | Acc: 99.763%
	I - Batch: 300 | Loss: 0.023 | Acc: 99.750%
	I - Batch: 310 | Loss: 0.023 | Acc: 99.738%
	I - Batch: 320 | Loss: 0.023 | Acc: 99.746%
	I - Batch: 330 | Loss: 0.022 | Acc: 99.754%
	I - Batch: 340 | Loss: 0.022 | Acc: 99.761%
	I - Batch: 350 | Loss: 0.022 | Acc: 99.768%
	I - Batch: 360 | Loss: 0.022 | Acc: 99.757%
	I - Batch: 370 | Loss: 0.022 | Acc: 99.764%
	I - Batch: 380 | Loss: 0.021 | Acc: 99.770%
	I - Batch: 390 | Loss: 0.021 | Acc: 99.760%
	I - Batch: 400 | Loss: 0.021 | Acc: 99.766%
	I - Batch: 410 | Loss: 0.021 | Acc: 99.771%
	I - Batch: 420 | Loss: 0.021 | Acc: 99.762%
	I - Batch: 430 | Loss: 0.022 | Acc: 99.709%
	I - Batch: 440 | Loss: 0.022 | Acc: 99.716%
	I - Batch: 450 | Loss: 0.022 | Acc: 99.722%
	I - Batch: 460 | Loss: 0.022 | Acc: 99.701%
	I - Batch: 470 | Loss: 0.022 | Acc: 99.694%
	I - Batch: 480 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 490 | Loss: 0.023 | Acc: 99.681%
	I - Batch: 500 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 510 | Loss: 0.023 | Acc: 99.694%
	I - Batch: 520 | Loss: 0.022 | Acc: 99.700%
	I - Batch: 530 | Loss: 0.022 | Acc: 99.705%
	I - Batch: 540 | Loss: 0.022 | Acc: 99.711%
	I - Batch: 550 | Loss: 0.022 | Acc: 99.716%
	I - Batch: 560 | Loss: 0.022 | Acc: 99.721%
	I - Batch: 570 | Loss: 0.022 | Acc: 99.715%
	I - Batch: 580 | Loss: 0.023 | Acc: 99.709%
	I - Batch: 590 | Loss: 0.023 | Acc: 99.703%
	I - Batch: 600 | Loss: 0.022 | Acc: 99.698%
	I - Batch: 610 | Loss: 0.023 | Acc: 99.693%
	I - Batch: 620 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 630 | Loss: 0.023 | Acc: 99.683%
	I - Batch: 640 | Loss: 0.023 | Acc: 99.688%
	I - Batch: 650 | Loss: 0.023 | Acc: 99.683%
	I - Batch: 660 | Loss: 0.024 | Acc: 99.669%
	I - Batch: 670 | Loss: 0.024 | Acc: 99.655%
	I - Batch: 680 | Loss: 0.024 | Acc: 99.660%
	I - Batch: 690 | Loss: 0.024 | Acc: 99.665%
	I - Batch: 700 | Loss: 0.024 | Acc: 99.670%
I - num batch: 701
I - Train -- Loss: 0.024 | Acc: 99.670% | LR: 1.250000e-04 | Dur: 556.56s
I - Confusion Matrix: [row->prediction - col->label]
[[1545.    0.    0.    0.    6.]
 [   0.  728.    4.    0.    1.]
 [   0.    4. 1055.    0.    9.]
 [   1.    2.    0. 1502.    3.]
 [   0.    1.    4.    2. 6349.]]

I - Val -- Loss: 2.644 | Acc: 55.541%
I - Confusion Matrix: [row->prediction - col->label]
[[ 291.    5.   12.   90.   41.]
 [   4.   68.   19.   20.   17.]
 [   7.  100.  119.   25.   67.]
 [  63.   13.   25.  185.   39.]
 [ 170.  184.  217.  254. 1051.]]

I - Epoch: 58
	I - Batch: 10 | Loss: 0.026 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.019 | Acc: 100.000%
	I - Batch: 30 | Loss: 0.017 | Acc: 100.000%
	I - Batch: 40 | Loss: 0.016 | Acc: 99.844%
	I - Batch: 50 | Loss: 0.022 | Acc: 99.750%
	I - Batch: 60 | Loss: 0.021 | Acc: 99.792%
	I - Batch: 70 | Loss: 0.023 | Acc: 99.732%
	I - Batch: 80 | Loss: 0.025 | Acc: 99.688%
	I - Batch: 90 | Loss: 0.026 | Acc: 99.583%
	I - Batch: 100 | Loss: 0.026 | Acc: 99.562%
	I - Batch: 110 | Loss: 0.025 | Acc: 99.602%
	I - Batch: 120 | Loss: 0.025 | Acc: 99.635%
	I - Batch: 130 | Loss: 0.024 | Acc: 99.663%
	I - Batch: 140 | Loss: 0.024 | Acc: 99.688%
	I - Batch: 150 | Loss: 0.023 | Acc: 99.708%
	I - Batch: 160 | Loss: 0.023 | Acc: 99.727%
	I - Batch: 170 | Loss: 0.023 | Acc: 99.743%
	I - Batch: 180 | Loss: 0.023 | Acc: 99.722%
	I - Batch: 190 | Loss: 0.023 | Acc: 99.737%
	I - Batch: 200 | Loss: 0.023 | Acc: 99.750%
	I - Batch: 210 | Loss: 0.022 | Acc: 99.762%
	I - Batch: 220 | Loss: 0.022 | Acc: 99.773%
	I - Batch: 230 | Loss: 0.022 | Acc: 99.783%
	I - Batch: 240 | Loss: 0.021 | Acc: 99.792%
	I - Batch: 250 | Loss: 0.021 | Acc: 99.800%
	I - Batch: 260 | Loss: 0.026 | Acc: 99.712%
	I - Batch: 270 | Loss: 0.025 | Acc: 99.722%
	I - Batch: 280 | Loss: 0.026 | Acc: 99.688%
	I - Batch: 290 | Loss: 0.026 | Acc: 99.698%
	I - Batch: 300 | Loss: 0.025 | Acc: 99.708%
	I - Batch: 310 | Loss: 0.025 | Acc: 99.718%
	I - Batch: 320 | Loss: 0.025 | Acc: 99.707%
	I - Batch: 330 | Loss: 0.025 | Acc: 99.678%
	I - Batch: 340 | Loss: 0.025 | Acc: 99.651%
	I - Batch: 350 | Loss: 0.026 | Acc: 99.643%
	I - Batch: 360 | Loss: 0.025 | Acc: 99.635%
	I - Batch: 370 | Loss: 0.025 | Acc: 99.611%
	I - Batch: 380 | Loss: 0.026 | Acc: 99.589%
	I - Batch: 390 | Loss: 0.026 | Acc: 99.583%
	I - Batch: 400 | Loss: 0.026 | Acc: 99.578%
	I - Batch: 410 | Loss: 0.026 | Acc: 99.558%
	I - Batch: 420 | Loss: 0.026 | Acc: 99.568%
	I - Batch: 430 | Loss: 0.026 | Acc: 99.578%
	I - Batch: 440 | Loss: 0.025 | Acc: 99.588%
	I - Batch: 450 | Loss: 0.025 | Acc: 99.597%
	I - Batch: 460 | Loss: 0.025 | Acc: 99.579%
	I - Batch: 470 | Loss: 0.025 | Acc: 99.588%
	I - Batch: 480 | Loss: 0.025 | Acc: 99.583%
	I - Batch: 490 | Loss: 0.025 | Acc: 99.592%
	I - Batch: 500 | Loss: 0.025 | Acc: 99.600%
	I - Batch: 510 | Loss: 0.024 | Acc: 99.608%
	I - Batch: 520 | Loss: 0.024 | Acc: 99.603%
	I - Batch: 530 | Loss: 0.024 | Acc: 99.611%
	I - Batch: 540 | Loss: 0.024 | Acc: 99.618%
	I - Batch: 550 | Loss: 0.024 | Acc: 99.602%
	I - Batch: 560 | Loss: 0.024 | Acc: 99.609%
	I - Batch: 570 | Loss: 0.024 | Acc: 99.605%
	I - Batch: 580 | Loss: 0.026 | Acc: 99.569%
	I - Batch: 590 | Loss: 0.026 | Acc: 99.576%
	I - Batch: 600 | Loss: 0.026 | Acc: 99.573%
	I - Batch: 610 | Loss: 0.026 | Acc: 99.570%
	I - Batch: 620 | Loss: 0.026 | Acc: 99.567%
	I - Batch: 630 | Loss: 0.025 | Acc: 99.573%
	I - Batch: 640 | Loss: 0.025 | Acc: 99.580%
	I - Batch: 650 | Loss: 0.025 | Acc: 99.587%
	I - Batch: 660 | Loss: 0.025 | Acc: 99.593%
	I - Batch: 670 | Loss: 0.025 | Acc: 99.599%
	I - Batch: 680 | Loss: 0.025 | Acc: 99.605%
	I - Batch: 690 | Loss: 0.025 | Acc: 99.611%
	I - Batch: 700 | Loss: 0.025 | Acc: 99.616%
I - num batch: 701
I - Train -- Loss: 0.024 | Acc: 99.617% | LR: 1.250000e-04 | Dur: 573.95s
I - Confusion Matrix: [row->prediction - col->label]
[[1543.    0.    0.    0.    5.]
 [   0.  730.    1.    0.    2.]
 [   0.    2. 1055.    0.    9.]
 [   0.    1.    0. 1501.    8.]
 [   3.    2.    7.    3. 6344.]]

I - Val -- Loss: 2.571 | Acc: 55.962%
I - Confusion Matrix: [row->prediction - col->label]
[[ 289.    7.   12.   85.   40.]
 [   5.   72.   20.   12.   20.]
 [   5.  106.  127.   27.   78.]
 [  79.   18.   32.  211.   49.]
 [ 157.  167.  201.  239. 1028.]]

I - Epoch: 59
	I - Batch: 10 | Loss: 0.014 | Acc: 100.000%
	I - Batch: 20 | Loss: 0.014 | Acc: 100.000%
	I - Batch: 30 | Loss: 0.016 | Acc: 100.000%
	I - Batch: 40 | Loss: 0.017 | Acc: 99.844%
	I - Batch: 50 | Loss: 0.016 | Acc: 99.875%
	I - Batch: 60 | Loss: 0.016 | Acc: 99.896%
	I - Batch: 70 | Loss: 0.016 | Acc: 99.911%
	I - Batch: 80 | Loss: 0.016 | Acc: 99.922%
	I - Batch: 90 | Loss: 0.016 | Acc: 99.931%
	I - Batch: 100 | Loss: 0.016 | Acc: 99.875%
	I - Batch: 110 | Loss: 0.016 | Acc: 99.886%
	I - Batch: 120 | Loss: 0.016 | Acc: 99.896%
	I - Batch: 130 | Loss: 0.016 | Acc: 99.856%
	I - Batch: 140 | Loss: 0.016 | Acc: 99.866%
	I - Batch: 150 | Loss: 0.017 | Acc: 99.833%
	I - Batch: 160 | Loss: 0.019 | Acc: 99.766%
	I - Batch: 170 | Loss: 0.018 | Acc: 99.779%
	I - Batch: 180 | Loss: 0.018 | Acc: 99.792%
	I - Batch: 190 | Loss: 0.019 | Acc: 99.770%
	I - Batch: 200 | Loss: 0.019 | Acc: 99.750%
	I - Batch: 210 | Loss: 0.019 | Acc: 99.762%
	I - Batch: 220 | Loss: 0.020 | Acc: 99.744%
	I - Batch: 230 | Loss: 0.020 | Acc: 99.755%
	I - Batch: 240 | Loss: 0.020 | Acc: 99.714%
	I - Batch: 250 | Loss: 0.021 | Acc: 99.675%
	I - Batch: 260 | Loss: 0.023 | Acc: 99.639%
	I - Batch: 270 | Loss: 0.023 | Acc: 99.653%
	I - Batch: 280 | Loss: 0.023 | Acc: 99.643%
	I - Batch: 290 | Loss: 0.023 | Acc: 99.634%
	I - Batch: 300 | Loss: 0.023 | Acc: 99.646%
	I - Batch: 310 | Loss: 0.022 | Acc: 99.657%
	I - Batch: 320 | Loss: 0.022 | Acc: 99.668%
	I - Batch: 330 | Loss: 0.022 | Acc: 99.659%
	I - Batch: 340 | Loss: 0.022 | Acc: 99.651%
	I - Batch: 350 | Loss: 0.022 | Acc: 99.661%
	I - Batch: 360 | Loss: 0.022 | Acc: 99.670%
	I - Batch: 370 | Loss: 0.022 | Acc: 99.679%
	I - Batch: 380 | Loss: 0.022 | Acc: 99.671%
	I - Batch: 390 | Loss: 0.023 | Acc: 99.663%
	I - Batch: 400 | Loss: 0.022 | Acc: 99.672%
	I - Batch: 410 | Loss: 0.022 | Acc: 99.680%
	I - Batch: 420 | Loss: 0.022 | Acc: 99.673%
	I - Batch: 430 | Loss: 0.022 | Acc: 99.666%
	I - Batch: 440 | Loss: 0.022 | Acc: 99.673%
	I - Batch: 450 | Loss: 0.022 | Acc: 99.681%
	I - Batch: 460 | Loss: 0.022 | Acc: 99.688%
	I - Batch: 470 | Loss: 0.022 | Acc: 99.694%
	I - Batch: 480 | Loss: 0.022 | Acc: 99.701%
	I - Batch: 490 | Loss: 0.021 | Acc: 99.707%
	I - Batch: 500 | Loss: 0.021 | Acc: 99.713%
	I - Batch: 510 | Loss: 0.021 | Acc: 99.718%
	I - Batch: 520 | Loss: 0.021 | Acc: 99.724%
	I - Batch: 530 | Loss: 0.021 | Acc: 99.717%
	I - Batch: 540 | Loss: 0.021 | Acc: 99.722%
	I - Batch: 550 | Loss: 0.021 | Acc: 99.705%
	I - Batch: 560 | Loss: 0.021 | Acc: 99.710%
	I - Batch: 570 | Loss: 0.021 | Acc: 99.715%
	I - Batch: 580 | Loss: 0.021 | Acc: 99.720%
	I - Batch: 590 | Loss: 0.020 | Acc: 99.725%
	I - Batch: 600 | Loss: 0.020 | Acc: 99.729%
	I - Batch: 610 | Loss: 0.020 | Acc: 99.734%
	I - Batch: 620 | Loss: 0.020 | Acc: 99.738%
	I - Batch: 630 | Loss: 0.020 | Acc: 99.732%
	I - Batch: 640 | Loss: 0.020 | Acc: 99.736%
	I - Batch: 650 | Loss: 0.020 | Acc: 99.740%
	I - Batch: 660 | Loss: 0.020 | Acc: 99.735%
	I - Batch: 670 | Loss: 0.020 | Acc: 99.739%
	I - Batch: 680 | Loss: 0.020 | Acc: 99.733%
	I - Batch: 690 | Loss: 0.020 | Acc: 99.737%
	I - Batch: 700 | Loss: 0.020 | Acc: 99.741%
I - num batch: 701
I - Train -- Loss: 0.020 | Acc: 99.741% | LR: 1.250000e-04 | Dur: 580.99s
I - Confusion Matrix: [row->prediction - col->label]
[[1543.    0.    0.    0.    5.]
 [   0.  733.    1.    0.    1.]
 [   0.    1. 1058.    0.    9.]
 [   0.    1.    0. 1503.    3.]
 [   3.    0.    4.    1. 6350.]]

I - Val -- Loss: 2.581 | Acc: 55.703%
I - Confusion Matrix: [row->prediction - col->label]
[[ 294.    8.   14.   94.   38.]
 [   3.   74.   23.   12.   21.]
 [   5.  106.  122.   23.   75.]
 [  63.   20.   25.  197.   49.]
 [ 170.  162.  208.  248. 1032.]]

I - Maximum validation set accuracy in current training:  58.94
